{"id": "1506.00312", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2015", "title": "Copeland Dueling Bandits", "abstract": "A version of the dueling bandit problem is addressed in which a Condorcet winner may not exist. Two algorithms are proposed that instead seek to minimize regret with respect to the Copeland winner, which, unlike the Condorcet winner, is guaranteed to exist. The first, Copeland Confidence Bound (CCB), is designed for small numbers of arms, while the second, Scalable Copeland Bandits (SCB), works better for large-scale problems. We provide theoretical results bounding the regret accumulated by CCB and SCB, both substantially improving existing results. Such existing results either offer bounds of the form $O(K \\log T)$ but require restrictive assumptions, or offer bounds of the form $O(K^2 \\log T)$ without requiring such assumptions. Our results offer the best of both worlds: $O(K \\log T)$ bounds without restrictive assumptions.", "histories": [["v1", "Mon, 1 Jun 2015 00:44:37 GMT  (2140kb,D)", "http://arxiv.org/abs/1506.00312v1", "33 pages, 8 figures"]], "COMMENTS": "33 pages, 8 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["masrour zoghi", "zohar s karnin", "shimon whiteson", "maarten de rijke"], "accepted": true, "id": "1506.00312"}, "pdf": {"name": "1506.00312.pdf", "metadata": {"source": "CRF", "title": "Copeland Dueling Bandits", "authors": ["Masrour Zoghi", "Zohar Karnin", "Shimon Whiteson", "Maarten de Rijke"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year it is as far as it has ever been until the next round."}, {"heading": "2 Problem Setting", "text": "The K-Armed Duel Bandit Problem [1] is a modification of the K-Armed Bandit Problem [15]. The K-Armed Duel Bandit Problem is a variant where instead of drawing a single arm, we select a pair (ai, aj) and get one of them as a better choice, with the likelihood that ai will be selected with an unknown constant pij being the same. (The K-Armed Duel Bandit Problem is a variant where instead of drawing a single arm, we select a pair (ai, aj) and get one of them as a better choice, with the probability that ai will be selected with an unknown constant pij being the same. (The K-Armed Duel Bandit Problem is a variant of pji = 1 \u2212 pij."}, {"heading": "3 Related Work", "text": "Numerous methods have been proposed for the K-armed duel bandit problem, including Interleaved Filter [1], Beat the Mean [3], Relative Confidence Sampling [8], Relative Upper Confidence Bound (RUCB) [13], Doubler andMultiSBM [16], and mergeRUCB [17], all of which require the existence of a Condorcet winner and often come with limits of form O (K log T). However, as observed in [13] and Appendix C.1, real problems do not always have Condorcet winners. There is another group of algorithms that do not assume the existence of a Condorcet winner, but have limits of form O (K2 log T) in the Copeland environment: sensitivity analysis of VAriables for Generic Exploration (SAVAGE), which are of importance."}, {"heading": "4 Method", "text": "We now introduce two algorithms that find Copeland the winner."}, {"heading": "4.1 Copeland Confidence Bound (CCB)", "text": "CCB (see algorithm 1) is based on the principle of optimism, followed by pessimism: it maintains optimistic and pessimistic estimates of the preference matrix, i.e., matrices U and L (line 0.5). It uses U to choose an optimistic Copeland winner (lines 7-9 and 11-12), i.e., an arm that has a certain chance of becoming a Copeland winner. Then it uses L to choose an opponent (line 13), i.e. an arm that is considered likely to discredit the hypothesis that ac is actually a Copeland winner. More precisely, an optimistic estimate of the Copeland score of each arm is calculated using U (line 7) and ac is selected, with preference for those on a shortlist, Bt (line 11). Theses are weapons that, roughly speaking, have been optimistic winners throughout history. To maintain Copeland's score, the other CCB discovers that the CCB is lower than the more optimistic one."}, {"heading": "4.2 Scalable Copeland Bandits (SCB)", "text": "It is based on an arm identification algorithm described in Algorithm 2, designed for a PAC setting, i.e. it finds a -Copeland winner with probability 1 \u2212 \u03b4, although we are primarily interested in the case with = 0. Algorithm 2 is based on a reduction to a K-armed bandit problem where we have direct access to a loud version of the Copeland score - and the process of estimating the score of the arm ai is to compare ai with a random arm until it becomes clear which arm beats the other. The example complexity bound, which results in the repentance bound, is achieved by combining a limit for K-armed bandits and algorithm 2."}, {"heading": "5 Theoretical Results", "text": "Assuming that the number of Copeland winners and the number of losses of each Copeland winner are limited, 2 CCB takes the form of O (K2 + K log T), while SCB takes the form of O (K logK log T). Note that these limits are not directly comparable. If there are relatively few arms, CCB is expected to perform better. In contrast, if there are many arms, SCB is expected to be superior. Appendix A provides empirical evidence to support these expectations. In this section, we set the preference matrix the following condition: A There are no ties, i.e. for all pairs (ai, aj) with i 6 = j, we have Pij 6 = 0.5. This assumption is not very restrictive in practice. For example, in the ranking, we note that each arm corresponds to a Ranker, a complex and highly engineered system, so that we actually consider these differences to be less likely."}, {"heading": "5.1 Copeland Confidence Bounds (CCB)", "text": "To analyze this, we consider a K-armed Copeland Bandit problem with the arms a1,. < aK and preference matrix P = [pij], so that arms a1,.., aC are the Copeland winners, with C being the number of Copeland winners. During this section, we assume that the parameter \u03b1 in algorithm 1, unless otherwise specified, is met. We first define the relevant quantities: Definition 3. Given the above setting, we define: 3 1. Li: = {aj | pij < 0.5}, i.e. the arms to which ai loses, and LC: = L1 | 2."}, {"heading": "5.2 Scalable Copeland Bandits", "text": "We now turn to our regret for SCB, which reduces the K2 dependence in the additive constant of CCB regret to K logK. We start with the definition of relevant quantities: Definition 12. Given a K-armed Copeland problem and an arm, we define the following: 1. Remember that cpld (ai)) (1 +)) (1 +) (K \u2212 1) is called the standardized Copeland score. 2. ai is a -Copeland winner if 1 \u2212 cpld (ai) \u2264 (a1))) (1 +)) (1) (1) (max {cpld (a1) \u2212 cpld (ai) \u2212 cpld (ai)} (Hi: = 6 = i1)."}, {"heading": "6 Conclusion", "text": "In many cases where learning human behavior is involved, feedback is more reliable when provided in the form of paired preferences. If in doubt, the goal is to use such paired feedback to find the most desirable choice out of a range of options. Most existing work in this area assumes the existence of a Condorcet winner, i.e. an arm that beats all other arms with a probability of more than 0.5. Although these results have the advantage that the limits they set are linear in the number of arms, their main disadvantage is that the condorcet assumption is too restrictive."}, {"heading": "A Experimental Results", "text": "In this context, it should be noted that this project is a project, which is primarily a project."}, {"heading": "B Ranker Evaluation Details", "text": "In fact, it is the case that we will be able to get to grips with the problems mentioned in order to solve them."}, {"heading": "C Assumptions and Key Quantities", "text": "In this context, it should be noted that most people who speak up for the rights of women and men are indeed very well able to defend their rights and benefits. (...) Most of them are not able to defend their rights and benefits. (...) Most of them are able to defend their rights and benefits. (...) Most of them are able to defend their rights and benefits. (...) Most of them are able to defend their rights and benefits. (...) Most of them are not able to defend their rights and benefits. (...) Most of them are able to defend their benefits and benefits. (...) Most of them are able to defend their rights and benefits. (...) Most of them are able to defend their benefits and benefits. (...) Most of them are able to defend their benefits and benefits. (...) Most of them are able to defend their benefits and benefits. (...) Most of them are able to defend their benefits and benefits."}, {"heading": "D Background Material", "text": "Maximum Azuma-Hoeffding limit [40, paragraph A.1.3]: In the face of random variables X1,.., XN with common range [0, 1] satisfactory E [Xn | X1,..., Xn \u2212 1] = \u00b5, we define the subsums Sn = X1 + \u00b7 \u00b7 \u00b7 + Xn. Then we have for all a > 0 P (max n \u2264 NSn > n\u00b5 + a) \u2264 e \u2212 2a2 / NP (min n \u2264 NSn < n\u00b5 \u2212 a) \u2264 e \u2212 2a2 / NHere is a useful Lemma, which we will repeatedly refer to in our evidence: Lemma 17 (Lemma 1 in [13]). Let P: = [pij] be the preference matrix of a K-armed dueling bandit problem with weapons {a1,..., aK}. Then for each dueling bandit algorithm and each \u03b1 > 12 and > 0, havej (C)."}, {"heading": "E Proof of Proposition 4", "text": "Before we start with the proof, we point out the following two properties derived from assumption A in Section 5: < p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p (p) p = p (p) p = p (p) p = p (p) p = p (p) p (p) p (p) p = p (p) p = p (p) p = p (p) p = p (p) p = p (p) p) p = p (p) p) p (p) p) p (p) p) p (p) p) p (p) p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \") p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p"}, {"heading": "F Proof of Lemma 7", "text": "Let us begin with the following direct sequence of Proposition 4: Corollary 18: Corollary 18 = compared to these qualities (32%). Given that each T > C (3%) and each sub-interval of length N \u00b2 (T): = 1 = 0.5 \u00b0 C (T) + 1, with probability 1 \u2212 6, there is at least one time step, if there is such a Cpld (ac) = Cpld (ac) and T, if algorithm 1 has not compared to itself: i.e. c and d in algorithm 1, with probability 1 \u2212 6, there is at most one such Cpld (T) between C (6%) and T, if algorithm 1 has not compared to itself: i.e. c and d in algorithm 1, we do not have the satisfaction c = d \u2264 C. In other words, in this time frame, in each sub-interval of length N (T): = 6 \u00b0 C = compared winners against themselves."}, {"heading": "G Proof of Lemma 8", "text": "The idea of the argument is sketched in the following sequence of facts: 1 - 7 - we know that with probability 1 - 2 > C and all times t > Ti > C and all times t > Ti. \"We will sketch the idea of the argument in the following sequence of facts: 1 - 2 - 2 - 2 - 3 - 3 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 5 5 5 5 5 5 5 5 5 - 5 5 5 5 5 5 5 5 - 5 5 5 5 5 5 5 5 - 5 5 5 5 5 5 - 5 5 5 5 - 5 5 5 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 -"}, {"heading": "H A Scalable Solution to the Copeland Bandit Problem", "text": "To simplify the proof, we begin with the solution of a slightly simpler variant of Lemma 14, in which the queries are only approximate erroneous. Specifically, instead of having a query to the pair (ai, aj), we can aim for a result of a Bernoulli r.v. with an expected value of pij that such a query simply provides the answer to the question of whether pij > 0.5. Obviously, a solution can be obtained using K \u2212 1) / 2 many queries, but we are aiming for a solution with a query complexity linearly in K. In this section, we will prove the following. Given the K arms and a parameter, Algorithm 2 finds a (1 +) approximate best weapon with a probability of at least 1 \u2212 2, by looking at mostlog (K / 3) \u00b7 O (K) \u00b7 O (K log (K) + min {K2 (1 \u2212 cld (1) in general)) many."}, {"heading": "I KL-based approximate best arm identification algorithm", "text": "Algorithm 4 KL-best arm identification problem using confidence bounds based on Chernoff's inequality stated w.r.t the KL divergence of two random variables. (1 \u2212 p) # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}], "references": [{"title": "The K-armed dueling bandits problem", "author": ["Y. Yue", "J. Broder", "R. Kleinberg", "T. Joachims"], "venue": "Journal of Computer and System Sciences, 78(5),", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Optimizing search engines using clickthrough data", "author": ["T. Joachims"], "venue": "KDD,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2002}, {"title": "Beat the mean bandit", "author": ["Y. Yue", "T. Joachims"], "venue": "ICML,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Balancing exploration and exploitation in listwise and pairwise online learning to rank for information retrieval", "author": ["K. Hofmann", "S. Whiteson", "M. de Rijke"], "venue": "Information Retrieval,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Multileaved comparisons for fast online evaluation", "author": ["A. Schuth", "F. Sietsma", "S. Whiteson", "D. Lefortier", "M. de Rijke"], "venue": "In CIKM,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Toward predicting the outcome of an A/B experiment for search relevance", "author": ["L. Li", "J. Kim", "I. Zitouni"], "venue": "WSDM,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Relative confidence sampling for efficient on-line ranker evaluation", "author": ["M. Zoghi", "S. Whiteson", "M. de Rijke", "R. Munos"], "venue": "In WSDM,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "A new monotonic, clone-independent, reversal symmetric, and Condorcet-consistent single-winner election method", "author": ["M. Schulze"], "venue": "Social Choice and Welfare, 36(2):267\u2013303,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Generic exploration and k-armed voting bandits", "author": ["T. Urvoy", "F. Clerot", "R. F\u00e9raud", "S. Naamane"], "venue": "ICML,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Top-k selection based on adaptive sampling of noisy preferences", "author": ["R. Busa-Fekete", "B. Sz\u00f6r\u00e9nyi", "P. Weng", "W. Cheng", "E. H\u00fcllermeier"], "venue": "ICML,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "PAC rank elicitation through adaptive sampling of stochastic pairwise preferences", "author": ["R. Busa-Fekete", "B. Sz\u00f6r\u00e9nyi", "E. H\u00fcllermeier"], "venue": "AAAI,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Relative upper confidence bound for the K-armed dueling bandits problem", "author": ["M. Zoghi", "S. Whiteson", "R. Munos", "M. de Rijke"], "venue": "In ICML,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Online controlled experiments at large scale", "author": ["R. Kohavi", "A. Deng", "B. Frasca", "T. Walker", "Y. Xu", "N. Pohlmann"], "venue": "KDD,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W.R. Thompson"], "venue": "Biometrika, pages 285\u2013294,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1933}, {"title": "Reducing dueling bandits to cardinal bandits", "author": ["N. Ailon", "Z. Karnin", "T. Joachims"], "venue": "ICML,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "MergeRUCB: A method for large-scale online ranker evaluation", "author": ["M. Zoghi", "S. Whiteson", "M. de Rijke"], "venue": "In WSDM,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Iterative ranking from pair-wise comparisons", "author": ["S. Negahban", "S. Oh", "D. Shah"], "venue": "NIPS,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Contextual dueling bandits", "author": ["M. Dud\u0131\u0301k", "K. Hofmann", "R.E. Schapire", "A. Slivkins", "M. Zoghi"], "venue": "In COLT,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Discrete prediction games with arbitrary feedback and loss", "author": ["A. Piccolboni", "C. Schindelhauer"], "venue": "COLT,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2001}, {"title": "An adaptive algorithm for finite stochastic partial monitoring", "author": ["G. Bart\u00f3k", "N. Zolghadr", "C. Szepesv\u00e1ri"], "venue": "ICML,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Kullback\u2013leibler upper confidence bounds for optimal sequential allocation", "author": ["O. Capp\u00e9", "A. Garivier", "O. Maillard", "R. Munos", "G. Stoltz"], "venue": "The Annals of Statistics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Multi-armed bandits in metric space", "author": ["R. Kleinberg", "A. Slivkins", "E. Upfa"], "venue": "STOC,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "X-armed bandits", "author": ["S. Bubeck", "R. Munos", "G. Stoltz", "C. Szepesvari"], "venue": "JMLR, 12,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Gaussian process optimization in the bandit setting: No regret and experimental design", "author": ["N. Srinivas", "A. Krause", "S.M. Kakade", "M. Seeger"], "venue": "ICML,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Optimistic optimization of a deterministic function without the knowledge of its smoothness", "author": ["R. Munos"], "venue": "NIPS,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Convergence rates of efficient global optimization algorithms", "author": ["A.D. Bull"], "venue": "JMLR, 12,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Exponential regret bounds for Gaussian process bandits with deterministic observations", "author": ["N. de Freitas", "A. Smola", "M. Zoghi"], "venue": "In ICML,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Stochastic simultaneous optimistic optimization", "author": ["M. Valko", "A. Carpentier", "R. Munos"], "venue": "ICML,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Interactively optimizing information retrieval systems as a dueling bandits problem", "author": ["Y. Yue", "T. Joachims"], "venue": "ICML,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Axiomatic foundations for ranking systems", "author": ["A. Altman", "M. Tennenholtz"], "venue": "JAIR,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "Introduction to Information Retrieval", "author": ["C. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": "Cambridge University Press,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "How does clickthrough data reflect retrieval quality", "author": ["F. Radlinski", "M. Kurup", "T. Joachims"], "venue": "In CIKM,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2008}, {"title": "A probabilistic method for inferring preferences from clicks", "author": ["K. Hofmann", "S. Whiteson", "M. de Rijke"], "venue": "In CIKM", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "An experimental comparison of click position-bias models", "author": ["N. Craswell", "O. Zoeter", "M. Taylor", "B. Ramsey"], "venue": "WSDM \u201908, pages 87\u201394,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficient multiple-click models in web search", "author": ["F. Guo", "C. Liu", "Y. Wang"], "venue": "WSDM \u201909, pages 124\u2013131, New York, NY, USA,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "Fidelity, soundness, and efficiency of interleaved comparison methods", "author": ["K. Hofmann", "S. Whiteson", "M. de Rijke"], "venue": "ACM Transactions on Information Systems,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "Mathematical games: The paradox of the nontransitive dice and the elusive principle of indifference", "author": ["M. Gardner"], "venue": "Scientific American, 223:110\u2013114,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1970}, {"title": "A survey of preference-based online learning with bandit algorithms", "author": ["R. Busa-Fekete", "E. H\u00fcllermeier"], "venue": "Algorithmic Learning Theory, pages 18\u201339. Springer,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": "Cambridge University Press,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction The dueling bandit problem [1] arises naturally in domains where feedback is more reliable when given as a pairwise preference (e.", "startOffset": 42, "endOffset": 45}, {"referenceID": 1, "context": "Examples include ranker evaluation [2\u20134] in information retrieval, ad placement and recommender systems.", "startOffset": 35, "endOffset": 40}, {"referenceID": 2, "context": "Examples include ranker evaluation [2\u20134] in information retrieval, ad placement and recommender systems.", "startOffset": 35, "endOffset": 40}, {"referenceID": 3, "context": "Examples include ranker evaluation [2\u20134] in information retrieval, ad placement and recommender systems.", "startOffset": 35, "endOffset": 40}, {"referenceID": 4, "context": "For example, in industrial ranker evaluation [6], when many rankers must be compared, each comparison corresponds to a costly live experiment and thus the potential for failure if no Condorcet winner exists is unacceptable [7].", "startOffset": 45, "endOffset": 48}, {"referenceID": 5, "context": "For example, in industrial ranker evaluation [6], when many rankers must be compared, each comparison corresponds to a costly live experiment and thus the potential for failure if no Condorcet winner exists is unacceptable [7].", "startOffset": 223, "endOffset": 226}, {"referenceID": 7, "context": "The non-existence of the Condorcet winner has been investigated extensively in social choice theory, where numerous definitions have been proposed, without a clear contender for the most suitable resolution [9].", "startOffset": 207, "endOffset": 210}, {"referenceID": 8, "context": ", SAVAGE [10], PBR [11] and RankEl [12], which use some of the notions proposed by social choice theorists, such as the Copeland score or the Borda score to measure the quality of each arm, hence determining what constitutes the best arm (or more generally the top-k arms).", "startOffset": 9, "endOffset": 13}, {"referenceID": 9, "context": ", SAVAGE [10], PBR [11] and RankEl [12], which use some of the notions proposed by social choice theorists, such as the Copeland score or the Borda score to measure the quality of each arm, hence determining what constitutes the best arm (or more generally the top-k arms).", "startOffset": 19, "endOffset": 23}, {"referenceID": 10, "context": ", SAVAGE [10], PBR [11] and RankEl [12], which use some of the notions proposed by social choice theorists, such as the Copeland score or the Borda score to measure the quality of each arm, hence determining what constitutes the best arm (or more generally the top-k arms).", "startOffset": 35, "endOffset": 39}, {"referenceID": 11, "context": "The first algorithm, called Copeland Confidence Bound (CCB), is inspired by the recently proposed Relative Upper Confidence Bound method [13], but modified and extended to address the unique challenges that arise when no Condorcet winner exists.", "startOffset": 137, "endOffset": 141}, {"referenceID": 12, "context": "For example, at Bing, 200 experiments are run concurrently on any given day [14], in which case the duration of the experiment needs to be longer than the age of the universe in nanoseconds before K log T becomes significant in comparison to K.", "startOffset": 76, "endOffset": 80}, {"referenceID": 0, "context": "The K-armed dueling bandit problem [1] is a modification of the K-armed bandit problem [15].", "startOffset": 35, "endOffset": 38}, {"referenceID": 13, "context": "The K-armed dueling bandit problem [1] is a modification of the K-armed bandit problem [15].", "startOffset": 87, "endOffset": 91}, {"referenceID": 8, "context": "Most previous work assumes the existence of a Condorcet winner [10]: an arm, which without loss of generality we label a1, such that p1i > 12 for all i > 1.", "startOffset": 63, "endOffset": 67}, {"referenceID": 6, "context": "However, Condorcet winners do not always exist [8, 13].", "startOffset": 47, "endOffset": 54}, {"referenceID": 11, "context": "However, Condorcet winners do not always exist [8, 13].", "startOffset": 47, "endOffset": 54}, {"referenceID": 0, "context": "3 Related Work Numerous methods have been proposed for the K-armed dueling bandit problem, including Interleaved Filter [1], Beat the Mean [3], Relative Confidence Sampling [8], Relative Upper Confidence Bound (RUCB) [13], Doubler and", "startOffset": 120, "endOffset": 123}, {"referenceID": 2, "context": "3 Related Work Numerous methods have been proposed for the K-armed dueling bandit problem, including Interleaved Filter [1], Beat the Mean [3], Relative Confidence Sampling [8], Relative Upper Confidence Bound (RUCB) [13], Doubler and", "startOffset": 139, "endOffset": 142}, {"referenceID": 6, "context": "3 Related Work Numerous methods have been proposed for the K-armed dueling bandit problem, including Interleaved Filter [1], Beat the Mean [3], Relative Confidence Sampling [8], Relative Upper Confidence Bound (RUCB) [13], Doubler and", "startOffset": 173, "endOffset": 176}, {"referenceID": 11, "context": "3 Related Work Numerous methods have been proposed for the K-armed dueling bandit problem, including Interleaved Filter [1], Beat the Mean [3], Relative Confidence Sampling [8], Relative Upper Confidence Bound (RUCB) [13], Doubler and", "startOffset": 217, "endOffset": 221}, {"referenceID": 14, "context": "MultiSBM [16], and mergeRUCB [17], all of which require the existence of a Condorcet winner, and often come with bounds of the form O(K log T ).", "startOffset": 9, "endOffset": 13}, {"referenceID": 15, "context": "MultiSBM [16], and mergeRUCB [17], all of which require the existence of a Condorcet winner, and often come with bounds of the form O(K log T ).", "startOffset": 29, "endOffset": 33}, {"referenceID": 11, "context": "However, as observed in [13] and Appendix C.", "startOffset": 24, "endOffset": 28}, {"referenceID": 8, "context": "There is another group of algorithms that do not assume the existence of a Condorcet winner, but have bounds of the form O(K2 log T ) in the Copeland setting: Sensitivity Analysis of VAriables for Generic Exploration (SAVAGE) [10], Preference-Based Racing (PBR) [11] and Rank Elicitation (RankEl) [12].", "startOffset": 226, "endOffset": 230}, {"referenceID": 9, "context": "There is another group of algorithms that do not assume the existence of a Condorcet winner, but have bounds of the form O(K2 log T ) in the Copeland setting: Sensitivity Analysis of VAriables for Generic Exploration (SAVAGE) [10], Preference-Based Racing (PBR) [11] and Rank Elicitation (RankEl) [12].", "startOffset": 262, "endOffset": 266}, {"referenceID": 10, "context": "There is another group of algorithms that do not assume the existence of a Condorcet winner, but have bounds of the form O(K2 log T ) in the Copeland setting: Sensitivity Analysis of VAriables for Generic Exploration (SAVAGE) [10], Preference-Based Racing (PBR) [11] and Rank Elicitation (RankEl) [12].", "startOffset": 297, "endOffset": 301}, {"referenceID": 8, "context": "In addition to the above, bounds have been proven for other notions of winners, including Borda [10\u201312], Random Walk [11, 18], and very recently von Neumann [19].", "startOffset": 96, "endOffset": 103}, {"referenceID": 9, "context": "In addition to the above, bounds have been proven for other notions of winners, including Borda [10\u201312], Random Walk [11, 18], and very recently von Neumann [19].", "startOffset": 96, "endOffset": 103}, {"referenceID": 10, "context": "In addition to the above, bounds have been proven for other notions of winners, including Borda [10\u201312], Random Walk [11, 18], and very recently von Neumann [19].", "startOffset": 96, "endOffset": 103}, {"referenceID": 9, "context": "In addition to the above, bounds have been proven for other notions of winners, including Borda [10\u201312], Random Walk [11, 18], and very recently von Neumann [19].", "startOffset": 117, "endOffset": 125}, {"referenceID": 16, "context": "In addition to the above, bounds have been proven for other notions of winners, including Borda [10\u201312], Random Walk [11, 18], and very recently von Neumann [19].", "startOffset": 117, "endOffset": 125}, {"referenceID": 17, "context": "In addition to the above, bounds have been proven for other notions of winners, including Borda [10\u201312], Random Walk [11, 18], and very recently von Neumann [19].", "startOffset": 157, "endOffset": 161}, {"referenceID": 18, "context": "A related setting is that of partial monitoring games [20].", "startOffset": 54, "endOffset": 58}, {"referenceID": 19, "context": "In [21], the authors present problem-dependent bounds from which a regret bound of the formO(K2 log T ) can be deduced for the dueling bandit problem, whereas our work achieves a linear dependence in K.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "To this end, we use the KL-based arm-elimination algorithm (a slight modification of Algorithm 2 in [22]) described in Algorithm 4 in Appendix I.", "startOffset": 100, "endOffset": 104}, {"referenceID": 0, "context": "Now that we have the high probability regret bound given in Theorem 11, we can deduce the expected regret result claimed in (1) for \u03b1 > 1, as a corollary by integrating \u03b4 over the interval [0, 1].", "startOffset": 189, "endOffset": 195}, {"referenceID": 21, "context": "Given that both our algorithms utilize confidence bounds to make their choices, we anticipate that continuous-armed UCB-style algorithms like those proposed in [23\u201329] can be combined with our ideas to produce a solution to the continuous-armed Copeland bandit problem that does not rely on the convexity assumptions made by algorithms such as the one proposed in [30].", "startOffset": 160, "endOffset": 167}, {"referenceID": 22, "context": "Given that both our algorithms utilize confidence bounds to make their choices, we anticipate that continuous-armed UCB-style algorithms like those proposed in [23\u201329] can be combined with our ideas to produce a solution to the continuous-armed Copeland bandit problem that does not rely on the convexity assumptions made by algorithms such as the one proposed in [30].", "startOffset": 160, "endOffset": 167}, {"referenceID": 23, "context": "Given that both our algorithms utilize confidence bounds to make their choices, we anticipate that continuous-armed UCB-style algorithms like those proposed in [23\u201329] can be combined with our ideas to produce a solution to the continuous-armed Copeland bandit problem that does not rely on the convexity assumptions made by algorithms such as the one proposed in [30].", "startOffset": 160, "endOffset": 167}, {"referenceID": 24, "context": "Given that both our algorithms utilize confidence bounds to make their choices, we anticipate that continuous-armed UCB-style algorithms like those proposed in [23\u201329] can be combined with our ideas to produce a solution to the continuous-armed Copeland bandit problem that does not rely on the convexity assumptions made by algorithms such as the one proposed in [30].", "startOffset": 160, "endOffset": 167}, {"referenceID": 25, "context": "Given that both our algorithms utilize confidence bounds to make their choices, we anticipate that continuous-armed UCB-style algorithms like those proposed in [23\u201329] can be combined with our ideas to produce a solution to the continuous-armed Copeland bandit problem that does not rely on the convexity assumptions made by algorithms such as the one proposed in [30].", "startOffset": 160, "endOffset": 167}, {"referenceID": 26, "context": "Given that both our algorithms utilize confidence bounds to make their choices, we anticipate that continuous-armed UCB-style algorithms like those proposed in [23\u201329] can be combined with our ideas to produce a solution to the continuous-armed Copeland bandit problem that does not rely on the convexity assumptions made by algorithms such as the one proposed in [30].", "startOffset": 160, "endOffset": 167}, {"referenceID": 27, "context": "Given that both our algorithms utilize confidence bounds to make their choices, we anticipate that continuous-armed UCB-style algorithms like those proposed in [23\u201329] can be combined with our ideas to produce a solution to the continuous-armed Copeland bandit problem that does not rely on the convexity assumptions made by algorithms such as the one proposed in [30].", "startOffset": 160, "endOffset": 167}, {"referenceID": 28, "context": "Given that both our algorithms utilize confidence bounds to make their choices, we anticipate that continuous-armed UCB-style algorithms like those proposed in [23\u201329] can be combined with our ideas to produce a solution to the continuous-armed Copeland bandit problem that does not rely on the convexity assumptions made by algorithms such as the one proposed in [30].", "startOffset": 364, "endOffset": 368}, {"referenceID": 10, "context": "Finally, it is also interesting to expand our results to handle scores other than the Copeland score, such as an insensitive variant of the Copeland score (as in [12]), or completely different notions of winners, such as the Borda, the Random Walk or the von Neumann winners (see, e.", "startOffset": 162, "endOffset": 166}, {"referenceID": 17, "context": ", [19, 31]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 29, "context": ", [19, 31]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 0, "context": "[1] Y.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] T.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Y.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] K.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] L.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] W.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] N.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] O.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[24] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] N.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[27] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[28] N.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[29] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[30] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[31] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[32] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[33] F.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[34] K.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[35] N.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[36] F.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[37] K.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[38] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[39] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[40] N.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "The first is a 5-armed problem arising from ranker evaluation in the field of information retrieval (IR) [32].", "startOffset": 105, "endOffset": 109}, {"referenceID": 2, "context": "All three experiments follow the experimental approach in [3, 13] and use the given preference matrix to simulate comparisons between each pair of arms (ai, aj) by drawing samples from Bernoulli random variables with mean pij .", "startOffset": 58, "endOffset": 65}, {"referenceID": 11, "context": "All three experiments follow the experimental approach in [3, 13] and use the given preference matrix to simulate comparisons between each pair of arms (ai, aj) by drawing samples from Bernoulli random variables with mean pij .", "startOffset": 58, "endOffset": 65}, {"referenceID": 11, "context": "We compare our two proposed algorithms against the state of the art K-armed dueling bandit algorithm, RUCB [13], and Copeland SAVAGE, PBR and RankEl.", "startOffset": 107, "endOffset": 111}, {"referenceID": 30, "context": "The first experiment uses a 5-armed problem arising from ranker evaluation in the field of information retrieval (IR) [32], detailed in Appendix B.", "startOffset": 118, "endOffset": 122}, {"referenceID": 8, "context": "We omit SAVAGE, PBR and RankEl from this experiment because they scale poorly in the number of arms [10\u201312].", "startOffset": 100, "endOffset": 107}, {"referenceID": 9, "context": "We omit SAVAGE, PBR and RankEl from this experiment because they scale poorly in the number of arms [10\u201312].", "startOffset": 100, "endOffset": 107}, {"referenceID": 10, "context": "We omit SAVAGE, PBR and RankEl from this experiment because they scale poorly in the number of arms [10\u201312].", "startOffset": 100, "endOffset": 107}, {"referenceID": 31, "context": "One effective way to achieve this is to use interleaved comparisons [33], which interleave the ranked lists of documents proposed by two rankers and present the resulting list to the user, whose subsequent click feedback is used to infer a noisy preference for one of the rankers.", "startOffset": 68, "endOffset": 72}, {"referenceID": 3, "context": "The ranker evaluation task in this context corresponds to determining which single feature constitutes the best ranker [4].", "startOffset": 119, "endOffset": 122}, {"referenceID": 32, "context": "To compare a pair of rankers, we use probabilistic interleave (PI) [34], a recently developed method for interleaved comparisons.", "startOffset": 67, "endOffset": 71}, {"referenceID": 32, "context": "To model the user\u2019s click behavior on the resulting interleaved lists, we employ a probabilistic user model [34, 35] that uses as input the manual labels (classifying documents as relevant or not for given queries) provided with the MSLR dataset.", "startOffset": 108, "endOffset": 116}, {"referenceID": 33, "context": "To model the user\u2019s click behavior on the resulting interleaved lists, we employ a probabilistic user model [34, 35] that uses as input the manual labels (classifying documents as relevant or not for given queries) provided with the MSLR dataset.", "startOffset": 108, "endOffset": 116}, {"referenceID": 34, "context": "Queries are sampled randomly and clicks are generated probabilistically by conditioning on these assessments in a way that resembles the behavior of an actual user [36].", "startOffset": 164, "endOffset": 168}, {"referenceID": 35, "context": "Specifically, we employ an informational click model in our ranker evaluation experiments [37].", "startOffset": 90, "endOffset": 94}, {"referenceID": 35, "context": "The informational click model is one of the three click models utilized in the ranker evaluation literature, along with the perfect and navigational click models [37].", "startOffset": 162, "endOffset": 166}, {"referenceID": 2, "context": "Following [3, 13], we first use the above approach to estimate the comparison probabilities pij for each pair of rankers and then use these probabilities to simulate comparisons between rankers.", "startOffset": 10, "endOffset": 17}, {"referenceID": 11, "context": "Following [3, 13], we first use the above approach to estimate the comparison probabilities pij for each pair of rankers and then use these probabilities to simulate comparisons between rankers.", "startOffset": 10, "endOffset": 17}, {"referenceID": 11, "context": "We hypothesize that this is because the informational click model explores more of the list of ranked documents than the navigational click model, which was used in [13], and so it is more likely to encounter non-transitivity phenomena of the sort described in [38].", "startOffset": 165, "endOffset": 169}, {"referenceID": 36, "context": "We hypothesize that this is because the informational click model explores more of the list of ranked documents than the navigational click model, which was used in [13], and so it is more likely to encounter non-transitivity phenomena of the sort described in [38].", "startOffset": 261, "endOffset": 265}, {"referenceID": 17, "context": "The Copeland winner, as discussed in this paper, and the von Neumann winner [19] satisfy this property, while the Borda (a.", "startOffset": 76, "endOffset": 80}, {"referenceID": 37, "context": "winners [39] do not.", "startOffset": 8, "endOffset": 12}, {"referenceID": 17, "context": "In the case of the von Neumann winner, which is defined as a probability distribution over the set of arms [19], we used the support of the distribution (i.", "startOffset": 107, "endOffset": 111}, {"referenceID": 0, "context": ", XN with common range [0, 1] satisfying E[Xn|X1, .", "startOffset": 23, "endOffset": 29}, {"referenceID": 11, "context": "Here, we will quote a useful Lemma that we will refer to repeatedly in our proofs: Lemma 17 (Lemma 1 in [13]).", "startOffset": 104, "endOffset": 108}, {"referenceID": 0, "context": "Algorithm 4 KL-best arm identification Input: Access to oracle giving a noisy approximation of the reward of arm i for K arms, success probability \u03b4 > 0, approximation parameter > 0 1: for all i \u2208 [K] do 2: T = 1 3: Si \u2190 reward(i) 4: Ii \u2190 [0, 1] 5: end for 6: B \u2190 [K] 7: t\u2190 2 8: while 1\u2212maxi\u2208B min Ii 1\u2212maxi\u2208B max Ii > (1 + ) do 9: For all i \u2208 B, Si \u2190 Si + reward(i) 10: For all i \u2208 B, let Ii = {q \u2208 [0, 1], t \u00b7 d(i t , q) \u2264 ln(4tK/\u03b4) + 2 ln ln(t)} 11: For all i \u2208 B for which there exist some j \u2208 B with max{q \u2208 Ii} < min{q \u2208 Ij}, remove i from B.", "startOffset": 239, "endOffset": 245}, {"referenceID": 0, "context": "Algorithm 4 KL-best arm identification Input: Access to oracle giving a noisy approximation of the reward of arm i for K arms, success probability \u03b4 > 0, approximation parameter > 0 1: for all i \u2208 [K] do 2: T = 1 3: Si \u2190 reward(i) 4: Ii \u2190 [0, 1] 5: end for 6: B \u2190 [K] 7: t\u2190 2 8: while 1\u2212maxi\u2208B min Ii 1\u2212maxi\u2208B max Ii > (1 + ) do 9: For all i \u2208 B, Si \u2190 Si + reward(i) 10: For all i \u2208 B, let Ii = {q \u2208 [0, 1], t \u00b7 d(i t , q) \u2264 ln(4tK/\u03b4) + 2 ln ln(t)} 11: For all i \u2208 B for which there exist some j \u2208 B with max{q \u2208 Ii} < min{q \u2208 Ij}, remove i from B.", "startOffset": 400, "endOffset": 406}], "year": 2015, "abstractText": "A version of the dueling bandit problem is addressed in which a Condorcet winner may not exist. Two algorithms are proposed that instead seek to minimize regret with respect to the Copeland winner, which, unlike the Condorcet winner, is guaranteed to exist. The first, Copeland Confidence Bound (CCB), is designed for small numbers of arms, while the second, Scalable Copeland Bandits (SCB), works better for large-scale problems. We provide theoretical results bounding the regret accumulated by CCB and SCB, both substantially improving existing results. Such existing results either offer bounds of the formO(K log T ) but require restrictive assumptions, or offer bounds of the formO(K log T ) without requiring such assumptions. Our results offer the best of both worlds: O(K log T ) bounds without restrictive assumptions.", "creator": "LaTeX with hyperref package"}}}