{"id": "1506.00333", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2015", "title": "Learning to Answer Questions from Image Using Convolutional Neural Network", "abstract": "In this paper, we propose to employ the convolutional neural network (CNN) for learning to answer questions from the image. Our proposed CNN provides an end-to-end framework for learning not only the image representation, the composition model for question, but also the inter-modal interaction between the image and question, for the generation of answer. More specifically, the proposed model consists of three components: an image CNN to extract the image representation, one sentence CNN to encode the question, and one multimodal convolution layer to fuse the multimodal input of the image and question to obtain the joint representation for the classification in the space of candidate answer words. We demonstrate the efficacy of our proposed model on DAQUAR and COCO-QA datasets, two datasets recently created for the image question answering (QA), with performance substantially outperforming the state-of-the-arts.", "histories": [["v1", "Mon, 1 Jun 2015 03:09:49 GMT  (529kb,D)", "https://arxiv.org/abs/1506.00333v1", "10 pages, 3 figures"], ["v2", "Fri, 13 Nov 2015 09:54:59 GMT  (887kb,D)", "http://arxiv.org/abs/1506.00333v2", "7 pages, 4 figures. Accepted by AAAI 2016"]], "COMMENTS": "10 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CL cs.CV cs.LG cs.NE", "authors": ["lin ma", "zhengdong lu", "hang li"], "accepted": true, "id": "1506.00333"}, "pdf": {"name": "1506.00333.pdf", "metadata": {"source": "CRF", "title": "Learning to Answer Questions From Image Using Convolutional Neural Network", "authors": ["Lin Ma", "Zhengdong Lu", "Hang Li"], "emails": ["forest.linma@gmail.com", "Lu.Zhengdong@huawei.com", "HangLi@huawei.com", "HL@huawei.com"], "sections": [{"heading": "Introduction", "text": "Recently, multimodal learning between image and language (Ma et al. 2015; Makamura et al. 2013; Xu et al. 2015b) has become an increasingly popular field of research in artificial intelligence (AI), with rapid advances in the tasks of bidirectional image and typesetting (Frome et al. 2013; Socher et al. 2014; Klein et al. 2015; Karpathy, Joulin and Li 2014; Ordonez, Kulkarni, and Berg 2011), and automatic caption (Chen and Zitnick 2014; Karpathy et al. 2014; Donahue et al. 2014; Fang et al. 2014; Kiros, Salakhutdinov, and Zemel 2014a; Kiros, Salakhutdinov, and Zemel 2014b, and Zemel 2014b; Klein et al. 2015; Mao et al."}, {"heading": "Related Work", "text": "Recently, the visual Turing test, an open question based on real-world images, was proposed to resemble the famous Turing test. In (Gao et al. 2015), a human judge is presented with an image, a question and the answer to the question by mathematical models or human commentators. Based on the answer, the human judge must determine whether the answer is given by a human being (i.e. pass the test) or by a machine (i.e. fail the test). Geman et al. (Geman et al. 2015) is proposed to produce a stochastic sequence of binary questions from a given test image, with the answer to the question limited to yes / no. Malinowski et al."}, {"heading": "Proposed CNN for Image QA", "text": "In image quality, the problem is to predict the answer that contains a given question q and the associated image I: a = argmax a \u043e\u043b\u0430\u0441\u0442\u0430p (a | q, I; \u03b8), (1) which contains all the answers. \u03b8 denotes all the parameters for performing the image quality. In order to make a reliable prediction of the answer, the question q and the image I must be adequately represented. Based on their representations, the relationships between the two multimodal inputs are further learned to produce the answer. In this essay, CNN's ability to not only model the image and the sentence individually, but also to capture the relationships and interactions between them is utilized. As illustrated in Figure 2, our proposed CNN image quality fragment consists of three individual CNNs: a CNN image encoding the image content, a CNN sentence generating the question formation, a multimodal folding layer that narrows the image and maximizes the joint representation to the end, and finally a CNN layer generating the joint representation into three."}, {"heading": "Image CNN", "text": "There are many research papers in which CNNs are used to produce image representations that provide the most advanced performance in the field of image recognition (Simonyan and Zisserman 2014; Szegedy et al. 2015).In this paper, we use the work (Simonyan and Zisserman 2014) to encode the image content for our image QA model: \u03bdim = \u03c3 (wim (CNNim (I)) + bim), (2) where \u03c3 is a nonlinear activation function, such as Sigmoid and ReLU (Dahl, Sainath and Hinton 2013).CNNim takes the image as an input and outputs a fixed length vector as an image representation. In this essay, by cutting out the top Softmax property and the last ReLU layer of CNN (Simonyan and Zisserman 2014), the output of the last completely connected layer is considered as an image representation, whereas the representation of the matrix of the 96-length vector with the 40xmeter is clearly a smaller dimension of the image 96."}, {"heading": "Sentence CNN", "text": "Since most folding models (Lecun and Bengio 1995; Kalchbrenner, Grefenstette, and Blunsom 2014) are called into question, we consider the folding unit with a local \"receptive field\" and divided weights to capture the rich structures and compositional properties between consecutive words. CNN's sentence to generate the question is illustrated in Figure 3 - for a given question with each word presented as word embedding (Mikolov et al. 2013), CNN's sentence is executed with multiple layers of folding and compositional properties to represent the question. \u2212 Convolution unit for each word presented as word embedding (Mikolov et al. 2013), the sentence CNN is executed with multiple layers of folding and maximum pooling to represent the question. \u2212 Convolution unit for a sequential input of two \u2212 oltic inputs for the feature card of type 'f' (def)."}, {"heading": "Multimodal Convolution Layer", "text": "We design a new multimodal folding layer over it, as shown in Figure 4, which merges the multimodal inputs to generate their common representation for further prediction of responses, but treats the image representation as an individual semantic component. Based on the image representation and the two successive semantic components from the question side, the multimodal convolutions are performed to capture the interactions and relationships between the two multimodal inputs. ~ \u03bdinmm def = empiqt, (7) \u03bdi + 1qt, (mm, f) def = \u03c3 (mm, f) owski in mm + b (mm, f)), (8), where the multimodal constellations of the multimodal folding unit. This model is the segment of the question."}, {"heading": "Experiments", "text": "In this section, we first present the configurations of our CNN model for image quality and how we train the proposed CNN model, then present the publicly available image QA data sets and evaluation measurements, and finally present and analyze the experimental results."}, {"heading": "Configurations and Training", "text": "The CNN sentence is based on a fixed architecture that must be designed to take into account the maximum length of the questions. In this essay, the maximum length of the question is 38. Word embedding is determined by the skip program model (Mikolov et al. 2013) with the dimension as 50. We use the VGG network (Simonyan and Zisserman 2014) as CNN image. The dimension of the word is set as 400. Multimodal CNN takes the image and sentence representations as input and generates the common representation with the number of feature cards as 400. The proposed CNN model is trained with stochastic gradient descent with mini-batches of 100 for optimization, selecting the negative log probability as loss. During the training process, all parameters, including the image parameters of the image layer are adjusted with the maximum image embedding, not with the image layer CNN adjustability."}, {"heading": "Image QA Datasets", "text": "We test and compare our proposed CNN model against the public QA databases, in particular the DAQUAR-All (Malinowski and Fritz 2014a) and COCO-QA (Ren, Kiros and Zemel 2015) datasets. DAQUAR-All (Malinowski and Fritz 2014a) This dataset consists of 6,795 training samples and 5,673 test samples generated from 795 and 654 images, respectively. Images come from all 894 object categories. In this dataset there are mainly three types of questions, in particular object type, object color and number of objects. The answer can be a single word or several words. DAQUAR-Reduced (Malinowski and Fritz 2014a) This dataset is a reduced version of DAQUAR-All consisting of 3,876 training samples and 297 test samples. Images are limited to 37 object categories. Only 25 images are used for test sample generation."}, {"heading": "Evaluation Measurements", "text": "An easy way to evaluate image quality is to use accuracy, which measures the ratio of the correctly answered test questions to the overall test questions. In addition to accuracy, the Wu-Palmer similarity (WUPS) (Wu and Palmer 1994; Malinowski and Fritz 2014a) is also used to measure the performance of different models in the image quality task. WUPS calculates the similarity between two words based on their common subsequence in a taxonomy tree. A threshold value is required to calculate WUPS. As in the previous work (Ren, Kiros and Zemel 2015; Malinowski and Fritz 2014a; Malinowski, Rohrbach and Fritz 2015), the threshold parameters are used for the measurements WUPS @ 0.0 and WUPS @ 0.9, respectively."}, {"heading": "Experimental Results and Analysis", "text": "In fact, it's so that most people are able to survive and survive themselves, \"he told the German Press Agency in an interview with\" Welt am Sonntag \":\" I don't think the world is able to fix the world. \"He added:\" I don't think the world is able to fix the world. \"He added:\" I don't think the world is OK, that it is OK, that it is OK, that it is OK, that it is OK, that it is OK, that it is OK, that it is OK, that it is OK, that it is OK, that it is OK, that it is OK, that it is OK, that it is OK, that it is OK, that it is OK, that it is OK, that it is OK, that it is OK, all right, all right, all right, all right, all right, all right, all right, all right, all right, all right, all right, all right, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all,"}, {"heading": "Conclusion", "text": "In this paper, we have proposed a CNN model to address the problem of image quality. CNN's proposed model relies on convoluted architectures to generate the image representation, compose consecutive words to the question, and learn the interactions and relationships between image and question for predicting the answer. Experimental results based on public image QA data sets demonstrate the superiority of our proposed model over modern methods."}, {"heading": "Acknowledgement", "text": "The work is partially supported by the China National 973 project 2014CB340301. The authors thank Baotian Hu and Zhenguo Li for their insightful discussions and comments."}], "references": [{"title": "D", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "Parikh"], "venue": "2015. VQA: visual question answering. arXiv", "citeRegEx": "Antol et al. 2015", "shortCiteRegEx": null, "year": 1505}, {"title": "2014", "author": ["X. Chen", "Zitnick", "C. L"], "venue": "Learning a recurrent visual representation for image caption generation. arXiv", "citeRegEx": "Chen and Zitnick 2014", "shortCiteRegEx": null, "year": 1411}, {"title": "G", "author": ["G.E. Dahl", "T.N. Sainath", "Hinton"], "venue": "E.", "citeRegEx": "Dahl. Sainath. and Hinton 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "T", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "Darrell"], "venue": "2014. Long-term recurrent convolutional networks for visual recognition and description. arXiv", "citeRegEx": "Donahue et al. 2014", "shortCiteRegEx": null, "year": 1411}, {"title": "G", "author": ["H. Fang", "S. Gupta", "F.N. Iandola", "R. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt", "C.L. Zitnick", "Zweig"], "venue": "2014. From captions to visual concepts and back. arXiv", "citeRegEx": "Fang et al. 2014", "shortCiteRegEx": null, "year": 1411}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["Frome"], "venue": null, "citeRegEx": "Frome,? \\Q2013\\E", "shortCiteRegEx": "Frome", "year": 2013}, {"title": "W", "author": ["H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "L. Wang", "Xu"], "venue": "2015. Are you talking to a machine? dataset and methods for multilingual image question answering. arXiv", "citeRegEx": "Gao et al. 2015", "shortCiteRegEx": null, "year": 1505}, {"title": "Visual turing test for computer vision systems", "author": ["Geman"], "venue": "In PNAS", "citeRegEx": "Geman,? \\Q2015\\E", "shortCiteRegEx": "Geman", "year": 2015}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Hu"], "venue": null, "citeRegEx": "Hu,? \\Q2014\\E", "shortCiteRegEx": "Hu", "year": 2014}, {"title": "A convolutional neural network for modelling sentences", "author": ["Grefenstette Kalchbrenner", "N. Blunsom 2014] Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "venue": null, "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "2014", "author": ["A. Karpathy", "Li", "F.-F"], "venue": "Deep visual-semantic alignments for generating image descriptions. arXiv", "citeRegEx": "Karpathy and Li 2014", "shortCiteRegEx": null, "year": 1412}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["Joulin Karpathy", "A. Li 2014] Karpathy", "A. Joulin", "F.-F. Li"], "venue": null, "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Multimodal neural language models", "author": ["R. Salakhutdinov", "R. Zemel"], "venue": "In ICML", "citeRegEx": "R. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "R. et al\\.", "year": 2014}, {"title": "Fisher vectors derived from hybrid gaussianlaplacian mixture models for image annotation", "author": ["Klein"], "venue": null, "citeRegEx": "Klein,? \\Q2015\\E", "shortCiteRegEx": "Klein", "year": 2015}, {"title": "and Bengio", "author": ["Y. Lecun"], "venue": "Y.", "citeRegEx": "Lecun and Bengio 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "Multimodal convolutional neural networks for matching image and sentence", "author": ["Ma"], "venue": null, "citeRegEx": "Ma,? \\Q2015\\E", "shortCiteRegEx": "Ma", "year": 2015}, {"title": "Mutual learning of an object concept and language model based on mlda and npylm", "author": ["Makamura"], "venue": "In IROS", "citeRegEx": "Makamura,? \\Q2013\\E", "shortCiteRegEx": "Makamura", "year": 2013}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["Malinowski", "M. Fritz 2014a] Malinowski", "M. Fritz"], "venue": null, "citeRegEx": "Malinowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Malinowski et al\\.", "year": 2014}, {"title": "M", "author": ["M. Malinowski", "Fritz"], "venue": "2014b. Towards a visual turing challenge. arXiv", "citeRegEx": "Malinowski and Fritz 2014b", "shortCiteRegEx": null, "year": 1410}, {"title": "M", "author": ["M. Malinowski", "Fritz"], "venue": "2015. Hard to cheat: A turing test based on answering questions about images. arXiv", "citeRegEx": "Malinowski and Fritz 2015", "shortCiteRegEx": null, "year": 1501}, {"title": "M", "author": ["M. Malinowski", "M. Rohrbach", "Fritz"], "venue": "2015. Ask your neurons: A neural-based approach to answering questions about images. arXiv", "citeRegEx": "Malinowski. Rohrbach. and Fritz 2015", "shortCiteRegEx": null, "year": 1505}, {"title": "2014a", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A. L Yuille"], "venue": "Deep captioning with multimodal recurrent neural networks (m-rnn). arXiv", "citeRegEx": "Mao et al. 2014a", "shortCiteRegEx": null, "year": 1412}, {"title": "2014b", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A. L Yuille"], "venue": "Explain images with multimodal recurrent neural networks. arXiv", "citeRegEx": "Mao et al. 2014b", "shortCiteRegEx": null, "year": 1410}, {"title": "J", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "Dean"], "venue": "2013. Efficient estimation of word representations in vector space. arXiv", "citeRegEx": "Mikolov et al. 2013", "shortCiteRegEx": null, "year": 1301}, {"title": "T", "author": ["V. Ordonez", "G. Kulkarni", "Berg"], "venue": "L.", "citeRegEx": "Ordonez. Kulkarni. and Berg 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "2015", "author": ["M. Ren", "R. Kiros", "R. S Zemel"], "venue": "Exploring models and data for image question answering. arXiv", "citeRegEx": "Ren. Kiros. and Zemel 2015", "shortCiteRegEx": null, "year": 1505}, {"title": "A", "author": ["K. Simonyan", "Zisserman"], "venue": "2014. Very deep convolutional networks for largescale image recognition. arXiv", "citeRegEx": "Simonyan and Zisserman 2014", "shortCiteRegEx": null, "year": 1409}, {"title": "A", "author": ["R. Socher", "A. Karpathy", "Q.V. Le", "C.D. Manning", "Ng"], "venue": "Y.", "citeRegEx": "Socher et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Szegedy"], "venue": null, "citeRegEx": "Szegedy,? \\Q2015\\E", "shortCiteRegEx": "Szegedy", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["D. Erhan"], "venue": null, "citeRegEx": "Erhan,? \\Q2014\\E", "shortCiteRegEx": "Erhan", "year": 2014}, {"title": "M", "author": ["Z. Wu", "Palmer"], "venue": "S.", "citeRegEx": "Wu and Palmer 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "Y", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Bengio"], "venue": "2015a. Show, attend and tell: Neural image caption generation with visual attention. arXiv", "citeRegEx": "Xu et al. 2015a", "shortCiteRegEx": null, "year": 1502}, {"title": "Jointly modeling deep video and compositional text to bridge vision and language in a unified framework", "author": ["Xu"], "venue": null, "citeRegEx": "Xu,? \\Q2015\\E", "shortCiteRegEx": "Xu", "year": 2015}], "referenceMentions": [], "year": 2015, "abstractText": "In this paper, we propose to employ the convolutional neural network (CNN) for the image question answering (QA). Our proposed CNN provides an end-to-end framework with convolutional architectures for learning not only the image and question representations, but also their inter-modal interactions to produce the answer. More specifically, our model consists of three CNNs: one image CNN to encode the image content, one sentence CNN to compose the words of the question, and one multimodal convolution layer to learn their joint representation for the classification in the space of candidate answer words. We demonstrate the efficacy of our proposed model on the DAQUAR and COCO-QA datasets, which are two benchmark datasets for the image QA, with the performances significantly outperforming the state-of-the-art.", "creator": "LaTeX with hyperref package"}}}