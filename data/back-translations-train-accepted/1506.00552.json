{"id": "1506.00552", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2015", "title": "Coordinate Descent Converges Faster with the Gauss-Southwell Rule Than Random Selection", "abstract": "There has been significant recent work on the theory and application of randomized coordinate descent algorithms, beginning with the work of Nesterov [SIAM J. Optim., 22(2), 2012], who showed that a random-coordinate selection rule achieves the same convergence rate as the Gauss-Southwell selection rule. This result suggests that we should never use the Gauss-Southwell rule, as it is typically much more expensive than random selection. However, the empirical behaviours of these algorithms contradict this theoretical result: in applications where the computational costs of the selection rules are comparable, the Gauss-Southwell selection rule tends to perform substantially better than random coordinate selection. We give a simple analysis of the Gauss-Southwell rule showing that---except in extreme cases---it's convergence rate is faster than choosing random coordinates. Further, in this work we (i) show that exact coordinate optimization improves the convergence rate for certain sparse problems, (ii) propose a Gauss-Southwell-Lipschitz rule that gives an even faster convergence rate given knowledge of the Lipschitz constants of the partial derivatives, (iii) analyze the effect of approximate Gauss-Southwell rules, and (iv) analyze proximal-gradient variants of the Gauss-Southwell rule.", "histories": [["v1", "Mon, 1 Jun 2015 16:04:37 GMT  (77kb,D)", "http://arxiv.org/abs/1506.00552v1", "ICML 2015, 34 pages"]], "COMMENTS": "ICML 2015, 34 pages", "reviews": [], "SUBJECTS": "math.OC cs.LG stat.CO stat.ML", "authors": ["julie nutini", "mark w schmidt", "issam h laradji", "michael p friedlander", "hoyt a koepke"], "accepted": true, "id": "1506.00552"}, "pdf": {"name": "1506.00552.pdf", "metadata": {"source": "CRF", "title": "Coordinate Descent Converges Faster with the Gauss-Southwell Rule Than Random Selection", "authors": ["Julie Nutini", "Mark Schmidt", "Issam H. Laradji", "Michael Friedlander", "Hoyt Koepke"], "emails": [], "sections": [{"heading": "1 Coordinate Descent Methods", "text": "There has been considerable interest lately in the application of coordinate descend methods to solve large-scale optimization problems, starting with the groundbreaking work of Nesterov [2012], who gave the first global convergence analysis of methods for minimizing convex functions. This analysis suggests that selecting a random coordinate for updating yields the same performance as selecting the \"best\" coordinate for updating via the more expensive Gauss-Southwell (GS) rule. (Nesterov also proposed a smarter randomized scheme, which we will consider later in this paper.) This result provides a compelling argument for using randomized coordinate descent in contexts where the GS rule is too expensive. It also suggests that there is no benefit in using the GS rule in contexts where it is relatively inexpensive. But in these contexts, the GS rule outperforms randomized coordinate descent in practice, which often suggests that the GS rule is either slow, or that the GS is not the rule for GS."}, {"heading": "2 Problems of Interest", "text": "In fact, the fact is that most of them are able to be able to be able to be able to be able to be able to be able to be able to be able to be in the position they are in, to be able to be in the position they are in, to be in the position they are in, to be in, to be able to be in the position they are in, to be able to be in, to be in the position they are able to be in, to be able to be in, to be able to hide and to be able to hide."}, {"heading": "3 Existing Analysis", "text": "We are interested in solving the convex optimization problem defined in x-Rn f (x), (1) 1We could also consider somewhat more general cases such as functions defined on hyperedges [Richta \"rik\" and Taka \"c,\" 2015], provided that we can continue to perform n-coordinate updates for costs similar to a gradient evaluation. 2https: / / recordsetter.com / world-record / facebook-friendswhere \"f\" is coordinate-wise \"L-Lipschitz\" continuous, \"i.e., for each i = 1,\" n, \"if (x + \u03b1ei) \u2212\" if (x), if (x), x-x-Rn and \u03b1-R, where \"ei\" is a vector with a one in position i and zero in all other positions. \"For doubly differentiable functions, this is equivalent to the assumption that the diagonal elements of the hessional L.\" pick \"-type\" -assumes that the \"-in the method is coordinative.\""}, {"heading": "3.1 Randomized Coordinate Descent", "text": "The conditioning on \u03c3 field Fk \u2212 1, which is generated by the sequence {x0, x1,..., xk \u2212 1}, and taking into account the expectations of both sides of (2), we obtain with uniform sampling the following: E [f (xk + 1)] \u2264 E [f (xk) \u2212 12L (xk) 2] = f (xk) \u2212 12L n [f (xk) = 1 n (if (xk))) 2 = f (xk) \u2212 12Ln (xk). (5) This is a special case of Nesterov [2012, Theorem 2] with \u03b1 = 0 in his notation."}, {"heading": "3.2 Gauss-Southwell", "text": "We now consider the progress implied by the GS rule. By defining ik, (\u0435ikf (xk)) 2 = \u0441\u0430\u043d\u0438\u0435 f (xk) \u04322. (6) Applying this inequality to (2), we obtain f (xk + 1) \u2264 f (xk) \u2212 1 2Ln \u00d1 f (xk) \u0445 2, which, together with (4) implies that f (xk + 1) \u2212 f (x) \u2264 (1 \u2212 \u00b5 Ln) [f (xk) \u2212 f (x \u043a)]. (7) This is a special case of Boyd and Vandenberghe [2004, \u00a7 9.4.3], who consider the GS rule to be the steepest drop within the 1 standard. Although this is faster than the known rates for cyclic coordinate selection [Beck and Tetruashvili, 2013] and more deterministic than expected, this rate is the same as the randomly indicated rate (5)."}, {"heading": "4 Refined Gauss-Southwell Analysis", "text": "The deficiency of the existing GS analysis is that too much is lost when we use the inequality in (6). To avoid the need to use this inequality, we measure instead the strong convexity in the 1 standard, i.e., f (y) \u2265 f (x) + < f (x), y \u2212 x > + 2 \u00b2 y \u2212 x \u00b2 21, which is the analogue of (3). To minimize both sides in relation to y, we get f (x) = f (x) \u2212 sup y (x) \u2212 sup y (x) < \u2212 f (x), y \u2212 x \u00b2 2 \u00b2 y \u2212 x \u00b2 21} = f (x) \u00b7 f (x) \u00b7 f (x) \u00b7 f (x) \u2212 1 2\u00b51 \u00b2 f (x) that the selection in (x) is better than the selection in (x), (8) that the use of the convex conjugate (\u00b512) \u00b7 2 \u00b2 1 = 12\u00b5\u00b2 \u00b2 n."}, {"heading": "4.1 Comparison for Separable Quadratic", "text": "We illustrate these two extremes with the simple example of a square function with a diagonal Hessian coordinate number of 2f (x) = diag () \u03bb1,. \u2212 \u03b2n). In this case, the lower limit of the parameter \u00b51 is reached if all elements of the gradient are equal, \u03bb1 = \u00b7 \u00b7 \u00b7 \u00b7 = \u03bbn = \u03b1 > 0, where case \u00b5 = \u03b1 and \u00b51 = \u03b1 /. Thus, the uniform selection works just as well as the GS rule if all elements of the gradient change at exactly the same rate. This is reasonable: under this condition, there is no obvious advantage in intelligently selecting the coordinate to be updated. Intuitively, one might expect that the favorable case for the Gauss-Southwell rule would be where one degree of gradient is much greater than the others. However, in this case, this coordinate number is similar to the other way around."}, {"heading": "4.2 \u2018Working Together\u2019 Interpretation", "text": "In the above divisible quadratic case, the greatest benefit is derived from the harmonic mean of the eigenvalues of the Hessian divided by n. The harmonic mean is dominated by its smallest values, and therefore a small value is a remarkable case. Moreover, the harmonic mean divided by n has an interpretation in the sense of \"cooperating\" [Ferger, 1931]. If each \u03bbi represents the time it takes for each process to complete a task (e.g. large values of \u03bbi correspond to slow workers), then \u00b5 is the time it takes the fastest worker to complete the task, and \u00b51 is the time it takes for all processes to cooperate (and have independent effects). In this interpretation, the GS rule offers the greatest advantage over random selection when cooperation is not efficient, which means that when the n processes cooperate, the task is not solved much faster than when the fastest worker does the task alone."}, {"heading": "4.3 Fast Convergence with Bias Term", "text": "Consider the standard framework for linear predictions, Argmin x, \u03b2 m \u2211 i = 1 f (aTi x + \u03b2) + \u03bb 2 \u0445 x 2 + \u03c3 2 \u03b22, in which we have included a bias variable \u03b2 (an example of problem h1). Typically, the regularization parameter \u03c3 of the bias variable is much smaller than the regularization parameter \u03bb of the other covariates in order to avoid bias against global shifting of the predictor. Assuming that there is no hidden strong convexity in the sum, this problem shows the structure described in the previous section (\u00b51 \u2248 \u00b5), where GS has the greatest advantage over random selection."}, {"heading": "5 Rates with Different Lipschitz Constants", "text": "Consider the more general scenario in which we have a Lipschitz constant Li for the partial derivative of f with respect to each coordinate i, and we use a coordinate-dependent step variable for each iteration: xk + 1 = xk \u2212 1 Lik-ikf (xk) eik. (10) According to the logic of (2), in this setting we have a havef (xk + 1) \u2264 f (xk) \u2212 1 2Lik (xk) 2, (11) and thus a convergence rate off (xk) \u2212 f (x-p)."}, {"heading": "5.1 Gauss-Southwell with Exact Optimization", "text": "For problems affecting functions of the form h1 and h2, we are often able to perform an exact (or numerically very precise) coordinate optimization, even if the objective function is not square (e.g. by using a line search or a closed-form update). Note that (12) is still valid when we use an exact coordinate optimization, instead of using a step size of 1 / Lik, as in this case, we can consider the graph havef (xk + 1) = min \u03b1 {f (xk + \u03b1eik) {f (xk) eik) eik) \u2264 f (xk) [xk) \u2212 x = 2, which is equivalent to (11). In practice, however, an exact coordinate optimization results in better performance. In this section, we show that using the GS rule leads to a convergence rate that is actually faster than (9) for problems with different function, if the function is not square, or if the Li is adratic."}, {"heading": "6 Rules Depending on Lipschitz Constants", "text": "If the Li are known, Nesterov [2012] has shown that we can achieve a faster convergence rate by scanning the Li proportionally. We check this result below and compare it with the GS rule and then propose an improved GS rule for this scenario. Although we assume in this section that the Li are known, this assumption can be loosened by a traceability procedure [Nesterov, 2012, \u00a7 6.1]."}, {"heading": "6.1 Lipschitz Sampling", "text": "If we take the expectation of (11) under the distribution pi = Li / \u2211 n j = 1 Lj and proceed as before, we obtain E [f (xk + 1) \u2212 f (x *) \u2264 (1 \u2212 \u00b5 nL) [f (xk) \u2212 f (x *), where L = 1n \u2211 n j = 1 Lj is the average of the Lipschitz constants. Leventhal and Lewis [2010] have shown this and is a special case of Nesterov [2012, Theorem 2] with \u03b1 = 1 in its notation. This rate is faster than (5) for a uniform sampling if Li deviates. According to our analysis, this rate can be faster or not than (9) for the GS rule. On the one hand, if \u00b51 = \u00b5 / n and all Li deviate, then this Lipschitz sampling scheme is faster than (5) for sampling Li deviations."}, {"heading": "6.2 Gauss-Southwell-Lipschitz Rule", "text": "Since neither Lipschitz sampling nor GS generally dominates the other, we are motivated to test whether faster rules are possible by combining the two approaches. In fact, we get a faster rate by choosing theik, which minimizes (11) what leads to the rule ruleik = argmax i, if (2) the GSL rule requires a convergence rate (xk + 1) \u2212 f (x) \u2212 f (xk) \u2212 f (xk) \u2212 f (x) \u2212 f \u2212 f (x) \u2212 f (x) \u2212 f, where \u00b5L is the strong convexity constant relative to the standard x \u00b2 L = a minimum convergence rate from (xk + 1) \u2212 f (x \u00b2 L) \u2212 f = a minimum convergence rate."}, {"heading": "6.3 Connection between GSL Rule and Normalized Nearest Neighbour Search", "text": "Dhillon et al. (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012)."}, {"heading": "7 Approximate Gauss-Southwell", "text": "In many applications, calculating the exact GS rule is too inefficient to be of practical use. However, a mathematically more favorable approximate GS rule may be available. Approximate GS rules under multiplicative and additive errors have been considered by Dhillon et al. [2011] in the convex case, but in this setting, the convergence rate corresponds to the rate achieved by random selection. In this section, we give rates depending on \u00b51 for approximate GS rules."}, {"heading": "7.1 Multiplicative Errors", "text": "In the multiplicative error regime, the approximate GS rule satisfactorily selects an ik as defined for each ik. We can include this type of error in our lower limit (8) to obtain f (x) \u2265 f (xk) \u2212 1 2\u00b51% limit for progress (2) as defined for each ik. This implies a convergence rate of f (xk + 1) \u2212 f (x) \u2264 (1 \u2212 1 (1 \u2212 k) 2L) [f (xk) \u2212 1 2\u00b51 (1 \u2212 k) 2 (f (xk) \u2212 1 2\u00b51 (1 \u2212 k) 2 |.Thus, the convergence rate of the method is almost identical to the exact GS rule for small k (and it deteriorates considerably with this decrease in error compared to Schmidt in 2012)."}, {"heading": "7.2 Additive Errors", "text": "In the additive error regime, the approximate GS rule selects an ik satisfactory part, unless this regime is closer to the convergence rate than the convergence rate. In Appendix G, we show that under this rule we have an ik (xk) \u2212 f (x) \u2264 (1 \u2212 \u00b51 L) \u2264 (1 \u2212 \u00b51 L) k [f (x0) \u2212 f (x \u0445) + Ak], where Ak \u2264 min {k \u2211 i = 1 (1 \u2212 \u00b51 L) \u2212 i \u221a 2L1 L \u221a f (x0) \u2212 f (x \u0445), k \u0445 i = 1 (1 \u2212 \u00b51 L) \u2212 i (i \u221a 2 L \u221a f (x0) \u2212 f (x \u0445) + 2 i 2L)}, where L1 is the Lipschitz constant of \u0394f (x0) \u2212 f. Note that L1 could be substantially larger than L, so that the second part of the maximum in Ak will probably be the smaller part if the regime is not closer to the i."}, {"heading": "8 Proximal-Gradient Gauss-Southwell", "text": "One of the main motivations for reviving interest in coordination methods is their performance in the field of formmin x-GS (x). (...) This also includes problems with \"1-GS rule\" and optimization with lower and / or higher limits in the field of variables. (...) Similar to the proximal gradient methods, we can apply the proximal operator to coordinate update, xk + 1 = prox 1 L gik [xk]. (...) L gik [xk] eik. \"(...) Random coordinate selection, xk + 1 = prox.\" (...) With random coordinate selection, directive \"rik\" and taka \"c.\" (...) we show that this method has a convergence rate of E (xk + 1). (...)"}, {"heading": "9 Experiments", "text": "We first compare the effectiveness of different coordination rules on the following simple examples of h1, 2, 3, 4, 5, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7"}, {"heading": "10 Discussion", "text": "It is clear that the GS rule is not workable for every problem where randomized methods are applicable. Nevertheless, we have shown that even approximate GS rules can achieve better limits on convergence rates than fully randomized methods. We have given a similar justification for the use of exact coordinate optimization, 3In order to reduce the cost of the GS method in this context, Shevade and Keerthi [2003] consider a variant in which we first calculate the GS-s rule for the non-zero variables and when an element is sufficiently large, they do not consider the zero variables. And we note that our reasoning could also be used to justify the use of exact coordinate optimization within randomized coordinate descending methods (as used in our experiments). We have also proposed the improved GSL rule and are considering approximate / proximal variants. We expect our analysis to also be applicable to block updates using Proveele, by using mixed methods for 2013, Scoq, and Scoq]."}, {"heading": "Acknowledgements", "text": "We would like to thank the anonymous referees for their useful comments, which have greatly improved the work. Julie Nutini is supported by an NSERC Canada Graduate Scholarship."}, {"heading": "Appendix A Efficient calculation of GS rules for sparse problems", "text": "We first provide further information on the way in which we use the GS ratio for the 3rd ratification of the EWS Ratio (A) i.D (A) i.D (A) i.D (A) i.D (A) i.D (A) i.D (A) i.D (A) i.D (A) i.D (A) i.D (A) i.D (A) i.D (A) i.D (A) i.D (A) i.D (A) i.D (A) i.D (A) i.D) i.D (A) i.D (A) i.D (A) i.D (A) i.D (A) i.D (A) i.D) i.D (D) i.D (D) (D) i.D (D) (D) (D) i.D (D) (D) (D) i.D (D) (D) (D) i.D (D) (D) (D) i.D (D) (D) (D) i.D (D) (D) i.D (D (D) (D) (D (D) (D) i.D (A) (D (D) (D) (D (D) (D) (D (D).D (A).D (D (A).D (A).D (D (A).D (A).D (A) i.D (D (A).D (A).D (A).D (A).D (D (A).D (A).D (D (A).D (A).D (D (A).D (A).D (A).D (.D (A) i.D (D (A).D (A).D (A).D (A) i.D (A) i.D (A) i.D (A) i.D (A) i.D (A) i.D (A) i.D (A) i.D (A) i.D (A (A) (A) (D (A) (A).D (A).D"}, {"heading": "Appendix B Relationship between \u00b51 and \u00b5", "text": "We can establish the relationship between \u00b5 and \u00b51 by using the known relationship between the 2 standard and the 1 standard, namely for all x standard, i.e. for all x and y that we have: f (x) + < f (x), y \u2212 x > + \u00b5 2 + < f (x), y \u2212 x > + \u00b5 2 + < f (x). Similarly, if we assume that a given f in the 1 standard is \u00b51-strong convex, then we have for all x \u2212 x > 2n - y \u2212 x 21, which implies that f in the 1 standard is at least \u00b5n -strong convex."}, {"heading": "Appendix C Analysis for separable quadratic case", "text": "We first establish an equivalent definition of strong convexity in the 1 standard, along the lines of Nesterov [2004, theorem 2.1.9], and then use this equivalent definition to derive it for a divisible square function. \u2212 C.1 Equivalent definition of strong-convexity assume that f in the 1 standard is 1 - strongly convex, so that for each x, y - IRn we havef (y) more f (x) + < f (x), y \u2212 x > + < f (x), y \u2212 1, y - 2, y - 2, y - 2, y - 2, y - 2, y - 2, and 2, and 2 - 2, and y - y - y - x (x), in the above givesf (x)."}, {"heading": "Appendix D Gauss-Southwell with exact optimization", "text": "We get faster convergence for GS using exact coordinate optimization for sparse variants of problems h1 and h2 by observing that the convergence rate can be expressed in terms of the sequence of (1 \u2212 \u00b51 / Lik) values. \u2212 \u2212 \u2212 The worst case scenario occurs when the product of the (1 \u2212 \u00b51 / Lik) values is as large as possible. However, using exact coordinate optimization guarantees that after updating the coordinate i, the GS rule will never select it again until one of its neighbors is selected."}, {"heading": "Appendix E Gauss-Southwell-Lipschitz rule: convergence rate", "text": "The coordinate descend method with a constant increment of Lik uses the iterationxk + 1 = xk \u2212 1 Lik-ikf (xk) eik. Since f is continuous in coordinate terms Lik \u2212 -Lipschitz, we get the following limit for each iteration: f (xk + 1) \u2264 f (xk) + 0 (xk) ikf (xk) (xk) (xk + 1 \u2212 xk) ik + Lik 2 (xk + 1 \u2212 xk) 2ik = f (xk) \u2212 1 Lik (xk) 2 + Lik 2 [1Lik-ikf (xk)] 2 = f (xk) \u2212 12Lik [\u0644ikf (xk)] 2 = f (xk) \u2212 2 = f (xk) \u2212 1 [2 [2] ikf (xk) ikf (xk)), 2 [2] ikf (xk), L (xk), 2 [f), f (f), f), (f), x, f)."}, {"heading": "Appendix F Comparing \u00b5L to \u00b51 and \u00b5", "text": "According to the logic of Appendix B, in order to establish a relationship between different convexity constants under different norms, it is sufficient to establish the relations between the square norms. In this section, we will use this to establish the relationship between the convexity constants defined in (22) and both the convexity constants and the convexity constants."}, {"heading": "Appendix G Approximate Gauss-Southwell with additive error", "text": "In the additive error rule, the approximate Gauss-Southwell rule produces a result that we assume first (< f = > f). (< f). (< f). (< f). (< f). (< f). (< f). (< f). (< f). (< f). (< f). (< f). (< f). (< f). (< f). (< f). (xk). ("}], "references": [{"title": "On the convergence of block coordinate descent type methods", "author": ["A. Beck", "L. Tetruashvili"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Beck and Tetruashvili.,? \\Q2013\\E", "shortCiteRegEx": "Beck and Tetruashvili.", "year": 2013}, {"title": "Label propagation and quadratic criterion", "author": ["Y. Bengio", "O. Delalleau", "N. Le Roux"], "venue": "Semi-Supervised Learning,", "citeRegEx": "Bengio et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Nonlinear Programming", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific, second edition,", "citeRegEx": "Bertsekas.,? \\Q1999\\E", "shortCiteRegEx": "Bertsekas.", "year": 1999}, {"title": "Greedy block coordinate descent for large scale gaussian process regression", "author": ["L. Bo", "C. Sminchisescu"], "venue": "Uncertainty in Artificial Intelligence,", "citeRegEx": "Bo and Sminchisescu.,? \\Q2008\\E", "shortCiteRegEx": "Bo and Sminchisescu.", "year": 2008}, {"title": "Convex Optimization", "author": ["S.P. Boyd", "L. Vandenberghe"], "venue": null, "citeRegEx": "Boyd and Vandenberghe.,? \\Q2004\\E", "shortCiteRegEx": "Boyd and Vandenberghe.", "year": 2004}, {"title": "Introduction to Algorithms", "author": ["T.H. Cormen", "C.E. Leiserson", "R.L. Rivest", "C. Stein"], "venue": null, "citeRegEx": "Cormen et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Cormen et al\\.", "year": 2001}, {"title": "Inducing features of random fields", "author": ["S. Della Pietra", "V. Della Pietra", "J. Lafferty"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Pietra et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Pietra et al\\.", "year": 1997}, {"title": "Nearest neighbor based greedy coordinate descent", "author": ["I.S. Dhillon", "P.K. Ravikumar", "A. Tewari"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Dhillon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dhillon et al\\.", "year": 2011}, {"title": "Accelerated, parallel and proximal coordinate descent", "author": ["O. Fercoq", "P. Richt\u00e1rik"], "venue": null, "citeRegEx": "Fercoq and Richt\u00e1rik.,? \\Q2013\\E", "shortCiteRegEx": "Fercoq and Richt\u00e1rik.", "year": 2013}, {"title": "The nature and use of the harmonic mean", "author": ["W.F. Ferger"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Ferger.,? \\Q1931\\E", "shortCiteRegEx": "Ferger.", "year": 1931}, {"title": "Hybrid deterministic-stochastic methods for data fitting", "author": ["M.P. Friedlander", "M. Schmidt"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Friedlander and Schmidt.,? \\Q2012\\E", "shortCiteRegEx": "Friedlander and Schmidt.", "year": 2012}, {"title": "A dual coordinate descent method for large-scale linear SVM", "author": ["C.-J. Hsieh", "K.-W. Chang", "C.-J. Lin", "S.S. Keerthi", "S. Sundararajan"], "venue": "International Conference on Machine Learning,", "citeRegEx": "Hsieh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hsieh et al\\.", "year": 2008}, {"title": "Efficient structure learning of Markov networks using `1regularization", "author": ["S.-I. Lee", "V. Ganapathi", "D. Koller"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Lee et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2006}, {"title": "Randomized methods for linear constraints: convergence rates and conditioning", "author": ["D. Leventhal", "A.S. Lewis"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Leventhal and Lewis.,? \\Q2010\\E", "shortCiteRegEx": "Leventhal and Lewis.", "year": 2010}, {"title": "Coordinate descent optimization for `1 minimization with application to compressed sensing; a greedy algorithm", "author": ["Y. Li", "S. Osher"], "venue": "Inverse Problems and Imaging,", "citeRegEx": "Li and Osher.,? \\Q2009\\E", "shortCiteRegEx": "Li and Osher.", "year": 2009}, {"title": "On convergence of the maximum block improvement method", "author": ["Z. Li", "A. Uschmajew", "S. Zhang"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Error bounds and convergence analysis of feasible descent methods: a general approach", "author": ["Z.-Q. Luo", "P. Tseng"], "venue": "Annals of Operations Research,", "citeRegEx": "Luo and Tseng.,? \\Q1993\\E", "shortCiteRegEx": "Luo and Tseng.", "year": 1993}, {"title": "Convergence rate analysis of MAP coordinate minimization algorithms", "author": ["O. Meshi", "T. Jaakkola", "A. Globerson"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Meshi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Meshi et al\\.", "year": 2012}, {"title": "Introductory Lectures on Convex Optimization: A Basic Course", "author": ["Y. Nesterov"], "venue": null, "citeRegEx": "Nesterov.,? \\Q2004\\E", "shortCiteRegEx": "Nesterov.", "year": 2004}, {"title": "Efficiency of coordinate descent methods on huge-scale optimization problems", "author": ["Y. Nesterov"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Nesterov.,? \\Q2012\\E", "shortCiteRegEx": "Nesterov.", "year": 2012}, {"title": "Five balltree construction algorithms", "author": ["S.M. Omohundro"], "venue": "Technical report, International Computer Science Institute,", "citeRegEx": "Omohundro.,? \\Q1989\\E", "shortCiteRegEx": "Omohundro.", "year": 1989}, {"title": "On the convergence of leveraging", "author": ["G. R\u00e4tsch", "S. Mika", "M.K. Warmuth"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "R\u00e4tsch et al\\.,? \\Q2001\\E", "shortCiteRegEx": "R\u00e4tsch et al\\.", "year": 2001}, {"title": "Gaussian Markov Random Fields: Theory and Applications", "author": ["H. Rue", "L. Held"], "venue": null, "citeRegEx": "Rue and Held.,? \\Q2005\\E", "shortCiteRegEx": "Rue and Held.", "year": 2005}, {"title": "Asymmetric LSH (ALSH) for sublinear time maximum inner product search", "author": ["A. Shrivastava", "P. Li"], "venue": null, "citeRegEx": "Shrivastava and Li.,? \\Q2003\\E", "shortCiteRegEx": "Shrivastava and Li.", "year": 2003}], "referenceMentions": [{"referenceID": 18, "context": "There has been substantial recent interest in applying coordinate descent methods to solve large-scale optimization problems, starting with the seminal work of Nesterov [2012], who gave the first global rate-ofconvergence analysis for coordinate-descent methods for minimizing convex functions.", "startOffset": 160, "endOffset": 176}, {"referenceID": 11, "context": ") The family of functions h1 includes core machinelearning problems such as least squares, logistic regression, lasso, and SVMs (when solved in dual form) [Hsieh et al., 2008].", "startOffset": 155, "endOffset": 175}, {"referenceID": 1, "context": "Family h2 includes quadratic functions, graph-based label propagation algorithms for semisupervised learning [Bengio et al., 2006], and finding the most likely assignments in continuous pairwise graphical models [Rue and Held, 2005].", "startOffset": 109, "endOffset": 130}, {"referenceID": 22, "context": ", 2006], and finding the most likely assignments in continuous pairwise graphical models [Rue and Held, 2005].", "startOffset": 89, "endOffset": 109}, {"referenceID": 17, "context": "For example, if each node has at most d neighbours, we can track the gradients of all the variables and use a max-heap structure to implement the GS rule in O(d log n) time [Meshi et al., 2012].", "startOffset": 173, "endOffset": 193}, {"referenceID": 1, "context": "Family h2 includes quadratic functions, graph-based label propagation algorithms for semisupervised learning [Bengio et al., 2006], and finding the most likely assignments in continuous pairwise graphical models [Rue and Held, 2005]. In general, the GS rule for problem h2 is as expensive as a full gradient evaluation. However, the structure of G often allows efficient implementation of the GS rule. For example, if each node has at most d neighbours, we can track the gradients of all the variables and use a max-heap structure to implement the GS rule in O(d log n) time [Meshi et al., 2012]. This is similar to the cost of the randomized algorithm if d \u2248 |E|/n (since the average cost of the randomized method depends on the average degree). This condition is true in a variety of applications. For example, in spatial statistics we often use two-dimensional grid-structured graphs, where the maximum degree is four and the average degree is slightly less than 4. As another example, for applying graph-based label propagation on the Facebook graph (to detect the spread of diseases, for example), the average number of friends is around 200 but no user has more than seven thousand friends. The maximum number of friends would be even smaller if we removed edges based on proximity. A non-sparse example where GS is efficient is complete graphs, since here the average degree and maximum degree are both (n\u22121). Thus, the GS rule is efficient for optimizing dense quadratic functions. On the other hand, GS could be very inefficient for star graphs. If each column of A has at most c non-zeroes and each row has at most r non-zeroes, then for many notable instances of problem h1 we can implement the GS rule in O(cr log n) time by maintaining Ax as well as the gradient and again using a max-heap (see Appendix A). Thus, GS will be efficient if cr is similar to the number of non-zeroes in A divided by n. Otherwise, Dhillon et al. [2011] show that we can approximate the GS rule for problem h1 with no gi functions by solving a nearest-neighbour problem.", "startOffset": 110, "endOffset": 1945}, {"referenceID": 1, "context": "Family h2 includes quadratic functions, graph-based label propagation algorithms for semisupervised learning [Bengio et al., 2006], and finding the most likely assignments in continuous pairwise graphical models [Rue and Held, 2005]. In general, the GS rule for problem h2 is as expensive as a full gradient evaluation. However, the structure of G often allows efficient implementation of the GS rule. For example, if each node has at most d neighbours, we can track the gradients of all the variables and use a max-heap structure to implement the GS rule in O(d log n) time [Meshi et al., 2012]. This is similar to the cost of the randomized algorithm if d \u2248 |E|/n (since the average cost of the randomized method depends on the average degree). This condition is true in a variety of applications. For example, in spatial statistics we often use two-dimensional grid-structured graphs, where the maximum degree is four and the average degree is slightly less than 4. As another example, for applying graph-based label propagation on the Facebook graph (to detect the spread of diseases, for example), the average number of friends is around 200 but no user has more than seven thousand friends. The maximum number of friends would be even smaller if we removed edges based on proximity. A non-sparse example where GS is efficient is complete graphs, since here the average degree and maximum degree are both (n\u22121). Thus, the GS rule is efficient for optimizing dense quadratic functions. On the other hand, GS could be very inefficient for star graphs. If each column of A has at most c non-zeroes and each row has at most r non-zeroes, then for many notable instances of problem h1 we can implement the GS rule in O(cr log n) time by maintaining Ax as well as the gradient and again using a max-heap (see Appendix A). Thus, GS will be efficient if cr is similar to the number of non-zeroes in A divided by n. Otherwise, Dhillon et al. [2011] show that we can approximate the GS rule for problem h1 with no gi functions by solving a nearest-neighbour problem. Their analysis of the GS rule in the convex case, however, gives the same convergence rate that is obtained by random selection (although the constant factor can be smaller by a factor of up to n). More recently, Shrivastava and Li [2014] give a general method for approximating the GS rule for problem h1 with no gi functions by writing it as a maximum inner-product search problem.", "startOffset": 110, "endOffset": 2301}, {"referenceID": 0, "context": "While this is faster than known rates for cyclic coordinate selection [Beck and Tetruashvili, 2013] and holds deterministically rather than in expectation, this rate is the same as the randomized rate given in (5).", "startOffset": 70, "endOffset": 99}, {"referenceID": 9, "context": "Furthermore, the harmonic mean divided by n has an interpretation in terms of processes \u2018working together\u2019 [Ferger, 1931].", "startOffset": 107, "endOffset": 121}, {"referenceID": 18, "context": "6 Rules Depending on Lipschitz Constants If the Li are known, Nesterov [2012] showed that we can obtain a faster convergence rate by sampling proportional to the Li.", "startOffset": 62, "endOffset": 78}, {"referenceID": 13, "context": "This was shown by Leventhal and Lewis [2010] and is a special case of Nesterov [2012, Theorem 2] with \u03b1 = 1 in his notation.", "startOffset": 18, "endOffset": 45}, {"referenceID": 21, "context": "This rule has been used in the context of boosting [R\u00e4tsch et al., 2001], graphical models [Della Pietra et al.", "startOffset": 51, "endOffset": 72}, {"referenceID": 3, "context": ", 2006, Scheinberg and Rish, 2009], Gaussian processes [Bo and Sminchisescu, 2008], and low-rank tensor approximations [Li et al.", "startOffset": 55, "endOffset": 82}, {"referenceID": 15, "context": ", 2006, Scheinberg and Rish, 2009], Gaussian processes [Bo and Sminchisescu, 2008], and low-rank tensor approximations [Li et al., 2015].", "startOffset": 119, "endOffset": 136}, {"referenceID": 7, "context": "3 Connection between GSL Rule and Normalized Nearest Neighbour Search Dhillon et al. [2011] discuss an interesting connection between the GS rule and the nearest-neighbour-search (NNS) problem for objectives of the form", "startOffset": 70, "endOffset": 92}, {"referenceID": 7, "context": "Dhillon et al. [2011] propose to approximate the above argmax by solving the following NNS problem ik = argmin i\u2208[2n] \u2016r(x)\u2212 ai\u2016,", "startOffset": 0, "endOffset": 22}, {"referenceID": 23, "context": "Shrivastava and Li [2014] have more recently considered the case where \u2016ai\u2016 \u2264 1 and incorporate powers of \u2016ai\u2016 in the NNS to yield a better approximation.", "startOffset": 0, "endOffset": 26}, {"referenceID": 7, "context": "Approximate GS rules under multiplicative and additive errors were considered by Dhillon et al. [2011] in the convex case, but in this setting the convergence rate is similar to the rate achieved by random selection.", "startOffset": 81, "endOffset": 103}, {"referenceID": 10, "context": "This is in contrast to having an error in the gradient [Friedlander and Schmidt, 2012], where the error must decrease to zero over time.", "startOffset": 55, "endOffset": 86}, {"referenceID": 20, "context": "Hence, we use a balltree structure [Omohundro, 1989] to implement an efficient approximate GS rule based on the connection to the NNS problem discovered by Dhillon et al.", "startOffset": 35, "endOffset": 52}, {"referenceID": 7, "context": "Hence, we use a balltree structure [Omohundro, 1989] to implement an efficient approximate GS rule based on the connection to the NNS problem discovered by Dhillon et al. [2011]. On the other hand, we can compute the exact GSL rule for this problem as a NNS problem as discussed in Section 6.", "startOffset": 156, "endOffset": 178}, {"referenceID": 1, "context": "We use the quadratic labeling criterion of Bengio et al. [2006], which allows exact coordinate optimization and is normally optimized with cyclic coordinate descent.", "startOffset": 43, "endOffset": 64}, {"referenceID": 8, "context": "We expect our analysis also applies to block updates by using mixed norms \u2016 \u00b7 \u2016p,q, and could be used for accelerated/parallel methods [Fercoq and Richt\u00e1rik, 2013], for primal-dual rates of dual coordinate ascent [Shalev-Shwartz and Zhang, 2013], for successive projection methods [Leventhal and Lewis, 2010], for boosting algorithms [R\u00e4tsch et al.", "startOffset": 135, "endOffset": 163}, {"referenceID": 13, "context": "We expect our analysis also applies to block updates by using mixed norms \u2016 \u00b7 \u2016p,q, and could be used for accelerated/parallel methods [Fercoq and Richt\u00e1rik, 2013], for primal-dual rates of dual coordinate ascent [Shalev-Shwartz and Zhang, 2013], for successive projection methods [Leventhal and Lewis, 2010], for boosting algorithms [R\u00e4tsch et al.", "startOffset": 281, "endOffset": 308}, {"referenceID": 21, "context": "We expect our analysis also applies to block updates by using mixed norms \u2016 \u00b7 \u2016p,q, and could be used for accelerated/parallel methods [Fercoq and Richt\u00e1rik, 2013], for primal-dual rates of dual coordinate ascent [Shalev-Shwartz and Zhang, 2013], for successive projection methods [Leventhal and Lewis, 2010], for boosting algorithms [R\u00e4tsch et al., 2001], and for scenarios without strong-convexity under general error bounds [Luo and Tseng, 1993].", "startOffset": 334, "endOffset": 355}, {"referenceID": 16, "context": ", 2001], and for scenarios without strong-convexity under general error bounds [Luo and Tseng, 1993].", "startOffset": 79, "endOffset": 100}], "year": 2015, "abstractText": "There has been significant recent work on the theory and application of randomized coordinate descent algorithms, beginning with the work of Nesterov [SIAM J. Optim., 22(2), 2012 ], who showed that a random-coordinate selection rule achieves the same convergence rate as the Gauss-Southwell selection rule. This result suggests that we should never use the Gauss-Southwell rule, because it is typically much more expensive than random selection. However, the empirical behaviours of these algorithms contradict this theoretical result: in applications where the computational costs of the selection rules are comparable, the Gauss-Southwell selection rule tends to perform substantially better than random coordinate selection. We give a simple analysis of the Gauss-Southwell rule showing that\u2014except in extreme cases\u2014its convergence rate is faster than choosing random coordinates. We also (i) show that exact coordinate optimization improves the convergence rate for certain sparse problems, (ii) propose a Gauss-Southwell-Lipschitz rule that gives an even faster convergence rate given knowledge of the Lipschitz constants of the partial derivatives, (iii) analyze the effect of approximate Gauss-Southwell rules, and (iv) analyze proximal-gradient variants of the Gauss-Southwell rule. 1 Coordinate Descent Methods There has been substantial recent interest in applying coordinate descent methods to solve large-scale optimization problems, starting with the seminal work of Nesterov [2012], who gave the first global rate-ofconvergence analysis for coordinate-descent methods for minimizing convex functions. This analysis suggests that choosing a random coordinate to update gives the same performance as choosing the \u201cbest\u201d coordinate to update via the more expensive Gauss-Southwell (GS) rule. (Nesterov also proposed a more clever randomized scheme, which we consider later in this paper.) This result gives a compelling argument to use randomized coordinate descent in contexts where the GS rule is too expensive. It also suggests that there is no benefit to using the GS rule in contexts where it is relatively cheap. But in these contexts, the GS rule often substantially outperforms randomized coordinate selection in practice. This suggests that either the analysis of GS is not tight, or that there exists a class of functions for which the GS rule is as slow as randomized coordinate descent. After discussing contexts in which it makes sense to use coordinate descent and the GS rule, we answer this theoretical question by giving a tighter analysis of the GS rule (under strong-convexity and standard smoothness assumptions) that yields the same rate as the randomized method for a restricted class of functions, but is otherwise faster (and in some cases substantially faster). We further show that, compared to the usual constant step-size update of the coordinate, the GS method with exact coordinate optimization has a provably faster rate for problems satisfying a certain sparsity constraint (Section 5). We believe that this is the first result showing a theoretical benefit of exact coordinate optimization; all previous analyses show that these strategies obtain the same rate as constant step-size updates, even though exact optimization tends to be faster in practice. Furthermore, in Section 6, we propose a variant of the GS rule that, similar to Nesterov\u2019s more clever randomized sampling scheme, uses knowledge of the Lipschitz constants of the coordinate-wise gradients to obtain a faster rate. We also analyze approximate GS rules (Section 7), which 1 ar X iv :1 50 6. 00 55 2v 1 [ m at h. O C ] 1 J un 2 01 5 provide an intermediate strategy between randomized methods and the exact GS rule. Finally, we analyze proximal-gradient variants of the GS rule (Section 8) for optimizing problems that include a separable nonsmooth term. 2 Problems of Interest The rates of Nesterov show that coordinate descent can be faster than gradient descent in cases where, if we are optimizing n variables, the cost of performing n coordinate updates is similar to the cost of performing one full gradient iteration. This essentially means that coordinate descent methods are useful for minimizing convex functions that can be expressed in one of the following two forms:", "creator": "LaTeX with hyperref package"}}}