{"id": "1506.00698", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2015", "title": "Statistical Machine Translation Features with Multitask Tensor Networks", "abstract": "We present a three-pronged approach to improving Statistical Machine Translation (SMT), building on recent success in the application of neural networks to SMT. First, we propose new features based on neural networks to model various non-local translation phenomena. Second, we augment the architecture of the neural network with tensor layers that capture important higher-order interaction among the network units. Third, we apply multitask learning to estimate the neural network parameters jointly. Each of our proposed methods results in significant improvements that are complementary. The overall improvement is +2.7 and +1.8 BLEU points for Arabic-English and Chinese-English translation over a state-of-the-art system that already includes neural network features.", "histories": [["v1", "Mon, 1 Jun 2015 22:52:36 GMT  (156kb,D)", "http://arxiv.org/abs/1506.00698v1", "11 pages (9 content + 2 references), 2 figures, accepted to ACL 2015 as a long paper"]], "COMMENTS": "11 pages (9 content + 2 references), 2 figures, accepted to ACL 2015 as a long paper", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hendra setiawan", "zhongqiang huang", "jacob devlin", "thomas lamar", "rabih zbib", "richard m schwartz", "john makhoul"], "accepted": true, "id": "1506.00698"}, "pdf": {"name": "1506.00698.pdf", "metadata": {"source": "CRF", "title": "Statistical Machine Translation Features with Multitask Tensor Networks", "authors": ["Hendra Setiawan", "Zhongqiang Huang", "Jacob Devlin", "Thomas Lamar", "Rabih Zbib", "Richard Schwartz", "John Makhoul"], "emails": ["hsetiawa@bbn.com", "zhuang@bbn.com", "tlamar@bbn.com", "rzbib@bbn.com", "schwartz@bbn.com", "makhoul@bbn.com", "jdevlin@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "The latest developments in this area show that most of them are not just a problem, but a problem that has arisen in recent years, \"he said.\" We have to solve it, \"he said.\" We have to solve it, \"he said.\" We have to manage to get it resolved. \""}, {"heading": "2 New Non-Local SMT Features", "text": "Existing SMT attributes typically focus on local information in the source sentence, in the target hypothesis, or both. For example, the n-gram language model (LM) predicts the translation of a source word by using previously generated target words as context (local at the destination), while the lexical translation model (LTM) predicts the translation of a source word by taking surrounding source words as context (local at the destination). In this work, we focus on non-local translation phenomena resulting from a non-monotonous reorganization where the local context on the other hand does not become local. We propose a number of powerful MT attributes motivated by this simple idea. To facilitate discussion, we categorize the attributes into hypothetical-enumerating attributes that estimate the probability for each generated target word (e.g. n-gram language model) and source-specific attributes that contain source model, the next (and) one source model."}, {"heading": "2.1 Hypothesis-Enumerating Features", "text": "As already mentioned, each word in the hypothesis is evaluated by hypothesis-enumerating features, typically by conditioning it to a context of previous n-1 target words, as in the n-gram language model. A current model of this kind, the Joint Model by Devlin et al. (2014), achieves great improvements in SMT's state of the art by using a large context window with 11 source words and 3 target words. JM's Joint Model with Offset Source Context (JMO) is an extension of JM that uses source words associated with the n-gram target story as context. JM and JMO's source contexts overlap greatly when the translation is monotonous, but complement each other when the translation requires a reordering of the words."}, {"heading": "2.1.1 Joint Model with Offset Source Context", "text": "Formally, JMO estimates the probability of the target hypothesis E due to the source sentence F = more specifically the target-to-source affiliation A: P (E | F, A) \u2248 | E | i = 1P (ei | ei \u2212 n + 1i \u2212 1, Cai \u2212 k = f ai \u2212 k + m ai \u2212 k) where ei is the word that is predicted; ei \u2212 n + 1i \u2212 1 is the string of n \u2212 1 previously generated words; Cai \u2212 k to the source context of m source words around fai \u2212 k, the source word associated with ei \u2212 k. We refer to k as an offset parameter. We use the definition of the word affiliation introduced in Devlin et al. (2014). If no source context is used, the model corresponds to an n-gram language model, while an offset parameter of k = 0 reduces the model to the JM of Devlin et al. (2014)."}, {"heading": "2.2 Source-Enumerating Features", "text": "Source enumerating features iterate over words in the source sentence, including unaligned words, and assign a score to them depending on which aspect of the translation they model. A source enumerating feature can be formulated as follows: P (E | F, A) \u2248 | F | \u0435j = 1P (Yj | Cj = f j + mj \u2212 m), where Caj is the source context (similar to the hypothesis enumerating features mentioned above) and Yj is the name predicted by the feature. We first describe existing source enumerating features: the lexical translation model, the orientation model, and the fertility model, and then discuss a new feature: Translation Context Model (TCM), which is an extension of the lexical translation model."}, {"heading": "2.2.1 Pre-existing Features", "text": "The lexical translation model (LTM) estimates the probability of translating a source word fj into a tar-get word l (fj) = ebj in the face of a source context Cj, bj-B is the word affiliation from source to destination, as defined in (Devlin et al., 2014). If fj is translated into more than one word, we arbitrarily retain the most left-hand word. Target vocabulary V is expanded by a zero sign to accommodate unaligned source words. The orientation model (ORI) describes the probability of orientation of the translation of phrases surrounding a source word fj in relation to its own translation. We follow (Setiawan et al., 2013) in modelling the orientation of the left and right phrases of fj with maximum orientation span (the longest adjacent phrase in accordance with the orientation) we run through Lj and Rj, respectively."}, {"heading": "2.2.2 Translation Context Model", "text": "As with JMO in Section 2.1.1, we try to capture translation phenomena that appear local on the target hypothesis but not local on the source side, extending the LTM feature to predict not only the translated word ebj, but also its surrounding context. Formally, we further model the TCM in + d \u0445 d \u2032 = \u2212 d P (ebj + d \u2032 | Cj) and implement each as a separate neural network-based feature. Note that TCM corresponds to the LTM when d = 0. In practice, we further break down the word TCM into + d \u0445 d \u2032 = \u2212 d P (ebj + d \u2032 | Cj) and implement each as a separate neural network-based feature. Note that TCM corresponds to the LTM when d = 0. Due to the word reordering, a given hypothesis word in l (fj) cannot be associated with fj or even with the words in Cj."}, {"heading": "2.2.3 Combined Model", "text": "Since the attribute slab for unaligned source words is not defined, we build the model hierarchically, based on whether the source word is aligned or not, and thus arrive at the following formulation: P (l (fj) \u00b7 P (ori (fj))) \u00b7 P (\u03c6 (fj)) = P (\u03c6p (fj) = 0) \u00b7 P (oLj (Rj))) P (\u03c6p (fj) \u2265 1) \u00b7 + d (ebj + d \u2032) \u00b7 P (oLj (fj), oRj (fj))))) We drop the common context (Cj) for readability. We use Fig. 1 to illustrate the attributes of the source listing. If we take d = 1, the values associated with f7 are P (f7) for the longest (f7) and A (f7) for the longest (ff7) and Rf7 for the longest (7)."}, {"heading": "3 Tensor Neural Networks", "text": "The second part of this paper improves SMT by improving the neural network architecture. Neural networks derive their strength from their ability to automatically learn a high-level representation of the input from data. This high-level representation is typically constructed layer by layer by a weighted sum of linear operations and a nonlinear activation function. With sufficient training data, neural networks often achieve state-of-the-art performance for many tasks, in sharp contrast to other algorithms that require lengthy manual feature engineering erings.For the properties presented in this paper, context words are delivered to the network with minimal engineering.We strengthen the network's ability to learn rich interactions between its units by introducing tensors in the hidden layers. The multiplicative property of the tensor reveals a close similarity to the collocation of context words that are produced in many natural language processing chains of the preceding network output by conventional output."}, {"heading": "4 Multitask Learning", "text": "The third part of this thesis deals with the challenge of effectively learning a large number of neural network parameters without overlapping; the challenge is even greater for the tensor network, since it virtually doubles the number of parameters. In this section, we propose to apply multitask learning (MTL) to partially address this problem. We implement MTL as parameter sharing among networks. This effectively reduces the number of parameters, and more importantly, it takes advantage of parameters learned for one trait to improve the parameters of the other traits. Another way of looking at this is that MTL facilitates regulation by learning the other tasks. MTL is suitable for SMT traits as it models different but closely related aspects of the same translation process. MTL has long been used by the broader machine learning community (Caruana, 1997) and more recently for natural language processing (Colal, 2008, Collobert and Colloal)."}, {"heading": "5 Experiments", "text": "We will demonstrate the impact of our extensive MT experiments on Arabic-English and Chinese-English translations for the DARPA BOLT Web Forum and the terms and conditions of NIST OpenMT12."}, {"heading": "5.1 Baseline MT System", "text": "We conduct our experiments with a state-of-the-art string-to-dependency hierarchical decoder (Shen et al., 2010). The baseline we use includes a number of powerful features such as: \u2022 Forward and backward rule probabilities \u2022 Contextual lexical smoothing (Devlin et al., 2009) \u2022 5-gram Kneser-Ney LM \u2022 Dependency LM (Shen et al., 2010) \u2022 Length distribution (Shen et al., 2010) \u2022 Trait features (Devlin and Matsoukas, 2012) \u2022 Factored source syntax (Huang et al., 2013) \u2022 Discriminative sparse feature, a total of 50kfeatures (Chiang et al., 2009) \u2022 Neural Network Joint Model (NNJM) andNeural Network Lexical Translation Model (NLTM) (Devlin et al., 2014)."}, {"heading": "5.2 BOLT Discussion Forum", "text": "The majority of our experiments take place on the domain of the BOLT Web Discussion Forum, which uses the data collected by the LDC. Parallel training data consists of all high-quality NIST training corpus plus an additional 3 million words of translated forum data. Tuning and test kits consist of approximately 5000 segments each, with two independent references in Arabic and three in Chinese."}, {"heading": "5.2.1 Effects of New Features", "text": "First, we look at the impact of the proposed characteristics compared to the baseline system. Table 1 summarizes the primary results of the Arabic-English and Sino-English experiments for the BOLT condition. We show the experimental results related to hypothesis-enumerating features (HypEn) in lines S2-S5, those related to source-enumerating features (SrcEn) in lines S6-S9, and the combination of the two in line S10. For all characteristics, we set the source context length to m = 5 (11-word window). For JM and JMO, we set the target context length to n = 4. For the offset parameter of JMO, we use values 1 to 3. For TCM, we model a word around the translation (d = 1). Larger values of d did not lead to further gains in BLAR and JMO yields. The baseline is comparable to the best results of Devlin (2014, al)."}, {"heading": "5.2.2 Effects of Tensor Network and Multitask Learning", "text": "This year it has come to the point where we will be able to reete.n nEi \"rE tis rf\u00fc ide nlrteeaeSn,\" he says. \"nI,\" he says, \"is as if we are able to reete.n\" rE tis rf\u00fc ide nlrteeaeSn. \"\" nI, \"he says,\" is that we are able to reete.n \""}, {"heading": "5.3 NIST OpenMT12", "text": "Our NIST system is compatible with the restricted OpenMT12 teaching path, which consists of 10M words of high-quality parallel training in Arabic and 25M words in Chinese. The n-gram LM is based on 5B words of data from the English GigaWord. For the test, we use the \"Arabic-ToEnglish Original Progress Test\" (1378 segments) and the \"Chinese-to-English Original Progress Test + OpenMT12 Current Test\" (2190 segments), which consists of a mixture of messaging wire and web data. All test segments have 4 references. Our tuning set contains 5000 segments and is a mixture of the MT02-05 uniform sentence and additional parallel data from the training company. We report on the experiments for the NIST condition in Table 4. Specifically, we examine the impact of the use of our new features (Feat column) and demonstrate the effects of the BLT architecture (BLT column 7)."}, {"heading": "6 Related Work", "text": "In fact, it is as if most people who have lived and worked in the United States in recent years are able to understand the world and how they have behaved. It is as if they do not understand the world. It is as if they do not understand the world. It is as if they do not understand the world. It is as if they do not understand the world. It is as if they do not understand the world. It is as if they do not understand the world. It is as if they do not understand the world. It is as if they do not understand the world. It is as if they do not understand the world. It is as if they do not understand the world. It is as if they do not understand the world. It is as if they do not understand the world. It is as if they do not understand the world."}, {"heading": "7 Conclusion", "text": "We support this argument by presenting a multi-pronged approach that addresses the modeling, architecture, and learning aspects of existing neural network-based SMT functions. Specifically, we are presenting a new set of neural network-based SMT functions to capture important translation phenomena, extend neural networks with layers of tensors, and apply multi-task learning to integrate SMT functions more tightly. Empirically, all of our proposals successfully lead to an improvement over state-of-the-art machine translation systems for Arabic-to-English and Chinese-to-English, as well as for BOLT web forums and NIST conditions. Building on the success of this paper, we plan to develop additional neural network-based functions and also ease the constraints of current rule extraction euristics by generating translations word for word."}, {"heading": "Acknowledgement", "text": "This work is supported by DARPA / I2O Contract No. HR0011-12-C-0014 under the BOLT Program. The views, opinions and / or findings contained in this article are those of the author and should not be interpreted as representing the official views or guidelines of the Defense Advanced Research Projects Agency or the Department of Defense, either expressed or implied."}], "references": [{"title": "Joint language and translation modeling with recurrent neural networks", "author": ["Auli et al.2013] Michael Auli", "Michel Galley", "Chris Quirk", "Geoffrey Zweig"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Auli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Auli et al\\.", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "Technical Report 1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["Brown et al.1993] Peter F. Brown", "Vincent J. Della Pietra", "Stephen A. Della Pietra", "Robert L. Mercer"], "venue": "Comput. Linguist.,", "citeRegEx": "Brown et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Improving statistical machine translation using word sense disambiguation", "author": ["Carpuat", "Wu2007] Marine Carpuat", "Dekai Wu"], "venue": "In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Com-", "citeRegEx": "Carpuat et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Carpuat et al\\.", "year": 2007}, {"title": "11,001 new features for statistical machine translation", "author": ["Chiang et al.2009] David Chiang", "Kevin Knight", "Wei Wang"], "venue": "In HLT-NAACL,", "citeRegEx": "Chiang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chiang et al\\.", "year": 2009}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Trait-based hypothesis selection for machine translation", "author": ["Devlin", "Matsoukas2012] Jacob Devlin", "Spyros Matsoukas"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association", "citeRegEx": "Devlin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2012}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Devlin et al.2014] Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard Schwartz", "John Makhoul"], "venue": null, "citeRegEx": "Devlin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Lexical features for statistical machine translation. Master\u2019s thesis, University of Maryland", "author": ["Jacob Devlin"], "venue": null, "citeRegEx": "Devlin.,? \\Q2009\\E", "shortCiteRegEx": "Devlin.", "year": 2009}, {"title": "Joint parsing and named entity recognition", "author": ["Finkel", "Manning2009] Jenny Rose Finkel", "Christopher D. Manning"], "venue": "In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter", "citeRegEx": "Finkel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Finkel et al\\.", "year": 2009}, {"title": "Morphological analysis and disambiguation for dialectal arabic", "author": ["Habash et al.2013] Nizar Habash", "Ryan Roth", "Owen Rambow", "Ramy Eskander", "Nadi Tomeh"], "venue": "In HLT-NAACL,", "citeRegEx": "Habash et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Habash et al\\.", "year": 2013}, {"title": "Factored soft source syntactic constraints for hierarchical machine translation", "author": ["Jacob Devlin", "Rabih Zbib"], "venue": "In EMNLP,", "citeRegEx": "Huang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "Tensor deep stacking networks", "author": ["Li Deng", "Dong Yu"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Hutchinson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hutchinson et al\\.", "year": 2013}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "A convolutional neural network for modelling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Continuous space translation models with neural networks", "author": ["Le et al.2012] Hai-Son Le", "Alexandre Allauzen", "Fran\u00e7ois Yvon"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguis-", "citeRegEx": "Le et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Le et al\\.", "year": 2012}, {"title": "An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation", "author": ["Lee", "Ng2002] Yoong Keok Lee", "Hwee Tou Ng"], "venue": "In Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language", "citeRegEx": "Lee et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2002}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Och", "Ney2003] Franz Josef Och", "Hermann Ney"], "venue": "Computational Linguistics,", "citeRegEx": "Och et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Och et al\\.", "year": 2003}, {"title": "Max-margin tensor neural network for chinese word segmentation", "author": ["Pei et al.2014] Wenzhe Pei", "Tao Ge", "Baobao Chang"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Pei et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pei et al\\.", "year": 2014}, {"title": "BBN system description for WMT10 system combination task", "author": ["Rosti et al.2010] Antti Rosti", "Bing Zhang", "Spyros Matsoukas", "Rich Schwartz"], "venue": "In WMT/MetricsMATR,", "citeRegEx": "Rosti et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rosti et al\\.", "year": 2010}, {"title": "Continuousspace language models for statistical machine translation", "author": ["Holger Schwenk"], "venue": "Prague Bull. Math. Linguistics,", "citeRegEx": "Schwenk.,? \\Q2010\\E", "shortCiteRegEx": "Schwenk.", "year": 2010}, {"title": "Continuous space translation models for phrase-based statistical machine translation", "author": ["Holger Schwenk"], "venue": "In COLING (Posters),", "citeRegEx": "Schwenk.,? \\Q2012\\E", "shortCiteRegEx": "Schwenk.", "year": 2012}, {"title": "Two-neighbor orientation model with cross-boundary global contexts", "author": ["Bowen Zhou", "Bing Xiang", "Libin Shen"], "venue": "In Proceedings of the 51st Annual Meeting", "citeRegEx": "Setiawan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Setiawan et al\\.", "year": 2013}, {"title": "String-to-dependency statistical machine translation", "author": ["Shen et al.2010] Libin Shen", "Jinxi Xu", "Ralph Weischedel"], "venue": "Computational Linguistics,", "citeRegEx": "Shen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2010}, {"title": "Joint inference of entities, relations, and coreference", "author": ["Singh et al.2013] Sameer Singh", "Sebastian Riedel", "Brian Martin", "Jiaping Zheng", "Andrew McCallum"], "venue": "In Proceedings of the 2013 Workshop on Automated Knowledge Base Construction,", "citeRegEx": "Singh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2013}, {"title": "Language and translation model adaptation using comparable corpora", "author": ["Bonnie Dorr", "Richard Schwartz"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Snover et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Snover et al\\.", "year": 2008}, {"title": "Parsing Natural Scenes and Natural Language with Recursive Neural Networks", "author": ["Cliff C. Lin", "Andrew Y. Ng", "Christopher D. Manning"], "venue": "In Proceedings of the 26th International Conference on Machine Learning", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Danqi Chen", "Christopher D Manning", "Andrew Ng"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Translation modeling with bidirectional recurrent neural networks", "author": ["Tamer Alkhouli", "Joern Wuebker", "Hermann Ney"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language", "citeRegEx": "Sundermeyer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. V Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A unigram orientation model for statistical machine translation", "author": ["Christoph Tillman"], "venue": "HLT-NAACL 2004: Short Papers,", "citeRegEx": "Tillman.,? \\Q2004\\E", "shortCiteRegEx": "Tillman.", "year": 2004}, {"title": "Large vocabulary speech recognition", "author": ["Yu et al.2012] Dong Yu", "Li Deng", "Frank Seide"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 30, "context": "ther develop neural network-based features that are used to score hypotheses generated from traditional translation grammars (Sundermeyer et al., 2014; Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012), or they implement", "startOffset": 125, "endOffset": 223}, {"referenceID": 9, "context": "ther develop neural network-based features that are used to score hypotheses generated from traditional translation grammars (Sundermeyer et al., 2014; Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012), or they implement", "startOffset": 125, "endOffset": 223}, {"referenceID": 0, "context": "ther develop neural network-based features that are used to score hypotheses generated from traditional translation grammars (Sundermeyer et al., 2014; Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012), or they implement", "startOffset": 125, "endOffset": 223}, {"referenceID": 17, "context": "ther develop neural network-based features that are used to score hypotheses generated from traditional translation grammars (Sundermeyer et al., 2014; Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012), or they implement", "startOffset": 125, "endOffset": 223}, {"referenceID": 23, "context": "ther develop neural network-based features that are used to score hypotheses generated from traditional translation grammars (Sundermeyer et al., 2014; Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012), or they implement", "startOffset": 125, "endOffset": 223}, {"referenceID": 1, "context": "the whole translation process as a single neural network (Bahdanau et al., 2014; Sutskever et al., 2014).", "startOffset": 57, "endOffset": 104}, {"referenceID": 31, "context": "the whole translation process as a single neural network (Bahdanau et al., 2014; Sutskever et al., 2014).", "startOffset": 57, "endOffset": 104}, {"referenceID": 9, "context": "We build on (Devlin et al., 2014) who proposed a simple yet powerful feedforward neural network model that estimates the translation probability conditioned on the target history and a large win-", "startOffset": 12, "endOffset": 33}, {"referenceID": 33, "context": "\u2022 We use a Tensor Neural Network Architecture (Yu et al., 2012) to automatically learn complex pairwise interactions between the network nodes.", "startOffset": 46, "endOffset": 63}, {"referenceID": 9, "context": "obtaining strong experimental results over the strongest previous results of (Devlin et al., 2014).", "startOffset": 77, "endOffset": 98}, {"referenceID": 9, "context": "9 BLEU points for Chinese-English on the NIST Open12 test sets over the best previously published results in (Devlin et al., 2014).", "startOffset": 109, "endOffset": 130}, {"referenceID": 8, "context": "One recent such model, the joint model of Devlin et al. (2014) achieves large improvements to the stateof-the-art SMT by using a large context window of 11 source words and 3 target words.", "startOffset": 42, "endOffset": 63}, {"referenceID": 8, "context": "We use the definition of word affiliation introduced in Devlin et al. (2014). When no source context is used, the model is equivalent to an n-gram language model, while an offset parameter of k = 0 reduces the model to the JM of Devlin et al.", "startOffset": 56, "endOffset": 77}, {"referenceID": 8, "context": "We use the definition of word affiliation introduced in Devlin et al. (2014). When no source context is used, the model is equivalent to an n-gram language model, while an offset parameter of k = 0 reduces the model to the JM of Devlin et al. (2014).", "startOffset": 56, "endOffset": 250}, {"referenceID": 9, "context": "Lexical Translation model (LTM) estimates the probability of translating a source word fj to a target word l(fj) = ebj given a source context Cj , bj \u2208 B is the source-to-target word affiliation as defined in (Devlin et al., 2014).", "startOffset": 209, "endOffset": 230}, {"referenceID": 24, "context": "We follow (Setiawan et al., 2013) in modeling the orientation of the left and right phrases of fj with maximal orientation span (the longest neighboring phrase consistent with alignment), which we denote by Lj and Rj respectively.", "startOffset": 10, "endOffset": 33}, {"referenceID": 33, "context": "In our implementation, we follow (Yu et al., 2012; Hutchinson et al., 2013) and use a low-rank approximation of Ul[k] = Ql[k] \u00b7 Rl[k] , where Ql[k], Rl[k] \u2208 Rn\u00d7r.", "startOffset": 33, "endOffset": 75}, {"referenceID": 14, "context": "In our implementation, we follow (Yu et al., 2012; Hutchinson et al., 2013) and use a low-rank approximation of Ul[k] = Ql[k] \u00b7 Rl[k] , where Ql[k], Rl[k] \u2208 Rn\u00d7r.", "startOffset": 33, "endOffset": 75}, {"referenceID": 7, "context": "the wider machine learning community (Caruana, 1997) and more recently for natural language processing (Collobert and Weston, 2008; Collobert et al., 2011).", "startOffset": 103, "endOffset": 155}, {"referenceID": 9, "context": "In this and in the other parts of the paper, we add the normalization regularization term described in (Devlin et al., 2014) to the loss function to avoid computing the normalization constant at model query/decoding time.", "startOffset": 103, "endOffset": 124}, {"referenceID": 25, "context": "We run our experiments using a state-of-the-art string-to-dependency hierarchical decoder (Shen et al., 2010).", "startOffset": 90, "endOffset": 109}, {"referenceID": 10, "context": "The baseline we use includes a set of powerful features as follow: \u2022 Forward and backward rule probabilities \u2022 Contextual lexical smoothing (Devlin, 2009) \u2022 5-gram Kneser-Ney LM \u2022 Dependency LM (Shen et al.", "startOffset": 140, "endOffset": 154}, {"referenceID": 25, "context": "The baseline we use includes a set of powerful features as follow: \u2022 Forward and backward rule probabilities \u2022 Contextual lexical smoothing (Devlin, 2009) \u2022 5-gram Kneser-Ney LM \u2022 Dependency LM (Shen et al., 2010) \u2022 Length distribution (Shen et al.", "startOffset": 194, "endOffset": 213}, {"referenceID": 25, "context": ", 2010) \u2022 Length distribution (Shen et al., 2010) \u2022 Trait features (Devlin and Matsoukas, 2012) \u2022 Factored source syntax (Huang et al.", "startOffset": 30, "endOffset": 49}, {"referenceID": 13, "context": ", 2010) \u2022 Trait features (Devlin and Matsoukas, 2012) \u2022 Factored source syntax (Huang et al., 2013) \u2022 Discriminative sparse feature, totaling 50k features (Chiang et al.", "startOffset": 79, "endOffset": 99}, {"referenceID": 5, "context": ", 2013) \u2022 Discriminative sparse feature, totaling 50k features (Chiang et al., 2009) \u2022 Neural Network Joint Model (NNJM) and Neural Network Lexical Translation Model", "startOffset": 63, "endOffset": 84}, {"referenceID": 9, "context": "(NNLTM) (Devlin et al., 2014)", "startOffset": 8, "endOffset": 29}, {"referenceID": 12, "context": "We use the MADA-ARZ tokenizer (Habash et al., 2013) for Arabic word tokenization.", "startOffset": 30, "endOffset": 51}, {"referenceID": 21, "context": "For tuning the weights of MT features including the new features, we use iterative k-best optimization with an ExpectedBLEU objective function (Rosti et al., 2010), and decode the test sets after 5 tuning iteration.", "startOffset": 143, "endOffset": 163}, {"referenceID": 9, "context": "The baseline is comparable to the best results of (Devlin et al., 2014).", "startOffset": 50, "endOffset": 71}, {"referenceID": 9, "context": "BLEU point in ZH-EN on top of the best results in (Devlin et al., 2014).", "startOffset": 50, "endOffset": 71}, {"referenceID": 8, "context": "Devlin et al. (2014) report 52.", "startOffset": 0, "endOffset": 21}, {"referenceID": 8, "context": "Our work is most closely related to Devlin et al. (2014). They use a simple feedforward neural network to model two important MT features: A joint language and translation model, and a lexical translation model.", "startOffset": 36, "endOffset": 57}, {"referenceID": 3, "context": "The features we propose in this paper address the major aspects of SMT modeling that have informed much of the research since the original IBM models (Brown et al., 1993): lexical translation, reordering, word fertility, and language models.", "startOffset": 150, "endOffset": 170}, {"referenceID": 32, "context": "into the models (Carpuat and Wu, 2007), formulate reordering as orientation prediction task (Tillman, 2004) and that use neural network language models (Bengio et al.", "startOffset": 92, "endOffset": 107}, {"referenceID": 2, "context": "into the models (Carpuat and Wu, 2007), formulate reordering as orientation prediction task (Tillman, 2004) and that use neural network language models (Bengio et al., 2003; Schwenk, 2010; Schwenk, 2012), and incorporate source-side con-", "startOffset": 152, "endOffset": 203}, {"referenceID": 22, "context": "into the models (Carpuat and Wu, 2007), formulate reordering as orientation prediction task (Tillman, 2004) and that use neural network language models (Bengio et al., 2003; Schwenk, 2010; Schwenk, 2012), and incorporate source-side con-", "startOffset": 152, "endOffset": 203}, {"referenceID": 23, "context": "into the models (Carpuat and Wu, 2007), formulate reordering as orientation prediction task (Tillman, 2004) and that use neural network language models (Bengio et al., 2003; Schwenk, 2010; Schwenk, 2012), and incorporate source-side con-", "startOffset": 152, "endOffset": 203}, {"referenceID": 9, "context": "text into them (Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012).", "startOffset": 15, "endOffset": 87}, {"referenceID": 0, "context": "text into them (Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012).", "startOffset": 15, "endOffset": 87}, {"referenceID": 17, "context": "text into them (Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012).", "startOffset": 15, "endOffset": 87}, {"referenceID": 23, "context": "text into them (Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012).", "startOffset": 15, "endOffset": 87}, {"referenceID": 9, "context": "tion of the source sentence, we follow (Devlin et al., 2014) in using a window around the affiliated source word.", "startOffset": 39, "endOffset": 60}, {"referenceID": 0, "context": "To name some other approaches, Auli et al. (2013) uses latent semantic analysis and source sentence embeddings learned from the recurrent neural network; Sundermeyer et al.", "startOffset": 31, "endOffset": 50}, {"referenceID": 0, "context": "To name some other approaches, Auli et al. (2013) uses latent semantic analysis and source sentence embeddings learned from the recurrent neural network; Sundermeyer et al. (2014) take the representation from a bidirectional LSTM recurrent neural network; and Kalchbrenner and Blunsom (2013) employ a convolutional sentence model.", "startOffset": 31, "endOffset": 180}, {"referenceID": 0, "context": "To name some other approaches, Auli et al. (2013) uses latent semantic analysis and source sentence embeddings learned from the recurrent neural network; Sundermeyer et al. (2014) take the representation from a bidirectional LSTM recurrent neural network; and Kalchbrenner and Blunsom (2013) employ a convolutional sentence model.", "startOffset": 31, "endOffset": 292}, {"referenceID": 0, "context": "(Auli et al., 2013; Sundermeyer et al., 2014) consider an unbounded history, at the expense of making their model only applicable for N-best rescoring.", "startOffset": 0, "endOffset": 45}, {"referenceID": 30, "context": "(Auli et al., 2013; Sundermeyer et al., 2014) consider an unbounded history, at the expense of making their model only applicable for N-best rescoring.", "startOffset": 0, "endOffset": 45}, {"referenceID": 16, "context": "works, other network architectures that have been applied to SMT include convolutional networks (Kalchbrenner et al., 2014) and recursive networks (Socher et al.", "startOffset": 96, "endOffset": 123}, {"referenceID": 28, "context": ", 2014) and recursive networks (Socher et al., 2011).", "startOffset": 31, "endOffset": 52}, {"referenceID": 33, "context": "(Yu et al., 2012; Hutchinson et al., 2013).", "startOffset": 0, "endOffset": 42}, {"referenceID": 14, "context": "(Yu et al., 2012; Hutchinson et al., 2013).", "startOffset": 0, "endOffset": 42}, {"referenceID": 29, "context": "Tensor Neural Networks have a wide application in other field, but have only been recently applied in NLP (Socher et al., 2013; Pei et al., 2014).", "startOffset": 106, "endOffset": 145}, {"referenceID": 20, "context": "Tensor Neural Networks have a wide application in other field, but have only been recently applied in NLP (Socher et al., 2013; Pei et al., 2014).", "startOffset": 106, "endOffset": 145}, {"referenceID": 26, "context": "and Manning (2009) successfully train name entity recognizers and syntactic parsers jointly, and Singh et al. (2013) train models for coreference resolution, named entity recognition and relation extraction jointly.", "startOffset": 97, "endOffset": 117}, {"referenceID": 6, "context": "Our work is most closely related to Collobert and Weston (2008; Collobert et al. (2011), who apply multitask learning to train neural networks for multiple NLP models: part-of-speech tagging, semantic role labeling, named-entity recognition and language model variations.", "startOffset": 64, "endOffset": 88}], "year": 2015, "abstractText": "We present a three-pronged approach to improving Statistical Machine Translation (SMT), building on recent success in the application of neural networks to SMT. First, we propose new features based on neural networks to model various nonlocal translation phenomena. Second, we augment the architecture of the neural network with tensor layers that capture important higher-order interaction among the network units. Third, we apply multitask learning to estimate the neural network parameters jointly. Each of our proposed methods results in significant improvements that are complementary. The overall improvement is +2.7 and +1.8 BLEU points for Arabic-English and ChineseEnglish translation over a state-of-the-art system that already includes neural network features.", "creator": "LaTeX with hyperref package"}}}