{"id": "1506.00779", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2015", "title": "Optimal Regret Analysis of Thompson Sampling in Stochastic Multi-armed Bandit Problem with Multiple Plays", "abstract": "We discuss a multiple-play multi-armed bandit (MAB) problem in which several arms are selected at each round. Recently, Thompson sampling (TS), a randomized algorithm with a Bayesian spirit, has attracted much attention for its empirically excellent performance, and it is revealed to have an optimal regret bound in the standard single-play MAB problem. In this paper, we propose the multiple-play Thompson sampling (MP-TS) algorithm, an extension of TS to the multiple-play MAB problem, and discuss its regret analysis. We prove that MP-TS for binary rewards has the optimal regret upper bound that matches the regret lower bound provided by Anantharam et al. (1987). Therefore, MP-TS is the first computationally efficient algorithm with optimal regret. A set of computer simulations was also conducted, which compared MP-TS with state-of-the-art algorithms. We also propose a modification of MP-TS, which is shown to have better empirical performance.", "histories": [["v1", "Tue, 2 Jun 2015 07:42:16 GMT  (660kb,D)", "http://arxiv.org/abs/1506.00779v1", "17 pages, 7 figures, to appear in ICML2015"], ["v2", "Tue, 24 May 2016 12:21:19 GMT  (673kb,D)", "http://arxiv.org/abs/1506.00779v2", "Appeared in ICML2015. Fixed the evaluation of term (B) in Lemma 3"]], "COMMENTS": "17 pages, 7 figures, to appear in ICML2015", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["junpei komiyama", "junya honda", "hiroshi nakagawa"], "accepted": true, "id": "1506.00779"}, "pdf": {"name": "1506.00779.pdf", "metadata": {"source": "META", "title": "Optimal Regret Analysis of Thompson Sampling in Stochastic Multi-armed Bandit Problem with Multiple Plays", "authors": ["Junpei Komiyama", "Junya Honda", "Hiroshi Nakagawa"], "emails": ["JUNPEI@KOMIYAMA.INFO", "HONDA@STAT.T.U-TOKYO.AC.JP", "NAKAGAWA@DL.ITC.U-TOKYO.AC.JP"], "sections": [{"heading": "1. Introduction", "text": "The Multi-Arm Bandit Problem (MAB) is one of the most well-known cases of sequential decision-making problems in uncertain environments that can model many real-world scenarios, involving conceptual entities known as arms. In each round, the forecaster pulls one of the K-arms and receives a corresponding reward. The goal of the forecaster is to maximize the cumulative reward over rounds, and the forecaster's performance is usually measured by a regret, which is the gap between his or her cumulative reward and that of an optimal drawing policy. During the rounds, the forecaster faces an \"exploration vs. exploitation dilemma.\" On the one hand, the forecaster wants to exploit the information he or she has gathered in the previous round by selecting seemingly good weapons. On the other hand, there is always the possibility that the other weapons have been underestimated, which motivates him or her to apparently use bad weapons to gather their information."}, {"heading": "1.1. Multiple-play MAB problem", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "2. Problem Setup", "text": "Each arm i-K = {1, 2,..., K-Weapons is associated with a probability distribution \u03bdi = Bernoulli (\u00b5i), \u00b5i-Weapons (0, 1). On each turn t = 1, 2,.., T the forecaster selects a set of L < K-Weapons I (t) and then receives the rewards of the selected weapons. So the reward Xi (t) of each selected arm i is 1 {A} = 1 if event A applies and otherwise 0. Letter Ni (t) is the number of arm moves i before round t (i.e., Ni (t) = 1 t-Weapons = 1 {i-I (t)}, where 1 {A} = 1 if event A applies and otherwise 0.), and \u00b5-Weapons i (t) are the empirical mean of the weapons i-Weapons at the beginning of the turn. The forecaster is interested in maximizing the sum of the weapons drawn."}, {"heading": "3. Regret Bounds", "text": "In this section we present the known lower limits of regret for the problems SP-MAB and MP-MAB and discuss the relationship between them."}, {"heading": "3.1. Regret bound for SP-MAB problem", "text": "The SP-MAB problem, which has been extensively studied in the fields of statistics and machine learning, is a special case of the MP-MAB problem with L = 1. Optimal regret in the SP-MAB problem was given by Lai & Robbins (1985), who proved that for any highly consistent algorithm (i.e. algorithms with subpolynomial regret for each group of weapons) there is a lower limit E [Ni (T + 1)] \u2265 (1 \u2212 o (1) d (\u00b5i, \u00b51)))) log T, (1) where d (p, q) = p log (p / q) + (1 \u2212 p) log (1 \u2212 p) / (1 \u2212 q))) the KL divergence is between two Bernoulli distributions with expectation p and q. Note, however, that when the arm i is drawn, the regret increases by \u0394i, 1 and the regret is written as E [Reg (T)."}, {"heading": "3.2. Extension to MP-MAB problem", "text": "The regret lower limit in the MP-MAB problem, which is the generalization of inequality (3), was provided by Anantharam et al. (1987). First, they proved that the number of arms that i pulls is lower than the number of arms that i pulls (Ni (T + 1)) \u2265 (1 \u2212 o (1) d (\u00b5i, \u00b5L) log T. (4) In contrast to the SP-MAB problem, regret in the MP-MAB problem is not clearly determined by the number of sub-optimal arms (T + 1). As shown in Figure 1, regret depends on the combinatorial structure of the armrests. Consider that a regret increase in each round is the gap of expected rewards between the optimal arms and those of the selected arms. If a sub-optimal arm is selected, an optimal arm is excluded from I (t) instead of the sub-optimal arm."}, {"heading": "3.3. Necessary condition for an optimal algorithm", "text": "In sections 3.1 and 3.2 we saw that the derivatives of regret limits are analogous between the SP-MAB and MP-MAB problems. However, there is a difference in the relationship between the regret andNi (T), the number of draws of suboptimal arms, is given as equation (2) in the SP-MAB problem, while there is given as inequality (6) in the MPMAB problem. This means that an algorithm that reaches the asymptotic lower limit (4) on Ni (T) is not always tied to the asymptotic regret (7).When suboptimal arm i is selected, one of the optimal arms is pushed instead of arm i, and the regret increases by the difference between the expected rewards of these two arms. The best scenario is that arm L, which is the optimal arm with the least expected reward, is almost always the arm that is pushed rather than a suboptimal arm."}, {"heading": "4. Multiple-play Thompson Sampling Algorithm", "text": "Like Kaufmann et al. (2012) and Agrawal & Goyal (2013b), we specify the uniform in front of each arm. In Section 3.3 we discussed that the necessary condition to reach the optimum limit of regret is to suppress the simultaneous drawings of two or more sub-optimal arms, which characterizes the difficulty of the MP-MAB problem. Note that it is easy to extend other asymptotically optimal SP-MAB algorithms, such as KL-UCB, to the MPMAB problem. However, we have not been able to prove the optimality of these algorithms for the MP-MAB problem, although the attainability of the limit (4) to Ni (T) can easily be proven."}, {"heading": "5. Optimal Regret Bound", "text": "The analysis leading to this theorem is discussed in Section 6.Theorem 1. (Regret upper limit of MP-TS) For all sufficiently small 1 > 0, 2 > 0, the regret of MP-TS is above the limit of remorse (Reg (T)] \u2264 [K]\\ [L] (((((1 + 1) \u2206 i, L log Td (\u00b5i, \u00b5L))) + Ca (1, \u00b51, \u00b52,.., \u00b5K) + Cb (T, 2, \u00b51, \u00b52,. \u2212 K), where Ca = Ca (1, \u00b51, \u00b52,.., \u00b5K) is a constant independent of T and is O (\u2212 21) if we consider {\u00b5i} Ki = 1 as a constant. \u2212 The value Cb = Cb (T, 2, \u00b51, \u00b52,)."}, {"heading": "6. Regret Analysis", "text": "We will first define some additional notations that are useful for our analysis in Section 6.1, and then analyze the regret defined in Section 6.2. The evidence of all of the terms, with the exception of Lemma 2, is listed in the appendix."}, {"heading": "6.1. Additional notation", "text": "We assume that these are sufficiently small units, so that the number of explorations that ensure that the arm i is not as good as the arm L.Events: Let's leave max. (m) i. \u2212 S ai the m-largest element of {ai} i. \u2212 S | S | S | i.e. maxi. (m) ai = maxS \u2032 S \u2032 S (m) ai is not as good as the arm L.Events: Let's leave max. (m) i. \u2212 T as the m-largest element of {ai} i."}, {"heading": "6.2. Proof of Theorem 1", "text": "We first decompose the regret for the contribution of each arm. Remember that the increase in regret is determined by the drawing of suboptimal regret (11) by the optimal arm (11), which is excluded in the selection group I (t) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) ((n) (n) ((n) (n) (n) (n) ((n) (n) ((n) ((n) ((n) ((n) ((n) ((n) (n) ((n) ((n) (n) ((n) (n) (((n) (n) (((n) ((n) ((n) (n) (((n) (n) (((n) ((n) (n) ((n) ("}, {"heading": "7. Experiment", "text": "In Scenarios 1 and 2, we used fixed arms similar to those of Garivier & Cappe (2011), and Scenario 3 is based on a click log record of ads on a commercial search engine. Algorithms: the simulations, the MP-TS / jkomiyama / multiplaybanditlib.MP-UCB et al., 2010), CUCB et al., 2013), and1The source code of the simulations is available at https: / / github.com / jkomiyama / multiplaybanditlib.MP-UCB et al."}, {"heading": "7.1. Improvement of MP-TS based on the empirical means", "text": "We now present an improved version of MP-TS (IMPTS). In the theoretical analysis of the MP-MAB problem, we observed that an additional loss occurs when several sub-optimal arms are drawn at the same time. In other words, this algorithm is further aimed at minimizing regret by purely exploiting the knowledge in the upper (L \u2212 1) arms; and so it limits exploration to just one arm. One might fear that this increase in exploitation could upset the balance between exploration and exploitation. Although we do not set limits of regret for the improved version of the algorithm, we expect that this algorithm will also reach the asymptotic limit for the following reason."}, {"heading": "8. Discussion", "text": "We have looked at the case where the total reward is linear to the individual rewards of selected weapons, and the analysis in this paper takes full advantage of the independent property of rear samples and paves the way for a detailed analysis of the multiple-play regret depending on the combinatorial structure of the arm selection. We now point to two promising directions for future work. \u2022 Position-dependent factors for online advertising: It is well known that the CTR of an ad depends on its position. Taking into account the position-dependent factor, the MP-MAB problem changes from the L selection problem to the L sequence selection problem where the position of L-weapons plays a role. As a starting point, we consider an extension of MP-TS for the cascade model (Kempe & Mahdian, 2008; Aggarwal et al., 2008), which corrects the position-dependent bias in Appendix A.2. \u2022 Non-Bernoulli distributions for the general 1996 case for the coronation problem (the only one corrects for the coronation result)."}, {"heading": "Acknowledgements", "text": "We thank the insightful advice of Issei Sato and Tao Qin. We thank the anonymous reviewers for their useful comments. This work was partially supported by JSPS KAKENHI Grant Number 26106506."}, {"heading": "A. Appendix", "text": "Here we look at cases in which some arms have the same expectations. Without losing the general public, we assume that a minor problem is less than a minor sub-optimal one. \"Each arm is either strictly optimal, marginal or strictly sub-optimal.\" Case 1: Assumption that all arms are strictly optimal, that there is only one marginal arm, and that there are several strictly sub-optimal arms with the same expectation. In this case, regret is limited by theorem 1. Our analysis deals with each sub-optimal arm separately. Case 2: Assumption that there is only one marginal arm, and that there are several sub-optimal arms with the same expectation."}, {"heading": "A.2.1. THOMPSON SAMPLING FOR CASCADE MODEL", "text": "In the cascade model, there is a certain probability that the arm at position l > 1 will not be drawn. The probability that the arm at position l will be drawn is the effective number of draws at position i. MP-TS (algorithm 1) keeps Ai and Bi, which correspond respectively to the number of rewards 1 and 0. The number of draws at arm i is Ni = Ai + Bi. Looking at the cascade model, we must take into account the effective number of draws. We introduce bias-corrected MP-TS (BC-MP-TS, algorithm 2). The point of BC-MP-TS is that Ni should not be increased by 1 for each selected arm, but by the effective number of draws for each position."}, {"heading": "A.2.2. OPTIMAL ARM SELECTION AND THE REGRET", "text": "Generally speaking, the discount factor \u03b3l (i), even if we have perfect information about the expectation of all arms {\u00b5i} Ki = 1, calculating the optimal order of L-arms in each round t (optimal arm selection) seems mathematically impossible when K is large, because we have to search for all possible allocations of K-indicators of L-positions. Kempe & Mahdian (2008) proposed a polynomial time approximation of the optimal arm selection. We can obtain the arm selection strategy for BC-MP-TS by using this approximation algorithm as an oracle and connecting the selection of K-indicators of L-positions. Li = 1 as estimated expected reward. Ad-independent discount factor: If the discount factor is independent of the display in this position (i.e., we l (i) = approl), the optimal arm selection is simple: choose \u00b5l (i.e., the best position in the arm)."}, {"heading": "A.2.3. EXPERIMENT OF CASCADE MODEL", "text": "This simulation adapts the cascade model and includes a constant discount factor \u03b3l (i) = 0.7 for each position and each arm. There are 9 Bernoulli arms with \u00b51 = 0.24, \u00b52 = 0.21,.., \u00b59 = 0.00 and L = 3. In this case, the optimal arm selection strategy is to choose {I1 (t), I2 (t), I3 (t)} = {\u00b51, \u00b52, \u00b53} (cf. Section A.2.2). The regret of the algorithms is evident in 4. On the one hand, MP-TS has shown no regret due to its ignorance of the discount factors. On the other hand, the slope of BC-MP-TS is rapidly approaching the assumed lower limit, which is empirical evidence for the ability of BC-MP-TS to correct the position-dependent bias."}, {"heading": "A.3. Key fact and lemmas", "text": "Fact 5 (Chernoff delimited for binary random variables) LetX1,.., Xn be i.i.d. binary random variables. Let X = 1 n \u2211 n i = 1Xi and \u00b5 = E [Xi]. Then, for any sample (0, 1 \u2212 \u00b5), Pr (X \u2265 \u00b5 +) \u2264 exp (\u2212 d (\u00b5 +, \u00b5) n). Fact 6 (Beta-Binomial equality) Let F beta\u03b1, \u03b2 (y) be the cdf of the beta distribution with integer parameters \u03b1 and \u03b2. Let FBn, p (\u00b7) be the cdf of the binomial distribution n).Fact 6 (Beta-Binomial equality) Let F beta\u03b1, \u03b2 (y) be the cdf of the integer parameters \u03b1 and \u03b2. Let FBn, p (\u00b7) be the cdf of the binomial distribution with parameters n, p. p."}, {"heading": "Then", "text": "Using the following inequality (T) (T) < z, S (t) and U (t) (T) (T) (1) < z, S (t) < z, S (t))) and U (t) the trivial events that always hold (q = 1), we get the following inequality (T) (T) < z, S (t), U (t), T (t) \u2264 n = 0 (\u00b5k) 2). (17) Proof. First, we have T (t) = 1 {\u03b8k (t) < z (t), S (t), U (t)."}, {"heading": "A.4. Proof of Lemma 3", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Evaluation of term (A):", "text": "Let us prove inequality here (13). Remember that (A) = T \u00b2 t = 1 {Bc (t)} = T \u00b2 t = 1 {\u03b8 * (t) < \u00b5 (\u2212) L \u00b2. Since in [L] there is at least one arm with its back sample smaller than \u00b5 (\u2212) L. Namely, this means that in [L] there is at least one arm with its back sample smaller than \u00b5 (\u2212) L. Namely, {\u03b8 (t) < \u00b5 (\u2212) L \u00b2 (\u2212). Means that in [L] there is at least one arm, and therefore in [T) < (\u2212) L \u00b2 (\u2212) L} = k \u00b2 [L] {\u03b8k (t) < \u00b5 (\u2212) L (\u2212 lt; L)."}, {"heading": "Evaluation of term (B):", "text": "(B) = > < i = > i = < i = 1 {Ai (t), Cci (t), = T \u2211 t = 1 j (K), (B) = > < i = > i = < i), (i), (i), (i), (i), (i), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t, (t), (t), (t), (t), (t), (t), (t), (t), (t), (t, (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t, (t), (t), (t, (t), (t), (t, (t), (t), (t, (t), (t), (t, (t), (t), (t, (t), (t, (t), (t), (t), (t, (t), (t, (t), (t, (t), (t), (t, (t), (t), (t), (t, (t), (t, (t), (t), (t, (t), (t, (t), (t), (t, (t), (t), (t, (t), (t, (t, (t), (t), (t, (t), (t, (t), (t, (t), (t), (t, (t, (t), (t), (t), (t), (t, (t, (t), (t, ("}, {"heading": "Evaluation of term (C):", "text": "Evidence of inequality (15), Ci (t), D (t), D (n), D (t), D (t), D (n), D (t), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n, D (n), D (n), D (n, D (n), D (n), D (n), D (n (n), D (n), D (n (n), D (n), D (n (n), D (n), D (n (n), D (n (n), D (n, D (n), D (n), D (n), D (n (n), D (n, D (n), D (n, D (n), D (n), D (n), D (n, D (n, D (n), D (n), D (n, D (n), D (n, D (n, D (n), D (n, D (n), D (n, D (n), D (n, D (n), D (n, D (n), D (n), D (n), D (n, D ("}, {"heading": "Evaluation of term (D):", "text": "The proof: Here we prove inequality (16). First, we divide the term (D) into two subconcepts as follows: E [(D) = E [T] = E [T] = T [s), B (t), B (t), Ni (t) \u2265 N (T)] \u2264 E [T) = 1 {Ai (t), B (t), \u00b5 (t) \u2264 i (t), Ni (t) \u2264 N (T). (30) On the one hand, the first term becomes in (30) + E [T [t) = 1 {Ai (t), B (t), \u00b5 (t) \u2264 i (t), Ni (t) [t). (30) The first term in (Y) is limited as: E [T [s). (t). (t)."}, {"heading": "A.5. Proof of Lemma 4", "text": "Suffice it to prove that for each a, b > 0inf 2 > 0 {T \u2212 a 2b + 2} = O (log Tlog T). By allowing 2 = (log T) / (a log T), we have 2 > 0 {T \u2212 a 2b + 2} = inf 2 > 0 {e \u2212 a 2 log T b + 2} \u2264 e \u2212 log Tb + log Ta log T = 1b log T + log Ta log T = O (log Tlog T) and the proof is complete."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>We discuss a multiple-play multi-armed ban-<lb>dit (MAB) problem in which several arms are<lb>selected at each round. Recently, Thompson<lb>sampling (TS), a randomized algorithm with a<lb>Bayesian spirit, has attracted much attention for<lb>its empirically excellent performance, and it is<lb>revealed to have an optimal regret bound in the<lb>standard single-play MAB problem. In this pa-<lb>per, we propose the multiple-play Thompson<lb>sampling (MP-TS) algorithm, an extension of TS<lb>to the multiple-play MAB problem, and discuss<lb>its regret analysis. We prove that MP-TS for bi-<lb>nary rewards has the optimal regret upper bound<lb>that matches the regret lower bound provided<lb>by Anantharam et al. (1987). Therefore, MP-<lb>TS is the first computationally efficient algorithm<lb>with optimal regret. A set of computer simula-<lb>tions was also conducted, which compared MP-<lb>TS with state-of-the-art algorithms. We also pro-<lb>pose a modification of MP-TS, which is shown<lb>to have better empirical performance.", "creator": "LaTeX with hyperref package"}}}