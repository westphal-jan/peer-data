{"id": "1506.01057", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2015", "title": "A Hierarchical Neural Autoencoder for Paragraphs and Documents", "abstract": "Natural language generation of coherent long texts like paragraphs or longer documents is a challenging problem for recurrent networks models. In this paper, we explore an important step toward this generation task: training an LSTM (Long-short term memory) auto-encoder to preserve and reconstruct multi-sentence paragraphs. We introduce an LSTM model that hierarchically builds an embedding for a paragraph from embeddings for sentences and words, then decodes this embedding to reconstruct the original paragraph. We evaluate the reconstructed paragraph using standard metrics like ROUGE and Entity Grid, showing that neural models are able to encode texts in a way that preserve syntactic, semantic, and discourse coherence. While only a first step toward generating coherent text units from neural models, our work has the potential to significantly impact natural language generation and summarization\\footnote{Code for the three models described in this paper can be found at www.stanford.edu/~jiweil/ .", "histories": [["v1", "Tue, 2 Jun 2015 20:53:53 GMT  (306kb,D)", "http://arxiv.org/abs/1506.01057v1", null], ["v2", "Sat, 6 Jun 2015 01:47:34 GMT  (302kb,D)", "http://arxiv.org/abs/1506.01057v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiwei li", "minh-thang luong", "dan jurafsky"], "accepted": true, "id": "1506.01057"}, "pdf": {"name": "1506.01057.pdf", "metadata": {"source": "CRF", "title": "A Hierarchical Neural Autoencoder for Paragraphs and Documents", "authors": ["Jiwei Li", "Minh-Thang Luong", "Dan Jurafsky"], "emails": ["jurafsky@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them will be able to orient themselves in a certain direction in which they are able to."}, {"heading": "2 Long-Short Term Memory (LSTM)", "text": "In this section we give a quick overview of the LSTM models. LSTM models (Hochreiter and Schmidhuber, 1997) are defined as follows: In view of a sequence of inputs X = {x1, x2,..., xnX}, an LSTM associates each timestamp with an input, memory, and output gate, each designated as it, ft, and ot, the state indicated by: it = \u03c3 (Wi \u00b7 et + Vi \u00b7 ht \u2212 1) ft = \u03c3 (Wf \u00b7 et + Vf \u00b7 ht \u2212 1) ot = \u03c3 (Wo \u00b7 et + Vo \u00b7 ht \u2212 1) (1) (1) For notations, we distinguish between e and h, where et denotes the vector for individual text units (e.g. word or sentence) in time step t, while the vector calculated by LSTM model is denoted in time span t and ht \u2212 1, the sigmoid function."}, {"heading": "3 Paragraph Autoencoder", "text": "In this section we present our proposed hierarchical LSTM model for the autoencoder."}, {"heading": "3.1 Notation", "text": "Let D denote a paragraph or document consisting of a sequence of ND sentences, D = {s1, s2,..., sND, endD}. An additional \"endD\" character is appended to each document. Each sentence s consists of a sequence of tokens s = {w1, w2,..., wNs}, where Ns denotes the length of the sentence, with each sentence ending with an \"end\" symbol. The word w is associated with a K-dimensional embedding ew, ew = {e1w, e2w,..., eKw}. Let V denote the word size. Each sentence s is associated with a K-dimensional representation there.An autocoder is a neural model in which output units are directly linked to input units or identical to input units. Typically, inputs are compressed using neural models (encoding) used to reconstruct it."}, {"heading": "3.2 Model 1: Standard LSTM", "text": "Following Sutskever et al. (2014) and 0), we trained an autoencoder that first maps input documents into vector representations from an LSTMencode and then reconstructs input by predicting tokens within the document sequentially from an LSTMdecode. Two separate LSTMs are implemented for encoding and decoding without considering sentence structures."}, {"heading": "3.3 Model 2: Hierarchical LSTM", "text": "Dei rf\u00fc ide rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the"}, {"heading": "3.4 Model 3: Hierarchical LSTM with Attention", "text": "This attention version of the hierarchical model is inspired by similar work in captions generation and machine translation (Xu et al., 2015; 0).Let H = {hs1 (e), hs2 (e),..., hsN (e)} be thecollection of sentence-level hidden vectors for each sentence from the inputs, output from LSTMSentenceencode. Each element in H contains information about input sequences with a strong focus on the parts surrounding each specific sentence (time-step).During decoding, attention models would first link the current step decoding information, i.e. hst (decode), which is decoded from LSTM sentencedecode with each sentence (time-step)."}, {"heading": "3.5 Training and Testing", "text": "A Softmax function is used to predict each token within output documents, the error of which is first propagated backwards to sentences by LSTM word decode, then by LSTM sentence decoding for document representation eD, and finally by LSTM sentence decoding and LSTM word encoding for input. Stochastic descent with minibatches is applied. For testing, we pursue a greedy strategy without beam search. For a certain document D, you first get eD based on already learned LSTMencode parameters and word embedding. Then, in the decoding, LSTM sentence decoding for word decoding is created at the sentence level in each time step, which is fed into the binary classifier to decide whether the sentence decoding is terminated and then into LSTM word decoding for sentence decoding."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Dataset", "text": "We implement the proposed autocoder on two sets of data, a highly domain-specific data set consisting of hotel reviews and a general data set from Wkipedia.Hotel reviews We use a subset of hotel reviews that has been searched by TripAdvisor. We only look at reviews that consist of sentences between 50 and 250 words; the model has problems with extremely long sentences, as we will discuss later. We maintain a vocabulary consisting of the 25,000 most common words. A special \"\" token is used to identify all remaining rarer tokens. Ratings that consist of more than 2 percent of unknown words are discarded. Our training data set consists of about 340,000 reviews; the test set consists of 40,000 reviews. Details of the data set are presented in Table 1.Wikipedia We have extracted paragraphs from Wikipedia corpus that meet the aforementioned 500,000 length requirements."}, {"heading": "4.2 Training Details and Implementation", "text": "Previous research has shown that deep LSTMs work better for sequence sequence sequence tasks than flat ones (Vinyals et al., 2014; Sutskever et al., 2014). We adopt an LSTM structure with four layers for encoding and four layers for decoding, each of which consists of a different set of parameters. \u2022 Each LSTM layer consists of 1,000 hidden neurons and the dimensionality of word embedding is set to 1,000. \u2022 Further training details are given below, some of which follow Sutsket al. (2014). \u2022 Input documents are reversed. \u2022 LSTM parameters and word embedding are distributed evenly between [-0.08, 0.08]. \u2022 Stochastic gradients are properly implemented without dynamics with a fixed learning rate of 0.1. We explained that the learning rate is halved every half epochs after 5 epochs."}, {"heading": "4.3 Evaluations", "text": "We must measure the proximity of the output (candidate) to the input (reference). We first adopt two standard yardsticks that are widely used in the summary (Lin, 2004; Lin and Hovy, 2003) and BLEU (Papineni et al., 2002).ROUGE is therefore a recall-oriented yardstick that is widely used in the summary. It measures the n-gram callback between the candidate text and the reference text (s).In this paper we have only one reference document (the input document) and the ROUGE score is therefore derived from: ROUGEn = \"grammar\" inputtmatch (gram), where countmatch co-occurs the number of n-grams in the input and output. We report on ROUGE-1, 2 and W (based on weighted longest common subsequences).BLEU purely measurement will adequately reward long outputs."}, {"heading": "4.4 Results", "text": "A summary of our test results can be found in Table 3. We observe better performance for the hotel review data set than for the open-domain Wikipedia data set, for the intuitive reason that documents and sentences are written in a firmer format and are easily predictable for hotel valuations.The hierarchical model, which takes into account the sentence structure, outperforms standard sequence sequence models. Attention models at the sentence level increase performance over vanilla hierarchical models. In terms of coherence rating, the original sentence sequence remains largely intact: the hierarchical model with attention reaches L = 1.57 on the hotel review data set, corresponding to the fact that the relative position of two input sets is permutated by an average of 1.57. Even for the Wikipedia data set, which observes inferior sentences, the original text drive sequence can still be adequately maintained at L = 2.04."}, {"heading": "5 Discussion and Future Work", "text": "We have trained an autoencoder to see how well LSTM models can reconstruct input documents of many sentences. We find that the proposed hierarchical LSTM models can partially preserve the semantic and syntactic integrity of multitext units. While our work on the automatic encoding of larger texts is only a preliminary effort to allow neural models to handle discourses, it suggests that neural models are capable of encoding complex clues to how coherent texts are connected to each other. The performance of this autoencoder task could certainly depend on the complex neural models involved in the discourse."}, {"heading": "6 Acknowledgement", "text": "The authors thank Gabor Angeli, Sam Bowman, Percy Liang and other members of the Stanford NLP group for their insightful comments and suggestions. We also thank the three anonymous ACL reviewers for their helpful comments. This work is supported by the Enlight Foundation Graduate Fellowship and a gift from Bloomberg L.P, which we gratefully acknowledge."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Modeling local coherence: An entity-based approach", "author": ["Regina Barzilay", "Mirella Lapata."], "venue": "Computational Linguistics, 34(1):1\u201334.", "citeRegEx": "Barzilay and Lapata.,? 2008", "shortCiteRegEx": "Barzilay and Lapata.", "year": 2008}, {"title": "Discovery of topically coherent sentences for extractive summarization", "author": ["Asli Celikyilmaz", "Dilek Hakkani-T\u00fcr."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,", "citeRegEx": "Celikyilmaz and Hakkani.T\u00fcr.,? 2011", "shortCiteRegEx": "Celikyilmaz and Hakkani.T\u00fcr.", "year": 2011}, {"title": "Coreference-inspired coherence modeling", "author": ["Micha Elsner", "Eugene Charniak."], "venue": "Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers, pages", "citeRegEx": "Elsner and Charniak.,? 2008", "shortCiteRegEx": "Elsner and Charniak.", "year": 2008}, {"title": "Textlevel discourse parsing with rich linguistic features", "author": ["Vanessa Wei Feng", "Graeme Hirst."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 60\u201368. Association", "citeRegEx": "Feng and Hirst.,? 2012", "shortCiteRegEx": "Feng and Hirst.", "year": 2012}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves."], "venue": "arXiv preprint arXiv:1308.0850.", "citeRegEx": "Graves.,? 2013", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Hilda: a discourse parser using support vector machine classification", "author": ["Hugo Hernault", "Helmut Prendinger", "Mitsuru Ishizuka"], "venue": "Dialogue & Discourse,", "citeRegEx": "Hernault et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hernault et al\\.", "year": 2010}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Representation learning for text-level discourse parsing", "author": ["Yangfeng Ji", "Jacob Eisenstein."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, volume 1, pages 13\u201324.", "citeRegEx": "Ji and Eisenstein.,? 2014", "shortCiteRegEx": "Ji and Eisenstein.", "year": 2014}, {"title": "Automatic evaluation of text coherence: Models and representations", "author": ["Mirella Lapata", "Regina Barzilay."], "venue": "IJCAI, volume 5, pages 1085\u20131090.", "citeRegEx": "Lapata and Barzilay.,? 2005", "shortCiteRegEx": "Lapata and Barzilay.", "year": 2005}, {"title": "Discourse relations and defeasible knowledge", "author": ["Alex Lascarides", "Nicholas Asher."], "venue": "Proceedings of the 29th annual meeting on Association for Computational Linguistics, pages 55\u201362. Association for Computational Linguistics.", "citeRegEx": "Lascarides and Asher.,? 1991", "shortCiteRegEx": "Lascarides and Asher.", "year": 1991}, {"title": "Generating discourse structures for written texts", "author": ["Huong LeThanh", "Geetha Abeysinghe", "Christian Huyck."], "venue": "Proceedings of the 20th international conference on Computational Linguistics, page 329. Association for Computational Linguis-", "citeRegEx": "LeThanh et al\\.,? 2004", "shortCiteRegEx": "LeThanh et al\\.", "year": 2004}, {"title": "A model of coherence based on distributed sentence representation", "author": ["Jiwei Li", "Eduard Hovy"], "venue": null, "citeRegEx": "Li and Hovy.,? \\Q2014\\E", "shortCiteRegEx": "Li and Hovy.", "year": 2014}, {"title": "The nlp engine: A universal turing machine for nlp", "author": ["Jiwei Li", "Eduard Hovy."], "venue": "arXiv preprint arXiv:1503.00168.", "citeRegEx": "Li and Hovy.,? 2015", "shortCiteRegEx": "Li and Hovy.", "year": 2015}, {"title": "Recursive deep models for discourse parsing", "author": ["Jiwei Li", "Rumeng Li", "Eduard Hovy."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2061\u20132069.", "citeRegEx": "Li et al\\.,? 2014", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Automatic evaluation of summaries using n-gram cooccurrence statistics", "author": ["Chin-Yew Lin", "Eduard Hovy."], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Hu-", "citeRegEx": "Lin and Hovy.,? 2003", "shortCiteRegEx": "Lin and Hovy.", "year": 2003}, {"title": "Automatically evaluating text coherence using discourse relations", "author": ["Ziheng Lin", "Hwee Tou Ng", "Min-Yen Kan."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume", "citeRegEx": "Lin et al\\.,? 2011", "shortCiteRegEx": "Lin et al\\.", "year": 2011}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin."], "venue": "Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74\u201381.", "citeRegEx": "Lin.,? 2004", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc V Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "arXiv preprint arXiv:1410.8206.", "citeRegEx": "Luong et al\\.,? 2014", "shortCiteRegEx": "Luong et al\\.", "year": 2014}, {"title": "Rhetorical structure theory: Toward a functional theory of text organization", "author": ["William C Mann", "Sandra A Thompson."], "venue": "Text, 8(3):243\u2013281.", "citeRegEx": "Mann and Thompson.,? 1988", "shortCiteRegEx": "Mann and Thompson.", "year": 1988}, {"title": "The rhetorical parsing of unrestricted texts: A surface-based approach", "author": ["Daniel Marcu."], "venue": "Computational linguistics, 26(3):395\u2013448.", "citeRegEx": "Marcu.,? 2000", "shortCiteRegEx": "Marcu.", "year": 2000}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318. Association for", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Overcoming the curse of sentence length for neural machine translation using automatic segmentation", "author": ["Jean Pouget-Abadie", "Dzmitry Bahdanau", "Bart van Merri\u00ebnboer", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.1257.", "citeRegEx": "Pouget.Abadie et al\\.,? 2014", "shortCiteRegEx": "Pouget.Abadie et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."], "venue": "Advances in Neural Information Processing Systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "arXiv preprint arXiv:1412.7449.", "citeRegEx": "Vinyals et al\\.,? 2014", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Representing discourse coherence: A corpus-based study", "author": ["Florian Wolf", "Edward Gibson."], "venue": "Computational Linguistics, 31(2):249\u2013287.", "citeRegEx": "Wolf and Gibson.,? 2005", "shortCiteRegEx": "Wolf and Gibson.", "year": 2005}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1502.03044.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Timeline generation through evolutionary trans-temporal summarization", "author": ["Rui Yan", "Liang Kong", "Congrui Huang", "Xiaojun Wan", "Xiaoming Li", "Yan Zhang."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Yan et al\\.,? 2011a", "shortCiteRegEx": "Yan et al\\.", "year": 2011}, {"title": "Evolutionary timeline summarization: a balanced optimization framework via iterative substitution", "author": ["Rui Yan", "Xiaojun Wan", "Jahna Otterbacher", "Liang Kong", "Xiaoming Li", "Yan Zhang."], "venue": "Proceedings of the 34th international ACM SIGIR con-", "citeRegEx": "Yan et al\\.,? 2011b", "shortCiteRegEx": "Yan et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 19, "context": "A wide variety of theories exist for representing relationships between text units, such as Rhetorical Structure Theory (Mann and Thompson, 1988) or Discourse Representation Theory (Lascarides and Asher, 1991), for extracting these relations from text units (Marcu, 2000; LeThanh et al.", "startOffset": 120, "endOffset": 145}, {"referenceID": 10, "context": "A wide variety of theories exist for representing relationships between text units, such as Rhetorical Structure Theory (Mann and Thompson, 1988) or Discourse Representation Theory (Lascarides and Asher, 1991), for extracting these relations from text units (Marcu, 2000; LeThanh et al.", "startOffset": 181, "endOffset": 209}, {"referenceID": 7, "context": "Recent LSTM models (Hochreiter and Schmidhuber, 1997) have shown powerful results on generating meaningful and grammatical sentences in", "startOffset": 19, "endOffset": 53}, {"referenceID": 23, "context": "sequence generation tasks like machine translation (Sutskever et al., 2014; 0; Luong et al., 2014) or parsing (Vinyals et al.", "startOffset": 51, "endOffset": 98}, {"referenceID": 18, "context": "sequence generation tasks like machine translation (Sutskever et al., 2014; 0; Luong et al., 2014) or parsing (Vinyals et al.", "startOffset": 51, "endOffset": 98}, {"referenceID": 24, "context": ", 2014) or parsing (Vinyals et al., 2014).", "startOffset": 19, "endOffset": 41}, {"referenceID": 7, "context": "LSTM models (Hochreiter and Schmidhuber, 1997) are defined as follows: given a sequence of inputs X = {x1, x2, .", "startOffset": 12, "endOffset": 46}, {"referenceID": 23, "context": "For beam search, (Sutskever et al., 2014) discovered that a beam size of 2 suffices to provide", "startOffset": 17, "endOffset": 41}, {"referenceID": 24, "context": "Previous research has shown that deep LSTMs work better than shallow ones for sequence-tosequence tasks (Vinyals et al., 2014; Sutskever et al., 2014).", "startOffset": 104, "endOffset": 150}, {"referenceID": 23, "context": "Previous research has shown that deep LSTMs work better than shallow ones for sequence-tosequence tasks (Vinyals et al., 2014; Sutskever et al., 2014).", "startOffset": 104, "endOffset": 150}, {"referenceID": 23, "context": ", 2014; Sutskever et al., 2014). We adopt a LSTM structure with four layer for encoding and four layer for decoding, each of which is comprised of a different set of parameters. Each LSTM layer consists of 1,000 hidden neurons and the dimensionality of word embeddings is set to 1,000. Other training details are given below, some of which follow Sutskever et al. (2014).", "startOffset": 8, "endOffset": 371}, {"referenceID": 17, "context": "We first adopt two standard evaluation metrics, ROUGE (Lin, 2004; Lin and Hovy, 2003) and BLEU (Papineni et al.", "startOffset": 54, "endOffset": 85}, {"referenceID": 15, "context": "We first adopt two standard evaluation metrics, ROUGE (Lin, 2004; Lin and Hovy, 2003) and BLEU (Papineni et al.", "startOffset": 54, "endOffset": 85}, {"referenceID": 21, "context": "We first adopt two standard evaluation metrics, ROUGE (Lin, 2004; Lin and Hovy, 2003) and BLEU (Papineni et al., 2002).", "startOffset": 95, "endOffset": 118}, {"referenceID": 21, "context": "For details, see Papineni et al. (2002).", "startOffset": 17, "endOffset": 40}, {"referenceID": 1, "context": ", input documents in our tasks) are coherent (Barzilay and Lapata, 2008), we compare generated outputs with input documents in terms of how much", "startOffset": 45, "endOffset": 72}, {"referenceID": 9, "context": "We develop a grid evaluation metric similar to the entity transition algorithms in (Barzilay and Lee, 2004; Lapata and Barzilay, 2005).", "startOffset": 83, "endOffset": 134}, {"referenceID": 28, "context": ", (Yan et al., 2011b; Yan et al., 2011a; Celikyilmaz and Hakkani-T\u00fcr, 2011)).", "startOffset": 2, "endOffset": 75}, {"referenceID": 27, "context": ", (Yan et al., 2011b; Yan et al., 2011a; Celikyilmaz and Hakkani-T\u00fcr, 2011)).", "startOffset": 2, "endOffset": 75}, {"referenceID": 2, "context": ", (Yan et al., 2011b; Yan et al., 2011a; Celikyilmaz and Hakkani-T\u00fcr, 2011)).", "startOffset": 2, "endOffset": 75}, {"referenceID": 15, "context": "Wolf and Gibson (2005) and Lin et al. (2011) proposed metrics based on discourse relations, but these are hard to apply widely since identifying discourse relations is a difficult problem.", "startOffset": 27, "endOffset": 45}], "year": 2015, "abstractText": "Natural language generation of coherent long texts like paragraphs or longer documents is a challenging problem for recurrent networks models. In this paper, we explore an important step toward this generation task: training an LSTM (Longshort term memory) auto-encoder to preserve and reconstruct multi-sentence paragraphs. We introduce an LSTM model that hierarchically builds an embedding for a paragraph from embeddings for sentences and words, then decodes this embedding to reconstruct the original paragraph. We evaluate the reconstructed paragraph using standard metrics like ROUGE and Entity Grid, showing that neural models are able to encode texts in a way that preserve syntactic, semantic, and discourse coherence. While only a first step toward generating coherent text units from neural models, our work has the potential to significantly impact natural language generation and summarization1 2.", "creator": "TeX"}}}