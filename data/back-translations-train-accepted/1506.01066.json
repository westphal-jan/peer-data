{"id": "1506.01066", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2015", "title": "Visualizing and Understanding Neural Models in NLP", "abstract": "While neural networks have been successfully applied to many NLP tasks the resulting vector-based models are very difficult to interpret. For example it's not clear how they achieve {\\em compositionality}, building sentence meaning from the meanings of words and phrases. In this paper we describe four strategies for visualizing compositionality in neural models for NLP, inspired by similar work in computer vision. We first plot unit values to visualize compositionality of negation, intensification, and concessive clauses, allow us to see well-known markedness asymmetries in negation. We then introduce three simple and straightforward methods for visualizing a unit's {\\em salience}, the amount it contributes to the final composed meaning: (1) gradient back-propagation, (2) the variance of a token from the average word node, (3) LSTM-style gates that measure information flow. We test our methods on sentiment using simple recurrent nets and LSTMs. Our general-purpose methods may have wide applications for understanding compositionality and other semantic properties of deep networks , and also shed light on why LSTMs outperform simple recurrent nets,", "histories": [["v1", "Tue, 2 Jun 2015 21:17:31 GMT  (1681kb,D)", "http://arxiv.org/abs/1506.01066v1", null], ["v2", "Fri, 8 Jan 2016 18:10:22 GMT  (2467kb,D)", "http://arxiv.org/abs/1506.01066v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiwei li", "xinlei chen", "eduard h hovy", "dan jurafsky"], "accepted": true, "id": "1506.01066"}, "pdf": {"name": "1506.01066.pdf", "metadata": {"source": "CRF", "title": "Visualizing and Understanding Neural Models in NLP", "authors": ["Jiwei Li", "Xinlei Chen", "Eduard Hovy", "Dan Jurafsky"], "emails": ["jiweil@stanford.edu", "jurafsky@stanford.edu", "xinleic@andrew.cmu.edu", "ehovy@andrew.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Neural models correspond to or exceed the performance of other state-of-the-art systems in a variety of NLP tasks. However, unlike traditional feature-based classifiers that assign and optimize weights to the types of human interpretable traits (parts of language, named entities, word forms, syntactic parse effects), the behavior of deep learning models is much less easy to interpret. Deeplearning models operate mainly on word embedding (low-dimensional, continuous, real-rated vectors) through multi-layered neural architectures, each layer characterized as an array of hidden neuron units. It is unclear how deeply learning models deal with composition, implement functions such as negation or intensification, or combinations of meanings from different parts of the sentence that filter informational chaff away from wheat to form sentence meanings."}, {"heading": "2 A Brief Review of Neural Visualization", "text": "Similarity is generally represented graphically by projecting space into two dimensions and observing that similar words tend to merge (e.g. Elman (1989), Ji and Eisenstein (2014), Faruqui and Dyer (2014). But methods of interpreting and visualizing neural models have been explored much more meaningfully in the vision, especially for Convolutionary Neural Networks (CNNs or ConvNets) (Krizhevsky et al., 2012), multi-layered neural networks in which the original matrix of image pixels is entangled and pooled while being passed on to hidden layers. ConvNet visualization techniques consist mainly of mapping the various layers of the network (or other features such as SIFT (Lowe, 2004) and HOG (Dalal and Triggs, 2005)."}, {"heading": "3 Dataset and Models to be Visualized", "text": "We chose the Stanford Sentiment Treebank dataset (Socher et al., 2013), a benchmark dataset widely used to evaluate neural models, which contains gold-standard sentiment labels for each parse tree component, from sentences to phrases to individual words, for 215,154 phrases in 11,855 sentences. The task is to perform both fine-grained (very positive, positive, neutral, negative and very negative) and coarse-grained (positive versus negative) classifications at both phrase and sentence level. Please refer to (Socher et al., 2013) for more details on the dataset. While many studies on this dataset use recursive parse tree models, in this work we only use standard sequence models (RNNNs and LSTMs) as they are the most widely used current neural models, and sequential visualization is easier."}, {"heading": "4 Representation Plotting", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "5 Model 1: First Derivatives", "text": "The first new model, inspired by the strategy of backward-looking vision (Erhan et al., 2009; Simonyan et al., 2013), measures how much each input unit contributes to the final decision that can be approximated by the first derivatives. Formally, however, an inputE is associated with a gold standard label. (Depending on how much the NLP task constitutes, an input could be the embedding of a word or a sequence of words, while labels could be POS labels, etc.) Given the embedding of E for input words with the associated gold class label c, the trained model associates the pair (E, c) with a score Sc (E). The goal is to decide which units of E make the most significant contribution to Sc (e), and thus the decision of class c.In the case of deep neural models, the class Score Sc (e) is \"highly linear.\""}, {"heading": "6 Model 2: Average and Variance", "text": "For environments where word embeddings are treated as parameters to optimize word embeddings from the ground up (as opposed to using pre-trained embeddings), we propose a second, surprisingly simple and straightforward way to visualize important indicators. First, we calculate the average of word embeddings for all words within sentences. The measure of highlighting or influencing a word is its deviation from that average. The idea is that during the training, models would learn to present indicators differently from non-indicator words, so that they stand out even after many layers of the computation.Figure 8 shows a map of variance; each grid corresponds to the value of | | ei, j \u2212 1NS \u2211 i \u2032 j | 2, where ei, j denotes the value for the j th dimension of the word i and N, the number of symbols within sentences. As the figure shows, the variance-based highlighting measure also does a good job of highlighting the relevant word (where the element can only be used)."}, {"heading": "7 Model 3: Gate Based Models", "text": "The final strategy, gate-based modeling, is based on the intuition of the LSTMs: gates are placed in each time step to control the flow of information, but modified by the LSTM gates to allow easier visualization. Let ht, l denotes the embedding for time step t in the lth level. ht, 0 denotes the embedding in the 0th level. We set a more direct control gate at each node: for layer l and time t we associate it with a scalar gate value Mt, l in the range [0,1]. Instead of the current representation (i.e., hl, t) for the downstream calculation than in the standard recurrent models that include the product of the gate value Mt, l and the current embedding of the current embedding in the range [0,1]."}, {"heading": "8 Conclusion", "text": "In this paper, we offer several methods to illustrate how neural models are able to compose meanings, showing asymmetries of negation and explaining some aspects of the strong performance of LSTMs in these tasks."}], "references": [{"title": "Psychology and language: An introduction to psycholinguistics", "author": ["Herbert H. Clark", "Eve V. Clark."], "venue": "Harcourt Brace Jovanovich.", "citeRegEx": "Clark and Clark.,? 1977", "shortCiteRegEx": "Clark and Clark.", "year": 1977}, {"title": "Histograms of oriented gradients for human detection", "author": ["Navneet Dalal", "Bill Triggs."], "venue": "Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 886\u2013893. IEEE.", "citeRegEx": "Dalal and Triggs.,? 2005", "shortCiteRegEx": "Dalal and Triggs.", "year": 2005}, {"title": "Representation and structure in connectionist models", "author": ["Jeffrey L. Elman."], "venue": "Technical Report 8903, Center for Research in Language, University of California, San Diego.", "citeRegEx": "Elman.,? 1989", "shortCiteRegEx": "Elman.", "year": 1989}, {"title": "Visualizing higher-layer features of a deep network", "author": ["Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pascal Vincent."], "venue": "Dept. IRO, Universit\u00e9 de Montr\u00e9al, Tech. Rep.", "citeRegEx": "Erhan et al\\.,? 2009", "shortCiteRegEx": "Erhan et al\\.", "year": 2009}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["Manaal Faruqui", "Chris Dyer."], "venue": "Proceedings of EACL, volume 2014.", "citeRegEx": "Faruqui and Dyer.,? 2014", "shortCiteRegEx": "Faruqui and Dyer.", "year": 2014}, {"title": "The meaning of negated adjectives", "author": ["Tamar Fraenkel", "Yaacov Schul."], "venue": "Intercultural Pragmatics, 5(4):517\u2013540.", "citeRegEx": "Fraenkel and Schul.,? 2008", "shortCiteRegEx": "Fraenkel and Schul.", "year": 2008}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Ross Girshick", "Jeff Donahue", "Trevor Darrell", "Jitendra Malik."], "venue": "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 580\u2013587. IEEE.", "citeRegEx": "Girshick et al\\.,? 2014", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "A natural history of negation, volume 960", "author": ["Laurence R. Horn."], "venue": "University of Chicago Press Chicago.", "citeRegEx": "Horn.,? 1989", "shortCiteRegEx": "Horn.", "year": 1989}, {"title": "Representation learning for text-level discourse parsing", "author": ["Yangfeng Ji", "Jacob Eisenstein."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, volume 1, pages 13\u201324.", "citeRegEx": "Ji and Eisenstein.,? 2014", "shortCiteRegEx": "Ji and Eisenstein.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton."], "venue": "Advances in neural information processing systems, pages 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["David G Lowe."], "venue": "International journal of computer vision, 60(2):91\u2013110.", "citeRegEx": "Lowe.,? 2004", "shortCiteRegEx": "Lowe.", "year": 2004}, {"title": "Understanding deep image representations by inverting them", "author": ["Aravindh Mahendran", "Andrea Vedaldi."], "venue": "arXiv preprint arXiv:1412.0035.", "citeRegEx": "Mahendran and Vedaldi.,? 2014", "shortCiteRegEx": "Mahendran and Vedaldi.", "year": 2014}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["Anh Nguyen", "Jason Yosinski", "Jeff Clune."], "venue": "arXiv preprint arXiv:1412.1897.", "citeRegEx": "Nguyen et al\\.,? 2014", "shortCiteRegEx": "Nguyen et al\\.", "year": 2014}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K Paliwal."], "venue": "Signal Processing, IEEE Transactions on, 45(11):2673\u20132681.", "citeRegEx": "Schuster and Paliwal.,? 1997", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman."], "venue": "arXiv preprint arXiv:1312.6034.", "citeRegEx": "Simonyan et al\\.,? 2013", "shortCiteRegEx": "Simonyan et al\\.", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts."], "venue": "Proceedings of the conference on", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus."], "venue": "arXiv preprint arXiv:1312.6199.", "citeRegEx": "Szegedy et al\\.,? 2013", "shortCiteRegEx": "Szegedy et al\\.", "year": 2013}, {"title": "Visualizing data using t-sne", "author": ["Laurens Van der Maaten", "Geoffrey Hinton."], "venue": "Journal of Machine Learning Research, 9(2579-2605):85.", "citeRegEx": "Maaten and Hinton.,? 2008", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Hoggles: Visualizing object detection features", "author": ["Carl Vondrick", "Aditya Khosla", "Tomasz Malisiewicz", "Antonio Torralba."], "venue": "Computer Vision (ICCV), 2013 IEEE International Conference on, pages 1\u20138. IEEE.", "citeRegEx": "Vondrick et al\\.,? 2013", "shortCiteRegEx": "Vondrick et al\\.", "year": 2013}, {"title": "Reconstructing an image from its local descriptors", "author": ["Philippe Weinzaepfel", "Herv\u00e9 J\u00e9gou", "Patrick P\u00e9rez."], "venue": "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 337\u2013 344. IEEE.", "citeRegEx": "Weinzaepfel et al\\.,? 2011", "shortCiteRegEx": "Weinzaepfel et al\\.", "year": 2011}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D Zeiler", "Rob Fergus."], "venue": "Computer Vision\u2013ECCV 2014, pages 818\u2013833. Springer.", "citeRegEx": "Zeiler and Fergus.,? 2014", "shortCiteRegEx": "Zeiler and Fergus.", "year": 2014}], "referenceMentions": [{"referenceID": 16, "context": "To simplify our explanation of the models, we focus on phrase-based sentiment analysis from the Stanford Sentiment Treebank dataset (Socher et al., 2013).", "startOffset": 132, "endOffset": 153}, {"referenceID": 10, "context": "But methods for interpreting and visualizing neural models have been much more significantly explored in vision, especially for Convolutional Neural Networks (CNNs or ConvNets) (Krizhevsky et al., 2012), multi-layer neural networks in which the original matrix of image pixels is convolved and pooled as it is passed on to", "startOffset": 177, "endOffset": 202}, {"referenceID": 2, "context": ", Elman (1989), Ji and Eisenstein (2014), Faruqui and Dyer (2014)).", "startOffset": 2, "endOffset": 15}, {"referenceID": 2, "context": ", Elman (1989), Ji and Eisenstein (2014), Faruqui and Dyer (2014)).", "startOffset": 2, "endOffset": 41}, {"referenceID": 2, "context": ", Elman (1989), Ji and Eisenstein (2014), Faruqui and Dyer (2014)).", "startOffset": 2, "endOffset": 66}, {"referenceID": 11, "context": "ConvNet visualizing techniques consist mainly in mapping the different layers of the network (or other features like SIFT (Lowe, 2004) and HOG (Dalal and Triggs, 2005)) back to the initial image input, thus capturing the humaninterpretable information they represent in the in-", "startOffset": 122, "endOffset": 134}, {"referenceID": 1, "context": "ConvNet visualizing techniques consist mainly in mapping the different layers of the network (or other features like SIFT (Lowe, 2004) and HOG (Dalal and Triggs, 2005)) back to the initial image input, thus capturing the humaninterpretable information they represent in the in-", "startOffset": 143, "endOffset": 167}, {"referenceID": 15, "context": "put, and how units in these layers contribute to any final decisions (Simonyan et al., 2013; Mahendran and Vedaldi, 2014; Nguyen et al., 2014; Szegedy et al., 2013; Girshick et al., 2014; Zeiler and Fergus, 2014).", "startOffset": 69, "endOffset": 212}, {"referenceID": 12, "context": "put, and how units in these layers contribute to any final decisions (Simonyan et al., 2013; Mahendran and Vedaldi, 2014; Nguyen et al., 2014; Szegedy et al., 2013; Girshick et al., 2014; Zeiler and Fergus, 2014).", "startOffset": 69, "endOffset": 212}, {"referenceID": 13, "context": "put, and how units in these layers contribute to any final decisions (Simonyan et al., 2013; Mahendran and Vedaldi, 2014; Nguyen et al., 2014; Szegedy et al., 2013; Girshick et al., 2014; Zeiler and Fergus, 2014).", "startOffset": 69, "endOffset": 212}, {"referenceID": 17, "context": "put, and how units in these layers contribute to any final decisions (Simonyan et al., 2013; Mahendran and Vedaldi, 2014; Nguyen et al., 2014; Szegedy et al., 2013; Girshick et al., 2014; Zeiler and Fergus, 2014).", "startOffset": 69, "endOffset": 212}, {"referenceID": 6, "context": "put, and how units in these layers contribute to any final decisions (Simonyan et al., 2013; Mahendran and Vedaldi, 2014; Nguyen et al., 2014; Szegedy et al., 2013; Girshick et al., 2014; Zeiler and Fergus, 2014).", "startOffset": 69, "endOffset": 212}, {"referenceID": 21, "context": "put, and how units in these layers contribute to any final decisions (Simonyan et al., 2013; Mahendran and Vedaldi, 2014; Nguyen et al., 2014; Szegedy et al., 2013; Girshick et al., 2014; Zeiler and Fergus, 2014).", "startOffset": 69, "endOffset": 212}, {"referenceID": 12, "context": "put images (Mahendran and Vedaldi, 2014; Vondrick et al., 2013; Weinzaepfel et al., 2011).", "startOffset": 11, "endOffset": 89}, {"referenceID": 19, "context": "put images (Mahendran and Vedaldi, 2014; Vondrick et al., 2013; Weinzaepfel et al., 2011).", "startOffset": 11, "endOffset": 89}, {"referenceID": 20, "context": "put images (Mahendran and Vedaldi, 2014; Vondrick et al., 2013; Weinzaepfel et al., 2011).", "startOffset": 11, "endOffset": 89}, {"referenceID": 3, "context": "(2) Back-propagation (Erhan et al., 2009; Simonyan et al., 2013) and Deconvolutional Networks (Zeiler and Fergus, 2014): Errors are back propagated from output layers to each intermediate layer and finally to the original image inputs.", "startOffset": 21, "endOffset": 64}, {"referenceID": 15, "context": "(2) Back-propagation (Erhan et al., 2009; Simonyan et al., 2013) and Deconvolutional Networks (Zeiler and Fergus, 2014): Errors are back propagated from output layers to each intermediate layer and finally to the original image inputs.", "startOffset": 21, "endOffset": 64}, {"referenceID": 21, "context": ", 2013) and Deconvolutional Networks (Zeiler and Fergus, 2014): Errors are back propagated from output layers to each intermediate layer and finally to the original image inputs.", "startOffset": 37, "endOffset": 62}, {"referenceID": 17, "context": "(3) Generation: This group of work generates images in a specific class from a sketch guided by already trained neural models (Szegedy et al., 2013; Nguyen et al., 2014).", "startOffset": 126, "endOffset": 169}, {"referenceID": 13, "context": "(3) Generation: This group of work generates images in a specific class from a sketch guided by already trained neural models (Szegedy et al., 2013; Nguyen et al., 2014).", "startOffset": 126, "endOffset": 169}, {"referenceID": 16, "context": "We chose the Stanford Sentiment Treebank dataset (Socher et al., 2013), a benchmark dataset widely used for neural model evaluations.", "startOffset": 49, "endOffset": 70}, {"referenceID": 16, "context": "For more details about the dataset, please refer to (Socher et al., 2013).", "startOffset": 52, "endOffset": 73}, {"referenceID": 8, "context": "This asymmetry has been widely discussed in linguistics, for example as arising from markedness, since \u2018good\u2019 is the unmarked direction of the scale (3; Horn, 1989; Fraenkel and Schul, 2008).", "startOffset": 149, "endOffset": 190}, {"referenceID": 5, "context": "This asymmetry has been widely discussed in linguistics, for example as arising from markedness, since \u2018good\u2019 is the unmarked direction of the scale (3; Horn, 1989; Fraenkel and Schul, 2008).", "startOffset": 149, "endOffset": 190}, {"referenceID": 8, "context": "This difference again suggests the model is able to capture negative asymmetry (3; Horn, 1989; Fraenkel and Schul, 2008).", "startOffset": 79, "endOffset": 120}, {"referenceID": 5, "context": "This difference again suggests the model is able to capture negative asymmetry (3; Horn, 1989; Fraenkel and Schul, 2008).", "startOffset": 79, "endOffset": 120}, {"referenceID": 3, "context": "Our first new model, inspired by the backpropagation strategy in vision (Erhan et al., 2009; Simonyan et al., 2013), measures how much each input unit contributes to the final decision, which can be approximated by first derivatives.", "startOffset": 72, "endOffset": 115}, {"referenceID": 15, "context": "Our first new model, inspired by the backpropagation strategy in vision (Erhan et al., 2009; Simonyan et al., 2013), measures how much each input unit contributes to the final decision, which can be approximated by first derivatives.", "startOffset": 72, "endOffset": 115}], "year": 2015, "abstractText": "While neural networks have been successfully applied to many NLP tasks the resulting vector-based models are very difficult to interpret. For example it\u2019s not clear how they achieve compositionality, building sentence meaning from the meanings of words and phrases. In this paper we describe four strategies for visualizing compositionality in neural models for NLP, inspired by similar work in computer vision. We first plot unit values to visualize compositionality of negation, intensification, and concessive clauses, allow us to see well-known markedness asymmetries in negation. We then introduce three methods for visualizing a unit\u2019s salience, the amount it contributes to the final composed meaning: (1) gradient back-propagation, (2) the variance of a token from the average word node, (3) LSTM-style gates that measure information flow. We test our methods on sentiment using simple recurrent nets and LSTMs. Our general-purpose methods may have wide applications for understanding compositionality and other semantic properties of deep networks , and also shed light on why LSTMs outperform simple recurrent nets,", "creator": "TeX"}}}