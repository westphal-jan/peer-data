{"id": "1506.01070", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2015", "title": "Do Multi-Sense Embeddings Improve Natural Language Understanding?", "abstract": "Learning a distinct representation for each sense of an ambiguous word could lead to more powerful and fine-grained models of vector-space representations. Yet while `multi-sense' methods have been proposed and tested on artificial word-similarity tasks, we don't know if they improve real natural language understanding tasks. In this paper we introduce a pipelined architecture for incorporating multi-sense embeddings into language understanding, and test the performance of a state-of-the-art multi-sense embedding model (based on Chinese Restaurant Processes).", "histories": [["v1", "Tue, 2 Jun 2015 21:30:21 GMT  (163kb)", "http://arxiv.org/abs/1506.01070v1", null], ["v2", "Tue, 18 Aug 2015 04:17:48 GMT  (197kb)", "http://arxiv.org/abs/1506.01070v2", null], ["v3", "Tue, 24 Nov 2015 18:29:40 GMT  (197kb)", "http://arxiv.org/abs/1506.01070v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiwei li", "dan jurafsky"], "accepted": true, "id": "1506.01070"}, "pdf": {"name": "1506.01070.pdf", "metadata": {"source": "CRF", "title": "Do Multi-Sense Embeddings Improve Natural Language Understanding?", "authors": ["Jiwei Li"], "emails": ["jiweil@stanford.edu", "jurafsky@stanford.edu"], "sections": [{"heading": null, "text": "ar Xiv: 150 6.01 070v 1 [cs.C L] 2J un2 015We apply the model to partial language marking, so-called entity recognition, mood analysis, semantic relationship identification and semantic kinship. We find that - if we carefully control the number of dimensions - meaning-specific embedding, whether alone or linked to standard embedding (a vector for all senses), produces a slight boost in semantic tasks, but has little effect on others that depend on the correct identification of some keywords such as mood analysis."}, {"heading": "1 Introduction", "text": "The fact is that we are able to hide, and that we are able to put ourselves in a different world, that we are able to put ourselves in another world, that we are able to put ourselves in another world, that we are able to put ourselves in another world, that we are able to put ourselves in another world, that we are able to put ourselves in a different world, that we are able to put ourselves in a different world, that we are able to put ourselves in a different world, that we are able to put ourselves in a different world, that we are able to put ourselves in a different world, that we are able to put ourselves in a different world. \""}, {"heading": "2 Related Work", "text": "Neural embedding learning frameworks represent each token with a dense vector representation, optimized through predicting neighboring words or decomposing co-occurrence matrices (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013; Mikolov et al., 2010; Pennington et al., 2014). Standard neural models represent each word with a single unique vector representation.Recent work has started to augment the multi-sense problembut do not explore the rest two steps. It is clear how existing multi-sense embedings in real world NLU tasksks.by associating each word with a series of sense specific embedding. The central idea is to augment standard embedding learning models like skip-grams by disambigubiguating word senses by disambigubic."}, {"heading": "3 Learning Sense-Specific Embeddings", "text": "We propose to build on this earlier literature, in particular Huang et al. (2012) and Neelakantan et al. (2014), to develop an algorithm for learning multiple embeddings for each type of word, each embedding corresponding to a specific induced sense of the word. Such an algorithm should have the property that a word should be associated with a new sense vector, precisely when evidence in context (e.g. adjacent words, document-level co-occurrence statistics) indicates that it is sufficiently different from its early senses. Such a way of thinking, of course, points to Chinese restaurant processes (CRP) (Griffiths and Tenenbaum, 2004; Teh et al., 2006) applied in the related area of word induction. In the analogy of CRP, the current word could either sit at one of the existing tables (belonging to one of the existing senses) or choose a new table (a new meaning) The decision is made by documenting the number of semantic words already being measured (the number of local ones)."}, {"heading": "3.1 Chinese Restaurant Processes", "text": "In this section, we offer a brief overview of the processes in Chinese restaurants; readers interested in more details can consult the original work (Griffiths and Tenenbaum, 2004; Teh et al., 2006; Pitman, 1995) on non-parametric clusters. By analogy, each data point is compared with a customer in a restaurant. The restaurant has a series of tables t, each of which serves a dish dt. This dish can be considered an index of a cluster or topic. The next customer to enter would either choose an existing table in which the dish (cluster) is already served, or select a new cluster based on the following probability distribution: Pr (tw = t)."}, {"heading": "3.2 Incorporating CRP into Distributed Language Models", "text": "We describe how we can integrate CRP (ew, eneigh) = F (ew, eneigh) (ew, eneigh, eneigh). We describe how we can integrate CRP (ew, eneigh, eneigh) = F (ew) into a standardized context. (D) We describe how we can integrate CRP (ew) into a standardized context. (D) We describe how we can integrate CRP (ew) into a new context. (D) It is a new context in which we can discover the number of sensory perceptions for word w (z). (D) n \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s. (D) s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s."}, {"heading": "4 Obtaining Word Representations for NLU tasks", "text": "The scenario is treated as a sequence of meaning markers, in which all global word embeddings and meaning-specific embeddings remain fixed. A document or sentence gives us an objective function in terms of meaning markers, by multiplying equivalents (5) over each symbol containing it. Calculating the global optimal meaning markup - in which each word is given an optimal meaning designation - requires searching across the space of all senses for all words, which can be costly. We therefore chose two simplified heuristic approaches: \u2022 Greedy Search: Assign each sign the locally optimal meaning designation to each sign and represent the current symbol with the associated embeddings. \u2022 Expectation: Calculate the probability of any possible meaning for the current word and represent the word with the expectation vector: ~ ew = \u0445 z-Zwp (w | z, context) \u00b7 ezw"}, {"heading": "5 Word Similarity Evaluation", "text": "We evaluate our embeddings by comparing them with other multi-sense embeddings at the standard artificial embedding level in order to put them into context with similar judgments. Early work used similarity data sets such as WS353 (Finkelstein et al., 2001) or RG (Rubenstein and Goodenough, 1965), whose context-free nature makes them poor ratings. Therefore, we use Stanford's contextual word similarities (SCWS) (Huang et al., 2012), in which human judgments are associated with word pairs in context. Thus, for example, \"bank\" in the context of \"river bank\" would have little relationship with \"deficit\" in the context of \"financial deficit.\" We first use greedy or expectation strategies to obtain word vectors for tokens in their context, which are then used as input to obtain the value of cosmic similarity between two words."}, {"heading": "6 Experiments on NLP Tasks", "text": "After we have shown that multi-sense embedding improves word similarity, we turn to the question whether it improves the real NLU tasks: POS tagging, NER tagging, sentiment analysis at the phrase and sentence level, semantic relationship identification, and semantic kinship at the sentence level. For each task, we experimented with the following embedding methods, which are trained on the same corpus using the word2vec package: \u2022 standard one-word-one-vector embedding from the skip program (50d). \u2022 sense disambiguated embedding from section 3 and 4 using greedy search and expectation (50d). \u2022 Concatenation of global word embeddings and sense-specific embeddings from skip-gram (50d). \u2022 standard one-word-vector-skip-gram embedding with dimensionality is doubled (100d models) because the 2014 word correspond to the dimensionality."}, {"heading": "6.1 The Tasks", "text": "This year it is more than ever before."}, {"heading": "6.2 Results", "text": "The results for the various tasks are given in Tables 3, 4 and 5. At first glance, it seems that multi-sense embeddings do indeed provide superior performance, since combining global vectors with sense-specific vectors introduces a consistent performance boost for each task when compared with the standard (50d), but this is of course an unfair comparison; combining global vectors with sense-specific vectors doubles the dimensionality of the vector to 100, making the comparison with the standard dimensionality (50d) unfair. Conclusions are far from clear. For each task, the 100d + Expectation method has performances that look higher than simple 100d baselines (skip-gram embeddings of 100d). The absolute values of these differences are greatest for tasks such as semantic relationality and smallest for tasks such as sensitivity. This is useful because the sentence meaning is sensitive to the semantics of a particular word that is directly reflected on the relativity."}, {"heading": "7 Conclusion", "text": "In this paper, we expand on the ongoing research on multi-sense embedding by first proposing a new version based on Chinese restaurant processes that delivers state-of-the-art performance with simple word similarity matches, then introducing a pipeline system for embedding multi-sense embedding in NLP applications, and investigating multiple NLP tasks to determine if and when multi-sense embedding can lead to performance increases. Our results suggest that simply increasing the dimensionality of basic embedding in the ski program is sufficient to achieve performance gains from using multi-sense embedding. That is, the easiest way to achieve better performance in these tasks is simply increasing the embedding dimension. Our results come with some limitations. Firstly, our conclusions are based on the pipeline system we have introduced. Therefore, there is a possibility that our conclusions may be limited to our proposed frameworks, or that the results from this multi-sense system may be completely different from those from those we have proposed."}], "references": [{"title": "Semeval-2007 task 02: Evaluating word sense induction and discrimination systems", "author": ["Eneko Agirre", "Aitor Soroa."], "venue": "Proceedings of the 4th International Workshop on Semantic Evaluations, pages 7\u201312. Association for Computational Linguis-", "citeRegEx": "Agirre and Soroa.,? 2007", "shortCiteRegEx": "Agirre and Soroa.", "year": 2007}, {"title": "Neural probabilistic language models", "author": ["Yoshua Bengio", "Holger Schwenk", "Jean-S\u00e9bastien Sen\u00e9cal", "Fr\u00e9deric Morin", "Jean-Luc Gauvain."], "venue": "Innovations in Machine Learning, pages 137\u2013186. Springer.", "citeRegEx": "Bengio et al\\.,? 2006", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "A unified model for word sense representation and disambiguation", "author": ["Xinxiong Chen", "Zhiyuan Liu", "Maosong Sun."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1025\u20131035.", "citeRegEx": "Chen et al\\.,? 2014", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of the 25th international conference on Machine learning, pages 160\u2013167. ACM.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "The Journal of Machine Learning Research, 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "An introduction to the bootstrap", "author": ["Bradley Efron", "Robert J Tibshirani."], "venue": "CRC press.", "citeRegEx": "Efron and Tibshirani.,? 1994", "shortCiteRegEx": "Efron and Tibshirani.", "year": 1994}, {"title": "A bayesian analysis of some nonparametric problems", "author": ["Thomas S Ferguson."], "venue": "The annals of statistics, pages 209\u2013230.", "citeRegEx": "Ferguson.,? 1973", "shortCiteRegEx": "Ferguson.", "year": 1973}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."], "venue": "Proceedings of the 10th international conference on World Wide Web, pages 406\u2013", "citeRegEx": "Finkelstein et al\\.,? 2001", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Hierarchical topic models and the nested chinese restaurant process", "author": ["DMBTL Griffiths", "MIJJB Tenenbaum."], "venue": "Advances in neural information processing systems, 16:17.", "citeRegEx": "Griffiths and Tenenbaum.,? 2004", "shortCiteRegEx": "Griffiths and Tenenbaum.", "year": 2004}, {"title": "Semeval-2010 task 8: Multi-way classification of semantic relations", "author": ["Iris Hendrickx", "Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid \u00d3 S\u00e9aghdha", "Sebastian Pad\u00f3", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz"], "venue": null, "citeRegEx": "Hendrickx et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hendrickx et al\\.", "year": 2009}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Eric H Huang", "Richard Socher", "Christopher D Manning", "Andrew Y Ng."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["Ozan Irsoy", "Claire Cardie."], "venue": "Advances in Neural Information Processing Systems, pages 2096\u20132104.", "citeRegEx": "Irsoy and Cardie.,? 2014", "shortCiteRegEx": "Irsoy and Cardie.", "year": 2014}, {"title": "When are tree structures necessary for deep learning of representations? arXiv preprint arXiv:1503.00185", "author": ["Jiwei Li", "Dan Jurafsky", "Eudard Hovy"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Topical word embeddings", "author": ["Yang Liu", "Zhiyuan Liu", "Tat-Seng Chua", "Maosong Sun"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual", "author": ["Marco Marelli", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli"], "venue": null, "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "INTERSPEECH, pages 1045\u20131048.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Three new graphical models for statistical language modelling", "author": ["Andriy Mnih", "Geoffrey Hinton."], "venue": "Proceedings of the 24th international conference on Machine learning, pages 641\u2013648. ACM.", "citeRegEx": "Mnih and Hinton.,? 2007", "shortCiteRegEx": "Mnih and Hinton.", "year": 2007}, {"title": "Efficient nonparametric estimation of multiple embeddings per word in vector space", "author": ["Arvind Neelakantan", "Jeevan Shankar", "Alexandre Passos", "Andrew McCallum."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Neelakantan et al\\.,? 2014", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2014}, {"title": "Thumbs up?: sentiment classification using machine learning techniques", "author": ["Bo Pang", "Lillian Lee", "Shivakumar Vaithyanathan."], "venue": "Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 79\u201386. As-", "citeRegEx": "Pang et al\\.,? 2002", "shortCiteRegEx": "Pang et al\\.", "year": 2002}, {"title": "Learning state space trajectories in recurrent neural networks", "author": ["Barak A Pearlmutter."], "venue": "Neural Computation, 1(2):263\u2013269.", "citeRegEx": "Pearlmutter.,? 1989", "shortCiteRegEx": "Pearlmutter.", "year": 1989}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A simple and efficient method to generate word sense representations", "author": ["Luis Nieto Pina", "Richard Johansson."], "venue": "arXiv preprint arXiv:1412.6045.", "citeRegEx": "Pina and Johansson.,? 2014", "shortCiteRegEx": "Pina and Johansson.", "year": 2014}, {"title": "Exchangeable and partially exchangeable random partitions", "author": ["Jim Pitman."], "venue": "Probability theory and related fields, 102(2):145\u2013158.", "citeRegEx": "Pitman.,? 1995", "shortCiteRegEx": "Pitman.", "year": 1995}, {"title": "Learning word representation considering proximity and ambiguity", "author": ["Lin Qiu", "Yong Cao", "Zaiqing Nie", "Yong Rui."], "venue": "Twenty-Eighth AAAI Conference on Artificial Intelligence.", "citeRegEx": "Qiu et al\\.,? 2014", "shortCiteRegEx": "Qiu et al\\.", "year": 2014}, {"title": "Multi-prototype vector-space models of word meaning", "author": ["Joseph Reisinger", "Raymond J Mooney."], "venue": "NAACL.", "citeRegEx": "Reisinger and Mooney.,? 2010", "shortCiteRegEx": "Reisinger and Mooney.", "year": 2010}, {"title": "Contextual correlates of synonymy", "author": ["Herbert Rubenstein", "John B Goodenough."], "venue": "Communications of the ACM, 8(10):627\u2013633.", "citeRegEx": "Rubenstein and Goodenough.,? 1965", "shortCiteRegEx": "Rubenstein and Goodenough.", "year": 1965}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D Manning", "Andrew Y Ng."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and", "citeRegEx": "Socher et al\\.,? 2012", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D Manning."], "venue": "arXiv preprint arXiv:1503.00075.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Hierarchical dirichlet processes", "author": ["Yee Whye Teh", "Michael I Jordan", "Matthew J Beal", "David M Blei."], "venue": "Journal of the american statistical association, 101(476).", "citeRegEx": "Teh et al\\.,? 2006", "shortCiteRegEx": "Teh et al\\.", "year": 2006}, {"title": "Sense-aware semantic analysis: A multi-prototype word representation model using wikipedia", "author": ["Zhaohui Wu", "C Lee Giles"], "venue": null, "citeRegEx": "Wu and Giles.,? \\Q2015\\E", "shortCiteRegEx": "Wu and Giles.", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": ", (Bengio et al., 2006)).", "startOffset": 2, "endOffset": 23}, {"referenceID": 26, "context": "(Reisinger and Mooney, 2010; Neelakantan et al., 2014; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Wu and Giles, 2015; Liu et al., 2015).", "startOffset": 0, "endOffset": 157}, {"referenceID": 19, "context": "(Reisinger and Mooney, 2010; Neelakantan et al., 2014; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Wu and Giles, 2015; Liu et al., 2015).", "startOffset": 0, "endOffset": 157}, {"referenceID": 11, "context": "(Reisinger and Mooney, 2010; Neelakantan et al., 2014; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Wu and Giles, 2015; Liu et al., 2015).", "startOffset": 0, "endOffset": 157}, {"referenceID": 2, "context": "(Reisinger and Mooney, 2010; Neelakantan et al., 2014; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Wu and Giles, 2015; Liu et al., 2015).", "startOffset": 0, "endOffset": 157}, {"referenceID": 23, "context": "(Reisinger and Mooney, 2010; Neelakantan et al., 2014; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Wu and Giles, 2015; Liu et al., 2015).", "startOffset": 0, "endOffset": 157}, {"referenceID": 32, "context": "(Reisinger and Mooney, 2010; Neelakantan et al., 2014; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Wu and Giles, 2015; Liu et al., 2015).", "startOffset": 0, "endOffset": 157}, {"referenceID": 14, "context": "(Reisinger and Mooney, 2010; Neelakantan et al., 2014; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Wu and Giles, 2015; Liu et al., 2015).", "startOffset": 0, "endOffset": 157}, {"referenceID": 27, "context": "Such sensespecific embeddings have shown improved peformance on simple artificial tasks like matching human word similarity judgements\u2014 WS353 (Rubenstein and Goodenough, 1965) or MC30 (Huang et al.", "startOffset": 142, "endOffset": 175}, {"referenceID": 11, "context": "Such sensespecific embeddings have shown improved peformance on simple artificial tasks like matching human word similarity judgements\u2014 WS353 (Rubenstein and Goodenough, 1965) or MC30 (Huang et al., 2012).", "startOffset": 184, "endOffset": 204}, {"referenceID": 1, "context": "Neural embedding learning frameworks represent each token with a dense vector representation, optimized through predicting neighboring words or decomposing co-occurrence matrices (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013; Mikolov et al., 2010; Pennington et al., 2014).", "startOffset": 179, "endOffset": 320}, {"referenceID": 3, "context": "Neural embedding learning frameworks represent each token with a dense vector representation, optimized through predicting neighboring words or decomposing co-occurrence matrices (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013; Mikolov et al., 2010; Pennington et al., 2014).", "startOffset": 179, "endOffset": 320}, {"referenceID": 18, "context": "Neural embedding learning frameworks represent each token with a dense vector representation, optimized through predicting neighboring words or decomposing co-occurrence matrices (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013; Mikolov et al., 2010; Pennington et al., 2014).", "startOffset": 179, "endOffset": 320}, {"referenceID": 17, "context": "Neural embedding learning frameworks represent each token with a dense vector representation, optimized through predicting neighboring words or decomposing co-occurrence matrices (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013; Mikolov et al., 2010; Pennington et al., 2014).", "startOffset": 179, "endOffset": 320}, {"referenceID": 16, "context": "Neural embedding learning frameworks represent each token with a dense vector representation, optimized through predicting neighboring words or decomposing co-occurrence matrices (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013; Mikolov et al., 2010; Pennington et al., 2014).", "startOffset": 179, "endOffset": 320}, {"referenceID": 22, "context": "Neural embedding learning frameworks represent each token with a dense vector representation, optimized through predicting neighboring words or decomposing co-occurrence matrices (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013; Mikolov et al., 2010; Pennington et al., 2014).", "startOffset": 179, "endOffset": 320}, {"referenceID": 25, "context": "Other relevant work includes (Qiu et al., 2014) that maintains separate representations for different part-of-speech tags of the same word.", "startOffset": 29, "endOffset": 47}, {"referenceID": 21, "context": "For example Reisinger and Mooney (2010) and Huang et al.", "startOffset": 12, "endOffset": 40}, {"referenceID": 10, "context": "For example Reisinger and Mooney (2010) and Huang et al. (2012) propose ways to develop multiple embeddings per word type by pre-clustering the contexts of each token to create a fixed number of senses for each word, and then relabeling each word token with the clustered sense before learning embeddings.", "startOffset": 44, "endOffset": 64}, {"referenceID": 10, "context": "For example Reisinger and Mooney (2010) and Huang et al. (2012) propose ways to develop multiple embeddings per word type by pre-clustering the contexts of each token to create a fixed number of senses for each word, and then relabeling each word token with the clustered sense before learning embeddings. Neelakantan et al. (2014) extend these models by relaxing the assumption that each word must have a fixed number of senses and using a non-parametric model setting a threshold to decide when a new sense cluster should be split off;; Liu et al.", "startOffset": 44, "endOffset": 332}, {"referenceID": 10, "context": "For example Reisinger and Mooney (2010) and Huang et al. (2012) propose ways to develop multiple embeddings per word type by pre-clustering the contexts of each token to create a fixed number of senses for each word, and then relabeling each word token with the clustered sense before learning embeddings. Neelakantan et al. (2014) extend these models by relaxing the assumption that each word must have a fixed number of senses and using a non-parametric model setting a threshold to decide when a new sense cluster should be split off;; Liu et al. (2015) learns sense/topic specific embeddings by combining neural frameworks with LDA topic models.", "startOffset": 44, "endOffset": 557}, {"referenceID": 10, "context": "For example Reisinger and Mooney (2010) and Huang et al. (2012) propose ways to develop multiple embeddings per word type by pre-clustering the contexts of each token to create a fixed number of senses for each word, and then relabeling each word token with the clustered sense before learning embeddings. Neelakantan et al. (2014) extend these models by relaxing the assumption that each word must have a fixed number of senses and using a non-parametric model setting a threshold to decide when a new sense cluster should be split off;; Liu et al. (2015) learns sense/topic specific embeddings by combining neural frameworks with LDA topic models. Wu and Giles (2015) disambiguates sense embeddings from Wikipedia by first clustering wiki documents.", "startOffset": 44, "endOffset": 670}, {"referenceID": 2, "context": "Chen et al. (2014) turned to external resources and used a predefined inventory of senses like Wordnet, building a distinct representation for every sense defined by the Wordnet dictionary.", "startOffset": 0, "endOffset": 19}, {"referenceID": 11, "context": "most specifically Huang et al. (2012) and Neelakantan et al.", "startOffset": 18, "endOffset": 38}, {"referenceID": 11, "context": "most specifically Huang et al. (2012) and Neelakantan et al. (2014), to develop an algorithm for learning multiple embeddings for each word type, each embedding corresponding to a distinct induced word sense.", "startOffset": 18, "endOffset": 68}, {"referenceID": 8, "context": "of thinking naturally points to Chinese Restaurant Processes (CRP) (Griffiths and Tenenbaum, 2004; Teh et al., 2006) which have been applied in the related field of word sense induction.", "startOffset": 67, "endOffset": 116}, {"referenceID": 31, "context": "of thinking naturally points to Chinese Restaurant Processes (CRP) (Griffiths and Tenenbaum, 2004; Teh et al., 2006) which have been applied in the related field of word sense induction.", "startOffset": 67, "endOffset": 116}, {"referenceID": 8, "context": "We offer a brief overview of Chinese Restaurant Processes in this section; readers interested in more details can consult the original papers (Griffiths and Tenenbaum, 2004; Teh et al., 2006; Pitman, 1995).", "startOffset": 142, "endOffset": 205}, {"referenceID": 31, "context": "We offer a brief overview of Chinese Restaurant Processes in this section; readers interested in more details can consult the original papers (Griffiths and Tenenbaum, 2004; Teh et al., 2006; Pitman, 1995).", "startOffset": 142, "endOffset": 205}, {"referenceID": 24, "context": "We offer a brief overview of Chinese Restaurant Processes in this section; readers interested in more details can consult the original papers (Griffiths and Tenenbaum, 2004; Teh et al., 2006; Pitman, 1995).", "startOffset": 142, "endOffset": 205}, {"referenceID": 6, "context": "CRP can be viewed as a practical interpretation of Dirichlet Processes (Ferguson, 1973) for non-parametric clustering.", "startOffset": 71, "endOffset": 87}, {"referenceID": 3, "context": "2 \u03b3 is set to 10 Due to space limits, we omit details about training standard distributed models, which can be readily found in (Collobert and Weston, 2008; Mikolov et al., 2013).", "startOffset": 128, "endOffset": 178}, {"referenceID": 17, "context": "2 \u03b3 is set to 10 Due to space limits, we omit details about training standard distributed models, which can be readily found in (Collobert and Weston, 2008; Mikolov et al., 2013).", "startOffset": 128, "endOffset": 178}, {"referenceID": 3, "context": "w\u2208neigh p(ew, ew\u2032) for skip-gram or F = p(ew, g(ew)) for SENNA(Collobert and Weston, 2008) and CBOW, where g(eneigh) denotes a function that projects the concatenation of neighboring vectors to a vector with the same dimension as ew for SENNA and the bag-or-word averaging for CBOW (Mikolov et al.", "startOffset": 62, "endOffset": 90}, {"referenceID": 17, "context": "w\u2208neigh p(ew, ew\u2032) for skip-gram or F = p(ew, g(ew)) for SENNA(Collobert and Weston, 2008) and CBOW, where g(eneigh) denotes a function that projects the concatenation of neighboring vectors to a vector with the same dimension as ew for SENNA and the bag-or-word averaging for CBOW (Mikolov et al., 2013).", "startOffset": 282, "endOffset": 304}, {"referenceID": 0, "context": "This idea is inspired by many early findings in WSD literature (0; Agirre and Soroa, 2007) that global (i.", "startOffset": 63, "endOffset": 90}, {"referenceID": 11, "context": "Similar strategy is also found in multisense embedding learning algorithm in (Huang et al., 2012).", "startOffset": 77, "endOffset": 97}, {"referenceID": 17, "context": "The training approach is implemented using skip-grammar (SG) (Mikolov et al., 2013).", "startOffset": 61, "endOffset": 83}, {"referenceID": 7, "context": "Early work used similarity datasets like WS353 (Finkelstein et al., 2001) or RG (Rubenstein and", "startOffset": 47, "endOffset": 73}, {"referenceID": 19, "context": "The performance from baselines is reprinted from (Neelakantan et al., 2014; Chen et al., 2014) and we report the best performance across all different settings mentioned in their paper.", "startOffset": 49, "endOffset": 94}, {"referenceID": 2, "context": "The performance from baselines is reprinted from (Neelakantan et al., 2014; Chen et al., 2014) and we report the best performance across all different settings mentioned in their paper.", "startOffset": 49, "endOffset": 94}, {"referenceID": 11, "context": "Stanford\u2019s Contextual Word Similarities (SCWS) (Huang et al., 2012), in which human judgments are associated with pairs of words in context.", "startOffset": 47, "endOffset": 67}, {"referenceID": 19, "context": ", (Neelakantan et al., 2014)), we find that multi-sense embeddings result in better performance in the context-dependent SCWS task (SG+Greedy and SG+Expect are better than SG).", "startOffset": 2, "endOffset": 28}, {"referenceID": 19, "context": ", (Neelakantan et al., 2014)).", "startOffset": 2, "endOffset": 28}, {"referenceID": 4, "context": ", NER, POS), we follow the protocols in (Collobert et al., 2011) using the concatenation of neighboring embeddings as input fea-", "startOffset": 40, "endOffset": 64}, {"referenceID": 30, "context": ", (Tai et al., 2015; Irsoy and Cardie, 2014)).", "startOffset": 2, "endOffset": 44}, {"referenceID": 12, "context": ", (Tai et al., 2015; Irsoy and Cardie, 2014)).", "startOffset": 2, "endOffset": 44}, {"referenceID": 5, "context": "done via the bootstrap test (Efron and Tibshirani, 1994).", "startOffset": 28, "endOffset": 56}, {"referenceID": 4, "context": "We follow the protocols in Collobert et al. (2011), using the concatenation of neighboring embeddings as input to a multi-layer neural model.", "startOffset": 27, "endOffset": 51}, {"referenceID": 20, "context": "Sentence-level Sentiment Classification (Pang) The sentiment dataset of Pang et al. (2002) consists of movie reviewes with a sentiment label", "startOffset": 72, "endOffset": 91}, {"referenceID": 21, "context": "Sentence level embeddings are achieved by using standard sequence recurrent neural models (Pearlmutter, 1989) and then fed into a sigmoid classifier.", "startOffset": 90, "endOffset": 109}, {"referenceID": 13, "context": "We followed the exact model architecture and training strategy of Li et al. (2015); refer to that paper for more details.", "startOffset": 66, "endOffset": 83}, {"referenceID": 29, "context": "Sentiment Analysis\u2013Stanford Treebank The Stanford Sentiment Treebank (Socher et al., 2013) contains gold-standard labels for each constituent in the parse tree (phrase level), thus allowing us to investigate a sentiment task at a finer granuarity than the dataset in Pang et al.", "startOffset": 69, "endOffset": 90}, {"referenceID": 20, "context": ", 2013) contains gold-standard labels for each constituent in the parse tree (phrase level), thus allowing us to investigate a sentiment task at a finer granuarity than the dataset in Pang et al. (2002) where labels are only sound at top of each sentence, The sentences in the treebank were split into a training(8544)/development(1101)/testing(2210) dataset.", "startOffset": 184, "endOffset": 203}, {"referenceID": 20, "context": ", 2013) contains gold-standard labels for each constituent in the parse tree (phrase level), thus allowing us to investigate a sentiment task at a finer granuarity than the dataset in Pang et al. (2002) where labels are only sound at top of each sentence, The sentences in the treebank were split into a training(8544)/development(1101)/testing(2210) dataset.", "startOffset": 184, "endOffset": 337}, {"referenceID": 20, "context": ", 2013) contains gold-standard labels for each constituent in the parse tree (phrase level), thus allowing us to investigate a sentiment task at a finer granuarity than the dataset in Pang et al. (2002) where labels are only sound at top of each sentence, The sentences in the treebank were split into a training(8544)/development(1101)/testing(2210) dataset.", "startOffset": 184, "endOffset": 351}, {"referenceID": 29, "context": "Following (Socher et al., 2013) we obtained embeddings for tree nodes by using a recursive neural network model, where the embedding for parent node is obtained in a bottom-up fashion based on its children.", "startOffset": 10, "endOffset": 31}, {"referenceID": 13, "context": "(2013) and Li et al. (2015) for more details.", "startOffset": 11, "endOffset": 28}, {"referenceID": 9, "context": "Semantic Relationship Classification SemEval-2010 Task 8 (Hendrickx et al., 2009) is to find semantic relationships between pairs of", "startOffset": 57, "endOffset": 81}, {"referenceID": 29, "context": "Note that this is different from the settings used in (Socher et al., 2013) where word vectors were treated as parameters to optimize.", "startOffset": 54, "endOffset": 75}, {"referenceID": 9, "context": "The dataset contains 9 ordered relationships, so the task is formalized as a 19-class classification problem, with directed relations treated as separate labels; see Hendrickx et al. (2009) for details.", "startOffset": 166, "endOffset": 190}, {"referenceID": 28, "context": "We follow the recursive implementations defined in Socher et al. (2012). The path in the parse tree between the two nominals is retrieved, and the embedding is calculated based on recursive models and fed to a softmax classifier.", "startOffset": 51, "endOffset": 72}, {"referenceID": 15, "context": "Sentence Semantic Relatedness We use the Sentences Involving Compositional Knowledge (SICK) dataset (Marelli et al., 2014) consisting of 9927 sentence pairs, split into training(4500)/development(500)/Testing(4927).", "startOffset": 100, "endOffset": 122}, {"referenceID": 10, "context": "We adopt two version of recurrent model, standard one and LSTM model (Hochreiter and Schmidhuber, 1997).", "startOffset": 69, "endOffset": 103}, {"referenceID": 10, "context": "We adopt two version of recurrent model, standard one and LSTM model (Hochreiter and Schmidhuber, 1997). We follow the protocols defined in Li et al. (2015). Please reference to that paper for more details.", "startOffset": 70, "endOffset": 157}], "year": 2017, "abstractText": "Learning a distinct representation for each sense of an ambiguous word could lead to more powerful and fine-grained models of vector-space representations. Yet while \u2018multi-sense\u2019 methods have been proposed and tested on artificial wordsimilarity tasks, we don\u2019t know if they improve real natural language understanding tasks. In this paper we introduce a pipelined architecture for incorporating multi-sense embeddings into language understanding, and test the performance of a state-of-the-art multi-sense embedding model (based on Chinese Restaurant Processes). We apply the model to part-of-speech tagging, named entity recognition, sentiment analysis, semantic relation identification and semantic relatedness. We find that\u2014 if we carefully control for the number of dimensions\u2014 sense-specific embeddings, whether alone or concatenated with standard (one vector for all senses) embeddings, introduce slight performance boost in semantic-related tasks, but is of little in others that depend on correctly identifying a few key words such as sentiment analysis.", "creator": null}}}