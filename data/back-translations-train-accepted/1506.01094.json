{"id": "1506.01094", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2015", "title": "Traversing Knowledge Graphs in Vector Space", "abstract": "Path queries on a knowledge graph can be used to answer compositional questions such as \"What languages are spoken by people living in Lisbon?\". However, knowledge graphs often have missing facts (edges) which disrupts path queries. Recent models for knowledge base completion impute missing facts by embedding knowledge graphs in vector spaces. We show that these models can be recursively applied to answer path queries, but that they suffer from cascading errors. This motivates a new \"compositional\" training objective, which dramatically improves all models' ability to answer path queries, in some cases more than doubling accuracy. On a standard knowledge base completion task, we also demonstrate that compositional training acts as a novel form of structural regularization, reliably improving performance across all base models (reducing errors by up to 43%) and achieving new state-of-the-art results.", "histories": [["v1", "Wed, 3 Jun 2015 00:38:25 GMT  (262kb,D)", "http://arxiv.org/abs/1506.01094v1", null], ["v2", "Wed, 19 Aug 2015 05:16:24 GMT  (264kb,D)", "http://arxiv.org/abs/1506.01094v2", "2015 Conference on Empirical Methods on Natural Language Processing (EMNLP)"]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.DB stat.ML", "authors": ["kelvin guu", "john miller", "percy liang"], "accepted": true, "id": "1506.01094"}, "pdf": {"name": "1506.01094.pdf", "metadata": {"source": "CRF", "title": "Traversing Knowledge Graphs in Vector Space", "authors": ["Kelvin Gu", "John Miller", "Percy Liang"], "emails": ["kgu@stanford.edu", "millerjp@stanford.edu", "pliang@cs.stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "Broad knowledge bases such as Freebase (Bollacker et al., 2008) support a rich range of arguments and questions answering applications, but they are known to suffer from incomplete coverage (Min et al., 2013). For example, as of May 2015, Freebase has a unit Tad Lincoln (Abraham Lincoln's son), but does not have his ethnicity. An elegant solution to incompleteness is the use of vector space representations: controlling the dimensionality of vector space forces generalization to new facts (Nickel al., 2011; Nickel et al., 2012; Socher et al., 2013; Riedel et al., 2013; Neelakantan et al., 2015)."}, {"heading": "2 Task", "text": "Let E be a set of entities, and R a set of binary relationships. A knowledge diagram G is defined as a set of triples of the form (s, r, t) in which s, t, E and r, R. An example of a triple is in the freebase (tad lincoln, parent, abraham lincoln).A path query q consists of an initial anchor unit, s, followed by a sequence of relationships to be traversed, p = (r1,., rk).The response or denotation of the query, JqK, is the set of entities that can be reached by s traversing p. Formally, this can be defined recursively: JsK def = (r1), (1) Jq / rK def = {t: Jqf, K, K (r)."}, {"heading": "3 Compositionalization", "text": "In this section, we will demonstrate how to assemble existing KBC models to answer path queries, starting with a motivational example in Section 3.1, and then introducing the general technique in Section 3.2, which suggests a new compositional training goal described in Section 3.3. Finally, we will demonstrate the technique for several other models in Section 3.4 that we use in our experiments."}, {"heading": "3.1 Example", "text": "A common vector space model for completing the knowledge base is the bilinear model. In this model, we learn a vector xe-Rd for each entity e-E and a matrix Wr-Rd-d for each relationship r-R. To motivate our composition techniques, we leave d = | E | and select Wr as an adjacence matrix for each relationship r and each entity vector xe as an indicator vector (with a 1 in the input corresponding to the entity e). (5) To answer a path query q = s / r1 /. / rk, we assume that the adjacency matrix for each relationship r and each entity vector should be an indicator vector (with a 1 in the input)."}, {"heading": "3.2 General technique", "text": "The strategy applied to extend the bilinear model from (5) to the compositional model in (6) can be applied to any composable model: namely, one that acts as the scoring function of the form: Score (s / r, t) = M (Tr (xs), xt) (7) for any membership operator M: Rd \u00b7 Rd \u2192 R and transversal operator Tr: Rd \u2192 Rd.We can now define the vector name of a query model JqKV analogous to the definition of JqK in (1) and (2): JsKV def = xs, (8) Jq / rKV def = Tr (JqKV). (9) The score function for a compositioned model is thenscore (q, t) = M (JqKV, JtKV). (10) We would like JqKV to be roughly the set JqK in the jewel meaning that each of the JqK, each of the JqK, each of the JK, each of the JqK, each of the JK, each of the qK, each of the information (qK, each of the JK, each of the qK, each of the qK)."}, {"heading": "3.3 Compositional training", "text": "The score function in (10), of course, suggests a new compositional training target. Let {(qi, ti)} Ni = 1 provide a series of training examples for the path query with a path length from 1 to L. We minimize the following margin target: J (\u044b) = N \u2211 i = 1 \u2211 t \"N (qi) [1 \u2212 margin (qi, ti, t\") +, margin (q, t \") = score (q, t\") \u2212 score (q, t \"), where the parameters are the membership operators, the traversal operators and the entity vectors. Note that this target explicitly promotes the notion of\" set vectors \": Since there are path queries of different lengths and types, the model must learn to generate a precise set vector JqKV according to an arbitrary sequence of transversals."}, {"heading": "3.4 Other composable models", "text": "There are many possible candidates for T and M. For example, T could be one of the preferred neural network mappings from Rd to Rd. Here, we focus on two composable models, both of which have recently shown that they provide state-of-the-art performance based on the knowledge base. TransE. The TransE model from Bordes et al. (2013) uses the scoring function score (s / r, t) = \u2212 xs + wr \u2212 xt. (11) where xs, wr and xt are all d-dimensional vectors. In this case, the model can be combined with the membership operator M (v, xt) = \u2212 xs \u2212 xt and the transversal operator Tr (xs) = xs + wr. Thus, TransE can use a path query q = s / r1 / r2 / \u00b7 / rk usingscore (q, t) = \u2212 xs \u2212 xt \u00b2 and the traverse operator Tr (xs) = xs + wr."}, {"heading": "3.5 Implementation", "text": "We use AdaGrad (Duchi et al., 2010) to optimize J (\u0443). We initialize all parameters with i.i.d. gaussers of variance 0.1 in each entry. Initialization scale, minibatch size, and step size have been cross-validated for all models. We first train on path queries from length 1 to convergence, and then train on all path queries until convergence. This ensures that the model masters basic edges before assembling them into paths. When training on path queries, we explicitly parameterize inverse relationships. In the bilinear model, we initialize Wr \u2212 1 with W > r. In TransE, we initialize wr \u2212 1 with \u2212 wr. In bilinear diag, we found that the initialization wr \u2212 1 with the exact inverse sum 1 / wr is numerically unstable, so we instead initialize wr \u2212 1 with i.i.d gaussiance of the sum of 0.1 in each entry is better than the one available at http."}, {"heading": "4 Datasets", "text": "In Section 4.1, we describe two standard base datasets consisting of edge queries, and in Section 4.2, we create path query datasets from these base datasets."}, {"heading": "4.1 Base datasets", "text": "Our experiments are performed with the subsets of WordNet and Freebase evaluated in Socher et al. (2013). Statistics of these data sets and their splits are in Table 1. The subsets of WordNet and Freebase show significant differences that can affect the performance of the model. Freebase is almost split into two, with most edges taking the form (person, r, t) for a person, relationship r and property t. However, in WordNet both the source and target units are arbitrary. Both WordNet and Freebase contain many relationships that are almost perfectly correlated to an inverse relationship. For example, WordNet contains both part and part of it, and the subset Freebase contains both parents and children. At the test date, a query concerning an edge (s, r, t) is simple if the inverse triple (t, r \u2212 1, s) in the training set was observed to include both a part and a part of it. Socher et al al (2013) was categorized by al (al)."}, {"heading": "4.2 Path query datasets", "text": "If we consider compositional training as a form of regularization, this approach allows us to generate extremely large amounts of training data to prevent overadjustment; the procedure is given below. Let Gtrain be the training graph consisting of the edges in the training set of the base data set. We then repeatedly generate training examples using the following procedure: 1. Select a relationship ri uniformly from the series of relationships that touch the current unit. (b) Select the next unit uniformly from the group of units that are reachable via a set of units. 3. Specify a relationship ri uniformly from the series of relationships that touch the current unit. (b) Select the next unit uniformly from the group of units that are reachable via a set of units. 3. Specify a query-response pair, (q, t) where q = s / r1 / \u00b7 \u00b7 / rL and the final unit of the series of units we can reach from all the units."}, {"heading": "5 Main results", "text": "In fact, it is so that most of them are able to survive themselves if they do not see themselves able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...)"}, {"heading": "6 Analysis", "text": "In this section we will try to understand why compositional training works. Specifically, everything is described in terms of the bilinear model. We will refer to the compositional model as COMP and the rigorous training model as SINGLE."}, {"heading": "6.1 Why does compositional training improve path query answering?", "text": "It is tempting to believe that if SINGLE has accurately modeled individual edges in a diagram, it should accurately model the paths resulting from those edges. This intuition turns out to be wrong, as the relatively weak performance of SINGLE in the path query data set shows. We assume that this is due to cascading errors on the path. For a given edge (s, r, t) on the path, xt encourages one-edged training to be closer to x > s Wr than to any other erroneous xt \u2032. However, once this is achieved with a margin of 1, xt does not push closer to x > s Wr. The remaining discrepancy is noise added at each step of the path crossing. This is illustrated schematically in Figure 2.To observe this phenomenon empirically, we examine how well a model handles each intermediate step of a path query."}, {"heading": "6.2 Why does compositional training improve knowledge base completion?", "text": "This is somewhat surprising since SINGLE is trained on a training set that is distributed according to the test set, whereas COMP is not. However, COMP's better performance on path queries indicates that there must be another factor in the game. At a high level, the following horn clause is considered: Parents (x, y): Parents (x, y): Parents (x, z): Parents (x, z): Parents (x, z): Parents (x): Parents (x): Parents (x): Parents (c): Parents (x): Parents (c): Parents (x): Parents (c): Parents (c): Parents (c): Parents (c): Parents (c): Parents (c): (c): (c): (c): Parents (c): (c): Parents (c): Parents (c): Parents (c): Parents (c): Parents (c): Parents (c): Parents (c): Parents (c): Parents (c): (c): Parents (c): (c): Parents (c): (c): (c): Parents (c): (c): (c): (c): (c): Parents (c): (c): Parents (c): (c): Parents (c): (c): Parents (c): (c): Parents (c): (c): Parents (c): (c): (c): Parents (c): (c: (c): (c): (c): ((c): (c): ((): ((c): (): Parents): ((): (c): ((): (((()): ((((())): Parents): (c): ((c): (): ((((():):): ((((((()))):): ((((((()))))): (((((("}, {"heading": "7 Related work", "text": "Many models have been proposed for completing the knowledge base (Bordes et al., 2013; Yang et al., 2015; Socher et al., 2013), relation extraction (Riedel et al., 2013) and knowledge fusion (Dong et al., 2014). Our compositional training technique is an orthogonal improvement that could help any composable model. Distributive compositional semantics. Previous work has also examined compositional vector space representations in the context of sentence interpretation (Socher et al., 2012; Socher et al., 2014; Grefenstette et al., 2013; Bowman et al., 2014). We show that compositional representations are also useful in the context of querying and completing the knowledge base."}, {"heading": "8 Discussion", "text": "We presented the task of answering path queries on an incomplete knowledge base and presented a general technique for composing a broad class of vector space models. Our experiments show that compositional training leads to state-of-the-art performance in both answering path queries and completing the knowledge base. There are several key ideas from this paper: regulation by enriching the dataset with paths, context-sensitive representation of sets as low-dimensional vectors, and performing functional compositions using vectors. We believe that these could be more applicable to conclusions in the development of vector space models."}], "references": [{"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["K. Bollacker", "C. Evans", "P. Paritosh", "T. Sturge", "J. Taylor."], "venue": "International Conference on Management of Data (SIGMOD), pages 1247\u20131250.", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["A. Bordes", "N. Usunier", "A. Garcia-Duran", "J. Weston", "O. Yakhnenko."], "venue": "Advances in Neural Information Processing Systems (NIPS), pages 2787\u20132795.", "citeRegEx": "Bordes et al\\.,? 2013", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Question answering with subgraph embeddings", "author": ["A. Bordes", "S. Chopra", "J. Weston."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Bordes et al\\.,? 2014", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Recursive neural networks can learn logical semantics", "author": ["S.R. Bowman", "C. Potts", "C.D. Manning."], "venue": "CoRR, (0).", "citeRegEx": "Bowman et al\\.,? 2014", "shortCiteRegEx": "Bowman et al\\.", "year": 2014}, {"title": "Knowledge vault: A web-scale approach to probabilistic knowledge fusion", "author": ["X. Dong", "E. Gabrilovich", "G. Heitz", "W. Horn", "N. Lao", "K. Murphy", "T. Strohmann", "S. Sun", "W. Zhang."], "venue": "International Conference on Knowledge Discovery and Data Min-", "citeRegEx": "Dong et al\\.,? 2014", "shortCiteRegEx": "Dong et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer."], "venue": "Conference on Learning Theory (COLT).", "citeRegEx": "Duchi et al\\.,? 2010", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "Incorporating vector space similarity in random walk inference over knowledge bases", "author": ["M. Gardner", "P. Talukdar", "J. Krishnamurthy", "T. Mitchell."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Gardner et al\\.,? 2014", "shortCiteRegEx": "Gardner et al\\.", "year": 2014}, {"title": "Towards a formal distributional semantics: Simulating logical calculi with tensors", "author": ["E. Grefenstette."], "venue": "arXiv preprint arXiv:1304.5823.", "citeRegEx": "Grefenstette.,? 2013", "shortCiteRegEx": "Grefenstette.", "year": 2013}, {"title": "Random walk inference and learning in a large scale knowledge base", "author": ["N. Lao", "T. Mitchell", "W.W. Cohen."], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 529\u2013539.", "citeRegEx": "Lao et al\\.,? 2011", "shortCiteRegEx": "Lao et al\\.", "year": 2011}, {"title": "Distant supervision for relation extraction with an incomplete knowledge base", "author": ["B. Min", "R. Grishman", "L. Wan", "C. Wang", "D. Gondek."], "venue": "North American Association for Computational Linguistics (NAACL), pages 777\u2013782.", "citeRegEx": "Min et al\\.,? 2013", "shortCiteRegEx": "Min et al\\.", "year": 2013}, {"title": "Compositional vector space models for knowledge base completion", "author": ["A. Neelakantan", "B. Roth", "A. McCallum."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Neelakantan et al\\.,? 2015", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "A three-way model for collective learning on multirelational data", "author": ["M. Nickel", "V. Tresp", "H. Kriegel."], "venue": "International Conference on Machine Learning (ICML), pages 809\u2013816.", "citeRegEx": "Nickel et al\\.,? 2011", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "Factorizing YAGO", "author": ["M. Nickel", "V. Tresp", "H. Kriegel."], "venue": "World Wide Web (WWW).", "citeRegEx": "Nickel et al\\.,? 2012", "shortCiteRegEx": "Nickel et al\\.", "year": 2012}, {"title": "Reducing the rank in relational factorization models by including observable patterns", "author": ["M. Nickel", "X. Jiang", "V. Tresp."], "venue": "Advances in Neural Information Processing Systems (NIPS), pages 1179\u20131187.", "citeRegEx": "Nickel et al\\.,? 2014", "shortCiteRegEx": "Nickel et al\\.", "year": 2014}, {"title": "Deepwalk: Online learning of social representations", "author": ["B. Perozzi", "R. Al-Rfou", "S. Skiena."], "venue": "International Conference on Knowledge Discovery and Data Mining (KDD), pages 701\u2013710.", "citeRegEx": "Perozzi et al\\.,? 2014", "shortCiteRegEx": "Perozzi et al\\.", "year": 2014}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["S. Riedel", "L. Yao", "A. McCallum."], "venue": "North American Association for Computational Linguistics (NAACL).", "citeRegEx": "Riedel et al\\.,? 2013", "shortCiteRegEx": "Riedel et al\\.", "year": 2013}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["R. Socher", "B. Huval", "C.D. Manning", "A.Y. Ng."], "venue": "Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL), pages", "citeRegEx": "Socher et al\\.,? 2012", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["R. Socher", "D. Chen", "C.D. Manning", "A. Ng."], "venue": "Advances in Neural Information Processing Systems (NIPS), pages 926\u2013934.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["R. Socher", "A. Karpathy", "Q.V. Le", "C.D. Manning", "A.Y. Ng."], "venue": "Transactions of the Association for Computational Linguistics (TACL), 2:207\u2013218.", "citeRegEx": "Socher et al\\.,? 2014", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Implementation of logical query languages for databases", "author": ["J.D. Ullman."], "venue": "ACM Transactions on Database Systems (TODS), 10(3):289\u2013321.", "citeRegEx": "Ullman.,? 1985", "shortCiteRegEx": "Ullman.", "year": 1985}, {"title": "Embedding entities and relations for learning and inference in knowledge bases", "author": ["B. Yang", "W. Yih", "X. He", "J. Gao", "L. Deng."], "venue": "arXiv preprint arXiv:1412.6575.", "citeRegEx": "Yang et al\\.,? 2015", "shortCiteRegEx": "Yang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Broad-coverage knowledge bases such as Freebase (Bollacker et al., 2008) support a rich array of reasoning and question answering applications, but they are known to suffer from incomplete coverage (Min et al.", "startOffset": 48, "endOffset": 72}, {"referenceID": 9, "context": ", 2008) support a rich array of reasoning and question answering applications, but they are known to suffer from incomplete coverage (Min et al., 2013).", "startOffset": 133, "endOffset": 151}, {"referenceID": 11, "context": "An elegant solution to incompleteness is using vector space representations: Controlling the dimensionality of the vector space forces generalization to new facts (Nickel et al., 2011; Nickel et al., 2012; Socher et al., 2013; Riedel et al., 2013; Neelakantan et al., 2015).", "startOffset": 163, "endOffset": 273}, {"referenceID": 12, "context": "An elegant solution to incompleteness is using vector space representations: Controlling the dimensionality of the vector space forces generalization to new facts (Nickel et al., 2011; Nickel et al., 2012; Socher et al., 2013; Riedel et al., 2013; Neelakantan et al., 2015).", "startOffset": 163, "endOffset": 273}, {"referenceID": 17, "context": "An elegant solution to incompleteness is using vector space representations: Controlling the dimensionality of the vector space forces generalization to new facts (Nickel et al., 2011; Nickel et al., 2012; Socher et al., 2013; Riedel et al., 2013; Neelakantan et al., 2015).", "startOffset": 163, "endOffset": 273}, {"referenceID": 15, "context": "An elegant solution to incompleteness is using vector space representations: Controlling the dimensionality of the vector space forces generalization to new facts (Nickel et al., 2011; Nickel et al., 2012; Socher et al., 2013; Riedel et al., 2013; Neelakantan et al., 2015).", "startOffset": 163, "endOffset": 273}, {"referenceID": 10, "context": "An elegant solution to incompleteness is using vector space representations: Controlling the dimensionality of the vector space forces generalization to new facts (Nickel et al., 2011; Nickel et al., 2012; Socher et al., 2013; Riedel et al., 2013; Neelakantan et al., 2015).", "startOffset": 163, "endOffset": 273}, {"referenceID": 19, "context": "However, what is missing from these vector space models is the original strength of knowledge bases: the ability to support compositional queries (Ullman, 1985).", "startOffset": 146, "endOffset": 160}, {"referenceID": 11, "context": "Our technique is applicable to a broad class of composable models that includes the bilinear model (Nickel et al., 2011) and TransE (Bordes et al.", "startOffset": 99, "endOffset": 120}, {"referenceID": 1, "context": ", 2011) and TransE (Bordes et al., 2013).", "startOffset": 19, "endOffset": 40}, {"referenceID": 1, "context": "The TransE model of Bordes et al. (2013) uses the scoring function", "startOffset": 20, "endOffset": 41}, {"referenceID": 20, "context": "The Bilinear-Diag model of Yang et al. (2015) is a special case of the bilinear model with the relation matrices constrained to be diagonal.", "startOffset": 27, "endOffset": 46}, {"referenceID": 15, "context": "It is important to point out that some models are not naturally composable\u2014for example, the matrix factorization model of Riedel et al. (2013) and the neural tensor network of Socher et al.", "startOffset": 122, "endOffset": 143}, {"referenceID": 15, "context": "It is important to point out that some models are not naturally composable\u2014for example, the matrix factorization model of Riedel et al. (2013) and the neural tensor network of Socher et al. (2013).", "startOffset": 122, "endOffset": 197}, {"referenceID": 5, "context": "We use AdaGrad (Duchi et al., 2010) to optimize J(\u0398).", "startOffset": 15, "endOffset": 35}, {"referenceID": 16, "context": "Our experiments are conducted using the subsets of WordNet and Freebase evaluated in Socher et al. (2013). The statistics of these datasets and their splits are given in Table 1.", "startOffset": 85, "endOffset": 106}, {"referenceID": 16, "context": "Socher et al. (2013) accounted for this by excluding such \u201ctrivial\u201d queries from their test set.", "startOffset": 0, "endOffset": 21}, {"referenceID": 16, "context": "We also compare directly against the KBC results of Socher et al. (2013), demonstrating that previously inferior models now match or outperform state of the art models after compositional training.", "startOffset": 52, "endOffset": 73}, {"referenceID": 16, "context": "Comparison with Socher et al. (2013). Here, we measure performance on the KBC task in terms of the accuracy metric of Socher et al.", "startOffset": 16, "endOffset": 37}, {"referenceID": 16, "context": "Comparison with Socher et al. (2013). Here, we measure performance on the KBC task in terms of the accuracy metric of Socher et al. (2013). This evaluation involves sampled negatives, and is hence noisier than mean quantile, but makes our results directly comparable to Socher et al.", "startOffset": 16, "endOffset": 139}, {"referenceID": 16, "context": "Comparison with Socher et al. (2013). Here, we measure performance on the KBC task in terms of the accuracy metric of Socher et al. (2013). This evaluation involves sampled negatives, and is hence noisier than mean quantile, but makes our results directly comparable to Socher et al. (2013). Our results show that previously inferior models can outperform state of the art models after compositional training.", "startOffset": 16, "endOffset": 291}, {"referenceID": 8, "context": "be important features for predicting the existence of single edges (Lao et al., 2011).", "startOffset": 67, "endOffset": 85}, {"referenceID": 1, "context": "Many models have been proposed for knowledge base completion (Bordes et al., 2013; Yang et al., 2015; Socher et al., 2013), relation extraction (Riedel et al.", "startOffset": 61, "endOffset": 122}, {"referenceID": 20, "context": "Many models have been proposed for knowledge base completion (Bordes et al., 2013; Yang et al., 2015; Socher et al., 2013), relation extraction (Riedel et al.", "startOffset": 61, "endOffset": 122}, {"referenceID": 17, "context": "Many models have been proposed for knowledge base completion (Bordes et al., 2013; Yang et al., 2015; Socher et al., 2013), relation extraction (Riedel et al.", "startOffset": 61, "endOffset": 122}, {"referenceID": 15, "context": ", 2013), relation extraction (Riedel et al., 2013), and knowledge fusion (Dong et al.", "startOffset": 29, "endOffset": 50}, {"referenceID": 4, "context": ", 2013), and knowledge fusion (Dong et al., 2014).", "startOffset": 30, "endOffset": 49}, {"referenceID": 16, "context": "Previous works have explored compositional vector space representations in the context of sentence interpretation (Socher et al., 2012; Socher et al., 2014; Grefenstette, 2013; Bowman et al., 2014).", "startOffset": 114, "endOffset": 197}, {"referenceID": 18, "context": "Previous works have explored compositional vector space representations in the context of sentence interpretation (Socher et al., 2012; Socher et al., 2014; Grefenstette, 2013; Bowman et al., 2014).", "startOffset": 114, "endOffset": 197}, {"referenceID": 7, "context": "Previous works have explored compositional vector space representations in the context of sentence interpretation (Socher et al., 2012; Socher et al., 2014; Grefenstette, 2013; Bowman et al., 2014).", "startOffset": 114, "endOffset": 197}, {"referenceID": 3, "context": "Previous works have explored compositional vector space representations in the context of sentence interpretation (Socher et al., 2012; Socher et al., 2014; Grefenstette, 2013; Bowman et al., 2014).", "startOffset": 114, "endOffset": 197}, {"referenceID": 7, "context": "Nickel et al. (2014), Gardner et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 4, "context": "(2014), Gardner et al. (2014), Neelakantan et al.", "startOffset": 8, "endOffset": 30}, {"referenceID": 4, "context": "(2014), Gardner et al. (2014), Neelakantan et al. (2015) and Bordes et al.", "startOffset": 8, "endOffset": 57}, {"referenceID": 1, "context": "(2015) and Bordes et al. (2014) Figure 4: We divide paths of length 2 into high precision (> 0.", "startOffset": 11, "endOffset": 32}, {"referenceID": 14, "context": "Perozzi et al. (2014) make use of random walks as training examples, with a different goal to classify nodes in a social network.", "startOffset": 0, "endOffset": 22}], "year": 2015, "abstractText": "Path queries on a knowledge graph can be used to answer compositional questions such as \u201cWhat languages are spoken by people living in Lisbon?\u201d. However, knowledge graphs often have missing facts (edges) which disrupts path queries. Recent models for knowledge base completion impute missing facts by embedding knowledge graphs in vector spaces. We show that these models can be recursively applied to answer path queries, but that they suffer from cascading errors. This motivates a new \u201ccompositional\u201d training objective, which dramatically improves all models\u2019 ability to answer path queries, in some cases more than doubling accuracy. On a standard knowledge base completion task, we also demonstrate that compositional training acts as a novel form of structural regularization, reliably improving performance across all base models (reducing errors by up to 43%) and achieving new state-of-the-art results.", "creator": "TeX"}}}