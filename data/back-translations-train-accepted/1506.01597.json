{"id": "1506.01597", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2015", "title": "Abstractive Multi-Document Summarization via Phrase Selection and Merging", "abstract": "We propose an abstraction-based multi-document summarization framework that can construct new sentences by exploring more fine-grained syntactic units than sentences, namely, noun/verb phrases. Different from existing abstraction-based approaches, our method first constructs a pool of concepts and facts represented by phrases from the input documents. Then new sentences are generated by selecting and merging informative phrases to maximize the salience of phrases and meanwhile satisfy the sentence construction constraints. We employ integer linear optimization for conducting phrase selection and merging simultaneously in order to achieve the global optimal solution for a summary. Experimental results on the benchmark data set TAC 2011 show that our framework outperforms the state-of-the-art models under automated pyramid evaluation metric, and achieves reasonably well results on manual linguistic quality evaluation.", "histories": [["v1", "Thu, 4 Jun 2015 14:04:10 GMT  (65kb,D)", "https://arxiv.org/abs/1506.01597v1", "11 pages, 1 figures, accepted as a full paper at ACL 2015"], ["v2", "Fri, 5 Jun 2015 15:02:46 GMT  (65kb,D)", "http://arxiv.org/abs/1506.01597v2", "11 pages, 1 figure, accepted as a full paper at ACL 2015"]], "COMMENTS": "11 pages, 1 figures, accepted as a full paper at ACL 2015", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["lidong bing", "piji li", "yi liao", "wai lam", "weiwei guo", "rebecca j passonneau"], "accepted": true, "id": "1506.01597"}, "pdf": {"name": "1506.01597.pdf", "metadata": {"source": "CRF", "title": "Abstractive Multi-Document Summarization via Phrase Selection and Merging\u2217", "authors": ["Lidong Bing", "Piji Li", "Yi Liao", "Wai Lam", "Weiwei Guo", "Rebecca J. Passonneau"], "emails": ["lbing@cs.cmu.edu,", "wlam}@se.cuhk.edu.hk", "wguo@yahoo-inc.com,", "becky@ccls.columbia.edu"], "sections": [{"heading": "1 Introduction", "text": "Most of the people who appear in this book are able to express their opinions to themselves and to others."}, {"heading": "2 Description of Our Framework", "text": "First, we present how to extract NPs and VPs from constituency trees and then calculate emphasis values from them. Then, we formulate the sentence generation task as an optimization problem and design constraints. Finally, we perform several post-processing steps to improve the order and legibility of the generated sentences."}, {"heading": "2.1 Phrase Salience Calculation", "text": "In fact, it is a matter of a way in which people are able to determine for themselves how they have behaved. (...) It is not as if they are able to behave. (...) It is as if they are able to behave. (...) It is as if they are able to behave. (...) It is as if they are able to behave. (...) It is as if they are able to behave. (...) It is as if they are able to hide. (...) It is as if they are able to hide. (...)"}, {"heading": "2.2 New Sentence Construction Model", "text": "The construction of new sentences is formulated as an optimization problem that is able to generate a group of sentences at the same time. Each new sentence consists of a NP and at least one VP, with NP and VPs coming from different sets of origin. In the process of new sentence generation, the compatibility relationships between NP and VP and a variety of summary requirements are considered jointly."}, {"heading": "2.2.1 Compatibility Relation", "text": "The compatibility relationship is designed to indicate whether a NP and a VP can be used to form a new sentence q. For example, the NP \"policy\" from another sentence should not be the subject of the VP \"sent the boys outside\" extracted from Figure 1. We use some heuristics to find compatibility, and then expand the compatibility relationship to more phrases by finding core reference NPs (different mentions for the same entity), we first perform core resolution for each document with Stanford core resolution package (Lee et al., 2013). We adopt these solution rules, which are able to achieve high quality and address our need for summarization. Specifically, Sieve 1, 2, 3, 5, 9 and 10 in the package are used. A number of clusters are achieved and each cluster contains the mentions that relate to the same entity in a document."}, {"heading": "2.2.2 Phrase-based Content Optimization", "text": "The general objective function of our optimization formulation for selecting NPs and VPs is defined as: max {\u2211 i \u03b1iS N i \u2212 \u2211 i < j \u03b1ij (S N i + S N j) R N ij + \u2211 i \u03b2iS V i \u2212 \u2211 i < j \u03b2ij (S V i + S V j) R V ij}, (3) where \u03b1i and \u03b2i are selection indicators for NP Ni and VP Vi respectively. SNi and S V i are the emphasis values of Ni and Vi. \u03b1ij and \u03b2ij are simultaneous occurrence indicators for pairs (Ni, Nj) and (Vi, Vj). RNij and R V ij are the similarity of pairs (Ni, Nj) and (Vi, Vj)."}, {"heading": "2.2.3 Sentence Generation Constraints", "text": "To summarize the relevant sentences in the documents, we must summarize the important facts in different VPs into a single sentence and hide the trivial facts. (i.e.) The same entity is also likely if the compatibility-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-activities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-activities-entities-entities-entities-entities-entities-entities-entities-entities-entities-activities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-activities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-activities-entities-entities-entities-entities-entities-entities-entities-entities-entities-activities-entities-entities-entities-entities-entities-entities-entities-entities-activities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities-entities"}, {"heading": "2.3 Postprocessing", "text": "Let's remember that we require a NP and at least one VP to compose a sentence. Thus, we form a raw sentence with a selected NP as a topic, followed by the corresponding selected VPs, which are specified by the sentence generation indicator \u03b3-ij with the value 1. VPs in a summary sentence are sorted according to their natural order if they come from the same document. Otherwise, they are sorted according to the timestamps of the corresponding documents. Then, if the total length is less than L, we add conjunctions like \"and\" and \"then\" to link the VPs to improve the legibility of the newly generated sentences. The pseudo-timestamp of a sentence is defined as the earliest timestamp of its VPs and the sentences are sorted based on their pseudo-timestamps."}, {"heading": "2.4 Relation to Existing MDS Approaches", "text": "Many existing extraction-based and compression-based MDS approaches could be considered special cases in our scope: (1) To simulate extraction-based summaries, we only need to restrict the selection of the highest NP and highest VP from the same set at the same time. Furthermore, no NP and VPs can be selected at lower levels. Therefore, the output only contains the original sets of source documents. (2) To simulate compression-based summaries, we can adjust our framework so that sentence selection and sentence compression are performed jointly. Specifically, we only need to limit that NP and VPs of a summary set must come from the same original set."}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Experimental Setup", "text": "Each topic falls into one of 5 predefined event categories and contains 10 related news documents. There are four authors who write model summaries for each topic. The data set of the traditional summary task in TAC 2010 is used as a development / tuning data set. This data set contains 46 topics from the same predefined categories. Each topic also has 10 documents and 4 model summaries. Based on the tuning set, the key parameters of our model are set as follows: the constants B and B in the weighting function are set to 6 and 0.5. The similarity threshold when reaching the alternative VPs is 0.75. We have no significant difference between cosmic similarity and Jaccard index.We evaluate the system primarily using pyramid evaluation. To gain a comprehensive understanding, we also evaluate using ROGE evaluation and manual evaluation."}, {"heading": "3.2 Results with Pyramid Evaluation", "text": "Pyramid evaluation metric (Nenkova and Passonneau, 2004) includes semantic matching of Summary Content Units (SCUs) to identify alternative realizations of the same meaning. Different weights are assigned to SCUs based on their frequency in model summaries. A weighted inventory of SCUs called a pyramid is created, which is a resource for studying alternative realizations of the same meaning. Such properties make pyramid methods more suitable for evaluating summaries. Another widely used evaluation method is ROUGE (Lin and Hovy, 2003) and it evaluates summaries from word-overlaps of perspective. Due to strict string matching, it ignores semantic content units and performs better summaries. Unlike ROUGE, pyramid scoring is robust with as few as four model summaries (Nenkova and Passonneau, 2004)."}, {"heading": "3.3 Results with ROUGE Evaluation", "text": "As mentioned above, we prefer the pyramid rating over the ROUGE score because it allows the quality of the summary to be measured beyond simple string matching. At this point, we also give the ROUGE score as a reference. ROUGE-1.5.5 Package3 is used with the same parameters as in TAC. Results are summarized in Table 3. Our performance is slightly better than System 22, and it is not as good as System 43 and 17. This is because System 43 and 17 used category-specific features for the summary, and the feature weights are coached with the category information3http: / / www.berouge.com / Pages / default.aspxin TAC 2010 data. These features help them in selecting better category-specific content for the summary. However, the applicability of such features depends on the availability of predefined categories in the summary task, as well as the availability of general training data with the same predefined framework for our 2010 assessment of certain characteristics. Therefore, the applicability of such features is limited only to the degree of adjustment of the specific category."}, {"heading": "3.4 Linguistic Quality Evaluation", "text": "The linguistic quality of summaries is evaluated on the basis of the five linguistic quality questions on grammar (Q1), redundancy (Q2), reference clarity (Q3), focus (Q4) and coherence (Q5) in Document Understanding Conferences (DUC). A five-level Likert scale is used, with 5 being very good and 1 being very bad. One summary was blindly rated by three evaluators on each question. System 22 performs better than System 43 and 17 in TAC 2011 when evaluating readability, which is a summary of the above questions. Considering the intensive manual assessment work, we only make a comparison with System 22. Results are given in Table 4. On average, the two systems perform very accurately. System 22 is an extraction-based method that selects the original sentences, and therefore achieves higher values in Q1 grammatical errors, while our approach has some new sentences with grammatical errors."}, {"heading": "4 Qualitative Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Analysis of Summary Sentence Type", "text": "There are three types of sentences in the summaries generated by our framework: new sentences, compressed sentences and original sentences. A new sentence is created by merging sentences from different original sentences. A compressed sentence is created by deleting phrases from an original sentence. An original sentence in the summary is extracted directly from the input documents. We calculate the percentage of different sentences in our summaries. Approximately 33% of the summary sentences are reconstructed. This shows that our framework is well able to merge phrases from the original sentences to convey more information in compressed summaries. In addition, about 44% of the summary sentences are generated by compression. It shows a unique feature of our framework: sentence building and sentence compression are carried out in a uniform model."}, {"heading": "4.2 Case Study", "text": "Table 5 shows the summary of the first topic, i.e., \"Amish Shooting,\" from our frame. \"The summary sentence ID and the sentence type are given in the form of\" [summary sentence ID: sentence type]. \"Each selected sentence and the original sentence ID, where the sentence originated, are given in the form of\" {selected phrase (original sentence ID). \"There are three compressed sentences with IDs 1, 2 and 4, a new sentence with ID 3, and two original sentences with IDs 5 and 6. The new sentence is constructed from the following original sentences, in which the extracted NPs and VPs are indicated with colored brackets: (84): On Monday morning (NP Charles Roberts IV) entered the West Nickel Mines Amish School in Lancaster County and (VP shot 10 girls), (VP killed five girls): (VP killed five): (VP killed five): (VP killed himself as police stormed the building)."}, {"heading": "5 Related Work", "text": "In fact, most of them are able to survive by themselves if they don't put themselves in a position to survive by themselves; most of them are able to survive by themselves; most of them are able to survive by themselves; most of them are able to survive by themselves; and most of them are able to survive by themselves; most of them are not able to survive by themselves; most of them are able to survive by themselves; most of them are able to survive by themselves; and most of them are able to survive by themselves."}, {"heading": "6 Conclusions and Future Work", "text": "We propose an abstract MDS framework that constructs new sentences by exploring finer-grained syntactic units, namely noun and verbal phrases. The designed optimization framework works at the sum level so that more complementary semantic units of content can be integrated. Phrase selection and merging are performed simultaneously to achieve global optimum. Meanwhile, the constructed sentences should meet the constraints associated with sum requirements such as NP / VP compatibility. Experimental results from the TAC 2011 sum dataset show that our framework outperforms the top systems of TAC 2011 under the pyramid metric. For future work, one aspect is to improve the grammar quality of the new sentences and compressed sentences generated. Another aspect is to improve the time efficiency of our framework, and its biggest bottleneck is the time-consuming ILP optimization."}], "references": [{"title": "Fast and robust compressive summarization with dual decomposition and multitask learning", "author": ["Almeida", "Martins2013] Miguel Almeida", "Andre Martins"], "venue": "In ACL,", "citeRegEx": "Almeida et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Almeida et al\\.", "year": 2013}, {"title": "Sentence fusion for multidocument news summarization", "author": ["Barzilay", "McKeown2005] Regina Barzilay", "Kathleen R. McKeown"], "venue": null, "citeRegEx": "Barzilay et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Barzilay et al\\.", "year": 2005}, {"title": "Query-chain focused summarization", "author": ["Baumel et al.2014] Tal Baumel", "Raphael Cohen", "Michael Elhadad"], "venue": "In ACL,", "citeRegEx": "Baumel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baumel et al\\.", "year": 2014}, {"title": "Jointly learning to extract and compress", "author": ["Dan Gillick", "Dan Klein"], "venue": "In HLT,", "citeRegEx": "Berg.Kirkpatrick et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Berg.Kirkpatrick et al\\.", "year": 2011}, {"title": "Ranking with recursive neural networks and its application to multidocument summarization", "author": ["Cao et al.2015] Ziqiang Cao", "Furu Wei", "Li Dong", "Sujian Li", "Ming Zhou"], "venue": null, "citeRegEx": "Cao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2015}, {"title": "Concept-based classification for multi-document summarization", "author": ["\u00c7elikyilmaz", "Dilek Hakkani-T\u00fcr"], "venue": "In ICASSP,", "citeRegEx": "\u00c7elikyilmaz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "\u00c7elikyilmaz et al\\.", "year": 2011}, {"title": "Towards robust abstractive multi-document summarization: A caseframe analysis of centrality and domain", "author": ["Cheung", "Penn2013] Jackie Chi Kit Cheung", "Gerald Penn"], "venue": "In ACL,", "citeRegEx": "Cheung et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cheung et al\\.", "year": 2013}, {"title": "Hierarchical summarization: Scaling up multidocument summarization", "author": ["Stephen Soderland", "Gagan Bansal", "Mausam"], "venue": "In ACL,", "citeRegEx": "Christensen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Christensen et al\\.", "year": 2014}, {"title": "Linear Programming 1: Introduction", "author": ["Dantzig", "Thapa1997] George B. Dantzig", "Mukund N. Thapa"], "venue": null, "citeRegEx": "Dantzig et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Dantzig et al\\.", "year": 1997}, {"title": "Modelling, visualising and summarising documents with a single convolutional neural network. arXiv preprint arXiv:1406.3830", "author": ["Denil et al.2014] Misha Denil", "Alban Demiraj", "Nal Kalchbrenner", "Phil Blunsom", "Nando de Freitas"], "venue": null, "citeRegEx": "Denil et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2014}, {"title": "Lexrank: Graph-based lexical centrality as salience in text summarization", "author": ["Erkan", "Radev2004] G\u00fcnes Erkan", "Dragomir R. Radev"], "venue": "J. Artif. Int. Res.,", "citeRegEx": "Erkan et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Erkan et al\\.", "year": 2004}, {"title": "A formal model for information selection in multi-sentence text extraction", "author": ["Filatova", "Vasileios Hatzivassiloglou"], "venue": "In COLING", "citeRegEx": "Filatova et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Filatova et al\\.", "year": 2004}, {"title": "Sentence fusion via dependency graph compression", "author": ["Filippova", "Strube2008] Katja Filippova", "Michael Strube"], "venue": "In EMNLP,", "citeRegEx": "Filippova et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Filippova et al\\.", "year": 2008}, {"title": "Multi-sentence compression: Finding shortest paths in word graphs", "author": ["Katja Filippova"], "venue": "In COLING,", "citeRegEx": "Filippova.,? \\Q2010\\E", "shortCiteRegEx": "Filippova.", "year": 2010}, {"title": "Opinosis: A graphbased approach to abstractive summarization of highly redundant opinions", "author": ["ChengXiang Zhai", "Jiawei Han"], "venue": "In COLING,", "citeRegEx": "Ganesan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ganesan et al\\.", "year": 2010}, {"title": "Framework for abstractive summarization using text-to-text generation", "author": ["Genest", "Guy Lapalme"], "venue": "In MTTG,", "citeRegEx": "Genest et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Genest et al\\.", "year": 2011}, {"title": "Fully abstractive approach to guided summarization", "author": ["Genest", "Guy Lapalme"], "venue": "In ACL,", "citeRegEx": "Genest et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Genest et al\\.", "year": 2012}, {"title": "A scalable global model for summarization", "author": ["Gillick", "Favre2009] Dan Gillick", "Benoit Favre"], "venue": "In Workshop on ILP for NLP,", "citeRegEx": "Gillick et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gillick et al\\.", "year": 2009}, {"title": "The icsi summarization system at tac", "author": ["Gillick et al.2007] Dan Gillick", "Benoit Favre", "Dilek Hakkani-t\u00fcr"], "venue": "In Proc. of Text Understanding Conference", "citeRegEx": "Gillick et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gillick et al\\.", "year": 2007}, {"title": "Multi-document summarization by sentence extraction", "author": ["Vibhu Mittal", "Jaime Carbonell", "Mark Kantrowitz"], "venue": "In NAACL-ANLP-AutoSum,", "citeRegEx": "Goldstein et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Goldstein et al\\.", "year": 2000}, {"title": "Modeling sentences in the latent space", "author": ["Guo", "Diab2012] Weiwei Guo", "Mona Diab"], "venue": "In ACL,", "citeRegEx": "Guo et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2012}, {"title": "Using topic themes for multi-document summarization", "author": ["Harabagiu", "Lacatusu2010] Sanda Harabagiu", "Finley Lacatusu"], "venue": "ACM Trans. Inf. Syst.,", "citeRegEx": "Harabagiu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Harabagiu et al\\.", "year": 2010}, {"title": "Automation of summary evaluation by the pyramid method", "author": ["Harnly et al.2005] Aaron Harnly", "Ani Nenkova", "Rebecca Passonneau", "Owen Rambow"], "venue": "RANLP", "citeRegEx": "Harnly et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Harnly et al\\.", "year": 2005}, {"title": "Cut and paste based text summarization", "author": ["Jing", "McKeown2000] Hongyan Jing", "Kathleen R. McKeown"], "venue": "In NAACL,", "citeRegEx": "Jing et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Jing et al\\.", "year": 2000}, {"title": "Extractive summarization using continuous vector space models", "author": ["Olof Mogren", "Nina Tahmasebi", "Devdatt Dubhashi"], "venue": "In CVSC@EACL,", "citeRegEx": "K\u00e5geb\u00e4ck et al\\.,? \\Q2014\\E", "shortCiteRegEx": "K\u00e5geb\u00e4ck et al\\.", "year": 2014}, {"title": "Accurate unlexicalized parsing", "author": ["Klein", "Manning2003] Dan Klein", "Christopher D. Manning"], "venue": "In ACL,", "citeRegEx": "Klein et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2003}, {"title": "Statistics-based summarization - step one: Sentence compression", "author": ["Knight", "Marcu2000] Kevin Knight", "Daniel Marcu"], "venue": "In AAAI-IAAI,", "citeRegEx": "Knight et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Knight et al\\.", "year": 2000}, {"title": "Deterministic coreference resolution based on entity-centric", "author": ["Lee et al.2013] Heeyoung Lee", "Angel Chang", "Yves Peirsman", "Nathanael Chambers", "Mihai Surdeanu", "Dan Jurafsky"], "venue": "precision-ranked rules. Comput. Linguist.,", "citeRegEx": "Lee et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2013}, {"title": "Pkutm participation in tac2011", "author": ["Li et al.2011] Huiying Li", "Yue Hu", "Zeyuan Li", "Xiaojun Wan", "Jianguo Xiao"], "venue": "In Proceedings of TAC", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Reader-aware multi-document summarization via sparse coding", "author": ["Li et al.2015] Piji Li", "Lidong Bing", "Wai Lam", "Hang Li", "Yi Liao"], "venue": "In IJCAI", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Multi-document summarization via budgeted maximization of submodular functions", "author": ["Lin", "Bilmes2010] Hui Lin", "Jeff Bilmes"], "venue": "In HLT,", "citeRegEx": "Lin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2010}, {"title": "Learning mixtures of submodular shells with application to document summarization", "author": ["Lin", "Bilmes2012] Hui Lin", "Jeff Bilmes"], "venue": "In UAI,", "citeRegEx": "Lin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2012}, {"title": "Automatic evaluation of summaries using ngram co-occurrence statistics", "author": ["Lin", "Hovy2003] Chin-Yew Lin", "Eduard Hovy"], "venue": "In NAACL,", "citeRegEx": "Lin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2003}, {"title": "Improving summarization performance by sentence compression: a pilot study", "author": ["Chin-Yew Lin"], "venue": "In Proceedings of the sixth international workshop on Information retrieval with Asian languages-Volume", "citeRegEx": "Lin.,? \\Q2003\\E", "shortCiteRegEx": "Lin.", "year": 2003}, {"title": "Query-oriented multi-document summarization via unsupervised deep learning", "author": ["Liu et al.2012] Yan Liu", "Sheng-hua Zhong", "Wenjie Li"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Summarization with a joint model for sentence extraction and compression", "author": ["Martins", "Smith2009] Andr\u00e9 F.T. Martins", "Noah A. Smith"], "venue": "In Workshop on ILP for NLP,", "citeRegEx": "Martins et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Martins et al\\.", "year": 2009}, {"title": "A study of global inference algorithms in multi-document summarization", "author": ["Ryan McDonald"], "venue": "In ECIR,", "citeRegEx": "McDonald.,? \\Q2007\\E", "shortCiteRegEx": "McDonald.", "year": 2007}, {"title": "Abstractive summarization of spoken and written conversations based on phrasal queries", "author": ["Mehdad et al.2014] Yashar Mehdad", "Giuseppe Carenini", "Raymond T. Ng"], "venue": "In ACL,", "citeRegEx": "Mehdad et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mehdad et al\\.", "year": 2014}, {"title": "Evaluating content selection in summarization: The pyramid method", "author": ["Nenkova", "Passonneau2004] Ani Nenkova", "Rebecca J. Passonneau"], "venue": "In HLT-NAACL,", "citeRegEx": "Nenkova et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Nenkova et al\\.", "year": 2004}, {"title": "Entity-driven rewrite for multi-document summarization", "author": ["Ani Nenkova"], "venue": "In Third International Joint Conference on Natural Language Processing,", "citeRegEx": "Nenkova.,? \\Q2008\\E", "shortCiteRegEx": "Nenkova.", "year": 2008}, {"title": "Swing: Exploiting category-specific information for guided summarization", "author": ["Ng et al.2011] Jun-Ping Ng", "Praveen Bysani", "Ziheng Lin", "Min yen Kan", "Chew lim Tan"], "venue": "Proceedings of TAC", "citeRegEx": "Ng et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2011}, {"title": "Exploiting timelines to enhance multi-document summarization", "author": ["Ng et al.2014] Jun-Ping Ng", "Yan Chen", "Min-Yen Kan", "Zhoujun Li"], "venue": "In ACL,", "citeRegEx": "Ng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2014}, {"title": "Automated pyramid scoring of summaries using distributional semantics", "author": ["Emily Chen", "Weiwei Guo", "Dolores Perin"], "venue": "ACL", "citeRegEx": "Passonneau et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Passonneau et al\\.", "year": 2013}, {"title": "Modelling events through memory-based, open-ie patterns for abstractive summarization", "author": ["Marco Cornolti", "Enrique Alfonseca", "Katja Filippova"], "venue": "In ACL,", "citeRegEx": "Pighin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pighin et al\\.", "year": 2014}, {"title": "Information status distinctions and referring expressions: An empirical study of references to people in news summaries", "author": ["Ani Nenkova", "Kathleen McKeown"], "venue": null, "citeRegEx": "Siddharthan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Siddharthan et al\\.", "year": 2011}, {"title": "Large-margin learning of submodular summarization models", "author": ["Sipos et al.2012] Ruben Sipos", "Pannaga Shivaswamy", "Thorsten Joachims"], "venue": "In EACL,", "citeRegEx": "Sipos et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sipos et al\\.", "year": 2012}, {"title": "Ctsum: Extracting more certain summaries for news articles", "author": ["Wan", "Zhang2014] Xiaojun Wan", "Jianmin Zhang"], "venue": "In SIGIR,", "citeRegEx": "Wan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2014}, {"title": "Manifold-ranking based topicfocused multi-document summarization", "author": ["Wan et al.2007] Xiaojun Wan", "Jianwu Yang", "Jianguo Xiao"], "venue": "In IJCAI,", "citeRegEx": "Wan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2007}, {"title": "Automatic generation of story highlights", "author": ["Woodsend", "Lapata2010] Kristian Woodsend", "Mirella Lapata"], "venue": "In ACL,", "citeRegEx": "Woodsend et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Woodsend et al\\.", "year": 2010}, {"title": "Multiple aspect summarization using integer linear programming", "author": ["Woodsend", "Lapata2012] Kristian Woodsend", "Mirella Lapata"], "venue": "In EMNLPCoNLL,", "citeRegEx": "Woodsend et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Woodsend et al\\.", "year": 2012}, {"title": "Evolutionary timeline summarization: A balanced optimization framework via iterative substitution", "author": ["Yan et al.2011] Rui Yan", "Xiaojun Wan", "Jahna Otterbacher", "Liang Kong", "Xiaoming Li", "Yan Zhang"], "venue": "In SIGIR,", "citeRegEx": "Yan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2011}, {"title": "Multidocument summarization by maximizing informative content-words", "author": ["Yih et al.2007] Wen-tau Yih", "Joshua Goodman", "Lucy Vanderwende", "Hisami Suzuki"], "venue": "In IJCAI,", "citeRegEx": "Yih et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2007}, {"title": "Sentence compression as a component of a multi-document summarization system", "author": ["Zajic et al.2006] David M. Zajic", "Bonnie J. Dorr", "Jimmy Lin", "Richard Schwartz"], "venue": "In DUC at NLT/NAACL", "citeRegEx": "Zajic et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zajic et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 47, "context": "The work was done when Weiwei Guo was in Columbia University summarization systems adopt the extractionbased approach which selects some original sentences from the source documents to create a short summary (Erkan and Radev, 2004; Wan et al., 2007).", "startOffset": 208, "endOffset": 249}, {"referenceID": 33, "context": "To this end, some researchers apply compression on the selected sentences by deleting words or phrases (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2006; Harabagiu and Lacatusu, 2010; Li et al., 2015), which is the compression-based method.", "startOffset": 103, "endOffset": 205}, {"referenceID": 52, "context": "To this end, some researchers apply compression on the selected sentences by deleting words or phrases (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2006; Harabagiu and Lacatusu, 2010; Li et al., 2015), which is the compression-based method.", "startOffset": 103, "endOffset": 205}, {"referenceID": 29, "context": "To this end, some researchers apply compression on the selected sentences by deleting words or phrases (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2006; Harabagiu and Lacatusu, 2010; Li et al., 2015), which is the compression-based method.", "startOffset": 103, "endOffset": 205}, {"referenceID": 13, "context": "One important work developed by Barzilay and McKeown (2005) employed sentence fusion, followed by (Filippova and Strube, 2008; Filippova, 2010).", "startOffset": 98, "endOffset": 143}, {"referenceID": 51, "context": "Different types of salience can be incorporated in our framework, such as position-based method (Yih et al., 2007), statistical feature based method (Woodsend and Lapata, 2012), concept-based method (Li et al.", "startOffset": 96, "endOffset": 114}, {"referenceID": 28, "context": ", 2007), statistical feature based method (Woodsend and Lapata, 2012), concept-based method (Li et al., 2011), etc.", "startOffset": 92, "endOffset": 109}, {"referenceID": 27, "context": "To find coreference NPs (different mentions for the same entity), we first conduct coreference resolution for each document with Stanford coreference resolution package (Lee et al., 2013).", "startOffset": 169, "endOffset": 187}, {"referenceID": 42, "context": "Since manual pyramid evaluation is timeconsuming, and the exact evaluation scores are not reproducible especially when the assessors for our results are different from those of TAC, we employ the automated version of pyramid proposed in (Passonneau et al., 2013).", "startOffset": 237, "endOffset": 262}, {"referenceID": 22, "context": "(2013) showed that the distributional similarity based method produces automated scores that correlate well with manual pyramid scores, yielding more accurate pyramid scores than string matching based automated methods (Harnly et al., 2005).", "startOffset": 219, "endOffset": 240}, {"referenceID": 42, "context": "In this paper, we adopt the same setting as in (Passonneau et al., 2013): a 100 dimension matrix factorization model is learned on a domain independent corpus, which is drawn from sense definitions of WordNet and Wiktionary2, and Brown corpus.", "startOffset": 47, "endOffset": 72}, {"referenceID": 41, "context": "Since manual pyramid evaluation is timeconsuming, and the exact evaluation scores are not reproducible especially when the assessors for our results are different from those of TAC, we employ the automated version of pyramid proposed in (Passonneau et al., 2013). The automated pyramid scoring procedure relies on distributional semantics to assign SCUs to a target summary. Specifically, all n-grams within sentence bounds are extracted, and converted into 100 dimension latent topical vectors via a weighted matrix factorization model (Guo and Diab, 2012). Similarly, the contributors and the label of an SCU are transformed into 100 dimensional vector representations. An SCU is assigned to a summary if there exists an n-gram such that the similarity score between the SCU low dimensional vector and the n-gram low dimensional vector exceeds a threshold. Passonneau et al. (2013) showed that the distributional similarity based method produces automated scores that correlate well with manual pyramid scores, yielding more accurate pyramid scores than string matching based automated methods (Harnly et al.", "startOffset": 238, "endOffset": 884}, {"referenceID": 42, "context": "65, similar to those used in (Passonneau et al., 2013).", "startOffset": 29, "endOffset": 54}, {"referenceID": 28, "context": "The top three systems in TAC 2011 evaluated with manual pyramid score were System 22 (Li et al., 2011), 43, and 17 (Ng et al.", "startOffset": 85, "endOffset": 102}, {"referenceID": 40, "context": ", 2011), 43, and 17 (Ng et al., 2011).", "startOffset": 20, "endOffset": 37}, {"referenceID": 19, "context": "Early studies mainly followed a greedy strategy in sentence selection (\u00c7elikyilmaz and Hakkani-T\u00fcr, 2011; Goldstein et al., 2000; Wan et al., 2007).", "startOffset": 70, "endOffset": 147}, {"referenceID": 47, "context": "Early studies mainly followed a greedy strategy in sentence selection (\u00c7elikyilmaz and Hakkani-T\u00fcr, 2011; Goldstein et al., 2000; Wan et al., 2007).", "startOffset": 70, "endOffset": 147}, {"referenceID": 36, "context": "Later, unified models are proposed to conduct sentence selection and redundancy control simultaneously (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2007; Lin and Bilmes, 2010; Lin and Bilmes, 2012; Sipos et al., 2012).", "startOffset": 103, "endOffset": 260}, {"referenceID": 51, "context": "Later, unified models are proposed to conduct sentence selection and redundancy control simultaneously (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2007; Lin and Bilmes, 2010; Lin and Bilmes, 2012; Sipos et al., 2012).", "startOffset": 103, "endOffset": 260}, {"referenceID": 18, "context": "Later, unified models are proposed to conduct sentence selection and redundancy control simultaneously (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2007; Lin and Bilmes, 2010; Lin and Bilmes, 2012; Sipos et al., 2012).", "startOffset": 103, "endOffset": 260}, {"referenceID": 45, "context": "Later, unified models are proposed to conduct sentence selection and redundancy control simultaneously (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2007; Lin and Bilmes, 2010; Lin and Bilmes, 2012; Sipos et al., 2012).", "startOffset": 103, "endOffset": 260}, {"referenceID": 33, "context": "As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009).", "startOffset": 93, "endOffset": 149}, {"referenceID": 52, "context": "As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009).", "startOffset": 93, "endOffset": 149}, {"referenceID": 3, "context": "Recently, integrated models have been proposed that jointly conduct sentence extraction and compression (Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011; Li et al., 2015).", "startOffset": 104, "endOffset": 231}, {"referenceID": 29, "context": "Recently, integrated models have been proposed that jointly conduct sentence extraction and compression (Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011; Li et al., 2015).", "startOffset": 104, "endOffset": 231}, {"referenceID": 39, "context": "Summary revision was also investigated to improve the quality of automatic summary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011).", "startOffset": 151, "endOffset": 192}, {"referenceID": 44, "context": "Summary revision was also investigated to improve the quality of automatic summary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011).", "startOffset": 151, "endOffset": 192}, {"referenceID": 14, "context": "Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014).", "startOffset": 105, "endOffset": 148}, {"referenceID": 37, "context": "Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014).", "startOffset": 105, "endOffset": 148}, {"referenceID": 41, "context": "Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011).", "startOffset": 86, "endOffset": 142}, {"referenceID": 50, "context": "Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011).", "startOffset": 86, "endOffset": 142}, {"referenceID": 2, "context": "Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a large set of documents (Christensen et al.", "startOffset": 188, "endOffset": 209}, {"referenceID": 7, "context": ", 2014), and hierarchical summarization that scales up MDS to summarize a large set of documents (Christensen et al., 2014).", "startOffset": 97, "endOffset": 123}, {"referenceID": 43, "context": "A data-driven method for mining sentence structures on large news archive was proposed and utilized to summarize unseen news events (Pighin et al., 2014).", "startOffset": 132, "endOffset": 153}, {"referenceID": 34, "context": "Moreover, some works (Liu et al., 2012; K\u00e5geb\u00e4ck et al., 2014; Denil et al., 2014; Cao et al., 2015) utilized deep learning techniques to tackle some summarization tasks.", "startOffset": 21, "endOffset": 100}, {"referenceID": 24, "context": "Moreover, some works (Liu et al., 2012; K\u00e5geb\u00e4ck et al., 2014; Denil et al., 2014; Cao et al., 2015) utilized deep learning techniques to tackle some summarization tasks.", "startOffset": 21, "endOffset": 100}, {"referenceID": 9, "context": "Moreover, some works (Liu et al., 2012; K\u00e5geb\u00e4ck et al., 2014; Denil et al., 2014; Cao et al., 2015) utilized deep learning techniques to tackle some summarization tasks.", "startOffset": 21, "endOffset": 100}, {"referenceID": 4, "context": "Moreover, some works (Liu et al., 2012; K\u00e5geb\u00e4ck et al., 2014; Denil et al., 2014; Cao et al., 2015) utilized deep learning techniques to tackle some summarization tasks.", "startOffset": 21, "endOffset": 100}], "year": 2015, "abstractText": "ive Multi-Document Summarization via Phrase Selection and Merging\u2217 Lidong Bing\u00a7 Piji Li Yi Liao Wai Lam Weiwei Guo\u2020 Rebecca J. Passonneau\u2021 \u00a7Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA USA Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong \u2020Yahoo Labs, Sunnyvale, CA, USA \u2021Center for Computational Learning Systems, Columbia University, New York, NY, USA \u00a7lbing@cs.cmu.edu, {pjli, yliao, wlam}@se.cuhk.edu.hk \u2020wguo@yahoo-inc.com, \u2021becky@ccls.columbia.edu", "creator": "LaTeX with hyperref package"}}}