{"id": "1506.01744", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2015", "title": "Spectral Learning of Large Structured HMMs for Comparative Epigenomics", "abstract": "We develop a latent variable model and an efficient spectral algorithm motivated by the recent emergence of very large data sets of chromatin marks from multiple human cell types. A natural model for chromatin data in one cell type is a Hidden Markov Model (HMM); we model the relationship between multiple cell types by connecting their hidden states by a fixed tree of known structure. The main challenge with learning parameters of such models is that iterative methods such as EM are very slow, while naive spectral methods result in time and space complexity exponential in the number of cell types. We exploit properties of the tree structure of the hidden states to provide spectral algorithms that are more computationally efficient for current biological datasets. We provide sample complexity bounds for our algorithm and evaluate it experimentally on biological data from nine human cell types. Finally, we show that beyond our specific model, some of our algorithmic ideas can be applied to other graphical models.", "histories": [["v1", "Thu, 4 Jun 2015 22:57:28 GMT  (76kb,D)", "http://arxiv.org/abs/1506.01744v1", "27 pages, 3 figures"]], "COMMENTS": "27 pages, 3 figures", "reviews": [], "SUBJECTS": "stat.ML cs.LG math.ST q-bio.GN stat.TH", "authors": ["chicheng zhang", "jimin song", "kamalika chaudhuri", "kevin c chen"], "accepted": true, "id": "1506.01744"}, "pdf": {"name": "1506.01744.pdf", "metadata": {"source": "CRF", "title": "Spectral Learning of Large Structured HMMs for Comparative Epigenomics", "authors": ["Chicheng Zhang", "Jimin Song", "Kevin C Chen"], "emails": ["chz038@eng.ucsd.edu", "song@dls.rutgers.edu", "kcchen@dls.rutgers.edu", "kamalika@eng.ucsd.edu"], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "1.1 Related Work", "text": "The first efficient spectral algorithm for learning HMM parameters was due to [18]. There has been an explosion of follow-up work on spectral algorithms for learning the parameters and structure of latent variable models [23, 6, 4]. [18] provides a spectral algorithm for learning an observable representation of an HMM's operations under certain precedence conditions. [23] and [3] extend this algorithm to the case where the transition matrix or observation matrix has a lack of precedence. [19] extends [18] to hidden Semi-Markov models. [2] provides a general spectral algorithm for learning latent variable models with a multi-view structure - there is a hidden node and three or more observable nodes that are not connected to other nodes and are conditioned independently of the hidden nodes."}, {"heading": "2 The Model", "text": "The natural probability for a single epigenomic sequence is then a hidden Markov model (HMM) in which the time corresponds to the position in the sequence. Observation at position t is the sequence value at position t, and the hidden state at position t is the regulatory function at that position. In comparative epigenomics, the goal is to observe all epigenomic sequences of multiple species or cell types together (G, T, W); Figure 1 shows an illustration. (V, E) is a straight tree with known structure, whose nodes represent individual cell types or species."}, {"heading": "3 Algorithm", "text": "In fact, it is such that it is a matter of a way in which people in a country in which they are able to live and live in a world, in which they are able to move, in which they are able to put themselves in a world, in which they are able to put themselves in a world, in which they are able to put themselves in a world, in which they are able to put themselves in a world, in which they are able to put themselves in a world, in which they are able to put themselves in a world, in which they are able to put themselves in a world, in which they live themselves and in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "3.1 The Full Algorithm", "text": "Algorithm 1 shows how to restore the observation matrices Ou at each node u. Once the Ous are restored, standard techniques can be used to restore T and W; details are described in Algorithm 2 in the appendix."}, {"heading": "3.2 Product Projections beyond HMMs with Tree-structured Hidden States", "text": "(U) > U (U) > U (U) > U (U) > U (U) > U (U), U (U), U (U), U (U), U (U), U (U), U (U), U (U), U (U), U (U), U (U), U (U), U (U), U (U), U (U), U (U), U (U), U (U), U (U), U (U), U (U), U (U), U (U), U (U), U (U), U (U), U (U), U (U), U (U), U (U), U (U), U (U), U (U), U (U), U (U), U (U)."}, {"heading": "3.3 Performance Guarantees", "text": "We offer performance guarantees for our algorithm. Since learning parameters of HMMs and many other graphical models are actually NP-hard, spectral algorithms simplify assumptions about the properties of the model that generates the data. Typically, these assumptions take the form of some conditions at the rank of certain parameter matrices. We specify under the conditions required for our algorithm to successfully learn parameters of a HMM with tree-structured hidden states. Note that we need two types of ranking conditions - node-wise and path-wise - to ensure that we can restore the full set of parameters on a root node-to-node path. Assumption 1 (node-wise ranking condition): The matrix Ou is rank m, and the common probability matrix Pu, u2.1 is rank m.Assumption 2 (path-wise ranking condition).Assumption 2 The mutrix Ou is high."}, {"heading": "4 Experiments", "text": "In fact, we are in a position to take the lead, \"he said in an interview with the German Press Agency.\" We have it in our hands, \"he said,\" but we have it in our hands. \""}, {"heading": "5 Acknowledgements", "text": "We thank NSF under IIS-1162581 for their support. Part of this work was done during Chaudhuri's visit to the Simons Foundation Spectral Learning Program in Berkeley."}, {"heading": "A Recovering the Transition Probabilities and Initial Probabilities", "text": "Theorem 2 provides finite example guarantees for algorithm 1 in conjunction with algorithm 2.Algorithm 2 Recovering the Transition Probabilities and Initial Probabilities1: Input: N samples of the first three observations (x1, x2, x3) N i = 1 generated by a tree HMM, Estimates of Observation Matrices O-u. 2: for u-V do 3: if u is root r then 4: Compute W-r = (O-u) \u2020 P-r1. 5: Compute Q-r = (O-r) \u2020 P-r, r2,1 (O-r) \u2020 >. 6: Normalization via zu2 coordinates to obtain T-u. 7: otherwise 8: Compute W-u = (O-u) \u2020 Pu (u) 1,1 (O-u) \u2020.9: Compute via zu2 coordinates (T-u) (2.0-u)."}, {"heading": "B Additional Notations", "text": "For a node u V, when it is clear out of context, we sometimes use H = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1: 1 = 1: 1: 1 = 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = = 1 = 1 = 1 = 1 = = 1 = 1 = 1 = = 1 = 1 = 1 = 1 = = 1 = 1 = 1 = 1 = 1 = 1 = 1 = = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = = 1 = 1 = 1 = 1 = = 1 = 1 = 1 = = 1 = 1 = 1 = = 1 = 1 = 1 = 1 = = 1 = 1 = 1 = 1 = 1 = 1 = = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 ="}, {"heading": "C Main Lemmas", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C.1 Partitioning Lemmas", "text": "It is not as if there is a question of distribution, but a question of distribution. (...) It is not as if there is a question of distribution. (...) It is not as if there is a question of distribution. (...) It is as if there is a question of distribution. (...) It is as if there is a question of distribution. (...) It is as if there is a question of distribution. (...) It is as if there is a question of distribution. (...) It is as if there is a question of distribution. (...) It is as if there is a question of distribution. (...) It is as if there is a question of distribution. (...) It is as if there is a question of distribution. (...) It is as if there is a question of distribution."}, {"heading": "C.2 Skeletensor Lemmas", "text": "In this subsection we justify our construction of a skeletal tensor. Let u be any node in the tree G = > Let H be the path from the root of G to u.Recall that we define OH1 to be the n d \u00b7 md matrix, whose entries are (OH1) (i1,..., id), (j1,..., jd) = P (xr1 = i1,. \u2212 P 1 = id | zr2 = j1,., zu2 = jd). Similarly, OH3 is a nd \u00b7 md matrix, with entries (OH3) (i1,..., id), (j1, jd) = P (r 3 = i1,.,., x 3 = id zr2 = jd,."}, {"heading": "C.3 Product Projections Lemmas", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C.3.1 Product Projections in HMM with Tree Hidden States", "text": "Lemma 4. OH, the observation matrix of the HMM, which generates the meta-states and meta-observations {zHt, xHt} t-N, corresponds v-H O v-Proof. we consider the observation matrix of the HMM, which generates the meta-states and meta-observations {zHt, xHt} t-N. The number of possible meta-hidden states zHt is md, indexed by (zvt) v-H and the number of possible meta-observations x H is nd, indexed by (xvt) v-H. Thus, the observation matrix OH is of dimension nd \u00d7 md. Entrywise, (OHu) (i1,..., id), (j1,..., jd) = P (xrt = i1,.., xut = id | zrt = j1,."}, {"heading": "C.3.2 Product Projections Beyond HMM with Tree Hidden States", "text": "We consider the case of a simple HMM when the observation matrix O = = complete rank. In this case, we first define the observation matrices forward and backward (j1,.., js), formally. For a fixed s, O-f s is a ns \u00b7 m matrix, with rows indexed by an s tuple (j1,.., js), and columns indexed by i-m. Similarly, we define the observation matrix O-bs = P (i1,..., ist), j = P (i1,..., ist), j-s (xt = i1,."}, {"heading": "D Finite Sample Guarantees", "text": "Theorem 2 (Accuracy of initial distribution and transition probabilities): There is a universal constant C, so the following values apply: Suppose algorithm 1 is given as an input value for each node u in the tree (xi1, xi2, xi3) Ni = 1 generated by a THS-HMM and gives estimates of the observation matrices O-u, for each node u in the tree. Then algorithm 2 is executed on the same sample and has {O-u-V as input value. If the size of the sample N is greater than: C max (D2\u03c322\u04452 3ln D), m\u044121\u04452 2ln D\u043c, m2\u044561\u04456 3\u04453 minln D\u043c, m\u04458 12 ln D\u043c, m2\u044563\u044563\u043c 14 1 \u04324 min2 ln D\u043c), whereby the probability of observation is always the same."}, {"heading": "E Proofs", "text": "During this section, we first assume a technical state of the sample size, which will result in a concentration of the projection and symmetry matrices. Assumption 3. Remember that the sample size N is large enough that (N, \u03b4) \u2264 min (Pu, u1,2) minu, V, md (PH, H1,3) 16D, minu, V, M (P u, u 1,2) minu, V, m (O u) 4, minu, V, md (PH, H 1,3) 3 minu, V, m (O u) 3\u03c0 3 / 2 min1536c1m) = min (\u03c32\u03c33 16D, \u03c32\u03c31 4 \u221a m, \u03c0 3 / 2 min\u03c3 3 31536c1m) (2), c1 > 0 being a constant given in Lemma 11 and defined in Theorem 2, \u03c32, 3 and \u03c0min."}, {"heading": "E.1 Raw Moments Concentration", "text": "s define a node in V, remember that H is the set of nodes along the path from root r to u.Let (N, \u03b4) = \u221a 1 + ln (10D / \u03b4) n. Define eventE = {for all u-V: \u2022 P-u, u1,2 \u2212 P u, u 1,2 \u2212 F-H, H-P-H, u1,2 \u2212 P-H, u-P-H, u-P-P-H-H, u-P-H, u-P-H, u-P-T-P-P-H, H1,3 \u2212 P-H, H-P-H-2,u-H, u, H1,2,3 \u2212 P-P-H, u-P-P-H, u-P-H-3 \u2212 P-H, u-P-2,u-P \u2212 P."}, {"heading": "E.2 Subspace Concentration", "text": "Next, we cite a useful problem, which is that due to event E, the execution of an SVD on the empirical version of Pu, u1,2 = E [xu1 xu2] gives us a good approximation of the area of Ou. Remember that Uu is a matrix whose columns form an orthonormal basis of Ou, and define UH as v HUu. Remember also a matrix U with orthonormal columns, the projection matrix on the area (U) is UU >.Lemma 7 (subspace concentration). Suppose that N is large enough to hold assumption 3."}, {"heading": "In particular,", "text": "u > u > u > u > u (PH, H 1,3) 8 (3) m (U, U) 0 (U, U) 0 (U, U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (U) 0 (S) (S) (S) (0) (S) (S) (0) (S) (S) (0) (S) (S) (S) (0) (S) (S) (0) (S) (S) (0) (S) (0) (S) (S) (0) (S) (S) (0) (S) (0) (S) (S) (0) (S) (S) (S) (0 (S) (S) (0) (S) (S) (0 (S) (S) (S) (S) (0 (S) (S) (0 (S) (S) (S) (0 (S) (S) (S (S) (0 (S) (S) (S) (0 (S) (S) (0 (S) (S) (S) (0 (S) (S) (S) (0 (S) (S) (S) (0 (S) (S) (0 (S) (S) (0) (S) (S) (S) (0 (S) (0) (S) (S) (0) (S) (S) (0) (S) 0 (S) (S) (0) (S) (0 (S) (S) (S) (0 ("}, {"heading": "E.3 Symmetrized Moment Concentration", "text": "If we look at a series of matrices given to us (= > SO = > SO (SO = > SO = > SO (SO = > SO = > SO = > SO = > SO (SO = > SO = > SO) > SO (SO = > SO), SO (SO), SO (SO), SO (SO), SO (SO), SO (SO), SO (SO), SO (SO), SO (SO), SO (SO), SO (SO), SO (SO), SO (SO), SO (SO), SO (SO), SO (SO), SO (SO), SO (SO), SO (SO), SO (SO), SO (SO), SO (SO), SO (SO), SO (SO), SO (SO), SO (SO), SO (SO), SO (SO), SO (SO), SO (SO), SO (SO), SO (SO), SO (SO), SO (SO), SO (SO, SO (SO), SO (SO), SO (SO), SO (SO), SO (SO (SO), SO (SO), SO (SO), SO (SO (SO), SO (SO), SO (SO), SO (SO), SO (SO (SO), SO (SO), SO (SO (SO), SO), SO (SO (SO), SO (SO), SO (SO), SO (SO), SO (SO (SO), SO (SO), SO (SO (SO), SO (SO), SO (SO), SO (SO), SO (S"}, {"heading": "E.4 Accucary of Tensor Decomposition", "text": "...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...). (...).). (...). (...).).). (...). (...).). (...).). (...).). (...).). (...).). (...). (...).). (...).). (...).). (...). (...).). (...). (...).). (...).). (...). (...). (...).). (...).). (...).). (...). (...).). (...). (...). (...).). (...). (...).).). (...). (...). (...).). (...). (...). (...).).). (...).). (...). (...)"}, {"heading": "F Putting Everything Together \u2013 Proof of Theorem 2", "text": "The last step of the recovery is O-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u"}, {"heading": "G Matrix Perturbation Lemmas", "text": "Theorem 3 (Weyls Theorem): If A, E matrices in Rm \u00b7 n with m \u2265 n. Then | \u03c3i (A + E) \u2212 \u03c3i (A) | \u2264 \u0442 E-theorem 4 (Wedins Theorem). If A, E matrices in Rm \u00b7 n with m \u2265 n. Let A have a singular value drop: U > 1U > 2U > 3A (V1 V2) = (1 00 \u041a2 0 0)"}, {"heading": "Let A\u0303 = A+ E have the singular value decomposition: U\u0303>1U\u0303>2", "text": "U-3 A-2 (V-1 V-2) = N-1 00-2 0 0 If there is N-0, A-0 so that Mini-1 (A-1), A-2 (A-2), A-2 (A-2), A-2 and E-3 is the matrix of the main angle between area (U-1) and area (U-1). Theorem 5. If A, E are matrices in Rm-n with m-n, leave A-2 = A + E. Then, A-A-A-2 max (A-2, A-2), E-2."}, {"heading": "H Compressed observation matrices produced by Spectral-Tree for eight", "text": "ENCODE cell types"}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "We develop a latent variable model and an efficient spectral algorithm motivated<lb>by the recent emergence of very large data sets of chromatin marks from multiple<lb>human cell types. A natural model for chromatin data in one cell type is a Hidden<lb>Markov Model (HMM); we model the relationship between multiple cell types by<lb>connecting their hidden states by a fixed tree of known structure.<lb>The main challenge with learning parameters of such models is that iterative meth-<lb>ods such as EM are very slow, while naive spectral methods result in time and<lb>space complexity exponential in the number of cell types. We exploit properties<lb>of the tree structure of the hidden states to provide spectral algorithms that are<lb>more computationally efficient for current biological datasets. We provide sample<lb>complexity bounds for our algorithm and evaluate it experimentally on biological<lb>data from nine human cell types. Finally, we show that beyond our specific model,<lb>some of our algorithmic ideas can be applied to other graphical models.", "creator": "LaTeX with hyperref package"}}}