{"id": "1506.01900", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2015", "title": "Communication Complexity of Distributed Convex Learning and Optimization", "abstract": "We study the fundamental limits to communication-efficient distributed methods for convex learning and optimization, under different assumptions on the information available to individual machines, and the types of functions considered. We identify cases where existing algorithms are already worst-case optimal, as well as cases where room for further improvement is still possible. Among other things, our results indicate that without similarity between the local objective functions (due to statistical data similarity or otherwise) many communication rounds may be required, even if the machines have unbounded computational power.", "histories": [["v1", "Fri, 5 Jun 2015 13:24:17 GMT  (29kb)", "http://arxiv.org/abs/1506.01900v1", null], ["v2", "Wed, 28 Oct 2015 19:02:22 GMT  (33kb)", "http://arxiv.org/abs/1506.01900v2", null]], "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["yossi arjevani", "ohad shamir"], "accepted": true, "id": "1506.01900"}, "pdf": {"name": "1506.01900.pdf", "metadata": {"source": "CRF", "title": "Communication Complexity of Distributed Convex Learning and Optimization", "authors": ["Yossi Arjevani"], "emails": ["yossi.arjevani@weizmann.ac.il", "ohad.shamir@weizmann.ac.il"], "sections": [{"heading": null, "text": "ar Xiv: 150 6.01 900v 1 [cs.L G] 5J un"}, {"heading": "1 Introduction", "text": "We look at the problem of distributed convex learning and optimization, where a number of M machines, each with access to a different local convex function (Rd 7 \u2192 R and a convex domain W-Rd, attempt to solve the optimization problem in some areas where each machine has access to a different subset of data. (1) A prominent application is empirical risk mitigation, where the goal is to minimize the average loss over some data, where each machine has access to a different subset of data. Renting {z1,., zm} is the dataset consisting of N examples, and which presuppose the loss of function (w, z), then the empirical risk mization problem is minw-W 1N = 1 (w, zi) can be written as in Eq."}, {"heading": "2 Notation and Framework", "text": "The only vector and matrix used in this paper are the Euclidean norms and the spectral norms (G = 2)."}, {"heading": "3 Lower Bounds Using a Structural Assumption", "text": "In this section, we will present a lesser number of points, each based on the number of communication rounds in which we have imposed a certain mild structural assumption on the operations performed by the algorithms. (This means that we are referring to a very large class of algorithms based on linear operations, which are initially based on techniques, gradients and vector products with local Hessians and their inverses, as well as on solving local optimization problems involving such quantities.) Formally, we will consider algorithms that meet the assumptions given below. For convenience, we will specify them for smooth functions (which are differentiable) and discuss the case of non-frictionless functions in Sec. 3.2.Assumption 1. For each machine, we will define a series of Wj-Rd = {0}. Between communication rounds, each machine j is calculated and added iteratively."}, {"heading": "3.1 Smooth Local Functions", "text": "We start by specifying a lower limit if the local functions Fi are strongly convex and smooth: Theorem 1. For each even number of machines, each distributed algorithm that meets the assumption 1, and for each one of these functions there are m local square functions via Rd (where d is sufficiently large), which is 1-smooth, strongly convex, and in relation to each one of these functions, so that if w \u00b2 s = argminw, the number of communication rounds required to obtain w \u00b2 s satisfactory F (w \u00b2) -F (w \u00b2) information (for any other type of communication) is the same number of least14 (1) \u2212 1) Log (w \u00b2 s 2 = 4) the number of communication rounds required to obtain w \u00b2 satisfactory F (w \u00b2) -F (w \u00b2) -E (for any other type of communication)."}, {"heading": "3.2 Non-smooth Local Functions", "text": "It's not like we don't do it. (...) It's not like we do it. (...) It's not like we do it. (...) It's not like we do it. (...) It's like we do it. (...) It's like we do it. (...) It's like we do it. (...) It's like we do it. (...) It's like we do it. (...) It's like we do it. (...) It's like we do it. (...) It's like we do it. (...) It's like we do it. (...) It's like we do it. (...) It's like we do it. (...) It's like we do it. (...) It's like we do it. (...) It's like we do it. (...) It's like we do it. (...) It's like we do it. (...) It's like we do it. (...) It's like we do it. (... It's like we do it. (...) It's like we do it. (...) It's like we do it. (... It's like we do it. (...) It's like we do it. (... it. (...) It's like we do it. (...) It's like we do it. (... it. (...) It's like we do it."}, {"heading": "4 One Round of Communication", "text": "This is a more difficult setting, and the result we present will then be limited to algorithms that use a single round of communication. We note that this still captures a realistic non-interactive column in which each machine optimizes its own distributed data, and the resulting points are averaged (e.g. [25, 23]).Intuitively, with only a single round of communication, we can get an arbitrarily small error that can be unfeasible.The following theorem establishes a lower limit at the Atlantic level, depending on the strong exact parameters and similarity between the local functions, and compares this to a \"trial zero communication,\" which can become a function of the Atlantic one."}, {"heading": "5 Summary and Open Questions", "text": "Our results suggest that when the local functions are unrelated, regardless of the computing power of the local machines, many rounds of communication may be necessary (a polynomic scale of 1 / 2 or 1 / 3), and that the worst possible optimal algorithm (at least for smooth functions) is only a simply distributed implementation of an accelerated gradient descent. If the functions are linked, we also show that the optimal performance is achieved by the algorithm of [24] for square and strongly convex functions, but the development of optimal algorithms remains open to more general functions. In addition to these results, which require a certain mild structural assumption for the algorithm used, we have also provided an assumption-free lower limit for one-sided and strongly convex algorithms, which implies that even for strongly convex square functions such algorithms sometimes offer only trite optimization possibilities. In addition to several interesting questions of multiplication practice, the less interesting for the remaining two-ways of communicating are the algorithms."}, {"heading": "Acknowledgments", "text": "This research is supported by the Intel ICRI-CI Institute, the Israel Science Foundation grant 425 / 13 and a Marie Curie CIG grant under the Seventh Framework Programme."}, {"heading": "A Proofs", "text": "The proof of the theorem is based on the division of the machines into two subgroups of the same size, each of which is associated with a finite dimensional constraint of F1 and F2 (see Eq. (3)) and the tracking of the maximum number of non-zero coordinates for vectors in Wj, the set of possible points. Remember that Fi is defined as follows: F1 (w) = number of non-zero coordinates for vectors in Wj, the set of realizable points (w) 4 w (w)."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>We study the fundamental limits to communication-efficient distributed methods for convex learning<lb>and optimization, under different assumptions on the information available to individual machines, and<lb>the types of functions considered. We identify cases where existing algorithms are already worst-case<lb>optimal, as well as cases where room for further improvement is still possible. Among other things, our<lb>results indicate that without similarity between the local objective functions (due to statistical data simi-<lb>larity or otherwise) many communication rounds may be required, even if the machines have unbounded<lb>computational power.", "creator": "LaTeX with hyperref package"}}}