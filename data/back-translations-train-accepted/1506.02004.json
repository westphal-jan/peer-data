{"id": "1506.02004", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2015", "title": "Sparse Overcomplete Word Vector Representations", "abstract": "Current distributed representations of words show little resemblance to theories of lexical semantics. The former are dense and uninterpretable, the latter largely based on familiar, discrete classes (e.g., supersenses) and relations (e.g., synonymy and hypernymy). We propose methods that transform word vectors into sparse (and optionally binary) vectors. The resulting representations are more similar to the interpretable features typically used in NLP, though they are discovered automatically from raw corpora. Because the vectors are highly sparse, they are computationally easy to work with. Most importantly, we find that they outperform the original vectors on benchmark tasks.", "histories": [["v1", "Fri, 5 Jun 2015 18:20:43 GMT  (3064kb,D)", "http://arxiv.org/abs/1506.02004v1", "Proceedings of ACL 2015"]], "COMMENTS": "Proceedings of ACL 2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["manaal faruqui", "yulia tsvetkov", "dani yogatama", "chris dyer", "noah a smith"], "accepted": true, "id": "1506.02004"}, "pdf": {"name": "1506.02004.pdf", "metadata": {"source": "CRF", "title": "Sparse Overcomplete Word Vector Representations", "authors": ["Manaal Faruqui", "Yulia Tsvetkov", "Dani Yogatama", "Chris Dyer", "Noah A. Smith"], "emails": ["mfaruqui@cs.cmu.edu", "ytsvetko@cs.cmu.edu", "dyogatama@cs.cmu.edu", "cdyer@cs.cmu.edu", "nasmith@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Distributed representations of words have proven useful for NLP tasks such as parsing (Lazaridou et al., 2013; Bansal et al., 2014), designated entity recognition (Guo et al., 2014), and sensory analysis (Socher et al., 2013). The attraction of word vectors is that they can be derived directly from raw, uncommented entities. Intrinsic evaluations of various tasks are guidelines for the discovery of a representation that captures many facts about lexical semantics (Turney, 2001; Turney and Pantel, 2010). However, word vectors do not look like the representations described in most lexical semantic theories, which focus on the identification of word classes (Levin, 1993; Baker et al., 1998; Schuler, 2005) and relationships between word meanings (Miller, 1995). Although expensive to construct, conceptualizing word meaning is important for understanding meaningful word meanings."}, {"heading": "2 Sparse Overcomplete Word Vectors", "text": "We will consider methods for converting dense word vectors into sparse binary supercomplete word vectors. Fig. 1 shows two approaches: The uppermost, Method A, converts dense vectors into sparse supercomplete vectors (\u00a7 2.1). The underlying, Method B, converts dense vectors into sparse and binary supercomplete vectors (\u00a7 2.2 and \u00a7 2.4). Let us take V as the vocabulary quantity. In the following X-RL-V is the matrix constructed by stacking V non-sparse \"input\" word vectors of length L (generated by any word vector estimator). We will refer to these as initializing vectors. A-RK-V contains V-sparse supercomplete word vectors of length K. \"Supercomplete\" representation learning implies that K > L."}, {"heading": "2.1 Sparse Coding", "text": "In a sparse encoding (Lee et al., 2006), the goal is to present each input vector xi as a sparse linear combination of base vectors, ai. Our experiments consider four initialization methods for these vectors, which are discussed in Appendix A. Considering the fact that X, we are trying to solve min D, A, X, DA, 22 + 1, where D, RL, K is the dictionary of base vectors. It is a regularization hyperparameter and vice versa. Here, we use the square loss for the reconstruction error, but other loss functions could also be used (Lee et al., 2009). To obtain sparse word representations, we insert an \"11https: / / github.com / mfaruqui / sparse encoding penalty on A. Equation 1 can be broken down into loss for each word vector that can be optimized in parallel (\u00a7 2.3): This problem, the II, the II, the method, the V, and the V, and the I, and the V."}, {"heading": "2.2 Sparse Nonnegative Vectors", "text": "It has often been shown that non-negativity in the attribute space corresponds to the ability to interpret (Lee and Seung, 1999; Cichocki et al., 2009; Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015). To obtain non-negative sparse word vectors, we use a variation of the non-negative sparse encoding method (Hoyer, 2002). Non-negative sparse encoding further confines the problem in Equation 2, so that D and ai are not negative. At this point, we apply this restriction only to the representation vectors {ai}. Therefore, the new target for non-negative sparse vectors will be: arg min D, RL, K, 0, A, R, K, V, V, i, 1, xi \u2212 Dai, 22 +."}, {"heading": "2.3 Optimization", "text": "To speed up the training, we use asynchronous updates of the parameters of the model in parallel for each word vector (Duchi et al., 2012; Heigold et al., 2014). However, the direct application of such a learning algorithm, the regulated dual average algorithm (Xiao, 2009), does not lead to sparse limited-time solutions that have motivated several specialized algorithms aimed at such goals. We use the AdaGrad variant of such a learning algorithm, the regulated dual average algorithm (Xiao, 2009), which does not track the online average algorithm when it is temporary."}, {"heading": "2.4 Binarizing Transformation", "text": "Our goal with Method B is to obtain word representations that can mimic the binary attribute."}, {"heading": "1975, 1976, 1968, 1970, 1977, 1969", "text": "We can specify this as an optimization problem: arg min D, RL, KB and VV, where B denotes the binary (and also sparse) representation. This is a mixed, integer two-dimensional program that is NP-hard (Al-Khayyal and Falk, 1983). Unfortunately, the number of variables in the problem \"KV\" is 100 million when V = 100, 000 and K = 1, 000, which cannot be solved with standard techniques. To avoid costly calculations, we first take the non-negative word vectors obtained with Eq. 3 and project non-zero values to values (i.e. ai, ai, RK \u2265 0; \u00a7 2.2) that we apply to the initial method of vectors."}, {"heading": "2.5 Hyperparameter Tuning", "text": "Methods A and B have three hyperparameters: the \"1-regularization penalty \u03bb,\" the \"2-regularization penalty \u03c4\" and the length of the overcomplete word vector representation K. We perform a grid search based on the word similarity task \"development\" (WS-353) discussed in \u00a7 B. We achieve a sparseness of at least 90% in supercomplete vectors. These hyperparameters were matched to a collection of initialization vectors (glove, discussed in \u00a7 A) so that the vectors in D are close to the standard. The four vector representations and the corresponding hyperparameters selected by this method are summarized in Table 1. These hyperparameters were selected for method A and maintained for method B."}, {"heading": "3 Experiments", "text": "Using methods A and B, we constructed sparsely supercomplete vector representations A and B, starting from four initial vector representations X; these are explained in Appendix A. We used a benchmark rating (WS-353) to adjust hyperparameters, leading to the settings shown in Table 1; seven other tasks were used to evaluate the quality of the sparsely supercomplete repetitions; the first task is a word similarity task, where the score correlates with human judgments, and the others are classification accuracies of a \"2-regulated logistic regression model trained with the help of word vectors. These tasks are described in detail in Appendix B."}, {"heading": "3.1 Effects of Transforming Vectors", "text": "First, we quantify the effects of our transformations by comparing their results with the initial (X) vectors. Table 3 shows consistent improvements in economical vectors (Method A), with the exception of the SimLex task, where our sparse vectors are worse than the initializer with skipped programs and on par with the multilingual initializer. Thrift is advantageous for all text classification tasks, for all initial vector representations. On average, sparse vectors outperform their corresponding initializers by 4.2 points across all vector types and tasks. 2Binarized overcomplete vectors (starting with Method B) are also generally better than the initial vectors (also shown in Table 3) and tend to outperform the sparse variants, except when initializing with gloves. On average of all vector types and all tasks, binarized overcomplete vectors outperform their corresponding initializers by 4.8 points and the continuous intermediators by 0.6 points, from Method A."}, {"heading": "3.2 Effect of Vector Length", "text": "How does the length of the overcomplete vector (K) affect performance? We focus here on the glove vectors, where L = 300 is located, and report on average performance across all tasks. We look at K = \u03b1L, where \u03b1 1, 2, 3, 5, 10, 15, 20}. Figure 2 records the average performance across all tasks against \u03b1. The earlier selection of K = 3,000 (\u03b1 = 10) yields the best result; gains are monotonous up to this point and then begin to decrease."}, {"heading": "3.3 Alternative Transformations", "text": "We consider two alternative transformations: the first preserves the original vector length, but 2We report correlation on a scale of 100 points, so that the average, which includes accuracy and correlation, is equally representative of both. The second transformation was proposed by Guo et al. (2014) Again, the original vector length is maintained, but the spareness is achieved by: ai, j = 1 if xi, j \u2265 M + \u2212 1 if xi, j \u2264 M \u2212 0 otherwise (8), where M + (M \u2212) is the mean of positively weighted (negatively weighted) elements of X. These vectors are obviously not binary. We find that on average, across initializing vectors and across all tasks, our sparse vectors perform better than any of the alternative transformations."}, {"heading": "4 Interpretability", "text": "Our hypothesis is that the dimensions of sparse supercomplete vectors are easier to interpret than dense word vectors. According to Murphy et al. (2012), we use a word intrusion experiment (Chang et al., 2009) to confirm this hypothesis. Furthermore, we perform qualitative analyses of interpretability, focusing on individual dimensions."}, {"heading": "4.1 Word Intrusion", "text": "In one case of the experiment, a human judge is confronted with five words in random order and asked to select the \"intruder.\" The words are selected by the experimenter by selecting a dimension j of the learned representation and then classifying the words in that dimension alone. Dimensions are selected in descending order of the variance of their values within the vocabulary. Four of the words are the uppermost words after j, and the \"true\" intruder is a word from the lower half of the list, chosen to be a word that occurs in the top 10% of another dimension. An example of an example is: Marine, Industry, Technology, Marine, Identity (The last word is the intruder.) We formed instances of initialization vectors and of our sparse supercomplete vectors (A). Each of these two combines the four different intruders Xotalier, Identity (The last word is the intruder.) We formed instances of initialization vectors and of our complete vectors (A)."}, {"heading": "4.2 Qualitative Evaluation of Interpretability", "text": "To verify this qualitatively, we select five dimensions with the highest variance of values in initial and sparse GC vectors. We compare the best words in the dimensions extracted from the two illustrations. The words are listed in Table 6, one dimension per line. Subjectively, we find the semantic groups in the sparse vectors better than in the initial vectors. Figure 3 illustrates the sparse GC vectors for six words. The dimensions are sorted by the average value of the three \"animal\" vectors. The animal-related words use many of the same dimensions (102 common active dimensions out of a total of 500); on the other hand, the three city names mostly use different vectors."}, {"heading": "5 Related Work", "text": "10 10 V 10 10 V 10 10 10 10 10 10 10 10 V 10 10 V 10 10 V 10 10 V 10 10 V 10 10 V 10 10 V 10 V 10 10 V 10 10 V 10 10 V 10 10 V 10 10 V 10 10 V 10 V 10 10 V 10 10 V 10 V 10 V 10 10 V 10 10 V 10 10 V 10 10 10 V 10 10 V 10 10 10 V 10 10 10 V 10 10 10 V 10 10 10 V 10 10 10 10 V 10 10 10 V 10 10 V 10 10 V 10 10 V 10 10 V 10 10 10 V 10 10 10 V 10 10 10 V 10 10 10 10 V 10 10 10 10 10 V 10 10 10 10 10 V 10 10 10 10 10 V 10 10 10 10 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 10 10 V 10 10 10 10 10 10 10 10 10 10 10 10 10 10 V 10 10 10 10 10 10 10 10 10 V 10 10 10 10 10 10 10 10 10 10 V 10 10 10 10 10 10 10 10 10 10 10 10 10 V 10 10 10 10 10 10 10 10 10 10 10 10 V 10 10 10 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 10 10 10 10 10 10 V 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 V 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 V 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 V 10 10 10 10 10 10 10 10 V 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 V 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 V 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 V 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 V 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 V 10 10 10 10 10 10 10 10 10 10 10 10 10 10 V 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 V 10 10 10 10 10 10 10 10 10 10 10"}, {"heading": "6 Conclusion", "text": "We presented a method of converting word vectors obtained using a modern word vector model into sparse and optionally binary word vectors. These transformed vectors seem to approach characteristics used in NLP tasks and surpass the original vectors from which they are derived based on a set of semantic and syntactic evaluation scales. We also found that the sparse vectors are easier to interpret than the dense human vectors after a word intrusion test."}, {"heading": "Acknowledgments", "text": "We thank Alona Fyshe for discussing vector interpretability and three anonymous reviewers for their feedback. This research was supported in part by the National Science Foundation with a grant of IIS-1251131 and the Defense Advanced Research Projects Agency with a grant of FA87501420244. This work was partially funded by the U.S. Army Research Laboratory and the U.S. Army Research Office under grant number W911NF-10-1-0533.A Initial Vector Representations (X) Our experiments look at four publicly available collections of pre-formed word vectors. They vary in the amount of data used and the estimation methodology. Glove. Global vectors for word representation (Pennington et al., 2014) are trained on aggregated global word-co-occurrence statistics from a corpus. These vectors were trained on 6 billion words from Wikipedia and English gigaword and are of length 300.33http: / www-lproject / p.u / Hedvar-layer / Hupvar-classification."}, {"heading": "B Evaluation Benchmarks", "text": "/ wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wort- / wor"}], "references": [{"title": "Jointly constrained biconvex programming", "author": ["Faiz A. Al-Khayyal", "James E. Falk."], "venue": "Mathematics of Operations Research, pages 273\u2013286.", "citeRegEx": "Al.Khayyal and Falk.,? 1983", "shortCiteRegEx": "Al.Khayyal and Falk.", "year": 1983}, {"title": "Inter-coder agreement for computational linguistics", "author": ["Ron Artstein", "Massimo Poesio."], "venue": "Computational Linguistics, 34(4):555\u2013596.", "citeRegEx": "Artstein and Poesio.,? 2008", "shortCiteRegEx": "Artstein and Poesio.", "year": 2008}, {"title": "The Berkeley FrameNet project", "author": ["Collin F. Baker", "Charles J. Fillmore", "John B. Lowe."], "venue": "Proc. of ACL.", "citeRegEx": "Baker et al\\.,? 1998", "shortCiteRegEx": "Baker et al\\.", "year": 1998}, {"title": "Tailoring continuous word representations for dependency parsing", "author": ["Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "Proc. of ACL.", "citeRegEx": "Bansal et al\\.,? 2014", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "Representation learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pascal Vincent."], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798\u20131828.", "citeRegEx": "Bengio et al\\.,? 2013", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Reading tea leaves: How humans interpret topic models", "author": ["Jonathan Chang", "Sean Gerrish", "Chong Wang", "Jordan L. Boyd-Graber", "David M. Blei."], "venue": "NIPS.", "citeRegEx": "Chang et al\\.,? 2009", "shortCiteRegEx": "Chang et al\\.", "year": 2009}, {"title": "Nonnegative Matrix and Tensor Factorizations: Applications to Exploratory Multi-way Data Analysis and Blind Source Separation", "author": ["Andrzej Cichocki", "Rafal Zdunek", "Anh Huy Phan", "Shun-ichi Amari."], "venue": "John Wiley & Sons.", "citeRegEx": "Cichocki et al\\.,? 2009", "shortCiteRegEx": "Cichocki et al\\.", "year": 2009}, {"title": "Measuring agreement for multinomial data", "author": ["Mark Davies", "Joseph L Fleiss."], "venue": "Biometrics, pages 1047\u20131051.", "citeRegEx": "Davies and Fleiss.,? 1982", "shortCiteRegEx": "Davies and Fleiss.", "year": 1982}, {"title": "Stable recovery of sparse overcomplete representations in the presence of noise", "author": ["David L. Donoho", "Michael Elad", "Vladimir N. Temlyakov."], "venue": "IEEE Transactions on Information Theory, 52(1).", "citeRegEx": "Donoho et al\\.,? 2006", "shortCiteRegEx": "Donoho et al\\.", "year": 2006}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "Technical Report EECS-2010-24, University of California Berkeley.", "citeRegEx": "Duchi et al\\.,? 2010", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "Dual averaging for distributed optimization: Convergence analysis and network scaling", "author": ["John C. Duchi", "Alekh Agarwal", "Martin J. Wainwright."], "venue": "IEEE Transactions on Automatic Control, 57(3):592\u2013606.", "citeRegEx": "Duchi et al\\.,? 2012", "shortCiteRegEx": "Duchi et al\\.", "year": 2012}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["Manaal Faruqui", "Chris Dyer."], "venue": "Proc. of EACL.", "citeRegEx": "Faruqui and Dyer.,? 2014", "shortCiteRegEx": "Faruqui and Dyer.", "year": 2014}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Manaal Faruqui", "Jesse Dodge", "Sujay K. Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A. Smith."], "venue": "Proc. of NAACL.", "citeRegEx": "Faruqui et al\\.,? 2015", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Placing search in context: the concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."], "venue": "Proc. of WWW.", "citeRegEx": "Finkelstein et al\\.,? 2001", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Sparse inverse covariance estimation with the graphical lasso", "author": ["Jerome Friedman", "Trevor Hastie", "Robert Tibshirani."], "venue": "Biostatistics, 9(3):432\u2013 441.", "citeRegEx": "Friedman et al\\.,? 2008", "shortCiteRegEx": "Friedman et al\\.", "year": 2008}, {"title": "Interpretable semantic vectors from a joint model of brain- and text- based meaning", "author": ["Alona Fyshe", "Partha P. Talukdar", "Brian Murphy", "Tom M. Mitchell."], "venue": "Proc. of ACL.", "citeRegEx": "Fyshe et al\\.,? 2014", "shortCiteRegEx": "Fyshe et al\\.", "year": 2014}, {"title": "A compositional and interpretable semantic space", "author": ["Alona Fyshe", "Leila Wehbe", "Partha P. Talukdar", "Brian Murphy", "Tom M. Mitchell."], "venue": "Proc. of NAACL.", "citeRegEx": "Fyshe et al\\.,? 2015", "shortCiteRegEx": "Fyshe et al\\.", "year": 2015}, {"title": "Posterior vs", "author": ["Kuzman Ganchev", "Ben Taskar", "Fernando Pereira", "Jo\u00e3o Gama."], "venue": "parameter sparsity in latent variable models. In NIPS.", "citeRegEx": "Ganchev et al\\.,? 2009", "shortCiteRegEx": "Ganchev et al\\.", "year": 2009}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio."], "venue": "Proc. of ICML.", "citeRegEx": "Glorot et al\\.,? 2011", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Exponential priors for maximum entropy models", "author": ["Joshua Goodman."], "venue": "Proc. of NAACL.", "citeRegEx": "Goodman.,? 2004", "shortCiteRegEx": "Goodman.", "year": 2004}, {"title": "Towards a formal distributional semantics: Simulating logical calculi with tensors", "author": ["E. Grefenstette."], "venue": "arXiv:1304.5823.", "citeRegEx": "Grefenstette.,? 2013", "shortCiteRegEx": "Grefenstette.", "year": 2013}, {"title": "Revisiting embedding features for simple semi-supervised learning", "author": ["Jiang Guo", "Wanxiang Che", "Haifeng Wang", "Ting Liu."], "venue": "Proc. of EMNLP.", "citeRegEx": "Guo et al\\.,? 2014", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Asynchronous stochastic optimization for sequence training of deep neural networks", "author": ["Georg Heigold", "Erik McDermott", "Vincent Vanhoucke", "Andrew Senior", "Michiel Bacchiani."], "venue": "Proc. of ICASSP.", "citeRegEx": "Heigold et al\\.,? 2014", "shortCiteRegEx": "Heigold et al\\.", "year": 2014}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen."], "venue": "CoRR, abs/1408.3456.", "citeRegEx": "Hill et al\\.,? 2014", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "Non-negative sparse coding", "author": ["Patrik O. Hoyer."], "venue": "Neural Networks for Signal Processing, 2002. Proc. of IEEE Workshop on.", "citeRegEx": "Hoyer.,? 2002", "shortCiteRegEx": "Hoyer.", "year": 2002}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Proc. of ACL.", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Evaluation and extension of maximum entropy models with inequality constraints", "author": ["Jun\u2019ichi Kazama", "Jun\u2019ichi Tsujii"], "venue": "In Proc. of EMNLP", "citeRegEx": "Kazama and Tsujii.,? \\Q2003\\E", "shortCiteRegEx": "Kazama and Tsujii.", "year": 2003}, {"title": "Detecting compositionality of multi-word expressions using nearest neighbours in vector space models", "author": ["Douwe Kiela", "Stephen Clark."], "venue": "Proc. of EMNLP.", "citeRegEx": "Kiela and Clark.,? 2013", "shortCiteRegEx": "Kiela and Clark.", "year": 2013}, {"title": "The measurement of observer agreement for categorical data", "author": ["J. Richard Landis", "Gary G. Koch."], "venue": "Biometrics, 33(1):159\u2013174.", "citeRegEx": "Landis and Koch.,? 1977", "shortCiteRegEx": "Landis and Koch.", "year": 1977}, {"title": "Fish transporters and miracle homes: How compositional distributional semantics can help NP parsing", "author": ["Angeliki Lazaridou", "Eva Maria Vecchi", "Marco Baroni."], "venue": "Proc. of EMNLP.", "citeRegEx": "Lazaridou et al\\.,? 2013", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2013}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["Daniel D. Lee", "H. Sebastian Seung."], "venue": "Nature, 401(6755):788\u2013791.", "citeRegEx": "Lee and Seung.,? 1999", "shortCiteRegEx": "Lee and Seung.", "year": 1999}, {"title": "Efficient sparse coding algorithms", "author": ["Honglak Lee", "Alexis Battle", "Rajat Raina", "Andrew Y. Ng."], "venue": "NIPS.", "citeRegEx": "Lee et al\\.,? 2006", "shortCiteRegEx": "Lee et al\\.", "year": 2006}, {"title": "Exponential family sparse coding with application to self-taught learning", "author": ["Honglak Lee", "Rajat Raina", "Alex Teichman", "Andrew Y. Ng."], "venue": "Proc. of IJCAI.", "citeRegEx": "Lee et al\\.,? 2009", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "English Verb Classes and Alternations: A Preliminary Investigation", "author": ["Beth Levin."], "venue": "University of Chicago Press.", "citeRegEx": "Levin.,? 1993", "shortCiteRegEx": "Levin.", "year": 1993}, {"title": "Learning overcomplete representations", "author": ["Michael Lewicki", "Terrence Sejnowski."], "venue": "Neural Computation, 12(2):337\u2013365.", "citeRegEx": "Lewicki and Sejnowski.,? 2000", "shortCiteRegEx": "Lewicki and Sejnowski.", "year": 2000}, {"title": "Combined distributional and logical semantics", "author": ["Mike Lewis", "Mark Steedman."], "venue": "Transactions of the ACL, 1:179\u2013192.", "citeRegEx": "Lewis and Steedman.,? 2013", "shortCiteRegEx": "Lewis and Steedman.", "year": 2013}, {"title": "Combining formal and distributional models of temporal and intensional semantics", "author": ["Mike Lewis", "Mark Steedman."], "venue": "Proc. of ACL.", "citeRegEx": "Lewis and Steedman.,? 2014", "shortCiteRegEx": "Lewis and Steedman.", "year": 2014}, {"title": "Learning question classifiers", "author": ["Xin Li", "Dan Roth."], "venue": "Proc. of COLING.", "citeRegEx": "Li and Roth.,? 2002", "shortCiteRegEx": "Li and Roth.", "year": 2002}, {"title": "Non-negative matrix factorization for visual coding", "author": ["Weixiang Liu", "Nanning Zheng", "Xiaofeng Lu."], "venue": "Proc. of ICASSP.", "citeRegEx": "Liu et al\\.,? 2003", "shortCiteRegEx": "Liu et al\\.", "year": 2003}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P. Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."], "venue": "Computational Linguistics, 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Structured sparsity in structured prediction", "author": ["Andr\u00e9 F.T. Martins", "Noah A. Smith", "Pedro M.Q. Aguiar", "M\u00e1rio A.T. Figueiredo."], "venue": "Proc. of EMNLP.", "citeRegEx": "Martins et al\\.,? 2011", "shortCiteRegEx": "Martins et al\\.", "year": 2011}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "WordNet: a lexical database for English", "author": ["George A. Miller."], "venue": "Communications of the ACM, 38(11):39\u201341.", "citeRegEx": "Miller.,? 1995", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Learning effective and interpretable semantic models using non-negative sparse embedding", "author": ["Brian Murphy", "Partha Talukdar", "Tom Mitchell."], "venue": "Proc. of COLING.", "citeRegEx": "Murphy et al\\.,? 2012", "shortCiteRegEx": "Murphy et al\\.", "year": 2012}, {"title": "Research Design & Statistical Analysis", "author": ["Jerome L. Myers", "Arnold D. Well."], "venue": "Routledge.", "citeRegEx": "Myers and Well.,? 1995", "shortCiteRegEx": "Myers and Well.", "year": 1995}, {"title": "Sparse coding with an overcomplete basis set: A strategy employed by v1", "author": ["Bruno A. Olshausen", "David J. Field"], "venue": "Vision Research,", "citeRegEx": "Olshausen and Field.,? \\Q1997\\E", "shortCiteRegEx": "Olshausen and Field.", "year": 1997}, {"title": "A practical and linguistically-motivated approach to compositional distributional semantics", "author": ["Denis Paperno", "Nghia The Pham", "Marco Baroni."], "venue": "Proc. of ACL.", "citeRegEx": "Paperno et al\\.,? 2014", "shortCiteRegEx": "Paperno et al\\.", "year": 2014}, {"title": "Factorial LDA: Sparse multi-dimensional text models", "author": ["Michael Paul", "Mark Dredze."], "venue": "NIPS.", "citeRegEx": "Paul and Dredze.,? 2012", "shortCiteRegEx": "Paul and Dredze.", "year": 2012}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Proc. of EMNLP.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Verbnet: A Broadcoverage, Comprehensive Verb Lexicon", "author": ["Karin Kipper Schuler."], "venue": "Ph.D. thesis, University of Pennsylvania.", "citeRegEx": "Schuler.,? 2005", "shortCiteRegEx": "Schuler.", "year": 2005}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts."], "venue": "Proc. of EMNLP.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "A bayesian lda-based model for semi-supervised partof-speech tagging", "author": ["Kristina Toutanova", "Mark Johnson."], "venue": "NIPS.", "citeRegEx": "Toutanova and Johnson.,? 2007", "shortCiteRegEx": "Toutanova and Johnson.", "year": 2007}, {"title": "From frequency to meaning : Vector space models of semantics", "author": ["Peter D. Turney", "Patrick Pantel."], "venue": "JAIR, 37(1):141\u2013188.", "citeRegEx": "Turney and Pantel.,? 2010", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "Mining the web for synonyms: PMI-IR versus LSA on TOEFL", "author": ["Peter D. Turney."], "venue": "Proc. of ECML.", "citeRegEx": "Turney.,? 2001", "shortCiteRegEx": "Turney.", "year": 2001}, {"title": "Studying the recursive behaviour of adjectival modification with compositional distributional semantics", "author": ["Eva Maria Vecchi", "Roberto Zamparelli", "Marco Baroni."], "venue": "Proc. of EMNLP.", "citeRegEx": "Vecchi et al\\.,? 2013", "shortCiteRegEx": "Vecchi et al\\.", "year": 2013}, {"title": "Dual averaging methods for regularized stochastic learning and online optimization", "author": ["Lin Xiao."], "venue": "NIPS.", "citeRegEx": "Xiao.,? 2009", "shortCiteRegEx": "Xiao.", "year": 2009}, {"title": "Rcnet: A general framework for incorporating knowledge into word representations", "author": ["Chang Xu", "Yalong Bai", "Jiang Bian", "Bin Gao", "Gang Wang", "Xiaoguang Liu", "Tie-Yan Liu."], "venue": "Proc. of CIKM.", "citeRegEx": "Xu et al\\.,? 2014", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Linguistic structured sparsity in text categorization", "author": ["Dani Yogatama", "Noah A Smith."], "venue": "Proc. of ACL.", "citeRegEx": "Yogatama and Smith.,? 2014", "shortCiteRegEx": "Yogatama and Smith.", "year": 2014}, {"title": "Learning word representations with hierarchical sparse coding", "author": ["Dani Yogatama", "Manaal Faruqui", "Chris Dyer", "Noah A. Smith."], "venue": "Proc. of ICML.", "citeRegEx": "Yogatama et al\\.,? 2015", "shortCiteRegEx": "Yogatama et al\\.", "year": 2015}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["Mo Yu", "Mark Dredze."], "venue": "Proc. of ACL.", "citeRegEx": "Yu and Dredze.,? 2014", "shortCiteRegEx": "Yu and Dredze.", "year": 2014}, {"title": "Sparse topical coding", "author": ["Jun Zhu", "Eric P Xing."], "venue": "arXiv:1202.3778.", "citeRegEx": "Zhu and Xing.,? 2012", "shortCiteRegEx": "Zhu and Xing.", "year": 2012}], "referenceMentions": [{"referenceID": 29, "context": "Distributed representations of words have been shown to benefit NLP tasks like parsing (Lazaridou et al., 2013; Bansal et al., 2014), named entity recognition (Guo et al.", "startOffset": 87, "endOffset": 132}, {"referenceID": 3, "context": "Distributed representations of words have been shown to benefit NLP tasks like parsing (Lazaridou et al., 2013; Bansal et al., 2014), named entity recognition (Guo et al.", "startOffset": 87, "endOffset": 132}, {"referenceID": 21, "context": ", 2014), named entity recognition (Guo et al., 2014), and sentiment analysis (Socher et al.", "startOffset": 34, "endOffset": 52}, {"referenceID": 50, "context": ", 2014), and sentiment analysis (Socher et al., 2013).", "startOffset": 32, "endOffset": 53}, {"referenceID": 53, "context": "Intrinsic evaluations on various tasks are guiding methods toward discovery of a representation that captures many facts about lexical semantics (Turney, 2001; Turney and Pantel, 2010).", "startOffset": 145, "endOffset": 184}, {"referenceID": 52, "context": "Intrinsic evaluations on various tasks are guiding methods toward discovery of a representation that captures many facts about lexical semantics (Turney, 2001; Turney and Pantel, 2010).", "startOffset": 145, "endOffset": 184}, {"referenceID": 33, "context": "Yet word vectors do not look anything like the representations described in most lexical semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005) and relationships among word meanings (Miller, 1995).", "startOffset": 152, "endOffset": 200}, {"referenceID": 2, "context": "Yet word vectors do not look anything like the representations described in most lexical semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005) and relationships among word meanings (Miller, 1995).", "startOffset": 152, "endOffset": 200}, {"referenceID": 49, "context": "Yet word vectors do not look anything like the representations described in most lexical semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005) and relationships among word meanings (Miller, 1995).", "startOffset": 152, "endOffset": 200}, {"referenceID": 42, "context": ", 1998; Schuler, 2005) and relationships among word meanings (Miller, 1995).", "startOffset": 61, "endOffset": 75}, {"referenceID": 35, "context": "On the surface, discrete theories seem incommensurate with the distributed approach, a problem now receiving much attention in computational linguistics (Lewis and Steedman, 2013; Kiela and Clark, 2013; Vecchi et al., 2013; Grefenstette, 2013; Lewis and Steedman, 2014; Paperno et al., 2014).", "startOffset": 153, "endOffset": 291}, {"referenceID": 27, "context": "On the surface, discrete theories seem incommensurate with the distributed approach, a problem now receiving much attention in computational linguistics (Lewis and Steedman, 2013; Kiela and Clark, 2013; Vecchi et al., 2013; Grefenstette, 2013; Lewis and Steedman, 2014; Paperno et al., 2014).", "startOffset": 153, "endOffset": 291}, {"referenceID": 54, "context": "On the surface, discrete theories seem incommensurate with the distributed approach, a problem now receiving much attention in computational linguistics (Lewis and Steedman, 2013; Kiela and Clark, 2013; Vecchi et al., 2013; Grefenstette, 2013; Lewis and Steedman, 2014; Paperno et al., 2014).", "startOffset": 153, "endOffset": 291}, {"referenceID": 20, "context": "On the surface, discrete theories seem incommensurate with the distributed approach, a problem now receiving much attention in computational linguistics (Lewis and Steedman, 2013; Kiela and Clark, 2013; Vecchi et al., 2013; Grefenstette, 2013; Lewis and Steedman, 2014; Paperno et al., 2014).", "startOffset": 153, "endOffset": 291}, {"referenceID": 36, "context": "On the surface, discrete theories seem incommensurate with the distributed approach, a problem now receiving much attention in computational linguistics (Lewis and Steedman, 2013; Kiela and Clark, 2013; Vecchi et al., 2013; Grefenstette, 2013; Lewis and Steedman, 2014; Paperno et al., 2014).", "startOffset": 153, "endOffset": 291}, {"referenceID": 46, "context": "On the surface, discrete theories seem incommensurate with the distributed approach, a problem now receiving much attention in computational linguistics (Lewis and Steedman, 2013; Kiela and Clark, 2013; Vecchi et al., 2013; Grefenstette, 2013; Lewis and Steedman, 2014; Paperno et al., 2014).", "startOffset": 153, "endOffset": 291}, {"referenceID": 59, "context": "Unlike recent approaches of incorporating semantics in distributional word vectors (Yu and Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015), the method does not rely on any external information source.", "startOffset": 83, "endOffset": 143}, {"referenceID": 56, "context": "Unlike recent approaches of incorporating semantics in distributional word vectors (Yu and Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015), the method does not rely on any external information source.", "startOffset": 83, "endOffset": 143}, {"referenceID": 12, "context": "Unlike recent approaches of incorporating semantics in distributional word vectors (Yu and Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015), the method does not rely on any external information source.", "startOffset": 83, "endOffset": 143}, {"referenceID": 45, "context": "The transformation results in longer, sparser vectors, sometimes called an \u201covercomplete\u201d representation (Olshausen and Field, 1997).", "startOffset": 105, "endOffset": 132}, {"referenceID": 45, "context": "Sparse, overcomplete representations have been motivated in other domains as a way to increase separability and interpretability, with each instance (here, a word) having a small number of active dimensions (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000), and to increase stability in the presence of noise (Donoho et al.", "startOffset": 207, "endOffset": 263}, {"referenceID": 34, "context": "Sparse, overcomplete representations have been motivated in other domains as a way to increase separability and interpretability, with each instance (here, a word) having a small number of active dimensions (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000), and to increase stability in the presence of noise (Donoho et al.", "startOffset": 207, "endOffset": 263}, {"referenceID": 8, "context": "Sparse, overcomplete representations have been motivated in other domains as a way to increase separability and interpretability, with each instance (here, a word) having a small number of active dimensions (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000), and to increase stability in the presence of noise (Donoho et al., 2006).", "startOffset": 316, "endOffset": 337}, {"referenceID": 43, "context": "Introducing sparsity in word vector dimensions has been shown to improve dimension interpretability (Murphy et al., 2012; Fyshe et al., 2014) and usability of word vectors as features in downstream tasks (Guo et al.", "startOffset": 100, "endOffset": 141}, {"referenceID": 15, "context": "Introducing sparsity in word vector dimensions has been shown to improve dimension interpretability (Murphy et al., 2012; Fyshe et al., 2014) and usability of word vectors as features in downstream tasks (Guo et al.", "startOffset": 100, "endOffset": 141}, {"referenceID": 21, "context": ", 2014) and usability of word vectors as features in downstream tasks (Guo et al., 2014).", "startOffset": 70, "endOffset": 88}, {"referenceID": 5, "context": "We also evaluate our word vectors in a word intrusion experiment with humans (Chang et al., 2009) and find that our sparse vectors are more interpretable than the original vectors (\u00a74).", "startOffset": 77, "endOffset": 97}, {"referenceID": 31, "context": "In sparse coding (Lee et al., 2006), the goal is to represent each input vector xi as a sparse linear combination of basis vectors, ai.", "startOffset": 17, "endOffset": 35}, {"referenceID": 32, "context": "Here, we use the squared loss for the reconstruction error, but other loss functions could also be used (Lee et al., 2009).", "startOffset": 104, "endOffset": 122}, {"referenceID": 30, "context": "2 Sparse Nonnegative Vectors Nonnegativity in the feature space has often been shown to correspond to interpretability (Lee and Seung, 1999; Cichocki et al., 2009; Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015).", "startOffset": 119, "endOffset": 224}, {"referenceID": 6, "context": "2 Sparse Nonnegative Vectors Nonnegativity in the feature space has often been shown to correspond to interpretability (Lee and Seung, 1999; Cichocki et al., 2009; Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015).", "startOffset": 119, "endOffset": 224}, {"referenceID": 43, "context": "2 Sparse Nonnegative Vectors Nonnegativity in the feature space has often been shown to correspond to interpretability (Lee and Seung, 1999; Cichocki et al., 2009; Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015).", "startOffset": 119, "endOffset": 224}, {"referenceID": 15, "context": "2 Sparse Nonnegative Vectors Nonnegativity in the feature space has often been shown to correspond to interpretability (Lee and Seung, 1999; Cichocki et al., 2009; Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015).", "startOffset": 119, "endOffset": 224}, {"referenceID": 16, "context": "2 Sparse Nonnegative Vectors Nonnegativity in the feature space has often been shown to correspond to interpretability (Lee and Seung, 1999; Cichocki et al., 2009; Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015).", "startOffset": 119, "endOffset": 224}, {"referenceID": 24, "context": "To obtain nonnegative sparse word vectors, we use a variation of the nonnegative sparse coding method (Hoyer, 2002).", "startOffset": 102, "endOffset": 115}, {"referenceID": 9, "context": "3 Optimization We use online adaptive gradient descent (AdaGrad; Duchi et al., 2010) for solving the optimization problems in Eqs.", "startOffset": 55, "endOffset": 84}, {"referenceID": 10, "context": "In order to speed up training we use asynchronous updates to the parameters of the model in parallel for every word vector (Duchi et al., 2012; Heigold et al., 2014).", "startOffset": 123, "endOffset": 165}, {"referenceID": 22, "context": "In order to speed up training we use asynchronous updates to the parameters of the model in parallel for every word vector (Duchi et al., 2012; Heigold et al., 2014).", "startOffset": 123, "endOffset": 165}, {"referenceID": 55, "context": "We use the AdaGrad variant of one such learning algorithm, the regularized dual averaging algorithm (Xiao, 2009), which keeps track of the online average gradient at time t: \u1e21t = 1t \u2211t t\u2032=1 gt\u2032 Here, the subgradients do not include terms for the regularizer; they are derivatives of the unregularized objective (\u03bb = 0, \u03c4 = 0)", "startOffset": 100, "endOffset": 112}, {"referenceID": 0, "context": "This is an mixed integer bilinear program, which is NP-hard (Al-Khayyal and Falk, 1983).", "startOffset": 60, "endOffset": 87}, {"referenceID": 21, "context": "The second transformation was proposed by Guo et al. (2014). Here, the original vector length is also preserved, but sparsity is achieved through:", "startOffset": 42, "endOffset": 60}, {"referenceID": 5, "context": "(2012), we use a word intrusion experiment (Chang et al., 2009) to corroborate this hypothesis.", "startOffset": 43, "endOffset": 63}, {"referenceID": 42, "context": "Following Murphy et al. (2012), we use a word intrusion experiment (Chang et al.", "startOffset": 10, "endOffset": 31}, {"referenceID": 21, "context": "8 (Guo et al., 2014) 75.", "startOffset": 2, "endOffset": 20}, {"referenceID": 1, "context": "Table 5: Accuracy of three human annotators on the word intrusion task, along with the average inter-annotator agreement (Artstein and Poesio, 2008) and Fleiss\u2019 \u03ba (Davies and Fleiss, 1982).", "startOffset": 121, "endOffset": 148}, {"referenceID": 7, "context": "Table 5: Accuracy of three human annotators on the word intrusion task, along with the average inter-annotator agreement (Artstein and Poesio, 2008) and Fleiss\u2019 \u03ba (Davies and Fleiss, 1982).", "startOffset": 163, "endOffset": 188}, {"referenceID": 28, "context": "The inter-annotator agreement on the sparse vectors increases substantially, from 57% to 71%, and the Fleiss\u2019 \u03ba increases from \u201cfair\u201d to \u201cmoderate\u201d agreement (Landis and Koch, 1977).", "startOffset": 158, "endOffset": 181}, {"referenceID": 45, "context": "However, overcomplete features have been widely used in image processing, computer vision (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000) and signal processing (Donoho et al.", "startOffset": 90, "endOffset": 146}, {"referenceID": 34, "context": "However, overcomplete features have been widely used in image processing, computer vision (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000) and signal processing (Donoho et al.", "startOffset": 90, "endOffset": 146}, {"referenceID": 8, "context": "However, overcomplete features have been widely used in image processing, computer vision (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000) and signal processing (Donoho et al., 2006).", "startOffset": 169, "endOffset": 190}, {"referenceID": 30, "context": "Nonnegative matrix factorization is often used for interpretable coding of information (Lee and Seung, 1999; Liu et al., 2003; Cichocki et al., 2009).", "startOffset": 87, "endOffset": 149}, {"referenceID": 38, "context": "Nonnegative matrix factorization is often used for interpretable coding of information (Lee and Seung, 1999; Liu et al., 2003; Cichocki et al., 2009).", "startOffset": 87, "endOffset": 149}, {"referenceID": 6, "context": "Nonnegative matrix factorization is often used for interpretable coding of information (Lee and Seung, 1999; Liu et al., 2003; Cichocki et al., 2009).", "startOffset": 87, "endOffset": 149}, {"referenceID": 26, "context": "Sparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004), like POS tagging (Ganchev et al.", "startOffset": 59, "endOffset": 122}, {"referenceID": 14, "context": "Sparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004), like POS tagging (Ganchev et al.", "startOffset": 59, "endOffset": 122}, {"referenceID": 19, "context": "Sparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004), like POS tagging (Ganchev et al.", "startOffset": 59, "endOffset": 122}, {"referenceID": 17, "context": ", 2008; Goodman, 2004), like POS tagging (Ganchev et al., 2009), dependency parsing (Martins et al.", "startOffset": 41, "endOffset": 63}, {"referenceID": 40, "context": ", 2009), dependency parsing (Martins et al., 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al.", "startOffset": 28, "endOffset": 50}, {"referenceID": 57, "context": ", 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al.", "startOffset": 29, "endOffset": 55}, {"referenceID": 4, "context": ", 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al., 2013; Yogatama et al., 2015).", "startOffset": 85, "endOffset": 129}, {"referenceID": 58, "context": ", 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al., 2013; Yogatama et al., 2015).", "startOffset": 85, "endOffset": 129}, {"referenceID": 51, "context": "Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of sparse Dirichlet priors has been shown to be useful for downstream tasks like POS-tagging (Toutanova and Johnson, 2007), and improving interpretation (Paul and Dredze, 2012; Zhu and Xing, 2012).", "startOffset": 185, "endOffset": 214}, {"referenceID": 47, "context": "Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of sparse Dirichlet priors has been shown to be useful for downstream tasks like POS-tagging (Toutanova and Johnson, 2007), and improving interpretation (Paul and Dredze, 2012; Zhu and Xing, 2012).", "startOffset": 245, "endOffset": 288}, {"referenceID": 60, "context": "Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of sparse Dirichlet priors has been shown to be useful for downstream tasks like POS-tagging (Toutanova and Johnson, 2007), and improving interpretation (Paul and Dredze, 2012; Zhu and Xing, 2012).", "startOffset": 245, "endOffset": 288}, {"referenceID": 48, "context": "Global vectors for word representations (Pennington et al., 2014) are trained on aggregated global word-word co-occurrence statistics from a corpus.", "startOffset": 40, "endOffset": 65}, {"referenceID": 41, "context": "The word2vec tool (Mikolov et al., 2013) is fast and widely-used.", "startOffset": 18, "endOffset": 40}, {"referenceID": 25, "context": "These vectors are learned using a recursive neural network that incorporates both local and global (documentlevel) context features (Huang et al., 2012).", "startOffset": 132, "endOffset": 152}, {"referenceID": 11, "context": "Faruqui and Dyer (2014) learned vectors by first performing SVD on text in different languages, then applying canonical correlation analysis on pairs of vectors for words that align in parallel corpora.", "startOffset": 0, "endOffset": 24}, {"referenceID": 13, "context": "The first is the WS-353 dataset (Finkelstein et al., 2001), which contains 353 pairs of English words that have been assigned similarity ratings by humans.", "startOffset": 32, "endOffset": 58}, {"referenceID": 23, "context": "A more recent dataset, SimLex-999 (Hill et al., 2014), has been constructed to specifically focus on similarity (rather than relatedness).", "startOffset": 34, "endOffset": 53}, {"referenceID": 44, "context": "We calculate cosine similarity between the vectors of two words forming a test item and report Spearman\u2019s rank correlation coefficient (Myers and Well, 1995) between the rankings produced by our model against the human rankings.", "startOffset": 135, "endOffset": 157}, {"referenceID": 50, "context": "Socher et al. (2013) created a treebank of sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts.", "startOffset": 0, "endOffset": 21}, {"referenceID": 37, "context": ", whether the question is about a location, about a person, or about some numeric information (Li and Roth, 2002).", "startOffset": 94, "endOffset": 113}, {"referenceID": 57, "context": "/test split in accordance with Yogatama and Smith (2014): (1) Sports: baseball vs.", "startOffset": 31, "endOffset": 57}, {"referenceID": 39, "context": "(2013) constructed a dataset from the Penn Treebank (Marcus et al., 1993) of noun phrases (NP) of", "startOffset": 52, "endOffset": 73}, {"referenceID": 29, "context": "Lazaridou et al. (2013) constructed a dataset from the Penn Treebank (Marcus et al.", "startOffset": 0, "endOffset": 24}], "year": 2015, "abstractText": "Current distributed representations of words show little resemblance to theories of lexical semantics. The former are dense and uninterpretable, the latter largely based on familiar, discrete classes (e.g., supersenses) and relations (e.g., synonymy and hypernymy). We propose methods that transform word vectors into sparse (and optionally binary) vectors. The resulting representations are more similar to the interpretable features typically used in NLP, though they are discovered automatically from raw corpora. Because the vectors are highly sparse, they are computationally easy to work with. Most importantly, we find that they outperform the original vectors on benchmark tasks.", "creator": "TeX"}}}