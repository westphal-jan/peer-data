{"id": "1506.02108", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2015", "title": "Deeply Learning the Messages in Message Passing Inference", "abstract": "Deep structured output learning shows great promise in tasks like semantic image segmentation. We proffer a new, efficient deep structured model learning scheme, in which we show how deep Convolutional Neural Networks (CNNs) can be used to estimate the messages in message passing inference for structured prediction with Conditional Random Fields (CRFs). With such CNN message estimators, we obviate the need to learn or evaluate potential functions for message calculation. This confers significant efficiency for learning, since otherwise when performing structured learning for a CRF with CNN potentials it is necessary to undertake expensive inference for every stochastic gradient iteration. The network output dimension for message estimation is the same as the number of classes, in contrast to the network output for general CNN potential functions in CRFs, which is exponential in the order of the potentials. Hence CNN message learning has fewer network parameters and is more scalable for cases that a large number of classes are involved. We apply our method to semantic image segmentation on the PASCAL VOC 2012 dataset. We achieve an intersection-over-union score of 73.4 on its test set, which is the best reported result for methods using the VOC training images alone. This impressive performance demonstrates the effectiveness and usefulness of our CNN message learning method.", "histories": [["v1", "Sat, 6 Jun 2015 02:52:38 GMT  (6473kb,D)", "https://arxiv.org/abs/1506.02108v1", "11 pages"], ["v2", "Wed, 10 Jun 2015 06:49:06 GMT  (6473kb,D)", "http://arxiv.org/abs/1506.02108v2", "11 pages. Fixed a typo"], ["v3", "Tue, 8 Sep 2015 04:29:45 GMT  (18kb)", "http://arxiv.org/abs/1506.02108v3", "11 pages. Appearing in Proc. The Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS), 2015, Montreal, Canada"]], "COMMENTS": "11 pages", "reviews": [], "SUBJECTS": "cs.CV cs.LG stat.ML", "authors": ["guosheng lin", "chunhua shen", "ian d reid 0001", "anton van den hengel"], "accepted": true, "id": "1506.02108"}, "pdf": {"name": "1506.02108.pdf", "metadata": {"source": "CRF", "title": "Deeply Learning the Messages in Message Passing Inference", "authors": ["Guosheng Lin Chunhua Shen", "Ian Reid", "Anton van den Hengel"], "emails": ["firstname.lastname@adelaide.edu.au"], "sections": [{"heading": null, "text": "ar Xiv: 150 6.02 108v 3 [cs.C V] 8S epAppearing in Proc. The Twenty-nine Annual Conference on Neural Information Processing Systems (NIPS), 2015, Montreal, Canada. This work was partially supported by the Data to Decisions CRC Centre. Authors thank NVIDIA for donating the K40 graphics cards. Correspondence should be sent to C. Shen.Contents"}, {"heading": "1 Introduction 3", "text": "1.1 Related work................................................."}, {"heading": "2 Learning CRF with CNN potentials 4", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 Learning CNN message estimators 5", "text": "3.1 CNN News Estimators................................... 63.2 News Estimator Network Details................... 73.3 CNN News Estimator Training...................... 83.4 News Estimators with Inference Time Budgets................"}, {"heading": "4 Experiments 9", "text": "5 Conclusion 10"}, {"heading": "1 Introduction", "text": "A popular approach for deep structured models is the formulation of conditional random fields (CRFs) using deep conventional neural networks (CNNs) for potential functions, combining the power of CNNs to represent characteristics and the ability of CRFs to model complex relationships. A typical approach to joint learning of CRFs and CNNs [1, 2, 4, 5] is to explore the CRF objective, such as maximizing log likes. Joint learning has shown impressive results for semantic image segmentation."}, {"heading": "1.1 Related work", "text": "The combination of the strengths of CNNs and CRFs for segmentation has been explored in several recent methodologies, some of which rely on a simple combination of CNN classifiers and CRFs without collaborative learning. DeepLab-CRF in [9] first full of CNN for pixel classification and applies a dense CRF method as a post-processing step. Later, the method is expanded in [2] DeepLab by collaborative learning of dense CRFs and CNNs. RNN-CRF in [1] also performs collaborative learning of CNNs and dense CRFs. They implement the medium-field inferencing of neural networks that facilitate end-to-end-to-learning, using CNNs to model the simple potentials only in CNNs and dense CRFs."}, {"heading": "2 Learning CRF with CNN potentials", "text": "Before describing our message learning method, we review the CRF-CNN common learning approach and discuss the limitations. An input mask is denoted by x-X and the corresponding markup mask is denoted by y-Y. The energy function is denoted by E (y, x), which measures the score of the prediction y. We consider the following form of conditional probability: P (y, x) = 1Z (x) exp [-E (y, x)] = CNN [\u2212 E (y, x)]] [-E (y, x)]. (1) Here is the partition function. The CRF model is decomposed by a number of factors. Generally, the energy function is written as the sum of potential functions (y, x) = F-F-EF-EF-EF (yF). (2) Here F-indexes are a factor in the factor diagram."}, {"heading": "3 Learning CNN message estimators", "text": "In conventional CRF approaches, the potential functions are first learned and then inferred based on the potential functions learned to generate the final prediction. In contrast, our approach directly optimizes the sequence method for the final prediction. In the CRF prediction method, the \"message factor\" vectors are recursively calculated based on the learned potentials. We propose to construct and learn CNNs to estimate these messages directly in the message delivery method, rather than learning the potential functions p. In particular, we learn factor to variable news estimators. Our message learning framework is general and can accommodate all message delivery algorithms such as loopy belief propagation (BP) [13], mid-range approximation [13] and their variants."}, {"heading": "3.1 CNN message estimators", "text": "The calculation of the factor-to-variable message \u03b2F \u2192 p depends on the factor-to-factor message \u03b2p from \u03b2\u03b2q \u2192 F. The substitution of the definition of \u03b2p \u2192 F in (8), \u03b2F \u2192 p can be rewritten as follows: \u03b2F \u2192 p (yp) = Logbook q \u00b2 y \u00b2 p, y \u2032 p = ypexp (y \"f\") = Logbook y \"p\" p \"(y\" q \"q\") = Logbook q \"q\" p \"(y\" q \"\" p \") = Logbook y\" p \"(y\" q \"\" q \"\" p \"\" q \") = Logbook y\" \"q\" \"q\" \"p.\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\""}, {"heading": "3.2 Details for message estimator networks", "text": "We formulate the estimator MF as CNN, so the estimation is the network output: \u03b2F \u2192 p (yp) = MF (xpF, dpF, yp; \u03b8F) = \u2211 K = 1\u03b4 (k = yp) zpF, k (x, dpF; \u03b8F). (14) Here it denotes the network parameter to learn. (\u00b7) is the indicator function that corresponds to 1 if the input is true and 0 otherwise. We call it the K-dimensional output vector (K is the number of classes) of the message estimator network for the node p and the factor F; zpF, k is the value in the network output zpF corresponding to the k-th class. We can consider all possible strategies for implementing zpF with CNNs. For example, we describe a strategy that is analogous to network design."}, {"heading": "3.3 Training CNN message estimators", "text": "Our goal is to estimate the variable margins in (3) that can be rewritten with the estimators: P (yp | x) = \u2211 y\\ ypP (y | x) = 1Zp exp [\u2211 F \u00b2 \u03b2Fp\u03b2F \u2192 p (yp)] = 1Zp exp \u0445 F \u00b2 FpMF (xpF, dpF, yp; \u03b8F).Here, Zp is the normalizer. The ideal variable margins have the probability of 1 for the basic truth class and 0 for the remaining classes. Here, we are looking at the cross-entropy loss between the ideal marginal and the estimated marginal characteristics. J (x, y \u00b2) = \u2212 quadratic marginal size NK \u00b2 yp = 1 (yp = y \u00b2 p) log P (yp | x; quadratic marginal size) = \u2212 quadratic marginal size for the learning results in the basic class and the estimated cross-marginal characteristics between the remaining ones - here."}, {"heading": "3.4 Message learning with inference-time budgets", "text": "One advantage of message learning is that we can explicitly include the expected number of inference iterations in the learning process; the number of inference iterations defines the learning sequence of message estimators; this is especially useful if we want to learn the estimators who are able to make a high-quality prediction for only a few inference iterations. In contrast, conventional potential function learning in CRFs is not able to directly include the expected number of inference iterations. We are particularly interested in learning message estimators as they use only one message iteration for which inference iteration can be very fast. In this case, it would be preferable to have wide-ranging neighborhood connections where the far-reaching interaction can be captured by executing an inference pass."}, {"heading": "4 Experiments", "text": "\"I think we will be able to get the situation under control,\" he told the German Press Agency in Berlin. \"I think we will be able to get the situation under control.\" He added: \"I don't think we will be able to get the situation under control.\" He added: \"I don't think we will be able to get the situation under control.\" He added: \"I don't think we will be able to get the situation under control.\" He added: \"I don't think we will be able to get the situation under control.\""}, {"heading": "5 Conclusion", "text": "We have proposed a new framework for learning deep messages for structured CRF predictions. Learning deep message estimators for message delivery reveals a new direction for learning more deeply structured models. Learning CNN message estimators is efficient, which does not involve expensive follow-up steps for gradient calculation. The dimension of network output for message estimation is the same as the number of classes that do not increase with the order of potentials, and therefore CNN message learning has fewer network parameters and is more scalable in the number of classes compared to conventional potential functional learning. Our impressive performance in semantic segmentation demonstrates the effectiveness and usefulness of the proposed deep message learning. Our framework is general and easily applicable to other structured predictive applications."}], "references": [{"title": "Conditional random fields as recurrent neural networks", "author": ["S. Zheng", "S. Jayasumana", "B. Romera-Paredes", "V. Vineet", "Z. Su", "D. Du", "C. Huang", "P. Torr"], "venue": "2015. [Online]. Available: http://arxiv.org/abs/1502.03240", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Fully connected deep structured networks", "author": ["A. Schwing", "R. Urtasun"], "venue": "2015. [Online]. Available: http://arxiv.org/abs/1503.02351 1 The result link provided by VOC evaluation server: http://host.robots.ox.ac.uk:8080/anonymous/DBD0SI.html 10", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient piecewise training of deep structured models for semantic segmentation", "author": ["G. Lin", "C. Shen", "I. Reid", "A. van den Hengel"], "venue": "2015. [Online]. Available: http://arxiv.org/abs/1504.01013", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep convolutional neural fields for depth estimation from a single image", "author": ["F. Liu", "C. Shen", "G. Lin"], "venue": "Proc. IEEE Conf. Comp. Vis. Pattern Recogn., 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning deep structured models", "author": ["L. Chen", "A. Schwing", "A. Yuille", "R. Urtasun"], "venue": "2014. [Online]. Available: http://arxiv.org/abs/1407.2538", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficiency of pseudolikelihood estimation for simple Gaussian fields", "author": ["J. Besag"], "venue": "Biometrika, 1977.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1977}, {"title": "Piecewise training for undirected models", "author": ["C. Sutton", "A. McCallum"], "venue": "Proc. Conf. Uncertainty Artificial Intelli, 2005.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning message-passing inference machines for structured prediction", "author": ["S. Ross", "D. Munoz", "M. Hebert", "J. Bagnell"], "venue": "Proc. IEEE Conf. Comp. Vis. Pattern Recogn., 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected CRFs", "author": ["L. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A. Yuille"], "venue": "2014. [Online]. Available: http://arxiv.org/abs/1412.7062", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient inference in fully connected CRFs with Gaussian edge potentials", "author": ["P. Kr\u00e4henb\u00fchl", "V. Koltun"], "venue": "Proc. Adv. Neural Info. Process. Syst., 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning depth from single monocular images using deep convolutional neural fields", "author": ["F. Liu", "C. Shen", "G. Lin", "I. Reid"], "venue": "2015. [Online]. Available: http://arxiv.org/abs/1502.07411", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Joint training of a convolutional network and a graphical model for human pose estimation", "author": ["J. Tompson", "A. Jain", "Y. LeCun", "C. Bregler"], "venue": "Proc. Adv. Neural Info. Process. Syst., 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Structured learning and prediction in computer vision", "author": ["S. Nowozin", "C. Lampert"], "venue": "Found. Trends. Comput. Graph. Vis., 2011.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Convergent tree-reweighted message passing for energy minimization", "author": ["V. Kolmogorov"], "venue": "IEEE T. Pattern Analysis & Machine Intelligence, 2006.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Generalized belief propagation", "author": ["J.S. Yedidia", "W.T. Freeman", "Y. Weiss"], "venue": "Proc. Adv. Neural Info. Process. Syst., 2000.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2000}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "Proc. IEEE Conf. Comp. Vis. Pattern Recogn., 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Feedforward semantic segmentation with zoom-out features", "author": ["M. Mostajabi", "P. Yadollahpour", "G. Shakhnarovich"], "venue": "2014. [Online]. Available: http://arxiv.org/abs/1412.0774", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "BoxSup: exploiting bounding boxes to supervise convolutional networks for semantic segmentation", "author": ["J. Dai", "K. He", "J. Sun"], "venue": "2015. [Online]. Available: http://arxiv.org/abs/1503.01640", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "The pascal visual object classes (VOC) challenge", "author": ["M. Everingham", "L.V. Gool", "C. Williams", "J. Winn", "A. Zisserman"], "venue": "Int. J. Comp. Vis., 2010.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Simultaneous detection and segmentation", "author": ["B. Hariharan", "P. Arbel\u00e1ez", "R. Girshick", "J. Malik"], "venue": "Proc. European Conf. Computer Vision, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Semantic contours from inverse detectors", "author": ["B. Hariharan", "P. Arbelaez", "L. Bourdev", "S. Maji", "J. Malik"], "venue": "Proc. Int. Conf. Comp. Vis., 2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "2014. [Online]. Available: http://arxiv.org/abs/1409.1556", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning deconvolution network for semantic segmentation", "author": ["H. Noh", "S. Hong", "B. Han"], "venue": "Proc. IEEE Conf. Comp. Vis. Pattern Recogn., 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Weakly-and semi-supervised learning of a DCNN for semantic image segmentation", "author": ["G. Papandreou", "L. Chen", "K. Murphy", "A. Yuille"], "venue": "2015. [Online]. Available: http://arxiv.org/abs/1502.02734 11", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "The typical approach for the joint learning of CRFs and CNNs [1, 2, 3, 4, 5], is to learn the CNN potential functions by optimizing the CRF objective, e.", "startOffset": 61, "endOffset": 76}, {"referenceID": 1, "context": "The typical approach for the joint learning of CRFs and CNNs [1, 2, 3, 4, 5], is to learn the CNN potential functions by optimizing the CRF objective, e.", "startOffset": 61, "endOffset": 76}, {"referenceID": 2, "context": "The typical approach for the joint learning of CRFs and CNNs [1, 2, 3, 4, 5], is to learn the CNN potential functions by optimizing the CRF objective, e.", "startOffset": 61, "endOffset": 76}, {"referenceID": 3, "context": "The typical approach for the joint learning of CRFs and CNNs [1, 2, 3, 4, 5], is to learn the CNN potential functions by optimizing the CRF objective, e.", "startOffset": 61, "endOffset": 76}, {"referenceID": 4, "context": "The typical approach for the joint learning of CRFs and CNNs [1, 2, 3, 4, 5], is to learn the CNN potential functions by optimizing the CRF objective, e.", "startOffset": 61, "endOffset": 76}, {"referenceID": 5, "context": "Applying an approximate training objective is a solution to avoid repeat inference; pseudo-likelihood learning [6] and piecewise learning [7, 3] are examples of this kind of approach.", "startOffset": 111, "endOffset": 114}, {"referenceID": 6, "context": "Applying an approximate training objective is a solution to avoid repeat inference; pseudo-likelihood learning [6] and piecewise learning [7, 3] are examples of this kind of approach.", "startOffset": 138, "endOffset": 144}, {"referenceID": 2, "context": "Applying an approximate training objective is a solution to avoid repeat inference; pseudo-likelihood learning [6] and piecewise learning [7, 3] are examples of this kind of approach.", "startOffset": 138, "endOffset": 144}, {"referenceID": 7, "context": "As discussed in [8], we can directly learn message estimators to output the required messages in the inference procedure, rather than learning the potential functions as in conventional CRF learning approaches.", "startOffset": 16, "endOffset": 19}, {"referenceID": 8, "context": "DeepLab-CRF in [9] first train fully CNN for pixel classification and applies a dense CRF [10] method as a post-processing step.", "startOffset": 15, "endOffset": 18}, {"referenceID": 9, "context": "DeepLab-CRF in [9] first train fully CNN for pixel classification and applies a dense CRF [10] method as a post-processing step.", "startOffset": 90, "endOffset": 94}, {"referenceID": 1, "context": "Later the method in [2] extends DeepLab by jointly learning the dense CRFs and CNNs.", "startOffset": 20, "endOffset": 23}, {"referenceID": 0, "context": "RNN-CRF in [1] also performs joint learning of CNNs and the dense CRFs.", "startOffset": 11, "endOffset": 14}, {"referenceID": 2, "context": "The work in [3] trains CNNs to model both the unary and pairwise potentials in order to capture contextual information.", "startOffset": 12, "endOffset": 15}, {"referenceID": 3, "context": "Jointly learning CNNs and CRFs has also been explored for other applications like depth estimation [4, 11].", "startOffset": 99, "endOffset": 106}, {"referenceID": 10, "context": "Jointly learning CNNs and CRFs has also been explored for other applications like depth estimation [4, 11].", "startOffset": 99, "endOffset": 106}, {"referenceID": 4, "context": "The work in [5] explores joint training of Markov random fields and deep networks for predicting words from noisy images and image classification.", "startOffset": 12, "endOffset": 15}, {"referenceID": 7, "context": "The inference machine proposed in [8] is relevant to our work in that it has discussed the idea of directly learning message estimators instead of learning potential functions for structured prediction.", "startOffset": 34, "endOffset": 37}, {"referenceID": 7, "context": "Unlike the approach in [8] which aims to learn variable-to-factor message estimators, our proposed method aims to learn the factor-to-variable message estimators.", "startOffset": 23, "endOffset": 26}, {"referenceID": 11, "context": "The approach in [12] jointly learns CNNs and CRFs for pose estimation, in which they learn the marginal likelihood of body parts but ignore the partition function in the likelihood.", "startOffset": 16, "endOffset": 20}, {"referenceID": 2, "context": "The recent method in [3] describes examples of constructing general CNN based unary and pairwise potentials.", "startOffset": 21, "endOffset": 24}, {"referenceID": 12, "context": "Message passing is a type of widely applied algorithms for approximate inference: loopy belief propagation (BP) [13], tree-reweighted message passing [14] and mean-field approximation [13] are examples of the message passing methods.", "startOffset": 112, "endOffset": 116}, {"referenceID": 13, "context": "Message passing is a type of widely applied algorithms for approximate inference: loopy belief propagation (BP) [13], tree-reweighted message passing [14] and mean-field approximation [13] are examples of the message passing methods.", "startOffset": 150, "endOffset": 154}, {"referenceID": 12, "context": "Message passing is a type of widely applied algorithms for approximate inference: loopy belief propagation (BP) [13], tree-reweighted message passing [14] and mean-field approximation [13] are examples of the message passing methods.", "startOffset": 184, "endOffset": 188}, {"referenceID": 12, "context": "Usually it is necessary to perform approximate marginal inference to calculate the gradients at each SGD iteration [13].", "startOffset": 115, "endOffset": 119}, {"referenceID": 2, "context": "However, repeated marginal inference can be extremely expensive, as discussed in [3].", "startOffset": 81, "endOffset": 84}, {"referenceID": 12, "context": "Our message learning framework is general and can accommodate all message passing based algorithms such as loopy belief propagation (BP) [13], mean-field approximation [13] and their variants.", "startOffset": 137, "endOffset": 141}, {"referenceID": 12, "context": "Our message learning framework is general and can accommodate all message passing based algorithms such as loopy belief propagation (BP) [13], mean-field approximation [13] and their variants.", "startOffset": 168, "endOffset": 172}, {"referenceID": 14, "context": "[15], loopy BP has a close relation with Bethe free energy approximation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "As discussed in [8], given that the goal is to estimate the marginals, it is not necessary to exactly follow the above equations, which involve learning potential functions, to calculate messages.", "startOffset": 16, "endOffset": 19}, {"referenceID": 2, "context": "For example, we here describe a strategy which is analogous to the network design in [3].", "startOffset": 85, "endOffset": 88}, {"referenceID": 15, "context": "We denote by C as a fully convolutional network (FCNN) [16] for convolutional feature generation, and C as a traditional fully connected network for message estimation.", "startOffset": 55, "endOffset": 59}, {"referenceID": 2, "context": "For example, it requiresK (K is the number of classes) outputs for the pairwise potentials [3].", "startOffset": 91, "endOffset": 94}, {"referenceID": 7, "context": "The work in [8] propose to learn the variable-to-factor message (\u03b2p\u2192F ).", "startOffset": 12, "endOffset": 15}, {"referenceID": 2, "context": ") IoU val set ContextDCRF [3] VOC extra 10k 70.", "startOffset": 26, "endOffset": 29}, {"referenceID": 16, "context": "3 Zoom-out [17] VOC extra 10k 63.", "startOffset": 11, "endOffset": 15}, {"referenceID": 1, "context": "5 Deep-struct [2] VOC extra 10k 64.", "startOffset": 14, "endOffset": 17}, {"referenceID": 8, "context": "1 DeepLab-CRF [9] VOC extra 10k 63.", "startOffset": 14, "endOffset": 17}, {"referenceID": 8, "context": "7 DeepLap-MCL [9] VOC extra 10k 68.", "startOffset": 14, "endOffset": 17}, {"referenceID": 17, "context": "7 BoxSup [18] VOC extra 10k 63.", "startOffset": 9, "endOffset": 13}, {"referenceID": 17, "context": "8 BoxSup [18] VOC extra + COCO 133k 68.", "startOffset": 9, "endOffset": 13}, {"referenceID": 18, "context": "We use the publicly available PASCAL VOC 2012 dataset [19].", "startOffset": 54, "endOffset": 58}, {"referenceID": 19, "context": "Following the common setting in [20, 9], the training set is augmented to 10582 images by including the extra annotations provided in [21] for the VOC images.", "startOffset": 32, "endOffset": 39}, {"referenceID": 8, "context": "Following the common setting in [20, 9], the training set is augmented to 10582 images by including the extra annotations provided in [21] for the VOC images.", "startOffset": 32, "endOffset": 39}, {"referenceID": 20, "context": "Following the common setting in [20, 9], the training set is augmented to 10582 images by including the extra annotations provided in [21] for the VOC images.", "startOffset": 134, "endOffset": 138}, {"referenceID": 18, "context": "We use intersection-over-union (IoU) score [19] to evaluate the segmentation performance.", "startOffset": 43, "endOffset": 47}, {"referenceID": 2, "context": "The recent work in [3] (referred to as ContextDCRF) learns multi-scale fully convolutional CNNs (FCNNs) for unary and pairwise potential functions to capture contextual information.", "startOffset": 19, "endOffset": 22}, {"referenceID": 2, "context": "We formulate our message estimators as multi-scale FCNNs, for which we apply a similar network configuration as in [3].", "startOffset": 115, "endOffset": 118}, {"referenceID": 21, "context": "Our networks are initialized using the VGG-16 model [22].", "startOffset": 52, "endOffset": 56}, {"referenceID": 8, "context": "p o tt ed sh ee p so fa tr a in tv DeepLab-CRF [9] 66.", "startOffset": 47, "endOffset": 50}, {"referenceID": 8, "context": "2 DeepLab-MCL [9] 71.", "startOffset": 14, "endOffset": 17}, {"referenceID": 15, "context": "7 FCN-8s [16] 62.", "startOffset": 9, "endOffset": 13}, {"referenceID": 0, "context": "1 CRF-RNN [1] 72.", "startOffset": 10, "endOffset": 13}, {"referenceID": 2, "context": ") IoU test set ContextDCRF [3] VOC extra 10k 70.", "startOffset": 27, "endOffset": 30}, {"referenceID": 16, "context": "7 Zoom-out [17] VOC extra 10k 64.", "startOffset": 11, "endOffset": 15}, {"referenceID": 15, "context": "4 FCN-8s [16] VOC extra 10k 62.", "startOffset": 9, "endOffset": 13}, {"referenceID": 19, "context": "2 SDS [20] VOC extra 10k 51.", "startOffset": 6, "endOffset": 10}, {"referenceID": 22, "context": "6 DeconvNet-CRF [23] VOC extra 10k 72.", "startOffset": 16, "endOffset": 20}, {"referenceID": 8, "context": "5 DeepLab-CRF [9] VOC extra 10k 66.", "startOffset": 14, "endOffset": 17}, {"referenceID": 8, "context": "4 DeepLab-MCL [9] VOC extra 10k 71.", "startOffset": 14, "endOffset": 17}, {"referenceID": 0, "context": "6 CRF-RNN [1] VOC extra 10k 72.", "startOffset": 10, "endOffset": 13}, {"referenceID": 23, "context": "0 DeepLab-CRF [24] VOC extra + COCO 133k 70.", "startOffset": 14, "endOffset": 18}, {"referenceID": 23, "context": "4 DeepLab-MCL [24] VOC extra + COCO 133k 72.", "startOffset": 14, "endOffset": 18}, {"referenceID": 17, "context": "7 BoxSup (semi) [18] VOC extra + COCO 133k 71.", "startOffset": 16, "endOffset": 20}, {"referenceID": 0, "context": "0 CRF-RNN [1] VOC extra + COCO 133k 74.", "startOffset": 10, "endOffset": 13}, {"referenceID": 20, "context": "4 IoU score, which is the so far best performance compared to the methods that use the same augmented VOC dataset [21] (marked as \u201cVOC extra\u201d in the table).", "startOffset": 114, "endOffset": 118}], "year": 2015, "abstractText": "Deep structured output learning shows great promise in tasks like semantic image segmentation. We proffer a new, efficient deep structured model learning scheme, in which we show how deep Convolutional Neural Networks (CNNs) can be used to directly estimate the messages in message passing inference for structured prediction with Conditional Random Fields (CRFs). With such CNN message estimators, we obviate the need to learn or evaluate potential functions for message calculation. This confers significant efficiency for learning, since otherwise when performing structured learning for a CRF with CNN potentials it is necessary to undertake expensive inference for every stochastic gradient iteration. The network output dimension of message estimators is the same as the number of classes, rather than exponentially growing in the order of the potentials. Hence it is more scalable for cases that a large number of classes are involved. We apply our method to semantic image segmentation and achieve impressive performance, which demonstrates the effectiveness and usefulness of our CNN message learning method. Appearing in Proc. The Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS), 2015, Montreal, Canada. This work was in part supported by Data to Decisions CRC Centre. The authors would like to thank NVIDIA for the donations of the K40 graphic cards. Correspondence should be addressed to C. Shen.", "creator": "LaTeX with hyperref package"}}}