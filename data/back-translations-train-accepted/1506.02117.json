{"id": "1506.02117", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2015", "title": "Learning Multiple Tasks with Deep Relationship Networks", "abstract": "Deep neural networks trained on large-scale dataset can learn transferable features that promote learning multiple tasks for inductive transfer and labeling mitigation. As deep features eventually transition from general to specific along the network, a fundamental problem is how to exploit the relationship structure across different tasks while accounting for the feature transferability in the task-specific layers. In this work, we propose a novel Deep Relationship Network (DRN) architecture for multi-task learning by discovering correlated tasks based on multiple task-specific layers of a deep convolutional neural network. DRN models the task relationship by imposing matrix normal priors over the network parameters of all task-specific layers, including higher feature layers and classifier layer that are not transferable safely. By jointly learning the transferable features and task relationships, DRN is able to alleviate the dilemma of negative-transfer in the feature layers and under-transfer in the classifier layer. Empirical evidence shows that DRN yields state-of-the-art classification results on standard multi-domain object recognition datasets.", "histories": [["v1", "Sat, 6 Jun 2015 04:38:48 GMT  (170kb)", "http://arxiv.org/abs/1506.02117v1", null], ["v2", "Thu, 16 Feb 2017 07:46:11 GMT  (398kb,D)", "http://arxiv.org/abs/1506.02117v2", null], ["v3", "Fri, 26 May 2017 00:27:14 GMT  (385kb,D)", "http://arxiv.org/abs/1506.02117v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mingsheng long", "jianmin wang", "philip s yu"], "accepted": true, "id": "1506.02117"}, "pdf": {"name": "1506.02117.pdf", "metadata": {"source": "CRF", "title": "Learning Multiple Tasks with Deep Relationship Networks", "authors": ["Mingsheng Long", "Jianmin Wang"], "emails": ["mingsheng@tsinghua.edu.cn,", "jimwang@tsinghua.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 150 6.02 117v 1 [cs.L G] 6J un"}, {"heading": "1 Introduction", "text": "In fact, we will be able to move to another world, where we can move to another world, where we can move to another world, where we can move to another world."}, {"heading": "2 Related Work", "text": "There are two categories of approaches to multi-task learning: multi-task learning, which requires a common presentation of the different tasks, and outlier task identification [2], and multi-task learning, which requires a common presentation of the tasks. [3] And identifying outlier types that are capable requires a differentiated approach."}, {"heading": "3 Deep Relationship Networks", "text": "In this thesis we learn several tasks by jointly modelling transferable characteristics and task relationships. In view of the learning tasks monitored by T with data {Xt, yt} Tt = 1, where Xt, yt, RD0 \u00b7 Nt and yt, yt, RNt are the design matrix and the caption vector of the t-th task, each drawn from the D0 dimensional attribute space and the C cardinality label space, i.e. from each training example xtn, RD0 and ytn, {1,..., C}. Our goal is to build a deep neural network for several tasks ytn = ft (x t n), which is able to learn transferable features and adaptive task relationships in order to bridge different tasks effectively and robustly."}, {"heading": "3.1 Model", "text": "In this sense, it is essential that both sides support each other when they feel able to support each other, independently of each other, when they are unable to compete against each other. It is with this principle that we draw up a concept that appeals to both sides equally."}, {"heading": "3.2 Algorithm", "text": "Since the optimization problem (4) is not convex in relation to the network parameters (4) and task covariances (4) in common, we adopt an alternate method that fixes a number of variables with the others. We first update W.t, which requires a reformulation of the objective (4) in relation to the objective (4). (4) We have discussed the task (4) in relation to the objective (4). (6) When discussing deep CNN (xtn), ytn) + L, the L0T (1). (4) We have discussed the task (4). (4) We have discussed the task (4). (4) We have discussed the task (4). (4) The task (4) is discussed."}, {"heading": "3.3 Discussion", "text": "We consider a fine-grained task relationship in which different tasks can be correlated in different ways for different categories. This can be beneficial if the training set for each class is relatively large, so that the class-by-class task relationship can be learned exactly, and the class-by-class distributions between the tasks are very different. This results in a DRN variant for class-by-class task relationships: min f, F, E, E, T, T, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W,"}, {"heading": "4 Experiments", "text": "We compare the DRN model with state-of-the-art multi-task learning methods for real object detection data sets and test the effectiveness of multi-layer and cross-class relationship learning. Unlike most methods that use multi-task data sets where each task is binary classification or univariate regression, we evaluate multi-task data sets where each task is multi-class classification."}, {"heading": "4.1 Setup", "text": "This dataset is the standard benchmark for multi-task learning and transfer learning. The office part [29] consists of 4,652 images in 31 categories collected from three different domains (tasks). This dataset is organized by selecting the 10 most common categories used by amazon.com (W) and DSLR (D). It consists of four different domains (tasks). This dataset is organized by selecting the office datasets and datasets of Caltech-256 (C) dataset (30), hence it of four domains (tasks).ImageCLEF-DA1 dataset is the benchmark for ImageCLEF domain adaptation challenge. It is organized by selecting the 12 common categories we share (tasks): Caltech-256 (tasks), ImageNet ILSVRC 2012 (I), Pascal VOC 2012, categories 12 and 12."}, {"heading": "4.2 Results and Discussion", "text": "The multiple task classification is derived from the Office-Caltech and ImageCLEF-DA multi-class data sets, based on 5%, 10% and 20% of sampled training data, as shown in Tables 1 and 2. We can observe that the proposed DRN model significantly exceeds the comparative methods for all multi-task comparisons, but the major performance boost confirms that our deep relationship networks are capable of learning both securely transferable characteristics and adaptive task relationships, creating more effective and robust multi-task solutions. We can make the following observations from the experimental results: Conventional flat multi-task methods MTFL, RMTL and MTRL generally exceed the single task solution method STSR, confirming the motivation of joint learning by learning multiple tasks together."}, {"heading": "4.3 Visualization Analysis", "text": "First, we show that DRN can learn adaptive task relationships with deep characteristics better than MTRL with flat characteristics by visualizing the covariance matrices of tasks learned by MTRL and DRN in Figures 2 (a) and 2 (b), respectively. Previous knowledge of the similarity of tasks in the Office Caltech dataset [29] indicates that tasks A, W, and D are more similar to each other while being significantly different from task C. DRN successfully captures this previous task relationship and further improves the task correlation between unequal tasks, creating greater portability for multitask learning. Furthermore, all tasks in the DRN are positively correlated (green color), implying that all tasks can better reinforce each other. However, some of the tasks (D and C) are still negatively correlated (red color) in MTRL, meaning that these tasks are far apart and cannot improve each other."}, {"heading": "5 Conclusion", "text": "We proposed a Deep Relationship Network (DRN) model that integrates standard neural networks with normal matrix priors using the network parameters of all task-specific layers. Priors define the covariance structure across tasks and enable inductive transfer across related tasks. We developed a learning algorithm that fine-tunes a pre-trained deep network and collectively learns transferable features and task relationships. Experiments show that the model achieves superior results with standard object recognition data sets."}], "references": [{"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["R.K. Ando", "T. Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Convex multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "A convex formulation for learning a shared predictive structure from multiple tasks", "author": ["J. Chen", "L. Tang", "J. Liu", "J. Ye"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Integrating low-rank and group-sparse structures for robust multi-task learning", "author": ["J. Chen", "J. Zhou", "J. Ye"], "venue": "In KDD,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "A probabilistic model for dirty multi-task feature selection", "author": ["D. Hern\u00e1ndez-Lobato", "J.M. Hern\u00e1ndez-Lobato", "Z. Ghahramani"], "venue": "In ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Clustered multi-task learning: A convex formulation", "author": ["L. Jacob", "J.-P. Vert", "F.R. Bach"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Learning with whom to share in multi-task feature learning", "author": ["Z. Kang", "K. Grauman", "F. Sha"], "venue": "In ICML,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Learning task grouping and overlap in multi-task learning", "author": ["A. Kumar", "H. Daume III"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Regularized multi-task learning", "author": ["T. Evgeniou", "M. Pontil"], "venue": "In KDD,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "A convex formulation for learning task relationships in multi-task learning", "author": ["Y. Zhang", "D.-Y. Yeung"], "venue": "In UAI,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Learning multiple tasks with a sparse matrix-normal penalty", "author": ["Y. Zhang", "J. Schneider"], "venue": "In NIPS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "A regularization approach to learning task relationships in multitask learning", "author": ["Y. Zhang", "D.-Y. Yeung"], "venue": "ACM Transactions on Knowledge Discovery from Data,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Convex learning of multiple tasks and their structure", "author": ["C. Ciliberto", "Y. Mroueh", "T. Poggio", "L. Rosasco"], "venue": "In ICML,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Discriminative transfer learning with tree-based priors", "author": ["N. Srivastava", "R. Salakhutdinov"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Multisource deep learning for human pose estimation", "author": ["W. Ouyang", "X. Chu", "X. Wang"], "venue": "In CVPR,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Facial landmark detection by deep multi-task learning", "author": ["Z. Zhang", "P. Luo", "C.C. Loy", "X. Tang"], "venue": "In ECCV,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "How transferable are features in deep neural networks", "author": ["J. Yosinski", "J. Clune", "Y. Bengio", "H. Lipson"], "venue": "In NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Learning transferable features with deep adaptation networks", "author": ["M. Long", "Y. Cao", "J. Wang", "M.I. Jordan"], "venue": "In ICML,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Matrix variate distributions", "author": ["A.K. Gupta", "D.K. Nagar"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2000}, {"title": "LSDA: Large scale detection through adaptation", "author": ["J. Hoffman", "S. Guadarrama", "E. Tzeng", "R. Hu", "J. Donahue", "R. Girshick", "T. Darrell", "K. Saenko"], "venue": "In NIPS,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "In ICML,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Multilingual acoustic models using distributed deep neural networks", "author": ["G. Heigold", "V. Vanhoucke", "A. Senior", "P. Nguyen", "M. Ranzato", "M. Devin", "J. Dean"], "venue": "In ICASSP,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "In ACM Multimedia,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Adapting visual category models to new domains", "author": ["K. Saenko", "B. Kulis", "M. Fritz", "T. Darrell"], "venue": "In ECCV,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Geodesic flow kernel for unsupervised domain adaptation", "author": ["B. Gong", "Y. Shi", "F. Sha", "K. Grauman"], "venue": "In CVPR,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Multi-task learning is based on the idea that the performance can be improved using related tasks as an inductive bias [1].", "startOffset": 119, "endOffset": 122}, {"referenceID": 1, "context": "This fundamental idea has motivated a variety of methods, including multi-task feature learning that learns a shared feature representation [2, 3, 4, 5, 6], and multi-task relationship learning that models the inherent task relationship [7, 8, 9, 10, 11, 12, 13, 14].", "startOffset": 140, "endOffset": 155}, {"referenceID": 2, "context": "This fundamental idea has motivated a variety of methods, including multi-task feature learning that learns a shared feature representation [2, 3, 4, 5, 6], and multi-task relationship learning that models the inherent task relationship [7, 8, 9, 10, 11, 12, 13, 14].", "startOffset": 140, "endOffset": 155}, {"referenceID": 3, "context": "This fundamental idea has motivated a variety of methods, including multi-task feature learning that learns a shared feature representation [2, 3, 4, 5, 6], and multi-task relationship learning that models the inherent task relationship [7, 8, 9, 10, 11, 12, 13, 14].", "startOffset": 140, "endOffset": 155}, {"referenceID": 4, "context": "This fundamental idea has motivated a variety of methods, including multi-task feature learning that learns a shared feature representation [2, 3, 4, 5, 6], and multi-task relationship learning that models the inherent task relationship [7, 8, 9, 10, 11, 12, 13, 14].", "startOffset": 140, "endOffset": 155}, {"referenceID": 5, "context": "This fundamental idea has motivated a variety of methods, including multi-task feature learning that learns a shared feature representation [2, 3, 4, 5, 6], and multi-task relationship learning that models the inherent task relationship [7, 8, 9, 10, 11, 12, 13, 14].", "startOffset": 140, "endOffset": 155}, {"referenceID": 6, "context": "This fundamental idea has motivated a variety of methods, including multi-task feature learning that learns a shared feature representation [2, 3, 4, 5, 6], and multi-task relationship learning that models the inherent task relationship [7, 8, 9, 10, 11, 12, 13, 14].", "startOffset": 237, "endOffset": 266}, {"referenceID": 7, "context": "This fundamental idea has motivated a variety of methods, including multi-task feature learning that learns a shared feature representation [2, 3, 4, 5, 6], and multi-task relationship learning that models the inherent task relationship [7, 8, 9, 10, 11, 12, 13, 14].", "startOffset": 237, "endOffset": 266}, {"referenceID": 8, "context": "This fundamental idea has motivated a variety of methods, including multi-task feature learning that learns a shared feature representation [2, 3, 4, 5, 6], and multi-task relationship learning that models the inherent task relationship [7, 8, 9, 10, 11, 12, 13, 14].", "startOffset": 237, "endOffset": 266}, {"referenceID": 9, "context": "This fundamental idea has motivated a variety of methods, including multi-task feature learning that learns a shared feature representation [2, 3, 4, 5, 6], and multi-task relationship learning that models the inherent task relationship [7, 8, 9, 10, 11, 12, 13, 14].", "startOffset": 237, "endOffset": 266}, {"referenceID": 10, "context": "This fundamental idea has motivated a variety of methods, including multi-task feature learning that learns a shared feature representation [2, 3, 4, 5, 6], and multi-task relationship learning that models the inherent task relationship [7, 8, 9, 10, 11, 12, 13, 14].", "startOffset": 237, "endOffset": 266}, {"referenceID": 11, "context": "This fundamental idea has motivated a variety of methods, including multi-task feature learning that learns a shared feature representation [2, 3, 4, 5, 6], and multi-task relationship learning that models the inherent task relationship [7, 8, 9, 10, 11, 12, 13, 14].", "startOffset": 237, "endOffset": 266}, {"referenceID": 12, "context": "This fundamental idea has motivated a variety of methods, including multi-task feature learning that learns a shared feature representation [2, 3, 4, 5, 6], and multi-task relationship learning that models the inherent task relationship [7, 8, 9, 10, 11, 12, 13, 14].", "startOffset": 237, "endOffset": 266}, {"referenceID": 13, "context": "This fundamental idea has motivated a variety of methods, including multi-task feature learning that learns a shared feature representation [2, 3, 4, 5, 6], and multi-task relationship learning that models the inherent task relationship [7, 8, 9, 10, 11, 12, 13, 14].", "startOffset": 237, "endOffset": 266}, {"referenceID": 14, "context": "This idea has motivated some latest deep learning methods for learning multiple tasks [15, 16, 17, 18], which learn a shared representation in the feature layers and multiple independent classifiers in the classifier layer but without inferring the task relationships.", "startOffset": 86, "endOffset": 102}, {"referenceID": 15, "context": "This idea has motivated some latest deep learning methods for learning multiple tasks [15, 16, 17, 18], which learn a shared representation in the feature layers and multiple independent classifiers in the classifier layer but without inferring the task relationships.", "startOffset": 86, "endOffset": 102}, {"referenceID": 16, "context": "This idea has motivated some latest deep learning methods for learning multiple tasks [15, 16, 17, 18], which learn a shared representation in the feature layers and multiple independent classifiers in the classifier layer but without inferring the task relationships.", "startOffset": 86, "endOffset": 102}, {"referenceID": 17, "context": "This idea has motivated some latest deep learning methods for learning multiple tasks [15, 16, 17, 18], which learn a shared representation in the feature layers and multiple independent classifiers in the classifier layer but without inferring the task relationships.", "startOffset": 86, "endOffset": 102}, {"referenceID": 18, "context": "in higher layers with increasing task discrepancy [19, 20], hence the sharing of all feature layers may be risky to negative-transfer.", "startOffset": 50, "endOffset": 58}, {"referenceID": 19, "context": "in higher layers with increasing task discrepancy [19, 20], hence the sharing of all feature layers may be risky to negative-transfer.", "startOffset": 50, "endOffset": 58}, {"referenceID": 20, "context": "DRN models the task relationship by imposing matrix normal priors [21] over network parameters of all task-specific layers, including higher feature layers and classifier layer that are not transferable safely.", "startOffset": 66, "endOffset": 70}, {"referenceID": 18, "context": "Since deep models pre-trained with large-scale repositories such as ImageNet are representative for general-purpose tasks [19, 22], the proposed DRN model is trained by fine-tuning from the AlexNet model pre-trained on ImageNet [23].", "startOffset": 122, "endOffset": 130}, {"referenceID": 21, "context": "Since deep models pre-trained with large-scale repositories such as ImageNet are representative for general-purpose tasks [19, 22], the proposed DRN model is trained by fine-tuning from the AlexNet model pre-trained on ImageNet [23].", "startOffset": 122, "endOffset": 130}, {"referenceID": 22, "context": "Since deep models pre-trained with large-scale repositories such as ImageNet are representative for general-purpose tasks [19, 22], the proposed DRN model is trained by fine-tuning from the AlexNet model pre-trained on ImageNet [23].", "startOffset": 228, "endOffset": 232}, {"referenceID": 0, "context": "2 Related Work Multi-task learning (MTL) [1] is an important learning paradigm that jointly learns multiple tasks by exploiting shared structural representation and improves generalization by using related tasks as an inductive bias.", "startOffset": 41, "endOffset": 44}, {"referenceID": 1, "context": "There are generally two categories of approaches to multi-task learning: multi-task feature learning, which learns a shared feature representation such that the dataset bias across different tasks can be reduced [2, 3, 4] or the outlier tasks can be identified [5, 6]; and multi-task relationship learning, which explicitly models the task relationship in the forms of task grouping [7, 8, 9] or task covariance [10, 11, 12, 13, 14].", "startOffset": 212, "endOffset": 221}, {"referenceID": 2, "context": "There are generally two categories of approaches to multi-task learning: multi-task feature learning, which learns a shared feature representation such that the dataset bias across different tasks can be reduced [2, 3, 4] or the outlier tasks can be identified [5, 6]; and multi-task relationship learning, which explicitly models the task relationship in the forms of task grouping [7, 8, 9] or task covariance [10, 11, 12, 13, 14].", "startOffset": 212, "endOffset": 221}, {"referenceID": 3, "context": "There are generally two categories of approaches to multi-task learning: multi-task feature learning, which learns a shared feature representation such that the dataset bias across different tasks can be reduced [2, 3, 4] or the outlier tasks can be identified [5, 6]; and multi-task relationship learning, which explicitly models the task relationship in the forms of task grouping [7, 8, 9] or task covariance [10, 11, 12, 13, 14].", "startOffset": 212, "endOffset": 221}, {"referenceID": 4, "context": "There are generally two categories of approaches to multi-task learning: multi-task feature learning, which learns a shared feature representation such that the dataset bias across different tasks can be reduced [2, 3, 4] or the outlier tasks can be identified [5, 6]; and multi-task relationship learning, which explicitly models the task relationship in the forms of task grouping [7, 8, 9] or task covariance [10, 11, 12, 13, 14].", "startOffset": 261, "endOffset": 267}, {"referenceID": 5, "context": "There are generally two categories of approaches to multi-task learning: multi-task feature learning, which learns a shared feature representation such that the dataset bias across different tasks can be reduced [2, 3, 4] or the outlier tasks can be identified [5, 6]; and multi-task relationship learning, which explicitly models the task relationship in the forms of task grouping [7, 8, 9] or task covariance [10, 11, 12, 13, 14].", "startOffset": 261, "endOffset": 267}, {"referenceID": 6, "context": "There are generally two categories of approaches to multi-task learning: multi-task feature learning, which learns a shared feature representation such that the dataset bias across different tasks can be reduced [2, 3, 4] or the outlier tasks can be identified [5, 6]; and multi-task relationship learning, which explicitly models the task relationship in the forms of task grouping [7, 8, 9] or task covariance [10, 11, 12, 13, 14].", "startOffset": 383, "endOffset": 392}, {"referenceID": 7, "context": "There are generally two categories of approaches to multi-task learning: multi-task feature learning, which learns a shared feature representation such that the dataset bias across different tasks can be reduced [2, 3, 4] or the outlier tasks can be identified [5, 6]; and multi-task relationship learning, which explicitly models the task relationship in the forms of task grouping [7, 8, 9] or task covariance [10, 11, 12, 13, 14].", "startOffset": 383, "endOffset": 392}, {"referenceID": 8, "context": "There are generally two categories of approaches to multi-task learning: multi-task feature learning, which learns a shared feature representation such that the dataset bias across different tasks can be reduced [2, 3, 4] or the outlier tasks can be identified [5, 6]; and multi-task relationship learning, which explicitly models the task relationship in the forms of task grouping [7, 8, 9] or task covariance [10, 11, 12, 13, 14].", "startOffset": 383, "endOffset": 392}, {"referenceID": 9, "context": "There are generally two categories of approaches to multi-task learning: multi-task feature learning, which learns a shared feature representation such that the dataset bias across different tasks can be reduced [2, 3, 4] or the outlier tasks can be identified [5, 6]; and multi-task relationship learning, which explicitly models the task relationship in the forms of task grouping [7, 8, 9] or task covariance [10, 11, 12, 13, 14].", "startOffset": 412, "endOffset": 432}, {"referenceID": 10, "context": "There are generally two categories of approaches to multi-task learning: multi-task feature learning, which learns a shared feature representation such that the dataset bias across different tasks can be reduced [2, 3, 4] or the outlier tasks can be identified [5, 6]; and multi-task relationship learning, which explicitly models the task relationship in the forms of task grouping [7, 8, 9] or task covariance [10, 11, 12, 13, 14].", "startOffset": 412, "endOffset": 432}, {"referenceID": 11, "context": "There are generally two categories of approaches to multi-task learning: multi-task feature learning, which learns a shared feature representation such that the dataset bias across different tasks can be reduced [2, 3, 4] or the outlier tasks can be identified [5, 6]; and multi-task relationship learning, which explicitly models the task relationship in the forms of task grouping [7, 8, 9] or task covariance [10, 11, 12, 13, 14].", "startOffset": 412, "endOffset": 432}, {"referenceID": 12, "context": "There are generally two categories of approaches to multi-task learning: multi-task feature learning, which learns a shared feature representation such that the dataset bias across different tasks can be reduced [2, 3, 4] or the outlier tasks can be identified [5, 6]; and multi-task relationship learning, which explicitly models the task relationship in the forms of task grouping [7, 8, 9] or task covariance [10, 11, 12, 13, 14].", "startOffset": 412, "endOffset": 432}, {"referenceID": 13, "context": "There are generally two categories of approaches to multi-task learning: multi-task feature learning, which learns a shared feature representation such that the dataset bias across different tasks can be reduced [2, 3, 4] or the outlier tasks can be identified [5, 6]; and multi-task relationship learning, which explicitly models the task relationship in the forms of task grouping [7, 8, 9] or task covariance [10, 11, 12, 13, 14].", "startOffset": 412, "endOffset": 432}, {"referenceID": 23, "context": "Deep neural networks learn nonlinear representations that disentangle and hide different explanatory factors of variation behind samples [24, 23].", "startOffset": 137, "endOffset": 145}, {"referenceID": 22, "context": "Deep neural networks learn nonlinear representations that disentangle and hide different explanatory factors of variation behind samples [24, 23].", "startOffset": 137, "endOffset": 145}, {"referenceID": 18, "context": "The learned deep representations can manifest invariant factors underlying different populations and are transferable across similar tasks [19].", "startOffset": 139, "endOffset": 143}, {"referenceID": 24, "context": "Hence, deep neural networks have been explored for domain adaptation [25, 20], multimodal learning [26, 27] and multi-task learning [15, 16, 17, 18], where significant performance gains have been obtained.", "startOffset": 69, "endOffset": 77}, {"referenceID": 19, "context": "Hence, deep neural networks have been explored for domain adaptation [25, 20], multimodal learning [26, 27] and multi-task learning [15, 16, 17, 18], where significant performance gains have been obtained.", "startOffset": 69, "endOffset": 77}, {"referenceID": 25, "context": "Hence, deep neural networks have been explored for domain adaptation [25, 20], multimodal learning [26, 27] and multi-task learning [15, 16, 17, 18], where significant performance gains have been obtained.", "startOffset": 99, "endOffset": 107}, {"referenceID": 26, "context": "Hence, deep neural networks have been explored for domain adaptation [25, 20], multimodal learning [26, 27] and multi-task learning [15, 16, 17, 18], where significant performance gains have been obtained.", "startOffset": 99, "endOffset": 107}, {"referenceID": 14, "context": "Hence, deep neural networks have been explored for domain adaptation [25, 20], multimodal learning [26, 27] and multi-task learning [15, 16, 17, 18], where significant performance gains have been obtained.", "startOffset": 132, "endOffset": 148}, {"referenceID": 15, "context": "Hence, deep neural networks have been explored for domain adaptation [25, 20], multimodal learning [26, 27] and multi-task learning [15, 16, 17, 18], where significant performance gains have been obtained.", "startOffset": 132, "endOffset": 148}, {"referenceID": 16, "context": "Hence, deep neural networks have been explored for domain adaptation [25, 20], multimodal learning [26, 27] and multi-task learning [15, 16, 17, 18], where significant performance gains have been obtained.", "startOffset": 132, "endOffset": 148}, {"referenceID": 17, "context": "Hence, deep neural networks have been explored for domain adaptation [25, 20], multimodal learning [26, 27] and multi-task learning [15, 16, 17, 18], where significant performance gains have been obtained.", "startOffset": 132, "endOffset": 148}, {"referenceID": 14, "context": "Based on the assumption that deep neural networks can learn transferable representations across different tasks, most prior multi-task deep learning methods for natural language processing [15] and computer vision [17, 18] learn a shared representation in the feature layers and multiple independent classifiers in the classifier layer without inferring the task relationships.", "startOffset": 189, "endOffset": 193}, {"referenceID": 16, "context": "Based on the assumption that deep neural networks can learn transferable representations across different tasks, most prior multi-task deep learning methods for natural language processing [15] and computer vision [17, 18] learn a shared representation in the feature layers and multiple independent classifiers in the classifier layer without inferring the task relationships.", "startOffset": 214, "endOffset": 222}, {"referenceID": 17, "context": "Based on the assumption that deep neural networks can learn transferable representations across different tasks, most prior multi-task deep learning methods for natural language processing [15] and computer vision [17, 18] learn a shared representation in the feature layers and multiple independent classifiers in the classifier layer without inferring the task relationships.", "startOffset": 214, "endOffset": 222}, {"referenceID": 15, "context": "[16] proposed tree-based priors to the weights in the classifier layer, which learns to organize the classes into a tree hierarchy and transfers knowledge to infrequent classes for label-scarcity mitigation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "While their method is specifically designed for multi-class classification instead of multi-task learning, the sharing of all feature layers may still be vulnerable to negative-transfer, as the higher layers of deep networks are tailored to fit specific tasks and may not be safely transferable [19].", "startOffset": 295, "endOffset": 299}, {"referenceID": 18, "context": "Since deep features eventually transition from general to specific along the network [19]: (1) convolutional layers conv1\u2013conv5 can learn transferable features, hence their parameters {F}l=1 are shared among the multiple tasks; (2) fully connected layers fc6\u2013fc8 are tailored to fit task-specific variations, hence their task-specific parameters {F t } 8 l=6 are jointly modeled via matrix normal priors for learning the task relationships.", "startOffset": 85, "endOffset": 89}, {"referenceID": 22, "context": "1 Model We start with the deep convolutional neural network (CNN) [23], a strong model to learn transferable features that are well adaptive to multiple tasks [18, 19, 22].", "startOffset": 66, "endOffset": 70}, {"referenceID": 17, "context": "1 Model We start with the deep convolutional neural network (CNN) [23], a strong model to learn transferable features that are well adaptive to multiple tasks [18, 19, 22].", "startOffset": 159, "endOffset": 171}, {"referenceID": 18, "context": "1 Model We start with the deep convolutional neural network (CNN) [23], a strong model to learn transferable features that are well adaptive to multiple tasks [18, 19, 22].", "startOffset": 159, "endOffset": 171}, {"referenceID": 21, "context": "1 Model We start with the deep convolutional neural network (CNN) [23], a strong model to learn transferable features that are well adaptive to multiple tasks [18, 19, 22].", "startOffset": 159, "endOffset": 171}, {"referenceID": 22, "context": "We extend the AlexNet architecture [23], which is comprised of five convolutional layers (conv1\u2013 conv5) and three fully connected layers (fc6\u2013fc8).", "startOffset": 35, "endOffset": 39}, {"referenceID": 18, "context": "We will not describe how to compute the convolutional layers as these layers can learn transferable features [19] and we will simply share their parameters {F t = F }l=1 across different tasks, without modeling the task relationships in these layers.", "startOffset": 109, "endOffset": 113}, {"referenceID": 18, "context": "To benefit from pre-training and fine-tuning, we copy these layers from a model pre-trained from ImageNet 2012 [19, 28], freeze conv1\u2013conv3 and fine-tune conv4\u2013conv5, which can preserve the efficacy of fragile co-adaptation.", "startOffset": 111, "endOffset": 119}, {"referenceID": 27, "context": "To benefit from pre-training and fine-tuning, we copy these layers from a model pre-trained from ImageNet 2012 [19, 28], freeze conv1\u2013conv3 and fine-tune conv4\u2013conv5, which can preserve the efficacy of fragile co-adaptation.", "startOffset": 111, "endOffset": 119}, {"referenceID": 18, "context": "As revealed by the latest literature findings [19], the deep features in standard CNNs must eventually transition from general to specific along the network, and the feature transferability decreases while the task discrepancy increases, making the features in higher layers fc6\u2013fc8 unsafely transferable across different tasks.", "startOffset": 46, "endOffset": 50}, {"referenceID": 14, "context": "Most previous methods generally assume that the multiple tasks are well correlated given the shared representation learned by the feature layers conv1\u2013fc7 of the deep neural network [15, 16, 17, 18].", "startOffset": 182, "endOffset": 198}, {"referenceID": 15, "context": "Most previous methods generally assume that the multiple tasks are well correlated given the shared representation learned by the feature layers conv1\u2013fc7 of the deep neural network [15, 16, 17, 18].", "startOffset": 182, "endOffset": 198}, {"referenceID": 16, "context": "Most previous methods generally assume that the multiple tasks are well correlated given the shared representation learned by the feature layers conv1\u2013fc7 of the deep neural network [15, 16, 17, 18].", "startOffset": 182, "endOffset": 198}, {"referenceID": 17, "context": "Most previous methods generally assume that the multiple tasks are well correlated given the shared representation learned by the feature layers conv1\u2013fc7 of the deep neural network [15, 16, 17, 18].", "startOffset": 182, "endOffset": 198}, {"referenceID": 23, "context": ", the network parameter of each layer is independent on the network parameters of the other layers, which is a common assumption made by most neural network methods [24].", "startOffset": 165, "endOffset": 169}, {"referenceID": 15, "context": "This parameter sharing strategy is a relaxation of existing methods [16, 17, 18], which share all lower-layers except for the classifier layer.", "startOffset": 68, "endOffset": 80}, {"referenceID": 16, "context": "This parameter sharing strategy is a relaxation of existing methods [16, 17, 18], which share all lower-layers except for the classifier layer.", "startOffset": 68, "endOffset": 80}, {"referenceID": 17, "context": "This parameter sharing strategy is a relaxation of existing methods [16, 17, 18], which share all lower-layers except for the classifier layer.", "startOffset": 68, "endOffset": 80}, {"referenceID": 18, "context": "We do not share task-specific layers (fc6\u2013fc8) so as to potentially mitigate the negative-transfer [19, 20].", "startOffset": 99, "endOffset": 107}, {"referenceID": 19, "context": "We do not share task-specific layers (fc6\u2013fc8) so as to potentially mitigate the negative-transfer [19, 20].", "startOffset": 99, "endOffset": 107}, {"referenceID": 20, "context": "In this paper, we define the following prior based on Gaussian and matrix normal distributions [21]: p ( W c )", "startOffset": 95, "endOffset": 99}, {"referenceID": 10, "context": "We take similar strategy as [11, 13] and minimize its convex upper bound \u2211L l=L0 (", "startOffset": 28, "endOffset": 36}, {"referenceID": 12, "context": "We take similar strategy as [11, 13] and minimize its convex upper bound \u2211L l=L0 (", "startOffset": 28, "endOffset": 36}, {"referenceID": 14, "context": "Our work contrasts from previous multi-task deep learning [15, 16, 17, 18] and relationship learning methods [11, 12, 13] in several important aspects: (1) We define the prior on multiple task-specific layers L0 6 l 6 L, while previous deep learning methods do not define the prior and merely rely on the bet that the shared deep features are good enough for multi-task learning (which may not be true due to Yosinski et al.", "startOffset": 58, "endOffset": 74}, {"referenceID": 15, "context": "Our work contrasts from previous multi-task deep learning [15, 16, 17, 18] and relationship learning methods [11, 12, 13] in several important aspects: (1) We define the prior on multiple task-specific layers L0 6 l 6 L, while previous deep learning methods do not define the prior and merely rely on the bet that the shared deep features are good enough for multi-task learning (which may not be true due to Yosinski et al.", "startOffset": 58, "endOffset": 74}, {"referenceID": 16, "context": "Our work contrasts from previous multi-task deep learning [15, 16, 17, 18] and relationship learning methods [11, 12, 13] in several important aspects: (1) We define the prior on multiple task-specific layers L0 6 l 6 L, while previous deep learning methods do not define the prior and merely rely on the bet that the shared deep features are good enough for multi-task learning (which may not be true due to Yosinski et al.", "startOffset": 58, "endOffset": 74}, {"referenceID": 17, "context": "Our work contrasts from previous multi-task deep learning [15, 16, 17, 18] and relationship learning methods [11, 12, 13] in several important aspects: (1) We define the prior on multiple task-specific layers L0 6 l 6 L, while previous deep learning methods do not define the prior and merely rely on the bet that the shared deep features are good enough for multi-task learning (which may not be true due to Yosinski et al.", "startOffset": 58, "endOffset": 74}, {"referenceID": 10, "context": "Our work contrasts from previous multi-task deep learning [15, 16, 17, 18] and relationship learning methods [11, 12, 13] in several important aspects: (1) We define the prior on multiple task-specific layers L0 6 l 6 L, while previous deep learning methods do not define the prior and merely rely on the bet that the shared deep features are good enough for multi-task learning (which may not be true due to Yosinski et al.", "startOffset": 109, "endOffset": 121}, {"referenceID": 11, "context": "Our work contrasts from previous multi-task deep learning [15, 16, 17, 18] and relationship learning methods [11, 12, 13] in several important aspects: (1) We define the prior on multiple task-specific layers L0 6 l 6 L, while previous deep learning methods do not define the prior and merely rely on the bet that the shared deep features are good enough for multi-task learning (which may not be true due to Yosinski et al.", "startOffset": 109, "endOffset": 121}, {"referenceID": 12, "context": "Our work contrasts from previous multi-task deep learning [15, 16, 17, 18] and relationship learning methods [11, 12, 13] in several important aspects: (1) We define the prior on multiple task-specific layers L0 6 l 6 L, while previous deep learning methods do not define the prior and merely rely on the bet that the shared deep features are good enough for multi-task learning (which may not be true due to Yosinski et al.", "startOffset": 109, "endOffset": 121}, {"referenceID": 18, "context": "[19]), and previous relationship learning methods are not designed in a deep architecture (which cannot learn transferable features for multi-task learning); (2) We define the prior by a task covariance matrix \u03a9 shared by all categories (which can learn the task relationship more accurately when category-wise data is scarce), while previous relationship learning methods define the prior for binary or regression problems; (3) We define the prior with a task mean parameter mc that captures the common task component of multiple tasks, while previous relationship learning methods define the prior with zero-mean Gaussian that cannot model the common task component.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "(7) Such a mini-batch SGD can be easily implemented via the Caffe framework for CNNs [28].", "startOffset": 85, "endOffset": 89}, {"referenceID": 18, "context": "Since training a deep CNN requires a large amount of labeled data, which is prohibitive for many multitask learning problems, we fine-tune from an AlexNet model pre-trained on ImageNet 2012 as [19].", "startOffset": 193, "endOffset": 197}, {"referenceID": 10, "context": "Though the derivation is similar to [11], our method learns the task relationship \u03a9 from multiple classes, while [11] learns the task relationship \u03a9 from binary classes.", "startOffset": 36, "endOffset": 40}, {"referenceID": 10, "context": "Though the derivation is similar to [11], our method learns the task relationship \u03a9 from multiple classes, while [11] learns the task relationship \u03a9 from binary classes.", "startOffset": 113, "endOffset": 117}, {"referenceID": 10, "context": "When the labeled data is very limited for each category, the binary-class method [11] may not infer the task relationship accurately.", "startOffset": 81, "endOffset": 85}, {"referenceID": 1, "context": "Learning shared features by exploiting feature covariance has been extensively studied for multitask feature learning [2, 3, 5, 4, 6].", "startOffset": 118, "endOffset": 133}, {"referenceID": 2, "context": "Learning shared features by exploiting feature covariance has been extensively studied for multitask feature learning [2, 3, 5, 4, 6].", "startOffset": 118, "endOffset": 133}, {"referenceID": 4, "context": "Learning shared features by exploiting feature covariance has been extensively studied for multitask feature learning [2, 3, 5, 4, 6].", "startOffset": 118, "endOffset": 133}, {"referenceID": 3, "context": "Learning shared features by exploiting feature covariance has been extensively studied for multitask feature learning [2, 3, 5, 4, 6].", "startOffset": 118, "endOffset": 133}, {"referenceID": 5, "context": "Learning shared features by exploiting feature covariance has been extensively studied for multitask feature learning [2, 3, 5, 4, 6].", "startOffset": 118, "endOffset": 133}, {"referenceID": 28, "context": "1 Setup Office-Caltech [29, 30] This dataset is the standard benchmark for multi-task learning and transfer learning.", "startOffset": 23, "endOffset": 31}, {"referenceID": 29, "context": "1 Setup Office-Caltech [29, 30] This dataset is the standard benchmark for multi-task learning and transfer learning.", "startOffset": 23, "endOffset": 31}, {"referenceID": 28, "context": "The Office part [29] consists of 4,652 images in 31 categories collected from three distinct domains (tasks): Amazon (A), which contains images downloaded from amazon.", "startOffset": 16, "endOffset": 20}, {"referenceID": 29, "context": "This dataset is organized by selecting the 10 common categories shared by the Office dataset and the Caltech-256 (C) dataset [30], hence it consists of four domains (tasks).", "startOffset": 125, "endOffset": 129}, {"referenceID": 2, "context": "We compare with a variety of methods: Single-Task Softmax Regression (STSR), Multi-Task Feature Learning (MTFL) [3], Robust Multi-Task Learning (RMTL) [5], Multi-Task Relationship Learning (MTRL) [11, 13], and Multi-Task Deep Convolutional Neural Network (MTCNN) [18].", "startOffset": 112, "endOffset": 115}, {"referenceID": 4, "context": "We compare with a variety of methods: Single-Task Softmax Regression (STSR), Multi-Task Feature Learning (MTFL) [3], Robust Multi-Task Learning (RMTL) [5], Multi-Task Relationship Learning (MTRL) [11, 13], and Multi-Task Deep Convolutional Neural Network (MTCNN) [18].", "startOffset": 151, "endOffset": 154}, {"referenceID": 10, "context": "We compare with a variety of methods: Single-Task Softmax Regression (STSR), Multi-Task Feature Learning (MTFL) [3], Robust Multi-Task Learning (RMTL) [5], Multi-Task Relationship Learning (MTRL) [11, 13], and Multi-Task Deep Convolutional Neural Network (MTCNN) [18].", "startOffset": 196, "endOffset": 204}, {"referenceID": 12, "context": "We compare with a variety of methods: Single-Task Softmax Regression (STSR), Multi-Task Feature Learning (MTFL) [3], Robust Multi-Task Learning (RMTL) [5], Multi-Task Relationship Learning (MTRL) [11, 13], and Multi-Task Deep Convolutional Neural Network (MTCNN) [18].", "startOffset": 196, "endOffset": 204}, {"referenceID": 17, "context": "We compare with a variety of methods: Single-Task Softmax Regression (STSR), Multi-Task Feature Learning (MTFL) [3], Robust Multi-Task Learning (RMTL) [5], Multi-Task Relationship Learning (MTRL) [11, 13], and Multi-Task Deep Convolutional Neural Network (MTCNN) [18].", "startOffset": 263, "endOffset": 267}, {"referenceID": 2, "context": "While using binary classifier for multi-class problems is less effective due to unbalanced classes, and inefficient for large number of categories, most existing methods [3, 5, 11] generally follow this paradigm.", "startOffset": 170, "endOffset": 180}, {"referenceID": 4, "context": "While using binary classifier for multi-class problems is less effective due to unbalanced classes, and inefficient for large number of categories, most existing methods [3, 5, 11] generally follow this paradigm.", "startOffset": 170, "endOffset": 180}, {"referenceID": 10, "context": "While using binary classifier for multi-class problems is less effective due to unbalanced classes, and inefficient for large number of categories, most existing methods [3, 5, 11] generally follow this paradigm.", "startOffset": 170, "endOffset": 180}, {"referenceID": 10, "context": "We mainly follow standard evaluation protocol for multi-task learning and randomly select 5%, 10%, and 20% samples from each task as the training set and utilize the rest of the samples as the test set [11, 13, 5].", "startOffset": 202, "endOffset": 213}, {"referenceID": 12, "context": "We mainly follow standard evaluation protocol for multi-task learning and randomly select 5%, 10%, and 20% samples from each task as the training set and utilize the rest of the samples as the test set [11, 13, 5].", "startOffset": 202, "endOffset": 213}, {"referenceID": 4, "context": "We mainly follow standard evaluation protocol for multi-task learning and randomly select 5%, 10%, and 20% samples from each task as the training set and utilize the rest of the samples as the test set [11, 13, 5].", "startOffset": 202, "endOffset": 213}, {"referenceID": 10, "context": "Table 1: Multi-class accuracy on the Office-Caltech dataset with standard evaluation protocol [11].", "startOffset": 94, "endOffset": 98}, {"referenceID": 10, "context": "Table 2: Multi-class accuracy on the ImageCLEF-DA dataset with standard evaluation protocol [11].", "startOffset": 92, "endOffset": 96}, {"referenceID": 2, "context": "We perform parameter selection for all methods using five-fold cross-validation on the training set, which is consistent with the baseline methods [3, 5, 11].", "startOffset": 147, "endOffset": 157}, {"referenceID": 4, "context": "We perform parameter selection for all methods using five-fold cross-validation on the training set, which is consistent with the baseline methods [3, 5, 11].", "startOffset": 147, "endOffset": 157}, {"referenceID": 10, "context": "We perform parameter selection for all methods using five-fold cross-validation on the training set, which is consistent with the baseline methods [3, 5, 11].", "startOffset": 147, "endOffset": 157}, {"referenceID": 18, "context": "For CNN-based methods, we adopt the fine-tuning architecture [19], however, due to limited training examples in our datasets, we fix convolutional layers conv1\u2013 conv3 that were copied from pre-trained model, fine-tune conv4\u2013conv5 and fully connected layers fc6\u2013fc7, and train classifier layer fc8, both via back propagation.", "startOffset": 61, "endOffset": 65}, {"referenceID": 27, "context": "9 momentum and the learning rate annealing strategy implemented in Caffe [28], and cross-validate the learning rate between 10 and 10 by a multiplicative step-size 10.", "startOffset": 73, "endOffset": 77}], "year": 2015, "abstractText": "Deep neural networks trained on large-scale dataset can learn transferable features that promote learning multiple tasks for inductive transfer and labeling mitigation. As deep features eventually transition from general to specific along the network, a fundamental problem is how to exploit the relationship structure across different tasks while accounting for the feature transferability in the task-specific layers. In this work, we propose a novel Deep Relationship Network (DRN) architecture for multi-task learning by discovering correlated tasks based on multiple task-specific layers of a deep convolutional neural network. DRN models the task relationship by imposing matrix normal priors over the network parameters of all task-specific layers, including higher feature layers and classifier layer that are not transferable safely. By jointly learning the transferable features and task relationships, DRN is able to alleviate the dilemma of negative-transfer in the feature layers and undertransfer in the classifier layer. Empirical evidence shows that DRN yields state-ofthe-art classification results on standard multi-domain object recognition datasets.", "creator": "LaTeX with hyperref package"}}}