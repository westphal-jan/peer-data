{"id": "1506.02142", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2015", "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning", "abstract": "Deep learning tools have recently gained much attention in applied machine learning. However such tools for regression and classification do not allow us to capture model uncertainty. Bayesian models offer us the ability to reason about model uncertainty, but usually come with a prohibitive computational cost.", "histories": [["v1", "Sat, 6 Jun 2015 12:30:43 GMT  (1715kb,D)", "http://arxiv.org/abs/1506.02142v1", "10 pages, 7 figures"], ["v2", "Thu, 27 Aug 2015 13:39:15 GMT  (2065kb,D)", "http://arxiv.org/abs/1506.02142v2", "10 pages, 7 figures"], ["v3", "Sun, 27 Sep 2015 15:15:31 GMT  (2068kb,D)", "http://arxiv.org/abs/1506.02142v3", "11 pages, 6 figures"], ["v4", "Sat, 31 Oct 2015 19:45:05 GMT  (2069kb,D)", "http://arxiv.org/abs/1506.02142v4", "11 pages, 6 figures; Minor corrections in experiments section"], ["v5", "Wed, 25 May 2016 18:48:52 GMT  (2384kb,D)", "http://arxiv.org/abs/1506.02142v5", "11 pages, 6 figures; ICML proceedings version"], ["v6", "Tue, 4 Oct 2016 16:50:26 GMT  (2383kb,D)", "http://arxiv.org/abs/1506.02142v6", "12 pages, 6 figures; fixed a mistake with standard error and added a new table with updated results (marked \"Update [October 2016]\"); Published in ICML 2016"]], "COMMENTS": "10 pages, 7 figures", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["yarin gal", "zoubin ghahramani"], "accepted": true, "id": "1506.02142"}, "pdf": {"name": "1506.02142.pdf", "metadata": {"source": "CRF", "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning", "authors": ["Yarin Gal", "Zoubin Ghahramani"], "emails": ["yg279@cam.ac.uk", "zg201@cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is so that most of us are able to survive themselves, and that they are able to survive themselves, \"he said.\" But it is not so that they are able to survive themselves. \"In the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the USA, in the USA, in the USA and in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century and in the third half of the 20th century, in the USA, in the USA and in the second half of the 20th century, in the USA and in the second half of the 20th century, in the USA, in the USA, in the USA, in the USA and in the USA, in the USA, in the USA, in the USA, in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA, in the USA in the USA, in the USA, in the USA and in the USA, in the USA in the USA, in the USA and in the USA, in the USA in the USA, in the USA, in the USA and in the USA in the USA and in the USA, in the USA in the USA, in the USA in the USA, in the USA in the USA, in the USA in the USA and in the USA in the USA, in the USA in the USA, in the USA in the USA, in the USA in the USA and in the USA in the USA, in the USA in the USA, in the USA, in the USA in the USA and in the USA in the USA and in the USA in the USA, in the USA, in the USA, in the USA in the USA, in the USA, in the USA in the USA and in the USA and in the USA, in the USA in the USA in the USA, in the USA in the USA, in the USA in the USA"}, {"heading": "2 Related Research", "text": "It has long been known that infinitely broad (single-layer) MLPs with distributions above their weights converge to Gaussian processes [15, 16]. This known relationship is based on a boundary argument that does not permit the transfer of properties from the Gaussian process to finite MLPs. Finite MLPs with distributions above the weights have been extensively studied as Bayesian neural networks [15, 17]. These provide robustness even for overfits, but with challenging conclusions and additional computational costs. Variation conclusions have been applied to these models, but with limited success [18, 19, 20]. Recent advances in variation reasoning have led to new techniques such as sample-based variation conclusions and stochastic variation conclusions [21, 22, 23, 24, 25] which have been used to obtain new approaches for Bayesian neural networks that are as powerful as failure rates [26]."}, {"heading": "3 Dropout as a Bayesian Approximation", "text": "We show that a multi-layer perceptron (MLP) with arbitrary depth and non-linearity applied after each weight layer is mathematically equivalent to an approximation of the probable deep Gaussian process model. [27] We would like to emphasize that no simplistic assumptions are made about the use of failure layers in the literature, and that the results derived from this are applicable to any network architecture that makes the use of failure layers exactly as it appears in practical applications. We show that the failure lens is the Kullback-Leibler divergence between an approximate model and the deep Gaussian process."}, {"heading": "4 Obtaining Model Uncertainty", "text": "Next, we derive new results extending to the above and show that the model uncertainty can be estimated from the dropout model MLP = forward q. 1The diag (\u00b7) operator maps a vector to a diagonal matrix whose diagonal is the elements of the vector. 2We derive the case for regression where y-RD, but the extension to classification is simple. This is givenin section 3.5 in the appendix. According to section 2.3 in the appendix, our approximate predictable distribution is given by q (y-x), where y-RD (y-RD, x-RD) q (n-RD) q (n-RD), in which II-Li = 1 applies our series of random variables to a model with L layers. We will perform moment matching and empirically estimate the first two moments of the predictive distribution. Specifically, we will amplify sets of vectors from the real Bernoullations."}, {"heading": "5 Experiments", "text": "Next, we will perform a comprehensive evaluation of the properties of uncertainty estimates derived from failing MLPs and conventions regarding the tasks of regression and classification. Finally, we will compare the uncertainty derived from various model architectures and nonlinearity, both in terms of the tasks of interpolation and extrapolation. We will show that the uncertainty of the model for classification tasks is important using the example of MNIST [29]. Finally, we will give an example of the use of the uncertainty of the model in a Bayesian pipeline. We will give a quantitative evaluation of the model's performance in determining amplification data using a task similar to learning from deep amplification work [14]. Using the results from the previous section, we will begin with the qualitative evaluation of the uncertainty of the failing MLP in relation to two regression tasks. We will use two regression datasets and modelling functions that are easy to visualize."}, {"heading": "5.1 Model Uncertainty in Regression Tasks \u2013 Extrapolation", "text": "We have several models for the CO2 dataset. We use MLPs with either 4 or 5 hidden layers and 1024 hidden units. We use either ReLU without linearity or TanH without linearity in each network, and use dropout probabilities in both areas. We have a stochastic gradient as an optimizer for 1,000,000 iterations (up to conformity) with a learning rate of 2,000 iterators."}, {"heading": "5.2 Model Uncertainty in Regression Tasks \u2013 Interpolation", "text": "For interpolation, we repeat the above experiment with ReLU networks with 5 hidden layers and the same structure on a new dataset - solar radiation. We use the base learning rate of 5e \u2212 3 and the weight decay of 5e \u2212 7. The results of the interpolation are in Fig. 3. Fig. 3a shows the interpolation of missing sections (delimited between pairs of dashed blue lines) for the Gaussian process with square exponential covariance function and the function value on the training set. In red is the observed function, in green is the missing sections and in blue is the model mean. Fig. The observed uncertainty in the MC failure model is similar to the prediction of [33]."}, {"heading": "5.3 Model Uncertainty in Classification Tasks \u2013 MNIST classification", "text": "To evaluate the classification of the model into a real example, we tested a Convolutionary Neural Network trained on the MNIST dataset. We trained the Convolutionary Neural Network model with a dropout applied after the fully connected inner product layer (after ReLU operation). We used Caffe [34] reference implementation for this experiment. We evaluated the trained model on a continuously rotating figure 1 image (shown on the X-axis of fig. 5) with the same learning rate policy as before. We dispersed 100 forward steps of Softmax input (the output from the last fully connected layer, fig. 5a), as well as the softmax output of Softmax output for each class."}, {"heading": "5.4 Model Uncertainty in Reinforcement Learning", "text": "In Reinforcement, an agent learns different rewards from different states, and his goal is to maximize his expected reward over time. The agent tries to learn to avoid the transition to states with low rewards, and to take measures that instead lead to better states. Uncertainty is of great importance in this task - with uncertainty information that an agent can decide when to exploit rewards, and when to explore his environment. Recent advances in RL have used the use of MLPs to estimate the Q-value functions of agents (called Q networks), a feature that can exploit the quality of the various actions of the agent in different states. This has led to impressive results on Atari game simulations, in which agents have displaced human performance in a variety of games."}, {"heading": "6 Conclusions and Future Research", "text": "We have studied the characteristics of this uncertainty in detail and identified possible applications of combining Bayesian models and deep learning models with a reinforcement learning example. Developments allow us to treat existing models in a new way without having to introduce additional parameters or computational loads in order to obtain uncertainty estimates. In future research, we aim to assess the uncertainty of the model on the basis of contradictory inputs, such as corrupt images that almost certainly misclassify [36]. Adding or subtracting a single pixel from each input dimension is perceived as virtually unchanged input to the human eye, but can significantly alter the classification probabilities. In the high-dimensional input space, the new corrupt image is far removed from the data, and one would expect the uncertainty of the model for such inputs to increase. We compared the failure uncertainty with that of the Gaussian process model on a variety of tasks that we are well aware of in other work with GP approximations. 33"}, {"heading": "Acknowledgements", "text": "The authors thank Dr. Yutian Chen, Mr. Christof Angermueller, Mr. Roger Frigola, Mr. Rowan McAllister, Dr. Gabriel Synnaeve, Mr. Mark van der Wilk and Mr. Yan Wu for their helpful comments. Yarin Gal is supported by the Google European Fellowship in Machine Learning."}], "references": [{"title": "Searching for exotic particles in high-energy physics with deep learning", "author": ["P Baldi", "P Sadowski", "D Whiteson"], "venue": "Nature communications, 5", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "J Mart\u0131\u0301nez", "author": ["O Anjos", "C Iglesias", "F Peres"], "venue": "\u00c1 Garc\u0131\u0301a, and J Taboada. Neural networks applied to discriminate botanical origin of honeys. Food chemistry, 175:128\u2013136", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "On the use of artificial neural networks in simulation-based manufacturing control", "author": ["S Bergmann", "S Stelzer", "S Strassburger"], "venue": "Journal of Simulation, 8(1):76\u201390", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Points of significance: Importance of being uncertain", "author": ["M Krzywinski", "N Altman"], "venue": "Nature methods, 10 (9)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Probabilistic machine learning and artificial intelligence", "author": ["Z Ghahramani"], "venue": "Nature, 521(7553)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Experimental biology: Sometimes Bayesian statistics are better", "author": ["S Herzog", "D Ostwald"], "venue": "Nature, 494", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Editorial", "author": ["D Trafimow", "M Marks"], "venue": "Basic and Applied Social Psychology, 37(1)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural network based intrusion detection system for critical infrastructures", "author": ["O Linda", "T Vollmer", "M Manic"], "venue": "Neural Networks, 2009. IJCNN 2009. International Joint Conference on. IEEE", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Algorithms for reinforcement learning", "author": ["C Szepesv\u00e1ri"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning, 4(1)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W R Thompson"], "venue": "Biometrika", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1933}, {"title": "Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)", "author": ["C E Rasmussen", "C K I Williams"], "venue": "The MIT Press", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N Srivastava", "G Hinton", "A Krizhevsky", "I Sutskever", "R Salakhutdinov"], "venue": "The Journal of Machine Learning Research, 15(1)", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "et al", "author": ["V Mnih", "K Kavukcuoglu", "D Silver", "A A Rusu", "J Veness"], "venue": "Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Bayesian learning for neural networks", "author": ["R M Neal"], "venue": "PhD thesis, University of Toronto", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1995}, {"title": "Computing with infinite networks", "author": ["C K I Williams"], "venue": "NIPS", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1997}, {"title": "A practical Bayesian framework for backpropagation networks", "author": ["D J C MacKay"], "venue": "Neural computation, 4 (3)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1992}, {"title": "Keeping the neural networks simple by minimizing the description length of the weights", "author": ["G E Hinton", "D Van Camp"], "venue": "Proceedings of the sixth annual conference on Computational learning theory", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1993}, {"title": "Ensemble learning in Bayesian neural networks", "author": ["D Barber", "C M Bishop"], "venue": "NATO ASI SERIES F COMPUTER AND SYSTEMS SCIENCES, 168:215\u2013238", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1998}, {"title": "Practical variational inference for neural networks", "author": ["A Graves"], "venue": "NIPS", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Variational Bayesian inference with stochastic search", "author": ["D M Blei", "M I Jordan", "J W Paisley"], "venue": "ICML", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Auto-encoding variational Bayes", "author": ["D P Kingma", "M Welling"], "venue": "arXiv preprint arXiv:1312.6114", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D J Rezende", "S Mohamed", "D Wierstra"], "venue": "ICML", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Doubly stochastic variational Bayes for non-conjugate inference", "author": ["M Titsias", "M L\u00e1zaro-Gredilla"], "venue": "ICML", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Stochastic variational inference", "author": ["M D Hoffman", "D M Blei", "C Wang", "J Paisley"], "venue": "The Journal of Machine Learning Research, 14(1):1303\u20131347", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Weight uncertainty in neural networks", "author": ["C Blundell", "J Cornebise", "K Kavukcuoglu", "D Wierstra"], "venue": "ICML", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep Gaussian processes", "author": ["A Damianou", "N Lawrence"], "venue": "AISTATS", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Marginalized kernels for biological sequences", "author": ["K Tsuda", "T Kin", "K Asai"], "venue": "Bioinformatics, 18", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2002}, {"title": "The mnist database of handwritten digits", "author": ["Y LeCun", "C Cortes"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1998}, {"title": "and the Carbon Dioxide Research Group", "author": ["C D Keeling", "T P Whorf"], "venue": "Atmospheric CO2 concentrations (ppmv) derived from in situ air samples collected at Mauna Loa Observatory, Hawaii", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2004}, {"title": "Solar irradiance reconstruction", "author": ["J Lean"], "venue": "NOAA/NGDC Paleoclimatology Program, USA", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2004}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y LeCun", "L Bottou", "Y Bengio", "P Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1998}, {"title": "Improving the Gaussian process sparse spectrum approximation by representing uncertainty in frequency inputs", "author": ["Y Gal", "R Turner"], "venue": "ICML", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y Jia", "E Shelhamer", "J Donahue", "S Karayev", "J Long", "R Girshick", "S Guadarrama", "T Darrell"], "venue": "arXiv preprint arXiv:1408.5093", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Intriguing properties of neural networks", "author": ["C Szegedy", "W Zaremba", "I Sutskever", "J Bruna", "D Erhan", "I Goodfellow", "R Fergus"], "venue": "ICLR", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Research in fields such as physics, biology, and manufacturing\u2014to name a few\u2014is using deep learning tools now more than ever [1, 2, 3].", "startOffset": 125, "endOffset": 134}, {"referenceID": 1, "context": "Research in fields such as physics, biology, and manufacturing\u2014to name a few\u2014is using deep learning tools now more than ever [1, 2, 3].", "startOffset": 125, "endOffset": 134}, {"referenceID": 2, "context": "Research in fields such as physics, biology, and manufacturing\u2014to name a few\u2014is using deep learning tools now more than ever [1, 2, 3].", "startOffset": 125, "endOffset": 134}, {"referenceID": 3, "context": "However, these are fields in which representing model uncertainty is of crucial importance [4, 5].", "startOffset": 91, "endOffset": 97}, {"referenceID": 4, "context": "However, these are fields in which representing model uncertainty is of crucial importance [4, 5].", "startOffset": 91, "endOffset": 97}, {"referenceID": 5, "context": "Furthermore, there has been a recent shift in many of these fields towards the use of Bayesian uncertainty [6, 7, 8].", "startOffset": 107, "endOffset": 116}, {"referenceID": 6, "context": "Furthermore, there has been a recent shift in many of these fields towards the use of Bayesian uncertainty [6, 7, 8].", "startOffset": 107, "endOffset": 116}, {"referenceID": 7, "context": "This can happen in a post office, sorting letters according to their zip code, or in a nuclear power plant with a system responsible for critical infrastructure [9].", "startOffset": 161, "endOffset": 164}, {"referenceID": 8, "context": "Uncertainty is important in reinforcement learning (RL) as well [10].", "startOffset": 64, "endOffset": 68}, {"referenceID": 9, "context": "With uncertainty estimates over the agent\u2019s Q-value function, techniques such as Thompson sampling [11] can be used to learn much faster.", "startOffset": 99, "endOffset": 103}, {"referenceID": 10, "context": "We show that the use of dropout in MLPs can be interpreted as a Bayesian approximation of a well known Bayesian model: the Gaussian process (GP) [12].", "startOffset": 145, "endOffset": 149}, {"referenceID": 11, "context": "Dropout is used in almost all models in deep learning as a way to avoid over-fitting [13].", "startOffset": 85, "endOffset": 89}, {"referenceID": 12, "context": "Lastly, we give a quantitative assessment of model uncertainty in the setting of reinforcement learning, on a task similar to that used in deep reinforcement learning [14].", "startOffset": 167, "endOffset": 171}, {"referenceID": 13, "context": "It has long been known that infinitely wide (single hidden layer) MLPs with distributions placed over their weights converge to Gaussian processes [15, 16].", "startOffset": 147, "endOffset": 155}, {"referenceID": 14, "context": "It has long been known that infinitely wide (single hidden layer) MLPs with distributions placed over their weights converge to Gaussian processes [15, 16].", "startOffset": 147, "endOffset": 155}, {"referenceID": 13, "context": "Finite MLPs with distributions placed over the weights have been studied extensively as Bayesian neural networks [15, 17].", "startOffset": 113, "endOffset": 121}, {"referenceID": 15, "context": "Finite MLPs with distributions placed over the weights have been studied extensively as Bayesian neural networks [15, 17].", "startOffset": 113, "endOffset": 121}, {"referenceID": 16, "context": "Variational inference has been applied to these models, but with limited success [18, 19, 20].", "startOffset": 81, "endOffset": 93}, {"referenceID": 17, "context": "Variational inference has been applied to these models, but with limited success [18, 19, 20].", "startOffset": 81, "endOffset": 93}, {"referenceID": 18, "context": "Variational inference has been applied to these models, but with limited success [18, 19, 20].", "startOffset": 81, "endOffset": 93}, {"referenceID": 19, "context": "Recent advances in variational inference introduced new techniques such as sampling-based variational inference and stochastic variational inference [21, 22, 23, 24, 25].", "startOffset": 149, "endOffset": 169}, {"referenceID": 20, "context": "Recent advances in variational inference introduced new techniques such as sampling-based variational inference and stochastic variational inference [21, 22, 23, 24, 25].", "startOffset": 149, "endOffset": 169}, {"referenceID": 21, "context": "Recent advances in variational inference introduced new techniques such as sampling-based variational inference and stochastic variational inference [21, 22, 23, 24, 25].", "startOffset": 149, "endOffset": 169}, {"referenceID": 22, "context": "Recent advances in variational inference introduced new techniques such as sampling-based variational inference and stochastic variational inference [21, 22, 23, 24, 25].", "startOffset": 149, "endOffset": 169}, {"referenceID": 23, "context": "Recent advances in variational inference introduced new techniques such as sampling-based variational inference and stochastic variational inference [21, 22, 23, 24, 25].", "startOffset": 149, "endOffset": 169}, {"referenceID": 24, "context": "These have been used to obtain new approximations for Bayesian neural networks that perform as well as dropout [26].", "startOffset": 111, "endOffset": 115}, {"referenceID": 25, "context": "We show that a multilayer perceptron (MLP) with arbitrary depth and non-linearities, with dropout applied after every weight layer, is mathematically equivalent to an approximation to the probabilistic deep Gaussian process model [27].", "startOffset": 230, "endOffset": 234}, {"referenceID": 26, "context": "It is straightforward to show that K(x,y) is a valid PSD covariance function \u2013 it is an example of a marginalised covariance function [28].", "startOffset": 134, "endOffset": 138}, {"referenceID": 27, "context": "We show that model uncertainty is important for classification tasks using MNIST [29] as an example.", "startOffset": 81, "endOffset": 85}, {"referenceID": 12, "context": "We give a quantitative assessment of the model\u2019s performance in the setting of reinforcement learning on a task similar to that used in deep reinforcement learning [14].", "startOffset": 164, "endOffset": 168}, {"referenceID": 28, "context": "We use a subset of the atmospheric CO2 concentrations dataset derived from in situ air samples collected at Mauna Loa Observatory, Hawaii (referred to as CO2) [30] to evaluate model extrapolation, and the reconstructed solar irradiance dataset (referred to as solar) [31] to assess model", "startOffset": 159, "endOffset": 163}, {"referenceID": 29, "context": "We use a subset of the atmospheric CO2 concentrations dataset derived from in situ air samples collected at Mauna Loa Observatory, Hawaii (referred to as CO2) [30] to evaluate model extrapolation, and the reconstructed solar irradiance dataset (referred to as solar) [31] to assess model", "startOffset": 267, "endOffset": 271}, {"referenceID": 30, "context": "For the task of classification we evaluate the LeNet convolutional neural network model [32] on the MNIST dataset [29].", "startOffset": 88, "endOffset": 92}, {"referenceID": 27, "context": "For the task of classification we evaluate the LeNet convolutional neural network model [32] on the MNIST dataset [29].", "startOffset": 114, "endOffset": 118}, {"referenceID": 31, "context": "The observed uncertainty in MC dropout is similar to that of [33].", "startOffset": 61, "endOffset": 65}, {"referenceID": 32, "context": "We used Caffe [34] reference implementation for this experiment.", "startOffset": 14, "endOffset": 18}, {"referenceID": 0, "context": "For the 12 images, the model predicts classes [1 1 1 1 1 5 5 7 7 7 7 7].", "startOffset": 46, "endOffset": 71}, {"referenceID": 0, "context": "For the 12 images, the model predicts classes [1 1 1 1 1 5 5 7 7 7 7 7].", "startOffset": 46, "endOffset": 71}, {"referenceID": 0, "context": "For the 12 images, the model predicts classes [1 1 1 1 1 5 5 7 7 7 7 7].", "startOffset": 46, "endOffset": 71}, {"referenceID": 0, "context": "For the 12 images, the model predicts classes [1 1 1 1 1 5 5 7 7 7 7 7].", "startOffset": 46, "endOffset": 71}, {"referenceID": 0, "context": "For the 12 images, the model predicts classes [1 1 1 1 1 5 5 7 7 7 7 7].", "startOffset": 46, "endOffset": 71}, {"referenceID": 4, "context": "For the 12 images, the model predicts classes [1 1 1 1 1 5 5 7 7 7 7 7].", "startOffset": 46, "endOffset": 71}, {"referenceID": 4, "context": "For the 12 images, the model predicts classes [1 1 1 1 1 5 5 7 7 7 7 7].", "startOffset": 46, "endOffset": 71}, {"referenceID": 6, "context": "For the 12 images, the model predicts classes [1 1 1 1 1 5 5 7 7 7 7 7].", "startOffset": 46, "endOffset": 71}, {"referenceID": 6, "context": "For the 12 images, the model predicts classes [1 1 1 1 1 5 5 7 7 7 7 7].", "startOffset": 46, "endOffset": 71}, {"referenceID": 6, "context": "For the 12 images, the model predicts classes [1 1 1 1 1 5 5 7 7 7 7 7].", "startOffset": 46, "endOffset": 71}, {"referenceID": 6, "context": "For the 12 images, the model predicts classes [1 1 1 1 1 5 5 7 7 7 7 7].", "startOffset": 46, "endOffset": 71}, {"referenceID": 6, "context": "For the 12 images, the model predicts classes [1 1 1 1 1 5 5 7 7 7 7 7].", "startOffset": 46, "endOffset": 71}, {"referenceID": 12, "context": "This has led to impressive results on Atari game simulations, where agents superseded human performance on a variety of games [14].", "startOffset": 126, "endOffset": 130}, {"referenceID": 9, "context": "With our uncertainty estimates given by a dropout Q-network we can use techniques such as Thompson sampling [11] to converge faster than epsilon greedy while avoiding over-fitting.", "startOffset": 108, "endOffset": 112}, {"referenceID": 12, "context": "We use code by [35] that replicated the results by [14] with a simpler 2D setting.", "startOffset": 51, "endOffset": 55}, {"referenceID": 33, "context": "In future research we aim to assess model uncertainty on adversarial inputs, such as corrupted images that classify incorrectly with high confidence [36].", "startOffset": 149, "endOffset": 153}, {"referenceID": 31, "context": "We compared the dropout uncertainty to that of the Gaussian process model on a variety of tasks on which other GP approximations have been compared as well [33].", "startOffset": 156, "endOffset": 160}, {"referenceID": 24, "context": "In future work we aim to compare the uncertainty of the model to that of Bayesian neural networks such as [26] as well.", "startOffset": 106, "endOffset": 110}], "year": 2015, "abstractText": "Deep learning tools have recently gained much attention in applied machine learning. However such tools for regression and classification do not allow us to capture model uncertainty. Bayesian models offer us the ability to reason about model uncertainty, but usually come with a prohibitive computational cost. We show that dropout in multilayer perceptron models (MLPs) can be interpreted as a Bayesian approximation. Results are obtained for modelling uncertainty for dropout MLP models \u2013 extracting information that has been thrown away so far, from existing models. This mitigates the problem of representing uncertainty in deep learning without sacrificing computational performance or test accuracy. We perform an exploratory study of the dropout uncertainty properties. Various network architectures and non-linearities are assessed on tasks of extrapolation, interpolation, and classification. We show that model uncertainty is important for classification tasks using MNIST as an example, and use the model\u2019s uncertainty in a Bayesian pipeline, with deep reinforcement learning as a concrete example.", "creator": "LaTeX with hyperref package"}}}