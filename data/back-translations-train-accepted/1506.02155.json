{"id": "1506.02155", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2015", "title": "Optimal Rates for Random Fourier Features", "abstract": "Kernel methods represent one of the most powerful tools in machine learning to tackle problems expressed in terms of function values and derivatives due to their capability to represent and model complex relations. While these methods show good versatility, they are computationally intensive and have poor scalability to large data as they require operations on Gram matrices. In order to mitigate this serious computational limitation, recently randomized constructions have been proposed in the literature, which allow the application of fast linear algorithms. Random Fourier features (RFF) are among the most popular and widely applied constructions: they provide an easily computable, low-dimensional feature representation for shift-invariant kernels. Despite the popularity of RFFs, very little is understood theoretically about their approximation quality. In this paper, we provide the first detailed theoretical analysis about the approximation quality of RFFs by establishing optimal (in terms of the RFF dimension) performance guarantees in uniform and $L^r$ ($1\\le r&lt;\\infty$) norms. We also propose a RFF approximation to derivatives of a kernel with a theoretical study on its approximation quality.", "histories": [["v1", "Sat, 6 Jun 2015 14:37:01 GMT  (25kb)", "http://arxiv.org/abs/1506.02155v1", null], ["v2", "Wed, 4 Nov 2015 22:58:57 GMT  (33kb)", "http://arxiv.org/abs/1506.02155v2", "To appear at NIPS-2015"]], "reviews": [], "SUBJECTS": "math.ST cs.LG math.FA stat.ML stat.TH", "authors": ["bharath k sriperumbudur", "zolt\u00e1n szab\u00f3 0001"], "accepted": true, "id": "1506.02155"}, "pdf": {"name": "1506.02155.pdf", "metadata": {"source": "CRF", "title": "Optimal Rates for Random Fourier Features", "authors": ["Bharath K. Sriperumbudur", "Zolt\u00e1n Szab\u00f3"], "emails": ["bks18@psu.edu", "zoltan.szabo@gatsby.ucl.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 150 6.02 155v 1 [mat h.ST]"}, {"heading": "1 Introduction", "text": "[19] have achieved tremendous success in solving various fundamental problems of machine learning, ranging from classification, regression, feature extraction, dependency estimation, causal discovery, Bayesian inference, and hypothesis testing. [16] Such success is due to their ability to depict and model complex relationships, but with high (possibly infinite) dimensional characteristics. [16] At the core of all these techniques is the kernel trick, which implicitly allows internal products between these high-dimensional characteristic maps to be calculated via a core function k: k (x, y) = < \u03bb (x), \u03bb (y), but this flexibility and richness of the kernels comes at a cost: by drawing implicit compressions on the gram matrix of the data, which poses serious compressing challenges in dealing with large-scale data."}, {"heading": "2 Notations & Preliminaries", "text": "In this section, we present notations used throughout the work, and then present the preliminary measurement based on random character maps introduced by [16]. (For a topological space, space is defined by all continuous (or limited) functions on X. (F) Definitions and notation: For a topological space, the space of f (X) is the overarching norm of f (we also write it as \"f\" X). Mb (X) and M1 + (X) is the set of all finite (X) measurements on X, or the probability on X. (F) The overarching norm of f (X, \u00b5) denotes the space of r-power (r). (R)"}, {"heading": "3 Main results: Approximation of k", "text": "As discussed in sections 1 and 2, while the random approximation of k values by [16] has many practical advantages, it does not seem to be well understood in theory. (see theorem 1) Existing theoretical results on the quality of the approximation do not provide a complete picture due to its non-optimality. (see theorem 1) In this section, we first present our main result (see theorem 2), which improves to (4) and provides a rate of m-1 / 2 with logarithmic dependence on | S | 2.) Then we discuss the consequences of theorem 1 along with its optimality in Remark 1. Next, in corollary 2 and theorem 3, we discuss the Lr convergence (1) of k to k with compact subsets of Rd.Theorem 1. Suppose k (x, y) = the ratio (x \u2212 y), y, y, Rd, in which Rd is defined positively (Rd)."}, {"heading": "4 Approximation of kernel derivatives", "text": "In this section, we propose an approximation of the derivatives of the kernel and analyze the derivative machine tasks that contain the derivatives of the proposed approximation (e.g.: [32, 21, 18, 23]. For this purpose, we consider k to be in (1) and define ha: = cos (2 +), a), a), a), b), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c). \""}, {"heading": "5 Discussion", "text": "In this paper, we presented the first detailed theoretical analysis of the approximate quality of random Fourier characteristics (RFF) proposed by [16] in connection with improving the computational complexity of nuclear machines. [16] While we set a probable limit for the uniform approximation (over compact subsets of Rd) of a core by random characteristics, the result is not optimal. We improved this result by providing a finite sample with optimal approximation rate and Lr norms over compact subsets of Rd.While all results in this paper (and also in the literature) dealt with the approximation quality of RFF for derivatives of a core and theoretical guarantees for the quality of approximation in uniform and Lr norms over compact subsets of Rd.While all results in this paper (and also in the literature) dealt with the approximation quality of RFF for RFF subsets of Rd, it is of interest to understand their overall behavior over approximation size."}, {"heading": "Acknowledgments", "text": "This work was supported by the Gatsby Charitable Foundation."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Kernel methods represent one of the most powerful tools in machine learning to tackle problems<lb>expressed in terms of function values and derivatives due to their capability to represent and model<lb>complex relations. While these methods show good versatility, they are computationally intensive<lb>and have poor scalability to large data as they require operations on Gram matrices. In order<lb>to mitigate this serious computational limitation, recently randomized constructions have been<lb>proposed in the literature, which allow the application of fast linear algorithms. Random Fourier<lb>features (RFF) are among the most popular and widely applied constructions: they provide an<lb>easily computable, low-dimensional feature representation for shift-invariant kernels. Despite the<lb>popularity of RFFs, very little is understood theoretically about their approximation quality. In this<lb>paper, we provide the first detailed theoretical analysis about the approximation quality of RFFs<lb>by establishing optimal (in terms of the RFF dimension) performance guarantees in uniform and<lb>L (1 \u2264 r < \u221e) norms. We also propose a RFF approximation to derivatives of a kernel with a<lb>theoretical study on its approximation quality.", "creator": "LaTeX with hyperref package"}}}