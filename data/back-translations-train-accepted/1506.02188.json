{"id": "1506.02188", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2015", "title": "Risk-Sensitive and Robust Decision-Making: a CVaR Optimization Approach", "abstract": "In this paper we address the problem of decision making within a Markov decision process (MDP) framework where risk and modeling errors are taken into account. Our approach is to minimize a risk-sensitive conditional-value-at-risk (CVaR) objective, as opposed to a standard risk-neutral expectation. We refer to such problem as CVaR MDP. Our first contribution is to show that a CVaR objective, besides capturing risk sensitivity, has an alternative interpretation as expected cost under worst-case modeling errors, for a given error budget. This result, which is of independent interest, motivates CVaR MDPs as a unifying framework for risk-sensitive and robust decision making. Our second contribution is to present an approximate value-iteration algorithm for CVaR MDPs and analyze its convergence rate. To our knowledge, this is the first solution algorithm for CVaR MDPs that enjoys error guarantees. Finally, we present results from numerical experiments that corroborate our theoretical findings and show the practicality of our approach.", "histories": [["v1", "Sat, 6 Jun 2015 19:52:23 GMT  (569kb,D)", "http://arxiv.org/abs/1506.02188v1", "Submitted to NIPS 15"]], "COMMENTS": "Submitted to NIPS 15", "reviews": [], "SUBJECTS": "cs.AI math.OC", "authors": ["yinlam chow", "aviv tamar", "shie mannor", "marco pavone"], "accepted": true, "id": "1506.02188"}, "pdf": {"name": "1506.02188.pdf", "metadata": {"source": "CRF", "title": "Risk-Sensitive and Robust Decision-Making: a CVaR Optimization Approach", "authors": ["Yinlam Chow", "Aviv Tamar", "Shie Mannor", "Marco Pavone"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2 Preliminaries, Problem Formulation, and Motivation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Conditional Value-at-Risk", "text": "Let Z be a limited medium random variable, i.e., E [| Z |] Va\u03b1 (Va\u03b1) < \u221e, on a probability space with cumulative distribution function F \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "2.2 Markov Decision Processes", "text": "A MDP is a Tupel M = (X, A, C, P, x0, \u03b3), whereby X and A are finite state and action areas; C (x, a) [\u2212 Cmax, Cmax] is a limited deterministic cost distribution; P (\u00b7 x, a) is the distribution of the transition probability; \u03b3 [0, 1) is the discounting factor, and x0 is the initial state. (Our results allow us to generalize slightly to random initial states and random costs.) Let us allow the space of the permissible history up to the time t Ht = Ht \u2212 1 \u00d7 X, for t \u2265 1 and H0 = X. A generic element ht-Ht is of the form ht = (x0, a0,...0,.x.,.x.,.x., 0.1, 0.at \u2212 1, 0.1, 0.x.) Let us Ht, t be the quantity of all deterministic historical guidelines with the property of each time t \u2265 1 and H0 = X. A generic element ht-Ht is 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,"}, {"heading": "2.3 Problem Formulation", "text": "Let the order of the random variables Zt indicate the incremental costs observed along a state / control curve in the MDP model, and let C0, T = \u2211 T = 0 \u03b3tZt specify the total discounted costs up to time T. The risk-sensitive problem with discounted costs, which we would like to address, is as follows: min-p-HCVaR\u03b1 (lim T \u2192 \u221e C0, T-p-x0, \u00b5), (3) where \u00b5 = {\u00b50, \u00b51,..} the policy sequence with measures at = \u00b5t (ht) for t-HCVaR\u03b1 {0, 1,..}. We refer to problem (3) as CVaR-MDP. (You can also consider a related formulation combining mean and CVaR, the details of which are presented in the supplement material.) The problem formulation in (3) speaks directly to the aspect of risk siveness in, e.g., the 20 financial applications, see VaD5 and VaPs, the Optimization of Numerous Applications."}, {"heading": "2.4 Motivation - Robustness to Modeling Errors", "text": "We show a new result in relation to the CVaR objective in (3) to the expected discounted costs in the presence of worst-case disturbances of the MDP parameters, in which the perturbations according to the \"number of things that can go wrong.\" So, by minimizing the CVaR, the decision maker also guarantees the duration of the transition phase, which will soon become clear. Overall probability of the transition phase is: x0, xT) = P0, P1 (x0) \u00b7 PT (x1) \u00b7 PT (xT \u2212 1), and we leave C0, T (xT \u2212 1)."}, {"heading": "3 Bellman Equation for CVaR", "text": "As we will see, the value function in this formulation depends both on the state and on the CVaR confidence level. We will then determine important properties of such a DP formulation, which will later enable us to derive an efficient DP-based approximate solution and provide correctness guarantees for the approximation error. Our starting point is a recursive decomposition of CVaR, the proof of which is in Theorem 10 of [17].Theorem 2 (CVaR decomposition theorem, Theorem 0, denote of Z = (Zt + 1, Zt + 2,.) the cost sequence from time t + 1. Conditional CVaR policy, i.e."}, {"heading": "1: Given:", "text": "\u2022 N (x) interpolation points Y (x) = {y1,..., yN (x)}. \u2022 Initial value function V0 (x, y), assuming 1 fulfilled."}, {"heading": "2: For t = 1, 2, . . .", "text": "\u2022 Update the estimate of the value function for each x-X and each yi-Y (x) as follows: Vt (x, yi) = TI [Vt \u2212 1] (x, yi), 3: Set the estimate of the convergent value repetition as V-Z (x, yi), for each x-X and yi-Y (x)."}, {"heading": "4 Value Iteration with Linear Interpolation", "text": "In this section, we present an approximate DP algorithm for solving CVaR MDPs (based on the theoretical results of Section 3. The value iteration algorithm in Eq. (9) presents two major implementation problems. The first challenge stems from the fact that the application of T results in maximizing the concavity of the concavity of yV (x, y). Our strategy is to use the concavity of the concavity of the concavity of the maximization problem to guarantee that such an optimization can actually be performed effectively. As discussed, our approach is based on the fact that the Bellman operator T receives concavity as established in Lemma 3. Accordingly, we need the following assumption for the initial assumption V0 (x, y), Assumption 1 The assumption for the initial function V0 (x, y) fulfills the following properties:"}, {"heading": "5 Experiments", "text": "We validate algorithm 1 on a rectangular grid world in which states represent grid points on a 2D terrain map = 21. An agent (e.g. a robot vehicle) starts in a safe region and his goal is to travel to a specific destination. At any given time, the agent can switch to one of his four neighboring states, resulting in a number of obstacles that the agent should avoid with the likelihood of moving to a random neighboring state. Gradually, the cost of each step leading up to reaching the goal is 1, to take fuel consumption into account. Between the starting point and the destination, there are a number of obstacles that the agent should avoid. Overcoming an obstacle costs M > > 1 and completes the mission. The goal is to calculate a safe (i.e., barrier-free) path that is fuel efficient. For our experiments, we choose a 64 \u00d7 53 grid world (see Figure 1), for a total of 3,312 states."}, {"heading": "6 Conclusion", "text": "These limits are useful for stopping the algorithm at a desired error threshold. Furthermore, we have uncovered an interesting relationship between the CVaR of total costs and the worst-case scenario costs of model disruption. In this formulation, the disruption correlates over time and results in a robustness framework that is considerably less conservative than the popular robust MDP framework, where uncertainty is time-independent. Overall, our work suggests CVaR MDPs as a unifying and practical framework for calculating control guidelines that are robust in both stochastity and model disruption. Future work should address expansions of large government areas. We suspect that a random, approximate approach should be feasible, as this approach has been proven to be roughly equivalent to CDP methods."}, {"heading": "A Proofs of Theoretical Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Proof of Proposition 1", "text": "By definition, we have that EP. [C (x1,.., xT) = \u2211 (x1,.., xT) P1 (x1) \u03b41 (x1) \u00b7 \u00b7 \u00b7 PT (xT | xT -1) \u03b4T (xT | xT -1) C (x1,..., xT) = \u2211 (x1,.., xT) P (x1,., xT) \u03b41 (x1,.., xT) \u03b41 (x1,..., xT) \u04211 (x1,.,., xT) C (x1,.,., xT) C (x1,.,., xT) C (x1,.,., xT) C (x1,.,., xT)."}, {"heading": "A.2 Proof of Lemma 3", "text": "The proof of monotonicity and constant displacement properties follows directly from the definitions of the Bellman operator by pointing out that the (x) P (x) P (x) P (x) P (x) P (x) P (x) P (x) P (x) P (x) P (x) P (x) P (x) P (x) P (x) P (x) P (x) P (x) P (x) P (x) P (x) P (x) P (x) P (x) P (x), V2) P (x) P (x), V2) P (x) P (x) P (x), x) P (x), x (x) P (x) P (x), x (P), x (P), x (P) P (x), x (P) P (x), x (P) P (x), x (P) P (x) P (x), x (x) P (x), P (x), P (x) P (x), x (x) P (x), P (x), P (x) P (x), P (x), P (x), P (x (x) P (x) P (x) P (x), x (x) P (x), P (x (x) P (x), x (x) P (x), P (x (x) P (x), x (x) P (x (x), x (x) P (x) P (x), x (x (x) P (x) P (x), x (x) P (x (x) P (x) P (x (x) P (x), x (x) P (x (x) P (x), x (x) P (x) P (x (x) P (x (x) P (x) P (x (x) P (x) P (x) P (x), x (x) P (x (x) P (x) P (x (x) P (x) P (x), x (x (x) P (x) P (x) P (x"}, {"heading": "A.3 Proof of Theorem 4", "text": "The first part of the proof shall show that for each (x, y), each (x, y), each (x, y), each (x, y), each (x, y), each (x, y), each (x, y), each (x, y), each (x, y), each (x, y), each (x, y), each (x, y), each (x, y), each (x, y), each (x, y), each (x, y), each (x, y), each (x, y), each (x, y), each (x), each (x), each (x), each (x), each (x), each (x), each (x), each (x), each (x), each (x), each (x), each (x), each (x), each (x), each (x), each (x), each (x, each (x), each (x), each (x), each (x), each (x, each (x), each (x), each (x), each (x), each (x), each (x), each (x), each (x, each (x), each (x), each (x), each (x), each (x), each (x), each (x, each (x), each (x), each (x, each (x), each (x), each (x), each (x, each (x), each (x), each (x), each (x, each (x), each (x), each (x), each (x), each (x), each (x, each (x), each (x), each (x), each (x), each (x, each (x), each (x), each (x), each (x), each (x), each (x), each (x, each (x), each (x), each (x), each (x), each (x), each (x), each (x), each (x), each (x), each ("}, {"heading": "A.4 Proof of Theorem 5", "text": "Similar to the definition of the optimal Bellman operator T, for any extended stationary Markovin policy u: X \u00b7 Y \u2192 A we define the induced Bellman operator Tu asTu [V] (x, y) = C (x, u (x, y) + 5 (x, y).Similar to theorem 4, we can easily show that the fixed point solution to Tu [V] (x, y) = V (x, y) is unique and the CVaR decomposition theorem (theorem 2) further implies that this solution to theorem 4, we can easily show that the fixed point solution to Tu [V] (x, y) = V (x, y) is unique to the rigid point solution to Tu [V] and the CVaR decomposition theorem (theorem 2) further implies this solution is equal to CVaRy (lim T)."}, {"heading": "A.5 Proof of Lemma 6", "text": "We first prove the monotony property. Based on the definition of Ix [V] (y] (y) (V1 (x, y) (V2) (x, y) (V) (x, y) (V) (x, y) (V) (x, y) (V) (x, y) (V) (x, y) (V) (x, y) (V) (V) (x) (V) (x) (V) (x) (V) (x) (V) (x) (V) (x) (V) (x) (V) (V) (x) (V) (x) (x)) (V) (x) (x) (V) (x) (V) (x) (X) (V) (X) () (V) (X) () (V) (x) (V) (X) () (V) (X) () () (V) (x) (V) () (V) (x) (V) () (V) (X) () (V) () () (V) (x) (V)."}, {"heading": "A.6 Useful Intermediate Results", "text": "Lemma 8 Let f (y): [0, 1] R be a concave function (j =)."}, {"heading": "A.7 Proof of Theorem 7", "text": "The proof of this theory is divided into three parts. In the first part, we have the difference Ix (y) (y) (y) (Vt) (x, y) (Vt) (x, y) (Vt) (x, y) (t) (Vt) (x, y) (t) (Vt) (x, y) (t) (Vt) (x, y) (t) (Vt) (x, y) (Vt) (x, y) (t) (Vt) (x, y) (t) (Vt). In the third part, we have bound the interpolation error using the contraction properties of Bellman recursions. First, we analyze the limits of Ix [Vt] (y) (y) (Vt) / y \u2212 Vt) (Vt) (x, y) (t) in the following four cases. Notice starting from Lemma 10, we have Idx [Vt] (y) (Vt) (y) (+ / (1) (Mt)."}, {"heading": "B Trajectory Plots", "text": "In Figure 2, we show simulated trajectories according to a greedy policy versus the value function according to Theorem 5."}, {"heading": "C Generalization to Mean-CVaR Optimization", "text": "In this section, we extend our approach to the MDPs with a mean CVaR objective of the form: Minimum (VII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII (VIII) (VIII) (VIII (VIII) (VIII) (VIII (VIII) (VIII) (VIII (VIII) (VIII) (VIII) (VIII (VIII) (VIII) (VIII) (VIII) (VIII (VIII) (VIII) (VIII) (VIII) (VIII) (VIII) (VIII (VIII) (VIII) (VIII) (VIII (VIII) (VIII) (VIII) (VIII (VIII) (VIII) (VIII (VIII) (VIII) (VIII (VIII) (VIII) (VIII (VIII) (VIII) (VIII (VIII) (VIII) (VIII (VIII) (VIII) (VIII (VIII) (VIII) (VIII (VIII) (VIII) (VIII) (VIII (VIII) (VIII) (VIII) (VIII (VIII) (VIII) (VIII (VIII) (VIII) (VIII) (VIII (VIII) (VIII) (VIII) (VIII (VIII) (VIII) (VIII) (VIII) (VIII) (VIII (VIII (VIII) (VIII) (VIII"}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "<lb>In this paper we address the problem of decision making within a Markov de-<lb>cision process (MDP) framework where risk and modeling errors are taken into<lb>account. Our approach is to minimize a risk-sensitive conditional-value-at-risk<lb>(CVaR) objective, as opposed to a standard risk-neutral expectation. We refer to<lb>such problem as CVaR MDP. Our first contribution is to show that a CVaR objec-<lb>tive, besides capturing risk sensitivity, has an alternative interpretation as expected<lb>cost under worst-case modeling errors, for a given error budget. This result, which<lb>is of independent interest, motivates CVaR MDPs as a unifying framework for<lb>risk-sensitive and robust decision making. Our second contribution is to present<lb>an approximate value-iteration algorithm for CVaR MDPs and analyze its conver-<lb>gence rate. To our knowledge, this is the first solution algorithm for CVaR MDPs<lb>that enjoys error guarantees. Finally, we present results from numerical exper-<lb>iments that corroborate our theoretical findings and show the practicality of our<lb>approach.", "creator": "LaTeX with hyperref package"}}}