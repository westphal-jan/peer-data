{"id": "1506.02216", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2015", "title": "A Recurrent Latent Variable Model for Sequential Data", "abstract": "In this paper, we explore the inclusion of random variables into the dynamic latent state of a recurrent neural network (RNN) by combining elements of the variational autoencoder. We argue that through the use of high-level latent random variables, our variational RNN (VRNN) is able to learn to model the kind of variability observed in highly-structured sequential data (such as speech). We empirically evaluate the proposed model against related sequential models on five sequence datasets, four of speech and one of handwriting. Our results show the importance of the role random variables can play in the RNN dynamic latent state.", "histories": [["v1", "Sun, 7 Jun 2015 04:23:50 GMT  (746kb,D)", "http://arxiv.org/abs/1506.02216v1", "8 pages"], ["v2", "Thu, 18 Jun 2015 02:25:53 GMT  (912kb,D)", "http://arxiv.org/abs/1506.02216v2", "Fixed Fig. 1, typos, notations, terminologies, 8 pages"], ["v3", "Fri, 19 Jun 2015 04:57:00 GMT  (912kb,D)", "http://arxiv.org/abs/1506.02216v3", "Fixed Fig. 1, typos, notations, terminologies, 8 pages"], ["v4", "Thu, 15 Oct 2015 18:10:41 GMT  (912kb,D)", "http://arxiv.org/abs/1506.02216v4", "Fixed Eq. (11), added typos"], ["v5", "Mon, 2 Nov 2015 18:56:13 GMT  (912kb,D)", "http://arxiv.org/abs/1506.02216v5", "Fixed typos"], ["v6", "Wed, 6 Apr 2016 20:52:32 GMT  (913kb,D)", "http://arxiv.org/abs/1506.02216v6", null]], "COMMENTS": "8 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["junyoung chung", "kyle kastner", "laurent dinh", "kratarth goel", "aaron c courville", "yoshua bengio"], "accepted": true, "id": "1506.02216"}, "pdf": {"name": "1506.02216.pdf", "metadata": {"source": "CRF", "title": "A Recurrent Latent Variable Model for Sequential Data", "authors": ["Junyoung Chung", "Kyle Kastner", "Laurent Dinh", "Kratarth Goel", "Aaron Courville", "Yoshua Bengio"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "However, there are also a few important differences between RNNs and DBNs.DNs, which are typically either limited to relatively simple transition processes (e.g. relatively simple transition processes) or limited to relatively simple transition processes (e.g. the domain of dynamic Bayes networks (DBNs) such as hidden Markov models (HMMs) and Kalman filters). The dominance of DBN-based approaches has recently been overturned by a resurgence of interest in recursive neural networks (RNN), which can be used to model the common probability distribution across sequences. Both RNNs and DBNs consist of two parts: (1) a transition function that determines the evolution of the internal latent state, and (2) a mapping from state to output. However, there are also a few important differences between RNNNs and DBNs.DNs, which are typically limited to relatively simple transition processes."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Sequence Modeling with Recurrent Neural Networks", "text": "An RNN model is a kind of neural network that can be regarded as input of a variable length sequence x = (x1, x2,., xT) by recursively processing each symbol while maintaining its internal hidden state. At any time, an RNN reads the xt symbol and updates its hidden state. [LSTM, 8] or gated recurrent unit [GRU, 5] where f is a non-linear transition function that is normally implemented as an affine transformation followed by tanh or a gated activation function such as a long-term short-term memory unit [LSTM, 8] or gated recurrent unit [GRU, 5]. RN model sequences after parameterizations of the common sequence probability distribution as a product of conditional probabilities such as thatp (x1, x2,., xT) = T."}, {"heading": "2.2 Variational Autoencoder", "text": "For non-sequential data, variational autoencoders [11, 15] have recently proven to be an effective modeling paradigm for restoring complex multimodal distributions across the data space. A variational autoencoder introduces a set of latent variables z designed to capture the variations in the observed variables x. As an example of a directed graphical model, the common distribution is defined as asp (x, z) = p (x | z) p (z) p (z) p (z). (3) the previous one above the latent variable p (z) is generally chosen as a simple Gaussian distribution, and the conditional p (x | z) is an arbitrary observation model whose parameters are calculated by a parametric function of z. It is important that the variational autoencoder typically uses a non-linear x-z shape with a highly flexible functional approximative z-proximal network."}, {"heading": "3 Variational Recurrent Neural Network", "text": "In this section we present a recursive version of the variable autoencoder, in which we model only the function q (= q) (= q). Inspired by simpler dynamic Bayes networks such as HMMs and Kalman filters, the proposed variable recursive neural network (VRNN) explicitly includes dependence between latent random variables over subsequent periods of time. However, unlike these simpler DBN models, these variable autoencoders are bound to ht \u2212 1 of a recurrent network state variable. This addition will help us take into account the temporal structure of the sequential data. Unlike a typical variable on the latent variable, this variable is no longer a normal distribution, but follows the distribution."}, {"heading": "4 Experiment Settings", "text": "In fact, most of them are able to go in search of a solution that is capable, that they have got to grips with, and that they are able to go in search of a solution that is capable, that they are able to get to grips with, and that they are able to go in search of a solution that is capable, that they are able to find a solution, that they are able to find a solution, that they are able to find a solution, that they are able to find a solution."}, {"heading": "5 Results and Analysis", "text": "In fact, we will be able to retaliate if we are able to retaliate, \"he said.\" We will be able to retaliate, \"he said.\" We will do it, \"he said,\" but we will not do it as if we are able to retaliate. \""}, {"heading": "6 Conclusion", "text": "We propose a novel model of complex sequential data that integrates latent variables into a recursive architecture of neural networks (RNN). We show that by modelling the dependencies between these latent variables, we are able to provide a model that more naturally reflects the variability observed in many sequential processes. Our experiments focus on unconditional speech generation involving various real data sets as well as unrestricted handwriting generation. We find that the introduction of latent variables provides a significant increase in performance for unconditional speech modeling and also shows the importance of time conditioning of these latent variables. Samples from VRNN models compete qualitatively with existing methods and appear to exhibit stylistic consistency over time."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the developers of Theano [1] and Kyunghyun Cho for their insightful comments and discussions. We would like to thank the following research funding and computer support agencies: Ubisoft, NSERC, Calcul Que'bec, Compute Canada, the Canada Research Chairs and CIFAR."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Learning stochastic recurrent networks", "author": ["J. Bayer", "C. Osendorfer"], "venue": "arXiv preprint arXiv:1411.7610,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Unsupervised learning of auditory filter banks using nonnegative matrix factorisation", "author": ["A. Bertrand", "K. Demuynck", "V. Stouten"], "venue": "In Acoustics, Speech and Signal Processing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Modeling temporal dependencies in highdimensional sequences: Application to polyphonic music generation and transcription", "author": ["N. Boulanger-Lewandowski", "Y. Bengio", "P. Vincent"], "venue": "In ICML\u20192012,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Variational Recurrent Auto-Encoders", "author": ["O. Fabius", "J.R. van Amersfoort"], "venue": "ArXiv e-prints,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "Technical report,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "The blizzard challenge", "author": ["S. King", "V. Karaiskos"], "venue": "In The Ninth annual Blizzard Challenge,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Unsupervised feature learning for audio classification using convolutional deep belief networks", "author": ["H. Lee", "P. Pham", "Y. Largman", "A. Ng"], "venue": "In NIPS\u201909,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Iam-ondb-an on-line english sentence database acquired from handwritten text on a whiteboard", "author": ["M. Liwicki", "H. Bunke"], "venue": "In Document Analysis and Recognition,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Learning visual motion in recurrent neural networks", "author": ["M. Pachitariu", "M. Sahani"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "In ICML\u20192014,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Speech synthesis based on hidden markov models", "author": ["K. Tokuda", "Y. Nankaku", "T. Toda", "H. Zen", "J. Yamagishi", "K. Oura"], "venue": "Proceedings of the IEEE,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "The speech accent archieve", "author": ["S. Weinberger"], "venue": "http://accent.gmu.edu/,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "We argue, as have others [4, 2], that these complex dependencies cannot be modeled efficiently by", "startOffset": 25, "endOffset": 31}, {"referenceID": 1, "context": "We argue, as have others [4, 2], that these complex dependencies cannot be modeled efficiently by", "startOffset": 25, "endOffset": 31}, {"referenceID": 10, "context": "In the context of standard neural network models for non-sequential data, the recently introduced variational autoencoder [11] offers an interesting combination highly flexible non-linear mapping between the latent random state and the observed output and effective approximate inference.", "startOffset": 122, "endOffset": 126}, {"referenceID": 3, "context": "[4], Bayer and Osendorfer [2], Fabius and van Amersfoort [6], we believe we are the first to integrate these dependencies between the random variables at neighboring timesteps.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[4], Bayer and Osendorfer [2], Fabius and van Amersfoort [6], we believe we are the first to integrate these dependencies between the random variables at neighboring timesteps.", "startOffset": 26, "endOffset": 29}, {"referenceID": 5, "context": "[4], Bayer and Osendorfer [2], Fabius and van Amersfoort [6], we believe we are the first to integrate these dependencies between the random variables at neighboring timesteps.", "startOffset": 57, "endOffset": 60}, {"referenceID": 1, "context": "We also compare to an alternative method of incorporating latent random variables into a recurrent structure [2].", "startOffset": 109, "endOffset": 112}, {"referenceID": 6, "context": "Graves [7] recently used a Gaussian mixture model (GMM) to model real-valued sequences in the context of, e.", "startOffset": 7, "endOffset": 10}, {"referenceID": 6, "context": "With the notable exception of [7], there have been few works investigating the structured output density model for RNNs with real-valued sequences.", "startOffset": 30, "endOffset": 33}, {"referenceID": 3, "context": "[4] extensively tested NADE and RBM-based output densities for modeling sequences of binary vector representations of music.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Bayer and Osendorfer [2] introduced a sequence of independent latent variables corresponding to the states of the RNN.", "startOffset": 21, "endOffset": 24}, {"referenceID": 13, "context": ") Similarly, Pachitariu and Sahani [14] earlier proposed both a sequence of independent latent variables and a stochastic internal state for the RNN.", "startOffset": 35, "endOffset": 39}, {"referenceID": 10, "context": "For non-sequential data, variational autoencoders [11, 15] have recently been shown to be an effective modeling paradigm to recover complex multimodal distributions over the data space.", "startOffset": 50, "endOffset": 58}, {"referenceID": 14, "context": "For non-sequential data, variational autoencoders [11, 15] have recently been shown to be an effective modeling paradigm to recover complex multimodal distributions over the data space.", "startOffset": 50, "endOffset": 58}, {"referenceID": 10, "context": "In Kingma and Welling [11], the variational posterior q(z | x) is a GaussianN (\u03bc, diag(\u03c3)) whose mean \u03bc and log-variance log(\u03c3) are the output of a highly non-linear function of x, once again typically a neural network.", "startOffset": 22, "endOffset": 26}, {"referenceID": 1, "context": "STORN [2] model can be considered an instance of the VRNN-I model family.", "startOffset": 6, "endOffset": 9}, {"referenceID": 8, "context": "Blizzard: This text-to-speech dataset made available by the Blizzard Challenge 2013 contains 300 hours of English spoken by a single female speaker [9].", "startOffset": 148, "endOffset": 151}, {"referenceID": 16, "context": "Accent: This dataset contains English paragraphs read by 2, 046 different native and nonnative English speakers [17].", "startOffset": 112, "endOffset": 116}, {"referenceID": 12, "context": "Handwriting Generation We let each model to learn a sequence of (x, y)-coordinates together with binary indicators of pen up / pen down, using the IAM-OnDB dataset which consists of 13, 040 handwritten lines written by 500 writers [13].", "startOffset": 231, "endOffset": 235}, {"referenceID": 6, "context": "We preprocess and split the dataset as done in [7].", "startOffset": 47, "endOffset": 50}, {"referenceID": 9, "context": "We trained each model by the stochastic gradient descent on the log-likelihood using the recently proposed Adam optimizer [10], with learning rate of 0.", "startOffset": 122, "endOffset": 126}, {"referenceID": 14, "context": "[15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "The authors would like to thank the developers of Theano [1].", "startOffset": 57, "endOffset": 60}], "year": 2015, "abstractText": "In this paper, we explore the inclusion of random variables into the dynamic latent state of a recurrent neural network (RNN) by combining elements of the variational autoencoder. We argue that through the use of high-level latent random variables, our variational RNN (VRNN) is able to learn to model the kind of variability observed in highly-structured sequential data (such as speech). We empirically evaluate the proposed model against related sequential models on five sequence datasets, four of speech and one of handwriting. Our results show the importance of the role random variables can play in the RNN dynamic latent state.", "creator": "LaTeX with hyperref package"}}}