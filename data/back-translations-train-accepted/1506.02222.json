{"id": "1506.02222", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2015", "title": "No penalty no tears: Least squares in high-dimensional linear models", "abstract": "Ordinary least squares (OLS) is the default method for fitting linear models, but is not applicable for problems with dimensionality larger than the sample size. For these problems, we advocate the use of a generalized version of OLS motivated by ridge regression, and propose two novel three-step algorithms involving least squares fitting and hard thresholding. The algorithms are methodologically simple to understand intuitively, computationally easy to implement efficiently, and theoretically appealing for choosing models consistently. Numerical exercises comparing our methods with penalization-based approaches in simulations and data analyses illustrate the great potential of the proposed algorithms.", "histories": [["v1", "Sun, 7 Jun 2015 05:45:24 GMT  (2975kb,D)", "http://arxiv.org/abs/1506.02222v1", "Correct several notation errors"], ["v2", "Wed, 10 Jun 2015 03:31:06 GMT  (2975kb,D)", "http://arxiv.org/abs/1506.02222v2", "Correct typos;"], ["v3", "Fri, 9 Oct 2015 21:30:39 GMT  (2979kb,D)", "http://arxiv.org/abs/1506.02222v3", "29 pages, 5 figures, 4 tables"], ["v4", "Mon, 23 Nov 2015 09:21:37 GMT  (2979kb,D)", "http://arxiv.org/abs/1506.02222v4", "corrected citation format"], ["v5", "Thu, 16 Jun 2016 07:13:40 GMT  (2984kb,D)", "http://arxiv.org/abs/1506.02222v5", "Added results for non-sparse models; Added results for elliptical distribution; Added simulations for adaptive lasso"]], "COMMENTS": "Correct several notation errors", "reviews": [], "SUBJECTS": "stat.ME cs.LG math.ST stat.ML stat.TH", "authors": ["xiangyu wang", "david b dunson", "chenlei leng"], "accepted": true, "id": "1506.02222"}, "pdf": {"name": "1506.02222.pdf", "metadata": {"source": "CRF", "title": "No penalty no tears: Least squares in high-dimensional linear models", "authors": ["Xiangyu Wang", "David Dusnon", "Chenlei Leng"], "emails": [], "sections": [{"heading": null, "text": "Ordinary least squares (OLS) are the standard method for adjusting linear models, but are not applicable to problems with dimensionality greater than sample size. For these problems, we advocate the use of a generalized version of OLS motivated by burr regression, and propose two new three-level algorithms that include least squares adjustment and hard thresholds. Methodologically, the algorithms are easy to understand intuitively, easy to implement in terms of computational efficiency, and theoretically attractive for consistent model selection. Numerical exercises that compare our methods with penalization-based approaches in simulations and data analysis illustrate the great potential of the proposed algorithms."}, {"heading": "1 Introduction", "text": "In fact, the fact is that most of them are able to move to another world in which they are able to find themselves."}, {"heading": "2 High dimensional ordinary least squares", "text": "(Consider the usual linear model X\u03b2 + \u03b5 0 (XX + Yp), where X is the n \u00b7 p design matrix, Y is the n \u00b7 1 response vector and \u03b2 is the coefficient. As usual in the high-dimensional literature, we assume that most \u03b2i's are zero, except for a small subset S = supp (\u03b2) with cardinality s; i.e., what is the correct form of the OLS in the high-dimensional setting? ii) How to use this estimator correctly? We rethink OLS from a different perspective. Indeed, OLS can be considered the limit of the comb estimators when the comb parameters go to zero, i.e."}, {"heading": "3 Theory", "text": "In this section, we also demonstrate the consistency of algorithm 1 in selecting the true model and provide concrete shapes for all the values needed for the algorithm to work. We look at the random design in which the rows of X are drawn from a multivariate Gaussian distribution. This random design allows for different correlation structures between the predictors and is widely used to illustrate methods based on the constrained eigenvalue conditions [14, 15]. Noise, as already mentioned, is only assumed as a second-order moment, i.e., var (var) structures among predictors, as opposed to the subGaussian / bounded error assumptions seen in most high dimensions of literature [4, 8, 9, 11]. This relativization is similar to [6], but we do not require any further assumptions that we need."}, {"heading": "4 Experiments", "text": "In this section, we offer extensive numerical experiments to evaluate the performance of LAT and RAT. In particular, we compare the two methods with existing punished methods such as lasso, elastic mesh (enet [5]), scad [3] and mc + [4]. Since the lasso estimator is known to be distorted, we also consider two variants of it by combining lasso with stage 2 and 3 of our LAT and RAT algorithms, called lasLAT (las1 in the figures) and lasRAT (las2 in the figures). We code LAT and RAT in the Matlab, use glmnet [17] for enet and lasso, and SparseReg [18, 19] for scad and mc +."}, {"heading": "4.1 Synthetic datasets", "text": "The model used in this section for comparison is the linear model Y = X\u03b2 + \u03b5, in which \u03b5 \u0445 N = \u03b2 = 3 (0, \u03c32) and X \u0445 N (0, \u03a3). To control the signal-to-noise ratio, we define r = 0, \u03b2 2 / \u03c3, which is selected as 2.3 for all experiments. Sample size and data dimension are chosen to be (n, p) = (200, 1000) or (n, p) = (500, 10000) for all experiments. For evaluation purposes, we consider four different structures from \u03a3 below. (i) Independent predictors. Support is specified as S = {1, 2, 4, 5}. We generate Xi from a multivariate normal distribution with independent components. The coefficients are specified as\u03b2i = (\u2212 1) ui (0, 1) ui (+ 1) | 1), where ui-Ber (0.5) for i-S and Size-S (6)."}, {"heading": "4.2 Real data", "text": "This data set, taken from [22], was collected to investigate diseases of the eye of mammals, with gene expression recorded for the eye tissue of 120 twelve-week-old male F2 rats. A gene coded as TRIM32, which is responsible for the development of Bardet-Biedl syndrome, is of particular interest and represents the response of interest.Following the method in [22], 18976 probes were selected as they showed sufficient signal for reliable analysis and at least double variation of expression. Since TRIM32 is linked to only a small number of genes, we limit our attention to the 5000 genes with the highest sample variance. The eight methods used in the simulation study are compared, with performance evaluated by means of a tenfold cross-validation. Since extended BIC does not provide competitive predictive accuracy (it focuses on ensuring good variable selection performance), we applied conventional BIC comparison methods to a conventional trade method."}, {"heading": "5 Conclusion", "text": "We have proposed two new algorithms Lat and Rat, which are based only on adaptation to a high-dimensional linear model and a hard threshold, based on a high-dimensional generalization of OLS. Both methods are simple, easy to implement and can be consistently adapted to a high-dimensional linear model and regain its support. The performance of both methods is competitive compared to existing regulatory methods. It is of great interest to extend this framework to other models such as generalized linear models and models for survival analysis."}, {"heading": "Appendix A: Proof of Theorem 1", "text": "The following two lmmas are then set for any C > 0 < c1 < c2 and c3 > 0 so that for each t > 0 and each i > Q, j = i, p > c0n for each i, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p"}], "references": [{"title": "Regression shrinkage and selection via the lasso", "author": ["Robert Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Statistical Methodology),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1996}, {"title": "On model selection consistency of lasso", "author": ["Peng Zhao", "Bin Yu"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Variable selection via nonconcave penalized likelihood and its oracle properties", "author": ["Jianqing Fan", "Runze Li"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "Nearly unbiased variable selection under minimax concave penalty", "author": ["Cun-Hui Zhang"], "venue": "The Annals of Statistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Regularization and variable selection via the elastic net", "author": ["Hui Zou", "Trevor Hastie"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Sup-norm convergence rate and sign concentration property of lasso and dantzig estimators", "author": ["Karim Lounici"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "On iterative hard thresholding methods for high-dimensional m-estimation", "author": ["Prateek Jain", "Ambuj Tewari", "Purushottam Kar"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Elementary estimators for highdimensional linear regression", "author": ["Eunho Yang", "Aurelie Lozano", "Pradeep Ravikumar"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Sharp thresholds for high-dimensional and noisy sparsity recovery using-constrained quadratic programming (lasso)", "author": ["Martin J Wainwright"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Orthogonal matching pursuit for sparse signal recovery with noise", "author": ["T Tony Cai", "Lie Wang"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "The sparsity and bias of the lasso selection in highdimensional linear regression", "author": ["Cun-Hui Zhang", "Jian Huang"], "venue": "The Annals of Statistics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "High-dimensional ordinary least-squares projection for screening", "author": ["Xiangyu Wang", "Chenlei Leng"], "venue": "variables. https://stat.duke.edu/~xw56/holp-paper.pdf,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Extended bayesian information criteria for model selection with large model spaces", "author": ["Jiahua Chen", "Zehua Chen"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Simultaneous analysis of lasso and dantzig selector", "author": ["Peter J Bickel", "Ya\u2019acov Ritov", "Alexandre B Tsybakov"], "venue": "The Annals of Statistics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Restricted eigenvalue properties for correlated gaussian designs", "author": ["Garvesh Raskutti", "Martin J Wainwright", "Bin Yu"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "On the consistency theory of high dimensional variable screening", "author": ["Xiangyu Wang", "Chenlei Leng", "David B Dunson"], "venue": "arXiv preprint arXiv:1502.06895,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Regularization paths for generalized linear models via coordinate descent", "author": ["Jerome Friedman", "Trevor Hastie", "Rob Tibshirani"], "venue": "Journal of Statistical Software,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Path following and empirical bayes model selection for sparse regression", "author": ["Hua Zhou", "Artin Armagan", "David B Dunson"], "venue": "arXiv preprint arXiv:1201.3528,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "A path algorithm for constrained estimation", "author": ["Hua Zhou", "Kenneth Lange"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Stability selection", "author": ["Nicolai Meinshausen", "Peter B\u00fchlmann"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "High dimensional variable selection via tilting", "author": ["Haeran Cho", "Piotr Fryzlewicz"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Regulation of gene expression in the mammalian eye and its relevance to eye disease", "author": ["Todd E Scheetz", "Kwang-Youn A Kim", "Ruth E Swiderski", "Alisdair R Philp", "Terry A Braun", "Kevin L Knudtson", "Anne M Dorrance", "Gerald F DiBona", "Jian Huang", "Thomas L Casavant"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "To deal with this problem, Tibshirani[1] proposed `1-penalized regression, a.", "startOffset": 37, "endOffset": 40}, {"referenceID": 1, "context": "It yields a sparse solution and achieves model selection consistency and estimation consistency under certain conditions [2, 3, 4, 5].", "startOffset": 121, "endOffset": 133}, {"referenceID": 2, "context": "It yields a sparse solution and achieves model selection consistency and estimation consistency under certain conditions [2, 3, 4, 5].", "startOffset": 121, "endOffset": 133}, {"referenceID": 3, "context": "It yields a sparse solution and achieves model selection consistency and estimation consistency under certain conditions [2, 3, 4, 5].", "startOffset": 121, "endOffset": 133}, {"referenceID": 4, "context": "It yields a sparse solution and achieves model selection consistency and estimation consistency under certain conditions [2, 3, 4, 5].", "startOffset": 121, "endOffset": 133}, {"referenceID": 1, "context": "On the one hand, methods using convex penalties, such as lasso, usually require strong conditions for model selection consistency[2, 6].", "startOffset": 129, "endOffset": 135}, {"referenceID": 5, "context": "On the one hand, methods using convex penalties, such as lasso, usually require strong conditions for model selection consistency[2, 6].", "startOffset": 129, "endOffset": 135}, {"referenceID": 2, "context": "On the other hand, methods using non-convex penalties[3, 4] that can achieve model selection consistency under mild conditions often require huge computational expense.", "startOffset": 53, "endOffset": 59}, {"referenceID": 3, "context": "On the other hand, methods using non-convex penalties[3, 4] that can achieve model selection consistency under mild conditions often require huge computational expense.", "startOffset": 53, "endOffset": 59}, {"referenceID": 6, "context": "These concerns have limited the practical use of regularized methods, motivating alternative strategies such as direct hard thresholding [7].", "startOffset": 137, "endOffset": 140}, {"referenceID": 7, "context": "Related works The work that is most closely related to ours is [8], in which the authors proposed an algorithm based on OLS and the ridge regression.", "startOffset": 63, "endOffset": 66}, {"referenceID": 6, "context": "[7] proposed an iterative hard thresholding algorithm for sparse regression, which shares a similar spirit of hard thresholding as our algorithm.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "The consistency of the algorithms only needs a conditional number constraint, as opposed to the strong irrepresentable condition[2, 9] required by lasso.", "startOffset": 128, "endOffset": 134}, {"referenceID": 8, "context": "The consistency of the algorithms only needs a conditional number constraint, as opposed to the strong irrepresentable condition[2, 9] required by lasso.", "startOffset": 128, "endOffset": 134}, {"referenceID": 3, "context": "[4, 8, 10, 9, 11, 12]) that work for log p = o(n) case rely on a sub-Gaussian tail/bounded error assumption, which might fail to hold for general noise.", "startOffset": 0, "endOffset": 21}, {"referenceID": 7, "context": "[4, 8, 10, 9, 11, 12]) that work for log p = o(n) case rely on a sub-Gaussian tail/bounded error assumption, which might fail to hold for general noise.", "startOffset": 0, "endOffset": 21}, {"referenceID": 9, "context": "[4, 8, 10, 9, 11, 12]) that work for log p = o(n) case rely on a sub-Gaussian tail/bounded error assumption, which might fail to hold for general noise.", "startOffset": 0, "endOffset": 21}, {"referenceID": 8, "context": "[4, 8, 10, 9, 11, 12]) that work for log p = o(n) case rely on a sub-Gaussian tail/bounded error assumption, which might fail to hold for general noise.", "startOffset": 0, "endOffset": 21}, {"referenceID": 10, "context": "[4, 8, 10, 9, 11, 12]) that work for log p = o(n) case rely on a sub-Gaussian tail/bounded error assumption, which might fail to hold for general noise.", "startOffset": 0, "endOffset": 21}, {"referenceID": 11, "context": "[4, 8, 10, 9, 11, 12]) that work for log p = o(n) case rely on a sub-Gaussian tail/bounded error assumption, which might fail to hold for general noise.", "startOffset": 0, "endOffset": 21}, {"referenceID": 5, "context": "[6] proved that lasso also works for a second-order condition similar to ours, but requires two additional strong assumptions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "A keen observation[12] reveals the following relationship immediately.", "startOffset": 18, "endOffset": 22}, {"referenceID": 11, "context": "Instead of directly estimating \u03b2 as \u03b2\u0302, however, this new estimator of \u03b2 may be used for dimension reduction by observing \u03b2\u0302 = X (XX )\u22121X\u03b2 + X (XX )\u22121\u03b5 = \u03a6\u03b2 + \u03b7 [12].", "startOffset": 161, "endOffset": 165}, {"referenceID": 12, "context": "Alternatively use eBIC in [13] in conjunction with the obtained variable importance to select the best submodel.", "startOffset": 26, "endOffset": 30}, {"referenceID": 13, "context": "This random design allows for various correlation structures among predictors and is widely used to illustrate methods that rely on the restricted eigenvalue conditions [14, 15].", "startOffset": 169, "endOffset": 177}, {"referenceID": 14, "context": "This random design allows for various correlation structures among predictors and is widely used to illustrate methods that rely on the restricted eigenvalue conditions [14, 15].", "startOffset": 169, "endOffset": 177}, {"referenceID": 3, "context": ", var(\u03b5) = \u03c3 < \u221e, in contrast to the subGaussian/bounded error assumption seen in most high dimension literature [4, 8, 10, 9, 11].", "startOffset": 113, "endOffset": 130}, {"referenceID": 7, "context": ", var(\u03b5) = \u03c3 < \u221e, in contrast to the subGaussian/bounded error assumption seen in most high dimension literature [4, 8, 10, 9, 11].", "startOffset": 113, "endOffset": 130}, {"referenceID": 9, "context": ", var(\u03b5) = \u03c3 < \u221e, in contrast to the subGaussian/bounded error assumption seen in most high dimension literature [4, 8, 10, 9, 11].", "startOffset": 113, "endOffset": 130}, {"referenceID": 8, "context": ", var(\u03b5) = \u03c3 < \u221e, in contrast to the subGaussian/bounded error assumption seen in most high dimension literature [4, 8, 10, 9, 11].", "startOffset": 113, "endOffset": 130}, {"referenceID": 10, "context": ", var(\u03b5) = \u03c3 < \u221e, in contrast to the subGaussian/bounded error assumption seen in most high dimension literature [4, 8, 10, 9, 11].", "startOffset": 113, "endOffset": 130}, {"referenceID": 5, "context": "This relaxation is similar to [6]; however we do not require any further assumptions needed by [6].", "startOffset": 30, "endOffset": 33}, {"referenceID": 5, "context": "This relaxation is similar to [6]; however we do not require any further assumptions needed by [6].", "startOffset": 95, "endOffset": 98}, {"referenceID": 12, "context": "However, the corresponding details will not be pursued here, as their consistency is straightforwardly implied by the results from this section and the existing literature on extended BIC and BIC [13].", "startOffset": 196, "endOffset": 200}, {"referenceID": 15, "context": "In particular, it is shown that the diagonal terms of \u03a6 are O( p ) while the off-diagonal terms are O( \u221a n p ) [16].", "startOffset": 111, "endOffset": 115}, {"referenceID": 12, "context": "Alternatively, we can construct a series of nested models formed by ranking the largest n coefficients and adopt the extended BIC [13] to select the best submodel.", "startOffset": 130, "endOffset": 134}, {"referenceID": 4, "context": "In particular, we compare the two methods to existing penalized methods including lasso, elastic net (enet [5]), scad [3] and mc+ [4].", "startOffset": 107, "endOffset": 110}, {"referenceID": 2, "context": "In particular, we compare the two methods to existing penalized methods including lasso, elastic net (enet [5]), scad [3] and mc+ [4].", "startOffset": 118, "endOffset": 121}, {"referenceID": 3, "context": "In particular, we compare the two methods to existing penalized methods including lasso, elastic net (enet [5]), scad [3] and mc+ [4].", "startOffset": 130, "endOffset": 133}, {"referenceID": 16, "context": "We code LAT and RAT in Matlab, use glmnet[17] for enet and lasso, and SparseReg[18, 19] for scad and mc+.", "startOffset": 41, "endOffset": 45}, {"referenceID": 17, "context": "We code LAT and RAT in Matlab, use glmnet[17] for enet and lasso, and SparseReg[18, 19] for scad and mc+.", "startOffset": 79, "endOffset": 87}, {"referenceID": 18, "context": "We code LAT and RAT in Matlab, use glmnet[17] for enet and lasso, and SparseReg[18, 19] for scad and mc+.", "startOffset": 79, "endOffset": 87}, {"referenceID": 4, "context": "This example is Example 4 in [5], for which we allocate the 15 true variables into three groups.", "startOffset": 29, "endOffset": 32}, {"referenceID": 19, "context": "This model is also considered in [20] and [21].", "startOffset": 33, "endOffset": 37}, {"referenceID": 20, "context": "This model is also considered in [20] and [21].", "startOffset": 42, "endOffset": 46}, {"referenceID": 12, "context": "We use the extended BIC [13] to choose the parameters for any regularized algorithm.", "startOffset": 24, "endOffset": 28}, {"referenceID": 21, "context": "This dataset, taken from [22], was collected to study mammalian eye diseases, with gene expression for the eye tissues of 120 twelve-week-old male F2 rats recorded.", "startOffset": 25, "endOffset": 29}, {"referenceID": 21, "context": "Following the method in [22], 18976 probes were selected as they exhibited sufficient signal for reliable analysis and at least 2-fold variation in expressions.", "startOffset": 24, "endOffset": 28}], "year": 2015, "abstractText": "Ordinary least squares (OLS) is the default method for fitting linear models, but is not applicable for problems with dimensionality larger than the sample size. For these problems, we advocate the use of a generalized version of OLS motivated by ridge regression, and propose two novel three-step algorithms involving least squares fitting and hard thresholding. The algorithms are methodologically simple to understand intuitively, computationally easy to implement efficiently, and theoretically appealing for choosing models consistently. Numerical exercises comparing our methods with penalization-based approaches in simulations and data analyses illustrate the great potential of the proposed algorithms.", "creator": "LaTeX with hyperref package"}}}