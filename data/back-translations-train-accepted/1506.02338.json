{"id": "1506.02338", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2015", "title": "Modeling Order in Neural Word Embeddings at Scale", "abstract": "Natural Language Processing (NLP) systems commonly leverage bag-of-words co-occurrence techniques to capture semantic and syntactic word relationships. The resulting word-level distributed representations often ignore morphological information, though character-level embeddings have proven valuable to NLP tasks. We propose a new neural language model incorporating both word order and character order in its embedding. The model produces several vector spaces with meaningful substructure, as evidenced by its performance of 85.8% on a recent word-analogy task, exceeding best published syntactic word-analogy scores by a 58% error margin. Furthermore, the model includes several parallel training methods, most notably allowing a skip-gram network with 160 billion parameters to be trained overnight on 3 multi-core CPUs, 14x larger than the previous largest neural network.", "histories": [["v1", "Mon, 8 Jun 2015 02:21:46 GMT  (446kb,D)", "https://arxiv.org/abs/1506.02338v1", null], ["v2", "Wed, 10 Jun 2015 15:42:42 GMT  (446kb,D)", "http://arxiv.org/abs/1506.02338v2", null], ["v3", "Thu, 11 Jun 2015 03:00:29 GMT  (445kb,D)", "http://arxiv.org/abs/1506.02338v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["andrew trask", "david gilmore", "matthew russell"], "accepted": true, "id": "1506.02338"}, "pdf": {"name": "1506.02338.pdf", "metadata": {"source": "META", "title": "Modeling Order in Neural Word Embeddings at Scale", "authors": ["Andrew Trask", "David Gilmore", "Matthew Russell"], "emails": ["ANDREW.TRASK@DIGITALREASONING.COM", "DAVID.GILMORE@DIGITALREASONING.COM", "MATTHEW.RUSSELL@DIGITALREASONING.COM"], "sections": [{"heading": "1. Introduction", "text": "These systems encounter difficulties due to the complexity and thriftiness of natural language. Traditional systems represent words as atomic units with success in a variety of tasks (Katz, 1987), but this approach is limited by the curse of dimensionality and has been surpassed by models of neuralism."}, {"heading": "2. Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Word-level Representations (Word2vec)", "text": "It is a question of whether this is a way in which each word is represented in a particular context, in which the whole word is used in a particular context to predict the existence of the entire context (positive education) or the absence of other randomly sampled words in the window (negative sampling); the resulting vector is used in another context to predict the existence of the entire context (positive education) or the absence of other randomly sampled words in the window (negative sampling); the scalar word is guided through a context window (1 + e) = (1 + e)), which returns the probability of the network that the distant word exists in the middle of the context without any limitation."}, {"heading": "2.2. Character-level Representations", "text": "Recent work has explored techniques for embedding word forms and morphological features in word embeddings, and the resulting embeddings have proven useful for a variety of NLP tasks."}, {"heading": "2.2.1. DEEP NEURAL NETWORK", "text": "(Santos & Zadrozny, 2014) proposed a Deep Neural Network (DNN) that \"learns the representation [s] of words at character level and associates them with common word representations to perform POS tagging.\" The resulting embeddings were used to produce state-of-the-art POS taggers for English and Portuguese datasets. The network architecture uses the revolutionary approach introduced in (Waibel et al., 1990) to generate local features around each character of the word and then combine them to form a fixed character level for embedding the word. Character-level word embeddings are then associated with word-level embeddings learned using word2vec."}, {"heading": "2.2.2. RECURSIVE NEURAL NETWORK", "text": "(Luong et al., 2013) proposed a \"novel model that is able to form representations of morphologically complex words from their morphemes.\" The model uses a recursive neural network (RNN) (Socher et al., 2011) to model the morphology in a word. Words are decomposed into morphemes using a morphological segment (Creutz et al., 2007). Using \"morphemic vectors,\" representations at the word level are constructed for complex words. In the experiments carried out by (Luong et al., 2013), word embeddings were borrowed from (Huang et al., 2012) and (Collobert et al., 2011). After performing a morphemic segmentation, complex words were then improved by morphologically embedding characteristics by using the morphological vectors in the RNN to calculate word representations \"on the fly.\""}, {"heading": "3. The Partitioned Embedding Neural Network Model (PENN)", "text": "PENN improves word2vec by modeling the order in which words occur. It models the order by partitioning both the embedding and the classification levels.The first property of PENN is that each word embedding is partitioned.Each partition is trained differently from each other partition based on the word order, so that each partition models a different probability distribution.These different probability distributions model different perspectives on the same word.The second property of PENN is that the classifier has different inputs for words from different window positions.The classifier is partitioned with the same partition dimensionality as the embed.It is possible to have fewer partitions in the classifier than the embedding in a larger word / number of classifications (with the ratio of mean to mean)."}, {"heading": "3.1. Plausible Configurations", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1.1. WINDOWED", "text": "The simplest configuration of a PENN architecture is the window configuration, where each partition corresponds to a unique window position in which a word occurs. In Figure 2, disillusioned: If there are two window positions (one on each side of the focus term), each embed would have two partitions. If a word is in the partition p = + 1 (the word before the focus term), the partition corresponding to that position is propagated forward and then back, leaving the p = -1 partition unchanged."}, {"heading": "3.1.2. DIRECTIONAL", "text": "The opposite configuration to window-based PENN is the directional configuration. Instead of each partition corresponding to a window position, there are only two partitions. One partition corresponds to each positive, forward predictive window position (left of the focus term) and the other partition corresponds to each negative, backward predictive window position (right of the focus term). For each partition, any embedding corresponding to that partition is summed or averaged when propagated forward."}, {"heading": "3.2. Training Styles", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.2.1. CONTINUOUS LIST OF WORDS (CLOW)", "text": "The continuous word list (CLOW) optimizes the following objective function within the framework of the PENN framework: argmax \u03b8 (BA (W, C), c \u2264 j \u2264 c, j 6 = 0 p (W = 1 | cjj; \u03b8), c (W, C), c \u2264 j \u2264 c, j = 0 p (W = 0 | cjj; \u03b8)), where cjj is the location-specific representation (partition j) for the word at the window position j relative to the focus word, etc. Closely related to the CBOW training method, the CLOW method models the probability that a particular word is present in the middle of the list in view of the presence and location of the other words. For each training example, the middle focus term is removed from a windowless word sequence. Subsequently, a partition based on the position of that word in relation to the focus term is selected from the embedding of each remaining word."}, {"heading": "3.2.2. SKIP-GRAM", "text": "(W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W (W) (W) (W) (W) (W) (W) (W (W) (W) (W) (W) (W) (W) (W (W) (W (W) (W) (W (W) (W (W) (W) (W) (W) (W (W) (W) (W (W) (W (W) (W) (W (W) (W) (W (W) (W) (W (W) (W) (W) (W) (W (W (W) (W (W) (W) (W (W) (W) (W (W) (W) (W) (W) (W ("}, {"heading": "3.3. Distributed Training Optimizations", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.3.1. SKIP-GRAM", "text": "When the Skip-gram is used to model ordered word groups under the PENN frame, each classifier partition and the associated embedding partitions can be trained completely in parallel (without intercommunication) and at the end of the training reach the exact same state as if they were not distributed. A special case for this is the window-based embedding configuration, where each window position can be fully parallel and concatenated (embedding and classifiers) at the end of the training. Thus, very large, rich embedding can be trained in a relatively small, cost-effective time on relatively small machines, with each machine optimizing a portion of the general objective function. Considering the machine j, the Skip-gram optimizes under the window-supported embedding configuration the following objective function: argmax: (w, C), dp: (wj = 1 | cjj;), while during the training many Pj results are trained via a single training module in the ENJ synchronization module."}, {"heading": "3.3.2. CLOW", "text": "The CLOW method is an excellent candidate for the ALOPEX Distributed Training algorithm (Unnikrishnan & Venugopal, 1994) because it trains with very few (often single) output probabilities. Different classification partitions can be trained on different machines, with each training example sending a short list of floats per machine over the network, all sharing the same global error and proceeding to the next iteration.A second, non-trivial optimization is found in the strong performance of the directional CLOW implementation with very small window sizes (pictured below at a window size of 1).Directional CLOW is able to achieve a parity value at a window size of 1, as opposed to word2vec at a window size of 10 when all other parameters are the same, reducing the total training time by a factor of 10."}, {"heading": "4. Dense Interpolated Embedding Model", "text": "We propose a second new neural language model, called the Dense Interpolated Embedding Model (DIEM). DIEM uses character-level learned neural embedding to create a syntactic fixed-length embedding at world level that is useful for syntactic word analogy tasks and uses patterns in signs as human power when syntactic features such as plurality are recognized."}, {"heading": "4.1. Method", "text": "The generation of syntactical embeddings begins with the creation of character embeddings. Character embeddings are created with Vanilla word2vec by predicting a focus charac algorithm 1 Dense Interpolated Embedding Pseudocode Input: word length I, list embeddings (e.g. the word) chari, multiple M, char dim C, vector vm for i = 0 to I \u2212 1 do s = M * i / l for m = 0 to M \u2212 1 do d = pow (1 - (abs (s - m) / M, 2) vm = vm + d * charizing for end-forter in view of its context. These cluster characters are intuitive, vowels with vowels, numbers and uppercase letters. In this way, character embeddings represent morphological building blocks that are more or less similar to each other."}, {"heading": "4.2. Distributed Use and Storage Optimizations", "text": "Syntactic vectors also offer significant scaling and generalization advantages over semantic vectors. New syntactic vectors can be generated cost-effectively for unprecedented words, allowing each word from the initial character formation to be generalized losslessly, provided that the word consists only of characters seen. Syntactic embeddings can be generated in a fully distributed manner and require only a small vector concatenation and vector matrix multiplication per word. Second, character vectors (typically length 32) and transformational embeddings (no more than 20 or so) can be stored very efficiently compared to semantic vocabularies, which can be several million vectors of dimensionality 1000 or more. Despite their significant positive quality effects, DIEM operates optimally with 6 + orders of size less memory and 5 + orders of size less training examples than semantic word embeddings."}, {"heading": "5. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Evaluation Methods", "text": "We are conducting experiments on the word analogy task (Mikolov et al., 2013a), which consists of a variety of word similarity tasks as described in (Luong et al., 2013). Known as the Google analogy data set, it contains 19,544 questions with the question \"a is to b as c is to\" and is divided into semantic and syntactic sections. Both sections are further divided into subcategories based on the analogy type, as shown in the results tables below. All training takes place via the data set available on the Google word2vec website2. The data set contains about 8 billion words collected from the English News Crawl, the 1 trillion word benchmark, the UMBC webbase, and the English Wikipedia. The data set used uses the standard dataphrase2.txt normalization in all training sessions, which contain individual tokens as well as parameters that are different for all school parameters, and non-specific for all school parameters."}, {"heading": "5.2. Embedding Partition Relative Evaluation", "text": "Figure 4 shows the relative accuracy of each partition in a PENN model using line-relative word-analogy values. Other experiments showed that the pattern in the thermal map is consistent across parameter settings. There is a clear difference in quality between window positions predicting forward (left side of the figure) and window positions predicting backward (right side of the figure). Currency achieves most of its predictive power in short-term predictions, whereas Capital Common Countries has a much more even trajectory over the window. These patterns support the intuition that different window positions play different roles in different tasks."}, {"heading": "5.3. Evaluation of CLOW and CBOW", "text": "Table 3 shows the performance of the standard CBOW implementation of word2vec relative to CLOW and DIEM when trying to perform 2000-dimensional embedding. Between tables 3 and 4, we see that an increasing dimension 2https: / / code.google.com / p / word2vec / ality of baseline CBOW word2vec past 500 achieves sub-optimal performance. Therefore, a fair comparison of two models between optimal (as opposed to just identical) parameterization should be made for each model. This is especially important because PENN models model a much richer probability distribution as the order is maintained. Therefore, optimal parameter settings often require larger dimensionality. In contrast to the original CBOW word2vec, we found that larger window sizes are not always better. Larger windows tend to generate a little more semantic embedding, while smaller window sizes tend to generate a little more syntactical emplacement."}, {"heading": "5.4. Evaluation of DIEM Syntactic Vectors on Syntactic Tasks", "text": "Table 4 documents the change in syntactical analogy quality as a result of the interpolated DIEM vectors. For the DIEM experiment, each analogy query was first performed by executing the query for CLOW and DIEM independently of each other and selecting the top thousand CLOW cosine similarities. We added up the square cosine similarity of each of the top thousand with each associated cosine similarity returned by DIEM and resorted to it. It turned out that this was an efficient estimate of the concatenation that did not reduce the quality. Table 5 documents the parameter selection for a combined neural network divided by several training styles and dimensions. As in the experiments in Table 3, each analogy query was first performed independently on each model by selecting the top thousand cosine similarities."}, {"heading": "5.5. High Level Comparisons", "text": "Our final results show an increase in quality and size compared to previous models, with a syntactical increase of 58% over the best published syntactic result and an increase of 40% over the best published overall result (Pennington et al., 2014). Table 5 also contains the highest word2vec values, which we were able to achieve through better parameterization (which also exceeds the best published word2vec values).Within the PENN models, there is a compromise between speed and performance between SGDIEM and CLOW-DIEM. In this case, we achieve a 20x higher degree of parallelism in SG-DIEM relative to CLOW, with each model having training parts of 250 dimensions (250 * 20 = 5000 final dimensionality).A 160 billion parameter network was also trained overnight on 3 multi-core CPUs, but it yielded 20,000 dimensional vectors for each word and then parameters for each word (and then exceeded the training data parameters)."}, {"heading": "6. Conclusion and Future Work", "text": "The encoding of both word and punctuation in neural word embeddings is advantageous for word analogy tasks, especially syntactic tasks. These findings are based on the intuition that order is important in human language and have been validated by the above methods. Future work will further investigate the scalability of these word embeddings to larger data sets with reduced runtime."}], "references": [{"title": "A neural probabilistic language model", "author": ["Bengio", "Yoshua", "Ducharme", "R\u00e9jean", "Vincent", "Pascal", "Janvin", "Christian"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Deep learning with cots hpc systems", "author": ["Coates", "Adam", "Huval", "Brody", "Wang", "Tao", "Wu", "David", "Catanzaro", "Bryan", "Andrew", "Ng"], "venue": "In Proceedings of The 30th International Conference on Machine Learning,", "citeRegEx": "Coates et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2013}, {"title": "Unsupervised models for morpheme segmentation and morphology learning", "author": ["Creutz", "Mathias", "Lagus", "Krista"], "venue": "ACM Transactions on Speech and Language Processing (TSLP),", "citeRegEx": "Creutz et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Creutz et al\\.", "year": 2007}, {"title": "Distributed representations. Parallel dis-tributed processing: Explorations in the microstructure of cognition", "author": ["G.E. Hinton", "J.L. McClelland", "D.E. Rumelhart"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1986}, {"title": "word2vec explained: deriving mikolov et al.\u2019s negative-sampling word-embedding method", "author": ["Goldberg", "Yoav", "Levy", "Omer"], "venue": "CoRR, abs/1402.3722,", "citeRegEx": "Goldberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2014}, {"title": "Estimation of probabilities from sparse data for the language model component of a speech recognizer", "author": ["Katz", "Slava M"], "venue": "In IEEE Transactions on Acoustics, Speech and Singal processing,", "citeRegEx": "Katz and M.,? \\Q1987\\E", "shortCiteRegEx": "Katz and M.", "year": 1987}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Quoc V", "Mikolov", "Tomas"], "venue": "CoRR, abs/1405.4053,", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Bringing machine learning and compositional semantics together", "author": ["P. Liang", "C. Potts"], "venue": "Annual Reviews of Linguistics,", "citeRegEx": "Liang and Potts,? \\Q2015\\E", "shortCiteRegEx": "Liang and Potts", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Luong", "Minh-Thang", "Socher", "Richard", "Manning", "Christopher D"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "A probabilistic model for semantic word vectors", "author": ["Maas", "Andrew L", "Ng", "Andrew Y"], "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "Maas et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2010}, {"title": "Introduction to information retrieval, volume 1", "author": ["Manning", "Christopher D", "Raghavan", "Prabhakar", "Sch\u00fctze", "Hinrich"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "CoRR, abs/1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Mikolov", "Tomas", "tau Yih", "Wen", "Zweig", "Geoffrey"], "venue": "In HLT-NAACL,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["Mnih", "Andriy", "Hinton", "Geoffrey E"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Mnih et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2009}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Morin", "Frederic", "Bengio", "Yoshua"], "venue": "In Proceedings of the international workshop on artificial intelligence and statistics,", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "Glove: Global vectors for word representation", "author": ["Pennington", "Jeffrey", "Socher", "Richard", "Manning", "Christopher"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Deep learning for character-based information extraction", "author": ["Qi", "Yanjun", "Das", "Sujatha G", "Collobert", "Ronan", "Weston", "Jason"], "venue": "In Advances in Information Retrieval,", "citeRegEx": "Qi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Qi et al\\.", "year": 2014}, {"title": "Machine learning in automated text categorization", "author": ["Sebastiani", "Fabrizio"], "venue": "ACM Comput. Surv.,", "citeRegEx": "Sebastiani and Fabrizio.,? \\Q2002\\E", "shortCiteRegEx": "Sebastiani and Fabrizio.", "year": 2002}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Socher", "Richard", "Lin", "Cliff C", "Manning", "Chris", "Ng", "Andrew Y"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Alopex: A correlation-based learning algorithm for feedforward and recurrent neural networks", "author": ["KP Unnikrishnan", "Venugopal", "Kootala P"], "venue": "Neural Computation,", "citeRegEx": "Unnikrishnan et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Unnikrishnan et al\\.", "year": 1994}], "referenceMentions": [{"referenceID": 15, "context": "8% on a recent word-analogy task, exceeding best published syntactic word-analogy scores by a 58% error margin (Pennington et al., 2014).", "startOffset": 111, "endOffset": 136}, {"referenceID": 1, "context": "Furthermore, the model includes several parallel training methods, most notably allowing a skip-gram network with 160 billion parameters to be trained overnight on 3 multi-core CPUs, 14x larger than the previous largest neural network (Coates et al., 2013).", "startOffset": 235, "endOffset": 256}, {"referenceID": 0, "context": "NNLMs overcome the curse of dimensionality by learning distributed representations for words (G.E. Hinton, 1986; Bengio et al., 2003).", "startOffset": 93, "endOffset": 133}, {"referenceID": 0, "context": "Specifically, neural language models embed a vocabulary into a smaller dimensional linear space that models \u201cthe probability function for word sequences, expressed in terms of these representations\u201d (Bengio et al., 2003).", "startOffset": 199, "endOffset": 220}, {"referenceID": 10, "context": "The result is a vector space model (Maas & Ng, 2010) that encodes semantic and syntactic relationships and has defined a new standard for feature generation in NLP (Manning et al., 2008; Sebastiani, 2002; Turian et al., 2010).", "startOffset": 164, "endOffset": 225}, {"referenceID": 8, "context": "However, including information about word structure in word representations has proven valuable for part of speech analysis (Santos & Zadrozny, 2014), word similarity (Luong et al., 2013), and information extraction (Qi et al.", "startOffset": 167, "endOffset": 187}, {"referenceID": 16, "context": ", 2013), and information extraction (Qi et al., 2014).", "startOffset": 36, "endOffset": 53}, {"referenceID": 8, "context": "(Luong et al., 2013) proposed a \u201cnovel model that is capable of building representations for morphologically complex words from their morphemes.", "startOffset": 0, "endOffset": 20}, {"referenceID": 18, "context": "\u201d The model leverages a recursive neural network (RNN) (Socher et al., 2011) to model morphology in a word embedding.", "startOffset": 55, "endOffset": 76}, {"referenceID": 8, "context": "In the experiments performed by (Luong et al., 2013), word embeddings were borrowed from (Huang et al.", "startOffset": 32, "endOffset": 52}, {"referenceID": 8, "context": "It is made up of a variety of word similarity tasks, as described in (Luong et al., 2013).", "startOffset": 69, "endOffset": 89}, {"referenceID": 15, "context": "Our final results show a lift in quality and size over previous models with a 58% syntactic lift over the best published syntactic result, and a 40% overall lift over the best published overall result (Pennington et al., 2014).", "startOffset": 201, "endOffset": 226}, {"referenceID": 1, "context": "2 billion parameters (Coates et al., 2013), whereas CLOW and the largest SG contain 16 billion (trained all together) and 160 billion (trained across a cluster) parameters respectively as measured by the number of weights.", "startOffset": 21, "endOffset": 42}], "year": 2015, "abstractText": "Natural Language Processing (NLP) systems commonly leverage bag-of-words co-occurrence techniques to capture semantic and syntactic word relationships. The resulting word-level distributed representations often ignore morphological information, though character-level embeddings have proven valuable to NLP tasks. We propose a new neural language model incorporating both word order and character order in its embedding. The model produces several vector spaces with meaningful substructure, as evidenced by its performance of 85.8% on a recent word-analogy task, exceeding best published syntactic word-analogy scores by a 58% error margin (Pennington et al., 2014). Furthermore, the model includes several parallel training methods, most notably allowing a skip-gram network with 160 billion parameters to be trained overnight on 3 multi-core CPUs, 14x larger than the previous largest neural network (Coates et al., 2013).", "creator": "LaTeX with hyperref package"}}}