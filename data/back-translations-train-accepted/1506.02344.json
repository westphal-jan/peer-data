{"id": "1506.02344", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2015", "title": "Stay on path: PCA along graph paths", "abstract": "We introduce a variant of (sparse) PCA in which the set of feasible support sets is determined by a graph. In particular, we consider the following setting: given a directed acyclic graph $G$ on $p$ vertices corresponding to variables, the non-zero entries of the extracted principal component must coincide with vertices lying along a path in $G$.", "histories": [["v1", "Mon, 8 Jun 2015 03:37:36 GMT  (2565kb,D)", "https://arxiv.org/abs/1506.02344v1", "12 pages, 5 figures, In Proceedings of International Conference on Machine Learning (ICML) 2015"], ["v2", "Fri, 19 Jun 2015 02:27:49 GMT  (2563kb,D)", "http://arxiv.org/abs/1506.02344v2", "12 pages, 5 figures, In Proceedings of International Conference on Machine Learning (ICML) 2015"]], "COMMENTS": "12 pages, 5 figures, In Proceedings of International Conference on Machine Learning (ICML) 2015", "reviews": [], "SUBJECTS": "stat.ML cs.IT cs.LG math.IT math.OC", "authors": ["megasthenis asteris", "anastasios kyrillidis", "alexandros g dimakis", "han-gyol yi", "bharath chandrasekaran"], "accepted": true, "id": "1506.02344"}, "pdf": {"name": "1506.02344.pdf", "metadata": {"source": "META", "title": "Stay on path: PCA along graph paths", "authors": ["Megasthenis Asteris", "Anastasios Kyrillidis", "Alexandros G. Dimakis", "Han-Gyol Yi", "Bharath Chandrasekaran"], "emails": ["MEGAS@UTEXAS.EDU", "ANASTASIOS@UTEXAS.EDU", "DIMAKIS@AUSTIN.UTEXAS.EDU", "GYOL@UTEXAS.EDU", "BCHANDRA@AUSTIN.UTEXAS.EDU"], "sections": [{"heading": null, "text": "From a statistical point of view, information about the underlying network can potentially reduce the number of observations required to restore the main component of the population. We look at the canonical estimator who makes optimal use of prior knowledge by solving a non-convex square maximization of empirical covariance. We introduce a simple network and analyze the estimator within the framework of the pointed covariance model. We show that side information potentially improves statistical complexity. We propose two algorithms to approximate the solution of the limited square maximization and restore a component with the desired properties."}, {"heading": "1. Introduction", "text": "It is about the question of whether it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, and in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way and in which it is about a way, in which it is about a way and in which it is about a way, in which it is about a way and in which it is about a way, in which it is about a way and in which it is about a way, in which it is about which it is about a way and in which it is about a way and in which it is about a way and in which it is about which it is about a way and in which it is about which it is about a way and in which it is about which it is about a way and in which it is about which it is about a way and in which it is about which it is about a way and in which it is about which it is about which it is about which it is about which it is about a way and in which it is about which it is about which it is about which it is about a way and in which it is about which it is about which it is about which it is about a way and in which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about a way and which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about"}, {"heading": "2. A Data Model \u2013 Sample Complexity", "text": "The layer diagram. Consider a directed acyclic graph G = (V, E) on p-2 vertices, with the following properties: \u2022 V = {S, T}, where S is a source vertex, T is an endpoint one, and V is the set of remaining p-2 vertices. \u2022 V \u00b2 can be divided into k fragmented subsets (layers). (layer) p \u00b2,....... p \u00b2, where Li = V, and Li \u00b2 Lj = V, i, j [k], i = j, so that: - (v) fragmented subsets (layer) p + 1,."}, {"heading": "2.1. Lower bound", "text": "Theorem 1 (Lower Bound) {1 + 2 (Lower Bound). Let us consider a (p, k, d) layer diagram G on p vertical, with k, 4, and log d, 4H (3 / 4). (Note that p \u2212 2 (0, Ip + \u03b2), and a signal x? (G), for some \u03b2 > 0. Let us leave D (n) p (x) p (x?), let us evaluate the product measurement by the n, n) n, x. \"(X). (D) p (n) p?), let us evaluate the problem of estimating x.\" (X)."}, {"heading": "2.2. Upper bound", "text": "Our upper limit is based on the estimate we obtained via the restricted square distribution. < < < < < / p > p > p > p > p > p > p > p > p >. We note that the analysis is not limited to the increased covariance model; it refers to a broader class of distributions (see Assum. & # 8222; & # 8220;. & # 8222; p & # 8220;. & # 8222;. & # 8222; & # 8220; p & # 8222; & # 8220;. & # 8220; & # 82p; & # 8222; p & # 8220; p & # 8220; p & # 8222; p & # 8220; p; p & # 82p; p; p; 8220; p; p & # 8222; p; p; p # 8222; p; p; p; & # 8220; p; p # 8220; p; p; p # 8220; p; & # 8220; & # 8220; & # 82p; & # 82p; & # 82p; & # 8220; & # 82p; p; p; & # 8222; p; p; p & # 8220; p; p; p; p; p & # 8222; p; p & # 8220; p; p; p; p; p; p; p; p & # 8220; p; p; p; p; p; p; p; p; p; p; p; 160 & # 160 & 160 & 160 & 160 & 160 & 160 & 160 & 160 & 160 & 160 p; p; p; p; & 160 & 160 & 160 & 160 p; p; p; p; p; 160 & 160 & 160 & 160 & 160 & 160 & 160 & 160 p; p; p; p; p; p & 160 & 160 & 160 & 160 & 160 p; p; p; p; p; p & 160 & 160 & 160 & 160 & 160 & 160 p; p; p; p; p; p; p & 160 & 160 & 160 & 160 & 160 p; p; p; p; p; p"}, {"heading": "3. Algorithmic approaches", "text": "We propose two algorithms to approximate the solution of the problem of restricted square maximization in (3): 1. The first is an adaptation of the method of truncated potentiality iteration of (Yuan & Zhang, 2013) to the problem of calculating sparse eigenvectors; 2. The second is based on an approximate solution (3) on a slight approximation to \u03a3, similar (Papailiopoulos et al., 2013; Asteris et al., 2014). Both algorithms are based on a projection operation of Rp on the realizable quantity X (G), for a given graph G = (V, E). Besides the projection step, the algorithms are not aware of the peculiarities of the constraint set, 2 and can adapt to different constraints by modifying the projection process."}, {"heading": "3.1. Graph-Truncated Power Method", "text": "Algorithm 1 Graph-Truncated Power Method input. (p) (p), G = (V, E), x0 \"Rp1: i\" 0 \"2: repeat 3: wi\" xi \"4: xi + 1\" ProjX (G) (wi) 5: i \"i + 1 6: until Convergence / Stop Criterion Output xiWe consider a simple iterative method, which resembles the intended solution of (Yuan & Zhang, 2013) for the problem of calculating sparse eigenvectors. Our algorithm produces sequences of vectors xi\" X \"(G), i\" 0 \"which serve as interim estimates of the desired solution of (3). The method is summarized in Algorithm 1. In ith\" iteration, \"the current estimate xi is multiplied by the empirical covariance xi\" (G)."}, {"heading": "3.2. Low-Dimensional Sample and Project", "text": "The second algorithm provides an estimate of the desired solution of (3) by (approximately) solving the limited square maximization not on the original matrix, but on a low rank. \u2212 \u2212 r The corresponding eigenvector is q = r \u00b2 i = 1 \u03bbiqiq > i = r \u00b2 i = 1 viv > i = VV >, (20) The approximate rank r is an accuracy parameter. \u2212 r Our algorithm typically operates on p \u00b2 r and seeksxr, arg max x \u00b2 X (G) x > p \u00b2 matrix. (21) The motivation is that a (approximate) solution to the low-level problem can be efficiently calculated in (21)."}, {"heading": "3.2.1. THE LOW RANK PROBLEM", "text": "The rank r maximization in (21) can be written as maximum x-x-x-x-x-x-x-x-x-x-x-x (see Asteris et al., 2014), as double maximization using the variables c-Sr \u2212 1 and x-Rp: max x-x-x-x (G). (23) The rank 1 solution is just a vector in Rp. (24) The x that maximizes the target in (23) (as a function of c) isx (c)."}, {"heading": "3.3. The Projection Operator", "text": "The algorithms 1 and 2 are based on a projection operation of Rp on the realizable quantity X (G) (Eq. (4)).We show that the projection effectively solves the longest path problem on (a weighted variant of) G.The projection operation defined in Eq. (19) can be equivalent to 4 written asProjX (G) (w), arg max x x (G) w > x.For each x x x (G), supp (x).P (G).For a given quantity \u03c0, by the Cauchy-Black inequality, w > x = [n]."}, {"heading": "4. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Synthetic Data.", "text": "We evaluate alg. 1 and 2 using synthetic data generated according to the model of Sec. 2. We consider two measurements: the loss function and the loss function and the loss function. We create a (p, k, d) layer diagram G with k = log and out-degree d = p / k, i.e. each vertex is connected to all vertex in the following layer. We add auxiliary sources and terminal vertex points S and T with edges to the original vertex points as shown in Fig. 1. For each random realization, we first construct a signal x? ig X (G) as follows: We randomly select an S-T path consisting of G and assign random zero averages of the Gaussian values to the entries of the x plane. The signal is scaled to the unit length."}, {"heading": "4.2. Finance Data.", "text": "This data set contains the daily closing prices for 425 shares of the S & P 500 Index over a period of 1259 days (5 years): 01 / 02 / 2010 - 28 / 01 / 2015, collected by Yahoo! Finance. Shares are classified according to the Global Industry Classification Standard (GICS) in 10 business sectors, e.g. energy, healthcare, information technology, etc. (see fig. 4 for the full list) We are looking for a series of shares that include a single representative from each GICS sector covering the largest part of the variance in the data set. Likewise, we want to calculate a structured main component that is limited to exactly 10 entries to zero; one for each GICS sector. Consider a layer chart G = (V, E) (similar to the one in fig. 1) on p = 425 nodes corresponding to the 425 shares, divided into k = 10 groups (layers) L1,."}, {"heading": "4.3. Neuroscience Data.", "text": "Participants were not instructed to perform any explicit cognitive task during the entire scan (Van Essen et al., 2013).Data were extracted from the Human Connectome Project, WU-Minn Consortium.6Mean timeseries of n = 1200 points for p = 111 regions of-interest (ROIs) based on the Harvard Oxford Atlas (Desikan et al., 2006).The time frame for the analysis is limited to 0.01-0.1Hz. Based on recent results on dormant state fMRI neural networks, we set the posterior cingulate cortex as source node S, and the prefrontal cortex as target node T (Greicius et al., 2009)."}, {"heading": "5. Conclusions", "text": "We introduced a new problem: sparse PCAs, where the amount of feasible support sets is determined by a graph of the variables. We focused on the specific case where practicable splitting patterns match the paths on the underlying graph. We proposed an upper limit on the statistical complexity of the constrained square maximization estimate (3), within the framework of a simple graph model, supplemented by a lower limit on the Minimax error. Finally, we proposed two algorithms to extract a component that takes into account the graph constraints, and applied them to real data from finance and neuroscience. One potential future direction is to expand the amount of graph-induced splitting patterns (beyond paths) that can lead to interpretable solutions and are computationally manageable. We hope that this work will trigger future efforts to incorporate such an underlying research structure into different research and applications."}, {"heading": "6. Acknowledgments", "text": "The authors would like to thank NSF CCF 1422549, 1344364, 1344179 and an ARO YIP Award for their support."}, {"heading": "7. Proof of Lemma 2.2 \u2013 Local Packing Set", "text": "To prove Lemma 2.2, we are developing a modified version of the Varshamov-Gilbert protocol that is adapted to our specific model: the set of characteristic vectors of the S-T paths of a (p, k, d) layer diagram G.Let a (p, k) layer diagram G on p vertices and the collection P (G) of S-T paths in G. Leti, {x, y) p: xi 6 = yi} |.Lemma 7.4. Consider a (p, k) layer diagram G on p vertices and the collection P (G) of S-T paths in G. Leti, {x, p: xi 6 = yi}."}, {"heading": "8. Details in proof of Lemma 1", "text": "We want to show that if2 = min {1, C \u2032 \u00b7 (1 + \u03b2) \u03b22 \u00b7 log p \u2212 2 k + k 4 \u00b7 log d n}, for an appropriate choice of C \u2032 > 0, then the following two conditions (eq. (13) are met: n \u00b7 2 2\u03b22 (1 + \u03b2) 1 log | X | \u2264 1 4 and log | X | 4 log 2.For the second inequality, note that in terms of lemmas 2.2, log | X | \u2265 log p \u2212 2 k + 1 4 \u00b7 k log d > 0. (35) Under the assumptions of Thm. 1 on parameters k and d (note that p \u2212 2 \u2265 k \u00b7 d according to the structure of G), log | X | \u2265 log p \u2212 2 k + k 4 \u00b7 log d 4 \u00b7 4 \u00b7 H (3 / 4) \u2265 4 log 2 (3 / 4), which is the desired result. For the first inequality, we consider two cases: \u2022 log \u2212 2 \u2212 C = 1, i.e., log \u2212 d \u00b7 \u00b7 k \u00b7 2 \u00b7 k + 2 \u00b7 k (2 / 4)."}, {"heading": "9. Other", "text": "Assumption 1. There are i.i.d. random vectors z1,..., zn-Rp, so that Ezi = 0 and Eziz > i = Ip, y = \u00b5 + \u03a3 1 / 2zi (38) and sup x-Sp \u2212 12 \u0445 z > i x-Rp 2 \u2264 K, (39) where \u00b5-Rp and K > 0 are a constant depending on the distribution of the Zis."}], "references": [{"title": "High-dimensional analysis of semidefinite relaxations for sparse principal components", "author": ["Amini", "Arash", "Wainwright", "Martin"], "venue": "In Information Theory,", "citeRegEx": "Amini et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Amini et al\\.", "year": 2008}, {"title": "High-dimensional analysis of semidefinite relaxations for sparse principal components", "author": ["Amini", "Arash", "Wainwright", "Martin"], "venue": "The Annals of Statistics,", "citeRegEx": "Amini et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Amini et al\\.", "year": 2009}, {"title": "Nonnegative sparse PCA with provable guarantees", "author": ["Asteris", "Megasthenis", "Papailiopoulos", "Dimitris", "Dimakis", "Alexandros"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Asteris et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Asteris et al\\.", "year": 2014}, {"title": "Group-sparse model selection: Hardness and relaxations", "author": ["Baldassarre", "Luca", "Bhan", "Nirav", "Cevher", "Volkan", "Kyrillidis", "Anastasios"], "venue": "arXiv preprint arXiv:1303.3207,", "citeRegEx": "Baldassarre et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Baldassarre et al\\.", "year": 2013}, {"title": "Model-based compressive sensing", "author": ["R. Baraniuk", "V. Cevher", "M. Duarte", "C. Hegde"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Baraniuk et al\\.,? \\Q1982\\E", "shortCiteRegEx": "Baraniuk et al\\.", "year": 1982}, {"title": "Introduction to Algorithms", "author": ["Cormen", "Thomas", "Stein", "Clifford", "Rivest", "Ronald", "Leiserson", "Charles"], "venue": "McGraw-Hill Higher Education, 2nd edition,", "citeRegEx": "Cormen et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Cormen et al\\.", "year": 2001}, {"title": "A direct formulation for sparse PCA using semidefinite programming", "author": ["d\u2019Aspremont", "Alexandre", "El Ghaoui", "Laurent", "Jordan", "Michael", "Lanckriet", "Gert"], "venue": "SIAM review,", "citeRegEx": "d.Aspremont et al\\.,? \\Q2007\\E", "shortCiteRegEx": "d.Aspremont et al\\.", "year": 2007}, {"title": "Optimal solutions for sparse principal component analysis", "author": ["d\u2019Aspremont", "Alexandre", "Bach", "Francis", "Ghaoui", "Laurent El"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "d.Aspremont et al\\.,? \\Q2008\\E", "shortCiteRegEx": "d.Aspremont et al\\.", "year": 2008}, {"title": "Resting-state functional connectivity reflects structural connectivity in the default mode network", "author": ["Greicius", "Michael D", "Supekar", "Kaustubh", "Menon", "Vinod", "Dougherty", "Robert F"], "venue": "Cerebral cortex,", "citeRegEx": "Greicius et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Greicius et al\\.", "year": 2009}, {"title": "Structured sparse principal component analysis", "author": ["Jenatton", "Rodolphe", "Obozinski", "Guillaume", "Bach", "Francis"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Jenatton et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jenatton et al\\.", "year": 2010}, {"title": "Sparse principal components analysis", "author": ["Johnstone", "Iain", "Lu", "Arthur Yu"], "venue": "Unpublished manuscript,", "citeRegEx": "Johnstone et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Johnstone et al\\.", "year": 2004}, {"title": "Combinatorial selection and least absolute shrinkage via the CLASH algorithm", "author": ["Kyrillidis", "Anastasios", "Cevher", "Volkan"], "venue": "In Information Theory Proceedings (ISIT),", "citeRegEx": "Kyrillidis et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kyrillidis et al\\.", "year": 2012}, {"title": "Path coding penalties for directed acyclic graphs", "author": ["Mairal", "Julien", "Yu", "Bin"], "venue": "In Proceedings of the 4th NIPS Workshop on Optimization for Machine Learning (OPTa\u0302A\u0306Z\u030111). Citeseer,", "citeRegEx": "Mairal et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2011}, {"title": "Empirical processes with a bounded \u03c8-1 diameter", "author": ["Mendelson", "Shahar"], "venue": "Geometric and Functional Analysis,", "citeRegEx": "Mendelson and Shahar.,? \\Q2010\\E", "shortCiteRegEx": "Mendelson and Shahar.", "year": 2010}, {"title": "Sparse PCA through low-rank approximations", "author": ["Papailiopoulos", "Dimitris", "Dimakis", "Alex", "Korokythakis", "Stavros"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Papailiopoulos et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Papailiopoulos et al\\.", "year": 2013}, {"title": "The WU-Minn human connectome project: An overview", "author": ["Van Essen", "David", "Smith", "Stephen", "Barch", "Deanna", "Behrens", "Timothy", "Yacoub", "Essa", "Ugurbil", "Kamil", "Consortium", "WUMinn HCP"], "venue": null, "citeRegEx": "Essen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Essen et al\\.", "year": 2013}, {"title": "Minimax rates of estimation for sparse PCA in high dimensions", "author": ["Vu", "Vincent", "Lei", "Jing"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Vu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Vu et al\\.", "year": 2012}, {"title": "Reward-related fMRI activation of dopaminergic midbrain is associated with enhanced hippocampus-dependent long-term memory formation", "author": ["Wittmann", "Bianca", "Schott", "Bj\u00f6rn", "Guderian", "Sebastian", "Frey", "Julietta", "Heinze", "Hans-Jochen", "D\u00fczel", "Emrah"], "venue": null, "citeRegEx": "Wittmann et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wittmann et al\\.", "year": 2005}, {"title": "Truncated power method for sparse eigenvalue problems", "author": ["Yuan", "Xiao-Tong", "Zhang", "Tong"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Yuan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2013}, {"title": "Sparse principal component analysis", "author": ["Zou", "Hui", "Hastie", "Trevor", "Tibshirani", "Robert"], "venue": "Journal of computational and graphical statistics,", "citeRegEx": "Zou et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 14, "context": "We propose two algorithms for approximating the solution of (3), based on those of (Yuan & Zhang, 2013) and (Papailiopoulos et al., 2013; Asteris et al., 2014) for the sparse PCA problem.", "startOffset": 108, "endOffset": 159}, {"referenceID": 2, "context": "We propose two algorithms for approximating the solution of (3), based on those of (Yuan & Zhang, 2013) and (Papailiopoulos et al., 2013; Asteris et al., 2014) for the sparse PCA problem.", "startOffset": 108, "endOffset": 159}, {"referenceID": 19, "context": "Related Work There is a large volume of work on algorithms and the statistical analysis of sparse PCA (Johnstone & Lu, 2004; Zou et al., 2006; d\u2019Aspremont et al., 2008; 2007; Johnstone & Lu, 2004; Vu & Lei, 2012; Amini & Wainwright, 2009).", "startOffset": 102, "endOffset": 238}, {"referenceID": 7, "context": "Related Work There is a large volume of work on algorithms and the statistical analysis of sparse PCA (Johnstone & Lu, 2004; Zou et al., 2006; d\u2019Aspremont et al., 2008; 2007; Johnstone & Lu, 2004; Vu & Lei, 2012; Amini & Wainwright, 2009).", "startOffset": 102, "endOffset": 238}, {"referenceID": 9, "context": "Motivated by a face recognition application, (Jenatton et al., 2010) introduce structured sparse PCA using a regularization that encodes higher-order information about the data.", "startOffset": 45, "endOffset": 68}, {"referenceID": 3, "context": "Finally, we note that the idea of pursuing additional structure on top of sparsity is not limited to PCA: Modelbased compressive sensing seeks sparse solutions under a restricted family of sparsity patterns (Baldassarre et al., 2013; Baraniuk et al., 2010; Kyrillidis & Cevher, 2012), while structure induced by an underlying network is found in (Mairal & Yu, 2011) for sparse linear regression.", "startOffset": 207, "endOffset": 283}, {"referenceID": 14, "context": "The second relies on approximately solving (3) on a low rank approximation of \u03a3\u0302, similar to (Papailiopoulos et al., 2013; Asteris et al., 2014).", "startOffset": 93, "endOffset": 144}, {"referenceID": 2, "context": "The second relies on approximately solving (3) on a low rank approximation of \u03a3\u0302, similar to (Papailiopoulos et al., 2013; Asteris et al., 2014).", "startOffset": 93, "endOffset": 144}, {"referenceID": 2, "context": "and in turn (see (Asteris et al., 2014) for details), as a double maximization over the variables c \u2208 Sr\u22121 and x \u2208 R:", "startOffset": 17, "endOffset": 39}, {"referenceID": 5, "context": "In the case of DAGs, however, it can be solved using standard algorithms relying on topological sorting in time O(|V |+ |E|) (Cormen et al., 2001), i.", "startOffset": 125, "endOffset": 146}, {"referenceID": 14, "context": "of (Papailiopoulos et al., 2013), respectively.", "startOffset": 3, "endOffset": 32}, {"referenceID": 8, "context": "Based on recent results on resting state fMRI neural networks, we set the posterior cingulate cortex as a source node S, and the prefrontal cortex as a target node T (Greicius et al., 2009).", "startOffset": 166, "endOffset": 189}, {"referenceID": 8, "context": "late cortex and the prefrontal cortex (Greicius et al., 2009).", "startOffset": 38, "endOffset": 61}, {"referenceID": 17, "context": "The nucleus accumbens receives input from the hippocampus, and plays an important role in memory consolidation (Wittmann et al., 2005).", "startOffset": 111, "endOffset": 134}], "year": 2015, "abstractText": "We introduce a variant of (sparse) PCA in which the set of feasible support sets is determined by a graph. In particular, we consider the following setting: given a directed acyclic graph G on p vertices corresponding to variables, the non-zero entries of the extracted principal component must coincide with vertices lying along a path in G. From a statistical perspective, information on the underlying network may potentially reduce the number of observations required to recover the population principal component. We consider the canonical estimator which optimally exploits the prior knowledge by solving a non-convex quadratic maximization on the empirical covariance. We introduce a simple network and analyze the estimator under the spiked covariance model. We show that side information potentially improves the statistical complexity. We propose two algorithms to approximate the solution of the constrained quadratic maximization, and recover a component with the desired properties. We empirically evaluate our schemes on synthetic and real datasets.", "creator": "LaTeX with hyperref package"}}}