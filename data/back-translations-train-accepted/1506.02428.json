{"id": "1506.02428", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2015", "title": "Robust Regression via Hard Thresholding", "abstract": "We study the problem of Robust Least Squares Regression (RLSR) where several response variables can be adversarially corrupted. More specifically, for a data matrix X \\in R^{p x n} and an underlying model w*, the response vector is generated as y = X'w* + b where b \\in R^n is the corruption vector supported over at most C.n coordinates. Existing exact recovery results for RLSR focus solely on L1-penalty based convex formulations and impose relatively strict model assumptions such as requiring the corruptions b to be selected independently of X.", "histories": [["v1", "Mon, 8 Jun 2015 10:13:53 GMT  (75kb,D)", "http://arxiv.org/abs/1506.02428v1", "24 pages, 3 figures"]], "COMMENTS": "24 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["kush bhatia", "prateek jain 0002", "purushottam kar"], "accepted": true, "id": "1506.02428"}, "pdf": {"name": "1506.02428.pdf", "metadata": {"source": "CRF", "title": "Robust Regression via Hard Thresholding", "authors": ["Kush Bhatia", "Prateek Jain", "Purushottam Kar"], "emails": ["t-kushb@microsoft.com", "prajain@microsoft.com", "t-purkar@microsoft.com"], "sections": [{"heading": null, "text": "In this thesis, we examine a simple hard-threshold algorithm called torrent that can restore exactly w \u0445 under mild conditions under X, even if b corrupts the response variables in an adversarial way, i.e. both the support and the entries of b are contractually selected after observing X and w x. Our results are based on deterministic assumptions that are fulfilled when X is sampled from any sub-Gaussian distribution. Finally, we propose descent torrent extensions that are efficiently applicable to major problems, such as a high-dimensional, sparse recovery, and similar recovery guarantees for these extensions. Empirically, we find torrent, and even more its extensions, which offer significantly faster recovery than the state-of-the-art L1 solvers. For example, even moderate datasets (with 50 p = 40% fewer errors than the proposed method Yver 1) will be the most equivalent."}, {"heading": "1 Introduction", "text": "This means that at the same time we want to solve the problem of miscalculations and miscalculations with respect to the number of regression coefficients in the presence of several arbitrary corruptions in the response vector. (RLSR) However, the problem of miscalculations is also that a critical component of several important real application areas cannot exist in a multitude of areas such as signal processing and the associated response vector. (RLSR) The aim of RLSR is to learn one (S) = arg min w = rpS (n): (1 \u2212 \u03b2) \u00b7 n (S) \u00b7 n (xTi \u2212 xTi w) 2, (1) This means that we want to simultaneously determine the problem of miscalculations and miscalculations of miscalculations. (RLSR)"}, {"heading": "2 Problem Formulation", "text": "We assume that the response vector y is generated with the following model: y = y + b + \u03b5, where the data is generated. (1) We assume that the response vector y can be selected with the following model, after the data has been generated. (1) The above model allows two types of upheaval to yi - dense, but bounded noise to yi. (e.g. white noise.) We assume that the model represents the regressor to be chosen in an adaptive manner after the data has been generated. (0, 2) that we introduce the possible unbounded corruption bi - to an adversary. The only condition we force is that the gross corruption is spared."}, {"heading": "3 Torrent: Thresholding Operator-based Robust Regression Method", "text": "In fact, it is not that we would be able to set such a development in motion. (...) In fact, it is that we would be able to set such a development in motion. (...) In fact, it is not that we would be able to set it in motion. (...) In fact, it is that we would be able to suppress it. (...) In a position to suppress it. (...) In a position to suppress it. (...) In a position to be able to suppress it. (...) In a position to be able to suppress it. (...) In a position to suppress it. (...) In a position to suppress it. (...) In a position to suppress it. (...) In a position to suppress it. (...) In a position to suppress it. (...) In a position to suppress it. (...) In a position to suppress it. (...) In a position to suppress it. (...) In a position to suppress it. (...) In a position to suppress it."}, {"heading": "4 Convergence Guarantees", "text": "For the sake of simplicity of presentation, we will first present our convergence analyses for cases where dense noise is not present, i.e. y = X > w + b, and will deal with cases of dense noise and sparse corruption that follow later. \u2212 We will first analyze the fully corrective torrent FC algorithm, which creates a new active set by selecting points with the least residual error on the current regressor.Theorem 3. Let X = [x1,. xn].Rp \u00b7 n be the given data matrix and y = XTw. + b be the corrupted output with the least residual error on the current regressor.Theorem 3. Let X = [x1,. xn]."}, {"heading": "5 High-dimensional Robust Regression", "text": "In this section, we expand our approach to the robust, high-dimensional, sparse recovery. As before, we assume that the response vector y is obtained as follows: y = X > w + b, where it refers to a constant, constant, constant, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, re"}, {"heading": "6 Experiments", "text": "The experiments show that torrent not only provides statistically better recovery properties compared to L1-style approaches, but that it can be more than one order of magnitude faster than one order of magnitude. Data: For low dimensionality, the regressor w-x was chosen as a random unit standard vector. Data were evaluated as xi-value N (0, Ip) and response variables as y-value i = < w-value of the corrupt points S-value as a uniformly random unit standard vector."}, {"heading": "6.1 Low Dimensional Results", "text": "Recovery Property: The phase transition parts shown in Figure 1 represent our recovery experiments in graphical form. Both the fully corrective and the hybrid variants of torrent show better recovery characteristics than the L1 minimization approach, which is indicated by the number of runs in which the algorithm was able to correctly recover w from 100 runs. Figure 2 shows the deviation in recovery errors as a function of alpha in the presence of white noise and shows the superiority of torrent FC and torrent HYB over L1-DALM. Again, torrent FC and torrent HYB achieve significantly lower recovery errors than L1-DALM for all alpha < = 0.5. Figure 3 in the appendix shows that the deviations of rents of rents very w-HYB over rents are not able. Also in Figure 2 Torrent FC and torrent HYB, as a resource error, all Torrent Rent Rent Rent Rent Rent Rent Errors achieve a very high < as a resource Rate Rate Rate Rate Rate Rate Rate Rate error < = 0.5."}, {"heading": "6.2 High Dimensional Results", "text": "Recovery Property: Figure 2 shows the deviation of the recovery error in the high-dimensional environment, because the number of damaged points varied. In these experiments, n was set to 5s \u0445 log (p) and the proportion of damaged points \u03b1 varied from 0.1 to 0.7. While L1-DALM does not recover w for \u03b1 > 0.5, Torrent-HD offers perfect recovery even for \u03b1 values up to 0.7 runtime: Figure 2 shows the deviation of the recovery error as a function of the runtime in this setting. L1-DALM proved to be an order of magnitude slower than Torrent-HD, making it impossible to perform for sparse high-dimensional settings. One main reason for this is that the L1-DALM solver is significantly slower in identifying the clean points. While Torrent-HD, for example, was able to identify the clean points in only 5 iterations, L1-DALM-Solver needed to do the same 250 or so."}, {"heading": "A Convergence Guarantees with Dense Noise and Sparse Corrup-", "text": "We will now present business cycle guarantees for the torrent FC algorithm if both dense noise and slight opposing corruptions are present. \u2212 Target and loss numbers for Torrent GD and Torrent HYB will follow in a similar manner. \u2212 Target and loss calculation 10. Let X = [x1,., xn] be executed on this data matrix and y = XTw-GD + b + will be the corrupt performance with sparse corruptions. \u2212 Target and sound velocity. Let algorithm 2 be executed on this data matrix with the threshold parameter set to \u03b2. Let it be a fixed matrix so that X = 1 / 20 X will fulfill the SSC and SSS properties at the level with constants. \u2212 GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GD-GD-GD-GD-GD-GD-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GD-GL-GL-GL-GL-GL-GL-GL-GL-GL-GL-GD-GL-GL-GL-GL-GL-GL-GL-GL"}, {"heading": "B Proof of Theorem 3", "text": "Theorem 3. Let X = [x1,., xn] \"Rp \u00b7 \u03b2\" = > \"the specified data matrix and y = XTw\" + \"s is the incomplete data matrix that meets the SSC and SSS properties at the level of constants.\" (see definition 1) If the data (1 + 2) is a fixed matrix, so that the SSC and SSS properties are fulfilled at the level of constants (see definition 1). (see definition 1) \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s."}, {"heading": "C Proof of Theorem 4", "text": "S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \""}, {"heading": "D Proof of Theorem 5", "text": "Theorem 5: Let's X = 1: 1,.. xn = 2: 1,. x: 1,. x: 1,. x: 1,. x: 1,. x: 1,. x: 1,. x: 1,. x: 1,. x: 1,. x: 1,. x: 1,. x: 1,. x: 2,. x: 2,. x: 1,. x: 1,. x: 1,. x: 1,. x: 1,. x: 1,. x: 1,. x: 1,. x: 1,. x: 1,. x: 2,. x: 1,. x: 1,. x: 1,. x: 1,. x: 1,. x: 2,. x: 2,. x: 2,. x: 1,. x: 2,. x: 1,. x: 1,. x: 2,. x: 1,. x: 2,. x: 2,. x: 2,."}, {"heading": "E Proof of Theorem 6", "text": "Theorem 6: Suppose that Algorithm 4 is carried out on the basis of data enabling algorithms 2 and 3 to converge between \"FC\" and \"FC\"; suppose we have 2 \"FC\" - \"FC\" - \"FC\" - \"FC\" - \"FC\" - \"FC\" - \"FC\" - \"FC\" - \"FC\" - \"FC\" - \"FC\" - \"FC\" - \"FC\" - \"FC\" - \"FC\" - \"FC\" - \"FC\" - \"FC\" - \"FC\" - \"-\" FC \"-\" - \"FC\" - \";\" FC \"-\" - \"FC\" - \"-\" FC \"-\"; \"FC\" - \"-\" FC \"-\" FC \"-\" - \"FC\" - \"-\"; \"FC\" - \"-\"; \"-\" FC \"-\"; \"-\" FC \"-\" - \";\" - \"FC\" - \"-\"; \"-\" FC \"-\" - \";\" - \"FC\" - \";\" - \"-\" FC \"-\"; \"-\" - \";\" - \"FC\" - \"-\"; \"-\" - \"-\"; \"-\" - \";\" - \"-\"; \"-\" \"-\"; \"-\"; \"-\" - \";\" - \"-\"; \";\" - \";\"; \"-\"; \";\" - \";\"; \";\" \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\" \"\" \";\"; \";\"; \"\" \"\" \";\"; \"\" \"\" \"\" \";\"; \";\"; \"\"; \"\" \"\"; \";\" \";\"; \";\" \";\"; \"\"; \";\"; \";\" \"\"; \"\""}, {"heading": "F Proof of Theorem 9", "text": "Theorem 9: Let us take X = 1: 1,., xn = 2: 1,.,..,..,..,..,...,..,...,...,...,...,...,...,...,...,...,...,...,....,...........,...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "G Robust Statistical Estimation", "text": "This section explains how the results of the convergence of our algorithms can be incorporated into the analysis to provide guarantees for robust statistical estimation problems. (We start with a few definitions of sampling models used in our results.) In addition, the smallest upper limit of this quantity is referred to as the Sub-Gaussian standard when the following quantity is referred to as \"x\" and is referred to as \"x.\" 2. Definition: A vector-weighted random x-Rp is referred to as the Sub-Gaussian standard of x-Gaussian. 2. Definition: A vector-weighted x-Rp is referred to as Sub-Gaussian when its unidimensional margins are called. < x > are Sub-Gaussian for all V-Gaussian types. In addition, its Sub-Gaussian standard is defined as: = Sub-Gaussian class."}, {"heading": "H Supplementary Results", "text": "Claim Set 19: Facing any vector vS3 2: 2: 2 2: 2 2: 2 2: 2 2: 2 2: 2 2: 2 2: 2 2: 2 2: 2 2: 2 2: 2 2: 2 2: 2 2: 2 2: 2 2: 2 2: 2 2 2: 2 2: 2 2 2: 2 2: 2 2: 2 2: 2 2: 2 2: 2 2: 2: 2 2: 2 2: 2 2: 2 2: 2 2: 2 2 2: 2 2 2: 2 2 2: 2 2 2: 2 2: 2 2 2: 2 2: 2 2: 2: 2: 2: 2 2: 2: 2: 2: 2: 2: 2: 2: 2: 2: 2: 2: 2: 2: 2: 2 2: 2: 2 2: 2: 2: 2 2: 2 2: 2 2: 2 2: 2 2 2: 2 2: 2 2 2: 2 2: 2 2 2: 2: 2 2 2: 2 2: 2: 2 2: 2: 2 2: 2 2: 2: 2 2: 2 2: 2: 2 2: 2: 2 2: 2 2: 2: 2 2: 2: 2: 2 2 2: 2: 2 2: 2: 2: 2 2: 2: 2 2: 2: 2: 2 2 2: 2: 2: 2 2: 2 2: 2: 2: 2: 2 2: 2: 2: 2: 2: 2 2: 2: 2: 2: 2: 2: 2 2: 2: 2: 2: 2: 2: 2: 2 2: 2: 2: 2: 2: 2 2: 2: 2: 2: 2: 2: 2 2: 2: 2: 2: 2: 2: 2: 2: 2: 2: 2: 2 2: 2: 2 2: 2: 2: 2: 2 2 2: 2: 2: 2 2: 2: 2 2 2: 2: 2: 2 2: 2 2 2: 2: 2: 2 2: 2: 2: 2 2: 2: 2 2: 2 2: 2: 2: 2 2: 2: 2 2: 2: 2: 2 2: 2: 2: 2 2: 2: 2: 2 2: 2 2: 2: 2: 2"}], "references": [{"title": "Sampling and reconstructing signals from a union of linear subspaces", "author": ["Thomas Blumensath"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Iterative Hard Thresholding for Compressed Sensing", "author": ["Thomas Blumensath", "Mike E. Davies"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Robust Sparse Regression under Adversarial Corruption", "author": ["Yudong Chen", "Constantine Caramanis", "Shie Mannor"], "venue": "In 30th International Conference on Machine Learning (ICML),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Gradient descent with sparsification: an iterative algorithm for sparse recovery with restricted isometry property", "author": ["Rahul Garg", "Rohit Khandekar"], "venue": "In 26th International Conference on Machine Learning (ICML),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "On Iterative Hard Thresholding Methods for High-dimensional M-Estimation", "author": ["Prateek Jain", "Ambuj Tewari", "Purushottam Kar"], "venue": "In 28th Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Adaptive estimation of a quadratic functional by model selection", "author": ["Beatrice Laurent", "Pascal Massart"], "venue": "The Annals of Statistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2000}, {"title": "Fast and Robust Least Squares Estimation in Corrupted Linear Models", "author": ["Brian McWilliams", "Gabriel Krummenacher", "Mario Lucic", "Joachim M. Buhmann"], "venue": "In 28th Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Exact recoverability from dense corrupted observations via L1 minimization", "author": ["Nam H. Nguyen", "Trac D. Tran"], "venue": "IEEE Transaction on Information Theory,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Least Median of Squares Regression", "author": ["Peter J. Rousseeuw"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1984}, {"title": "Computing LTS Regression for Large Data Sets", "author": ["Peter J. Rousseeuw", "Katrien Driessen"], "venue": "Journal of Data Mining and Knowledge Discovery,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Robust Regression and Outlier Detection", "author": ["Peter J. Rousseeuw", "Annick M. Leroy"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1987}, {"title": "Recovery of Sparsely Corrupted Signals", "author": ["Christoph Studer", "Patrick Kuppinger", "Graeme Pope", "Helmut B\u00f6lcskei"], "venue": "IEEE Transaction on Information Theory,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["Roman Vershynin"], "venue": "Compressed Sensing, Theory and Applications,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Dense Error Correction via ` Minimization", "author": ["John Wright", "Yi Ma"], "venue": "IEEE Transaction on Information Theory,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Robust Face Recognition via Sparse Representation", "author": ["John Wright", "Alan Y. Yang", "Arvind Ganesh", "S. Shankar Sastry", "Yi Ma"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}], "referenceMentions": [{"referenceID": 11, "context": "Owing to the wideapplicability of regression, RLSR features as a critical component of several important real-world applications in a variety of domains such as signal processing [13], economics [12], computer vision [16, 15], and astronomy [12].", "startOffset": 179, "endOffset": 183}, {"referenceID": 10, "context": "Owing to the wideapplicability of regression, RLSR features as a critical component of several important real-world applications in a variety of domains such as signal processing [13], economics [12], computer vision [16, 15], and astronomy [12].", "startOffset": 195, "endOffset": 199}, {"referenceID": 14, "context": "Owing to the wideapplicability of regression, RLSR features as a critical component of several important real-world applications in a variety of domains such as signal processing [13], economics [12], computer vision [16, 15], and astronomy [12].", "startOffset": 217, "endOffset": 225}, {"referenceID": 13, "context": "Owing to the wideapplicability of regression, RLSR features as a critical component of several important real-world applications in a variety of domains such as signal processing [13], economics [12], computer vision [16, 15], and astronomy [12].", "startOffset": 217, "endOffset": 225}, {"referenceID": 10, "context": "Owing to the wideapplicability of regression, RLSR features as a critical component of several important real-world applications in a variety of domains such as signal processing [13], economics [12], computer vision [16, 15], and astronomy [12].", "startOffset": 241, "endOffset": 245}, {"referenceID": 11, "context": "Indeed there exist reformulations of this problem that are known to be NP-hard to optimize [13].", "startOffset": 91, "endOffset": 95}, {"referenceID": 13, "context": "Recently, [15] and [9] obtained a surprising result which shows that one can recover w\u2217 exactly even when \u03b1 .", "startOffset": 10, "endOffset": 14}, {"referenceID": 7, "context": "Recently, [15] and [9] obtained a surprising result which shows that one can recover w\u2217 exactly even when \u03b1 .", "startOffset": 19, "endOffset": 22}, {"referenceID": 13, "context": "Moreover, the results impose severe restrictions on the data distribution, requiring that the data be either sampled from an isotropic Gaussian ensemble [15], or row-sampled from an incoherent orthogonal matrix [9].", "startOffset": 153, "endOffset": 157}, {"referenceID": 7, "context": "Moreover, the results impose severe restrictions on the data distribution, requiring that the data be either sampled from an isotropic Gaussian ensemble [15], or row-sampled from an incoherent orthogonal matrix [9].", "startOffset": 211, "endOffset": 214}, {"referenceID": 2, "context": "In contrast, [4] studied RLSR with less stringent assumptions, allowing arbitrary corruptions in response variables as well as in the data matrix X, and proposed a trimmed inner product based algorithm for the problem.", "startOffset": 13, "endOffset": 16}, {"referenceID": 6, "context": "A similar result was obtained by [8], albeit using a sub-sampling based algorithm with stronger assumptions on b.", "startOffset": 33, "endOffset": 36}, {"referenceID": 8, "context": "This intuitive algorithm seems to embody a long standing heuristic first proposed by Legendre [1] over two centuries ago (see introductory quotation in this paper) that has been adopted in later literature [10, 11] as well.", "startOffset": 206, "endOffset": 214}, {"referenceID": 9, "context": "This intuitive algorithm seems to embody a long standing heuristic first proposed by Legendre [1] over two centuries ago (see introductory quotation in this paper) that has been adopted in later literature [10, 11] as well.", "startOffset": 206, "endOffset": 214}, {"referenceID": 13, "context": "We would like to stress three key advantages of our result over the results of [15, 9]: a) we allow b to be adversarial, i.", "startOffset": 79, "endOffset": 86}, {"referenceID": 7, "context": "We would like to stress three key advantages of our result over the results of [15, 9]: a) we allow b to be adversarial, i.", "startOffset": 79, "endOffset": 86}, {"referenceID": 1, "context": "We would also like to stress that while hard-thresholding based methods have been studied rigorously for the sparse-recovery problem [3, 6], hard-thresholding has not been studied formally for the robust regression problem.", "startOffset": 133, "endOffset": 139}, {"referenceID": 4, "context": "We would also like to stress that while hard-thresholding based methods have been studied rigorously for the sparse-recovery problem [3, 6], hard-thresholding has not been studied formally for the robust regression problem.", "startOffset": 133, "endOffset": 139}, {"referenceID": 13, "context": "Indeed, the work of [15] assumes the Bouquet Model where distributions are restricted to be isotropic Gaussians whereas the work of [9] assumes a more stringent model of suborthonormal matrices, something that even Gaussian designs do not satisfy.", "startOffset": 20, "endOffset": 24}, {"referenceID": 7, "context": "Indeed, the work of [15] assumes the Bouquet Model where distributions are restricted to be isotropic Gaussians whereas the work of [9] assumes a more stringent model of suborthonormal matrices, something that even Gaussian designs do not satisfy.", "startOffset": 132, "endOffset": 135}, {"referenceID": 1, "context": "We reiterate that it is not possible to use existing results from sparse recovery (such as [3, 6]) directly to solve this problem.", "startOffset": 91, "endOffset": 97}, {"referenceID": 4, "context": "We reiterate that it is not possible to use existing results from sparse recovery (such as [3, 6]) directly to solve this problem.", "startOffset": 91, "endOffset": 97}, {"referenceID": 1, "context": "Assuming X satisfies the RSC/RSS properties (defined below), (3) can be solved efficiently using results from sparse recovery (for example the IHT algorithm [3, 5] analyzed in [6]).", "startOffset": 157, "endOffset": 163}, {"referenceID": 3, "context": "Assuming X satisfies the RSC/RSS properties (defined below), (3) can be solved efficiently using results from sparse recovery (for example the IHT algorithm [3, 5] analyzed in [6]).", "startOffset": 157, "endOffset": 163}, {"referenceID": 4, "context": "Assuming X satisfies the RSC/RSS properties (defined below), (3) can be solved efficiently using results from sparse recovery (for example the IHT algorithm [3, 5] analyzed in [6]).", "startOffset": 176, "endOffset": 179}, {"referenceID": 4, "context": "The sample complexity required by Theorem 9 is identical to the one required by analyses for high dimensional sparse recovery [6], save constants.", "startOffset": 126, "endOffset": 129}, {"referenceID": 13, "context": "Algorithms: We compared various variants of our algorithm Torrent to the regularized L1 algorithm for robust regression [15, 9].", "startOffset": 120, "endOffset": 127}, {"referenceID": 7, "context": "Algorithms: We compared various variants of our algorithm Torrent to the regularized L1 algorithm for robust regression [15, 9].", "startOffset": 120, "endOffset": 127}, {"referenceID": 0, "context": "[2] Thomas Blumensath.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[3] Thomas Blumensath and Mike E.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[4] Yudong Chen, Constantine Caramanis, and Shie Mannor.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] Rahul Garg and Rohit Khandekar.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6] Prateek Jain, Ambuj Tewari, and Purushottam Kar.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] Beatrice Laurent and Pascal Massart.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] Brian McWilliams, Gabriel Krummenacher, Mario Lucic, and Joachim M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] Nam H.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] Peter J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11] Peter J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] Peter J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] Christoph Studer, Patrick Kuppinger, Graeme Pope, and Helmut B\u00f6lcskei.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] Roman Vershynin.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] John Wright and Yi Ma.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] John Wright, Alan Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Using tail bounds on Chi-squared distributions [7], we get, with probability at least 1\u2212 \u03b4,", "startOffset": 47, "endOffset": 50}, {"referenceID": 5, "context": "However, we note that using a more finely tuned setting of the constant in the proof of Theorem 15 and a more careful proof using tight tail inequalities for chi-squared distributions [7], we can achieve a better corruption level tolerance of \u03b1 < 1 65 .", "startOffset": 184, "endOffset": 187}, {"referenceID": 4, "context": "Let Algorithm 2 be executed on this data with the IHT update from [6] and thresholding parameter set to \u03b2 \u2265 \u03b1.", "startOffset": 66, "endOffset": 69}, {"referenceID": 4, "context": "The results of [6], for example, indicate that if the input to the algorithm indeed satisfies the RSC and RSS properties at the level (1 \u2212 \u03b2, 2s + s\u2217) with constants \u03b12s+s\u2217 and L2s+s\u2217 for s \u2265 32 ( L2s+s\u2217 \u03b12s+s\u2217 ) , then in time \u03c4 =", "startOffset": 15, "endOffset": 18}, {"referenceID": 0, "context": "We refer the reader to the work of [2] which describes this technique in great detail.", "startOffset": 35, "endOffset": 38}, {"referenceID": 12, "context": "We will use the technique of covering numbers [14] to establish the above.", "startOffset": 46, "endOffset": 50}], "year": 2015, "abstractText": "We study the problem of Robust Least Squares Regression (RLSR) where several response variables can be adversarially corrupted. More specifically, for a data matrix X \u2208 Rp\u00d7n and an underlying model w\u2217, the response vector is generated as y = XTw\u2217+b where b \u2208 R is the corruption vector supported over at most C \u00b7n coordinates. Existing exact recovery results for RLSR focus solely on L1-penalty based convex formulations and impose relatively strict model assumptions such as requiring the corruptions b to be selected independently of X. In this work, we study a simple hard-thresholding algorithm called Torrent which, under mild conditions on X, can recover w\u2217 exactly even if b corrupts the response variables in an adversarial manner, i.e. both the support and entries of b are selected adversarially after observing X and w\u2217. Our results hold under deterministic assumptions which are satisfied if X is sampled from any sub-Gaussian distribution. Finally unlike existing results that apply only to a fixed w\u2217, generated independently of X, our results are universal and hold for any w\u2217 \u2208 R. Next, we propose gradient descent-based extensions of Torrent that can scale efficiently to large scale problems, such as high dimensional sparse recovery and prove similar recovery guarantees for these extensions. Empirically we find Torrent, and more so its extensions, offering significantly faster recovery than the state-of-the-art L1 solvers. For instance, even on moderate-sized datasets (with p = 50K) with around 40% corrupted responses, a variant of our proposed method called Torrent-HYB is more than 20\u00d7 faster than the best L1 solver. \u201cIf among these errors are some which appear too large to be admissible, then those equations which produced these errors will be rejected, as coming from too faulty experiments, and the unknowns will be determined by means of the other equations, which will then give much smaller errors.\u201d A. M. Legendre, On the Method of Least Squares. 1805.", "creator": "LaTeX with hyperref package"}}}