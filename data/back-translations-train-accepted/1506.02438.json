{"id": "1506.02438", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2015", "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation", "abstract": "This paper is concerned with developing policy gradient methods that gracefully scale up to challenging problems with high-dimensional state and action spaces. Towards this end, we develop a scheme that uses value functions to substantially reduce the variance of policy gradient estimates, while introducing a tolerable amount of bias. This scheme, which we call generalized advantage estimation (GAE), involves using a discounted sum of temporal difference residuals as an estimate of the advantage function, and can be interpreted as a type of automated cost shaping. It is simple to implement and can be used with a variety of policy gradient methods and value function approximators. Along with this variance-reduction scheme, we use trust region algorithms to optimize the policy and value function, both represented as neural networks. We present experimental results on a number of highly challenging 3D loco- motion tasks, where our approach learns complex gaits for bipedal and quadrupedal simulated robots. We also learn controllers for the biped getting up off the ground. In contrast to prior work that uses hand-crafted low-dimensional policy representations, our neural network policies map directly from raw kinematics to joint torques.", "histories": [["v1", "Mon, 8 Jun 2015 11:12:48 GMT  (1282kb,D)", "http://arxiv.org/abs/1506.02438v1", null], ["v2", "Thu, 26 Nov 2015 20:07:11 GMT  (1229kb,D)", "http://arxiv.org/abs/1506.02438v2", null], ["v3", "Tue, 8 Dec 2015 20:16:20 GMT  (1232kb,D)", "http://arxiv.org/abs/1506.02438v3", null], ["v4", "Thu, 21 Jan 2016 19:44:56 GMT  (1232kb,D)", "http://arxiv.org/abs/1506.02438v4", null], ["v5", "Fri, 9 Sep 2016 17:54:07 GMT  (1232kb,D)", "http://arxiv.org/abs/1506.02438v5", null]], "reviews": [], "SUBJECTS": "cs.LG cs.RO cs.SY", "authors": ["john schulman", "philipp moritz", "sergey levine", "michael jordan", "pieter abbeel"], "accepted": true, "id": "1506.02438"}, "pdf": {"name": "1506.02438.pdf", "metadata": {"source": "CRF", "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation", "authors": ["John Schulman", "Philipp Moritz", "Sergey Levine", "Michael Jordan", "Pieter Abbeel"], "emails": ["joschu@eecs.berkeley.edu", "pcmoritz@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu", "jordan@eecs.berkeley.edu", "pabbeel@eecs.berkeley.edu"], "sections": [{"heading": "1 Introduction", "text": "In this context, it should be noted that this is an attempt to control the effects of the crisis on the economy, both on the economy and on the economy."}, {"heading": "2 Preliminaries", "text": "There are several different terms for the policy of optimization. The initial state s1 is sampled from [P = 1] at random (P = 1). A cost = c (st, at, st + 1) is generated by sampling actions in accordance with policy at each step. The goal is to minimize the expected total cost T t = 1 ct. Although we use episodes of length T to avoid notorious convenience, we will often write sums that go to infinity, where it is implied that ct = V (st) = 0 for t > T. Political gradients optimize the expected cost by using gradients g: = 1 ct."}, {"heading": "3 Advantage function estimation", "text": "This section is about creating an accurate estimate of the true part function Ap = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p p = p = p = p = p"}, {"heading": "4 Interpretation as Cost Shaping", "text": "In this section, we discuss how \u03bb can be interpreted as an additional discount factor applied to the MDP after a cost-forming transformation has been carried out. (We also introduce the concept of a response function, around which \u03b3 and \u03bb.Cost Shaping refer to the following transformation of the cost function of an MDP: let \u03a6: S \u2192 R be an arbitrary scalar-rated function on state space, and define the transforming cost function c-byc (s, a, s \u2032) = c (s \u2032) \u2212 \u03a6 (s \u2032) \u2212 p (s) \u2212 p (s). (15) As shown by Ng et al. [10], this shaping transformation does not alter the optimal policy (in the discounted setting). In fact, it leaves the advantage functionA\u03c0 unchanged for any policy. This results from the observation that we consider two possible trajectories, the difference in their discounted costs remaining unchanged."}, {"heading": "5 Value Function Estimation", "text": "When using a nonlinear function approximator to represent the value function, the simplest approach is to solve a nonlinear regression problem: minimize? N? N? N = 1? V\u03c6 (sn) \u2212 V? n? 2, (18) where V? t = 0? l = 0? lct + l is the discounted sum of costs and n indices over all time in a series of trajectories, sometimes referred to as the Monte Carlo approach or TD (1) approach to estimating the value function [13].1For the experiments in this paper, we used a trust region method to optimize the value function in each iteration of a policy iteration.The trust region helps us avoid overadjustment to the latest batch of data. To formulate the trust regions problem, we first calculate a solution of? N \u2212 n = 1? V? N? N? N? N? N?"}, {"heading": "6 Experiments", "text": "We designed a series of experiments to investigate the following questions: 1. What empirical effect do different \u03bb [0, 1] and \u03b3 [0, 1] have on the optimization of episodic total costs using generalized benefit estimation? 2. Can generalized benefit estimation, together with trust region algorithms, be used to optimize policy and value function to optimize large neural network policies for difficult control problems?"}, {"heading": "6.1 Policy Optimization Algorithm", "text": "While a general estimate of the benefits can be used in conjunction with a variety of different methods of the policy gradient, we performed the policy updates for these experiments using the Trustregion Policy Optimization (TRPO). Specifically, we used the instance that minimizes gT (\u03b8 \u2212 \u03b8old) under a square constraint that yields a step in the direction \u03b8 \u2212 \u03b8old - A \u2212 1g, where A is the average Fisher information matrix. Therefore, the policy optimization algorithm is similar to the natural policy gradient [6] / the natural actor-critic [11]. However, it uses the methods stated in [12] for optimization and step size determination and uses in the gradient estimator A \u0109GAE (\u03b3, \u03bb). Since previous work [12] compared TRPO with a variety of different policy optimization algorithms, we will not repeat these comparisons; instead, we will focus on varying the process parameters of the policy gradient estimator while maintaining the fixed algorithm."}, {"heading": "6.2 Experimental Setup", "text": "We evaluated our approach to the classic problem of cart balancing and several challenging 3D motion tasks: (1) two-legged locomotion; (2) four-legged locomotion; (3) dynamic standing up for the two-legged person starting on his back. Models are shown in Figure 1.1. Another natural choice is to calculate target values using an estimator based on the TD (\u03bb) fuse [2, 13] which reflects the expression we use to estimate the political slope: V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-L-L-L + L. While experimenting with this choice, we noticed no difference in performance compared to the 1 V-V-V-V-V-V-V-V-V-V-V in Equation (18)."}, {"heading": "6.2.1 Architecture", "text": "We used the same architecture for all 3D robot tasks, which was a feedback network with three hidden layers of 100, 50 and 25 tanh units each. The last output layer had a linear activation. The valuer used the same architecture, but only a scalar output. For the simpler cart pole task, we used a linear policy and a neural network with a hidden layer of 20 units as a value function."}, {"heading": "6.2.2 Task details", "text": "The simulated robot tasks were simulated using the MuJoCo physics engine [17].The humanoid model has 33 state dimensions and 10 degrees of freedom activated, while the four-legged model has 29 state dimensions and 8 degrees of freedom activated.The initial state for these tasks consisted of a uniform distribution centered on a reference configuration.The cart pole task was implemented according to the description given by Barto et al. [1].For the cart pole balance task, we collected 20 trajectories per batch, whereas we collected 50,000 time steps per batch for two-legged episodes of locomotion and 200,000 time steps per batch for four-legged locomotion and two-legged standing.The cost functions are given in the table below.Task costs 3D two-legged locomotion operations: vfwd + 10 \u2212 5 \u2212 5 effects on two-legged episodes of locomotion, 2 \u2212 2 \u2212 the four-legged locomotion \u2212 5 effects in the table below."}, {"heading": "6.3 Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.3.1 Cart-pole", "text": "The results are averaged over 21 experiments with different random seeds. Results are shown in Figure 2 and indicate that the best results are obtained with intermediate values of the parameters: \u03b3 value [0.96, 0.99] and \u03bb value [0.92, 0.99]."}, {"heading": "6.3.2 3D bipedal locomotion", "text": "Each optimization took about 2 hours to be performed on a 16-core machine. In this case, the results are averaged over 9 trials with different random seeds. Again, the best performance is achieved on the basis of intermediate values of N [0.99, 0.995], N [0.96, 0.99]."}, {"heading": "6.3.3 Other 3D robot tasks", "text": "The other two motor behaviors considered are four-legged locomotion and standing up for the 3D bipedal. Here, too, we performed 5 experiments per experimental condition with different random seeds (and initializations), lasting about 4 hours per experiment on a 32-core machine. We performed a more limited comparison in these areas (due to the considerable computing resources required to perform these experiments), specifying ig = 0.995, but varying = {1, 0.96}, and an experimental condition without value function. In four-legged locomotion, the best results are obtained with a value function with \u03bb = 0.96, section 6.3.2. In 3D standing, the value function was always helpful, but the results are roughly the same for \u03bb = 0.96 and \u03bb = 1."}, {"heading": "7 Discussion", "text": "Our most important experimental validation of the general estimation of benefits lies in the area of simulated robotic locomotion, which is challenging due to the high dimensionality of the state and scope of action and the complexity of the approximate functional values for the policy and value function that we represent with large neural networks. In these areas, the \u03bb = 0 estimate usually leads to an excessively high bias, and the \u03bb = 1 estimate has a high variance. As shown in our experiments, the selection of a reasonable intermediate value of \u03bb in the range [0.9, 0.99] usually leads to the best performance. A possible topic for future work is how to adjust the estimation parameters in an adaptive or automatic way. Our method learned all gaits and motor behaviors with universal strategies and simple cost functions, using minimal prior knowledge. This is in contrast to most previous methods for learning locomotion controllers, which are typically based on policies and principles."}, {"heading": "Acknowledgements", "text": "We thank Emo Todorov for providing the simulator and for engaging discussions, and we thank Greg Wayne, Yuval Tassa, Dave Silver, and others at Google DeepMind for engaging discussions. This research was partially funded by the Office of Naval Research through a Young Investigator Award and funded by N00014-11-1-0688, DARPA through a Young Faculty Award, and the Army Research Office through the MAST program."}], "references": [{"title": "Neuronlike adaptive elements that can solve difficult learning control problems", "author": ["Andrew G Barto", "Richard S Sutton", "Charles W Anderson"], "venue": "Systems, Man and Cybernetics, IEEE Transactions on,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1983}, {"title": "Dynamic programming and optimal control, volume 2", "author": ["Dimitri P Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Convergent temporal-difference learning with arbitrary smooth function approximation", "author": ["Shalabh Bhatnagar", "Doina Precup", "David Silver", "Richard S Sutton", "Hamid R Maei", "Csaba Szepesv\u00e1ri"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Fast biped walking with a reflexive controller and realtime policy searching", "author": ["T. Geng", "B. Porr", "F. W\u00f6rg\u00f6tter"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Variance reduction techniques for gradient estimates in reinforcement learning", "author": ["Evan Greensmith", "Peter L Bartlett", "Jonathan Baxter"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "A natural policy gradient", "author": ["Sham Kakade"], "venue": "In NIPS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2001}, {"title": "Optimizing average reward using discounted rewards. In Computational Learning Theory, pages 605\u2013615", "author": ["Sham Kakade"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "An analysis of actor/critic algorithms using eligibility traces: Reinforcement learning with imperfect value function", "author": ["Hajime Kimura", "Shigenobu Kobayashi"], "venue": "In ICML,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "On actor-critic algorithms", "author": ["Vijay R Konda", "John N Tsitsiklis"], "venue": "SIAM journal on Control and Optimization,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Policy invariance under reward transformations: Theory and application to reward shaping", "author": ["Andrew Y Ng", "Daishi Harada", "Stuart Russell"], "venue": "In ICML,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}, {"title": "Trust region policy optimization", "author": ["John Schulman", "Sergey Levine", "Philipp Moritz", "Michael I Jordan", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1502.05477,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Introduction to reinforcement learning", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Richard S Sutton", "David A McAllester", "Satinder P Singh", "Yishay Mansour"], "venue": "In NIPS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1999}, {"title": "Stochastic policy gradient reinforcement learning on a simple 3d biped", "author": ["R. Tedrake", "T. Zhang", "H. Seung"], "venue": "In IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Bias in natural actor-critic algorithms", "author": ["Philip Thomas"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Mujoco: A physics engine for model-based control", "author": ["Emanuel Todorov", "Tom Erez", "Yuval Tassa"], "venue": "In Intelligent Robots and Systems (IROS),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Optimal gait and form for animal locomotion", "author": ["Kevin Wampler", "Zoran Popovi\u0107"], "venue": "In ACM Transactions on Graphics (TOG),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Real-time reinforcement learning by sequential actor\u2013critics and experience replay", "author": ["Pawe\u0142 Wawrzy\u0144ski"], "venue": "Neural Networks,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams"], "venue": "Machine learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1992}, {"title": "Numerical optimization", "author": ["Stephen J Wright", "Jorge Nocedal"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1999}], "referenceMentions": [{"referenceID": 18, "context": "Simple Monte Carlo estimators of the policy gradient [20, 14] are unbiased and do not require a value function, but their variance scales unfavorably with the time horizon, since the effect of an action is confounded with the effects of past and future actions.", "startOffset": 53, "endOffset": 61}, {"referenceID": 12, "context": "Simple Monte Carlo estimators of the policy gradient [20, 14] are unbiased and do not require a value function, but their variance scales unfavorably with the time horizon, since the effect of an action is confounded with the effects of past and future actions.", "startOffset": 53, "endOffset": 61}, {"referenceID": 8, "context": "Actor-critic methods avoid this issue by making use of a value function instead of Monte Carlo estimates [9], at the cost of introducing bias when the value function is not exact, which may cause the algorithm to diverge or converge to a suboptimal solution.", "startOffset": 105, "endOffset": 108}, {"referenceID": 0, "context": "We call this estimation scheme, parameterized by \u03b3 \u2208 [0, 1] and \u03bb \u2208 [0, 1], the generalized advantage estimator (GAE).", "startOffset": 53, "endOffset": 59}, {"referenceID": 0, "context": "We call this estimation scheme, parameterized by \u03b3 \u2208 [0, 1] and \u03bb \u2208 [0, 1], the generalized advantage estimator (GAE).", "startOffset": 68, "endOffset": 74}, {"referenceID": 7, "context": "Related methods have been proposed in the context of online actorcritic methods [8, 19].", "startOffset": 80, "endOffset": 87}, {"referenceID": 17, "context": "Related methods have been proposed in the context of online actorcritic methods [8, 19].", "startOffset": 80, "endOffset": 87}, {"referenceID": 9, "context": "We provide a more general analysis, which is applicable in both the online and batch settings, and discuss an interpretation of our method as an instance of cost shaping [10], where the approximate value function is used to shape the cost.", "startOffset": 170, "endOffset": 174}, {"referenceID": 18, "context": "Empirically, GAE achieves significantly faster policy improvement than both REINFORCE [20] and actor-critic methods, which are both special cases of GAE.", "startOffset": 86, "endOffset": 90}, {"referenceID": 7, "context": "While the formula has been proposed in prior work [8, 19], our analysis is novel and enables GAE to be applied with a more general set of algorithms, including the batch trust-region algorithm we use for our experiments.", "startOffset": 50, "endOffset": 57}, {"referenceID": 17, "context": "While the formula has been proposed in prior work [8, 19], our analysis is novel and enables GAE to be applied with a more general set of algorithms, including the batch trust-region algorithm we use for our experiments.", "startOffset": 50, "endOffset": 57}, {"referenceID": 14, "context": "This parameter is related to the discount factor used in discounted formulations of MDPs, but we treat it as a variance reduction parameter, following prior work [16, 7], and analyze different settings in our experiments.", "startOffset": 162, "endOffset": 169}, {"referenceID": 6, "context": "This parameter is related to the discount factor used in discounted formulations of MDPs, but we treat it as a variance reduction parameter, following prior work [16, 7], and analyze different settings in our experiments.", "startOffset": 162, "endOffset": 169}, {"referenceID": 4, "context": "In general, the quantity inside the expectation of Equation (6) has lower variance than the quantity inside the expectation of Equation (7), since subtracting the value function acts as a baseline [5].", "startOffset": 197, "endOffset": 200}, {"referenceID": 11, "context": ", the TD residual of V [13], and consider the discounted sum of these \u03b4 terms.", "startOffset": 23, "endOffset": 27}, {"referenceID": 11, "context": "The TD(\u03bb) method for value function estimation [13] uses a similar \u03b3\u03bb-discounted sum of TD errors, though the goal in that case is to estimate the state-value function rather than the advantage function.", "startOffset": 47, "endOffset": 51}, {"referenceID": 18, "context": "This estimator reduces to baselined REINFORCE [20] (with advantage estimate from Equation (11)) when \u03bb = 1, and to the actor-critic method [9] (with advantage estimate from Equation (12)) when \u03bb = 0.", "startOffset": 46, "endOffset": 50}, {"referenceID": 8, "context": "This estimator reduces to baselined REINFORCE [20] (with advantage estimate from Equation (11)) when \u03bb = 1, and to the actor-critic method [9] (with advantage estimate from Equation (12)) when \u03bb = 0.", "startOffset": 139, "endOffset": 142}, {"referenceID": 9, "context": "[10], this shaping transformation does not alter the optimal policy (in the discounted setting).", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": ", [2]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 11, "context": "This is sometimes called the Monte Carlo or TD(1) approach for estimating the value function [13].", "startOffset": 93, "endOffset": 97}, {"referenceID": 19, "context": "We compute an approximate solution to the trust region problem using the conjugate gradient algorithm [21].", "startOffset": 102, "endOffset": 106}, {"referenceID": 0, "context": "What is the empirical effect of varying \u03bb \u2208 [0, 1] and \u03b3 \u2208 [0, 1] when optimizing episodic total cost using generalized advantage estimation? 2.", "startOffset": 44, "endOffset": 50}, {"referenceID": 0, "context": "What is the empirical effect of varying \u03bb \u2208 [0, 1] and \u03b3 \u2208 [0, 1] when optimizing episodic total cost using generalized advantage estimation? 2.", "startOffset": 59, "endOffset": 65}, {"referenceID": 10, "context": "1 Policy Optimization Algorithm While generalized advantage estimation can be used along with a variety of different policy gradient methods, for these experiments, we performed the policy updates using trust region policy optimization (TRPO) [12].", "startOffset": 243, "endOffset": 247}, {"referenceID": 5, "context": "Hence, the policy optimization algorithm is similar to the natural policy gradient [6] / natural actor-critic [11].", "startOffset": 83, "endOffset": 86}, {"referenceID": 10, "context": "However, it uses the methods specified in [12] for optimization and stepsize determination, and it uses \u00c2 in the gradient estimator.", "startOffset": 42, "endOffset": 46}, {"referenceID": 10, "context": "Since prior work [12] compared TRPO to a variety of different policy optimization algorithms, we will not repeat these comparisons; rather, we will focus on varying the \u03b3, \u03bb parameters of policy gradient estimator while keeping the underlying algorithm fixed.", "startOffset": 17, "endOffset": 21}, {"referenceID": 1, "context": "1Another natural choice is to compute target values with an estimator based on the TD(\u03bb) backup [2, 13], mirroring the expression we use for policy gradient estimation: V\u0302 \u03bb t = V\u03c6old (sn) + \u2211\u221e l=0(\u03b3\u03bb) \u03b4t+l.", "startOffset": 96, "endOffset": 103}, {"referenceID": 11, "context": "1Another natural choice is to compute target values with an estimator based on the TD(\u03bb) backup [2, 13], mirroring the expression we use for policy gradient estimation: V\u0302 \u03bb t = V\u03c6old (sn) + \u2211\u221e l=0(\u03b3\u03bb) \u03b4t+l.", "startOffset": 96, "endOffset": 103}, {"referenceID": 15, "context": "2 Task details The simulated robot tasks were simulated using the MuJoCo physics engine [17].", "startOffset": 88, "endOffset": 92}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "This is in contrast with most prior methods for learning locomotion controllers, which typically rely on hand-architected policy classes that explicitly encode notions of balance and stepping [15, 4, 18].", "startOffset": 192, "endOffset": 203}, {"referenceID": 3, "context": "This is in contrast with most prior methods for learning locomotion controllers, which typically rely on hand-architected policy classes that explicitly encode notions of balance and stepping [15, 4, 18].", "startOffset": 192, "endOffset": 203}, {"referenceID": 16, "context": "This is in contrast with most prior methods for learning locomotion controllers, which typically rely on hand-architected policy classes that explicitly encode notions of balance and stepping [15, 4, 18].", "startOffset": 192, "endOffset": 203}, {"referenceID": 2, "context": "Some candidates for such an error metric might include the Bellman error or projected Bellman error, as described in [3].", "startOffset": 117, "endOffset": 120}], "year": 2015, "abstractText": "This paper is concerned with developing policy gradient methods that gracefully scale up to challenging problems with high-dimensional state and action spaces. Towards this end, we develop a scheme that uses value functions to substantially reduce the variance of policy gradient estimates, while introducing a tolerable amount of bias. This scheme, which we call generalized advantage estimation (GAE), involves using a discounted sum of temporal difference residuals as an estimate of the advantage function, and can be interpreted as a type of automated cost shaping. It is simple to implement and can be used with a variety of policy gradient methods and value function approximators. Along with this variance-reduction scheme, we use trust region algorithms to optimize the policy and value function, both represented as neural networks. We present experimental results on a number of highly challenging 3D locomotion tasks, where our approach learns complex gaits for bipedal and quadrupedal simulated robots. We also learn controllers for the biped getting up off the ground. In contrast to prior work that uses hand-crafted low-dimensional policy representations, our neural network policies map directly from raw kinematics to joint torques.", "creator": "LaTeX with hyperref package"}}}