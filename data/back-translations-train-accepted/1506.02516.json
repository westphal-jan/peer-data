{"id": "1506.02516", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2015", "title": "Learning to Transduce with Unbounded Memory", "abstract": "Recently, strong results have been demonstrated by Deep Recurrent Neural Networks on natural language transduction problems. In this paper we explore the representational power of these models using synthetic grammars designed to exhibit phenomena similar to those found in real transduction problems such as machine translation. These experiments lead us to propose new memory-based recurrent networks that implement continuously differentiable analogues of traditional data structures such as Stacks, Queues, and DeQues. We show that these architectures exhibit superior generalisation performance to Deep RNNs and are often able to learn the underlying generating algorithms in our transduction experiments.", "histories": [["v1", "Mon, 8 Jun 2015 14:23:30 GMT  (541kb,D)", "http://arxiv.org/abs/1506.02516v1", "12 pages, 4 figures"], ["v2", "Fri, 30 Oct 2015 16:24:40 GMT  (696kb,D)", "http://arxiv.org/abs/1506.02516v2", "12 pages, 4 figures"], ["v3", "Tue, 3 Nov 2015 14:07:29 GMT  (699kb,D)", "http://arxiv.org/abs/1506.02516v3", "14 pages, 4 figures, NIPS 2015"]], "COMMENTS": "12 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.NE cs.CL cs.LG", "authors": ["edward grefenstette", "karl moritz hermann", "mustafa suleyman", "phil blunsom"], "accepted": true, "id": "1506.02516"}, "pdf": {"name": "1506.02516.pdf", "metadata": {"source": "CRF", "title": "Learning to Transduce with Unbounded Memory", "authors": ["Edward Grefenstette", "Karl Moritz Hermann"], "emails": ["etg@google.com", "kmh@google.com", "mustafasul@google.com", "pblunsom@google.com"], "sections": [{"heading": "1 Introduction", "text": "Recursive neural networks (RNNs) provide a compelling tool for processing natural speech input in a simple sequential manner. Many natural language processing (NLP) tasks can be considered transduction problems, i.e. they learn to transform one string into another. Machine translation is a prototype example of transduction, and recent results suggest that deep RNNs have the ability to encode long source strings and produce coherent translations [1, 2]. Applying RNNs to border translation tasks requires hidden layers large enough to store representations of the longest strings likely to be encountered, implying waste on shorter strings and a strong dependence on the number of parameters in the model and its memory. In this paper, we use a number of linguistically inspired synthetic transactional tasks to explore the ability of the longest strings to transform the neural."}, {"heading": "2 Related Work", "text": "The most common approach uses symbolic finite state converters [6, 7], although approaches based on context-free representations are also popular [8]. RNNs offer an attractive alternative to symbolic transducers due to their simple algorithms and expressive representations. However, as we show in this paper, such models are limited in their ability to generalize beyond their training data and have a storage capacity that scales with the number of their supporting parameters. Previous work touched on the issue of displaying discrete data structures as they are continuous, especially in the context of modelling push-down automata with neural networks [10, 11, 3]. We were inspired by the continuous pop and push operations of these architectures and the idea of RNN control of the data structure in the development of our own models."}, {"heading": "3 Models", "text": "In this section, we present an expandable memory expansion to recurring layers that can be built as a continuous version of a classic stack, queue, or DeQue. We begin by describing the operations and dynamics of a neural stack, before showing how to modify it to function as a queue, and expand it to a DeQue."}, {"heading": "3.1 Neural Stack", "text": "rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc"}, {"heading": "3.2 Neural Queue", "text": "A neural queue works the same way as a neural stack, except that the pop operation reads the lowest index of the strength vector st and not the highest index, which is the pop, and 1The functions max (x, y) and min (x, y) are technically indistinguishable for x = y. These operations are described in equations 4-5.st [i] = max (0, st \u2212 1 [i] \u2212 max (0, ut \u2212 i \u2212 1 = 1 st \u2212 1 [j])) when 1 \u2264 i < tdt if i = t (4) rt = t \u2211 i = 1 (min (st [i], max (0, 1 \u2212 i \u2212 1 = 1 st [j])) \u00b7 Vt [i (5)"}, {"heading": "3.3 Neural DeQue", "text": "A neural DeQue operates like a neural stack unless it needs a push, pop and a value as input for both \"ends\" of the structure (which we call top and bot) and outputs a read for both ends. We write utopt and u bot t instead of ut, v top t and v bot t instead of vt, and so on. The state Vt and st are now a 2t x m-dimensional matrix and a 2t-dimensional vector, respectively. At each timestep, a bang from above is followed by a bang from the bottom of the DeQue, followed by the steps and readings. The dynamics of a DeQue, which, unlike a neural stack or queue, \"grows\" in two directions, is described in Equations 6-11, below. Equations 7-9 break down the strength vector into three steps (ltst) [ltst] [t [t] purely for notational clarity."}, {"heading": "3.4 Interaction with a Controller", "text": "While the three described modules can be regarded as recurring layers, the operations for the next state and the output of data from the input and the previous state must be completely differentiated, as they do not contain coordinated parameters for optimization. As such, they must be connected to a controller to be used for all practical purposes. The logical size of the data is unlimited and decoupled from the nature and parameters of the controller. Here, we describe how each RNN controller can be moved from one neural stack to another."}, {"heading": "4 Experiments", "text": "In each experiment, integer-encoded source and target sequence pairs are presented to the candidate model as stacks of individual common sequences, starting with a start-of-sequence symbol (SOS) and ending with an end-of-sequence symbol (EOS), with a separator separating the source and target sequences. Integer-encoded symbols are converted to 64-dimensional embedding via an embedding matrix, which is randomly initialized and matched during the training. Separate word-to-index mappings are used for source and target vocabularies. Separate embedding matrices are used to encode embedding and output (predicted) embedding."}, {"heading": "4.1 Synthetic Transduction Tasks", "text": "The goal of each of the following tasks is to read an input sequence and generate a transformed version of the source sequence as > target sequence, followed by an EOS symbol. Source sequences are randomly generated from a vocabulary of 128 meaningless symbols. The length of each training source sequence is taken uniformly from unif {8, 64}, and each symbol in the source sequence is substituted by a uniform distribution across the source vocabulary (disregarding SOS and separators). A deterministic task-specific transformation, which is described for each task below, is applied to the source sequence to obtain the target sequence. Since the training sequences are fully determined by the source sequence, there are nearly 10135 training sequences for each task and training examples are sampled from this space due to the random generation of the source sequences."}, {"heading": "4.2 ITG Transduction Tasks", "text": "In fact, at a time when we are not yet in a position, we are at a stage where we are not yet in a position to find a solution, in which we are in a position to find a solution, in which we are in a position to find a solution, in which we are in a position to find a solution, in which we are in a position to find a solution, in which we are in a position to find a solution, in which we are in a position to find a solution, in which we are in a position to find a solution, in which we are in a position to find a solution."}, {"heading": "4.3 Evaluation", "text": "For each task, test data are generated using the same procedure as training data, with the crucial difference that the length of the source sequence is sampled from unif {65, 128}. As a result of this change, we are assured not only that the models cannot observe test sequences during the training, but also how well the sequence transduction capabilities of the evaluated models are generalized beyond the sequence lengths observed during the training. To control the generalization capability, we also report accuracy values for sequences that are sampled separately from the training set, which is unlikely to have ever been observed during the actual model training due to the size of the sample room. For each test round, we take 1000 sequences from the corresponding test set. For each sequence, the model reads the source sequence and the separator symbol # and begins to generate the next symbol by extracting the maximum probable symbol from the Softmax distribution of the symbols produced by the model # sequence."}, {"heading": "4.4 Models Compared and Experimental Setup", "text": "For each task, we use as benchmarks the 1, 2, 4, and 8-layer Deep LSTMs described in [1]. Using these benchmarks, we evaluate neural stack, queue, and DeQue-enhanced LSTMs. In conducting experiments, we trained and tested a version of each model where all LSTMs in each model have a hidden layer size of 256 and one for a hidden layer size of 512. The embedding size of stacks / queue, and DeQue-enhanced LSTMs was arbitrarily set at 256, half of the maximum hidden size. The number of parameters for each model is given for each architecture in Table 1. \u2212 We see that the neural stack, queue, and DeQue-enhanced LSTMs have the same number of traceable parameters as a two-layer Deep LSTM. These are all derived from the additional connections to and from the untraceable memory module, which does not have any localization parameters."}, {"heading": "5 Results and Discussion", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able, that we are able to hide ourselves, and that we are able, that we will be able, that we will be able."}, {"heading": "6 Conclusions", "text": "The experiments carried out in this paper show that single-layer LSTMs, which are augmented by an unlimited, differentiated memory and can operate at the boundary like a classic stack, queue, or DeQue, are capable of solving sequence-to-sequence transmission tasks where deep LSTMs falter. Even in tasks for which benchmarks achieve high accuracy, memory-enhanced LSTMs converge earlier and with higher accuracy, while requiring considerably fewer parameters than all other deep LSTMs. We therefore believe that they are a critical complement to our neural network toolbox, and that more complex linguistic transmission tasks such as machine translation or parsing are easier to understand through their inclusion. We thank Alex Graves, Demis Hassabis, Tomas Kocisky, Tim Rockta \ufffd schel, Geoff Hinton, Ilya Sutskever, and many others for their helpful comments."}, {"heading": "A Analysis of the Backwards Dynamics of a Neural Stack", "text": "We describe here the reverse dynamics of the neural stack, by looking at the relevant partial derivatives of the results with regard to the inputs, as they are defined in the equations 1-3rd. We use \u03b4ij to display the Kronecker delta (1 if i = j, 0 otherwise). The following equations apply to all valid line numbers i and n. \u2202 Vt [i] = min (st [n], max (0, 1 \u2212 t \u00b2 j = n 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200; 200;"}, {"heading": "C Full Results", "text": "In Table 4 we show the complete results for each task of the most powerful models. The procedure for selecting the most powerful model is described in Section 5."}], "references": [{"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. V Le"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "The neural network pushdown automaton: Model, stack and learning", "author": ["GZ Sun", "C Lee Giles", "HH Chen", "YC Lee"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Supervised Sequence Labelling with Recurrent Neural Networks, volume 385 of Studies in Computational Intelligence", "author": ["Alex Graves"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Latent-variable modeling of string transductions with finite-state methods", "author": ["Markus Dreyer", "Jason R. Smith", "Jason Eisner"], "venue": "In Proceedings of the Conference on Empirical Methods in 9  Natural Language Processing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Open- FST: A general and efficient weighted finite-state transducer library. In Implementation and Application of Automata, volume 4783 of Lecture Notes in Computer Science, pages 11\u201323", "author": ["Cyril Allauzen", "Michael Riley", "Johan Schalkwyk", "Wojciech Skut", "Mehryar Mohri"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Stochastic inversion transduction grammars and bilingual parsing of parallel corpora", "author": ["Dekai Wu"], "venue": "Computational linguistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Sequence transduction with recurrent neural networks", "author": ["Alex Graves"], "venue": "In Representation Learning Worksop,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Learning context-free grammars: Capabilities and limitations of a recurrent neural network with an external stack memory", "author": ["Sreerupa Das", "C Lee Giles", "Guo-Zheng Sun"], "venue": "In Proceedings of The Fourteenth Annual Conference of Cognitive Science Society. Indiana University,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1992}, {"title": "Using prior knowledge in a {NNPDA} to learn context-free languages. Advances in neural information processing", "author": ["Sreerupa Das", "C Lee Giles", "Guo-Zheng Sun"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1993}, {"title": "Reinforcement learning neural turing machines", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1505.00521,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Armand Joulin", "Tomas Mikolov"], "venue": "arXiv preprint arXiv:1503.01007,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "The theory of parsing, translation, and compiling", "author": ["Alfred V Aho", "Jeffrey D Ullman"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1972}, {"title": "Machine translation with a stochastic grammatical channel", "author": ["Dekai Wu", "Hongsing Wong"], "venue": "In Proceedings of the 17th international conference on Computational linguistics-Volume", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Understanding the exploding gradient problem", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": "Computing Research Repository (CoRR) abs/1211.5063,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Machine translation is a prototypical example of transduction and recent results indicate that Deep RNNs have the ability to encode long source strings and produce coherent translations [1, 2].", "startOffset": 186, "endOffset": 192}, {"referenceID": 1, "context": "Machine translation is a prototypical example of transduction and recent results indicate that Deep RNNs have the ability to encode long source strings and produce coherent translations [1, 2].", "startOffset": 186, "endOffset": 192}, {"referenceID": 2, "context": "Further, inspired by prior work on neural network implementations of stack data structures [3], we propose and evaluate transduction models based on Neural Stacks, Queues, and DeQues (double ended queues).", "startOffset": 91, "endOffset": 94}, {"referenceID": 0, "context": "While Deep RNNs based on long short-term memory (LSTM) cells [1, 5] can learn some transductions when tested on inputs of the same length as seen in training, they fail to consistently generalise to longer strings.", "startOffset": 61, "endOffset": 67}, {"referenceID": 3, "context": "While Deep RNNs based on long short-term memory (LSTM) cells [1, 5] can learn some transductions when tested on inputs of the same length as seen in training, they fail to consistently generalise to longer strings.", "startOffset": 61, "endOffset": 67}, {"referenceID": 4, "context": "The most common approach leverages symbolic finite state transducers [6, 7], with approaches based on context free representations also being popular [8].", "startOffset": 69, "endOffset": 75}, {"referenceID": 5, "context": "The most common approach leverages symbolic finite state transducers [6, 7], with approaches based on context free representations also being popular [8].", "startOffset": 69, "endOffset": 75}, {"referenceID": 6, "context": "The most common approach leverages symbolic finite state transducers [6, 7], with approaches based on context free representations also being popular [8].", "startOffset": 150, "endOffset": 153}, {"referenceID": 7, "context": "RNNs offer an attractive alternative to symbolic transducers due to their simple algorithms and expressive representations [9].", "startOffset": 123, "endOffset": 126}, {"referenceID": 8, "context": "Previous work has touched on the topic of rendering discrete data structures such as stacks continuous, especially within the context of modelling pushdown automata with neural networks [10, 11, 3].", "startOffset": 186, "endOffset": 197}, {"referenceID": 9, "context": "Previous work has touched on the topic of rendering discrete data structures such as stacks continuous, especially within the context of modelling pushdown automata with neural networks [10, 11, 3].", "startOffset": 186, "endOffset": 197}, {"referenceID": 2, "context": "Previous work has touched on the topic of rendering discrete data structures such as stacks continuous, especially within the context of modelling pushdown automata with neural networks [10, 11, 3].", "startOffset": 186, "endOffset": 197}, {"referenceID": 10, "context": "The NTM and Memory Networks [4, 12, 13] provide powerful random access memory operations, whereas we focus on a more efficient and restricted class of models which we believe are sufficient for natural language transduction tasks.", "startOffset": 28, "endOffset": 39}, {"referenceID": 11, "context": "More closely related to our work, [14] have sought to develop a continuous stack controlled by an RNN.", "startOffset": 34, "endOffset": 38}, {"referenceID": 2, "context": "Inspired by the neural pushdown automaton of [3], we render these traditionally discrete operations continuous by letting push and pop operations be real values in the interval (0, 1).", "startOffset": 45, "endOffset": 48}, {"referenceID": 1, "context": "The third step shows how setting the strength s3[2] to 0 for V3[2] logically removes v2 from the stack, and how it is ignored during the read.", "startOffset": 48, "endOffset": 51}, {"referenceID": 1, "context": "The third step shows how setting the strength s3[2] to 0 for V3[2] logically removes v2 from the stack, and how it is ignored during the read.", "startOffset": 63, "endOffset": 66}, {"referenceID": 12, "context": "Following the work on rectified linear units [15], we arbitrarily take the partial differentiation of the left argument in these cases.", "startOffset": 45, "endOffset": 49}, {"referenceID": 6, "context": "The following tasks examine how well models can approach sequence transduction problems where the source and target sequence are jointly generated by Inversion Transduction Grammars (ITG) [8], a subclass of Synchronous Context-Free Grammars [16] often used in machine translation [17].", "startOffset": 188, "endOffset": 191}, {"referenceID": 13, "context": "The following tasks examine how well models can approach sequence transduction problems where the source and target sequence are jointly generated by Inversion Transduction Grammars (ITG) [8], a subclass of Synchronous Context-Free Grammars [16] often used in machine translation [17].", "startOffset": 241, "endOffset": 245}, {"referenceID": 14, "context": "The following tasks examine how well models can approach sequence transduction problems where the source and target sequence are jointly generated by Inversion Transduction Grammars (ITG) [8], a subclass of Synchronous Context-Free Grammars [16] often used in machine translation [17].", "startOffset": 280, "endOffset": 284}, {"referenceID": 6, "context": "We generate training data by rejecting samples that are outside of the range [8, 64], and testing data by rejecting samples outside of the range [65, 128].", "startOffset": 77, "endOffset": 84}, {"referenceID": 0, "context": "For each task, we use as benchmarks the Deep LSTMs described in [1], with 1, 2, 4, and 8 layers.", "startOffset": 64, "endOffset": 67}, {"referenceID": 15, "context": "Models are trained with minibatch RMSProp [18], with a batch size of 10.", "startOffset": 42, "endOffset": 46}, {"referenceID": 16, "context": "We used gradient clipping [19], clipping all gradients above 1.", "startOffset": 26, "endOffset": 30}, {"referenceID": 0, "context": "Finally, to study the effect of random initialisation, we furthermore re-ran the experiments using random seeds in [1, 12], which were used to initialise the random number generator at the start of training.", "startOffset": 115, "endOffset": 122}], "year": 2015, "abstractText": "Recently, strong results have been demonstrated by Deep Recurrent Neural Networks on natural language transduction problems. In this paper we explore the representational power of these models using synthetic grammars designed to exhibit phenomena similar to those found in real transduction problems such as machine translation. These experiments lead us to propose new memory-based recurrent networks that implement continuously differentiable analogues of traditional data structures such as Stacks, Queues, and DeQues. We show that these architectures exhibit superior generalisation performance to Deep RNNs and are often able to learn the underlying generating algorithms in our transduction experiments.", "creator": "LaTeX with hyperref package"}}}