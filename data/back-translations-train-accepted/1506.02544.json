{"id": "1506.02544", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2015", "title": "Learning with Group Invariant Features: A Kernel Perspective", "abstract": "We analyze in this paper a random feature map based on a theory of invariance I-theory introduced recently. More specifically, a group invariant signal signature is obtained through cumulative distributions of group transformed random projections. Our analysis bridges invariant feature learning with kernel methods, as we show that this feature map defines an expected Haar integration kernel that is invariant to the specified group action. We show how this non-linear random feature map approximates this group invariant kernel uniformly on a set of $N$ points. Moreover, we show that it defines a function space that is dense in the equivalent Invariant Reproducing Kernel Hilbert Space. Finally, we quantify error rates of the convergence of the empirical risk minimization, as well as the reduction in the sample complexity of a learning algorithm using such an invariant representation for signal classification, in a classical supervised learning setting.", "histories": [["v1", "Mon, 8 Jun 2015 15:19:30 GMT  (2724kb,D)", "http://arxiv.org/abs/1506.02544v1", null], ["v2", "Fri, 4 Dec 2015 20:49:25 GMT  (76kb)", "http://arxiv.org/abs/1506.02544v2", "NIPS 2015"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV stat.ML", "authors": ["youssef mroueh", "stephen voinea", "tomaso a poggio"], "accepted": true, "id": "1506.02544"}, "pdf": {"name": "1506.02544.pdf", "metadata": {"source": "CRF", "title": "Learning with Group Invariant Features: A Kernel Perspective", "authors": ["Youssef Mroueh", "Stephen Voinea", "Tomaso Poggio"], "emails": ["mroueh@us.ibm.com"], "sections": [{"heading": "1 Introduction", "text": "The coding of signals or the construction of cores of similarity that are invariant to the action of a group is a key problem in unattended learning, as it reduces the complexity of the learning task and mimics how our brain represents information invariably on symmetries and various interfering factors (changing lighting in image classification and pitch variation in speech recognition).Convolutionary neural networks [5, 6] reach state-of-the-art in many computer vision and speech recognition tasks, but require a large amount of labeled examples and extended data, in which we reflect symmetries of the world using virtual examples [7, 8] as we do by transforming training examples with identity-preserving transformations such as scissors, rotation, translation, etc. In this work, we adopt the approach of [1], where the representation of the signal is designed to reflect invariant characteristics and model symmetries of the world with group actions."}, {"heading": "1.1 Group Invariant Kernels", "text": "We assume that it is a measurement on X, such as a basic radial function on X, with a standardized hair measurement. G is assumed to be a compact and unitary group. G is assumed to be a compact and unitary group. G is assumed to be an invariant group between x, z and X replaced by hair integration. G is assumed to be a compact and unitary group. G is assumed to be an invariant group between x, z and X."}, {"heading": "1.2 Contributions", "text": "Furthermore, a group invariant kernel implies the presence of a decision function that is invariant for the group action, as well as a reduction in sample complexity, since we have to scan the learning amount in a reduced sentence, also known as core sentence X0. Core methods with hair integration cores come at a very expensive computational price in both the training and the test time: Calculation of the core is computationally cumbersome, since we have to integrate through the group and produce virtual examples by explicitly transforming points through the group action, and also scaling the training complexity of the core methods in sample size cubically. These practical considerations make the usefulness of such cores very limited, since we provide the contributions of this work on three folder basis ITS: 1. First, we show that a non-linear random sketch of the characteristics x: X \u2192 RD is derived from a memory theory of inventories."}, {"heading": "2 From Group Invariant Kernels to Feature Maps", "text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "3 An Equivalent Expected Kernel and a Uniform Concentration Result", "text": "In this section we present our main results, with proofs given in the supplementary material. Theorem 1 shows that the random map (+ 1) defined in the previous section corresponds to Ks (x, z) in anticipation of a group-invariant, hair-integrative error chain. Furthermore, s \u2212 Ks (x, z) calculates the average paired distance between all points in the orbits of x and z, defining the orbit as a collection of all the group-transformed points of a particular point x: Ox = {gx, g \u00b2 G}. Theorem 1 (Expectation). Let us leave the distance dG between the orbits of Ox and Oz: dG (x, z) = 1 \u00b0 G of the elements we do not have in group G (g)."}, {"heading": "4 Learning with Group Invariant Random Features", "text": "In this section we show that learning is a linear model in the invariant, accidental functional world in which we find ourselves (cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf.. cf.. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf."}, {"heading": "5 Relation to Previous Work", "text": "We relativize our contributions by outlining some of the previous work on invariant nuclei and approximate nuclei with random features. Several schemes have been proposed to approach a nonlinear kernel with an explicit nonlinear characteristic map in conjunction with linear methods, such as the Nystro-m method [17] or random scanning techniques in the Fourier domain for translation invariant nuclei [15]. Our features fall under random scanning techniques, where we select both projections and group elements to generate inventory with a holistic representation. We note that the relationship between random features and square rules has been thoroughly studied, where sharper boundaries and error rates are derived, and can be applied to our environment."}, {"heading": "6 Numerical Evaluation", "text": "In this thesis, namely with theorems 2 and 3, we showed that the random, group invariant character map captures the invariant distance between the points and that learning a linear model formed in the invariant, random character category will generalize well to invisible test points. In this section, we will test these claims through three experiments. For the claims of theorem 2, we will use a closest neighbor classifier, while for theorem 3, we will rely on the regularized minimum squares (RLS) classifier, one of the simplest algorithms for supervised learning. While our evidence focuses on the regulation of norms infinity, RLS corresponds to the regulation of Tikhonov with square loss. Specifically, for conducting T \u2212 way classification on a batch of N training points in Rd, summarized in the data matrix X-RN and identification matrix X-T-N are."}, {"heading": "A Proofs of Theorems 1 and 2", "text": "11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 1) Ks (x, z) = 1, 1, 2, 3, 4, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 6, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,"}, {"heading": "B Proof of Theorem 3", "text": "The proof for Lemma 1 is that we all (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n (n) (n) (n) (n) (n) (n) (n) (n (n) (n) (n (n) (n) (n) (n) (n) (n (n) (n) (n (n) (n) (n (n) (n) ((n) (n) (n (n) (n) (n) (n) (n"}, {"heading": "C Technical tools", "text": "Theorem 4th [29] Let X1, X2,..., Xm i.i.d. chance variables with cumulative distribution function F, and let F-m use the associated empirical cumulative density function F-m = 1m \u00b2 m i = 1 1IXi \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p = 1.K \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 1.K \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p p p \u00b2 p p p \u00b2 p p p \u00b2 p p \u00b2 p p p \u00b2 p p \u00b2 p p p \u00b2 p \u00b2 p p \u00b2 p p \u00b2 p p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p \u00b2 p p p \u00b2 p p \u00b2 p p \u00b2 p p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p p p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p p p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p p p \u00b2 p \u00b2 p p p p p \u00b2 p p p p p p) p p p \u00b2 p \u00b2 p p p p \u00b2 p p p \u00b2 p \u00b2 p p p p \u00b2 p p \u00b2 p p p p \u00b2 p p) p p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p p p p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p p \u00b2 p p \u00b2 p p \u00b2 p p p \u00b2 p p \u00b2 p p p \u00b2 p \u00b2 p p p \u00b2 p p \u00b2 p p p \u00b2 p p \u00b2 p p p \u00b2 p) p p p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p \u00b2 p p p p p \u00b2 p p p \u00b2 p \u00b2 p \u00b2 p p p) p p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p p \u00b2 p p p \u00b2 p p p p p \u00b2 p p p p p \u00b2 p p p p) p p p p p p p"}, {"heading": "D Numerical Evaluation", "text": "rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the eaeaeaJnlhsrrr\u00fccnlhr the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc-eaeaeaJnlrrgneei-eaJnlrgneei-eSrrrrrrrrrteeeaeaeaeaeaeaeaeaeaeaeeaeaeetnllrlrlllrlueeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeaeaeeaeeeaeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee"}], "references": [{"title": "Unsupervised learning of invariant representations in hierarchical architectures", "author": ["F. Anselmi", "J.Z. Leibo", "L. Rosasco", "J. Mutch", "A. Tacchetti", "T. Poggio"], "venue": "CoRR, vol. abs/1311.4158, 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Invariant scattering convolution networks", "author": ["J. Bruna", "S. Mallat"], "venue": "CoRR, vol. abs/1203.1513, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A.C. Courville", "P. Vincent"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 8, pp. 1798\u20131828, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1828}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, pp. 2278\u20132324, 1998.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1998}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, pp. 1106\u20131114, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Incorporating prior information in machine learning by creating virtual examples", "author": ["P. Niyogi", "F. Girosi", "T. Poggio"], "venue": "Proceedings of the IEEE, pp. 2196\u20132209, 1998.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1998}, {"title": "Learning from hints in neural networks", "author": ["Y.-A. Mostafa"], "venue": "Journal of complexity, vol. 6, pp. 192\u2013198, June 1990.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1990}, {"title": "Statistical learning theory", "author": ["V.N. Vapnik"], "venue": "A Wiley-Interscience Publication", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "Support vector machines", "author": ["I. Steinwart", "A. Christmann"], "venue": "Information Science and Statistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Invariance in kernel methods by haar-integration kernels", "author": ["B. Haasdonk", "A. Vossen", "H. Burkhardt"], "venue": "SCIA , Springer, 2005.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Convexity, classification, and risk bounds", "author": ["P.L. Bartlett", "M.I. Jordan", "J.D. McAuliffe"], "venue": "Journal of the American Statistical Association, vol. 101, no. 473, pp. 138\u2013156, 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Spline models for observational data, vol. 59 of CBMS-NSF", "author": ["G. Wahba"], "venue": "Regional Conference Series in Applied Mathematics. Philadelphia, PA: SIAM,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1990}, {"title": "Extensions of lipschitz mappings into a hilbert space", "author": ["W.B. Johnson", "J. Lindenstrauss"], "venue": "Conference in modern analysis and probability, 1984.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1984}, {"title": "Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning", "author": ["A. Rahimi", "B. Recht"], "venue": "NIPS 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Uniform approximation of functions with random bases", "author": ["A. Rahimi", "B. Recht"], "venue": "Proceedings of the 46th Annual Allerton Conference, 2008.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Using the nystrm method to speed up kernel machines", "author": ["C. Williams", "M. Seeger"], "venue": "NIPS, 2001.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2001}, {"title": "On the equivalence between quadrature rules and random features", "author": ["F.R. Bach"], "venue": "CoRR, vol. abs/1502.06800, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning with transformation invariant kernels", "author": ["C. Walder", "O. Chapelle"], "venue": "NIPS, 2007.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Kernel methods for deep learning", "author": ["Y. Cho", "L.K. Saul"], "venue": "NIPS, pp. 342\u2013350, 2009.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Kernel descriptors for visual recognition", "author": ["L. Bo", "X. Ren", "D. Fox"], "venue": "NIPS., 2010.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Convolutional kernel networks", "author": ["J. Mairal", "P. Koniusz", "Z. Harchaoui", "C. Schmid"], "venue": "NIPS, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Gurls: a least squares library for supervised learning", "author": ["A. Tacchetti", "P.K. Mallapragada", "M. Santoro", "L. Rosasco"], "venue": "CoRR, vol. abs/1303.0934, 2013.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Word-level invariant representations from acoustic waveforms", "author": ["S. Voinea", "C. Zhang", "G. Evangelopoulos", "L. Rosasco", "T. Poggio"], "venue": "vol. 14, pp. 3201\u20133205, September 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic speech recognition and speech variability: A review", "author": ["M. Benzeghiba", "R. De Mori", "O. Deroo", "S. Dupont", "T. Erbes", "D. Jouvet", "L. Fissore", "P. Laface", "A. Mertins", "C. Ris", "R. Rose", "V. Tyagi", "C. Wellekens"], "venue": "Speech Communication, vol. 49, pp. 763\u2013786, 01 2007.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "The greatest of a finite set of random variables", "author": ["C.E. Clark"], "venue": "Operations Research, vol. 9, pp. 145\u2013162, Mar-Apr 1961.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1961}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["R. Vershynin"], "venue": "Compressed Sensing: Theory and Applications, Y. Eldar and G. Kutyniok, Eds. Cambridge University Press., 2011.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Inequalities for quantiles of the chi-square distribution", "author": ["T.Inglot"], "venue": "Probability and Mathematical Statistics, vol. 30(2):339351, 2010.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Asymptotic minimax character of the sample distribution function and of the classical multinomial estimator", "author": ["A. Dvoretzky", "J. Kiefer", "J. Wolfowitz"], "venue": "Ann. Math. Statist., vol. 27, pp. 642\u2013669, 09 1956. 20", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1956}], "referenceMentions": [{"referenceID": 0, "context": "We analyze in this paper a random feature map based on a theory of invariance (I-theory) introduced in [1].", "startOffset": 103, "endOffset": 106}, {"referenceID": 0, "context": "Encoding signals or building similarity kernels that are invariant to the action of a group is a key problem in unsupervised learning as it reduces the complexity of the learning task and mimics how our brain represents information invariantly to symmetries and various nuisance factors (change in lighting in image classification and pitch variation in speech recognition) [1, 2, 3, 4].", "startOffset": 374, "endOffset": 386}, {"referenceID": 1, "context": "Encoding signals or building similarity kernels that are invariant to the action of a group is a key problem in unsupervised learning as it reduces the complexity of the learning task and mimics how our brain represents information invariantly to symmetries and various nuisance factors (change in lighting in image classification and pitch variation in speech recognition) [1, 2, 3, 4].", "startOffset": 374, "endOffset": 386}, {"referenceID": 2, "context": "Encoding signals or building similarity kernels that are invariant to the action of a group is a key problem in unsupervised learning as it reduces the complexity of the learning task and mimics how our brain represents information invariantly to symmetries and various nuisance factors (change in lighting in image classification and pitch variation in speech recognition) [1, 2, 3, 4].", "startOffset": 374, "endOffset": 386}, {"referenceID": 3, "context": "Convolutional neural networks [5, 6] achieve state of the art performance in many computer vision and speech recognition tasks but require a large amount of labeled examples as well as augmented data, where we reflect symmetries of the world through virtual examples [7, 8] obtained by transforming the training examples with identity-preserving transformations such as shearing, rotation, translation, etc.", "startOffset": 30, "endOffset": 36}, {"referenceID": 4, "context": "Convolutional neural networks [5, 6] achieve state of the art performance in many computer vision and speech recognition tasks but require a large amount of labeled examples as well as augmented data, where we reflect symmetries of the world through virtual examples [7, 8] obtained by transforming the training examples with identity-preserving transformations such as shearing, rotation, translation, etc.", "startOffset": 30, "endOffset": 36}, {"referenceID": 5, "context": "Convolutional neural networks [5, 6] achieve state of the art performance in many computer vision and speech recognition tasks but require a large amount of labeled examples as well as augmented data, where we reflect symmetries of the world through virtual examples [7, 8] obtained by transforming the training examples with identity-preserving transformations such as shearing, rotation, translation, etc.", "startOffset": 267, "endOffset": 273}, {"referenceID": 6, "context": "Convolutional neural networks [5, 6] achieve state of the art performance in many computer vision and speech recognition tasks but require a large amount of labeled examples as well as augmented data, where we reflect symmetries of the world through virtual examples [7, 8] obtained by transforming the training examples with identity-preserving transformations such as shearing, rotation, translation, etc.", "startOffset": 267, "endOffset": 273}, {"referenceID": 0, "context": "In this work, we adopt the approach of [1] where the representation of the signal is designed to reflect the invariant properties and model the world symmetries with group actions.", "startOffset": 39, "endOffset": 42}, {"referenceID": 7, "context": "The ultimate aim is to bridge unsupervised learning of invariant representations with invariant kernel methods, where we can easily address the statistical consistency and sample complexity questions, using tools from classical supervised learning [9, 10].", "startOffset": 248, "endOffset": 255}, {"referenceID": 8, "context": "The ultimate aim is to bridge unsupervised learning of invariant representations with invariant kernel methods, where we can easily address the statistical consistency and sample complexity questions, using tools from classical supervised learning [9, 10].", "startOffset": 248, "endOffset": 255}, {"referenceID": 9, "context": "We refer the reader to the related work section for a review (Section 5) and we start by showing how to accomplish this invariance through Haar-integration group-invariant kernels [11], and then show how random features derived from a memory based theory of invariances introduced in [1] approximate such a kernel.", "startOffset": 180, "endOffset": 184}, {"referenceID": 0, "context": "We refer the reader to the related work section for a review (Section 5) and we start by showing how to accomplish this invariance through Haar-integration group-invariant kernels [11], and then show how random features derived from a memory based theory of invariances introduced in [1] approximate such a kernel.", "startOffset": 284, "endOffset": 287}, {"referenceID": 9, "context": "1 Group Invariant Kernels We start by reviewing Haar-integration group-invariant kernels introduced in [11], and their use in a binary classification problem.", "startOffset": 103, "endOffset": 107}, {"referenceID": 9, "context": "Define an invariant kernel K between x, z \u2208 X through Haar-integration [11] as follows: K(x, z) = \u222b", "startOffset": 71, "endOffset": 75}, {"referenceID": 9, "context": "Moreover, if k0 is a positive definite kernel, it follows that K is positive definite as well [11].", "startOffset": 94, "endOffset": 98}, {"referenceID": 10, "context": "In order to learn a decision function f : X \u2192 Y , we minimize the following empirical risk induced by an L-Lipschitz and convex loss function V , with V \u2032(0) < 0 [12]: minf\u2208HK \u00caV (f) := 1 N \u2211N i=1 V (yif(xi)), where we restrict f to belong to a hypothesis class induced by the invariant kernel K, the so called reproducing kernel hilbert space HK.", "startOffset": 162, "endOffset": 166}, {"referenceID": 11, "context": "The representer theorem [13] shows that the solution of such a problem, or the optimal decision boundary f\u2217 N has the following form: f \u2217 N (x) = \u2211N i=1 \u03b1 \u2217 iK(x, xi).", "startOffset": 24, "endOffset": 28}, {"referenceID": 10, "context": "EV (f) is a proxy to the misclassification risk [12].", "startOffset": 48, "endOffset": 52}, {"referenceID": 0, "context": "We first show that a non linear random feature map \u03a6 : X \u2192 R derived from a memory based theory of invariances introduced in [1] induces an expected Haar-integration groupinvariant kernel K.", "startOffset": 125, "endOffset": 128}, {"referenceID": 0, "context": "2 From Group Invariant Kernels to Feature Maps In this paper we show that a random feature map based on I-theory [1]: \u03a6 : X \u2192 R approximates a Haar-integration group-invariant kernel K having the form given in Equation (1): \u3008\u03a6(x),\u03a6(z)\u3009 \u2248 K(x, z).", "startOffset": 113, "endOffset": 116}, {"referenceID": 0, "context": "1) The main advantage of such a feature map as outlined in [1], is that we store transformed templates in order to compute \u03a6, while if we needed to compute an invariant kernel of typeK (Equation (1)), we need to expliclitly transform the points.", "startOffset": 59, "endOffset": 62}, {"referenceID": 0, "context": "It falls in the category of memory-based learning, and is biologically plausible [1].", "startOffset": 81, "endOffset": 84}, {"referenceID": 12, "context": "Theorem 2 is in a sense an invariant Johnson Lindenstrauss [14] type result where we show that the dot product defined by the random feature map \u03a6 , i.", "startOffset": 59, "endOffset": 63}, {"referenceID": 13, "context": "The architecture of the proof follows ideas from [15] and [16].", "startOffset": 49, "endOffset": 53}, {"referenceID": 14, "context": "The architecture of the proof follows ideas from [15] and [16].", "startOffset": 58, "endOffset": 62}, {"referenceID": 14, "context": "Similarly to [16], we define the following infinite-dimensional function space: Fp = { f(x) = \u222b \u222b s", "startOffset": 13, "endOffset": 17}, {"referenceID": 15, "context": "Several schemes have been proposed for approximating a non-linear kernel with an explicit non-linear feature map in conjunction with linear methods, such as the Nystr\u00f6m method [17] or random sampling techniques in the Fourier domain for translation-invariant kernels [15].", "startOffset": 176, "endOffset": 180}, {"referenceID": 13, "context": "Several schemes have been proposed for approximating a non-linear kernel with an explicit non-linear feature map in conjunction with linear methods, such as the Nystr\u00f6m method [17] or random sampling techniques in the Fourier domain for translation-invariant kernels [15].", "startOffset": 267, "endOffset": 271}, {"referenceID": 16, "context": "We note that the relation between random features and quadrature rules has been thoroughly studied in [18], where sharper bounds and error rates are derived, and can apply to our setting.", "startOffset": 102, "endOffset": 106}, {"referenceID": 9, "context": "We focused in this paper on Haar-integration kernels [11], since they have an integral representation and hence can be represented with random features [18].", "startOffset": 53, "endOffset": 57}, {"referenceID": 16, "context": "We focused in this paper on Haar-integration kernels [11], since they have an integral representation and hence can be represented with random features [18].", "startOffset": 152, "endOffset": 156}, {"referenceID": 17, "context": "Other invariant kernels have been proposed: In [19] authors introduce transformation invariant kernels, but unlike our general setting, the analysis is concerned with dilation invariance.", "startOffset": 47, "endOffset": 51}, {"referenceID": 18, "context": "In [20], multilayer arccosine kernels are built by composing kernels that have an integral representation, but does not explicitly induce invariance.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "More closely related to our work is [21], where kernel descriptors are built for visual recognition by introducing a kernel view of histogram of gradients that corresponds in our case to the cumulative distribution on the group variable.", "startOffset": 36, "endOffset": 40}, {"referenceID": 20, "context": "Finally the convolutional kernel network of [22] builds a sequence of multilayer kernels that have an integral representation, by convolution, considering spatial neighborhoods in an image.", "startOffset": 44, "endOffset": 48}, {"referenceID": 1, "context": "Our future work will consider the composition of Haar-integration kernels, where the convolution is applied not only to the spatial variable but to the group variable akin to [2].", "startOffset": 175, "endOffset": 178}, {"referenceID": 21, "context": "All RLS experiments in this paper were completed with the GURLS toolbox [23].", "startOffset": 72, "endOffset": 76}, {"referenceID": 23, "context": "We seek local invariance to pitch and speaking rate [25], and so all random templates are pitch shifted up and down by 400 cents and warped to play at half and double speed.", "startOffset": 52, "endOffset": 56}, {"referenceID": 22, "context": "See [24] for more detail.", "startOffset": 4, "endOffset": 8}, {"referenceID": 23, "context": "[25].", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "The following Lemma from [26] gives the expectation and the variance of the maximum of two gaussians with correllation coefficient \u03c1.", "startOffset": 25, "endOffset": 29}, {"referenceID": 24, "context": "Lemma 2 (Mean and Variance of Maximum of Correlated Gaussians [26] ).", "startOffset": 62, "endOffset": 66}, {"referenceID": 25, "context": "\u2022 Upper Bound for the upper tail [27]: P ( 1 kX \u2265 1 + \u03b5 ) \u2264 e\u2212k\u03b52/8.", "startOffset": 33, "endOffset": 37}, {"referenceID": 26, "context": "\u2022 Upper Bound for the lower tail [28]: For all k \u2265 2, u \u2265 k \u2212 1 we have: P (X < u) \u2264 1\u2212 1 2 exp ( \u2212 2 (u\u2212 k \u2212 (k \u2212 2) log(u/k) + log(k)) ) .", "startOffset": 33, "endOffset": 37}, {"referenceID": 27, "context": "By the theorem on convergence of the empirical CDF [29] (Theorem 4 given in Appendix D ) we have, for \u03b3 > 0:", "startOffset": 51, "endOffset": 55}, {"referenceID": 14, "context": "Our proof parallels similar proofs in [16].", "startOffset": 38, "endOffset": 42}, {"referenceID": 27, "context": "[29] Let X1, X2, .", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Lemma 7 ([15],Concentration of the mean of bounded random variables in a Hilbert Space).", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "Theorem 5 ([15]).", "startOffset": 11, "endOffset": 15}], "year": 2017, "abstractText": "We analyze in this paper a random feature map based on a theory of invariance (I-theory) introduced in [1]. More specifically, a group invariant signal signature is obtained through cumulative distributions of group transformed random projections. Our analysis bridges invariant feature learning with kernel methods, as we show that this feature map defines an expected Haar integration kernel that is invariant to the specified group action. We show how this non-linear random feature map approximates this group invariant kernel uniformly on a set of N points. Moreover, we show that it defines a function space that is dense in the equivalent Invariant Reproducing Kernel Hilbert Space. Finally, we quantify error rates of the convergence of the empirical risk minimization, as well as the reduction in the sample complexity of a learning algorithm using such an invariant representation for signal classification, in a classical supervised learning setting.", "creator": "LaTeX with hyperref package"}}}