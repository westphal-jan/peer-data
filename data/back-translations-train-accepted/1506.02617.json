{"id": "1506.02617", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2015", "title": "Path-SGD: Path-Normalized Optimization in Deep Neural Networks", "abstract": "We revisit the choice of SGD for training deep neural networks by reconsidering the appropriate geometry in which to optimize the weights. We argue for a geometry invariant to rescaling of weights that does not affect the output of the network, and suggest Path-SGD, which is an approximate steepest descent method with respect to a path-wise regularizer related to max-norm regularization. Path-SGD is easy and efficient to implement and leads to empirical gains over SGD and AdaGrad.", "histories": [["v1", "Mon, 8 Jun 2015 19:01:33 GMT  (452kb,D)", "http://arxiv.org/abs/1506.02617v1", "12 pages, 5 figures"]], "COMMENTS": "12 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE stat.ML", "authors": ["behnam neyshabur", "ruslan salakhutdinov", "nathan srebro"], "accepted": true, "id": "1506.02617"}, "pdf": {"name": "1506.02617.pdf", "metadata": {"source": "CRF", "title": "Path-SGD: Path-Normalized Optimization in Deep Neural Networks", "authors": ["Behnam Neyshabur", "Ruslan Salakhutdinov", "Nathan Srebro"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "\"It is a difficult problem [16, 2] and various heuristics and optimization algorithms to improve the effectiveness of training.\" (D) \"It is as if we can improve the effectiveness of training.\" (D). \"(D).\" (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D. (D). (D). (D). (D. (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D. (D). (D). (D). (D). (D. (D). (D). (D). (D). (D. (D). (D). (D). (D. (D). (D). (D). (D). (D. (D). (D). (D. (D). (D). (D). (D). (D. (D). (D. (D). (D). (D). (D). (D. (D). (D). (D). (D). (D). (D). (D). (D. (D). (D). (D). (D). (D). (D). (D). (D). (D). (D)"}, {"heading": "2 Rescaling and Unbalanceness", "text": "It is one of the special features of RELU (1) u1 (1) u1 (1) u2 (1) u2 (1) u2 (1) u2 (1) u2 (1) u2 (1) u2 (1) u2) u2 (1) u2 (1) u2 (1) u2) u2 (1) u2 (1) u2 (1) u2) u2 (1) u2 (1) u2) u2 (1) u2 (1) u2) u2 (1) u2 (1) u2) u2 (1) u2 (1) u2) u2 (1) u2 u2 (1) u2 u2 (1) u2 u2 u2 (1) u2 u2 u2 (1) u2 u2 (1) u2 u2 u2 (1) u2 u2 (1) u2 u2 (2) u2 u2 (1) u2 u2 (1) u2 u2 u2 (1) u2 u2 (1) u2 u2 (1) u2) u2 (1) u2 u2 (1) u2 u2 (1) u2 (1) u2) u2 u2 (1) u2 (1) u2 (1) u2 u2 (1) u2) u2 (1) u2 (1) u2 (1) u2) u2 (1) u2 (1) u2) u2 (1) u2 (1) u2) u2 (1) u2 (1) u2) u2 (1) u2 (1) u2) u2 (1) u2 (1) u2) u2 (1) u2 u2 (1) u2 (1) u2 (1) u2) u2 u2 (1) u2 u2 (1) u2 u2 (1) u2 (1) u2 (1) u2) u2 u2 u2 (1) u2 (1) u2 (1) u2) u2 u2 (1) u2 (1) u2 u2 (1) u2 u2 (1) u2 u2 (1) u2"}, {"heading": "3 Magnitude/Scale measures for deep networks", "text": "The following type of regulation is possible: \"The maximum regulation covering all units of the norm up to Unit1 [3, 15]; the maximum regulation of Unit1 [4]; the maximum regulation of Unit1 [4]; the maximum regulation of Unit1 [5]; the maximum regulation of Unit1 [5]; the maximum regulation of Unit1 [5]; the maximum regulation of Unit1 [6]; the maximum regulation of Unit1 [7]; the maximum norm of Unit1 [7]; the maximum norm of Regulation corresponds to\" per unit. \"The maximum norm corresponds to\" per unit, \"if we define the equation (4) and can be written as follows:\" per unit. \""}, {"heading": "4 Path-SGD : An Approximate Path-Regularized Steepest Descent", "text": "It is not the first time that we have looked at the question of whether we are in the question, whether we are in the question, whether we are in the question, whether we are in the question, whether we are in the question, whether we are in the question, whether we are in the question, whether we are in the question, whether we are in the question, whether we are in the question, whether we are in the question, whether we are in the question, whether we are in the question, whether we are in the question, whether we are in the question, whether we are in the question, whether we are in the question, whether we are in the question, whether we are in the question, whether we are in the question, whether we are in the question, whether we are in the question, whether we are in the question, whether we are in the question, whether we are in the question, whether we are in the question, whether we are in the question, whether we are in the question, whether we are in the question, whether we are in the question, whether we are in the question, whether we are in the question, whether we are in the question, whether we are in the question, whether we are in the question, whether we are in the question, whether we are in the question."}, {"heading": "5 Experiments", "text": "In fact, it is in such a way that the greater people are able to survive themselves if they are able to survive themselves, \"he told the German Press Agency in an interview with the\" Welt am Sonntag \":\" I do not believe that they will be able to change the world. \"He pointed out that\" the people of the world are able to change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, to change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change,"}, {"heading": "6 Discussion", "text": "We reviewed the choice of Euclidean geometry for the weights of RELU networks, proposed an alternative optimization method that roughly matches another geometry, and demonstrated that the use of such an alternative geometry can be beneficial. In this work, we demonstrate the success of the concept, and we expect that Path-SGD will also be useful in large-scale training for very deep Constitutional Networks. Combining Path-SGD with AdaGrad, dynamics, or other optimization euristics could further improve the results. Although we believe that Path-SGD is a very good optimization method and a simple plug-in for SGD, we hope that this work will inspire others to consider other geometries, other regulators, and perhaps better updating rules. A special feature of Path-SGD is its recalcifying inventory, which we consider appropriate for RELU networks."}, {"heading": "Acknowledgments", "text": "The research was partly funded by the NSF Prize IIS-1302662 and Intel ICRI-CI. We thank Hao Tang for the insightful discussions."}], "references": [{"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In AISTATS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1502.01852,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "In arXiv,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "CoRR, abs/1412.6980", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": "Computer Science Department,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "Optimizing neural networks with kronecker-factored approximate curvature", "author": ["James Martens", "Roger Grosse"], "venue": "In ICML,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Yuval Netzer", "Tao Wang", "Adam Coates", "Alessandro Bissacco", "Bo Wu", "Andrew Y Ng"], "venue": "In NIPS workshop on deep learning and unsupervised feature learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "In search of the real inductive bias: On the role of implicit regularization in deep learning", "author": ["Behnam Neyshabur", "Ryota Tomioka", "Nathan Srebro"], "venue": "International Conference on Learning Representations (ICLR) workshop track,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Norm-based capacity control in neural networks", "author": ["Behnam Neyshabur", "Ryota Tomioka", "Nathan Srebro"], "venue": "COLT,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Rank, trace-norm and max-norm", "author": ["Nathan Srebro", "Adi Shraibman"], "venue": "In Learning Theory,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "On the universality of online mirror descent", "author": ["Nathan Srebro", "Karthik Sridharan", "Ambuj Tewari"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1929}, {"title": "George Dahl", "author": ["I. Sutskever", "J. Martens"], "venue": "and Geoffery Hinton. On the importance of momentum and initialization in deep learning. In ICML", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 14, "context": "1 Introduction Training deep networks is a challenging problem [16, 2] and various heuristics and optimization algorithms have been suggested in order to improve the efficiency of the training [5, 9, 4].", "startOffset": 63, "endOffset": 70}, {"referenceID": 1, "context": "1 Introduction Training deep networks is a challenging problem [16, 2] and various heuristics and optimization algorithms have been suggested in order to improve the efficiency of the training [5, 9, 4].", "startOffset": 63, "endOffset": 70}, {"referenceID": 3, "context": "1 Introduction Training deep networks is a challenging problem [16, 2] and various heuristics and optimization algorithms have been suggested in order to improve the efficiency of the training [5, 9, 4].", "startOffset": 193, "endOffset": 202}, {"referenceID": 7, "context": "1 Introduction Training deep networks is a challenging problem [16, 2] and various heuristics and optimization algorithms have been suggested in order to improve the efficiency of the training [5, 9, 4].", "startOffset": 193, "endOffset": 202}, {"referenceID": 2, "context": "1 Introduction Training deep networks is a challenging problem [16, 2] and various heuristics and optimization algorithms have been suggested in order to improve the efficiency of the training [5, 9, 4].", "startOffset": 193, "endOffset": 202}, {"referenceID": 14, "context": "Many of the current training methods rely on good initialization and then performing Stochastic Gradient Descent (SGD), sometimes together with an adaptive stepsize or momentum term [16, 1, 6].", "startOffset": 182, "endOffset": 192}, {"referenceID": 0, "context": "Many of the current training methods rely on good initialization and then performing Stochastic Gradient Descent (SGD), sometimes together with an adaptive stepsize or momentum term [16, 1, 6].", "startOffset": 182, "endOffset": 192}, {"referenceID": 4, "context": "Many of the current training methods rely on good initialization and then performing Stochastic Gradient Descent (SGD), sometimes together with an adaptive stepsize or momentum term [16, 1, 6].", "startOffset": 182, "endOffset": 192}, {"referenceID": 12, "context": "There is therefore also a strong link between regularization for optimization and regularization for learning: optimization may provide implicit regularization in terms of its corresponding geometry, and for ideal optimization performance the optimization geometry should be aligned with inductive bias driving the learning [14].", "startOffset": 324, "endOffset": 328}, {"referenceID": 13, "context": "We consider here a geometry inspired by max-norm regularization (regularizing the maximum norm of incoming weights into any unit) which seems to provide a better inductive bias compared to the `2 norm (weight decay) [3, 15].", "startOffset": 216, "endOffset": 223}, {"referenceID": 0, "context": "Notations A feedforward neural network that computes a function f : RD \u2192 RC can be represented by a directed acyclic graph (DAG) G(V,E) with D input nodes vin[1], .", "startOffset": 158, "endOffset": 161}, {"referenceID": 0, "context": ", vin[D] \u2208 V , C output nodes vout[1], .", "startOffset": 34, "endOffset": 37}, {"referenceID": 10, "context": "3 Magnitude/Scale measures for deep networks Following [12], we consider the grouping of weights going into each node of the network.", "startOffset": 55, "endOffset": 59}, {"referenceID": 13, "context": "Another form of regularization that is shown to be very effective in RELU networks is the max-norm regularization, which is the maximum over all units of norm of incoming edge to the unit1 [3, 15].", "startOffset": 189, "endOffset": 196}, {"referenceID": 13, "context": "In these cases, per-unit `2 regularization has shown to be very effective [15].", "startOffset": 74, "endOffset": 78}, {"referenceID": 10, "context": "The `p-path regularizer is then defined as the `p norm of \u03c0(w) [12]: \u03c6p(w) = \u2016\u03c0(w)\u2016p = \uf8ec\uf8ed \u2211 vin[i] e1 \u2192v1 e2 \u2192v2.", "startOffset": 63, "endOffset": 67}, {"referenceID": 10, "context": "1 ([12]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "\u03c6p(w) = min w\u0303\u223cw ( \u03bcp,\u221e(w\u0303) )d This definition of max-norm is a bit different than the one used in the context of matrix factorization [13].", "startOffset": 135, "endOffset": 139}, {"referenceID": 6, "context": "We conduct our experiments on four common benchmark datasets: the standard MNIST dataset of handwritten digits [8]; CIFAR-10 and CIFAR-100 datasets of tiny images of natural scenes [7]; and Street View House Numbers (SVHN) dataset containing color images of house numbers collected by Google Street View [10].", "startOffset": 111, "endOffset": 114}, {"referenceID": 5, "context": "We conduct our experiments on four common benchmark datasets: the standard MNIST dataset of handwritten digits [8]; CIFAR-10 and CIFAR-100 datasets of tiny images of natural scenes [7]; and Street View House Numbers (SVHN) dataset containing color images of house numbers collected by Google Street View [10].", "startOffset": 181, "endOffset": 184}, {"referenceID": 8, "context": "We conduct our experiments on four common benchmark datasets: the standard MNIST dataset of handwritten digits [8]; CIFAR-10 and CIFAR-100 datasets of tiny images of natural scenes [7]; and Street View House Numbers (SVHN) dataset containing color images of house numbers collected by Google Street View [10].", "startOffset": 304, "endOffset": 308}], "year": 2015, "abstractText": "We revisit the choice of SGD for training deep neural networks by reconsidering the appropriate geometry in which to optimize the weights. We argue for a geometry invariant to rescaling of weights that does not affect the output of the network, and suggest Path-SGD, which is an approximate steepest descent method with respect to a path-wise regularizer related to max-norm regularization. Path-SGD is easy and efficient to implement and leads to empirical gains over SGD and AdaGrad.", "creator": "LaTeX with hyperref package"}}}