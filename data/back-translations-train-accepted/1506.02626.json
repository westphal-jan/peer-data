{"id": "1506.02626", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2015", "title": "Learning both Weights and Connections for Efficient Neural Networks", "abstract": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy, by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG16 found that the network as a whole can be reduced 6.8x just by pruning the fully-connected layers, again with no loss of accuracy.", "histories": [["v1", "Mon, 8 Jun 2015 19:28:43 GMT  (2968kb,D)", "http://arxiv.org/abs/1506.02626v1", null], ["v2", "Wed, 29 Jul 2015 22:27:31 GMT  (1484kb,D)", "http://arxiv.org/abs/1506.02626v2", null], ["v3", "Fri, 30 Oct 2015 23:29:27 GMT  (1075kb,D)", "http://arxiv.org/abs/1506.02626v3", "Published as a conference paper at NIPS 2015"]], "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["song han", "jeff pool", "john tran", "william j dally"], "accepted": true, "id": "1506.02626"}, "pdf": {"name": "1506.02626.pdf", "metadata": {"source": "CRF", "title": "Learning both Weights and Connections for Efficient Neural Networks", "authors": ["Song Han", "Jeff Pool", "John Tran", "William J. Dally"], "emails": ["dally}@stanford.edu", "johntran}@nvidia.com"], "sections": [{"heading": "1 Introduction", "text": "In recent years, it has been shown that the number of unemployed able to do their jobs has increased many times over previous years. (...) In the last ten years, the number of unemployed in Germany has doubled. (...) In the last ten years, the number of unemployed in Germany has doubled. (...) In the last ten years, the number of unemployed in Germany has tripled. (...) In the last ten years, the number of unemployed in Germany has tripled. (...) In the last ten years, the number of unemployed in Germany has tripled. (...) In the last ten years, the number of unemployed in Germany has tripled. (...) In the last ten years, the number of unemployed in Germany has tripled. (...) In the last ten years, the number of unemployed in Germany has tripled. (...) In the last ten years, the number of unemployed in Germany has tripled. (...) In the last ten years, the number of unemployed has tripled."}, {"heading": "2 Related Work", "text": "This leads to a waste of computing power and memory usage. There are various proposals that exploit the linear structures of the neural network by finding an appropriate approximation of the parameters and maintaining accuracy within the original model."}, {"heading": "3 Learning Connections in Addition to Weights", "text": "Our intersection method uses a three-step process, as illustrated in Figure 2, which begins with learning connectivity through normal network training. However, unlike traditional training, we do not learn the final values of weights, but rather which connections are important. The second step is cutting the connections with light weight. All connections with weights below a threshold are removed from the network - and convert a dense network into a sparse network, as shown in Figure 3.The final step retrains the network to learn the final weights for the remaining sparse connections. However, this step is crucial if the cropped network is used without retraining, accuracy is significantly compromised."}, {"heading": "3.1 Regularization", "text": "Choosing the right regulation influences the performance of pruning and retraining. L1 regulation penalises parameters that do not deviate from zero, resulting in more parameters close to zero. This gives better accuracy after pruning but before retraining. However, the remaining connections are not as good as with L2 regulation, which results in lower accuracy after retraining."}, {"heading": "3.2 Dropout and Capacity Control", "text": "During retraining, however, the dropout rate needs to be adjusted to take into account the change in model capacity. Hinton's dropout [19] can be considered a \"soft dropout,\" as each parameter is dropped with a probability of failure, not definitive failure. Cropping can be considered a \"hard dropout,\" where parameters are eliminated forever after truncation and have no chance of coming back. As the parameters become sparse, the classifier selects the most informative predictors and thus has much less predictive variance, which reduces overfitting. As already reduced model capacities are truncated, the dropout rate should be lower. Quantitatively, Ci is the number of connections in layer i, Cio for the original network, Cir for the network after retraining, Ni the number of neurons in layer i. Since dropout factors act on neurons."}, {"heading": "3.3 Local Pruning and Parameter Co-adaptation", "text": "During the retraining, it is better to maintain the weights from the initial training phase for the connections that have survived the pruning than to re-initialize the pruned layers. [25] shows that CNN's fragile co-adapted properties: the decrease in the gradient is able to find a good solution if the network is first trained, but not after some layers have been re-initialized and re-trained. Thus, if we re-initialize the pruned layer, we need to re-educate the entire network; if we re-educate only the pruned layers, we need to re-educate ourselves while retaining the remaining parameters. Cutting the pruned layers, starting with the retained weights, requires less composition, because we do not need to reproduce ourselves through the entire network. Moreover, neural networks tend to suffer the problem of the retrograde [26], as the networks become deeper, making the recovery of the pruned layers more difficult for us to re-attach the network during the initial pulling process, while only adjusting the parameters of the network we are making it more difficult for the initial one to retrain."}, {"heading": "3.4 Iterative Pruning", "text": "Learning the right connections is an iterative process. Pruning followed by retraining is an iteration, after many such iterations the minimum number of connections could be found. Without loss of accuracy, this method can increase the pruning rate on AlexNet from 5 x to 9 x compared to single-stage aggressive pruning. Each iteration is a greedy quest, as we find the best connections. We also experimented with probabilistically pruned parameters based on their absolute value, but this resulted in worse results."}, {"heading": "3.5 Pruning Neurons", "text": "The retraining phase automatically leads to the conclusion that dead neurons have both zero input and zero output connections, due to the drop in gradient and regulation. A neuron that has zero input connections (or zero output connections) does not contribute to the final loss, which results in the gradient for its output connection (or input connection) being zero. Only the regularization term presses the weights to zero. Thus, the dead neurons are automatically removed during the retraining."}, {"heading": "4 Experiments", "text": "Caffe has been modified to add a mask for all weight sensors that do not take trimmed parameters into account during network operation. The trim threshold is chosen as a quality parameter multiplied by the standard deviation of the weight of a layer. We performed the experiments on Nvidia TitanX and GTX980 GPUs. We trimmed 4 networks: 2 on MNIST and 2 on ImageNet datasets. Network parameters and 1 accuracy before and after trimming are shown in Table 1."}, {"heading": "4.1 LeNet on MNIST", "text": "We first experimented with the MNIST dataset with LeNet-300-100 and LeNet-5 network [4]. LeNet-300100 is a fully connected network with two hidden layers, each with 300 and 100 neurons, which achieves a 1.6% error rate in Mnist. LeNet-5 is a revolutionary network with two revolutionary layers and two fully connected layers, which achieves an error rate of 0.8% in Mnist. After the cut, the network is retrained with 1 / 10 of the original learning rate of the original network. Table 1 shows that pruning1Reference accuracy values were generated without data augmentation, which achieve savings of 12 x parameters in these networks. For each layer of the network, the table (from left to right) shows the original number of weights, the number of flowpoint operations used to calculate the activations of this layer, the average percentage of activations of the activations that are non-zero, the percentage of the remaining weights, and the remainder of the savings in the remaining layer."}, {"heading": "4.2 AlexNet on ImageNet", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "4.3 VGG16 on ImageNet", "text": "With promising results on AlexNet, we also looked at a larger, newer network, VGG16 [30], on the same ILSVRC 2012 dataset. VGG16 has many more revolutionary layers, but still only three full-2We have confirmed that the Caffe model is fully trained, networked layers. Using a similar methodology, we have aggressively pruned these fully connected layers to achieve a significant reduction in the weights shown in Table 6. Specifically, we used three iterations of pruning on the pre-trained network of Caffe.The results of VGG16, like those of AlexNet, are very promising; the network as a whole, has been reduced to 14.6% of its original size, and we have not yet had a chance to prune the Convolutionary layers, which are 6 times larger than AlexNet's. That is, all the gains so far come from pruning the fully connected layers alone."}, {"heading": "5 Conclusion", "text": "We have presented a method to improve the efficiency of neural networks without compromising accuracy by finding the right connections. Our method, which is partly motivated by the functioning of learning in the mammalian brain, works by learning which connections are important by cutting the unimportant connections and then retraining the remaining sparse network. We highlight our experiments on AlexNet on ImageNet and show that both fully connected layers and twisted layers can be clipped, which reduces the number of connections by 9-fold or 12-fold without loss of accuracy, while they only cause a 0.3% loss of accuracy. Similar results are shown for VGG16 and LeNet networks, resulting in reduced storage capacities and bandwidth requirements for real-time image processing."}], "references": [{"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "Leon Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Yaniv Taigman", "Ming Yang", "Marc\u2019Aurelio Ranzato", "Lior Wolf"], "venue": "In CVPR,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Deep learning with cots hpc systems", "author": ["Adam Coates", "Brody Huval", "Tao Wang", "David Wu", "Bryan Catanzaro", "Ng Andrew"], "venue": "In 30th ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Neuronal mechanisms of developmental plasticity in the cat\u2019s visual system", "author": ["JP Rauschecker"], "venue": "Human neurobiology,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1983}, {"title": "Peter huttenlocher (1931-2013)", "author": ["Christopher A Walsh"], "venue": "Nature, 502(7470):172\u2013172,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Predicting parameters in deep learning", "author": ["Misha Denil", "Babak Shakibi", "Laurent Dinh", "Nando de Freitas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Improving the speed of neural networks on cpus", "author": ["Vincent Vanhoucke", "Andrew Senior", "Mark Z Mao"], "venue": "In Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Emily L Denton", "Wojciech Zaremba", "Joan Bruna", "Yann LeCun", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Yunchao Gong", "Liu Liu", "Ming Yang", "Lubomir Bourdev"], "venue": "arXiv preprint arXiv:1412.6115,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Comparing biases for minimal network construction with back-propagation", "author": ["Stephen Jos\u00e9 Hanson", "Lorien Y Pratt"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1989}, {"title": "Optimal brain damage", "author": ["Yann Le Cun", "John S. Denker", "Sara A. Solla"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1990}, {"title": "Second order derivatives for network pruning: Optimal brain surgeon", "author": ["Babak Hassibi", "David G Stork"], "venue": "Advances in neural information processing systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1993}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Regularization of neural networks using dropconnect", "author": ["Li Wan", "Matthew Zeiler", "Sixin Zhang", "Yann L Cun", "Rob Fergus"], "venue": "In ICML,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Improving generalization of neural networks through pruning", "author": ["Hans Henrik Thodberg"], "venue": "International Journal of Neural Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1991}, {"title": "Compressing neural networks with the hashing trick", "author": ["Wenlin Chen", "James T. Wilson", "Stephen Tyree", "Kilian Q. Weinberger", "Yixin Chen"], "venue": "arXiv preprint arXiv:1504.04788,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Hash kernels for structured data", "author": ["Qinfeng Shi", "James Petterson", "Gideon Dror", "John Langford", "Alex Smola", "SVN Vishwanathan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Feature hashing for large scale multitask learning", "author": ["Kilian Weinberger", "Anirban Dasgupta", "John Langford", "Alex Smola", "Josh Attenberg"], "venue": "In ICML,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "How transferable are features in deep neural networks", "author": ["Jason Yosinski", "Jeff Clune", "Yoshua Bengio", "Hod Lipson"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1994}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Deep fried convnets", "author": ["Zichao Yang", "Marcin Moczulski", "Misha Denil", "Nando de Freitas", "Alex Smola", "Le Song", "Ziyu Wang"], "venue": "arXiv preprint arXiv:1412.7149,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Memory bounded deep convolutional networks", "author": ["Maxwell D Collins", "Pushmeet Kohli"], "venue": "arXiv preprint arXiv:1412.1442,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Neural networks have become ubiquitous in applications ranging from computer vision [1] to speech recognition [2] and natural language processing [3].", "startOffset": 84, "endOffset": 87}, {"referenceID": 1, "context": "Neural networks have become ubiquitous in applications ranging from computer vision [1] to speech recognition [2] and natural language processing [3].", "startOffset": 110, "endOffset": 113}, {"referenceID": 2, "context": "Neural networks have become ubiquitous in applications ranging from computer vision [1] to speech recognition [2] and natural language processing [3].", "startOffset": 146, "endOffset": 149}, {"referenceID": 3, "context": "In 1998 Lecun classified handwritten digits with less than 1M parameters [4], while in 2012, Krizhevsky et al.", "startOffset": 73, "endOffset": 76}, {"referenceID": 0, "context": "[1] won the ImageNet competition with 60M parameters.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Deepface classified human faces with 120M parameters [5], and Coates et al.", "startOffset": 53, "endOffset": 56}, {"referenceID": 5, "context": "[6] scaled up a network to 10B parameters.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "In effect, this training process learns the network connectivity in addition to the weights - much as in the mammalian brain [8] [9], where synapses are created in the first few months of a child\u2019s development, followed by gradual pruning of little-used connections, falling to typical adult values.", "startOffset": 125, "endOffset": 128}, {"referenceID": 7, "context": "In effect, this training process learns the network connectivity in addition to the weights - much as in the mammalian brain [8] [9], where synapses are created in the first few months of a child\u2019s development, followed by gradual pruning of little-used connections, falling to typical adult values.", "startOffset": 129, "endOffset": 132}, {"referenceID": 8, "context": "Neural networks are typically over-parameterized, and there is significant redundancy for deep learning models [10].", "startOffset": 111, "endOffset": 115}, {"referenceID": 9, "context": "[11] explored a fixed-point implementation with 8-bit integer (vs 32-bit floating point) activations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] exploited the linear structure of the neural network by finding an appropriate low-rank approximation of the parameters and keeping the accuracy within 1% of the original model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] compressed deep convnets using vector quantization, which reduced storage but added one level of indirection in memory reference due to accessing the codebook.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "The Network in Network architecture [14] and GoogLenet [15] achieves state-of-the-art results on several benchmarks by adopting this idea.", "startOffset": 55, "endOffset": 59}, {"referenceID": 12, "context": "This problem is noted by Szegedy et al [15] and motivates them to add a linear layer on the top of their networks to enable transfer learning.", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "An early approach to pruning was biased weight decay [16].", "startOffset": 53, "endOffset": 57}, {"referenceID": 14, "context": "Optimal Brain Damage [17] and Optimal Brain Surgeon [18] prune networks to reduce the number of connections based on the Hessian of the loss function and suggest that such pruning is more accurate than magnitude-based pruning such as weight decay.", "startOffset": 21, "endOffset": 25}, {"referenceID": 15, "context": "Optimal Brain Damage [17] and Optimal Brain Surgeon [18] prune networks to reduce the number of connections based on the Hessian of the loss function and suggest that such pruning is more accurate than magnitude-based pruning such as weight decay.", "startOffset": 52, "endOffset": 56}, {"referenceID": 16, "context": "Dropout [19] and DropConnect [20] zeros out activations and connections in the network to reduce over-fitting rather than to improve efficiency.", "startOffset": 8, "endOffset": 12}, {"referenceID": 17, "context": "Dropout [19] and DropConnect [20] zeros out activations and connections in the network to reduce over-fitting rather than to improve efficiency.", "startOffset": 29, "endOffset": 33}, {"referenceID": 18, "context": "A similar approach was originally described in [21].", "startOffset": 47, "endOffset": 51}, {"referenceID": 19, "context": "HashedNets [22] is a recent technique to reduce model sizes by using a hash function to randomly group connection weights into hash buckets, so that all connections within the same hash bucket share a single parameter value.", "startOffset": 11, "endOffset": 15}, {"referenceID": 20, "context": "[23] and Weinberger et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[24], sparsity will minimize hash collision making feature hashing even more effective.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Hinton\u2019s dropout [19] can be regarded as a \u201dsoft dropout,\u201d since each parameter is dropped with a probability, not definitely dropped out.", "startOffset": 17, "endOffset": 21}, {"referenceID": 22, "context": "[25] shows that CNNs contain fragile co-adapted features: gradient descent is able to find a good solution when the network is initially trained, but not after re-initializing some layers and retraining them.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "Also, neural networks are prone to suffer the vanishing gradient problem [26] as the networks get deeper, which makes pruning errors harder to recover for deep networks.", "startOffset": 73, "endOffset": 77}, {"referenceID": 24, "context": "We implemented network pruning on Caffe [27].", "startOffset": 40, "endOffset": 44}, {"referenceID": 3, "context": "We first experimented on MNIST dataset with LeNet-300-100 and LeNet-5 network [4].", "startOffset": 78, "endOffset": 81}, {"referenceID": 25, "context": "Network Top-1 Error Top-5 Error Parameters Compression Rate Fastfood-32-AD [28] 41.", "startOffset": 75, "endOffset": 79}, {"referenceID": 25, "context": "8M 2\u00d7 Fastfood-16-AD [28] 42.", "startOffset": 21, "endOffset": 25}, {"referenceID": 26, "context": "7\u00d7 Collins & Kohli [29] 44.", "startOffset": 19, "endOffset": 23}, {"referenceID": 26, "context": "[29] reduced the parameters by 4\u00d7 and with inferior accuracy.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "Deep Fried Convnets [28] worked on fully connected layers only and reduced the parameters by less than 4\u00d7.", "startOffset": 20, "endOffset": 24}, {"referenceID": 10, "context": "[12] exploited linear structure of convnets and reduced the parameters of each layer separately, where model compression on a single layer incurred 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "The original AlexNet took 450K iterations to train [27].", "startOffset": 51, "endOffset": 55}, {"referenceID": 27, "context": "With promising results on AlexNet, we also looked at a larger, more recent network, VGG16 [30], on the same ILSVRC-2012 dataset.", "startOffset": 90, "endOffset": 94}], "year": 2015, "abstractText": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy, by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9\u00d7, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG16 found that the network as a whole can be reduced 6.8\u00d7 just by pruning the fully-connected layers, again with no loss of accuracy.", "creator": "LaTeX with hyperref package"}}}