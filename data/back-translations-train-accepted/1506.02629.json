{"id": "1506.02629", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2015", "title": "Generalization in Adaptive Data Analysis and Holdout Reuse", "abstract": "Overfitting is the bane of data analysts, even when data are plentiful. Formal approaches to understanding this problem focus on statistical inference and generalization of individual analysis procedures. Yet the practice of data analysis is an inherently interactive and adaptive process: new analyses and hypotheses are proposed after seeing the results of previous ones, parameters are tuned on the basis of obtained results, and datasets are shared and reused. An investigation of this gap has recently been initiated by the authors in (Dwork et al., 2014), where we focused on the problem of estimating expectations of adaptively chosen functions.", "histories": [["v1", "Mon, 8 Jun 2015 19:34:29 GMT  (58kb,D)", "http://arxiv.org/abs/1506.02629v1", null], ["v2", "Fri, 25 Sep 2015 19:04:32 GMT  (67kb,D)", "http://arxiv.org/abs/1506.02629v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["cynthia dwork", "vitaly feldman", "moritz hardt", "toniann pitassi", "omer reingold", "aaron roth"], "accepted": true, "id": "1506.02629"}, "pdf": {"name": "1506.02629.pdf", "metadata": {"source": "CRF", "title": "Generalization in Adaptive Data Analysis and Holdout Reuse", "authors": ["Cynthia Dwork", "Vitaly Feldman", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Aaron Roth"], "emails": [], "sections": [{"heading": null, "text": "In this paper, we will give a simple and practical method for reusing a holdout set (or test set) to validate the accuracy of hypotheses created by a learning algorithm based on a training set. Repeated reuse of a holdout set can easily lead to overadjustment of the holdout set itself. We will give an algorithm that allows validation of a large number of adaptively selected hypotheses while demonstrably avoiding overadjustment. We will illustrate the advantages of our algorithm over the standard use of the holdout set through a simple synthetic experiment. We will also formalize and address the general problem of data reuse in adaptive data analysis. We will show how the differential data protection-based approach specified in [DFH + 14] is much broader applicable to adaptive data analysis. We will then show that a simple approach based on the descriptive length, we can also provide the validity for both of the analytical ones, if both of the validity can be used in both."}, {"heading": "1 Introduction", "text": "The idea behind it is that it is a way in which people are able to understand the world and understand what they are doing to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change it, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to save the world, to change the world, to save the world, to save the world, to save the world, to save the world, to change the world, to save the world, to save the world, to save the world, to save the world, to change the world, to save the world, to save the world, to save the world, to change the world, to save the world, to save the world, to save the world, to save the world, to change the world, to save the world, to save the world, to save the world, to save the world to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world to change the world, to save the world, to save the world to change the world"}, {"heading": "1.1 Our Results", "text": "We propose a simple and general formulation of the problem of maintaining statistical validity in adaptive data analysis. We show that the link between differentiated private algorithms and the generalization of [DFH + 14] can be extended to this more general environment, and we show that similar (but sometimes incomparable) guarantees can be obtained from algorithms whose results can be described by short strings. We then define a new term, the approximate maximum information, that combines these two basic techniques and gives a new perspective on the problem. In particular, we provide an adaptive compositional set of maximum information that provides an easy way to obtain generalization guarantees for analyses where some of the procedures are differentiated private and others have short description length outputs. We apply our techniques to the problem of reusing the holdout set for validation in the adaptive environment."}, {"heading": "1.1.1 A Reusable Holdout", "text": "We describe a simple and general method, along with two specific instances, for reusing a holdout used to validate results while demonstrably avoiding exceeding the holdout method. The analyst can apply any analysis to the training data set, but can only access the holdout set by an algorithm that allows further averaging to be applied across different partitions in the cross-validation. Our first algorithm, called the thresholdout, derives its guarantees from the different privacy and results in [DFH + 14, NS15]. For each function we perform: X \u2192 [0, 1] given by the analyst, Thresholdout uses validation that does not go beyond training."}, {"heading": "1.2 Generalization in Adaptive Data Analysis", "text": "Most of them are able to survive themselves, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are not able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are not able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own."}, {"heading": "1.3 Related Work", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2 Preliminaries and Basic Techniques", "text": "In the discussion below, we refer to binary logarithm and ln refers to the natural logarithm. For convenience, we limit our random variables to finite ranges (extension of claims to continuous ranges).For two random variables X and Y within the same domain, the maximum divergence of X is defined. (X-Y) = log max x-X P [X-Y] P [Y-O]. We say that a real weighted function over datasets f: Xn \u2192 R has sensitivity c for all i values. (n-Y) = log maxO X, P [X-O] > \u03b5 P [X-Y-O]. We say that a real weighted function over datasets f: Xn \u2192 ES has sensitivity c for all i values. (n-Y) and x1, x2."}, {"heading": "2.1 Differential Privacy", "text": "On an intuitive level, differential privacy hides the data of each individual. We are therefore interested in pairs of data sets that differ in a single element. In this case, we say that S and S \u00b2 are different. \u00b7 Definition 2. [DMNS06, DKM + 06] A randomized algorithm A with domain Xn for n > 0 is differentially private if for all pairs of data sets that differ in a single element."}, {"heading": "2.2 Generalization via Differential Privacy", "text": "Generalization in special cases of our general adaptive analysis setting can be derived directly from the results in [DFH + 14] and composition properties of differentiated private algorithms. In the case of purely differential private algorithms with general outputs over i.i.d. datasets, in [DFH + 14] we will demonstrate the following results. Let's be a differential private algorithm with a range Y and let S be a random variable drawn from a distribution Pn over Xn. Let Y = A (S) be the corresponding output distribution. Assume that for each element there is a subset Y (y)."}, {"heading": "2.3 Generalization via Description Length", "text": "Suppose that the probability (via a random selection of a row) of such a bad result can be limited. Theorem 9. Let A: Xn \u2192 Y be an algorithm and let S be a random row over Xn and S (e.g. overfits). Suppose that the probability (via a random selection of a row) of such a bad result can be limited. Theorem 9. Let A: Xn \u2192 Y be an algorithm and let S be a random row over Xn. Suppose R: Y \u2192 2Xn is such a row."}, {"heading": "3 Max-Information", "text": "Consider two algorithms A: Xn \u2192 Y and B: Xn \u00b7 Y \u2192 Y, \"which are adaptive and assume that for each fixed input Y = \u03b2 \u03b2 \u03b2 Y, B can be generalized for all but a fraction \u03b2 of the datasets. Here, we are informally speaking of generalization: Our definitions will support any property of input Y and its relativization. Intuitively, we want to maintain the generalization of B to ensure that the output of A \u2264 m does not reveal too much information about the datasets S. We show that this intuition can be captured by an idea of maximum information and its relativization of approximately maximum information. For two random variables X and Y, we use X \u00d7 Y to name the random variable obtained by drawing X and Y independently of their probability distributions. Let X and Y jointly distribute random variables."}, {"heading": "3.1 Bounds on Max-information", "text": "We now show that the basic approaches, which are based on the length of the description and the (pure) differential privacy, are covered by approximate maximum information."}, {"heading": "3.1.1 Description Length", "text": "Description length k gives the following limit for the maximum information distribution. Theorem 15. Let A be a random Y algorithm that uses an n-element dataset as input and prints a value in a finite set of Y. Then, for each \u03b2 > 0, let us designate I\u03b2 \u221e (A, n) \u2264 log (| Y | / \u03b2). Let X and Y be two random variables over the same domain X. Let X and Y be two random variables over the same domain X. IfP x \u00b2 p (X) [P = x] P [Y = x] \u2265 2k = then D\u03b2 \u00b2 \u00b2 \u00b2 s (X \u00b2 Y) \u2264 Y \u00b2) \u2264 \u00b2. Let S x x \u00b2 p (P = x] p \u00b2 \u00b2 \u00b2 \u00b2 then D\u03b2 \u00b2 \u00b2 \u00b2 \u00b2 s (X \u00b2 Y) \u2264 \u00b2 k\u00b2."}, {"heading": "3.1.2 Differential Privacy", "text": "We start with a simple limitation to maximum information from differential private algorithms that applies to all distributions of data sets. This limitation applies to all distributions of data sets and implies that a differential, privacy-based approach can be used beyond the i.i.d.d. setting in [DFH + 14]. Theorem 17. Let A be a -differential private algorithm. We will argue that I \u2264 P (Y; S) \u2264 log e \u00b7 n, that I have any random variable beyond n-element input records for A and let Y have the corresponding output distribution Y = (S) \u2264 P \u2264; S \u2264 log e \u00b7 n, that I pro-element (S; Y) \u2264 log e \u00b7 n, that we immediately follow the Bayes rule. Clearly, two data sets S and S \u2032 differ in most n elements."}, {"heading": "4 Reusable Holdout", "text": "We describe two simple algorithms that allow access to the training datasets and limit the budget. \"We describe two simple algorithms that validate the queries in the adaptive settings.4.1 ThresholdoutOur first algorithm ThresholdoutOur first algorithm ThresholdoutOur first algorithm ThresholdoutOur first algorithm ThresholdoutOur first algorithm Thresholdout follows the approach in [DFH + 14], in which different private algorithms are used to answer adaptively selected statistical queries. This approach can also be applied to all the low-sensitivity functions of the dataset, but for simplicity we present the results for statistical queries. Here we address a simpler problem in which the queries of the analysts only need to be answered when they are overfit. Also, unlike [DFH + 14], the analyst has full access to the training datasets and the holdout algorithms that only prevent them from overfitting the data aset."}, {"heading": "5 Experiments", "text": "We describe a simple experiment based on synthetic data that illustrates the danger of reusing a standard holdout set, and how this problem can be solved by our reusable holdout. (In our experiment, the analyst wants to build a classifier using the following common strategy.) First, the analyst finds a series of individual attributes that are correlated with the class label. (Then, the analyst aggregates the correlated variables into a single model of higher accuracy (for example, using boosting or dredging methods). Formally, the analyst is given a d-dimensional dataset S of size 2n and randomly divides it into a training that sets St and a holdout model of the same accuracy. We designate an element of S by a tuple (x, y) where x is a d-dimensional vector and y. (1) is the corresponding class designation we want to select."}, {"heading": "6 Conclusions", "text": "In this paper, we give a unifying look at two techniques (differential privacy and description length limits) that preserve the generalization guarantees of subsequent algorithms in adaptively selected sequences of data analysis. Although these two techniques both imply low maximum information - and thus can be composed while retaining their guarantees - the types of guarantees that can be achieved by both alone are incomparable, suggesting that the problem of generalization guarantees under adaptability is ripe for future studies on two fronts. First, the existing theory is likely to be strong enough to develop practical algorithms with rigorous generalization guarantees, of which Thresholdout is an example. However, additional empirical work is needed to better understand when and how the theory should be applied in specific application scenarios. At the same time, new theory is also needed. As an example of a fundamental question, do we still not know the answer to Holdiboan, which is known as a simple adjustment to the expectations of oneself?"}], "references": [{"title": "Practical privacy: the SuLQ framework", "author": ["Avrim Blum", "Cynthia Dwork", "Frank McSherry", "Kobbi Nissim"], "venue": "In PODS,", "citeRegEx": "Blum et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Blum et al\\.", "year": 2005}, {"title": "The ladder: A reliable leaderboard for machine learning competitions", "author": ["Avrim Blum", "Moritz Hardt"], "venue": "CoRR, abs/1502.04585,", "citeRegEx": "Blum and Hardt.,? \\Q2015\\E", "shortCiteRegEx": "Blum and Hardt.", "year": 2015}, {"title": "More general queries and less generalization error in adaptive data analysis", "author": ["Raef Bassily", "Adam Smith", "Thomas Steinke", "Jonathan Ullman"], "venue": "CoRR, abs/1503.04843,", "citeRegEx": "Bassily et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bassily et al\\.", "year": 2015}, {"title": "On over-fitting in model selection and subsequent selection bias in performance evaluation", "author": ["Gavin C. Cawley", "Nicola L.C. Talbot"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Cawley and Talbot.,? \\Q2010\\E", "shortCiteRegEx": "Cawley and Talbot.", "year": 2010}, {"title": "Preserving statistical validity in adaptive data analysis", "author": ["Cynthia Dwork", "Vitaly Feldman", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Aaron Roth"], "venue": "CoRR, abs/1411.2664,", "citeRegEx": "Dwork et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2014}, {"title": "Efficient multiple hyperparameter learning for log-linear models", "author": ["Chuong B. Do", "Chuan-Sheng Foo", "Andrew Y. Ng"], "venue": "In NIPS,", "citeRegEx": "Do et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Do et al\\.", "year": 2007}, {"title": "Our data, ourselves: Privacy via distributed noise generation", "author": ["Cynthia Dwork", "Krishnaram Kenthapadi", "Frank McSherry", "Ilya Mironov", "Moni Naor"], "venue": "In EUROCRYPT,", "citeRegEx": "Dwork et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2006}, {"title": "Differential privacy and robust statistics", "author": ["C. Dwork", "J. Lei"], "venue": "In Proceedings of the 2009 International ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "Dwork and Lei.,? \\Q2009\\E", "shortCiteRegEx": "Dwork and Lei.", "year": 2009}, {"title": "Calibrating noise to sensitivity in private data analysis", "author": ["Cynthia Dwork", "Frank McSherry", "Kobbi Nissim", "Adam Smith"], "venue": "In Theory of Cryptography,", "citeRegEx": "Dwork et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2006}, {"title": "Revealing information while preserving privacy", "author": ["Irit Dinur", "Kobbi Nissim"], "venue": "In PODS,", "citeRegEx": "Dinur and Nissim.,? \\Q2003\\E", "shortCiteRegEx": "Dinur and Nissim.", "year": 2003}, {"title": "Privacy-preserving datamining on vertically partitioned databases", "author": ["Cynthia Dwork", "Kobbi Nissim"], "venue": "In CRYPTO,", "citeRegEx": "Dwork and Nissim.,? \\Q2004\\E", "shortCiteRegEx": "Dwork and Nissim.", "year": 2004}, {"title": "The algorithmic foundations of differential privacy", "author": ["Cynthia Dwork", "Aaron Roth"], "venue": "Foundations and Trends in Theoretical Computer Science,", "citeRegEx": "Dwork and Roth.,? \\Q2014\\E", "shortCiteRegEx": "Dwork and Roth.", "year": 2014}, {"title": "A firm foundation for private data analysis", "author": ["Cynthia Dwork"], "venue": "CACM, 54(1):86\u201395,", "citeRegEx": "Dwork.,? \\Q2011\\E", "shortCiteRegEx": "Dwork.", "year": 2011}, {"title": "A note on screening regression equations", "author": ["David A. Freedman"], "venue": "The American Statistician,", "citeRegEx": "Freedman.,? \\Q1983\\E", "shortCiteRegEx": "Freedman.", "year": 1983}, {"title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction", "author": ["T. Hastie", "R. Tibshirani", "J.H. Friedman"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2009}, {"title": "Preventing false discovery in interactive data analysis is hard", "author": ["Moritz Hardt", "Jonathan Ullman"], "venue": "In FOCS,", "citeRegEx": "Hardt and Ullman.,? \\Q2014\\E", "shortCiteRegEx": "Hardt and Ullman.", "year": 2014}, {"title": "Learning theory: stability is sufficient for generalization and necessary and sufficient for consistency of empirical risk minimization", "author": ["Sayan Mukherjee", "Partha Niyogi", "Tomaso Poggio", "Ryan Rifkin"], "venue": "Advances in Computational Mathematics,", "citeRegEx": "Mukherjee et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Mukherjee et al\\.", "year": 2006}, {"title": "Preventing \u201doverfitting\u201d of cross-validation data", "author": ["Andrew Y. Ng"], "venue": "In ICML, pages 245\u2013253,", "citeRegEx": "Ng.,? \\Q1997\\E", "shortCiteRegEx": "Ng.", "year": 1997}, {"title": "On the generalization properties of differential privacy", "author": ["Kobbi Nissim", "Uri Stemmer"], "venue": "CoRR, abs/1504.05800,", "citeRegEx": "Nissim and Stemmer.,? \\Q2015\\E", "shortCiteRegEx": "Nissim and Stemmer.", "year": 2015}, {"title": "General conditions for predictivity in learning", "author": ["Tomaso Poggio", "Ryan Rifkin", "Sayan Mukherjee", "Partha Niyogi"], "venue": "theory. Nature,", "citeRegEx": "Poggio et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Poggio et al\\.", "year": 2004}, {"title": "Overfitting in making comparisons between variable selection methods", "author": ["Juha Reunanen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Reunanen.,? \\Q2003\\E", "shortCiteRegEx": "Reunanen.", "year": 2003}, {"title": "On the dangers of cross-validation. an experimental evaluation", "author": ["R. Bharat Rao", "Glenn Fung"], "venue": "In International Conference on Data Mining,", "citeRegEx": "Rao and Fung.,? \\Q2008\\E", "shortCiteRegEx": "Rao and Fung.", "year": 2008}, {"title": "Understanding Machine Learning: From Theory to Algorithms", "author": ["Shai Shalev-Shwartz", "Shai Ben-David"], "venue": null, "citeRegEx": "Shalev.Shwartz and Ben.David.,? \\Q2014\\E", "shortCiteRegEx": "Shalev.Shwartz and Ben.David.", "year": 2014}, {"title": "Learnability, stability and uniform convergence", "author": ["Shai Shalev-Shwartz", "Ohad Shamir", "Nathan Srebro", "Karthik Sridharan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2010}, {"title": "Interactive fingerprinting codes and the hardness of preventing false discovery", "author": ["Thomas Steinke", "Jonathan Ullman"], "venue": "arXiv preprint arXiv:1410.1228,", "citeRegEx": "Steinke and Ullman.,? \\Q2014\\E", "shortCiteRegEx": "Steinke and Ullman.", "year": 2014}, {"title": "Learning with differential privacy: Stability, learnability and the sufficiency and necessity of ERM principle", "author": ["Yu-Xiang Wang", "Jing Lei", "Stephen E. Fienberg"], "venue": "CoRR, abs/1502.06309,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}], "referenceMentions": [], "year": 2015, "abstractText": "Overfitting is the bane of data analysts, even when data are plentiful. Formal approaches to understanding this problem focus on statistical inference and generalization of individual analysis procedures. Yet the practice of data analysis is an inherently interactive and adaptive process: new analyses and hypotheses are proposed after seeing the results of previous ones, parameters are tuned on the basis of obtained results, and datasets are shared and reused. An investigation of this gap has recently been initiated by the authors in [DFH14], where we focused on the problem of estimating expectations of adaptively chosen functions. In this paper, we give a simple and practical method for reusing a holdout (or testing) set to validate the accuracy of hypotheses produced by a learning algorithm operating on a training set. Reusing a holdout set adaptively multiple times can easily lead to overfitting to the holdout set itself. We give an algorithm that enables the validation of a large number of adaptively chosen hypotheses, while provably avoiding overfitting. We illustrate the advantages of our algorithm over the standard use of the holdout set via a simple synthetic experiment. We also formalize and address the general problem of data reuse in adaptive data analysis. We show how the differential-privacy based approach given in [DFH14] is applicable much more broadly to adaptive data analysis. We then show that a simple approach based on description length can also be used to give guarantees of statistical validity in adaptive settings. Finally, we demonstrate that these incomparable approaches can be unified via the notion of approximate max-information that we introduce. This, in particular, allows the preservation of statistical validity guarantees even when an analyst adaptively composes algorithms which have guarantees based on either of the two approaches.", "creator": "LaTeX with hyperref package"}}}