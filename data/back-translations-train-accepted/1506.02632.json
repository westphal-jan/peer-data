{"id": "1506.02632", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2015", "title": "Cumulative Prospect Theory Meets Reinforcement Learning: Prediction and Control", "abstract": "Cumulative prospect theory (CPT) is known to model human decisions well, with substantial empirical evidence supporting this claim. CPT works by distorting probabilities and is more general than the classic expected utility and coherent risk measures. We bring this idea to a risk-sensitive reinforcement learning (RL) setting and design algorithms for both estimation and control. The estimation scheme that we propose uses the empirical distribution in order to estimate the CPT-value of a random variable. We then use this scheme in the inner loop of policy optimization procedures for a Markov decision process (MDP). We propose both gradient-based as well as gradient-free policy optimization algorithms. The former includes both first-order and second-order methods that are based on the well-known simulation optimization idea of simultaneous perturbation stochastic approximation (SPSA), while the latter is based on a reference distribution that concentrates on the global optima. Using an empirical distribution over the policy space in conjunction with Kullback-Leibler (KL) divergence to the reference distribution, we get a global policy optimization scheme. We provide theoretical convergence guarantees for all the proposed algorithms.", "histories": [["v1", "Mon, 8 Jun 2015 19:37:55 GMT  (40kb)", "http://arxiv.org/abs/1506.02632v1", null], ["v2", "Sun, 20 Sep 2015 04:19:53 GMT  (46kb,D)", "http://arxiv.org/abs/1506.02632v2", null], ["v3", "Fri, 26 Feb 2016 21:30:04 GMT  (79kb,D)", "http://arxiv.org/abs/1506.02632v3", null]], "reviews": [], "SUBJECTS": "cs.LG math.OC", "authors": ["prashanth l a", "cheng jie", "michael c fu", "steven i marcus", "csaba szepesv\u00e1ri"], "accepted": true, "id": "1506.02632"}, "pdf": {"name": "1506.02632.pdf", "metadata": {"source": "CRF", "title": "Cumulative Prospect Theory Meets Reinforcement Learning: Estimation and Control", "authors": ["Prashanth L.A", "Jie Cheng", "Michael Fu", "Steve Marcus"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 150 6.02 632v 1 [cs.L G"}, {"heading": "1 Introduction", "text": "In contrast to a random variable X, let Pi, i = 1,.. denote K the probability of suffering a gain / loss xi, i = 1,.., K. Given a utility function u and a weighting function w, the value of the prospectus theory (PT) is defined as V (X) = \u2211 K i = 1 u (xi) w (pi). The idea is to take a utility function shaped to satisfy the decreasing sensitivity characteristic S \u2212 \u2212 \u2212. If we take the weighting function w as identity, then the classic expected benefit is restored. A general weight function inflates low probabilities and attenuates high probabilities, and this is close to the way humans make decisions (see Kahneman and Tversky (1979), Fennema and Wakker (1997) for justification, especially by means of human experimental tests."}, {"heading": "CPT-value estimation.", "text": "Against this background, the first objective of this paper is to develop a scheme for estimating the CPT value, using only random samples of the random variable X. In particular, we want to estimate the following equivalent form of the CPT value: V (X) = 1 + 1 + 0 w + (P (u + (X) > x) dx \u2212 0 w \u2212 (P (u \u2212 (X) > x) dx. (1) We derive an estimate of the first integral as follows: First, we calculate the empirical distribution function for u + (X), then combine it with the weight function w + and finally integrate the resulting composition to obtain the final estimate; the second integral in (1) is similarly estimated and the CPT value is the difference between the estimates of the two integrals in (1). Assuming that the weight functions are lipschitz, we establish convergence (Cymptotic value of our PT value) is also a high probability that the PT value is complete."}, {"heading": "Optimizing CPT-value in MDPs.", "text": "In this context, it should be noted that this is a case of an injury to a person who was unable to escape to safety."}, {"heading": "Related work", "text": "In contrast to earlier approaches to economic policy (Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)), however, previous work considers either an exponential benefit formula (cf. Borkar (2010), which implicitly controls the variance or a restricted formulation with explicit limitations on the variance of cost-to-go (cf. Tamar et al.); Prashanth and Ghavamzadeh (2013). Another limitation of the alternatives is to bind the CVaR while minimizing the usual cost targets (cf. Borkar and Jain); Prashanth and we the risk metric we apply is inspired by CPT, and this measure is not coherent and non-convex."}, {"heading": "2 CPT-value estimation", "text": "In traditional environments, one tries to estimate an expected value by taking samples from the distribution according to (1) from which the expectation is taken. However, in our environment, one obtains samples of the underlying random variables X using their distribution, but the CPT value integral in (1) distorts this distribution using a nonlinear weight function w. Therefore, one cannot apply in our environment classical stochastic approximation schemes Robbins and Monro (1951). Previous work on risk-sensitive RL (cf. Borkar (2010), Tamar and Mannor (2013), Prashanth and Ghavamzadeh (2013) included the estimation of the value function using some form of time differential learning, which is a stochastic approximation version of a fixed point algorithm. Since the integral in (1) requires the CDF estimation (across the entire domain), our approach is to perform the empirical function of distributive elasticity (EDF) and subsequent integration (EDF) in CDF and CDF."}, {"heading": "2.1 Basic algorithm", "text": "Let us leave Xi, i = 1,.., n for n samples of the random variable X. With conventional notation, we define the empirical distribution function (EDF) for u + (X) and u \u2212 (X) for any real weighted functions u + and u \u2212 as follows: F + n (x) = 1nn \u2211 i = 11 (u + (Xi) \u2264 x) and F + n (x) = 1nn \u2211 i = 11 (u \u2212 (Xi) \u2264 x). Using EDFs, the CPT value (1) is estimated as follows: V + n (X) = 0 w + (1 \u2212 F + n (x)) dx \u2212 0 w \u2212 (1 \u2212 F \u2212 n (x) dx. (2) Note that we have replaced 1 \u2212 F + n (x) (or 1 \u2212 F + n (x))) for P (u \u2212 F \u2212 n (x) > x (resp) and then the function DF () (compressed)."}, {"heading": "2.2 Main results", "text": "To overcome this difficulty, we start from the following assumption: Assumption (A1). The weighting functions w +, w \u2212 are Lipschitz with a common constant L. The above assumption covers a broad class of functions that occur in practice. The following result shows that the estimate (2) almost certainly corresponds to the actual CPT value and to the (almost) canonical Monte Carlo asymptotic value. Assumption 1. (Asymptotic convergence and rate.) Assumption 2. (Asymptotic convergence and rate.) Assumption 2. (Asymptotic convergence and rate.) Assumption 3. (Asymptotic convergence and rate.) 1. (Asymptotic convergence and rate.) 3. (Asymptotic convergence and rate.)"}, {"heading": "3 CPT-value objective for MDPs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Setting", "text": "We consider a stochastic problem of the shortest path (SSP) with state space X = {0, 1,.., l}, where 0 denotes the absorbing terminating state. In each x-S, A (x) denotes the set of actions in state X. Let g (x, a) denote the one-step costs incurred by selecting act a in state s. A stationary randomised policy \u03c0 represents the probability distributions of actions. We parameterise the guidelines and assume that each policy (identified by its parameter \u03b8) is consistently differentiable within this defined framework of state 3. Furthermore, we proceed from the standard assumption that all actions in the parameterised class that we deem correct, i.e. there is a positive probability that the terminal state 0 is reached by each state x-X. In other words, an appropriate policy makes state 0 recurrent and the rest of states transitory for the underlying Markov chain."}, {"heading": "3.2 CPT-value objective", "text": "An episode is a simulated sample SSP path starting in state x0 and ending in recurring state 0. Let D\u03c0 (x0) be a random variable (r.v) that indicates the total cost of an episode simulated with the help of politics \u03c0 (\u00b7, sm), i.e., D\u03c0 (x0) = \u03a3 m = 0g (sm, am), 3In this essay, we use \u03c0 and \u03b8 interchangeable to denote a policy. In this essay, we apply the CPT approach and aim to minimize the CPT value function, i.e. the measures are selected using politics \u03c0 and it is the first time that we minimize the expected value of the total costs defined above."}, {"heading": "4 Gradient-based algorithms for optimizing CPT-value", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Policy gradient algorithm (PG-CPT-SPSA)", "text": "Update Rule: We update the political parameter in descending order as follows: \u03b8n + 1 = \u03b8n \u2212 an incremental (x0), (5) where incremental increment of the CPT value function (4) is an estimate of the increment of the CPT value function (6) chosen to satisfy (6) below. Figure 1 illustrates the general flow of the political increment algorithm based on SPSA, while algorithm 1 represents the pseudo code."}, {"heading": "Gradient estimation", "text": "In view of the fact that we operate in a learning environment and only have biased estimates of the CPT value of (2), we need a simulation optimization scheme that results in (1), (2), (2), (2), (2), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), \"(5),\" (5), \"(5),\" (5, \"(5),\" (5), \"(5),\" (5), \"(5),\" (5), (5), (5), (5), (5, (5), (5), (5, (5), (5), (5, (5), (5), (5), (5), (5, (5), (5), (5, (5), (5), (5, (5), (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5), (5), (5), (5), ("}, {"heading": "Convergence result", "text": "Theorem 1. (Strong Convergence) Let us suppose (A1) - (A3). Let us almost certainly include phenomena 4 and let us create an asymptotically stable equilibrium of the conventional differential equation (ODE): \u03b8-t = \u2212 VP-V phenomenon (x0), with the range of attraction D (\u03b8). Then, if there is a compact subset D of D (\u03b8), so that phenomena visit D endlessly, we have phenomena like phenomena like n \u2192 \u0430 a.s. as phenomena. See section 6.2."}, {"heading": "4.2 Policy Newton algorithm (PN-CPT-SPSA)", "text": "The idea is to use larger step sizes (1 / 2, 1), and then combine it with averaging."}, {"heading": "Gradient and Hessian estimation", "text": "We estimate the Hessian CPT value function on the basis of the scheme proposed by Bhatnagar and Prashanth (2015). As in the case of the first-order method, we use Rademacher random variables to disrupt all coordinates at the same time. In this case, however, we need three system sequences with corresponding political parameters, namely n + n (n + n), n \u2212 n (n + n) and n \u2212 n, whereby {in, in,. in, i = 1,.., d, n = 1, 2,..} are i.i.d. Rademacher random variables. On the basis of the CPT value estimates for the above-mentioned political parameters, we estimate the Hessian value and the gradient of the CPT value as follows: For i, j = 1,., V \u2212 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n (n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n), the Hessian value (n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n) becomes (n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n (n \u00b2 n \u00b2 n \u00b2 n \u00b2 n)."}, {"heading": "Convergence result", "text": "Theorem 2. (Strong Convergence) Let's say (A1) - (A3). Let's almost certainly assume that one,,, two (6) and also one (n + 1) 2\u03b42n <. Further, let's assume that (i), three (n), three (n), four (n), four (0) and five (1) exist independently, so that (n), four (Hn), five (Hn), five (Hn), six (Hn), eight (Hn), eight (Hn), eight (Hn), eight (Hn), eight (Hn), eight (Hn), eight (Hn), eight (Hn), eight (Hn) and eight (Hn)."}, {"heading": "5 Gradient-free algorithm for optimizing CPT-value", "text": "We perform a non-trivial adjustment of the algorithm of Chang et al. (2013) to our adjustment of the optimization of the CPT value in MDPs. We need there to be a unique global optimal distribution for the problem. (2013) To illustrate the main idea in the algorithm, we assume that we know the shape of the V-value (x0). (2013) The idea is to generate a sequence of reference distributions (x0) gk \u2212 1) in the political space, so that it ultimately focuses on the global optimum. An easy way, which in Chapter 4 of Chang et al. (2013) isgk (2013) isgk (H-value) gk \u2212 1 (V-0) gk \u2212 H (x0) gk \u2212 1 (x0) gk \u2212 1 (x0) gk \u2212 1 (x0) gk \u2212 1 (x0) gk \u2212 1 (x0)."}, {"heading": "Convergence result", "text": "Theorem 3. Assumption (A1) - (A2) - (A2). Suppose multivariate normal densities are used for sample distribution, i.e., \"n\" = (A1), where \"n\" and \"n\" denote the mean and covariance of the normal densities. (Then, \"lim n\" denotes the \"n\" values and \"limn\" denotes the \"n\" values. (13) See Section 6.4.1Here V \"n\" n \"(x 0) denotes the\" n \"values of the statistical order. (2) Here I\" (z), \"n\" values: \"n,\" (z. \"\u2212 n) See Section 6.4.1Here V\" n \"(x) n\" n \"numbers:\" n. \""}, {"heading": "6 Convergence Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Proofs for CPT-value estimator", "text": "To prove theorem 1, we need the dominated convergence theorem in its generalized form, which is set out below. Theorem 4. (General theorem on dominated convergence) Let {fn} \u221e n = 1 be a sequence of measurable functions to E converging pointwise a.e. in a measurable space from E to f. Suppose there is a sequence of integrable functions to E converging pointwise a.e. to E to g, so that | fn | \u2264 gn converge for all n-N."}, {"heading": "Proof of Proposition 1: Asymptotic convergence", "text": "For notational convenience we will henceforth use u + (X) and u \u2212 (X) of (U + >) (U + > x) of (U + > x (U + > x) of (U + > x) of (U + > x) of (U + > x) of (U + > x) of (P \u2212 x) of (P \u2212 x) of (X \u2212 x) of (X) of empirical distribution as follows: V (X) of (U + > x) of (U + > x) of (P + > x) of (P \u2212 x) of empirical distribution as follows: V (X) of (X) of (X + (U + > x) of (F + (1 \u2212 F) of (X) of (X) of (X + (X) of) of (X) of (X)."}, {"heading": "Proof of Proposition 2", "text": "For the proof of Proposition 2, we need the following known inequality, which provides a finite time span between empirical distribution and true distribution. \u2212 \u2212 pnp \u2212 pnp \u2212 pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; np; np; np; np; np; np; np; np; np; np; np; pnp; np; np; pnp; pp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp; pnp;"}, {"heading": "6.2 Proofs for PG-CPT-SPSA", "text": "To prove the main result in Theorem 1, we first show that the gradient estimation using SPSA is only an order O (2) (2) that deviates from the true gradient (2). The following problem makes this claim and its proof can be derived from Spall (1992), although we are here for completeness. (2) Following this problem, we complete the proof of Theorem 1 by invoking the well-known Kushner-Clark problem Kushner and Clark (1978), and this includes the verification conditions A2.2.1-A2.4 there.Lemma 7. Let Fn = 1), m \u00b2 n, m \u00b2 n < n), n \u00b2 n \u00b2 n \u00b2 n \u00b2 n), n \u00b2 n \u00b2 n \u00b2 n. Then we almost certainly have the Veri.V (2), that we have the Veri.V (2) -V (2), \u2212 V \u00b2 -V \u00b2), that we have the Veri.V (iV)"}, {"heading": "Proof of Theorem 1", "text": "The proof that the update rule (5) can be rewritten as follows: \"We are satisfied.\" \"We are satisfied.\" \"We are satisfied.\" \"We are satisfied.\" \"We are satisfied.\" \"We are satisfied.\" \"We are satisfied.\" \"We are satisfied.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"\" We. \"\". \"\" \"We.\". \"\" \"\" We. \"\" \"\" \"We.\" \"\" \"We.\" \"\".... \"\" \"We.\" \"\"........... \"\" We. \"..........\" \"We.\""}, {"heading": "6.3 Proofs for PN-CPT-SPSA", "text": "Before we prove Theorem 2, we have the bias in the SPSA-based estimate of the Hessian in the following lexicon 8. Let Fn = Biased (Biased) + Biased (Biased) + Biased (Biased) + Biased (Biased) n (Biased) n (Biased) n (Biased) n (Biased) n (Biased) n (Biased) n (Biased) n (Biased + Biased) n (Biased) n (Biased + Biased) n (Biased) n (Biased) n (Biased) n (Biased) n (Biased) n (Biased) n (Biased + Biased) n (Biased + Biased) n (Biased + Biased) + Biased (Biased) + (Biased) + (Biased) (Biased +) (Biased) (Biased) n (Biased) n (Biased) n (Biased) n (Biased) + Biased (Biased +) (Biased (Biased) + (Biased)"}, {"heading": "Proof of Theorem 2", "text": "Evidence. The assertion, which refers to the convergence of the Laha and Rohatgi phenomena (1979) to phenomena 1a of Spall (2000), can be substantiated in a manner similar to theorem 1a of Spall (2000), after it was established that term 9 implies that the SPSA-based gradient estimate in (7) is only an expression of the order O (\u03b42n) that deviates from the true gradient. To prove the assertion in connection with the convergence of Hessian recursion (8), we use the evidential technique of Spall (2000). The proof goes as follows: Let us leave Wm = H, m \u2212 E [H, m, m, m, m, m, m, m, m, m, p."}, {"heading": "6.4 Proofs for gradient-free policy optimization algorithm", "text": "We begin by noting that there is a crucial difference between our algorithms and MRAS2. (2013): MRAS2 has an expected functional value, i.e., it aims to minimize a function by applying random observations showing a zero mean noise. (2013): The goal in our environment is the CPT value, which distorts the underlying transition probabilities. The implication is that MRAS2 can estimate the expected value from sample averages, while we have to resort to integrating the empirical distribution. As we obtain samples of the objective (CPT) in a way that differs from MRAS2, we have to find that the thresholds in Algorithm 3 achieve the same effect as in MRAS2. This is achieved by the following explanation, which is a variant of Lemma 4.13 from Chang et al. (2013), adapted to our settings."}, {"heading": "Proof of Theorem 3", "text": "Evidence. Once we have established Lemma 10, the rest of the evidence follows in the same way as the evidence for the concomitant phenomenon 4.18 of Chang et al. (2013). This is because our algorithm works similarly to MRAS2 w.r.t. by generating the candidate solution using a parameterized family f (\u00b7, \u03b7) and updating the distribution parameter \u03b7. The difference, as already mentioned, is in the way the samples are generated and the objective (CPT value) function is estimated. The problem mentioned above stated that the elite sample and threshold achieve the same effect as MRAS2 and therefore the rest of the evidence follows from Chang et al. (2013)."}, {"heading": "7 Numerical Experiments for CPT-value estimation scheme", "text": "Setup. We look at a random variable X that is evenly distributed in [0, 5]. We set the utility function as identity and define the weight function w as follows: w (x) = {2 \u2212 3 (2x \u2212 x2) 0 \u2264 x < 12 1 3 + 2 3x 2 (2 \u2264 x \u2264 1The graph of w (x) can be seen in Figure 2. \u2212 Thus the CPT value of X is to beV (X) = 0.0 w (P (X > x)))))) dx, (34) Since we only look at wins, the second integral component in V (X) is of (1) zero.Background. Let us leave X1,. \u2212 Xn become generally random variables with underlying distribution U [0, 5]. Then the empirical distribution function is defined as F (x)."}, {"heading": "8 Conclusions and Future Work", "text": "We looked at the problem of optimizing the CPT value of an MDP. To this end, we first designed an estimation scheme that evaluates the CPT value of a given policy. Next, using this estimator as an inner loop, we proposed three policy optimization algorithms: the first two algorithms use SPSA to estimate the gradient and / or the Hessian of the CPT value function, and then perform a descent in the political parameter, while the third algorithm is gradient-free and based on a reference distribution focused on the global optimum. Using an empirical distribution across the political space in conjunction with the KL divergence to the reference distribution, we obtain a global political optimization scheme. We provided theoretical convergence guarantees for all the proposed algorithms. In particular, we first showed that the CPT value estimator converges asymptotically and at the optimal interest rate."}], "references": [{"title": "Measure theory and probability theory", "author": ["K.B. Athreya", "S.N. Lahiri"], "venue": "Springer Science & Business Media,", "citeRegEx": "Athreya and Lahiri.,? \\Q2006\\E", "shortCiteRegEx": "Athreya and Lahiri.", "year": 2006}, {"title": "Abstract Dynamic Programming", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "Bertsekas.,? \\Q2013\\E", "shortCiteRegEx": "Bertsekas.", "year": 2013}, {"title": "Simultaneous perturbation Newton algorithms for simulation optimization", "author": ["S. Bhatnagar", "L.A. Prashanth"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "Bhatnagar and Prashanth.,? \\Q2015\\E", "shortCiteRegEx": "Bhatnagar and Prashanth.", "year": 2015}, {"title": "Stochastic Recursive Algorithms for Optimization, volume 434", "author": ["S. Bhatnagar", "H.L. Prasad", "L.A. Prashanth"], "venue": null, "citeRegEx": "Bhatnagar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bhatnagar et al\\.", "year": 2013}, {"title": "Stochastic Approximation: A Dynamical Systems Viewpoint", "author": ["V. Borkar"], "venue": null, "citeRegEx": "Borkar.,? \\Q2008\\E", "shortCiteRegEx": "Borkar.", "year": 2008}, {"title": "Learning algorithms for risk-sensitive control", "author": ["V. Borkar"], "venue": "In Proceedings of the 19th International Symposium on Mathematical Theory of Networks and Systems\u2013MTNS,", "citeRegEx": "Borkar.,? \\Q2010\\E", "shortCiteRegEx": "Borkar.", "year": 2010}, {"title": "Risk-constrained Markov decision processes", "author": ["V. Borkar", "R. Jain"], "venue": "In IEEE Conference on Decision and Control (CDC),", "citeRegEx": "Borkar and Jain.,? \\Q2010\\E", "shortCiteRegEx": "Borkar and Jain.", "year": 2010}, {"title": "Simulation-based Algorithms for Markov Decision Processes", "author": ["H.S. Chang", "J. Hu", "M.C. Fu", "S.I. Marcus"], "venue": null, "citeRegEx": "Chang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2013}, {"title": "Optimal rates for zero-order convex optimization: the power of two function evaluations", "author": ["J.C. Duchi", "M.I. Jordan", "M.J. Wainwright", "A. Wibisono"], "venue": "arXiv preprint arXiv:1312.2139,", "citeRegEx": "Duchi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2013}, {"title": "Transport-entropy inequalities and deviation estimates for stochastic approximation schemes", "author": ["M. Fathi", "N. Frikha"], "venue": "Electron. J. Probab,", "citeRegEx": "Fathi and Frikha.,? \\Q2013\\E", "shortCiteRegEx": "Fathi and Frikha.", "year": 2013}, {"title": "Original and cumulative prospect theory: A discussion of empirical differences", "author": ["H. Fennema", "P. Wakker"], "venue": "Journal of Behavioral Decision Making,", "citeRegEx": "Fennema and Wakker.,? \\Q1997\\E", "shortCiteRegEx": "Fennema and Wakker.", "year": 1997}, {"title": "Online convex optimization in the bandit setting: gradient descent without a gradient", "author": ["A.D. Flaxman", "A.T. Kalai", "H.B. McMahan"], "venue": "In SODA,", "citeRegEx": "Flaxman et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Flaxman et al\\.", "year": 2005}, {"title": "Practical Optimization", "author": ["P.E. Gill", "W. Murray", "M.H. Wright"], "venue": null, "citeRegEx": "Gill et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Gill et al\\.", "year": 1981}, {"title": "Online Convex Optimization", "author": ["E. Hazan"], "venue": null, "citeRegEx": "Hazan.,? \\Q2015\\E", "shortCiteRegEx": "Hazan.", "year": 2015}, {"title": "Prospect theory: An analysis of decision under risk", "author": ["D. Kahneman", "A. Tversky"], "venue": "Econometrica: Journal of the Econometric Society,", "citeRegEx": "Kahneman and Tversky.,? \\Q1979\\E", "shortCiteRegEx": "Kahneman and Tversky.", "year": 1979}, {"title": "Stochastic Approximation Methods for Constrained and Unconstrained Systems", "author": ["H. Kushner", "D. Clark"], "venue": null, "citeRegEx": "Kushner and Clark.,? \\Q1978\\E", "shortCiteRegEx": "Kushner and Clark.", "year": 1978}, {"title": "Stochastic Systems with Cumulative Prospect Theory", "author": ["K. Lin"], "venue": "Ph.D. Thesis,", "citeRegEx": "Lin.,? \\Q2013\\E", "shortCiteRegEx": "Lin.", "year": 2013}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["B.T. Polyak", "A.B. Juditsky"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "Polyak and Juditsky.,? \\Q1992\\E", "shortCiteRegEx": "Polyak and Juditsky.", "year": 1992}, {"title": "Policy Gradients for CVaR-Constrained MDPs. In Algorithmic Learning Theory, pages 155\u2013169", "author": ["L.A. Prashanth"], "venue": null, "citeRegEx": "Prashanth.,? \\Q2014\\E", "shortCiteRegEx": "Prashanth.", "year": 2014}, {"title": "Actor-critic algorithms for risk-sensitive MDPs", "author": ["L.A. Prashanth", "M. Ghavamzadeh"], "venue": "In Proceedings of Advances in Neural Information Processing Systems", "citeRegEx": "Prashanth and Ghavamzadeh.,? \\Q2013\\E", "shortCiteRegEx": "Prashanth and Ghavamzadeh.", "year": 2013}, {"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Robbins and Monro.,? \\Q1951\\E", "shortCiteRegEx": "Robbins and Monro.", "year": 1951}, {"title": "Optimization of conditional value-at-risk", "author": ["R.T. Rockafellar", "S. Uryasev"], "venue": "Journal of risk,", "citeRegEx": "Rockafellar and Uryasev.,? \\Q2000\\E", "shortCiteRegEx": "Rockafellar and Uryasev.", "year": 2000}, {"title": "Stochastic approximation. Handbook of Sequential Analysis, pages 503\u2013529", "author": ["D. Ruppert"], "venue": null, "citeRegEx": "Ruppert.,? \\Q1991\\E", "shortCiteRegEx": "Ruppert.", "year": 1991}, {"title": "Multivariate stochastic approximation using a simultaneous perturbation gradient approximation", "author": ["J.C. Spall"], "venue": "IEEE Trans. Auto. Cont.,", "citeRegEx": "Spall.,? \\Q1992\\E", "shortCiteRegEx": "Spall.", "year": 1992}, {"title": "Adaptive stochastic approximation by the simultaneous perturbation method", "author": ["J.C. Spall"], "venue": "IEEE Trans. Autom. Contr.,", "citeRegEx": "Spall.,? \\Q2000\\E", "shortCiteRegEx": "Spall.", "year": 2000}, {"title": "Introduction to Stochastic Search and Optimization: Estimation, Simulation, and Control, volume 65", "author": ["J.C. Spall"], "venue": null, "citeRegEx": "Spall.,? \\Q2005\\E", "shortCiteRegEx": "Spall.", "year": 2005}, {"title": "Learning to predict by the methods of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine Learning,", "citeRegEx": "Sutton.,? \\Q1988\\E", "shortCiteRegEx": "Sutton.", "year": 1988}, {"title": "Variance adjusted actor-critic algorithms", "author": ["A. Tamar", "S. Mannor"], "venue": "arXiv preprint arXiv:1310.3697,", "citeRegEx": "Tamar and Mannor.,? \\Q2013\\E", "shortCiteRegEx": "Tamar and Mannor.", "year": 2013}, {"title": "Policy gradients with variance related risk criteria", "author": ["A. Tamar", "D.D. Castro", "S. Mannor"], "venue": "In Proceedings of the Twenty-Ninth International Conference on Machine Learning,", "citeRegEx": "Tamar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tamar et al\\.", "year": 2012}, {"title": "Advances in prospect theory: Cumulative representation of uncertainty", "author": ["A. Tversky", "D. Kahneman"], "venue": "Journal of Risk and Uncertainty,", "citeRegEx": "Tversky and Kahneman.,? \\Q1992\\E", "shortCiteRegEx": "Tversky and Kahneman.", "year": 1992}], "referenceMentions": [{"referenceID": 13, "context": "A general weight function inflates low probabilities and deflates high probabilities and this has been shown to be close to the way humans make decisions (see Kahneman and Tversky (1979), Fennema and Wakker (1997) for a justification, in particular via empirical tests using human subjects).", "startOffset": 159, "endOffset": 187}, {"referenceID": 10, "context": "A general weight function inflates low probabilities and deflates high probabilities and this has been shown to be close to the way humans make decisions (see Kahneman and Tversky (1979), Fennema and Wakker (1997) for a justification, in particular via empirical tests using human subjects).", "startOffset": 188, "endOffset": 214}, {"referenceID": 10, "context": "A general weight function inflates low probabilities and deflates high probabilities and this has been shown to be close to the way humans make decisions (see Kahneman and Tversky (1979), Fennema and Wakker (1997) for a justification, in particular via empirical tests using human subjects). However, PT is lacking in some theoretical aspects as it violates first-order stochastic dominance1. Cumulative prospect theory (CPT) (Tversky and Kahneman 1992) uses a similar measure as PT, except that the weights are a function of cumulative probabilities. First, separate the gains and losses as Consider the following example from Fennema and Wakker (1997): Suppose there are 20 prospects (outcomes) ranging from \u221210 to 180, each with probability 0.", "startOffset": 188, "endOffset": 654}, {"referenceID": 7, "context": "Gradient-free method: We perform a non-trivial adaptation of the algorithm from Chang et al. (2013) to devise a globally optimizing policy update scheme.", "startOffset": 80, "endOffset": 100}, {"referenceID": 7, "context": "Gradient-free method: We perform a non-trivial adaptation of the algorithm from Chang et al. (2013) to devise a globally optimizing policy update scheme. The idea is to use a reference model that eventually concentrates on the global minimum and then empirically approximate this reference distribution well-enough. The latter is achieved via natural exponential families in conjunction with KullbackLeibler (KL) divergence to measure the \u201cdistance\u201d from the reference distribution. Unlike the setting of Chang et al. (2013), we neither observe the objective function (CPT-value) perfectly nor with zero-mean noise.", "startOffset": 80, "endOffset": 525}, {"referenceID": 3, "context": "Borkar (2010); Borkar and Jain (2010); Tamar et al.", "startOffset": 0, "endOffset": 14}, {"referenceID": 3, "context": "Borkar (2010); Borkar and Jain (2010); Tamar et al.", "startOffset": 0, "endOffset": 38}, {"referenceID": 3, "context": "Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)).", "startOffset": 0, "endOffset": 59}, {"referenceID": 3, "context": "Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)).", "startOffset": 0, "endOffset": 93}, {"referenceID": 3, "context": "Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). However, previous works consider either an exponential utility formulation (cf. Borkar (2010)) that implicitly controls the variance or a constrained formulation with explicit constraints on the variance of the cost-to-go (cf.", "startOffset": 0, "endOffset": 189}, {"referenceID": 3, "context": "Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). However, previous works consider either an exponential utility formulation (cf. Borkar (2010)) that implicitly controls the variance or a constrained formulation with explicit constraints on the variance of the cost-to-go (cf. Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)).", "startOffset": 0, "endOffset": 342}, {"referenceID": 3, "context": "Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). However, previous works consider either an exponential utility formulation (cf. Borkar (2010)) that implicitly controls the variance or a constrained formulation with explicit constraints on the variance of the cost-to-go (cf. Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)).", "startOffset": 0, "endOffset": 376}, {"referenceID": 3, "context": "Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). However, previous works consider either an exponential utility formulation (cf. Borkar (2010)) that implicitly controls the variance or a constrained formulation with explicit constraints on the variance of the cost-to-go (cf. Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). Another constraint alternative is to bound the CVaR, while minimizing the usual cost objective (cf. Borkar and Jain (2010); Prashanth (2014)).", "startOffset": 0, "endOffset": 501}, {"referenceID": 3, "context": "Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). However, previous works consider either an exponential utility formulation (cf. Borkar (2010)) that implicitly controls the variance or a constrained formulation with explicit constraints on the variance of the cost-to-go (cf. Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). Another constraint alternative is to bound the CVaR, while minimizing the usual cost objective (cf. Borkar and Jain (2010); Prashanth (2014)).", "startOffset": 0, "endOffset": 519}, {"referenceID": 3, "context": "Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). However, previous works consider either an exponential utility formulation (cf. Borkar (2010)) that implicitly controls the variance or a constrained formulation with explicit constraints on the variance of the cost-to-go (cf. Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). Another constraint alternative is to bound the CVaR, while minimizing the usual cost objective (cf. Borkar and Jain (2010); Prashanth (2014)). However, the risk measure we adopt is inspired by CPT, and this measure is both non-coherent and non-convex and hence, departs from the approach used in aforementioned references. For instance, expected utility and variance constraint approaches used some form of temporal difference learning Sutton (1988) for policy evaluation, while CVaR-based formulation could perform gradient descent since there was a well-known convex optimization formulation for CVaR Rockafellar and Uryasev (2000).", "startOffset": 0, "endOffset": 828}, {"referenceID": 3, "context": "Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). However, previous works consider either an exponential utility formulation (cf. Borkar (2010)) that implicitly controls the variance or a constrained formulation with explicit constraints on the variance of the cost-to-go (cf. Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). Another constraint alternative is to bound the CVaR, while minimizing the usual cost objective (cf. Borkar and Jain (2010); Prashanth (2014)). However, the risk measure we adopt is inspired by CPT, and this measure is both non-coherent and non-convex and hence, departs from the approach used in aforementioned references. For instance, expected utility and variance constraint approaches used some form of temporal difference learning Sutton (1988) for policy evaluation, while CVaR-based formulation could perform gradient descent since there was a well-known convex optimization formulation for CVaR Rockafellar and Uryasev (2000). On the other hand, the CPT-value in (1) does not lend itself to stochastic approximation-based estimation schemes since the underlying probabilities are distorted via a weighting function.", "startOffset": 0, "endOffset": 1012}, {"referenceID": 3, "context": "Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). However, previous works consider either an exponential utility formulation (cf. Borkar (2010)) that implicitly controls the variance or a constrained formulation with explicit constraints on the variance of the cost-to-go (cf. Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). Another constraint alternative is to bound the CVaR, while minimizing the usual cost objective (cf. Borkar and Jain (2010); Prashanth (2014)). However, the risk measure we adopt is inspired by CPT, and this measure is both non-coherent and non-convex and hence, departs from the approach used in aforementioned references. For instance, expected utility and variance constraint approaches used some form of temporal difference learning Sutton (1988) for policy evaluation, while CVaR-based formulation could perform gradient descent since there was a well-known convex optimization formulation for CVaR Rockafellar and Uryasev (2000). On the other hand, the CPT-value in (1) does not lend itself to stochastic approximation-based estimation schemes since the underlying probabilities are distorted via a weighting function. Unlike previous applications of SPSA and the algorithm from Chang et al. (2013), our he policy optimization algorithms suffer from biased estimates from the policy evaluation procedure, where the bias is non-zero and bounded.", "startOffset": 0, "endOffset": 1282}, {"referenceID": 3, "context": "Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). However, previous works consider either an exponential utility formulation (cf. Borkar (2010)) that implicitly controls the variance or a constrained formulation with explicit constraints on the variance of the cost-to-go (cf. Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). Another constraint alternative is to bound the CVaR, while minimizing the usual cost objective (cf. Borkar and Jain (2010); Prashanth (2014)). However, the risk measure we adopt is inspired by CPT, and this measure is both non-coherent and non-convex and hence, departs from the approach used in aforementioned references. For instance, expected utility and variance constraint approaches used some form of temporal difference learning Sutton (1988) for policy evaluation, while CVaR-based formulation could perform gradient descent since there was a well-known convex optimization formulation for CVaR Rockafellar and Uryasev (2000). On the other hand, the CPT-value in (1) does not lend itself to stochastic approximation-based estimation schemes since the underlying probabilities are distorted via a weighting function. Unlike previous applications of SPSA and the algorithm from Chang et al. (2013), our he policy optimization algorithms suffer from biased estimates from the policy evaluation procedure, where the bias is non-zero and bounded. We overcome the bias asymptotically by slowly increasing the number of samples and show that the resulting policy optimization algorithms converge. The closest related work is Lin (2013), where the authors propose a CPT-measure for an abstract MDP setting (see Bertsekas (2013)).", "startOffset": 0, "endOffset": 1615}, {"referenceID": 1, "context": "The closest related work is Lin (2013), where the authors propose a CPT-measure for an abstract MDP setting (see Bertsekas (2013)).", "startOffset": 113, "endOffset": 130}, {"referenceID": 1, "context": "The closest related work is Lin (2013), where the authors propose a CPT-measure for an abstract MDP setting (see Bertsekas (2013)). While the CPT-value (1) that we aim to optimize is based on that in Lin (2013), we extend the latter work in several ways:", "startOffset": 113, "endOffset": 211}, {"referenceID": 16, "context": "(i) Unlike Lin (2013), we do not assume model information and develop an estimation scheme for the CPT-value function; (ii) Further, we also propose control algorithms using SPSA and model-based policy search in order to find a policy that optimizes the CPT-value.", "startOffset": 11, "endOffset": 22}, {"referenceID": 16, "context": "Thus, one cannot employ classic stochastic approximation schemes Robbins and Monro (1951) in our setting.", "startOffset": 65, "endOffset": 90}, {"referenceID": 4, "context": "Borkar (2010), Tamar and Mannor (2013), Prashanth and Ghavamzadeh (2013)) involved estimating the value function using some form of temporal difference learning, which is a stochastic approximation version of a fixed point algorithm.", "startOffset": 0, "endOffset": 14}, {"referenceID": 4, "context": "Borkar (2010), Tamar and Mannor (2013), Prashanth and Ghavamzadeh (2013)) involved estimating the value function using some form of temporal difference learning, which is a stochastic approximation version of a fixed point algorithm.", "startOffset": 0, "endOffset": 39}, {"referenceID": 4, "context": "Borkar (2010), Tamar and Mannor (2013), Prashanth and Ghavamzadeh (2013)) involved estimating the value function using some form of temporal difference learning, which is a stochastic approximation version of a fixed point algorithm.", "startOffset": 0, "endOffset": 73}, {"referenceID": 4, "context": "Lemma 1 in Chapter 2 of Borkar (2008)): sup l\u22650 (\u03b6n+l \u2212 \u03b6n) \u2192 0 as n \u2192 \u221e.", "startOffset": 24, "endOffset": 38}, {"referenceID": 14, "context": "A standard approach to overcome this step-size dependency is to use iterate averaging, suggested independently by Polyak Polyak and Juditsky (1992) and Ruppert Ruppert (1991).", "startOffset": 121, "endOffset": 148}, {"referenceID": 14, "context": "A standard approach to overcome this step-size dependency is to use iterate averaging, suggested independently by Polyak Polyak and Juditsky (1992) and Ruppert Ruppert (1991). The idea is to use larger step-sizes an = 1/n\u03b1, where \u03b1 \u2208 (1/2, 1), and then combine it with averaging of the iterates.", "startOffset": 121, "endOffset": 175}, {"referenceID": 8, "context": "2 in Fathi and Frikha (2013)).", "startOffset": 5, "endOffset": 29}, {"referenceID": 8, "context": "2 in Fathi and Frikha (2013)). Thus, it is optimal to average iterates only after a sufficient number of iterations have passed and all the iterates are very close to the optimum. However, the latter situation serves as a stopping condition in practice. An alternative approach is to employ step-sizes of the form an = (a0/n)Mn, where Mn converges to ( \u22072V \u03b8\u2217(x0) )\u22121 , i.e., the inverse of the Hessian of the CPT-value at the optimum \u03b8\u2217. Such a scheme gets rid of the step-size dependency (one can set a0 = 1) and still obtains optimal convergence rates. This is the motivation behind having a second-order policy optmization scheme and we next adapt the scheme from Spall (2000) for estimating the Hessian inverse.", "startOffset": 5, "endOffset": 681}, {"referenceID": 8, "context": "2 in Fathi and Frikha (2013)). Thus, it is optimal to average iterates only after a sufficient number of iterations have passed and all the iterates are very close to the optimum. However, the latter situation serves as a stopping condition in practice. An alternative approach is to employ step-sizes of the form an = (a0/n)Mn, where Mn converges to ( \u22072V \u03b8\u2217(x0) )\u22121 , i.e., the inverse of the Hessian of the CPT-value at the optimum \u03b8\u2217. Such a scheme gets rid of the step-size dependency (one can set a0 = 1) and still obtains optimal convergence rates. This is the motivation behind having a second-order policy optmization scheme and we next adapt the scheme from Spall (2000) for estimating the Hessian inverse. This is not a restrictive assumption, as one can project to a compact set to ensure boundedness. See the discussion pp. 40-41 of Kushner and Clark (1978) and also remark E.", "startOffset": 5, "endOffset": 871}, {"referenceID": 3, "context": "1 of Bhatnagar et al. (2013).", "startOffset": 5, "endOffset": 29}, {"referenceID": 12, "context": "A simple way is to have \u03a5(Hn) as a diagonal matrix and then add a positive scalar \u03b4n to the diagonal elements so as to ensure invertibility - see Gill et al. (1981), Spall (2000) for a similar operator.", "startOffset": 146, "endOffset": 165}, {"referenceID": 12, "context": "A simple way is to have \u03a5(Hn) as a diagonal matrix and then add a positive scalar \u03b4n to the diagonal elements so as to ensure invertibility - see Gill et al. (1981), Spall (2000) for a similar operator.", "startOffset": 146, "endOffset": 179}, {"referenceID": 2, "context": "Gradient and Hessian estimation We estimate the Hessian of the CPT-value function using the scheme suggested by Bhatnagar and Prashanth (2015). As in the case of the first-order method, we use Rademacher random variables to simultaneously", "startOffset": 112, "endOffset": 143}, {"referenceID": 23, "context": "Notice that the above estimates require three samples, while the the second order SPSA algorithm proposed first in Spall (2000) required four.", "startOffset": 115, "endOffset": 128}, {"referenceID": 7, "context": "5 Gradient-free algorithm for optimizing CPT-value We perform a non-trivial adaptation of the algorithm from Chang et al. (2013) to our setting of optimizing CPT-value in MDPs.", "startOffset": 109, "endOffset": 129}, {"referenceID": 7, "context": "5 Gradient-free algorithm for optimizing CPT-value We perform a non-trivial adaptation of the algorithm from Chang et al. (2013) to our setting of optimizing CPT-value in MDPs. We require that there exists a unique global optimum \u03b8\u2217 for the problem min\u03b8\u2208\u0398 V \u03b8(x0). To illustrate the main idea in the algorithm, assume we know the form of V \u03b8(x0). Then, the idea is to generate a sequence of reference distributions gk(\u03b8) on the policy space \u0398, such that it eventually concentrates on the global optimum \u03b8\u2217. One simple way, suggested in Chapter 4 of Chang et al. (2013) is gk(\u03b8) = H(V (x))gk\u22121(\u03b8) \u222b \u0398H(V \u03b8 (x0))gk\u22121(\u03b8\u2032)\u03bd(d\u03b8\u2032) , \u2200 \u03b8 \u2208 \u0398, (9) where \u03bd is the Lebesgue/counting measure on \u0398 and H is a strictly decreasing function.", "startOffset": 109, "endOffset": 569}, {"referenceID": 0, "context": "11 in Athreya and Lahiri (2006).", "startOffset": 6, "endOffset": 32}, {"referenceID": 22, "context": "The following lemma establishes this claim and its proof can be inferred from Spall (1992), though we give it here for the sake of completeness.", "startOffset": 78, "endOffset": 91}, {"referenceID": 15, "context": "Following this lemma, we complete the proof of Theorem 1 by invoking the well-known Kushner-Clark lemma Kushner and Clark (1978) and this involves verfying conditions A2.", "startOffset": 104, "endOffset": 129}, {"referenceID": 15, "context": "29 of Kushner and Clark (1978), provided we verify that the assumptions A2.", "startOffset": 6, "endOffset": 31}, {"referenceID": 15, "context": "29 of Kushner and Clark (1978), provided we verify that the assumptions A2.2.1 to A2.2.3 and A2.2.4\u201d of Kushner and Clark (1978) are satisfied for \u03b8n governed by (5).", "startOffset": 6, "endOffset": 129}, {"referenceID": 22, "context": "4\u201d using arguments similar to those used in Spall (1992) for the classic SPSA algorithm: We first recall Doob\u2019s martingale inequality (see (2.", "startOffset": 44, "endOffset": 57}, {"referenceID": 15, "context": "27 of Kushner and Clark (1978)):", "startOffset": 6, "endOffset": 31}, {"referenceID": 2, "context": "2 of Bhatnagar and Prashanth (2015). We provide the proof here for the sake of completeness.", "startOffset": 5, "endOffset": 36}, {"referenceID": 23, "context": "The claim related to convergence of \u03b8n can be proven in a manner similar to Theorem 1a of Spall (2000) after observing that Lemma 9 implies that the SPSA based gradient estimate in (7) is only an order O(\u03b42 n) term away from the true gradient \u2207V \u03b8n(x0).", "startOffset": 90, "endOffset": 103}, {"referenceID": 23, "context": "The claim related to convergence of \u03b8n can be proven in a manner similar to Theorem 1a of Spall (2000) after observing that Lemma 9 implies that the SPSA based gradient estimate in (7) is only an order O(\u03b42 n) term away from the true gradient \u2207V \u03b8n(x0). For proving the claim related to convergence of the Hessian recursion (8), we use the proof technique from Spall (2000). The proof proceeds as follows: Let Wm = \u0124m \u2212 E [ \u0124m \u2223\u2223 \u03b8m ] .", "startOffset": 90, "endOffset": 374}, {"referenceID": 23, "context": "The claim related to convergence of \u03b8n can be proven in a manner similar to Theorem 1a of Spall (2000) after observing that Lemma 9 implies that the SPSA based gradient estimate in (7) is only an order O(\u03b42 n) term away from the true gradient \u2207V \u03b8n(x0). For proving the claim related to convergence of the Hessian recursion (8), we use the proof technique from Spall (2000). The proof proceeds as follows: Let Wm = \u0124m \u2212 E [ \u0124m \u2223\u2223 \u03b8m ] . Then, EWm = 0 and \u2211 m E\u2016Wm\u2016 m2 < \u221e, since E [ \u03b42 m \u2225\u2225\u0124m \u2225\u2225 2 ] < \u221e,\u2200m and n 1 (n+1)2\u03b42 n < \u221e by assumption. Now, applying a martingale convergence result from p. 397 of Laha and Rohatgi (1979) to Wm, we obtain 1 n+ 1 n \u2211", "startOffset": 90, "endOffset": 630}, {"referenceID": 7, "context": "4 Proofs for gradient-free policy optimization algorithm We begin by remarking that there is one crucial difference between our algorithm and MRAS2 from Chang et al. (2013): MRAS2 has an expected function value objective, i.", "startOffset": 153, "endOffset": 173}, {"referenceID": 7, "context": "4 Proofs for gradient-free policy optimization algorithm We begin by remarking that there is one crucial difference between our algorithm and MRAS2 from Chang et al. (2013): MRAS2 has an expected function value objective, i.e., it aims to minimize a function by using sample observations that have zero-mean noise. On the other hand, the objective in our setting is the CPT-value, which distorts the underlying transition probabilities. The implication here is that MRAS2 can estimate the expected value using sample averages, while we have to resort to integrating the empirical distribution. Since we obtain samples of the objective (CPT) in a manner that differs from MRAS2, we need to establish that the thresholding step in Algorithm 3 achieves the same effect as it did in MRAS2. This is achieved by the following lemma, which is a variant of Lemma 4.13 from Chang et al. (2013), adapted to our setting.", "startOffset": 153, "endOffset": 885}, {"referenceID": 7, "context": "18 of Chang et al. (2013). This is because our algorithm operates in a similar manner as MRAS2 w.", "startOffset": 6, "endOffset": 26}, {"referenceID": 7, "context": "18 of Chang et al. (2013). This is because our algorithm operates in a similar manner as MRAS2 w.r.t. generating the candidate solution using a parameterized family f(\u00b7, \u03b7) and updating the distribution parameter \u03b7. The difference, as mentioned earlier, is the manner in which the samples are generated and the objective (CPT-value) function is estimated. The aforementioned lemma established that the elite sampling and thresholding achieve the same effect as that in MRAS2 and hence the rest of the proof follows from Chang et al. (2013).", "startOffset": 6, "endOffset": 540}], "year": 2017, "abstractText": "Cumulative prospect theory (CPT) is known to model human decisions well, with substantial empirical evidence supporting this claim. CPT works by distorting probabilities and is more general than the classic expected utility and coherent risk measures. We bring this idea to a risk-sensitive reinforcement learning (RL) setting and design algorithms for both estimation and control. The estimation scheme that we propose uses the empirical distribution in order to estimate the CPT-value of a random variable. We then use this scheme in the inner loop of policy optimization procedures for a Markov decision process (MDP). We propose both gradient-based as well as gradient-free policy optimization algorithms. The former includes both first-order and second-order methods that are based on the well-known simulation optimization idea of simultaneous perturbation stochastic approximation (SPSA), while the latter is based on a reference distribution that concentrates on the global optima. Using an empirical distribution over the policy space in conjunction with Kullback-Leibler (KL) divergence to the reference distribution, we get a global policy optimization scheme. We provide theoretical convergence guarantees for all the proposed algorithms.", "creator": "LaTeX with hyperref package"}}}