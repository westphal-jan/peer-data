{"id": "1506.02690", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2015", "title": "Adaptive Normalized Risk-Averting Training for Deep Neural Networks", "abstract": "This paper proposes a set of new error criteria and learning approaches, Adaptive Normalized Risk-Averting Training (ANRAT), to attack the non-convex optimization problem in training deep neural networks (DNNs). Theoretically, we demonstrate its effectiveness on global and local convexity lower-bounded by the standard $L_p$-norm error. By analyzing the gradient on the convexity index $\\lambda$, we explain the reason why to learn $\\lambda$ adaptively using gradient descent works. In practice, we show how this method improves training of deep neural networks to solve visual recognition tasks on the MNIST and CIFAR-10 datasets. Without using pretraining or other tricks, we obtain results comparable or superior to those reported in recent literature on the same tasks using standard ConvNets + MSE/cross entropy. Performance on deep/shallow multilayer perceptrons and Denoised Auto-encoders is also explored. ANRAT can be combined with other quasi-Newton training methods, innovative network variants, regularization techniques and other specific tricks in DNNs. Other than unsupervised pretraining, it provides a new perspective to address the non-convex optimization problem in DNNs.", "histories": [["v1", "Mon, 8 Jun 2015 20:42:12 GMT  (769kb,D)", "https://arxiv.org/abs/1506.02690v1", "17 pages, 4 figures. Submitted to NIPS-2015"], ["v2", "Fri, 7 Aug 2015 14:53:46 GMT  (768kb,D)", "http://arxiv.org/abs/1506.02690v2", "17 pages, 4 figures"], ["v3", "Thu, 9 Jun 2016 04:10:22 GMT  (585kb,D)", "http://arxiv.org/abs/1506.02690v3", "AAAI 2016, 0.39%~0.4% ER on MNIST with single 32-32-256-10 ConvNets, code available atthis https URL"]], "COMMENTS": "17 pages, 4 figures. Submitted to NIPS-2015", "reviews": [], "SUBJECTS": "cs.LG cs.NE stat.ML", "authors": ["zhiguang wang", "tim oates", "james lo"], "accepted": true, "id": "1506.02690"}, "pdf": {"name": "1506.02690.pdf", "metadata": {"source": "CRF", "title": "Adaptive Normalized Risk-Averting Training For Deep Neural Networks", "authors": ["Zhiguang Wang", "Tim Oates", "James Lo"], "emails": ["zgwang813@gmail.com,", "oates@umbc.edu", "jameslo@umbc.edu"], "sections": [{"heading": "Introduction", "text": "While ConvNets use the de facto state-of-the-art approaches to visual recognition, Deep Belief Networks (DBN), Deep Belief Networks (DBN), Deep Boltzmann Machines (DBM) and Stacked Auto-Encoders (SA), which provide insights as generative models for the complete dissemination of input data, researchers have explored various techniques to improve the learning capacity of DNNs. Unattended pretrainings using Restrict Boltzmann Machines (RBM), Denoised Autoencoders (DA) or Topographic ICA have proven helpful for training DNNs with better weight initialization (Ngiam et al. 2010; Coates and Ng 2011). Rectified Linear Unit (ReLU) and variants are proposed as optimal activation functions for copyrights."}, {"heading": "Convexification on Error Criterion", "text": "Let's start with the definition of RAE for the Lp standard and the theoretical rationale for its convexity properties. RAE is not suitable for real-world applications because it is not limited. Instead, NRAE is bound to address the register overflow in real-world implementations. We prove that NRAE is quasi-convex and therefore shares the same global and local optimum with RAE. Furthermore, we show that the lower limit of its performance is just as good as Lp-standard errors if the convexity index meets a constraint that theoretically supports the ANRAT method proposed in the next section."}, {"heading": "Risk-averting Error Criterion", "text": "In view of the training samples {X, y} = {(x1, y1), (x2, y2), (xm, ym)}, function f (xi, W) is the learning model with parameters W. The loss function of Lp-standard error is defined as: lp (f (xi, W), yi) = 1m m \u00b2 i = 1 | f (xi, W) \u2212 yi | p (1) If p = 2, Eqn. 1 corresponds to the standard Mean Square Error (MSE), the risk-averting error criterion (RAE) corresponding to the Lp-standard error is increased by RAEp, q (f (xi, W), yi) = 1 e\u03bb q | f (xi, W) \u2212 yi | p (2) \u03bb is the convectivity index. It controls the size of the convectivity region."}, {"heading": "Normalized Risk-Averting Error Criterion", "text": "It is not as if the function f (xi, W) is the learning model with parameters W. However, the normalized risk criterion (NRAE) is defined according to the Lp-norm error as: NRAEp, q (f, W) is the learning model with parameters W. The normalized risk criterion (NRAE) is defined according to the Lp-norm error as: NRAEp, q (f, W) is the learning model with parameters W. The normalized risk criterion (NRAE) is defined according to the Lp-norm error as: NRAEp, q (f, W) is the learning model with parameters W."}, {"heading": "Learning Methods", "text": "We propose a new learning method for training DNNs with NRAE, called the Adaptive Normalized Risk-Avering Training (ANRAT) approach. Instead of manually finding solutions such as GDC (Lo, Gui, and NRNRG 2012), we learn adaptive error backpropagation by treating it as a parameter instead of a hyperparameter. Apparently, the learning procedure is standard batch SGD. We show that it works quite well in theory and practice. ANRAT's loss function isl (W, \u03bb) = 1\u03bbq log1m m [1]."}, {"heading": "Experiments", "text": "We present the results of a series of experiments designed on the MNIST and CIFAR-10 dataset to test the effectiveness of ANRAT for visual recognition with DNNs. We have the full hyperparameters in Eqn. 6. Instead, we set the hyperparameters to p = 2, q = 2 and r = 1 to compare them mainly with MSE. Thus, the final loss function of ANRAT is minimized by batch SGD without complex methods such as dynamics, adaptive / hand-tuned learning rates or tangent prop. The learning rate and penalty weight a are selected in {1, 0.5, 0.1} and {1, 0.1, 0.001} in the images."}, {"heading": "Results and Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Results on ConvNets", "text": "In fact, it is a matter of a way in which the people move in the most different areas of the world: from the USA to the USA, from the USA to the USA, from the USA to the USA, from Europe to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the United States to the USA, from the United States to the United States, from the United States to the United States, from the United States to the United States, from the United States to the United States, from the United States to the United States, from the United States to the United States, from the United States to the United States, from the United States to the United States, from the United States to the United States, from the United States to the United States, from the United States to the United States, from the United States to the United States to the United States, from the United States to the United States, from the United States to the United States, from the United States to the United States to the United States, from the United States to the United States, from the United States to the United States, from the United States to the United States, from the United States to the United States, from the United States to the United States to the United States, from the United States to the United States, from the United States to the United States to the United States, from the United States from the United States to the United States, from the United States from the United States to the United States from the United States to the United States from the United States to the United States from the United States to the United States from the United States from the United States to the United States from the United States to the United States from the United States from the United States to the United States from the United States to the United States from the United States to the United States from the United States from the United States to the United States from the"}, {"heading": "Results on Multilayer Perceptron", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "Conclusions and Outlook", "text": "In this paper, we present a new approach, Adaptive Normalized Risk-Averting Training (ANRAT), to help train deep neural networks. Theoretically, we prove the effectiveness of Normalized Risk-Averting Error on its arithmetic global convexity and local convexity, which is undercut by standard Lp-standard errors when convexity index \u03bb \u2265 1. By analyzing the gradient at \u03bb, we explained the reason why the use of back propagation works on \u03bb. Experiments on deep / flat network layouts show comparable or better performance with the same experimental configurations under pure ConvNets and MLP + Batch SGD on MSE and CE (with or without dropout). Unlike uncontrolled pre-training, it offers a new perspective to address the non-convex optimization strategy in DNNs. Finally, these early results are justified, but very encouraging to address in this way."}], "references": [{"title": "I", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "Goodfellow"], "venue": "J.; Bergeron, A.; Bouchard, N.; and Bengio, Y.", "citeRegEx": "Bastien et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "and Zisserman", "author": ["A. Blake"], "venue": "A.", "citeRegEx": "Blake and Zisserman 1987", "shortCiteRegEx": null, "year": 1987}, {"title": "A", "author": ["A. Coates", "Ng"], "venue": "Y.", "citeRegEx": "Coates and Ng 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "and Shashua", "author": ["N. Cohen"], "venue": "A.", "citeRegEx": "Cohen and Shashua 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Y", "author": ["Dauphin"], "venue": "N.; Pascanu, R.; Gulcehre, C.; Cho, K.; Ganguli, S.; and Bengio, Y.", "citeRegEx": "Dauphin et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Pylearn2: a machine learning research", "author": ["Goodfellow"], "venue": null, "citeRegEx": "Goodfellow,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow", "year": 2013}, {"title": "J", "author": ["Gui, Y.", "Lo"], "venue": "T.-H.; and Peng, Y.", "citeRegEx": "Gui. Lo. and Peng 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["Jarrett"], "venue": "In Computer Vision,", "citeRegEx": "Jarrett,? \\Q2009\\E", "shortCiteRegEx": "Jarrett", "year": 2009}, {"title": "and Hinton", "author": ["A. Krizhevsky"], "venue": "G.", "citeRegEx": "Krizhevsky and Hinton 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "G", "author": ["A. Krizhevsky", "I. Sutskever", "Hinton"], "venue": "E.", "citeRegEx": "Krizhevsky. Sutskever. and Hinton 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Exploring strategies for training deep neural networks. The Journal of Machine Learning Research 10:1\u201340", "author": ["Larochelle"], "venue": null, "citeRegEx": "Larochelle,? \\Q2009\\E", "shortCiteRegEx": "Larochelle", "year": 2009}, {"title": "P", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "Haffner"], "venue": "1998. Gradient-based learning applied to document recognition. Proceedings of the IEEE 86(11):2278\u2013", "citeRegEx": "LeCun et al. 1998", "shortCiteRegEx": null, "year": 2324}, {"title": "C", "author": ["W. Liu", "Floudas"], "venue": "A.", "citeRegEx": "Liu and Floudas 1993", "shortCiteRegEx": null, "year": 1993}, {"title": "Y", "author": ["J.T.-H. Lo", "Y. Gui", "Peng"], "venue": "2012. Overcoming the local-minimum problem in training multilayer perceptrons with the nrae training method. In Advances in Neural Networks\u2013ISNN", "citeRegEx": "Lo. Gui. and Peng 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "J", "author": ["Lo"], "venue": "T.-H.", "citeRegEx": "Lo 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "C", "author": ["J. Mairal", "P. Koniusz", "Z. Harchaoui", "Schmid"], "venue": "2014. Convolutional kernel networks. In Advances in Neural Information Processing Systems, 2627\u2013", "citeRegEx": "Mairal et al. 2014", "shortCiteRegEx": null, "year": 2635}, {"title": "S", "author": ["Min Lin", "Qiang Chen"], "venue": "Y.", "citeRegEx": "Min Lin 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "A", "author": ["J. Ngiam", "Z. Chen", "D. Chia", "P.W. Koh", "Q.V. Le", "Ng"], "venue": "Y.", "citeRegEx": "Ngiam et al. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "A", "author": ["J. Ngiam", "A. Coates", "A. Lahiri", "B. Prochnow", "Q.V. Le", "Ng"], "venue": "Y.", "citeRegEx": "Ngiam et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Y", "author": ["C. Poultney", "S. Chopra", "Cun"], "venue": "L.; et al.", "citeRegEx": "Poultney et al. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Y", "author": ["M. Ranzato", "F.J. Huang", "Y.L. Boureau", "LeCun"], "venue": "2007. Unsupervised learning of invariant feature hierarchies with applications to object recognition. In Computer Vision and Pattern Recognition,", "citeRegEx": "Ranzato et al. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "J", "author": ["Speyer"], "venue": "L.; Deyst, J.; and Jacobson, D.", "citeRegEx": "Speyer. Deyst. and Jacobson 1974", "shortCiteRegEx": null, "year": 1974}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "Srivastava,? \\Q2014\\E", "shortCiteRegEx": "Srivastava", "year": 2014}, {"title": "and Fergus", "author": ["M.D. Zeiler"], "venue": "R.", "citeRegEx": "Zeiler and Fergus 2013", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [], "year": 2016, "abstractText": "This paper proposes a set of new error criteria and a learning approach, called Adaptive Normalized RiskAverting Training (ANRAT) to attack the non-convex optimization problem in training deep neural networks without pretraining. Theoretically, we demonstrate its effectiveness based on the expansion of the convexity region. By analyzing the gradient on the convexity index \u03bb, we explain the reason why our learning method using gradient descent works. In practice, we show how this training method is successfully applied for improved training of deep neural networks to solve visual recognition tasks on the MNIST and CIFAR10 datasets. Using simple experimental settings without pretraining and other tricks, we obtain results comparable or superior to those reported in recent literature on the same tasks using standard ConvNets + MSE/cross entropy. Performance on deep/shallow multilayer perceptron and Denoised Auto-encoder is also explored. ANRAT can be combined with other quasiNewton training methods, innovative network variants, regularization techniques and other common tricks in DNNs. Other than unsupervised pretraining, it provides a new perspective to address the non-convex optimization strategy in training DNNs.", "creator": "LaTeX with hyperref package"}}}