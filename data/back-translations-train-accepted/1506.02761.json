{"id": "1506.02761", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2015", "title": "WordRank: Learning Word Embeddings via Robust Ranking", "abstract": "Embedding words in a vector space has gained a lot of research attention in recent years. While state-of-the-art methods provide efficient computation of word similarities via a low-dimensional matrix embedding, their motivation is often left unclear. In this paper, we argue that word embedding can be naturally viewed as a ranking problem. Then, based on this insight, we propose a novel framework WordRank that efficiently estimates word representations via robust ranking. The performance of WordRank is measured in word similarity and word analogy benchmarks, and the results are compared to the state-of-the-art word embedding techniques. Our algorithm produces a vector space with meaningful substructure, as evidenced by its performance of 77.4% accuracy on a popular word similarity benchmark and 76% on the Google word analogy benchmark. WordRank performs especially well on small corpora.", "histories": [["v1", "Tue, 9 Jun 2015 03:08:06 GMT  (108kb,D)", "http://arxiv.org/abs/1506.02761v1", null], ["v2", "Sat, 6 Feb 2016 06:02:56 GMT  (71kb,D)", "http://arxiv.org/abs/1506.02761v2", null], ["v3", "Tue, 23 Feb 2016 06:40:14 GMT  (69kb,D)", "http://arxiv.org/abs/1506.02761v3", null], ["v4", "Tue, 27 Sep 2016 21:11:56 GMT  (69kb,D)", "http://arxiv.org/abs/1506.02761v4", "Conference on Empirical Methods in Natural Language Processing (EMNLP), November 1-5, 2016, Austin, Texas, USA"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG stat.ML", "authors": ["shihao ji", "hyokun yun", "pinar yanardag", "shin matsushima", "s v n vishwanathan"], "accepted": true, "id": "1506.02761"}, "pdf": {"name": "1506.02761.pdf", "metadata": {"source": "CRF", "title": "WordRank: Learning Word Embeddings via Robust Ranking", "authors": ["Shihao Ji", "Hyokun Yun", "Pinar Yanardag"], "emails": ["shihao.ji@intel.com", "yunhyoku@amazon.com", "ypinar@purdue.edu", "shin_matsushima@mist.", "vishy@ucsc.edu"], "sections": [{"heading": "1 Introduction", "text": "This is best illustrated by a concrete example: \"We expect the model of the production of women.\" The impressive performance of words to improve terminology is an important sub-task for many areas of natural language processing. (It has been shown that the analogy of the word requires answering questions of form: \"c:?, where a, b, and c are words from the vocabulary, and the answer to the question must be semantically related to c in the same way as a particular word.\" This is best illustrated by a concrete example:? we expect the model of the production of women. \"The answer to the question must be semantically related to c in the same way as b.\""}, {"heading": "2 Word Embedding via Ranking", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Notation", "text": "We use w to denote a word, and c to denote a context. We will use the set of all words, i.e. the vocabulary is asW, and the set of all contextual words is asW. We will use the set of all word-context pairs observed in the data to indicate the set of contexts that coincided with a given word w, and the set of words that coincided with a given contextual word. The size of a set is called | \u00b7 |. The inner product between vectors is called < \u00b7, \u00b7 >."}, {"heading": "2.2 Ranking Model", "text": "s collect the k-dimensional embedding of a word w, and vc = \u03b2, that of a context c = >. For convenience we collect embedding parameters for words and contexts as U: = {uw} w, and V: = {vc} c, c, c, p, c, c, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p"}, {"heading": "2.3 Stochastic Optimization", "text": "In order to avoid this problem, we propose to optimize a linear upper limit of the objective function, which is achieved by a first order."}, {"heading": "2.4 Parallelization", "text": "The updates on lines 8-10 have a remarkable feature: to update the uw, vc, and vc variables, we only need to read the uw, vc, vc, \"and \u0394w, c\" variables, which means that updates of another triplet of variables uw, \"vc,\" and vc \"can be performed independently of each other. This observation is the key to developing a parallel optimization strategy by distributing the calculation of updates across multiple processors. Due to lack of space, details including pseudo-code are banished to Appendix A."}, {"heading": "2.5 Interpreting \u03b1 and \u03b2", "text": "The update (12) shows that this problem is proportional to rank (w, \u03b2, c). On the other hand, one can observe that the loss function (\u00b7) in (14) is \"weighted\" by expressing the value (w, c) and thus the value (1w, c). Since \"p\" (\u00b7) is a concave function, its course \"p\" (\u00b7) is a function that does not increase monotonously [22]. Consequently, one can simply set the auxiliary variables if rank (w, c) and thus \"p\" \u2212 1w, \"c\" is large, \"p\" (w, c) small. \"In other words, the loss function\" gives up \"c\" on contexts. 2One can simply set the auxiliary variables, w \"c = 1 if it is a linear function. Algorithm 1 WordRank algorithm 1\" and \"c,\" c \": Step 2: Repeat 3: / / / / / Level 1: Update U and 4: Repeat 5 (example c)."}, {"heading": "3 Related Work", "text": "The use of score functions < uw, vc > for ranking is inspired by the latent collaborative retrieval system of Weston et al. [25] The use of score functions < uw, vc > for ranking is equivalent to the known paired ranking loss (see e.g. Lee and Lin [12]). On the other hand, Yun et al. [27] note that if we set \u03c1 = 2 as in (7), then \u2212 J (U, V) corresponds to the known paired ranking loss (see e.g. Lee and Lin [12])."}, {"heading": "4 Experiments", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "4.1 Evaluation", "text": "Word Similarity We use six datasets to evaluate word similarity: WS-353 / 14 mchmark / http: http: / / www.artartart.we [9] partitioned into two subgroups: WordSim Similarity and WordSim Relatedness [1]; MEN [5]; Mechanical Turk [21]; Rare words [15]; and SimLex-999 [11]. They contain word pairs along with human-assigned similarity judgments. Word representations are evaluated by ranking the pairs according to their cosmic similarities, and measuring the rank correlation of the Spearmans with human judgments. Word analogies For this task, we use the Google Analogy Dataset [18]. It contains 19544 word analogies, divided into 8869 semantic and 10675 syntactic questions. The semantic questions contain five types of semantic analogies, such as capitals (Paris: France; Tokyo:)."}, {"heading": "4.3 Comparison to state-of-the-art", "text": "In this section, we compare the performance of WordRank with word2vec8 and GloVe9 by using the code provided by the respective authors. To make a fair comparison, GloVe and WordRank are seen as input of the same primary event matrix X; this eliminates performance differences due to window size and other such artifacts, and the same parameters are used for word2vec. Furthermore, the embedding dimensions used for each of the three methods are the same (see Table 1). With word2vec, we train the skip graph with negative sampling (SGNS) model, as it produces state-of-the-art performance and is widely used in the NLP community. For GloVe, we use the standard parameters proposed by us [20]. The results can be seen in Figure 1 (see Table 4 in Appendix C for details)."}, {"heading": "5 Discussion", "text": "We found that WordRank achieves an accuracy of 77.4% on WS-353 using the \u03c10 loss on the 7.2 billion token dataset (see Table 4 in Appendix C). To our knowledge, this is better than any other published result on this benchmark. Unfortunately, the corresponding performance of the word analogy task is lower than other ranking losses. This is because word analogy is a retrieval problem, and therefore it is important to emphasize the performance at the top of the list in order to achieve good performance.The added flexibility of the WordRank framework means that it is mathematically more expensive than word2vec or GloVe. Therefore, we recommend using WordRank in settings where the data is sparse, and one is willing to trade additional calculations for a better generalization."}, {"heading": "B Modifying \u03c11 using \u03b1 and \u03b2", "text": "Algorithm 2: Distributed WordRank algorithm. 1: \u03b7: Step size 2: Repeat 3: / / Start of outer iteration 4: Example of a partition via contexts {C (1),.., C (q)} 5: / / Level 1: Update U and V in parallel for all machine q (1, 2,.., p) perform in parallel 6: Get all vc (q) 7: Repeat 8: Example (w, c) uniform from machine q (W (q) \u00b7 C (q) \u00b7 C (q)) 9: Example c (c) uniform from machine C (q)\\ {c} 10: / / / following three updates are performed simultaneously 11: uw (w, c)."}, {"heading": "C Additional Experimental Details", "text": "Table 4 is the tabular view of the data shown in Figure 1 to provide additional experimental details. D Visualization of the resultsTo understand whether WordRank delivers syntatically and semantically meaningful results, we conducted the following experiment: We use the model created with 7.2 billion tokens and calculate the most serious neighbors of the word \"cat.\" We then visualize the words in two dimensions using t-SNE, a well-known dimension reduction technique [16]. As shown in Figure 3, our model is actually able to capture both semantic (e.g. cat, cat, kitten, tabby) and syntactic (e.g. leash, leash, leash) regularities of the English language."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Embedding words in a vector space has gained a lot of research attention in re-<lb>cent years. While state-of-the-art methods provide efficient computation of word<lb>similarities via a low-dimensional matrix embedding, their motivation is often left<lb>unclear. In this paper, we argue that word embedding can be naturally viewed<lb>as a ranking problem. Then, based on this insight, we propose a novel frame-<lb>work WordRank that efficiently estimates word representations via robust ranking.<lb>The performance of WordRank is measured in word similarity and word analogy<lb>benchmarks, and the results are compared to the state-of-the-art word embedding<lb>techniques. Our algorithm produces a vector space with meaningful substructure,<lb>as evidenced by its performance of 77.4% accuracy on a popular word similarity<lb>benchmark and 76% on the Google word analogy benchmark. WordRank per-<lb>forms especially well on small corpora.", "creator": "TeX"}}}