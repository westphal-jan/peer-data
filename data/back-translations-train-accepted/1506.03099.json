{"id": "1506.03099", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2015", "title": "Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks", "abstract": "Recurrent Neural Networks can be trained to produce sequences of tokens given some input, as exemplified by recent results in machine translation and image captioning. The current approach to training them consists in maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence. We propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead. Experiments on several sequence prediction tasks show that this approach yields significant improvements.", "histories": [["v1", "Tue, 9 Jun 2015 20:33:47 GMT  (117kb,D)", "http://arxiv.org/abs/1506.03099v1", null], ["v2", "Mon, 15 Jun 2015 15:29:22 GMT  (117kb,D)", "http://arxiv.org/abs/1506.03099v2", null], ["v3", "Wed, 23 Sep 2015 16:35:42 GMT  (117kb,D)", "http://arxiv.org/abs/1506.03099v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.CV", "authors": ["samy bengio", "oriol vinyals", "navdeep jaitly", "noam shazeer"], "accepted": true, "id": "1506.03099"}, "pdf": {"name": "1506.03099.pdf", "metadata": {"source": "CRF", "title": "Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks", "authors": ["Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer"], "emails": ["bengio@google.com", "vinyals@google.com", "ndjaitly@google.com", "noam@google.com"], "sections": [{"heading": null, "text": "Recurrent neural networks can be trained to generate sequences of tokens based on specific inputs, as shown by recent machine translation and caption results. The current approach to training is to maximize the likelihood of each token in the sequence based on the current (recurring) state and previous token. Concluding, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can lead to errors that can quickly accumulate along the generated sequence. We suggest a learning strategy to gently change the training process from a fully managed scheme using the real previous token to a less guided scheme that instead largely uses the generated token. Experiments on multiple sequence prediction tasks show that this approach brings significant improvements."}, {"heading": "1 Introduction", "text": "In both cases, the sequences are used either as input, as output, or as both. While, they are difficult to learn when it comes to long-term dependencies in the data, some versions, such as Long Short-Term Memory (LSTM), are more appropriate. In this paper, we look at the problems that occur in generating variable-size tokens, such as the problem of machine translation [4], image signature [5], or video transmission [6]."}, {"heading": "2 Proposed Approach", "text": "We consider supervised tasks where the training set is given in the form of N-input / output pairs {Xi, Y i} Ni = 1, where Xi is the input and can be either static (like an image) or dynamic (like a sequence), while the target output Y i is a sequence yi1, y i 2,..., y i Tiof a variable number of tokens belonging to a fixed known dictionary."}, {"heading": "2.1 Model", "text": "Given a single input / output pair (X, Y), the log probability P (Y | X) can be calculated as follows: logP (Y | X) = logP (yT1 | X) = T \u2211 t = 1 logP (yt | yt \u2212 11, X) (1), where Y is a sequence of lengths T represented by tokens y1, y2,., yT. The latter term in the above equation is estimated by a recursive neural network of parameters \u03b8 by introducing a state vector, ht, which is a function of the previous state ht \u2212 1, and the previous output token, yt \u2212 1, i.e.logP (yt | yt \u2212 11, X; \u03b8) = logP (yt | ht; \u03b8) (2), where ht is calculated by a recursive neural network ht \u2212 1, and the previous output token state is as follows: {= 1, when projecting \u2212 f = 1."}, {"heading": "2.2 Training", "text": "The formation of recurrent neural networks to solve such tasks is usually achieved by using stochastic gradient pedigree in the minibatch to search for a set of parameters \u03b8? that maximize the log probability of generating the correct target sequence Y i using the input data Xi for all training pairs (Xi, Y i): \u03b8? = argmax \u0432 (Xi, Y i) logP (Y i | Xi; \u03b8). (4)"}, {"heading": "2.3 Inference", "text": "When a < EOS > character is generated, it means the end of the sequence. For this process, the model currently requires the output token yt \u2212 1 from the last time step to generate yt. Since we do not have access to the true previous token, we can instead either select the most likely sequence underlying our model or select a sample for it. Searching for the sequence Y with the highest probability that X gave is too expensive due to the combinatorial growth of the number of sequences. Instead, we use a beam search method to generate k \"best\" sequences. We do this by maintaining a pile of m best candidate sequences. At each step, new candidates are generated by adding a token to each candidate and adding it to the pile."}, {"heading": "2.4 Bridging the Gap with Scheduled Sampling", "text": "The main difference between training and inference for sequence prediction tasks in predicting token yt = is whether we use the true previous token yt \u2212 1 or an estimate y \u2212 1 that comes from the model itself. We propose here a sampling mechanism that randomly decides whether we use yt \u2212 1 or y \u2212 1. Suppose we use a mini-batch-based stochastic downward model for each token to predict yt-Y of the ith mini-batch of the training algorithm, we propose to flip a coin and use the true previous token with the probability i, or an estimate that comes from the model itself with the probability (1 \u2212 i) 2. Estimation of the model can be achieved by sampling a token according to the probability distribution modeled by P (yt \u2212 1 | ht \u2212 1), or an estimate from the model itself."}, {"heading": "3 Related Work", "text": "The discrepancy between the training and the inference distributions has already been noted in the literature, especially for control and reinforcement learning tasks. SEARN [9] has been proposed to address problems where monitored training examples could deviate from actual test examples if each example is made in a sequence of decisions, such as in a complex environment where a few errors of the model at the beginning of the sequential decision process could produce very poor global performance. Their proposed approach includes a meta-algorithm in which a new model is trained according to current policy (essentially the expected decisions for each situation) for each meta-iteration, and the next iteration policy is modified to take into account the previous decisions and mistakes. Therefore, the new policy is a combination of the previous and the actual behavior of the model. Compared to SEARN, our proposed approach is fully online: a single model is trained and the policy evolves slowly, rather than a batch approach, which makes it much faster to learn."}, {"heading": "4 Experiments", "text": "In this section, we describe experiments on three different tasks to show that scheduled scanning in different settings can be helpful: We report on results for caption, constituency parsing, and speech recognition."}, {"heading": "4.1 Image Captioning", "text": "The task can be formulated as mapping an image on a sequence of words describing its content in a natural language, and most of the suggested approaches deal with a form of recurring network structure with simple decoding schemes [5, 6, 12, 13, 14]. A notable exception is the system used in [15], which does not directly optimize the log probability of captioning and instead suggests a pipelined approach. As an image can have many valid captions, the evaluation of this task is still an open problem. Some attempts have been made to design metrics that positively correlate to human evaluation, and a common set of tools have been published by the MSCOCO team. We have used the MSCOCO dataset from [17] to train our model."}, {"heading": "4.2 Constituency Parsing", "text": "Another, less obvious link with the paradigm of any sequence is the parsing of constituencies. Recent work [4] has suggested an interpretation of a parse tree as a sequence of linear \"operations\" that build the tree. This linearization procedure enabled them to train a model that can map a sentence to its parse tree without making any changes to the formulation of any sequence. [19] The trained model has a layer of 512 LSTM cells and words represented by the embedding of size 512 vectors. However, we used an attention mechanism similar to that described in [19], which helps to focus on a part of the input sequence when considering the next output token to generate Yt by applying a softmax via the LSTM state vectors corresponding to the input sequence."}, {"heading": "4.3 Speech Recognition", "text": "The question of the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way in the way, in the way in the way, in the way, in the way, in the way, in the way in the way, in the way, in the way, in the way, in the way, in the way in the way, in the way in the way, in the way in the way, in the way, in the way, in the way, in the way, in the way, in the way in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way, in the way"}, {"heading": "5 Conclusion", "text": "The use of recurring neural networks to predict token sequences has many useful applications, such as machine translation and image description. However, the current approach to training, in which one token after another is predicted depending on the state and previous correct token symbol, differs from actual use and is therefore prone to accumulating errors along the decision paths. In this paper, we proposed a curriculum learning approach to slowly change the training target from a simple task where the previous token is known to a realistic task where it is provided by the model itself. Experiments with multiple sequence prediction tasks lead to performance improvements without taking longer training times. Future work will include re-propagating the errors through sample decisions and exploring better sample strategies, including conditioning some confidence measures from the model itself."}], "references": [{"title": "Learning long term dependencies is hard", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE Transactions on Neural Networks, 5(2):157\u2013166", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1994}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, 9(8)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q. Le"], "venue": "Advances in Neural Information Processing Systems, NIPS", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Grammar as a foreign language", "author": ["O. Vinyals", "L. Kaiser", "T. Koo", "S. Petrov", "I. Sutskever", "G. Hinton"], "venue": "arXiv:1412.7449", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Curriculum learning", "author": ["Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston"], "venue": "Proceedings of the International Conference on Machine Learning, ICML", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J.D. Lafferty", "A. McCallum", "F.C.N. Pereira"], "venue": "Proceedings of the Eighteenth International Conference on Machine Learning, ICML, pages 282\u2013289, San Francisco, CA, USA", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "Search-based structured prediction as classification", "author": ["H. Daum\u00e9 III", "J. Langford", "D. Marcu"], "venue": "Machine Learning Journal", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["S. Ross", "G.J. Gordon", "J.A. Bagnell"], "venue": "Proceedings of the Workshop on Artificial Intelligence and Statistics, AISTATS", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Incremental parsing with the perceptron algorithm", "author": ["M. Collins", "B. Roark"], "venue": "Proceedings of the Association for Computational Linguistics, ACL", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "H. Huangzhi", "A. Yuille"], "venue": "International Conference on Learning Representations, ICLR", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R. Zemel"], "venue": "TACL", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "F.-F. Li"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "From captions to visual concepts and back", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R.K. Srivastava", "L. Deng", "P. Dollar", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt", "C.L. Zitnick", "G. Zweig"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "CIDEr: Consensus-based image description evaluation", "author": ["R. Vedantam", "C.L. Zitnick", "D. Parikh"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "arXiv:1405.0312", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "Proceedings of the International Conference on Machine Learning, ICML", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "International Conference on Learning Representations, ICLR", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Ontonotes: The 90% solution", "author": ["E. Hovy", "M. Marcus", "M. Palmer", "L. Ramshaw", "R. Weischedel"], "venue": "In Proceedings of the Human Language Technology Conference of the NAACL, Short Papers,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "The kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz", "J. Silovsky", "G. Stemmer", "K. Vesely"], "venue": "In IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. IEEE Signal Processing Society,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Exploring Deep Learning Methods for discovering features in speech signals", "author": ["N. Jaitly"], "venue": "PhD thesis, University of Toronto", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "End-to-end continuous speech recognition using attention-based recurrent nn: First results", "author": ["Jan Chorowski", "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.1602,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "While they are known to be hard to train when there are long term dependencies in the data [1], some versions like the Long Short-Term Memory (LSTM) [2] are better suited for this.", "startOffset": 91, "endOffset": 94}, {"referenceID": 1, "context": "While they are known to be hard to train when there are long term dependencies in the data [1], some versions like the Long Short-Term Memory (LSTM) [2] are better suited for this.", "startOffset": 149, "endOffset": 152}, {"referenceID": 2, "context": "In fact, they have recently shown impressive performance in several sequence prediction problems including machine translation [3], contextual parsing [4], image captioning [5] or even video description [6].", "startOffset": 127, "endOffset": 130}, {"referenceID": 3, "context": "In fact, they have recently shown impressive performance in several sequence prediction problems including machine translation [3], contextual parsing [4], image captioning [5] or even video description [6].", "startOffset": 151, "endOffset": 154}, {"referenceID": 4, "context": "In fact, they have recently shown impressive performance in several sequence prediction problems including machine translation [3], contextual parsing [4], image captioning [5] or even video description [6].", "startOffset": 173, "endOffset": 176}, {"referenceID": 5, "context": "In fact, they have recently shown impressive performance in several sequence prediction problems including machine translation [3], contextual parsing [4], image captioning [5] or even video description [6].", "startOffset": 203, "endOffset": 206}, {"referenceID": 6, "context": "Here, we propose a curriculum learning approach [7] to gently bridge the gap between training and inference for sequence prediction tasks using recurrent neural networks.", "startOffset": 48, "endOffset": 51}, {"referenceID": 7, "context": "Thus, the model represents the probability distribution of sequences in the most general form - unlike Conditional Random Fields [8] and other models that assume independence between between outputs at different time steps, given latent variable states.", "startOffset": 129, "endOffset": 132}, {"referenceID": 8, "context": "SEARN [9] was proposed to tackle problems where supervised training examples might be different from actual test examples when each example is made of a sequence of decisions, like acting in a complex environment where a few mistakes of the model early in the sequential decision process might compound and yield a very poor global performance.", "startOffset": 6, "endOffset": 9}, {"referenceID": 9, "context": "In comparison to SEARN, our proposed approach is completely online: a single model is trained and the policy slowly evolves during training, instead of a batch approach, which makes it much faster to train3 Furthermore, SEARN, as well as similar approaches like [10], have been proposed in the context of reinforcement learning, while we consider the supervised learning setting trained using stochastic gradient descent on the overall objective.", "startOffset": 262, "endOffset": 266}, {"referenceID": 10, "context": "Other approaches have considered the problem from a ranking perspective, in particular for parsing tasks [11] where the target output is a tree.", "startOffset": 105, "endOffset": 109}, {"referenceID": 4, "context": "The task can be formulated as a mapping of an image onto a sequence of words describing its content in some natural language, and most proposed approaches employ some form of recurrent network structure with simple decoding schemes [5, 6, 12, 13, 14].", "startOffset": 232, "endOffset": 250}, {"referenceID": 5, "context": "The task can be formulated as a mapping of an image onto a sequence of words describing its content in some natural language, and most proposed approaches employ some form of recurrent network structure with simple decoding schemes [5, 6, 12, 13, 14].", "startOffset": 232, "endOffset": 250}, {"referenceID": 11, "context": "The task can be formulated as a mapping of an image onto a sequence of words describing its content in some natural language, and most proposed approaches employ some form of recurrent network structure with simple decoding schemes [5, 6, 12, 13, 14].", "startOffset": 232, "endOffset": 250}, {"referenceID": 12, "context": "The task can be formulated as a mapping of an image onto a sequence of words describing its content in some natural language, and most proposed approaches employ some form of recurrent network structure with simple decoding schemes [5, 6, 12, 13, 14].", "startOffset": 232, "endOffset": 250}, {"referenceID": 13, "context": "The task can be formulated as a mapping of an image onto a sequence of words describing its content in some natural language, and most proposed approaches employ some form of recurrent network structure with simple decoding schemes [5, 6, 12, 13, 14].", "startOffset": 232, "endOffset": 250}, {"referenceID": 14, "context": "A notable exception is the system proposed in [15], which does not directly optimize the log likelihood of the caption given the image, and instead proposes a pipelined approach.", "startOffset": 46, "endOffset": 50}, {"referenceID": 15, "context": "Some attempts have been made to design metrics that positively correlate with human evaluation [16], and a common set of tools have been published by the MSCOCO team [17].", "startOffset": 95, "endOffset": 99}, {"referenceID": 16, "context": "Some attempts have been made to design metrics that positively correlate with human evaluation [16], and a common set of tools have been published by the MSCOCO team [17].", "startOffset": 166, "endOffset": 170}, {"referenceID": 16, "context": "We used the MSCOCO dataset from [17] to train our model.", "startOffset": 32, "endOffset": 36}, {"referenceID": 17, "context": "The image is preprocessed by a pretrained convolutional neural network (without the last classification layer) similar to the one described in [18], and the resulting image embedding is treated as if it was the first word from which the model starts generating language.", "startOffset": 143, "endOffset": 147}, {"referenceID": 3, "context": "Recent work [4] has proposed an interpretation of a parse tree as a sequence of linear \u201coperations\u201d that build up the tree.", "startOffset": 12, "endOffset": 15}, {"referenceID": 18, "context": "We used an attention mechanism similar to the one described in [19] which helps, when considering the next output token to produce yt, to focus on part of the input sequence only by applying a softmax over the LSTM state vectors corresponding to the input sequence.", "startOffset": 63, "endOffset": 67}, {"referenceID": 19, "context": "In this table we report the F1 score on the WSJ 22 development set [20].", "startOffset": 67, "endOffset": 71}, {"referenceID": 20, "context": "The targets used were HMM-state labels generated from a GMM-HMM recipe, using the Kaldi toolkit [21] but could very well have been phoneme labels.", "startOffset": 96, "endOffset": 100}, {"referenceID": 21, "context": "We generated data for these experiments using the TIMIT4 corpus and the KALDI toolkit as described in [22].", "startOffset": 102, "endOffset": 106}, {"referenceID": 22, "context": "A model such as the attention model of [23] which predicts phone sequences directly, instead of the highly redundant HMM state sequences, would not suffer from this problem because it would need to exploit both the acoustic signal and the language model sufficiently to make predictions.", "startOffset": 39, "endOffset": 43}], "year": 2015, "abstractText": "Recurrent Neural Networks can be trained to produce sequences of tokens given some input, as exemplified by recent results in machine translation and image captioning. The current approach to training them consists in maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence. We propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead. Experiments on several sequence prediction tasks show that this approach yields significant improvements.", "creator": "LaTeX with hyperref package"}}}