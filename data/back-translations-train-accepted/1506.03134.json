{"id": "1506.03134", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2015", "title": "Pointer Networks", "abstract": "We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence and Neural Turing Machines, because the number of target classes in each step of the output depends on the length of the input, which is variable. Problems such as sorting variable sized sequences, and various combinatorial optimization problems belong to this class. Our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention. It differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output. We call this architecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn approximate solutions to three challenging geometric problems -- finding planar convex hulls, computing Delaunay triangulations, and the planar Travelling Salesman Problem -- using training examples alone. Ptr-Nets not only improve over sequence-to-sequence with input attention, but also allow us to generalize to variable size output dictionaries. We show that the learnt models generalize beyond the maximum lengths they were trained on. We hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems.", "histories": [["v1", "Tue, 9 Jun 2015 23:38:16 GMT  (277kb,D)", "http://arxiv.org/abs/1506.03134v1", null], ["v2", "Mon, 2 Jan 2017 10:25:29 GMT  (277kb,D)", "http://arxiv.org/abs/1506.03134v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.CG cs.LG cs.NE", "authors": ["oriol vinyals", "meire fortunato", "navdeep jaitly"], "accepted": true, "id": "1506.03134"}, "pdf": {"name": "1506.03134.pdf", "metadata": {"source": "CRF", "title": "Pointer Networks", "authors": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This development has led to the problems with embedding in an output sequence being highlighted. Bahdanau et. al. augmented the decoder by propagating extra contextual information from the input-based attentional mechanism [5, 2, 6]. augmented the input-based attenationism from the input-based attentional mechanism [6] and other (possibly the same) RNN to map the embedding to an output sequence. Bahdanau et. al. augmented the decoder that propagating extra contextual information from the input-based attentional mechanism [7, 2, 6]. These developments have made it possible to apply RNNNs to new domains that achieve state-of-the-art results in natural language processing such as translation."}, {"heading": "2 Models", "text": "We review the sequence-to-sequence models [1] and the input-attention models [5] that form the basis for this work in Section 2.1 and 2.2, and then describe our model - Ptr-Net in Section 2.3."}, {"heading": "2.1 Sequence-to-Sequence Model", "text": "Given a training pair, (P, CP), the sequence-to-sequence model calculates the conditional probability p (CP | P; \u03b8) using a parametric model (an RNN with parameters \u03b8) to estimate the conditions of the probability chain rule (see also Figure 1), i.e., Pn} is a sequence of n vectors and CP = {C1,..., Cm (P)} is a sequence of m (P) indices, each between 1 and n. The parameters of the model are learned by maximizing the conditional probabilities for the training scope."}, {"heading": "2.2 Content Based Input Attention", "text": "The vanilla sequence-to-sequence model generates the entire output sequence CP using the specified dimensional state of detection RNN at the end of the input sequence P. This limits the amount of information and calculations that can flow into the generative model.The attention model of [5] improves this problem by adding an additional neural network to the encoder and decoder RNNNs, which employs an attention mechanism across the entire sequence of encoder RNN states. For notation purposes, let us define the encoder and decoder the hidden states as (e1,., en) or (d1,., dm (P)).For the LSTM-RNNNNNs, we use the state after the output gate has been multiplied component-wise by the cell activations. We calculate the attention vector for each output time as follows Wej + WiT = 2j."}, {"heading": "2.3 Ptr-Net", "text": "We will now describe a very simple modification of the attention model that allows us to apply the method to solve combinatorial optimization problems where the size of the output dictionary depends on the number of elements in the input sequence. However, the sequence-to-sequence model of Section 2.1 uses a soft-max distribution over a rigid output dictionary to calculate p (Ci | C1,.., Ci \u2212 1, P) in Equation 1. Therefore, it cannot be used for our problems where the size of the output dictionary is equal to the length of the input sequence. To solve this problem, we model p (Ci | C1,.., Ci \u2212 1, P) using the attention mechanism of Equation 3 as follows: uij = v T tanh (W1ej + W2di) j (1,., n) p (Ci | maxima), we draw the attention to the maximum (W2fti) length."}, {"heading": "3 Motivation and Datasets Structure", "text": "In the following sections we look at each of the three problems we have considered, as well as our data generation protocol. In the training data, the inputs are planar point sets P = {P1,.., Pn} with n elements each, where Pj = (xj, yj) are the cartesian coordinates of the points over which we find the convex hull, the Delaunay triangulation or the solution to the corresponding Travelling Salesman problem. In all cases, we derive an even distribution in [0, 1] \u00d7 [0, 1]. The outputs CP = {C1,.., Cm (P)} are sequences that represent the solution associated with the P point series. In Figure 2, we find an input-output pair (P, CP) for the convex hull and the Delaunay problems."}, {"heading": "3.1 Convex Hull", "text": "We used this example as a starting point to develop our models and understand the difficulty of solving combinatorial problems with data-driven approaches. Finding the convex hull of a finite number of points is a well-understood problem in computational geometry, and there are several exact solutions available (see [13, 14, 15]). Generally, these solutions have a time complexity of O (n log n), where n is the number of points to be factored.The vectors Pj are uniformly sampled from [0, 1] \u00d7 [0, 1]. (a) The elements Ci are indexes between 1 and n that correspond to positions in the sequence P, or special marks that represent the beginning or end of the sequence. See Figure 2 (a) for an illustration. (1) We will release all data sets hidden for reference. (a) Input P = {P1,..,., P10,}, and the output sequence {2 (a) for an illustration. (1) We will release all data sets hidden for reference. (a) Input P = {P1,..,., P10, and the output sequence {4, 4, Cullsequence, {7, 4, 4, P4, P4, 7, P4, P4, P4, P4, {, 7, 4, 4, P4, P4, P4, P4, P4."}, {"heading": "3.2 Delaunay Triangulation", "text": "A Delaunay triangulation for a set value of points in a plane is a triangulation, so that each circle of each triangle is empty, i.e. there is no point of P inside it. Exact O (n log n) solutions are available [16], where n is the number of points in P. In this example, the outputs CP = {C1,..., Cm (P)} are the corresponding sequences representing the triangulation of point P. Each Ci is a triangle of integer numbers from 1 to n, corresponding to the position of triangles in P or the beginning / end of sequence markers. See Figure 2 (b). We note that each permutation of the sequence CP represents the same triangulation for P, in addition, each triangular representation of Ci can be permutated by three integer numbers."}, {"heading": "3.3 Travelling Salesman Problem (TSP)", "text": "TSP arises in many areas of theoretical informatics and is an important algorithm used for microchip design or DNA sequencing. In our work, we focused on the planar symmetrical TSP: facing a list of cities, we want to find the shortest possible path that visits each city exactly once and returns to the starting point. Furthermore, we assume that the distance between two cities is equal in each opposite direction. It is a NP-hard problem that allows us to randomly test the capabilities and limitations of our model. The input-output pairs (P, CP) have a format similar to the convex Hull problem described in Section 3.1. P will be the cartesian coordinates that represent the cities randomly selected in the [0, 1] \u00d7 [0, 1] square. CP = {C1,.., Cn} will be a permutation of integrators from 1 to n representing the optimal path (tour)."}, {"heading": "4 Empirical Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Architecture and Hyperparameters", "text": "The work presented here did not involve an extensive search for the architecture or hyperparameters of the Ptr network, and we used practically the same architecture for all experiments and data sets. Although some advantages were likely to be achieved by matching the model, we felt that using the same hyperparameters for all problems would be the main message of the paper. As a result, all our models used a single-layer LSTM with 256 or 512 hidden units, trained with stochastic gradient decrease with a learning rate of 1.0, batch size of 128, random initialization of a uniform weight of -0.08 to 0.08, and L2 gradient cutout of 2.0. We generated 1 million training sample pairs, and we observed matching in some cases where the task was easier (i.e. for small n)."}, {"heading": "4.2 Convex Hull", "text": "We used the convex hull as the guiding principle, which enabled us to understand the deficiencies of standard models such as the sequence-to-sequence approach, as well as our expectations of what a purely data-driven model could achieve in terms of an exact solution. We reported on two metrics: accuracy and area of the true convex hull (note that each simple polygon has full intersections with the true convex hull). To calculate accuracy, we considered two initial sequences to be equal when they represent the same polygon. For simplicity, we calculated the area coverage for the test examples in which production represents a single polygon (i.e., without self-overlapping). If an algorithm is unable to produce a simple polygon, we will represent the same polygon."}, {"heading": "4.3 Delaunay Triangulation", "text": "The Delaunay triangulation test is related to our first problem of finding the convex hull. In fact, for a given set of points, the Delaunay triangulation triangulates the convex hull of these points. We reported on two measures: accuracy and triangular coverage in percent (the percentage of triangles correctly predicted by the model). Note that in this case, the initial sequence C (P) is actually a set. As a result, each permutation of its elements represents the same triangulation. With the Ptr-Net model for n = 5, we achieved an accuracy of 80.7% and a triangular coverage of 93.0%. For n = 10, the accuracy was 22.6% and the triangular coverage 81.3%. For n = 50, we did not produce a precise correct triangulation, but a triangular coverage of 52.8%. See the mean column of Figure 3 of an example of 50."}, {"heading": "4.4 Travelling Salesman Problem", "text": "However, given that PtrNet implements an O (n2) algorithm, it was unclear whether it would have sufficient capacity to learn a useful algorithm solely from data. As discussed in Section 3.3, it is possible to generate accurate solutions for relatively small values of n used as training data. For larger n, due to the importance of TSP, good and efficient algorithms exist that provide reasonable approximate solutions. We used three different algorithms in our experiments - A1, A2 and A3 (see Section 3.3 for references). Table 2 shows all of our results on TSP. The number reported is the length of the proposed tour, as opposed to the convex hulls and delaunay triangulation cases where the decoder was unrestricted."}, {"heading": "5 Conclusions", "text": "In this article, we described Ptr-Net, a new architecture that allows us to learn the conditional probability of a sequence of CP in another sequence P, where CP is a sequence of discrete tokens corresponding to positions in P. We show that Ptr networks can be used to learn solutions to three different combinatorial optimization problems. Our method works on different sized input factors (which lead to different sized output dictionaries), which the base models (sequence-to-sequence with or without attention) cannot provide directly. What is even more impressive is that they exceed the baseline for fixed input size problems - to which both models can be applied. Our model refers to attention models previously proposed in [2, 5] and has strong connections to memory networks [6] that use content-based attention. We use these mechanisms to select input outputs and thus open up a new class of networks that can be applied to neural problems."}, {"heading": "Acknowledgments", "text": "We thank Rafal Jozefowicz, Ilya Sutskever, Quoc Le and Samy Bengio for useful discussions on this topic and Daniel Gillick for his help in preparing the final manuscript."}], "references": [{"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Learning internal representations by error propagation", "author": ["David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams"], "venue": "Technical report, DTIC Document,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1985}, {"title": "An application of recurrent nets to phone probability estimation", "author": ["Anthony J Robinson"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1994}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "ICLR", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "arXiv preprint arXiv:1412.7449,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In CVPR 2015, arXiv preprint arXiv:1411.4555,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Jeff Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1411.4389,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Learning to execute", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1410.4615,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1997}, {"title": "Unsupervised learning of video representations using lstms", "author": ["Nitish Srivastava", "Elman Mansimov", "Ruslan Salakhutdinov"], "venue": "ICML", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "On the identification of the convex hull of a finite set of points in the plane", "author": ["Ray A Jarvis"], "venue": "Information Processing Letters,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1973}, {"title": "An efficient algorith for determining the convex hull of a finite planar set", "author": ["Ronald L. Graham"], "venue": "Information processing letters,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1972}, {"title": "Convex hulls of finite sets of points in two and three dimensions", "author": ["Franco P. Preparata", "Se June Hong"], "venue": "Communications of the ACM,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1977}, {"title": "Efficient unstructured mesh generation by means of delaunay triangulation and bowyer-watson algorithm", "author": ["S1 Rebay"], "venue": "Journal of computational physics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1993}, {"title": "Dynamic programming treatment of the travelling salesman problem", "author": ["Richard Bellman"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1962}], "referenceMentions": [{"referenceID": 0, "context": "Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence [1] and Neural Turing Machines [2], because the number of target classes in each step of the output depends on the length of the input, which is variable.", "startOffset": 96, "endOffset": 99}, {"referenceID": 1, "context": "Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence [1] and Neural Turing Machines [2], because the number of target classes in each step of the output depends on the length of the input, which is variable.", "startOffset": 127, "endOffset": 130}, {"referenceID": 2, "context": "Recurrent Neural Networks (RNNs) have been used for learning functions over sequences from examples for more than three decades [3].", "startOffset": 128, "endOffset": 131}, {"referenceID": 3, "context": "[4]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "The recently introduced sequence-to-sequence paradigm [1] removed these constraints by using one RNN to map an input sequence to an embedding and another (possibly the same) RNN to map the embedding to an output sequence.", "startOffset": 54, "endOffset": 57}, {"referenceID": 4, "context": "augmented the decoder by propagating extra contextual information from the input using a content-based attentional mechanism [5, 2, 6].", "startOffset": 125, "endOffset": 134}, {"referenceID": 1, "context": "augmented the decoder by propagating extra contextual information from the input using a content-based attentional mechanism [5, 2, 6].", "startOffset": 125, "endOffset": 134}, {"referenceID": 0, "context": "These developments have made it possible to apply RNNs to new domains, achieving state-of-the-art results in core problems in natural language processing such as translation [1, 5] and parsing [7], image and video captioning [8, 9], and even learning to execute small programs [2, 10].", "startOffset": 174, "endOffset": 180}, {"referenceID": 4, "context": "These developments have made it possible to apply RNNs to new domains, achieving state-of-the-art results in core problems in natural language processing such as translation [1, 5] and parsing [7], image and video captioning [8, 9], and even learning to execute small programs [2, 10].", "startOffset": 174, "endOffset": 180}, {"referenceID": 5, "context": "These developments have made it possible to apply RNNs to new domains, achieving state-of-the-art results in core problems in natural language processing such as translation [1, 5] and parsing [7], image and video captioning [8, 9], and even learning to execute small programs [2, 10].", "startOffset": 193, "endOffset": 196}, {"referenceID": 6, "context": "These developments have made it possible to apply RNNs to new domains, achieving state-of-the-art results in core problems in natural language processing such as translation [1, 5] and parsing [7], image and video captioning [8, 9], and even learning to execute small programs [2, 10].", "startOffset": 225, "endOffset": 231}, {"referenceID": 7, "context": "These developments have made it possible to apply RNNs to new domains, achieving state-of-the-art results in core problems in natural language processing such as translation [1, 5] and parsing [7], image and video captioning [8, 9], and even learning to execute small programs [2, 10].", "startOffset": 225, "endOffset": 231}, {"referenceID": 1, "context": "These developments have made it possible to apply RNNs to new domains, achieving state-of-the-art results in core problems in natural language processing such as translation [1, 5] and parsing [7], image and video captioning [8, 9], and even learning to execute small programs [2, 10].", "startOffset": 277, "endOffset": 284}, {"referenceID": 8, "context": "These developments have made it possible to apply RNNs to new domains, achieving state-of-the-art results in core problems in natural language processing such as translation [1, 5] and parsing [7], image and video captioning [8, 9], and even learning to execute small programs [2, 10].", "startOffset": 277, "endOffset": 284}, {"referenceID": 4, "context": "In this paper, we address this limitation by repurposing the attention mechanism of [5] to create pointers to input elements.", "startOffset": 84, "endOffset": 87}, {"referenceID": 0, "context": "The output dimensionality is fixed by the dimensionality of the problem and it is the same during training and inference [1].", "startOffset": 121, "endOffset": 124}, {"referenceID": 4, "context": "At each step, the generating network produces a vector that modulates a content-based attention mechanism over inputs ([5, 2]).", "startOffset": 119, "endOffset": 125}, {"referenceID": 1, "context": "At each step, the generating network produces a vector that modulates a content-based attention mechanism over inputs ([5, 2]).", "startOffset": 119, "endOffset": 125}, {"referenceID": 0, "context": "We review the sequence-to-sequence [1] and input-attention models [5] that are the baselines for this work in Sections 2.", "startOffset": 35, "endOffset": 38}, {"referenceID": 4, "context": "We review the sequence-to-sequence [1] and input-attention models [5] that are the baselines for this work in Sections 2.", "startOffset": 66, "endOffset": 69}, {"referenceID": 0, "context": "As in [1], we use an Long Short Term Memory (LSTM) [11] to model p\u03b8(Ci|C1, .", "startOffset": 6, "endOffset": 9}, {"referenceID": 9, "context": "As in [1], we use an Long Short Term Memory (LSTM) [11] to model p\u03b8(Ci|C1, .", "startOffset": 51, "endOffset": 55}, {"referenceID": 4, "context": "The attention model of [5] ameliorates this problem by augmenting the encoder and decoder RNNs with an additional neural network that uses an attention mechanism over the entire sequence of encoder RNN states.", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "Both our method and the attention model can be seen as an application of content-based attention mechanisms proposed in [6, 5, 2].", "startOffset": 120, "endOffset": 129}, {"referenceID": 1, "context": "Both our method and the attention model can be seen as an application of content-based attention mechanisms proposed in [6, 5, 2].", "startOffset": 120, "endOffset": 129}, {"referenceID": 10, "context": "Without the constraints, the predictions are bound to become blurry over longer sequences as shown in sequence-to-sequence models for videos [12].", "startOffset": 141, "endOffset": 145}, {"referenceID": 0, "context": "In all cases, we sample from a uniform distribution in [0, 1] \u00d7 [0, 1].", "startOffset": 55, "endOffset": 61}, {"referenceID": 0, "context": "In all cases, we sample from a uniform distribution in [0, 1] \u00d7 [0, 1].", "startOffset": 64, "endOffset": 70}, {"referenceID": 11, "context": "Finding the convex hull of a finite number of points is a well understood task in computational geometry, and there are several exact solutions available (see [13, 14, 15]).", "startOffset": 159, "endOffset": 171}, {"referenceID": 12, "context": "Finding the convex hull of a finite number of points is a well understood task in computational geometry, and there are several exact solutions available (see [13, 14, 15]).", "startOffset": 159, "endOffset": 171}, {"referenceID": 13, "context": "Finding the convex hull of a finite number of points is a well understood task in computational geometry, and there are several exact solutions available (see [13, 14, 15]).", "startOffset": 159, "endOffset": 171}, {"referenceID": 0, "context": "The vectors Pj are uniformly sampled from [0, 1] \u00d7 [0, 1].", "startOffset": 42, "endOffset": 48}, {"referenceID": 0, "context": "The vectors Pj are uniformly sampled from [0, 1] \u00d7 [0, 1].", "startOffset": 51, "endOffset": 57}, {"referenceID": 14, "context": "Exact O(n log n) solutions are available [16], where n is the number of points in P .", "startOffset": 41, "endOffset": 45}, {"referenceID": 0, "context": "P will be the cartesian coordinates representing the cities, which are chosen randomly in the [0, 1] \u00d7 [0, 1] square.", "startOffset": 94, "endOffset": 100}, {"referenceID": 0, "context": "P will be the cartesian coordinates representing the cities, which are chosen randomly in the [0, 1] \u00d7 [0, 1] square.", "startOffset": 103, "endOffset": 109}, {"referenceID": 15, "context": "To generate exact data, we implemented the Held-Karp algorithm [17] which finds the optimal solution in O(2n) (we used it up to n = 20).", "startOffset": 63, "endOffset": 67}, {"referenceID": 0, "context": "LSTM [1] 50 50 1.", "startOffset": 5, "endOffset": 8}, {"referenceID": 4, "context": "9% FAIL +ATTENTION [5] 50 50 38.", "startOffset": 19, "endOffset": 22}, {"referenceID": 0, "context": "9% LSTM [1] 5 5 87.", "startOffset": 8, "endOffset": 11}, {"referenceID": 0, "context": "6% LSTM [1] 10 10 29.", "startOffset": 8, "endOffset": 11}, {"referenceID": 1, "context": "Our model draws inspiration from attention models proposed previously in [2, 5] and has strong connections to memory networks [6] that use content based attention.", "startOffset": 73, "endOffset": 79}, {"referenceID": 4, "context": "Our model draws inspiration from attention models proposed previously in [2, 5] and has strong connections to memory networks [6] that use content based attention.", "startOffset": 73, "endOffset": 79}], "year": 2015, "abstractText": "We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence [1] and Neural Turing Machines [2], because the number of target classes in each step of the output depends on the length of the input, which is variable. Problems such as sorting variable sized sequences, and various combinatorial optimization problems belong to this class. Our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention. It differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output. We call this architecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn approximate solutions to three challenging geometric problems \u2013 finding planar convex hulls, computing Delaunay triangulations, and the planar Travelling Salesman Problem \u2013 using training examples alone. Ptr-Nets not only improve over sequence-to-sequence with input attention, but also allow us to generalize to variable size output dictionaries. We show that the learnt models generalize beyond the maximum lengths they were trained on. We hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems.", "creator": "LaTeX with hyperref package"}}}