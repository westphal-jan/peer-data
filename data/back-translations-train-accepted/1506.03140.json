{"id": "1506.03140", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2015", "title": "On-the-Job Learning with Bayesian Decision Theory", "abstract": "How can we deploy a high-accuracy system starting with zero training examples? We consider an \"on-the-job\" setting, where as inputs arrive, we use crowdsourcing to resolve uncertainty where needed and output our prediction when confident. As the model improves over time, the reliance on crowdsourcing queries decreases. We cast our setting as a stochastic game based on Bayesian decision theory, which allows us to balance latency, cost, and accuracy objectives in a principled way. Computing the optimal policy is intractable, so we develop an approximation based on Monte Carlo Tree Search. We tested our approach across three datasets---named-entity recognition, sentiment classification, and image classification. On the NER task we obtained a 6--7 fold reduction in cost compared to full human annotation. We also achieve a 17% F$_1$ improvement over having a single human label the whole set, and a 28% F$_1$ improvement over online learning.", "histories": [["v1", "Wed, 10 Jun 2015 00:40:34 GMT  (2894kb,D)", "https://arxiv.org/abs/1506.03140v1", null], ["v2", "Mon, 7 Dec 2015 21:44:07 GMT  (1717kb,D)", "http://arxiv.org/abs/1506.03140v2", "As appearing in NIPS 2015"]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["keenon werling", "arun tejasvi chaganty", "percy liang", "christopher d manning"], "accepted": true, "id": "1506.03140"}, "pdf": {"name": "1506.03140.pdf", "metadata": {"source": "CRF", "title": "On-the-Job Learning with Bayesian Decision Theory", "authors": ["Keenon Werling", "Arun Chaganty", "Christopher D. Manning"], "emails": ["keenon@cs.stanford.edu", "chaganty@cs.stanford.edu", "pliang@cs.stanford.edu", "manning@cs.stanford.edu"], "sections": [{"heading": null, "text": "Our goal is to deploy a high-precision system that starts with zero training examples. We look at a workplace environment where, when input arrives, we use real-time crowdsourcing to eliminate uncertainty where it is needed, and make our predictions when we are safe. As the model improves over time, the reliance on crowdsourcing queries decreases. We have designed our setting as a stochastic game based on Bayean decision theory that allows us to balance latency, cost, and accuracy goals in principle. Calculating the optimal guidelines is insoluble, so we are developing an approach based on Monte Carlo tree search. We tested our approach using three sets of data - entity detection, mood classification, and image classification. In the NER task, we achieved more than an order of magnitude of cost reduction compared to full human annotation, while increasing performance compared to the labels provided by the experts."}, {"heading": "1 Introduction", "text": "There are two ways to create a precise AI system today: (i) collect an enormous amount of labeled training data [1] and engage in supervised learning [2]; or (ii) use crowdsourcing to perform the task directly [3, 4]. However, both solutions do not require trivial amounts of time and money. In many situations, one would like a new system - e.g., extracting Twitter information [5] to assist in disaster relief efforts or monitor public opinion - but simply lack the resources to follow either the pure ML or the pure crowdsourcing path. In this paper, we propose a framework that targets (formalizes and expands) workplace learning ideas that are first implemented by producing high quality without requiring a trained model. When a new input arrives, we can choose to asynchronously question the amount of input."}, {"heading": "2 Problem formulation", "text": "Consider a structured prediction problem from the input x = (x1,.., xn) to the output y = (y1,.., yn). Unlike NONE LOCATION LOCATION on tweets, x is a sequence of words in the tweet (e.g. \"on George str.\") and y is the corresponding sequence of labels (e.g. NONE LOCATION LOCATION). The complete set of labels from PERSON, LOCATION, RESOURCE, and NONE. In the workplace learning environment, inputs get into a stream. With each input x, we make zero or more queries q1, q2,. on the crowd to get labels (potentially more than once) for each position in x. The answers r1, r2,.. come back asynchronously, which are integrated into our current prediction model."}, {"heading": "3 Model", "text": "The game begins with the system receiving input and ends when the system transforms into a series of labels, which are then labeled as follows: \"The system can also select the wait action.\" \"The system can also wait for the set to respond to a pending query or return action (\" q = q \") in order to exit the game and return the answers it has received to date.\" \"The system can make as many queries in a row (\" q \") as it wants before responding to an upcoming query or return action (\" q = R \") in order to exit the game and return its predictions.\" \"The system can make as many queries in a row (\" i.e.) as it wants before waiting or turning. \"\" If the wait action is selected, the system switches to the set, which provides an answer to an upcoming query, and shifts the game clock by the time it wants the set to respond to the system immediately. \""}, {"heading": "4 Game playing", "text": "Our algorithm (algorithm 1) combines ideas from tree search in Monte Carlo [12] to systematically explore the state space and the progressive expansion [?] in order to deal with the challenge of continuous variables (time). Some intuition about the algorithm is provided below. In the simulation of the system state, the next state (and thus the action) is selected using the supreme confidence tree rule (UCT), which compares the value of the next state (exploitation) with the number of visits (exploration). In the simulation of the system state, the next state (and thus the action) is chosen when the next state (UCT) decides to offset the value of the next state (exploitation) with the number of visits (exploration)."}, {"heading": "5 Experiments", "text": "In fact, it is the case that most people are able to survive themselves by blaming themselves and others. (...) In fact, it is the case that most people are able to survive themselves. (...) It is not the case that they are able to survive themselves. (...) It is not the case that they are able to survive themselves. (...) It is not the case that they are able to survive themselves. (...) It is the case that they are able to survive themselves. (...) It is the case that they are able to survive themselves. (...) (...) (...) (...) (...) () (...) () (...) () () ()) Most people are able to survive themselves. (...) (...) () (...) () () (...) () () (...) () () (...) () () ())"}, {"heading": "6 Related Work", "text": "The basic premise of online learning is that algorithms get better over time, and there is a wealth of work in this area [7]. In our environment, however, algorithms are not only improved over time, but also maintained to a high degree of accuracy. Online active learning leads to active learning in the online environment. Active learning (see [19] for a survey) leads to most informative examples of building a classification system. Online active learning leads to active learning in the environment."}, {"heading": "7 Conclusion", "text": "We have introduced a new framework that learns from (noisy) crowds in the workplace to maintain high accuracy and significantly reduce costs over time. The technical core of our approach is to model the workplace environment as a stochastic game and use ideas from the game to approximate the optimal policy. We have developed a system, LENSE, that achieves significant cost reductions over a pure crowd approach and significant accuracy improvements over a pure ML approach."}], "references": [{"title": "ImageNet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L. Li", "K. Li", "L. Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Soylent: a word processor with a crowd inside", "author": ["M.S. Bernstein", "G. Little", "R.C. Miller", "B. Hartmann", "M.S. Ackerman", "D.R. Karger", "D. Crowell", "K. Panovich"], "venue": "In Symposium on User Interface Software and Technology,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Emailvalet: Managing email overload through private, accountable crowdsourcing", "author": ["N. Kokkalis", "T. K\u00f6hn", "C. Pfeiffer", "D. Chornyi", "M.S. Bernstein", "S.R. Klemmer"], "venue": "In Conference on Computer Supported Cooperative Work,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Twiner: named entity recognition in targeted twitter stream", "author": ["C. Li", "J. Weng", "Q. He", "Y. Yao", "A. Datta", "A. Sun", "B. Lee"], "venue": "In ACM Special Interest Group on Information Retreival (SIGIR),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Real-time crowd labeling for deployable activity recognition", "author": ["Walter S Lasecki", "Young Chol Song", "Henry Kautz", "Jeffrey P Bigham"], "venue": "In Proceedings of the 2013 conference on Computer supported cooperative work,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Prediction, learning, and games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Some label efficient learning results", "author": ["D. Helmbold", "S. Panizza"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Online active learning methods for fast label-efficient spam filtering", "author": ["D. Sculley"], "venue": "In Conference on Email and Anti-spam (CEAS),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Unbiased online active learning in data streams", "author": ["W. Chu", "M. Zinkevich", "L. Li", "A. Thomas", "B. Tseng"], "venue": "In International Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Active classification based on value of classifier", "author": ["T. Gao", "D. Koller"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Bandit based Monte-Carlo planning", "author": ["L. Kocsis", "C. Szepesv\u00e1ri"], "venue": "In European Conference on Machine Learning (ECML),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Regret minimization under partial monitoring", "author": ["N. Cesa-Bianchi", "G. Lugosi", "G. Stoltz"], "venue": "Mathematics of Operations Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Incorporating non-local information into information extraction systems by Gibbs sampling", "author": ["J.R. Finkel", "T. Grenager", "C. Manning"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Learning word vectors for sentiment analysis", "author": ["Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "A.Y. Ng", "C. Potts"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Attribute and Simile Classifiers for Face Verification", "author": ["N. Kumar", "A.C. Berg", "P.N. Belhumeur", "S.K. Nayar"], "venue": "In IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Crowds in two seconds: Enabling realtime crowd-powered interfaces", "author": ["M.S. Bernstein", "J. Brandt", "R.C. Miller", "D.R. Karger"], "venue": "In User Interface Software and Technology,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Active learning literature survey", "author": ["B. Settles"], "venue": "Technical report, University of Wisconsin,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Proactive learning: cost-sensitive active learning with multiple imperfect oracles", "author": ["P. Donmez", "J.G. Carbonell"], "venue": "In Conference on Information and Knowledge Management (CIKM),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Near-optimal Bayesian active learning with noisy observations", "author": ["D. Golovin", "A. Krause", "D. Ray"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Active learning from crowds", "author": ["Y. Yan", "G.M. Fung", "R. Rosales", "J.G. Dy"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Large-scale live active learning: Training object detectors with crawled data and crowds", "author": ["Sudheendra Vijayanarasimhan", "Kristen Grauman"], "venue": "International Journal of Computer Vision,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Learning cost-sensitive active classifiers", "author": ["R. Greiner", "A.J. Grove", "D. Roth"], "venue": "Artificial Intelligence,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2002}, {"title": "Test-cost sensitive naive Bayes classification", "author": ["X. Chai", "L. Deng", "Q. Yang", "C.X. Ling"], "venue": "In International Conference on Data Mining,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2004}, {"title": "Anytime induction of cost-sensitive trees", "author": ["S. Esmeir", "S. Markovitch"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "Flock: Hybrid Crowd-Machine learning classifiers", "author": ["J. Cheng", "M.S. Bernstein"], "venue": "In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Decision-theoretic control of crowd-sourced workflows", "author": ["P. Dai", "Mausam", "D.S. Weld"], "venue": "In Association for the Advancement of Artificial Intelligence (AAAI),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Learning from measurements in exponential families", "author": ["P. Liang", "M.I. Jordan", "D. Klein"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "Combining distant and partial supervision for relation extraction", "author": ["G. Angeli", "J. Tibshirani", "J.Y. Wu", "C.D. Manning"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "There are two roads to an accurate AI system today: (i) gather a huge amount of labeled training data [1] and do supervised learning [2]; or (ii) use crowdsourcing to directly perform the task [3, 4].", "startOffset": 102, "endOffset": 105}, {"referenceID": 1, "context": "There are two roads to an accurate AI system today: (i) gather a huge amount of labeled training data [1] and do supervised learning [2]; or (ii) use crowdsourcing to directly perform the task [3, 4].", "startOffset": 133, "endOffset": 136}, {"referenceID": 2, "context": "There are two roads to an accurate AI system today: (i) gather a huge amount of labeled training data [1] and do supervised learning [2]; or (ii) use crowdsourcing to directly perform the task [3, 4].", "startOffset": 193, "endOffset": 199}, {"referenceID": 3, "context": "There are two roads to an accurate AI system today: (i) gather a huge amount of labeled training data [1] and do supervised learning [2]; or (ii) use crowdsourcing to directly perform the task [3, 4].", "startOffset": 193, "endOffset": 199}, {"referenceID": 4, "context": ", to do Twitter information extraction [5] to aid in disaster relief efforts or monitor public opinion \u2014 but one simply lacks the resources to follow either the pure ML or pure crowdsourcing road.", "startOffset": 39, "endOffset": 42}, {"referenceID": 5, "context": "In this paper, we propose a framework called on-the-job learning (formalizing and extending ideas first implemented in [6]), in which we produce high quality results from the start without requiring a trained model.", "startOffset": 119, "endOffset": 122}, {"referenceID": 6, "context": "Online learning [7] and online active learning [8, 9, 10] are different in that they do not actively seek new information prior to making a prediction, and cannot maintain high accuracy independent of the number of data instances seen so far.", "startOffset": 16, "endOffset": 19}, {"referenceID": 7, "context": "Online learning [7] and online active learning [8, 9, 10] are different in that they do not actively seek new information prior to making a prediction, and cannot maintain high accuracy independent of the number of data instances seen so far.", "startOffset": 47, "endOffset": 57}, {"referenceID": 8, "context": "Online learning [7] and online active learning [8, 9, 10] are different in that they do not actively seek new information prior to making a prediction, and cannot maintain high accuracy independent of the number of data instances seen so far.", "startOffset": 47, "endOffset": 57}, {"referenceID": 9, "context": "Online learning [7] and online active learning [8, 9, 10] are different in that they do not actively seek new information prior to making a prediction, and cannot maintain high accuracy independent of the number of data instances seen so far.", "startOffset": 47, "endOffset": 57}, {"referenceID": 10, "context": "Active classification [11], like us,", "startOffset": 22, "endOffset": 26}, {"referenceID": 11, "context": "Computing the optimal policy is intractable, so we develop an approximation based on Monte Carlo tree search [12] and progressive widening to reason about continuous time [?].", "startOffset": 109, "endOffset": 113}, {"referenceID": 12, "context": "observed\u2014the only feedback is via the responses, like in partial monitoring games [13].", "startOffset": 82, "endOffset": 86}, {"referenceID": 11, "context": "Our algorithm (Algorithm 1) combines ideas from Monte Carlo tree search [12] to systematically explore the state space and progressive widening [?] to deal with the challenge of continuous variables (time).", "startOffset": 72, "endOffset": 76}, {"referenceID": 13, "context": "We used standard features [14]: the current word, current lemma, previous and next lemmas, lemmas in a window of size three to the left and right, word shape and word prefix and suffixes, as well as word embeddings.", "startOffset": 26, "endOffset": 30}, {"referenceID": 14, "context": "Sentiment (1800) We evaluate on a subset of the IMDB sentiment dataset [15] that consists of 2000 polar movie reviews; the goal is binary classification of documents into classes POS and NEG.", "startOffset": 71, "endOffset": 75}, {"referenceID": 15, "context": "We used two feature sets, the first (UNIGRAMS) containing only word unigrams, and the second (RNN) that also contains sentence vector embeddings from [16].", "startOffset": 150, "endOffset": 154}, {"referenceID": 16, "context": "Face (1784) We evaluate on a celebrity face classification task [17].", "startOffset": 64, "endOffset": 68}, {"referenceID": 1, "context": "We used the last layer of a 11layer AlexNet [2] trained on ImageNet as input feature embeddings, though we leave back-propagating into the net to future work.", "startOffset": 44, "endOffset": 47}, {"referenceID": 17, "context": "We implemented the retainer model of [18] on Amazon Mechanical Turk to create a \u201cpool\u201d of crowd workers that could respond to queries in real-time.", "startOffset": 37, "endOffset": 41}, {"referenceID": 6, "context": "The fundamental premise of online learning is that algorithms should improve with time, and there is a rich body of work in this area [7].", "startOffset": 134, "endOffset": 137}, {"referenceID": 18, "context": "Active learning (see [19] for a survey) algorithms strategically select most informative examples to build a classifier.", "startOffset": 21, "endOffset": 25}, {"referenceID": 7, "context": "Online active learning [8, 9, 10] performs active learning in the online setting.", "startOffset": 23, "endOffset": 33}, {"referenceID": 8, "context": "Online active learning [8, 9, 10] performs active learning in the online setting.", "startOffset": 23, "endOffset": 33}, {"referenceID": 9, "context": "Online active learning [8, 9, 10] performs active learning in the online setting.", "startOffset": 23, "endOffset": 33}, {"referenceID": 19, "context": "Several authors have also considered using crowd workers as a noisy oracle [20, 21, 22, 23].", "startOffset": 75, "endOffset": 91}, {"referenceID": 20, "context": "Several authors have also considered using crowd workers as a noisy oracle [20, 21, 22, 23].", "startOffset": 75, "endOffset": 91}, {"referenceID": 21, "context": "Several authors have also considered using crowd workers as a noisy oracle [20, 21, 22, 23].", "startOffset": 75, "endOffset": 91}, {"referenceID": 22, "context": "Several authors have also considered using crowd workers as a noisy oracle [20, 21, 22, 23].", "startOffset": 75, "endOffset": 91}, {"referenceID": 23, "context": "Active classification [24, 25, 26] asks what are the most informative features to measure at test time.", "startOffset": 22, "endOffset": 34}, {"referenceID": 24, "context": "Active classification [24, 25, 26] asks what are the most informative features to measure at test time.", "startOffset": 22, "endOffset": 34}, {"referenceID": 25, "context": "Active classification [24, 25, 26] asks what are the most informative features to measure at test time.", "startOffset": 22, "endOffset": 34}, {"referenceID": 5, "context": "A notable exception is Legion:AR [6], which like us operates in on-the-job learning setting to for real-time activity classification.", "startOffset": 33, "endOffset": 36}, {"referenceID": 26, "context": "One example is Flock [27], which first crowdsources the identification of features for an image classification task, and then asks the crowd to annotate these features so it can learn a decision tree.", "startOffset": 21, "endOffset": 25}, {"referenceID": 27, "context": "In another line of work, TurKontrol [28] models individual crowd worker reliability to optimize the number of human votes needed to achieve confident consensus using a POMDP.", "startOffset": 36, "endOffset": 40}, {"referenceID": 28, "context": "Making active partial observations on structures and has been explored in the measurements framework of [29] and in the distant supervision setting [30].", "startOffset": 104, "endOffset": 108}, {"referenceID": 29, "context": "Making active partial observations on structures and has been explored in the measurements framework of [29] and in the distant supervision setting [30].", "startOffset": 148, "endOffset": 152}], "year": 2015, "abstractText": "Our goal is to deploy a high-accuracy system starting with zero training examples. We consider an on-the-job setting, where as inputs arrive, we use real-time crowdsourcing to resolve uncertainty where needed and output our prediction when confident. As the model improves over time, the reliance on crowdsourcing queries decreases. We cast our setting as a stochastic game based on Bayesian decision theory, which allows us to balance latency, cost, and accuracy objectives in a principled way. Computing the optimal policy is intractable, so we develop an approximation based on Monte Carlo Tree Search. We tested our approach on three datasets\u2014named-entity recognition, sentiment classification, and image classification. On the NER task we obtained more than an order of magnitude reduction in cost compared to full human annotation, while boosting performance relative to the expert provided labels. We also achieve a 8% F1 improvement over having a single human label the whole set, and a 28% F1 improvement over online learning. \u201cPoor is the pupil who does not surpass his master.\u201d \u2013 Leonardo da Vinci", "creator": "LaTeX with hyperref package"}}}