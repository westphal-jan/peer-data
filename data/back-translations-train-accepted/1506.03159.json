{"id": "1506.03159", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2015", "title": "Copula variational inference", "abstract": "We develop a general methodology for variational inference which preserves dependency among the latent variables. This is done by augmenting the families of distributions used in mean-field and structured approximation with copulas. Copulas allow one to separately model the dependency given a factorization of the variational distribution, and can guarantee us better approximations to the posterior as measured by KL divergence. We show that inference on the augmented distribution is highly scalable using stochastic optimization. Furthermore, the addition of a copula is generic and can be applied straightforwardly to any inference procedure using the original mean-field or structured approach. This reduces bias, sensitivity to local optima, sensitivity to hyperparameters, and significantly helps characterize and interpret the dependency among the latent variables.", "histories": [["v1", "Wed, 10 Jun 2015 04:14:22 GMT  (573kb,D)", "http://arxiv.org/abs/1506.03159v1", null], ["v2", "Sat, 31 Oct 2015 06:52:07 GMT  (998kb,D)", "http://arxiv.org/abs/1506.03159v2", "Appears in Neural Information Processing Systems, 2015"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG stat.CO stat.ME", "authors": ["dustin tran", "david m blei", "edoardo m airoldi"], "accepted": true, "id": "1506.03159"}, "pdf": {"name": "1506.03159.pdf", "metadata": {"source": "CRF", "title": "Variational inference with copula augmentation", "authors": ["Dustin Tran", "David M. Blei", "Edoardo M. Airoldi"], "emails": [], "sections": [{"heading": null, "text": "We develop a general methodology for variation conclusions that maintains dependence between latent variables by extending the distribution families in the center and structured approximation with copulas. Copulas allow us to model dependence separately by factorizing the variation distribution, and can guarantee better approximations to the back end, as measured by KL divergence. We show that the inference is highly scalable to the augmented distribution using stochastic optimization. In addition, the addition of a copula is generic and can easily be applied to any inference method that uses the original center or structured approach, reducing bias, sensitivity to local optimizations, sensitivity to hyperparameters, and significantly contributing to characterizing and interpreting dependence between the latent variables. Keywords: Bayesian inference, Variation Conclusion, Copulas, Xistoza, Xi.06v-0.3, Optistostic Network."}, {"heading": "1 Introduction 3", "text": "1.1.............................................."}, {"heading": "2 Background 4", "text": "............................................. 42.2 Copulas......................................... 52.2.1 Copulas....................................."}, {"heading": "3 Copula variational inference 8", "text": "3.1 Sample from the copula-augmented variation distribution...... 103.2 Calculation of the slope of the copula-augmented variation distribution... 11"}, {"heading": "4 Experiments 13", "text": "4.1 Mixture of Gaussians.................................. 134.2 Latent spatial model........................................"}, {"heading": "5 Conclusion 16", "text": "A Selection of tree structure and mating of copula families 18"}, {"heading": "1 Introduction", "text": "Variational inference is a computationally efficient approach to approaching posterior distributions, and has thus seen broad applicability for scaling complex statistical models (Wainwright and Jordan, 2008). Nevertheless, to make the calculation tractable, one either makes a mid-range independence assumption or loosens it easily by maintaining some of the original structures among the latent variables (Saul and Jordan, 1995). We propose a general methodology for variable inference that augments the variation distribution with a copula to maintain dependence. We list our contributions as follows: A generalization of the original method in variable inference. The algo rithma generalizes the variation inference for the center field and structured factorizations - accordingly, only one step of the algorithm runs."}, {"heading": "1.1 Related work", "text": "Preservation of structure in variation conclusions was first investigated by Saul and Jordan (1995) in the case of probabilistic neural networks. More recently, it has been investigated whether we can work with more scalable solutions for certain classes of models (Mimno et al., 2012; Salimans and Knowles, 2013; Hoffman and Blei, 2015).Our work differs from these lines in that we learn the dependence structure automatically during inference and therefore do not require explicit knowledge of the model. In fact, our augmentation strategy works more broadly on each posterior distribution and each factorized variation family, and therefore generalizes these approaches. In the case of Giordano and Broderick (2015), the only previous augmentation strategy is to use a calculation to correct the estimated posterior covariance from the center. The method assumes the use of exponential families and the midfield, variation on the true midfields, and that a posterior covariation does not offer any covariation or any of the respective one."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Variational inference", "text": "Let x be a series of observations, z latent variables, and \u03bb the free parameters of a variation distribution q (z | \u03bb). We aim to find the best approximation of the trailing p (z | x) based on the variation distribution q (z | \u03bb), measuring the distance in KL divergence. This corresponds (Wainwright and Jordan, 2008) to maximizing the quantity L (\u03bb) \u2261 Eq [log p (x, z) q (z | \u03bb)] = Eq [log p (x, z)] (z | \u03bb) is referred to as the Evidence Lower BOund (Lead et al., 2003)."}, {"heading": "2.2 Copulas", "text": "Consider the factorization of a d-dimensional random variable z = {z1,.., zd} with the density q (z) asq (z) = [d \u00b2 i = 1 q (zi)] c (Q (z1),.., Q (zd))) (3), where Q (zi) is the cumulative boundary distribution function (CDF) of the random variables zi and c is a common distribution.1 We say the distribution c is the copula of z (Sklar, 1959), which is the common multivariate density of Q (z1).., Q (zd) with uniform boundary distributions zi \u2212 is the probability integral transformation (PIT) of zi, i.e. Q (zi) and U [0, 1]. Such factorization in a product of boundary densities and a copula is always present and integrates the variable."}, {"heading": "2.2.1 Vine copulas", "text": "The uuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu"}, {"heading": "3 Copula variational inference", "text": "We present our method for performing accurate and scalable variation conclusions. Let's look at the mean field approach with a copula, where it is easy to expand on any structured factorization, as we will find later. Let's look at the original parameters, i.e., the center field requirements, and thus the extended parameters, i.e., the factorization of the variable distribution isq (z) = [d] i = 1 q = 1 q (zi] s-field, and p-field, and p-field, and p-field, and p-field, and p-field, and p-type, and p-type, and p-type, and p-type, and p-type, and p-type, and p-type, and p-type, and p-type,."}, {"heading": "3.1 Sampling from the copula-augmented variational distribution", "text": "We sample the copula-extended distribution by repeatedly performing inverse transformation processes (Devroye, 1986), also known as inverse CDF, on the individual copulas and finally on the marginals. This can be efficiently calculated in the worst case scenario in the complexity of O (md2), where m is the number of desired samples and d is the dimension of the distribution. Specifically, the sampling method is as follows: 1. Generate u = (u1,.., ud) where each ui-U (0, 1).2. Calculate v = (v1,., vd), which follows a common even distribution with dependencies given by the copula: v1 (11) v2 = Q \u2212 1 2 | 1 (u2) Qula (12) v3 = Q \u2212 k."}, {"heading": "3.2 Calculating the gradient of the copula-augmented variational", "text": "DistributionMan can efficiently take gradients by separating the gradient of the Log Q field from the gradient of the Log Q field. DistributionMan can efficiently take the gradients by separating the gradient of the Log Q field from the gradient of the Log Q field."}, {"heading": "4 Experiments", "text": "To demonstrate the effectiveness of the method, we perform benchmarks for both synthetic data and real data sets using the Gaussian mixing model. We choose this model because it is the classic example that emphasizes the difficulty of modelling dependencies. We also demonstrate the potential of CVI by extending the feasibility of variation conclusions to more complicated classes of models, such as the latent spatial model; in such cases, dependence in the latent variables is critical and the center field provides arbitrarily poor estimates. Due to the flexibility of the CVI framework presented in Algorithm 1, we make several modifications to achieve better results in practice. In particular, we use a minibatch setting of m = 1024, where we generate m samples from the variation distribution and take the average of stochastic gradients to reduce the variance at each iteration."}, {"heading": "4.1 Mixture of Gaussians", "text": "We follow the set of duties in Giordano and Broderick (2015), the posterior covariance of VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI VI"}, {"heading": "4.2 Latent space model", "text": "We simulate a 100.000-node-network with latent attributes from a K = 10-dimensional normal distribution zn \u0445 N (\u00b5, F = 1). For each pair (i, f) we draw a Bernoulli-Zufallsvariable with the probability logit (p) =??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "5 Conclusion", "text": "We extend the variation distribution with a copula to obtain the dependence between the latent variables of the posterior region. Afterwards, we propose a copula variation inference (CVI), a principal and scalable algorithm for performing inferences on the copula augmented distribution. In the CVI, the center and copula estimation is alternately and highly scalable using stochastic optimization. It is a generalization of both the center field and the structured variation inference, which means that it can be easily inserted into any current software. CVI significantly reduces bias and posterior variance and estimates it better and is more precise than other forms of variation approximation. Thus, the CVI expands the possibilities of variation inference to achieve scalable estimates while minimizing bias."}, {"heading": "Acknowledgements", "text": "We would like to thank Robin Gong and Luke Bornn, who helped to lay the foundation for the methodology, and Alp Kucukelbir for helpful discussions."}, {"heading": "A Choosing the tree structure and pair copula families", "text": "We assume that the structure of the vines and the pairs of the copula families are specified in order to perform the CVI selection, in the same way one must specify the family of middle fields for the variable inference of the black box (Ranganath et al., 2014). Generally, however, when factoring the distribution of variations, one can determine the tree structure and the pairs of the copula families based on synthetic data of the latent variables. (During tree selection, the enumeration and calculation of all possibilities is mathematically insoluble, since the number of possible vines on d variables increases factorally: there are d! / 2 (d \u2212 2) many choices (Morales-Na \u00b2 -Pole et al., 2010). The most common approach in practice is the sequential selection of the maximum tree starting from the original tree T1, where the weights of an edge are assigned to the respective random variables by absolute values of the Kendall correlation."}], "references": [{"title": "Probabilistic density decomposition for conditionally dependent random variables modeled by vines", "author": ["Tim Bedford", "Roger M. Cooke"], "venue": "Annals of mathematics and Artificial Intelligence,", "citeRegEx": "Bedford and Cooke.,? \\Q2001\\E", "shortCiteRegEx": "Bedford and Cooke.", "year": 2001}, {"title": "Vines - a new graphical model for dependent random variables", "author": ["Tim Bedford", "Roger M. Cooke"], "venue": "Annals of Statistics,", "citeRegEx": "Bedford and Cooke.,? \\Q2002\\E", "shortCiteRegEx": "Bedford and Cooke.", "year": 2002}, {"title": "Latent dirichlet allocation", "author": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["Arthur P. Dempster", "Nan M. Laird", "Donald B. Rubin"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Non-Uniform Random Variate Generation", "author": ["Luc Devroye"], "venue": null, "citeRegEx": "Devroye.,? \\Q1986\\E", "shortCiteRegEx": "Devroye.", "year": 1986}, {"title": "Selecting and estimating regular vine copulae and application to financial returns", "author": ["Jeffrey Dissmann", "Eike Christian Brechmann", "Claudia Czado", "Dorota Kurowicka"], "venue": "arXiv preprint arXiv:1202.2002,", "citeRegEx": "Dissmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dissmann et al\\.", "year": 2012}, {"title": "Editorial to the special issue on modeling and measurement of multivariate risk in insurance and finance", "author": ["Christian Genest", "Hans U. Gerber", "Marc J. Goovaerts", "Roger Laeven"], "venue": "Insurance: Mathematics and Economics,", "citeRegEx": "Genest et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Genest et al\\.", "year": 2009}, {"title": "Covariance matrices and influence scores for mean field variational bayes", "author": ["Ryan Giordano", "Tamara Broderick"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Giordano and Broderick.,? \\Q2015\\E", "shortCiteRegEx": "Giordano and Broderick.", "year": 2015}, {"title": "Sequential bayesian model selection of regular vine copulas", "author": ["Lutz Gruber", "Claudia Czado"], "venue": "International Society for Bayesian Analysis,", "citeRegEx": "Gruber and Czado.,? \\Q2015\\E", "shortCiteRegEx": "Gruber and Czado.", "year": 2015}, {"title": "Finite-sample properties of some alternative gmm estimators", "author": ["Lars Hansen", "John Heaton", "Amir Yaron"], "venue": "Journal of Business & Economic Statistics,", "citeRegEx": "Hansen et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Hansen et al\\.", "year": 1996}, {"title": "Latent space approaches to social network analysis", "author": ["Peter D. Hoff", "Adrian E. Raftery", "Mark S. Handcock"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Hoff et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hoff et al\\.", "year": 2001}, {"title": "Structured Stochastic Variational Inference", "author": ["Matthew D. Hoffman", "David M. Blei"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Hoffman and Blei.,? \\Q2015\\E", "shortCiteRegEx": "Hoffman and Blei.", "year": 2015}, {"title": "Families of m-variate distributions with given margins and m(m\u2212 1)/2 bivariate dependence parameters, pages 120\u2013141", "author": ["Harry Joe"], "venue": "Institute of Mathematical Statistics,", "citeRegEx": "Joe.,? \\Q1996\\E", "shortCiteRegEx": "Joe.", "year": 1996}, {"title": "Adam: a Method for Stochastic Optimization", "author": ["Diederik P. Kingma", "Jimmy Lei Ba"], "venue": "International Conference on Learning Representations,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Uncertainty Analysis with High Dimensional Dependence Modelling", "author": ["Dorota Kurowicka", "Roger M. Cooke"], "venue": null, "citeRegEx": "Kurowicka and Cooke.,? \\Q2006\\E", "shortCiteRegEx": "Kurowicka and Cooke.", "year": 2006}, {"title": "Sampling algorithms for generating joint uniform distributions using the vine-copula method", "author": ["Dorota Kurowicka", "Roger M. Cooke"], "venue": "Computational Statistics & Data Analysis,", "citeRegEx": "Kurowicka and Cooke.,? \\Q2007\\E", "shortCiteRegEx": "Kurowicka and Cooke.", "year": 2007}, {"title": "Sparse stochastic inference for latent dirichlet allocation", "author": ["David M. Mimno", "Matthew D. Hoffman", "David M. Blei"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Mimno et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mimno et al\\.", "year": 2012}, {"title": "Black box variational inference", "author": ["New York", "Inc", "2006. Rajesh Ranganath", "Sean Gerrish", "David M. Blei"], "venue": null, "citeRegEx": "York et al\\.,? \\Q2006\\E", "shortCiteRegEx": "York et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 16, "context": "It has more recently been studied to work with more scalable solutions on particular classes of models (Mimno et al., 2012; Salimans and Knowles, 2013; Hoffman and Blei, 2015).", "startOffset": 103, "endOffset": 175}, {"referenceID": 11, "context": "It has more recently been studied to work with more scalable solutions on particular classes of models (Mimno et al., 2012; Salimans and Knowles, 2013; Hoffman and Blei, 2015).", "startOffset": 103, "endOffset": 175}, {"referenceID": 7, "context": "The only previous augmentation strategy is in the case of Giordano and Broderick (2015), who apply a calculation in order to correct the estimated posterior covariance from the mean-field.", "startOffset": 58, "endOffset": 88}, {"referenceID": 2, "context": "L(\u03bb) is referred to as the Evidence Lower BOund (ELBO) (Blei et al., 2003), or the variational free energy.", "startOffset": 55, "endOffset": 74}, {"referenceID": 6, "context": "However, their multivariate extensions lack the flexibility of accurately modelling dependencies in higher dimensions (Genest et al., 2009).", "startOffset": 118, "endOffset": 139}, {"referenceID": 12, "context": "A successful approach in recent literature has been through the construction of a set of conditional bivariate copulas called a vine (Joe, 1996; Bedford and Cooke, 2001, 2002; Kurowicka and Cooke, 2006).", "startOffset": 133, "endOffset": 202}, {"referenceID": 14, "context": "A successful approach in recent literature has been through the construction of a set of conditional bivariate copulas called a vine (Joe, 1996; Bedford and Cooke, 2001, 2002; Kurowicka and Cooke, 2006).", "startOffset": 133, "endOffset": 202}, {"referenceID": 5, "context": "They are a generalization of other types of pair copula constructions such as the canonical vines and drawable vines (Dissmann et al., 2012).", "startOffset": 117, "endOffset": 140}, {"referenceID": 9, "context": "This alternating set of optimizations falls under the class of minorize-maximization (MM) iterative methods, which includes many procedures such as the alternating least squares algorithm, the iterative procedure for the generalized method of moments (Hansen et al., 1996), and the EM algorithm (Dempster et al.", "startOffset": 251, "endOffset": 272}, {"referenceID": 3, "context": ", 1996), and the EM algorithm (Dempster et al., 1977).", "startOffset": 30, "endOffset": 53}, {"referenceID": 5, "context": "One can fit them however, and in our experiments we automatically learn the tree structure and pair copula families using sequential tree selection (Dissmann et al., 2012) and Bayesian model selection (Gruber and Czado, 2015) among a choice of 16 bivariate copula families, c.", "startOffset": 148, "endOffset": 171}, {"referenceID": 8, "context": ", 2012) and Bayesian model selection (Gruber and Czado, 2015) among a choice of 16 bivariate copula families, c.", "startOffset": 37, "endOffset": 61}, {"referenceID": 4, "context": "We sample from the copula-augmented distribution by repeatedly doing inverse transform sampling (Devroye, 1986), also known as inverse CDF, on the individual pair copulas and finally the marginals.", "startOffset": 96, "endOffset": 111}, {"referenceID": 14, "context": "Explicit calculations of the inverse of the conditional CDFs Q\u22121 i|12\u00b7\u00b7\u00b7i\u22121 can be found in Kurowicka and Cooke (2007). In general, one calculates the conditional CDF Qi|D(e) used in the pair copula cik|D(e) at tree Tj by Qi|D(e) = \u2202 \u2202Qk|D(e)\\k Cik|D(e)\\k = \u2202 \u2202Qk|D(e)\\k C(Qi|D(e)\\k, Qk|D(e)\\k|uj : j \u2208 D(e)\\k) (15) where Cik|D(e)\\k, which is the CDF of the pair copula cik|D(e)\\k, and Qi|D(e)\\k, Qk|D(e)\\k are obtained recursively from the previous tree Tj\u22121.", "startOffset": 92, "endOffset": 119}, {"referenceID": 13, "context": "We also use ADAM (Kingma and Ba, 2015), which is an adaptive learning rate that combines ideas from both AdaGrad and RMSprop.", "startOffset": 17, "endOffset": 38}, {"referenceID": 7, "context": "We follow the set of tasks in Giordano and Broderick (2015), which is to estimate the posterior covariance for Gaussian mixture models.", "startOffset": 30, "endOffset": 60}, {"referenceID": 7, "context": "We also compare our results to linear response variational Bayes (LRVB) (Giordano and Broderick, 2015), which is a correction technique for covariance estimation using a linear perturbation argument on the posterior.", "startOffset": 72, "endOffset": 102}, {"referenceID": 7, "context": "Following Giordano and Broderick (2015), we set N = 10, 000 samples, K = 2 components, and P = 2 dimensional Gaussian distributions.", "startOffset": 10, "endOffset": 40}, {"referenceID": 10, "context": "We demonstrate the potential of CVI by performing inference on the latent space model (Hoff et al., 2001), which is a Bernoulli latent factor model for relational data.", "startOffset": 86, "endOffset": 105}], "year": 2017, "abstractText": "We develop a general methodology for variational inference which preserves dependency among the latent variables. This is done by augmenting the families of distributions used in mean-field and structured approximation with copulas. Copulas allow one to separately model the dependency given a factorization of the variational distribution, and can guarantee us better approximations to the posterior as measured by KL divergence. We show that inference on the augmented distribution is highly scalable using stochastic optimization. Furthermore, the addition of a copula is generic and can be applied straightforwardly to any inference procedure using the original meanfield or structured approach. This reduces bias, sensitivity to local optima, sensitivity to hyperparameters, and significantly helps characterize and interpret the dependency among the latent variables.", "creator": "LaTeX with hyperref package"}}}