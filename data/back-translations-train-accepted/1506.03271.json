{"id": "1506.03271", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2015", "title": "Explore no more: Improved high-probability regret bounds for non-stochastic bandits", "abstract": "This work addresses the problem of regret minimization in non-stochastic multi-armed bandit problems, focusing on performance guarantees that hold with high probability. Such results are rather scarce in the literature since proving them requires a large deal of technical effort and significant modifications to the standard, more intuitive algorithms that come only with guarantees that hold on expectation. One of these modifications is forcing the learner to sample the losses of every arm at least $\\Omega(\\sqrt{T})$ times over $T$ rounds, which can adversely affect performance if many of the arms are obviously suboptimal. While it is widely conjectured that this property is essential for proving high-probability regret bounds, we show in this paper that it is possible to achieve such strong results without this undesirable exploration component. Our result relies on a simple and intuitive loss-estimation strategy called \\emph{Implicit eXploration} (IX) that allows a remarkably clean analysis. To demonstrate the flexibility of our technique, we derive several improved high-probability bounds for various extensions of the standard multi-armed bandit framework. Finally, we conduct a simple experiment that illustrates the robustness of our implicit exploration technique.", "histories": [["v1", "Wed, 10 Jun 2015 12:19:21 GMT  (125kb,D)", "https://arxiv.org/abs/1506.03271v1", null], ["v2", "Thu, 16 Jul 2015 12:59:46 GMT  (125kb,D)", "http://arxiv.org/abs/1506.03271v2", null], ["v3", "Tue, 3 Nov 2015 08:42:39 GMT  (121kb,D)", "http://arxiv.org/abs/1506.03271v3", "To appear at NIPS 2015"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["gergely neu"], "accepted": true, "id": "1506.03271"}, "pdf": {"name": "1506.03271.pdf", "metadata": {"source": "CRF", "title": "Explore no more: Improved high-probability regret bounds for non-stochastic bandits", "authors": ["Gergely Neu"], "emails": ["gergely.neu@gmail.com"], "sections": [{"heading": null, "text": "\u221a T) -times over T rounds, which can have a negative impact on performance when many of the weapons are suboptimal. Although it is widely believed that this property is indispensable for detecting highly probable remorse limits, we show in this paper that it is possible to achieve such strong results without this unwanted exploration component. Our result is based on a simple and intuitive loss estimation strategy called Implicit eXploration (IX), which allows for remarkably clean analysis. To demonstrate the flexibility of our technique, we deduce several enhanced maximum probability limits for various extensions of the standard multi-armed band framework. Finally, we conduct a simple experiment demonstrating the robustness of our implicit exploration technique."}, {"heading": "1 Introduction", "text": "Consider the problem of regret minimization in non-stochastic multi-armed bandits, as defined in the classical work of Auer, Cesa-Bianchi, Friend, and Schapire. (This sequential decision problem can be formalized as a repetitive game between a learner and an environment sometimes referred to as the opponent.) In each turn t = 1, 2,., the two players interact as follows: The learner selects an arm (also referred to as action) It is formalized as a repetitive game between a learner and an environment (sometimes referred to as the opponent) and the environment selects a loss function: [K] \u2192, where the loss is associated with the i arm, it is called \"t, the learner incurs and observes the loss.\" It is based exclusively on these observations, the goal of the learner is to choose his actions in such a way that he accumulates his actions as possible."}, {"heading": "2 Explicit and implicit exploration", "text": "Most of the principal learning algorithms for the non-stochastic bandit problem are constructed using a standard online learning algorithm such as the exponentially weighted forecasts ([26, 20, 13]) or following the disturbed leaders ([14, 18]), with the true (unobserved) loss replaced by some reasonable estimates. One of the most important challenges is to provide reliable estimates of the losses used in many settings (see, Bubeck and Cesa-Bianchi [9]). Our result reinforces these observations further by using the value-weighted losses / reward estimates of the form, i = \"t, i pt, i pt\" t, i \"t, i\" t, i \"t, i\" t, i \"t, i\" t, i \"t, i\" t, i \"t, i\" t \"t.\""}, {"heading": "3 High-probability regret bounds via implicit exploration", "text": "In this section, we present a concentration result with respect to the IX loss estimates of Equation (3) and apply this result to prove a high probability of a number of non-stochastic Bandit problems. \u2212 The following problem is our concentration result in its most general form: 2Explicit exploration is considered inevitable to prove limitations in the reward game for various other reasons - see Bubeck and Cesa-Bianchi [9] for a discussion problem. \u2212 Leave (t) a fixed, non-increasing sequence with the result of 0 and admit it, I will not be negative, Ft \u2212 1 measurable random variables that allow satisfactory variables to be satisfied, i \u2264 2\u03b3t for all t and i. Then, with probability at least 1 \u2212 T = 1 K sequence that is non-increasing."}, {"heading": "3.2 Bandits with expert advice", "text": "We now turn to the hiring of multi-armed bandits with expert advice, as defined in Auer et al. [5] and later taken up again by McMahan and Streeter [22] and Beygelzimer et al. [7]. In this setting, we assume that in each turn t = 1, 2,.., T, the learner observes a series of N probability distributions, not (1), but (n),....,.... in relation to (Ft). [0, 1] K above the K arms, so that. K i = 1,., i (n) = 1 for all n. [N]. We assume that the sequences (t (n)) are measurable in relation to (Ft). These vectors represent the likely advice of the corresponding n-th expert. The goal of the learner in this setting is to select a sequence of weapons to minimize regret against the best expert."}, {"heading": "3.3 Tracking the best sequence of arms", "text": "In this section, we look at the problem of competing with action sequences. Similar to Herbster and Warmuth [17], we look at the class of sequences that alternate at most S times between actions. We measure the learner's performance in this context in terms of regretting the best sequence from this class C (S) [K] T, defined as RST = T \u2211 t = 1 't, It \u2212 min (Jt) \u0445 C (S) T \u2211 t = 1' t, Jt.Similar to Auer et al. [5], we now propose to adapt the fixed share algorithm of Herbster and Warmuth [17] to our constellation. Our algorithm, called EXP3-SIX, updates a series of weights wt, \u00b7 over the arms in a recursive manner. In the first round, the fixed share algorithm of Herbster and Warmuth [17] is adapted to the ability of Kendith [17]. Our algorithm, called EXP3-SIX-arm series of weights, is called EXPSIT-SIX-SIX."}, {"heading": "3.4 Bandits with side-observations", "text": "Let us now turn to the problem of online learning in bandit problems in the presence of marginal observations, as defined by Mannor and Shamir and later performed by Alon et al. [1] In this context, the learner and the environment interact exactly as in the multi-armed bandit problem, the main difference being that in each turn the learner observes the losses of some arms other than his actual chosen arm. G: Node of G. The structure of the marginal observations is described by the directed graph G: Node of G. [19] For this precise setting, the presence of Arc i \u2192 j implies that the learner, when selecting Es = i. Implicit exploration and EXP3-IX, first proposed factors by G. [19] To describe this variant, let us introduce the notations Ot, i = I = i} + I {(It \u2192 i) and ot, i = E [Ot, i | Ft] in this context."}, {"heading": "4 Empirical evaluation", "text": "We are conducting a simple experiment to demonstrate the robustness of EXP3-IX compared to EXP3 and its superior performance compared to EXP3.P. Our setting is a 10-arm bandit problem where all losses are independent drawings of Bernoulli random variables, the mean losses of arms 1 to 8 are 1 / 2 and the mean loss of arm 9 is 1 / 2. This choice ensures that arm 9 is significantly better than other weapons until at least turn T / 2. In the second half of the game, arm 10 begins to surpass arm 9 and eventually becomes the leader. We have rated the performance of EXP3, EXP3.P and EXP3-IX in the setting above as T = 106 and pp = 0.1."}, {"heading": "5 Discussion", "text": "In this paper, we have shown that, contrary to popular belief, explicit exploration is not necessary to achieve a high probability of exceeding the limits of regret for non-stochastic bandit problems. Interestingly, however, we have observed in several of our experiments that our IX-based algorithms still pull each arm, although this is not explicitly enforced by the algorithm. (This indicates the need for a more complete study of the role of exploration to determine whether it is necessary to pull each individual arm in order to achieve approximately optimal warranties. It can be argued that tuning the IX parameter that we are introducing may be as difficult in practice as tuning the parameters of exploration 3.P. However, every aspect of our analysis suggests that it is the most natural choice for these parameters, and that this is the choice we recommend. A limitation of our current analysis is that only deterrent parameters are explicit, and that the study explicitly accepts (see the question)."}, {"heading": "A The proof of Lemma 1", "text": "For convenience, we use the notation \u03b2t = 2\u03b3t = > i = i = long and i = long. First, note that for each i, i = \"t, i pt, i + \u03b3t I {It = i} \u2264\" t, i pt, i + \u03b3t, i I {It = i} = 1 2\u03b3t, i / pt, i 1 + \u03b3t, i / pt, i \u2212 t, i (It = i) \u2264 1 \u03b2t \u00b7 log (1 + \u03b2t, i), with the first step from \"t, i [0, 1] and the last from the elementary inequality z1 + z / 2 \u2264 \u2212 log (1 + z), which applies to all z."}, {"heading": "B Further proofs", "text": "B. 1 The proof of theorem 2Fix on arbitrary digits i = digits i = digits i = digits i = digits i = digits i = digits i = digits i = digits i = digits i = digits i = digits i = digits i = digits i = digits i = digits i = digits i = digits i (digits), digits n / (digits) i (digits) i (digits), digits n \u2212 K + digits T = digits K + digits K + digits K + digits K + digits T (digits)."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "<lb>This work addresses the problem of regret minimization in non-stochastic multi-<lb>armed bandit problems, focusing on performance guarantees that hold with high<lb>probability. Such results are rather scarce in the literature since proving them re-<lb>quires a large deal of technical effort and significant modifications to the standard,<lb>more intuitive algorithms that come only with guarantees that hold on expectation.<lb>One of these modifications is forcing the learner to sample arms from the uniform<lb>distribution at least \u03a9(<lb>\u221a<lb>T ) times over T rounds, which can adversely affect per-<lb>formance if many of the arms are suboptimal. While it is widely conjectured that<lb>this property is essential for proving high-probability regret bounds, we show in<lb>this paper that it is possible to achieve such strong results without this undesirable<lb>exploration component. Our result relies on a simple and intuitive loss-estimation<lb>strategy called Implicit eXploration (IX) that allows a remarkably clean analy-<lb>sis. To demonstrate the flexibility of our technique, we derive several improved<lb>high-probability bounds for various extensions of the standard multi-armed bandit<lb>framework. Finally, we conduct a simple experiment that illustrates the robustness<lb>of our implicit exploration technique.", "creator": "LaTeX with hyperref package"}}}