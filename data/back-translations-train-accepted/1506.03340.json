{"id": "1506.03340", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2015", "title": "Teaching Machines to Read and Comprehend", "abstract": "Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.", "histories": [["v1", "Wed, 10 Jun 2015 14:54:39 GMT  (5920kb,D)", "http://arxiv.org/abs/1506.03340v1", "13 pages, 11 figures"], ["v2", "Thu, 1 Oct 2015 15:04:49 GMT  (29696kb,D)", "http://arxiv.org/abs/1506.03340v2", "13 pages, 11 figures"], ["v3", "Thu, 19 Nov 2015 15:43:23 GMT  (5978kb,D)", "http://arxiv.org/abs/1506.03340v3", "Appears in: Advances in Neural Information Processing Systems 28 (NIPS 2015). 14 pages, 13 figures"]], "COMMENTS": "13 pages, 11 figures", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.NE", "authors": ["karl moritz hermann", "tom\u00e1s kocisk\u00fd", "edward grefenstette", "lasse espeholt", "will kay", "mustafa suleyman", "phil blunsom"], "accepted": true, "id": "1506.03340"}, "pdf": {"name": "1506.03340.pdf", "metadata": {"source": "CRF", "title": "Teaching Machines to Read and Comprehend", "authors": ["Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "emails": ["kmh@google.com", "etg@google.com", "lespeholt@google.com", "wkay@google.com", "mustafasul@google.com", "pblunsom@google.com", "tomas@kocisky.eu"], "sections": [{"heading": "1 Introduction", "text": "Progress from a flat-shell approach to gathering information to machines capable of reading and understanding documents is slow. Traditional approaches to machine reading and understanding are based either on artisanal grammars [1] or on information gathering methods that triple predicate arguments that can later be questioned as relational databases. Supervised Machine Learning approaches have largely failed in this space, both due to the lack of large-scale training data sets and the difficulties in structuring statistical models that are flexible enough to exploit document structures."}, {"heading": "2 Supervised training data for reading comprehension", "text": "Specifically, we are trying to estimate the conditional probability p (a | c, q), where c is a context document, q is a query relating to this document, and an answer to that query. For a focused evaluation, we would like to be able to exclude additional information, such as the knowledge gained from coexistence statistics, in order to test the core capability of a model to recognize and understand the linguistic relationships between entities in the context documentation. Such an approach requires a large training corpus of document query response triples, and until now, such corpses have been limited to hundreds of examples, and thus mostly served only for testing [9]. This limitation has resulted in most work in this area taking the form of unattended approaches that use templates or syntactical / semantic analyzers to extract correlations from the document in order to form a knowledge that can be queried."}, {"heading": "2.1 Qualitative Analysis of Answer Difficulty", "text": "To gain a perspective on the difficulty of the task in absolute terms, we perform a small qualitative analysis on a subset of validation data. We classify requests by the depth of the conclusions required to solve them and sort them into six categories. Simple requests can be answered without syntactical or semantic complexity. Lexical requests require some form of lexical generalization - something that is self-evident when embedding based systems, but creates difficulties for methods that rely primarily on syntactic analysis. Finally, the Coref category we group all unanswerable requests that require coreference resolution with a common Coref / Lex category for requests that require both coreference resolution and lexical generalization. Complex requests require some form of causality or other complex conclusions to be answered."}, {"heading": "2.2 Entity replacement and permutation", "text": "Note that the emphasis of this work is to provide a corpus to evaluate the ability of a model to read and understand a single document, not world knowledge or parallel events. To understand this distinction, consider, for example, the following Cloze form queries (created from headlines in the Daily Mail's validation kit): a) The high-tech bra that helps you beat chest X; b) Could saccharin help beat X?; c) Can fish oils help fight prostate X? A Ngram language model traced in the Daily Mail would easily correctly predict that (X = cancer), regardless of the contents of the context document, depends on (X = cancer) simply because it is a very frequently healed entity in the corpus Daily Mail. To prevent such degenerated solutions and create a focused task, we anonymize and randomize our corpora with the following procedure; a) We use a corpus system to assign corpus to each of the corpus, using abstract data systems to each of the corpus response points; and b) We use each of the corpus to replace the corpus with corpus reference systems in each of the corpus."}, {"heading": "3 Models", "text": "We go through a series of baselines, benchmarks and new models that we have to evaluate based on this paradigm. We define two simple baselines: the majority baseline (maximum frequency) selects the entity most frequently observed in the context document, while the exclusive majority (exclusive frequency) selects the entity most frequently observed in the context but not observed in the query. The idea behind this exclusion is that the placeholder is unlikely to be mentioned twice in a single cloze query."}, {"heading": "3.1 Symbolic Matching Models", "text": "There is no significant advantage in this area, as the frame semantic approach here does not have the ability to use linguistic annotations, structured world knowledge and semantic parsing, as well as similar NLP pipeline outputs. Building on these approaches, we define a number of NLP-centric models for our machine read task. Frame semantic parsing frame semantic analyses attempt to identify predicates and their arguments by giving models access to information about \"who did what to whom.\" Of course, this type of annotation is suitable for answering questions. We are developing a benchmark that allows the use of frame semantic annotations, which we obtained through the parsing model with a state-of-the-art frame semantic parser."}, {"heading": "3.2 Neural Network Models", "text": "This includes classification tasks such as sentiment analysis [15] or POS marking [16], as well as generative problems such as language modeling or machine translation [17]. We propose three neural models to estimate the probability of the word type a from document d, which allows answering word types more than indexes. Note that we do not have privileges or variables that the model needs to distinguish in the input sequence. g (d, q) gives a vector embedding of a document and the query pair. The Deep LSTM Reader Long-Memory (LSTM Long-Term Memory) has recently shown considerable success in tasks such as machine translation and translation."}, {"heading": "4 Empirical Evaluation", "text": "This year it is more than ever before in the history of the city."}, {"heading": "5 Conclusion", "text": "The supervised paradigm for the training of reading and comprehension models for machines offers a promising way to make progress towards building complete natural speech comprehension systems. Our analysis shows that attentive and impatient readers are able to disseminate and integrate semantic information over long distances. In particular, we believe that the inclusion of an attention mechanism is the key factor in these outcomes. The attention mechanism we use is only an instantiation of a very general idea that can be further exploited. However, the inclusion of world knowledge and multi-document queries also requires the development of attention and the embedding of mechanisms whose complexity is not scaled linearly with the size of the datasets. There are still many queries that require complex conclusions and reference resolutions over long distances that our models cannot yet answer significantly, as our collected data represent a greater challenge than those that we should undoubtedly support in future school sections."}, {"heading": "A Additional Heatmap Analysis", "text": "We extend the analysis of the attention mechanism presented in the essay by including visualizations for additional queries from the CNN validation dataset below. We consider examples from the Attentive Reader as well as the Impatient Reader in this Appendix. A.1 Attentive ReaderPositive Instances Figure 4, however, shows two positive examples from the CNN validation set that require a reasonable level of lexical generalization and co-reference to be answered. The first query in Figure 5 contains strong lexical references through the quote, but requires the identification of the entity cited, which is not trivial in the context document. The final positive example (also in Figure 5) shows the fearlessness of our model. Negative Instances Figures 6 and 7 show examples of queries where the Attentive Reader does not select the correct answer. The two examples in Figure 6 underscore a fairly common phenomenon - namely, that in both cases we evaluate the susimilarity process, at least for both of which the answer is ambiguous."}], "references": [{"title": "A rule-based question answering system for reading comprehension tests", "author": ["Ellen Riloff", "Michael Thelen"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "Machine reading at the University of Washington", "author": ["Hoifung Poon", "Janara Christensen", "Pedro Domingos", "Oren Etzioni", "Raphael Hoffmann", "Chloe Kiddon", "Thomas Lin", "Xiao Ling", "Mausam", "Alan Ritter", "Stefan Schoenmackers", "Stephen Soderland", "Dan Weld", "Fei Wu", "Congle Zhang"], "venue": "In Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Understanding Natural Language", "author": ["Terry Winograd"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1972}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Recurrent models of visual attention", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves", "Koray Kavukcuoglu"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Daan Wierstra"], "venue": "CoRR, abs/1502.04623,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Mctest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Matthew Richardson", "Christopher J.C. Burges", "Erin Renshaw"], "venue": "In EMNLP,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Enhancing single-document summarization by combining RankNet and third-party sources", "author": ["Krysta Svore", "Lucy Vanderwende", "Christopher Burges"], "venue": "In Proceedings of the Joint Conference of EMNLP-CoNLL,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Automatic generation of story highlights", "author": ["Kristian Woodsend", "Mirella Lapata"], "venue": "In Proceedings of ACL,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Cloze procedure\u201d: a new tool for measuring readability", "author": ["Wilson L Taylor"], "venue": "Journalism Quarterly,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1953}, {"title": "Semantic frame identification with distributed word representations", "author": ["Karl Moritz Hermann", "Dipanjan Das", "Jason Weston", "Kuzman Ganchev"], "venue": "In Proceedings of ACL,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom"], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. V Le"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1997}, {"title": "Supervised Sequence Labelling with Recurrent Neural Networks, volume 385 of Studies in Computational Intelligence", "author": ["Alex Graves"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Traditional approaches to machine reading and comprehension have been based on either hand engineered grammars [1], or information extraction methods of detecting predicate argument triples that can later be queried as a relational database [2].", "startOffset": 111, "endOffset": 114}, {"referenceID": 1, "context": "Traditional approaches to machine reading and comprehension have been based on either hand engineered grammars [1], or information extraction methods of detecting predicate argument triples that can later be queried as a relational database [2].", "startOffset": 241, "endOffset": 244}, {"referenceID": 2, "context": "Historically, however, many similar approaches in Computational Linguistics have failed to manage the transition from synthetic data to real environments, as such closed worlds inevitably fail to capture the complexity, richness, and noise of natural language [5].", "startOffset": 260, "endOffset": 263}, {"referenceID": 3, "context": "These models draw on recent developments for incorporating attention mechanisms into recurrent neural network architectures [6, 7, 8].", "startOffset": 124, "endOffset": 133}, {"referenceID": 4, "context": "These models draw on recent developments for incorporating attention mechanisms into recurrent neural network architectures [6, 7, 8].", "startOffset": 124, "endOffset": 133}, {"referenceID": 5, "context": "These models draw on recent developments for incorporating attention mechanisms into recurrent neural network architectures [6, 7, 8].", "startOffset": 124, "endOffset": 133}, {"referenceID": 6, "context": "Such an approach requires a large training corpus of document\u2013query\u2013answer triples and until now such corpora have been limited to hundreds of examples and thus mostly of use only for testing [9].", "startOffset": 192, "endOffset": 195}, {"referenceID": 7, "context": "Inspired by work in summarisation [10, 11], we create two machine reading corpora by exploiting online newspaper articles and their matching summaries.", "startOffset": 34, "endOffset": 42}, {"referenceID": 8, "context": "Inspired by work in summarisation [10, 11], we create two machine reading corpora by exploiting online newspaper articles and their matching summaries.", "startOffset": 34, "endOffset": 42}, {"referenceID": 9, "context": "We construct a corpus of document\u2013query\u2013 answer triples by turning these bullet points into Cloze [12] style questions by replacing one entity at a time with a placeholder.", "startOffset": 98, "endOffset": 102}, {"referenceID": 10, "context": "We develop a benchmark that makes use of frame-semantic annotations which we obtained by parsing our model with a state-ofthe-art frame-semantic parser [13, 14].", "startOffset": 152, "endOffset": 160}, {"referenceID": 11, "context": "This includes classification tasks such as sentiment analysis [15] or POS tagging [16], as well as generative problems such as language modelling or machine translation [17].", "startOffset": 62, "endOffset": 66}, {"referenceID": 12, "context": "This includes classification tasks such as sentiment analysis [15] or POS tagging [16], as well as generative problems such as language modelling or machine translation [17].", "startOffset": 82, "endOffset": 86}, {"referenceID": 13, "context": "This includes classification tasks such as sentiment analysis [15] or POS tagging [16], as well as generative problems such as language modelling or machine translation [17].", "startOffset": 169, "endOffset": 173}, {"referenceID": 14, "context": "The Deep LSTM Reader Long short-term memory (LSTM, [18]) networks have recently seen considerable success in tasks such as machine translation and language modelling [17].", "startOffset": 51, "endOffset": 55}, {"referenceID": 13, "context": "The Deep LSTM Reader Long short-term memory (LSTM, [18]) networks have recently seen considerable success in tasks such as machine translation and language modelling [17].", "startOffset": 166, "endOffset": 170}, {"referenceID": 15, "context": "When used for translation, Deep LSTMs [19] have shown a remarkable ability to embed long sequences into a vector representation which contains enough information to generate a full translation in another language.", "startOffset": 38, "endOffset": 42}, {"referenceID": 3, "context": "The fixed width hidden vector forms a bottleneck for this information flow that we propose to circumvent using an attention mechanism inspired by recent results in translation and image recognition [6, 7].", "startOffset": 198, "endOffset": 204}, {"referenceID": 4, "context": "The fixed width hidden vector forms a bottleneck for this information flow that we propose to circumvent using an attention mechanism inspired by recent results in translation and image recognition [6, 7].", "startOffset": 198, "endOffset": 204}, {"referenceID": 15, "context": "This attention model first encodes the document and the query using separate bidirectional single layer LSTMs [19].", "startOffset": 110, "endOffset": 114}, {"referenceID": 0, "context": "For the Deep LSTM Reader, we consider hidden layer sizes [64, 128, 256], depths [1, 2, 4], initial learning rates [1E\u22123, 5E\u22124, 1E\u22124, 5E\u22124] and batch sizes [16, 32].", "startOffset": 80, "endOffset": 89}, {"referenceID": 1, "context": "For the Deep LSTM Reader, we consider hidden layer sizes [64, 128, 256], depths [1, 2, 4], initial learning rates [1E\u22123, 5E\u22124, 1E\u22124, 5E\u22124] and batch sizes [16, 32].", "startOffset": 80, "endOffset": 89}, {"referenceID": 12, "context": "For the Deep LSTM Reader, we consider hidden layer sizes [64, 128, 256], depths [1, 2, 4], initial learning rates [1E\u22123, 5E\u22124, 1E\u22124, 5E\u22124] and batch sizes [16, 32].", "startOffset": 155, "endOffset": 163}, {"referenceID": 12, "context": "For the attention models we consider hidden layer sizes [64, 128, 256] (Attentive) and [64, 128] (Impatient), single layer, initial learning rates [1E\u22122, 5E\u22123, 1E\u22123, 5E\u22124], and batch sizes [16, 32] (Attentive) and [8, 12] (Impatient).", "startOffset": 189, "endOffset": 197}, {"referenceID": 5, "context": "For the attention models we consider hidden layer sizes [64, 128, 256] (Attentive) and [64, 128] (Impatient), single layer, initial learning rates [1E\u22122, 5E\u22123, 1E\u22123, 5E\u22124], and batch sizes [16, 32] (Attentive) and [8, 12] (Impatient).", "startOffset": 214, "endOffset": 221}, {"referenceID": 9, "context": "For the attention models we consider hidden layer sizes [64, 128, 256] (Attentive) and [64, 128] (Impatient), single layer, initial learning rates [1E\u22122, 5E\u22123, 1E\u22123, 5E\u22124], and batch sizes [16, 32] (Attentive) and [8, 12] (Impatient).", "startOffset": 214, "endOffset": 221}], "year": 2015, "abstractText": "Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.", "creator": "LaTeX with hyperref package"}}}