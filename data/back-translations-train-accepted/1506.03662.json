{"id": "1506.03662", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2015", "title": "Variance Reduced Stochastic Gradient Descent with Neighbors", "abstract": "Stochastic Gradient Descent (SGD) is a workhorse in machine learning, yet it is also known to be slow relative to steepest descent. The variance in the stochastic update directions only allows for sublinear or (with iterate averaging) linear convergence rates. Recently, variance reduction techniques such as SVRG and SAGA have been proposed to overcome this weakness. With asymptotically vanishing variance, a constant step size can be maintained, resulting in geometric convergence rates. However, these methods are either based on occasional computations of full gradients at pivot points (SVRG), or on keeping per data point corrections in memory (SAGA). This has the disadvantage that one cannot employ these methods in a streaming setting and that speed-ups relative to SGD may need a certain number of epochs in order to materialize. This paper investigates a new class of algorithms that can exploit neighborhood structure in the training data to share and re-use information about past stochastic gradients across data points. While not meant to be offering advantages in an asymptotic setting, there are significant benefits in the transient optimization phase, in particular in a streaming or single-epoch setting. We investigate this family of algorithms in a thorough analysis and show supporting experimental results. As a side-product we provide a simple and unified proof technique for a broad class of variance reduction algorithms.", "histories": [["v1", "Thu, 11 Jun 2015 13:14:33 GMT  (793kb,D)", "http://arxiv.org/abs/1506.03662v1", null], ["v2", "Thu, 5 Nov 2015 12:30:49 GMT  (332kb,D)", "http://arxiv.org/abs/1506.03662v2", null], ["v3", "Tue, 17 Nov 2015 22:00:11 GMT  (330kb,D)", "http://arxiv.org/abs/1506.03662v3", null], ["v4", "Fri, 26 Feb 2016 19:55:56 GMT  (334kb,D)", "http://arxiv.org/abs/1506.03662v4", "Appears in: Advances in Neural Information Processing Systems 28 (NIPS 2015). 13 pages"]], "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["thomas hofmann", "aur\u00e9lien lucchi", "simon lacoste-julien", "brian mcwilliams"], "accepted": true, "id": "1506.03662"}, "pdf": {"name": "1506.03662.pdf", "metadata": {"source": "CRF", "title": "Neighborhood Watch: Stochastic Gradient Descent with Neighbors", "authors": ["Thomas Hofmann", "Aurelien Lucchi", "Brian McWilliams"], "emails": ["@inf.ethz.ch"], "sections": [{"heading": "1 Introduction", "text": "We consider a general problem that is ubiquitous in machine learning, namely the optimization of an empirical or regulated convex risk function. In the face of a convex loss l and a strongly convex regulator, the aim is to find a parameter vector w that minimizes the (empirical) expectation: w * argmin w (w), f (w): = Efi (w) = 1 n * (w), fi (w): l (xi, yi) + (w). We assume that in all data-dependent functions fi, Lipschitz continuous gradients are present."}, {"heading": "2 Algorithms", "text": "The goal is to define updates of the asymptotically vanishing variance, i.e., gi (w) \u2192 0 as w. \"w.\" w. \"w.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \""}, {"heading": "3 Analysis", "text": "The evolutionary equation (2) in expectation implies recurrence (2) in expectation (2) in expectation (2) in expectation (2) in expectation (2) in expectation (2) in expectation (2) in expectation (2) in expectation (2) in expectation (2) in expectation (2) in expectation (2) in expectation (2) in expectation (2) in expectation (2) in expectation (2) in expectation (2) in expectation (2) in expectation (2) in expectation (2) in expectation (2) in expectation (2) in expectation (2) in expectation (2) in expectation (2) in expectation (2) in expectation (2) in expectation (2) in expectation (2) in expectation (2) in expectation (2) in expectation (2)."}, {"heading": "4 Experimental Results", "text": "We present experimental results on the performance of the different variants of memory algorithms for variance reduction SGD = 15 points as discussed in this paper. Since SGD has shown a uniform superior behavior as SVRG in our experiments, SGD is almost uniformly better than qN -SAGA as a reference point for speed-ups (although, sometimes with small differences), we focus on these algorithms alongside SGD as Straw Man and q-SAGA as a reference point for speed-ups. We have chosen q = 50 in q-SAGA and (based on some rough experiments) that the average neighborhood size q-20 has been used the same setting across all datasets and experiments. Data sets As special cases for the choice of loss function and regulator in Eq. (1) we consider two common problems in machine learning, namely at least quadratic regression and \"2-quadratic regression.\""}, {"heading": "5 Conclusion", "text": "We proposed a novel analysis method for reducing variance of SGD methods, which shows geometric convergence rates and provides a number of new insights, in particular on the role of the freshness of stochastic gradients evaluated in previous iterations. We also investigated the effects of additional errors in the variance correction conditions on convergence behavior. For this reason, we proposed N-SAGA, a modification of SAGA that can achieve consistent and sometimes significant accelerations in the initial phase of optimization. Most noteworthy is that this algorithm can be executed in a streaming mode, which, to our knowledge, is the first of its kind within the family of variance-reduced methods."}], "references": [{"title": "Optimal data-dependent hashing for approximate near neighbors", "author": ["A. Andoni", "I. Razenshteyn"], "venue": "arXiv preprint arXiv:1501.01062,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "In COMPSTAT,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Saga: A fast incremental gradient method with support for non-strongly convex composite objectives", "author": ["A. Defazio", "F. Bach", "S. Lacoste-Julien"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Scalable training of mixture models via coresets", "author": ["D. Feldman", "M. Faulkner", "A. Krause"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["R. Johnson", "T. Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Semi-stochastic gradient descent methods", "author": ["J. Kone\u010dn\u1ef3", "P. Richt\u00e1rik"], "venue": "arXiv preprint arXiv:1312.1666,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "The annals of mathematical statistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1951}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["M. Schmidt", "N.L. Roux", "F. Bach"], "venue": "arXiv preprint arXiv:1309.2388,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter"], "venue": "Mathematical programming,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Stochastic dual coordinate ascent methods for regularized loss", "author": ["S. Shalev-Shwartz", "T. Zhang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}], "referenceMentions": [{"referenceID": 1, "context": "Stochastic gradient descent (SGD) is a popular alternative, in particular in the context of large-scale learning [2, 9].", "startOffset": 113, "endOffset": 119}, {"referenceID": 8, "context": "Stochastic gradient descent (SGD) is a popular alternative, in particular in the context of large-scale learning [2, 9].", "startOffset": 113, "endOffset": 119}, {"referenceID": 9, "context": "It is a surprising recent finding [10, 5, 8, 6] that the additive structure of f allows for significantly faster convergence in expectation.", "startOffset": 34, "endOffset": 47}, {"referenceID": 4, "context": "It is a surprising recent finding [10, 5, 8, 6] that the additive structure of f allows for significantly faster convergence in expectation.", "startOffset": 34, "endOffset": 47}, {"referenceID": 7, "context": "It is a surprising recent finding [10, 5, 8, 6] that the additive structure of f allows for significantly faster convergence in expectation.", "startOffset": 34, "endOffset": 47}, {"referenceID": 5, "context": "It is a surprising recent finding [10, 5, 8, 6] that the additive structure of f allows for significantly faster convergence in expectation.", "startOffset": 34, "endOffset": 47}, {"referenceID": 6, "context": "While the classic convergence proof of SGD requires vanishing step sizes, typically at a rate of O(1/t) [7],", "startOffset": 104, "endOffset": 107}, {"referenceID": 2, "context": "SAGA The SAGA algorithm [3] maintains variance corrections \u03b1i after selecting data point i by memorizing stochastic gradients.", "startOffset": 24, "endOffset": 27}, {"referenceID": 4, "context": "SVRG To show the fruitfulness of our framework, we reformulate a variant of Stochastic Variance Reduced Gradient (SVRG) [5].", "startOffset": 120, "endOffset": 123}, {"referenceID": 5, "context": "We use a randomization argument similar to (but much simpler than) the one suggested in [6] and define a real-valued parameter q > 0.", "startOffset": 88, "endOffset": 91}, {"referenceID": 3, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "(ii) We can use locality-sensitive hashing to index these k data points, perhaps even using data-dependent hashing [1], whenever this cost can be amortized.", "startOffset": 115, "endOffset": 118}, {"referenceID": 2, "context": "[3]), which exploit strong convexity of f (wherever \u03bc appears) as well as Lipschitz continuity of the fi-gradients (wherever L appears): \u3008f \u2032 i(w), w \u2212 w\u2217\u3009 \u2265 f(w)\u2212 f(w\u2217) + \u03bc 2 \u2016w \u2212 w\u2217\u20162 , (5) E\u2016gi(w)\u2016 \u2264 (1 + \u03b2)E\u2016f \u2032 i(w)\u2212 f \u2032 i(w)\u2016 \u2212 \u03b2\u2016f \u2032(w)\u20162 + ( 1 + \u03b2\u22121 ) E\u2016\u1fb1i \u2212 f \u2032 i(w)\u2016 (\u03b2 > 0), (6) \u2016f \u2032(w)\u20162 \u2265 2\u03bc (f(w)\u2212 f(w\u2217)) , (7) \u2016f \u2032 i(w)\u2212 f \u2032 i(w)\u2016 \u2264 2Lhi(w), hi(w) := fi(w)\u2212 fi(w) + \u3008w \u2212 w\u2217, f \u2032 i(w)\u3009 , (8) E\u2016f \u2032 i(w)\u2212f \u2032 i(w)\u2016 \u2264 2L(f(w)\u2212 f(w\u2217)) , (9) E\u2016\u1fb1i \u2212 f \u2032 i(w))\u2016 = E\u2016\u03b1i \u2212 f \u2032 i(w)\u2016 \u2212 \u2016\u1fb1\u2016 \u2264 E\u2016\u03b1i \u2212 f \u2032 i(w)\u2016 (10)", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Inspired by the Lyapunov function method in [3] we define upper bounds Hi \u2265 \u2016\u03b1i \u2212 f \u2032 i(w)\u2016 such that Hi \u2192 0 as w \u2192 w\u2217.", "startOffset": 44, "endOffset": 47}, {"referenceID": 2, "context": "We now define a Lyapunov function (which is much simpler than in [3])", "startOffset": 65, "endOffset": 68}], "year": 2017, "abstractText": "Stochastic Gradient Descent (SGD) is a workhorse in machine learning, yet it is also known to be slow relative to steepest descent. The variance in the stochastic update directions only allows for sublinear or (with iterate averaging) linear convergence rates. Recently, variance reduction techniques such as SVRG and SAGA have been proposed to overcome this weakness. With asymptotically vanishing variance, a constant step size can be maintained, resulting in geometric convergence rates. However, these methods are either based on occasional computations of full gradients at pivot points (SVRG), or on keeping per data point corrections in memory (SAGA). This has the disadvantage that one cannot employ these methods in a streaming setting and that speed-ups relative to SGD may need a certain number of epochs in order to materialize. This paper investigates a new class of algorithms that can exploit neighborhood structure in the training data to share and re-use information about past stochastic gradients across data points. While not meant to be offering advantages in an asymptotic setting, there are significant benefits in the transient optimization phase, in particular in a streaming or singleepoch setting. We investigate this family of algorithms in a thorough analysis and show supporting experimental results. As a side-product we provide a simple and unified proof technique for a broad class of variance reduction algorithms.", "creator": "LaTeX with hyperref package"}}}