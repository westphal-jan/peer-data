{"id": "1506.03736", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2015", "title": "GAP Safe screening rules for sparse multi-task and multi-class models", "abstract": "High dimensional regression benefits from sparsity promoting regularizations. Screening rules leverage the known sparsity of the solution by ignoring some variables in the optimization, hence speeding up solvers. When the procedure is proven not to discard features wrongly the rules are said to be \\emph{safe}. In this paper we derive new safe rules for generalized linear models regularized with $\\ell_1$ and $\\ell_1/\\ell_2$ norms. The rules are based on duality gap computations and spherical safe regions whose diameters converge to zero. This allows to discard safely more variables, in particular for low regularization parameters. The GAP Safe rule can cope with any iterative solver and we illustrate its performance on coordinate descent for multi-task Lasso, binary and multinomial logistic regression, demonstrating significant speed ups on all tested datasets with respect to previous safe rules.", "histories": [["v1", "Thu, 11 Jun 2015 16:25:36 GMT  (504kb,D)", "https://arxiv.org/abs/1506.03736v1", null], ["v2", "Wed, 18 Nov 2015 10:07:20 GMT  (534kb,D)", "http://arxiv.org/abs/1506.03736v2", "in Proceedings of the 29-th Conference on Neural Information Processing Systems (NIPS), 2015"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG math.OC stat.CO", "authors": ["eug\u00e8ne ndiaye", "olivier fercoq", "alexandre gramfort", "joseph salmon"], "accepted": true, "id": "1506.03736"}, "pdf": {"name": "1506.03736.pdf", "metadata": {"source": "CRF", "title": "GAP Safe screening rules for sparse multi-task and multi-class models", "authors": ["Eug\u00e8ne Ndiaye", "Olivier Fercoq", "Alexandre Gramfort", "Joseph Salmon"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, if we want to solve the problems, we are able to get to grips with them."}, {"heading": "2 GAP Safe rules", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Model and notations", "text": "We denote by rds the set t1,.., you for each integer d P N, and by QJ the transposition of a matrix Q. Our observation matrix is Y P RnB, where n represents the number of samples and q the number of tasks or classes. Design matrix X \"rxp1q,., xppqs\" rx1,., xnsJ P RnB has p explanatory variables (or characters) column by column and n observations row by row. Default matrix X \"2 is written,\" the \"1 norm,\" the \"8 norm.\" The \"2-unit sphere is denoted by B2 (or simply B), and we write Bpc, rq the\" 2-sphere with center c and radius r. \"For a matrix B P Rp1.\""}, {"heading": "2.2 Basic properties", "text": "First, let us recall the state of Fermat and a dual formulation of the optimization problem: 1q is often referred to as the (convex) conjugate of a function theorem (see [3, sentence 26.1] for a more general result).A dual formulation of (1) is given for each convex function f: Rn C: x P arg min xPRn fpxq 0 P Bfpxq. (2) Theorem 2 ([9]. In fact, a dual formulation of (1) is circumvented."}, {"heading": "2.3 Critical parameter: \u03bbmax", "text": "Due to the nature of the dual standard, this is equivalent to the double standard. Since the double standard is the double standard, 0 is a primary solution to the double standard if and only if it applies: \"maxjPrps\" xpjqJGp0q \"2.\" This development shows that the problem (1) is trivial from the outset. Therefore, from now on, we will only focus on the case in question."}, {"heading": "2.4 Screening rules description", "text": "Safe screening rules are based on a simple sequence of the state of the fermat:} xpjqJp\u03bbq} 2 \"1\" pBp\u03bbqj,: \"0.\" (6) Expressed in this way, this relationship is useless, because p\u0121p\u03bbq is unknown (unless it is \u03bb. \"max.\" However, it is often possible to construct a series R. \"Rn\u0441q,\" a so-called safe region, which mitigates the problem. Note then that the variable J is removed from the previous test, since pBp\u03bbqj,: is then guaranteed to be zero. This property results in considerable acceleration in practice, especially with active set strategies, see for example [11] for the lasso case. A natural goal is to find safe regions as close as possible: smaller safe regions can only increase the number of selected pjq."}, {"heading": "2.5 Spheres as safe regions", "text": "In practice, different shapes have been considered for the set R, such as balls (referred to as balls) [9], domes [11] or more refined quantities (see [23] for an overview).Here we look at the so-called \"spherical regions,\" which select a ball R \"Bpc, rq as a safe region. It is easy to gain control over max\u0442 PBpc, rq} xpjqJ\u044b} 2 by extending the calculation of the support function of a ball [11, eq. (9)] to the matrix case: maxPBpc, rq} xpjqJ\u043a} 2 \u0445} xpjqJc} 2'r} xpjqJq} 2. Note that the center c here is a matrix in Rp\u0439 q. We can now specify the test of the safe ball: If} xpjqJc} 2'r} xpjq} 1, then pBpqqj,\" 0 \"(8)."}, {"heading": "2.6 GAP Safe rule description", "text": "In this section we derive a GAP safe screening rule that extends the one introduced in [11]. For this we rely on the strong convexity of dual lens function and on weak duality find a radius: Remember that @ i P rns, fi with a 1 {\u03b3-lipschitz gradient differentiable ist.Infolgedem is @ i P rns, D.p.1q 'x-D.p.1q' x-D.p.p.p.1q 'x-p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p..p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p"}, {"heading": "By weak duality @B P Rp\u02c6q, D\u03bbpp\u0398p\u03bbqq \u010f P\u03bbpBq, so : @B P Rp\u02c6q,@\u0398 P \u2206X , D\u03bbp\u0398q \u010f P\u03bbpBq\u00b4 \u03b3\u03bb22 }p\u0398p\u03bbq\u00b4\u0398}2,", "text": "And we deduct the following theorems: \"We have a problem that we cannot solve.\" \"We need a dynamic center to improve screening.\" \"We need a dynamic center.\" \"We need a dynamic center.\" \"We need it.\" \"We.\" \"We.\" \"We.\" \"We.\". \"\" We. \"\" \"We.\" \"\" We. \"\" \"We.\" \"\". \"\" \"\" \"We.\" \"\" \"\" \"\" We. \"\" \"\" \"\". \"\" \"\". \"\". \".\". \".\". \".\" \".\" \".\" \".\" \"\" \".\" \"\" \"\". \"\" \"\" \".\" \"\" \".\" \"\". \".\". \".\" \".\". \".\" \"\". \".\". \"\" \".\" \".\" \"\". \".\". \"\" \"\" \".\" \".\" \".\" \".\". \".\" \"\". \"\". \"\" \"\". \".\" \"\" \"\". \"\". \".\" \".\" \"\". \".\". \"\" \".\" \".\" \".\" \"\". \".\" \".\" \".\" \"\". \".\". \"\" \".\". \"\" \"\". \".\". \"\". \"\". \".\". \"\" \"\". \".\" \"\". \".\". \"\". \".\" \"\". \".\" \"\" \".\". \"\". \".\". \"\" \".\". \"\" \".\". \"\". \".\". \".\". \".\" \".\". \".\". \".\" \".\". \".\". \"\". \".\". \"\". \".\". \"\". \".\". \".\" \".\". \".\" \"\". \".\" \"\". \".\" \"\". \"\". \".\". \".\". \"\" \"\" \".\". \".\" \".\". \".\" \".\". \".\". \".\". \".\". \".\". \".\" \"\". \".\". \".\""}, {"heading": "3 Special cases of interest", "text": "We now specialise our results in relevant monitored learning problems, see also Table 1."}, {"heading": "3.1 Lasso", "text": "In lasso case q \"1, the parameter is a vector: B\" \u03b2 P Rp, F p\u03b2q \"1 {2} y\" X\u03b2} 22 \"\u0159n i\" 1pyi \"xJi \u03b2q2, which means that fipzq\" pyi \"zq2 {2 and \u0435p\u03b2q\"} \u03b2} 1.3.2 \"1 {\" 2 Multi-task regression In the multi-task lasso, which is a special case of group lasso, we assume that it is Y P Rn\u0445 q, F pBq \"12} Y'XB} 22\" 12 \u0159n i \"1} Yi,:\" xJi B} 22 (i.e. fipzq \"} Yi,:\" z} 2 {2) and to \"pBq\" \u0159p \"j\" 1} Bj,:} 2. In signal processing, this model is also referred to as Multiple Measurement Vector (MV). It allows us to select the various tasks together with the Remark-1, which allows us to parse most of them in the frame."}, {"heading": "3.3 `1 regularized logistic regression", "text": "In such a context one considers for each i P rns a class name ci P t1, 2u. This information can be reformulated as yi \"1tci\" 1u, and it is then customary to minimize (1), whereas F \"n\" 1 \"ixJi\" 1. \"exp\" xJi. \"(11) with B\" \u03b2 P Rp (i.e., q \"1), fipzq\" yiz \"logp1\" exppzqq \"and the penalty is simply the\" 1. \"exp\" xJi \"1. Let us define Nh, the (binary) negative entropy function defined by 2: Nhpxq\" # x logpxq \"p1.\""}, {"heading": "4 Experiments", "text": "In this section we present the results obtained with the GAP-Safe rule. Results are based on high-dimensional data, both dense and sparse. Implementation was carried out in Python and Cython for low critical parts. They are based on the multi-task lasso implementation of Scikit Learn [17] and coordinate Descent Logistic Regression Solver in the Lightning software [4]. In all experiments, the used Coordinate Descent algorithm follows the pseudo-code of [11] with a screening step every 10 iterations. Note that we have not performed a comparison with the sequential screening rule, commonly known as the state-of-the-art \"safe\" screening rule (such as th EDDP + [21]), as we can show that this type of rule is not safe. In fact, the stop criterion is based on two gap accuracy, and comparisons would be unfair, as such methods sometimes do not occur with the prescribed accuracy."}, {"heading": "4.2 `1 binary logistic regression", "text": "We compare the dynamic strategy of GAP Safe with a sequential and non-dynamic rule such as Slores [22]. We do not compare it with the actual Slores rule, as it requires the previous dual optimal solution, which is not available. Slores is not a safe method in fact (see Section B in the Supplementary Materials). However, it can be observed that dynamic strategies exceed pure sequential screening, see Section C in the Supplementary Material).4.3 '1' 2 Multinomial logistic regression We have also applied GAP Safe to a multinomial logistic regression problem on a sparse dataset. Data are wordbook features extracted from the News20 dataset (TF-IDF, which removes English stopwords and words that occur only once or more than 95% of the time)."}, {"heading": "5 Conclusion", "text": "This paper describes new safe rules for accelerating algorithms to solve generalized linear models regulated by '1 and' 1 {'2 standards; the proposed rules are safe, easy to implement, dynamic, and converge, discarding significantly more variables than alternative safe rules; the positive impact on computing time has been observed on all the data sets tested and demonstrated here in a high-dimensional regression task using brain imaging data, as well as problems with binary and multi-class classifications on dense and sparse data; extensions to other generalized linear models, such as Poisson regression, are likely to come to the same conclusion; future work could examine the optimal frequency of screening and determine when screening correctly detected the carrier."}, {"heading": "Acknowledgment", "text": "We appreciate the support of the Chair of Machine Learning for Big Data at Te'le'com ParisTech and the Orange / Te'le'com ParisTech think tank phi-TAB. This work benefited from the support of the \"Gaspard Monge FMJH Programme in Optimisation and Operations Research\" and EDF's support for this programme."}, {"heading": "A Proofs", "text": "\"There's no reason why we shouldn't be able to see ourselves not being able to play by the rules,\" he said. \"There's no reason why we shouldn't play by the rules.\" \"There's no reason why we shouldn't play by the rules.\" \"There's no reason why we shouldn't play by the rules.\" \"There's no reason why we shouldn't play by the rules.\" \"There's no reason why we shouldn't play by the rules.\" \"There's no reason why we shouldn't play by the rules.\" \"There's no reason why we shouldn't play by the rules.\" \"There's no reason why we shouldn't play by the rules.\" \"There's no reason why we shouldn't play by the rules.\" \"There's no reason why we shouldn't play by the rules.\""}, {"heading": "B EDPP is not safe", "text": "In the last two sections, we present a study of the EDDP method [21], a screening rule based on the double optimal point reached for the previous sections. [21] Note: The same conclusion would apply to the generalization of the sequential approach given in [22], as well as to any other screening rule that requires an exact dual solution in a single step. [21] To simplify the reading, we use the vectorial (non-capital letters) notation that was previously used. [21] In practice, we choose the common grid [7]. [20] \"We do not have an exact sequence of T.\" Wang et al has proposed a sequential screening rule based on the properties of the projection. [21] Your rule is based on the exact knowledge of the true optimal solution for the previous parameters."}, {"heading": "C Making EDDP screening rule safe", "text": "In the present work, we give calculable guarantees about the distance between the current double point and the solution of the problem. \"To make it easier, we first consider the original version of Wang et al.\" \"The first version of Wang et al.\" \"The second version of Wang et al.\" \"The first version of Wang et al.\" \"The second version of Wang et al.\" \"The first version of Wang et al.\" \"The second version of Wang et al.\" \"The first version of Wang et al.\" \"The second version of Wang et al.\" \"The second version of Wang et al.\". \"\" The second version of Wang et al. \"\" The third version of Wang et al.. \"\" \"The third version of Wang et al.\" \"\" The third version of Wang et al. \"\" \"The third version of Wang et al.\" \"\" The third version of Wang et al. \"\" \"The third version of Wang et al.\" \"\" \""}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "<lb>High dimensional regression benefits from sparsity promoting regularizations. Screening rules leverage<lb>the known sparsity of the solution by ignoring some variables in the optimization, hence speeding up<lb>solvers. When the procedure is proven not to discard features wrongly the rules are said to be safe. In<lb>this paper we derive new safe rules for generalized linear models regularized with `1 and<lb>`1{`2 norms.<lb>The rules are based on duality gap computations and spherical safe regions whose diameters converge to<lb>zero. This allows to discard safely more variables, in particular for low regularization parameters. The<lb>GAP Safe rule can cope with any iterative solver and we illustrate its performance on coordinate descent<lb>for multi-task Lasso, binary and multinomial logistic regression, demonstrating significant speed ups on<lb>all tested datasets with respect to previous safe rules.", "creator": "LaTeX with hyperref package"}}}