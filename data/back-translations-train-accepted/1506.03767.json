{"id": "1506.03767", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2015", "title": "Spectral Representations for Convolutional Neural Networks", "abstract": "Discrete Fourier transforms provide a significant speedup in the computation of convolutions in deep learning. In this work, we demonstrate that, beyond its advantages for efficient computation, the spectral domain also provides a powerful representation in which to model and train convolutional neural networks (CNNs).", "histories": [["v1", "Thu, 11 Jun 2015 18:23:18 GMT  (3263kb,D)", "http://arxiv.org/abs/1506.03767v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["oren rippel", "jasper snoek", "ryan p adams"], "accepted": true, "id": "1506.03767"}, "pdf": {"name": "1506.03767.pdf", "metadata": {"source": "CRF", "title": "Spectral Representations for Convolutional Neural Networks", "authors": ["Oren Rippel"], "emails": ["rippel@math.mit.edu", "jsnoek@seas.harvard.edu", "rpa@seas.harvard.edu"], "sections": [{"heading": null, "text": "In this paper, we demonstrate that the spectral range, in addition to its advantages for efficient calculation, also provides a powerful representation for modelling and training Convolutionary Neural Networks (CNNs).We use spectral representations to introduce a number of innovations into CNN design. First, we propose spectral pooling, which performs dimensionality reduction by reducing the representation in the frequency domain. This approach preserves significantly more information per parameter than other pooling strategies and allows flexibility in the choice of output dimensionality. This representation also allows a new form of stochastic regulation by random resolution modification. We show that these methods achieve competitive results in classification and approximation tasks without exposure or maximum pooling.Finally, we demonstrate the effectiveness of complex-coefficient spectral parameters by random resolution modification."}, {"heading": "1 Introduction", "text": "In fact, it is the case that one is able to go in search of a solution that is capable of finding a solution, that is capable of finding a solution, and that is able to find a solution that is capable of finding a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution."}, {"heading": "2 The Discrete Fourier Transform", "text": "In this section, we present a series of components of the DFT that are limited to the two-dimensional DFT, although all properties and results can be extended to other input dimensions. (D) D \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \""}, {"heading": "2.1 Conjugate symmetry constraints", "text": "In the following sections of the paper, we will propagate signals and their gradients through DFT and inverse DFT layers. In these layers, we will represent the frequency domain in the complex field. However, for all layers apart, we want to ensure that both the signal and its gradient are bound to reality. A necessary and sufficient condition to achieve this is frequency conjugate symmetry. Namely, for each transformation y = F (x) of any input x, it must contain the attribute = (M \u2212 m) ModM, (N \u2212 n) ModN (0,.,., M \u2212 1), namely n (0,.., N \u2212 1). (3) Thus, intuitively, given the left half of our frequency map, the reduced number of degrees of freedom must allow us to reconstruct the right domain. This allows us to store approximately half of the parameters that would otherwise be necessary to reduce effectiveness."}, {"heading": "2.2 Differentiation", "text": "Here we will discuss how to propagate the gradient through a Fourier transformation layer. Similarly, this analysis can be applied to the inverse DFT layer. Let's define x-RM \u00b7 N and y = F (x) as the input and output of a DFT layer, or R: RM \u00b7 N \u2192 R as a real loss function applied to y, which can be considered as the rest of the forward process. Since the DFT is a linear operator, its gradient is simply the transformation matrix itself. (4) During the reverse propagation, this gradient is then conjugated, and this corresponds to the application of the reverse transformation by DFT unit: XX = F \u2212 1 (Yankee R-X-Y). (4) There is a complexity that makes things a bit more complicated."}, {"heading": "3 Spectral Pooling", "text": "In fact, it is such that it is a matter of a way in which people are able to put themselves in the world, in which they are able to put themselves in the world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they live."}, {"heading": "3.1 Information preservation", "text": "Spectral pooling can significantly increase the amount of information stored in comparison to max pooling in two ways: First, its representation obtains more information for the same number of degrees of freedom. Spectral pooling reduces the information capacity by precisely adjusting the resolution of the input to the desired output dimensionality, which can also be considered linear low-pass filtering and takes advantage of the unevenness of the spectral density of the data in relation to frequency, which means that the power spectra of spatial inputs, such as natural images, carry most of their mass at lower frequencies. As the amplitudes of higher frequencies tend to be small, Parseval's theorem from Section 2 informs us that their elimination will result in a representation that minimizes the distortion of '2 after reconstruction."}, {"heading": "3.2 Regularization via resolution corruption", "text": "We find that the low-pass filter radii, say RH and RW, can be selected to be smaller than the dimensions of the output board H, W. Because while we shorten our input frequency card to size H \u00b7 W, we can continue to set all frequencies outside the central RH \u00b7 RW square from zero to zero. This allows us to regularize in the form of a random resolution. We apply this stochastically by assigning a distribution pR (\u00b7) to the frequency truncation radius (for convenience, we apply the same cut to both axes), scan a random radius from it at each iteration, and wipe out all frequencies outside the square of that size. Note that this can be considered an application of a nested dropout (Ripple al 2014, \u00b7 Success in selecting the dimensions of U = equal resolution)."}, {"heading": "4 Spectral Parametrization of CNNs", "text": "This offers considerable advantages over the traditional spatial representation we see in Section 5.Let us suppose that we are trying to learn filters of size H \u00d7 W for one layer of our Convolutionary Neural Network. To do this, we parameterise each filter f-CH \u00d7 W in our network directly in the frequency domain. To achieve its spatial representation, we simply calculate its inverse DFT as F-1 (f) and RH \u00b7 W. From this point on, we proceed as with any standard CNN, by calculating the convolution of the filter with inputs in our mini-batch, and so on. The backpropagation through the inverse DFT is virtually identical to the spectral pooling method described in Section 3.We calculate the gradient as outlined in Section 2.2, taking care to obey the conjugate symmetry."}, {"heading": "4.1 Leveraging filter structure", "text": "This idea takes advantage of the observation that CNN filters have a very characteristic structure that reappears across datasets and problem areas, which means that CNN weights can typically be detected with a small number of degrees of freedom. However, in the spatial range, this results in significant redundancy. On the other hand, the frequency domain provides an attractive basis for the representation of filters: Characteristic filters (e.g. gabor filters) are often very local in their spectral representations, which follows from the observation that filters tend to have very specific length scales and orientations, and therefore they tend not to have support in a narrow set of frequency components. This hypothesis can be observed qualitatively in Figure 3 (a) and quantitatively in Figure 3 (b). Empirically, in Section 5, spectral representations of filters lead to convergence that is two or five-fold accelerated."}, {"heading": "5 Experiments", "text": "We demonstrated the effectiveness of spectral imaging in a number of different experiments. We performed all experiments with code optimized for the Xeon Phi coprocessor. We used Spearmint (Snoek et al., 2015) for Bayesian hyperparameter optimization with 5-20 simultaneous evaluations."}, {"heading": "5.1 Spectral pooling", "text": "For the different pooling strategies, we record the average approximation loss resulting from pooling to different dimensions, as can be seen in Figure 4. We observe the two aspects discussed in Figure 3.1: First, spectral pooling provides a significantly better reconstruction for the same number of parameters; second, the only button that controls the coarseness of the approximation that results in severe quantification and restriction of the lower limit of the information obtained (marked as a horizontal red line in the figure); and, in contrast, spectral pooling allows the selection of any output dimension, resulting in a smooth curve across all frequency separations. Classification with Convolutionary Neural Networks We test spectral pooling on different classification tasks."}, {"heading": "5.2 Spectral parametrization of CNNs", "text": "We show the effectiveness of spectral parameterization on a number of CNN optimization tasks, for different architectures and for different filter sizes. We use the MPTS notation to denote a maximum pooling layer with size S and increment T, and FCF is a fully connected layer with F filters. The first architecture is the generic one used in a variety of deep learning tasks, such as Krizhevsky et al. (2012); Snoek et al. (2012); Krizhevsky (2009); Kingma & Ba (2015): C963 x 3 \u2192 MP23 x 3 \u2192 C1923 x 3 \u2192 MP23 x 3 \u2192 FC1024 \u2192 Softmax (6) The second architecture we are looking at is the one used in Snoek et al al al al al al. (2015), which has been shown to achieve competitive classification rates. It is deeper and more complex: C 96 x 3 \u2192 C963 \u2192 Cx \u2192 Cropx 19x C23 x x x Cx x x 9x x x x MP3."}, {"heading": "6 Discussion and remaining open problems", "text": "In this paper, we have shown that spectral representations offer a wide range of applications. We have introduced spectral pooling, which enables pooling to any desired output dimensionality while providing significantly more information than other pooling approaches. Furthermore, we have shown that the Fourier functions provide a suitable basis for filter parameterization, as evidenced by the faster convergence of optimization processes. A possible future line of work is to embed the network in its entirety in the frequency domain. In models that convert Fourier to constitute volumes, the input layer is FFT-ed and the elementary multiplication outputs are then inverse FFT-ed."}], "references": [{"title": "Scaling learning algorithms towards AI", "author": ["Bengio", "Yoshua", "LeCun", "Yann"], "venue": "Large Scale Kernel Machines. MIT Press,", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Invariant scattering convolution networks", "author": ["Bruna", "Joan", "Mallat", "Stephane"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Bruna et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bruna et al\\.", "year": 2013}, {"title": "URL http://dblp.uni-trier.de/db/journals/corr/ corr1302.html#abs-1302-4389", "author": ["Goodfellow", "Ian J", "Warde-Farley", "David", "Mirza", "Mehdi", "Courville", "Aaron C", "Bengio", "Yoshua"], "venue": "Maxout networks. CoRR,", "citeRegEx": "Goodfellow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "What\u2019s wrong with convolutional nets? MIT Brain and Cognitive Sciences - Fall Colloquium Series, Dec 2014a. URL http://techtv.mit.edu/collections/bcs/videos/ 30698-what-s-wrong-with-convolutional-nets", "author": ["Hinton", "Geoffrey"], "venue": null, "citeRegEx": "Hinton and Geoffrey.,? \\Q2014\\E", "shortCiteRegEx": "Hinton and Geoffrey.", "year": 2014}, {"title": "Ask me anything: Geoffrey hinton", "author": ["Hinton", "Geoffrey"], "venue": "Reddit Machine Learning,", "citeRegEx": "Hinton and Geoffrey.,? \\Q2014\\E", "shortCiteRegEx": "Hinton and Geoffrey.", "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "shift. CoRR,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Largescale video classification with convolutional neural networks", "author": ["Karpathy", "Andrej", "Toderici", "George", "Shetty", "Sanketh", "Leung", "Thomas", "Sukthankar", "Rahul", "Fei-Fei", "Li"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex"], "venue": "Technical report,", "citeRegEx": "Krizhevsky and Alex.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Alex.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Handwritten digit recognition with a back-propagation network", "author": ["LeCun", "Yann", "Boser", "Bernhard", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Fast training of convolutional networks through FFTs", "author": ["Mathieu", "Micha\u00ebl", "Henaff", "Mikael", "LeCun", "Yann"], "venue": "CoRR, abs/1312.5851,", "citeRegEx": "Mathieu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mathieu et al\\.", "year": 2013}, {"title": "Generic deep networks with wavelet scattering", "author": ["Oyallon", "Edouard", "Mallat", "St\u00e9phane", "Sifre", "Laurent"], "venue": "CoRR, abs/1312.5940,", "citeRegEx": "Oyallon et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Oyallon et al\\.", "year": 2013}, {"title": "Learning ordered representations with nested dropout", "author": ["Rippel", "Oren", "Gelbart", "Michael A", "Adams", "Ryan P"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Rippel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rippel et al\\.", "year": 2014}, {"title": "Practical Bayesian optimization of machine learning algorithms", "author": ["Snoek", "Jasper", "Larochelle", "Hugo", "Adams", "Ryan Prescott"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Scalable Bayesian optimization using deep neural networks", "author": ["Snoek", "Jasper", "Rippel", "Oren", "Swersky", "Kevin", "Kiros", "Ryan", "Satish", "Nadathur", "Sundaram", "Narayanan", "Patwary", "Md. Mostofa Ali", "Prabhat", "Adams", "Ryan P"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Snoek et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2015}, {"title": "Statistics of natural image", "author": ["Torralba", "Antonio", "Oliva", "Aude"], "venue": "categories. Network,", "citeRegEx": "Torralba et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Torralba et al\\.", "year": 2003}, {"title": "Fast convolutional nets with fbfft: A GPU performance evaluation", "author": ["Vasilache", "Nicolas", "Johnson", "Jeff", "Mathieu", "Micha\u00ebl", "Chintala", "Soumith", "Piantino", "Serkan", "LeCun", "Yann"], "venue": "CoRR, abs/1412.7580,", "citeRegEx": "Vasilache et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vasilache et al\\.", "year": 2014}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "CoRR, abs/1301.3557,", "citeRegEx": "Zeiler et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 10, "context": "Convolutional neural networks (CNNs) (LeCun et al., 1989) have been used to achieve unparalleled results across a variety of benchmark machine learning problems, and have been applied successfully throughout science and industry for tasks such as large scale image and video classification (Krizhevsky et al.", "startOffset": 37, "endOffset": 57}, {"referenceID": 9, "context": ", 1989) have been used to achieve unparalleled results across a variety of benchmark machine learning problems, and have been applied successfully throughout science and industry for tasks such as large scale image and video classification (Krizhevsky et al., 2012; Karpathy et al., 2014).", "startOffset": 240, "endOffset": 288}, {"referenceID": 6, "context": ", 1989) have been used to achieve unparalleled results across a variety of benchmark machine learning problems, and have been applied successfully throughout science and industry for tasks such as large scale image and video classification (Krizhevsky et al., 2012; Karpathy et al., 2014).", "startOffset": 240, "endOffset": 288}, {"referenceID": 11, "context": "More recently, Mathieu et al. (2013); Vasilache et al.", "startOffset": 15, "endOffset": 37}, {"referenceID": 11, "context": "More recently, Mathieu et al. (2013); Vasilache et al. (2014) have demonstrated that convolution can be computed significantly faster using discrete Fourier transforms than directly in the spatial domain, even for tiny filters.", "startOffset": 15, "endOffset": 62}, {"referenceID": 13, "context": "Note that this can be regarded as an application of nested dropout (Rippel et al., 2014) on both dimensions of the frequency decomposition of our input.", "startOffset": 67, "endOffset": 88}, {"referenceID": 2, "context": "(b) Test errors on CIFAR-10/100 without data augmentation of the optimal spectral pooling architecture, as compared to current state-of-the-art approaches: stochastic pooling (Zeiler & Fergus, 2013), Maxout (Goodfellow et al., 2013), networkin-network (Lin et al.", "startOffset": 207, "endOffset": 232}, {"referenceID": 15, "context": "We used Spearmint (Snoek et al., 2015) for Bayesian optimization of hyperparameters with 5-20 concurrent evaluations.", "startOffset": 18, "endOffset": 38}, {"referenceID": 9, "context": "The first architecture is the generic one used in a variety of deep learning papers, such as Krizhevsky et al. (2012); Snoek et al.", "startOffset": 93, "endOffset": 118}, {"referenceID": 9, "context": "The first architecture is the generic one used in a variety of deep learning papers, such as Krizhevsky et al. (2012); Snoek et al. (2012); Krizhevsky (2009); Kingma & Ba (2015): C 3\u00d73 \u2192 MP3\u00d73 \u2192 C 3\u00d73 \u2192 MP3\u00d73 \u2192 FC \u2192 FC \u2192 Softmax (6)", "startOffset": 93, "endOffset": 139}, {"referenceID": 9, "context": "The first architecture is the generic one used in a variety of deep learning papers, such as Krizhevsky et al. (2012); Snoek et al. (2012); Krizhevsky (2009); Kingma & Ba (2015): C 3\u00d73 \u2192 MP3\u00d73 \u2192 C 3\u00d73 \u2192 MP3\u00d73 \u2192 FC \u2192 FC \u2192 Softmax (6)", "startOffset": 93, "endOffset": 158}, {"referenceID": 9, "context": "The first architecture is the generic one used in a variety of deep learning papers, such as Krizhevsky et al. (2012); Snoek et al. (2012); Krizhevsky (2009); Kingma & Ba (2015): C 3\u00d73 \u2192 MP3\u00d73 \u2192 C 3\u00d73 \u2192 MP3\u00d73 \u2192 FC \u2192 FC \u2192 Softmax (6)", "startOffset": 93, "endOffset": 178}, {"referenceID": 14, "context": "The second architecture we consider is the one employed in Snoek et al. (2015), which was shown to attain competitive classification rates.", "startOffset": 59, "endOffset": 79}, {"referenceID": 12, "context": "While wavelets have been employed throughout machine learning with great promise (Bruna & Mallat, 2013; Oyallon et al., 2013), to our knowledge they have not been used in an adaptive way to learn CNNs.", "startOffset": 81, "endOffset": 125}], "year": 2015, "abstractText": "Discrete Fourier transforms provide a significant speedup in the computation of convolutions in deep learning. In this work, we demonstrate that, beyond its advantages for efficient computation, the spectral domain also provides a powerful representation in which to model and train convolutional neural networks (CNNs). We employ spectral representations to introduce a number of innovations to CNN design. First, we propose spectral pooling, which performs dimensionality reduction by truncating the representation in the frequency domain. This approach preserves considerably more information per parameter than other pooling strategies and enables flexibility in the choice of pooling output dimensionality. This representation also enables a new form of stochastic regularization by randomized modification of resolution. We show that these methods achieve competitive results on classification and approximation tasks, without using any dropout or max-pooling. Finally, we demonstrate the effectiveness of complex-coefficient spectral parameterization of convolutional filters. While this leaves the underlying model unchanged, it results in a representation that greatly facilitates optimization. We observe on a variety of popular CNN configurations that this leads to significantly faster convergence during training.", "creator": "LaTeX with hyperref package"}}}