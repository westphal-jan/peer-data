{"id": "1506.03877", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2015", "title": "Bidirectional Helmholtz Machines", "abstract": "Unsupervised training of deep generative models containing latent variables and performing inference remains a challenging problem for complex, high dimen- sional distributions. One basic approach to this problem is the so called Helmholtz machine and it involves training an auxiliary model that helps to perform approx- imate inference jointly with the generative model which is to be fitted to the train- ing data. The top-down generative model is typically realized as a directed model that starts from some prior at the top, down to the empirical distribution at the bot- tom. The approximate inference model runs in the opposite direction and is typi- cally trained to efficiently infer high probability latent states given some observed data. Here we propose a new method, referred to as geometric mean matching (GMM), that is based on the idea that the generative model should be close to the class of distributions that can be modeled by our approximate inference dis- tribution. We achieve this by interpreting both the top-down and the bottom-up directed models as approximate inference distributions and by defining the tar- get distribution we fit to the training data to be the geometric mean of these two. We present an upper-bound for the log-likelihood of this model and we show that optimizing this bound will pressure the model to stay close to the approximate inference distributions. In the experimental section we demonstrate that we can use this approach to fit deep generative models with many layers of hidden binary stochastic variables to complex and high dimensional training distributions.", "histories": [["v1", "Fri, 12 Jun 2015 00:08:20 GMT  (701kb,D)", "http://arxiv.org/abs/1506.03877v1", null], ["v2", "Tue, 7 Jul 2015 19:08:07 GMT  (691kb,D)", "http://arxiv.org/abs/1506.03877v2", null], ["v3", "Fri, 20 Nov 2015 18:48:00 GMT  (942kb,D)", "http://arxiv.org/abs/1506.03877v3", null], ["v4", "Mon, 4 Jan 2016 00:07:47 GMT  (1034kb,D)", "http://arxiv.org/abs/1506.03877v4", null], ["v5", "Wed, 25 May 2016 02:54:26 GMT  (880kb,D)", "http://arxiv.org/abs/1506.03877v5", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["j\u00f6rg bornschein", "samira shabanian", "asja fischer", "yoshua bengio"], "accepted": true, "id": "1506.03877"}, "pdf": {"name": "1506.03877.pdf", "metadata": {"source": "CRF", "title": "Training opposing directed models using geometric mean matching", "authors": ["J\u00f6rg Bornschein", "Samira Shabanian", "Asja Fischer", "Yoshua Bengio"], "emails": [], "sections": [{"heading": "1 Introduction and background", "text": "This is particularly true for models with multiple layers of deterministic or stochastic variables, which is regrettable because it has previously been argued [1, 2] that deeper generative models have the potential to capture higher abstractions and thus better generalize. Helmholtz Machine [3, 4] introduced a concept that proposed not only to fit a powerful but intractable generative model p (x, h) to the training data, but also to jointly perform an approximate inference model q (h | x), which would be used to efficiently perform approximate inferencing via the latent variables h of the generative model. This basic idea has been applied and improved many times, first with the wake-sleep algorithm x (WS, 3, 5] and more recently with the variable autograph model x."}, {"heading": "2 Model definition and properties", "text": "We introduce the concept by defining a common probability distribution of three variables, whereby an observed vector x = q and two latent variable vectors q = q = = q = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "2.1 Alternative view using the Bhattacharyya distance", "text": "5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5.. 5.. 5. 5.. 5. 5.. 5. 5. 5.. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. (3. 5. 5. 5. 5. 5. 5. 5. (3. (3. (3.) 5. 5.) 5.) 5.) 5. 5. (3.) 5.) 5. 5. (3.) 5.) 5. 5. (3.) 5.) 5. 5.) 5.) 5."}, {"heading": "3 Inference and training with importance sampling", "text": "Based on the construction of p * (x) outlined in the previous section, we can define a wide range of possible models (q = q = q). We also have a wide range of potential training and appropriate inference methods that we could use to maximize the log p * * (x). In this text, we focus on binary latent and observed variables x and hl (q = q), where B (a) refers to the Bernoulli distribution for the value a with the probability parameter b and is the logistic sigmoid function. For our top level before p (h2), we use a factored Bernoulli distribution: p (h2) = b) refers to the Bernoulli distribution for the Bernoulli distribution (h2).We form an estimate of p * (x)."}, {"heading": "3.1 Sampling from the model p\u2217(x)", "text": "One way to roughly evaluate the normalization constant Z as a resample procedure is to select candidates from p (x, h1, h2) to sample and then obtain samples based on repetitions of meaning, which are effectively distributed according to p * (x).The resample weights can be written so that they are as\u03c9 = p * (x, h1, h2) p (x, h2) = \u221a q (x, h1, h2) p (x, h1, h2) p (x, h2, h2) p (x, h1, h2) p (x) p (12) = 1Z \u221a p (x) (x) q (h1 | x) q (h1) p (x, h2). (13) Unfortunately, these cannot be resolved because we have to evaluate the threshold constant q (x) = p * (x)."}, {"heading": "3.2 Conditional sampling and inpainting", "text": "We can use a similar method of meaning repetition to extract the samples from the conditional distributions, e.g. from p (h1 | x, h2). Here we select the sample from the mix distribution p (h1 | x, h2) = 1 / 2 p (h1 | h2) + 1 / 2 q (h1 | x), which should ensure that the sample from p (x) using important resamples for k = 1 to K do \u2022 Layer-wise sample x (k), h (k) 1, h (k) 2 p (x, h1, h2) for l = 1 to K do \u2022 Layer-wise sample latent variables h (l) 1, h (l) 2 (h1, h2 | k) End for the sample (k), h (k) End for estimating the limit p (x, h2, h2)."}, {"heading": "4 Experimental results", "text": "In this section, we present experimental results obtained when applying the algorithm to various small and medium-sized datasets. Our main goal is to ensure that the theoretical properties discussed in Section 2 are translated into a robust algorithm that delivers competitive results even when used with simple sigmoid network layers as conditional distributions. All models have been trained with RMSProp (see, for example, [12] for a description) with a mini-batch size of 100. We initialize all weights according to [13] and set all distortions to -1. Our implementation is available anonymized at http: / /."}, {"heading": "4.1 UCI binary datasets", "text": "To determine that GMM works with the importance of sampling as a training method in general, we applied it to the 8 binary data sets from the UCI data archive, evaluated e.g. in [14]. Results are summarized in Table 4.1. We report on two upper limits of the negative log probability of the model: Log p \u043a (x) and Log p (x) (see Equations 9 and 10). Both are conservative estimates of the model NLL because they overestimate by an unknown offset: the former because it is a conservative unbiased estimate of the probability of the approximate generative model p and not the model p *; the latter because it evaluates Log p * under the assumption Z = 1."}, {"heading": "4.2 Binarized MNIST", "text": "We use the MNIST dataset, which still exists in binarized form according to [17] and [17]. We train a model with 6 hidden layers containing 800,800,600,450,150 latent variables. We set the learning rate to 3-4 and otherwise use the same hyperparameters as in the previous experiments. The training of the model converts after \u2248 200 epochs. The importance of the sampling-based estimators for log p and log probability of 92.7 and 95.1 (on the MNIST test set). Also, the remark that both models overestimate the true model NLL by an unknown amount (see previous section). To show that the model uses a reasonable concept of binarized digital numbers to draw samples, the results are visualized in Figure 1."}, {"heading": "4.3 Toronto Face Database", "text": "Each training example is 48 x 48 pixels in size and we interpret the grayscale as the Bernoulli probability for the lowest layer. We observe that the training progresses rapidly in the first periods, but mostly only the midface learns. Over the next few hundred epochs, the training proceeds much slower, but the log-in-log probability increases steadily. Figure 3 A shows random samples of a model with 1000,700,700,300 latent variables in 4 hidden layers. It was trained at a learning rate of 3-10 \u2212 5; all other hyperparameters were set to the same values as before. Figure 3 B shows the results of painting experiments with the same model (50 ups and downs)."}, {"heading": "5 Conclusion and future work", "text": "We have introduced a new scheme to construct probabilistic generative models that are automatically regulated to be close to the approximate sequence distributions that we have available. Using the Bhattacharyya distance, we have derived a lower limit for the log probability and we have shown that the limit can be used to adapt deep generative models with multiple layers of latent variables to complex training distributions. Note that our definition of p forced us to choose a previous distribution q (x) that will be part of our generative model p (x, h), which differs from the typical variable approaches to the formation of Helmholtz machines, where we consider q (h | x) exclusively as an approximate sequence method that is a training example x, and where q (x) would be the (empirical) training distribution - something we cannot assume because q (x) is part of our model p."}, {"heading": "Acknowledgments", "text": "We thank the developers of Theano [22] and Blocks [23] for their great work."}], "references": [{"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E. Hinton", "Simon Osindero", "Yee Whye Teh"], "venue": "Neural Computation,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Learning deep architectures for AI", "author": ["Yoshua Bengio"], "venue": "Now Publishers,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "The wake-sleep algorithm for unsupervised neural networks", "author": ["Geoffrey E. Hinton", "Peter Dayan", "Brendan J. Frey", "Radford M. Neal"], "venue": "Science, 268:1558\u20131161,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1995}, {"title": "The Helmholtz machine", "author": ["Peter Dayan", "Geoffrey E Hinton", "Radford M Neal", "Richard S Zemel"], "venue": "Neural computation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1995}, {"title": "Varieties of helmholtz machine", "author": ["Peter Dayan", "Geoffrey E Hinton"], "venue": "Neural Networks,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1996}, {"title": "Auto-encoding variational bayes", "author": ["Durk P. Kingma", "Max Welling"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Danilo J. Rezende", "Shakir Mohamed", "Daan Wierstra"], "venue": "In ICML\u20192014,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Neural variational inference and learning in belief networks", "author": ["Andriy Mnih", "Karol Gregor"], "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Reweighted wake-sleep", "author": ["Jorg Bornschein", "Yoshua Bengio"], "venue": "In International Conference on Learning Representations (ICLR\u20192015),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Deep unsupervised learning using nonequilibrium thermodynamics", "author": ["Jascha Sohl-Dickstein", "Eric A. Weiss", "Niru Maheswaranathan", "Surya Ganguli"], "venue": "CoRR, abs/1503.03585,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Deep boltzmann machines", "author": ["Ruslan Salakhutdinov", "Geoffrey E Hinton"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Unit tests for stochastic optimization", "author": ["Tom Schaul", "Ioannis Antonoglou", "David Silver"], "venue": "arXiv preprint arXiv:1312.6055,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In AISTATS\u20192010,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "The Neural Autoregressive Distribution Estimator", "author": ["Hugo Larochelle", "Ian Murray"], "venue": "In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS\u20192011),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "A deep and tractable density estimator", "author": ["Benigno Uria Iain Murray", "Hugo Larochelle"], "venue": "In ICML\u20192014,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Deep autoregressive networks", "author": ["Karol Gregor", "Ivo Danihelka", "Andriy Mnih", "Charles Blundell", "Daan Wierstra"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Evaluating probabilities under high-dimensional latent variable models", "author": ["Iain Murray", "Ruslan Salakhutdinov"], "venue": "In NIPS\u201908,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Binarized mnist dataset. http://www.cs.toronto.edu/ \u0303larocheh/ public/datasets/binarized_mnist/binarized_mnist_[train|valid|test] .amat, 2011. URL http://www.cs.toronto.edu/ \u0303larocheh/public/datasets/ binarized_mnist/binarized_mnist_train.amat", "author": ["Hugo Larochelle"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "The Toronto face dataset", "author": ["Joshua Susskind", "Adam Anderson", "Geoffrey E. Hinton"], "venue": "Technical Report UTML TR 2010-001, U. Toronto,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Annealed importance sampling", "author": ["Radford M. Neal"], "venue": "Statistics and Computing,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2001}, {"title": "Accurate and conservative estimates of MRF log-likelihood using reverse annealing", "author": ["Yuri Burda", "Roger B. Grosse", "Ruslan Salakhutdinov"], "venue": "CoRR, abs/1412.8566,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In Proc. SciPy,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Blocks and Fuel: Frameworks for deep learning", "author": ["B. van Merri\u00ebnboer", "D. Bahdanau", "V. Dumoulin", "D. Serdyuk", "D. Warde-Farley", "J. Chorowski", "Y. Bengio"], "venue": "ArXiv e-prints,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "This is especially true for models with multiple layers of deterministic or stochastic variables, which is unfortunate because it has been argued previously [1, 2] that deeper generative models have the potential to capture higher-level abstractions and thus generalize better.", "startOffset": 157, "endOffset": 163}, {"referenceID": 1, "context": "This is especially true for models with multiple layers of deterministic or stochastic variables, which is unfortunate because it has been argued previously [1, 2] that deeper generative models have the potential to capture higher-level abstractions and thus generalize better.", "startOffset": 157, "endOffset": 163}, {"referenceID": 2, "context": "With the Helmholtz machine [3, 4], a concept was introduced that proposed to not only fit a powerful but intractable generative model p(x,h) to the training data, but also to jointly train an approximate inference model q(h|x).", "startOffset": 27, "endOffset": 33}, {"referenceID": 3, "context": "With the Helmholtz machine [3, 4], a concept was introduced that proposed to not only fit a powerful but intractable generative model p(x,h) to the training data, but also to jointly train an approximate inference model q(h|x).", "startOffset": 27, "endOffset": 33}, {"referenceID": 2, "context": "This basic idea has been applied and enhanced many times; initially with the wake-sleep algorithm (WS, [3, 5]) and more recently with the variational autoencoder (VAE, [6]), stochastic backpropagation and approximate inference in deep generative models [7], neural variational inference and learning (NVIL, [8]) and reweighted wake-sleep (RWS, [9]).", "startOffset": 103, "endOffset": 109}, {"referenceID": 4, "context": "This basic idea has been applied and enhanced many times; initially with the wake-sleep algorithm (WS, [3, 5]) and more recently with the variational autoencoder (VAE, [6]), stochastic backpropagation and approximate inference in deep generative models [7], neural variational inference and learning (NVIL, [8]) and reweighted wake-sleep (RWS, [9]).", "startOffset": 103, "endOffset": 109}, {"referenceID": 5, "context": "This basic idea has been applied and enhanced many times; initially with the wake-sleep algorithm (WS, [3, 5]) and more recently with the variational autoencoder (VAE, [6]), stochastic backpropagation and approximate inference in deep generative models [7], neural variational inference and learning (NVIL, [8]) and reweighted wake-sleep (RWS, [9]).", "startOffset": 168, "endOffset": 171}, {"referenceID": 6, "context": "This basic idea has been applied and enhanced many times; initially with the wake-sleep algorithm (WS, [3, 5]) and more recently with the variational autoencoder (VAE, [6]), stochastic backpropagation and approximate inference in deep generative models [7], neural variational inference and learning (NVIL, [8]) and reweighted wake-sleep (RWS, [9]).", "startOffset": 253, "endOffset": 256}, {"referenceID": 7, "context": "This basic idea has been applied and enhanced many times; initially with the wake-sleep algorithm (WS, [3, 5]) and more recently with the variational autoencoder (VAE, [6]), stochastic backpropagation and approximate inference in deep generative models [7], neural variational inference and learning (NVIL, [8]) and reweighted wake-sleep (RWS, [9]).", "startOffset": 307, "endOffset": 310}, {"referenceID": 8, "context": "This basic idea has been applied and enhanced many times; initially with the wake-sleep algorithm (WS, [3, 5]) and more recently with the variational autoencoder (VAE, [6]), stochastic backpropagation and approximate inference in deep generative models [7], neural variational inference and learning (NVIL, [8]) and reweighted wake-sleep (RWS, [9]).", "startOffset": 344, "endOffset": 347}, {"referenceID": 9, "context": "related to the Helmholtz machine, but similar in spirit is the approach described in [10].", "startOffset": 85, "endOffset": 89}, {"referenceID": 10, "context": "Analogous to a Deep Boltzmann Machine [11] we think of these as layers in a neural network with links between x and h1 on the one side, and h1 and h2 on the other side.", "startOffset": 38, "endOffset": 42}, {"referenceID": 8, "context": "Analogous to the parameter updates in reweighted wake-sleep (RWS, [9]) we can derive an important sampling based estimate for the parameter gradients and use them to optimize towards a", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "These properties are basically inherited from the RWS training algorithm [9].", "startOffset": 73, "endOffset": 76}, {"referenceID": 11, "context": "[12] for a description) with a mini-batch size of 100.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "We initialize all weights according to [13] and set all biases to -1.", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "in [14].", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "Model ADULT CONNECT4 DNA MUSHROOMS NIPS-0-12 OCR-LETTERS RCV1 WEB auto regressive NADE[14] 13.", "startOffset": 86, "endOffset": 90}, {"referenceID": 14, "context": "39 EoNADE[15] 13.", "startOffset": 9, "endOffset": 13}, {"referenceID": 15, "context": "87 DARN[16] 13.", "startOffset": 7, "endOffset": 11}, {"referenceID": 8, "context": "83 RWS - NADE[9] 13.", "startOffset": 13, "endOffset": 16}, {"referenceID": 8, "context": "38 RWS - SBN[9] 13.", "startOffset": 12, "endOffset": 15}, {"referenceID": 16, "context": "We use the MNIST dataset that was binarized according to [17] and downloaded in binarized from [18].", "startOffset": 57, "endOffset": 61}, {"referenceID": 17, "context": "We use the MNIST dataset that was binarized according to [17] and downloaded in binarized from [18].", "startOffset": 95, "endOffset": 99}, {"referenceID": 18, "context": "We also trained models on the 98058 examples from the unlabeled section of the Toronto face database (TFD, [19]).", "startOffset": 107, "endOffset": 111}, {"referenceID": 19, "context": "timate Z could certainly be made by applying AIS [20] or RAISE [21].", "startOffset": 49, "endOffset": 53}, {"referenceID": 20, "context": "timate Z could certainly be made by applying AIS [20] or RAISE [21].", "startOffset": 63, "endOffset": 67}, {"referenceID": 21, "context": "We thank the developers of Theano [22] and Blocks [23] for their awesome work.", "startOffset": 34, "endOffset": 38}, {"referenceID": 22, "context": "We thank the developers of Theano [22] and Blocks [23] for their awesome work.", "startOffset": 50, "endOffset": 54}], "year": 2017, "abstractText": "Unsupervised training of deep generative models containing latent variables and performing inference remains a challenging problem for complex, high dimensional distributions. One basic approach to this problem is the so called Helmholtz machine and it involves training an auxiliary model that helps to perform approximate inference jointly with the generative model which is to be fitted to the training data. The top-down generative model is typically realized as a directed model that starts from some prior at the top, down to the empirical distribution at the bottom. The approximate inference model runs in the opposite direction and is typically trained to efficiently infer high probability latent states given some observed data. Here we propose a new method, referred to as geometric mean matching (GMM), that is based on the idea that the generative model should be close to the class of distributions that can be modeled by our approximate inference distribution. We achieve this by interpreting both the top-down and the bottom-up directed models as approximate inference distributions and by defining the target distribution we fit to the training data to be the geometric mean of these two. We present an upper-bound for the log-likelihood of this model and we show that optimizing this bound will pressure the model to stay close to the approximate inference distributions. In the experimental section we demonstrate that we can use this approach to fit deep generative models with many layers of hidden binary stochastic variables to complex and high dimensional training distributions.", "creator": "LaTeX with hyperref package"}}}