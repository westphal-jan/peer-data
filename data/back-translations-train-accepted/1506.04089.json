{"id": "1506.04089", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2015", "title": "Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to Action Sequences", "abstract": "We take an end-to-end, sequence-to-sequence learning approach to the task of following natural language route instructions, i.e., mapping natural language instructions to action sequences. Our model is an alignment-based long short-term memory recurrent neural network (attention-based LSTM-RNN) that encodes the free-form navigational instruction sentence and the corresponding representation of the environment state. We integrate alignment as part of our network, thereby empowering the model to focus on important sentence \"regions.\" The alignment-based LSTM then decodes the learned representation to obtain the inferred action sequence. Adding bidirectionality to the network helps further. In contrast with existing methods, our model uses no additional information or resources about the task or language at all (e.g., parsers or seed lexicons) and still achieves state-of-the-art on a single-sentence benchmark dataset and strong results in the limited-training multi-sentence setting. Moreover, our model is more stable across runs than previous work. We evaluate our model through a series of ablation studies that elucidate the contributions of the primary components of our model.", "histories": [["v1", "Fri, 12 Jun 2015 18:05:00 GMT  (284kb,D)", "https://arxiv.org/abs/1506.04089v1", null], ["v2", "Thu, 2 Jul 2015 19:22:33 GMT  (324kb,D)", "http://arxiv.org/abs/1506.04089v2", null], ["v3", "Wed, 2 Dec 2015 20:46:09 GMT  (2619kb,D)", "http://arxiv.org/abs/1506.04089v3", "To appear at AAAI 2016"], ["v4", "Thu, 17 Dec 2015 17:57:42 GMT  (2617kb,D)", "http://arxiv.org/abs/1506.04089v4", "To appear at AAAI 2016 (and an extended version of a NIPS 2015 Multimodal Machine Learning workshop paper)"]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG cs.NE cs.RO", "authors": ["hongyuan mei", "mohit bansal", "matthew r walter"], "accepted": true, "id": "1506.04089"}, "pdf": {"name": "1506.04089.pdf", "metadata": {"source": "META", "title": "Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to Action Sequences", "authors": ["Hongyuan Mei", "Mohit Bansal", "Matthew R. Walter"], "emails": ["hongyuan@ttic.edu", "mbansal@ttic.edu", "mwalter@ttic.edu"], "sections": [{"heading": "Introduction", "text": "Robots must be able to understand and successfully execute navigation instructions in natural language if they are to work seamlessly alongside humans. Someone using a voice-controlled wheelchair, for example, could instruct them to \"take me into the room opposite the kitchen,\" or a soldier could instruct a micro-aircraft to \"fly down the hallway to the second room on the right.\" However, the interpretation of such release instructions (especially in unknown environments) is difficult due to their ambiguity and complexity, such as uncertainty in their interpretation (e.g. to which hallway the instruction refers), long-term dependencies between instructions as well as actions, differences in the amount of detail, and the manifold ways in which language can be composed. Figure 1 is an example instruction that our method succeeds in following."}, {"heading": "Objects", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Wall paintings Tower", "text": "In fact, most people who are able are able to put themselves in the situation they are in, in which they are, in which they are, in which they are, in which they are, in which they are, in which they are, in which they are, in which they are able to learn the temporal sequences, in which they are able to decipher, in which they are able to grasp the temporal sequences, in which they are able to decipher, in which they are able to decipher."}, {"heading": "Related Work", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...)"}, {"heading": "Task Definition", "text": "We consider the problem of assigning natural language to action sequences based only on knowledge of the local, observable environment, which can take the form of isolated sentences (single sentence) or complete paragraphs (plural sentence), and we are interested in learning this assignment from corpora of training data of the form (x (i), a (i), y (i) for i = 1, 2,.., n, where x (i) is a variable length of natural language teaching, a (i) is the corresponding action sequence, and y (i) is the observable environmental representation. The model learns to produce the correct action sequence a (i) that has not been seen before (x (i), y (i))))) pair. The challenges arise from the fact that the instructions are free and complex, contain numerous spelling and grammatical errors and are ambiguous in their meaning."}, {"heading": "The Model", "text": "We formulate the problem of the interpretation of natural language routes as a conclusion about a probabilistic model P (a1: T = 1: 1: N). We formulate the problem of the interpretation of natural language routes as a conclusion about a probable model P (a1: T = 1: 1). We formulate the problem of the interpretation of natural language routes as a consequence of a probable model P (a1: T = 1: 1). We formulate the problem as one of the mapping of the given action sequence x1: 1. We formulate the action sequence a1: 1. We formulate the effective means of learning this sequence for sequence definition."}, {"heading": "Experimental Setup", "text": "We are able to go in search of a solution that meets the needs of the individual. (...) We are looking for a solution that meets the needs of the individual. (...) We are looking for a solution that meets the needs of the individual. (...) We are looking for a solution that meets the needs of the individual. (...) We are looking for a solution that meets the needs of the individual. (...) We are looking for a solution that meets the needs of the individual. (...) We are looking for a solution that meets the needs of the individual. (...) We are looking for a solution that meets the needs of the individual. (...)"}, {"heading": "Results and Analysis", "text": "This year is the highest in the history of the country."}, {"heading": "Conclusion", "text": "We presented an end-to-end, sequence-to-sequence approach to mapping natural language navigation instructions to action plans taking into account the local observable state of the world using a bidirectional LSTM-RNN model with a multi-level aligner. We evaluated our model using a benchmark route instruction data set and demonstrated that it achieves a state-of-the-art in executing a set and delivers competitive results in the more difficult multi-sentence range, although we work with very small training data sets and do not use specialized linguistic knowledge or resources. We also conducted a series of ablation studies to illustrate the contributions of our primary model components."}, {"heading": "Acknowledgments", "text": "We thank Yoav Artzi, David Chen, Oriol Vinyals and Kelvin Xu for their helpful comments. This work was supported in part by the Robotics Consortium of the US Army Research Laboratory under the Collaborative Technology Alliance Program, Collaborative Agreement W911NF-10-20016 and an IBM Faculty Award."}], "references": [], "referenceMentions": [], "year": 2004, "abstractText": "We propose a neural sequence-to-sequence model for direction following, a task that is essential to realizing effective autonomous agents. Our alignment-based encoder-decoder model with long short-term memory recurrent neural networks (LSTM-RNN) translates natural language instructions to action sequences based upon a representation of the observable world state. We introduce a multi-level aligner that empowers our model to focus on sentence \u201cregions\u201d salient to the current world state by using multiple abstractions of the input sentence. In contrast to existing methods, our model uses no specialized linguistic resources (e.g., parsers) or taskspecific annotations (e.g., seed lexicons). It is therefore generalizable, yet still achieves the best results reported to-date on a benchmark single-sentence dataset and competitive results for the limited-training multi-sentence setting. We analyze our model through a series of ablations that elucidate the contributions of the primary components of our model.", "creator": "TeX"}}}