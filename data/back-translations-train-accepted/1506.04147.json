{"id": "1506.04147", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2015", "title": "On the Accuracy of Self-Normalized Log-Linear Models", "abstract": "Calculation of the log-normalizer is a major computational obstacle in applications of log-linear models with large output spaces. The problem of fast normalizer computation has therefore attracted significant attention in the theoretical and applied machine learning literature. In this paper, we analyze a recently proposed technique known as \"self-normalization\", which introduces a regularization term in training to penalize log normalizers for deviating from zero. This makes it possible to use unnormalized model scores as approximate probabilities. Empirical evidence suggests that self-normalization is extremely effective, but a theoretical understanding of why it should work, and how generally it can be applied, is largely lacking. We prove generalization bounds on the estimated variance of normalizers and upper bounds on the loss in accuracy due to self-normalization, describe classes of input distributions that self-normalize easily, and construct explicit examples of high-variance input distributions. Our theoretical results make predictions about the difficulty of fitting self-normalized models to several classes of distributions, and we conclude with empirical validation of these predictions.", "histories": [["v1", "Fri, 12 Jun 2015 20:00:29 GMT  (1017kb,D)", "http://arxiv.org/abs/1506.04147v1", null], ["v2", "Thu, 18 Jun 2015 15:22:50 GMT  (1017kb,D)", "http://arxiv.org/abs/1506.04147v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.CL cs.LG stat.ME", "authors": ["jacob andreas", "maxim rabinovich", "michael i jordan", "dan klein"], "accepted": true, "id": "1506.04147"}, "pdf": {"name": "1506.04147.pdf", "metadata": {"source": "CRF", "title": "On the accuracy of self-normalized log-linear models", "authors": ["Jacob Andreas", "Maxim Rabinovich", "Dan Klein", "Michael I. Jordan"], "emails": ["jda@cs.berkeley.edu", "rabinovich@cs.berkeley.edu", "klein@cs.berkeley.edu", "jordan@cs.berkeley.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is not an attempt to solve the problem of self-exploitation, but rather to get a grip on the problem of self-exploitation. (...) It is not that it is self-exploitation. (...) It is not that it is self-exploitation. (...) It is not that it is self-exploitation. (...) It is as if it is self-exploitation. (...) It is as if it is self-exploitation. (...) It is as if it is self-exploitation. (...) It is as if it is self-exploitation. (...) It is as if it is self-exploitation. (...) It is as if it is self-exploitation. (...) It is as if it is self-exploitation. (...)"}, {"heading": "2 Problem background", "text": "The immediate motivation for this work is a method proposed to accelerate decoding in a machine translation system with a language model for neural networks [3]. The language model used is a standardized, forward-facing neural network with a \"softmax\" output layer that turns the predictions of the network into a vocabulary distribution in which each probability is logproportional to its output activation. It is observed that, with a sufficiently large vocabulary, it becomes prohibitive to obtain probabilities from this model (which must be queried millions of times during decoding). To address this, the language model is trained with the following objective: max W \u00b2 i [N (yi | xi; W) \u2212 log \u00b2 y \u00b2 eN (y \u00b2; W) \u2212 \u03b1 (log \u00b2 eN (y \u00b2 eN (y \u00b2; W)))), which can be parsed in terms of output parameters in terms of normatization."}, {"heading": "Related work", "text": "The approach described at the beginning of this section is closely related to an alternative trick for self-normalization, which is described on the basis of a Noise Contrastive Estimation (NCE) [8]. NCE is an alternative to the direct optimization of probability, instead training a classifier to distinguish between real samples from the model and \"noise samples\" from a different distribution. The structure of the training target makes it possible to replace the explicit calculation of each protocol normalizer with an estimate. In the traditional NCE, these values are treated as part of the parameter space and estimated simultaneously with the model parameters; there are guarantees that the estimates of the normalizer will eventually converge with their true values. Instead, it is possible to fix all these estimates to one. In this case, empirical evidence suggests that the resulting model also exhibits self-normalizing behavior [4]."}, {"heading": "3 Self-normalizable distributions", "text": "We start with a more formal characterization of a general loglinear model: definition 1 (log-linear models). If we define a space of inputs X, a space of outputs Y, a measure \u00b5 to Y, a nonnegative function h: Y \u2192 R and a function T: X \u00b7 Y \u2192 Rd, which is measurable in relation to its second argument \u00b5, we can define a log-linear model indexed by the parameters \u043e Rd, with which formp\u03b7 (y | x) = h (y) e\u03b7 > T (x, y) \u2212 A (x, \u03b7) \u2212 A (x, \u03b7) \u0445 = log-Y h (y) e\u03b7 > T (x, y) d\u00b5 (y) y). (4) If A (x, \u03b7) is a ppi (y | x) a normal, then this y model is a normal one."}, {"heading": "Then for either x \u2208 S,", "text": "A (x, \u03b7) = log (elog 2 + log (2 / 5) + e \u2212 log 2 + log (2 / 5)) = log (2 / 5) (2 + 1 / 2)) = 0, and \u03b7 is self-normalizing with respect to S. It is also easy to choose parameters that do not come close to a self-normalized distribution, and actually construct a target distribution that cannot be self-normalized: Example. SupposeX = {(1, 0), (1)} Y = {\u2212 1} T (x, y) = (x1y, x2y, 1) Then there is no such that A (x, 0) is constant for all x, and A (x,) is constant if and only if we are previously motivated, downstream applications of these models can be robust to small errors resulting from improvised normalization, so it would be useful to normalize this definition."}, {"heading": "4 Normalization and model accuracy", "text": "As far as the problem is concerned, finding conditional distributions that normalize themselves without caring how well they actually work in modeling the data. Here, the relationship between the approximate self-normalized distribution and the actual distribution p (y) x (which we have so far ignored) is essential. In fact, unless we are busy building a good model, it is always trivial to create a self-normalized distribution - simply to achieve this characterization by wanting both good self-normalization and good data probability, and in this section we characterize the trade between maximizing data probability and satisfying a self-normalized distribution gap."}, {"heading": "5 Experiments", "text": "The high intuition behind the results in the previous section can be summarized as follows: 1) for predictive distributions that lie in anticipation of high entropy or low entropy, self-normalization leads to a relatively small probability gap; 2) for mixtures of distributions with high and low entropy, self-normalization can lead to a large probability gap. More generally, we expect that increased tolerance for the variance of normalizers is associated with a decreased probability gap. In this section, we offer an experimental confirmation of these predictions. We begin by generating a series of random, sparse feature vectors and an initial weight vector L-0. To generate a sequence of label distributions that are gently interpolated between low entropy and high entropy, we introduce a temperature parameter equality, and for various settings of the normal distribution."}, {"heading": "6 Conclusions", "text": "Motivated by the empirical success of the self-normalization of parameter estimation procedures for log-linear models, we have attempted to establish a theoretical basis for understanding such procedures. We have characterized both self-normalizable distributions by constructing demonstrably simple examples, and self-normalizing training procedures by limiting the loss of probability associated with self-normalization. While we have addressed many of the important initial theoretical questions surrounding self-normalization, this study of the problem is by no means complete. We hope that this family of problems will attract further studies in the larger machine learning community; to this end, we are asking the following list of open questions: 1. How else can the approximate self-normalizable distributions be characterized? The class of approximate normalizable distributions that we have described is unlikely to match perfectly with real-world data."}, {"heading": "A Normalizable distributions", "text": "Proof of sentence 1 (distributions that are approximately normalizable).Leave T (x, y) = T * (x, y) + T \u2212 (x, y), where T * (x, y) = arg min T (x, y): x * S | | T (X, y) \u2212 T (x, y) | | 2.Then, E (log (x, y)))) 2 = E (log (x, y))) 2 (log (T (X, y) + T \u2212 (X, y) dy))) 2 \u2264 E (log (e\u03b7 > T (X, y) dy))) 2 for T = arg maxT (X, y)"}, {"heading": "B Normalization and likelihood", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1 General bound", "text": "The deviation is due to the fact that each log partition within the meaning (X, y) d\u00b5 (y, y) d\u00b5 (y) d\u00b5 (y, y) d\u00b5 (y) d\u00b5 (y) is analog, instead it is replaced by \u2212 T (x, y) \u2212 log\u00b5 (Y). The deviation is due to the fact that each log partition is within the meaning (1)."}, {"heading": "B.2 All-nonuniform bound", "text": "We assume the following assumptions: \u2022 Etiketten y \u00b2 \u00b2 \u00b2 \u00b2 are discreet. That is, Y = > \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2."}], "references": [{"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J.D. Lafferty", "A. McCallum", "F.C.N. Pereira"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Generalized linear models", "author": ["P. McCullagh", "J.A. Nelder"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1989}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["J. Devlin", "R. Zbib", "Z. Huang", "T. Lamar", "R. Schwartz", "J. Makhoul"], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Decoding with large-scale neural language models improves translation", "author": ["A. Vaswani", "Y. Zhao", "V. Fossum", "D. Chiang"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "When and why are log-linear models self-normalizing", "author": ["J. Andreas", "D. Klein"], "venue": "Proceedings of the Annual Meeting of the North American Chapter of the Association for Computational Linguistics", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Neural network learning: theoretical foundations", "author": ["M. Anthony", "P. Bartlett"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["M. Gutmann", "A. Hyv\u00e4rinen"], "venue": "Proceedings of the International Conference on Artificial Intelligence and Statistics", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Journal of statistical planning and inference", "author": ["A. O\u2019Hagan"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1991}, {"title": "An introduction to sequential Monte Carlo", "author": ["A. Doucet", "N. De Freitas", "N. Gordon"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "Log-linear models, a general class that includes conditional random fields (CRFs) and generalized linear models (GLMs), offer a flexible yet tractable approach modeling conditional probability distributions p(x|y) [1, 2].", "startOffset": 214, "endOffset": 220}, {"referenceID": 1, "context": "Log-linear models, a general class that includes conditional random fields (CRFs) and generalized linear models (GLMs), offer a flexible yet tractable approach modeling conditional probability distributions p(x|y) [1, 2].", "startOffset": 214, "endOffset": 220}, {"referenceID": 2, "context": "The machine translation community has recently described several procedures for training \u201cselfnormalized\u201d log-linear models [3, 4].", "startOffset": 124, "endOffset": 130}, {"referenceID": 3, "context": "The machine translation community has recently described several procedures for training \u201cselfnormalized\u201d log-linear models [3, 4].", "startOffset": 124, "endOffset": 130}, {"referenceID": 4, "context": "Previous work [5] bounds the sample complexity of self-normalizing training procedures for a restricted class of models, but leaves open the question of how self-normalization interacts with the predictive power of the learned model.", "startOffset": 14, "endOffset": 17}, {"referenceID": 2, "context": "The immediate motivation for this work is a procedure proposed to speed up decoding in a machine translation system with a neural-network language model [3].", "startOffset": 153, "endOffset": 156}, {"referenceID": 5, "context": "More usefully, all of the results presented here apply directly to trained neural nets in which the last layer only is retrained to self-normalize [7].", "startOffset": 147, "endOffset": 150}, {"referenceID": 6, "context": "The approach described at the beginning of this section is closely related to an alternative selfnormalization trick described based on noise-contrastive estimation (NCE) [8].", "startOffset": 171, "endOffset": 174}, {"referenceID": 3, "context": "In this case, empirical evidence suggests that the resulting model will also exhibit self-normalizing behavior [4].", "startOffset": 111, "endOffset": 114}, {"referenceID": 7, "context": "Many of these involve approximating the associated sum or integral using quadrature [9], herding [10], or Monte Carlo methods [11].", "startOffset": 84, "endOffset": 87}, {"referenceID": 8, "context": "Many of these involve approximating the associated sum or integral using quadrature [9], herding [10], or Monte Carlo methods [11].", "startOffset": 126, "endOffset": 130}, {"referenceID": 4, "context": "Generalizing a result from [5], we have: It will occasionally be instructive to consider the special case where X is the Boolean hypercube, and we will explicitly note where this assumption is made.", "startOffset": 27, "endOffset": 30}, {"referenceID": 2, "context": "In addition to the synthetic data, we compare our results to empirical data [3] from a self-normalized language model.", "startOffset": 76, "endOffset": 79}, {"referenceID": 2, "context": "Lines marked \u201cKL=\u201d are from synthetic data; the line marked \u201cLM\u201d is from [3].", "startOffset": 73, "endOffset": 76}], "year": 2017, "abstractText": "Calculation of the log-normalizer is a major computational obstacle in applications of log-linear models with large output spaces. The problem of fast normalizer computation has therefore attracted significant attention in the theoretical and applied machine learning literature. In this paper, we analyze a recently proposed technique known as \u201cself-normalization\u201d, which introduces a regularization term in training to penalize log normalizers for deviating from zero. This makes it possible to use unnormalized model scores as approximate probabilities. Empirical evidence suggests that self-normalization is extremely effective, but a theoretical understanding of why it should work, and how generally it can be applied, is largely lacking. We prove generalization bounds on the estimated variance of normalizers and upper bounds on the loss in accuracy due to self-normalization, describe classes of input distributions that self-normalize easily, and construct explicit examples of high-variance input distributions. Our theoretical results make predictions about the difficulty of fitting self-normalized models to several classes of distributions, and we conclude with empirical validation of these predictions.", "creator": "LaTeX with hyperref package"}}}