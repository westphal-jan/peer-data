{"id": "1506.04359", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2015", "title": "Multi-class SVMs: From Tighter Data-Dependent Generalization Bounds to Novel Algorithms", "abstract": "This paper studies the generalization performance of multi-class classification algorithms, for which we obtain, for the first time, a data-dependent generalization error bound with a logarithmic dependence on the class size, substantially improving the state-of-the-art linear dependence in the existing data-dependent generalization analysis. The theoretical analysis motivates us to introduce a new multi-class classification machine based on $\\ell_p$-norm regularization, where the parameter $p$ controls the complexity of the corresponding bounds. We derive an efficient optimization algorithm based on Fenchel duality theory. Benchmarks on several real-world datasets show that the proposed algorithm can achieve significant accuracy gains over the state of the art.", "histories": [["v1", "Sun, 14 Jun 2015 08:07:23 GMT  (23kb)", "http://arxiv.org/abs/1506.04359v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yunwen lei", "\u00fcr\u00fcn dogan", "alexander binder", "marius kloft"], "accepted": true, "id": "1506.04359"}, "pdf": {"name": "1506.04359.pdf", "metadata": {"source": "CRF", "title": "Multi-class SVMs: From Tighter Data-Dependent Generalization Bounds to Novel Algorithms", "authors": ["Yunwen Lei", "\u00dcr\u00fcn Dogan", "Alexander Binder"], "emails": ["yunwen.lei@hotmail.com", "urundogan@gmail.com", "alexander.binder@tu-berlin.de", "kloft@hu-berlin.de"], "sections": [{"heading": null, "text": "ar Xiv: 150 6.04 359v 1 [cs.L G] 1"}, {"heading": "1 Introduction", "text": "It is a question of the extent to which it is a question of the way in which people are able to survive themselves and the question to what extent they are able to survive themselves. (...) It is a question of the extent to which people are able to survive themselves. (...) It is a question of the extent to which people are able to survive themselves. (...) It is a question to what extent people are able to survive themselves. (...) It is a question to what extent the question is to be asked, to what extent the question is to be asked. (...) It is a question to what extent the question is to be asked. (...) It is a question to what extent the question is being asked. \"(...)"}, {"heading": "2 Theory", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Problem Setting", "text": "Suppose we get a sequence of examples S = {(x1, y1),.., (xn, yn), (X \u00b7 Y) n, which are drawn independently of each other according to a probability variable P defined on the sample space Z = X \u00b7 Y. Based on the training examples S, we would like to learn a prediction rule hz from a room H of the hypothesis mapping from Z to R and use the mapping x \u2192 argmaxyhz (x, y) to make predictions. For each hypothesis h \u0432H (x, y) the prediction rule h in a designated example (x, y) is a prediction rule h (x, y). The prediction rule h makes an error (x, y) when an error (x, y) is expected."}, {"heading": "2.2 Notation", "text": "Every function h: X # # Y \u00b7 R can be characterized by the functional vektor (h1,.., hc) with hj (x) = h (x, j), x \u2212 j = 1,.., c: We denote with H: = {\u03c1h (x, y): h \u00b2 the class of peripheral functions that are connected with H. Let us be k: X \u00b7 X \u2212 R a Mercer Kernel, wherein (x) the associated functional card is, i.e. k (x, y) = < y) amp amp amp amp amp amp; amp amp amp; amp amp amp; amp # 160; amp # amp; amp # 160; amp # amp # 160; amp # 160; amp # 160; amp # 160 # 160; amp # 160 # 160 amp # 160; amp # 160 160; amp # 160 160; amp # 160 160; amp # 160 160; amp # 160 160; amp # 160"}, {"heading": "2.3 Main results", "text": "Our discussion of data-dependent generalization error boundaries is based on the established methodology of the Rademacher and Gaussian complexity (21).Definition 3 (Rademacher and Gaussian complexity).Let H be a family of really evaluated functions defined on Z and S = (z1,.., zn) a fixed sample of magnitude n with elements in Z. Then, the empirical Rademacher and Gaussian complexities of H defined in relation to the sample S are defined by RS (H) = evolution [sup h,.,.,. n, i = 1\u03c3ih (zi), GS (H) = Eg [sup h h h,. H,.) n defined in relation to the sample S (zi). Lemn are independent random variables with the same probability as the values + 1 or \u2212 1, and g1, and gn."}, {"heading": "2.4 Comparison of the Achieved Bounds to the State of the Art", "text": "The large body of theoretical work on multiclass learning considers data-independent boundaries. (Based on the number of limited linear operators, [15] we obtain a generalization that exhibits a linear dependence on class size, resulting in [9] a radical dependence on the form O (n \u2212 1 2 (log 3 2 n) c). (Under conditions comparable to Corollary 8, [23] we derive a class-independent generalization guarantee. (However, its boundary is based on a delicate definition of margin, which is why it is often not used in mainstream multi-class literature.) The following generalization results from the following classes: [1 p log] y = y ep (r). (< w \u2212 y) y \u2212 y y y y (x) >)."}, {"heading": "3 Algorithms", "text": "Motivated by the generalization analysis presented in Section 2, we now present a new multi-level learning algorithm based on the implementation of empirical risk minimization in the hypotheses space (5), corresponding to the following result: p-norm multi-class SVM (p \u2265 1): Problem 10 (primary problem: p-norm multi-class SVM).min w12 [c-norm multi-class SVM = 1] 2 p + C n \u2211 i = 1 (ti), s.t. ti = < wyi, \u03c6 (xi) > \u2212 max y 6 = yi < wy, \u03c6 (xi) >, (P) For p = 2, we restore the pioneering multi-class algorithm of Crammer & Singer [20], which is therefore a special case of the proposed formulation. An advantage of the proposed approach over [20] may be that, as shown in episode 8, the dependence of the generalization performance on the class size decrease is reduced to 1."}, {"heading": "3.1 Dual problems", "text": "Since the optimization problem (P) is convex, we can derive the related dual problem for the construction of efficient optimization algorithms; the derivation of the following dual problem is shifted to the supplementary material C, where all components are zero; Problem 11 (Completely dualized problem for general loss functions); the lagrange dual problem of (10) is: sup-te unit in R c and 1-th vector in Rc, where all components are zero; Problem 11 (Completely dualized problem for general loss functions); the camp dual problem of (10) is: sup-te unit in Rn \u00b7 c \u2212 1 2 [c-te-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s)."}, {"heading": "3.2 Optimization Algorithms", "text": "x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "4 Empirical Analysis", "text": "We have implemented the proposed multi-level SVM algorithm (algorithm 1) in C + + and solved the related MC-SVM problem with the help of the two-step ascent [25]. We are experimenting with three benchmark datasets: the sector data set examined in [26], the News 20 dataset originally used for text classification by Crammer & Singer [27], and the Rcv1 dataset collected by [28]. We are comparing this with the classic multi-level classification algorithm [20] proposed by Crammer & Singer, which represents a strong baseline for these datasets [25]. We are using a five-fold cross-validation on the training set to adjust the regulation parameter C by grid search over the set {2 \u2212 12, 2 \u2212 11,.,.,., 212}."}, {"heading": "5 Conclusion", "text": "Motivated by the ever-growing size of multi-class data sets in real-world applications such as image descriptions and web advertising, covering tens or hundreds of thousands of classes, we investigated the influence of class size on the generalization behavior of multi-class classifiers. Here, we focus on data-dependent generalizations that enjoy the ability to grasp the characteristics of the distribution that generated the data. Of independent interest are hypotheses classes given as a maximum above base classes. We developed a new structural result on Gaussian complexities that is able to maintain the coupling between different components while the existing structural results can ignore this coupling and yield suboptimal generalizations. We applied the new structural result to learning rates for multi-class classifiers and for the first time derived a data-dependent coupling between different components, while the existing structural results ignore this coupling and have suboptimal generalizations as a result."}, {"heading": "Supplementary Material", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A Proofs on Structural Results on Gaussian Complexity", "text": "Our discussion of the limit of complexity is based on the following comparison between the various Gaussian processes. Lemma A.1 (theorem 1 in [29]). Leave the two Gaussian processes (max.) (max. \u2212 \u2212 \u2212 \u2212 \u2212 xi] 2 (each h). (A.1) Then, E [sup.Y.]. (sup.Y.). (sup.Y.). [sup.Y.]. (sup.Y.]. (sup.Y.). (sup.Y.). (sup.Y.). (sup.Y.). (sup.Y.). (sup.Y.). (sup.Y.). (sup.Y.). (sup.Y.) (su.Y.) (suxi. Y. (su.Y.). (su.Y.). (sup.Y.). (sup.Y.). (sup.Y.). (su.Y.). (sup.Y.). (sup.Y.). (sup.Y.). (sup.Y.). (sup.Y.). (sup.Y.). (sup.Y.). (sup.Y.). (su.Y.). (sup.Y.). (su.Y.). (su.Y. (suxi.). (sup.Y.). (su.Yxi.). (su.Y.). (su.Y.). (su.Y.). (su.Y.). (su.Y.). (su.Y. (suxi.). (su.Y.). (su.Y.).)."}, {"heading": "B Proofs on Generalization bounds For Multi-class Classifiers", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1 Proof of Theorem 5", "text": "D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D (D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D ="}, {"heading": "B.2 Proof of Theorem 7", "text": "To use Theorem 5, we must use the term suph * H * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *"}, {"heading": "C Proofs on the Dual Problems", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C.1 Equivalent Representation of \u2113p-norm Multi-class Classification", "text": "s leave ai \u2265 0, i-Nd and 1 \u2264 r <. Thenmin \u03b7: \u03b7i \u2265 0, i-Nd-Nd-r i \u2264 1-i-Ndai \u03b7i = (. i-Nd ar + 1 i) 1 + 1 rand the minimum is reached at a1 r + 1i (. k-Nd a r + 1 k) 1 r. Proof 16. Fixation w, partial optimization of equation (8) w.r.t. \u03b2 ismin \u03b2c-j = 1-wj-22 2\u03b2js.t. \u03b2-p-1, p-p-p (2 \u2212 p) \u2212 1, \u03b2j-0.The given result now follows directly by applying Lemma C.1 with r = p-js.t."}, {"heading": "C.2 Derivation of the Completely Dualized Problem (Problem 11)", "text": "Problem 11 (P) derivation means that the following problem (P) must be translated to the following problem (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply = P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) = Reply (P) =) = Reply (P) = Reply (P) = (P) = Reply (P) = Reply (P) = Reply (P) = (P) = Reply = (P) = (P) = Reply (P) = (P) = Reply = (P) = (P) = Reply (P) = (P) = (P) = Reply = (P) = Reply = (P) = (P) = Reply = (P) = (P) = Reply = (P) = (P) = (P) = (P) = (P) = (P) = (P) = P) = (P) = (P) = P) = (P) = (P) = P) = (P) = (P"}, {"heading": "C.3 Proof of the Representer Theorem (Theorem 12)", "text": "Let H1,. - xi \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2."}, {"heading": "C.4 Derivation of Partially Dualized Problem (Problem 14)", "text": "\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 (2) \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 (2) \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 (2) \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "<lb>This paper studies the generalization performance of multi-class classification algorithms, for<lb>which we obtain\u2014for the first time\u2014a data-dependent generalization error bound with a logarith-<lb>mic dependence on the class size, substantially improving the state-of-the-art linear dependence<lb>in the existing data-dependent generalization analysis. The theoretical analysis motivates us to<lb>introduce a new multi-class classification machine based on lp-norm regularization, where the<lb>parameter p controls the complexity of the corresponding bounds. We derive an efficient opti-<lb>mization algorithm based on Fenchel duality theory. Benchmarks on several real-world datasets<lb>show that the proposed algorithm can achieve significant accuracy gains over the state of the art.", "creator": "LaTeX with hyperref package"}}}