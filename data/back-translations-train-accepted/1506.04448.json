{"id": "1506.04448", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2015", "title": "Fast and Guaranteed Tensor Decomposition via Sketching", "abstract": "Tensor CANDECOMP/PARAFAC (CP) decomposition has wide applications in statistical learning of latent variable models and in data mining. In this paper, we propose fast and randomized tensor CP decomposition algorithms based on sketching. We build on the idea of count sketches, but introduce many novel ideas which are unique to tensors. We develop novel methods for randomized computation of tensor contractions via FFTs, without explicitly forming the tensors. Such tensor contractions are encountered in decomposition methods such as tensor power iterations and alternating least squares. We also design novel colliding hashes for symmetric tensors to further save time in computing the sketches. We then combine these sketching ideas with existing whitening and tensor power iterative techniques to obtain the fastest algorithm on both sparse and dense tensors. The quality of approximation under our method does not depend on properties such as sparsity, uniformity of elements, etc. We apply the method for topic modeling and obtain competitive results.", "histories": [["v1", "Sun, 14 Jun 2015 23:07:38 GMT  (122kb,D)", "http://arxiv.org/abs/1506.04448v1", "29 pages"], ["v2", "Tue, 20 Oct 2015 14:45:41 GMT  (127kb,D)", "http://arxiv.org/abs/1506.04448v2", "29 pages. Appeared in Proceedings of Advances in Neural Information Processing Systems (NIPS), held at Montreal, Canada in 2015"]], "COMMENTS": "29 pages", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["yining wang", "hsiao-yu fish tung", "alexander j smola", "anima anandkumar"], "accepted": true, "id": "1506.04448"}, "pdf": {"name": "1506.04448.pdf", "metadata": {"source": "CRF", "title": "Fast and Guaranteed Tensor Decomposition via Sketching", "authors": ["Yining Wang", "Hsiao-Yu Tung"], "emails": [], "sections": [{"heading": null, "text": "Keywords: tensor-CP decomposition, counter sketch, randomised methods, spectral methods, topic modelling"}, {"heading": "1 Introduction", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2 Preliminaries", "text": "It is not the only way in which it comes to such a result: \"It is not the only way in which it comes to such a result.\" (\"It is the only way in which it comes to such a result.\" (\"It is the only way in which it comes to such a result.\"), \"It is the second way in which it comes to such a result.\" (\"It is the second way in which it comes to such a result.\" (\"It is the second way in which it comes to such a result.\"), \"It is the third way in which it comes to such a result.\" (\"It is the third way in which it comes to such a result.\"), \"It is the third way in which it comes to such a result.\" (\"It is the third way in which it comes to such a result.\"), \"It is the third way in which it comes to such a result.\" (\"The third way in which it comes to such a result.\"), \"It is the third way in which it comes to such a result.\" (\"The third way in which it comes to such a result.\"), and it is the third way in which it is the third way in which it comes to such a result. \""}, {"heading": "3 Fast tensor decomposition via sketching", "text": "In this section, we will first present the sketch of tensors [22] and show how sketches can be efficiently calculated for factorial or empirical tensors. Then, we will show how the tensor force method can be performed directly on the sketch with reduced computational complexity. Furthermore, if the input tensor is symmetrical (i.e. Tijk is equal for all permutations of i, j, k), we propose a novel \"colliding hash\" design that speeds up the sketching process. For reasons of space, we will only consider the robust tensor potential method in the main text. Methods and experiments for sketching based on the ALS method are presented in Appendix C. To avoid confusion, we emphasize that n is used to indicate the dimension of the tensor to be disassembled, which does not necessarily correspond to the dimension of the original data sensors."}, {"heading": "3.1 Tensor sketch", "text": "For a tensor T of dimension n1 \u00b7 \u00b7 \u00b7 \u00b7 np, the random hash h1, \u00b7 \u00b7 \u00b7, hp: [n] \u2192 [b] works with Prhj [hj (i) = t = 1 / b for each i [n], j [p], t [b] and random Bernoulli variables \u043f1, \u00b7 \u00b7 \u00b7, [p] with Prhj [hj (i) = 1] = Pr\u0438j [i) = 1 / 2, the tensor sketches sT: [b] \u2192 \u00b7 \u00b7 T (t) = i1, \u00b7 \u00b7 \u00b7 \u00b7, \u2212 1} with Pr\u044bj [i) = 1] = Pr\u0430j (i) = \u2212 1 / 2, the tensor sketches sT: [b] \u00b7 \u00b7 T (t) = i1, \u00b7 \u00b7 \u00b7 \u00b7 (ip), ip (ip)."}, {"heading": "3.2 Efficient sketching of empirical moment tensors", "text": "2L is normally a linear function of k and T is logarithmic in n; see theorem 5.1 in [1]. 3F and F \u2212 1 represent the FFT and inverse FFT operators. We present efficient algorithms for outlining an empirical moment tensor. The proposed method scales linearly with the tensor dimension, which is much more efficient than the explicit construction of the data tensor, which in the worst case requires cubic time. The basic idea is to break down an empirical moment tensor into the sum of many rank-1 components and then apply FFT for each component."}, {"heading": "3.2.1 Sketching a rank-1 tensor", "text": "For a rank 1 tensor T = u v w with u, v, w Rn, its b-dimensional tensor sketch sT can be efficiently calculated with the following expression: sT = s1, u s2, v s3, w = F \u2212 1 (F (s1, u) \u0445 F (s1, u) \u0445 F (s1, u)))), (3) where \u043c stands for folding and \u0445 stands for elementary vector product. s1, u (t) = \u2211 h1 (i) = t1 (i) ui is the counting sketch of u and s2, v, s3, w are similarly defined. sT and F \u2212 1 stand for the Fast Fourier Transform (FFT) and its inverse operator. By using FFT, we reduce the folding calculation in elementary product evaluation in the Fourier space. Therefore, sT can be calculated with O (n + b) b operations in which FFT is evaluated."}, {"heading": "3.2.2 Extension to factored and empirical moment tensors", "text": "For a tensor T with known rank factorization T = \u2211 N i = 1 aiui vi wi we can efficiently calculate its tensor sketch sT by applying techniques directly to Sec. 3.2.1 because the sketch operator is linear; that is, s\u03bbA + \u00b5B = \u03bbsA + \u00b5sB for arbitrary scalars \u03bb, \u00b5 and tensors A, B. Consequently, the calculation assumes sT operations O (n + b log b) that are linear in the tensor dimension n. On the other hand, most empirical moment sensors appeared in the spectral learning of latent variable models with known rank factorizations. For example, an empirical 3rd order moment E [x 3] can be written as E [x 3] when most empirical moment sensors appear in the spectral learning of latent variable models."}, {"heading": "3.3 Fast robust tensor power method", "text": "In this section we show how to accelerate the calculation of these products. We show that the sketch of an input tensor T, you can approximate both T (I, u) and T (u, u) (u, u) and T (u, u)."}, {"heading": "3.4 Colliding hash and symmetric tensor sketch", "text": "The idea is that we only have to prove ourselves independent if we use the same hash function and Rademacher random variable for each order, that is, h1 (i) = h2 (i) = h2 (i) = h2 (i) = h3 (i) = h3 (i) = h3 (i) = h3 (i) = h3 (i) = h3 (i) = h3 (i) = h3 (i) = h3 (i). In this way, we will collide all permutations of (i), k) with each other. However, such a construction has a problem with repeated entries, because we can only assume \u00b1 1 values."}, {"heading": "4 Error analysis", "text": "In this section, we offer theoretical analysis of approximation errors of both the tensor sketch and the quickly sketched robust tensor force method. We mainly focus on symmetrical tensor sketches, while extending to asymmetrical settings is trivial. Due to space constraints, all evidence is placed in the appendix."}, {"heading": "4.1 Tensor sketch concentration bounds", "text": "Theorem 1 limits the approximate error of symmetrical tensor diagrams when calculating T (u, u, u) and T (I, u, u). Its detection is moved to Appendix E.2.Theorem 1. Attach a symmetrical real tensor T-Rn \u00b7 n \u00b7 n and a real vector u-Rn when using B-independent symmetrical tensor diagrams; that is, \u03b51, T-R and \u03b52, T-U-Rn are estimation errors of T (u, u, u) and T (I, u, u) when using independent symmetrical tensor diagrams; that is, \u03b51, T (u, u) \u2212 T (u, u) and \u03b52, T (u) = T-T (I, u, u, u)."}, {"heading": "4.2 Analysis of the fast tensor power method", "text": "Suppose T = T + E-Rn \u00b7 n, where T = \u2211 k i = 1 \u03bbiv 3 i with an orthonormal basis {vi} ki = 1, \u03bb1 > \u00b7 \u00b7 \u00b7 \u00b7 Inequality (n / \u043c) + Log (1 /) maxi \u0432i \u2212 1)} ki = 1 eigenvalue / eigenvector pairs obtained by algorithm 2. Suppose that the randomness of the tensor sketch is independent of tensors or product ratings. Suppose B = inequality (n / \u03bbi \u2212 1) and L grows linearly with k (2). Suppose that the randomness of the tensor sketch is independent of tensors or product ratings."}, {"heading": "5 Experiments", "text": "We demonstrate the effectiveness and efficiency of our proposed sketch-based tensor power method on both synthetic tensors and real-world thematic modeling problems. Experimental results using the fast ALS method are presented in Appendix C.3. All methods are implemented in C + + and tested on a single machine with 8 Intel X5550 @ 2.67 Ghz CPUs and 32 GB of memory."}, {"heading": "5.1 Synthetic tensors", "text": "In Table 5, we compare our proposed algorithms with exact decomposition methods of synthetic tensors. Let n = 1000 be the dimension of the input tensor. We first create a random orthonormal base {vi} ni = 1 and then normalize the input tensor T to T = (\u2211 n i = 1 \u03bbiv 3 i) + E, with eigenvalues \u03bbi = 1 / i. The normalization step results in us comparing the recovery error and runtime only for the top 10 restored eigenvectors of the full-fledged input tensor T. Both L and T are shown on Table 30. Table 3 shows that our proposed algorithms achieve a reasonable approximation error within minutes, which is much faster than A."}, {"heading": "5.2 Topic modeling", "text": "We implement a fast spectral inference algorithm for Latent Dirichlet Allocation (LDA [3]) by combining tensor sketches with existing whitening techniques for dimensionality reduction. Implementation details are provided in Appendix D. We compare our proposed fast spectral LDA algorithms with baseline spectral methods and collapsed Gibbs sampling methods (with the help of Gibbs + [24] implementations) on two real datasets: Wikipedia and Enron. Dataset details are held in A Only the most common V words and the vocabulary size V is set to 10000. For the robust tensor power method, the parameters are set to L = 50 and T = 30. For ALS, we iterate to convergence, or a maximum number of 1000 iterations is achieved."}, {"heading": "Appendix A Supplementary experimental results", "text": "The Enron dataset comes from the Enron e-mail corpus [17]. According to standard purification steps, the Wikipedia dataset contains 114 274 documents with an average of 512 words per document; the Enron dataset contains 186 501 e-mails with an average of 91 words per e-mail."}, {"heading": "Appendix B Fast tensor power method via symmetric sketching", "text": "In this section we show how to perform a quick tensor power method using symmetrical tensor sketches. < u > u > u > u > u > u (u, u) u (u, u) u (u) u (u, u) u (u, u) u (u, u) u (u, u) u (u) s (u, u) s (u) s (u) s (u) s (u) s (u) s (u) s (u) s (u) s (u) s (u) s (u) s \"s (u) s\" s (u) s (u) s (u) s (u) s (u) s (u) s u u u u u u u u u (u) s u u u u (u) s u (u) s (u) u (u) s (u) s (u) u (u) s (u) s (u) s (u) u (u) s (u) s (u) s (u) s (u) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s (s) s (s) s (s) s (s) s (s (s) s (s) s (s (s) s (s (s) s (s) s (s) u u u u u u u u u u u u u u u (u (u) u (u) u (u (s) s) s (s) s (s) s (s) s (s) s (s (s) s (s) s (s) s (s) s (s (s) s (s) (s (s) (s) (s (s)"}, {"heading": "Appendix C Fast ALS: method and simulation result", "text": "In this section, we describe how to use tensor sketches to accelerate the Alternating Least Squares (ALS) method for tensor-CP decomposition; we also provide experimental results on synthetic data; and compare our rapid ALS implementation with the Matlab tensor toolbox [?,?], widely considered the state of the art in tensor decomposition. (C.1 Alternating Least Squares Alternating Least Squares (ALS) is a popular method for tensor-CP decomposition [?,?]; the algorithm maintains the tensor decomposition methodology. (A, B, C, C, C, C, Rn \u00b7 k, and iteratively the following updating steps. (C > C, B > B > B). (17) B = T (1)."}, {"heading": "1 B = 20 .71 .41 .25 .17 .12 10 9 7 6 4 .11 .22 .49 1.1 2.4", "text": "(B = 30.34.14.11 9 7 5.17.33.75 1.6 B = 40.46.17.10.07 9 6 5.23.45 1.0 2.2 4.7 Exact \u2020.07 1 22.844.7 B = 40.38.28.35.28.23 10 7 6.13.78 1.5 2.2 B = 30.44.24.21 9 7 5 5.21.50 1.1 4.7 B = 40.38.28.16 9 8 4.29.69 0.3 Exact \u2020.17 2 32.3 \u2020 Cp than in Matlab tensor toolbox. It is used for exactly T = 30 iterations.When k is much smaller than the ambient tensor dimension n the computational bottleneck of Eq. (18) is T (1)."}, {"heading": "Appendix D Spectral LDA and fast spectral LDA", "text": "In this section, we will first review the LDA model and introduce the decay method for learning LDA models, which is in [1]. We will then provide full details of our proposed fast spectral LDA algorithms. [2] Pseudo code for fast spectral LDA algorithms is listed in Algorithm 4.D1 LDA models, which represent a collection of documents through a topic. [3] RV \u00b7 K and a Dirichlet Priority Rk, where V is the vocabulary size and number of topics. Each column in a probability distribution (i.e.) represents the word distribution of a particular topic."}, {"heading": "Appendix E Proofs", "text": "\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "Appendix F Summary of notations for matrix/vector products", "text": "We assume that Vektors a, b Cn from 0 are indexed; that is, a \u00b7 \u00b7 (a0, a1, \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "<lb>Tensor CANDECOMP/PARAFAC (CP) decomposition has wide applications in statistical learning of<lb>latent variable models and in data mining. In this paper, we propose fast and randomized tensor CP de-<lb>composition algorithms based on sketching. We build on the idea of count sketches, but introduce many<lb>novel ideas which are unique to tensors. We develop novel methods for randomized computation of tensor<lb>contractions via FFTs, without explicitly forming the tensors. Such tensor contractions are encountered in<lb>decomposition methods such as tensor power iterations and alternating least squares. We also design novel<lb>colliding hashes for symmetric tensors to further save time in computing the sketches. We then combine<lb>these sketching ideas with existing whitening and tensor power iterative techniques to obtain the fastest al-<lb>gorithm on both sparse and dense tensors. The quality of approximation under our method does not depend<lb>on properties such as sparsity, uniformity of elements, etc. We apply the method for topic modeling and<lb>obtain competitive results.<lb>", "creator": "LaTeX with hyperref package"}}}