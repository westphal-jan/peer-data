{"id": "1506.04573", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2015", "title": "A New PAC-Bayesian Perspective on Domain Adaptation", "abstract": "We study the issue of domain adaptation: we want to adapt a model from a source distribution to a target one. We focus on models expressed as a majority vote. Our main contribution is a novel theoretical analysis of the target risk that is formulated as an upper bound expressing a trade-off between only two terms: (i) the voters' joint errors on the source distribution, and (ii) the voters' disagreement on the target one; both easily estimable from samples. Hence, this new study is more precise than other analyses that usually rely on three terms (including a hardly controllable term). Moreover, we derive a PAC-Bayesian generalization bound, and specialize the result to linear classifiers to propose a learning algorithm.", "histories": [["v1", "Mon, 15 Jun 2015 12:46:45 GMT  (800kb,D)", "https://arxiv.org/abs/1506.04573v1", null], ["v2", "Mon, 21 Sep 2015 10:49:00 GMT  (800kb,D)", "http://arxiv.org/abs/1506.04573v2", null], ["v3", "Mon, 14 Mar 2016 19:44:22 GMT  (381kb,D)", "http://arxiv.org/abs/1506.04573v3", null], ["v4", "Tue, 26 Jul 2016 10:29:33 GMT  (422kb,D)", "http://arxiv.org/abs/1506.04573v4", "Published at ICML 2016"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["pascal germain", "amaury habrard", "fran\u00e7ois laviolette", "emilie morvant"], "accepted": true, "id": "1506.04573"}, "pdf": {"name": "1506.04573.pdf", "metadata": {"source": "META", "title": "A New PAC-Bayesian Perspective on Domain Adaptation", "authors": ["Pascal Germain", "Amaury Habrard", "Fran\u00e7ois Laviolette", "Emilie Morvant"], "emails": ["PASCAL.GERMAIN@INRIA.FR", "AMAURY.HABRARD@UNIV-ST-ETIENNE.FR", "FRANCOIS.LAVIOLETTE@IFT.ULAVAL.CA", "EMILIE.MORVANT@UNIV-ST-ETIENNE.FR"], "sections": [{"heading": "1. Introduction", "text": "In fact, most people who are in a position to put themselves in the world, to put themselves in another world, to put themselves in another world, to put themselves in another world, in which they are able to understand the world they live in, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live in which they, live in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, live, in which they, live, in which they, in which they, in which they, in which they, in which they, in which, in which they, in fact, live, in which they, live, in which they, in which they, in which they, in which, in fact, in which they, in fact, live, in which they, in fact, live, in which they, in which, in which they, in which they, in fact, in fact, in the majority of the world, are able to put themselves, in"}, {"heading": "2. Unsupervised Domain Adaptation Setting", "text": "Our goal is to match the domain from a distribution S - the source domain - to another (related) distribution T - the target domain - to X \u00b7 Y; SX; and TX as associated marginal distributions to X. Given a distribution D, we call (D) m the distribution of the am sample, which is made up of elements drawn by the majority i.i.d. We consider the unattended domain fit setting, in which the algorithm with a designated source mssample S = (xi, yi)} msi = 1 percent (S) ms and with an undesignated target mt sample T = {xi} mti = 1 percent (TX) mt."}, {"heading": "3. Some Previous Domain Adaptation Bounds", "text": "Many approaches to domain adaptation share the same underlying \"philosophy\" that has its origins in the work of Ben-David et al. (2006; 2010), which proposed domain adaptation (Theorem 1, below). To summarize the domain adaptation, the domain adaptation limits are reviewed in this section (see Zhang et al., 2012; Cortes et al., 2015, for other terms: (i) the source risks, (ii) the distance between source and target marginal distributions via X, (iii) an incalculable term (without a target tag) that quantifies the difficulty of the task. Ben-David et al. (2006) assumes that domain adaptation is related in the sense that there is a (unknown) model that works well on both domains."}, {"heading": "4. A New Domain Adaptation Perspective", "text": "In this section, we present an original approach to bridge the unpredictable risk of a PPP = q q majority Voting on a target distribution T thanks to one term depending on its marginal distribution TX, another on a related source domain S, and a term that captures the \"volume\" of the source distribution is not meaningful for the target task. We rely on the expected disagreement dD (r) and the expected common source eD (r), defined aseD (r) = E (x, y). We rely on the expected disagreement dD (r) 6 = y) I [h) I [h \u2032 (x) 6 = y] (5) Indeed, Lacasse et al. (2006); Germain et al. (2015) observes that given a domain D on X \u00d7 Y and a distribution onH, we can decompose the risk asRD (G)."}, {"heading": "5. Comparison With Related Works", "text": "In this section, we discuss how our domain adaptation boundary can be associated with some previous work."}, {"heading": "5.1. On the previous PAC-Bayesian bound", "text": "It is instructive to compare the new boundary of theorem 3 with the previous PAC-Baye domain adaptation of theorem 2. Unlike the uncontrollable term \u03bb (\u03c1) of theorem 2, these terms are not dependent on the learned posterior distribution \u03c1: for each \u03c1 on H \u03b2q (T-S) and \u03b7T-S are constant values measuring the relationship between the domains. Furthermore, the fact that domain divergence \u03b2q (T-S) is not an additive term, but a multiplicative term (unlike Disrupt (SX, TX) + \u03bb (\u03c1) in theorem 2), is a contribution to our new analysis. Consequently, \u03b2q (T-S) can be considered a hyperparameter that allows us to successfully match the trade-off between the rejection of the target selectors and the common error of the selected section 8."}, {"heading": "5.2. On some domain adaptation assumptions", "text": "To characterize which domain adaptation task can be learned, Ben-David et al. (2012) presented three assumptions that can support domain adaptation. Our Theorem 3 does not rely on these assumptions, but they can be interpreted within our framework as discussed below. A domain adaptation task fulfills the covariate weight assumption (Shimodaira, 2000) if the source and target domains differ only in their marginalities, i.e., TY | x (y) = SY | x (y). In this scenario, we can estimate \u03b2q (TX-SX), and even the source domain T\\ S using unattended density estimation methods. Interestingly, assuming that the domains share the same support, we have the source domain T\\ S = 0."}, {"heading": "5.3. On representation learning", "text": "The main assumption underlying our domain matching algorithm in Section 7 is that target domain support is largely included in source domain support, i.e. the value of the term \u03b7T\\ S is low. If T\\ S is sufficiently large to prevent appropriate matching, one could try to reduce its volume while at the same time maintaining a good compromise between dT (\u03c1) and eS (\u03c1) by taking a representative learning approach, i.e. projecting source and target examples into a new common space, as Chen et al. (2012); Ganin et al. (2016) have done."}, {"heading": "6. PAC-Bayesian Generalization Guarantees", "text": "The PAC-Bayesian theory provides tools to turn the boundary of Theorem 3 into a generalization tied to the target risk. These results are presented as corollaries of Theorem 4 below, generalizing a PAC-Bayesian theory from Catoni (2007) to arbitrary loss functions."}, {"heading": "7. Specialization to Linear Classifiers", "text": "To derive an overall picture from this, we must engage with the limitations of Theorems 3 and 6, which relate to the risk of a linear classifier. (D) The course of action taken is one privilege in numerous PACBayesian works (e.g.). (D) The course of action taken is the other. (D) The course of action taken is the other. (D) The course of action taken is the other. (D) The course of action taken is the one privilege in many PACBayesian works (e.g.). (D) The course of action taken is the other. (D) The course of action taken is the other. (D) The course of action taken is the other. (D) The course of action taken is the risk of a linear classifier. (D) The course of action taken is the second (D)."}, {"heading": "8. Experimental Results", "text": "First, Figure 2 illustrates the behavior of the decision limit of our algorithm DALC on an interweaving of moons toy problem10, where each moon corresponds to a label. 9It is not trivial to show that the kernel trick applies when \u03c00 and \u03c1w are more than reasonable Gaussian over infinite dimensional feature space. However, as mentioned by McAllester & Keshet (2011), it is the case, provided we consider Gauss processes as a measurement of distributions \u03c00 and \u03c1w over (infinite) H.10We generate each pair of moons using the Make-Moons function provided in scikit-learn (Pedregosa et al., 2011).The target domain for which we have no label is source one rotation. The figure clearly shows that DALC succeeds in adapting to the target domain, even for a rotation angle of 50. We see that DALC does not rely on the restrictive codependent shift."}, {"heading": "9. Conclusion", "text": "We propose a new domain adaptation analysis for the learning of majority decisions. It is based on an upper limit of the target risk, expressed as a trade-off between the disagreement of voters about the target range, the common errors of voters about the source range and a term that reflects the worst-case error in regions where the source range is not informative. To our knowledge, a key novelty of our contribution is that the trade-off is controlled by the divergence \u03b2q (equation 7) between domains: divergence is not an additive term (as in many domain adaptation limits), but a factor that weights the importance of source information. Our analysis, combined with a PAC-Bayean generalization limit, leads to a new domain adaptation algorithm for linear classifiers. Empirical experiments show that our new algorithm surpasses the previous PAC-Bayean approach (2013, Germain)."}, {"heading": "Acknowledgements", "text": "This work was supported partly by the French project LIVES ANR-15-CE23-0026-03 and partly by the NSERC Discovery Funding 262067."}, {"heading": "A. Proof of Theorem 4", "text": "We use the following abbreviated formula: LD (h) = E (x, y) \u00b7 D '(h, x, y) \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "C. Experimental Protocol", "text": "To obtain the DALCRCV results from Table 1, the reverse validation method searches on a 20 x 20 parameter grid for a C between 0.01 and 106 and a parameter B between 1.0 and 108, both on a logarithmic scale. The results of the other algorithms are reported from Germain et al. (2013)."}], "references": [{"title": "Tighter PAC-Bayes bounds", "author": ["A. Ambroladze", "E. Parrado-Hern\u00e1ndez", "J. ShaweTaylor"], "venue": "In NIPS, pp", "citeRegEx": "Ambroladze et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ambroladze et al\\.", "year": 2006}, {"title": "Analysis of representations for domain adaptation", "author": ["S. Ben-David", "J. Blitzer", "K. Crammer", "F. Pereira"], "venue": "In NIPS, pp", "citeRegEx": "Ben.David et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 2006}, {"title": "A theory of learning from different domains", "author": ["S. Ben-David", "J. Blitzer", "K. Crammer", "A. Kulesza", "F. Pereira", "Vaughan", "J. Wortman"], "venue": null, "citeRegEx": "Ben.David et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 2010}, {"title": "Domain adaptation\u2013can quantity compensate for quality", "author": ["S. Ben-David", "S. Shalev-Shwartz", "R. Urner"], "venue": "In ISAIM,", "citeRegEx": "Ben.David et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 2012}, {"title": "Active nearest neighbors in changing environments", "author": ["C. Berlind", "R. Urner"], "venue": "In ICML, pp", "citeRegEx": "Berlind and Urner,? \\Q2015\\E", "shortCiteRegEx": "Berlind and Urner", "year": 2015}, {"title": "Domain adaptation of natural language processing systems", "author": ["J. Blitzer"], "venue": "PhD thesis, UPenn,", "citeRegEx": "Blitzer,? \\Q2007\\E", "shortCiteRegEx": "Blitzer", "year": 2007}, {"title": "Domain adaptation with structural correspondence learning", "author": ["J. Blitzer", "R. McDonald", "F. Pereira"], "venue": "In EMNLP, pp", "citeRegEx": "Blitzer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "Domain adaptation problems: A DASVM classification technique and a circular validation strategy", "author": ["L. Bruzzone", "M. Marconcini"], "venue": "IEEE Trans. Pattern Anal. Mach. Intel.,", "citeRegEx": "Bruzzone and Marconcini,? \\Q2010\\E", "shortCiteRegEx": "Bruzzone and Marconcini", "year": 2010}, {"title": "PAC-Bayesian supervised classification: the thermodynamics of statistical learning, volume 56", "author": ["O. Catoni"], "venue": "Inst. of Mathematical Statistic,", "citeRegEx": "Catoni,? \\Q2007\\E", "shortCiteRegEx": "Catoni", "year": 2007}, {"title": "Co-training for domain adaptation", "author": ["M. Chen", "K.Q. Weinberger", "J. Blitzer"], "venue": "In NIPS, pp", "citeRegEx": "Chen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "Marginalized denoising autoencoders for domain adaptation", "author": ["M. Chen", "Z.E. Xu", "K.Q. Weinberger", "F. Sha"], "venue": "In ICML, pp", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Domain adaptation and sample bias correction theory and algorithm for regression", "author": ["C. Cortes", "M. Mohri"], "venue": "Theor. Comput. Sci.,", "citeRegEx": "Cortes and Mohri,? \\Q2014\\E", "shortCiteRegEx": "Cortes and Mohri", "year": 2014}, {"title": "Learning bounds for importance weighting", "author": ["C. Cortes", "Y. Mansour", "M. Mohri"], "venue": "In NIPS, pp", "citeRegEx": "Cortes et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2010}, {"title": "Adaptation algorithm and theory based on generalized discrepancy", "author": ["C. Cortes", "M. Mohri", "Medina", "A. Mu\u00f1oz"], "venue": "In ACM SIGKDD,", "citeRegEx": "Cortes et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2015}, {"title": "Frustratingly easy domain adaptation", "author": ["III H. Daum\u00e9"], "venue": "In ACL,", "citeRegEx": "Daum\u00e9,? \\Q2007\\E", "shortCiteRegEx": "Daum\u00e9", "year": 2007}, {"title": "Unsupervised domain adaptation by backpropagation", "author": ["Y. Ganin", "V.S. Lempitsky"], "venue": "In ICML, pp", "citeRegEx": "Ganin and Lempitsky,? \\Q2015\\E", "shortCiteRegEx": "Ganin and Lempitsky", "year": 2015}, {"title": "PAC-Bayesian learning of linear classifiers", "author": ["P. Germain", "A. Lacasse", "F. Laviolette", "M. Marchand"], "venue": "In ICML, pp", "citeRegEx": "Germain et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Germain et al\\.", "year": 2009}, {"title": "A PAC-Bayesian approach for domain adaptation with specialization to linear classifiers", "author": ["P. Germain", "A. Habrard", "F. Laviolette", "E. Morvant"], "venue": "In ICML,", "citeRegEx": "Germain et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Germain et al\\.", "year": 2013}, {"title": "Risk bounds for the majority vote: From a PAC-Bayesian analysis to a learning", "author": ["P. Germain", "A. Lacasse", "F. Laviolette", "Marchand", "Roy", "J.-F"], "venue": null, "citeRegEx": "Germain et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Germain et al\\.", "year": 2015}, {"title": "A PAC-Bayesian margin bound for linear classifiers: Why svms work", "author": ["R. Herbrich", "T. Graepel"], "venue": "In NIPS, pp", "citeRegEx": "Herbrich and Graepel,? \\Q2000\\E", "shortCiteRegEx": "Herbrich and Graepel", "year": 2000}, {"title": "Correcting sample selection bias by unlabeled data", "author": ["J. Huang", "A. Smola", "A. Gretton", "K. Borgwardt", "B. Sch\u00f6lkopf"], "venue": "In NIPS, pp", "citeRegEx": "Huang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2006}, {"title": "A literature survey on domain adaptation of statistical classifiers", "author": ["J. Jiang"], "venue": null, "citeRegEx": "Jiang,? \\Q2008\\E", "shortCiteRegEx": "Jiang", "year": 2008}, {"title": "SciPy: Open source scientific tools for Python, 2001", "author": ["E. Jones", "T. Oliphant", "P Peterson"], "venue": "URL http: //www.scipy.org/", "citeRegEx": "Jones et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Jones et al\\.", "year": 2001}, {"title": "PAC-Bayes bounds for the risk of the majority vote and the variance of the Gibbs classifier", "author": ["A. Lacasse", "F. Laviolette", "M. Marchand", "P. Germain", "N. Usunier"], "venue": "In NIPS, pp", "citeRegEx": "Lacasse et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lacasse et al\\.", "year": 2006}, {"title": "PAC-Bayes & margins", "author": ["J. Langford", "J. Shawe-Taylor"], "venue": "In NIPS, pp", "citeRegEx": "Langford and Shawe.Taylor,? \\Q2002\\E", "shortCiteRegEx": "Langford and Shawe.Taylor", "year": 2002}, {"title": "A Bayesian divergence prior for classiffier adaptation", "author": ["X. Li", "J. Bilmes"], "venue": "In AISTATS, pp", "citeRegEx": "Li and Bilmes,? \\Q2007\\E", "shortCiteRegEx": "Li and Bilmes", "year": 2007}, {"title": "Evigan: a hidden variable model for integrating gene evidence for eukaryotic gene", "author": ["Q. Liu", "A.J. Mackey", "D.S. Roos", "F. Pereira"], "venue": "prediction. Bioinformatics,", "citeRegEx": "Liu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2008}, {"title": "Domain adaptation: Learning bounds and algorithms", "author": ["Y. Mansour", "M. Mohri", "A. Rostamizadeh"], "venue": "In COLT,", "citeRegEx": "Mansour et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mansour et al\\.", "year": 2009}, {"title": "A literature review of domain adaptation with unlabeled data", "author": ["A. Margolis"], "venue": null, "citeRegEx": "Margolis,? \\Q2011\\E", "shortCiteRegEx": "Margolis", "year": 2011}, {"title": "A note on the PAC-Bayesian theorem", "author": ["A. Maurer"], "venue": "CoRR, cs.LG/0411099,", "citeRegEx": "Maurer,? \\Q2004\\E", "shortCiteRegEx": "Maurer", "year": 2004}, {"title": "A PAC-Bayesian tutorial with a dropout", "author": ["D. McAllester"], "venue": "bound. CoRR,", "citeRegEx": "McAllester,? \\Q2013\\E", "shortCiteRegEx": "McAllester", "year": 2013}, {"title": "Generalization bounds and consistency for latent structural probit and ramp loss", "author": ["D.A. McAllester", "J. Keshet"], "venue": "In NIPS,", "citeRegEx": "McAllester and Keshet,? \\Q2011\\E", "shortCiteRegEx": "McAllester and Keshet", "year": 2011}, {"title": "Parsimonious Unsupervised and Semi-Supervised Domain Adaptation with Good Similarity Functions", "author": ["E. Morvant", "A. Habrard", "S. Ayache"], "venue": "KAIS, 33(2):309\u2013349,", "citeRegEx": "Morvant et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Morvant et al\\.", "year": 2012}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "T. Knowl. Data En.,", "citeRegEx": "Pan and Yang,? \\Q2010\\E", "shortCiteRegEx": "Pan and Yang", "year": 2010}, {"title": "PAC-Bayes bounds with data dependent", "author": ["E. Parrado-Hern\u00e1ndez", "A. Ambroladze", "J. Shawe-Taylor", "S. Sun"], "venue": "priors. JMLR,", "citeRegEx": "Parrado.Hern\u00e1ndez et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Parrado.Hern\u00e1ndez et al\\.", "year": 2012}, {"title": "Visual domain adaptation: A survey of recent advances", "author": ["V.M. Patel", "R. Gopalan", "R. Li", "R. Chellappa"], "venue": "IEEE Signal Proc. Mag.,", "citeRegEx": "Patel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Patel et al\\.", "year": 2015}, {"title": "Dataset shift in machine learning", "author": ["J. Quionero-Candela", "M. Sugiyama", "A. Schwaighofer", "N.D. Lawrence"], "venue": null, "citeRegEx": "Quionero.Candela et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Quionero.Candela et al\\.", "year": 2009}, {"title": "PAC-Bayesian analysis of coclustering and beyond", "author": ["Y. Seldin", "N. Tishby"], "venue": "JMLR, 11:3595\u20133646,", "citeRegEx": "Seldin and Tishby,? \\Q2010\\E", "shortCiteRegEx": "Seldin and Tishby", "year": 2010}, {"title": "Improving predictive inference under covariate shift by weighting the log-likelihood function", "author": ["H. Shimodaira"], "venue": "J. Statist. Plann. Inference,", "citeRegEx": "Shimodaira,? \\Q2000\\E", "shortCiteRegEx": "Shimodaira", "year": 2000}, {"title": "Direct importance estimation with model selection and its application to covariate shift adaptation", "author": ["M. Sugiyama", "S. Nakajima", "H. Kashima", "P. von B\u00fcnau", "M. Kawanabe"], "venue": "In NIPS,", "citeRegEx": "Sugiyama et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Sugiyama et al\\.", "year": 2007}, {"title": "Access to unlabeled data can speed up prediction time", "author": ["R. Urner", "S. Shalev-Shwartz", "S. Ben-David"], "venue": "In ICML, pp", "citeRegEx": "Urner et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Urner et al\\.", "year": 2011}, {"title": "Generalization bounds for domain adaptation", "author": ["C. Zhang", "L. Zhang", "J. Ye"], "venue": "In NIPS, pp", "citeRegEx": "Zhang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2012}, {"title": "Cross validation framework to choose amongst models and datasets for transfer learning", "author": ["E. Zhong", "W. Fan", "Q. Yang", "O. Verscheure", "J. Ren"], "venue": "In ECML-PKDD,", "citeRegEx": "Zhong et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhong et al\\.", "year": 2010}, {"title": "Applying consecutively Jensen\u2019s Inequality and the change of measure inequality", "author": [], "venue": "(see Seldin & Tishby (2010,", "citeRegEx": "h et al\\.,? \\Q2013\\E", "shortCiteRegEx": "h et al\\.", "year": 2013}, {"title": "For our experiments, we minimize this objective function using a Broyden-Fletcher-Goldfarb-Shanno method (BFGS) implemented in the scipy python library", "author": ["\u03b1i\u03b1jKi"], "venue": null, "citeRegEx": "\u03b1i\u03b1jKi and .,? \\Q2001\\E", "shortCiteRegEx": "\u03b1i\u03b1jKi and .", "year": 2001}], "referenceMentions": [{"referenceID": 21, "context": "Machine learning practitioners are commonly exposed to the issue of domain adaptation1 (Jiang, 2008; Margolis, 2011): One usually learns a model from a corpus, i.", "startOffset": 87, "endOffset": 116}, {"referenceID": 28, "context": "Machine learning practitioners are commonly exposed to the issue of domain adaptation1 (Jiang, 2008; Margolis, 2011): One usually learns a model from a corpus, i.", "startOffset": 87, "endOffset": 116}, {"referenceID": 35, "context": "Therefore, domain adaptation is widely studied in a lot of application fields like computer vision (Patel et al., 2015; Ganin & Lempitsky, 2015), bioinformatics (Liu et al.", "startOffset": 99, "endOffset": 144}, {"referenceID": 26, "context": ", 2015; Ganin & Lempitsky, 2015), bioinformatics (Liu et al., 2008), natural language processing (Blitzer, 2007; Daum\u00e9 III, 2007), etc.", "startOffset": 49, "endOffset": 67}, {"referenceID": 5, "context": ", 2008), natural language processing (Blitzer, 2007; Daum\u00e9 III, 2007), etc.", "startOffset": 37, "endOffset": 69}, {"referenceID": 36, "context": "Domain adaptation is associated with transfer learning (Pan & Yang, 2010; Quionero-Candela et al., 2009).", "startOffset": 55, "endOffset": 104}, {"referenceID": 20, "context": "This is generally performed by reweighting the importance of labeled data (Huang et al., 2006; Sugiyama et al., 2007; Cortes et al., 2010; 2015), and/or by learning a common representation for the source and target distributions (Chen et al.", "startOffset": 74, "endOffset": 144}, {"referenceID": 39, "context": "This is generally performed by reweighting the importance of labeled data (Huang et al., 2006; Sugiyama et al., 2007; Cortes et al., 2010; 2015), and/or by learning a common representation for the source and target distributions (Chen et al.", "startOffset": 74, "endOffset": 144}, {"referenceID": 12, "context": "This is generally performed by reweighting the importance of labeled data (Huang et al., 2006; Sugiyama et al., 2007; Cortes et al., 2010; 2015), and/or by learning a common representation for the source and target distributions (Chen et al.", "startOffset": 74, "endOffset": 144}, {"referenceID": 10, "context": ", 2010; 2015), and/or by learning a common representation for the source and target distributions (Chen et al., 2012; Ganin et al., 2016), and/or by minimizing a measure of divergence between the distributions (Morvant et al.", "startOffset": 98, "endOffset": 137}, {"referenceID": 32, "context": ", 2016), and/or by minimizing a measure of divergence between the distributions (Morvant et al., 2012; Germain et al., 2013; Cortes & Mohri, 2014).", "startOffset": 80, "endOffset": 146}, {"referenceID": 17, "context": ", 2016), and/or by minimizing a measure of divergence between the distributions (Morvant et al., 2012; Germain et al., 2013; Cortes & Mohri, 2014).", "startOffset": 80, "endOffset": 146}, {"referenceID": 27, "context": "The divergence-based approach has especially been explored to derive generalization bounds for domain adaptation (e.g., Ben-David et al., 2006; 2010; Mansour et al., 2009; Li & Bilmes, 2007; Zhang et al., 2012).", "startOffset": 113, "endOffset": 210}, {"referenceID": 41, "context": "The divergence-based approach has especially been explored to derive generalization bounds for domain adaptation (e.g., Ben-David et al., 2006; 2010; Mansour et al., 2009; Li & Bilmes, 2007; Zhang et al., 2012).", "startOffset": 113, "endOffset": 210}, {"referenceID": 17, "context": "Recently, this issue has been studied through the PAC-Bayesian framework (Germain et al., 2013), which focuses on learning weighted majority votes2 without target label.", "startOffset": 73, "endOffset": 95}, {"referenceID": 1, "context": ", Ben-David et al., 2006; 2010; Mansour et al., 2009; Li & Bilmes, 2007; Zhang et al., 2012). Recently, this issue has been studied through the PAC-Bayesian framework (Germain et al., 2013), which focuses on learning weighted majority votes2 without target label. Even the latter result opened the door to tackle domain adaptation in a PAC-Bayesian fashion, it shares the same philosophy as the seminal works of Ben-David et al. (2006; 2010); Mansour et al. (2009): The risk of the target model is upper-bounded jointly by the model\u2019s risk on the source distribution, the divergence between the marginal distributions, and a nonestimable term3 related to the ability to adapt in the current space.", "startOffset": 2, "endOffset": 465}, {"referenceID": 1, "context": ", Ben-David et al., 2006; 2010; Mansour et al., 2009; Li & Bilmes, 2007; Zhang et al., 2012). Recently, this issue has been studied through the PAC-Bayesian framework (Germain et al., 2013), which focuses on learning weighted majority votes2 without target label. Even the latter result opened the door to tackle domain adaptation in a PAC-Bayesian fashion, it shares the same philosophy as the seminal works of Ben-David et al. (2006; 2010); Mansour et al. (2009): The risk of the target model is upper-bounded jointly by the model\u2019s risk on the source distribution, the divergence between the marginal distributions, and a nonestimable term3 related to the ability to adapt in the current space. Note that Li & Bilmes (2007) proposed a PACBayesian generalization bound for domain adaptation but they considered target labels.", "startOffset": 2, "endOffset": 727}, {"referenceID": 16, "context": "More precisely, we adopt the PACBayesian domain adaptation setting previously studied in Germain et al. (2013). GivenH, a set of voters h : X\u2192 Y , the elements of this approach are a prior distribution \u03c0 on H, a pair of source-target learning samples (S, T ) and a posterior distribution \u03c1 on H.", "startOffset": 89, "endOffset": 111}, {"referenceID": 20, "context": "To address this issue, Lacasse et al. (2006) (refined in Germain et al.", "startOffset": 23, "endOffset": 45}, {"referenceID": 16, "context": "The quantity dD(\u03c1) is also used in the domain adaptation bound of Germain et al. (2013) to measure divergence between distributions.", "startOffset": 66, "endOffset": 88}, {"referenceID": 1, "context": "Ben-David et al. (2006) assumed that the domains are related in the sense that there exists a (unknown) model performing well on both domains.", "startOffset": 0, "endOffset": 24}, {"referenceID": 1, "context": "Theorem 1 (Ben-David et al., 2006; 2010).", "startOffset": 10, "endOffset": 40}, {"referenceID": 27, "context": "Pursuing in the same line of research, Mansour et al. (2009) generalizes the H\u2206H-distance to real-valued loss functions L : [\u22121, 1] \u2192 R, to express a similar theorem for regression.", "startOffset": 39, "endOffset": 61}, {"referenceID": 27, "context": "The accuracy of the Mansour et al. (2009)\u2019s bound also relies on a non-estimable term assumed to be low when adaptation is achievable.", "startOffset": 20, "endOffset": 42}, {"referenceID": 16, "context": "Building on previous domain adaptation analyses, Germain et al. (2013) derived a PAC-Bayesian domain adaptation bound.", "startOffset": 49, "endOffset": 71}, {"referenceID": 17, "context": "Theorem 2 (Germain et al., 2013).", "startOffset": 10, "endOffset": 32}, {"referenceID": 16, "context": "In Germain et al. (2013), the bound of Theorem 2 inspired an algorithm\u2014named PBDA\u2014selecting \u03c1 over H that achieves a trade-off between RS(G\u03c1) and dis\u03c1(SX, TX).", "startOffset": 3, "endOffset": 25}, {"referenceID": 20, "context": "Indeed, Lacasse et al. (2006); Germain et al.", "startOffset": 8, "endOffset": 30}, {"referenceID": 16, "context": "(2006); Germain et al. (2015) observed that, given a domain D on X\u00d7Y and a distribution \u03c1 onH, we can decompose the Gibbs risk as", "startOffset": 8, "endOffset": 30}, {"referenceID": 12, "context": "Moreover, we can link \u03b2q(T \u2016S) to the R\u00e9nyi divergence7, which has led to generalization bounds in the context of importance weighting (Cortes et al., 2010).", "startOffset": 135, "endOffset": 156}, {"referenceID": 38, "context": "A domain adaptation task fulfills the covariate shift assumption (Shimodaira, 2000) if the source and target domains only differ in their marginals according to the input space, i.", "startOffset": 65, "endOffset": 83}, {"referenceID": 1, "context": "In order to characterize which domain adaptation task may be learnable, Ben-David et al. (2012) presented three assumptions that can help domain adaptation.", "startOffset": 72, "endOffset": 96}, {"referenceID": 3, "context": "The weight ratio (Ben-David et al., 2012) of source and target domains, with respect to a collection of input space subsets B \u2286 2, is given by", "startOffset": 17, "endOffset": 41}, {"referenceID": 9, "context": ", by projecting source and target examples into a new common space, as done for example by Chen et al. (2012); Ganin et al.", "startOffset": 91, "endOffset": 110}, {"referenceID": 9, "context": ", by projecting source and target examples into a new common space, as done for example by Chen et al. (2012); Ganin et al. (2016). 6.", "startOffset": 91, "endOffset": 131}, {"referenceID": 8, "context": "These results are presented as corollaries of Theorem 4 below, that generalizes a PAC-Bayesian theorem of Catoni (2007) to arbitrary loss functions.", "startOffset": 106, "endOffset": 120}, {"referenceID": 30, "context": "Note that, similarly to McAllester & Keshet (2011), we could choose to restrict c \u2208 (0, 2) to obtain a slightly looser but simpler bound.", "startOffset": 24, "endOffset": 51}, {"referenceID": 16, "context": "PAC-Bayesian bounds on these quantities appeared in Germain et al. (2015), but under different forms.", "startOffset": 52, "endOffset": 74}, {"referenceID": 29, "context": "To do so, we exploit a result of Maurer (2004) that allows to generalize PAC-Bayes theorems to arbitrary bounded loss function (see the proof of Theorem 4 in supplemental).", "startOffset": 33, "endOffset": 47}, {"referenceID": 16, "context": "From an optimization perspective, the problem suggested by the bound of Theorem 6 is much more convenient to minimize than the PAC-Bayesian bound derived from Theorem 2 in Germain et al. (2013). The former is smoother than the latter: the absolute value related to the domain disagreement dis\u03c1(SX, TX) of Equation (4) disappears in benefit of the domain divergence \u03b2\u221e(T \u2016S), which is constant and can be considered as an hyperparameter of the algorithm.", "startOffset": 172, "endOffset": 194}, {"referenceID": 16, "context": "From an optimization perspective, the problem suggested by the bound of Theorem 6 is much more convenient to minimize than the PAC-Bayesian bound derived from Theorem 2 in Germain et al. (2013). The former is smoother than the latter: the absolute value related to the domain disagreement dis\u03c1(SX, TX) of Equation (4) disappears in benefit of the domain divergence \u03b2\u221e(T \u2016S), which is constant and can be considered as an hyperparameter of the algorithm. Additionally, Theorem 2 requires equal source and target sample sizes while Theorem 6 allows ms 6=mt. Moreover, recall that in Germain et al. (2013) the \u03c1-dependent non-constant term \u03bb(\u03c1) is ignored.", "startOffset": 172, "endOffset": 603}, {"referenceID": 0, "context": "The taken approach is the one privileged in numerous PACBayesian works (e.g., Langford & Shawe-Taylor, 2002; Ambroladze et al., 2006; McAllester & Keshet, 2011; Parrado-Hern\u00e1ndez et al., 2012; Germain et al., 2009; 2013), as it makes the risk of the linear classifier hw and the risk of a (properly parametrized) majority vote coincide, while in the same time promoting large margin classifiers.", "startOffset": 71, "endOffset": 220}, {"referenceID": 34, "context": "The taken approach is the one privileged in numerous PACBayesian works (e.g., Langford & Shawe-Taylor, 2002; Ambroladze et al., 2006; McAllester & Keshet, 2011; Parrado-Hern\u00e1ndez et al., 2012; Germain et al., 2009; 2013), as it makes the risk of the linear classifier hw and the risk of a (properly parametrized) majority vote coincide, while in the same time promoting large margin classifiers.", "startOffset": 71, "endOffset": 220}, {"referenceID": 16, "context": "The taken approach is the one privileged in numerous PACBayesian works (e.g., Langford & Shawe-Taylor, 2002; Ambroladze et al., 2006; McAllester & Keshet, 2011; Parrado-Hern\u00e1ndez et al., 2012; Germain et al., 2009; 2013), as it makes the risk of the linear classifier hw and the risk of a (properly parametrized) majority vote coincide, while in the same time promoting large margin classifiers.", "startOffset": 71, "endOffset": 220}, {"referenceID": 16, "context": "Indeed, RD(G\u03c1w) tends to RD(hw) as \u2016w\u2016 grows, which can provide very tight bounds (see the empirical analyses of Ambroladze et al., 2006; Germain et al., 2009).", "startOffset": 82, "endOffset": 159}, {"referenceID": 17, "context": "The blue dashed line shows the decision boundaries of algorithm PBDA (Germain et al., 2013).", "startOffset": 69, "endOffset": 91}, {"referenceID": 16, "context": "As shown in Germain et al. (2013) the former is given by", "startOffset": 12, "endOffset": 34}, {"referenceID": 30, "context": "As mentioned by McAllester & Keshet (2011), it is, however, the case provided we consider Gaussian processes as measure of distributions \u03c00 and \u03c1w over (infinite)H.", "startOffset": 16, "endOffset": 43}, {"referenceID": 6, "context": "com Reviews benchmark (Blitzer et al., 2006) according to the setting used by Chen et al.", "startOffset": 22, "endOffset": 44}, {"referenceID": 9, "context": "We compare DALC with the classical non-adaptive algorithm SVM (trained only on the source sample), the adaptive algorithm DASVM (Bruzzone & Marconcini, 2010), the adaptive cotraining CODA (Chen et al., 2011), and the PAC-Bayesian domain adaptation algorithm PBDA (Germain et al.", "startOffset": 188, "endOffset": 207}, {"referenceID": 17, "context": ", 2011), and the PAC-Bayesian domain adaptation algorithm PBDA (Germain et al., 2013) based on Theorem 2.", "startOffset": 63, "endOffset": 85}, {"referenceID": 5, "context": "com Reviews benchmark (Blitzer et al., 2006) according to the setting used by Chen et al. (2011); Germain et al.", "startOffset": 23, "endOffset": 97}, {"referenceID": 5, "context": "com Reviews benchmark (Blitzer et al., 2006) according to the setting used by Chen et al. (2011); Germain et al. (2013). This dataset contains reviews of four types of products (books, DVDs, electronics, and kitchen appliances) described with about 100, 000 attributes.", "startOffset": 23, "endOffset": 120}, {"referenceID": 5, "context": "com Reviews benchmark (Blitzer et al., 2006) according to the setting used by Chen et al. (2011); Germain et al. (2013). This dataset contains reviews of four types of products (books, DVDs, electronics, and kitchen appliances) described with about 100, 000 attributes. Originally, the reviews were labeled with a rating from 1 to 5. Chen et al. (2011) proposed a simplified binary setting by regrouping ratings into two classes (products rated lower than 3 and products rated higher than 4).", "startOffset": 23, "endOffset": 353}, {"referenceID": 5, "context": "com Reviews benchmark (Blitzer et al., 2006) according to the setting used by Chen et al. (2011); Germain et al. (2013). This dataset contains reviews of four types of products (books, DVDs, electronics, and kitchen appliances) described with about 100, 000 attributes. Originally, the reviews were labeled with a rating from 1 to 5. Chen et al. (2011) proposed a simplified binary setting by regrouping ratings into two classes (products rated lower than 3 and products rated higher than 4). Moreover, they reduced the dimensionality to about 40,000 by only keeping the features appearing at least ten times for a given domain adaptation task. Finally, the data are pre-processed with a tf-idf re-weighting. A domain corresponds to a kind of product. Therefore, we perform twelve domain adaptation tasks. For instance, \u201cbooks\u2192DVD\u2019s\u201d is the task for which the source domain is \u201cbooks\u201d and the target one is \u201cDVDs\u201d. We compare DALC with the classical non-adaptive algorithm SVM (trained only on the source sample), the adaptive algorithm DASVM (Bruzzone & Marconcini, 2010), the adaptive cotraining CODA (Chen et al., 2011), and the PAC-Bayesian domain adaptation algorithm PBDA (Germain et al., 2013) based on Theorem 2. Note that, in Germain et al. (2013), DASVM has shown better accuracy than SVM, CODA and PBDA.", "startOffset": 23, "endOffset": 1257}, {"referenceID": 5, "context": "com Reviews benchmark (Blitzer et al., 2006) according to the setting used by Chen et al. (2011); Germain et al. (2013). This dataset contains reviews of four types of products (books, DVDs, electronics, and kitchen appliances) described with about 100, 000 attributes. Originally, the reviews were labeled with a rating from 1 to 5. Chen et al. (2011) proposed a simplified binary setting by regrouping ratings into two classes (products rated lower than 3 and products rated higher than 4). Moreover, they reduced the dimensionality to about 40,000 by only keeping the features appearing at least ten times for a given domain adaptation task. Finally, the data are pre-processed with a tf-idf re-weighting. A domain corresponds to a kind of product. Therefore, we perform twelve domain adaptation tasks. For instance, \u201cbooks\u2192DVD\u2019s\u201d is the task for which the source domain is \u201cbooks\u201d and the target one is \u201cDVDs\u201d. We compare DALC with the classical non-adaptive algorithm SVM (trained only on the source sample), the adaptive algorithm DASVM (Bruzzone & Marconcini, 2010), the adaptive cotraining CODA (Chen et al., 2011), and the PAC-Bayesian domain adaptation algorithm PBDA (Germain et al., 2013) based on Theorem 2. Note that, in Germain et al. (2013), DASVM has shown better accuracy than SVM, CODA and PBDA. Each parameter is selected with a grid search thanks to a usual cross-validation (CV) on the source sample for SVM, and thanks to a reverse validation procedure11 (RCV) For details on the reverse validation procedure, see Bruzzone & Marconcini (2010); Zhong et al.", "startOffset": 23, "endOffset": 1566}, {"referenceID": 5, "context": "com Reviews benchmark (Blitzer et al., 2006) according to the setting used by Chen et al. (2011); Germain et al. (2013). This dataset contains reviews of four types of products (books, DVDs, electronics, and kitchen appliances) described with about 100, 000 attributes. Originally, the reviews were labeled with a rating from 1 to 5. Chen et al. (2011) proposed a simplified binary setting by regrouping ratings into two classes (products rated lower than 3 and products rated higher than 4). Moreover, they reduced the dimensionality to about 40,000 by only keeping the features appearing at least ten times for a given domain adaptation task. Finally, the data are pre-processed with a tf-idf re-weighting. A domain corresponds to a kind of product. Therefore, we perform twelve domain adaptation tasks. For instance, \u201cbooks\u2192DVD\u2019s\u201d is the task for which the source domain is \u201cbooks\u201d and the target one is \u201cDVDs\u201d. We compare DALC with the classical non-adaptive algorithm SVM (trained only on the source sample), the adaptive algorithm DASVM (Bruzzone & Marconcini, 2010), the adaptive cotraining CODA (Chen et al., 2011), and the PAC-Bayesian domain adaptation algorithm PBDA (Germain et al., 2013) based on Theorem 2. Note that, in Germain et al. (2013), DASVM has shown better accuracy than SVM, CODA and PBDA. Each parameter is selected with a grid search thanks to a usual cross-validation (CV) on the source sample for SVM, and thanks to a reverse validation procedure11 (RCV) For details on the reverse validation procedure, see Bruzzone & Marconcini (2010); Zhong et al. (2010). Other details on our for CODA, DASVM, PBDA, and DALC.", "startOffset": 23, "endOffset": 1587}, {"referenceID": 5, "context": "com Reviews benchmark (Blitzer et al., 2006) according to the setting used by Chen et al. (2011); Germain et al. (2013). This dataset contains reviews of four types of products (books, DVDs, electronics, and kitchen appliances) described with about 100, 000 attributes. Originally, the reviews were labeled with a rating from 1 to 5. Chen et al. (2011) proposed a simplified binary setting by regrouping ratings into two classes (products rated lower than 3 and products rated higher than 4). Moreover, they reduced the dimensionality to about 40,000 by only keeping the features appearing at least ten times for a given domain adaptation task. Finally, the data are pre-processed with a tf-idf re-weighting. A domain corresponds to a kind of product. Therefore, we perform twelve domain adaptation tasks. For instance, \u201cbooks\u2192DVD\u2019s\u201d is the task for which the source domain is \u201cbooks\u201d and the target one is \u201cDVDs\u201d. We compare DALC with the classical non-adaptive algorithm SVM (trained only on the source sample), the adaptive algorithm DASVM (Bruzzone & Marconcini, 2010), the adaptive cotraining CODA (Chen et al., 2011), and the PAC-Bayesian domain adaptation algorithm PBDA (Germain et al., 2013) based on Theorem 2. Note that, in Germain et al. (2013), DASVM has shown better accuracy than SVM, CODA and PBDA. Each parameter is selected with a grid search thanks to a usual cross-validation (CV) on the source sample for SVM, and thanks to a reverse validation procedure11 (RCV) For details on the reverse validation procedure, see Bruzzone & Marconcini (2010); Zhong et al. (2010). Other details on our for CODA, DASVM, PBDA, and DALC. The algorithms use a linear kernel and consider 2,000 labeled source examples and 2,000 unlabeled target examples. Table 1 reports the error rates of all the methods evaluated on the same separate target test sets proposed by Chen et al. (2011).", "startOffset": 23, "endOffset": 1887}, {"referenceID": 16, "context": "This test tends to confirm that our new bound improves the analysis done previously in Germain et al. (2013), in addition to being more interpretable.", "startOffset": 87, "endOffset": 109}, {"referenceID": 17, "context": "The empirical experiments show that our new algorithm outperforms the previous PAC-Bayesian approach (Germain et al., 2013).", "startOffset": 101, "endOffset": 123}], "year": 2016, "abstractText": "We study the issue of PAC-Bayesian domain adaptation: We want to learn, from a source domain, a majority vote model dedicated to a target one. Our theoretical contribution brings a new perspective by deriving an upper-bound on the target risk where the distributions\u2019 divergence\u2014 expressed as a ratio\u2014controls the trade-off between a source error measure and the target voters\u2019 disagreement. Our bound suggests that one has to focus on regions where the source data is informative. From this result, we derive a PACBayesian generalization bound, and specialize it to linear classifiers. Then, we infer a learning algorithm and perform experiments on real data.", "creator": "LaTeX with hyperref package"}}}