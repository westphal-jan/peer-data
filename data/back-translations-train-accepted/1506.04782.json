{"id": "1506.04782", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2015", "title": "Cheap Bandits", "abstract": "We consider stochastic sequential learning problems where the learner can observe the \\textit{average reward of several actions}. Such a setting is interesting in many applications involving monitoring and surveillance, where the set of the actions to observe represent some (geographical) area. The importance of this setting is that in these applications, it is actually \\textit{cheaper} to observe average reward of a group of actions rather than the reward of a single action. We show that when the reward is \\textit{smooth} over a given graph representing the neighboring actions, we can maximize the cumulative reward of learning while \\textit{minimizing the sensing cost}. In this paper we propose CheapUCB, an algorithm that matches the regret guarantees of the known algorithms for this setting and at the same time guarantees a linear cost again over them. As a by-product of our analysis, we establish a $\\Omega(\\sqrt{dT})$ lower bound on the cumulative regret of spectral bandits for a class of graphs with effective dimension $d$.", "histories": [["v1", "Mon, 15 Jun 2015 21:42:45 GMT  (2914kb,D)", "https://arxiv.org/abs/1506.04782v1", "To be presented at ICML 2015"], ["v2", "Thu, 18 Jun 2015 22:40:51 GMT  (2918kb,D)", "http://arxiv.org/abs/1506.04782v2", "To be presented at ICML 2015"]], "COMMENTS": "To be presented at ICML 2015", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["manjesh kumar hanawal", "venkatesh saligrama", "michal valko", "r\u00e9mi munos"], "accepted": true, "id": "1506.04782"}, "pdf": {"name": "1506.04782.pdf", "metadata": {"source": "META", "title": "Cheap Bandits", "authors": ["Manjesh Kumar Hanawal", "Venkatesh Saligrama", "Michal Valko"], "emails": ["MHANAWAL@BU.EDU", "SRV@BU.EDU", "MICHAL.VALKO@INRIA.FR", "REMI.MUNOS@INRIA.FR"], "sections": [{"heading": null, "text": "\u221a dT) falls below the limit of cumulative regret of spectral bandits for a class of graphs with the effective dimension d."}, {"heading": "1. Introduction", "text": "This year, it has come to the point that it will only be once before there is such a process, in which there is such a process."}, {"heading": "2. Related Work", "text": "There are several other bandits and online learning settings that take costs into account (Tran-Thanh et al., 2012; Badanidiyuru et al., 2013; Ding et al., 2013; Badanidiyuru et al., 2014; Zolghadr et al., 2013; Cesa-Bianchi et al., 2013a). The first sentence is called budgeted bandits (Tran-Thanh et al., 2012) or banknote bandits (Badanidiyuru et al., 2013), where each individual arm is associated with costs. These costs may be known or unknown (Ding et al., 2013) and may depend on a particular context (Badanidiyuru et al., 2014). The goal there is generally to minimize regret as a function of the budget or to minimize regret."}, {"heading": "3. Problem Setup", "text": "Let G = (V, E) denote an undirected graph with the number of nodes | V | = N. Let's assume that the degree of all nodes is limited by \u0443. Let s: V \u2192 R denote a signal on G and S denote the set of all possible signals on G. Let L = D \u2212 A denote the unnormalized laplaker of graph G, where A = {aij} is the adjacent matrix and D is the diagonal matrix with Dii = \u2211 j aij. Let's emphasize that our main results extend to weighted graphs when we replace matrix A with the edge weight matrix W. Let's work with matrix A to simplify exposure. We denote the eigenvalues of L as 0 = \u03bb1 \u2264 2 \u2264 \u00b7 \u00b7 \u2264 \u03bb N, and the corresponding eigenvectors as q1, q2, \u00b7 \u00b7, qN. Equally, let's write L = Q.LveQ \u2032, with L = 1 \u00d7 V and eigenvectors (V) as \u03bcV and eigenvectors."}, {"heading": "3.1. Reward function", "text": "We define a reward function on a diagram G as a linear combination of eigenvectors. For a given parameter vector \u03b1-RN, f\u03b1: V \u2192 R denotes the reward function on the node, asf\u03b1 = Q\u03b1.The parameter \u03b1 can be punished accordingly to control the smoothness of the reward function. For example, if we choose \u03b1 so that large coefficients correspond to eigenvectors associated with small eigenvalues, then f\u03b1 is a smooth function of G (Belkin et al., 2008).We denote the unknown parameter that defines the true reward function as \u03b1 \u0445. To formalize this notion, we associate the reward of the node i as f\u03b2 (i).In our setting, the arms are nodes and the subsets of their neighbors. When an arm is selected, we observe only the average of the rewards of the nodes selected by that arm. To formalize this notion, we associate arms with probe signals on diagrams."}, {"heading": "3.2. Probes", "text": "We use the word probe and action interchangeable. A probe is a signal with its width corresponding to the support of signal s. For example, it could correspond to the region of coverage or the region of interest driven by a radar pulse. Thus, each s-S is the average reward of the supp (s) number of nodes. We parameterise a probe in relation to its width w- \u00b7 \u00b7 \u00b7 \u00b7, N where supp (s) denotes the number of positive elements in s. The inner product of f\u0430 and a probe s is the average reward of the supp (s) number of nodes. We parameterise a probe in relation to its width w- \u00b7 [N] and let the amount of probes width w be a snode."}, {"heading": "3.3. Cost of probes", "text": "The cost of the arms is defined using the spectral properties of their associated graph probes. Let's use the Graphene Fourier Transformation (GFT) of the probe s \"S.\" Similar to the Fourier transformation of a continuous function, GFT gives amplitudes associated with graph frequencies. The GFT coefficient of a probe at frequency \"i, i = 1, 2 \u00b7 \u00b7, N is achieved by projecting it onto Qi, i.e., s\" Q \"s, where s\" i, \"i\" 2, \"\u00b7, N is the GFT coefficient of a probe that is associated with frequency.\" Let the probe \"S\" R \"denotes the cost function of the probe s,\" i.e. the cost of the probe \"s\" is described by C (s) = \"i.\" (si \u2212 sj) 2, \"where the summation of all disordered nodes\". \""}, {"heading": "3.4. Learning setting and performance metrics", "text": "Our learning environment is as follows: The learner uses a political algorithm \u03c0: {1, 2, \u00b7 \u00b7, T} \u2192 SD, which assigns \u03c0 (t) to exams at step t \u2264 T. In each step t, the recommender causes costs C (\u03c0 (t)) and receives a loud reward such as thatrt = FG (\u03c0 (t)) + \u03b5t, whereby \u03b5t is independent of R-Sub Gauss for everyone. (4) The cumulative regret of policy \u03c0 is defined as RT = TFG (s) \u2212 T \u0445 t = 1 FG (\u03c0 (t)) (3) and the total cost incurred to date is indicated by CT = T \u0445 t = 1 C (t). (4) The goal of the learner is to learn a political discipline that minimizes the total cost of CT while keeping the cumulative cost of CT (pseudo) as low as possible."}, {"heading": "4. Node Actions: Spectral Bandits", "text": "If we limit the action to SD = {ei: i = 1, 2, \u00b7 \u00b7, n}, with ei setting a binary vector with a component set to 1 and all other components to 0, then only node actions are allowed in each step. In this setting, the cost of all actions is equal, i.e., C (ei) = 1 for all i. Using these node actions, Valko et al. (2014) developed SpectralUCB, which aims to minimize regret on the assumption that the reward function is smooth. The state of smoothness is characterized as follows: c > 0 so that the overall performance of SpectralUCB grows. (5) Here, SpectralUCB is used to minimize regret on the assumption that the reward function is smooth. The tied c characterizes the smoothness of the reward (c is small, the reward on the adjacent nodes is more similar."}, {"heading": "5. Group Actions: Cheap Bandits", "text": "In this section, we develop a learning algorithm aimed at minimizing the total cost without compromising remorse for the use of group actions. Specifically, given T and a chart with effective dimension d, our goal is as follows: min \u03c0CT are subject to RT. d. D. T (7), with optimization going beyond the guidelines defined in the amount of measures SD specified in subsection 3.2."}, {"heading": "5.1. Lower bound", "text": "The set of measures used in the above optimization problem is larger than the unit of measurement used in the SpectralUCB. This raises the question of whether the regret sequence of d \u221a T is too loose or not, especially if SpectralUCB can realize this limit with a much smaller set of probes. In this section, we derive a lower limit for the expected regret (worst case) for each algorithm that uses the action space SD on charts with effective dimension d. While this means that our goal should be in (7) dT, we follow Valko et al. (2014) and develop a variant of SpectralUCB that gets the target remorse from d \u221a T. We leave it as future work to develop an algorithm that fulfills the goal remorse of \u221a dT while minimizing the costs.Let us call Gd a series of charts with effective dimension d."}, {"heading": "5.2. Local smoothness", "text": "In this subsection, we show that a smooth reward function on a graph with a low effective dimension implies a local smoothness of the reward function around each node. Specifically, we find that the average reward around the neighborhood of a node provides good information about the reward of the node itself. However, instead of exploring a node, we can use group actions to examine its neighborhood and get good estimates of the reward at a low cost. From the discussion in Section 4, when d is small and there is a large gap between the eigenvalues and the + 1 reward, SpectralUCB enjoys a small limit on regret over a wide range of values in the interval [d \u2212 1), which implies a large gap between the eigenvalues that there is a good division of the graph into narrow clusters. Furthermore, the smoothness assumption implies that the reward of a node and its neighbors within each cluster are similar."}, {"heading": "5.3. Algorithm: CheapUCB", "text": "The main difference between our algorithm and the UCB spectral algorithm is the expanded scope of action, which allows the selection of subsets of nodes and the associated realization of average rewards. Note: When we examine a specific node instead of a subset of nodes, we get more accurate information about the node, but this leads to higher costs. Since our goal is to minimize costs while maintaining a low regret, we handle this requirement by moving sequentially from the least expensive samples to expensive as we progress. Specifically, we divide the time horizon into J stages, and how we move from the j + 1 state to the more expensive samples. That means that we use samples with smaller widths as we move through the different stages of learning."}, {"heading": "5.4. Computational complexity and scalability", "text": "The calculation and scalability problems of CheapUCB are essentially those associated with the SpectralUCB, i.e., the calculation of the eigenbasis of the graph Laplacian, matrixinversion and calculation of the UCBs. Although CheapUCB uses larger groups of arms or probes at each step, it only needs to calculate N UCBs as | Sw | = N for allw. The i-th probe in the sentence Sw can be calculated by sorting the elements of the edge weights W (i,:) and assigning weight 1 / w to the first w components. Like Valko et al. (2014), we accelerate matrixinversion by iterative updating (Zhang, 2005) and calculate the eigenbase of the symmetric laplac matrix using fast symmetric diagonally dominant solvers such as CMG (Koutis et al., 2011)."}, {"heading": "6. Experiments", "text": "We evaluate and compare our algorithm with SpectralUCB, which has been proven to outperform its competitor LinUCB when it comes to learning from graphs with a large number of nodes. To demonstrate the potential of our algorithm in a more realistic scenario, we also offer experiments with Forest Cover Type datasets. We use \u03b4 = 0.001, R = 0.01 and \u03bb = 0.01."}, {"heading": "6.1. Random graphs models", "text": "We created graphs from two graph models that are widely used to analyze connectivity in social networks. First, we created an Erdo-s-Re-nyi (ER) graph with a probability of 0.05 independent of other edges. Second, we created a Baraba-si-Albert (BA) graph with the degree parameter 3. We randomly assigned the edge weights of these graphs uniformly. To get a reward function f, we randomly create a sparse vector \u03b1 with a small k N and use it to combine the eigenvectors of the graph laplacianically linearly as f = Q\u03b1 \u0445, where Q is the orthonorthonormal matrix derived from the self-decomposition of the laplacic graph. We executed our algorithm on each graph of the T < N regime. In the graphs shown, we used N = UC250, UCT = 100 rounds, and UCT = 150. We averaged the experiments."}, {"heading": "6.2. Stochastic block models", "text": "Many nodes can be grouped naturally to form a tightly knit collection of clusters with sparse connections between the different clusters (Girvan & Newman, 2002).The diagram of such networks often shows dense clusters with sparse connections to each other. Stochastic block models are based on connecting nodes within each block / cluster with high probability and nodes that are in two different blocks / clusters with low probability. For our simulations, we generated an SBM as follows: We grouped N = 250 nodes into 4 blocks of size 100, 60, 40 and 50 and connected nodes within each block with a probability of 0.7. Nodes from the different blocks are associated with a probability of 0.02."}, {"heading": "6.3. Forest Cover Type data", "text": "Since our motivation for cheap bandits stems from the cost sensitization scenario, we have conducted experiments on forest cover, so that a collection of 581021 samples mentioned provide observations at 30m x 30m in each forest area. This data collection was chosen to match the radar from the outset, namely, that we can view the forest area from above if vague sensations are present in a particular region. The observations are 12 \"cartographic measures of the regions and are used as independent variables to evaluate coverage by Filippi et al. Labels in forest cover refer to the dominant species of trees (coverage) in a particular region."}, {"heading": "7. Conclusion", "text": "We have introduced Cheap Bandits, a new setting that aims to minimize the capture costs of group actions while achieving the most advanced regret guarantees in terms of the effective dimension. The main advantage over typical bandit settings is that it models situations where it is cheaper to get the average reward from a series of neighboring actions than a reward from a single one. For stochastic rewards, we have proposed and evaluated CheapUCB, an algorithm that guarantees a cost advantage that is linear over time. In the future, we plan to extend this new sensing setting to other settings with limited feedback, such as contextual, combinatory, and non-stochastic bandits. As a by-product of our analysis, we are establishing a lower limit on cumulative regret for a class of graphs with an effective dimension."}, {"heading": "Acknowledgment", "text": "This material is based on work partially supported by NSF grants CNS-1330008, CIF-1320566, CIF-1218992 and the U.S. Department of Homeland Security, Science and Technology Directorate, Office of University Programs under Grant Award 2013-ST-061-ED0001. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies of the U.S. Department of Homeland Security or the National Science Foundation, neither express nor implied."}, {"heading": "8. Proof of Proposition 1", "text": "We assume that the two cases are a reward case. (T, \u03c0, \u03b1, G) = E [T \u2211 t = 1 s, s, s, s, s, s, s, s, s, s, s, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S,"}, {"heading": "9. Proof of Proposition 2", "text": "Definition 2 (k-way expansion constant (Lee et al., 2012) Consider a diagram G and X-V let\u03c6G (X): For all k > 0, k-way expansion constant is defined as\u03c1G (k) = min {max\u03c6 (Vi): Number of nodes in Xand | x | Number of edges between nodes in X and V\\ X. For all k > 0, k \u2212 way expansion constant is defined as\u03c1G (k) = min {max\u03c6 (Vi): Number of nodes in Xand | Vi | Number of edges between nodes in X and V\\ X., \u2264 \u00b5N = eigenvalues of the normalized Laplacian of G.Theorem 3 (Gharan & Trevisan, 2014)."}, {"heading": "10. Analysis of CheapUCB", "text": "For a given confidence parameter \u03b4 define\u03b2 = 2R \u221a d log (1 + T\u03bb) + 2 log 1\u03b4 + c, and look at the ellipsoid by the estimate \u03b1-tCt = {\u03b1: \u0432-\u03b1-Vt \u2264 \u03b2}. We first output the following results (Abbasi-Yadkori et al., 2011), (Dani et al., 2008), and (Valko et al., 2014) Lemma 3 (Self-Normalized Bound). We have: log det (Vt) det (Vt) det (\u03bbI) and \u03bb > 0. Then we have > 0 for each other possibility, with a probability of at least 1 \u2212 \u043c and for all t > 0, as well as for each other possibility."}, {"heading": "10.1. Proof of Theorem 2", "text": "We first prove the case in which the degree of each node is at least equal. \u2212 In this step, look at a probe of latitude J \u2212 j + 1. \u2212 See that in this step a probe of latitude J \u2212 j + 1 is selected. \u2212 The probe selected at this time is called st. Note that both st and s are the probe of latitude J \u2212 j + 1 associated with the optimal probe s + 1. \u2212 In this step, see that both st and s wj are in the set SJ \u2212 j + 1."}, {"heading": "10.2. For the case when (10) holds:", "text": "In this case, we use h (j) = c's \u221a T (J \u2212 j + 1) / \u03bbd + 1. First, we regret that 2j \u2212 1h (j) in 1 \u2264 j \u2264 J \u2212 1. We have J \u2212 1 \u2211 j = 1 2j \u2212 1c's \u221a T (J \u2212 j + 1) \u03bbd + 1 \u2264 (J \u2212 1) 2 J \u2212 1 \u221a Tc \u2032 \u03bbd + 1 \u2264 (J \u2212 1) 2 log2 T \u2212 1c's \u221a T (J \u2212 1). In the second line, we apply the definition of effective dimension.10.3. In the case where we use + 1 / \u03bbd + 1 / \u03bbd = O (d2) \u2264 dc's \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s"}], "references": [{"title": "Improved algorithms for linear stochastic bandits", "author": ["Y. Abbasi-Yadkori", "D. Pal", "C. Szepesvari"], "venue": "In Proceeding of NIPS,", "citeRegEx": "Abbasi.Yadkori et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Abbasi.Yadkori et al\\.", "year": 2011}, {"title": "Efficient sensor management policies for distributed target tracking in multihop sensor networks", "author": ["Aeron", "Shuchin", "Saligrama", "Venkatesh", "Castanon", "David A"], "venue": "IEEE Transactions on Signal Processing (TSP),", "citeRegEx": "Aeron et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Aeron et al\\.", "year": 2008}, {"title": "From Bandits to Experts: A Tale of Domination and Independence", "author": ["Alon", "Noga", "Cesa-Bianchi", "Nicol\u00f2", "Gentile", "Claudio", "Mansour", "Yishay"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Alon et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Alon et al\\.", "year": 2013}, {"title": "The non-stochastic multi-armed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Robert", "Y. Freund", "E. Schapire"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2003}, {"title": "Using confidence bounds for exploitationexploration trade-offs", "author": ["Auer", "Peter"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Auer and Peter.,? \\Q2002\\E", "shortCiteRegEx": "Auer and Peter.", "year": 2002}, {"title": "The Nonstochastic Multiarmed Bandit Problem", "author": ["Auer", "Peter", "Cesa-Bianchi", "Nicol\u00f2", "Freund", "Yoav", "Schapire", "Robert E"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Resourceful contextual bandits", "author": ["A. Badanidiyuru", "J. Langford", "A. Slivkins"], "venue": "In Proceeding of Conference on Learning Theory,", "citeRegEx": "Badanidiyuru et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Badanidiyuru et al\\.", "year": 2014}, {"title": "Bandits with knapsacks", "author": ["Badanidiyuru", "Ashwinkumar", "Kleinberg", "Robert", "Slivkins", "Aleksandrs"], "venue": "In Proceedings - Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "Badanidiyuru et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Badanidiyuru et al\\.", "year": 2013}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Belkin et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Belkin et al\\.", "year": 2008}, {"title": "Leveraging Side Observations in Stochastic Bandits", "author": ["Caron", "St\u00e9phane", "Kveton", "Branislav", "Lelarge", "Marc", "Bhagat", "Smriti"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Caron et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Caron et al\\.", "year": 2012}, {"title": "Online Learning with Switching Costs and Other Adaptive Adversaries", "author": ["Cesa-Bianchi", "Nicol\u00f2", "Dekel", "Ofer", "Shamir", "Ohad"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2013}, {"title": "A Gang of Bandits", "author": ["Cesa-Bianchi", "Nicol\u00f2", "Gentile", "Claudio", "Zappella", "Giovanni"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2013}, {"title": "Stochastic linear optimization under bandit feedback", "author": ["V. Dani", "T.P. Hayes", "S.M. Kakade"], "venue": "In Proceeding of Conference on Learning Theory, COLT, Helsinki,", "citeRegEx": "Dani et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dani et al\\.", "year": 2008}, {"title": "Multi-Armed Bandit with Budget Constraint and Variable Costs", "author": ["Ding", "Wenkui", "Qin", "Tao", "Zhang", "Xu-dong", "Liu", "Tieyan"], "venue": "In Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence,", "citeRegEx": "Ding et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2013}, {"title": "Adaptive statistical sampling methods for decentralized estimation and detection of localized phenomena", "author": ["Ermis", "Erhan Baki", "Saligrama", "Venkatesh"], "venue": "Proceedings of Information Processing in Sensor Networks (IPSN),", "citeRegEx": "Ermis et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ermis et al\\.", "year": 2005}, {"title": "Distributed detection in sensor networks with limited range multimodal sensors", "author": ["Ermis", "Erhan Baki", "Saligrama", "Venkatesh"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Ermis et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ermis et al\\.", "year": 2010}, {"title": "Parametric bandits: The generalized linear case", "author": ["L. Filippi", "O. Cappe", "A. Garivier", "C. Szepesvari"], "venue": "In Proceeding of NIPS,", "citeRegEx": "Filippi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Filippi et al\\.", "year": 2010}, {"title": "Smart sleeping policies for energy efficient tracking in sensor networks", "author": ["Fuemmeler", "Jason A", "Veeravalli", "Venugopal V"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Fuemmeler et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fuemmeler et al\\.", "year": 2008}, {"title": "Online Clustering of Bandits", "author": ["Gentile", "Claudio", "Li", "Shuai", "Zappella", "Giovanni"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Gentile et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gentile et al\\.", "year": 2014}, {"title": "Partitioning into expanders", "author": ["S.O. Gharan", "L. Trevisan"], "venue": "In Proceeding of Symposium of Discrete Algorithms,", "citeRegEx": "Gharan and Trevisan,? \\Q2014\\E", "shortCiteRegEx": "Gharan and Trevisan", "year": 2014}, {"title": "Community structure in social and biological networks", "author": ["M. Girvan", "M.E. Newman"], "venue": "In Proceedings of Natl Acad Sci USA,", "citeRegEx": "Girvan and Newman,? \\Q2002\\E", "shortCiteRegEx": "Girvan and Newman", "year": 2002}, {"title": "Efficient learning by implicit exploration in bandit problems with side observations", "author": ["Koc\u00e1k", "Tom\u00e1\u0161", "Neu", "Gergely", "Valko", "Michal", "Munos", "R\u00e9mi"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Koc\u00e1k et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Koc\u00e1k et al\\.", "year": 2014}, {"title": "Combinatorial preconditioners and multilevel solvers for problems in computer vision and image processing", "author": ["Koutis", "Ioannis", "Miller", "Gary L", "Tolliver", "David"], "venue": "Computer Vision and Image Understanding,", "citeRegEx": "Koutis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Koutis et al\\.", "year": 2011}, {"title": "Multi-way spectral partitioning and higher-order cheeger inequalities", "author": ["Lee", "James R", "Gharan", "Shayan Oveis", "Trevisan", "Luca"], "venue": "In Proceeding of STOC,", "citeRegEx": "Lee et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2012}, {"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["L. Li", "C. Wei", "J. Langford", "R.E. Schapire"], "venue": "In Proceeding of International Word Wide Web conference,", "citeRegEx": "Li et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "From Bandits to Experts: On the Value of Side-Observations", "author": ["Mannor", "Shie", "Shamir", "Ohad"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Mannor et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mannor et al\\.", "year": 2011}, {"title": "Signal processing techniques for interpolation in graph structured data", "author": ["S.K. Narang", "A. Gadde", "A. Ortega"], "venue": "In Proceedings of International Conference of Acoustics, Speech and Signal Processing,", "citeRegEx": "Narang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Narang et al\\.", "year": 2013}, {"title": "The emerging filed of signal processing on graphs", "author": ["D.I. Shuman", "S.K. Narang", "P. Frossard", "A. Ortega", "P. Vanderghenyst"], "venue": "In IEEE Signal Processing Magazine,", "citeRegEx": "Shuman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Shuman et al\\.", "year": 2013}, {"title": "Knapsack Based Optimal Policies for Budget-Limited Multi-Armed Bandits", "author": ["Tran-Thanh", "Long", "Chapman", "Archie C", "Rogers", "Alex", "Jennings", "Nicholas R"], "venue": null, "citeRegEx": "Tran.Thanh et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tran.Thanh et al\\.", "year": 2012}, {"title": "Spectral Bandits for Smooth Graph Functions", "author": ["Valko", "Michal", "Munos", "R\u00e9mi", "Kveton", "Branislav", "Koc\u00e1k", "Tom\u00e1\u0161"], "venue": "In 31th International Conference on Machine Learning,", "citeRegEx": "Valko et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Valko et al\\.", "year": 2014}, {"title": "The schur complement and its", "author": ["F. Zhang"], "venue": "application. Springer,", "citeRegEx": "Zhang,? \\Q2005\\E", "shortCiteRegEx": "Zhang", "year": 2005}, {"title": "Graph spectral compressed sensing for sensor networks", "author": ["X. Zhu", "M. Rabbat"], "venue": "In Proceedings of International Conference of Acoustics, Speech and Signal Processing,", "citeRegEx": "Zhu and Rabbat,? \\Q2012\\E", "shortCiteRegEx": "Zhu and Rabbat", "year": 2012}, {"title": "Online Learning with Costly Features and Labels", "author": ["Zolghadr", "Navid", "Bartok", "Gabor", "Greiner", "Russell", "Gy\u00f6rgy", "Andr\u00e1s", "Szepesvari", "Csaba"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zolghadr et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zolghadr et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 1, "context": "To conserve battery power, sleep/awake scheduling is used (Fuemmeler & Veeravalli, 2008; Aeron et al., 2008), wherein a group of sensors is woken up sequentially based on probable locations of target.", "startOffset": 58, "endOffset": 108}, {"referenceID": 26, "context": "Rewards in many applications are typically smooth band-limited graph signals (Narang et al., 2013) with the sensing field decaying smoothly with distance from the target.", "startOffset": 77, "endOffset": 98}, {"referenceID": 27, "context": "small subset of nodes (Shuman et al., 2013).", "startOffset": 22, "endOffset": 43}, {"referenceID": 12, "context": "We model this problem as an instance of linear bandits (Auer, 2002; Dani et al., 2008; Li et al., 2010) that links the reward of nodes through an unknown parameter.", "startOffset": 55, "endOffset": 103}, {"referenceID": 24, "context": "We model this problem as an instance of linear bandits (Auer, 2002; Dani et al., 2008; Li et al., 2010) that links the reward of nodes through an unknown parameter.", "startOffset": 55, "endOffset": 103}, {"referenceID": 12, "context": "We model this problem as an instance of linear bandits (Auer, 2002; Dani et al., 2008; Li et al., 2010) that links the reward of nodes through an unknown parameter. A bandit setting for smooth signals was recently studied by Valko et al. (2014), however neglecting the signal cost.", "startOffset": 68, "endOffset": 245}, {"referenceID": 28, "context": "There are several other bandit and online learning settings that consider costs (Tran-Thanh et al., 2012; Badanidiyuru et al., 2013; Ding et al., 2013; Badanidiyuru et al., 2014; Zolghadr et al., 2013; Cesa-Bianchi et al., 2013a).", "startOffset": 80, "endOffset": 229}, {"referenceID": 7, "context": "There are several other bandit and online learning settings that consider costs (Tran-Thanh et al., 2012; Badanidiyuru et al., 2013; Ding et al., 2013; Badanidiyuru et al., 2014; Zolghadr et al., 2013; Cesa-Bianchi et al., 2013a).", "startOffset": 80, "endOffset": 229}, {"referenceID": 13, "context": "There are several other bandit and online learning settings that consider costs (Tran-Thanh et al., 2012; Badanidiyuru et al., 2013; Ding et al., 2013; Badanidiyuru et al., 2014; Zolghadr et al., 2013; Cesa-Bianchi et al., 2013a).", "startOffset": 80, "endOffset": 229}, {"referenceID": 6, "context": "There are several other bandit and online learning settings that consider costs (Tran-Thanh et al., 2012; Badanidiyuru et al., 2013; Ding et al., 2013; Badanidiyuru et al., 2014; Zolghadr et al., 2013; Cesa-Bianchi et al., 2013a).", "startOffset": 80, "endOffset": 229}, {"referenceID": 32, "context": "There are several other bandit and online learning settings that consider costs (Tran-Thanh et al., 2012; Badanidiyuru et al., 2013; Ding et al., 2013; Badanidiyuru et al., 2014; Zolghadr et al., 2013; Cesa-Bianchi et al., 2013a).", "startOffset": 80, "endOffset": 229}, {"referenceID": 28, "context": "The first set is referred to as budgeted bandits (Tran-Thanh et al., 2012) or bandits with knapsacks (Badanidiyuru et al.", "startOffset": 49, "endOffset": 74}, {"referenceID": 7, "context": ", 2012) or bandits with knapsacks (Badanidiyuru et al., 2013), where each single arm is associated with a cost.", "startOffset": 34, "endOffset": 61}, {"referenceID": 13, "context": "This cost can be known or unknown (Ding et al., 2013) and can depend on a given context (Badanidiyuru et al.", "startOffset": 34, "endOffset": 53}, {"referenceID": 6, "context": ", 2013) and can depend on a given context (Badanidiyuru et al., 2014).", "startOffset": 42, "endOffset": 69}, {"referenceID": 32, "context": "Another cost setting considers cost for observing features from which the learner can build its prediction (Zolghadr et al., 2013).", "startOffset": 107, "endOffset": 130}, {"referenceID": 2, "context": "Another graph bandit setting considers side information, when the learner obtains besides the reward of the node it chooses, also the rewards of the neighbors (Mannor & Shamir, 2011; Alon et al., 2013; Caron et al., 2012; Koc\u00e1k et al., 2014).", "startOffset": 159, "endOffset": 241}, {"referenceID": 9, "context": "Another graph bandit setting considers side information, when the learner obtains besides the reward of the node it chooses, also the rewards of the neighbors (Mannor & Shamir, 2011; Alon et al., 2013; Caron et al., 2012; Koc\u00e1k et al., 2014).", "startOffset": 159, "endOffset": 241}, {"referenceID": 21, "context": "Another graph bandit setting considers side information, when the learner obtains besides the reward of the node it chooses, also the rewards of the neighbors (Mannor & Shamir, 2011; Alon et al., 2013; Caron et al., 2012; Koc\u00e1k et al., 2014).", "startOffset": 159, "endOffset": 241}, {"referenceID": 18, "context": ", 2013b) and online clustering of bandits in (Gentile et al., 2014).", "startOffset": 45, "endOffset": 67}, {"referenceID": 23, "context": "The most related graph bandits setting to ours is by Valko et al. (2014) on which we build this paper.", "startOffset": 53, "endOffset": 73}, {"referenceID": 29, "context": "This regret bound is of the same order as SpectralUCB (Valko et al., 2014) that does not take cost into consideration.", "startOffset": 54, "endOffset": 74}, {"referenceID": 8, "context": "For instance, if we choose \u03b1 such that large coefficients correspond to the eigenvectors associated with small eigenvalues then f\u03b1 is a smooth function of G (Belkin et al., 2008).", "startOffset": 157, "endOffset": 178}, {"referenceID": 29, "context": "In the following, we first state the regret performance of the SpectralUCB algorithm (Valko et al., 2014) that uses only node actions.", "startOffset": 85, "endOffset": 105}, {"referenceID": 29, "context": "Using these node actions, Valko et al. (2014) developed SpectralUCB that aims to minimize the regret under the assumption that the reward function is smooth.", "startOffset": 26, "endOffset": 46}, {"referenceID": 29, "context": "To characterize the regret performance of SpectralUCB, Valko et al. (2014) introduced the notion of effective dimension defined as follows:", "startOffset": 55, "endOffset": 75}, {"referenceID": 29, "context": "Theorem 1 (Valko et al., 2014) The cumulative regret of SpectralUCB is bounded with probability at least 1\u2212 \u03b4 as:", "startOffset": 10, "endOffset": 30}, {"referenceID": 29, "context": "While this implies that our target in (7) should be \u221a dT , we follow Valko et al. (2014) and develop a variation of SpectralUCB that obtains the target regret of d \u221a T .", "startOffset": 69, "endOffset": 89}, {"referenceID": 3, "context": "1 of Auer et al. (2003) and lower bound the minimax risk.", "startOffset": 5, "endOffset": 24}, {"referenceID": 24, "context": "Below we present an algorithm similar to LinUCB (Li et al., 2010) and SpectralUCB (Valko et al.", "startOffset": 48, "endOffset": 65}, {"referenceID": 29, "context": ", 2010) and SpectralUCB (Valko et al., 2014) for regret minimization.", "startOffset": 24, "endOffset": 44}, {"referenceID": 30, "context": "(2014), we speed up matrix inversion using iterative update (Zhang, 2005), and compute the eigenbasis of symmetric Laplacian matrix using fast symmetric diagonally dominant solvers as CMG (Koutis et al.", "startOffset": 60, "endOffset": 73}, {"referenceID": 22, "context": "(2014), we speed up matrix inversion using iterative update (Zhang, 2005), and compute the eigenbasis of symmetric Laplacian matrix using fast symmetric diagonally dominant solvers as CMG (Koutis et al., 2011).", "startOffset": 188, "endOffset": 209}, {"referenceID": 28, "context": "As Valko et al. (2014), we speed up matrix inversion using iterative update (Zhang, 2005), and compute the eigenbasis of symmetric Laplacian matrix using fast symmetric diagonally dominant solvers as CMG (Koutis et al.", "startOffset": 3, "endOffset": 23}, {"referenceID": 16, "context": "This dataset was already used to evaluate a bandit setting by Filippi et al. (2010).", "startOffset": 62, "endOffset": 84}, {"referenceID": 16, "context": "To find the regions of high concentration of a given cover type, we first clustered the samples using only the quantitative attributes ignoring all the qualitative measurements as done in (Filippi et al., 2010).", "startOffset": 188, "endOffset": 210}], "year": 2015, "abstractText": "We consider stochastic sequential learning problems where the learner can observe the average reward of several actions. Such a setting is interesting in many applications involving monitoring and surveillance, where the set of the actions to observe represent some (geographical) area. The importance of this setting is that in these applications, it is actually cheaper to observe average reward of a group of actions rather than the reward of a single action. We show that when the reward is smooth over a given graph representing the neighboring actions, we can maximize the cumulative reward of learning while minimizing the sensing cost. In this paper we propose CheapUCB, an algorithm that matches the regret guarantees of the known algorithms for this setting and at the same time guarantees a linear cost again over them. As a by-product of our analysis, we establish a \u03a9( \u221a dT ) lower bound on the cumulative regret of spectral bandits for a class of graphs with effective dimension d.", "creator": "LaTeX with hyperref package"}}}