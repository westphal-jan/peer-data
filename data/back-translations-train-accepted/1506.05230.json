{"id": "1506.05230", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2015", "title": "Non-distributional Word Vector Representations", "abstract": "Data-driven representation learning for words is a technique of central importance in NLP. While indisputably useful as a source of features in downstream tasks, such vectors tend to consist of uninterpretable components whose relationship to the categories of traditional lexical semantic theories is tenuous at best. We present a method for constructing interpretable word vectors from hand-crafted linguistic resources like WordNet, FrameNet etc. These vectors are binary (i.e, contain only 0 and 1) and are 99.9% sparse. We analyze their performance on state-of-the-art evaluation methods for distributional models of word vectors and find they are competitive to standard distributional approaches.", "histories": [["v1", "Wed, 17 Jun 2015 07:40:14 GMT  (21kb)", "http://arxiv.org/abs/1506.05230v1", "Proceedings of ACL 2015"]], "COMMENTS": "Proceedings of ACL 2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["manaal faruqui", "chris dyer"], "accepted": true, "id": "1506.05230"}, "pdf": {"name": "1506.05230.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["cdyer}@cs.cmu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 150 6.05 230v 1 [cs.C L] 17 Jun 2015"}, {"heading": "1 Introduction", "text": "Distributed representations of words have been shown to benefit from a variety of different NLP tasks, including syntactical analysis (Lazaridou et al., 2013; Bansal et al., 2014), designated entity recognition (Guo et al., 2014), and sentiment analysis (Socher et al., 2013). Furthermore, intrinsic assessments of various tasks help to discover vector learning methods that capture many facts about lexical semantics (Turney, 2001; Turney and Pantel, 2010). However, induced word vectors do not look like the word learning methods described in most lexical semantic theories, which focus on the identification of word classes (Levin, 1993; Baker et al., 1998; Schuler, 2005; Miller, 1995)."}, {"heading": "2 Linguistic Word Vectors", "text": "We construct linguistic word vectors by extracting word level information from linguistic resources. Table 1 shows the size of the vocabulary and the number of attributes evoked by each lexicon. We now describe various linguistic resources that we use to construct linguistic words."}, {"heading": "3 Experiments", "text": "We first briefly describe the evaluation tasks and then present the results."}, {"heading": "3.1 Evaluation Tasks", "text": "The first is the widely used WS-353 dataset (Finkelstein et al., 2001), 3http: / / www.gutenberg.org / ebooks / 10681 4https: / / archive.org / details / synonymsantonyms00ordwialawhich contains 353 pairs of English words that have been assigned similarity ratings by humans. The second is the RG-65 dataset (Rubenstein and Goodenough, 1965) of 65 word pairs. The third dataset is SimLex (Hill et al., 2014), which was designed to overcome the shortcomings of WS-353 and contains 999 pairs of adjectives, nouns and verbs. Word similarity is computed using cossimilarity between two words and Spearman's rank correlation is reported between the ranking produced by vector model against the human.Lazotined Sentze created by sound72."}, {"heading": "3.2 Linguistic Vs. Distributional Vectors", "text": "To make our linguistic vectors comparable to publicly available distributional word vectors, we perform a singular value decomposition (SVD) on the linguistic matrix to obtain lower-dimensionality word vectors. If the L matrix with N word types and D linguistic characteristics is the L matrix, we can obtain U-RN-K from the SVD of L as follows: L = UB-V, where K is the desired length of the underdimensional space. We compare both sparse and dense linguistic vectors with three widely used distributional word vector models. The first two are the pre-trained skip-gram (Mikolov et al., 2013) 5 and the glove (Pennington et al., 2014) 6 word vectors of 300 billion and 6 billion words, respectively. We used latent setic analysis (LSA) to obtain word vectors from the SVD context of a word composition of 5 billion words and SVD."}, {"heading": "3.3 Results", "text": "Table 3 shows the performance of different word vector types in assessment tasks. Although Skip-Gram, Glove & LSA perform better than linguistic vectors in WS-353, the linguistic vectors in SimLex are significantly better than them. Linguistic vectors also perform better in RG-65. In mood analysis, linguistic vectors compete with Skip-Gram vectors, and in the NP allocation task, they outperform all distribution vectors by a statistically significant margin (p < 0.05, McNemars Test Dietterich (1998)). We append the sparse linguistic vectors to skip-gram vectors and rate the resulting vectors as shown in the bottom row of Table 3."}, {"heading": "4 Discussion", "text": "Linguistic resources such as WordNet have found extensive applications in lexical semantics, for example for word sense deentanglement, word similarity, etc. (Resnik, 1995; Agirre et al., 2009). Recently, there has been interest in the use of linguistic resources to enrich word vector representations. In these approaches, relational information between words from WordNet, Freebase, etc. is used as a constraint to encourage words with similar properties in lexical ontologies to have similar word vectors (Xu et al., 2014; Yu and Dredze, 2014; Bian et al., 2014; Fried and Duh, 2014; Faruqui et al., 2015a). Distributional representations are also improved by using experiential data in addition to the distribution context (Andrews et al., 2009). We have shown that simple vector concatenation can also be used to improve presentation strategies."}, {"heading": "5 Conclusion", "text": "We have presented a new method of constructing word vector representations using only linguistic knowledge from existing linguistic resources. These non-distributional, linguistic word vectors compete with current models of distributed word vectors, which are evaluated on the basis of a series of tasks. Language vectors are fully interpretable because each dimension is a linguistic feature and is therefore extremely sparse, making them computationally easy to work on."}, {"heading": "Acknowledgement", "text": "We thank Nathan Schneider for his comments on an earlier draft of this essay and the anonymous reviewers for their feedback."}], "references": [{"title": "A study on similarity and relatedness using distributional and wordnet-based approaches", "author": ["Agirre et al.2009] Eneko Agirre", "Enrique Alfonseca", "Keith Hall", "Jana Kravalova", "Marius Pa\u015fca", "Aitor Soroa"], "venue": "In Proc. of NAACL", "citeRegEx": "Agirre et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2009}, {"title": "Integrating experiential and distributional data to learn semantic representations", "author": ["Andrews et al.2009] Mark Andrews", "Gabriella Vigliocco", "David Vinson"], "venue": "Psychological review,", "citeRegEx": "Andrews et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Andrews et al\\.", "year": 2009}, {"title": "The berkeley framenet project", "author": ["Charles J. Fillmore", "John B. Lowe"], "venue": "In Proc. of ACL", "citeRegEx": "Baker et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Baker et al\\.", "year": 1998}, {"title": "Tailoring continuous word representations for dependency parsing", "author": ["Bansal et al.2014] Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": "In Proc. of ACL", "citeRegEx": "Bansal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "Knowledge-powered deep learning for word embedding", "author": ["Bian et al.2014] Jiang Bian", "Bin Gao", "Tie-Yan Liu"], "venue": "In Proc. of MLKDD", "citeRegEx": "Bian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bian et al\\.", "year": 2014}, {"title": "Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger", "author": ["Ciaramita", "Yasemin Altun"], "venue": "In Proc. of EMNLP", "citeRegEx": "Ciaramita et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ciaramita et al\\.", "year": 2006}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "author": ["Michael Collins"], "venue": "In Proc. of EMNLP", "citeRegEx": "Collins.,? \\Q2002\\E", "shortCiteRegEx": "Collins.", "year": 2002}, {"title": "Approximate statistical tests for comparing supervised classification learning algorithms", "author": ["Thomas G. Dietterich"], "venue": "Neural Computation", "citeRegEx": "Dietterich.,? \\Q1998\\E", "shortCiteRegEx": "Dietterich.", "year": 1998}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Jesse Dodge", "Sujay K. Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A. Smith"], "venue": "In Proc. of NAACL", "citeRegEx": "Faruqui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Sparse overcomplete word vector representations", "author": ["Yulia Tsvetkov", "Dani Yogatama", "Chris Dyer", "Noah A. Smith"], "venue": "In Proc. of ACL", "citeRegEx": "Faruqui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Connotation lexicon: A dash of sentiment beneath the surface meaning", "author": ["Feng et al.2013] Song Feng", "Jun Seok Kang", "Polina Kuznetsova", "Yejin Choi"], "venue": "In Proc. of ACL", "citeRegEx": "Feng et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2013}, {"title": "Lexicographic relevance: selecting information from corpus evidence", "author": ["Christopher Johnson", "Miriam Petruck"], "venue": "International Journal of Lexicography", "citeRegEx": "Fillmore et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Fillmore et al\\.", "year": 2003}, {"title": "Placing search in context: the concept revisited", "author": ["Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": "In Proc. of WWW", "citeRegEx": "Finkelstein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Incorporating both distributional and relational semantics in word representations. arXiv preprint arXiv:1412.4369", "author": ["Fried", "Duh2014] Daniel Fried", "Kevin Duh"], "venue": null, "citeRegEx": "Fried et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fried et al\\.", "year": 2014}, {"title": "Interpretable semantic vectors from a joint model of brain- and text- based meaning", "author": ["Fyshe et al.2014] Alona Fyshe", "Partha P. Talukdar", "Brian Murphy", "Tom M. Mitchell"], "venue": "In Proc. of ACL", "citeRegEx": "Fyshe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fyshe et al\\.", "year": 2014}, {"title": "A compositional and interpretable semantic space", "author": ["Fyshe et al.2015] Alona Fyshe", "Leila Wehbe", "Partha P. Talukdar", "Brian Murphy", "Tom M. Mitchell"], "venue": "In Proc. of NAACL", "citeRegEx": "Fyshe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fyshe et al\\.", "year": 2015}, {"title": "Revisiting embedding features for simple semi-supervised learning", "author": ["Jiang Guo", "Wanxiang Che", "Haifeng Wang", "Ting Liu"], "venue": "In Proc. of EMNLP", "citeRegEx": "Guo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Hill et al.2014] Felix Hill", "Roi Reichart", "Anna Korhonen"], "venue": null, "citeRegEx": "Hill et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "Fish transporters and miracle homes: How compositional distributional semantics can help NP parsing", "author": ["Eva Maria Vecchi", "Marco Baroni"], "venue": "In Proc. of EMNLP", "citeRegEx": "Lazaridou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2013}, {"title": "English verb classes and alternations : a preliminary investigation", "author": ["Beth Levin"], "venue": null, "citeRegEx": "Levin.,? \\Q1993\\E", "shortCiteRegEx": "Levin.", "year": 1993}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": null, "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Online learning of approximate dependency parsing algorithms", "author": ["McDonald", "Pereira2006] Ryan T McDonald", "Fernando CN Pereira"], "venue": "In Proc. of EACL", "citeRegEx": "McDonald et al\\.,? \\Q2006\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2006}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["George A Miller"], "venue": "Communications of the ACM", "citeRegEx": "Miller.,? \\Q1995\\E", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Crowdsourcing a wordemotion association lexicon", "author": ["Mohammad", "Turney2013] Saif M. Mohammad", "Peter D. Turney"], "venue": "Computational Intelligence,", "citeRegEx": "Mohammad et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mohammad et al\\.", "year": 2013}, {"title": "Colourful language: Measuring word-colour associations", "author": ["Saif Mohammad"], "venue": "In Proc. of the Workshop on Cognitive Modeling and Computational Linguistics", "citeRegEx": "Mohammad.,? \\Q2011\\E", "shortCiteRegEx": "Mohammad.", "year": 2011}, {"title": "Learning effective and interpretable semantic models using non-negative sparse embedding", "author": ["Murphy et al.2012] Brian Murphy", "Partha Talukdar", "Tom Mitchell"], "venue": "In Proc. of COLING", "citeRegEx": "Murphy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Murphy et al\\.", "year": 2012}, {"title": "Unsupervised allwords word sense disambiguation with grammatical dependencies", "author": ["Vivi Nastase"], "venue": "In Proc. of IJCNLP", "citeRegEx": "Nastase.,? \\Q2008\\E", "shortCiteRegEx": "Nastase.", "year": 2008}, {"title": "Algorithms for deterministic incremental dependency parsing", "author": ["Joakim Nivre"], "venue": "Computational Linguistics,", "citeRegEx": "Nivre.,? \\Q2008\\E", "shortCiteRegEx": "Nivre.", "year": 2008}, {"title": "Synonyms and Antonyms: An Alphabetical List of Words in Common Use, Grouped with Others of Similar and Opposite Meaning", "author": ["Edith Bertha Ordway"], "venue": "Sully and Kleinteich", "citeRegEx": "Ordway.,? \\Q1913\\E", "shortCiteRegEx": "Ordway.", "year": 1913}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In Proc. of EMNLP", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A maximum entropy model for part-of-speech tagging", "author": ["Adwait Ratnaparkhi"], "venue": "In Proc. of EMNLP", "citeRegEx": "Ratnaparkhi.,? \\Q1996\\E", "shortCiteRegEx": "Ratnaparkhi.", "year": 1996}, {"title": "Using information content to evaluate semantic similarity in a taxonomy", "author": ["Philip Resnik"], "venue": "In Proc. of IJCAI", "citeRegEx": "Resnik.,? \\Q1995\\E", "shortCiteRegEx": "Resnik.", "year": 1995}, {"title": "Contextual correlates of synonymy", "author": ["Rubenstein", "John B. Goodenough"], "venue": "Commun. ACM,", "citeRegEx": "Rubenstein et al\\.,? \\Q1965\\E", "shortCiteRegEx": "Rubenstein et al\\.", "year": 1965}, {"title": "Verbnet: A Broad-coverage", "author": ["Karin Kipper Schuler"], "venue": "Comprehensive Verb Lexicon. Ph.D. thesis,", "citeRegEx": "Schuler.,? \\Q2005\\E", "shortCiteRegEx": "Schuler.", "year": 2005}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts"], "venue": "In Proc. of EMNLP", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Augmenting english adjective senses with supersenses", "author": ["Nathan Schneider", "Dirk Hovy", "Archna Bhatia", "Manaal Faruqui", "Chris Dyer"], "venue": "In Proc. of LREC", "citeRegEx": "Tsvetkov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tsvetkov et al\\.", "year": 2014}, {"title": "From frequency to meaning : Vector space models of semantics", "author": ["Turney", "Pantel2010] Peter D. Turney", "Patrick Pantel"], "venue": null, "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}, {"title": "Mining the web for synonyms: Pmi-ir versus lsa on toefl", "author": ["Peter D. Turney"], "venue": "In Proc. of ECML", "citeRegEx": "Turney.,? \\Q2001\\E", "shortCiteRegEx": "Turney.", "year": 2001}, {"title": "Rc-net: A general framework for incorporating knowledge into word representations", "author": ["Xu et al.2014] Chang Xu", "Yalong Bai", "Jiang Bian", "Bin Gao", "Gang Wang", "Xiaoguang Liu", "Tie-Yan Liu"], "venue": "In Proc. of CIKM", "citeRegEx": "Xu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["Yu", "Dredze2014] Mo Yu", "Mark Dredze"], "venue": "In Proc. of ACL", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 18, "context": "Distributed representations of words have been shown to benefit a diverse set of NLP tasks including syntactic parsing (Lazaridou et al., 2013; Bansal et al., 2014), named entity recognition (Guo et al.", "startOffset": 119, "endOffset": 164}, {"referenceID": 3, "context": "Distributed representations of words have been shown to benefit a diverse set of NLP tasks including syntactic parsing (Lazaridou et al., 2013; Bansal et al., 2014), named entity recognition (Guo et al.", "startOffset": 119, "endOffset": 164}, {"referenceID": 16, "context": ", 2014), named entity recognition (Guo et al., 2014) and sentiment analysis (Socher et al.", "startOffset": 34, "endOffset": 52}, {"referenceID": 35, "context": ", 2014) and sentiment analysis (Socher et al., 2013).", "startOffset": 31, "endOffset": 52}, {"referenceID": 38, "context": "Intrinsic evaluations on various tasks are helping refine vector learning methods to discover representations that captures many facts about lexical semantics (Turney, 2001; Turney and Pantel, 2010).", "startOffset": 159, "endOffset": 198}, {"referenceID": 19, "context": "cal semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005; Miller, 1995).", "startOffset": 67, "endOffset": 129}, {"referenceID": 2, "context": "cal semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005; Miller, 1995).", "startOffset": 67, "endOffset": 129}, {"referenceID": 34, "context": "cal semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005; Miller, 1995).", "startOffset": 67, "endOffset": 129}, {"referenceID": 23, "context": "cal semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005; Miller, 1995).", "startOffset": 67, "endOffset": 129}, {"referenceID": 23, "context": "Our contribution to this discussion is a new technique that constructs task-independent word vector representations using linguistic knowledge derived from pre-constructed linguistic resources like WordNet (Miller, 1995), FrameNet (Baker et al.", "startOffset": 206, "endOffset": 220}, {"referenceID": 2, "context": "Our contribution to this discussion is a new technique that constructs task-independent word vector representations using linguistic knowledge derived from pre-constructed linguistic resources like WordNet (Miller, 1995), FrameNet (Baker et al., 1998), Penn Treebank (Marcus et al.", "startOffset": 231, "endOffset": 251}, {"referenceID": 20, "context": ", 1998), Penn Treebank (Marcus et al., 1993) etc.", "startOffset": 23, "endOffset": 44}, {"referenceID": 28, "context": ", parsing (McDonald and Pereira, 2006; Nivre, 2008) or part of speech tagging (Ratnaparkhi, 1996; Collins, 2002).", "startOffset": 10, "endOffset": 51}, {"referenceID": 31, "context": ", parsing (McDonald and Pereira, 2006; Nivre, 2008) or part of speech tagging (Ratnaparkhi, 1996; Collins, 2002).", "startOffset": 78, "endOffset": 112}, {"referenceID": 6, "context": ", parsing (McDonald and Pereira, 2006; Nivre, 2008) or part of speech tagging (Ratnaparkhi, 1996; Collins, 2002).", "startOffset": 78, "endOffset": 112}, {"referenceID": 23, "context": "WordNet (Miller, 1995) is an English lexical database that groups words into sets of synonyms called synsets and records a number of relations among these synsets or their members.", "startOffset": 8, "endOffset": 22}, {"referenceID": 27, "context": "WordNet partitions nouns and verbs into semantic field categories known as supsersenses (Ciaramita and Altun, 2006; Nastase, 2008).", "startOffset": 88, "endOffset": 130}, {"referenceID": 36, "context": "These supersenses were further extended to adjectives (Tsvetkov et al., 2014).", "startOffset": 54, "endOffset": 77}, {"referenceID": 2, "context": "FrameNet (Baker et al., 1998; Fillmore et al., 2003) is a rich linguistic resource that contains information about lexical", "startOffset": 9, "endOffset": 52}, {"referenceID": 11, "context": "FrameNet (Baker et al., 1998; Fillmore et al., 2003) is a rich linguistic resource that contains information about lexical", "startOffset": 9, "endOffset": 52}, {"referenceID": 25, "context": "Mohammad and Turney (2013) constructed two different lexicons that associate words to sentiment polarity and to emotions resp.", "startOffset": 0, "endOffset": 27}, {"referenceID": 10, "context": "Feng et al. (2013) construct a lexicon that contains information about connotation of words that are seemingly objective but often allude nuanced sentiment.", "startOffset": 0, "endOffset": 19}, {"referenceID": 10, "context": "Feng et al. (2013) construct a lexicon that contains information about connotation of words that are seemingly objective but often allude nuanced sentiment. They assign positive, negative and neutral connotations to these words. This lexicon differs from Mohammad and Turney (2013) in that it has a more subtle shade of sentiment and it extends to many more words.", "startOffset": 0, "endOffset": 282}, {"referenceID": 25, "context": "The word-color associtation lexicon produced by Mohammad (2011) using crowdsourcing lists the colors that a word evokes in English.", "startOffset": 48, "endOffset": 64}, {"referenceID": 20, "context": "The Penn Treebank (Marcus et al., 1993) annotates naturally occur-", "startOffset": 18, "endOffset": 39}, {"referenceID": 29, "context": "The antonymous words for a given word were collected from Ordway (1913).4 An example would be of impartiality, which has features ANTO.", "startOffset": 58, "endOffset": 72}, {"referenceID": 12, "context": "The first one is the widely used WS-353 dataset (Finkelstein et al., 2001),", "startOffset": 48, "endOffset": 74}, {"referenceID": 17, "context": "The third dataset is SimLex (Hill et al., 2014) which has been constructed to overcome the shortcomings of WS-353 and contains 999 pairs of adjectives, nouns and verbs.", "startOffset": 28, "endOffset": 47}, {"referenceID": 35, "context": "Socher et al. (2013) created a treebank containing sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts.", "startOffset": 0, "endOffset": 21}, {"referenceID": 20, "context": "(2013) constructed a dataset from the Penn TreeBank (Marcus et al., 1993) of noun phrases (NP) of", "startOffset": 52, "endOffset": 73}, {"referenceID": 18, "context": "Lazaridou et al. (2013) constructed a dataset from the Penn TreeBank (Marcus et al.", "startOffset": 0, "endOffset": 24}, {"referenceID": 22, "context": "The first two are the pre-trained Skip-Gram (Mikolov et al., 2013)5 and Glove (Pennington et al.", "startOffset": 44, "endOffset": 66}, {"referenceID": 30, "context": ", 2013)5 and Glove (Pennington et al., 2014)6 word vectors each of length 300, trained on 300 billion and 6 billion words respectively.", "startOffset": 19, "endOffset": 44}, {"referenceID": 7, "context": "05, McNemar\u2019s test Dietterich (1998)).", "startOffset": 19, "endOffset": 37}, {"referenceID": 32, "context": "(Resnik, 1995; Agirre et al., 2009).", "startOffset": 0, "endOffset": 35}, {"referenceID": 0, "context": "(Resnik, 1995; Agirre et al., 2009).", "startOffset": 0, "endOffset": 35}, {"referenceID": 39, "context": "is used as a constraint to encourage words with similar properties in lexical ontologies to have similar word vectors (Xu et al., 2014; Yu and Dredze, 2014; Bian et al., 2014; Fried and Duh, 2014; Faruqui et al., 2015a).", "startOffset": 118, "endOffset": 219}, {"referenceID": 4, "context": "is used as a constraint to encourage words with similar properties in lexical ontologies to have similar word vectors (Xu et al., 2014; Yu and Dredze, 2014; Bian et al., 2014; Fried and Duh, 2014; Faruqui et al., 2015a).", "startOffset": 118, "endOffset": 219}, {"referenceID": 1, "context": "tributional representations have also been shown to improve by using experiential data in addition to distributional context (Andrews et al., 2009).", "startOffset": 125, "endOffset": 147}, {"referenceID": 26, "context": "dimensions that are relatively more interpretable (Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015; Faruqui et al., 2015b).", "startOffset": 50, "endOffset": 134}, {"referenceID": 14, "context": "dimensions that are relatively more interpretable (Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015; Faruqui et al., 2015b).", "startOffset": 50, "endOffset": 134}, {"referenceID": 15, "context": "dimensions that are relatively more interpretable (Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015; Faruqui et al., 2015b).", "startOffset": 50, "endOffset": 134}], "year": 2015, "abstractText": "Data-driven representation learning for words is a technique of central importance in NLP. While indisputably useful as a source of features in downstream tasks, such vectors tend to consist of uninterpretable components whose relationship to the categories of traditional lexical semantic theories is tenuous at best. We present a method for constructing interpretable word vectors from hand-crafted linguistic resources like WordNet, FrameNet etc. These vectors are binary (i.e, contain only 0 and 1) and are 99.9% sparse. We analyze their performance on state-of-the-art evaluation methods for distributional models of word vectors and find they are competitive to standard distributional approaches.", "creator": "LaTeX with hyperref package"}}}