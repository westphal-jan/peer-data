{"id": "1506.05232", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2015", "title": "On the Depth of Deep Neural Networks: A Theoretical View", "abstract": "Deep neural networks (DNN) have achieved huge practical success in recent years. However, its theoretical properties (in particular generalization ability) are not yet very clear, since existing error bounds for neural networks cannot be directly used to explain the statistical behaviors of practically adopted DNN models (which are multi-class in their nature and may contain convolutional layers). To tackle the challenge, we derive a new margin bound for DNN in this paper, in which the expected 0-1 error of a DNN model is upper bounded by its empirical margin error plus a Rademacher Average based capacity term. This new bound is very general and is consistent with the empirical behaviors of DNN models observed in our experiments. According to the new bound, minimizing the empirical margin error can effectively improve the test performance of DNN. We therefore propose large margin DNN algorithms, which impose margin penalty terms to the cross entropy loss of DNN, so as to reduce the margin error during the training process. Experimental results show that the proposed algorithms can achieve significantly smaller empirical margin errors, as well as better test performances than the standard DNN algorithm.", "histories": [["v1", "Wed, 17 Jun 2015 07:51:42 GMT  (78kb)", "http://arxiv.org/abs/1506.05232v1", null], ["v2", "Sat, 28 Nov 2015 14:21:41 GMT  (182kb,D)", "http://arxiv.org/abs/1506.05232v2", "AAAI 2016"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shizhao sun", "wei chen", "liwei wang 0001", "xiaoguang liu", "tie-yan liu"], "accepted": true, "id": "1506.05232"}, "pdf": {"name": "1506.05232.pdf", "metadata": {"source": "CRF", "title": "Large Margin Deep Neural Networks: Theory and Algorithms", "authors": ["Shizhao Sun", "Wei Chen", "Liwei Wang"], "emails": ["sunshizhao@mail.nankai.edu.cn", "wche@microsoft.com", "wanglw@cis.pku.edu.cn", "tie-yan.liu@microsoft.com"], "sections": [{"heading": null, "text": "ar Xiv: 150 6.05 232v 1 [cs"}, {"heading": "1 Introduction", "text": "This year, the time has come for us to put ourselves at the top of the world in order to conquer the world."}, {"heading": "2 Preliminaries", "text": "In the face of a multi-level classification problem, X = Rd is designated as the input space, Y = {1, \u00b7 \u00b7 K} as the output space, and P as the common distribution over X \u00b7 Y. Here d denotes the dimension of the input space, and K denotes the number of categories in the output space. The goal is to learn a predictive model f: X \u00b7 Y \u2192 R from the education environment that generates an output vector (f (x, k) that indicates the probability of belonging to category k for each instance x \u00b7 Y. Then, the final classification is determined by argmaxk (x, k).The classification accuracy of the predictive model f is measured by its expected 0-1 error, i.e., errf (P)."}, {"heading": "3 Margin Bound for Multi-class Deep Neural Networks", "text": "In this section, we present our margin for multi-stage DNN, followed by its detection and empirical validation."}, {"heading": "3.1 Margin Bound", "text": "Our limit is based on the empirical margin error and the Rademacher average (RA) of DNN = 1. First of all, we give definitions of the empirical margin errors and RA values. Definition 1. Let us assume f-F: X-Y-R is a multi-stage predictive model. (7) Definition 2. [4] Suppose F: X-R is a model room with a one-dimensional output. The Rademacher average (RA) of F is defined as follows: Rm (F) = Ex-F-F2mm-1 = 1-F2mm-1, (8), where x-F-M-W-W values."}, {"heading": "3.2 Empirical Validation", "text": "In this section, we conduct experiments to investigate how weight restriction A and network depth L influence the test performance of DNN networks more deeply, and to validate the reasonableness of our margin limit, we conduct experiments on two sets of data, MNIST [19] and CIFAR-10 [17]. The MNIST datasets (for handwritten digit classification) consist of 28 x 28 black-and-white images, each with a digit from 0 to 9. There are 60k training examples and 10k test examples in this dataset. The CIFAR-10 datasets (for object recognition) consist of 32 x 32 RGB images, each containing an object, e.g. cat, dog or ship depth. There are 50k training examples and 10k test examples in this dataset. For eachdataset, we divide the 10k test examples into two subsets of the same size, one for validation and the other for validation."}, {"heading": "4 Large Margin Deep Neural Networks", "text": "Inspired by Theorem 1, we propose to refine the existing DNN algorithms by explicitly increasing the margin (and thus reducing the margin error) during the training process. To make it easier, we call the new algorithms Large Margin DNN. As we know, a widespread loss function for multi-class DNN is cross-entropy loss. As mentioned in the introduction, minimizing cross-entropy loss could not minimize the non-trivial margin error (with a margin coefficient that is not zero). Specifically, cross-entropy loss focuses on maximizing the model output for the true category, while the non-trivial margin error refers to whether the gap between model output for the true category and maximum output for the wrong categories is greater than the margin coefficient. In this sense, the non-trivial margin error can guide the training process in a finer granularity, even if the prediction 1 has already made a correction in the model."}, {"heading": "4.1 Algorithm Description", "text": "We propose to add two types of margin penalty terms to the original cross-entropy loss: the first penalty term refers to the margin and the second to an upper margin limit. Specifically, the corresponding new loss functions are defined as follows (for simplicity's sake, we call them C1 or C2): for model f, example x, y, C1 (f; x, y) = C (f; x, y) + \u03bb (1 \u2212 \u03c1 (f; x, y)))) 2, (18) C2 (f; x, y) = C (f; x, y) + \u03bbK \u2212 1 \u2211 k 6 = y (1 \u2212 (f (x, y) \u2212 f (x, k)) 2. (19) We call the algorithms that minimize the above new loss margin DNN algorithms (LMDNN) the standard DNN mini-system."}, {"heading": "4.2 Experimental Results", "text": "We compare the performance of LMDNNs with the size of 64, the dynamics as 0.9 and the efficient 1000 network. We use the well-coordinated neural network structures as given in the Caffe [14] tutorial (i.e., LeNet5 for MNIST and AlexNet6 for CIFAR-10), and adopt the same training and fine-tuning processes for all the algorithms studied. As for data preprocessing, we can set the pixel values in MNIST up to [0, 1] and subtract the average errors calculated per pixel via the training set in CIFAR-10. On both datasets, we do not use data augmentation for simplicity. For the training process, the weights are initialized randomly and updated by mini-batch SGD. We use the model in the last iteration as our final model."}, {"heading": "5 Conclusion and Future Work", "text": "In this work, we have derived a novel margin boundary for DNN that is very general and can cover the multi-class environment and Convolutionary Neural Networks. Based on the theory, we have proposed two large margin DNN algorithms that achieve significant increases in performance over the standard DNN algorithm. In the future, we plan to investigate how other factors affect DNN testing performance, such as cross-layer assignment of nodes, connection types, regulation tricks, etc. We will also work on designing effective algorithms that can further enhance DNN performance."}], "references": [{"title": "Do deep nets really need to be deep", "author": ["J. Ba", "R. Caruana"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network", "author": ["P.L. Bartlett"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Almost linear vc-dimension bounds for piecewise polynomial networks", "author": ["P.L. Bartlett", "V. Maiorov", "R. Meir"], "venue": "Neural computation,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["P.L. Bartlett", "S. Mendelson"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "On the complexity of neural network classifiers: A comparison between shallow and deep architectures", "author": ["M. Bianchini", "F. Scarselli"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Multi-column deep neural networks for image classification", "author": ["D. Ciresan", "U. Meier", "J. Schmidhuber"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Deep sparse rectifier networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&CP Volume,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Bounding the vapnik-chervonenkis dimension of concept classes parameterized by real numbers", "author": ["P.W. Goldberg", "M.R. Jerrum"], "venue": "Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1995}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G.E. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["K. Hornik", "M. Stinchcombe", "H. White"], "venue": "Neural networks,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1989}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Polynomial bounds for vc dimension of sigmoidal neural networks", "author": ["M. Karpinski", "A. Macintyre"], "venue": "In Proceedings of the twenty-seventh annual ACM symposium on Theory of computing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1995}, {"title": "Empirical margin distributions and bounding the generalization error of combined classifiers", "author": ["V. Koltchinskii", "D. Panchenko"], "venue": "Annals of Statistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Technical report, University of Toronto,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1998}, {"title": "Max-margin deep generative models", "author": ["C. Li", "J. Zhu", "T. Shi", "B. Zhang"], "venue": "arXiv preprint arXiv:1504.06787,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "On the number of linear regions of deep neural networks", "author": ["G.F. Montufar", "R. Pascanu", "K. Cho", "Y. Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Universal approximation using feedforward neural networks: A survey of some existing methods, and some new results", "author": ["F. Scarselli", "A.C. Tsoi"], "venue": "Neural networks,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Best practices for convolutional neural networks applied to visual document analysis", "author": ["P.Y. Simard", "D. Steinkraus", "J.C. Platt"], "venue": "In 12th International Conference on Document Analysis and Recognition,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Deep learning via semi-supervised embedding", "author": ["J. Weston", "F. Ratle", "H. Mobahi", "R. Collobert"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}], "referenceMentions": [{"referenceID": 6, "context": "1 Introduction Deep neural networks (DNN) have achieved great practical success in many machine learning tasks, such as speech recognition, image classification, and natural language processing [7, 10, 11, 18, 25].", "startOffset": 194, "endOffset": 213}, {"referenceID": 9, "context": "1 Introduction Deep neural networks (DNN) have achieved great practical success in many machine learning tasks, such as speech recognition, image classification, and natural language processing [7, 10, 11, 18, 25].", "startOffset": 194, "endOffset": 213}, {"referenceID": 10, "context": "1 Introduction Deep neural networks (DNN) have achieved great practical success in many machine learning tasks, such as speech recognition, image classification, and natural language processing [7, 10, 11, 18, 25].", "startOffset": 194, "endOffset": 213}, {"referenceID": 17, "context": "1 Introduction Deep neural networks (DNN) have achieved great practical success in many machine learning tasks, such as speech recognition, image classification, and natural language processing [7, 10, 11, 18, 25].", "startOffset": 194, "endOffset": 213}, {"referenceID": 24, "context": "1 Introduction Deep neural networks (DNN) have achieved great practical success in many machine learning tasks, such as speech recognition, image classification, and natural language processing [7, 10, 11, 18, 25].", "startOffset": 194, "endOffset": 213}, {"referenceID": 2, "context": "For example, in [3, 9, 15], error bounds for neural networks were derived based on Vapnik-Chervonenkis (VC) dimension.", "startOffset": 16, "endOffset": 26}, {"referenceID": 8, "context": "For example, in [3, 9, 15], error bounds for neural networks were derived based on Vapnik-Chervonenkis (VC) dimension.", "startOffset": 16, "endOffset": 26}, {"referenceID": 14, "context": "For example, in [3, 9, 15], error bounds for neural networks were derived based on Vapnik-Chervonenkis (VC) dimension.", "startOffset": 16, "endOffset": 26}, {"referenceID": 1, "context": "In [2, 16], a margin bound was given to fully connected neural networks in the setting of binary classification.", "startOffset": 3, "endOffset": 10}, {"referenceID": 15, "context": "In [2, 16], a margin bound was given to fully connected neural networks in the setting of binary classification.", "startOffset": 3, "endOffset": 10}, {"referenceID": 12, "context": "For example, in [13, 22] it is shown that neural networks with at least one hidden layer are universal approximators of any continuous function, and in [5, 6, 21], it is demonstrated that deeper neural networks can compute more complex functions than shallower neural networks.", "startOffset": 16, "endOffset": 24}, {"referenceID": 21, "context": "For example, in [13, 22] it is shown that neural networks with at least one hidden layer are universal approximators of any continuous function, and in [5, 6, 21], it is demonstrated that deeper neural networks can compute more complex functions than shallower neural networks.", "startOffset": 16, "endOffset": 24}, {"referenceID": 4, "context": "For example, in [13, 22] it is shown that neural networks with at least one hidden layer are universal approximators of any continuous function, and in [5, 6, 21], it is demonstrated that deeper neural networks can compute more complex functions than shallower neural networks.", "startOffset": 152, "endOffset": 162}, {"referenceID": 5, "context": "For example, in [13, 22] it is shown that neural networks with at least one hidden layer are universal approximators of any continuous function, and in [5, 6, 21], it is demonstrated that deeper neural networks can compute more complex functions than shallower neural networks.", "startOffset": 152, "endOffset": 162}, {"referenceID": 20, "context": "For example, in [13, 22] it is shown that neural networks with at least one hidden layer are universal approximators of any continuous function, and in [5, 6, 21], it is demonstrated that deeper neural networks can compute more complex functions than shallower neural networks.", "startOffset": 152, "endOffset": 162}, {"referenceID": 18, "context": "Third, in many real tasks, convolutional neural networks (CNN) [19] are widely used and proven to be very effective [7, 18, 24], however, most existing bounds are derived for fully connected neural networks only.", "startOffset": 63, "endOffset": 67}, {"referenceID": 6, "context": "Third, in many real tasks, convolutional neural networks (CNN) [19] are widely used and proven to be very effective [7, 18, 24], however, most existing bounds are derived for fully connected neural networks only.", "startOffset": 116, "endOffset": 127}, {"referenceID": 17, "context": "Third, in many real tasks, convolutional neural networks (CNN) [19] are widely used and proven to be very effective [7, 18, 24], however, most existing bounds are derived for fully connected neural networks only.", "startOffset": 116, "endOffset": 127}, {"referenceID": 23, "context": "Third, in many real tasks, convolutional neural networks (CNN) [19] are widely used and proven to be very effective [7, 18, 24], however, most existing bounds are derived for fully connected neural networks only.", "startOffset": 116, "endOffset": 127}, {"referenceID": 19, "context": "One related work is [20], which combines the generative deep learning methods (e.", "startOffset": 20, "endOffset": 24}, {"referenceID": 3, "context": "[4] Suppose F : X \u2192 R is a model space with a single dimensional output.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "In order to prove the theorem, we leverage the following margin bound for general multiclass prediction models (see Theorem 11 in [16]), errP (f) \u2264 inf \u03b3\u2208(0,1] {", "startOffset": 130, "endOffset": 134}, {"referenceID": 3, "context": "(15) According to [4], Rm(F\u0304 A) can be bounded by: Rm(F\u0304 1 A) \u2264 cAM \u221a", "startOffset": 18, "endOffset": 21}, {"referenceID": 1, "context": "(3) As compared to previous margin bound for neural networks ([2, 16]), our margin bound can not only cover binary classification and fully connected DNN, but also multi-class classification and convolutional neural networks (CNN).", "startOffset": 62, "endOffset": 69}, {"referenceID": 15, "context": "(3) As compared to previous margin bound for neural networks ([2, 16]), our margin bound can not only cover binary classification and fully connected DNN, but also multi-class classification and convolutional neural networks (CNN).", "startOffset": 62, "endOffset": 69}, {"referenceID": 18, "context": "We conduct experiments on two datasets, MNIST [19] and CIFAR-10 [17].", "startOffset": 46, "endOffset": 50}, {"referenceID": 16, "context": "We conduct experiments on two datasets, MNIST [19] and CIFAR-10 [17].", "startOffset": 64, "endOffset": 68}, {"referenceID": 7, "context": "Following the practices in [8, 23], we set the numbers of weights of the DNN model for MNIST and CIFAR10 as 0.", "startOffset": 27, "endOffset": 34}, {"referenceID": 22, "context": "Following the practices in [8, 23], we set the numbers of weights of the DNN model for MNIST and CIFAR10 as 0.", "startOffset": 27, "endOffset": 34}, {"referenceID": 0, "context": "For simplicity and also following many previous works [1, 8, 12, 23], we assume that each hidden layer has the same number of nodes in the experiment.", "startOffset": 54, "endOffset": 68}, {"referenceID": 7, "context": "For simplicity and also following many previous works [1, 8, 12, 23], we assume that each hidden layer has the same number of nodes in the experiment.", "startOffset": 54, "endOffset": 68}, {"referenceID": 11, "context": "For simplicity and also following many previous works [1, 8, 12, 23], we assume that each hidden layer has the same number of nodes in the experiment.", "startOffset": 54, "endOffset": 68}, {"referenceID": 22, "context": "For simplicity and also following many previous works [1, 8, 12, 23], we assume that each hidden layer has the same number of nodes in the experiment.", "startOffset": 54, "endOffset": 68}, {"referenceID": 13, "context": "We use the well-tuned neural network structures as given in the Caffe [14] tutorial (i.", "startOffset": 70, "endOffset": 74}, {"referenceID": 0, "context": "As for data pre-processing, we scale the pixel values in MNIST to [0, 1], and subtract the per-pixel mean computed over the training set from each image in CIFAR-10.", "startOffset": 66, "endOffset": 72}], "year": 2017, "abstractText": "Deep neural networks (DNN) have achieved huge practical success in recent years. However, its theoretical properties (in particular generalization ability) are not yet very clear, since existing error bounds for neural networks cannot be directly used to explain the statistical behaviors of practically adopted DNN models (which are multi-class in their nature and may contain convolutional layers). To tackle the challenge, we derive a new margin bound for DNN in this paper, in which the expected 0-1 error of a DNN model is upper bounded by its empirical margin error plus a Rademacher Average based capacity term. This new bound is very general and is consistent with the empirical behaviors of DNN models observed in our experiments. According to the new bound, minimizing the empirical margin error can effectively improve the test performance of DNN. We therefore propose large margin DNN algorithms, which impose margin penalty terms to the cross entropy loss of DNN, so as to reduce the margin error during the training process. Experimental results show that the proposed algorithms can achieve significantly smaller empirical margin errors, as well as better test performances than the standard DNN algorithm.", "creator": "LaTeX with hyperref package"}}}