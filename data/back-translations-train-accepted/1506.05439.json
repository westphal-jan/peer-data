{"id": "1506.05439", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2015", "title": "Learning with a Wasserstein Loss", "abstract": "Learning to predict multi-label outputs is challenging, but in many problems there is a natural metric on the outputs that can be used to improve predictions. In this paper we develop a loss function for multi-label learning, based on the Wasserstein distance. The Wasserstein distance provides a natural notion of dissimilarity for probability measures. Although optimizing with respect to the exact Wasserstein distance is costly, recent work has described a regularized approximation that is efficiently computed. We describe efficient learning algorithms based on this regularization, extending the Wasserstein loss from probability measures to unnormalized measures. We also describe a statistical learning bound for the loss and show connections with the total variation norm and the Jaccard index. The Wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space. We demonstrate this property on a real-data tag prediction problem, using the Yahoo Flickr Creative Commons dataset, achieving superior performance over a baseline that doesn't use the metric.", "histories": [["v1", "Wed, 17 Jun 2015 19:36:41 GMT  (2371kb,D)", "http://arxiv.org/abs/1506.05439v1", null], ["v2", "Fri, 6 Nov 2015 03:46:05 GMT  (2370kb,D)", "http://arxiv.org/abs/1506.05439v2", "NIPS 2015"], ["v3", "Wed, 30 Dec 2015 01:08:11 GMT  (2376kb,D)", "http://arxiv.org/abs/1506.05439v3", "NIPS 2015; v3 updates Algorithm 1 and Equations 6, 8"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV stat.ML", "authors": ["charlie frogner", "chiyuan zhang", "hossein mobahi", "mauricio araya-polo", "tomaso a poggio"], "accepted": true, "id": "1506.05439"}, "pdf": {"name": "1506.05439.pdf", "metadata": {"source": "CRF", "title": "Learning with a Wasserstein Loss", "authors": ["Charlie Frogner", "Chiyuan Zhang", "Hossein Mobahi", "Mauricio Araya-Polo"], "emails": ["frogner@mit.edu,", "chiyuan@mit.edu,", "tp@ai.mit.edu", "hmobahi@csail.mit.edu", "Mauricio.Araya@shell.com"], "sections": [{"heading": null, "text": "In this paper, we develop a loss function for multi-level learning based on the Waterstone distance. Waterstone distance provides a natural notion of dissimilarity in probability measurements. Although optimizing for the exact Waterstone distance is costly, recent work has described a regulated approach that is efficiently calculated. We describe efficient learning algorithms based on this regulation and extend the Waterstone loss of probability measurements to non-normalized metrics. We also describe a statistical learning problem tied to loss, and show relationships with the overall variation standard and the Jaccard index. Waterstone loss can enhance the smoothness of predictions in relation to a selected metric in the output range. We demonstrate this characteristic in a problem of predicting real data by using the Yahoo Flickr Creative Commons dataset that achieves superior performance over a baseline that does not use metrics."}, {"heading": "1 Introduction", "text": "This year, it will be able to fix and fix the mentioned bugs."}, {"heading": "2 Related work", "text": "are very popular for probabilistic [1] or vector-weighted [5] predictions, as each component can be evaluated independently, often resulting in simple and efficient algorithms. The idea of exploiting smoothness in the label space according to an earlier metric has been explored in many different forms, including regulation [6] and post-processing with graphic models [7]. Optimal transport provides a natural distance for probability distributions across metric spaces. In [2, 8] optimal transport is used to formulate the hydrocalculus barycenter as a probability distribution with minimal hydrocalculus distance to a set of predetermined points on the probability simplex. [9] propagates histogram values on a graph by minimizing a dirichlet energy induced by optimal transport. The hydrocalculus distance is also used to reproduce a metric for the comparison of our cluster image [10], as well as for this problem [11]."}, {"heading": "3 Learning with a Wasserstein loss", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Problem setup and notation", "text": "Consider the problem of learning a map of X-RDX to the space Y-RK + of the measured quantities over a finite quantity K of size | K-K-K. Suppose that K is a subset of a metric space with metric dK (\u00b7, \u00b7). dK is referred to as soil metrics and measures the semantic similarity in the label space. We conduct the learning over a hypotheses space H of the predictors h\u03b8: X-Y, parameterized by \u03b8. In the standard statistical learning environment, we obtain an i.i.d. sequence of training examples S = (((x1, y1),.., (xN, yN))), sampled from an unknown joint distribution PX-Y. For a measure of performance (a.k.a. risk) E (\u00b7, \u00b7 \u00b7) the goal is to find the predictor h\u03b8-H, which minimizes the expected risk E [E (hTB (x), y)."}, {"heading": "3.2 Optimal transport and the exact Wasserstein loss", "text": "Information divergence-based loss functions are widely used in learning with probability-weighted results. However, along with other popular metrics such as Hellinger distance and \u03c72 distance, these divergences are invariant to the permutation of elements in K and ignore any metric structure on K. Given a cost function c: K \u00b7 K \u2192 R measures the optimal transport distance [15] the cheapest way to transport a probability measurement that \u00b51 with \u00b52 relative to c: Wc (\u00b51, \u00b52) = inf \u03b3 (\u00b51, \u00b52) = inf. An important case is when the cost is determined by a metric dK (\u00b7 pp) or its p-te performance dpK (\u00b7, \u00b52), with the value of the common probability measurement set to K \u00b7 K \u00b7 K with \u00b51 and \u00b52 as margins. An important case is when the cost is specified by a metric dK (\u00b7 p)."}, {"heading": "4 Efficient optimization", "text": "Waterstone loss (2) is a linear program, and Lagrange's duality provides a means of calculating the parentage direction in relation to h (x).The dual LP of (2) isdW pp (h (x), y) = sup \u03b1, \u03b2, CM \u03b1 > h (x) + \u03b2 > y, CM = {(\u03b1, \u03b2), RK \u00b7 K: \u03b1\u0432 + \u03b2\u043a \"\u2264 M\u0443, \u0432\"}. (4) Since (2) is a linear program, the values of the dual and the primary are optimally equal (see e.g. [17]), so the dual optimal \u03b1 is a subgradient of the loss in relation to its first argument. Calculation of \u03b1 is costly, since it requires the solution of a linear program with O (K2) contracts, where K is the dimension of the production space. These costs may be prohibitive in optimization by gradient descent."}, {"heading": "4.1 Entropic regularization of optimal transport", "text": "Cuturi [18] proposes a smoothed transport object that enables an efficient approximation of both the transport matrix in (2) and the loss subgradients. [18] introduces an entropic regularization concept that leads to a strictly convex problem: \u03bbW pp (h (\u00b7 | x), y (\u00b7) = inf T (h (x), y) < T, M > + \u03bbH (T), H (T) = \u2212 \u2211, \u0432 \u2032 T\u0432, \u0432, \u0445 log T\u0432, \u0432. \"(5) It is important that the transport matrix that solves (5) is a diagonal scaling of a matrix K = e \u2212 \u03bbM \u2212 1: T \u0445 = diag (u) Kdiag (v) (6) for u = e\u03b2\u03b1 and v = e\u03b2\u03b2, where \u03b1 and \u03b2 are the storage dual variables for (5)."}, {"heading": "4.2 Extending smoothed transport to the learning setting", "text": "If the output vectors h (x) and y are in the simplex, (5) can be used directly as a substitute for (2). In this case, \u03b1 is a subgradient of the objective and can be derived from the optimal scaling vector u as \u03b1 = 1\u03bb log u. Note that there is a translation ambiguity here: Any upscaling of the vector u can be paired with a corresponding downscaling of the vector v without changing the matrix T. This means that \u03b1 is defined only up to a constant shift. In [2], the authors recommend selecting \u03b1 = 1\u03bb log u \u2212 1 K\u03bb log u > 1 so that \u03b1 is related to Simplex. However, in many learning problems, a normalized output assumption is unnatural. In image segmentation, for example, the target form is not represented in a natural way as a histogram, and even if the prediction and the fundamental truth is limited to the simplex, the measurement effectiveness lies between the observed characteristics being lower than the actual."}, {"heading": "4.3 Relaxed transport", "text": "We propose a novel relaxation that extends smoothed transport to non-normalized measures. By replacing the equal transport margin constraints in (5) with soft penalties in relation to KL divergence, we obtain an unrestricted, approximate transport problem. The resulting goal is: \u03bb, \u03b3a, \u03b3bWKL (h (\u00b7 x), y (\u00b7) = min T-RK \u00b7 K + < T, M > + \u03bbH (T) + \u03b3aK \u00b2 L (T1))) + \u03b3bK \u00b2 L (T > 1) (7), where K \u00b2 L (w \u00b2 z) = w > log (w) \u2212 1 > z is the generalized KL divergence between w, z \u00b2 RK +. Here, an elementary division represents. As with the previous formulation, the optimal transport situation in relation to (7) is the optimal matrix."}, {"heading": "5 Properties of the Wasserstein loss", "text": "In this section we examine the statistical properties of learning with the exact Waterstone loss (2) as well as correlations with two standard yardsticks. Complete proofs can be found in the appendix."}, {"heading": "5.1 Generalization error", "text": "Let S = (x1, y1),., (xN, yN), (xN, yN), become i.i.d. samples and h\u03b8, (x1, y1),., (xN, yN), become i.i.d. samples and h\u03b8, (x1, yi), minimize empirical risk, (x1, yi). Go on assuming that H = s, and any other solution > 0, with a probability of at least 1 \u2212 Celsius and a base hyposity space Ho of the functional mapping in RK. The Softmax layer gives a prediction that lies in the Simplex-K theory. Theorem 5.1, and any other solution > 0, with a probability of at least 1 \u2212 Celsius, it holds thatE [W 11 (horizon, yx), y, y), and the Softmax layer gives a prediction that lies in the Simplex-K theory, with a probability of at least 1 \u2212 Celsius, each functional space > 5.o and a functional space > 1 and a \u2212 shield of at least one."}, {"heading": "5.2 Connection with other standard measures", "text": "The special case, in which no previous similarity between the points is assumed, is captured by the 0-1 soil metric, which is measured by M0 \u2212 1\u0445, \u0432 \"= 1\u0445 6 = \u043c.\" In this case, it is known that the Waterstone distance is reduced to the total variable distance TV (\u00b7, \u00b7): Proposition 5.3. For the 0-1 soil metric, the probability is measured in the segmentation [23]. For two regions A and B in the image plane, the Waterstone loss is also closely related to the Jaccard index [22], also known as the intersection union (IoU), which is a popular measure of performance in segmentation [23]."}, {"heading": "6 Empirical study", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Impact of the ground metric", "text": "In this section, we show that the Waterstone loss promotes smoothness in relation to an artificial metric on the handwritten data set of the MNIST digit, which is a multi-level classification problem with output dimensions corresponding to the 10 digits, and we apply a grounded metric that is numerically close to the actual number. We independently build a model for each p value and plot the average predicted probabilities of the different digits on the test set in Figure 4. Note that the metric approaches as p \u2192 0 of the 0 \u2212 1 digit d0. In this case, as shown in Figure 4. Note, the metric approaches the 0 \u2212 1 digit d0 = 1kp = 1kp, which treats all false digits as equally unfavourable."}, {"heading": "6.2 Flickr tag prediction", "text": "We apply the Waterstone loss to a real multi-label learning problem by using the recently released Yahoo / Flickr Creative Commons 100M dataset [24]. Our goal is tag prediction: we select 1000 descriptive tags along with two random sets of 10,000 images each associated with these tags for training and testing. We derive a distance metric between tags by using word2vec [25] to embed the tags as unit vectors, and then take their euclidean distances. To extract image characteristics, we use MatConvNet [26]. Note that the set of tags is highly redundant and can often make many semantically equivalent or similar tags applicable to an image. The images are also incompletely tagged, as different users may prefer different tags. We measure the prediction performance by the top K costs defined as Waterstone = 1 K-K (K = 1 K-K)."}, {"heading": "7 Conclusions and future work", "text": "In this paper, we have described a loss function that allows us to learn to predict a measurement over a finite quantity based on the Waterstone distance. Optimizing for the exact Waterstone loss is mathematically costly, and we describe efficient algorithms based on entropic regularization to learn both normalized and unnormalized measurements. We have also described a statistical learning limit for loss and demonstrated relationships with both the overall variation norm and the Jaccard index. Waterstone loss can enhance the smoothness of predictions in relation to a selected metric on the production area, and we demonstrate this property on a problem with real data-day predictions by achieving superior performance over a baseline that does not take the measurement into account. An interesting direction for future work could be to investigate the relationship between Waterstone loss and Markov random fields, as the latter are often used to promote the prediction of the pre-time."}, {"heading": "A Relaxed transport", "text": "The equation (7) gives the relaxed traffic target as \u03bb, \u03b3a, \u03b3bWKL (h (\u00b7 | x), y (\u00b7) = minT \u0440RK \u00b7 K + \u03b3lt; T, M > + \u03bbH (T) + \u03b3aK \u0394L (T1 \u0394h (x))) + \u03b3bK \u0394L (T > 1 \u0441y) with K \u0432 L (w \u00b2 z) = w > log (w > z) \u2212 1 > w + 1 > z.Proof of Proposition 4.1. The first order condition for the optimization of T \u00b2 (7) is Mij + 1\u03bb (log T \u00b2 ij + 1) + \u03b3a (log T \u00b2) 1 h (x)) i + \u03b3b (log (T \u00b2) > 1 \u00b2 aag) j = 0."}, {"heading": "B Statistical Learning Bounds", "text": "For simpler notation, for a sequence S = (x1, y1),.., (xN, yN) of the i.i.d. training samples, we designate the empirical risk R-S and the risk R asR-S (h\u03b8) = E-S [W pp (\u2212 x), y-S (\u00b7))], R-S-S (\u00b7 x), y-S (\u00b7 x))] (10) Lemma B.1. Let-S-S-S [W pp-S-S], h-S-S-S-S."}, {"heading": "C Connection with other standard measures", "text": "C1 Connection with multiclass classificationProof of Proposition 5,2. Given that the label is a \"uniform\" vector (= \"a hot\" vector y = \"a hot vector\" = \"a hot vector\" = \"a hot vector\" = \"a hot vector\" = \"a hot vector\" = \"a hot vector.\" Furthermore, the restriction contains T1 = \"a workable traffic plan,\" so that (2) the \"fourth column of T is actually equal to\" (\u00b7 x). In other words, the defined \"third column of T\" (\u00b7 x) contains only one workable traffic plan, so that (2) the \"fourth column of T\" is actually equal to \"(\u00b7 x),\" K \"is a minimum.\" \""}, {"heading": "D Empirical study", "text": "It is not only the way in which people in the USA, but also the way in which they live in the USA, in Europe, in the USA, in Europe, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA,"}], "references": [{"title": "Fully convolutional networks for semantic segmentation", "author": ["Jonathan Long", "Evan Shelhamer", "Trevor Darrell"], "venue": "CVPR (to appear),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Fast Computation of Wasserstein Barycenters", "author": ["Marco Cuturi", "Arnaud Doucet"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Wasserstein Propagation for Semi-Supervised Learning", "author": ["Justin Solomon", "Raif M Rustamov", "Leonidas J Guibas", "Adrian Butscher"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Kernels for vector-valued functions: A review", "author": ["Neil D. Lawrence"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Nonlinear total variation based noise removal algorithms", "author": ["Leonid I Rudin", "Stanley Osher", "Emad Fatemi"], "venue": "Physica D: Nonlinear Phenomena,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1992}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "author": ["Liang-Chieh Chen", "George Papandreou", "Iasonas Kokkinos", "Kevin Murphy", "Alan L Yuille"], "venue": "In ICLR,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "A Smoothed Dual Approach for Variational Wasserstein Problems", "author": ["Marco Cuturi", "Gabriel Peyr\u00e9", "Antoine Rolet"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Wasserstein propagation for semi-supervised learning", "author": ["Justin Solomon", "Raif Rustamov", "Leonidas Guibas", "Adrian Butscher"], "venue": "In ICML,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Comparing clusterings in space", "author": ["Michael Coen", "Hidayath Ansari", "Nathanael Fillmore"], "venue": "In ICML,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "The earth mover\u2019s distance as a metric for image retrieval", "author": ["Yossi Rubner", "Carlo Tomasi", "Leonidas J Guibas"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "Fast contour matching using approximate earth mover\u2019s distance", "author": ["Kristen Grauman", "Trevor Darrell"], "venue": "In CVPR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Approximate earth mover\u2019s distance in linear time", "author": ["S Shirdhonkar", "D W Jacobs"], "venue": "In CVPR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "On minimum kantorovich distance estimators", "author": ["Federico Bassetti", "Antonella Bodini", "Eugenio Regazzini"], "venue": "Stat. Probab. Lett., 76(12):1298\u20131302,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Optimal Transport: Old and New", "author": ["C\u00e9dric Villani"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "The Monge-Kantorovich problem: achievements, connections, and perspectives", "author": ["Vladimir I Bogachev", "Aleksandr V Kolesnikov"], "venue": "Russian Math. Surveys, 67(5):785,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Introduction to Linear Optimization", "author": ["Dimitris Bertsimas", "John N. Tsitsiklis", "John Tsitsiklis"], "venue": "Athena Scientific, Boston, third printing edition,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1997}, {"title": "Sinkhorn Distances: Lightspeed Computation of Optimal Transport", "author": ["Marco Cuturi"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "A fast algorithm for matrix balancing", "author": ["Philip A Knight", "Daniel Ruiz"], "venue": "IMA Journal of Numerical Analysis,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Fast and robust Earth Mover\u2019s Distances", "author": ["Ofir Pele", "Michael Werman"], "venue": "ICCV, pages 460\u2013467,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["Peter L Bartlett", "Shahar Mendelson"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2003}, {"title": "Optimal Decisions from Probabilistic Models: The Intersection-over-Union Case", "author": ["Sebastian Nowozin"], "venue": "CVPR, pages 548\u2013555,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "The new data and new challenges in multimedia research", "author": ["Bart Thomee", "David A. Shamma", "Gerald Friedland", "Benjamin Elizalde", "Karl Ni", "Douglas Poland", "Damian Borth", "Li-Jia Li"], "venue": "arXiv preprint arXiv:1503.01817,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In NIPS,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "MatConvNet \u2013 Convolutional Neural Networks for MATLAB", "author": ["A. Vedaldi", "K. Lenc"], "venue": "CoRR, abs/1412.4564,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Probability in Banach Spaces: Isoperimetry and Processes. Classics in Mathematics", "author": ["M. Ledoux", "M. Talagrand"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Another example is semantic segmentation [1], where the set consists of the pixel locations, and a segment can be modeled as a uniform measure supported on a subset.", "startOffset": 41, "endOffset": 44}, {"referenceID": 1, "context": "The Wasserstein distance is defined as the cost of the optimal transport plan for moving the mass in the predicted measure to match that in the target, and has been applied to a wide range of problems, including barycenter estimation [2], label propagation [3], and clustering [4].", "startOffset": 234, "endOffset": 237}, {"referenceID": 2, "context": "The Wasserstein distance is defined as the cost of the optimal transport plan for moving the mass in the predicted measure to match that in the target, and has been applied to a wide range of problems, including barycenter estimation [2], label propagation [3], and clustering [4].", "startOffset": 257, "endOffset": 260}, {"referenceID": 0, "context": "2 Related work Decomposable loss functions like KL Divergence and `p distances are very popular for probabilistic [1] or vector-valued [5] predictions, as each component can be evaluated independently, often leading to simple and efficient algorithms.", "startOffset": 114, "endOffset": 117}, {"referenceID": 3, "context": "2 Related work Decomposable loss functions like KL Divergence and `p distances are very popular for probabilistic [1] or vector-valued [5] predictions, as each component can be evaluated independently, often leading to simple and efficient algorithms.", "startOffset": 135, "endOffset": 138}, {"referenceID": 4, "context": "The idea of exploiting smoothness in the label space according to a prior metric has been explored in many different forms, including regularization [6] and post-processing with graphical models [7].", "startOffset": 149, "endOffset": 152}, {"referenceID": 5, "context": "The idea of exploiting smoothness in the label space according to a prior metric has been explored in many different forms, including regularization [6] and post-processing with graphical models [7].", "startOffset": 195, "endOffset": 198}, {"referenceID": 1, "context": "In [2, 8], the optimal transport is used to formulate the Wasserstein Barycenter as a probability distribution with minimum Wasserstein distance to a set of", "startOffset": 3, "endOffset": 9}, {"referenceID": 6, "context": "In [2, 8], the optimal transport is used to formulate the Wasserstein Barycenter as a probability distribution with minimum Wasserstein distance to a set of", "startOffset": 3, "endOffset": 9}, {"referenceID": 7, "context": "[9] propagates histogram values on a graph by minimizing a Dirichlet energy induced by the optimal transport.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "The Wasserstein distance is also used to formulate a metric for comparing clusters in [10], as well as applied for image retrieval [11], contour matching [12], and many other problems that can be formulated as histogram matching [13].", "startOffset": 86, "endOffset": 90}, {"referenceID": 9, "context": "The Wasserstein distance is also used to formulate a metric for comparing clusters in [10], as well as applied for image retrieval [11], contour matching [12], and many other problems that can be formulated as histogram matching [13].", "startOffset": 131, "endOffset": 135}, {"referenceID": 10, "context": "The Wasserstein distance is also used to formulate a metric for comparing clusters in [10], as well as applied for image retrieval [11], contour matching [12], and many other problems that can be formulated as histogram matching [13].", "startOffset": 154, "endOffset": 158}, {"referenceID": 11, "context": "The Wasserstein distance is also used to formulate a metric for comparing clusters in [10], as well as applied for image retrieval [11], contour matching [12], and many other problems that can be formulated as histogram matching [13].", "startOffset": 229, "endOffset": 233}, {"referenceID": 12, "context": "The closest work to this paper is a theoretical study [14] of an estimator that minimizes the optimal transport cost between the empirical distribution and the estimated distribution in the setting of statistical parameter estimation.", "startOffset": 54, "endOffset": 58}, {"referenceID": 13, "context": "Given a cost function c : K \u00d7 K \u2192 R, the optimal transport distance [15] measures the cheapest way to transport a probability measure \u03bc1 to match \u03bc2 with respect to c: Wc(\u03bc1, \u03bc2) = inf \u03b3\u2208\u03a0(\u03bc1,\u03bc2) \u222b", "startOffset": 68, "endOffset": 72}, {"referenceID": 14, "context": "In this case, they are called Wasserstein distances [16], also known as the earth mover\u2019s distances [11].", "startOffset": 52, "endOffset": 56}, {"referenceID": 9, "context": "In this case, they are called Wasserstein distances [16], also known as the earth mover\u2019s distances [11].", "startOffset": 100, "endOffset": 104}, {"referenceID": 15, "context": "[17]), hence the dual optimal \u03b1 is a subgradient of the loss with respect to its first argument.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "1 Entropic regularization of optimal transport Cuturi [18] proposes a smoothed transport objective that enables efficient approximation of both the transport matrix in (2) and the subgradient of the loss.", "startOffset": 54, "endOffset": 58}, {"referenceID": 16, "context": "[18] introduces an entropic regularization term that results in a strictly convex problem: W p p (h(\u00b7|x), y(\u00b7)) = inf T\u2208\u03a0(h(x),y) \u3008T,M\u3009+ \u03bbH(T ), H(T ) = \u2212 \u2211", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Identifying such a matrix subject to equality constraints on the row and column sums is exactly a matrix balancing problem, which is well-studied in numerical linear algebra and for which efficient iterative algorithms exist [19].", "startOffset": 225, "endOffset": 229}, {"referenceID": 16, "context": "[18] and [2] use the well-known Sinkhorn-Knopp algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[18] and [2] use the well-known Sinkhorn-Knopp algorithm.", "startOffset": 9, "endOffset": 12}, {"referenceID": 1, "context": "In [2] the authors recommend choosing \u03b1 = 1 \u03bb log u\u2212 1 K\u03bb log u >1 so that \u03b1 is tangent to the simplex.", "startOffset": 3, "endOffset": 6}, {"referenceID": 19, "context": "RN (H) is the Rademacher complexity [21] measuring the complexity of the hypothesis spaceH.", "startOffset": 36, "endOffset": 40}, {"referenceID": 16, "context": "In figures 3a-c, h(x), y and M are generated as described in [18] section 5.", "startOffset": 61, "endOffset": 65}, {"referenceID": 16, "context": "In 3c, convergence is defined as in [18].", "startOffset": 36, "endOffset": 40}, {"referenceID": 18, "context": "The unregularized Wasserstein distance was computed using FastEMD [20].", "startOffset": 66, "endOffset": 70}, {"referenceID": 19, "context": "The Rademacher complexity RN (H) for commonly used models like neural networks and kernel machines [21] decays with the training set size.", "startOffset": 99, "endOffset": 103}, {"referenceID": 20, "context": "The Wasserstein loss is also closely related to the Jaccard index [22], also known as intersectionover-union (IoU), which is a popular measure of performance in segmentation [23].", "startOffset": 174, "endOffset": 178}, {"referenceID": 21, "context": "We apply the Wasserstein loss to a real world multi-label learning problem, using the recently released Yahoo/Flickr Creative Commons 100M dataset [24].", "startOffset": 147, "endOffset": 151}, {"referenceID": 22, "context": "We derive a distance metric between tags by using word2vec [25] to embed the tags as unit vectors, then taking their Euclidean distances.", "startOffset": 59, "endOffset": 63}, {"referenceID": 23, "context": "To extract image features we use MatConvNet [26].", "startOffset": 44, "endOffset": 48}], "year": 2015, "abstractText": "Learning to predict multi-label outputs is challenging, but in many problems there is a natural metric on the outputs that can be used to improve predictions. In this paper we develop a loss function for multi-label learning, based on the Wasserstein distance. The Wasserstein distance provides a natural notion of dissimilarity for probability measures. Although optimizing with respect to the exact Wasserstein distance is costly, recent work has described a regularized approximation that is efficiently computed. We describe efficient learning algorithms based on this regularization, extending the Wasserstein loss from probability measures to unnormalized measures. We also describe a statistical learning bound for the loss and show connections with the total variation norm and the Jaccard index. The Wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space. We demonstrate this property on a real-data tag prediction problem, using the Yahoo Flickr Creative Commons dataset, achieving superior performance over a baseline that doesn\u2019t use the metric.", "creator": "LaTeX with hyperref package"}}}