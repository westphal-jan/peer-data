{"id": "1506.05790", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2015", "title": "Scalable Semi-Supervised Aggregation of Classifiers", "abstract": "We present and empirically evaluate an efficient algorithm that learns to predict using an ensemble of binary classifiers. It uses the structure of the ensemble predictions on unlabeled data to yield classification performance gains without making assumptions on the predictions or their origin, and does this as scalably as linear learning.", "histories": [["v1", "Thu, 18 Jun 2015 19:53:12 GMT  (189kb,D)", "https://arxiv.org/abs/1506.05790v1", null], ["v2", "Wed, 11 Nov 2015 01:06:07 GMT  (189kb,D)", "http://arxiv.org/abs/1506.05790v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["akshay balsubramani", "yoav freund"], "accepted": true, "id": "1506.05790"}, "pdf": {"name": "1506.05790.pdf", "metadata": {"source": "CRF", "title": "Scalable Semi-Supervised Aggregation of Classifiers", "authors": ["Akshay Balsubramani", "Yoav Freund"], "emails": ["abalsubr@cs.ucsd.edu", "yfreund@cs.ucsd.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, it will only take one year for an agreement to be reached."}, {"heading": "2 Preliminaries", "text": "Write [a] + = max (0, a) and [n] = {1, 2,.., n}. All vector inequalities are component-wise. First, we look at an ensemble H = {h1,.., hp} and unlabeled data x1,.., xn on which we want to predict. As in [9], predictions and labels may be randomized, represented by values in [\u2212 1,.] instead of just the two values {\u2212 1, 1}. The predictions of the ensemble on the unlabeled data are denoted by F: F = h1 (x1) h1 (x2) \u00b7 h1 (xn)... \u00b7 h1 (xn)....... hp (x1) hp (x2) \u00b7 hp (xn)."}, {"heading": "2.1 The Transductive Binary Classification Game", "text": "The idea of [9] is to formulate the ensemble aggregation problem as a two-player zero-sum game between a predictor and an opponent. (In this game, the predictor is the first player to specify g = (g1; g2;...; gn), a randomized designation gi [\u2212 1, 1] for each example {xi} ni = 1. The opponent then sets the designations z [\u2212 1, 1] n below the error constraints of the ensemble classifier defined by b. 2. The goal of the predictor is to minimize the worst expected classification error on the test data (w.r.t. the randomized designations z and g), which are only 12 (1 \u2212 1nz > g). This is also considered a maximizing worst-case correlation 1nz > g."}, {"heading": "2.2 Interpretation", "text": "Theorem 2 proposes a statistical learning algorithm for the aggregation of the p-ensemble classifiers > > Predictions: Estimation b from the specified (labeled) formation, optimization of the convex slip function \u03b3 (\u03c3) to determine the final trailing behavior, and finally prediction with gj (\u03c3) on each example j in the test set. The resulting predictions are guaranteed to have low errors as measured by V. In particular, it is easy to prove [9] that V is at least maxi bi, the performance of the best classification. The slip function (3) deserves further verification. Its first term depends only on the labeled data and not on the unlabeled set, while the second term 1n slip angle n j = 1 [envisaged x > j slip angle - 1] + contains only unlabeled information. These two terms are gently interchangeable - as the problem is completely monitored and not labeled."}, {"heading": "2.3 Advantages and Disadvantages", "text": "This formulation has several significant advantages that seem to recommend its use in practical situations. It is very efficient - once b is estimated (a scalable task given the quantity labeled), the slip function \u03b3 is effectively an average compared to convex functions of i.i.d. unlabeled examples, and is therefore susceptible to standard convex optimization techniques [9] such as stochastic gradient descent (SGD) and variants, which work only in p dimensions, independent of n (which is p).The slip function is lip-sharp and well-educated, resulting in stable approximate learning. Furthermore, the test time prediction is extremely efficient because it requires only p-dimensional weighting of p dimensions and can be calculated exemplarily from the test set, using only one point product in Rp. The form of g elements and their dependence on \u043c facilitates the interpretation, as it links objects known for signature functions:"}, {"heading": "3 Learning with Specialists", "text": "To address this problem, we examine a generalized scenario in which any classifier within the ensemble can do without any subset of examples instead of forecasting them. It is a specialist who predicts only a subset of the input, and we think of his participation decision, which is randomized in the same way as the randomized label on each example. In this section, we expand the framework of Section 2.1 to include arbitrary specialists, and discuss the semi-supervised learning algorithm that leads to the results. In our formulation, we assume that for a classifier i [p] and an example x, the classifier decides to settle for the probability 1 \u2212 vi. But when the decision to participate, the classifier becomes calculatshi (x), as before. Our only assumption on {vi (x1) is, vi (xn)."}, {"heading": "3.1 Creating Specialists for an Algorithm", "text": "The algorithm, HEDGECLIPPER, is given in Fig. 1, and instantiates our specialized learning system with a random forest [3]. As the first exploration of the framework here, random forests are a suitable base ensemble because they are known to show state-of-the-art performance [10]. Their well-known advantages also include scalability, robustness (to corrupt data and parameter decisions) and interpretability; each of these advantages is shared by our aggregation algorithm, which consequently inherits them all. Moreover, decision trees are a natural fit as ensemble classifiers because they are inherently hierarchical. Intuitively (and indeed formally too [11], they act like the closest (NN) predictors w.r.t. a distance that is \"adaptable\" to represent the data of each tree in a random, random space."}, {"heading": "3.2 Discussion", "text": "Trees in random forests have thousands of leaves or more in practice. Since we advocate adding so many additional specialists to the ensemble for optimization, it is natural to ask if this undermines some of the benefits we used to claim. Mathematically, it is not fruitful. If \u03c1j (xi) = 0, i.e. classifier j renounces xi, then the value of hj (xi) is irrelevant. Therefore, storing S in a sparse matrix format in our setup is natural, with the accompanying performance gain in the calculation S > \u03c3, while learning and predicting with it. This turns out to be crucial for efficiency - each tree induces a division of the data, so that the series of rows corresponding to each tree contains n nonzero entries in total. This is seen in Fig. 1.Statistically, the situation is more complex than the situation we are in."}, {"heading": "4 Experimental Evaluation", "text": "We now turn to evaluating HEDGECLIPPER on publicly available datasets. Our implementation uses minibatch SGD for optimization (6), runs in Python on top of the popular open source learning package scikit-learn, and runs out-of-core (n-independent memory), taking advantage of the scalability of our formulation.3 The datasets are pulled by UCI / LibSVM as well as data mining sites like Kaggle, and no further pre-processing of the data has been carried out. We point to \"Base RF\" as the forest of limited trees from which our implementation draws its specialists. We limit the training data available to the algorithm, and use mostly verified datasets as these are far more than semi-monitored public datasets. Unused marked examples are combined with the test examples (and the additional unlabeled sets, if they are provided)."}, {"heading": "5 Related and Future Work", "text": "In fact, it is an interesting open problem to incorporate ideas into our formulation, especially since the two boosting methods accentuate themselves. It is possible to make this footprint independent of the features of the experiment that are not made here. It is an interesting open problem to include ideas in our formulation, especially since the two boosting methods accentuate themselves. It is possible to make this footprint independent of the features of the experiment that are not made here."}, {"heading": "Acknowledgements", "text": "The authors acknowledge the support of the National Science Foundation under the funding program IIS-1162581."}, {"heading": "A Additional Information on Experiments", "text": "The results of the study are averaged over 10 random splits of the training datasets: Dataset Data Size (m /) n Dim. d Comments kagg-prot 3751 1776 Kaggle challenge [23] ssl-text 1500 11960 [7] kagg-cred 150000 10 Kaggle challenge [24] a1a 1605 / 30956 LibSVM w1a 2477 / 47272 300 LibSVM covtype 581012 54 LibSVM ssl-secstr 83679 (unla-beled: 1189472) 315 [7] SUSY 5000000 18 UCI HIGGS 28 UCI-Typ 500000 2000 PASCAL Large Scale Learning Challenge 2008 webspam-uni 350000 254 LibSVMAll data from the challenge (e.g. kagg-cred) lacked test labels, so the results are averaged over 10 random splits of the dataset A.2"}], "references": [{"title": "Boosting: Foundations and Algorithms", "author": ["Robert E. Schapire", "Yoav Freund"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "The bellkor solution to the netflix grand prize", "author": ["Yehuda Koren"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Boosting the margin: A new explanation for the effectiveness of voting methods", "author": ["Robert E Schapire", "Yoav Freund", "Peter Bartlett", "Wee Sun Lee"], "venue": "Annals of statistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "Introduction to semi-supervised learning", "author": ["Xiaojin Zhu", "Andrew B Goldberg"], "venue": "Synthesis lectures on artificial intelligence and machine learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Optimally combining classifiers using unlabeled data", "author": ["Akshay Balsubramani", "Yoav Freund"], "venue": "In Conference on Learning Theory,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "An empirical evaluation of supervised learning in high dimensions", "author": ["Rich Caruana", "Nikos Karampatziakis", "Ainur Yessenalina"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Random forests and adaptive nearest neighbors", "author": ["Yi Lin", "Yongho Jeon"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Feature hashing for large scale multitask learning", "author": ["Kilian Weinberger", "Anirban Dasgupta", "John Langford", "Alex Smola", "Josh Attenberg"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Greedy function approximation: a gradient boosting machine", "author": ["Jerome H Friedman"], "venue": "Annals of statistics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2001}, {"title": "Game theory, on-line prediction and boosting", "author": ["Yoav Freund", "Robert E Schapire"], "venue": "In Proceedings of the ninth annual conference on Computational learning theory,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1996}, {"title": "Semiboost: Boosting for semi-supervised learning", "author": ["P Kumar Mallapragada", "Rong Jin", "Anil K Jain", "Yi Liu"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2000}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E. Schapire"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1997}, {"title": "Transductive inference for text classification using support vector machines", "author": ["Thorsten Joachims"], "venue": "In Proceedings of the Sixteenth International Conference on Machine Learning,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}, {"title": "Maximum likelihood estimation of observer error-rates using the em algorithm", "author": ["Alexander Philip Dawid", "Allan M Skene"], "venue": "Applied statistics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1979}, {"title": "Ranking and combining multiple predictors without labeled data", "author": ["Fabio Parisi", "Francesco Strino", "Boaz Nadler", "Yuval Kluger"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Fast discriminative visual codebooks using randomized clustering forests", "author": ["Frank Moosmann", "Bill Triggs", "Frederic Jurie"], "venue": "In Twentieth Annual Conference on Neural Information Processing Systems", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "Using and combining predictors that specialize", "author": ["Yoav Freund", "Robert E Schapire", "Yoram Singer", "Manfred K Warmuth"], "venue": "In Proceedings of the twenty-ninth annual ACM symposium on Theory of computing,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "Ensemble-based learning is a very successful approach to learning classifiers, including well-known methods like boosting [1], bagging [2], and random forests [3].", "startOffset": 122, "endOffset": 125}, {"referenceID": 1, "context": "The power of these methods has been clearly demonstrated in open large-scale learning competitions such as the Netflix Prize [4] and the ImageNet Challenge [5].", "startOffset": 125, "endOffset": 128}, {"referenceID": 2, "context": "By aggregating over classifiers, ensemble methods reduce the variance of the predictions, and sometimes also reduce the bias [6].", "startOffset": 125, "endOffset": 128}, {"referenceID": 3, "context": "Our work is therefore at the intersection of semi-supervised learning [7, 8] and ensemble learning.", "startOffset": 70, "endOffset": 76}, {"referenceID": 4, "context": "This paper is based on recent theoretical results of the authors [9].", "startOffset": 65, "endOffset": 68}, {"referenceID": 4, "context": "For the sake of completeness, we provide an intuitive introduction to the analysis given in [9].", "startOffset": 92, "endOffset": 95}, {"referenceID": 4, "context": "The above setup was recently introduced and analyzed in [9].", "startOffset": 56, "endOffset": 59}, {"referenceID": 4, "context": "In this paper, we build on the worst-case framework of [9] to devise an efficient and practical semisupervised aggregation algorithm for random forests.", "startOffset": 55, "endOffset": 58}, {"referenceID": 4, "context": "We incorporate these, and the targeted information they carry, into the worst-case framework of [9].", "startOffset": 96, "endOffset": 99}, {"referenceID": 4, "context": "We develop these ideas in the rest of this paper, reviewing the core worst-case setting of [9] in Section 2, and specifying how to incorporate specialists and the resulting learning algorithm in Section 3.", "startOffset": 91, "endOffset": 94}, {"referenceID": 4, "context": "Though the framework of [9] and our extensions can be applied to any ensemble of arbitrary origin, in this paper we focus on random forests, which have been repeatedly demonstrated to have state-of-theart, robust classification performance in a wide variety of situations [10].", "startOffset": 24, "endOffset": 27}, {"referenceID": 5, "context": "Though the framework of [9] and our extensions can be applied to any ensemble of arbitrary origin, in this paper we focus on random forests, which have been repeatedly demonstrated to have state-of-theart, robust classification performance in a wide variety of situations [10].", "startOffset": 272, "endOffset": 276}, {"referenceID": 4, "context": "A few definitions are required to discuss these issues concretely, following [9].", "startOffset": 77, "endOffset": 80}, {"referenceID": 4, "context": "As in [9], the predictions and labels are allowed to be randomized, represented by values in [\u22121, 1] instead of just the two values {\u22121, 1}.", "startOffset": 6, "endOffset": 9}, {"referenceID": 4, "context": ", in a standard way also used by empirical risk minimization (ERM) methods that simply predict with the minimum-error classifier [9].", "startOffset": 129, "endOffset": 132}, {"referenceID": 4, "context": "The idea of [9] is to formulate the ensemble aggregation problem as a two-player zero-sum game between a predictor and an adversary.", "startOffset": 12, "endOffset": 15}, {"referenceID": 0, "context": "The minimax theorem ([1], p.", "startOffset": 21, "endOffset": 24}, {"referenceID": 4, "context": "The main result of [9] uses these to describe the minimax equilibrium of the game (2).", "startOffset": 19, "endOffset": 22}, {"referenceID": 4, "context": "Theorem 2 ([9]).", "startOffset": 11, "endOffset": 14}, {"referenceID": 4, "context": "In particular, it is easy to prove [9] that V is at least maxi bi, the performance of the best classifier.", "startOffset": 35, "endOffset": 38}, {"referenceID": 4, "context": "Theorem 3 ([9]).", "startOffset": 11, "endOffset": 14}, {"referenceID": 4, "context": "unlabeled examples, and consequently is amenable to standard convex optimization techniques [9] like stochastic gradient descent (SGD) and variants.", "startOffset": 92, "endOffset": 95}, {"referenceID": 0, "context": "So for some [bS ]i \u2208 [0, 1],", "startOffset": 21, "endOffset": 27}, {"referenceID": 5, "context": "As an initial exploration of the framework here, random forests are an appropriate base ensemble because they are known to exhibit state-of-the-art performance [10].", "startOffset": 160, "endOffset": 164}, {"referenceID": 6, "context": "Intuitively (and indeed formally too [11]), they act like nearest-neighbor (NN) predictors w.", "startOffset": 37, "endOffset": 41}, {"referenceID": 7, "context": "(pseudo)-secondorder methods [12], whose effect would be interesting to explore in future work.", "startOffset": 29, "endOffset": 33}, {"referenceID": 2, "context": "The awake ensemble prediction values x>\u03c3 on the unlabeled set are a natural way to visualize and explore the operation of the algorithm on the data, in an analogous way to the margin distribution in boosting [6].", "startOffset": 208, "endOffset": 211}, {"referenceID": 0, "context": "This paper\u2019s framework and algorithms are superficially reminiscent of boosting, another paradigm that uses voting behavior to aggregate an ensemble and has a game-theoretic intuition [1, 15].", "startOffset": 184, "endOffset": 191}, {"referenceID": 10, "context": "This paper\u2019s framework and algorithms are superficially reminiscent of boosting, another paradigm that uses voting behavior to aggregate an ensemble and has a game-theoretic intuition [1, 15].", "startOffset": 184, "endOffset": 191}, {"referenceID": 11, "context": "There is some work on semi-supervised versions of boosting [16], but it departs from this principled structure and has little in common with our approach.", "startOffset": 59, "endOffset": 63}, {"referenceID": 12, "context": "Classical boosting algorithms like AdaBoost [17] make no attempt to use unlabeled data.", "startOffset": 44, "endOffset": 48}, {"referenceID": 8, "context": "It is possible to make this footprint independent of d as well by hashing features [13], not done here.", "startOffset": 83, "endOffset": 87}, {"referenceID": 9, "context": "Dataset # training HEDGECLIPPER Random Forest Base RF AdaBoost Trees MART [14] Logistic Regression", "startOffset": 74, "endOffset": 78}, {"referenceID": 13, "context": "Popular variants on supervised learning such as the transductive SVM [18] and graph-based or nearest-neighbor algorithms, which dominate the semi-supervised literature [8], have shown promise largely in data-poor regimes because they face major scalability challenges.", "startOffset": 69, "endOffset": 73}, {"referenceID": 3, "context": "Popular variants on supervised learning such as the transductive SVM [18] and graph-based or nearest-neighbor algorithms, which dominate the semi-supervised literature [8], have shown promise largely in data-poor regimes because they face major scalability challenges.", "startOffset": 168, "endOffset": 171}, {"referenceID": 14, "context": "Largely unsupervised ensemble methods have been explored especially in applications like crowdsourcing, in which the method of [19] gave rise to a plethora of Bayesian methods under various conditional independence generative assumptions on F [20].", "startOffset": 127, "endOffset": 131}, {"referenceID": 15, "context": "Largely unsupervised ensemble methods have been explored especially in applications like crowdsourcing, in which the method of [19] gave rise to a plethora of Bayesian methods under various conditional independence generative assumptions on F [20].", "startOffset": 243, "endOffset": 247}, {"referenceID": 16, "context": "Using tree structure to construct new features has been applied successfully, though without guarantees [21].", "startOffset": 104, "endOffset": 108}, {"referenceID": 17, "context": "[22].", "startOffset": 0, "endOffset": 4}], "year": 2015, "abstractText": "We present and empirically evaluate an efficient algorithm that learns to aggregate the predictions of an ensemble of binary classifiers. The algorithm uses the structure of the ensemble predictions on unlabeled data to yield significant performance improvements. It does this without making assumptions on the structure or origin of the ensemble, without parameters, and as scalably as linear learning. We empirically demonstrate these performance gains with random forests.", "creator": "LaTeX with hyperref package"}}}