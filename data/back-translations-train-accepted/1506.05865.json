{"id": "1506.05865", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jun-2015", "title": "LCSTS: A Large Scale Chinese Short Text Summarization Dataset", "abstract": "Automatic text summarization is widely regarded as the highly difficult problem, partially because of the lack of large text summarization data set. Due to the great challenge of constructing the large scale summaries for full text, in this paper, we introduce a large corpus of Chinese short text summarization dataset constructed from the Chinese microblogging website Sina Weibo, which will be released to public soon. This corpus consists of over 2 million real Chinese short texts with short summaries given by the writer of each text. We also manually tagged the relevance of 10,666 short summaries with their corresponding short texts. Based on the corpus, we introduce recurrent neural network for the summary generation and achieve promising results, which not only shows the usefulness of the proposed corpus for short text summarization research, but also provides a baseline for further research on this topic.", "histories": [["v1", "Fri, 19 Jun 2015 02:40:42 GMT  (1912kb,D)", "https://arxiv.org/abs/1506.05865v1", null], ["v2", "Mon, 22 Jun 2015 14:33:39 GMT  (1908kb,D)", "http://arxiv.org/abs/1506.05865v2", null], ["v3", "Mon, 17 Aug 2015 02:43:38 GMT  (1904kb,D)", "http://arxiv.org/abs/1506.05865v3", null], ["v4", "Fri, 19 Feb 2016 16:35:35 GMT  (1310kb,D)", "http://arxiv.org/abs/1506.05865v4", "Recently, we received feedbacks from Yuya Taguchi from NAIST in Japan and Qian Chen from USTC of China, that the results in the EMNLP2015 version seem to be underrated. So we carefully checked our results and find out that we made a mistake while using the standard ROUGE. Then we re-evaluate all methods in the paper and get corrected results listed in Table 2 of this version"]], "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG", "authors": ["baotian hu", "qingcai chen", "fangze zhu"], "accepted": true, "id": "1506.05865"}, "pdf": {"name": "1506.05865.pdf", "metadata": {"source": "CRF", "title": "LCSTS: A Large Scale Chinese Short Text Summarization Dataset", "authors": ["Baotian Hu", "Qingcai Chen", "Fangze Zhu"], "emails": ["zhufangze123}@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "This year, it is so far that it will only take a few more days before there is a result in which there is a result."}, {"heading": "2 Related Work", "text": "Our work relates to current work on automatic text aggregation and natural language processing based on naturally annotated web resources, briefly presented as the following. Automatic text aggregation in some form has been studied since 1950. Since then, most research has focused on extractive summaries by analyzing the organization of words in the document in a natural way (Nenkova and McKeown, 2011) (Luhn, 1998); since it requires labeled records for supervised machine learning methods and markup records, some research is focused on the uncontrolled methods (Mihalcea, 2004). The scope of existing records is usually very small (most of them are less than 1000). For example, DUC2002 contains records 567 documents and each document is provided with two 100-word human summaries. Our work also relates to the headline generation, which is a task to generate a set of the text it names."}, {"heading": "3 Data Collection", "text": "Many popular Chinese media and organizations have set up accounts on Sina Weibo. They use their accounts to post news and information, which are verified on Weibo and marked with a blue \"V.\" To guarantee the quality of the text scratched, we only search the Weibos of the verified organizations, which are more likely to be clean, formal and informative. At each step, a lot of human intervention is required. The process of data collection is presented as Figure 2 and summarized as follows: 1) We first collect 50 very popular users of the organization as seeds. They come from the fields of politics, economics, military, film, game and so on, such as People's Daily, the Economic Observe Press, the Ministry of National Defense and so on. 2) We then rummage through fixers, followed by these seed users and filter them by using human written rules, such as the user must be blue verified, the number of followers is more than 1 million, and we will continue to filter the shorthand shorthand shorthand shorthand shorthand we will use them."}, {"heading": "4 Data Properties", "text": "The data set consists of three parts, which are presented as Table 1, and the length distributions of texts are presented as summary sentences. Part I is the main content of the LCSTS, which contains 2,400,591 (short text, summary) pairs, which can be used to train the supervised learning model for the summary generation.Part II contains the 10,666 human-designated (short text, summary) pairs with a score of 1 to 5, which indicate the relevance between the short text and the corresponding summary. \"1\" designates \"the least relevant\" and \"5\" designates \"the most relevant.\" To comment on this part, we recruit 5 volunteers, each pair being labeled by only one annotator. These pairs are randomly scanned from Part I and used to analyze the distribution of the pairs in Part I. Figure 4 illustrates examples of different data sets, which we evaluate using 3, 4 or 5 sets of data, and the text appears to be very relevant to the relevant part 5, and the corresponding summaries are highly informative."}, {"heading": "5 Experiment", "text": "rE \"s tis rf\u00fc \u00fc ide rf\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R"}, {"heading": "6 Conclusion and Future Work", "text": "We have created an extensive Chinese short text dataset and based on it have implemented RNN-based methods that have produced promising results. This is just the beginning of deep models of this task and there is a lot of room for improvement. We take the entire short text as one sequence, which may not make much sense as most short texts contain multiple sentences. A hierarchical RNN (Li et al., 2015) is one possible direction. The rare word problem is also very important for creating the summaries, especially if the input is word-based rather than character-based. It is also a hot topic in neural generative models such as the Neural Translation Engine (NTM) (Luong et al., 2014), which can benefit from this task. We also plan to create a large dataset to summarize documents using naturally annotated web resources."}, {"heading": "Acknowledgments", "text": "This work is supported by the National Natural Science Foundation of China: 61473101, 61173075 and 61272383, Strategic Emerging Industry Development Special Funds of Shenzhen: JCYJ20140417172417105, JCYJ20140508161040764 and JCYJ20140627163809422. We thank Baolin Peng, Lin Ma, Li Yu and the anonymous critics for their insightful comments."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Abstractive multi-document summarization via phrase selection and merging", "author": ["Bing et al.2015] Lidong Bing", "Piji Li", "Yi Liao", "Wai Lam", "Weiwei Guo", "Rebecca Passonneau"], "venue": "In Proceedings of the ACL-IJCNLP,", "citeRegEx": "Bing et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bing et al\\.", "year": 2015}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555", "author": ["Chung et al.2014] Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Gated feedback recurrent neural networks. CoRR, abs/1502.02367", "author": ["Chung et al.2015] Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Chung et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "Heads: Headline generation as sequence prediction using an abstract feature-rich space", "author": ["Marina Litvak", "Amin Mantrach", "Fabrizio Silvestri"], "venue": "In Proceddings of 2015 Conference", "citeRegEx": "Colmenares et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Colmenares et al\\.", "year": 2015}, {"title": "Speech recognition with deep recurrent neural networks. CoRR, abs/1303.5778", "author": ["Graves et al.2013] Alex Graves", "Abdel-rahman Mohamed", "Geoffrey E. Hinton"], "venue": null, "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Automated text summarization and the summarist system", "author": ["Hovy", "Lin1998] Eduard Hovy", "Chin-Yew Lin"], "venue": "In Proceedings of a Workshop on Held at Baltimore, Maryland: October", "citeRegEx": "Hovy et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Hovy et al\\.", "year": 1998}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Hu et al.2014] Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Hu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "Meme-tracking and the dynamics of the news cycle", "author": ["Lars Backstrom", "Jon Kleinberg"], "venue": "In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201909,", "citeRegEx": "Leskovec et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Leskovec et al\\.", "year": 2009}, {"title": "A hierarchical neural autoencoder", "author": ["Li et al.2015] Jiwei Li", "Minh-Thang Luong", "Dan Jurafsky"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Automatic evaluation of summaries using n-gram co-occurrence statistics", "author": ["Lin", "Hovy2003] Chin-Yew Lin", "E.H. Hovy"], "venue": "In Proceedings of 2003 Language Technology Conference (HLTNAACL", "citeRegEx": "Lin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2003}, {"title": "The automatic creation of literature abstracts", "author": ["H.P. Luhn"], "venue": "IBM Journal of Research and Development,", "citeRegEx": "Luhn.,? \\Q1998\\E", "shortCiteRegEx": "Luhn.", "year": 1998}, {"title": "Addressing the rare word problem in neural machine translation. CoRR, abs/1410.8206", "author": ["Luong et al.2014] Thang Luong", "Ilya Sutskever", "Quoc V. Le", "Oriol Vinyals", "Wojciech Zaremba"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2014}, {"title": "Graph-based ranking algorithms for sentence extraction, applied to text summarization", "author": ["Rada Mihalcea"], "venue": "In Proceedings of the 42nd Annual Meeting of the Association", "citeRegEx": "Mihalcea.,? \\Q2004\\E", "shortCiteRegEx": "Mihalcea.", "year": 2004}, {"title": "Neural responding machine for short-text conversation. CoRR, abs/1503.02364", "author": ["Shang et al.2015] Lifeng Shang", "Zhengdong Lu", "Hang Li"], "venue": null, "citeRegEx": "Shang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "Natural language procesing based on naturaly annotated web resources", "author": ["Mao Song Sun"], "venue": "Journal of Chinese Information Processing,", "citeRegEx": "Sun.,? \\Q2011\\E", "shortCiteRegEx": "Sun.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. V Le"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "ADADELTA: an adaptive learning rate method. CoRR, abs/1212.5701", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Answer sequence learning with neural networks for answer selection in community question answering", "author": ["Zhou et al.2015] Xiaoqiang Zhou", "Baotian Hu", "Qingcai Chen", "Buzhou Tang", "Xiaolong Wang"], "venue": "In Proceedings of the ACL-IJCNLP,", "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 15, "context": "Such data is regarded as naturally annotated web resources (Sun, 2011).", "startOffset": 59, "endOffset": 70}, {"referenceID": 1, "context": "Most of traditional abstractive summarization methods divide the process into two phrases (Bing et al., 2015).", "startOffset": 90, "endOffset": 109}, {"referenceID": 7, "context": "Recently, deep learning methods have shown potential abilities to learn representation (Hu et al., 2014; Zhou et al., 2015) and generate language (Bahdanau et al.", "startOffset": 87, "endOffset": 123}, {"referenceID": 18, "context": "Recently, deep learning methods have shown potential abilities to learn representation (Hu et al., 2014; Zhou et al., 2015) and generate language (Bahdanau et al.", "startOffset": 87, "endOffset": 123}, {"referenceID": 0, "context": ", 2015) and generate language (Bahdanau et al., 2014; Sutskever et al., 2014) from large scale data by utilizing GPUs.", "startOffset": 30, "endOffset": 77}, {"referenceID": 16, "context": ", 2015) and generate language (Bahdanau et al., 2014; Sutskever et al., 2014) from large scale data by utilizing GPUs.", "startOffset": 30, "endOffset": 77}, {"referenceID": 11, "context": "Since then, most researches are related to extractive summarizations by analyzing the organization of the words in the document (Nenkova and McKeown, 2011) (Luhn, 1998); Since it needs labeled data sets for supervised machine learning methods and labeling dataset is very intensive, some researches focused on the unsupervised methods (Mihalcea, 2004).", "startOffset": 156, "endOffset": 168}, {"referenceID": 13, "context": "Since then, most researches are related to extractive summarizations by analyzing the organization of the words in the document (Nenkova and McKeown, 2011) (Luhn, 1998); Since it needs labeled data sets for supervised machine learning methods and labeling dataset is very intensive, some researches focused on the unsupervised methods (Mihalcea, 2004).", "startOffset": 335, "endOffset": 351}, {"referenceID": 4, "context": "3 million financial news headline dataset written in English for headline generation (Colmenares et al., 2015).", "startOffset": 85, "endOffset": 110}, {"referenceID": 15, "context": "Naturally Annotated Web Resources based Natural Language Processing is proposed by Sun (Sun, 2011).", "startOffset": 87, "endOffset": 98}, {"referenceID": 8, "context": "6 million mainstream media sites and blogs and mine a set of novel and persistent temporal patterns in the news cycle (Leskovec et al., 2009).", "startOffset": 118, "endOffset": 141}, {"referenceID": 5, "context": "Recently, recurrent neural network (RNN) have shown powerful abilities on speech recognition (Graves et al., 2013), machine translation (Sutskever et al.", "startOffset": 93, "endOffset": 114}, {"referenceID": 16, "context": ", 2013), machine translation (Sutskever et al., 2014) and automatic dialog response (Shang et al.", "startOffset": 29, "endOffset": 53}, {"referenceID": 14, "context": ", 2014) and automatic dialog response (Shang et al., 2015).", "startOffset": 38, "endOffset": 58}, {"referenceID": 0, "context": "We use the RNN as encoder and it\u2019s last hidden state as the input of decoder, as shown in Figure 5; 2) The context is used during decoding, following (Bahdanau et al., 2014), we use the combination of all the hidden states of encoder as input of the decoder, as shown in Figure 6.", "startOffset": 150, "endOffset": 173}, {"referenceID": 3, "context": "For the RNN, we adopt the gated recurrent unit (GRU) which is proposed by (Chung et al., 2015) and has been proved comparable to LSTM (Chung et al.", "startOffset": 74, "endOffset": 94}, {"referenceID": 2, "context": ", 2015) and has been proved comparable to LSTM (Chung et al., 2014).", "startOffset": 47, "endOffset": 67}, {"referenceID": 17, "context": "All the parameters (including the embeddings) of the two architectures are randomly initialized and ADADELTA (Zeiler, 2012) is used to update the learning rate.", "startOffset": 109, "endOffset": 123}, {"referenceID": 9, "context": "A hierarchical RNN (Li et al., 2015) is one possible direction.", "startOffset": 19, "endOffset": 36}, {"referenceID": 12, "context": "It is also a hot topic in the neural generative models such as neural translation machine(NTM) (Luong et al., 2014), which can benefit to this task.", "startOffset": 95, "endOffset": 115}], "year": 2016, "abstractText": "Automatic text summarization is widely regarded as the highly difficult problem, partially because of the lack of large text summarization data set. Due to the great challenge of constructing the large scale summaries for full text, in this paper, we introduce a large corpus of Chinese short text summarization dataset constructed from the Chinese microblogging website Sina Weibo, which is released to the public1. This corpus consists of over 2 million real Chinese short texts with short summaries given by the author of each text. We also manually tagged the relevance of 10,666 short summaries with their corresponding short texts. Based on the corpus, we introduce recurrent neural network for the summary generation and achieve promising results, which not only shows the usefulness of the proposed corpus for short text summarization research, but also provides a baseline for further research on this topic.", "creator": "LaTeX with hyperref package"}}}