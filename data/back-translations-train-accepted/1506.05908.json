{"id": "1506.05908", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jun-2015", "title": "Deep Knowledge Tracing", "abstract": "Knowledge tracing---where a machine models the knowledge of a student as they interact with coursework---is a well established problem in computer supported education. Though effectively modeling student knowledge would have high educational impact, the task has many inherent challenges. In this paper we explore the utility of using Recurrent Neural Networks (RNNs) to model student learning. The RNN family of models have important advantages over previous methods in that they do not require the explicit encoding of human domain knowledge, and can capture more complex representations of student knowledge. Using neural networks results in substantial improvements in prediction performance on a range of knowledge tracing datasets. Moreover the learned model can be used for intelligent curriculum design and allows straightforward interpretation and discovery of structure in student tasks. These results suggest a promising new line of research for knowledge tracing and an exemplary application task for RNNs.", "histories": [["v1", "Fri, 19 Jun 2015 08:29:00 GMT  (1163kb,D)", "http://arxiv.org/abs/1506.05908v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CY cs.LG", "authors": ["chris piech", "jonathan bassen", "jonathan huang", "surya ganguli", "mehran sahami", "leonidas j guibas", "jascha sohl-dickstein"], "accepted": true, "id": "1506.05908"}, "pdf": {"name": "1506.05908.pdf", "metadata": {"source": "CRF", "title": "Deep Knowledge Tracing", "authors": ["Chris Piech", "Jonathan Spencer", "Jonathan Huang", "Surya Ganguli", "Mehran Sahami", "Leonidas Guibas", "Jascha Sohl-Dickstein"], "emails": ["piech@cs.stanford.edu,", "jspencer@cs.stanford.edu,", "jascha@stanford.edu,"], "sections": [{"heading": "1 Introduction", "text": "Computer-aided education promises open access to world-class instruction and a reduction in the growing cost of learning. We can deliver on this promise by using models of large-scale student traceability data on popular educational platforms such as Khan Academy, Coursera and EdX.Knowledge tracing is designed to model student knowledge over time so that we can accurately predict how students will respond to future interactions. Improving this task means that students can be offered resources based on their individual needs, and content that proves too easy or too difficult can be skipped or delayed. Already, hand-crafted intelligent tutoring systems that attempt to customize content can show promising results [28]. One-to-one-human tutoring can produce learning gains of the order of two standard deviations [5] for the average student."}, {"heading": "1.1 Knowledge Tracing", "text": "The task of knowledge tracking can be formalized as follows: Given the observation of x0.. xt interactions performed by a student on a particular learning task, they predict aspects of their next interaction xt + 1. In the most ubiquitous instantiation of knowledge tracking, interactions take the form of a tuple of xt = {qt, at} that combines a day for the task to be answered qt with the question of whether the task was answered correctly or not. In making a prediction, the model takes the day of the task that qt is answered and must predict whether the student will solve the task correctly at all. Figure 1 shows a visualization of knowledge tracking for an individual student learning math in 8th grade. The student answers two square root problems correctly first and then gets a single x-intercept exercise wrong. In the subsequent 47 interactions, the student solves a series of x-intercept, y-intercept and graphic exercises. Each time the student answers a relevant exercise, we can only make a prediction as to whether or not they can perform the correct interaction."}, {"heading": "2 Related Work", "text": "The task of modeling and predicting how people learn is influenced by fields as diverse as education, psychology, neuroscience, and cognitive sciences. From a social science perspective, learning is influenced by complex interactions at the macro level, including affects [21], motivation [10], and even identity [4]. Current challenges are further illustrated at the micro level. Learning is essentially a reflection of human cognition, which is a highly complex process. Two topics in the field of cognitive sciences that are particularly relevant are theories that the human mind and its learning process are recursive [12] and analogous [13]. The problem of tracking knowledge was first raised and has been intensively studied within the intelligent tutor community. Given the above challenges, a primary goal was to develop models that may not capture all cognitive processes, but are nonetheless useful."}, {"heading": "2.1 Bayesian Knowledge Tracing", "text": "Bayesian Knowledge Tracing (CCT) is the most popular approach to creating temporal models of student learning. CCT models a learner's latent level of knowledge as a set of binary variables, each representing the understanding or non-understanding of a single concept. [6] A Hidden Markov Model (HMM) is used to update the probabilities within each of these binary variables, as a learner answers exercises of a given concept correctly or incorrectly. [23] The formulation of the original model assumed that once a skill is learned, it will never be forgotten. Recent extensions of this model include the contextualization of estimates and slippage estimates [7], the assessment of prior knowledge for individual learners [33] and the assessment of problem difficulties [23]. With or without such extensions, Knowledge Tracing suffers from several difficulties. First, the binary representation of student understanding may be unrealistic. Second, the importance of hidden attributions and multiple exercises may be."}, {"heading": "2.2 Other Dynamic Probabilistic Models", "text": "Partially observable Markov decision-making processes (POMDPs) have been used to model the behavior of learners over time, in cases where the learner follows an unlimited path to reach a solution [29]. Although POMDPs represent an extremely flexible framework, they require the exploration of an exponentially large state space. Current implementations are also limited to a discrete state space, with hard-coded meanings of latent variables. This makes them insoluble or inflexible in practice, although they have the potential to overcome both of these limitations. Simpler models from the framework of Performance Factors Analysis (PFA) [24] and Learning Factors Analysis (LFA) [3] have shown a predictive power comparable to CCT [14]. To achieve better predictable results than with a single model alone, different ensemble methods have been used to combine CCT and PT."}, {"heading": "2.3 Recurrent Neural Networks", "text": "Recursive neural networks are a family of flexible dynamic models that connect artificial neurons over time. Unlike the hidden Markov models found in education, which are also dynamic, RNNs have a high-dimensional, continuous representation of latent states. A notable advantage of the richer representation of RNNs is their ability to use information from an input in a prediction at a much later stage, particularly in Long Short Term Memory (LSTM) networks - a popular type of RNN [16]. Recurrent neural networks are competitive for multiple time series tasks or state-of-the-art - for example, language into text [15], translation [22], and image capacity [17] - where large amounts of training data are available. These results suggest that we could be much more successful if we pursue students \"knowledge as a temporary application of neural networks."}, {"heading": "3 Deep Knowledge Tracing", "text": "We believe that human learning is determined by many different characteristics - the material, the context, the timing of the presentation and the individual involved - many of which are difficult to quantify if one relies only on initial principles to assign attributes to exercises or structure a graphical model. At this point, we apply two different types of RNs - a vanilla RNN model with sigmoid units and a Long Short Term Memory (LSTM) model - to the problem of predicting student responses to exercises based on their past activities."}, {"heading": "3.1 Model", "text": "Traditional recurrent Neural Networks (RNNs) map an input sequence of vectors x1,.., xT to an output sequence of vectors y1,..., yT. See Figure 2 for a cartoon illustration. This is achieved by calculating a sequence of \"hidden\" states h1,.., hT, which can be considered as successive encodings of relevant information from past observations useful for future predictions.The variables refer to a simple network that is represented by the equation h1,.,. (1) yt = \u03c3 (Wyhht + by), (2) applying both tanh and the sigmoid function \u03c3 (\u00b7) to each dimension of the input. The model is represented by an input request weight matrix Whx, recursive weight matrix Whh, explicit states h0, and the read weight matrix Wyh0 for the input units."}, {"heading": "3.2 Input and Output Time Series", "text": "In order to train an RNN or LSTM on the students \"interactions, it is necessary to convert these interactions into a sequence of fixed-length input vectors. We do this using two methods that depend on the nature of these interactions: For datasets with a small number of M unique exercises, we use xt as a uniform encoding of the interaction stage ht = {qt, at}, which represents the combination from which the exercise was answered, and if the exercise was answered correctly, so that we can assign a random vector nq, a vector nq, a vector N (0, I) to each input tuple and if performance deteriorates. For large attribute spaces, uniform encoding can quickly become impractically large. Instead, for datasets with a large number of unique exercises, we assign a random vector nq, a vector nq, a vector N (0, I) to each input tuple, where nq, a vector, a vsensiver and a vector."}, {"heading": "3.3 Optimization", "text": "The training target is the negative log probability of the observed sequence of student reactions within the model. The loss for a given prediction is \"(yTi \u03b4 (qt + 1), at + 1), and the loss for an individual student is: L = \u2211 t\" (yTi \u03b4 (qt + 1), at + 1) (3) This target was minimized by stochastic gradient descent on minibatches. To prevent overwork during training, the drop-out to ht was applied when calculating the yt selection, but not when calculating the next hidden state is + 1. We prevent gradients from \"exploding\" by shortening the length of gradients whose norm is above a threshold over time. For all models in this paper, we consistently used hidden dimensions of 200 and a minibatch of 100."}, {"heading": "4 Educational Applications", "text": "The training objective for knowledge tracking is to predict a student's future performance based on their previous activity. This is immediately useful - for example, formal tests are no longer necessary when a student's ability is continuously assessed. As experimentally examined in Section 6, the DKT model can also drive a number of other advancements."}, {"heading": "4.1 Improving Curricula", "text": "One of the biggest potential impacts of our model is in selecting the best order of learning content to be presented to a student. As the student has an estimated hidden level of knowledge, we can query our RNN to calculate what his expected level of knowledge would be if we assigned him a specific task. In Figure 1, for example, after the student has answered 50 exercises, we can test any next exercise we show him and calculate their expected level of knowledge when this choice is made. The predicted optimal next problem for this student is to reconsider solving the y-section. Generally, selecting the entire sequence of the next exercises to maximize the predicted accuracy can be formulated as a Markov decision problem. In Section 6.1, we compare solving this problem with two classic curriculum rules from educational literature: Mixing, where exercises from different topics are mixed, and blocking, where students answer a series of exercises of the same type [30]."}, {"heading": "4.2 Discovering Exercise Relationships", "text": "The DKT model can also be applied to the task of discovering latent structures or concepts in the data, a task typically performed by human experts. We approached this problem by assigning an influence jij to each directed pair of exercises i and j, jij = y (j | i) \u2211 k y (j | k), (4) where y (j | i) is the probability of correctness assigned by the RNN to exercise j if exercise i is correctly answered in the first step. In Section 6.2 we show that this characterization of the dependencies covered by the RNN restores the prerequisites associated with the exercises."}, {"heading": "5 Datasets", "text": "To evaluate performance, we test knowledge-tracking models on three sets of data: simulated data, Khan Academy Data, and the Assistents benchmark dataset. On each set of data, we measure the area below the curve (AUC). For the non-simulated data, we evaluate our results using 5-fold cross-validation and in all cases, hyperparameters are learned using training data. We compare the results of deep knowledge tracing with standard CCTC and, if possible, with optimal variations of CCTC. In addition, we compare our results with predictions made by simply calculating a student's marginal probability of obtaining a specific exercise correction. Simulated data: We simulate virtual students learning virtual concepts and test how well we can predict reactions in this controlled environment. For each run of this experiment, we generate two thousand students answering 50 exercises drawn from k-1. 5 concepts. Each student has a latent knowledge level for each concept, each concept and each concept has a concept as well as an exercise."}, {"heading": "6 Results", "text": "(...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...). (...).\" (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...)."}, {"heading": "6.2 Discovered Exercise Relationships", "text": "The predictive accuracy of the synthetic dataset suggests that it may be possible to use DKT models to extract the latent structure between the ratings in the dataset. The graph of the conditional effects of our model on the synthetic dataset shows perfect clustering of the five latent concepts (see Figure 4), with directed edges using the influence function set in Equation 4. An interesting observation is that some of the exercises from the same concept were far apart in time. Thus, for example, in the synthetic dataset, where node numbers represent the sequence, the fifth exercise in the synthetic dataset came from the hidden concept 1, and although only the 22nd problem challenged another problem from the same concept, a strong conditional dependence between the two. We analyzed the Khan dataset using the same technique. The resulting graph is a convincing articulation of how the concepts answered the question of answering an annex from the same concept were answered."}, {"heading": "7 Discussion", "text": "During this time, we are dealing with a problem that we have in education, namely a problem that we have not been able to solve in recent years: namely, a problem that we have got to grips with, namely a problem that we have got to grips with, namely a problem that we have got to grips with."}, {"heading": "Acknowledgments", "text": "Many thanks to John Mitchell for his guide and the Khan Academy for their support. CP is supported by NSF-GRFP grant number DGE-114747."}, {"heading": "A LSTM Equations", "text": "it = \u03c3 (Wixxt + Wihht \u2212 1 + bi) (5) gt = \u03c3 (Wgxxt + Wghht \u2212 1 + bg) (6) ft = \u03c3 (Wfxxt + Wfhht \u2212 1 + bf) (7) ot = \u03c3 (Woxxt + Wghht \u2212 1 + bo) (8) ht = ot mt (9) mt = ft mt \u2212 1 + it gt (10) zt = Wzmmt + bz (11) yt = \u03c3 (zt) (12)"}, {"heading": "B Concept Clustering", "text": "Insights into the C-model"}], "references": [{"title": "Compressive sensing", "author": ["R. BARANIUK"], "venue": "IEEE signal processing magazine 24,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Learning factors analysis\u2013a general method for cognitive model evaluation and improvement. In Intelligent tutoring systems", "author": ["H. CEN", "K. KOEDINGER", "B. JUNKER"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Identity, belonging, and achievement a model, interventions, implications", "author": ["G.L. COHEN", "J. GARCIA"], "venue": "Current Directions in Psychological Science 17,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Cognitive computer tutors: Solving the two-sigma problem", "author": ["A. CORBETT"], "venue": "In User Modeling 2001. Springer,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Knowledge tracing: Modeling the acquisition of procedural knowledge. User modeling and user-adapted interaction", "author": ["A.T. CORBETT", "J.R. ANDERSON"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1994}, {"title": "AND ALEVEN, V. More accurate student modeling through contextual estimation of slip and guess probabilities in bayesian knowledge tracing", "author": ["R.S.J. D BAKER", "A.T. CORBETT"], "venue": "In Intelligent Tutoring Systems", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Ensembling predictions of student knowledge within intelligent tutoring systems", "author": ["R.S.J. D BAKER", "Z.A. PARDOS", "S.M. GOWDA", "B.B. NOORAEI", "N.T. HEFFERNAN"], "venue": "In User Modeling, Adaption and Personalization. Springer,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Addressing the assessment challenge with an online system that tutors as it assesses", "author": ["M. FENG", "N. HEFFERNAN", "K. KOEDINGER"], "venue": "User Modeling and User-Adapted Interaction 19,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "The evolution of the language faculty: clarifications and implications", "author": ["W.T. FITCH", "M.D. HAUSER", "N. CHOMSKY"], "venue": "Cognition 97,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. GRAVES", "MOHAMED", "A.-R", "G. HINTON"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. KARPATHY", "L. FEI-FEI"], "venue": "arXiv preprint arXiv:1412.2306", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Incorporating latent factors into knowledge tracing to predict individual differences in learning", "author": ["M. KHAJAH", "R.M. WING", "R.V. LINDSEY", "M.C. MOZER"], "venue": "Proceedings of the 7th International Conference on Educational Data Mining", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Integrating knowledge tracing and item response theory: A tale of two frameworks", "author": ["M.M. KHAJAH", "Y. HUANG", "J.P. GONZ\u00c1LEZ-BRENES", "M.C. MOZER", "P. BRUSILOVSKY"], "venue": "Proceedings of the 4th International Workshop on Personalization Approaches in Learning Environments", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Time-varying learning and content analytics via sparse factor analysis", "author": ["A.S. LAN", "C. STUDER", "R.G. BARANIUK"], "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining (2014),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Role of affect in cognitive processing in academic contexts. Motivation, emotion, and cognition: Integrative perspectives on intellectual functioning and development", "author": ["E.A. LINNENBRINK", "P.R. PINTRICH"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}, {"title": "Recurrent neural network based language model", "author": ["T. MIKOLOV", "M. KARAFI\u00c1T", "L. BURGET", "J. CERNOCK\u1ef2", "S. KHUDANPUR"], "venue": "In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Kt-idem: Introducing item difficulty to the knowledge tracing model", "author": ["Z.A. PARDOS", "N.T. HEFFERNAN"], "venue": "In User Modeling, Adaption and Personalization. Springer,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Performance Factors Analysis\u2013A New Alternative to Knowledge Tracing", "author": ["JR P.I. PAVLIK", "CEN H", "KOEDINGER K. R"], "venue": "Online Submission", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Learning program embeddings to propagate feedback on student code", "author": ["C. PIECH", "J. HUANG", "A. NGUYEN", "M. PHULSUKSOMBATI", "M. SAHAMI", "L.J. GUIBAS"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Faster teaching by POMDP planning. In Artificial intelligence in education", "author": ["A.N. RAFFERTY", "E. BRUNSKILL", "T.L. GRIFFITHS", "P. SHAFTO"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "The effects of spacing and mixing practice problems", "author": ["D. ROHRER"], "venue": "Journal for Research in Mathematics Education", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Cognitive task analysis", "author": ["J.M. SCHRAAGEN", "S.F. CHIPMAN", "V.L. SHALIN"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2000}, {"title": "A learning algorithm for continually running fully recurrent neural networks", "author": ["R.J. WILLIAMS", "D. ZIPSER"], "venue": "Neural computation 1,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1989}], "referenceMentions": [{"referenceID": 3, "context": "One-on-one human tutoring can produce learning gains for the average student on the order of two standard deviations [5].", "startOffset": 117, "endOffset": 120}, {"referenceID": 4, "context": "xt taken by a student on a particular learning task, predict aspects of their next interaction xt+1 [6].", "startOffset": 100, "endOffset": 103}, {"referenceID": 14, "context": "From a social science perspective learning has been understood to be influenced by complex macro level interactions including affect [21], motivation [10] and even identity [4].", "startOffset": 133, "endOffset": 137}, {"referenceID": 2, "context": "From a social science perspective learning has been understood to be influenced by complex macro level interactions including affect [21], motivation [10] and even identity [4].", "startOffset": 173, "endOffset": 176}, {"referenceID": 8, "context": "Two themes in the field of cognitive science that are particularly relevant are theories that the human mind, and its learning process, are recursive [12] and driven by analogy [13].", "startOffset": 150, "endOffset": 154}, {"referenceID": 4, "context": "BKT models a learner\u2019s latent knowledge state as a set of binary variables, each of which represents understanding or non-understanding of a single concept [6].", "startOffset": 156, "endOffset": 159}, {"referenceID": 5, "context": "Recent extensions to this model include contextualization of guessing and slipping estimates [7], estimating prior knowledge for individual learners [33], and estimating problem difficulty [23].", "startOffset": 93, "endOffset": 96}, {"referenceID": 16, "context": "Recent extensions to this model include contextualization of guessing and slipping estimates [7], estimating prior knowledge for individual learners [33], and estimating problem difficulty [23].", "startOffset": 189, "endOffset": 193}, {"referenceID": 21, "context": "The current gold standard, Cognitive Task Analysis [31] is an arduous and iterative process where domain experts ask learners to talk through their thought processes while solving problems.", "startOffset": 51, "endOffset": 55}, {"referenceID": 19, "context": "2 Other Dynamic Probabilistic Models Partially Observable Markov Decision Processes (POMDPs) have been used to model learner behavior over time, in cases where the learner follows an open-ended path to arrive at a solution [29].", "startOffset": 223, "endOffset": 227}, {"referenceID": 17, "context": "Simpler models from the Performance Factors Analysis (PFA) framework [24] and Learning Factors Analysis (LFA) framework [3] have shown predictive power comparable to BKT [14].", "startOffset": 69, "endOffset": 73}, {"referenceID": 1, "context": "Simpler models from the Performance Factors Analysis (PFA) framework [24] and Learning Factors Analysis (LFA) framework [3] have shown predictive power comparable to BKT [14].", "startOffset": 120, "endOffset": 123}, {"referenceID": 6, "context": "To obtain better predictive results than with any one model alone, various ensemble methods have been used to combine BKT and PFA [8].", "startOffset": 130, "endOffset": 133}, {"referenceID": 13, "context": "Recent work has explored combining Item Response Theory (IRT) models with switched nonlinear Kalman filters [20], as well as with Knowledge Tracing [19, 18].", "startOffset": 108, "endOffset": 112}, {"referenceID": 12, "context": "Recent work has explored combining Item Response Theory (IRT) models with switched nonlinear Kalman filters [20], as well as with Knowledge Tracing [19, 18].", "startOffset": 148, "endOffset": 156}, {"referenceID": 11, "context": "Recent work has explored combining Item Response Theory (IRT) models with switched nonlinear Kalman filters [20], as well as with Knowledge Tracing [19, 18].", "startOffset": 148, "endOffset": 156}, {"referenceID": 22, "context": "The propagation of information is recursive in that hidden neurons evolve based on both the input to the system and on their previous activation [32].", "startOffset": 145, "endOffset": 149}, {"referenceID": 9, "context": "Recurrent neural networks are competitive or state-of-the-art for several time series tasks\u2013for instance, speech to text [15], translation [22], and image captioning [17]\u2013where large amounts of training data are available.", "startOffset": 121, "endOffset": 125}, {"referenceID": 15, "context": "Recurrent neural networks are competitive or state-of-the-art for several time series tasks\u2013for instance, speech to text [15], translation [22], and image captioning [17]\u2013where large amounts of training data are available.", "startOffset": 139, "endOffset": 143}, {"referenceID": 10, "context": "Recurrent neural networks are competitive or state-of-the-art for several time series tasks\u2013for instance, speech to text [15], translation [22], and image captioning [17]\u2013where large amounts of training data are available.", "startOffset": 166, "endOffset": 170}, {"referenceID": 0, "context": "Compressed sensing states that a k\u2212sparse signal in d dimensions can be recovered exactly from k log d random linear projections (up to scaling and additive constants) [2].", "startOffset": 168, "endOffset": 171}, {"referenceID": 20, "context": "1 we compare solving this problem using expectimax to two classic curricula rules from education literature: mixing where exercises from different topics are intermixed, and blocking where students answer series of exercises of the same type [30].", "startOffset": 242, "endOffset": 246}, {"referenceID": 7, "context": "Benchmark Dataset: In order to understand how our model compared to other models we evaluated models on the Assistments 2009-2010 public benchmark dataset [11].", "startOffset": 155, "endOffset": 159}, {"referenceID": 16, "context": "69 respectively) [23].", "startOffset": 17, "endOffset": 21}], "year": 2015, "abstractText": "Knowledge tracing\u2014where a machine models the knowledge of a student as they interact with coursework\u2014is a well established problem in computer supported education. Though effectively modeling student knowledge would have high educational impact, the task has many inherent challenges. In this paper we explore the utility of using Recurrent Neural Networks (RNNs) to model student learning. The RNN family of models have important advantages over previous methods in that they do not require the explicit encoding of human domain knowledge, and can capture more complex representations of student knowledge. Using neural networks results in substantial improvements in prediction performance on a range of knowledge tracing datasets. Moreover the learned model can be used for intelligent curriculum design and allows straightforward interpretation and discovery of structure in student tasks. These results suggest a promising new line of research for knowledge tracing and an exemplary application task for RNNs.", "creator": "LaTeX with hyperref package"}}}