{"id": "1506.05934", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jun-2015", "title": "Expectation Particle Belief Propagation", "abstract": "We propose an original particle-based implementation of the Loopy Belief Propagation (LPB) algorithm for pairwise Markov Random Fields (MRF) on a continuous state space. The algorithm constructs adaptively efficient proposal distributions approximating the local beliefs at each note of the MRF. This is achieved by considering proposal distributions in the exponential family whose parameters are updated iterately in an Expectation Propagation (EP) framework. The proposed particle scheme provides consistent estimation of the LBP marginals as the number of particles increases. We demonstrate that it provides more accurate results than the Particle Belief Propagation (PBP) algorithm of Ihler and McAllester (2009) at a fraction of the computational cost and is additionally more robust empirically. The computational complexity of our algorithm at each iteration is quadratic in the number of particles. We also propose an accelerated implementation with sub-quadratic computational complexity which still provides consistent estimates of the loopy BP marginal distributions and performs almost as well as the original procedure.", "histories": [["v1", "Fri, 19 Jun 2015 09:34:21 GMT  (1512kb,D)", "http://arxiv.org/abs/1506.05934v1", "submitted to NIPS 2015"]], "COMMENTS": "submitted to NIPS 2015", "reviews": [], "SUBJECTS": "stat.CO cs.AI stat.ML", "authors": ["thibaut lienart", "yee whye teh", "arnaud doucet"], "accepted": true, "id": "1506.05934"}, "pdf": {"name": "1506.05934.pdf", "metadata": {"source": "META", "title": "Expectation Particle Belief Propagation", "authors": ["Thibaut Lienart", "Yee Whye Teh", "Arnaud Doucet"], "emails": ["lienart@stats.ox.ac.uk", "teh@stats.ox.ac.uk", "doucet@stats.ox.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Undirected Graphical Models (also known as Markov Random Fields) provide a flexible framework to represent networks of random variables and have been used in a variety of applications in machine learning, statistics, signal processing, and related fields [2]. Defining MRF at continuous state can be advantageous for many applications such as tracking, sensor networks, or imaging. Considering paired MRF, we are interested here in calculating the marginal distributions at the graph nodes. A popular approach to this is to look at the Loopy Belief Propagation (LBP) algorithms."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Notations", "text": "We are looking at a pair-wise MRF, i.e. a distribution over a series of p random variables indexed by a set V = {1,.., p}, which according to an undirected diagram G = (V, E) with p (xV). The positive functions of u: X 7 \u2192 R + and \u0442 uv: X \u00d7 X 7 \u2192 R + are each known as nodes and edge potentials. The aim is to find the marginals pu (xu) for all u-V. A popular approach is the previously discussed LBP algorithm. This algorithm is a fixation scheme that gives approximate values at each node [4, 2]."}, {"heading": "2.2 Related work", "text": "The crux of any generic implementation of LBP for continuous state spaces is to choose a way to represent the messages and design an appropriate method of calculating / approximating the messages. (In theory, the calculation of the product of such messages is analytically feasible, but in practice this is impractical due to the exponential growth in the number of terms to be considered. (To circumvent this problem, the authors suggest an importance that aims at the beliefs and adjustment of the mixtures of Gaussians to the resulting weighted particles.) The calculation of the update (2) is then always based on a constant number of terms.A limitation of the \"vanilla\" Nonparametric BP is that the messages must be endlessly integral to the message representation in order to make sense."}, {"heading": "2.3 Our contribution", "text": "Our proposed method is based on PBP, as PBP is theoretically better suited than NBP because, as discussed above, it does not have to meet the conditions (3) and, provided that a sample of the proposals delivers exactly consistent estimates of the LBP messages, while NBP does not. Furthermore, the development of our method also formally shows that consideration of proposals that come close to convictions, as proposed by [1], is a good idea. Our core observation is that, since the sample from a Form (5) proposal using MCMC simulation is very expensive, we should consider using a more traceable offer distribution. However, it is important that the offer distribution is designed appropriately, taking into account the evidence collected through the transmission of the message itself, and we propose to achieve this by using application distributions that lie in a traceable exponential family, and using the Expectation (7) framework to be adapted."}, {"heading": "3 Expectation Particle Belief Propagation", "text": "It is our goal to solve the problem of selecting the PBP algorithms. (u) We propose that we (u) n (u) n (u) n (u) n (u) n (u) n (u) n (v) n (v) n (v) n (v) n (v) n (v) n (v) n (v) n (v) n (v) n (v) n (v) n (v) n (v) n (v) n (v) n (v) n (v) n (n n n n (n) n (n) n (n) n (n) n (v) n (n) n (n) n (n) n (n) n (n (n) n (n (n) n (n (n) n (n (n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n n n (n) n v) n (n) n (n) n (n) n (n) n v) n (n v) n (n) n (n) n v) n (n v) n (n) n (n) n v) n (n v) n (n v) n (n) n v n v) n (n v) n (n v) n (n n) n v n (n) n (n) n (n) n v v n (n) n v n (n) n (n n n) n (n n n) n (n n n n n) n (n) v v v n (n) n (n n (n) n n n (n) n v v n (n) n n (n (n) n v v n (n) n n (n) n v v v n (n) n (n) n (n n n) n (n) n v v v v n (v) n (v) n (n) n n n (n n) n n n (v) n n n (v v (v) n n) n (v) n n) n n n (v (v) n) n n n (v n) n n n (v"}, {"heading": "3.1 Computational complexity and sub-quadratic implementation", "text": "Each EP projection step costs O (N) calculations, because the message m-wu is a mixture of N components (see (4)). Calculation of N particles from the exponential family proposal qu costs O (N). The step with the highest computational complexity consists in evaluating the particle weights in (4). In fact, evaluating the mixture representation of a message on a single point requires O (N), and we must calculate this for each of the N particles. Similarly, evaluating the estimator of faith on N sample points at the node u O (2) requires this to be reduced, since the algorithm still provides consistent estimators when we instead consider evaluating unbiased estimators of the messages. Since the messages have the form m-uv (xv) = N i = 1 w i-uv (xv), we can follow a method in which M indices are related to total weights."}, {"heading": "4 Experiments", "text": "We examine the performance of our MRF method using two simple diagrams. This allows us to compare the performance of EPBP with the performance of PBP in depth. We also illustrate the behavior of the sub-square version of EPBP. Finally, we show that EPBP delivers good results with a simple application for denocialization."}, {"heading": "4.1 Comparison with PBP", "text": "We start by comparing EPBP with PBP as implemented by Ihler et al. (Figure 1) with random variables that take values to R. The node and edge potentials are selected so that the marginal are multimodal, non-Gaussian and distorted approaches with {xu (xu) = \u03b11N (xu \u2212 yu; \u2212 2, 1) + \u03b12G (xu \u2212 yu; 2, 1.3), that the marginal ones are also multimodal, non-Gaussian and distorted views with {xu \u2212 xv), (14), where yu observes at a node u, N (x; \u00b5, 3) the views at a location (x2 / 2\u03c32) (density of a normal distribution), G (x; \u03b2), exp (\u2212 \u00b5) exp (\u2212 \u00b5) + exp (\u2212 \u00b5) / \u03b2 \"exp\") (density of a rubber distribution) and (\u03b2)."}, {"heading": "4.2 Sub-quadratic implementation and denoising application", "text": "As outlined in Section 3.1, when implementing EPBP, one can use an unbiased estimator of edge weights based on a drawing of M components from a multinomial. The complexity of the resulting algorithm is O (MN). We apply this method to the example of the 3 \u00b7 3 grid, in which M is selected as a roughly ordered log (N): i.e. for N = {10, 20, 50, 100, 200, 500} we select M = {5, 6, 8, 10, 11, 13}. The results are presented in Figure 6, where one can see that the N logN implementation is very well compared to the original quadratic implementation at greatly reduced cost. We apply this sub-quadratic method to a simple probability model for an image denosing problem. The aim of this example is to show that the method on larger graphs can still provide good results."}, {"heading": "5 Discussion", "text": "We have presented an original way to design adaptively efficient and easy-to-feel suggestions for a particle implementation of Loopy Belief Propagation. Our proposal is inspired by the framework of expectation propagation. We have shown empirically that the resulting algorithm is significantly faster and more accurate than an implementation of PBP, using the estimated beliefs as suggestions and evaluating them randomly using MCMC as suggested in [1]. It is also more precise than EP due to the non-parametric nature of the messages and provides consistent estimates of the LBP messages. A sub-square version of the method has also been outlined and showed to work almost as well as the original method, it has also been successfully applied in an image that, for example, denosifies. We believe that our method could be successfully applied to a wide range of applications, such as smoothing for hidden Markov models [11], computer vision [12] or [13]."}, {"heading": "Acknowledgments", "text": "We thank Alexander Ihler and Drew Frank for their joint implementation of Particle Belief Propagation. TL thanks the EPSRC and the European Scholarship Program Scatcherd. YWT's research that led to these results was supported by EPSRC (grant EP / K009362 / 1) and ERC within the EU FP7 program (grant agreement No. 617411). AD research was supported by EPSRC (grant EP / K000276 / 1, EP / K009850 / 1) and AFOSR / AOARD (grant AOARD-144042)."}], "references": [{"title": "Particle belief propagation", "author": ["Alexander T. Ihler", "David A. McAllester"], "venue": "In Proc. 12th AIS- TATS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Graphical models, exponential families, and variational inference", "author": ["Martin J. Wainwright", "Michael I. Jordan"], "venue": "Found. and Tr. in Mach. Learn.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Nonparametric belief propagation", "author": ["Erik B. Sudderth", "Alexander T. Ihler", "Michael Isard", "William T. Freeman", "Alan S. Willsky"], "venue": "Commun. ACM,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Probabilistic Reasoning in Intelligent Systems", "author": ["Judea Pearl"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1988}, {"title": "Constructing free energy approximations and generalized belief propagation algorithms", "author": ["Jonathan S. Yedidia", "William T. Freeman", "Yair Weiss"], "venue": "MERL Technical Report,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Nonparametric belief propagation", "author": ["Erik B. Sudderth", "Alexander T. Ihler", "William T. Freeman", "Alan S. Willsky"], "venue": "In Procs. IEEE Comp. Vis. Patt. Rec.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Expectation propagation for approximate Bayesian inference", "author": ["Thomas P. Minka"], "venue": "In Proc. 17th UAI,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Loopy belief propagation for approximate inference: an empirical study", "author": ["Kevin P. Murphy", "Yair Weiss", "Michael I. Jordan"], "venue": "In Proc. 15th UAI,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1999}, {"title": "Thresholding implied by truncated quadratic regularization", "author": ["Mila Nikolova"], "venue": "IEEE Trans. Sig. Proc.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2000}, {"title": "Sequential auxiliary particle belief propagation", "author": ["Mark Briers", "Arnaud Doucet", "Sumeetpal S. Singh"], "venue": "In Proc. 8th ICIF,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Smoothing algorithms for state-space models", "author": ["M. Briers", "A. Doucet", "S. Maskell"], "venue": "Ann. Inst. Stat. Math.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Visual hand tracking using nonparametric belief propagation", "author": ["Erik B. Sudderth", "Michael I. Mandel", "William T. Freeman", "Alan S. Willsky"], "venue": "In Procs. IEEE Comp. Vis. Patt. Rec.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Sequential monte carlo for graphical models", "author": ["Lindsten Fredrik Sch\u00f6n Naesseth", "Christian A"], "venue": "In NIPS", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "We demonstrate that it provides more accurate results than the Particle Belief Propagation (PBP) algorithm of [1] at a fraction of the computational cost and is additionally more robust empirically.", "startOffset": 110, "endOffset": 113}, {"referenceID": 1, "context": "Undirected Graphical Models (also known as Markov Random Fields) provide a flexible framework to represent networks of random variables and have been used in a large variety of applications in machine learning, statistics, signal processing and related fields [2].", "startOffset": 260, "endOffset": 263}, {"referenceID": 0, "context": "For many applications such as tracking, sensor networks or imaging [1, 3], it can be beneficial to define MRF on continuous state-spaces.", "startOffset": 67, "endOffset": 73}, {"referenceID": 2, "context": "For many applications such as tracking, sensor networks or imaging [1, 3], it can be beneficial to define MRF on continuous state-spaces.", "startOffset": 67, "endOffset": 73}, {"referenceID": 3, "context": "A popular approach to do this is to consider the Loopy Belief Propagation (LBP) algorithm [4, 5, 2].", "startOffset": 90, "endOffset": 99}, {"referenceID": 4, "context": "A popular approach to do this is to consider the Loopy Belief Propagation (LBP) algorithm [4, 5, 2].", "startOffset": 90, "endOffset": 99}, {"referenceID": 1, "context": "A popular approach to do this is to consider the Loopy Belief Propagation (LBP) algorithm [4, 5, 2].", "startOffset": 90, "endOffset": 99}, {"referenceID": 5, "context": "The Nonparametric Belief Propagation (NBP) algorithm [6] represents the messages with mixtures of Gaussians while the Particle Belief Propagation (PBP) algorithm [1] uses an importance sampling approach.", "startOffset": 53, "endOffset": 56}, {"referenceID": 0, "context": "The Nonparametric Belief Propagation (NBP) algorithm [6] represents the messages with mixtures of Gaussians while the Particle Belief Propagation (PBP) algorithm [1] uses an importance sampling approach.", "startOffset": 162, "endOffset": 165}, {"referenceID": 0, "context": "Practically the authors of [1] only sample approximately from them using short MCMC runs, leading to biased estimators.", "startOffset": 27, "endOffset": 30}, {"referenceID": 6, "context": "tion framework [7].", "startOffset": 15, "endOffset": 18}, {"referenceID": 0, "context": "The method is empirically shown to yield better approximations to the LBP beliefs than the implementation suggested in [1], at a much reduced computational cost, and than EP.", "startOffset": 119, "endOffset": 122}, {"referenceID": 3, "context": "This algorithm is a fixed point iteration scheme yielding approximations called the beliefs at each node [4, 2].", "startOffset": 105, "endOffset": 111}, {"referenceID": 1, "context": "This algorithm is a fixed point iteration scheme yielding approximations called the beliefs at each node [4, 2].", "startOffset": 105, "endOffset": 111}, {"referenceID": 7, "context": "However, even in these cases, LBP has been shown to provide good approximations in a wide range of situations [8, 5].", "startOffset": 110, "endOffset": 116}, {"referenceID": 4, "context": "However, even in these cases, LBP has been shown to provide good approximations in a wide range of situations [8, 5].", "startOffset": 110, "endOffset": 116}, {"referenceID": 5, "context": "In Nonparametric BP (NBP) [6], the messages are represented by mixtures of Gaussians.", "startOffset": 26, "endOffset": 29}, {"referenceID": 2, "context": "These conditions do however not hold in a number of important cases as acknowledged in [3].", "startOffset": 87, "endOffset": 90}, {"referenceID": 8, "context": "Similarly, in imaging applications for example, the edge potential can encode similarity between pixels which also need not verify the integrability condition as in [9].", "startOffset": 165, "endOffset": 168}, {"referenceID": 0, "context": "Particle BP (PBP) [1] offers a way to overcome the shortcomings of NBP: the authors also consider importance sampling to tackle the update of the messages but without fitting a mixture of Gaussians.", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "Further, the development of our method also formally shows that considering proposals close to the beliefs, as suggested by [1], is a good idea.", "startOffset": 124, "endOffset": 127}, {"referenceID": 6, "context": "However it is important that the proposal distribution is constructed adaptively, taking into account evidence collected through the message passing itself, and we propose to achieve this by using proposal distributions lying in a tractable exponential family, and adapted using the Expectation Propagation (EP) framework [7].", "startOffset": 322, "endOffset": 325}, {"referenceID": 0, "context": "thus supporting the claim in [1] that a good proposal to use is the current estimate of the node belief.", "startOffset": 29, "endOffset": 32}, {"referenceID": 6, "context": "Using the framework of expectation propogation (EP) [7], we can iteratively find good exponential family approximations as follows.", "startOffset": 52, "endOffset": 55}, {"referenceID": 6, "context": "in the Gaussian case that the optimal \u03b7 has a negative variance, we simply revert \u03b7wu to its previous value [7].", "startOffset": 108, "endOffset": 111}, {"referenceID": 9, "context": "Since the messages have the form m\u0302uv(xv) = \u2211N i=1 w i uv\u03c8 i uv(xv), we can follow a method presented in [10] where one draws M indices {i`}`=1 from a multinomial with weights {wi uv}i=1 and evaluates the corresponding M components \u03c8 ? ` uv .", "startOffset": 105, "endOffset": 109}, {"referenceID": 0, "context": "PBP as presented in [1] is implemented using the same parameters than those in an implementation code provided by the authors: the proposal on each node is the last estimated belief and sampled with a 20-step MCMC chain, the MH proposal is a normal distribution.", "startOffset": 20, "endOffset": 23}, {"referenceID": 0, "context": "We have demonstrated empirically that the resulting algorithm is significantly faster and more accurate than an implementation of PBP using the estimated beliefs as proposals and sampling from them using MCMC as proposed in [1].", "startOffset": 224, "endOffset": 227}, {"referenceID": 10, "context": "We believe that our method could be applied successfully to a wide range of applications such as smoothing for Hidden Markov Models [11], tracking or computer vision [12, 13].", "startOffset": 132, "endOffset": 136}, {"referenceID": 11, "context": "We believe that our method could be applied successfully to a wide range of applications such as smoothing for Hidden Markov Models [11], tracking or computer vision [12, 13].", "startOffset": 166, "endOffset": 174}, {"referenceID": 12, "context": "[15].", "startOffset": 0, "endOffset": 4}], "year": 2015, "abstractText": "We propose an original particle-based implementation of the Loopy Belief Propagation (LPB) algorithm for pairwise Markov Random Fields (MRF) on a continuous state space. The algorithm constructs adaptively efficient proposal distributions approximating the local beliefs at each note of the MRF. This is achieved by considering proposal distributions in the exponential family whose parameters are updated iterately in an Expectation Propagation (EP) framework. The proposed particle scheme provides consistent estimation of the LBP marginals as the number of particles increases. We demonstrate that it provides more accurate results than the Particle Belief Propagation (PBP) algorithm of [1] at a fraction of the computational cost and is additionally more robust empirically. The computational complexity of our algorithm at each iteration is quadratic in the number of particles. We also propose an accelerated implementation with sub-quadratic computational complexity which still provides consistent estimates of the loopy BP marginal distributions and performs almost as well as the original procedure.", "creator": "LaTeX with hyperref package"}}}