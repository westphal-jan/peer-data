{"id": "1506.06081", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jun-2015", "title": "A Convergent Gradient Descent Algorithm for Rank Minimization and Semidefinite Programming from Random Linear Measurements", "abstract": "We propose a simple, scalable, and fast gradient descent algorithm to optimize a nonconvex objective for the rank minimization problem and a closely related family of semidefinite programs. With $O(r^2 \\kappa^2 n \\log n)$ random measurements of a positive semidefinite $n \\times n$ matrix of rank $r$ and condition number $\\kappa$, our method is guaranteed to converge linearly to the global optimum.", "histories": [["v1", "Fri, 19 Jun 2015 16:41:08 GMT  (94kb,D)", "https://arxiv.org/abs/1506.06081v1", null], ["v2", "Thu, 8 Oct 2015 22:46:11 GMT  (95kb,D)", "http://arxiv.org/abs/1506.06081v2", "Sample complexity updated. Accepted to NIPS 2015"], ["v3", "Thu, 24 Mar 2016 16:27:51 GMT  (95kb,D)", "http://arxiv.org/abs/1506.06081v3", "Fix a minor error in Appendix E"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["qinqing zheng", "john d lafferty"], "accepted": true, "id": "1506.06081"}, "pdf": {"name": "1506.06081.pdf", "metadata": {"source": "CRF", "title": "A Convergent Gradient Descent Algorithm for Rank Minimization and Semidefinite Programming from Random Linear Measurements", "authors": ["Qinqing Zheng", "John Lafferty"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight."}, {"heading": "2 Semidefinite Programming and Rank Minimization", "text": "Before repeating our work and presenting our algorithm, we pause to explain the link between semidefinitive programming and rank minimization, which allows us to apply and analyze our scalable gradient descendant algorithm for certain classes of SDPs. If we look at a standard form of semidefinitive programs X-0tr (C-X) with respect to tr (A-iX) = bi, i = 1,.., m (3), where C-X-1,.., A-m-DP (C-X) is positively defined, then we can write C-L > where L-Rn \u00b7 n is unchangeable. Consequently, the minimum of problem (3) is the same asmin X-1-tr (X) with respect to tr (AiX) = bi, i = 1,.,., m (4), where Ai-L \u2212 1A-iL \u2212 1 >."}, {"heading": "3 Related Work", "text": "In fact, it is the case that most of them are able to abide by the rules that they have imposed on themselves, and that they are able to abide by the rules that they have imposed on themselves. (...) Most of them are able to understand the rules. (...) Most of them are not able to understand the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to understand the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...)"}, {"heading": "4 A Gradient Descent Algorithm for Rank Minimization", "text": "Our method is described in algorithms 1 (z). It is parallel to Wirtinger Flow (WF) = > i > a > i = > x = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \u00b7 = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "5 Convergence Analysis", "text": "In this section, we present our main result, which analyzes the gradient algorithm, and give a sketch of the proof. To begin, it should be noted that the symmetrical composition of X-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z"}, {"heading": "6 Experiments", "text": "In this section we report on the results of experiments with synthetic data sets. We compare our gradient descend algorithm with nuclear standardization relief, SVP and AltMinSense, for which we drop the positive semi-definition constraint justified by the observation in Section 2. We use ADMM for nuclear standardization, based on the blending algorithm in Tomioka et al. [19]; see Appendix G. For simplicity, we assume that AltMinSense, SVP and the gradient scheme know the true rank. Krylov subspace techniques such as the Lanczos method could be used to calculate partial self-decomposition; we use the randomized algorithm from Halko et al. [9] to calculate the low-rank SVD. All methods are implemented in MATLAB and the experiments were performed on a MacBook Pro with 2.5 GHz Intel processor i7 and 16 GB memory."}, {"heading": "6.1 Computational Complexity", "text": "It is instructive to compare the cost per iteration of the various approaches; see Table 1. Let us assume that the density (fraction of entries is non-zero) of each Ai \u03c1. For AltMinSense, the cost of solving the problem of the smallest squares is O (mn2r2 + n3r3 + mn2r\u03c1). The other three methods have O (mn2\u03c1) costs for calculating affinity transformation. For the nuclear standard approach, the O (n3) costs come from the SVD and the O (m2) costs are due to the update of the dual variables.The gradient scheme requires 2n2r operations to calculate ZkZk > and multiply Zk by n \u00d7 n matrix to obtain the gradient.SVP needs O (n2r) operations to calculate the uppermost r singular vectors.However, in practice, this partial cost is more expensive than the 2nr in SVD."}, {"heading": "6.2 Runtime Comparison", "text": "In the first scenario, we randomly generate a 400 x 400-400-rank-2 matrix X? = xx > + yy > where x, y and N (0, I). We also generate m = 6n matrices A1,.., Bin from the GOE and then take b = A (X?). We test the relative error measured in the Frobenius standard, defined as \"X-X-X-X-X-X-X-X.\" For the nuclear standard approach, we set the regulation parameter to \u03bb = 10 \u2212 5. We test three values \u03b7 = 10, 100, 200 for the penalty parameter and select target value = 100, as it leads to the fastest convergence. Similarly, we evaluate the three values 5 x-5, 10 \u2212 5, 10 \u2212 4, 2 \u2212 4 for the general step size, we select SVP = 10 and SVP = 4 \u2212 1."}, {"heading": "6.3 Sample Complexity", "text": "We also evaluate the number of measurements required for each method to accurately restore X? what we call sample complexity. A solution with a relative error of less than 10 \u2212 5 is considered successful. We perform 40 experiments and calculate the empirical probability of successful recovery. We consider cases where n = 60 or 100 and X? rank first or second. Results are shown in Figure 2c. For SDP and our approach, phase transitions occur by m = 1.5 n when X? ranks first and m = 2.5 n when X? ranks second. In any case, this scale corresponds to the number of degrees of freedom; this confirms that the sample complexity scale is linear with rank r. Phase transitions represent the approach of the core standard. Results suggest that the sample for the complexity scale (SDP) should also be recorded on the scale 11 and rn on the core standard scale."}, {"heading": "7 Conclusion", "text": "We are combining a specific case of affective rank minimization with a class of semi-defined programs with random constraints. Building on a recently proposed first-order phase query algorithm [6], we are developing a method of rank minimization in gradient lineage and establishing convergence to the optimal solution with O (r3n log n) measurements. We suspect that O (rn log n) measurements are sufficient to bring the method to convergence, and that conditions on the Ai sample matrices can be significantly attenuated. More generally, the technique used in this paper - factoring the semi-defined matrix variables, reformulation of convex optimization as non-convex optimization, and application of first-order algorithms - first proposed by Burer and Monteiro [4] may be effective for a much broader class of SDPs, and merit further investigation."}, {"heading": "Acknowledgements", "text": "Research supported in part by the NSF grant IIS-1116730 and the ONR grant N00014-12-1-0762. The authors thank Afonso Bandeira, Ryota Tomioka and the authors of Tu et al. [21] for their helpful comments on this work."}, {"heading": "A Proof of Lemma 1", "text": "Let's say A = (aij) is a random matrix distributed GOE, so aij (0, 1) is for i = j and aii (0, 2), we have E (M) = 1 E (z? s > Az? s) A), so it is enough to show that E (x > Ax) A) = 2xx > for each x-R (x > Ax), the (i, j) input of (x > Ax) A awaits valuesE (x > Ax) aij) = E (xxlaklaij) = 2xx."}, {"heading": "C Linear Convergence", "text": "Evidence of the theorem 3Let Hk = Zk \u2212 Zk. Then we have that \"Zk + 1 \u2212 Zk\" 2 F = \"Zk\" 2 F = \"Zk\" 2 F \"2 F\" 2F \"(Zk) \u2212 Zk\" 2F \"2F\" 2F \"2F\" 2F \"2F\" 2F \"2F\" 2F \"2F\" 2F \"2F\" 2F \"2F\" 2F \"2F\" 2F \"2F\" 2F \"2F\" 2F \"2F\" 2F \"2F\" 2F \"2F\" 2F \"2F\" 2F \"2F\" 2F \"2F\" 2F \"2F\" 2F \"2F\" 2F \"2F\" 2F \"(Zk) = (1 \u2212 2K\" l \"s\" s \"2F\" s \"s\" s \"2F\" 2F \"(Zk\" s \"s) 2F (Zk\" s \"s\" s) 2F (2F \"s\" s \"s\" s \"s\" 2F \"s) 2F (Zk\" s \"s\" s \"s\" s \"2F\" s) 2F (2F (Zk \"s)."}, {"heading": "D Regularity Condition", "text": "As already mentioned, Nesterov [16, Theorem 2.1.11] shows that the gradient schema = > Q = > Q = > Q = > Q = > Q = > Q = > Q = > Q = > Q = > Q = > Q = > Q = > Q = > Q = > H \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212"}, {"heading": "F Sample Complexity", "text": "In this section we find that our assumptions are highly likely to be correct if we rely on a constant that depends on the size, size and size of the matrices. \u2212 Our assumptions are based on the following disparity in concentration. \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 / \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 / \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 / \u2212 \u2212 \u2212 \u2212 / \u2212 \u2212 / \u2212 \u2212 \u2212 / \u2212 / \u2212 \u2212 \u2212 / \u2212 / \u2212 / \u2212 \u2212 / \u2212 / \u2212 / \u2212 / \u2212 \u2212 / \u2212 / \u2212 / \u2212 / \u2212 / \u2212 / \u2212 / \u2212 / \u2212 / \u2212 / \u2212 / \u2212 / \u2212 / \u2212 / \u2212 / \u2212 / \u2212 / \u2212 / \u2212 / \u2212 / \u2212 / \u2212 / \u2212 / \u2212 / \u2212 / \u2212 / \u2212 / \u2212 / \u2212 / \u2212 / \u2212 / \u2212 / \u2212 /"}, {"heading": "G ADMM for Nuclear Norm Minimization", "text": "We reformulate the nuclear standard so that the problem is minimized asmin X-Rn-n-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p)."}], "references": [{"title": "High-dimensional analysis of semidefinite relaxations for sparse principal components", "author": ["Arash A. Amini", "Martin J. Wainwright"], "venue": "The Annals of Statistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Adaptivity of averaged stochastic gradient descent to local strong convexity for logistic regression", "author": ["Francis Bach"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Non-asymptotic analysis of stochastic approximation algorithms for machine learning", "author": ["Francis Bach", "Eric Moulines"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization", "author": ["Samuel Burer", "Renato DC Monteiro"], "venue": "Mathematical Programming,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "A singular value thresholding algorithm for matrix completion", "author": ["Jian-Feng Cai", "Emmanuel J Cand\u00e8s", "Zuowei Shen"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1956}, {"title": "Phase retrieval via wirtinger flow: Theory and algorithms", "author": ["Emmanuel Cand\u00e8s", "Xiaodong Li", "Mahdi Soltanolkotabi"], "venue": "arXiv preprint arXiv:1407.1065,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "A direct formulation for sparse PCA using semidefinite programming", "author": ["A. d\u2019Aspremont", "L. El Ghaoui", "M.I. Jordan", "G. Lanckriet"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming", "author": ["Michel X. Goemans", "David P. Williamson"], "venue": "Journal of the ACM,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1995}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["Nathan Halko", "Per-Gunnar Martinsson", "Joel A Tropp"], "venue": "SIAM review,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Stochastic variational inference", "author": ["Matt Hoffman", "David M. Blei", "Chong Wang", "John Paisley"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Guaranteed rank minimization via singular value projection", "author": ["Prateek Jain", "Raghu Meka", "Inderjit S Dhillon"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Low-rank matrix completion using alternating minimization", "author": ["Prateek Jain", "Praneeth Netrapalli", "Sujay Sanghavi"], "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Adaptive estimation of a quadratic functional by model selection", "author": ["Beatrice Laurent", "Pascal Massart"], "venue": "Annals of Statistics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2000}, {"title": "Small deviations for beta ensembles", "author": ["Michel Ledoux", "Brian Rider"], "venue": "Electron. J. Probab.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Rank minimization via online learning", "author": ["Raghu Meka", "Prateek Jain", "Constantine Caramanis", "Inderjit S Dhillon"], "venue": "In Proceedings of the 25th International Conference on Machine learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Introductory lectures on convex optimization, volume 87", "author": ["Yurii Nesterov"], "venue": "Springer Science & Business Media,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "Phase retrieval using alternating minimization", "author": ["Praneeth Netrapalli", "Prateek Jain", "Sujay Sanghavi"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization", "author": ["Benjamin Recht", "Maryam Fazel", "Pablo A Parrilo"], "venue": "SIAM review,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Estimation of low-rank tensors via convex optimization", "author": ["Ryota Tomioka", "Kohei Hayashi", "Hisashi Kashima"], "venue": "arXiv preprint arXiv:1010.0789,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "An introduction to matrix concentration inequalities", "author": ["Joel A Tropp"], "venue": "arXiv preprint arXiv:1501.01571,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "SDPs often arise naturally from the problem structure, or are derived as surrogate optimizations that are relaxations of difficult combinatorial problems [7, 1, 8].", "startOffset": 154, "endOffset": 163}, {"referenceID": 0, "context": "SDPs often arise naturally from the problem structure, or are derived as surrogate optimizations that are relaxations of difficult combinatorial problems [7, 1, 8].", "startOffset": 154, "endOffset": 163}, {"referenceID": 7, "context": "SDPs often arise naturally from the problem structure, or are derived as surrogate optimizations that are relaxations of difficult combinatorial problems [7, 1, 8].", "startOffset": 154, "endOffset": 163}, {"referenceID": 2, "context": "In many areas of machine learning and signal processing such as classification, deep learning, and phase retrieval, gradient descent methods, in particular first order stochastic optimization, have led to remarkably efficient algorithms that can attack very large scale problems [3, 2, 10, 6].", "startOffset": 279, "endOffset": 292}, {"referenceID": 1, "context": "In many areas of machine learning and signal processing such as classification, deep learning, and phase retrieval, gradient descent methods, in particular first order stochastic optimization, have led to remarkably efficient algorithms that can attack very large scale problems [3, 2, 10, 6].", "startOffset": 279, "endOffset": 292}, {"referenceID": 9, "context": "In many areas of machine learning and signal processing such as classification, deep learning, and phase retrieval, gradient descent methods, in particular first order stochastic optimization, have led to remarkably efficient algorithms that can attack very large scale problems [3, 2, 10, 6].", "startOffset": 279, "endOffset": 292}, {"referenceID": 5, "context": "In many areas of machine learning and signal processing such as classification, deep learning, and phase retrieval, gradient descent methods, in particular first order stochastic optimization, have led to remarkably efficient algorithms that can attack very large scale problems [3, 2, 10, 6].", "startOffset": 279, "endOffset": 292}, {"referenceID": 17, "context": "This problem is a direct generalization of compressed sensing, and subsumes many machine learning problems such as image compression, low rank matrix completion and low-dimensional metric embedding [18, 12].", "startOffset": 198, "endOffset": 206}, {"referenceID": 11, "context": "This problem is a direct generalization of compressed sensing, and subsumes many machine learning problems such as image compression, low rank matrix completion and low-dimensional metric embedding [18, 12].", "startOffset": 198, "endOffset": 206}, {"referenceID": 14, "context": "Without conditions on the transformation A or the minimum rank solution X, it is generally NP hard [15].", "startOffset": 99, "endOffset": 103}, {"referenceID": 17, "context": "Existing methods, such as nuclear norm relaxation [18], singular value projection (SVP) [11], and alternating least squares (AltMinSense) [12], assume that a certain restricted isometry property (RIP) holds for A.", "startOffset": 50, "endOffset": 54}, {"referenceID": 10, "context": "Existing methods, such as nuclear norm relaxation [18], singular value projection (SVP) [11], and alternating least squares (AltMinSense) [12], assume that a certain restricted isometry property (RIP) holds for A.", "startOffset": 88, "endOffset": 92}, {"referenceID": 11, "context": "Existing methods, such as nuclear norm relaxation [18], singular value projection (SVP) [11], and alternating least squares (AltMinSense) [12], assume that a certain restricted isometry property (RIP) holds for A.", "startOffset": 138, "endOffset": 142}, {"referenceID": 17, "context": "In the random measurement setting, this essentially means that at least O(r(n + p) log(n + p)) measurements are available, where r = rank(X) [18].", "startOffset": 141, "endOffset": 145}, {"referenceID": 5, "context": "[6], and develop a gradient descent algorithm for optimizing f(Z), using a carefully constructed initialization and step size.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "3 Related Work Burer and Monteiro [4] proposed a general approach for solving semidefinite programs using factored, nonconvex optimization, giving mostly experimental support for the convergence of the algorithms.", "startOffset": 34, "endOffset": 37}, {"referenceID": 17, "context": "[18], based on replacing the rank function by the convex surrogate nuclear norm, as already mentioned in the previous section.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "The most popular algorithms are proximal methods that perform singular value thresholding [5] at every iteration.", "startOffset": 90, "endOffset": 93}, {"referenceID": 10, "context": "[11] proposed a projected gradient descent algorithm SVP (Singular Value Projection) that solves min X\u2208Rn\u00d7p \u2016A(X)\u2212 b\u2016 subject to rank(X) \u2264 r, where \u2016\u00b7\u2016 is the `2 vector norm and r is the input rank.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] proposes an alternating least squares algorithm AltMinSense that avoids the per-iteration SVD.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "It is parallel to the Wirtinger Flow (WF) algorithm for phase retrieval [6], to recover a complex vector x \u2208 C given the squared magnitudes of its linear measurements bi = |\u3008ai, x\u3009|, i \u2208 [m], where a1, .", "startOffset": 72, "endOffset": 75}, {"referenceID": 5, "context": "[6] propose a first-order method to minimize the sum of squared residuals", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "To obtain a sufficiently accurate initialization, we use a spectral method, similar to those used in [17, 6].", "startOffset": 101, "endOffset": 108}, {"referenceID": 5, "context": "To obtain a sufficiently accurate initialization, we use a spectral method, similar to those used in [17, 6].", "startOffset": 101, "endOffset": 108}, {"referenceID": 15, "context": "In the analysis of convex functions, Nesterov [16] shows that for unconstrained optimization, the gradient descent", "startOffset": 46, "endOffset": 50}, {"referenceID": 0, "context": "The underlying truth is Z = [1, 1]>.", "startOffset": 28, "endOffset": 34}, {"referenceID": 0, "context": "The underlying truth is Z = [1, 1]>.", "startOffset": 28, "endOffset": 34}, {"referenceID": 15, "context": "This provides a local regularity property that is similar to the Nesterov [16] criteria that the objective function is strongly convex and has a Lipschitz continuous gradient.", "startOffset": 74, "endOffset": 78}, {"referenceID": 18, "context": "[19]; see Appendix G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[9] to compute the low rank SVD.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "The results suggest that the sample complexity of our method should also scale as O(rn log n) as for SVP and the nuclear norm approach [11, 18].", "startOffset": 135, "endOffset": 143}, {"referenceID": 17, "context": "The results suggest that the sample complexity of our method should also scale as O(rn log n) as for SVP and the nuclear norm approach [11, 18].", "startOffset": 135, "endOffset": 143}, {"referenceID": 5, "context": "Building on a recently proposed first-order algorithm for phase retrieval [6], we develop a gradient descent procedure for rank minimization and establish convergence to the optimal solution with O(rn log n) measurements.", "startOffset": 74, "endOffset": 77}, {"referenceID": 3, "context": "More broadly, the technique used in this paper\u2014factoring the semidefinite matrix variable, recasting the convex optimization as a nonconvex optimization, and applying first-order algorithms\u2014first proposed by Burer and Monteiro [4], may be effective for a much wider class of SDPs, and deserves further study.", "startOffset": 227, "endOffset": 230}, {"referenceID": 0, "context": "References [1] Arash A.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Francis Bach.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Francis Bach and Eric Moulines.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Samuel Burer and Renato DC Monteiro.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Jian-Feng Cai, Emmanuel J Cand\u00e8s, and Zuowei Shen.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Emmanuel Cand\u00e8s, Xiaodong Li, and Mahdi Soltanolkotabi.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Michel X.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Matt Hoffman, David M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Prateek Jain, Raghu Meka, and Inderjit S Dhillon.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Beatrice Laurent and Pascal Massart.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Michel Ledoux and Brian Rider.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Raghu Meka, Prateek Jain, Constantine Caramanis, and Inderjit S Dhillon.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Yurii Nesterov.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Praneeth Netrapalli, Prateek Jain, and Sujay Sanghavi.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Benjamin Recht, Maryam Fazel, and Pablo A Parrilo.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Ryota Tomioka, Kohei Hayashi, and Hisashi Kashima.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Joel A Tropp.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "(Matrix Bernstein Inequality [20]) Let S1, .", "startOffset": 29, "endOffset": 33}, {"referenceID": 12, "context": "By the corollary of Lemma 1 in Laurent and Massart [13], we have P(|a11 \u2212 2| > 4( \u221a n+ n)) \u2264 2e\u2212n.", "startOffset": 51, "endOffset": 55}], "year": 2016, "abstractText": "We propose a simple, scalable, and fast gradient descent algorithm to optimize a nonconvex objective for the rank minimization problem and a closely related family of semidefinite programs. WithO(r3\u03ba2n log n) random measurements of a positive semidefinite n\u00d7nmatrix of rank r and condition number \u03ba, our method is guaranteed to converge linearly to the global optimum.", "creator": "LaTeX with hyperref package"}}}