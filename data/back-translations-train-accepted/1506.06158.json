{"id": "1506.06158", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jun-2015", "title": "Structured Training for Neural Network Transition-Based Parsing", "abstract": "We present structured perceptron training for neural network transition-based dependency parsing. We learn the neural network representation using a gold corpus augmented by a large number of automatically parsed sentences. Given this fixed network representation, we learn a final layer using the structured perceptron with beam-search decoding. On the Penn Treebank, our parser reaches 94.26% unlabeled and 92.41% labeled attachment accuracy, which to our knowledge is the best accuracy on Stanford Dependencies to date. We also provide in-depth ablative analysis to determine which aspects of our model provide the largest gains in accuracy.", "histories": [["v1", "Fri, 19 Jun 2015 21:05:01 GMT  (2714kb,D)", "http://arxiv.org/abs/1506.06158v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["david weiss", "chris alberti", "michael collins", "slav petrov"], "accepted": true, "id": "1506.06158"}, "pdf": {"name": "1506.06158.pdf", "metadata": {"source": "CRF", "title": "Structured Training for Neural Network Transition-Based Parsing", "authors": ["David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov"], "emails": ["djweiss@google.com", "chrisalberti@google.com", "mjcollins@google.com", "slav@google.com"], "sections": [{"heading": "1 Introduction", "text": "Lately, it has been shown that most of them are people who are able to survive themselves, not people who are able to survive themselves. First, they are people who are able to survive themselves. Second, they are people who are able to survive themselves. Second, they are people who are able to survive themselves. Second, they are people who are able to survive themselves. Third, they are people who are able to survive themselves. Third, they are people who are able to survive themselves. Third, they are people who are able to survive themselves. Third, they are people who are able to survive themselves. Third, they are people who are able to survive themselves."}, {"heading": "2 Neural Network Model", "text": "In this section we describe the architecture of our model, which is summarized in Figure 1. Note that for clarity we divide the embedding processing into its own \"embedding layer.\" Our model is based on that of Chen and Manning (2014) and we discuss the differences between our model and theirs in detail at the end of this section. We use the transition system of the sheet standard (Nivre, 2004)."}, {"heading": "2.1 Input layer", "text": "Following Chen and Manning (2014), we group these characteristics according to their input source: words, POS tags, and arc labels. Characteristics extracted for each group are represented as a sparse F x V matrix X, where V is the size of the vocabulary of the attribute group and F is the number of attributes. The value of the element X f v is 1 when the f'te property takes on a value. We create three input matrices: Xword for word attributes, Xtag for POS attributes, and Xlabel for arc labels, where Fword = Ftag = 20 and Bottle = 12 (Figure 1). For all attribute groups, we add additional special values for \"ROOT\" (indicating the vocabulary or the word of the root character), \"NULL\" (indicating that there may be no valid attribute or an uncalculated element)."}, {"heading": "2.2 Embedding layer", "text": "The first learned level h0 in the network transforms the sparse, discrete characteristics X into a dense, continuous embedded representation. For each characteristic group Xg we learn a Vg \u00b7 Dg embedding matrix Eg, which applies the conversion: h0 = [XgEg | g-word, tag, label}], (1) where we apply the calculation for each group g separately and link the results. Thus, the embedding layer E = \u2211 g has FgDg output, which we convert to a vector h0. We can freely choose the embedding dimensionality D for each group. Since POS tags and arc labels have much smaller vocabularies, we show in our experiments (Section 5.1) that we can use smaller Dtags and Dlabels without any loss of accuracy."}, {"heading": "2.3 Hidden layers", "text": "We experimented with one and two hidden layers consisting of linear (Relu) units with M-rectification (Nair and Hinton, 2010), each unit in the hidden layers is fully connected to the previous layer: hi = max {0, Wihi \u2212 1 + bi}, (2) where W1 is an M1 \u00d7 E weight matrix for the first hidden layer, and Wi matrices with Wi \u00b7 Wi \u2212 1 for all subsequent layers. Weights bi are bias terms. Relu layers have been well studied in the literature for neural networks and have been shown to work well for a wide range of problems (Krizhevsky et al., 2012; Zeiler et al., 2013)."}, {"heading": "2.4 Relationship to Chen and Manning (2014)", "text": "Our model is clearly inspired and based on the work of Chen and Manning (2014). There are a few structural differences: (1) we allow for much smaller embedding of POS tags and labels, (2) we use relay units in our hidden layers, and (3) we use a deeper model with two hidden layers. To our surprise, we found these changes combined with an SGD training program (Section 3.1) during the \"pre-training phase\" of the model, which resulted in an accuracy gain of almost 1% over Chen and Manning (2014). This trend persisted despite careful coordination of hyperparameters for each training method and structure combination.Our most important contribution from an algorithmic perspective is our training method: As described in the next section, we use the structured perceptron to learn the final layer of our model."}, {"heading": "3 Semi-Supervised Structured Learning", "text": "In this thesis, we examine a semi-supervised structured learning scheme that yields significant improvements in accuracy compared to the baseline neural network model. There are two complementary contributions to our approach: (1) the inclusion of structured learning of the model and (2) the use of unmarked data. In both cases, we use the neural network to model the probability of each parsing action y as a soft-max function, using the last hidden layer as input: P (y) exp {\u03b2 > y hi + by}, (3) where \u03b2y is a Mi-dimensional vector of weights for class y and i is the index of the last hidden layer of the network. At a high level, our approach can be summarized as follows: \u2022 First, we practice the hidden representations of the network by learning the probabilities of parsing actions. By combating the hidden representations, we learn an additional final output layer of the network using this structured layer under the precision layer, which we can use the 0.6% accuracy of the output."}, {"heading": "3.1 Backpropagation Pretraining", "text": "To learn the hidden representations, we use mini-batched averaged stochastic gradient descent (ASGD) (Bottou, 2010) with momentum (Hinton, 2012) to learn the parameters \u044b of the network, using the reverse propagation to minimize the multinomic logistical loss: L (\u044b) = \u2212 \u2211 jlog P (y j | c j, \u044b) + \u03bb i | Wi | | 22, (4) where \u03bb is a regulation hyperparameter over the hidden layer parameters (we use \u03bb = 10 \u2212 4 in all experiments) and j sums over all decisions and configurations {y j, c j} extracted from gold parse trees in the database. The specific update rule that we apply in the iteration is the following: gt \u2212 1 \u2212 layer L (enhanced), (5) straightened + brightly weighted (6% weighted), with the dynamics not weighted with the previous 1 component."}, {"heading": "3.2 Structured Perceptron Training", "text": "The perceptron algorithm with early updates (Collins and Roark, 2004) requires a feature vector definition \u03c6, which maps a set x together with a configuration c to a feature vector \u03c6 (x, c). However, there is a one-to-one mapping between the configurations c and the decision sequence 1.. y j \u2212 1 for each integer j \u2265 1: we will use c and y1. y j \u2212 1 interchangeable. For a set x, we define GEN (x) as a set of parse trees for x. Each y \u2212 GEN (x) is a sequence of decisions y1... We use Y to represent the set of possible decisions in the parse model."}, {"heading": "3.3 Incorporating Unlabeled Data", "text": "Given the high capacity, non-linear nature of the deep network, we assume that our model can1If the gold parse tree stay within the bar until the end of the sentence, conventional perceptron updates are used.be significantly improved by incorporating more data. However, one way to use blank data is through unattended methods such as word clusters (Koo et al., 2008); we follow Chen and Manning (2014) and use pre-trained word embeddings to initialize our model. Word embeddings capture similar distribution information as word clusters and provide consistent improvements by providing good initialization and information about words not seen in the tree base data. However, obtaining more training data is even more important than good initialization. One potential way to obtain additional training data is to analyze non-described data with previously trained models. McClosky et al al al al al al al al al al al al. (2006) and Huang and Harper (2009) demonstrated that self-training can be used."}, {"heading": "4 Experiments", "text": "In this section we present our experimental setup and the most important results of our work."}, {"heading": "4.1 Experimental Setup", "text": "We run our experiments on two English benchmarks: (1) the Standard Wall Street Journal (WSJ) part of Penn Treebank (Marcus et al., 1993) and (2) a more comprehensive association of publicly available tree banks from several areas. For the WSJ experiments, we follow standard practice and use Sections 2-21 for training, Section 22 for development and Section 23 as the final test kit. Since there are many hyperparameters in our models, we also use Section 24 for voting. We convert the constituency trees into Stanford-style dependencies (De Marneffe et al., 2006) using version 3.0 of the converter. We use a CRF-based POS tagger to generate 5x jack-knifed POS tags on the training kit and predict tags on the Dev, Test and Tuning sets; our tagger gets a comparable accuracy on the Stanford POS tagger."}, {"heading": "4.2 Model Initialization & Hyperparameters", "text": "In all cases, we randomly initialized Wi and \u03b2 using a Gaussian distribution with variance 10 \u2212 4. We used fixed initialization with bi = 0.2 to ensure that most of the Relu units were activated during the first training rounds. We did not systematically compare this random scheme with others, but we found that it was sufficient for our purposes. We initialized the parameters for the Eword Embedding Matrix using pre-trained word embeddings. We used the publicly available tool word2vec2 (Mikolov et al., 2013) to learn CBOW embeddings according to the sample configuration provided with the tool. For words that did not appear in the unattended data and the special \"NULL\" etc. Tokens, we used random initialization. In preliminary experiments, we found no difference between the training of the word embeddings on 1 billion or 10 billion tokens. Therefore, we tracked the same word on the Correct."}, {"heading": "4.3 Results", "text": "Table 1 shows our final results on the WSJ test set, and Table 2 shows the cross-domain results of the Treebank Union. We compare with the best dependency savers in the literature. For (Chen and Manning, 2014) and (Dyer et al., 2015) we use reported results; the other baselines were developed by Bernd Bohnet using version 3.3.0 of Stanford dependencies and our predicted POS tags for all records we use to make comparisons as fair as possible. For WSJ and web tasks, our parser outperforms all dependency savers in our comparison by a significant margin. The Question (QTB) datasets are more sensitive to the smaller beam size we use to train the models in a reasonable time; if we only increase to B = 32 in the inference time, our perceptron parsor training does not achieve absolute accuracy with the SAS.As many of the baselines are not directly comparable to our Susemi-training, we will not be able to our supervised training again in 2011, and the ASvar will be highest."}, {"heading": "5 Discussion", "text": "In this section, we will examine the contribution of the various components of our approach through ablation studies and other systematic experiments. We will adjust to Section 24 and use Section 22 for comparisons so as not to pollute the official test set (Section 23). We will focus on the UAS as we have found that the LAS values are strongly correlated. Unless otherwise stated, we will use 200 hidden units in each shift to conduct more ablative experiments in a reasonable amount of time."}, {"heading": "5.1 Impact of Network Structure", "text": "In addition to initialization and hyperparameter setting, there are several additional decisions about model structure and size that a practitioner has in front of him when implementing a neural network model = 48. We examine these questions and justify the particular decisions we use below. Note that we do not use a beam for this analysis and therefore do not train the final perception layer, in order to reduce training times and because trends persist in all areas. \u2212 Reduction of variance with pre-formed embeddings. Since the learning problem is not convex, different initializations of the parameters result in different solutions to the learning problem. Therefore, for each given experiment, we have performed several random restarts for each setting of our hyperparameters and selected the model that is best executed with the provided melody. We found it important to allow the model to finish training early if the accuracy of the settings is reduced. We visualize the performance of 32 random restarts with one or two hidden layers and with no pre-formed results in the summary and 3."}, {"heading": "5.2 Impact of Structured Perceptron", "text": "To evaluate the effects of structured training, we compare the estimates P (y) from the neural network directly for beam search with the activations from all levels as characteristics in the structured perceptron. The use of the probability estimates is very similar to Ratnaparkhi (1997), where a maximum entropy model was used to model the distribution of possible actions in each parser state, and the beam search was used to search for the most likely analysis. A known problem in beam search in this environment is the label bias problem. Table 5 shows the effects of using structured perceptron training via the use of the Softmax function during beam search as a function of the beam size used. For reference, our Reimplementation of Zhang and Nivre (2011). We also show the effects on beam size when the beam layer is used for training."}, {"heading": "5.3 Impact of Tri-Training", "text": "To assess the effects of the tri-training approach, we compared up training with the BerkelyParser alone (Petrov et al., 2006), and the results are shown in Figure 4 for the greedy and perceptronic neural networks and for our newly implemented Zhang and Nivre (2011) baseline. In our neural network model, training on the output of the BerkeleyParser yields only modest gains, while training on the data where the two parsers match significantly better outcomes. This was particularly pronounced in the greedy models: after tri-training, the greedy neural network model outperforms the BerkeleyParser in accuracy. It is also interesting that the up training has improved the results far more than the tri-training for the baseline. We suspect that this is due to the lack of diversity of tri-training data for this model, as the same basic model with the BerkeleyParser was used for tricelling."}, {"heading": "5.4 Error Analysis", "text": "Notwithstanding the Tri-Training, the structured perceptron error rate improved by > 1% on some of the common and difficult labels: ROOT, ccomp, cc, conj, and nsubj. We examined the learned perceptron weights v for the Softmax probabilities P (y) (see appendix) and found that the perctron reweighted the Softmax probabilities based on common confusion, e.g. a strong negative weight for the RIGHT (ccomp) action in light of Softmax model output RIGHT (conj). However, this trend did not continue when \u03c6 (x, c) = [P (y)]; without the hidden layer, the perctron was unable to reweight the Softmax probabilities to account for the distortions of the greedy model."}, {"heading": "6 Conclusion", "text": "We introduced a new state of the art in dependency parsing: a transition-based neural network parser trained with structured perceptron and ASGD. We then combined this approach with unlabelled data and tri-training to further advance the state of the art in semi-supervised dependency parsing. Nevertheless, our ablative analysis suggests that further progress is possible simply by extending our system to even larger representations. In future work, we will apply our method to other languages, explore the end-to-end training of the system through structured learning, and extend the method to larger datasets and network structures."}, {"heading": "Acknowledgements", "text": "We would like to thank Bernd Bohnet for training his parsers and TurboParsers in our setup. This article benefited enormously from discussions with Ryan McDonald, Greg Coppola, Emily Pitler and Fernando Pereira. Finally, we would like to thank all members of the Google Parsing Team."}], "references": [{"title": "The best of both worlds: a graph-based completion model for transition-based parsers", "author": ["Bernd Bohnet", "Jonas Kuhn."], "venue": "Proc. EACL, pages 77\u2013", "citeRegEx": "Bohnet and Kuhn.,? 2012", "shortCiteRegEx": "Bohnet and Kuhn.", "year": 2012}, {"title": "Top accuracy and fast dependency parsing is not a contradiction", "author": ["Bernd Bohnet."], "venue": "Proc. COLING, pages 89\u201397.", "citeRegEx": "Bohnet.,? 2010", "shortCiteRegEx": "Bohnet.", "year": 2010}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L\u00e9on Bottou."], "venue": "Proc. COMPSTAT, pages 177\u2013186.", "citeRegEx": "Bottou.,? 2010", "shortCiteRegEx": "Bottou.", "year": 2010}, {"title": "Conll-x shared task on multilingual dependency parsing", "author": ["Sabine Buchholz", "Erwin Marsi."], "venue": "Proc. CoNLL, pages 149\u2013164.", "citeRegEx": "Buchholz and Marsi.,? 2006", "shortCiteRegEx": "Buchholz and Marsi.", "year": 2006}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["Ciprian Chelba", "Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn."], "venue": "CoRR.", "citeRegEx": "Chelba et al\\.,? 2013", "shortCiteRegEx": "Chelba et al\\.", "year": 2013}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D. Manning."], "venue": "Proc. EMNLP, pages 740\u2013750.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Semi-supervised feature transformation for dependency parsing", "author": ["Wenliang Chen", "Min Zhang", "Yue Zhang."], "venue": "Proc. 2013 EMNLP, pages 1303\u2013 1313.", "citeRegEx": "Chen et al\\.,? 2013", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Incremental parsing with the perceptron algorithm", "author": ["Michael Collins", "Brian Roark."], "venue": "Proc. ACL, Main Volume, pages 111\u2013118, Barcelona, Spain.", "citeRegEx": "Collins and Roark.,? 2004", "shortCiteRegEx": "Collins and Roark.", "year": 2004}, {"title": "Generating typed dependency parses from phrase structure parses", "author": ["Marie-Catherine De Marneffe", "Bill MacCartney", "Christopher D. Manning."], "venue": "Proc. LREC, pages 449\u2013454.", "citeRegEx": "Marneffe et al\\.,? 2006", "shortCiteRegEx": "Marneffe et al\\.", "year": 2006}, {"title": "Neural conditional random fields", "author": ["Trinh Minh Tri Do", "Thierry Artires."], "venue": "AISTATS, volume 9, pages 177\u2013184.", "citeRegEx": "Do and Artires.,? 2010", "shortCiteRegEx": "Do and Artires.", "year": 2010}, {"title": "Transitionbased dependency parsing with stack long shortterm memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "Proc. ACL.", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Discriminative training of a neural network statistical parser", "author": ["James Henderson."], "venue": "Proc. ACL, Main Volume, pages 95\u2013102.", "citeRegEx": "Henderson.,? 2004", "shortCiteRegEx": "Henderson.", "year": 2004}, {"title": "A practical guide to training restricted boltzmann machines", "author": ["Geoffrey E. Hinton."], "venue": "Neural Networks: Tricks of the Trade (2nd ed.), Lecture Notes in Computer Science, pages 599\u2013619. Springer.", "citeRegEx": "Hinton.,? 2012", "shortCiteRegEx": "Hinton.", "year": 2012}, {"title": "Ontonotes: The 90% solution", "author": ["Eduard Hovy", "Mitchell Marcus", "Martha Palmer", "Lance Ramshaw", "Ralph Weischedel."], "venue": "Proc. HLT-NAACL, pages 57\u2013", "citeRegEx": "Hovy et al\\.,? 2006", "shortCiteRegEx": "Hovy et al\\.", "year": 2006}, {"title": "Selftraining PCFG grammars with latent annotations across languages", "author": ["Zhongqiang Huang", "Mary Harper."], "venue": "Proc. 2009 EMNLP, pages 832\u2013841, Singapore.", "citeRegEx": "Huang and Harper.,? 2009", "shortCiteRegEx": "Huang and Harper.", "year": 2009}, {"title": "Questionbank: Creating a corpus of parseannotated questions", "author": ["John Judge", "Aoife Cahill", "Josef van Genabith."], "venue": "Proc. ACL, pages 497\u2013504.", "citeRegEx": "Judge et al\\.,? 2006", "shortCiteRegEx": "Judge et al\\.", "year": 2006}, {"title": "Simple semi-supervised dependency parsing", "author": ["Terry Koo", "Xavier Carreras", "Michael Collins."], "venue": "Proc. ACL-HLT, pages 595\u2013603.", "citeRegEx": "Koo et al\\.,? 2008", "shortCiteRegEx": "Koo et al\\.", "year": 2008}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton."], "venue": "Proc. NIPS, pages 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Ambiguity-aware ensemble training for semisupervised dependency parsing", "author": ["Zhenghua Li", "Min Zhang", "Wenliang Chen."], "venue": "Proc. ACL, pages 457\u2013467.", "citeRegEx": "Li et al\\.,? 2014", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz."], "venue": "Computational Linguistics, 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Turning on the turbo: Fast third-order nonprojective turbo parsers", "author": ["Andre Martins", "Miguel Almeida", "Noah A. Smith."], "venue": "Proc. ACL, pages 617\u2013 622.", "citeRegEx": "Martins et al\\.,? 2013", "shortCiteRegEx": "Martins et al\\.", "year": 2013}, {"title": "Effective self-training for parsing", "author": ["David McClosky", "Eugene Charniak", "Mark Johnson."], "venue": "Proc. HLT-NAACL, pages 152\u2013159.", "citeRegEx": "McClosky et al\\.,? 2006", "shortCiteRegEx": "McClosky et al\\.", "year": 2006}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "CoRR, abs/1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E. Hinton."], "venue": "Proc. 27th ICML, pages 807\u2013814.", "citeRegEx": "Nair and Hinton.,? 2010", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "The CoNLL 2007 shared task on dependency parsing", "author": ["Joakim Nivre", "Johan Hall", "Sandra K\u00fcbler", "Ryan McDonald", "Jens Nilsson", "Sebastian Riedel", "Deniz Yuret."], "venue": "Proc. CoNLL, pages 915\u2013932.", "citeRegEx": "Nivre et al\\.,? 2007", "shortCiteRegEx": "Nivre et al\\.", "year": 2007}, {"title": "Incrementality in deterministic dependency parsing", "author": ["Joakim Nivre."], "venue": "Proc. ACL Workshop on Incremental Parsing, pages 50\u201357.", "citeRegEx": "Nivre.,? 2004", "shortCiteRegEx": "Nivre.", "year": 2004}, {"title": "Algorithms for deterministic incremental dependency parsing", "author": ["Joakim Nivre."], "venue": "Computational Linguistics, 34(4):513\u2013553.", "citeRegEx": "Nivre.,? 2008", "shortCiteRegEx": "Nivre.", "year": 2008}, {"title": "Conditional neural fields", "author": ["Jian Peng", "Liefeng Bo", "Jinbo Xu."], "venue": "Proc. NIPS, pages 1419\u2013 1427.", "citeRegEx": "Peng et al\\.,? 2009", "shortCiteRegEx": "Peng et al\\.", "year": 2009}, {"title": "Overview of the 2012 shared task on parsing the web", "author": ["Slav Petrov", "Ryan McDonald."], "venue": "Notes of the First Workshop on Syntactic Analysis of NonCanonical Language (SANCL).", "citeRegEx": "Petrov and McDonald.,? 2012", "shortCiteRegEx": "Petrov and McDonald.", "year": 2012}, {"title": "Learning accurate, compact, and interpretable tree annotation", "author": ["Slav Petrov", "Leon Barrett", "Romain Thibaux", "Dan Klein."], "venue": "Proc. ACL, pages 433\u2013 440.", "citeRegEx": "Petrov et al\\.,? 2006", "shortCiteRegEx": "Petrov et al\\.", "year": 2006}, {"title": "Uptraining for accurate deterministic question parsing", "author": ["Slav Petrov", "Pi-Chuan Chang", "Michael Ringgaard", "Hiyan Alshawi."], "venue": "Proc. EMNLP, pages 705\u2013713.", "citeRegEx": "Petrov et al\\.,? 2010", "shortCiteRegEx": "Petrov et al\\.", "year": 2010}, {"title": "A linear observed time statistical parser based on maximum entropy models", "author": ["Adwait Ratnaparkhi."], "venue": "Proc. EMNLP, pages 1\u201310.", "citeRegEx": "Ratnaparkhi.,? 1997", "shortCiteRegEx": "Ratnaparkhi.", "year": 1997}, {"title": "An empirical study of semisupervised structured conditional models for dependency parsing", "author": ["Jun Suzuki", "Hideki Isozaki", "Xavier Carreras", "Michael Collins."], "venue": "Proc. 2009 EMNLP, pages 551\u2013 560.", "citeRegEx": "Suzuki et al\\.,? 2009", "shortCiteRegEx": "Suzuki et al\\.", "year": 2009}, {"title": "Learning condensed feature representations from large unsupervised data sets for supervised learning", "author": ["Jun Suzuki", "Hideki Isozaki", "Masaaki Nagata."], "venue": "Proc. ACL-HLT, pages 636\u2013641.", "citeRegEx": "Suzuki et al\\.,? 2011", "shortCiteRegEx": "Suzuki et al\\.", "year": 2011}, {"title": "Fast and robust multilingual dependency parsing with a generative latent variable model", "author": ["Ivan Titov", "James Henderson."], "venue": "Proc. EMNLP, pages 947\u2013951.", "citeRegEx": "Titov and Henderson.,? 2007", "shortCiteRegEx": "Titov and Henderson.", "year": 2007}, {"title": "A latent variable model for generative dependency pars", "author": ["Ivan Titov", "James Henderson"], "venue": null, "citeRegEx": "Titov and Henderson.,? \\Q2010\\E", "shortCiteRegEx": "Titov and Henderson.", "year": 2010}, {"title": "Feature-rich part-ofspeech tagging with a cyclic dependency network", "author": ["Kristina Toutanova", "Dan Klein", "Christopher D. Manning", "Yoram Singer."], "venue": "NAACL.", "citeRegEx": "Toutanova et al\\.,? 2003", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey E. Hinton."], "venue": "arXiv:1412.7449.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Statistical dependency analysis with support vector machines", "author": ["Hiroyasu Yamada", "Yuji Matsumoto."], "venue": "Proc. IWPT, pages 195\u2013206.", "citeRegEx": "Yamada and Matsumoto.,? 2003", "shortCiteRegEx": "Yamada and Matsumoto.", "year": 2003}, {"title": "On rectified linear units for speech", "author": ["Matthew D. Zeiler", "Marc\u2019Aurelio Ranzato", "Rajat Monga", "Mark Z. Mao", "K. Yang", "Quoc Viet Le", "Patrick Nguyen", "Andrew W. Senior", "Vincent Vanhoucke", "Jeffrey Dean", "Geoffrey E. Hinton"], "venue": null, "citeRegEx": "Zeiler et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2013}, {"title": "A tale of two parsers: investigating and combining graphbased and transition-based dependency parsing using beam-search", "author": ["Yue Zhang", "Stephen Clark"], "venue": "Proc. ICASSP,", "citeRegEx": "Zhang and Clark.,? \\Q2008\\E", "shortCiteRegEx": "Zhang and Clark.", "year": 2008}, {"title": "Enforcing structural diversity in cube-pruned dependency parsing", "author": ["Hao Zhang", "Ryan McDonald."], "venue": "Proc. ACL, pages 656\u2013661.", "citeRegEx": "Zhang and McDonald.,? 2014", "shortCiteRegEx": "Zhang and McDonald.", "year": 2014}, {"title": "Transition-based dependency parsing with rich non-local features", "author": ["Yue Zhang", "Joakim Nivre."], "venue": "Proc. ACL-HLT, pages 188\u2013193.", "citeRegEx": "Zhang and Nivre.,? 2011", "shortCiteRegEx": "Zhang and Nivre.", "year": 2011}, {"title": "A neural probabilistic structured-prediction model for transition-based dependency parsing", "author": ["Hao Zhou", "Yue Zhang", "Jiajun Chen."], "venue": "Proc. ACL.", "citeRegEx": "Zhou et al\\.,? 2015", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "Lately, dependency parsing has emerged as a popular approach to this problem due to the availability of dependency treebanks in many languages (Buchholz and Marsi, 2006; Nivre et al., 2007; McDonald et al., 2013) and the efficiency of dependency parsers.", "startOffset": 143, "endOffset": 212}, {"referenceID": 24, "context": "Lately, dependency parsing has emerged as a popular approach to this problem due to the availability of dependency treebanks in many languages (Buchholz and Marsi, 2006; Nivre et al., 2007; McDonald et al., 2013) and the efficiency of dependency parsers.", "startOffset": 143, "endOffset": 212}, {"referenceID": 26, "context": "Transition-based parsers (Nivre, 2008) have been shown to provide a good balance between efficiency and accuracy.", "startOffset": 25, "endOffset": 38}, {"referenceID": 42, "context": "Nonetheless, significant manual feature engineering was required before transitionbased systems provided competitive accuracy with graph-based parsers (Zhang and Nivre, 2011), and only by incorporating graph-based scoring functions were Bohnet and Kuhn (2012) able to exceed the accuracy of graph-based approaches.", "startOffset": 151, "endOffset": 174}, {"referenceID": 1, "context": "Lately, dependency parsing has emerged as a popular approach to this problem due to the availability of dependency treebanks in many languages (Buchholz and Marsi, 2006; Nivre et al., 2007; McDonald et al., 2013) and the efficiency of dependency parsers. Transition-based parsers (Nivre, 2008) have been shown to provide a good balance between efficiency and accuracy. In transition-based parsing, sentences are processed in a linear left to right pass; at each position, the parser needs to choose from a set of possible actions defined by the transition strategy. In greedy models, a classifier is used to independently decide which transition to take based on local features of the current parse configuration. This classifier typically uses hand-engineered features and is trained on individual transitions extracted from the gold transition sequence. While extremely fast, these greedy models typically suffer from search errors due to the inability to recover from incorrect decisions. Zhang and Clark (2008) showed that a beamsearch decoding algorithm utilizing the structured perceptron training algorithm can greatly improve accuracy.", "startOffset": 144, "endOffset": 1015}, {"referenceID": 0, "context": "Nonetheless, significant manual feature engineering was required before transitionbased systems provided competitive accuracy with graph-based parsers (Zhang and Nivre, 2011), and only by incorporating graph-based scoring functions were Bohnet and Kuhn (2012) able to exceed the accuracy of graph-based approaches.", "startOffset": 237, "endOffset": 260}, {"referenceID": 5, "context": "In contrast to these carefully hand-tuned approaches, Chen and Manning (2014) recently presented a neural network version of a greedy transition-based parser.", "startOffset": 54, "endOffset": 78}, {"referenceID": 19, "context": "Training and testing on the Penn Treebank (Marcus et al., 1993), our transition-based parser achieves 93.", "startOffset": 42, "endOffset": 63}, {"referenceID": 17, "context": "Training and testing on the Penn Treebank (Marcus et al., 1993), our transition-based parser achieves 93.99% unlabeled (UAS) / 92.05% labeled (LAS) attachment accuracy, outperforming the 93.22% UAS / 91.02% LAS of Zhang and McDonald (2014) and 93.", "startOffset": 43, "endOffset": 240}, {"referenceID": 0, "context": "19 LAS of Bohnet and Kuhn (2012). In addition, by incorporating unlabeled data into training, we further improve the accuracy of our model to 94.", "startOffset": 10, "endOffset": 33}, {"referenceID": 5, "context": "In our approach we start with the basic structure of Chen and Manning (2014), but with a deeper architecture and improvements to the optimization procedure.", "startOffset": 53, "endOffset": 77}, {"referenceID": 18, "context": "This approach is known as \u201ctri-training\u201d (Li et al., 2014) and we show that it benefits our neural network parser significantly more than other approaches.", "startOffset": 41, "endOffset": 58}, {"referenceID": 11, "context": "Finally, we also note that neural network representations have a long history in syntactic parsing (Henderson, 2004; Titov and Henderson, 2007; Titov and Henderson, 2010); however, like Chen and Manning (2014), our network avoids any recurrent structure so as to keep inference fast and efficient and to allow the use of simple backpropagation to compute gradients.", "startOffset": 99, "endOffset": 170}, {"referenceID": 34, "context": "Finally, we also note that neural network representations have a long history in syntactic parsing (Henderson, 2004; Titov and Henderson, 2007; Titov and Henderson, 2010); however, like Chen and Manning (2014), our network avoids any recurrent structure so as to keep inference fast and efficient and to allow the use of simple backpropagation to compute gradients.", "startOffset": 99, "endOffset": 170}, {"referenceID": 35, "context": "Finally, we also note that neural network representations have a long history in syntactic parsing (Henderson, 2004; Titov and Henderson, 2007; Titov and Henderson, 2010); however, like Chen and Manning (2014), our network avoids any recurrent structure so as to keep inference fast and efficient and to allow the use of simple backpropagation to compute gradients.", "startOffset": 99, "endOffset": 170}, {"referenceID": 5, "context": "Finally, we also note that neural network representations have a long history in syntactic parsing (Henderson, 2004; Titov and Henderson, 2007; Titov and Henderson, 2010); however, like Chen and Manning (2014), our network avoids any recurrent structure so as to keep inference fast and efficient and to allow the use of simple backpropagation to compute gradients.", "startOffset": 186, "endOffset": 210}, {"referenceID": 5, "context": "Finally, we also note that neural network representations have a long history in syntactic parsing (Henderson, 2004; Titov and Henderson, 2007; Titov and Henderson, 2010); however, like Chen and Manning (2014), our network avoids any recurrent structure so as to keep inference fast and efficient and to allow the use of simple backpropagation to compute gradients. Our work is also not the first to apply structured training to neural networks (see e.g. Peng et al. (2009) and Do and Artires (2010) for Conditional Random Field (CRF) training of neural networks).", "startOffset": 186, "endOffset": 474}, {"referenceID": 5, "context": "Finally, we also note that neural network representations have a long history in syntactic parsing (Henderson, 2004; Titov and Henderson, 2007; Titov and Henderson, 2010); however, like Chen and Manning (2014), our network avoids any recurrent structure so as to keep inference fast and efficient and to allow the use of simple backpropagation to compute gradients. Our work is also not the first to apply structured training to neural networks (see e.g. Peng et al. (2009) and Do and Artires (2010) for Conditional Random Field (CRF) training of neural networks).", "startOffset": 186, "endOffset": 500}, {"referenceID": 18, "context": "This idea comes from tri-training (Li et al., 2014) and while applicable to other parsers as well, we show that it benefits neural network", "startOffset": 34, "endOffset": 51}, {"referenceID": 27, "context": "Neural network representations have been used in structured models before (Peng et al., 2009; Do and Artires, 2010), and have also been used for syntactic parsing (Titov and Hen-", "startOffset": 74, "endOffset": 115}, {"referenceID": 9, "context": "Neural network representations have been used in structured models before (Peng et al., 2009; Do and Artires, 2010), and have also been used for syntactic parsing (Titov and Hen-", "startOffset": 74, "endOffset": 115}, {"referenceID": 5, "context": "Features that are included in addition to those from Chen and Manning (2014) are marked with ?.", "startOffset": 53, "endOffset": 77}, {"referenceID": 5, "context": "Following Chen and Manning (2014), we group these features by their input source:", "startOffset": 10, "endOffset": 34}, {"referenceID": 10, "context": "Dyer et al. (2015) concurrently developed the Stack Long S ort-T rm Memory (S-LSTM) architectur , which does incorporate recurrent architecture and look-ahead, and which yields comparable accuracy on the Penn Treebank to our greedy model.", "startOffset": 0, "endOffset": 19}, {"referenceID": 25, "context": "We use the arc-standard (Nivre, 2004) transition system.", "startOffset": 24, "endOffset": 37}, {"referenceID": 5, "context": "Our model is based upon that of Chen and Manning (2014) and we discuss the differences between our model and theirs in detail at the end of this section.", "startOffset": 32, "endOffset": 56}, {"referenceID": 5, "context": "Following Chen and Manning (2014), we group these features by their input source: words, POS tags, and arc labels.", "startOffset": 10, "endOffset": 34}, {"referenceID": 23, "context": "We experimented with one and two hidden layers composed of M rectified linear (Relu) units (Nair and Hinton, 2010).", "startOffset": 91, "endOffset": 114}, {"referenceID": 17, "context": "Relu layers have been well studied in the neural network literature and have been shown to work well for a wide domain of problems (Krizhevsky et al., 2012; Zeiler et al., 2013).", "startOffset": 131, "endOffset": 177}, {"referenceID": 39, "context": "Relu layers have been well studied in the neural network literature and have been shown to work well for a wide domain of problems (Krizhevsky et al., 2012; Zeiler et al., 2013).", "startOffset": 131, "endOffset": 177}, {"referenceID": 5, "context": "4 Relationship to Chen and Manning (2014)", "startOffset": 18, "endOffset": 42}, {"referenceID": 5, "context": "Our model is clearly inspired by and based on the work of Chen and Manning (2014). There are a few structural differences: (1) we allow for much smaller embeddings of POS tags and labels, (2) we use Relu units in our hidden layers, and (3) we use a deeper model with two hidden layers.", "startOffset": 58, "endOffset": 82}, {"referenceID": 5, "context": "Our model is clearly inspired by and based on the work of Chen and Manning (2014). There are a few structural differences: (1) we allow for much smaller embeddings of POS tags and labels, (2) we use Relu units in our hidden layers, and (3) we use a deeper model with two hidden layers. Somewhat to our surprise, we found these changes combined with an SGD training scheme (Section 3.1) during the \u201cpre-training\u201d phase of the model to lead to an almost 1% accuracy gain over Chen and Manning (2014). This trend held despite carefully tuning hyperparameters for each method of training and structure combination.", "startOffset": 58, "endOffset": 498}, {"referenceID": 2, "context": "To learn the hidden representations, we use mini-batched averaged stochastic gradient descent (ASGD) (Bottou, 2010) with momentum (Hinton, 2012) to learn the parameters \u0398 of the network, where \u0398 = {Eg,Wi,bi, \u03b2y | \u2200g, i, y}.", "startOffset": 101, "endOffset": 115}, {"referenceID": 12, "context": "To learn the hidden representations, we use mini-batched averaged stochastic gradient descent (ASGD) (Bottou, 2010) with momentum (Hinton, 2012) to learn the parameters \u0398 of the network, where \u0398 = {Eg,Wi,bi, \u03b2y | \u2200g, i, y}.", "startOffset": 130, "endOffset": 144}, {"referenceID": 7, "context": "The perceptron algorithm with early updates (Collins and Roark, 2004) requires a feature-vector definition \u03c6 that maps a sentence x together with a configuration c to a feature vector \u03c6(x, c) \u2208 Rd.", "startOffset": 44, "endOffset": 69}, {"referenceID": 16, "context": "One way to use unlabeled data is through unsupervised methods such as word clusters (Koo et al., 2008); we follow Chen and Manning (2014) and use pretrained word embeddings to initialize our model.", "startOffset": 84, "endOffset": 102}, {"referenceID": 5, "context": ", 2008); we follow Chen and Manning (2014) and use pretrained word embeddings to initialize our model.", "startOffset": 19, "endOffset": 43}, {"referenceID": 20, "context": "McClosky et al. (2006) and Huang and Harper (2009) showed that iteratively re-training a single model (\u201cself-training\u201d) can be used to improve parsers in certain settings; Petrov et al.", "startOffset": 0, "endOffset": 23}, {"referenceID": 14, "context": "(2006) and Huang and Harper (2009) showed that iteratively re-training a single model (\u201cself-training\u201d) can be used to improve parsers in certain settings; Petrov et al.", "startOffset": 11, "endOffset": 35}, {"referenceID": 14, "context": "(2006) and Huang and Harper (2009) showed that iteratively re-training a single model (\u201cself-training\u201d) can be used to improve parsers in certain settings; Petrov et al. (2010) built on this work and showed that a slow and accurate parser can be used to \u201cup-train\u201d a faster but less accurate parser.", "startOffset": 11, "endOffset": 177}, {"referenceID": 18, "context": "In this work, we adopt the \u201ctri-training\u201d approach of Li et al. (2014): Two parsers are used to process the unlabeled corpus and only sentences for which both parsers produced the same parse tree are added to the training data.", "startOffset": 54, "endOffset": 71}, {"referenceID": 29, "context": "(2014), intersecting the output of the BerkeleyParser (Petrov et al., 2006), and a reimplementation of ZPar (Zhang and Nivre, 2011) as our baseline parsers.", "startOffset": 54, "endOffset": 75}, {"referenceID": 42, "context": ", 2006), and a reimplementation of ZPar (Zhang and Nivre, 2011) as our baseline parsers.", "startOffset": 40, "endOffset": 63}, {"referenceID": 18, "context": "We use same setup as Li et al. (2014), intersecting the output of the BerkeleyParser (Petrov et al.", "startOffset": 21, "endOffset": 38}, {"referenceID": 19, "context": "We conduct our experiments on two English language benchmarks: (1) the standard Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1993) and (2) a more comprehensive union of publicly available treebanks spanning multiple domains.", "startOffset": 132, "endOffset": 153}, {"referenceID": 36, "context": "We use a CRF-based POS tagger to generate 5fold jack-knifed POS tags on the training set and predicted tags on the dev, test and tune sets; our tagger gets comparable accuracy to the Stanford POS tagger (Toutanova et al., 2003) with 97.", "startOffset": 203, "endOffset": 227}, {"referenceID": 13, "context": "(2015), we use (in addition to the WSJ), the OntoNotes corpus version 5 (Hovy et al., 2006), the English Web Treebank (Petrov and McDonald, 2012), and the updated and corrected Question Treebank (Judge et al.", "startOffset": 72, "endOffset": 91}, {"referenceID": 28, "context": ", 2006), the English Web Treebank (Petrov and McDonald, 2012), and the updated and corrected Question Treebank (Judge et al.", "startOffset": 34, "endOffset": 61}, {"referenceID": 15, "context": ", 2006), the English Web Treebank (Petrov and McDonald, 2012), and the updated and corrected Question Treebank (Judge et al., 2006).", "startOffset": 111, "endOffset": 131}, {"referenceID": 34, "context": "Following Vinyals et al. (2015), we use (in addition to the WSJ), the OntoNotes corpus version 5 (Hovy et al.", "startOffset": 10, "endOffset": 32}, {"referenceID": 29, "context": "We process it with the BerkeleyParser (Petrov et al., 2006), a latent variable constituency parser, and a reimplementation of ZPar (Zhang and Nivre, 2011), a transition-based parser with beam search.", "startOffset": 38, "endOffset": 59}, {"referenceID": 42, "context": ", 2006), a latent variable constituency parser, and a reimplementation of ZPar (Zhang and Nivre, 2011), a transition-based parser with beam search.", "startOffset": 79, "endOffset": 102}, {"referenceID": 4, "context": "In our semi-supervised experiments, we use the corpus from Chelba et al. (2013) as our source of unlabeled data.", "startOffset": 59, "endOffset": 80}, {"referenceID": 10, "context": "60 1 S-LSTM (Dyer et al., 2015) 93.", "startOffset": 12, "endOffset": 31}, {"referenceID": 21, "context": "Transition-based Zhang and Nivre (2011) 93.", "startOffset": 27, "endOffset": 40}, {"referenceID": 0, "context": "95 32 Bohnet and Kuhn (2012) 93.", "startOffset": 6, "endOffset": 29}, {"referenceID": 0, "context": "95 32 Bohnet and Kuhn (2012) 93.27 91.19 40 Chen and Manning (2014) 91.", "startOffset": 6, "endOffset": 68}, {"referenceID": 25, "context": "Tri-training Zhang and Nivre (2011) 92.", "startOffset": 23, "endOffset": 36}, {"referenceID": 5, "context": "All methods except Chen and Manning (2014) and Dyer et al.", "startOffset": 19, "endOffset": 43}, {"referenceID": 5, "context": "All methods except Chen and Manning (2014) and Dyer et al. (2015) were run using predicted tags from our POS tagger.", "startOffset": 19, "endOffset": 66}, {"referenceID": 22, "context": "We used the publicly available word2vec2 tool (Mikolov et al., 2013) to learn CBOW embeddings following the sample configuration provided with the tool.", "startOffset": 46, "endOffset": 68}, {"referenceID": 4, "context": "We therefore trained the word embeddings on the same corpus we used for tri-training (Chelba et al., 2013).", "startOffset": 85, "endOffset": 106}, {"referenceID": 23, "context": "Transition-based Zhang and Nivre (2011) 91.", "startOffset": 27, "endOffset": 40}, {"referenceID": 0, "context": "46 Bohnet and Kuhn (2012) 91.", "startOffset": 3, "endOffset": 26}, {"referenceID": 25, "context": "Tri-training Zhang and Nivre (2011) 91.", "startOffset": 23, "endOffset": 36}, {"referenceID": 5, "context": "For (Chen and Manning, 2014) and (Dyer et al.", "startOffset": 4, "endOffset": 28}, {"referenceID": 10, "context": "For (Chen and Manning, 2014) and (Dyer et al., 2015), we use reported results; the other baselines were run by Bernd Bohnet using version 3.", "startOffset": 33, "endOffset": 52}, {"referenceID": 25, "context": "Since many of the baselines could not be directly compared to our semi-supervised approach, we re-implemented Zhang and Nivre (2011) and trained on the tri-training corpus.", "startOffset": 120, "endOffset": 133}, {"referenceID": 31, "context": "Unfortunately we are not able to compare to several semi-supervised dependency parsers that achieve some of the highest reported accuracies on the WSJ, in particular Suzuki et al. (2009), Suzuki et al.", "startOffset": 166, "endOffset": 187}, {"referenceID": 31, "context": "Unfortunately we are not able to compare to several semi-supervised dependency parsers that achieve some of the highest reported accuracies on the WSJ, in particular Suzuki et al. (2009), Suzuki et al. (2011) and Chen et al.", "startOffset": 166, "endOffset": 209}, {"referenceID": 6, "context": "(2011) and Chen et al. (2013). These parsers use the Yamada and Matsumoto (2003) dependency conversion and the accuracies are therefore not directly comparable.", "startOffset": 11, "endOffset": 30}, {"referenceID": 6, "context": "(2011) and Chen et al. (2013). These parsers use the Yamada and Matsumoto (2003) dependency conversion and the accuracies are therefore not directly comparable.", "startOffset": 11, "endOffset": 81}, {"referenceID": 6, "context": "(2011) and Chen et al. (2013). These parsers use the Yamada and Matsumoto (2003) dependency conversion and the accuracies are therefore not directly comparable. The highest of these is Suzuki et al. (2011), with a reported accuracy of 94.", "startOffset": 11, "endOffset": 206}, {"referenceID": 29, "context": "Using the probability estimates directly is very similar to Ratnaparkhi (1997), where a maximum-entropy model was used to model the distribution over possible actions at each parser state, and beam search was used to search for the highest probability parse.", "startOffset": 60, "endOffset": 79}, {"referenceID": 25, "context": "For reference, our reimplementation of Zhang and Nivre (2011) is trained equivalently for each setting.", "startOffset": 49, "endOffset": 62}, {"referenceID": 29, "context": "To evaluate the impact of the tri-training approach, we compared to up-training with the BerkelyParser (Petrov et al., 2006) alone.", "startOffset": 103, "endOffset": 124}, {"referenceID": 25, "context": "The results are summarized in Figure 4 for the greedy and perceptron neural net models as well as our reimplementated Zhang and Nivre (2011) baseline.", "startOffset": 128, "endOffset": 141}], "year": 2015, "abstractText": "We present structured perceptron training for neural network transition-based dependency parsing. We learn the neural network representation using a gold corpus augmented by a large number of automatically parsed sentences. Given this fixed network representation, we learn a final layer using the structured perceptron with beam-search decoding. On the Penn Treebank, our parser reaches 94.26% unlabeled and 92.41% labeled attachment accuracy, which to our knowledge is the best accuracy on Stanford Dependencies to date. We also provide indepth ablative analysis to determine which aspects of our model provide the largest gains in accuracy.", "creator": "TeX"}}}