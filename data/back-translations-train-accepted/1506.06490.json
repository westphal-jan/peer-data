{"id": "1506.06490", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2015", "title": "Answer Sequence Learning with Neural Networks for Answer Selection in Community Question Answering", "abstract": "In this paper, the answer selection problem in community question answering (CQA) is regarded as an answer sequence labeling task, and a novel approach is proposed based on the recurrent architecture for this problem. Our approach applies convolution neural networks (CNNs) to learning the joint representation of question-answer pair firstly, and then uses the joint representation as input of the long short-term memory (LSTM) to learn the answer sequence of a question for labeling the matching quality of each answer. Experiments conducted on the SemEval 2015 CQA dataset shows the effectiveness of our approach.", "histories": [["v1", "Mon, 22 Jun 2015 07:26:51 GMT  (231kb,D)", "http://arxiv.org/abs/1506.06490v1", "6 pages"]], "COMMENTS": "6 pages", "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG", "authors": ["xiaoqiang zhou", "baotian hu", "qingcai chen", "buzhou tang", "xiaolong wang"], "accepted": true, "id": "1506.06490"}, "pdf": {"name": "1506.06490.pdf", "metadata": {"source": "CRF", "title": "Answer Sequence Learning with Neural Networks for Answer Selection in Community Question Answering", "authors": ["Xiaoqiang Zhou", "Baotian Hu", "Qingcai Chen", "Buzhou Tang", "Xiaolong Wang"], "emails": ["xiaoqiang.jeseph@gmail.com", "baotianchina@gmail.com", "qingcai.chen@gmail.com", "tangbuzhou@gmail.com", "wangxl@insun.hit.edu.cn"], "sections": [{"heading": null, "text": "In this paper, the problem of Community Question Answering (CQA) is considered as a task for labeling sequences of answers, and a new approach is proposed based on the recurring architecture of this problem. Our approach uses Convolution Neural Networks (CNNs) to first learn the common representation of pairs of questions and then to use the common representation as input of long-term memory (LSTM) to learn the response sequence of a question to label the matching quality of each answer. Experiments conducted with the SemEval 2015 CQA dataset show the effectiveness of our approach."}, {"heading": "1 Introduction", "text": "In order to identify suitable answers to a question, typical approaches to semantic matching between question and answer are studied by studying different characteristics (Wang et al., 2009a; Shah and Pomerantz, 2010). Some studies use syntactic tree structures (Wang et al., 2009b; Moschitti et al., 2007) to measure the semantic matching between question and answer. However, these approaches require high-quality data and various external resources that can be quite difficult to obtain. To take advantage of a large amount of raw data, deeper learning-based approaches (Wang et al., 2010; Hu et al., 2013) are proposed to directly learn the distributed representation of question and answer pairs."}, {"heading": "2 Related Work", "text": "Previous studies on answer selection generally addressed this challenge as a classification problem through the use of machine learning methods based on the exploration of different characteristics to represent a QA pair. Huang et al. (2007) integrated textual features with structural features of forum threads to represent the candidate QA pairs, and used Support Vector Machine (SVM) to classify the candidate pairs. In addition to typical characteristics, Shah and Pomerantz (2010) trained a logistic regression classifier (LR) with user metadata to predict the quality of answers in CQA. Ding et al. (2008) proposed an approach to conditional random field matching (CRF), which can capture contextual features from the response sequence for semantic matching between question and answer. In addition, the translation-based language model was also used for QA matching by translating the corresponding question (s) to each one."}, {"heading": "3 Approach", "text": "To characterize the matching quality of each answer for a given question, our approach models the semantic connections between consecutive answers and the semantic relevance between question and answer. Figure 1 summarizes the recurring architecture of our model (RCNN).R-CNN's motivation is to learn the useful context to improve the performance of the answer selection.The response sequence is modeled to enrich semantic characteristics. At each step, our approach uses pre-trained word embeddings to encode the sentences of the QA pair, which is then used as input vectors of the model. Based on the common representation of the QA pair learned from CNNs, the LSTM is applied to our model for learning the answer sequence, which predicts each answer of the question with Softmax function."}, {"heading": "3.1 Convolutional Neural Networks for QA Joint Learning", "text": "In view of a question-answer pair in step t, we use Convolutionary Neural Networks (CNNs) to learn the common representation pt for the pair. Figure 2 illustrates the process of joint QA learning, which comprises two stages: summarizing the meaning of the question and answering and generating the common representation of the QA pair. To obtain high-level sentence representations of the question and answer, we define 3 hidden layers each in two Convolutionary Sentence Models. The output of each hidden layer is composed of a series of 2-dimensional arrays called characteristic parameters (wm, bm). Each characteristic map is the result of a revolutionary or pooling filter. Each pooling layer follows an activation function. The output of the hidden layer is calculated as Eq. 1: Hm = \u03c3 (pool (wmHm \u2212 1 + bm))."}, {"heading": "3.2 LSTM for Answer Sequence Learning", "text": "Based on the common representation of the QA pair, the LSTM unit of our model performs response sequences that learn to model semantic linkages between continuous responses. In contrast to the traditional recursive unit, the LSTM unit modulates the memory at each time step rather than overwriting the states. The key component of the LSTM unit is the memory cell that has a state over time, and the LSTM unit decides to modify and add the memory in the cell via the sigmoid gates: input gate it, forget gate ft and output gate ot. The implementation of the LSTM unit in our study is close to the one discussed by Graves (2013). Given the common representation of p sequence, the memory cell is updated by activating the input gate it, forget gate ft and output gate ft. The updating equation is updated by type c from equation bc + ct = 1 \u2212 ct = 1 ft- h."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Experiment Setup", "text": "Experimental dataset: We conduct experiments on the public dataset of the answer selection challenge in SemEval 2015. This dataset consists of three subgroups: training, development and test sets and contains 3,229 questions with 21,062 answers. The answers fall into three classes: Good, Bad, and Potential, accounting for 51%, 39%, and 10% respectively. Statistics of the dataset are summarized in Table 1, where # question / answer denotes the number of questions / answers, and length represents the average number of answers for a question. Competitor methods: We compare our approach against the following competitor methods: SVM (Huang et al, 2007): An SVM-based method with bag characteristics (textual characteristics), non-textual characteristics, and characteristics are based on topic model (i.e., latent dirichlet allocation, LDA).CRF (Ding et al, 2008): A CRF-based method with the same characteristics as the SVM classes."}, {"heading": "4.2 Results and Analysis", "text": "The main reason for this is that R-CNN prefers the semantic correlations between successive answers from LSTM, in addition to the semantic relationship between question and answer. Sharing the QA pair learned from CNN also captures richer matching patterns between question and answer than other methods.It is noteworthy that methods based on deep learning are more powerful than SVM and CRF, especially for complicated answers (e.g. potential answers).In contrast, SVM and CRF, with a wide range of features, are better for answers that have obvious tendencies (e.g. good and bad answers).The main reason is that the distributed representation learned from the architecture of deep learning can capture the sectarian relationship between question and answer."}, {"heading": "5 Conclusions and Future Work", "text": "Based on the recurring architecture of our model, our approach is able to model the semantic connection between successive answers, in addition to the semantic relevance between question and answer. Experimental results show that our approach can learn the useful context from the response sequence to improve the performance of response selection in the real world. In the future, we plan to explore the methods of forming the imbalance data to improve the overall performance of our approach. Based on this work, further research on topic recognition and semantic role designation for human-human conversations in the real world can be conducted."}], "references": [{"title": "Improving deep neural networks for lvcsr using rectified linear units and dropout", "author": ["Dahl et al.2013] George E. Dahl", "Tara N. Sainath", "Geoffrey E. Hinton"], "venue": "In ICASSP,", "citeRegEx": "Dahl et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2013}, {"title": "Using conditional random fields to extract contexts and answers of questions from online forums", "author": ["Ding et al.2008] Shilin Ding", "Gao Cong", "Chin yew Lin", "Xiaoyan Zhu"], "venue": "Proceedings of ACL08: HLT", "citeRegEx": "Ding et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2008}, {"title": "Generating sequences with recurrent neural networks. CoRR, abs/1308.0850", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors. CoRR, abs/1207.0580", "author": ["Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies", "author": ["Yoshua Bengio", "Paolo Frasconi", "J\u00fcrgen Schmidhuber"], "venue": null, "citeRegEx": "Hochreiter et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "Multimodal dbn for predicting high-quality answers in cqa portals", "author": ["Hu et al.2013] Haifeng Hu", "Bingquan Liu", "Baoxun Wang", "Ming Liu", "Xiaolong Wang"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Hu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2013}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Hu et al.2014] Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Hu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "Extracting chatbot knowledge from online discussion forums", "author": ["Huang et al.2007] Jizhou Huang", "Ming Zhou", "Dan Yang"], "venue": "In Proceedings of the 20th International Joint Conference on Artifical Intelligence,", "citeRegEx": "Huang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2007}, {"title": "Finding similar questions in large question and answer archives", "author": ["Jeon et al.2005] Jiwoon Jeon", "W. Bruce Croft", "Joon Ho Lee"], "venue": "In Proceedings of the 14th ACM International Conference on Information and Knowledge Management,", "citeRegEx": "Jeon et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Jeon et al\\.", "year": 2005}, {"title": "Convolutional neural networks for sentence classification. CoRR, abs/1408.5882", "author": ["Yoon Kim"], "venue": null, "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Semeval-2015 task 3: Answer selection in community question answering", "author": ["James Glass", "Walid Magdy", "Alessandro Moschitti", "Preslav Nakov", "Bilal Randeree"], "venue": "In Proceedings of the 9th International Workshop", "citeRegEx": "M\u00e0rquez et al\\.,? \\Q2015\\E", "shortCiteRegEx": "M\u00e0rquez et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space. CoRR, abs/1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Exploiting syntactic and shallow semantic kernels for question answer classification", "author": ["Silvia Quarteroni", "Roberto Basili", "Suresh Manandhar"], "venue": "In Proceedings of the 45th Annual Meeting", "citeRegEx": "Moschitti et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Moschitti et al\\.", "year": 2007}, {"title": "Evaluating and predicting answer quality in community qa", "author": ["Shah", "Pomerantz2010] Chirag Shah", "Jefferey Pomerantz"], "venue": "In Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "Shah et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shah et al\\.", "year": 2010}, {"title": "Unsupervised learning of video representations using lstms", "author": ["Elman Mansimov", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks. CoRR, abs/1409.3215", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Extracting chinese question-answer pairs from online forums", "author": ["Wang et al.2009a] Baoxun Wang", "Bingquan Liu", "Chengjie Sun", "Xiaolong Wang", "Lin Sun"], "venue": "In IEEE International Conference on Systems, Man, and Cybernetics (SMC),", "citeRegEx": "Wang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2009}, {"title": "A syntactic tree matching approach to finding similar questions in communitybased qa services", "author": ["Wang et al.2009b] Kai Wang", "Zhaoyan Ming", "TatSeng Chua"], "venue": "In Proceedings of the 32Nd International ACM SIGIR Conference on Research and", "citeRegEx": "Wang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2009}, {"title": "Modeling semantic relevance for question-answer pairs in web social communities", "author": ["Wang et al.2010] Baoxun Wang", "Xiaolong Wang", "Chengjie Sun", "Bingquan Liu", "Lin Sun"], "venue": "In Proceedings of the 48th Annual Meeting of the Association", "citeRegEx": "Wang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2010}, {"title": "Retrieval models for question and answer archives", "author": ["Xue et al.2008] Xiaobing Xue", "Jiwoon Jeon", "W. Bruce Croft"], "venue": "In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information", "citeRegEx": "Xue et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2008}, {"title": "Deep learning for answer sentence selection. CoRR, abs/1412.1632", "author": ["Yu et al.2014] Lei Yu", "Karl Moritz Hermann", "Phil Blunsom", "Stephen Pulman"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "ADADELTA: an adaptive learning rate method. CoRR, abs/1212.5701", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Phrase-based translation model for question retrieval in community question answer archives", "author": ["Zhou et al.2011] Guangyou Zhou", "Li Cai", "Jun Zhao", "Kang Liu"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Zhou et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 12, "context": "Some studies exploit syntactic tree structures (Wang et al., 2009b; Moschitti et al., 2007) to measure the semantic matching between question and answer.", "startOffset": 47, "endOffset": 91}, {"referenceID": 18, "context": "To take advantage of a large quantity of raw data, deep learning based approaches (Wang et al., 2010; Hu et al., 2013) are proposed to learn the distributed representation of question-answer pair directly.", "startOffset": 82, "endOffset": 118}, {"referenceID": 5, "context": "To take advantage of a large quantity of raw data, deep learning based approaches (Wang et al., 2010; Hu et al., 2013) are proposed to learn the distributed representation of question-answer pair directly.", "startOffset": 82, "endOffset": 118}, {"referenceID": 4, "context": "Recently, recurrent neural network (RNN), especially Long Short-Term Memory (LSTM) (Hochreiter et al., 2001), has been proved superiority in various tasks (Sutskever et al.", "startOffset": 83, "endOffset": 108}, {"referenceID": 15, "context": ", 2001), has been proved superiority in various tasks (Sutskever et al., 2014; Srivastava et al., 2015) and it models long term and short term information of the sequence.", "startOffset": 54, "endOffset": 103}, {"referenceID": 14, "context": ", 2001), has been proved superiority in various tasks (Sutskever et al., 2014; Srivastava et al., 2015) and it models long term and short term information of the sequence.", "startOffset": 54, "endOffset": 103}, {"referenceID": 9, "context": "And also, there are some works on using convolutional neural networks (CNNs) to learn the representations of sentence or short text, which achieve state-of-the-art performance on sentiment classification (Kim, 2014) and short text matching (Hu et al.", "startOffset": 204, "endOffset": 215}, {"referenceID": 6, "context": "And also, there are some works on using convolutional neural networks (CNNs) to learn the representations of sentence or short text, which achieve state-of-the-art performance on sentiment classification (Kim, 2014) and short text matching (Hu et al., 2014).", "startOffset": 240, "endOffset": 257}, {"referenceID": 8, "context": "Additionally, the translation-based language model was also used for QA matching by transferring the answer to the corresponding question (Jeon et al., 2005; Xue et al., 2008; Zhou et al., 2011).", "startOffset": 138, "endOffset": 194}, {"referenceID": 19, "context": "Additionally, the translation-based language model was also used for QA matching by transferring the answer to the corresponding question (Jeon et al., 2005; Xue et al., 2008; Zhou et al., 2011).", "startOffset": 138, "endOffset": 194}, {"referenceID": 22, "context": "Additionally, the translation-based language model was also used for QA matching by transferring the answer to the corresponding question (Jeon et al., 2005; Xue et al., 2008; Zhou et al., 2011).", "startOffset": 138, "endOffset": 194}, {"referenceID": 6, "context": "Huang et al. (2007) integrated textual features with structural features of forum threads to represent the candidate QA pairs, and used support vector machine (SVM) to classify the candidate pairs.", "startOffset": 0, "endOffset": 20}, {"referenceID": 6, "context": "Huang et al. (2007) integrated textual features with structural features of forum threads to represent the candidate QA pairs, and used support vector machine (SVM) to classify the candidate pairs. Beyond typical features, Shah and Pomerantz (2010) trained a logistic regression (LR) classifier with user metadata to predict the quality of answers in CQA.", "startOffset": 0, "endOffset": 249}, {"referenceID": 1, "context": "Ding et al. (2008) proposed an approach based on conditional random fields (CRF), which can capture contextual features from the answer sequence for the semantic matching between question and answer.", "startOffset": 0, "endOffset": 19}, {"referenceID": 14, "context": "In contrast to symbolic representation, Wang et al. (2010) proposed a deep belief nets (DBN) based semantic relevance model to learn the distributed representation of QA pair.", "startOffset": 40, "endOffset": 59}, {"referenceID": 14, "context": "In contrast to symbolic representation, Wang et al. (2010) proposed a deep belief nets (DBN) based semantic relevance model to learn the distributed representation of QA pair. Recently, the convolutional neural networks (CNNs) based sentence representation models have achieved successes in neural language processing (NLP) tasks. Yu et al. (2014) proposed a convolutional sentence model to identify answer contents of a question from Q&A archives via means of distributed representations.", "startOffset": 40, "endOffset": 348}, {"referenceID": 5, "context": "The work in Hu et al. (2014) demonstrated that 2-dimensional convolutional sentence models can represent the hierarchical structures of sentences and capture rich matching patterns between two language objects.", "startOffset": 12, "endOffset": 29}, {"referenceID": 2, "context": "The implementation of the LSTM unit in our study is close the one discussed by Graves (2013). Given the joint representation pt at time t, the memory cell ct is updated by the input gate\u2019s activation it and the forget gate\u2019s activation ft.", "startOffset": 79, "endOffset": 93}, {"referenceID": 7, "context": "SVM (Huang et al., 2007): An SVM-based method with bag-of-words (textual features), nontextual features, and features based on topic model (i.", "startOffset": 4, "endOffset": 24}, {"referenceID": 1, "context": "CRF (Ding et al., 2008): A CRF-based method using the same features as the SVM approach.", "startOffset": 4, "endOffset": 23}, {"referenceID": 18, "context": "DBN (Wang et al., 2010): Taking bag-of-words representation, the method applies deep belief nets to learning the distributed representation of QA pair, and predicts the class of answers using a logistic regression classifier on the top layer.", "startOffset": 4, "endOffset": 23}, {"referenceID": 5, "context": "mDBN (Hu et al., 2013): In contrast to DBN, multimodal DBN learns the joint representations of textual features and non-textual features rather than bag-of-words.", "startOffset": 5, "endOffset": 22}, {"referenceID": 5, "context": "CNN: Using word embedding, the CNNs based model in Hu et al. (2014) is used to learn the representations of questions and answers, and a logistic regression classifier is used to predict the class of answers.", "startOffset": 51, "endOffset": 68}, {"referenceID": 3, "context": "To prevent serious overfitting, early stopping and dropout (Hinton et al., 2012) are used during the training procedure.", "startOffset": 59, "endOffset": 80}, {"referenceID": 21, "context": "01 and is updated dynamically according to the gradient descent using the ADADELTA method (Zeiler, 2012).", "startOffset": 90, "endOffset": 104}, {"referenceID": 0, "context": "The activation functions (\u03c3, \u03b3) in our model adopt the rectified linear unit (ReLU) (Dahl et al., 2013).", "startOffset": 84, "endOffset": 103}, {"referenceID": 11, "context": "In addition, the word embeddings for encoding sentences are pre-trained with the unsupervised neural language model (Mikolov et al., 2013) on the Qatar Living data2.", "startOffset": 116, "endOffset": 138}, {"referenceID": 10, "context": "In fact, the Potential answers are most difficult to identify among the three types of answers as Potential is an intermediate category (M\u00e0rquez et al., 2015).", "startOffset": 136, "endOffset": 158}], "year": 2015, "abstractText": "In this paper, the answer selection problem in community question answering (CQA) is regarded as an answer sequence labeling task, and a novel approach is proposed based on the recurrent architecture for this problem. Our approach applies convolution neural networks (CNNs) to learning the joint representation of questionanswer pair firstly, and then uses the joint representation as input of the long shortterm memory (LSTM) to learn the answer sequence of a question for labeling the matching quality of each answer. Experiments conducted on the SemEval 2015 CQA dataset shows the effectiveness of our approach.", "creator": "LaTeX with hyperref package"}}}