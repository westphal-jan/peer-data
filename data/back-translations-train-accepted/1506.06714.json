{"id": "1506.06714", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2015", "title": "A Neural Network Approach to Context-Sensitive Generation of Conversational Responses", "abstract": "We present a novel response generation system that can be trained end to end on large quantities of unstructured Twitter conversations. A neural network architecture is used to address sparsity issues that arise when integrating contextual information into classic statistical models, allowing the system to take into account previous dialog utterances. Our dynamic-context generative models show consistent gains over both context-sensitive and non-context-sensitive Machine Translation and Information Retrieval baselines.", "histories": [["v1", "Mon, 22 Jun 2015 18:29:03 GMT  (355kb,D)", "http://arxiv.org/abs/1506.06714v1", "A. Sordoni, M. Galley, M. Auli, C. Brockett, Y. Ji, M. Mitchell, J.-Y. Nie, J. Gao, B. Dolan. 2015. A Neural Network Approach to Context-Sensitive Generation of Conversational Responses. In Proc. of NAACL-HLT. Pages 196-205"]], "COMMENTS": "A. Sordoni, M. Galley, M. Auli, C. Brockett, Y. Ji, M. Mitchell, J.-Y. Nie, J. Gao, B. Dolan. 2015. A Neural Network Approach to Context-Sensitive Generation of Conversational Responses. In Proc. of NAACL-HLT. Pages 196-205", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG cs.NE", "authors": ["alessandro sordoni", "michel galley", "michael auli", "chris brockett", "yangfeng ji", "margaret mitchell", "jian-yun nie", "jianfeng gao", "bill dolan"], "accepted": true, "id": "1506.06714"}, "pdf": {"name": "1506.06714.pdf", "metadata": {"source": "CRF", "title": "A Neural Network Approach to Context-Sensitive Generation of Conversational Responses\u2217", "authors": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan"], "emails": ["donia@iro.umontreal.ca)", "ley@microsoft.com)."], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "2 Related Work", "text": "Our work is, of course, that of Ritter et al. (2011), but we generalize their approach by using information from a larger context. Ritter et al. and our work represent a radical paradigm shift from other work in dialogue. In fact, traditional dialog systems only tease dialogue management (Young, 2002) from response generation (Stent and Bangalore, 2014), while our holistic approach can be seen as a first attempt to perform both tasks together. While there are previous uses of machine learning for response generation (Walker et al., 2010), dialog tracking (Georgila et al., 2006), many components of typical dialog systems remain hand-coded: in particular the labels and attributes that define dialog states. In contrast, the dialog state in our neural network model is completely latent and directly optimized."}, {"heading": "3 Recurrent Language Model", "text": "We give a brief overview of the Recurrent Language Model (RLM) = > seev et al., 2010) architecture that extends our models. An RLM is a generative model of sentences, i.e., given sentences s = s1,..., sT, itestimates: p (s) = T-t = 1 p (st | s1,.., st \u2212 1). (1) The model architecture is parameterized by three weight matrices, RNN = < Win, Wout, Whh >: an input matrixWin, a recurent matrixWhh and an output matrix Wout, which are usually initialized by three weight matrices. Lines of the input matrix Win-RV \u00b7 K contain the K-dimensional embeddings for each word in the language vocabulary of size V. Let us token through st both the vocabulary and its one-hot representation."}, {"heading": "4 Context-Sensitive Models", "text": "We distinguish three linguistic units in a conversation between two users A and B: the context1 c, the message m, and the response r. Context c represents a sequence of past dialog exchanges of any length; then B sends a message m, to which A responds by formulating its response r (see Figure 1). We use three context-based generation models to estimate a generational model of the response r, r = r1,..., rT, conditioned by past information c and m: p (r | c, m) = T-t = 1 p (rt | r1,..., rt \u2212 1, c, m). (5) These three models differ in the way they compose the context message pair (c, m)."}, {"heading": "4.1 Tripled Language Model", "text": "In our first model, called RLMT, we simply concatenate each utterance c, m, r into a single sentence s and train the RLM to minimize L (s). Given c and m, we calculate the probability of the answer as follows: We perform forward propagation over the known utterances c and m to obtain a hidden state that encodes useful information about previous utterances. Then, we calculate the probability of the answer from this hidden law. One problem with this simple approach is that the concatenated sentences will be very long on average, especially if the context includes multiple utterances. Modelling such far-reaching dependencies with an RLM is difficult and is still considered an open problem (Pascanu et al., 2013). In this paper, we will consider that the context is purely linguistic, but future work could incorporate additional contextual information, e.g. geographical location, time information, or other forms of grounding RLMT as additional context models for the next."}, {"heading": "4.2 Dynamic-Context Generative Model I", "text": "In our second model (DCGM-I), this is illustrated in Figure 3 (left). First, we treat c andm as a single sentence and compute a single word set representation bcm-RV. Then bcm is provided as input to a multi-layered nonlinear forward architecture that produces a fixed length representation that is used to distort the recursive state of the RLM decoder. At the time of training, both the context encoder and the RLM decoder are learned to minimize the negative protocol probability of the generated response. Parameters of the model are: \"DCGM-I = < Win, Whh, kout, {W'L '= 1 >. The negative protocol probability of the generated response is minimized by the response generated."}, {"heading": "4.3 Dynamic-Context Generative Model II", "text": "Since DCGM-I does not differentiate between c and m, this model tends to underestimate the strong dependence that exists between m and r. Our third model (DCGM-II) solves this problem by concatenating the two linear images of the baggy word representations bc and bm in the input layer of the feed network that represents c and m (see Figure 3 on the right).The concatenation of continuous representations before deep architectures is a common strategy for obtaining order-sensitive representations (Bengio et al., 2003; Devlin et al., 2014).The forward equations for the context encoder are: k1 = [b > c W 1 f, b > mW 1 f], k '= \u03c3 (k >' \u2212 1W 'f) for' = 2, \u00b7, L (8), where [x, y] denotes the concatenation of x and y vectors. In DCGM-II, the probability of the next recursive layer is put together, and the recursive layer x is the probability of the state of the 7 and the next."}, {"heading": "5 Experimental Setting", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Dataset Construction", "text": "To increase computing efficiency and reduce the burden on human evaluators, we limit the context sequence c to a single set. Therefore, our data set consists of \"triples\" \u2261 (c\u03c4, m\u03c4, r\u03c4) consisting of three sets. To minimize noise, we chose triples that contained at least one frequent bigram that appeared more than three times in the corpus, resulting in a corpus of 29 million Twitter triples. Additionally, we engaged crowdsourcing rating agencies to evaluate approximately 33,000 candidate triples. Ratings on a 5-point scale were obtained by 3 raters apiece, resulting in a set of 4232 triples with an average of 4 or better, which was then randomly divided into a series of 21 points and 3 triples."}, {"heading": "5.2 Automatic Evaluation", "text": "We evaluate all systems using BLEU (Papandreou et al., 2002) and METEOR (Banerjee and Lavie, 2005) and supplement these results with more targeted human pairwise comparisons in Section 6.3. A major challenge in using these automated metrics for reaction generation is that the set of reasonable responses in our task is potentially extensive and extremely diverse. As we see in Section 6.3, optimizing systems toward BLEU using mined multireferences results in BLEU rankings being well aligned with human judgments, laying the groundwork for interesting future correlation studies, which we will use the following algorithm to better cover reasonable responses."}, {"heading": "5.3 Feature Sets", "text": "The reaction generation systems examined in this paper are parameterized as loglinear models within a framework typical of statistical machine translation (Och and Ney, 2004), and these loglinear models include the following features: MT characteristics are derived from a large response generation system built along the lines of Ritter et al. (2011) and based on a phrase-based MT decoder similar to Moses (Koehn et al., 2007). Our MT characteristic set includes the following features common in Moses: forward and backward maximum probability of \"translation\" probabilities, word and phrase penalties, linear distortions, and a modified Kneser-Ney language model (Kneser and Ney, 1995). For the translation probabilities, we have created a very large phrase table with 160.7 million entries, first filtering out twitter sequences (e.g. Fisher and Fisher sequence pairs, long sequence pairs)."}, {"heading": "5.4 Model Training", "text": "The vocabulary consists of the most common V = 50K words. To speed up the training, we use the loss of Noise-Contrastive Estimation (NCE), which avoids repeated sums above V by approximating the probability of the target word (Gutmann and Hyva \ufffd rinen, 2010). Parameter optimization is performed using Adagrad (Duchi et al., 2011) with a mini batch size of 100 and a learning rate of \u03b1 = 0.1, which we considered to work well based on the data provided. To stabilize learning, we cut the gradients to a fixed range [\u2212 10, 10] as proposed in Mikolov et al. (2010). All parameters of the neuronal models are sampled from a normal distribution N (0, 0.01), while the recursive weight Whh is initialized as a random orthogonal matrix and scaled by 0.01."}, {"heading": "5.5 Rescoring Setup", "text": "We evaluate the proposed models by resorbing the n-best candidate responses we received with the MT phrase decoder and the IR system. Unlike the MT, the candidate responses provided by the IR were human-generated and are less affected by fluid problems. The various n-best lists provide a comprehensive test bed for our experiments. First, we add the values of the interesting model to the n-best list of the tuning set. Then, we perform an iteration of MERT (Och, 2003) to estimate the log-linear weights of the new features. At test time, we resorb the n-best list of the test with the new weights."}, {"heading": "6 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Lower and Upper Bounds", "text": "Table 2 shows the expected upper and lower limits for this task, as suggested by the BLEU values for human responses and a random response base. RANDOM includes answers that are randomly extracted from the triple corpus. HUMAN is calculated by selecting one reference from the multi-reference set for each context status pair. 4 Although the score for the human score BLEU is calculated at the corpus level using a sampling scheme that randomly excludes one reference - the human rate to the score - for each reference set, this sampling scheme (repeated with 100 studies) is also applied to the MT and is lower than those normally given for SMT tasks, but the ranking of the three systems is clear."}, {"heading": "6.2 BLEU and METEOR", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "6.3 Human Evaluation", "text": "Annotators were asked to compare the quality of the system output responses in pairs (\"Which is better?\") with respect to the context and message strings in item 2114. Identical strings were held up so that the annotators saw only the results that differed. Paired responses were presented to the annotators in random order, and each pair of responses was judged by 5 annotators. Table 4 summarizes the results of the human evaluation and gives the difference in averages (paired preference margin) between systems and 95% confidence intervals generated by Welch t-test. Identical strings that were not displayed by raters are automatically rated with a score of 0.5. The pattern in these results is clear and consistent: context-sensitive systems (+ CMM) perform better than non-context-sensitive systems, with preference gains in the case of DCGM-II + CMM compared to IR and 3.0% context-MGM (both contextual MGM and MGM-MGM)."}, {"heading": "6.4 Discussion", "text": "The reactions from this system are on average shorter (8.95 tokens) than the original human # Context c Message m Generated Response r1 noone can help its just i need a support system well im here to support you. Whenever you need me, I appreciate it very much, and what is the intention of the book?! But I feel only halfway through the world. I am so bored that I have the book, but I am bored."}, {"heading": "7 Conclusion", "text": "We have developed a neural network architecture for data-driven response generation that is trained through social media conversations, with response generation based on previous dialog expressions that provide context-sensitive information. We have proposed a novel multi-reference extraction technology that enables robust automated evaluation using standard SMT metrics such as BLEU and METEOR. Our context-sensitive models consistently outperform both context-sensitive and context-sensitive baselines by up to 11% relative improvements in BLEU setting and 24% in IR setting, albeit with a minimal number of features. As our models are fully data-driven and self-contained, they have the potential to improve frequency and context-sensitive relevance in other types of dialog systems.Our work suggests several directions for future research. We expect there to be much room for improvement in this area when we deploy more complex word sequencing models within the message and the neural message."}, {"heading": "Acknowledgments", "text": "We thank Alan Ritter, Ray Mooney, Chris Quirk, Lucy Vanderwende, Susan Hendrich and Mouni Reddy for helpful discussions and the three anonymous reviewers for their comments."}], "references": [{"title": "Joint language and translation modeling with recurrent neural networks", "author": ["Auli et al.2013] Michael Auli", "Michel Galley", "Chris Quirk", "Geoffrey Zweig"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Auli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Auli et al\\.", "year": 2013}, {"title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments", "author": ["Banerjee", "Lavie2005] Satanjeev Banerjee", "Alon Lavie"], "venue": "In Proc. of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Transla-", "citeRegEx": "Banerjee et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2005}, {"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "Rejean Ducharme", "Pascal Vincent"], "venue": "Journ. Mach. Learn. Res.,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "Proc. of EMNLP", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Proc. of ICML,", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Devlin et al.2014] Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard Schwartz", "John Makhoul"], "venue": "In Proc. of ACL", "citeRegEx": "Devlin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journ. Mach. Learn. Res.,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Learning continuous phrase representations for translation modeling", "author": ["Gao et al.2014a] Jianfeng Gao", "Xiaodong He", "Wen tau Yih", "Li Deng"], "venue": "In Proc. of ACL,", "citeRegEx": "Gao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2014}, {"title": "Modeling interestingness with deep neural networks", "author": ["Gao et al.2014b] Jianfeng Gao", "Patrick Pantel", "Michael Gamon", "Xiaodong He", "Li Deng"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Gao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2014}, {"title": "User simulation for spoken dialogue systems: Learning and evaluation", "author": ["James Henderson", "Oliver Lemon"], "venue": "In Proc. of Interspeech/ICSLP", "citeRegEx": "Georgila et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Georgila et al\\.", "year": 2006}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["Gutmann", "Hyv\u00e4rinen2010] Michael Gutmann", "Aapo Hyv\u00e4rinen"], "venue": "In Proc. of AISTATS,", "citeRegEx": "Gutmann et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gutmann et al\\.", "year": 2010}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["Huang et al.2013] Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Acero", "Larry Heck"], "venue": "In Proc. of CIKM,", "citeRegEx": "Huang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "Proc. of EMNLP,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Improved backing-off for M-gram language modeling", "author": ["Kneser", "Ney1995] Reinhard Kneser", "Hermann Ney"], "venue": "In Proc. of ICASSP,", "citeRegEx": "Kneser et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Kneser et al\\.", "year": 1995}, {"title": "Context Dependent Recurrent Neural Network Language Model", "author": ["Mikolov", "Zweig2012] Tomas Mikolov", "Geoffrey Zweig"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u00fd", "Sanjeev Khudanpur"], "venue": "In Proc. of INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "The alignment template approach to machine translation", "author": ["Och", "Ney2004] Franz Josef Och", "Hermann Ney"], "venue": null, "citeRegEx": "Och et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Och et al\\.", "year": 2004}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och"], "venue": "In Proc. of ACL,", "citeRegEx": "Och.,? \\Q2003\\E", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proc. of ACL,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Tomas Mikolov", "Yoshua Bengio"], "venue": "Proc. of ICML,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Data-driven response generation in social media", "author": ["Ritter et al.2011] Alan Ritter", "Colin Cherry", "William B. Dolan"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Ritter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "Learning representations by back-propagating errors", "author": ["David E. Rumelhart", "Geoffrey E. Hinton", "Ronald J. Williams"], "venue": "Neurocomputing: Foundations of Research,", "citeRegEx": "Rumelhart et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "A latent semantic model with convolutional-pooling structure for information retrieval", "author": ["Shen et al.2014] Yelong Shen", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Gr\u00e9goire Mesnil"], "venue": "In Proc. of CIKM,", "citeRegEx": "Shen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2014}, {"title": "Natural Language Generation in Interactive Systems", "author": ["Stent", "Bangalore2014] Amanda Stent", "Srinivas Bangalore"], "venue": null, "citeRegEx": "Stent et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Stent et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. V Le"], "venue": "Proc. of NIPS", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A trainable generator for recommendations in multimodal dialog", "author": ["Rashmi Prasad", "Amanda Stent"], "venue": "In Proc. of EUROSPEECH", "citeRegEx": "Walker et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Walker et al\\.", "year": 2003}, {"title": "The hidden information state model: A practical framework for pomdp-based spoken dialogue management", "author": ["Young et al.2010] Steve Young", "Milica Ga\u0161i\u0107", "Simon Keizer", "Fran\u00e7ois Mairesse", "Jost Schatzmann", "Blaise Thomson", "Kai Yu"], "venue": null, "citeRegEx": "Young et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Young et al\\.", "year": 2010}, {"title": "Talking to machines (statistically speaking)", "author": ["Steve Young"], "venue": "In Proc. of INTERSPEECH", "citeRegEx": "Young.,? \\Q2002\\E", "shortCiteRegEx": "Young.", "year": 2002}], "referenceMentions": [{"referenceID": 20, "context": "The work of Ritter et al. (2011), for example, demonstrates that a response generation system can be constructed from Twitter conversations using statistical machine translation techniques, where a status post by a Twitter user is \u201ctranslated\u201d into a plausible looking response.", "startOffset": 12, "endOffset": 33}, {"referenceID": 20, "context": "However, an approach such as that presented in Ritter et al. (2011) does not address the challenge of generating responses that are sensitive to the context of the conversation.", "startOffset": 47, "endOffset": 68}, {"referenceID": 20, "context": "We argue that embedding-based models afford flexibility to model the transitions between consecutive utterances and to capture long-span dependencies in a domain where traditional word and phrase alignment is difficult (Ritter et al., 2011).", "startOffset": 219, "endOffset": 240}, {"referenceID": 15, "context": "To this end, we present two simple, context-sensitive response-generation models utilizing the Recurrent Neural Network Language Model (RLM) architecture of (Mikolov et al., 2010).", "startOffset": 157, "endOffset": 179}, {"referenceID": 27, "context": "Unlike typical complex task-oriented multi-modular dialog systems (Young, 2002; Stent and Bangalore, 2014), our architecture is completely data-driven and can easily be trained end-to-end using unstructured data without requiring human annotation, scripting, or automatic parsing.", "startOffset": 66, "endOffset": 106}, {"referenceID": 27, "context": "More traditional dialog systems typically tease apart dialog management (Young, 2002) from response generation (Stent and Bangalore, 2014), while our holistic approach can be considered a first attempt to accomplish both tasks jointly.", "startOffset": 72, "endOffset": 85}, {"referenceID": 25, "context": "While there are previous uses of machine learning for response generation (Walker et al., 2003), dialog state tracking (Young et al.", "startOffset": 74, "endOffset": 95}, {"referenceID": 26, "context": ", 2003), dialog state tracking (Young et al., 2010), and user modeling (Georgila et al.", "startOffset": 31, "endOffset": 51}, {"referenceID": 9, "context": ", 2010), and user modeling (Georgila et al., 2006), many components of typical dialog systems remain hand-coded: in particular, the labels and attributes defining dialog states.", "startOffset": 27, "endOffset": 50}, {"referenceID": 19, "context": "Our work naturally lies in the path opened by Ritter et al. (2011), but we generalize their approach by exploiting information from a larger context.", "startOffset": 46, "endOffset": 67}, {"referenceID": 11, "context": "Continuous representations of words and phrases estimated by neural network models have been applied on a variety of tasks ranging from Information Retrieval (IR) (Huang et al., 2013; Shen et al., 2014), Online Recommendation (Gao et al.", "startOffset": 163, "endOffset": 202}, {"referenceID": 22, "context": "Continuous representations of words and phrases estimated by neural network models have been applied on a variety of tasks ranging from Information Retrieval (IR) (Huang et al., 2013; Shen et al., 2014), Online Recommendation (Gao et al.", "startOffset": 163, "endOffset": 202}, {"referenceID": 0, "context": ", 2014b), Machine Translation (MT) (Auli et al., 2013; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), and Language Modeling (LM) (Bengio et al.", "startOffset": 35, "endOffset": 128}, {"referenceID": 3, "context": ", 2014b), Machine Translation (MT) (Auli et al., 2013; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), and Language Modeling (LM) (Bengio et al.", "startOffset": 35, "endOffset": 128}, {"referenceID": 24, "context": ", 2014b), Machine Translation (MT) (Auli et al., 2013; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), and Language Modeling (LM) (Bengio et al.", "startOffset": 35, "endOffset": 128}, {"referenceID": 2, "context": ", 2014), and Language Modeling (LM) (Bengio et al., 2003; Collobert and Weston, 2008).", "startOffset": 36, "endOffset": 85}, {"referenceID": 0, "context": ", 2014b), Machine Translation (MT) (Auli et al., 2013; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), and Language Modeling (LM) (Bengio et al., 2003; Collobert and Weston, 2008). Gao et al. (2014a) successfully use an embedding model to refine the estimation of rare phrase-translation probabilities, which is traditionally affected by sparsity problems.", "startOffset": 36, "endOffset": 227}, {"referenceID": 15, "context": "Our work extends the Recurrent Neural Network Language Model (RLM) of (Mikolov et al., 2010), which uses continuous representations to estimate a probability function over natural language sentences.", "startOffset": 70, "endOffset": 92}, {"referenceID": 13, "context": "Our work extends the Recurrent Neural Network Language Model (RLM) of (Mikolov et al., 2010), which uses continuous representations to estimate a probability function over natural language sentences. We propose a set of conditional RLMs where contextual information (i.e., past utterances) is encoded in a continuous context vector to help generate the response. Our models differ from most previous work in the way the context vector is constructed. For example, Mikolov and Zweig (2012) and Auli et al.", "startOffset": 71, "endOffset": 489}, {"referenceID": 0, "context": "For example, Mikolov and Zweig (2012) and Auli et al. (2013) use a pre-trained topic model.", "startOffset": 42, "endOffset": 61}, {"referenceID": 15, "context": "We give a brief overview of the Recurrent Language Model (RLM) (Mikolov et al., 2010) architecture that our models extend.", "startOffset": 63, "endOffset": 85}, {"referenceID": 21, "context": "The recurrence is unrolled backwards in time using the back-propagation through time (BPTT) algorithm (Rumelhart et al., 1988), and gradients are accumulated over multiple time-steps.", "startOffset": 102, "endOffset": 126}, {"referenceID": 19, "context": "Modelling such long-range dependencies with an RLM is difficult and is still considered an open problem (Pascanu et al., 2013).", "startOffset": 104, "endOffset": 126}, {"referenceID": 19, "context": "We found that this helps learning the embedding matrix as it reduces the vanishing gradient effect partially due to stacking of squashing non-linearities (Pascanu et al., 2013).", "startOffset": 154, "endOffset": 176}, {"referenceID": 18, "context": "We evaluate all systems using BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), and supplement these results with more targeted human pairwise comparisons in Section 6.", "startOffset": 35, "endOffset": 58}, {"referenceID": 20, "context": "MT MT features are derived from a large response generation system built along the lines of Ritter et al. (2011), which is based on a phrase-based MT decoder similar to Moses (Koehn et al.", "startOffset": 92, "endOffset": 113}, {"referenceID": 20, "context": ", long sequences of vowels, hashtags), and then selecting candidate phrase pairs using Fisher\u2019s exact test (Ritter et al., 2011).", "startOffset": 107, "endOffset": 128}, {"referenceID": 20, "context": "IR We also use an IR feature built from an index of triples, whose implementation roughly matches the IRstatus approach described in Ritter et al. (2011): For a test triple \u03c4 , we choose r\u03c4\u0303 as the candidate response iff \u03c4\u0303 = arg max\u03c4\u0303 d(m\u03c4 ,m\u03c4\u0303 ).", "startOffset": 133, "endOffset": 154}, {"referenceID": 6, "context": "Parameter optimization is done using Adagrad (Duchi et al., 2011) with a mini-batch size of 100 and a learning rate \u03b1 = 0.", "startOffset": 45, "endOffset": 65}, {"referenceID": 6, "context": "Parameter optimization is done using Adagrad (Duchi et al., 2011) with a mini-batch size of 100 and a learning rate \u03b1 = 0.1, which we found to work well on held-out data. In order to stabilize learning, we clip the gradients to a fixed range [\u221210, 10], as suggested in Mikolov et al. (2010). All the parameters of the neural models are sampled from a normal distribution N (0, 0.", "startOffset": 46, "endOffset": 291}, {"referenceID": 17, "context": "Then, we run an iteration of MERT (Och, 2003) to estimate the log-linear weights of the new features.", "startOffset": 34, "endOffset": 45}, {"referenceID": 18, "context": "88 BLEU over the model based on Ritter et al. (2011). METEOR improvements similarly align with BLEU improvements both for MT and IR lists.", "startOffset": 32, "endOffset": 53}, {"referenceID": 7, "context": "We take this as evidence that CMM exact matches and DCGM semantic matches interact positively, a finding that comports with Gao et al. (2014a), who show that semantic relationships mined through phrase embeddings correlate positively with classic co-occurrencebased estimations.", "startOffset": 124, "endOffset": 143}], "year": 2015, "abstractText": "We present a novel response generation system that can be trained end to end on large quantities of unstructured Twitter conversations. A neural network architecture is used to address sparsity issues that arise when integrating contextual information into classic statistical models, allowing the system to take into account previous dialog utterances. Our dynamic-context generative models show consistent gains over both context-sensitive and non-context-sensitive Machine Translation and Information Retrieval baselines.", "creator": "LaTeX with hyperref package"}}}