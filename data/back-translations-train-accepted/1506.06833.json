{"id": "1506.06833", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2015", "title": "A Survey of Current Datasets for Vision and Language Research", "abstract": "Integrating vision and language has long been a dream in work on artificial intelligence (AI). In the past two years, we have witnessed an explosion of work that brings together vision and language from images to videos and beyond. The available corpora have played a crucial role in advancing this area of research. In this paper, we propose a set of quality metrics for evaluating and analyzing the vision &amp; language datasets and classify them accordingly. Our analyses show that the most recent datasets have been using more complex language and more abstract concepts, however, there are different strengths and weaknesses in each.", "histories": [["v1", "Tue, 23 Jun 2015 00:59:27 GMT  (29kb,D)", "http://arxiv.org/abs/1506.06833v1", "In submission to EMNLP 2015 short. F.F. and N.M. contributed equally to this work"], ["v2", "Wed, 19 Aug 2015 04:33:37 GMT  (38kb,D)", "http://arxiv.org/abs/1506.06833v2", "To appear in EMNLP 2015, short proceedings. Dataset analysis and discussion expanded, including an initial examination into reporting bias for one of them. F.F. and N.M. contributed equally to this work"]], "COMMENTS": "In submission to EMNLP 2015 short. F.F. and N.M. contributed equally to this work", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV cs.GL", "authors": ["francis ferraro", "nasrin mostafazadeh", "ting-hao huang", "lucy vanderwende", "jacob devlin", "michel galley", "margaret mitchell"], "accepted": true, "id": "1506.06833"}, "pdf": {"name": "1506.06833.pdf", "metadata": {"source": "CRF", "title": "On Available Corpora for Empirical Methods in Vision & Language", "authors": ["Francis Ferraro", "Nasrin Mostafazadeh", "Ting-Hao Huang", "Lucy Vanderwende", "Jacob Devlin", "Michel Galley", "Margaret Mitchell"], "emails": ["lucyv@microsoft.com", "jdevlin@microsoft.com", "mgalley@microsoft.com", "memitc@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "Bringing language and vision together in an intelligent system has long been a goal in AI research, starting with SHRDLU as one of the first vision speech integration systems (Winograd, 1972) and continuing in more recent experiments with conversation robots based on the visual world (Kollar et al., 2013; Cantrell et al., 2010; Matuszek et al., 2012; Kruijff et al., 2007; Roy et al., 2003). In recent years, the influence of new, large vision & speech corpora, along with dramatic advances in vision research, has sparked renewed interest in linking vision and speech. Vision & speech corpora now offer alignments between visual content that can be recognized with computer vision (CV) algorithms and language that can be understood and generated with Natural Language Processing (NLP)."}, {"heading": "2 Quality Criteria for Language & Vision Datasets", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to integrate themselves, in which they are able to live, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they"}, {"heading": "3 The Available Datasets", "text": "We group a representative set of available records according to their contents. A complete list of records and their descriptions can be found on the supplementary website.2"}, {"heading": "3.1 Captioned Images", "text": "The captions of these data sets are either the original image title and the descriptions of the online users (Ordonez et al., 2011; Thomee et al., 2015) or the crowdworker captions for existing images (Yatskar et al., 2014). The earlier data sets tend to be larger and contain more contextual descriptions."}, {"heading": "3.1.1 User-generated Captions", "text": "\u2022 De \u0301 ja \u0445 Images Dataset (Chen et al., 2015) consists of 180K unique, user-generated captions associated with approximately 4M Flickr images, with one caption matching several images. Captured by querying Flickr for 693 radio frequency nouns, then further filtered to have at least one verb and rated \"good\" by Turks."}, {"heading": "3.1.2 Crowd-sourced Captions", "text": "\u2022 Flickr 30K Images (Young et al., 2014) augments previous Flickr datasets (Rashtchian et al., 2010) and includes 158,915 crowdsourced captions describing 31,783 images of people involved in everyday activities and events. \u2022 Microsoft COCO dataset (MS COCO) (Lin et al., 2014) includes complex everyday scenes with common objects in naturally occurring contexts. Objects in the scene are labeled by instance segmentation. In total, this dataset contains photos of 91 basic object types with 2.5 million instances labeled in 328k images, paired with 5 captions each. This dataset led to the challenge of captioning CVPR 2015 and continues to be a benchmark for comparing different aspects of visual and language research. \u2022 Abstract Scenes Dataset (Zitnick et al., 2013) was created with the goal of isolating scenes from the real world with clickpart identification of objects."}, {"heading": "3.2 Video Description and Instruction", "text": "Video data sets geared to descriptions (Chen et al., 2010; Rohrbach et al., 2012; Regneri et al., 2013; Naim et al., 2015; Malmaud et al., 2015) generally represent limited areas and small encyclopedias, due to the fact that video processing and understanding is a very computationally intensive task. Data sets available include: \u2022 Short Videos Described with Sentences (Yu and Siskind, 2013) comprises 61 video clips (each 35 seconds long, filmed in three different outdoor environments) showing multiple simultaneous events between a subset of four objects: a person, a backpack, a chair, and a garbage can. Each video has been commented manually (with very limited grammar and a lexicon) with multiple sentences describing what occurs in the video. \u2022 Microsoft Research Video Description Corpus (MS VDC) (Chen and Dolan, 2011) contains parallel video descriptions of 8550 or short descriptions (each)."}, {"heading": "3.3 Beyond Visual Description", "text": "Recent work has shown that N-gram language modeling coupled with scene understanding of an image trained on sufficiently large datasets can lead to reasonable auto-generated captions (Fang et al., 2014; Donahue et al., 2014). Some work has suggested going beyond the generation of descriptions to deeper AI tasks such as answering questions (Ren et al., 2015; Malinowski and Fritz, 2014). We present two of these attempts below: \u2022 Visual Madlibs dataset (VML) (Yu et al., 2015) is a subset of 10,783 images from the MS COCO dataset aimed at going beyond the description of the objects in the image. For a given image, three Amazon Turkers were asked to complete one of 12 completable template questions, e.g. \"when I look at this image, I feel -\" automatically selected MS datasets based on this 360,000 dataset are selected."}, {"heading": "4 Analysis", "text": "We analyze the datasets introduced in Section 3 using the metrics defined in Section 2. We find evidence that the VQA dataset captures more abstract concepts than other datasets, with nearly 20% of the words found in our abstract concept resource. The Deja corpus has the least amount of abstract concepts, followed by COCO and VDC. This reflects some of the differences in capturing the different corpora: For example, the Deja corpus was collected to find specific visual phrases that can be used to describe multiple images. This corpus also has the most syntactically simple phrases measured by both Frazier and Yngve; this is likely caused by the phrases that need to be generic enough to capture multiple image sets. The most syntactically complex sets are found in the Patchr30K, COCO and CQA datasets."}, {"heading": "5 Conclusion", "text": "We detail the recent growth of visual and speech corpora, comparing and contrasting several recently released large data sets. We argue that a newly introduced corpus can measure comparison with similar data sets by measuring, among other things, perplexity, syntactic complexity, abstract: concrete word relationships. By using such metrics and comparing across corporations, research can be limited to how data sets are distorted or distorted in different directions, and define new corpora accordingly."}], "references": [{"title": "VQA: visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh."], "venue": "arXiv preprint arXiv:1505.00468.", "citeRegEx": "Antol et al\\.,? 2015", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "Robust spoken instruction understanding for hri", "author": ["Rehj Cantrell", "Matthias Scheutz", "Paul W. Schermerhorn", "Xuan Wu."], "venue": "Pamela J. Hinds, Hiroshi Ishiguro, Takayuki Kanda, and Peter H. Kahn Jr., editors, HRI, pages 275\u2013282. ACM.", "citeRegEx": "Cantrell et al\\.,? 2010", "shortCiteRegEx": "Cantrell et al\\.", "year": 2010}, {"title": "Collecting highly parallel data for paraphrase evaluation", "author": ["David L. Chen", "William B. Dolan."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT \u201911,", "citeRegEx": "Chen and Dolan.,? 2011", "shortCiteRegEx": "Chen and Dolan.", "year": 2011}, {"title": "Training a multilingual sportscaster: Using perceptual context to learn language", "author": ["David L. Chen", "Joohyun Kim", "Raymond J. Mooney."], "venue": "J. Artif. Int. Res., 37(1):397\u2013436, January.", "citeRegEx": "Chen et al\\.,? 2010", "shortCiteRegEx": "Chen et al\\.", "year": 2010}, {"title": "D\u00e9j\u00e0 image-captions: A corpus of expressive descriptions in repetition", "author": ["Jianfu Chen", "Polina Kuznetsova", "David Warren", "Yejin Choi."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Jeff Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell."], "venue": "CoRR, abs/1411.4389.", "citeRegEx": "Donahue et al\\.,? 2014", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "From captions to visual concepts and back", "author": ["Hao Fang", "Saurabh Gupta", "Forrest N. Iandola", "Rupesh Srivastava", "Li Deng", "Piotr Doll\u00e1r", "Jianfeng Gao", "Xiaodong He", "Margaret Mitchell", "John C. Platt", "C. Lawrence Zitnick", "Geoffrey Zweig."], "venue": "CoRR,", "citeRegEx": "Fang et al\\.,? 2014", "shortCiteRegEx": "Fang et al\\.", "year": 2014}, {"title": "Syntactic complexity", "author": ["L. Frazier."], "venue": "D. R. Dowty, L. Karttunen, and A. M. Zwicky, editors, Natural Language Parsing: Psychological, Computational, and Theoretical Perspectives, pages 129\u2013 189. Cambridge University Press, Cambridge.", "citeRegEx": "Frazier.,? 1985", "shortCiteRegEx": "Frazier.", "year": 1985}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question answering", "author": ["Haoyuan Gao", "Junhua Mao", "Jie Zhou", "Zhiheng Huang", "Lei Wang", "Wei Xu."], "venue": "CoRR, abs/1505.05612.", "citeRegEx": "Gao et al\\.,? 2015", "shortCiteRegEx": "Gao et al\\.", "year": 2015}, {"title": "Reporting bias and knowledge extraction", "author": ["Jonathan Gordon", "Benjamin Van Durme."], "venue": "Automated Knowledge Base Construction (AKBC) 2013: The 3rd Workshop on Knowledge Extraction, at CIKM 2013, AKBC\u201913.", "citeRegEx": "Gordon and Durme.,? 2013", "shortCiteRegEx": "Gordon and Durme.", "year": 2013}, {"title": "ReferItGame: Referring to Objects in Photographs of Natural Scenes", "author": ["Sahar Kazemzadeh", "Vicente Ordonez", "Mark Matten", "Tamara Berg."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Kazemzadeh et al\\.,? 2014", "shortCiteRegEx": "Kazemzadeh et al\\.", "year": 2014}, {"title": "Joint Photo Stream and Blog Post Summarization and Exploration", "author": ["Gunhee Kim", "Seungwhan Moon", "Leonid Sigal."], "venue": "28th IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2015).", "citeRegEx": "Kim et al\\.,? 2015", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Toward interactive grounded language acquisition", "author": ["Thomas Kollar", "Jayant Krishnamurthy", "Grant Strimel."], "venue": "Robotics: Science and Systems.", "citeRegEx": "Kollar et al\\.,? 2013", "shortCiteRegEx": "Kollar et al\\.", "year": 2013}, {"title": "Situated dialogue and spatial organization: What, where", "author": ["Geert-Jan M. Kruijff", "Hendrik Zender", "Patric Jensfelt", "Henrik I. Christensen."], "venue": ". . and why? International Journal of Advanced Robotic Systems, Special Issue on Human and Robot Interactive Com-", "citeRegEx": "Kruijff et al\\.,? 2007", "shortCiteRegEx": "Kruijff et al\\.", "year": 2007}, {"title": "Microsoft COCO: common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollar", "C. Lawrence Zitnick."], "venue": "CoRR, abs/1405.0312.", "citeRegEx": "Lin et al\\.,? 2014", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "A multiworld approach to question answering about realworld scenes based on uncertain input", "author": ["Mateusz Malinowski", "Mario Fritz."], "venue": "Advances in Neural Information Processing Systems 27, pages 1682\u20131690.", "citeRegEx": "Malinowski and Fritz.,? 2014", "shortCiteRegEx": "Malinowski and Fritz.", "year": 2014}, {"title": "Whats cookin? interpreting cooking videos using text, speech and vision", "author": ["Jonathan Malmaud", "Jonathan Huang", "Vivek Rathod", "Nicholas Johnston", "Andrew Rabinovich", "Kevin Murphy."], "venue": "North American Chapter of the Association for Computa-", "citeRegEx": "Malmaud et al\\.,? 2015", "shortCiteRegEx": "Malmaud et al\\.", "year": 2015}, {"title": "A Joint Model of Language and Perception for Grounded Attribute Learning", "author": ["Cynthia Matuszek", "Nicholas FitzGerald", "Luke Zettlemoyer", "Liefeng Bo", "Dieter Fox."], "venue": "Proc. of the 2012 International Conference on Machine Learning, Edinburgh,", "citeRegEx": "Matuszek et al\\.,? 2012", "shortCiteRegEx": "Matuszek et al\\.", "year": 2012}, {"title": "Discriminative unsupervised alignment of natural language instructions with corresponding video segments", "author": ["Iftekhar Naim", "Young C. Song", "Qiguang Liu", "Liang Huang", "Henry Kautz", "Jiebo Luo", "Daniel Gildea."], "venue": "North American Chapter of the", "citeRegEx": "Naim et al\\.,? 2015", "shortCiteRegEx": "Naim et al\\.", "year": 2015}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["Vicente Ordonez", "Girish Kulkarni", "Tamara L. Berg."], "venue": "Neural Information Processing Systems (NIPS).", "citeRegEx": "Ordonez et al\\.,? 2011", "shortCiteRegEx": "Ordonez et al\\.", "year": 2011}, {"title": "Collecting image annotations using amazon\u2019s mechanical turk", "author": ["Cyrus Rashtchian", "Peter Young", "Micah Hodosh", "Julia Hockenmaier."], "venue": "Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechan-", "citeRegEx": "Rashtchian et al\\.,? 2010", "shortCiteRegEx": "Rashtchian et al\\.", "year": 2010}, {"title": "Grounding action descriptions in videos", "author": ["Michaela Regneri", "Marcus Rohrbach", "Dominikus Wetzel", "Stefan Thater", "Bernt Schiele", "Manfred Pinkal."], "venue": "Transactions of the Association for Computational Linguistics (TACL), 1:25\u201336.", "citeRegEx": "Regneri et al\\.,? 2013", "shortCiteRegEx": "Regneri et al\\.", "year": 2013}, {"title": "Question answering about images using visual semantic embeddings", "author": ["Mengye Ren", "Ryan Kiros", "Richard Zemel."], "venue": "Deep Learning Workshop, ICML 2015.", "citeRegEx": "Ren et al\\.,? 2015", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "A database for fine grained activity detection of cooking activities", "author": ["Marcus Rohrbach", "Sikandar Amin", "Mykhaylo Andriluka", "Bernt Schiele."], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, IEEE, June.", "citeRegEx": "Rohrbach et al\\.,? 2012", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2012}, {"title": "Conversational robots: Building blocks for grounding word meaning", "author": ["Deb Roy", "Kai-Yuh Hsiao", "Nikolaos Mavridis."], "venue": "Proceedings of the HLT-NAACL 2003 Workshop on Learning Word Meaning from Non-linguistic Data - Volume 6, HLT-", "citeRegEx": "Roy et al\\.,? 2003", "shortCiteRegEx": "Roy et al\\.", "year": 2003}, {"title": "The new data and new challenges in multimedia research", "author": ["Bart Thomee", "David A. Shamma", "Gerald Friedland", "Benjamin Elizalde", "Karl Ni", "Douglas Poland", "Damian Borth", "Li-Jia Li."], "venue": "arXiv preprint arXiv:1503.01817.", "citeRegEx": "Thomee et al\\.,? 2015", "shortCiteRegEx": "Thomee et al\\.", "year": 2015}, {"title": "Unbiased look at dataset bias", "author": ["A. Torralba", "A.A. Efros."], "venue": "Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition, CVPR \u201911, pages 1521\u20131528, Washington, DC, USA. IEEE Computer Society.", "citeRegEx": "Torralba and Efros.,? 2011", "shortCiteRegEx": "Torralba and Efros.", "year": 2011}, {"title": "Mindnet: An automatically-created lexical resource", "author": ["Lucy Vanderwende", "Gary Kacmarcik", "Hisami Suzuki", "Arul Menezes."], "venue": "HLT/EMNLP 2005, Human Language Technology Conference and Conference on Empirical Methods", "citeRegEx": "Vanderwende et al\\.,? 2005", "shortCiteRegEx": "Vanderwende et al\\.", "year": 2005}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["Subhashini Venugopalan", "Huijuan Xu", "Jeff Donahue", "Marcus Rohrbach", "Raymond Mooney", "Kate Saenko."], "venue": "Proceedings the 2015 Conference of the North", "citeRegEx": "Venugopalan et al\\.,? 2015", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2015}, {"title": "Understanding Natural Language", "author": ["Terry Winograd."], "venue": "Academic Press, New York.", "citeRegEx": "Winograd.,? 1972", "shortCiteRegEx": "Winograd.", "year": 1972}, {"title": "See no evil, say no evil: Description generation from densely labeled images", "author": ["Mark Yatskar", "Michel Galley", "Lucy Vanderwende", "Luke Zettlemoyer."], "venue": "Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014),", "citeRegEx": "Yatskar et al\\.,? 2014", "shortCiteRegEx": "Yatskar et al\\.", "year": 2014}, {"title": "A model and an hypothesis for language structure", "author": ["Victor H. Yngve."], "venue": "Proceedings of the American Philosophical Society, 104:444\u2013466.", "citeRegEx": "Yngve.,? 1960", "shortCiteRegEx": "Yngve.", "year": 1960}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier."], "venue": "Transactions of the Association for Computational Linguis-", "citeRegEx": "Young et al\\.,? 2014", "shortCiteRegEx": "Young et al\\.", "year": 2014}, {"title": "Grounded language learning from video described with sentences", "author": ["Haonan Yu", "Jeffrey Mark Siskind."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 53\u201363,", "citeRegEx": "Yu and Siskind.,? 2013", "shortCiteRegEx": "Yu and Siskind.", "year": 2013}, {"title": "Visual Madlibs: Fill in the blank Image Generation and Question Answering", "author": ["Licheng Yu", "Eunbyung Park", "Alexander C. Berg", "Tamara L. Berg."], "venue": "arXiv preprint arXiv:1506.00278.", "citeRegEx": "Yu et al\\.,? 2015", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Learning the visual interpretation of sentences", "author": ["C. Lawrence Zitnick", "Devi Parikh", "Lucy Vanderwende."], "venue": "IEEE International Conference on Computer Vision, ICCV 2013, Sydney, Australia, December 1-8, 2013, pages 1681\u20131688.", "citeRegEx": "Zitnick et al\\.,? 2013", "shortCiteRegEx": "Zitnick et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 29, "context": "ligent system has long been an ambition in AI research, beginning with SHRDLU as one of the first vision-language integration systems (Winograd, 1972) and continuing in more recent attempts on conversational robots grounded in the visual world", "startOffset": 134, "endOffset": 150}, {"referenceID": 12, "context": "(Kollar et al., 2013; Cantrell et al., 2010; Matuszek et al., 2012; Kruijff et al., 2007; Roy et al., 2003).", "startOffset": 0, "endOffset": 107}, {"referenceID": 1, "context": "(Kollar et al., 2013; Cantrell et al., 2010; Matuszek et al., 2012; Kruijff et al., 2007; Roy et al., 2003).", "startOffset": 0, "endOffset": 107}, {"referenceID": 17, "context": "(Kollar et al., 2013; Cantrell et al., 2010; Matuszek et al., 2012; Kruijff et al., 2007; Roy et al., 2003).", "startOffset": 0, "endOffset": 107}, {"referenceID": 13, "context": "(Kollar et al., 2013; Cantrell et al., 2010; Matuszek et al., 2012; Kruijff et al., 2007; Roy et al., 2003).", "startOffset": 0, "endOffset": 107}, {"referenceID": 24, "context": "(Kollar et al., 2013; Cantrell et al., 2010; Matuszek et al., 2012; Kruijff et al., 2007; Roy et al., 2003).", "startOffset": 0, "endOffset": 107}, {"referenceID": 6, "context": "the past year, recent work has proposed methods for image and video captioning (Fang et al., 2014; Donahue et al., 2014; Venugopalan et al., 2015), summarization (Kim et al.", "startOffset": 79, "endOffset": 146}, {"referenceID": 5, "context": "the past year, recent work has proposed methods for image and video captioning (Fang et al., 2014; Donahue et al., 2014; Venugopalan et al., 2015), summarization (Kim et al.", "startOffset": 79, "endOffset": 146}, {"referenceID": 28, "context": "the past year, recent work has proposed methods for image and video captioning (Fang et al., 2014; Donahue et al., 2014; Venugopalan et al., 2015), summarization (Kim et al.", "startOffset": 79, "endOffset": 146}, {"referenceID": 11, "context": ", 2015), summarization (Kim et al., 2015), reference (Kazemzadeh et al.", "startOffset": 23, "endOffset": 41}, {"referenceID": 10, "context": ", 2015), reference (Kazemzadeh et al., 2014), and question an-", "startOffset": 19, "endOffset": 44}, {"referenceID": 0, "context": "swering (Antol et al., 2015; Gao et al., 2015), to name just a few.", "startOffset": 8, "endOffset": 46}, {"referenceID": 8, "context": "swering (Antol et al., 2015; Gao et al., 2015), to name just a few.", "startOffset": 8, "endOffset": 46}, {"referenceID": 9, "context": "reporting bias (Gordon and Durme, 2013), where", "startOffset": 15, "endOffset": 39}, {"referenceID": 26, "context": "the frequency with which people write about actions, events, or states does not directly reflect real-world frequencies of those phenomena; they are also affected by photographer\u2019s bias (Torralba and Efros, 2011), where photographs are somewhat predictable within a given domain.", "startOffset": 186, "endOffset": 212}, {"referenceID": 7, "context": "1960) and Frazier metrics (Frazier, 1985).", "startOffset": 26, "endOffset": 41}, {"referenceID": 27, "context": "For this purpose, we use a list of most common abstract terms in English (Vanderwende et al., 2005), and define concrete terms as all other words except for a small set of function words.", "startOffset": 73, "endOffset": 99}, {"referenceID": 19, "context": "The captions of these datasets are either the original photo title and descriptions provided by online users (Ordonez et al., 2011; Thomee et al., 2015), or the captions generated by crowd workers for existing images (Yatskar et al.", "startOffset": 109, "endOffset": 152}, {"referenceID": 25, "context": "The captions of these datasets are either the original photo title and descriptions provided by online users (Ordonez et al., 2011; Thomee et al., 2015), or the captions generated by crowd workers for existing images (Yatskar et al.", "startOffset": 109, "endOffset": 152}, {"referenceID": 30, "context": ", 2015), or the captions generated by crowd workers for existing images (Yatskar et al., 2014).", "startOffset": 72, "endOffset": 94}, {"referenceID": 4, "context": "\u2022 D\u00e9j\u00e0 Images Dataset (Chen et al., 2015) consists of 180K unique user-generated captions associated with about 4M Flickr images, where one", "startOffset": 22, "endOffset": 41}, {"referenceID": 32, "context": "\u2022 Flickr 30K Images (Young et al., 2014) extends previous Flickr datasets (Rashtchian et al.", "startOffset": 20, "endOffset": 40}, {"referenceID": 20, "context": ", 2014) extends previous Flickr datasets (Rashtchian et al., 2010), and includes 158,915 crowd-sourced captions that describe 31,783 images of people involved in everyday activities and events.", "startOffset": 41, "endOffset": 66}, {"referenceID": 14, "context": "\u2022Microsoft COCO Dataset (MS COCO) (Lin et al., 2014) includes complex everyday scenes with common objects in naturally occurring contexts.", "startOffset": 34, "endOffset": 52}, {"referenceID": 35, "context": "\u2022 Abstract Scenes Dataset (Clipart) (Zitnick et al., 2013) was created with the goal of representing real-world scenes with clipart to study scene", "startOffset": 36, "endOffset": 58}, {"referenceID": 3, "context": "Video datasets aligned with descriptions (Chen et al., 2010; Rohrbach et al., 2012; Regneri et al., 2013; Naim et al., 2015; Malmaud et al., 2015) generally represent limited domains and small lexicons, which is due to the fact that video processing and understanding is a very compute-intensive task.", "startOffset": 41, "endOffset": 146}, {"referenceID": 23, "context": "Video datasets aligned with descriptions (Chen et al., 2010; Rohrbach et al., 2012; Regneri et al., 2013; Naim et al., 2015; Malmaud et al., 2015) generally represent limited domains and small lexicons, which is due to the fact that video processing and understanding is a very compute-intensive task.", "startOffset": 41, "endOffset": 146}, {"referenceID": 21, "context": "Video datasets aligned with descriptions (Chen et al., 2010; Rohrbach et al., 2012; Regneri et al., 2013; Naim et al., 2015; Malmaud et al., 2015) generally represent limited domains and small lexicons, which is due to the fact that video processing and understanding is a very compute-intensive task.", "startOffset": 41, "endOffset": 146}, {"referenceID": 18, "context": "Video datasets aligned with descriptions (Chen et al., 2010; Rohrbach et al., 2012; Regneri et al., 2013; Naim et al., 2015; Malmaud et al., 2015) generally represent limited domains and small lexicons, which is due to the fact that video processing and understanding is a very compute-intensive task.", "startOffset": 41, "endOffset": 146}, {"referenceID": 16, "context": "Video datasets aligned with descriptions (Chen et al., 2010; Rohrbach et al., 2012; Regneri et al., 2013; Naim et al., 2015; Malmaud et al., 2015) generally represent limited domains and small lexicons, which is due to the fact that video processing and understanding is a very compute-intensive task.", "startOffset": 41, "endOffset": 146}, {"referenceID": 33, "context": "Available datasets include: \u2022 Short Videos Described with Sentences (Yu and Siskind, 2013) includes 61 video clips (each 35 seconds in length, filmed in three different outdoor environments), showing multiple simultaneous events between a subset of four objects: a person, a backpack, a chair, and a trash-can.", "startOffset": 68, "endOffset": 90}, {"referenceID": 2, "context": "\u2022 Microsoft Research Video Description Corpus (MS VDC) (Chen and Dolan, 2011) contains parallel descriptions (85,550 English ones) of 2,089 short video snippets (10-25 seconds long).", "startOffset": 55, "endOffset": 77}, {"referenceID": 22, "context": "Some works have proposed to step beyond description generation, towards deeper AI tasks such as question answering (Ren et al., 2015; Malinowski and Fritz, 2014).", "startOffset": 115, "endOffset": 161}, {"referenceID": 15, "context": "Some works have proposed to step beyond description generation, towards deeper AI tasks such as question answering (Ren et al., 2015; Malinowski and Fritz, 2014).", "startOffset": 115, "endOffset": 161}, {"referenceID": 34, "context": "We present two of these attempts below: \u2022 Visual Madlibs Dataset (VML) (Yu et al., 2015) is a subset of 10,783 images from the MS COCO dataset which aims to go beyond describing which objects are in the image.", "startOffset": 71, "endOffset": 88}, {"referenceID": 0, "context": "\u2022 Visual Question Answering (VQA) Dataset (Antol et al., 2015) is created for the task of openended VQA, where a system can be presented with an image and a free-form natural-language question (e.", "startOffset": 42, "endOffset": 62}, {"referenceID": 22, "context": "\u2022 Toronto COCO-QA Dataset (CQA) (Ren et al., 2015) is also a visual question answering dataset, where the questions are automatically generated from image captions of MS COCO dataset.", "startOffset": 32, "endOffset": 50}], "year": 2017, "abstractText": "Integrating vision and language has long been a dream in work on artificial intelligence (AI). In the past two years, we have witnessed an explosion of work that brings together vision and language from images to videos and beyond. The available corpora have played a crucial role in advancing this area of research. In this paper, we propose a set of quality metrics for evaluating and analyzing the vision & language datasets and classify them accordingly. Our analyses show that the most recent datasets have been using more complex language and more abstract concepts, however, there are different strengths and weaknesses in each.", "creator": "TeX"}}}