{"id": "1506.06840", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2015", "title": "On Variance Reduction in Stochastic Gradient Descent and its Asynchronous Variants", "abstract": "We study optimization algorithms based on variance reduction for stochastic gradient descent (SGD). Remarkable recent progress has been made in this direction through development of algorithms like SAG, SVRG, SAGA. These algorithms have been shown to outperform SGD, both theoretically and empirically. However, asynchronous versions of these algorithms---a crucial requirement for modern large-scale applications---have not been studied. We bridge this gap by presenting a unifying framework for many variance reduction techniques. Subsequently, we propose an asynchronous algorithm grounded in our framework, and prove its fast convergence. An important consequence of our general approach is that it yields asynchronous versions of variance reduction algorithms such as SVRG and SAGA as a byproduct. Our method achieves near linear speedup in sparse settings common to machine learning. We demonstrate the empirical performance of our method through a concrete realization of asynchronous SVRG.", "histories": [["v1", "Tue, 23 Jun 2015 01:57:19 GMT  (1467kb,D)", "http://arxiv.org/abs/1506.06840v1", null], ["v2", "Mon, 25 Jan 2016 01:12:06 GMT  (396kb,D)", "http://arxiv.org/abs/1506.06840v2", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["sashank j reddi", "ahmed hefny", "suvrit sra", "barnab\u00e1s p\u00f3czos", "alexander j smola"], "accepted": true, "id": "1506.06840"}, "pdf": {"name": "1506.06840.pdf", "metadata": {"source": "CRF", "title": "On Variance Reduction in Stochastic Gradient Descent and its Asynchronous Variants", "authors": ["Sashank J. Reddi", "Ahmed Hefny"], "emails": ["sjakkamr@cs.cmu.edu", "ahefny@cs.cmu.edu", "suvrit@mit.edu", "bapoczos@cs.cmu.edu", "alex@smola.org"], "sections": [{"heading": "1 Introduction", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "2 A General Framework for VR Stochastic Methods", "text": "We focus on the instances of the (1,1), where the cost function f (x) leads to the general reduction of the stochastic reduction of predisposition. We denote the collection of functions that lead f (1,1) of F (1,1) to an additional predisposition. We focus on the (1,1) of F (1,1). We focus on the (1,1) of F (1,1) on the (1,1) of F (1,1). We focus on the collection of functions that make f (1,1) of F (1,1)."}, {"heading": "2.1 Convergence Analysis", "text": "In this section, we provide the convergence analysis for algorithm 1 with HSAG schemes. As previously observed, SVRG and SAGA are special cases of this constellation. Our analysis assumes the impartiality of the gradient estimates for each iteration, so that they do not include SAG. To simplify the representation, we assume that all si = m are for all i [n]. Since HSAG is epoch-based, our analysis focuses on the iterations obtained for each epoch. Similar to [9] (see SVRG Option II in [9]), our analysis is focused on the case in which the iteration at the end (k + 1) of the first epoch, xkm + m, is replaced by a randomly selected element from {xkm,..., xkm + m \u2212 1} with probability (p1, \u00b7, pm)."}, {"heading": "Suppose the probabilities pi \u221d (1\u2212 1\u03ba )", "text": "m \u2212 i, and that c, \u03b2, \u03ba, step \u03b7 and epoch size are chosen in such a way that the following conditions are met: 1 \u03ba + 2Lc\u03b72 (1 + 1\u03b2) \u2264 1 n, \u03b3 > 0, \u03b8 < 1.Then we have for iterates of algorithm 1 according to the HSAG plan E [f (x-k + 1) \u2212 f (x-k) + 1\u03b3 G-k + 1] \u2264 \u03b8 E [f (x-k) \u2212 f (x-k) + 1\u03b3 G-k].As a logical consequence, we immediately get an expected linear convergence rate for HSAG.Episode 1. Note that G-k \u2265 0 and therefore under the conditions given in theorem 1 and under the conditions \u03b8-k = TB (1 + 1 / g) < 1 we have E [f (x-k) \u2212 f (x-p-n)."}, {"heading": "3 Asynchronous Stochastic Variance Reduction", "text": "We are now ready to present asynchronous versions of the algorithms covered by our general framework. We will first describe our lineup before delving into the details of these algorithms. Our calculation model is similar to those used in Hogwild! [20] and AsySCD [14]. However, we are assuming a multi-core architecture in which each core performs stochastic gradient updates to a centrally stored vector x in an asynchronous manner. 2. Read Schedule iterate: Read the schedule iterate A and calculate the gradients required for an update to algorithm 1. 3. Update: Update the itterate x with the calculated incremental update in algorithm 1. Read Schedule iterate: Read the schedule iterate A and calculate the gradients required for an update to algorithm 1. Update: Update the itterate x with the calculated incremental update in algorithm 1. We assume that each iterate is running a valid update for the current iteration 3."}, {"heading": "3.1 Convergence Analysis", "text": "The main components of the success of asynchronous algorithms for multicore stochastic gradient descent = = limited descent are sparsity and \"disjointness\" of the data matrix [20]. We also use these properties of the data for our convergence analysis. Formally, one considers the norm of x in relation to the non-zero coordinates of the function fi; then the convergence depends on what the smallest convergence analysis for asynchronous SVRG can look like. The general case is similar, but much more involved. Hence, it is instructive to go first through the analysis of asynchronous SVRG."}, {"heading": "Suppose probabilities pi \u221d (1\u2212 1\u03ba )", "text": "m \u2212 i, parameter \u03b2, \u03ba, step \u03b7, and epoch size m are selected in such a way that the following conditions are met: 1 \u0445 + 8\u0445L (1 + 1\u03b2) + 96\u0445L\u03c4n (1 \u2212 1\u0432) \u2212 \u03c4 \u2264 1 n, \u03b72 \u2264 (1 \u2212 1 \u0445) m \u2212 1 12L2 \u0445 2, \u03b3a > 0, \u03b8a < 1.Then we have for the iteration of the asynchronous variant of algorithm 1 with the HSAG scheme E [f (x-k + 1) \u2212 f (x-p) + 1\u03b3a G-k + 1] \u2264 \u03b8aE [f (x-k) \u2212 f (x-k) + 1\u03b3a G-k].Conclusion 2. Note that G-k \u2265 0 and therefore, under the conditions given in theorem 3, the properties of theorem 3 and the same theorem (1 + 1 / 2-p) can be synchronized."}, {"heading": "4 Experiments", "text": "We will continue to work closely with our partners and partners to ensure that we are able to deliver the best possible service to our customers, and we will continue to do so, and we will continue to do so, and we will continue to do so, and we will continue to do so, and we will continue to do so, and we will continue to do so, and we will continue to do so, and we will continue to do so, and we will continue to do so, and we will continue to do so, and we will continue to do so, and we will continue to do so, and we will continue to do so, and we will continue to do so."}, {"heading": "5 Discussion & Future Work", "text": "In this paper, we presented a unified framework based on [5] that covers many popular techniques for reducing variance for stochastic gradient parentage. To this end, we provided convergence analyses for the framework under certain conditions. More importantly, we propose an asynchronous algorithm for the framework with verifiable convergence guarantees. The main consequence of our approach is that we obtain asynchronous variants of multiple algorithms such as SVRG, 2http: / / eigen.tuxfamily.org / 3http: / / www.csie.ntu.edu.tw / cjlin / libsvmtools / datasets / binary.htmlSAGA and S2GD. Our asynchronous algorithms exploit the scarcity of data to obtain near-linear speed dups in settings that would typically be interesting in comparison between different work areas."}, {"heading": "A Appendix", "text": "Notation: We use Df to denote the Bregman divergence (defined below) for function f.Df (x, y) = f (x) \u2212 f (y) \u2212 < f (y), x \u2212 y >. To simplify the representation, we use E [X] to denote the expectation of the random variable X with respect to indexes {i1,..., es} if X is exactly from these indexes to step t. This dependence is clear from the context. We use 1 to denote the indicator function."}, {"heading": "Proof of Theorem 1", "text": "We extend the f function as f (x) = g (x) + h (x), where g (x) = 1n (x) and g (x) = 1 n (x). We define the current epoch as k + 1. We define the following: vt = 1\u03b7 (xt + 1 \u2212 xt) = \u2212 fit (xt) \u2212 fit (\u03b1tit) + 1n (\u03b1 t)] Gt = 1n (fi (fi) \u2212 fi (x) \u2212 fi (x) \u2212 fi (x) \u2212 fi (f) \u2212 fi (x) \u2212 fi (x), p (x) >) Rt = E [c) xt \u2212 x + Gt. First, we determine that E [vt] = \u2212 f (xt) \u2212 fi (xt) \u2212 fi (x) \u2212 fi (x) \u2212 f (f), f) f (x), f) f) f (x), f) f (x)."}, {"heading": "Proof of Theorem 2", "text": "Proof that the current epoch is k + 1."}, {"heading": "Proof of Theorem 3", "text": "We assume that the iterations used in the respective iteration are from the same time step (index), i.e., D (t) = D (t) = D (t) = D (t) for all t). Remember that D (t) and D (t) denote the index used in the tth iteration of the algorithm. Our analysis can be easily extended to the case of D (t) 6 = D (t). We extend the function f as f (x) = g (x) + h (x), where g) = 1n (x) and g (x) = 1 n). We define the following: ut = 1 (xt + 1)."}, {"heading": "Other Lemmatta", "text": "Lemma 1. [9] The proof is trivial from the fact that x-fold is the optimal solution and non-negative properties of the Bregman."}], "references": [{"title": "A lower bound for the optimization of finite sums", "author": ["Alekh Agarwal", "Leon Bottou"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Distributed delayed stochastic optimization", "author": ["Alekh Agarwal", "John C Duchi"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Incremental gradient, subgradient, and proximal methods for convex optimization: A survey", "author": ["Dimitri P Bertsekas"], "venue": "Optimization for Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "New Optimization Methods for Machine Learning", "author": ["Aaron Defazio"], "venue": "PhD thesis, Australian National University,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives", "author": ["Aaron Defazio", "Francis Bach", "Simon Lacoste-Julien"], "venue": "In NIPS", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Finito: A faster, permutable incremental gradient method for big data problems", "author": ["Aaron J Defazio", "Tib\u00e9rio S Caetano", "Justin Domke"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Optimal distributed online prediction using mini-batches", "author": ["Ofer Dekel", "Ran Gilad-Bachrach", "Ohad Shamir", "Lin Xiao"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "A globally convergent incremental Newton method", "author": ["M. G\u00fcrb\u00fczbalaban", "A. Ozdaglar", "P. Parrilo"], "venue": "Mathematical Programming,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In NIPS", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Mini-Batch Semi-Stochastic Gradient Descent in the Proximal Setting", "author": ["Jakub Kone\u010dn\u00fd", "Jie Liu", "Peter Richt\u00e1rik", "Martin Tak\u00e1\u010d"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Communication Efficient Distributed Machine Learning with the Parameter Server", "author": ["Mu Li", "David G Andersen", "Alex J Smola", "Kai Yu"], "venue": "In NIPS", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Asynchronous stochastic coordinate descent: Parallelism and convergence properties", "author": ["Ji Liu", "Stephen J. Wright"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "An asynchronous parallel stochastic coordinate descent algorithm", "author": ["Ji Liu", "Steve Wright", "Christopher R\u00e9", "Victor Bittorf", "Srikrishna Sridhar"], "venue": "ICML", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Optimization with first-order surrogate functions", "author": ["Julien Mairal"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Distributed asynchronous incremental subgradient methods", "author": ["A Nedi\u0107", "Dimitri P Bertsekas", "Vivek S Borkar"], "venue": "Studies in Computational Mathematics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Efficiency of coordinate descent methods on huge-scale optimization problems", "author": ["Yu Nesterov"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Stochastic Proximal Gradient Descent with Acceleration Techniques", "author": ["Atsushi Nitanda"], "venue": "In NIPS", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent", "author": ["Benjamin Recht", "Christopher Re", "Stephen Wright", "Feng Niu"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function", "author": ["Peter Richt\u00e1rik", "Martin Tak\u00e1\u010d"], "venue": "Mathematical Programming,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1951}, {"title": "Minimizing Finite Sums with the Stochastic Average Gradient", "author": ["Mark W. Schmidt", "Nicolas Le Roux", "Francis R. Bach"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Accelerated mini-batch stochastic dual coordinate ascent", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "In NIPS", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Stochastic dual coordinate ascent methods for regularized loss", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "On distributed stochastic optimization and learning", "author": ["Ohad Shamir", "Nathan Srebro"], "venue": "In Proceedings of the 52nd Annual Allerton Conference on Communication, Control, and Computing,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "A proximal stochastic gradient method with progressive variance reduction", "author": ["Lin Xiao", "Tong Zhang"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "There has been a steep rise in recent work [5, 6, 8\u201311, 23, 25, 27] on \u201cvariance reduced\u201d stochastic gradient algorithms for convex problems of the finite-sum form:", "startOffset": 43, "endOffset": 67}, {"referenceID": 5, "context": "There has been a steep rise in recent work [5, 6, 8\u201311, 23, 25, 27] on \u201cvariance reduced\u201d stochastic gradient algorithms for convex problems of the finite-sum form:", "startOffset": 43, "endOffset": 67}, {"referenceID": 7, "context": "There has been a steep rise in recent work [5, 6, 8\u201311, 23, 25, 27] on \u201cvariance reduced\u201d stochastic gradient algorithms for convex problems of the finite-sum form:", "startOffset": 43, "endOffset": 67}, {"referenceID": 8, "context": "There has been a steep rise in recent work [5, 6, 8\u201311, 23, 25, 27] on \u201cvariance reduced\u201d stochastic gradient algorithms for convex problems of the finite-sum form:", "startOffset": 43, "endOffset": 67}, {"referenceID": 9, "context": "There has been a steep rise in recent work [5, 6, 8\u201311, 23, 25, 27] on \u201cvariance reduced\u201d stochastic gradient algorithms for convex problems of the finite-sum form:", "startOffset": 43, "endOffset": 67}, {"referenceID": 21, "context": "There has been a steep rise in recent work [5, 6, 8\u201311, 23, 25, 27] on \u201cvariance reduced\u201d stochastic gradient algorithms for convex problems of the finite-sum form:", "startOffset": 43, "endOffset": 67}, {"referenceID": 23, "context": "There has been a steep rise in recent work [5, 6, 8\u201311, 23, 25, 27] on \u201cvariance reduced\u201d stochastic gradient algorithms for convex problems of the finite-sum form:", "startOffset": 43, "endOffset": 67}, {"referenceID": 25, "context": "There has been a steep rise in recent work [5, 6, 8\u201311, 23, 25, 27] on \u201cvariance reduced\u201d stochastic gradient algorithms for convex problems of the finite-sum form:", "startOffset": 43, "endOffset": 67}, {"referenceID": 15, "context": "Under strong convexity assumptions such variance reduced (VR) stochastic algorithms attain better convergence rates (in expectation) than stochastic gradient descent (SGD) [17, 22], both in theory and practice.", "startOffset": 172, "endOffset": 180}, {"referenceID": 20, "context": "Under strong convexity assumptions such variance reduced (VR) stochastic algorithms attain better convergence rates (in expectation) than stochastic gradient descent (SGD) [17, 22], both in theory and practice.", "startOffset": 172, "endOffset": 180}, {"referenceID": 1, "context": "And in this setting, asynchronous variants of SGD remain indispensable [2, 7, 12, 20, 26, 28].", "startOffset": 71, "endOffset": 93}, {"referenceID": 6, "context": "And in this setting, asynchronous variants of SGD remain indispensable [2, 7, 12, 20, 26, 28].", "startOffset": 71, "endOffset": 93}, {"referenceID": 10, "context": "And in this setting, asynchronous variants of SGD remain indispensable [2, 7, 12, 20, 26, 28].", "startOffset": 71, "endOffset": 93}, {"referenceID": 18, "context": "And in this setting, asynchronous variants of SGD remain indispensable [2, 7, 12, 20, 26, 28].", "startOffset": 71, "endOffset": 93}, {"referenceID": 24, "context": "And in this setting, asynchronous variants of SGD remain indispensable [2, 7, 12, 20, 26, 28].", "startOffset": 71, "endOffset": 93}, {"referenceID": 8, "context": "Our methods are inspired by the influential SVRG [9], S2GD [11], SAG [23] and SAGA [5] family of algorithms.", "startOffset": 49, "endOffset": 52}, {"referenceID": 21, "context": "Our methods are inspired by the influential SVRG [9], S2GD [11], SAG [23] and SAGA [5] family of algorithms.", "startOffset": 69, "endOffset": 73}, {"referenceID": 4, "context": "Our methods are inspired by the influential SVRG [9], S2GD [11], SAG [23] and SAGA [5] family of algorithms.", "startOffset": 83, "endOffset": 86}, {"referenceID": 4, "context": "Our paper has two core components: (i) a formal general framework for variance reduced stochastic methods based on discussions in [5]; and (ii) asynchronous parallel VR algorithms within the framework.", "startOffset": 130, "endOffset": 133}, {"referenceID": 21, "context": "As already mentioned, our work is closest to (and generalizes) SAG [23], SAGA [5], SVRG [9] and S2GD [11], which are primal methods.", "startOffset": 67, "endOffset": 71}, {"referenceID": 4, "context": "As already mentioned, our work is closest to (and generalizes) SAG [23], SAGA [5], SVRG [9] and S2GD [11], which are primal methods.", "startOffset": 78, "endOffset": 81}, {"referenceID": 8, "context": "As already mentioned, our work is closest to (and generalizes) SAG [23], SAGA [5], SVRG [9] and S2GD [11], which are primal methods.", "startOffset": 88, "endOffset": 91}, {"referenceID": 23, "context": "Also closely related are dual methods such as SDCA [25] and Finito [6], and in its convex incarnation MISO [15]; a more precise relation between these dual methods and VR stochastic methods is described in Defazio\u2019s thesis [4].", "startOffset": 51, "endOffset": 55}, {"referenceID": 5, "context": "Also closely related are dual methods such as SDCA [25] and Finito [6], and in its convex incarnation MISO [15]; a more precise relation between these dual methods and VR stochastic methods is described in Defazio\u2019s thesis [4].", "startOffset": 67, "endOffset": 70}, {"referenceID": 13, "context": "Also closely related are dual methods such as SDCA [25] and Finito [6], and in its convex incarnation MISO [15]; a more precise relation between these dual methods and VR stochastic methods is described in Defazio\u2019s thesis [4].", "startOffset": 107, "endOffset": 111}, {"referenceID": 3, "context": "Also closely related are dual methods such as SDCA [25] and Finito [6], and in its convex incarnation MISO [15]; a more precise relation between these dual methods and VR stochastic methods is described in Defazio\u2019s thesis [4].", "startOffset": 223, "endOffset": 226}, {"referenceID": 2, "context": "By their algorithmic structure, these VR methods trace back to classical non-stochastic incremental gradient algorithms [3], but by now it is well-recognized that randomization helps obtain much sharper convergence results (in expectation).", "startOffset": 120, "endOffset": 123}, {"referenceID": 25, "context": "Proximal [27] and accelerated VR methods have also been proposed [19, 24]; we leave a study of such variants of our framework as future work.", "startOffset": 9, "endOffset": 13}, {"referenceID": 17, "context": "Proximal [27] and accelerated VR methods have also been proposed [19, 24]; we leave a study of such variants of our framework as future work.", "startOffset": 65, "endOffset": 73}, {"referenceID": 22, "context": "Proximal [27] and accelerated VR methods have also been proposed [19, 24]; we leave a study of such variants of our framework as future work.", "startOffset": 65, "endOffset": 73}, {"referenceID": 0, "context": "Finally, there is recent work on lower-bounds for finite-sum problems [1].", "startOffset": 70, "endOffset": 73}, {"referenceID": 18, "context": "Within asynchronous SGD algorithms, both parallel [20] and distributed [2, 16] variants are known.", "startOffset": 50, "endOffset": 54}, {"referenceID": 1, "context": "Within asynchronous SGD algorithms, both parallel [20] and distributed [2, 16] variants are known.", "startOffset": 71, "endOffset": 78}, {"referenceID": 14, "context": "Within asynchronous SGD algorithms, both parallel [20] and distributed [2, 16] variants are known.", "startOffset": 71, "endOffset": 78}, {"referenceID": 11, "context": "A different line of methods is that of (primal) coordinate descent methods, and their parallel and distributed variants [13, 14, 18, 21].", "startOffset": 120, "endOffset": 136}, {"referenceID": 12, "context": "A different line of methods is that of (primal) coordinate descent methods, and their parallel and distributed variants [13, 14, 18, 21].", "startOffset": 120, "endOffset": 136}, {"referenceID": 16, "context": "A different line of methods is that of (primal) coordinate descent methods, and their parallel and distributed variants [13, 14, 18, 21].", "startOffset": 120, "endOffset": 136}, {"referenceID": 19, "context": "A different line of methods is that of (primal) coordinate descent methods, and their parallel and distributed variants [13, 14, 18, 21].", "startOffset": 120, "endOffset": 136}, {"referenceID": 9, "context": "Finally, the recent work [10] generalizes S2GD to the mini-batch setting, thereby also permitting parallel processing, albeit with more synchronization and allowing only small mini-batches.", "startOffset": 25, "endOffset": 29}, {"referenceID": 25, "context": "1) While our analysis focuses on strongly convex functions, we can extend it to just smooth convex functions along the lines of [27].", "startOffset": 128, "endOffset": 132}, {"referenceID": 4, "context": "Inspired by the discussion on a general view of variance reduced techniques in [5], we now describe a formal general framework for variance reduction in stochastic gradient descent.", "startOffset": 79, "endOffset": 82}, {"referenceID": 21, "context": "In particular, we consider incremental methods SAG [23], SVRG [9] and SAGA [5], and classic gradient descent GRADIENTDESCENT for demonstrating our framework.", "startOffset": 51, "endOffset": 55}, {"referenceID": 8, "context": "In particular, we consider incremental methods SAG [23], SVRG [9] and SAGA [5], and classic gradient descent GRADIENTDESCENT for demonstrating our framework.", "startOffset": 62, "endOffset": 65}, {"referenceID": 4, "context": "In particular, we consider incremental methods SAG [23], SVRG [9] and SAGA [5], and classic gradient descent GRADIENTDESCENT for demonstrating our framework.", "startOffset": 75, "endOffset": 78}, {"referenceID": 8, "context": "In case of SVRG, SCHEDULEUPDATE is triggered every m iterations (here m denotes precisely the number of inner iterations used in [9]); so A remains unchanged for the m iterations and all \u03b1 i are updated to the current iterate at the mth iteration.", "startOffset": 129, "endOffset": 132}, {"referenceID": 8, "context": "Similar to [9] (see Option II of SVRG in [9]), our analysis will be for the case where the iterate at the end of (k + 1)st epoch, x, is replaced with an element chosen randomly from {x, .", "startOffset": 11, "endOffset": 14}, {"referenceID": 8, "context": "Similar to [9] (see Option II of SVRG in [9]), our analysis will be for the case where the iterate at the end of (k + 1)st epoch, x, is replaced with an element chosen randomly from {x, .", "startOffset": 41, "endOffset": 44}, {"referenceID": 4, "context": "5) with m = O(n) epoch size (similar to [5, 9]).", "startOffset": 40, "endOffset": 46}, {"referenceID": 8, "context": "5) with m = O(n) epoch size (similar to [5, 9]).", "startOffset": 40, "endOffset": 46}, {"referenceID": 18, "context": "Our model of computation is similar to the ones used in Hogwild! [20] and AsySCD [14].", "startOffset": 65, "endOffset": 69}, {"referenceID": 12, "context": "Our model of computation is similar to the ones used in Hogwild! [20] and AsySCD [14].", "startOffset": 81, "endOffset": 85}, {"referenceID": 12, "context": ", [14, 20]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 18, "context": ", [14, 20]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 18, "context": "1 Convergence Analysis The key ingredients to the success of asynchronous algorithms for multicore stochastic gradient descent are sparsity and \u201cdisjointness\u201d of the data matrix [20].", "startOffset": 178, "endOffset": 182}, {"referenceID": 4, "context": "For example, in the case of SAGA, one can obtain per iteration convergence guarantees (see [5]) rather than those corresponding to per epoch presented in the paper.", "startOffset": 91, "endOffset": 94}, {"referenceID": 4, "context": "Furthermore, in this case, our analysis for both synchronous and asynchronous cases can be easily modified to obtain convergence properties similar to the ones obtained in [5].", "startOffset": 172, "endOffset": 175}, {"referenceID": 18, "context": "Instead, similar to [20], we rewrite problem in (4.", "startOffset": 20, "endOffset": 24}, {"referenceID": 21, "context": "This problem can be circumvented by using a \u2018just-in-time\u2019 update scheme similar to the one mentioned in [23].", "startOffset": 105, "endOffset": 109}, {"referenceID": 18, "context": "Parameter updates are performed through atomic compare-and-swap instruction facilitated by modern processors [20].", "startOffset": 109, "endOffset": 113}, {"referenceID": 18, "context": "\u2022 Lock-Free SGD: This is the lock-free asynchronous variant of the SGD algorithm (see [20]).", "startOffset": 86, "endOffset": 90}, {"referenceID": 25, "context": "Similar to [27], we normalize each example in the dataset so that \u2016zi\u20162 = 1 for all i \u2208 [n].", "startOffset": 11, "endOffset": 15}, {"referenceID": 8, "context": "The epoch size m is chosen as 2n (as recommended in [9]) in all our experiments.", "startOffset": 52, "endOffset": 55}, {"referenceID": 18, "context": "Similar speedup behavior was reported for this dataset in [20].", "startOffset": 58, "endOffset": 62}, {"referenceID": 18, "context": "It should be noted that this dataset is not sparse and hence, is a bad case for the algorithm (similar to [20]).", "startOffset": 106, "endOffset": 110}, {"referenceID": 8, "context": "The performance gains are qualitatively similar to those reported in [9] for the synchronous versions of these algorithms.", "startOffset": 69, "endOffset": 72}, {"referenceID": 4, "context": "In this paper, we presented a unifying framework based on [5], that captures many popular variance reduction techniques for stochastic gradient descent.", "startOffset": 58, "endOffset": 61}, {"referenceID": 0, "context": "Bibliography [1] Alekh Agarwal and Leon Bottou.", "startOffset": 13, "endOffset": 16}, {"referenceID": 1, "context": "[2] Alekh Agarwal and John C Duchi.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Dimitri P Bertsekas.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Aaron Defazio.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Aaron J Defazio, Tib\u00e9rio S Caetano, and Justin Domke.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Rie Johnson and Tong Zhang.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Jakub Kone\u010dn\u00fd, Jie Liu, Peter Richt\u00e1rik, and Martin Tak\u00e1\u010d.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] Mu Li, David G Andersen, Alex J Smola, and Kai Yu.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] Ji Liu and Stephen J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] Ji Liu, Steve Wright, Christopher R\u00e9, Victor Bittorf, and Srikrishna Sridhar.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] Julien Mairal.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] A Nedi\u0107, Dimitri P Bertsekas, and Vivek S Borkar.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] Yu Nesterov.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] Atsushi Nitanda.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21] Peter Richt\u00e1rik and Martin Tak\u00e1\u010d.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23] Mark W.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[24] Shai Shalev-Shwartz and Tong Zhang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] Shai Shalev-Shwartz and Tong Zhang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] Ohad Shamir and Nathan Srebro.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[27] Lin Xiao and Tong Zhang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "The first inequality and second inequalities on T2 directly follows from Lemma 3 of [5] and simple application of Lemma 1 respectively.", "startOffset": 84, "endOffset": 87}, {"referenceID": 4, "context": "From Lemma 3 of [5] (also see [9]), we have E[\u2016v\u2016] \u2264 4LE [ f(x)\u2212 f(x\u2217) + f(x\u0303)\u2212 f(x\u2217) ] .", "startOffset": 16, "endOffset": 19}, {"referenceID": 8, "context": "From Lemma 3 of [5] (also see [9]), we have E[\u2016v\u2016] \u2264 4LE [ f(x)\u2212 f(x\u2217) + f(x\u0303)\u2212 f(x\u2217) ] .", "startOffset": 30, "endOffset": 33}, {"referenceID": 8, "context": "[9] For any \u03b1i \u2208 R where i \u2208 [n] and x\u2217, we have E [ \u2016\u2207fit(\u03b1it)\u2212\u2207fit(x)\u2016 ] \u2264 2L n \u2211", "startOffset": 0, "endOffset": 3}], "year": 2015, "abstractText": "We study optimization algorithms based on variance reduction for stochastic gradient descent (SGD). Remarkable recent progress has been made in this direction through development of algorithms like SAG, SVRG, SAGA. These algorithms have been shown to outperform SGD, both theoretically and empirically. However, asynchronous versions of these algorithms\u2014a crucial requirement for modern large-scale applications\u2014have not been studied. We bridge this gap by presenting a unifying framework for many variance reduction techniques. Subsequently, we propose an asynchronous algorithm grounded in our framework, and prove its fast convergence. An important consequence of our general approach is that it yields asynchronous versions of variance reduction algorithms such as SVRG and SAGA as a byproduct. Our method achieves near linear speedup in sparse settings common to machine learning. We demonstrate the empirical performance of our method through a concrete realization of asynchronous SVRG.", "creator": "LaTeX with hyperref package"}}}