{"id": "1506.06863", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2015", "title": "deltaBLEU: A Discriminative Metric for Generation Tasks with Intrinsically Diverse Targets", "abstract": "We introduce Discriminative BLEU (deltaBLEU), a novel metric for intrinsic evaluation of generated text in tasks that admit a diverse range of possible outputs. Reference strings are scored for quality by human raters on a scale of [-1, +1] to weight multi-reference BLEU. In tasks involving generation of conversational responses, deltaBLEU correlates reasonably with human judgments and outperforms sentence-level and IBM BLEU in terms of both Spearman's who and Kendall's tau.", "histories": [["v1", "Tue, 23 Jun 2015 05:24:53 GMT  (54kb,D)", "https://arxiv.org/abs/1506.06863v1", "To appear at ACL 2015, 6 pages"], ["v2", "Wed, 24 Jun 2015 01:09:50 GMT  (54kb,D)", "http://arxiv.org/abs/1506.06863v2", "6 pages, to appear at ACL 2015"]], "COMMENTS": "To appear at ACL 2015, 6 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["michel galley", "chris brockett", "alessandro sordoni", "yangfeng ji", "michael auli", "chris quirk", "margaret mitchell", "jianfeng gao", "bill dolan"], "accepted": true, "id": "1506.06863"}, "pdf": {"name": "1506.06863.pdf", "metadata": {"source": "CRF", "title": "\u2206BLEU: A Discriminative Metric for Generation Tasks with Intrinsically Diverse Targets\u2217", "authors": ["Michel Galley", "Chris Brockett", "Alessandro Sordoni", "Yangfeng Ji", "Michael Auli", "Chris Quirk", "Margaret Mitchell", "Jianfeng Gao", "Bill Dolan"], "emails": ["mgalley@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "The tasks with intrinsically different objectives range from machine translation, summary, sentence compression, paraphrase generation and picture-to-text generation to the generation of conversation interactions. A major hurdle for these tasks is the automation of evaluation, since the space of plausible results can be enormous, and it is impractical to perform a new human evaluation every time a new model is built or parameters are modified. In statistical machine translation (SMT), the automation problem is largely due to metrics such as BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) Although BLEU is not immune to criticism (e.g. Callison-Burch et al. (2006), its properties in the ACL are July 2015 (filed April 30, 2015, accepted June 9, 2015)."}, {"heading": "2 Evaluating Conversational Responses", "text": "Considering an input message m and a previous conversation history c, the goal of a response generation system is to create a hypothesis h (BLP) that contains both well-formed and relevant answers to question m (example in Fig. 1). We assume that a set of J references {ri, j} for context i and message mi, where i-BLP (1) finds a unique answer to the question of whether the automatic evaluation of system output h1. hI is defined as: BLEU = BP \u00b7 exp (1) with: BP = (1) if the EU task (1 \u2212 2) is otherwise where the automatic evaluation of system output h1. hI level n-gram precision is defined as: pn = i-gram precision. (hi) n-grams (hi) maxj {g (hi, ri, j)."}, {"heading": "4 Data", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Multi-reference Datasets", "text": "To use the multi-reference BLEU dev and test sets used in this study, we customized and expanded themethodology of Sordoni et al. (2015). From a corpus of 29M Twitter context message response conversation triples, we were randomly extracted about 33K candidate responses, which were then assessed for conversation quality on a 5-point scale of likert type by using 3 crowd-sourced annotators. Of these 4232 triples scored an average of 4 or higher; these were randomly selected to create seed dev and test sets of 2118 triples and 2114 triples respectively. Note that the dev set is not used in the experiments of this paper, as BLEU and IBM BLEU are metrics that do not require training. However, the dev set will be published along with a test set in the dataset publication we accompanied this paper.We then tried to identify candidate triples in the corpus 29pus."}, {"heading": "4.2 Human Evaluation of System Outputs", "text": "The responses generated by the 7 systems used in this study based on the 2114 triple test set were hand-rated by 5 crowdsourced raters on a 5-point Likert scale. Of these 7 systems, 12 pairs of systems were rated, for a total of approximately 126K ratings (12 \u00b7 5 \u00b7 2114). Here, too, the rating agencies were asked to evaluate the responses in terms of their relevance both to the context and to the message. Results from different systems were randomly linked to each other to present them to the rating agencies. We received human ratings for the following systems: Phrase-based MT: A Phrase-based MT system similar (Ritter et al., 2011), whose weights were manually matched. We also included four variants of this system, which we matched to MERT (Och, 2003). These variants differ in their number of characteristics and audit (Ritter et al., 2011), with the following phratical characteristics, the target-level distance and cosmic similarity:"}, {"heading": "5 Setup", "text": "In order to assess the level of correlation between human qualitative assessments (\u00a7 4.2) and automated metric values, we use two rank correlation coefficients - Kendall's \u03c4 and Spearmans \u03c1. More formally, we calculate each correlation coefficient based on a series of paired observations (m1, q1), \u00b7 \u00b7 \u00b7, (mN, qN), which are differences in automatic metric values and qualitative values for two systems A and B on a certain subset of the test set. 4While many previous work evaluated automatic metrics for MT and other tasks (Lavie and Agarwal, 2007; Hodosh et al., 2013) by calculating correlations on observations consisting of deployment system results, it has been shown (e.g. Przybocki et al. (2008) that correlation coefficients increase significantly at the MT level (observation units increase). E.g. EU or metric correlations on the MT level are increasing."}, {"heading": "6 Results", "text": "The main results of our study are shown in Table 2. \u2206 BLEU achieves a better correlation with humans than BLEU when comparing the best configuration of each metric. [8] In the case of Spearmans \u03c1, 4For each given observation pair (mi, qi), we randomize the order in which A and B are presented to rating agencies to avoid any kind of distortion. [8] We do not intend to minimize the benefit of a metric that would be competitive at the sentence level, which would be particularly useful for detailed error analysis. [9] However, our main goal is to reliably evaluate generational systems on test sets of thousands of judgments, with any metric with good corpus-level correlation (such as BLEU as shown in (Graham and Baldwin, 2014) to be BLEU-wide enough. [9] All possible ways of mapping judgments to observations would lead to observations leading to observations leading to a combinatorial explosion."}, {"heading": "7 Conclusions", "text": "\u2206 BLEU correlates well with assessments of human-quality generated conversation responses, surpassing both IBM BLEU and the BLEU sentence level in this task, and demonstrates that it can serve as a plausible intrinsic metric for system development. [9] Human evaluation of the reference set comes at an upfront cost, but afterwards, the need for further human evaluation during system development can be minimized. [BLEU] can help with other tasks where multiple references are used for intrinsic evaluation, including image-to-text, sentence compression and paraphrase generation, and even statistical machine translation."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers, Jian-Yun Nie and Alan Ritter, for their helpful comments and suggestions. An implementation of BLEU, multi-reference developers and test kits and humanely evaluated results is available at: http: / / research.microsoft.com / conveno"}], "references": [{"title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments", "author": ["Banerjee", "Lavie2005] Satanjeev Banerjee", "Alon Lavie"], "venue": "In Proc. of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine", "citeRegEx": "Banerjee et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2005}, {"title": "Reevaluating the role of BLEU in machine translation research", "author": ["Miles Osborne", "Philipp Koehn"], "venue": "In EACL,", "citeRegEx": "Callison.Burch et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Callison.Burch et al\\.", "year": 2006}, {"title": "Correlating automated and human assessments of machine translation quality", "author": ["Deborah Coughlin"], "venue": "In Proc. of MT Summit IX,", "citeRegEx": "Coughlin.,? \\Q2003\\E", "shortCiteRegEx": "Coughlin.", "year": 2003}, {"title": "Automatic evaluation of machine translation quality using n-gram co-occurrence statistics", "author": ["George Doddington"], "venue": "In Proc. of HLT,", "citeRegEx": "Doddington.,? \\Q2002\\E", "shortCiteRegEx": "Doddington.", "year": 2002}, {"title": "HyTER: Meaning-equivalent semantics for translation evaluation", "author": ["Dreyer", "Marcu2012] Markus Dreyer", "Daniel Marcu"], "venue": "In Proc. of HLTNAACL,", "citeRegEx": "Dreyer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dreyer et al\\.", "year": 2012}, {"title": "Testing for significance of increased correlation with human judgment", "author": ["Graham", "Baldwin2014] Yvette Graham", "Timothy Baldwin"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Graham et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graham et al\\.", "year": 2014}, {"title": "Accurate evaluation of segment-level machine translation metrics", "author": ["Graham et al.2015] Yvette Graham", "Timothy Baldwin", "Nitika Mathur"], "venue": "In Proc. of NAACL-HLT,", "citeRegEx": "Graham et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Graham et al\\.", "year": 2015}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["Hodosh et al.2013] Micah Hodosh", "Peter Young", "Julia Hockenmaier"], "venue": "J. Artif. Int. Res.,", "citeRegEx": "Hodosh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["Huang et al.2013] Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Acero", "Larry Heck"], "venue": "In Proc. of the 22nd ACM International Conference on Information", "citeRegEx": "Huang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "METEOR: An automatic metric for mt evaluation with high levels of correlation with human judgments", "author": ["Lavie", "Agarwal2007] Alon Lavie", "Abhaya Agarwal"], "venue": "In Proc. of the Workshop on Statistical Machine Translation (StatMT),", "citeRegEx": "Lavie et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lavie et al\\.", "year": 2007}, {"title": "Optimizing for SentenceLevel BLEU+1 Yields Short Translations", "author": ["Nakov et al.2012] Preslav Nakov", "Francisco Guzman", "Stephan Vogel"], "venue": "In Proc. of COLING", "citeRegEx": "Nakov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nakov et al\\.", "year": 2012}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och"], "venue": "In Proc. of ACL,", "citeRegEx": "Och.,? \\Q2003\\E", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proc. of ACL,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Official results of the NIST 2008 \u201dMetrics for MAchine TRanslation", "author": ["K. Peterson", "S. Bronsart"], "venue": null, "citeRegEx": "Przybocki et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Przybocki et al\\.", "year": 2008}, {"title": "Data-driven response generation in social media", "author": ["Ritter et al.2011] Alan Ritter", "Colin Cherry", "William B. Dolan"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Ritter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "A neural network approach to contextsensitive generation of conversational responses", "author": ["Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Meg Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan"], "venue": null, "citeRegEx": "Sordoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Joint learning of a dual SMT system for paraphrase generation", "author": ["Sun", "Zhou2012] Hong Sun", "Ming Zhou"], "venue": "In ACL,", "citeRegEx": "Sun et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2012}, {"title": "CIDEr: Consensus-based image description evaluation", "author": ["Vedantam", "C. Lawrence Zitnick", "Devi Parikh."], "venue": "CVPR.", "citeRegEx": "Vedantam et al\\.,? 2015", "shortCiteRegEx": "Vedantam et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 12, "context": "In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) Although BLEU is not immune from criticism (e.", "startOffset": 128, "endOffset": 151}, {"referenceID": 1, "context": ", Callison-Burch et al. (2006)), its properties are well", "startOffset": 2, "endOffset": 31}, {"referenceID": 3, "context": "understood, BLEU scores have been shown to correlate well with human judgments (Doddington, 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed.", "startOffset": 79, "endOffset": 160}, {"referenceID": 2, "context": "understood, BLEU scores have been shown to correlate well with human judgments (Doddington, 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed.", "startOffset": 79, "endOffset": 160}, {"referenceID": 6, "context": "understood, BLEU scores have been shown to correlate well with human judgments (Doddington, 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed.", "startOffset": 79, "endOffset": 160}, {"referenceID": 14, "context": "Our testbed for this metric is data-driven conversation, a field that has begun to attract interest (Ritter et al., 2011; Sordoni et al., 2015) as an alternative to conventional rule-driven or scripted dialog systems.", "startOffset": 100, "endOffset": 143}, {"referenceID": 15, "context": "Our testbed for this metric is data-driven conversation, a field that has begun to attract interest (Ritter et al., 2011; Sordoni et al., 2015) as an alternative to conventional rule-driven or scripted dialog systems.", "startOffset": 100, "endOffset": 143}, {"referenceID": 13, "context": "It has been demonstrated that metrics such as BLEU show increased correlation with human judgment as the number of references increases (Przybocki et al., 2008; Dreyer and Marcu, 2012).", "startOffset": 136, "endOffset": 184}, {"referenceID": 12, "context": "Unless mentioned otherwise, BLEU refers to the original IBM BLEU as first described in (Papineni et al., 2002).", "startOffset": 87, "endOffset": 110}, {"referenceID": 15, "context": "methodology of Sordoni et al. (2015). From a corpus of 29M Twitter context-message-response conversational triples, we randomly extracted approximately 33K candidate triples that were then judged for conversational quality on a 5-point Likert-type scale by 3 crowdsourced annotators.", "startOffset": 15, "endOffset": 37}, {"referenceID": 15, "context": ", 1995), as detailed in Sordoni et al. (2015), to extract the top 15 responses for each messageresponse pair.", "startOffset": 24, "endOffset": 46}, {"referenceID": 15, "context": ", 1995), as detailed in Sordoni et al. (2015), to extract the top 15 responses for each messageresponse pair. Unlike Sordoni et al. (2015), we further appended the original messages (as if parroted back).", "startOffset": 24, "endOffset": 139}, {"referenceID": 15, "context": ", 1995), as detailed in Sordoni et al. (2015), to extract the top 15 responses for each messageresponse pair. Unlike Sordoni et al. (2015), we further appended the original messages (as if parroted back). The new triples were then scored for quality of the response in light of both context and message by 5 crowdsourced raters each on a 5point Likert-type scale.3 Crucially, and again in contradistinction to Sordoni et al. (2015), we did not impose a score cutoff on these synthetic multireference sets.", "startOffset": 24, "endOffset": 432}, {"referenceID": 14, "context": "Phrase-based MT: A phrase-based MT system similar to (Ritter et al., 2011), whose weights have been manually tuned.", "startOffset": 53, "endOffset": 74}, {"referenceID": 11, "context": "We also included four variants of that system, which we tuned with MERT (Och, 2003).", "startOffset": 72, "endOffset": 83}, {"referenceID": 14, "context": "These variants differ in their number of features, and augment (Ritter et al., 2011) with the following phrase-level features: edit distance between source and target, cosine similarity, Jaccard index and distance, length ratio, and DSSM score (Huang et al.", "startOffset": 63, "endOffset": 84}, {"referenceID": 8, "context": ", 2011) with the following phrase-level features: edit distance between source and target, cosine similarity, Jaccard index and distance, length ratio, and DSSM score (Huang et al., 2013).", "startOffset": 167, "endOffset": 187}, {"referenceID": 15, "context": "RNN-based MT: the log-probability according to the RNN model of (Sordoni et al., 2015).", "startOffset": 64, "endOffset": 86}, {"referenceID": 7, "context": "4While much prior work assesses automatic metrics for MT and other tasks (Lavie and Agarwal, 2007; Hodosh et al., 2013) by computing correlations on observations consisting of single-sentence system outputs, it has been shown (e.", "startOffset": 73, "endOffset": 119}, {"referenceID": 7, "context": "4While much prior work assesses automatic metrics for MT and other tasks (Lavie and Agarwal, 2007; Hodosh et al., 2013) by computing correlations on observations consisting of single-sentence system outputs, it has been shown (e.g., Przybocki et al. (2008)) that correlation coefficients significantly increase as observation units become larger.", "startOffset": 99, "endOffset": 257}, {"referenceID": 7, "context": "4While much prior work assesses automatic metrics for MT and other tasks (Lavie and Agarwal, 2007; Hodosh et al., 2013) by computing correlations on observations consisting of single-sentence system outputs, it has been shown (e.g., Przybocki et al. (2008)) that correlation coefficients significantly increase as observation units become larger. For instance, corpus-level or system-level correlations tend to be much higher than sentence-level correlations; Graham and Baldwin (2014) show that BLEU is competitive with more recent and advanced metrics when assessed at the system level.", "startOffset": 99, "endOffset": 486}, {"referenceID": 10, "context": "The last computes sentence-level BLEU scores (Nakov et al., 2012) and averages them on the M sentences (akin to macro-averaging).", "startOffset": 45, "endOffset": 65}, {"referenceID": 13, "context": "This may come as a surprise, because it has been suggested elsewhere that sBLEU has much worse correlation than BLEU computed at the corpus level (Przybocki et al., 2008).", "startOffset": 146, "endOffset": 170}], "year": 2015, "abstractText": "We introduce Discriminative BLEU (\u2206BLEU), a novel metric for intrinsic evaluation of generated text in tasks that admit a diverse range of possible outputs. Reference strings are scored for quality by human raters on a scale of [\u22121, +1] to weight multi-reference BLEU. In tasks involving generation of conversational responses, \u2206BLEU correlates reasonably with human judgments and outperforms sentence-level and IBM BLEU in terms of both Spearman\u2019s \u03c1 and Kendall\u2019s \u03c4 .", "creator": "LaTeX with hyperref package"}}}