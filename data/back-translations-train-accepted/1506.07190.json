{"id": "1506.07190", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2015", "title": "Multi-domain Dialog State Tracking using Recurrent Neural Networks", "abstract": "Dialog state tracking is a key component of many modern dialog systems, most of which are designed with a single, well-defined domain in mind. This paper shows that dialog data drawn from different dialog domains can be used to train a general belief tracking model which can operate across all of these domains, exhibiting superior performance to each of the domain-specific models. We propose a training procedure which uses out-of-domain data to initialise belief tracking models for entirely new domains. This procedure leads to improvements in belief tracking performance regardless of the amount of in-domain data available for training the model.", "histories": [["v1", "Tue, 23 Jun 2015 20:16:06 GMT  (926kb,D)", "http://arxiv.org/abs/1506.07190v1", "Accepted as a short paper in the 53rd Annual Meeting of the Association for Computational Linguistics (ACL 2015)"]], "COMMENTS": "Accepted as a short paper in the 53rd Annual Meeting of the Association for Computational Linguistics (ACL 2015)", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["nikola mrksic", "diarmuid \u00f3 s\u00e9aghdha", "blaise thomson", "milica gasic", "pei-hao su", "david vandyke", "tsung-hsien wen", "steve j young"], "accepted": true, "id": "1506.07190"}, "pdf": {"name": "1506.07190.pdf", "metadata": {"source": "CRF", "title": "Multi-domain Dialog State Tracking using Recurrent Neural Networks", "authors": ["Nikola Mrk\u0161i\u0107", "Diarmuid \u00d3 S\u00e9aghdha", "Blaise Thomson", "Milica Ga\u0161i\u0107", "Pei-Hao Su", "David Vandyke", "Tsung-Hsien Wen", "Steve Young"], "emails": ["nm480@cam.ac.uk", "mg436@cam.ac.uk", "phs26@cam.ac.uk", "djv27@cam.ac.uk", "thw28@cam.ac.uk", "sjy@cam.ac.uk", "blaise}@vocaliq.com"], "sections": [{"heading": "1 Introduction", "text": "Modern dialog systems are typically designed with a well-defined domain in mind, such as restaurant searches, travel bookings, or shopping for a new laptop. However, the goal of building open domain dialog systems capable of exchanging information on any topic remains a long way off. In this work, we are moving toward that goal by showing how to build dialog status tracking models that can operate across completely different domains. The state-tracking component of a dialog system is responsible for interpreting users \"expressions and thus updating the belief state of the system: a probability distribution across all possible states of the dialog. This belief state is used by the system to decide what to do. Recurrent Neural Networks (RNNNs) are well suited for dialog status tracking systems because their ability to capture contextual information well serves to model and characterize complex dynamic sequences."}, {"heading": "2 Related Work", "text": "Traditional rule-based approaches to understanding in dialog systems (e.g. Goddeau et al. (1996)) have been replaced by data-driven systems that are more robust and can provide the likely dialog status distributions needed by POMDP-based dialogue managers. Recent Dialog State Tracking Challenge (DSTC) shared tasks (Williams et al., 2013; Henderson et al., 2014a; Henderson et al., 2014b) saw a variety of novel approaches, including robust sets of handmade rules (Wang and Lemon, 2013), conditional random fields (Lee and Eskenazi, 2013; Ren et al., 2013), maximum entropy models (Williams, 2013) and web-style ranking (Williams, 2014). ar Xiv: 150 6.07 190v 1 [cs.C L] 23 Jun 2015Henderson et al."}, {"heading": "3 Dialog State Tracking using RNNs", "text": "It is not as if we are able to create a new system in which we engage in a new system in which we can rely on a new system that can rely on a new system. (It is also a memory vector that stores internal information about the dialogical context.) The input for each user consists of the ASR hypotheses, the last system action, the current memory vector and the previous speech communication (SLU). Rather than the use of a spoken language understanding in a meaningful representation. (It is the last system action, the current memory vector and the previous belief. \"(SLU)"}, {"heading": "4 Hierarchical Model Training", "text": "This model should be able to track the state of belief in all of these areas.The training process begins with performing a common initialization: the RNN parameters of all slots are bound and all slot values are replaced by a single generic tag. These slotagnostic delicalized dialogs are then used to train the parameters of the common RNN model.Extending the common initialization to training across multiple domains is straightforward. We delicalize all slot values for all slots in the different domains in the training data.These combined (delicalized) datasets are then used to train the multi-domain shared model.The common RNN model is equally designed for a purpose that is not very extensive."}, {"heading": "5 Dialog domains considered", "text": "We use the experimental structure of Dialog State Tracking Challenges. The key metric for measuring the success of faith control is target accuracy, which represents the system's ability to correctly derive user limitations (Table 1). We report on the common target accuracy, which represents the marginal test accuracy across all slots in the domain. We evaluate data from six areas that differ thematically and geographically (Table 1). Data from Cambridge restaurants is data from DSTC 2. Data from San Francisco restaurants and coffee shops were collected during the Parlance project (Gas ic et al., 2014). The domain of tourist information is the DSTC 3 dataset: It contains dialogues about hotels, restaurants, pubs and coffee shops in Michigan. Restaurant and laptop data collections are collections of dialogues related to Amazon Mechanical Turk. The domain of laptops contains conversations with users who have been instructed to find laptops with specific features."}, {"heading": "6 Results", "text": "As part of the evaluation, we use the three combinations of our dialog domains to build increasingly general belief control models. Domain-specific models, which are only trained on data from each of the six dialog domains, provide the basic performance for the three general models."}, {"heading": "6.1 Training General Models", "text": "The training of the common RNN models is the first step in the training process. Table 2 shows the performance of common models trained using dialogs from the six individual domains and the three combined domains; the common accuracies are not comparable between the domains, as each of them has a different number of slots; the geometric mean of the six accuracies is calculated to determine how well these models operate in different dialog areas; the parameters of the three multi-domain models are not slot or even domain specific; however, all of them improve slightly over the domain-specific model for all but one of their constituent domains; the R + T + H model outperforms the R + T + H + L model in four domains, showing that the use of laptop-related dialogs slightly reduces performance in other closely related domains; however, the latter model is much better at balancing its performance in all six domains, achieving the highest geometric mean value, and still improving over all to specific models."}, {"heading": "6.2 Slot-specialising the General Models", "text": "The specialization of the slot model enables the training process to learn the relative importance of different delicate characteristics for each slot in a particular domain. Table 3 shows the effects of slot specialization on common models in the six dialog areas. If you move down in these tables, you add more extra-domain specific training data and move to the right, corresponding to the specialization of the slot model for each slot in the current domain. Slot specialization improved performance in the vast majority of experiments. All three slot specialized general models exceeded the performance of the RNN model reported in DSTC 2."}, {"heading": "6.3 Out of Domain Initialisation", "text": "The hierarchical training process can exploit the available out-of-domain dialogs to initialize improved shared models for new dialog domains. In our experiments, we select one of the domains to act as a new domain, and we use a subset of the remaining as out-of-domain data. The number of in-domain dialogs that are available for training is increased at each stage of the experiment and used to train and compare the performance of two slot-specialized models. These models specialize in two different common models. One is trained only with in-domain data, and the other is trained on all out-of-domain data as well. The two experiments differ in terms of the degree of similarity between the in-domain and out-of-domain dialogs. In the first experiment, Michigan restaurants act as the new domain and the remaining R + T + H dialogs are trained as out-of-domain data."}, {"heading": "7 Conclusion", "text": "We have shown that it is possible to train general belief control models that are able to talk about many different topics at once. The most general model shows robust performance across all domains and surpasses most domain-specific models. This shows that training with different dialog areas allows the model to better understand the general dialogue dynamics applicable to different domains at once. The proposed hierarchical training method can also be used to adapt the general model to new dialog areas, requiring very small datasets in domains for customization."}, {"heading": "7.1 Further Work", "text": "The proposed domain customization process requires a small collection of annotated in-domain dialogs to adjust the general model to a new domain. In our future work, we will focus on initializing models for tracking trustworthiness when no annotated dialogs are available for the new dialog domain."}], "references": [{"title": "Biographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification", "author": ["Blitzer et al.2007] John Blitzer", "Mark Dredze", "Fernando Pereira"], "venue": "In Proceedings of ACL", "citeRegEx": "Blitzer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2007}, {"title": "Incremental on-line adaptation of POMDPbased dialogue managers to extended domains", "author": ["Ga\u0161i\u0107 et al.2014] Milica Ga\u0161i\u0107", "Dongho Kim", "Pirros Tsiakoulis", "Catherine Breslin", "Matthew Henderson", "Martin Szummer", "Blaise Thomson", "Steve Young"], "venue": null, "citeRegEx": "Ga\u0161i\u0107 et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ga\u0161i\u0107 et al\\.", "year": 2014}, {"title": "Domain adaptation for largescale sentiment classification: A deep learning approach", "author": ["Glorot et al.2011] Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In Proceedings of ICML", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "A form-based dialogue manager for spoken language applications", "author": ["Goddeau et al.1996] D. Goddeau", "H. Meng", "J. Polifroni", "S. Seneff", "S. Busayapongchai"], "venue": "In Proceedings of ICSLP", "citeRegEx": "Goddeau et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Goddeau et al\\.", "year": 1996}, {"title": "Supervised Sequence Labelling with Recurrent Neural Networks", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2012\\E", "shortCiteRegEx": "Graves.", "year": 2012}, {"title": "Deep neural network approach for the Dialog State Tracking Challenge", "author": ["Blaise Thomson", "Steve Young"], "venue": "In Proceedings of SIGDIAL", "citeRegEx": "Henderson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2013}, {"title": "The Second Dialog State Tracking Challenge", "author": ["Blaise Thomson", "Jason D. Wiliams"], "venue": "In Proceedings of SIGDIAL", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "The Third Dialog State Tracking Challenge", "author": ["Blaise Thomson", "Jason D. Wiliams"], "venue": "In Proceedings of IEEE SLT", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Robust dialog state tracking using delexicalised recurrent neural networks and unsupervised adaptation", "author": ["Blaise Thomson", "Steve Young"], "venue": "In Proceedings of IEEE SLT", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Word-based dialog state tracking with recurrent neural networks", "author": ["Blaise Thomson", "Steve Young"], "venue": "In Proceedings of SIGDIAL", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Recipe for building robust spoken dialog state trackers: Dialog State Tracking Challenge system description", "author": ["Lee", "Eskenazi2013] Sungjin Lee", "Maxine Eskenazi"], "venue": "In Proceedings of SIGDIAL", "citeRegEx": "Lee et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2013}, {"title": "Structured discriminative model for dialog state tracking", "author": ["Sungjin Lee"], "venue": "In Proceedings of SIGDIAL", "citeRegEx": "Lee.,? \\Q2013\\E", "shortCiteRegEx": "Lee.", "year": 2013}, {"title": "Domain adaptation with unlabeled data for dialog act tagging", "author": ["Karen Livescu", "Mari Ostendorf"], "venue": "In Proceedings of the ACL Workshop on Domain Adaptation", "citeRegEx": "Margolis et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Margolis et al\\.", "year": 2010}, {"title": "Effective selftraining for parsing", "author": ["Eugene Charniak", "Mark Johnson"], "venue": "In Proceedings of HLT-NAACL", "citeRegEx": "McClosky et al\\.,? \\Q2006\\E", "shortCiteRegEx": "McClosky et al\\.", "year": 2006}, {"title": "Automatic domain adaptation for parsing", "author": ["Eugene Charniak", "Mark Johnson"], "venue": "In Proceedings of NAACL HLT", "citeRegEx": "McClosky et al\\.,? \\Q2010\\E", "shortCiteRegEx": "McClosky et al\\.", "year": 2010}, {"title": "Dialog state tracking using conditional random fields", "author": ["Ren et al.2013] Hang Ren", "Weiqun Xu", "Yan Zhang", "Yonghong Yan"], "venue": "In Proceedings of SIGDIAL", "citeRegEx": "Ren et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2013}, {"title": "Model adaptation for dialog act tagging", "author": ["Tur et al.2007] Gokhan Tur", "Umit Guz", "Dilek Hakkani-T\u00fcr"], "venue": "In Proceedings of IEEE SLT", "citeRegEx": "Tur et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Tur et al\\.", "year": 2007}, {"title": "Individual and domain adaptation in sentence planning for dialogue", "author": ["Amanda Stent", "Fran\u00e7ois Mairesse", "Rashmi Prasad"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Walker et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Walker et al\\.", "year": 2007}, {"title": "A simple and generic belief tracking mechanism for the Dialog State Tracking Challenge: On the believability of observed information", "author": ["Wang", "Lemon2013] Zhuoran Wang", "Oliver Lemon"], "venue": "Proceedings of SIGDIAL", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "The Dialogue State Tracking Challenge", "author": ["Antoine Raux", "Deepak Ramachandran", "Alan W. Black"], "venue": "In Proceedings of SIGDIAL", "citeRegEx": "Williams et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2013}, {"title": "Multidomain learning and generalization in dialog state tracking", "author": ["Jason D. Williams"], "venue": "In Proceedings of SIGDIAL", "citeRegEx": "Williams.,? \\Q2013\\E", "shortCiteRegEx": "Williams.", "year": 2013}, {"title": "Web-style ranking and slu combination for dialog state tracking", "author": ["Jason D. Williams"], "venue": "In Proceedings of SIGDIAL", "citeRegEx": "Williams.,? \\Q2014\\E", "shortCiteRegEx": "Williams.", "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "Recurrent Neural Networks (RNNs) are well suited to dialog state tracking, as their ability to capture contextual information allows them to model and label complex dynamic sequences (Graves, 2012).", "startOffset": 183, "endOffset": 197}, {"referenceID": 19, "context": "The recent Dialog State Tracking Challenge (DSTC) shared tasks (Williams et al., 2013; Henderson et al., 2014a; Henderson et al., 2014b) saw a variety of novel approaches, including robust sets of hand-crafted rules (Wang and Lemon, 2013), conditional random fields (Lee and Eskenazi, 2013; Lee, 2013; Ren et al.", "startOffset": 63, "endOffset": 136}, {"referenceID": 11, "context": ", 2014b) saw a variety of novel approaches, including robust sets of hand-crafted rules (Wang and Lemon, 2013), conditional random fields (Lee and Eskenazi, 2013; Lee, 2013; Ren et al., 2013), maximum entropy models (Williams, 2013) and web-style ranking (Williams, 2014).", "startOffset": 138, "endOffset": 191}, {"referenceID": 15, "context": ", 2014b) saw a variety of novel approaches, including robust sets of hand-crafted rules (Wang and Lemon, 2013), conditional random fields (Lee and Eskenazi, 2013; Lee, 2013; Ren et al., 2013), maximum entropy models (Williams, 2013) and web-style ranking (Williams, 2014).", "startOffset": 138, "endOffset": 191}, {"referenceID": 20, "context": ", 2013), maximum entropy models (Williams, 2013) and web-style ranking (Williams, 2014).", "startOffset": 32, "endOffset": 48}, {"referenceID": 21, "context": ", 2013), maximum entropy models (Williams, 2013) and web-style ranking (Williams, 2014).", "startOffset": 71, "endOffset": 87}, {"referenceID": 3, "context": "Goddeau et al. (1996)) have been superseded by data-driven systems that are more robust and can provide the probabilistic dialog state distributions that are needed by POMDPbased dialog managers.", "startOffset": 0, "endOffset": 22}, {"referenceID": 13, "context": "Researchers have investigated methods for mitigating this problem, with NLP applications in parsing (McClosky et al., 2006; McClosky et al., 2010), sentiment analysis (Blitzer et al.", "startOffset": 100, "endOffset": 146}, {"referenceID": 14, "context": "Researchers have investigated methods for mitigating this problem, with NLP applications in parsing (McClosky et al., 2006; McClosky et al., 2010), sentiment analysis (Blitzer et al.", "startOffset": 100, "endOffset": 146}, {"referenceID": 0, "context": ", 2010), sentiment analysis (Blitzer et al., 2007; Glorot et al., 2011) and many other tasks.", "startOffset": 28, "endOffset": 71}, {"referenceID": 2, "context": ", 2010), sentiment analysis (Blitzer et al., 2007; Glorot et al., 2011) and many other tasks.", "startOffset": 28, "endOffset": 71}, {"referenceID": 0, "context": ", 2010), sentiment analysis (Blitzer et al., 2007; Glorot et al., 2011) and many other tasks. There has been a small amount of previous work on domain adaptation for dialog systems. Tur et al. (2007) and Margolis et al.", "startOffset": 29, "endOffset": 200}, {"referenceID": 0, "context": ", 2010), sentiment analysis (Blitzer et al., 2007; Glorot et al., 2011) and many other tasks. There has been a small amount of previous work on domain adaptation for dialog systems. Tur et al. (2007) and Margolis et al. (2010) investigated domain adaptation", "startOffset": 29, "endOffset": 227}, {"referenceID": 12, "context": "Walker et al. (2007) trained a sentence planner/generator that adapts to different individuals and domains.", "startOffset": 0, "endOffset": 21}, {"referenceID": 4, "context": "For training, the model is unrolled across turns and trained using backpropagation through time and stochastic gradient descent (Graves, 2012).", "startOffset": 128, "endOffset": 142}, {"referenceID": 1, "context": "tels data was collected during the Parlance project (Ga\u0161i\u0107 et al., 2014).", "startOffset": 52, "endOffset": 72}], "year": 2015, "abstractText": "Dialog state tracking is a key component of many modern dialog systems, most of which are designed with a single, welldefined domain in mind. This paper shows that dialog data drawn from different dialog domains can be used to train a general belief tracking model which can operate across all of these domains, exhibiting superior performance to each of the domainspecific models. We propose a training procedure which uses out-of-domain data to initialise belief tracking models for entirely new domains. This procedure leads to improvements in belief tracking performance regardless of the amount of in-domain data available for training the model.", "creator": "LaTeX with hyperref package"}}}