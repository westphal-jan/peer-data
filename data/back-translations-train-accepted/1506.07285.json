{"id": "1506.07285", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2015", "title": "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing", "abstract": "Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a unified neural network framework which processes input sequences and questions, forms semantic and episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state of the art results on several types of tasks and datasets: question answering (Facebook's bAbI dataset), sequence modeling for part of speech tagging (WSJ-PTB), and text classification for sentiment analysis (Stanford Sentiment Treebank). The model relies exclusively on trained word vector representations and requires no string matching or manually engineered features.", "histories": [["v1", "Wed, 24 Jun 2015 08:27:02 GMT  (270kb,D)", "http://arxiv.org/abs/1506.07285v1", null], ["v2", "Fri, 24 Jul 2015 22:21:29 GMT  (270kb,D)", "http://arxiv.org/abs/1506.07285v2", null], ["v3", "Tue, 29 Sep 2015 05:02:29 GMT  (267kb,D)", "http://arxiv.org/abs/1506.07285v3", null], ["v4", "Tue, 9 Feb 2016 08:19:30 GMT  (518kb,D)", "http://arxiv.org/abs/1506.07285v4", null], ["v5", "Sat, 5 Mar 2016 20:18:55 GMT  (507kb,D)", "http://arxiv.org/abs/1506.07285v5", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["ankit kumar", "ozan irsoy", "peter ondruska", "mohit iyyer", "james bradbury", "ishaan gulrajani", "victor zhong", "romain paulus", "richard socher"], "accepted": true, "id": "1506.07285"}, "pdf": {"name": "1506.07285.pdf", "metadata": {"source": "CRF", "title": "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing", "authors": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher"], "emails": ["firstname@metamind.io"], "sections": [{"heading": null, "text": "We introduce the Dynamic Memory Network (DMN), a unified neural network that processes input sequences and questions, shapes semantic and episodic memories, and generates relevant answers. Questions trigger an iterative attention process that allows the model to condition its attention to the outcome of previous iterations, and these results are then thought through in a hierarchically recurring sequence model to generate replies. DMN can be trained throughout, obtaining state-of-the-art results for various types of tasks and data sets: answering questions (Facebook's bAbI dataset), sequence modeling for part of speech tagging (WSJ-PTB), and text classification for sentiment analysis (Stanford Sentiment Treebank). The model relies exclusively on trained word vector representations and does not require manual matching or attributes."}, {"heading": "1 Introduction", "text": "Most, if not all, tasks in natural language processing can be considered a question-and-answer problem: high-level tasks such as machine translation (What is French translation?); sequence modelling tasks such as entity recognition [1] (NER) (What are the designated entity tags in this sentence?) or language tagging (POS) (What is the part of language tags?); classification problems such as sentiment analysis [2] (What is the feeling?); even multi-sentence common classification problems such as correlation resolution (Who refers to \"theirs\"?). Dynamic Storage Network (DMN) is a neural network-based model that can be trained in end-to-end mode for any QA task."}, {"heading": "2 The Dynamic Memory Network Framework", "text": "The DMN is a general modeling framework for questions about input. We will give an overview of the complete model and then go into more details for each module, as well as present our specific instance of each module for processing natural language. The goal of the DMN is to first compute a vector representation of an input, and then generate the correct answer. It includes the following modules: Input module: The input processes raw input and maps it to a representation that is useful to ask questions about that input. Input can be, for example, an image, a video, or an audio signal. We focus on NLP in this paper. Input can be a sentence, a long story, a news article, or all of Wikipedia.Semantic Memory Module: The Semantic Memory Module stores general knowledge of concepts and facts."}, {"heading": "2.1 Input Module", "text": "In general, the input module can be seen as a calculation of the intermediate steps of a function, which ultimately returns a final vector representation. The input module sends these intermediate values to the episodic memory module, which will complete the calculation due to its attention mechanism. We refer to the representations that the input module provides to the episodic memory as facts established by the episodic memory module. (In natural language processing, we have a sequence of TI words wI1,., w I TI. The input module calculates the hidden states of a recursive sequence model [7]. We use Gloves [3] vectors to capture context-independent representations and activate the embedding of the DMN with these values."}, {"heading": "2.2 Semantic Memory Module", "text": "The semantic memory consists of (i) stored word concepts and (ii) facts about them. We initialize embedding in glove vectors as described above. This module might include glances or other forms of explicit knowledge bases, but we do not use them in this work."}, {"heading": "2.3 Question Module", "text": "This module maps a question into a representation that can then be used to query specific facts from the input module. We have questions that consist of sequences of TQ words w Q t. We calculate a hidden state for each of them via qt = GRU (L [w Q t], qt \u2212 1), dividing the GRU and the embedding weight with the input module. The last question vector is defined as q = qTQ."}, {"heading": "2.4 Episodic Memory Module", "text": "The episodic memory module retrieves facts from the input module that is linked to the question. There are then reasons about these facts to produce a definitive representation that the answer module will use to generate an answer. We refer to this representation as memory. It is important that we focus our attention more selectively on specific facts in each pass, as it can take care of other important facts in a later run. It also allows a kind of transitive inference, since the first pass can reveal the need to retrieve additional facts. For example, in the example in Fig. 3 we are asked that the football, the model should take care of sentence 7."}, {"heading": "2.5 Answer Sequence", "text": "The sequence module decrypts the memory into a sequence of words representing the answer. We use a GRU and set the initial hidden state to memory a0 = m. The subsequent hidden states take as input the last hidden state and the previously predicted output yt \u2212 1, as well as the question: at = GRU ([yt \u2212 1, q], at \u2212 1), yt = softmax (W (a) at) (7), where W (a) is a standard softmax layer. The output is trained with the cross entropy error classification of the correct sequence, which is appended with a special end-of-sequence token. At test time, we generate words until an end-of-of-sequence is generated."}, {"heading": "2.6 Training", "text": "For data sets with gate monitoring such as bAbI, we also include the cross-entropy error of the gates in the total costs. Since all modules communicate with gates via vector representations and various types of differentiable and deep neural networks, the entire DMN model can be trained via back propagation and gradient descent."}, {"heading": "3 Related Work", "text": "Given the many shoulders used for machine translation and the many applications to which our model is applied, it is then impossible to do related fields justice.Deep Learning. There are several in-depth learning models that have been applied to many different tasks in NLP. For example, recursive neural networks are used for analysis [2], sensory analysis, and questions that require a transitive reasoning across multiple sentences. Another commonly used model is the chain of structured neural networks as we use them. Recurrent neural networks have been successfully used in speech modeling [14], speech recognition, and sentence generation. Also relevant is the sequential model of sequence."}, {"heading": "4 Experiments", "text": "We use experiments to answer questions, part of language marking, mood analysis, and preliminary results on machine translation. For all data sets, we use either the official train, developer, test splitter, or, if no developer replacement has been defined, 10% of the training set for development. Hyperparameter tuning and model selection (with early stop) are done on the development set. The DMN is trained using backpropagation and adagrad [34]. We use L2 regulation and \"word drop-out,\" where each word vector is set to 0 with a certain probability. Word vectors are pre-trained with gloves [3]."}, {"heading": "4.1 Question Answering", "text": "The Facebook bAbI dataset is a synthetic dataset intended to test the ability of a model to retrieve facts and reason about it. Each task tests a different ability that a good question-answer model should have, such as coreference resolution, deduction and induction. In practice, we start with training on the bAbI datasets with the following objective function: J = \u03b1ECE (Gates) + \u03b2ECE (Answers), where ECE are the standard cross-entropy costs and \u03b1 and \u03b2 hyperparameters. In practice, we start with setting the bAbI datasets to 1 and \u03b2 to 0, and then switch to 1 later while staying \u03b1 at 1. We subscribe the facts from the input module by transverse sentence tokens. The goal of the gate control is to select one set per pass; therefore, we are also experimenting with modifying Eq. 6 to a simple softmax instead of a GRU."}, {"heading": "4.2 Sequence Tagging: Part of Speech Tagging", "text": "The marking of a part of the language is traditionally modeled as a problem of sequence marking: each word in a sentence is classified into its part of the language class (see Figure 1). We evaluate using the standard Wall Street Journal dataset contained in Penn-III [26]. We use the standard splits of sections 0-18 for training, 19-21 for development, and 22-24 for test records [27]. As this is a tagging task at word level, DMN memories are generated at word level and not at sentence level. We compare the DMNs with the results in [27]. The DMN achieves state-of-the-art accuracy with a single model and achieves an accuracy of the development sets of 97.5. The DMN is similar to the top 4 development models and reaches 97.58 Dev and 97.56 test accuracies, thereby achieving a new state of the art (Table 2)."}, {"heading": "4.3 Text Classification: Sentiment Analysis", "text": "Stanford Sentiment Treebank (SST) [2] is a popular data set for classifying sentiments q. It provides fine-grained phrase-level labels and comes with a train / dev / test split. We present the results in two formats: fine-grained root prediction, where all complete sentences (root nodes) of the test set must be classified as either very negative, negative, neutral, positive or very positive and binary root prediction, with all non-neutral complete sentences of the test set classified as positive or negative. To train the model on the fine-grained task, we use all phrase-level labels. To train on the binary task, we use all non-neutral phrase-plane labels. For sentiment analysis, our goal function G requires only the first 3 components c, m, q of the function z as defined in Eq. 3. The DMN model achieves high-grade accuracy in classification."}, {"heading": "5 Conclusion", "text": "We believe that the DMN is a potentially generic model for a variety of NLP applications, and the entire model can be trained end-to-end with an objective function, albeit complex, using some ideas from neuroscience, such as semantic and episodic memories, which are needed for complex ways of thinking. Future work will explore additional tasks, larger multi-task models, and multimodal inputs and questions."}, {"heading": "Acknowledgements", "text": "We thank Sam Gershman for useful discussions."}], "references": [{"title": "Lexicon infused phrase embeddings for named entity resolution", "author": ["A. Passos", "V. Kumar", "A. McCallum"], "venue": "Conference on Computational Natural Language Learning. Association for Computational Linguistics, June", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J. Wu", "J. Chuang", "C. Manning", "A. Ng", "C. Potts"], "venue": "EMNLP,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "EMNLP,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "ICLR (workshop),", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Reasoning With Neural Tensor Networks For Knowledge Base Completion", "author": ["R. Socher", "D. Chen", "C.D. Manning", "A.Y. Ng"], "venue": "NIPS.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing", "author": ["A. Bordes", "X. Glorot", "J. Weston", "Y. Bengio"], "venue": "AISTATS,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Distributed representations, simple recurrent networks, and grammatical structure", "author": ["J.L. Elman"], "venue": "Machine Learning, 7(2-3):195\u2013225,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1991}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["K. Cho", "B. van Merrienboer", "D. Bahdanau", "Y. Bengio"], "venue": "CoRR, abs/1409.1259,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "\u00c7. G\u00fcl\u00e7ehre", "K. Cho", "Y. Bengio"], "venue": "CoRR, abs/1412.3555,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, 9(8):1735\u20131780, Nov", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection", "author": ["R. Socher", "E.H. Huang", "J. Pennington", "A.Y. Ng", "C.D. Manning"], "venue": "NIPS,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "A neural network for factoid question answering over paragraphs", "author": ["M. Iyyer", "J. Boyd-Graber", "L. Claudino", "R. Socher", "H. Daum\u00e9 III"], "venue": "EMNLP,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Recursive neural networks for learning logical semantics", "author": ["S.R. Bowman", "C. Potts", "C.D. Manning"], "venue": "CoRR, abs/1406.1827,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Context dependent recurrent neural network language model", "author": ["T. Mikolov", "G. Zweig"], "venue": "SLT, pages 234\u2013239. IEEE,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CVPR,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "ICLR,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov"], "venue": "CoRR, abs/1502.05698,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep Networks with Internal Selective Attention through Feedback Connections", "author": ["M.F. Stollenga", "J. Schmidhuber J. Masci", "F. Gomez"], "venue": "In NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "CoRR, abs/1502.03044,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "CoRR, abs/1410.5401,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Textrunner: Open information extraction on the web", "author": ["A. Yates", "M. Banko", "M. Broadhead", "M.J. Cafarella", "O. Etzioni", "S. Soderland"], "venue": "HLT-NAACL (Demonstrations),", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": "Computational Linguistics, 19(2), June", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1993}, {"title": "Semisupervised condensed nearest neighbor for part-of-speech tagging", "author": ["A. S\u00f8gaard"], "venue": "ACL-HLT,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "The neurobiology of semantic memory", "author": ["J.R. Binder", "R.H. Desai"], "venue": "Trends in Cognitive Sciences, 15(11):527\u2013536,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "From Conditioning to Conscious Recollection: Memory Systems of the Brain (Oxford Psychology)", "author": ["H. Eichenbaum", "N.J. Cohen"], "venue": "Oxford University Press, 1 edition,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2004}, {"title": "Hippocampal activation during transitive inference in humans", "author": ["S. Heckers", "M. Zalesak", "A.P. Weiss", "T. Ditman", "D. Titone"], "venue": "Hippocampus, 14:153\u201362,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2004}, {"title": "The hippocampus and memory for orderly stimulusrelations", "author": ["J.A. Dusek", "H. Eichenbaum"], "venue": "Proceedings of the National Academy of Sciences, 94(13):7109\u20137114,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1997}, {"title": "A distributed representation of temporal context", "author": ["Marc W. Howard", "Michael J. Kahana"], "venue": "Journal of Mathematical Psychology,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2002}, {"title": "A bayesian analysis of dynamics in free recall", "author": ["R. Socher", "S. Gershman", "A. Perotte", "P. Sederberg", "D. Blei", "K. Norman"], "venue": "Advances in Neural Information Processing Systems 22.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "JMLR, 12, July", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "Improved semantic representations from tree-structured long short-term memory", "author": ["K.S. Tai", "R. Socher", "C.D. Manning"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["N. Kalchbrenner", "P. Blunsom"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Most, if not all, tasks in natural language processing can be cast as a question answering problem: high level tasks like machine translation (What is the translation into French?); sequence modeling tasks like named entity recognition [1] (NER) (What are the named entity tags in this sentence?) or part of speech tagging (POS) (What are the part of speech tags?); classification problems like sentiment analysis [2] (What is the sentiment?); even multi-sentence joint classification problems like coreference resolution (Who does \u201dtheir\u201d refer to?).", "startOffset": 236, "endOffset": 239}, {"referenceID": 1, "context": "Most, if not all, tasks in natural language processing can be cast as a question answering problem: high level tasks like machine translation (What is the translation into French?); sequence modeling tasks like named entity recognition [1] (NER) (What are the named entity tags in this sentence?) or part of speech tagging (POS) (What are the part of speech tags?); classification problems like sentiment analysis [2] (What is the sentiment?); even multi-sentence joint classification problems like coreference resolution (Who does \u201dtheir\u201d refer to?).", "startOffset": 414, "endOffset": 417}, {"referenceID": 2, "context": "Initialization strategies such as distributed word vectors (Glove [3], Word2Vec [4]) are popular semantic memory components that have been shown to improve performance on many NLP tasks.", "startOffset": 66, "endOffset": 69}, {"referenceID": 3, "context": "Initialization strategies such as distributed word vectors (Glove [3], Word2Vec [4]) are popular semantic memory components that have been shown to improve performance on many NLP tasks.", "startOffset": 80, "endOffset": 83}, {"referenceID": 4, "context": "More complex information can be stored in the form of knowledge bases that capture relationships in the form of triplets [5] or gazetteers, which have been useful for tasks such as named entity recognition or question answering [6].", "startOffset": 121, "endOffset": 124}, {"referenceID": 5, "context": "More complex information can be stored in the form of knowledge bases that capture relationships in the form of triplets [5] or gazetteers, which have been useful for tasks such as named entity recognition or question answering [6].", "startOffset": 228, "endOffset": 231}, {"referenceID": 6, "context": "The input module computes the hidden states of a recurrent sequence model [7].", "startOffset": 74, "endOffset": 77}, {"referenceID": 2, "context": "We use Glove [3] vectors to capture context-independent representations, and initalize the embeddings of the DMN with these values.", "startOffset": 13, "endOffset": 16}, {"referenceID": 7, "context": "In particular, we use a gated recurrent network (GRU) [8, 9].", "startOffset": 54, "endOffset": 60}, {"referenceID": 8, "context": "In particular, we use a gated recurrent network (GRU) [8, 9].", "startOffset": 54, "endOffset": 60}, {"referenceID": 9, "context": "We also explored the more complex LSTM [10] but it performed similarly and is more computationally expensive.", "startOffset": 39, "endOffset": 43}, {"referenceID": 9, "context": "Both work much better than the standard tanh RNN and we postulate that the main strength comes from having gates that allow the model to suffer less from the vanishing gradient problem [10].", "startOffset": 185, "endOffset": 189}, {"referenceID": 10, "context": "For instance, recursive neural networks have been used for parsing [11], sentiment analysis [2], paraphrase detection [11] and question answering [12] and logical inference [13], among other tasks.", "startOffset": 67, "endOffset": 71}, {"referenceID": 1, "context": "For instance, recursive neural networks have been used for parsing [11], sentiment analysis [2], paraphrase detection [11] and question answering [12] and logical inference [13], among other tasks.", "startOffset": 92, "endOffset": 95}, {"referenceID": 10, "context": "For instance, recursive neural networks have been used for parsing [11], sentiment analysis [2], paraphrase detection [11] and question answering [12] and logical inference [13], among other tasks.", "startOffset": 118, "endOffset": 122}, {"referenceID": 11, "context": "For instance, recursive neural networks have been used for parsing [11], sentiment analysis [2], paraphrase detection [11] and question answering [12] and logical inference [13], among other tasks.", "startOffset": 146, "endOffset": 150}, {"referenceID": 12, "context": "For instance, recursive neural networks have been used for parsing [11], sentiment analysis [2], paraphrase detection [11] and question answering [12] and logical inference [13], among other tasks.", "startOffset": 173, "endOffset": 177}, {"referenceID": 13, "context": "Recurrent neural networks have been successfully used in language modeling [14], speech recognition, and sentence generation from images [15].", "startOffset": 75, "endOffset": 79}, {"referenceID": 14, "context": "Recurrent neural networks have been successfully used in language modeling [14], speech recognition, and sentence generation from images [15].", "startOffset": 137, "endOffset": 141}, {"referenceID": 15, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "on memory networks [17] focuses on", "startOffset": 19, "endOffset": 23}, {"referenceID": 17, "context": "We compare directly to their model on the bAbI dataset [18].", "startOffset": 55, "endOffset": 59}, {"referenceID": 18, "context": "Attention mechanisms are generally useful and can improve image classification [19], automatic image captioning [20] and machine translation [21, 22].", "startOffset": 79, "endOffset": 83}, {"referenceID": 19, "context": "Attention mechanisms are generally useful and can improve image classification [19], automatic image captioning [20] and machine translation [21, 22].", "startOffset": 112, "endOffset": 116}, {"referenceID": 20, "context": "Attention mechanisms are generally useful and can improve image classification [19], automatic image captioning [20] and machine translation [21, 22].", "startOffset": 141, "endOffset": 149}, {"referenceID": 21, "context": "Attention mechanisms are generally useful and can improve image classification [19], automatic image captioning [20] and machine translation [21, 22].", "startOffset": 141, "endOffset": 149}, {"referenceID": 22, "context": "Neural Turing machines use memory to solve algorithmic problems such as list sorting [23].", "startOffset": 85, "endOffset": 89}, {"referenceID": 23, "context": "There are many different approaches to question answering: some build large knowledge bases (KBs) with open information extraction systems [24], some use neural networks, dependency trees and KBs [6], others only sentences [12].", "startOffset": 139, "endOffset": 143}, {"referenceID": 5, "context": "There are many different approaches to question answering: some build large knowledge bases (KBs) with open information extraction systems [24], some use neural networks, dependency trees and KBs [6], others only sentences [12].", "startOffset": 196, "endOffset": 199}, {"referenceID": 11, "context": "There are many different approaches to question answering: some build large knowledge bases (KBs) with open information extraction systems [24], some use neural networks, dependency trees and KBs [6], others only sentences [12].", "startOffset": 223, "endOffset": 227}, {"referenceID": 17, "context": "While synthetic datasets [18] have problems and can often be solved easily with manual feature engineering, they let us disentangle failure modes of models and understand necessary QA capabilities.", "startOffset": 25, "endOffset": 29}, {"referenceID": 1, "context": "Sentiment analysis is a very useful classification task and recently the Stanford Sentiment Treebank [2] has become a standard benchmark dataset.", "startOffset": 101, "endOffset": 104}, {"referenceID": 24, "context": "Kim [25] reports the previous state of the art result based on a convolutional neural network that uses multiple word vector representations.", "startOffset": 4, "endOffset": 8}, {"referenceID": 25, "context": "The previous best model for part of speech tagging on the Wall Street Journal section of the Penn Tree Bank [26] was Sogaard [27] who used a semisupervised nearest neighbor approach.", "startOffset": 108, "endOffset": 112}, {"referenceID": 26, "context": "The previous best model for part of speech tagging on the Wall Street Journal section of the Penn Tree Bank [26] was Sogaard [27] who used a semisupervised nearest neighbor approach.", "startOffset": 125, "endOffset": 129}, {"referenceID": 27, "context": "In humans both language-specific and supramodal concept representations are seated in the semantic memory whose existence is well established [28].", "startOffset": 142, "endOffset": 146}, {"referenceID": 28, "context": ", relations between spatial, sensory and temporal information) and that the hippocampus is responsible for general relational learning [29].", "startOffset": 135, "endOffset": 139}, {"referenceID": 29, "context": "Interestingly, it also appears that the hippocampus is active during transitive inference [30], and disruption of the hippocampus impairs this ability [31].", "startOffset": 90, "endOffset": 94}, {"referenceID": 30, "context": "Interestingly, it also appears that the hippocampus is active during transitive inference [30], and disruption of the hippocampus impairs this ability [31].", "startOffset": 151, "endOffset": 155}, {"referenceID": 31, "context": "This module also has similarities to the Temporal Context Model [32] and its Bayesian extensions [33] which were developed to analyze human behavior in word recall experiments.", "startOffset": 64, "endOffset": 68}, {"referenceID": 32, "context": "This module also has similarities to the Temporal Context Model [32] and its Bayesian extensions [33] which were developed to analyze human behavior in word recall experiments.", "startOffset": 97, "endOffset": 101}, {"referenceID": 33, "context": "The DMN is trained via backpropagation and Adagrad [34].", "startOffset": 51, "endOffset": 55}, {"referenceID": 2, "context": "Word vectors are pre-trained using Glove [3].", "startOffset": 41, "endOffset": 44}, {"referenceID": 17, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "We evaluate on the standard Wall Street Journal dataset included in Penn-III [26].", "startOffset": 77, "endOffset": 81}, {"referenceID": 26, "context": "We use the standard splits of sections 0-18 for training, 19-21 for development and 22-24 for test sets [27].", "startOffset": 104, "endOffset": 108}, {"referenceID": 26, "context": "with the results in [27].", "startOffset": 20, "endOffset": 24}, {"referenceID": 1, "context": "Stanford Sentiment Treebank (SST) [2] is a popular dataset for sentiment classification.", "startOffset": 34, "endOffset": 37}, {"referenceID": 1, "context": "Table 3: Test accuracies on SST [2].", "startOffset": 32, "endOffset": 35}, {"referenceID": 34, "context": "All results as reported in [35]", "startOffset": 27, "endOffset": 31}, {"referenceID": 15, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "Initial experiments on the smaller WMT13 English-toFrench News Commentary dataset used in Kalchbrenner [36] show promising results.", "startOffset": 103, "endOffset": 107}, {"referenceID": 15, "context": "For the Seq-to-Seq LSTM, we use the hyperparameters listed in [16].", "startOffset": 62, "endOffset": 66}], "year": 2015, "abstractText": "Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a unified neural network framework which processes input sequences and questions, forms semantic and episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state of the art results on several types of tasks and datasets: question answering (Facebook\u2019s bAbI dataset), sequence modeling for part of speech tagging (WSJ-PTB), and text classification for sentiment analysis (Stanford Sentiment Treebank). The model relies exclusively on trained word vector representations and requires no string matching or manually engineered features.", "creator": "LaTeX with hyperref package"}}}