{"id": "1506.07477", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2015", "title": "Efficient Learning for Undirected Topic Models", "abstract": "Replicated Softmax model, a well-known undirected topic model, is powerful in extracting semantic representations of documents. Traditional learning strategies such as Contrastive Divergence are very inefficient. This paper provides a novel estimator to speed up the learning based on Noise Contrastive Estimate, extended for documents of variant lengths and weighted inputs. Experiments on two benchmarks show that the new estimator achieves great learning efficiency and high accuracy on document retrieval and classification.", "histories": [["v1", "Wed, 24 Jun 2015 17:27:28 GMT  (406kb,D)", "http://arxiv.org/abs/1506.07477v1", "Accepted by ACL-IJCNLP 2015 short paper. 6 pages"]], "COMMENTS": "Accepted by ACL-IJCNLP 2015 short paper. 6 pages", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.IR stat.ML", "authors": ["jiatao gu", "victor o k li"], "accepted": true, "id": "1506.07477"}, "pdf": {"name": "1506.07477.pdf", "metadata": {"source": "CRF", "title": "Efficient Learning for Undirected Topic Models", "authors": ["Jiatao Gu", "Victor O.K. Li"], "emails": ["vli}@eee.hku.hk"], "sections": [{"heading": "1 Introduction", "text": "They are mainly constructed by directional structures such as pLSA (Hofmann, 2000) and LDA (Lead et al., 2003). Accompanied by the enormous developments in the field of deep learning, several undirected topic models such as (Salakhutdinov and Hinton, 2009; Srivastava et al., 2013) have recently achieved major improvements in efficiency and accuracy. Replica Softmax models (RSM) (Hinton and Salakhutdinov, 2009), a kind of typical undirected topic model, consists of a family of restricted Boltzmann machines (RBMs). RSM is learned like standard RBMs using approximate methods such as contrasting divergence (CD). However, CD is not really designed for RSM."}, {"heading": "2 Replicated Softmax Model", "text": "In general, it consists of a series of RBMs, ar Xiv: 150 6.07 477v 1 [cs.L G] 24 Jun 2015, each of which contains a variant softmax visible unit, but the same binary hidden units. Suppose K is the vocabulary size. For a document with D words, if the ith word in the document corresponds to the kth word of the dictionary, a vector vi- {0, 1} K is assigned, using only the kth element vik = 1. An RBM is formed by assigning a hidden state h-0, 1} H. V = {v1,..., vD}, where the energy function: E words (V, h) is assigned."}, {"heading": "2.1 Learning Strategies for RSM", "text": "RSM is, of course, learned by minimizing the negative log likelihood function (ML) as follows: L (\u03b8) = \u2212 EV \u0445 Pdata [logP\u03b8 (V)] (5) However, the gradient is insoluble for the combinatorial normalization term ZD. Common strategies for overcoming this intractability are MCMC-based approaches such as Contrastive Divergence (CD) (Hinton, 2002) and Persistent CD (PCD) (Tieleman, 2008), both of which require the repetition of Gibb's steps in h (i), h (h | V (i)) and V (i + 1) \u2012 PTB (V | h (i))) to generate model samples to approximate the gradient. Typically, performance and consistency improve when more steps are assumed. Nevertheless, even a Gibbs step is time consuming for RM, as the Peterjjohn multajohnson is typically used during time calculation."}, {"heading": "3 Efficient Learning for RSM", "text": "In contrast to (Dahl et al., 2012), which maintains the CD, we chose NCE as the basic learning strategy. Since RSM is designed for documents, we modified NCE by two novel heuristics and developed the approach \"Partial Noise Uniform Contrastive Estimate\" (\u03b1-NCE for short)."}, {"heading": "3.1 Noise Contrastive Estimate", "text": "Another estimator for training models with conflicting partition functions is the \"Noise Contrastive Estimate\" (NCE), similar to CD. NCE solves the intractability by adding the partition function ZcD as an additional parameter ZcD to the \"Proxy Classification Problem,\" which makes the probability predictable. However, the model cannot be trained by ML, since the probability tends to be arbitrarily high by setting ZcD to large numbers. Instead, NCE learns the model in a proxy classification problem with noise samples. In the face of document collection (data) {Vd} Td and further acquisition (noise) {Vn} Tn with Tn = kTd, NCE distinguishes these (1 + k) Td documents simply on the basis of Bayes \"theorem VDP, assuming data samples that coincide with our model VCE II (data) {Vd} Td, and a further acquisition (noise) VDP (VDP) with VDP (VDP), whereby VCE II (VDP) and VDP (n) data are simply differentiated from VCE (VDP)."}, {"heading": "3.2 Partial Noise Sampling", "text": "Unlike (Mnih and Teh, 2012), which generate noise per word, RSM requires the estimator to scan the noise at the document level. An intuitive approach is to stamp from the empirical distribution p-forD times in which the log probability is calculated: logPn (V) = V [vT log p].For a set k, Gutmann and Hyva \ufffd rinen (2010) suggested selecting the noise near the data for a sufficient learning result, suggesting that complete noise may not be satisfactory. We proposed an alternative: \"Partial noise sampling (PNS)\" to generate noise by replacing a portion of the data with sampled words. See Algorithm 1, in which we use the algorithm 1 Partial noise sampling 1: Initialize: k, equivalent: (0, 1) 2: for each Vd = {d} {De = Tw}: Vd = 3."}, {"heading": "3.3 Uniform Contrastive Estimate", "text": "When we first implemented NCE for RSM, we found that the document lengths distorted the log ratio terribly, resulting in poor parameters. Therefore, \"Uniform Contrastive Estimate (UCE)\" was proposed to take variant document lengths into account by adding the uniform assumption: X-V-D-1 log [P\u03b8 (V) / Pn (V) (10), where UCE assumed the uniform probabilities D-P\u03b8and D-Pn for classification in order to keep the modeling capability on word level on average. Note that D in UCE is not necessarily an integer and makes it possible to select a real weighting on the document, e.g. the idf weighting (Salton and McGill, 1983). Typically, it is defined as a weighting vector w, where wk = logTd | V-W (d-W) is multiplied with the vik-V = V-V in the word."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Datasets and Details of Learning", "text": "We evaluated the new estimator to train RSMs on two sets of text data: 20 newsgroups and IMDB.The 20 newsgroups2 dataset is a collection of Usenet posts containing 11,345 training sessions and 7,531 test cases. Both the training and test sets are marked in 20 classes. Removal of stop words and production of stop words were performed. IMDB dataset 3 is a benchmark for sentimental analysis consisting of 100,000 movie reviews from IMDB. The dataset is divided into 75,000 training instances (1 / 3 labeled and 2 / 3 unlabeled) and 25,000 test cases. Two types of labels, positive and negative, are given to show mood. Following (Maas et al., 2011) no stop words are removed from this dataset. For each data set, we randomly selected 10% of the training set for validation."}, {"heading": "4.2 Evaluation of Efficiency", "text": "To evaluate learning efficiency, we used the most common words as dictionaries with sizes from 100 to 20,000 for both sets of data, and tested the computing time for both CDs with variable Gibbs steps and \u03b1-NCE with variable noise sample sizes. The comparison of the average runtime per minibatch is clearly shown in Figure 1, which is averaged on both sets of data. Typically, \u03b1-NCE achieves a 10 to 500-fold acceleration compared to CD. Although both CD and \u03b1-NCE are slower when the input dimension increases, the CD tends to take much longer with each iteration due to multinomial sampling, especially when more Gibbs steps are used. In contrast, the runtime in \u03b1-NCE remains reasonable even when a larger noise size or dimension is applied."}, {"heading": "4.3 Evaluation of Performance", "text": "A direct measurement to evaluate modeling performance is the evaluation of RSM as a generative model for estimating the log probability per word as a perplexity. However, since \u03b1-NCE RSM learns by distinguishing the data and noise from their respective characteristics, parameters are trained more like a feature extractor than a generative model. It is not fair to use perplexity to evaluate performance. Therefore, we evaluated modeling performance with some indirect measurements. For 20 newsgroups, we trained RSMs on the \u03b1 CE training set and reported on the results on document retrieval and document classification. For retrieval, we treated the test set as queries and obtained documents with the same labels in the training set of cosine similarity. Precision recall (P-R) curves and average precision (MAP) are two metrics we used for evaluation."}, {"heading": "4.4 Choice of Noise Level-\u03b1", "text": "In order to determine the best sound level (\u03b1) for PNS, we learned RSMs that used \u03b1-NCE with different noise levels for both word count and idf weighting input on the two datasets. Figure 3 shows that \u03b1-NCE learning with partial noise (\u03b1 > 0) outperforms full noise (\u03b1 = 0) in most situations and achieves better results than CD when retrieving and classifying both datasets. However, learning tends to be extremely difficult when the noise becomes too close to the data, and this explains why performance decreases quickly when \u03b1 \u2192 1. In addition, curves in Figure 3 also imply the choice of \u03b1, with larger sets such as IMDB requiring relatively smaller \u03b1. Nevertheless, a systematic strategy for selecting optimal \u03b1 is examined in future work. In practice, a range of 0.3 to 0.5 is recommended."}, {"heading": "5 Conclusions", "text": "We propose a novel approach \u03b1-NCE to efficiently learn undirected topic models such as RSM, which allows large vocabulary sizes. It is a new estimator based on NCE and adapted to documents with variant lengths and weighted inputs. We learn RSMs with \u03b1-NCE based on two classic benchmarks, where it achieves both efficiency in learning as well as accuracy in retrieval and classification tasks."}], "references": [{"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "The Journal of Machine Learning Research, 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio."], "venue": "Proceedings", "citeRegEx": "Bergstra et al\\.,? 2010", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan."], "venue": "the Journal of machine Learning research, 3:993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Training restricted boltzmann machines on word observations", "author": ["George E Dahl", "Ryan P Adams", "Hugo Larochelle."], "venue": "arXiv preprint arXiv:1202.5695.", "citeRegEx": "Dahl et al\\.,? 2012", "shortCiteRegEx": "Dahl et al\\.", "year": 2012}, {"title": "Stochastic ratio matching of rbms for sparse high-dimensional inputs", "author": ["Yann Dauphin", "Yoshua Bengio."], "venue": "Advances in Neural Information Processing Systems, pages 1340\u20131348.", "citeRegEx": "Dauphin and Bengio.,? 2013", "shortCiteRegEx": "Dauphin and Bengio.", "year": 2013}, {"title": "Noisecontrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["Michael Gutmann", "Aapo Hyv\u00e4rinen."], "venue": "International Conference on Artificial Intelligence and Statistics, pages 297\u2013304.", "citeRegEx": "Gutmann and Hyv\u00e4rinen.,? 2010", "shortCiteRegEx": "Gutmann and Hyv\u00e4rinen.", "year": 2010}, {"title": "Replicated softmax: an undirected topic model", "author": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov."], "venue": "Advances in neural information processing systems, pages 1607\u20131614.", "citeRegEx": "Hinton and Salakhutdinov.,? 2009", "shortCiteRegEx": "Hinton and Salakhutdinov.", "year": 2009}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["Geoffrey Hinton."], "venue": "Neural computation, 14(8):1771\u20131800.", "citeRegEx": "Hinton.,? 2002", "shortCiteRegEx": "Hinton.", "year": 2002}, {"title": "A practical guide to training restricted boltzmann machines", "author": ["Geoffrey Hinton."], "venue": "Momentum, 9(1):926.", "citeRegEx": "Hinton.,? 2010", "shortCiteRegEx": "Hinton.", "year": 2010}, {"title": "Learning the similarity of documents: An information-geometric approach to document retrieval and categorization", "author": ["Thomas Hofmann"], "venue": null, "citeRegEx": "Hofmann.,? \\Q2000\\E", "shortCiteRegEx": "Hofmann.", "year": 2000}, {"title": "Some extensions of score matching", "author": ["Aapo Hyv\u00e4rinen."], "venue": "Computational statistics & data analysis, 51(5):2499\u20132512.", "citeRegEx": "Hyv\u00e4rinen.,? 2007", "shortCiteRegEx": "Hyv\u00e4rinen.", "year": 2007}, {"title": "On the alias method for generating random variables from a discrete distribution", "author": ["Richard A Kronmal", "Arthur V Peterson Jr."], "venue": "The American Statistician, 33(4):214\u2013218.", "citeRegEx": "Kronmal and Jr.,? 1979", "shortCiteRegEx": "Kronmal and Jr.", "year": 1979}, {"title": "A probabilistic model for semantic word vectors", "author": ["Andrew L Maas", "Andrew Y Ng."], "venue": "NIPS Workshop on Deep Learning and Unsupervised Feature Learning.", "citeRegEx": "Maas and Ng.,? 2010", "shortCiteRegEx": "Maas and Ng.", "year": 2010}, {"title": "Learning word vectors for sentiment analysis", "author": ["Andrew L Maas", "Raymond E Daly", "Peter T Pham", "Dan Huang", "Andrew Y Ng", "Christopher Potts."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Maas et al\\.,? 2011", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["Andriy Mnih", "Geoffrey E Hinton."], "venue": "Advances in neural information processing systems, pages 1081\u20131088.", "citeRegEx": "Mnih and Hinton.,? 2009", "shortCiteRegEx": "Mnih and Hinton.", "year": 2009}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["Andriy Mnih", "Yee Whye Teh."], "venue": "arXiv preprint arXiv:1206.6426.", "citeRegEx": "Mnih and Teh.,? 2012", "shortCiteRegEx": "Mnih and Teh.", "year": 2012}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Frederic Morin", "Yoshua Bengio."], "venue": "Proceedings of the international workshop on artificial intelligence and statistics, pages 246\u2013252. Citeseer.", "citeRegEx": "Morin and Bengio.,? 2005", "shortCiteRegEx": "Morin and Bengio.", "year": 2005}, {"title": "Semantic hashing", "author": ["Ruslan Salakhutdinov", "Geoffrey Hinton."], "venue": "International Journal of Approximate Reasoning, 50(7):969\u2013978.", "citeRegEx": "Salakhutdinov and Hinton.,? 2009", "shortCiteRegEx": "Salakhutdinov and Hinton.", "year": 2009}, {"title": "Introduction to modern information retrieval", "author": ["Gerard Salton", "Michael J McGill"], "venue": null, "citeRegEx": "Salton and McGill.,? \\Q1983\\E", "shortCiteRegEx": "Salton and McGill.", "year": 1983}, {"title": "Modeling documents with deep boltzmann machines", "author": ["Nitish Srivastava", "Ruslan R Salakhutdinov", "Geoffrey E Hinton."], "venue": "arXiv preprint arXiv:1309.6865.", "citeRegEx": "Srivastava et al\\.,? 2013", "shortCiteRegEx": "Srivastava et al\\.", "year": 2013}, {"title": "Training restricted boltzmann machines using approximations to the likelihood gradient", "author": ["Tijmen Tieleman."], "venue": "Proceedings of the 25th international conference on Machine learning, pages 1064\u2013 1071. ACM.", "citeRegEx": "Tieleman.,? 2008", "shortCiteRegEx": "Tieleman.", "year": 2008}], "referenceMentions": [{"referenceID": 9, "context": "They are mainly constructed by directed structure like pLSA (Hofmann, 2000) and LDA (Blei et al.", "startOffset": 60, "endOffset": 75}, {"referenceID": 2, "context": "They are mainly constructed by directed structure like pLSA (Hofmann, 2000) and LDA (Blei et al., 2003).", "startOffset": 84, "endOffset": 103}, {"referenceID": 19, "context": "Accompanied by the vast developments in deep learning, several undirected topic models, such as (Salakhutdinov and Hinton, 2009; Srivastava et al., 2013), have recently been reported to achieve great improvements in efficiency and accuracy.", "startOffset": 96, "endOffset": 153}, {"referenceID": 21, "context": "Accompanied by the vast developments in deep learning, several undirected topic models, such as (Salakhutdinov and Hinton, 2009; Srivastava et al., 2013), have recently been reported to achieve great improvements in efficiency and accuracy.", "startOffset": 96, "endOffset": 153}, {"referenceID": 6, "context": "Replicated Softmax model (RSM) (Hinton and Salakhutdinov, 2009), a kind of typical undirected topic model, is composed of a family of Restricted Boltzmann Machines (RBMs).", "startOffset": 31, "endOffset": 63}, {"referenceID": 16, "context": "A similar architecture was used in word representations like (Mnih and Hinton, 2009; Mikolov et al., 2013a).", "startOffset": 61, "endOffset": 107}, {"referenceID": 14, "context": "A similar architecture was used in word representations like (Mnih and Hinton, 2009; Mikolov et al., 2013a).", "startOffset": 61, "endOffset": 107}, {"referenceID": 5, "context": "Recently, a new estimator Noise Contrastive Estimate (NCE) (Gutmann and Hyv\u00e4rinen, 2010) is proposed for unnormalized models, and shows great efficiency in learning word representations such as in (Mnih and Teh, 2012; Mikolov et al.", "startOffset": 59, "endOffset": 88}, {"referenceID": 17, "context": "Recently, a new estimator Noise Contrastive Estimate (NCE) (Gutmann and Hyv\u00e4rinen, 2010) is proposed for unnormalized models, and shows great efficiency in learning word representations such as in (Mnih and Teh, 2012; Mikolov et al., 2013b).", "startOffset": 197, "endOffset": 240}, {"referenceID": 15, "context": "Recently, a new estimator Noise Contrastive Estimate (NCE) (Gutmann and Hyv\u00e4rinen, 2010) is proposed for unnormalized models, and shows great efficiency in learning word representations such as in (Mnih and Teh, 2012; Mikolov et al., 2013b).", "startOffset": 197, "endOffset": 240}, {"referenceID": 0, "context": "Bengio et al. (2003) pointed this problem out when normalizing the softmax probability in the neural language model (NNLM), and Morin and Bengio (2005) solved it based on a hierarchical binary tree.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Bengio et al. (2003) pointed this problem out when normalizing the softmax probability in the neural language model (NNLM), and Morin and Bengio (2005) solved it based on a hierarchical binary tree.", "startOffset": 0, "endOffset": 152}, {"referenceID": 0, "context": "Bengio et al. (2003) pointed this problem out when normalizing the softmax probability in the neural language model (NNLM), and Morin and Bengio (2005) solved it based on a hierarchical binary tree. A similar architecture was used in word representations like (Mnih and Hinton, 2009; Mikolov et al., 2013a). Directed tree structures cannot be applied to undirected models like RSM, but stochastic approaches can work well. For instance, Dahl et al. (2012) found that several Metropolis Hastings sampling (MH) approaches approximate the softmax distribution in CD well, although MH requires additional complexity in computation.", "startOffset": 0, "endOffset": 456}, {"referenceID": 0, "context": "Bengio et al. (2003) pointed this problem out when normalizing the softmax probability in the neural language model (NNLM), and Morin and Bengio (2005) solved it based on a hierarchical binary tree. A similar architecture was used in word representations like (Mnih and Hinton, 2009; Mikolov et al., 2013a). Directed tree structures cannot be applied to undirected models like RSM, but stochastic approaches can work well. For instance, Dahl et al. (2012) found that several Metropolis Hastings sampling (MH) approaches approximate the softmax distribution in CD well, although MH requires additional complexity in computation. Hyv\u00e4rinen (2007) proposed Ratio Matching (RM) to train unnormalized models, and Dauphin and Bengio (2013) added stochastic approaches in RM to accommodate high-dimensional inputs.", "startOffset": 0, "endOffset": 645}, {"referenceID": 0, "context": "Bengio et al. (2003) pointed this problem out when normalizing the softmax probability in the neural language model (NNLM), and Morin and Bengio (2005) solved it based on a hierarchical binary tree. A similar architecture was used in word representations like (Mnih and Hinton, 2009; Mikolov et al., 2013a). Directed tree structures cannot be applied to undirected models like RSM, but stochastic approaches can work well. For instance, Dahl et al. (2012) found that several Metropolis Hastings sampling (MH) approaches approximate the softmax distribution in CD well, although MH requires additional complexity in computation. Hyv\u00e4rinen (2007) proposed Ratio Matching (RM) to train unnormalized models, and Dauphin and Bengio (2013) added stochastic approaches in RM to accommodate high-dimensional inputs.", "startOffset": 0, "endOffset": 734}, {"referenceID": 21, "context": "As RSM is usually used as the first layer in many deeper undirected models like Deep Boltzmann Machines (Srivastava et al., 2013), \u03b1-NCE can be readily extended to learn them efficiently.", "startOffset": 104, "endOffset": 129}, {"referenceID": 7, "context": "Common strategies to overcome this intractability are MCMCbased approaches such as Contrastive Divergence (CD) (Hinton, 2002) and Persistent CD (PCD) (Tieleman, 2008), both of which require repeating Gibbs steps of h(i) \u223c P\u03b8(h|V (i)) and V (i+1) \u223c P\u03b8(V |h(i)) to generate model samples to approximate the gradient.", "startOffset": 111, "endOffset": 125}, {"referenceID": 22, "context": "Common strategies to overcome this intractability are MCMCbased approaches such as Contrastive Divergence (CD) (Hinton, 2002) and Persistent CD (PCD) (Tieleman, 2008), both of which require repeating Gibbs steps of h(i) \u223c P\u03b8(h|V (i)) and V (i+1) \u223c P\u03b8(V |h(i)) to generate model samples to approximate the gradient.", "startOffset": 150, "endOffset": 166}, {"referenceID": 3, "context": "Unlike (Dahl et al., 2012) that retains CD, we adopted NCE as the basic learning strategy.", "startOffset": 7, "endOffset": 26}, {"referenceID": 5, "context": "Gutmann and Hyv\u00e4rinen (2010) showed that the NCE gradient\u2207\u03b8J(\u03b8) will reach the ML gradient when k \u2192 \u221e.", "startOffset": 0, "endOffset": 29}, {"referenceID": 17, "context": "Different from (Mnih and Teh, 2012), which generates noise per word, RSM requires the estimator to sample the noise at the document level.", "startOffset": 15, "endOffset": 35}, {"referenceID": 5, "context": "For a fixed k, Gutmann and Hyv\u00e4rinen (2010) suggested choosing the noise close to the data for a sufficient learning result, indicating full noise might not be satisfactory.", "startOffset": 15, "endOffset": 44}, {"referenceID": 20, "context": "Note that D is not necessarily an integer in UCE, and allows choosing a real-valued weights on the document such as idf -weighting (Salton and McGill, 1983).", "startOffset": 131, "endOffset": 156}, {"referenceID": 13, "context": "Following (Maas et al., 2011), no stop words are removed from this dataset.", "startOffset": 10, "endOffset": 29}, {"referenceID": 8, "context": "We implemented \u03b1-NCE according to the parameter settings in (Hinton, 2010) using SGD in minibatches of size 128 and an initialized learning rate of 0.", "startOffset": 60, "endOffset": 74}, {"referenceID": 17, "context": "Although learning the partition function Zc D separately for every length D is nearly impossible, as in (Mnih and Teh, 2012) we also surprisingly found freezing Zc D as a constant function of D without updating never harmed but actually enhanced the performance.", "startOffset": 104, "endOffset": 124}, {"referenceID": 1, "context": "All the experiments were run on a single GPU GTX970 using the library Theano (Bergstra et al., 2010).", "startOffset": 77, "endOffset": 100}, {"referenceID": 12, "context": "Bag of Words (BoW) (Maas and Ng, 2010) 86.", "startOffset": 19, "endOffset": 38}, {"referenceID": 13, "context": "75% LDA (Maas et al., 2011) 67.", "startOffset": 8, "endOffset": 27}, {"referenceID": 13, "context": "42% LSA (Maas et al., 2011) 83.", "startOffset": 8, "endOffset": 27}, {"referenceID": 3, "context": "44% WRRBM (Dahl et al., 2012) 87.", "startOffset": 10, "endOffset": 29}, {"referenceID": 11, "context": "Bag of Words (BoW) (Maas and Ng, 2010) 86.75% LDA (Maas et al., 2011) 67.42% LSA (Maas et al., 2011) 83.96% Maas et al. (2011)\u2019s \u201cfull\u201d model 87.", "startOffset": 20, "endOffset": 127}], "year": 2015, "abstractText": "Replicated Softmax model, a well-known undirected topic model, is powerful in extracting semantic representations of documents. Traditional learning strategies such as Contrastive Divergence are very inefficient. This paper provides a novel estimator to speed up the learning based on Noise Contrastive Estimate, extended for documents of variant lengths and weighted inputs. Experiments on two benchmarks show that the new estimator achieves great learning efficiency and high accuracy on document retrieval and classification.", "creator": "TeX"}}}