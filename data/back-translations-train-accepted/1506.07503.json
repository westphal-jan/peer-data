{"id": "1506.07503", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2015", "title": "Attention-Based Models for Speech Recognition", "abstract": "Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks in- cluding machine translation, handwriting synthesis and image caption gen- eration. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in reaches a competitive 18.7% phoneme error rate (PER) on the TIMIT phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18% PER in single utterances and 20% in 10-times longer (repeated) utterances. Finally, we propose a change to the at- tention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6% level.", "histories": [["v1", "Wed, 24 Jun 2015 19:10:33 GMT  (4995kb,D)", "http://arxiv.org/abs/1506.07503v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE stat.ML", "authors": ["jan chorowski", "dzmitry bahdanau", "dmitriy serdyuk", "kyunghyun cho", "yoshua bengio"], "accepted": true, "id": "1506.07503"}, "pdf": {"name": "1506.07503.pdf", "metadata": {"source": "CRF", "title": "Attention-Based Models for Speech Recognition", "authors": ["Jan Chorowski", "Dzmitry Bahdanau"], "emails": ["jan.chorowski@ii.uni.wroc.pl"], "sections": [{"heading": "1 Introduction", "text": "This year it is so far that it will be able to erenen.n the aforementioned lcihsrcsrteeSe"}, {"heading": "2 Attention-Based Model for Speech Recognition", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 General Framework", "text": "An attention-based sequence generator (ARSG) is a recursive neural network that stochastically generates an output sequence (y1,., yT) from an input mechanism. \u2212 In practice, x is often processed by an encoder that outputs a sequential input representation h = (h1,., hL) that is more suitable for the attention mechanism to work with. In the context of this work, the output y is a sequence of phonemes, and the input x = (x1,., xL \u2032) is a sequence of feature vectors. Each vector is extracted from a small overlapping window of audio frames. The encoder is implemented as a deep bidirectional recursive network (BiRNN) to form a sequential representation of the length L = L \u00b2."}, {"heading": "2.2 Proposed Model: ARSG with Convolutional Features", "text": "We start from the ARSG-based model with the content-based attention mechanism proposed in [2]. (7) w and b are vectors, W and V are matrices. We extend this content-based attention mechanism of the original model to be location-aware, taking into account the alignment generated in the previous step. First, we extract k vectors fi, j-Rk for each position j of the previous alignment \u03b1i-1, engulfing it with a matrix F-Rk-r: fi = F-Rk-1. (8) These additional vectors fi, j are then used by the scoring mechanism ei, j: ei, j = w > tanh (Wsi \u2212 1 + V hj + Ufi, j + b) (9)."}, {"heading": "2.3 Score Normalization: Sharpening and Smoothing", "text": "There are three potential problems with normalization in Eq. (6). Firstly, if the input sequence h is long, the glance gi will probably contain noisy information from many irrelevant feature vectors hj, since the normalized results \u03b1i, j all have positive and summary results to 1. This, however, makes it difficult for the proposed ARSG to clearly focus on a few relevant frames. Secondly, the attention mechanism is required to look at all L-frames every time it decodes a single output yi, while the output of the length T is decoded, resulting in a computational complexity of O (LT). This can easily become prohibitively expensive if the input sequences are long (and the problem that is less serious for machine translation, because in this case the input sequence consists of words, not 20ms acoustic frames).The other side of the coin is that the use of normalization mostly focuses on a max (6) in Eq."}, {"heading": "3 Related Work", "text": "Speech recognition mechanisms based on the connectionist temporal classification (CTC, [13]) and its extension, RNN Transducer [14], are the closest to the ARSG model taken into account in this paper. They follow previous work on consistently traceable deep learning about sequences with gradient signals flowing through the alignment process [15]. Furthermore, it was recently found that the CTC is capable of directly transcribing text from speech without intermediate phonetic representation [17]. The ARSG considered differs from both the CTC and the RNN Transducer in two respects. While the attention mechanism deterministically aligns the input and output sequences, the CTC and RNN Transducers treat alignment as a latently random variable."}, {"heading": "4 Experimental Setup", "text": "All experiments were performed on the TIMIT corpus [19]. We used the train dev test split from the Kaldi [20] TIMIT s5 recipe. We trained on the standard speaker set 462, removing all SA expressions, and used the 50 speaker set for early stop. We tested the core test set of 24 loudspeakers. All networks were trained on filter bank functions on a 40-mel scale, along with the energy in each frame and the first and second time differences, resulting in a total of 123 features per frame. Each feature was re-scaled to have zero mean and unit variance over the training set. Networks were trained on the entire 61-phone set, enhanced with an additional \"end-of-sequence\" symbol attached to each target sequence. Similarly, we added an all-zero frame at the end of each input sequence to indicate the end of the utterance."}, {"heading": "4.1 Training Procedure", "text": "One feature of the ARSG models is that different subsets of parameters are reused at different rates; L times for encoder parameters, LT for attention weights, and T times for all other FDHC0 _ SX209: Michael colored the bedroom wall with crayons. ARSG parameters. As a result, the scales of derivatives w.r.t. parameters vary significantly, and we handle them using an adaptive learning rate algorithm, AdaDelta [21], which has two hyperparameters and \u03c1. All weight matrices have been initialized from a normal Gaussian distribution with their standard deviation to 0.01. Recurring weights have also been orthogonized. As TIMIT is a relatively small data set, proper regulation is crucial. We used adaptive weight noise as the main regulator [22], while we initially used our models with a slit norm limitation [95 to maximum] of 0.1% of the norm was achieved."}, {"heading": "4.2 Details of Evaluated Models", "text": "We evaluated the ARSGs with different attention mechanisms. The encoder was a 3-layer BiRNN with 256 GRU units in each direction, and the activation of the 512 top layer units was used as a representation h. The generator had a single recurring layer of 256 GRU units. The generation in equivalent (3) had a hidden layer of 64 maximum units. The initial states of both the encoder and the generator were treated as additional parameters. Our base model is that with a purely content-based attention mechanism (see equivalent (5) - (7). The scoring network in equivalent (7) had 512 hidden units. The other two models use the convolution characteristics in equivalent (8) with k = 10 and r = 201. One of them uses the smoothing of equivalent (5) - (7)."}, {"heading": "5 Results", "text": "All models achieved competitive PER (see Table 1), with wave characteristics showing a relative improvement of 3.7% over baseline and a further 5.9% in smoothing. To our surprise (see Figure 2.1.), the baseline model has learned to align correctly. An alignment generated by the baseline model on a sequence of repeated phonemes (expression FDHC0 SX209) is shown in Fig. 3, which shows that the baseline model is not confused by short-term repetitions. We can also see from the figure that it selects frames that are close to the beginning or 3 The use of weight noise from the beginning of training caused severe mismatches - even slightly before the phonemposition provided as part of the dataset. Visually, the alignments generated by the other models were very similar."}, {"heading": "5.1 Forced Alignment of Long Utterances", "text": "In fact, most of them are able to play by the rules they have established in the past, and they are able to play by the rules they have established in the past."}, {"heading": "5.2 Decoding Long Utterances", "text": "The results are presented in Fig. 5. The base model does not decode long statements, even when a narrow window is used to restrict the alignments it generates; the other two location-based networks are able to decode statements made by concatenating up to 11 test statements; better results were obtained with a wider window, presumably because it resembles the training conditions when the attention mechanism saw the entire input sequence at each step. In the wide window, both networks scored about 20% PRO on the long statements, suggesting that the proposed location-aware attention mechanism can scale to sequences for much longer with only minor changes required in decoding than those in the training set."}, {"heading": "6 Conclusions", "text": "One desirable feature of the proposed model is that it can recognize utterances for much longer than those on which it has been trained. In the future, we expect this model to be used for direct recognition of text from speech [10, 17]. In this case, it could become important to incorporate a monolingual language model into the ARSG architecture [26]. This work has brought two new ideas for attention mechanisms: a better normalization approach that leads to smoother alignments, and a generic principle for extracting and utilizing features from the previous alignments. Both can potentially be applied beyond speech recognition. For example, the proposed attention can be achieved without modification in neural turing machines or by using 2-D folding instead of 1-D to improve image caption generation [3]."}, {"heading": "Acknowledgments", "text": "All experiments were carried out using the libraries Theano [27, 28], PyLearn2 [29] and Blocks [30]. The authors would like to thank the following institutions for research funding and computer support: National Science Center (Poland), NSERC, Calcul Que \ufffd bec, Compute Canada, the Canada Research Chairs and CIFAR. Bahdanau also thanks Planet Intelligent Systems GmbH and Yandex."}, {"heading": "B Detailed results of experiments", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Recurrent models of visual attention", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "End-to-end continuous speech recognition using attention-based recurrent NN: First results", "author": ["Jan Chorowski", "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "The application of hidden markov models in speech recognition", "author": ["Mark Gales", "Steve Young"], "venue": "Found. Trends Signal Process.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "Li Deng", "Dong Yu", "G.E. Dahl", "A Mohamed", "N. Jaitly", "A Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Deepspeech: Scaling up end-to-end speech recognition", "author": ["Awni Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos", "Erich Elsen", "Ryan Prenger", "Sanjeev Satheesh", "Shubho Sengupta", "Adam Coates"], "venue": "arXiv preprint arXiv:1412.5567,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural. Comput.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1997}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In EMNLP 2014,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks", "author": ["Alex Graves", "Santiago Fern\u00e1ndez", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "In ICML-06,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Sequence transduction with recurrent neural networks", "author": ["Alex Graves"], "venue": "In ICML-12,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Gradient based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proc. IEEE,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1998}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Alex Graves", "Navdeep Jaitly"], "venue": "In ICML-14,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Weakly supervised memory networks", "author": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus"], "venue": "arXiv preprint arXiv:1503.08895,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "DARPA TIMIT acoustic phonetic continuous speech", "author": ["J.S. Garofolo", "L.F. Lamel", "W.M. Fisher", "J.G. Fiscus", "D.S. Pallett", "N.L. Dahlgren"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1993}, {"title": "The kaldi speech recognition toolkit", "author": ["Daniel Povey", "Arnab Ghoshal", "Gilles Boulianne", "Lukas Burget", "Ondrej Glembek", "Nagendra Goel", "Mirko Hannemann", "Petr Motlicek", "Yanmin Qian", "Petr Schwarz", "others"], "venue": "In Proc. ASRU,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Practical variational inference for neural networks", "author": ["Alex Graves"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "arXiv preprint arXiv:1409.3215,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Combining time-and frequency-domain convolution in convolutional neural network-based phone recognition", "author": ["L\u00e1szl\u00f3 T\u00f3th"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "On using monolingual corpora in neural machine translation", "author": ["Caglar Gulcehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Loic Barrault", "Huei-Chi Lin", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1503.03535,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Pylearn2: a machine learning research", "author": ["Ian J. Goodfellow", "David Warde-Farley", "Pascal Lamblin", "Vincent Dumoulin", "Mehdi Mirza", "Razvan Pascanu", "James Bergstra", "Fr\u00e9d\u00e9ric Bastien", "Yoshua Bengio"], "venue": "library. arXiv preprint arXiv:1308.4214,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks including machine translation, handwriting synthesis [1, 2] and image caption generation [3].", "startOffset": 202, "endOffset": 208}, {"referenceID": 1, "context": "Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks including machine translation, handwriting synthesis [1, 2] and image caption generation [3].", "startOffset": 202, "endOffset": 208}, {"referenceID": 2, "context": "Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks including machine translation, handwriting synthesis [1, 2] and image caption generation [3].", "startOffset": 238, "endOffset": 241}, {"referenceID": 1, "context": "We show that while an adaptation of the model used for machine translation in [2] reaches a competitive 18.", "startOffset": 78, "endOffset": 81}, {"referenceID": 0, "context": "Recently, attention-based recurrent networks have been successfully applied to a wide variety of tasks, such as handwriting synthesis [1], machine translation [2], image caption generation [3] and visual object classification [4].", "startOffset": 134, "endOffset": 137}, {"referenceID": 1, "context": "Recently, attention-based recurrent networks have been successfully applied to a wide variety of tasks, such as handwriting synthesis [1], machine translation [2], image caption generation [3] and visual object classification [4].", "startOffset": 159, "endOffset": 162}, {"referenceID": 2, "context": "Recently, attention-based recurrent networks have been successfully applied to a wide variety of tasks, such as handwriting synthesis [1], machine translation [2], image caption generation [3] and visual object classification [4].", "startOffset": 189, "endOffset": 192}, {"referenceID": 3, "context": "Recently, attention-based recurrent networks have been successfully applied to a wide variety of tasks, such as handwriting synthesis [1], machine translation [2], image caption generation [3] and visual object classification [4].", "startOffset": 226, "endOffset": 229}, {"referenceID": 1, "context": "From this perspective it is similar to machine translation and handwriting synthesis tasks, for which attention-based methods have been found suitable [2, 1].", "startOffset": 151, "endOffset": 157}, {"referenceID": 0, "context": "From this perspective it is similar to machine translation and handwriting synthesis tasks, for which attention-based methods have been found suitable [2, 1].", "startOffset": 151, "endOffset": 157}, {"referenceID": 4, "context": "The An early version of this work was presented at the NIPS 2014 Deep Learning Workshop [5].", "startOffset": 88, "endOffset": 91}, {"referenceID": 5, "context": "dominant approach is still based on hybrid systems consisting of a deep neural acoustic model, a triphone HMM model and an n-gram language model [8, 9].", "startOffset": 145, "endOffset": 151}, {"referenceID": 6, "context": "dominant approach is still based on hybrid systems consisting of a deep neural acoustic model, a triphone HMM model and an n-gram language model [8, 9].", "startOffset": 145, "endOffset": 151}, {"referenceID": 7, "context": "Excellent results by an HMM-less recognizer have recently been reported, with the system consisting of a CTC-trained neural network and a language model [10].", "startOffset": 153, "endOffset": 157}, {"referenceID": 1, "context": "We start with a model proposed in [2] for the machine translation task as the baseline.", "startOffset": 34, "endOffset": 37}, {"referenceID": 1, "context": "In order to circumvent this undesired behavior, in this paper, we propose to modify the attention mechanism such that it explicitly takes into account both (a) the location of the focus from the previous step, as in [6] and (b) the features of the input sequence, as in [2].", "startOffset": 270, "endOffset": 273}, {"referenceID": 1, "context": "where si\u22121 is the (i \u2212 1)-th state of the recurrent neural network to which we refer as the generator, \u03b1i \u2208 R is a vector of the attention weights, also often called the alignment [2].", "startOffset": 180, "endOffset": 183}, {"referenceID": 3, "context": "Using the terminology from [4], we call gi a glimpse.", "startOffset": 27, "endOffset": 30}, {"referenceID": 8, "context": "The step is completed by computing a new generator state: si = Recurrency(si\u22121, gi, yi) (4) Long short-term memory units (LSTM, [11]) and gated recurrent units (GRU, [12]) are typically used as a recurrent activation, to which we refer as a recurrency.", "startOffset": 128, "endOffset": 132}, {"referenceID": 9, "context": "The step is completed by computing a new generator state: si = Recurrency(si\u22121, gi, yi) (4) Long short-term memory units (LSTM, [11]) and gated recurrent units (GRU, [12]) are typically used as a recurrent activation, to which we refer as a recurrency.", "startOffset": 166, "endOffset": 170}, {"referenceID": 1, "context": ", [2] or [3]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 2, "context": ", [2] or [3]).", "startOffset": 9, "endOffset": 12}, {"referenceID": 1, "context": "a BiRNN [2] or a deep convolutional network [3] that encode contextual information into every element of h .", "startOffset": 8, "endOffset": 11}, {"referenceID": 2, "context": "a BiRNN [2] or a deep convolutional network [3] that encode contextual information into every element of h .", "startOffset": 44, "endOffset": 47}, {"referenceID": 0, "context": "For instance, Graves [1] used the location-based attention mechanism using a Gaussian mixture model in his handwriting synthesis model.", "startOffset": 21, "endOffset": 24}, {"referenceID": 1, "context": "2 Proposed Model: ARSG with Convolutional Features We start from the ARSG-based model with the content-based attention mechanism proposed in [2].", "startOffset": 141, "endOffset": 144}, {"referenceID": 10, "context": "Speech recognizers based on the connectionist temporal classification (CTC, [13]) and its extension, RNN Transducer [14], are the closest to the ARSG model considered in this paper.", "startOffset": 76, "endOffset": 80}, {"referenceID": 11, "context": "Speech recognizers based on the connectionist temporal classification (CTC, [13]) and its extension, RNN Transducer [14], are the closest to the ARSG model considered in this paper.", "startOffset": 116, "endOffset": 120}, {"referenceID": 12, "context": "They follow earlier work on end-to-end trainable deep learning over sequences with gradient signals flowing through the alignment process [15].", "startOffset": 138, "endOffset": 142}, {"referenceID": 13, "context": "They have been shown to perform well on the phoneme recognition task [16].", "startOffset": 69, "endOffset": 73}, {"referenceID": 14, "context": "Furthermore, the CTC was recently found to be able to directly transcribe text from speech without any intermediate phonetic representation [17].", "startOffset": 140, "endOffset": 144}, {"referenceID": 15, "context": "[18] describes a similar hybrid attention mechanism, where location embeddings are used as input to the attention model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "We closely followed the procedure in [16].", "startOffset": 37, "endOffset": 41}, {"referenceID": 16, "context": "All experiments were performed on the TIMIT corpus [19].", "startOffset": 51, "endOffset": 55}, {"referenceID": 17, "context": "We used the train-dev-test split from the Kaldi [20] TIMIT s5 recipe.", "startOffset": 48, "endOffset": 52}, {"referenceID": 18, "context": "parameters vary significantly, and we handle it by using an adaptive learning rate algorithm, AdaDelta [21] which has two hyperparameters and \u03c1.", "startOffset": 103, "endOffset": 107}, {"referenceID": 19, "context": "We used the adaptive weight noise as a main regularizer [22].", "startOffset": 56, "endOffset": 60}, {"referenceID": 20, "context": "We first trained our models with a column norm constraint [23] with the maximum norm 1 until the lowest development negative log-likelihood is achieved.", "startOffset": 58, "endOffset": 62}, {"referenceID": 21, "context": "Decoding Procedure A left-to-right beam search over phoneme sequences was used during decoding [24].", "startOffset": 95, "endOffset": 99}, {"referenceID": 13, "context": "6% RNN Transducer [16] N/A 17.", "startOffset": 18, "endOffset": 22}, {"referenceID": 22, "context": "7% HMM over Time and Frequency Convolutional Net [25] 13.", "startOffset": 49, "endOffset": 53}, {"referenceID": 7, "context": "In the future, we expect this model to be used to directly recognize text from speech [10, 17], in which case it may become important to incorporate a monolingual language model to the ARSG architecture [26].", "startOffset": 86, "endOffset": 94}, {"referenceID": 14, "context": "In the future, we expect this model to be used to directly recognize text from speech [10, 17], in which case it may become important to incorporate a monolingual language model to the ARSG architecture [26].", "startOffset": 86, "endOffset": 94}, {"referenceID": 23, "context": "In the future, we expect this model to be used to directly recognize text from speech [10, 17], in which case it may become important to incorporate a monolingual language model to the ARSG architecture [26].", "startOffset": 203, "endOffset": 207}, {"referenceID": 2, "context": "For instance, the proposed attention can be used without modification in neural Turing machines, or by using 2\u2013D convolution instead of 1\u2013D, for improving image caption generation [3].", "startOffset": 180, "endOffset": 183}], "year": 2015, "abstractText": "Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks including machine translation, handwriting synthesis [1, 2] and image caption generation [3]. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in [2] reaches a competitive 18.7% phoneme error rate (PER) on the TIMIT phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18% PER in single utterances and 20% in 10-times longer (repeated) utterances. Finally, we propose a change to the attention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6% level.", "creator": "LaTeX with hyperref package"}}}