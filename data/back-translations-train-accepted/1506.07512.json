{"id": "1506.07512", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2015", "title": "Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization", "abstract": "We develop a family of accelerated stochastic algorithms that minimize sums of convex functions. Our algorithms improve upon the fastest running time for empirical risk minimization (ERM), and in particular linear least-squares regression, across a wide range of problem settings. To achieve this, we establish a framework based on the classical proximal point algorithm. Namely, we provide several algorithms that reduce the minimization of a strongly convex function to approximate minimizations of regularizations of the function. Using these results, we accelerate recent fast stochastic algorithms in a black-box fashion. Empirically, we demonstrate that the resulting algorithms exhibit notions of stability that are advantageous in practice. Both in theory and in practice, the provided algorithms reap the computational benefits of adding a large strongly convex regularization term, without incurring a corresponding bias to the original problem.", "histories": [["v1", "Wed, 24 Jun 2015 19:53:45 GMT  (60kb,D)", "http://arxiv.org/abs/1506.07512v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.DS cs.LG", "authors": ["roy frostig", "rong ge 0001", "sham kakade", "aaron sidford"], "accepted": true, "id": "1506.07512"}, "pdf": {"name": "1506.07512.pdf", "metadata": {"source": "CRF", "title": "Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization", "authors": ["Roy Frostig", "Rong Ge", "Sham M. Kakade", "Aaron Sidford"], "emails": ["rf@cs.stanford.edu,", "rongge@microsoft.com,", "skakade@microsoft.com,", "sidford@mit.edu."], "sections": [{"heading": null, "text": "To achieve this, we are establishing a framework based on the classical proximal point algorithm: namely, we are providing several algorithms that reduce the minimization of a strongly convex function in order to approximately minimize the minimization of the regulations of the function. On the basis of these results, we are accelerating the latest fast stochastic algorithms in a black box manner. Empirically, we are showing that the resulting algorithms have notions of stability that are advantageous in practice. In theory as well as in practice, the provided algorithms benefit from the computational advantages of adding a large strongly convex concept of regulation without causing a corresponding distortion of the original problem."}, {"heading": "1 Introduction", "text": "E. E. (E). E. (E). E. (E). E. (E). E. (E). E. (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E). (E. (E). (E). (E). (E). (E). (E). (E. (E). (E). (E. (E). (E). (E). (E). (E).). (E.). (E. (E). (E.). (E). (E. (E. (E).). (E. (E.)."}, {"heading": "1.1 Formal setup", "text": "We consider the ERM issue (1) in the following general constellation: acceptance 1.2 (regularity). Each loss function is valid as L-x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "1.2 Running times and related work", "text": "In Table 1, we compare our results with the runtime of the two classic and newer algorithms for solving the ERM problem (1) and the linear least square regression. Here, we briefly explain the runtimes and related workflows. In the context of the ERM problem, GD refers to the canonical gradient lineage of F, Accel et al. (2012) and Defazio et al. (2014), SVRG is the stochastic variance-reduced gradient of Johnson and Zhang (2013), SAG is the stochastic average gradient of Roux et al. (2012) and Defazio et al. (2014), SDCA is the stochastic coordination of Shalev-Shwartz and Zhang (2013), AP-SDCA is the accelerated SDCA of Shalev-Shwartz and Zhang. (2014) and APCG is the accelerated coordination of three algorithms."}, {"heading": "1.3 Main results", "text": "All formal results in this work are achieved by a framework we are developing for iterative application and acceleration of various minimization algorithms. When we deal with newly developed fast minimizers, we receive, on assumptions of 1,2 and 1,3 algorithms that are guaranteed to solve the ERM problem in the time it takes to solve. (nd) Our framework is based on critical cognition of classic proximal point algorithms (PPA) or proximal iteration: to minimize F (or general algorithms, any convex function), it is enough to become iterative minimizers (x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x - x - x - x - x - x - x - x - x - x - x - x - x - x - x - x - x - x - x - x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "1.4 Paper organization", "text": "The rest of this paper is structured as follows: In Section 2, Section 3, and Section 4, we describe and analyze the approximate proximal point algorithms described above. In Section 5, we discuss practical concerns and cover numerical experiments with dual APPA and related stochastic algorithms. In Appendix A, we provide general technical lemmas used throughout the paper, and in Appendix B, we provide a derivative of the regulated ERM duality and related technical lemmas."}, {"heading": "2 Approximate proximal point algorithm (APPA)", "text": "In this section, we describe our Approximal Point Algorithm (APPA), which is perhaps the simplest in its description and analysis compared to the other procedures described in this paper. In this section, we also present technical machines used throughout the sequence. First, we present our formal abstraction of the inner minimizers (Section 2.1), then we present our algorithm (Section 2.2) and finally, we proceed to its analysis (Section 2.3)."}, {"heading": "2.1 Approximate primal oracles", "text": "To design APPA, we first quantify the error that can be tolerated by an inner minimizer, taking into account the calculation costs for ensuring such an error. The abstraction we use is the following concept of inner approximation: Definition 2.1. An algorithm P is a primary error (c, \u03bb) -oracle if it is initialized at x with an approximate minimizer of fx, \u03bb in time TP ([fx, \u03bb (x) -foptx, \u03bb] / c), and regularity gap of fx. 1In other words, a primary oracle is an algorithm initialized at x that implies the error of fx, \u03bb (x) -foptx in which depends on time, and regularity gap of F. Typical iterative first order algorithms (such as the one in Table 1, Return of Primary (c) -oracle)."}, {"heading": "2.2 Algorithm", "text": "Our approximate proximal dot algorithm (APPA) is derived from the following algorithm 1.1 If the oracle is a randomized algorithm, we require that the expected error be the same, i.e. that the solution is expected to be -approximate. 2AP-SDCA could probably also serve as a primary oracle with the same guarantees. However, the results are given in ShalevShwartz and Zhang (2014) assuming that the original and dual variables are zero. It is not immediately clear how to achieve a general relative error reduction from this specific initial primary-dual pair. Algorithm 1 approximates PPA (APPA) input x (0)."}, {"heading": "2.3 Analysis", "text": "This section provides a proof for Lemma 2,4. First, we consider the effect of an exact inner minimizer, namely, we prove the following problem in relation to the minimum of the inner problem fs, \u03bb to F opt.Lemma 2,7 (relation between minima). For all s'Rd and \u03bb's + xopt. The \u00b5-strong convexity of F opt. (F (s) \u2212 F opt.) Proof. Let xopt = argminx F (x) and for all \u03b1 [0, 1] let x\u03b1 = (1 \u2212 \u03b1) s + xopt. The \u00b5-strong convexity of F opt. (F opt.) implies that for all x-fields (0, 1), F (x\u03b1) F (s) x-opt."}, {"heading": "3 Accelerated APPA", "text": "In this section we show how in general the APPA algorithm of Section 2. Accelerated APPA (algorithm 2) uses the internal minimizers more efficiently, but requires a smaller minimization factor than APPA. The algorithm and its analysis immediately prove theorem 1.1 and in turn provide another means by which we achieve the accelerated runtime guarantees for solving (1). We first present the algorithm and state its runtime guarantees (Section 3.1), then we demonstrate the guarantees in the context of the analysis (Section 3.2)."}, {"heading": "3.1 Algorithm", "text": "Our accelerated APPA algorithm is given by algorithm 2. In each iteration it still makes a single call to a primordial oracle, but instead of requiring a fixed constant minimization polynom depends on the ratio of \u03bb and \u00b5.Algorithm 2 accelerators APPA-Input x (0): 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0:"}, {"heading": "3.2 Analysis", "text": "We point out that the aspects of the evidence in this section are similar to the analysis in ShalevShwartz and Zhang (2014), which produce similar results in a more specific constellation. \u2022 In Lemma 3.3, we show that applying a primary oracle to the inner minimization problem gives us a square lower limit to F (x). \u2022 In Lemma 3.4, we use this lower limit to construct a series of lower objective functions f, and accelerate the APPA algorithms that comprise the bulk of the analysis. \u2022 In Lemma 3.5, we show that the requirements of Lemma 3.4 can be met by using a primary oracle."}, {"heading": "4 Dual APPA", "text": "In this section, we will develop Dual APPA (Algorithm 3), a natural approximal point algorithm that works entirely in the regulated ERM dual. Here, we will focus on the theoretical properties of Dual APPA; Section 5 will deal more practically with aspects of Dual APPA. We will first present an abstraction for dual internal minimizers (Section 4.1), then present the algorithm (Section 4.2), and finally go through its runtime analysis (Section 4.3)."}, {"heading": "4.1 Approximate dual oracles", "text": "Our primary goal in this section is to quantify how much objective functional progress an algorithm needs to make in the dual problem gs, \u03bb (see Section 1.1) in order to ensure primary progress at a speed similar to that of APPA (Algorithm 1). Here, similar to Section 2.1, we formally define our requirements for an approximate dual inner dual. In particular, we use the following term of dual oracle. Definition 4.1. An algorithm D is a dual (c, \u03bb) oracle when it outputs a (gs, \u03bb) -gopts, \u03bb / c) approximate minimizer of gs in the face of s-Rd and y-Rn D (s, y), which is a dual oracle. In particular, we note that APAL is an Aistral Dual Oracle (4.CPC). 4Dual-based algorithms for regulated ERM and variants of coordinate descend can typically be used as a dual oracle."}, {"heading": "4.2 Algorithm", "text": "Our dual APPA is achieved by the following algorithm 3.Algorithm 3 Dual APPA Input x (0), \u03bb > 0 Input dual (\u03c3, \u03bb) -oracle D (see theorem 4.3 for \u03c3) y (0) \u2190 y (x (0)) for t = 1,.., T do y (t) \u2190 D (x (t \u2212 1), y (t \u2212 1))) x (t) \u2190 x x (t \u2212 1)) end for output x (T) Dual APPA (algorithm 3) repeatedly requires a dual oracle, while the production of theoracles via the dual to primary mapping (5) along the path can be achieved. We show that it reaches the following runtime limit: Theorem 4.3 (Un-regulation in Dual APPA) that the dual APPA (Un-regulation in Dual APPA) -oracle D, where the original iterate can be achieved via the dual to primary mapping (5)."}, {"heading": "4.3 Analysis", "text": "This marks the initial error at the beginning of every dual APPA iteration. \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212"}, {"heading": "5 Implementation", "text": "In the following two subsections, we discuss the details of implementation and report on an empirical assessment of the APPA framework."}, {"heading": "5.1 Practical concerns", "text": "Although an amortized runtime analysis cannot perform these costs in step with the iterative updates of the previous stage, since the exact problem is only calculated at a certain point, we briefly point out some of the fine-grained differences between algorithms that affect their implementation or empirical behavior. To bring together the terminology used for SVRG in Johnson and Zhang (2013), we refer to a \"stage\" as a single step of APPA, i.e. the time used to execute the internal minimization of fx (t), \u03bb or gx (t), \u03bb (as in (3) and (4)). Re-centering over the head of Dual APPA vs. SVRG At the end of each of their phases, SVRG pauses are used to calculate an exact gradient by a complete pass over the dataset (cost) time in which n gradients are calculated. Although an amortized runtime analysis of these costs cannot be performed in step, this operation cannot be performed in step with the iterative updates."}, {"heading": "5.2 Empirical analysis", "text": "We are experimenting with Dual APPA compared to SDCA, SVRG and SGD compared to the other categories of CIFAR. We are experimenting with Dual APPA compared to SDCA, SVRG and SGD in several binary classification schemes. Beyond general benchmarks, the experiments also show the benefits of the unusual \"bias-variance compromise,\" which is less severe than with typical \"2 shrinkage.\" Even if a certain amount of \"2 shrinkage\" is desirable, Dual APPA can place even higher weights on its \"2 term, enjoy improved speed and stability, and after a few phases we will achieve roughly the desired bias.Datasets In this section we show results for three binary classification tasks derived from MNIST, 7 CIFAR-10,8 and Protein: 9 in MNIST we classify the digits {1, 2, 4, 7} compared to the IAR categories."}, {"heading": "Acknowledgments", "text": "Part of this work took place during the residency of RF and AS at Microsoft Research, New England, and another part during the visit of AS at Simons Institute for the Theory of Computing, UC Berkeley. This work was partially supported by NSF grants 0843915 and 1111109, NSF Graduate Research Fellowship (grant no. 1122374)."}, {"heading": "A Technical lemmas", "text": "In this section, we provide several stand-alone partition clamps that we use throughout the work. First, we provide lem A.1 with some common inequalities in terms of smooth or strongly convex functions (then lem A.1 (default boundaries for smooth, strongly convex functions). Let f: Rk \u2192 R be a differentiable function that has its minimum value at xopt.If f: \"L-smooth\" is then for all x-smooth combinations of square functions, then for all x-smooth, x-smooth, x-smooth, x-smooth, x-smooth, x-smooth, x-smooth, x-smooth, x-smooth, x-smooth, x-smooth, x-smooth, x-smooth, x-smooth, x-smooth, x-smooth, x-smooth, x-smooth, x-smooth, x-, x-smooth."}, {"heading": "B Regularized ERM duality", "text": "In this section, we deduce the dual (4) problem of the proximal operator for the ERM problem (3) (Section B.1) and prove several limits for primary and dual errors (Section B.2). During this section, we assume that F is given by the ERM problem (1) and that we apply the notation and assumptions in Section 1.1.B.1 Dual derivationWe can rewrite the primary problem, minx fs (x), asmin x x-Rd, z-Rn (i = 1) + 2 x-Raps (2), x-Raps (2), for i = 1,."}], "references": [{"title": "The tradeoffs of large scale learning", "author": ["L. Bottou", "O. Bousquet"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Bottou and Bousquet.,? \\Q2008\\E", "shortCiteRegEx": "Bottou and Bousquet.", "year": 2008}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Boyd et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boyd et al\\.", "year": 2011}, {"title": "Uniform sampling for matrix approximation", "author": ["M.B. Cohen", "Y.T. Lee", "C. Musco", "R. Peng", "A. Sidford"], "venue": "In Innovations in Theoretical Computer Science (ITCS),", "citeRegEx": "Cohen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2015}, {"title": "Saga: A fast incremental gradient method with support for non-strongly convex composite objectives", "author": ["A. Defazio", "F. Bach", "S. Lacoste-Julien"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Defazio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Defazio et al\\.", "year": 2014}, {"title": "New proximal point algorithms for convex minimization", "author": ["O. Guler"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Guler.,? \\Q1992\\E", "shortCiteRegEx": "Guler.", "year": 1992}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["R. Johnson", "T. Zhang"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Johnson and Zhang.,? \\Q2013\\E", "shortCiteRegEx": "Johnson and Zhang.", "year": 2013}, {"title": "Efficient accelerated coordinate descent methods and faster algorithms for solving linear systems", "author": ["Y.T. Lee", "A. Sidford"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "Lee and Sidford.,? \\Q2013\\E", "shortCiteRegEx": "Lee and Sidford.", "year": 2013}, {"title": "Iterative row sampling", "author": ["M. Li", "G.L. Miller", "R. Peng"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "Li et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Li et al\\.", "year": 2013}, {"title": "A universal catalyst for first-order optimization", "author": ["H. Lin", "J. Mairal", "Z. Harchaoui"], "venue": null, "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "An accelerated proximal coordinate gradient method", "author": ["Q. Lin", "Z. Lu", "L. Xiao"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Stochastic gradient descent, weighted sampling, and the randomized kaczmarz algorithm", "author": ["D. Needell", "N. Srebro", "R. Ward"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Needell et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Needell et al\\.", "year": 2014}, {"title": "OSNAP: Faster numerical linear algebra algorithms via sparser subspace embeddings", "author": ["J. Nelson", "H.L. Nguyen"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "Nelson and Nguyen.,? \\Q2013\\E", "shortCiteRegEx": "Nelson and Nguyen.", "year": 2013}, {"title": "A method of solving a convex programming problem with convergence rate O(1/k2)", "author": ["Y. Nesterov"], "venue": "Soviet Mathematics Doklady,", "citeRegEx": "Nesterov.,? \\Q1983\\E", "shortCiteRegEx": "Nesterov.", "year": 1983}, {"title": "Introductory Lectures on Convex Optimization: A Basic Course", "author": ["Y. Nesterov"], "venue": null, "citeRegEx": "Nesterov.,? \\Q2004\\E", "shortCiteRegEx": "Nesterov.", "year": 2004}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Rahimi and Recht.,? \\Q2007\\E", "shortCiteRegEx": "Rahimi and Recht.", "year": 2007}, {"title": "Monotone operators and the proximal point algorithm", "author": ["R.T. Rockafellar"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "Rockafellar.,? \\Q1976\\E", "shortCiteRegEx": "Rockafellar.", "year": 1976}, {"title": "A stochastic gradient method with an exponential convergence rate for finite training sets", "author": ["N.L. Roux", "M. Schmidt", "F. Bach"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Roux et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Roux et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "sample the \u03c6i have been shown to outperform standard first-order methods under mild assumptions (Bottou and Bousquet, 2008; Johnson and Zhang, 2013; Xiao and Zhang, 2014; Defazio et al., 2014; Shalev-Shwartz and Zhang, 2014).", "startOffset": 96, "endOffset": 224}, {"referenceID": 5, "context": "sample the \u03c6i have been shown to outperform standard first-order methods under mild assumptions (Bottou and Bousquet, 2008; Johnson and Zhang, 2013; Xiao and Zhang, 2014; Defazio et al., 2014; Shalev-Shwartz and Zhang, 2014).", "startOffset": 96, "endOffset": 224}, {"referenceID": 3, "context": "sample the \u03c6i have been shown to outperform standard first-order methods under mild assumptions (Bottou and Bousquet, 2008; Johnson and Zhang, 2013; Xiao and Zhang, 2014; Defazio et al., 2014; Shalev-Shwartz and Zhang, 2014).", "startOffset": 96, "endOffset": 224}, {"referenceID": 5, "context": "Solves the ERM problem (1), under an assumption of strong convexity, with convergence that depends linearly on the problem\u2019s condition number (Johnson and Zhang, 2013; Defazio et al., 2014).", "startOffset": 142, "endOffset": 189}, {"referenceID": 3, "context": "Solves the ERM problem (1), under an assumption of strong convexity, with convergence that depends linearly on the problem\u2019s condition number (Johnson and Zhang, 2013; Defazio et al., 2014).", "startOffset": 142, "endOffset": 189}, {"referenceID": 15, "context": "The key to our reductions are approximate variants of the classical proximal point algorithm (PPA) (Rockafellar, 1976; Parikh and Boyd, 2014).", "startOffset": 99, "endOffset": 141}, {"referenceID": 15, "context": "Several of the algorithmic tools and analysis techniques in this paper are similar in principle to (and sometimes appear indirectly in) work scattered throughout the machine learning and optimization literature \u2013 from classical treatments of error-tolerant PPA (Rockafellar, 1976; Guler, 1992) to the effective proximal term used by Accelerated Proximal SDCA Shalev-Shwartz and Zhang (2014) in enabling its acceleration.", "startOffset": 261, "endOffset": 293}, {"referenceID": 4, "context": "Several of the algorithmic tools and analysis techniques in this paper are similar in principle to (and sometimes appear indirectly in) work scattered throughout the machine learning and optimization literature \u2013 from classical treatments of error-tolerant PPA (Rockafellar, 1976; Guler, 1992) to the effective proximal term used by Accelerated Proximal SDCA Shalev-Shwartz and Zhang (2014) in enabling its acceleration.", "startOffset": 261, "endOffset": 293}, {"referenceID": 4, "context": "Several of the algorithmic tools and analysis techniques in this paper are similar in principle to (and sometimes appear indirectly in) work scattered throughout the machine learning and optimization literature \u2013 from classical treatments of error-tolerant PPA (Rockafellar, 1976; Guler, 1992) to the effective proximal term used by Accelerated Proximal SDCA Shalev-Shwartz and Zhang (2014) in enabling its acceleration.", "startOffset": 281, "endOffset": 391}, {"referenceID": 4, "context": "GD is Nesterov\u2019s accelerated gradient decent (Nesterov, 1983, 2004), SVRG is the stochastic variance-reduced gradient of Johnson and Zhang (2013), SAG is the stochastic average gradient of Roux et al.", "startOffset": 121, "endOffset": 146}, {"referenceID": 4, "context": "GD is Nesterov\u2019s accelerated gradient decent (Nesterov, 1983, 2004), SVRG is the stochastic variance-reduced gradient of Johnson and Zhang (2013), SAG is the stochastic average gradient of Roux et al. (2012) and Defazio et al.", "startOffset": 121, "endOffset": 208}, {"referenceID": 3, "context": "(2012) and Defazio et al. (2014), SDCA is the stochastic dual coordinate ascent of Shalev-Shwartz and Zhang (2013), AP-SDCA is the Accelerated Proximal SDCA of Shalev-Shwartz and Zhang (2014) and APCG is the accelerated coordinate algorithm of Lin et al.", "startOffset": 11, "endOffset": 33}, {"referenceID": 3, "context": "(2012) and Defazio et al. (2014), SDCA is the stochastic dual coordinate ascent of Shalev-Shwartz and Zhang (2013), AP-SDCA is the Accelerated Proximal SDCA of Shalev-Shwartz and Zhang (2014) and APCG is the accelerated coordinate algorithm of Lin et al.", "startOffset": 11, "endOffset": 115}, {"referenceID": 3, "context": "(2012) and Defazio et al. (2014), SDCA is the stochastic dual coordinate ascent of Shalev-Shwartz and Zhang (2013), AP-SDCA is the Accelerated Proximal SDCA of Shalev-Shwartz and Zhang (2014) and APCG is the accelerated coordinate algorithm of Lin et al.", "startOffset": 11, "endOffset": 192}, {"referenceID": 3, "context": "(2012) and Defazio et al. (2014), SDCA is the stochastic dual coordinate ascent of Shalev-Shwartz and Zhang (2013), AP-SDCA is the Accelerated Proximal SDCA of Shalev-Shwartz and Zhang (2014) and APCG is the accelerated coordinate algorithm of Lin et al. (2014). The latter three algorithms are more restrictive in that they only solve the explicitly regularized problem F + \u03bbr, even if F is itself strongly convex (such algorithms run in time inversely proportional to \u03bb).", "startOffset": 11, "endOffset": 262}, {"referenceID": 10, "context": "The table also lists algorithms based on the randomized Kaczmarz method (Strohmer and Vershynin, 2009; Needell et al., 2014) and their accelerated variant (Lee and Sidford, 2013), as well as algorithms based on subspace embedding (OSNAP) or row sampling (Nelson and Nguyen, 2013; Li et al.", "startOffset": 72, "endOffset": 124}, {"referenceID": 6, "context": ", 2014) and their accelerated variant (Lee and Sidford, 2013), as well as algorithms based on subspace embedding (OSNAP) or row sampling (Nelson and Nguyen, 2013; Li et al.", "startOffset": 38, "endOffset": 61}, {"referenceID": 11, "context": ", 2014) and their accelerated variant (Lee and Sidford, 2013), as well as algorithms based on subspace embedding (OSNAP) or row sampling (Nelson and Nguyen, 2013; Li et al., 2013; Cohen et al., 2015).", "startOffset": 137, "endOffset": 199}, {"referenceID": 7, "context": ", 2014) and their accelerated variant (Lee and Sidford, 2013), as well as algorithms based on subspace embedding (OSNAP) or row sampling (Nelson and Nguyen, 2013; Li et al., 2013; Cohen et al., 2015).", "startOffset": 137, "endOffset": 199}, {"referenceID": 2, "context": ", 2014) and their accelerated variant (Lee and Sidford, 2013), as well as algorithms based on subspace embedding (OSNAP) or row sampling (Nelson and Nguyen, 2013; Li et al., 2013; Cohen et al., 2015).", "startOffset": 137, "endOffset": 199}, {"referenceID": 15, "context": "We remark that notions of error-tolerance in the typical proximal point algorithm \u2013 for both its plain and accelerated variants \u2013 have been defined and studied in prior work (Rockafellar, 1976; Guler, 1992).", "startOffset": 174, "endOffset": 206}, {"referenceID": 4, "context": "We remark that notions of error-tolerance in the typical proximal point algorithm \u2013 for both its plain and accelerated variants \u2013 have been defined and studied in prior work (Rockafellar, 1976; Guler, 1992).", "startOffset": 174, "endOffset": 206}, {"referenceID": 1, "context": "Additional related work There is an immense body of literature on proximal point methods and alternating direction method of multipliers (ADMM) that are relevant to the approach in this paper; see Boyd et al. (2011); Parikh and Boyd (2014) for modern surveys.", "startOffset": 197, "endOffset": 216}, {"referenceID": 1, "context": "Additional related work There is an immense body of literature on proximal point methods and alternating direction method of multipliers (ADMM) that are relevant to the approach in this paper; see Boyd et al. (2011); Parikh and Boyd (2014) for modern surveys.", "startOffset": 197, "endOffset": 240}, {"referenceID": 1, "context": "Additional related work There is an immense body of literature on proximal point methods and alternating direction method of multipliers (ADMM) that are relevant to the approach in this paper; see Boyd et al. (2011); Parikh and Boyd (2014) for modern surveys. We also note that the independent work of Lin et al. (2015) contains results similar to some of those in this paper.", "startOffset": 197, "endOffset": 320}, {"referenceID": 9, "context": "Instantiating this algorithm with an accelerated, regularized ERM solver \u2013 such as APCG (Lin et al., 2014) \u2013 as its inner minimizer yields the improved accelerated running time for the ERM problem (1).", "startOffset": 88, "endOffset": 106}, {"referenceID": 5, "context": "Instantiating this algorithm with SVRG (Johnson and Zhang, 2013) as its inner minimizer yields the improved accelerated running time for both the ERM problem (1) as well as the general ERM problem (2).", "startOffset": 39, "endOffset": 64}, {"referenceID": 9, "context": "Instantiating this algorithm with an accelerate, regularized ERM solver \u2013 such as APCG (Lin et al., 2014) \u2013 as its inner minimizer yields the improved accelerated running time for the ERM problem (1).", "startOffset": 87, "endOffset": 105}, {"referenceID": 5, "context": "SVRG (Johnson and Zhang, 2013) is a primal (c, \u03bb)oracle with runtime complexity TP = O(ndmin{\u03ba, \u03ba\u03bb} log c) for both the ERM problem (1) and the general ERM problem (2).", "startOffset": 5, "endOffset": 30}, {"referenceID": 9, "context": "Using APCG (Lin et al., 2014) we can obtain a primal (c, \u03bb)-oracle with runtime complexity TP = \u00d5(nd \u221a \u03ba\u03bb log c) for the ERM problem (1).", "startOffset": 11, "endOffset": 29}, {"referenceID": 5, "context": "1 with SVRG (Johnson and Zhang, 2013) as the primal oracle and taking \u03bb = 2\u03bc + LR2 yields the running time bound \u00d5(nd \u221a \u03ba log( 0/ )) for the general ERM problem (2).", "startOffset": 12, "endOffset": 37}, {"referenceID": 9, "context": "APCG (Lin et al., 2014) is a dual (c, \u03bb)-oracle with runtime complexity TD = \u00d5(nd \u221a \u03ba\u03bb log c).", "startOffset": 5, "endOffset": 23}, {"referenceID": 5, "context": "To match the terminology used for SVRG in Johnson and Zhang (2013), we refer to a \u201cstage\u201d as a single step of APPA, i.", "startOffset": 42, "endOffset": 67}, {"referenceID": 14, "context": "We then take take n/5 random Fourier features per the randomized scheme of Rahimi and Recht (2007). This yields 12K features for MNIST (60K training examples, 10K test) and 10K for CIFAR (50K training examples, 10K test).", "startOffset": 75, "endOffset": 99}, {"referenceID": 5, "context": "(See Johnson and Zhang (2013) for a comparison of SVRG to a more thoroughly tuned SGD under different decay schemes.", "startOffset": 5, "endOffset": 30}], "year": 2015, "abstractText": "We develop a family of accelerated stochastic algorithms that minimize sums of convex functions. Our algorithms improve upon the fastest running time for empirical risk minimization (ERM), and in particular linear least-squares regression, across a wide range of problem settings. To achieve this, we establish a framework based on the classical proximal point algorithm. Namely, we provide several algorithms that reduce the minimization of a strongly convex function to approximate minimizations of regularizations of the function. Using these results, we accelerate recent fast stochastic algorithms in a black-box fashion. Empirically, we demonstrate that the resulting algorithms exhibit notions of stability that are advantageous in practice. Both in theory and in practice, the provided algorithms reap the computational benefits of adding a large strongly convex regularization term, without incurring a corresponding bias to the original problem.", "creator": "LaTeX with hyperref package"}}}