{"id": "1506.07643", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jun-2015", "title": "Conservativeness of Untied Auto-Encoders", "abstract": "We discuss necessary and sufficient conditions for an auto-encoder to define a conservative vector field, in which case it is associated with an energy function akin to the unnormalized log-probability of the data. We show that the conditions for conservativeness are more general than for encoder and decoder weights to be the same (\"tied weights\"), and that they also depend on the form of the hidden unit activation function, but that contractive training criteria, such as denoising, will enforce these conditions locally. Based on these observations, we show how we can use auto-encoders to extract the conservative component of a vector field.", "histories": [["v1", "Thu, 25 Jun 2015 07:23:56 GMT  (4434kb,D)", "https://arxiv.org/abs/1506.07643v1", null], ["v2", "Tue, 30 Jun 2015 14:18:50 GMT  (4434kb,D)", "http://arxiv.org/abs/1506.07643v2", null], ["v3", "Mon, 21 Sep 2015 22:11:41 GMT  (4452kb,D)", "http://arxiv.org/abs/1506.07643v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["daniel jiwoong im", "mohamed ishmael diwan belghazi", "roland memisevic"], "accepted": true, "id": "1506.07643"}, "pdf": {"name": "1506.07643.pdf", "metadata": {"source": "CRF", "title": "Conservativeness of untied auto-encoders", "authors": ["Daniel Jiwoong Im", "Mohamed Ishmael Diwan Belghanzi", "Roland Memisevic"], "emails": ["imdaniel@iro.umontreal.ca", "mohamed.2.belghazi@hec.ca", "roland.memisevic@umontreal.ca"], "sections": [{"heading": "Introduction", "text": "An auto encoder is a feature learning model that learns to reconstruct its input by going through one or more capacity-limited \"bottleneck\" layers (Seung 1998).Since it defines a mapping r: Rn \u2192 Rn, an auto encoder can also be considered a dynamic system trained to have fixed points on the data (Seung 1998).The renewed interest in the perspective of dynamic systems led to a variety of results that help to clarify the role of auto encoders and their relationship to probability models. For example, (Vincent et al. 2008; Swersky et al. 2011) showed that the formation of an auto encoder to denoise corrupt input is closely related to the execution of score matching (Hyva \ufffd rinen 2005) in an undirected model together (Alain and Bengio 2014) showed that the formation of the model to denoise input or reconstruct it is identical."}, {"heading": "Background", "text": "We will focus on the auto-encoders of formr (x) = Rh (WTx + b) + c (1), where x-Rn is an observation, R and W are decoders and encoder weights, or b and c are biases, and h (\u00b7) is an elementary hidden activation function. An auto-encoder can be identified by its vector field, r (x) \u2212 x, which is the set of vectors indicating their reconstruction from observations. The vector field is called conservative if it can be written as the gradient of a scalar function F (x) designated as potential or energy function: r (x) \u2212 x = Q (x) The energy function corresponds to the unnormalized probability of data. In this case, we can integrate the vector field to find the energy function (Kamyshanska 2013)."}, {"heading": "Conservative auto-encoders", "text": "One of the central objectives of this paper is to understand the conditions under which an auto encoder should be conservative and thus have a well-defined energy function.In the following section, we derive and explain these conditions."}, {"heading": "Conditions for conservative auto-encoders", "text": "It is only a matter of time before an agreement is reached. (...) It is only a matter of time before an agreement is reached. (...) It is only a matter of time before an agreement is reached. (...) It is only a matter of time before an agreement is reached. (...) It is a matter of time before an agreement is reached. (...) It is a matter of time before an agreement is reached. (...) It is a matter of time before an agreement is reached. (...) It is a matter of time before an agreement is reached. (...) It is a matter of time before an agreement is reached. (...) It is a matter of time before an agreement is reached. (...) It is a matter of time before an agreement is reached. (...) It is a matter of time before an agreement is reached. (...) It is a matter of time before an agreement is reached. (...) It is a matter of time before an agreement is reached. (...) It is a matter of time before an agreement is reached."}, {"heading": "Understanding the symmetricity condition", "text": "A sufficient condition for the symmetry of the Jacobians is that R can be written: R = CWDh \u2032 E. (8) where C and E are symmetrical matrices and C oscillates with WDh \u2032 EDh \u2032 WT, as this ensures the symmetry of the partial derivatives: \u2202 r (x) \u2202 x = RDh \u2032 WT = CWDh \u2032 W T (9) = WDh \u2032 EDh \u2032 W TC = WDh \u2032 R T = (\u2202 r (x) \u2202 x) T. The case of bound weights (R = W) follows if C and E are the identity, because then it is (x) \u2202 x = RDh \u2032 W T = WDh \u2032 W T = WDh \u2032 W T. It is noteworthy that R = CWDh \u2032 E is other special cases of condition R = WDh \u2032, since the identity files (R = WDh \u2032 E) are also conservative and the addition (C)."}, {"heading": "Conservativeness of trained auto-encoders", "text": "In the following (Alain and Bengio 2014), we will first assume that the true distribution of data is known and that the auto encoder is trained. Then, we will analyze the conservativity of auto encoders around fixed points of the data diversity. Afterwards, we will empirically investigate and explain why trained auto encoders tend to become conservative, away from data diversity. Finally, we will use the results obtained to explain why the product of encoder and decoder weights is becoming increasingly symmetrical in response to training."}, {"heading": "Local Conservativeness", "text": "Let r (x) be an auto encoder that minimizes a contracted quadratic loss function averaged over the true data distribution p, L\u03c3 (x) = \u0432 Rd p (x) - x (x) - x (22) x (x) x (10) A point x (Rd) is a fixed point of the auto encoder, if and only if r (x) = x. Proposition 3. Let r (x) be an unbound single-layer auto encoder that minimizes equation 10. Then r (x) is locally conservative, as the contraction parameter tends to zero. An expansion of r (x) by a fixed point x (x +) results in a dynamic r (x) that xT + o () as \u2192 0. (11) (Alain and Bengio 2014) x indicates that the reconstruction r (x) \u2212 x explicitly becomes an estimator of the score if x (dynamics)."}, {"heading": "Empirical Conservativeness", "text": "We measure symmetricity with sym (A) = 3 (A + AT) / 2 (A) 2, which results in values between [0, 1] and 1 representing complete symmetricity. Figure 2a and 2c show the evolution of the symmetricity of \"X\" and \"W\" during training. For the unbound auto encoders, we observe that the Jacobians are becoming increasingly symmetrical, while Proposition 2, the auto encoder, is becoming increasingly conservative. Contractive auto encoders tend to be more symmetrical than the unbound auto encoders."}, {"heading": "Symmetricity of weights product", "text": "The product of the weight RWT tends to become increasingly symmetrical during training, which is more pronounced with sigmoid activations than with ReLUs, as shown in Figures 2b and 2d. This can be explained by taking into account the Jacobic symmetry. We have approximately H \u2211 l = 1 (RilWlj \u2212 RjlWli) h \u2032 l (x) = 0, \u0442 1 \u2264 i, j \u2264 d (15) This means that the activation of sigmoid hidden units, at least for training data points, is independent of h \u2032 (x) ora constants. As shown in the supplementary material, most hidden units concentrate in the highest curvature region during training with weight length limitations. This forces hl (x) to concentrate on high curvature regions of sigmoid activation. This may be due to the fact that either h \u2032 l (x) for all given x is nearly constant or the Wh (x) is close to the identity in both cases."}, {"heading": "Decomposing the Vector Field", "text": "In this section, we consider the search for the narrowest conservative vector field in the smallest quadratic sense as a response to a non-conservative vector field. Searching for this vector field is of great practical importance in many areas of science and technology (Bhatia et al. 2013). Here, we show that conservative auto-encoders can provide a powerful, profound perspective based on this problem. The basic theorem of vector computation, also known as Helmhotz decomposition, states that any vector field in R3 can be expressed as the orthogonal sum of an irrotational and a solenoidal field. Hodge decomposition is a generalization of this result into high-dimensional space (James 1966). A complete statement of the result requires careful analysis of the boundary conditions and differential formalism. However, since 1 forms correspond to the vector field and our interest in the latter lies, we need to give the quratic form + 1 as a special guarantee in the case of emergency adratic."}, {"heading": "Extracting the Conservative Vector Field through Learning", "text": "This year it is so far that it will be able to do the aforementioned for the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green"}, {"heading": "Discussion", "text": "In this paper, we have derived necessary and sufficient conditions for the conservation of autoencoders and investigated why the Jakobian of the autoencoder tends to become symmetrical during training. In addition, we have introduced a way to extract the conservative component of a vector field based on these characteristics of auto encoders. An interesting direction for future research is the use of annealed importance sample or similar sampling-based approaches for global normalization of energy function values obtained from unbound autoencoders. Another interesting direction is the use of parameterization during training that automatically meets the sufficient conditions for conservation but is less restrictive than the weight binding."}, {"heading": "Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Conservative auto-encoders", "text": "In this section, detailed derivatives of proposition 1 in section 3.Proposition 1 are presented as an autobound difference. Consider an m-hidden layer of auto-encoder defining asr (x; \u03b8) = W (m) h (m \u2212 1) h (\u00b7 \u00b7 W (1) h (1) (x) \u00b7 \u00b7) + c (m \u2212 1) + c (m), which is a smooth activation function at level k. Then, the auto-encoder is described as conservative over a smooth domain K RD if and only if its reconstruction is the jotted properties of the model, and h (k) is a smooth elemental activation function at level k."}], "references": [{"title": "and Bengio", "author": ["G. Alain"], "venue": "Y.", "citeRegEx": "Alain and Bengio 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "The helmtholz-hodge decomposition-a survey", "author": ["Bhatia"], "venue": "IEEE Transactions on visualization and computer graphics 19:1386\u20131404", "citeRegEx": "Bhatia,? \\Q2013\\E", "shortCiteRegEx": "Bhatia", "year": 2013}, {"title": "D", "author": ["Edelen"], "venue": "G.", "citeRegEx": "Edelen 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "and Castro", "author": ["I. Macedo"], "venue": "R.", "citeRegEx": "Macedo and Castro 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "On autoencoders and score matching for energy based models", "author": ["Swersky"], "venue": "In International Conference on Machine Learning (ICML)", "citeRegEx": "Swersky,? \\Q2011\\E", "shortCiteRegEx": "Swersky", "year": 2011}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Vincent"], "venue": "In Proceedings of the 25th International Conference on Machine Learning (ICML)", "citeRegEx": "Vincent,? \\Q2008\\E", "shortCiteRegEx": "Vincent", "year": 2008}], "referenceMentions": [], "year": 2015, "abstractText": "We discuss necessary and sufficient conditions for an auto-encoder to define a conservative vector field, in which case it is associated with an energy function akin to the unnormalized log-probability of the data. We show that the conditions for conservativeness are more general than for encoder and decoder weights to be the same (\u201ctied weights\u201d), and that they also depend on the form of the hidden unit activation function, but that contractive training criteria, such as denoising, will enforce these conditions locally. Based on these observations, we show how we can use auto-encoders to extract the conservative component of a vector field. Introduction An auto-encoder is a feature learning model that learns to reconstruct its inputs by going though one or more capacityconstrained \u201cbottleneck\u201d-layers. Since it defines a mapping r : R \u2192 R, an auto-encoder can also be viewed as dynamical system, that is trained to have fixed points at the data (Seung 1998). Recent renewed interest in the dynamical systems perspective led to a variety of results that help clarify the role of auto-encoders and their relationship to probabilistic models. For example, (Vincent et al. 2008; Swersky et al. 2011) showed that training an auto-encoder to denoise corrupted inputs is closely related to performing score matching (Hyv\u00e4rinen 2005) in an undirected model. Similarly, (Alain and Bengio 2014) showed that training the model to denoise inputs, or to reconstruct them under a suitable choice of regularization penalty, lets the autoencoder approximate the derivative of the empirical data density. And (Kamyshanska 2013) showed that, regardless of training criterion, any auto-encoder whose weights are tied (decoder-weights are identical to the encoder weights) can be written as the derivative of a scalar \u201cpotential-\u201d or energy-function, which in turn can be viewed as unnormalized data log-probability. For sigmoid hidden units the potential function is exactly identical to the free energy of an RBM, which shows that there is tight link between these two types of model. The same is not true for untied auto-encoders, for which it has not been clear whether such an energy function exists. It has also not been clear under which conditions an \u2217Authors constributed equally. energy function exists or does not exist, or even how to define it in the case where decoder-weights differ from encoder weights. In this paper, we describe necessary and sufficient conditions for the existence of an energy function and we show that suitable learning criteria will lead to an autoencoder that satisfies these conditions at least locally, near the training data. We verify our results experimentally. We also show how we can use an auto-encoder to extract the conservative part of a vector field. Background We will focus on auto-encoders of the form r(x) = Rh ( Wx + b ) + c (1) where x \u2208 R is an observation, R and W are decoder and encoder weights, respectively, b and c are biases, and h(\u00b7) is an elementwise hidden activation function. An auto-encoder can be identified with its vector field, r(x) \u2212 x, which is the set of vectors pointing from observations to their reconstructions. The vector field is called conservative if it can be written as the gradient of a scalar function F (x), called potential or energy function: r(x)\u2212 x = \u2207F (x) (2) The energy function corresponds to the unnormalized probability of data. In this case, we can integrate the vector field to find the energy function (Kamyshanska 2013). For an auto-encoder with tied weights and real-valued observations it takes the form F (x) = \u222b h(u)du\u2212 1 2 \u2016x\u2212 c\u20162 + const (3) where u = Wx + b is an auxiliary variable and h(\u00b7) can be any elementwise activation function with known antiderivative. For example, the energy function of an autoencoder with sigmoid activation function is identical to the (Gaussian) RBM free energy (Hinton 2010):", "creator": "LaTeX with hyperref package"}}}