{"id": "1506.07650", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jun-2015", "title": "Semantic Relation Classification via Convolutional Neural Networks with Simple Negative Sampling", "abstract": "Syntactic features play an essential role in identifying relationship in a sentence. Previous neural network models often suffer from irrelevant information introduced when subjects and objects are in a long distance. In this paper, we propose to learn more robust relation representations from the shortest dependency path through a convolution neural network. We further propose a straightforward negative sampling strategy to improve the assignment of subjects and objects. Experimental results show that our method outperforms the state-of-the-art methods on the SemEval-2010 Task 8 dataset.", "histories": [["v1", "Thu, 25 Jun 2015 07:51:55 GMT  (136kb,D)", "http://arxiv.org/abs/1506.07650v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["kun xu", "yansong feng", "songfang huang", "dongyan zhao"], "accepted": true, "id": "1506.07650"}, "pdf": {"name": "1506.07650.pdf", "metadata": {"source": "CRF", "title": "Semantic Relation Classification via Convolutional Neural Networks with Simple Negative Sampling", "authors": ["Kun Xu", "Yansong Feng", "Songfang Huang", "Dongyan Zhao"], "emails": ["zhaodongyan}@pku.edu.cn,", "huangsf@cn.ibm.com"], "sections": [{"heading": "1 Introduction", "text": "The relation between the individual countries (RE) can be defined as follows: a set of S with a pair of denominations e1 and e2, we aim to identify the relationship between e1 and e2. RE is typically studied in a classification style in which many characteristics have been proposed, e.g., Hendrickx et al. (2010) 16 types of characteristics are designed, including kernel-based methods, trait dependencies, etc., looking at syntactic characteristics to bring about significant improvements in extraction accuracy (Bunescu and Mooney, 2005a). Previous attempts to encrypt syntactic information, such as linking family tree-based characteristics (Qian et al), subsequences."}, {"heading": "2 The Shortest Path Hypothesis", "text": "If e1 and e2 are two nominals mentioned in the same sentence, we assume that the shortest path between e1 and e2 describes their relationship; this is because (1) if e1 and e2 are arguments of the same predicate, then their shortest path should pass through that predicate; (2) if e1 and e2 belong to different predicate argument structures, their shortest path will pass through a sequence of predicates, and all successive predicates will share a common argument. Note that the order of predicates on the path indicates the correct mapping of subjects and objects for that relationship. In Figure 1, for example, the dependency path is successively traversed and received, which together implies that subject and object each play a sender and receiver role in the Instrument Agency relationship."}, {"heading": "3 A Convolutional Neural Network Model", "text": "Our model successively takes the shortest dependency path (i.e. the words, dependency edges, and dependency markers) from the subject to the object as input, guides it through the lookup table, produces local attributes around each node on the dependency path, and combines these attributes into a global feature vector, which is then fed into a softmax classifier. Each dimension of the output vector indicates the trustworthiness of the corresponding ratio. In the lookup table step, each node (i.e. word, label, or arrow) in the dependency path is transformed into a vector by looking at the embedding matrix. Each dimension of the output vector indicates the value of the corresponding ratio."}, {"heading": "4 Negative Sampling", "text": "First, we present three pilot experiments on the development set. In the first, we assume that the assignment of subject and object to a relationship is not given (blind), but that we simply extract characteristics from e1 to e2 and test them in a blind setting. In the second experiment, we assume that the assignment is given (seen) during the training, but is still blind in the test phase. In the latter, it is assumed that the assignment takes place both during the training and during the test steps. 2We leave out detailed formulas for the limitation of space.The third experiment can be considered as an upper limit, where we do not have to worry about the first grade, that there may be more than one relationship between two nominals. 2We leave out detailed formulas for the limitation of space.By comparing the first and second assignments, we can see that when adding assignment information during the training, our dependence on based representation can be better used to understand the assignment during training."}, {"heading": "5 Experimental Evaluation", "text": "We evaluate our model on the basis of SemEval-2010 Task 8 (Hendrickx et al., 2010), which contains 10,717 commented examples, including 8,000 examples of training and 2,717 for the test. We randomly took 2,182 samples from the training data for validation. Considering a sentence, we first find the shortest dependency path that connects two marked nominations, resulting in two dependency paths that correspond to two opposing subject / object directions, and then make predictions for the two paths, or if both predictions are different. And, for the rest of the cases, we choose the non-other relationship with the highest confidence than the output, because ideally our model will issue the correct designation for the right subject / object direction and another label for the wrong direction. We evaluate our models using the official evaluation scripts."}, {"heading": "6 Conclusion", "text": "In this paper, we use a neural folding network to learn more robust and effective relationship representations from the shortest dependency pathways for relationship extraction. We also propose a simple negative sample method to correctly assign subjects and objects within a relationship. Experimental results show that our model is significantly superior to modern systems and that our treatment of dependency pathways can well capture the syntactic characteristics for relationship extraction."}], "references": [{"title": "A shortest path dependency kernel for relation extraction", "author": ["Bunescu", "Mooney2005a] Razvan C. Bunescu", "Raymond J. Mooney"], "venue": "HLT/EMNLP", "citeRegEx": "Bunescu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bunescu et al\\.", "year": 2005}, {"title": "Subsequence kernels for relation extraction", "author": ["Bunescu", "Mooney2005b] Razvan C. Bunescu", "Raymond J. Mooney"], "venue": "In Advances in Neural Information Processing Systems 18 [Neural Information Processing Systems,", "citeRegEx": "Bunescu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bunescu et al\\.", "year": 2005}, {"title": "Encoding relation requirements for relation extraction via joint inference", "author": ["Chen et al.2014] Liwei Chen", "Yansong Feng", "Songfang Huang", "Yong Qin", "Dongyan Zhao"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computa-", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel P. Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John C. Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Semeval-2010 task 8: Multi-way classification", "author": ["Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid \u00d3 S\u00e9aghdha", "Sebastian Pad\u00f3", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz"], "venue": null, "citeRegEx": "Hendrickx et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hendrickx et al\\.", "year": 2010}, {"title": "Exploiting constituent dependencies for tree kernelbased semantic relation extraction", "author": ["Qian et al.2008] Longhua Qian", "Guodong Zhou", "Fang Kong", "Qiaoming Zhu", "Peide Qian"], "venue": "COLING", "citeRegEx": "Qian et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Qian et al\\.", "year": 2008}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Brody Huval", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Lev-Arie Ratinov", "Yoshua Bengio"], "venue": "ACL", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Relation classification via convolutional deep neural network", "author": ["Zeng et al.2014] Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao"], "venue": "In Proceedings of COLING", "citeRegEx": "Zeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 6, "context": "Earlier attempts to encode syntactic information are mainly kernel-based methods, such as the convolution tree kernel (Qian et al., 2008), subsequence kernel (Bunescu and Mooney, 2005b), and dependency tree kernel (Bunescu and Mooney, 2005a).", "startOffset": 118, "endOffset": 137}, {"referenceID": 5, "context": ", Hendrickx et al. (2010) designed 16 types of features including POS, WordNet, FrameNet, dependency parse features, etc.", "startOffset": 2, "endOffset": 26}, {"referenceID": 9, "context": "With the recent success of neural networks in NLP, different neural network models are proposed to learn syntactic features from raw sequences of words or constituent parse trees(Zeng et al., 2014; Socher et al., 2012), which have been", "startOffset": 178, "endOffset": 218}, {"referenceID": 7, "context": "With the recent success of neural networks in NLP, different neural network models are proposed to learn syntactic features from raw sequences of words or constituent parse trees(Zeng et al., 2014; Socher et al., 2012), which have been", "startOffset": 178, "endOffset": 218}, {"referenceID": 3, "context": "Therefore, we perform a max pooling over Z to produce a global feature vector in order to capture the most useful local features produced by the convolutional layer (Collobert et al., 2011), which has a fixed size of n1, independent of the dependency path length.", "startOffset": 165, "endOffset": 189}, {"referenceID": 4, "context": "To minimize J(\u03b8), we apply stochastic gradient descent (SGD) with AdaGrad (Duchi et al., 2011) in our experiments2.", "startOffset": 74, "endOffset": 94}, {"referenceID": 5, "context": "8 (Hendrickx et al., 2010), which contains 10,717 annotated examples, including 8,000 instances for training and 2,717 for test.", "startOffset": 2, "endOffset": 26}, {"referenceID": 8, "context": "We initialized We with 50-dimensional word vectors trained by Turian et al. (2010). We tuned", "startOffset": 62, "endOffset": 83}, {"referenceID": 9, "context": "9 (Zeng et al., 2014) +WordNet,words around nominals 82.", "startOffset": 2, "endOffset": 21}, {"referenceID": 5, "context": "Results and Discussion Table 2 summarizes the performances of our model, depLCNN+NS(+), and state-of-the-art models, SVM(Hendrickx et al., 2010), RNN, MV-RNN(Socher et al.", "startOffset": 120, "endOffset": 144}, {"referenceID": 7, "context": ", 2010), RNN, MV-RNN(Socher et al., 2012),", "startOffset": 20, "endOffset": 41}, {"referenceID": 9, "context": "and CNN(Zeng et al., 2014).", "startOffset": 7, "endOffset": 26}, {"referenceID": 2, "context": "As a baseline, we randomly sampled 8,000 negative examples from the NYT dataset (Chen et al., 2014).", "startOffset": 80, "endOffset": 99}], "year": 2015, "abstractText": "Syntactic features play an essential role in identifying relationship in a sentence. Previous neural network models often suffer from irrelevant information introduced when subjects and objects are in a long distance. In this paper, we propose to learn more robust relation representations from the shortest dependency path through a convolution neural network. We further propose a straightforward negative sampling strategy to improve the assignment of subjects and objects. Experimental results show that our method outperforms the state-of-the-art methods on the SemEval-2010 Task 8 dataset.", "creator": "LaTeX with hyperref package"}}}