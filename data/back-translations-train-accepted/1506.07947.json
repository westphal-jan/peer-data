{"id": "1506.07947", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jun-2015", "title": "Collaboratively Learning Preferences from Ordinal Data", "abstract": "In applications such as recommendation systems and revenue management, it is important to predict preferences on items that have not been seen by a user or predict outcomes of comparisons among those that have never been compared. A popular discrete choice model of multinomial logit model captures the structure of the hidden preferences with a low-rank matrix. In order to predict the preferences, we want to learn the underlying model from noisy observations of the low-rank matrix, collected as revealed preferences in various forms of ordinal data. A natural approach to learn such a model is to solve a convex relaxation of nuclear norm minimization. We present the convex relaxation approach in two contexts of interest: collaborative ranking and bundled choice modeling. In both cases, we show that the convex relaxation is minimax optimal. We prove an upper bound on the resulting error with finite samples, and provide a matching information-theoretic lower bound.", "histories": [["v1", "Fri, 26 Jun 2015 03:06:27 GMT  (50kb,D)", "http://arxiv.org/abs/1506.07947v1", "38 pages 2 figures"]], "COMMENTS": "38 pages 2 figures", "reviews": [], "SUBJECTS": "cs.LG cs.IT math.IT stat.ML", "authors": ["sewoong oh", "kiran koshy thekumparampil", "jiaming xu"], "accepted": true, "id": "1506.07947"}, "pdf": {"name": "1506.07947.pdf", "metadata": {"source": "CRF", "title": "Collaboratively Learning Preferences from Ordinal Data", "authors": ["Sewoong Oh", "Kiran K. Thekumparampil", "Jiaming Xu"], "emails": ["swoh@illinois.edu", "thekump2@illinois.edu", "jxu18@illinois.edu"], "sections": [{"heading": "1 Introduction", "text": "In many cases, it is important to predict the preferences of people who have not been seen by a user or anticipate the results of comparisons that have never been compared with each other. In this case, it is the way in which the user puts the preferences of people first. In this paper, we consider the following two concrete scenarios: an online market that collects the preferences of each user, and a ranking of objects that are seen by the user. Such data can be used directly to compare some elements with each other, or online activities that are viewed on the site, or how the user evaluates the articles."}, {"heading": "2 Model and Algorithm", "text": "In this section, we present a discrete selection model for collaborative ranking and propose a follow-up algorithm to learn the model from ordinary data."}, {"heading": "2.1 MultiNomial Logit (MNL) model for comparative judgment", "text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "2.2 Nuclear norm minimization", "text": "Suppose that the number of those who are able to, are able to, are able to, are able to, are able to, are able to, are able to, are able to, are able to, are able to, are able to, are able to, are able to, are able to, are able to, are able to, are able to, are able to, are able to, are able to, are able to, are able to, are able to, are able to, are able to, are able to, are able to help oneself. Suppose that we are able, are able to, are able to, are able to, be able to, be able to, be able to, be able to, be able to, be able to, be able to, be able to, be able to be able, be able to be, be able to be, be able to be, be able to be, be able to live in the world, to live in the world."}, {"heading": "3.1 Performance guarantee", "text": "We provide an upper limit for the resulting error of our convex relaxation if a multiple set of items Si (1) presented to the user is drawn uniformly at random. (1) Furthermore, if an item is patterned more than once, i.e. if there is an item, \"1 = ji\" 2 for some items and \"1 = 2, then we assume that the user treats these two items as if they were two different items with the same MNL weights. (2) The resulting preference is therefore always over k items (with possibly multiple copies of the same item), and distributed according to (1)."}, {"heading": "3.2 Information-theoretic lower bound for low-rank matrices", "text": "For a polynomial-time algorithm of convex relaxation, we have set a limit for the achievable error in the previous section. Next, we compare this to the fundamental limit of this problem by setting a lower limit for the achievable error by any algorithm (efficient or not). A simple parameter narrative argument indicates that the number of samples must be scaled as a degree of freedom, i.e., kd1 \u00b2 r (d1 + d2), in order to estimate a d1 \u00d7 d2 dimensional matrix of rank r. We construct an appropriate packaging over the set of low-level matrices with limited entries in the d3 level entries defined as (7) and show that no algorithm can estimate the true matrix with high probability using the generalized fano inequality."}, {"heading": "4 Choice modeling for bundled purchase history", "text": "In this section, we will use the MNL model to examine another scenario of practical interest: the choice of alternatives from the bundled purchase history. (In this context, we assume that we have bundled purchasing history of users. (Strictly speaking, there are two categories of interest with d1 and d2 alternatives in each category.) For example, there are d1 toothpastes that we can select from the second category and choose d2 toothbrushes. (For the i-th user, a subset of alternatives from the first category is presented along with a subset of Ti [d2] of alternatives from the second category. We use k1 and k2 to name the number of alternatives presented to an individual user, i.e. k1 = Si and k2 = | Ti |, and we assume that the number of alternatives presented to each user is fixed to simplify the notations."}, {"heading": "4.1 Performance guarantee", "text": "We offer an upper limit for the error achieved by our convex relaxation model, if the multiple of the alternatives Si from the first category and Ti from the second category is uniformly marked according to the random principle. Exactly, for the given k1 and k2 we have Si = {j (i) 1,1,., j (i) 1, k1) and Ti = (i) 2,1,., j (i) 2, \"s and j (i) 2,\" s \"drawn independently of each other.,\" s, \"log dn d1 and d2 alternatives, each drawn at random. Similar to the previous section, this sampling with substitutes is necessary for the analysis.,\" s and j. \""}, {"heading": "5 Discussion", "text": "We want first-order methods that are efficient and provide verifiable guarantees, and the biggest challenge is to provide a good initialization for starting such non-convex approaches. (b) For simpler models, such as the PL model, a more general sample has been studied using a graph. We want analytical results for more general samples."}, {"heading": "Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A Proof of Theorem 1", "text": "We will first introduce some additional notations that will be used in the proof."}, {"heading": "A.1 Proof of Lemma A.1", "text": "The orthogonal projection of any matrix M (Rd1 \u00d7 d2) on room T is given by PT (M) = UrUTr M + MVrr Tr Tr Tr Tr Tr Tr (B).The orthogonal projection of any matrix M (Rd1 \u00d7 d2) on room T is given by PT (M) = UrUTr Tr Tr Tr (B).The orthogonal projection of any matrix M (M) is given by PT (M) = UrUTr Tr Tr Tr Tr (A).The orthogonal projection of any matrix M (M) is given by PT (M) = UrUTr Tr Tr Tr Tr (T) is given by Tr (T)."}, {"heading": "A.2 Proof of Lemma A.2", "text": "Define Xi = \u2212 k '= 1 (evi,' \u2212 pi ',') T = 1 (vis-\u00e0-vis), (vis-\u00e0-vis), (vis-vis), (vis-vis), (vis-\u00e0-vis), (vis-vis), (vis-vis), (vis-vis), (vis-vis), (vis-vis), (vis-vis), (vis-vis), (vis-\u00e0-vis), (vis-vis), (vis-vis), (vis-vis), (vis-vis), (vis-vis), (vis-vis), (vis), (vis), (vis), (vis), (vis), (vis), (vis), (vis), (vis), (vis), (vis), (vis), (vis), (vis), (vis), (vis), (vis), (vis), (vis), (vis), (vis), (vis), (vis), (vis), (vis), (vis), (vis), (vis), (vis), (vis), (vis), (vis), (vis), (vis), (vis), (vis), (vis), (vis), (vis), (vis), ((), (vis), ((), (), ((), (), ((), (), ((), (), (), (), ((), (), (), (), ((), (), (), (), (), (), (), (), (), (), (, (), (), (), (), (), (), (, (), (), (, (), (), (, (), (), (), (), (), (, (, (), (), (), (, (), (), (, (), (, (), (), (, (, (), (), (, (, (), (, (, (), (), (, (,"}, {"heading": "A.3 Proof of Lemma A.3", "text": "Recall that the Hessian matrix is a block diagonal matrix with the i-tenth block H (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i), (i) (i) (i) (i) (i), (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) ((i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) ("}, {"heading": "A.4 Proof of Lemma A.5", "text": "Remember that H (\u2206) = e \u2212 2\u03b12 k3 d1 d1 \u2211 i = 1 k \u2211 \",\" max, \"<\" \", ei,\" \"i,\" \"i,\" \"i,\" \"i,\" \"\", \"\" i, \"\" \",\" i, \"\" i, \"\" \",\" \"i,\" \"\", \"\" i, \"\" \"i,\" \"\" i, \"\" \"\", \"\" i, \"\" i, \"\" \",\" \"i,\" \"i,\" \"i,\" \"\" \"i,\" \"\" \"i,\" \"\" \"\", \"\" i, \"\" \"i,\" \"i,\" i, \"\" i, \"\" i, \"i,\" i, \"\" i, \"i,\" i, \"i,\" i, \"i,\" \"i,\" \"\" i. \""}, {"heading": "A.5 Proof of Lemma A.6", "text": "Remember that Yi, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \",\", \",\", \"\", \",\" \",\" \",\" \",\" \",\" \",\" \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\" \",\", \",\" \",\" \",\", \"\", \"\", \"\", \"\", \",\" \",\" \",\" \",\" \",\", \"\", \"\", \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\", \",\" \",\", \",\" \",\", \"\", \"\", \"\", \"\", \"\" \",\" \",\" \"\", \"\", \"\", \"\", \"\", \"\", \"\" \"\", \"\" \",\" \"\", \"\" \"\", \"\", \"\" \"\", \"\" \"\", \"\" \"\", \"\", \"\" \"\" \",\" \"\" \"\", \"\" \"\", \"\" \"\" \"\", \"\" \"\" \",\" \"\" \"\" \",\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \",\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\", \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \""}, {"heading": "B Proof of estimating approximate low-rank matrices in Corollary", "text": "3.2We follow the proof of a similar logical sequence in [24]. First, we set a threshold \u0432 > 0 and set r = max {d1, d2}; j = 1, d2}; min {d1, d2}; j = r + 1, d2); min {d1, d2}; j = 1, d2); min {d1, d2}; j = 1 (d2); q); q = 1 \u2212 qqqqqqqqqqqqqqqq. Also since rj = 1, d1 (d1) qqqq; since rj = 1, qqq (d1); since rq = 1, qqqq (d1); since rq = 1, qqq (d1); since q = 1, qq (d1); q); q / 2, q (d2)."}, {"heading": "C Proof of the information-theoretic lower bound in Theorem 2", "text": "The proof uses information theoretical methods that reduce the estimation problem to a multi-path hypothesis. To prove a lower limit in terms of the expected error, it is sufficient to prove the above assertion that we are following the standard recipe of constructing a packaging according to the random principle. Consider a family [1),.,.,. (M (3)} of the d1 \u00b7 d2 dimensional matrices contained in the d1 \u00b7 d2 dimensional matrices. (4),. (4). We will use M (4) to simplify the notation. We draw an index L (4), and we will get direct observations according to the random principle."}, {"heading": "C.1 Proof of Lemma C.1", "text": "We will show that the following procedure produces the desired family with a probability of at least half that proves its existence. (') Leave d = (d1 + d2), and assume d2 (') without any loss of generality. (') Leave d = (d1 + d2), and assume d2 (') without any loss of generality. (') For the choice of M \u00b2 (') T (') T (') 2 \u00b7 d2 \u2212 d2 \u2212 d2 \u2212 1d2 11T), (60), in which U \u00b2 Rd1 \u00b7 r is a random orthogonal basis, so that UTU = Ir \u00b7 r \u00b7 r and V (')."}, {"heading": "D Proof of Theorem 3", "text": "We use similar notations and techniques as proof for Theorem 1 in Appendix A. Starting from the definition of L (1) in Eq. (17), we have the following unit of measurement for the true parameter that we evaluate by the actual number, which is measured by the actual number: L (1), 1 (1), 1 (2), 1 (2), 1 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2, (2), 2, (2), (2), (2), 2, (2), (2, 2, (2), (2), (2, 2, 2, (2), 2, (2), (2, 2, 2, 2, \"(2), (2), (2), (2, 2, (2), (2), (2), (2, (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2,\" (2), \"(2),\" (2), \"(2,\"), \"(2,\" (2), \"(2), (2),\" (2), (2), (2), (2, (2), (2), (2), (2), (2), (2), (2), (2), (2, (2, (2), (2, (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2, (2), (2)."}, {"heading": "D.1 Proof of Lemma D.1", "text": "Define Xi = \u2212 (euieTvi \u2212 pi) such that \"L\" = (1 / n), \"n\" (1 / 1), \"n\" (1 / 2), \"n\" (1 / 2), \"n\" (1 / 2), \"n\" (1 / 2), \"n\" (2), \"n\" (2), \"n\" (2), \"n\" (2), \"n\" (2), \"n\" (2), \"n\" (2), \"n\" (2), \"c\" (2), \"c\" (2), \"c\" (2), \"c\" (2), \"c\" (2), c), c), c), c), c, c), c, c), c), c), c),), c), (), c),),), c)."}, {"heading": "D.2 Proof of Lemma D.2", "text": "The square form of Hessian, which is defined in (75), may be lower than that of Vec (79). (93) The square form of Hessian, which is defined in (75), may be lower than that of Vec (79). (93) The square form of Hessian, which is defined in (75), may be lower than that of Vec (79). (93) The square form of Hessian, which is defined in Vec (92). (93) The square form of Hessian, which is defined in (93), the square form of Vec (93), the square form of Vec (93), the square form of Vec (93), the square form of Vec (93), the square form of Vec (93), the square form of Vec (81), square (81), (81), square form of Hessian, which is defined in (81)."}, {"heading": "D.3 Proof of Lemma D.3", "text": "Note that Z a limited difference of (8\u03b12e) \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2."}, {"heading": "E Proof of the information-theoretic lower bound in Theorem 4", "text": "This evidence closely follows the evidence of Theorem 2 in Appendix C. We apply the generalized inequality in the same way to obtain Eq. (49) P '6 = L' 6 in this case is that we can no longer directly apply the RUM interpretation to compete with DKL (\"1) DKL (\" 2 \"). (1) This leads to an overestimation of the KL divergence, because this approach does not take into account that we can only take the highest winner from these k1k2 alternatives. Instead, we calculate the divergence directly and provide an appropriate limit. Let us choose the amount of k1 rows and k2 columns in one of the KL divergences because we have kj kj k2\" j \"j\" j \"and kj\" j \"j\" j \"j\" i. \""}, {"heading": "E.1 Proof of Lemma E.1", "text": "(...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (... (...) (...) (...) (...) (...) (...) (...) (...) (...) (... (...) (...) (... (...) (...) (...) (... (...) (...) (...) (... (...) (...) (...) (... (...) (...) (...) (...) (... (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (... (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (... (...) (...) (...) (...) (...) (... (...) (...) (...) (...) (...) () (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (..."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "<lb>In applications such as recommendation systems and revenue management, it is important to<lb>predict preferences on items that have not been seen by a user or predict outcomes of comparisons<lb>among those that have never been compared. A popular discrete choice model of multinomial<lb>logit model captures the structure of the hidden preferences with a low-rank matrix. In order to<lb>predict the preferences, we want to learn the underlying model from noisy observations of the<lb>low-rank matrix, collected as revealed preferences in various forms of ordinal data. A natural<lb>approach to learn such a model is to solve a convex relaxation of nuclear norm minimization.<lb>We present the convex relaxation approach in two contexts of interest: collaborative ranking<lb>and bundled choice modeling. In both cases, we show that the convex relaxation is minimax<lb>optimal. We prove an upper bound on the resulting error with finite samples, and provide a<lb>matching information-theoretic lower bound.", "creator": "LaTeX with hyperref package"}}}