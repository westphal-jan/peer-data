{"id": "1506.08941", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2015", "title": "Language Understanding for Text-based Games using Deep Reinforcement Learning", "abstract": "In this paper, we consider the task of learning control policies for text-based games. In these games, all interactions in the virtual world are through text and the underlying state is not observed. The resulting language barrier makes such environments challenging for automatic game players. We employ a deep reinforcement learning framework to jointly learn state representations and action policies using game rewards as feedback. This framework enables us to map text descriptions into vector representations that capture the semantics of the game states. We evaluate our approach on two game worlds, comparing against a baseline with a bag-of-words state representation. Our algorithm outperforms the baseline on quest completion by 54% on a newly created world and by 14% on a pre-existing fantasy game.", "histories": [["v1", "Tue, 30 Jun 2015 05:51:11 GMT  (786kb,D)", "http://arxiv.org/abs/1506.08941v1", "10 pages"], ["v2", "Fri, 11 Sep 2015 23:16:13 GMT  (790kb,D)", "http://arxiv.org/abs/1506.08941v2", "11 pages, Appearing at EMNLP, 2015"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["karthik narasimhan", "tejas d kulkarni", "regina barzilay"], "accepted": true, "id": "1506.08941"}, "pdf": {"name": "1506.08941.pdf", "metadata": {"source": "CRF", "title": "Language Understanding for Text-based Games Using Deep Reinforcement Learning", "authors": ["Karthik Narasimhan", "Tejas Kulkarni", "Regina Barzilay"], "emails": ["karthikn@csail.mit.edu", "tejask@mit.edu", "regina@csail.mit.edu"], "sections": [{"heading": "1 Introduction", "text": "In this paper, we focus on the control of text modules. Players read descriptions of the current state and react to it by taking action. As the underlying state is not directly observable, they must understand the text in order to act."}, {"heading": "2 Related Work", "text": "Unlike the strategies mentioned above, our model is a combination of text strategies and strategies learned directly from the software (Branavan et al., 2010) that navigate with instructions (Vogel and Jurafsky, 2010; Kollar et al., 2010; Artzi and Zettlemoyer, 2013; Matuszek et al., 2013) and computer games (Eisenstein et al., 2009; Branavan et al., 2011a).Games offer a rich domain for grounded language analysis. Prior work has adopted a perfect knowledge of the underlying state of the game in order to learn strategies. Gorniak and Roy (2005) have developed a game character that can be controlled by spoken instructions that can be adapted to the game situation. Grounding commands to actions is learned from a transcript that is commented manually with actions and state attributes. Eisenstein et al. (2009) learn game rules through a collection of game-related documents and traces compiled from the game."}, {"heading": "3 Background", "text": "We represent a game by the tupel < H, A, T, R, Q, Q, Q >, where Q is the set of all possible game states, A = {(a, o)} is the set of all commands (action-object pairs), T (h, a, o) is the stochastic transition function between states, and R (h, a, o) is the reward function. The game state H is hidden to the player, who only receives a different text description produced by a stochastic function. Specifically, the underlying state h in the game engine requires the tracking of attributes such as the location of the player, his health points, his time of day, etc. The function of the game (also part of the game framework) then converts this state into a textual description of the place where the player is standing or a message indicating low health. We do not assume that access to either H or to our agents during the testing and experiments is available."}, {"heading": "4 Learning Representations and Control Policies", "text": "In this section, we describe our DQN model and describe its use in learning good Q-value approximations for games with stochastic textual descriptions. We divide the DQN model into two parts. The first module is a representation generator that converts the textual description of the current state into a vector, which is then entered into the second module, which is an action artist. Figure 2 shows the general architecture of our model. Together, we learn the parameters of both the representation generator and the action scorpion by using the reward feedback.Representation Generator (R) The representation generator reads raw texts displayed to the agent and converts them into a vector representation vs. A bag-of-words. Representation is not sufficient to capture higher structures of sentences and paragraphs. The need for a better semantic representation of the text is obvious."}, {"heading": "5 Experimental Setup", "text": "This year, it will be able to fix and fix the mentioned bugs."}, {"heading": "6 Results", "text": "In fact, we are able to set out in search of new paths that will lead us into the future."}, {"heading": "7 Conclusions", "text": "The resulting language barrier makes such environments tedious for automatic game participants. To jointly learn government representations and action strategies based on game rewards as feedback, we use a profound reinforcement learning framework. This framework allows us to map text descriptions into vector representations that capture the semantics of game states. We evaluate our approach on two game worlds, comparing a baseline with a dead end of words as state representation. Our algorithm exceeds the baseline in quest completion by 54% on a newly created world and by 14% on an existing fantasy game. Future guidelines include mastering planning and strategy learning at the highest level to improve the performance of intelligent agents."}], "references": [{"title": "High-level reinforcement learning in strategy games", "author": ["Amato", "Shani2010] Christopher Amato", "Guy Shani"], "venue": "In Proceedings of the 9th International Conference on Autonomous Agents and Multi-", "citeRegEx": "Amato et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Amato et al\\.", "year": 2010}, {"title": "Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics, 1(1):49\u201362", "author": ["Artzi", "Zettlemoyer2013] Yoav Artzi", "Luke Zettlemoyer"], "venue": null, "citeRegEx": "Artzi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Artzi et al\\.", "year": 2013}, {"title": "Reading between the lines: Learning to map high-level instructions to commands", "author": ["Luke S Zettlemoyer", "Regina Barzilay"], "venue": "In Proceedings of the 48th Annual Meeting of the Association", "citeRegEx": "Branavan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Branavan et al\\.", "year": 2010}, {"title": "Learning to win by reading manuals in a monte-carlo framework", "author": ["David Silver", "Regina Barzilay"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Branavan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Branavan et al\\.", "year": 2011}, {"title": "Non-linear monte-carlo search in civilization", "author": ["David Silver", "Regina Barzilay"], "venue": "ii. AAAI Press/International Joint Conferences on Artificial Intelligence", "citeRegEx": "Branavan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Branavan et al\\.", "year": 2011}, {"title": "Mudding: Social phenomena in text-based virtual realities", "author": ["Pavel Curtis"], "venue": "High noon on the electronic frontier: Conceptual issues in cyberspace,", "citeRegEx": "Curtis.,? \\Q1992\\E", "shortCiteRegEx": "Curtis.", "year": 1992}, {"title": "Reading to learn: Constructing features from semantic abstracts", "author": ["James Clarke", "Dan Goldwasser", "Dan Roth"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Eisenstein et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Eisenstein et al\\.", "year": 2009}, {"title": "Speaking with your sidekick: Understanding situated speech in computer role playing games", "author": ["Gorniak", "Roy2005] Peter Gorniak", "Deb Roy"], "venue": "Proceedings of the First Artificial Intelligence and Inter-", "citeRegEx": "Gorniak et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Gorniak et al\\.", "year": 2005}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Toward understanding natural language directions", "author": ["Kollar et al.2010] Thomas Kollar", "Stefanie Tellex", "Deb Roy", "Nicholas Roy"], "venue": "In HumanRobot Interaction (HRI),", "citeRegEx": "Kollar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kollar et al\\.", "year": 2010}, {"title": "Learning to automatically solve algebra word problems", "author": ["Kushman et al.2014] Nate Kushman", "Yoav Artzi", "Luke Zettlemoyer", "Regina Barzilay"], "venue": "ACL", "citeRegEx": "Kushman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kushman et al\\.", "year": 2014}, {"title": "Learning to parse natural language commands to a robot control system", "author": ["Evan Herbst", "Luke Zettlemoyer", "Dieter Fox"], "venue": "In Experimental Robotics,", "citeRegEx": "Matuszek et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Matuszek et al\\.", "year": 2013}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis."], "venue": "Nature, 518(7540):529\u2013533, 02.", "citeRegEx": "Antonoglou et al\\.,? 2015", "shortCiteRegEx": "Antonoglou et al\\.", "year": 2015}, {"title": "Prioritized sweeping: Reinforcement learning with less data and less time", "author": ["Moore", "Atkeson1993] Andrew W Moore", "Christopher G Atkeson"], "venue": "Machine Learning,", "citeRegEx": "Moore et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Moore et al\\.", "year": 1993}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Reinforcement learning of local shape in the game of go", "author": ["Silver et al.2007] David Silver", "Richard S Sutton", "Martin M\u00fcller"], "venue": "In IJCAI,", "citeRegEx": "Silver et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2007}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Introduction to reinforcement learning", "author": ["Sutton", "Barto1998] Richard S Sutton", "Andrew G Barto"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Reinforcement learning in games", "author": ["Istv\u00e1n Szita"], "venue": "In Reinforcement Learning,", "citeRegEx": "Szita.,? \\Q2012\\E", "shortCiteRegEx": "Szita.", "year": 2012}, {"title": "Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075", "author": ["Tai et al.2015] Kai Sheng Tai", "Richard Socher", "Christopher D Manning"], "venue": null, "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tieleman", "Hinton2012] Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}, {"title": "Visualizing data using t-sne", "author": ["Van der Maaten", "Geoffrey Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Learning to follow navigational directions", "author": ["Vogel", "Jurafsky2010] Adam Vogel", "Dan Jurafsky"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Vogel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vogel et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 11, "context": "An alternative approach is to convert text descriptions to pre-specified representations using annotated training data, commonly used in language grounding tasks (Matuszek et al., 2013; Kushman et al., 2014).", "startOffset": 162, "endOffset": 207}, {"referenceID": 10, "context": "An alternative approach is to convert text descriptions to pre-specified representations using annotated training data, commonly used in language grounding tasks (Matuszek et al., 2013; Kushman et al., 2014).", "startOffset": 162, "endOffset": 207}, {"referenceID": 5, "context": "We evaluate our model using two Multi-User Dungeon (MUD) games (Curtis, 1992).", "startOffset": 63, "endOffset": 77}, {"referenceID": 2, "context": "Example applications include interpreting help documentation for software (Branavan et al., 2010), navigating with directions (Vogel and Jurafsky, 2010; Kollar et al.", "startOffset": 74, "endOffset": 97}, {"referenceID": 9, "context": ", 2010), navigating with directions (Vogel and Jurafsky, 2010; Kollar et al., 2010; Artzi and Zettlemoyer, 2013; Matuszek et al., 2013) and playing computer games (Eisenstein et al.", "startOffset": 36, "endOffset": 135}, {"referenceID": 11, "context": ", 2010), navigating with directions (Vogel and Jurafsky, 2010; Kollar et al., 2010; Artzi and Zettlemoyer, 2013; Matuszek et al., 2013) and playing computer games (Eisenstein et al.", "startOffset": 36, "endOffset": 135}, {"referenceID": 6, "context": ", 2013) and playing computer games (Eisenstein et al., 2009; Branavan et al., 2011a).", "startOffset": 35, "endOffset": 84}, {"referenceID": 6, "context": "Eisenstein et al. (2009) learn game rules by analyzing a collection of game-related documents and precompiled traces of the game.", "startOffset": 0, "endOffset": 25}, {"referenceID": 16, "context": "Q-Learning Reinforcement Learning is a commonly used framework for learning control policies in game environments (Silver et al., 2007; Amato and Shani, 2010; Branavan et al., 2011b; Szita, 2012).", "startOffset": 114, "endOffset": 195}, {"referenceID": 19, "context": "Q-Learning Reinforcement Learning is a commonly used framework for learning control policies in game environments (Silver et al., 2007; Amato and Shani, 2010; Branavan et al., 2011b; Szita, 2012).", "startOffset": 114, "endOffset": 195}, {"referenceID": 17, "context": "In recent work, LSTMs have been used successfully in NLP tasks such as machine translation (Sutskever et al., 2014) and sentiment analysis (Tai et al.", "startOffset": 91, "endOffset": 115}, {"referenceID": 20, "context": ", 2014) and sentiment analysis (Tai et al., 2015) to compose vector representations of sentences from word-level embeddings (Mikolov et al.", "startOffset": 31, "endOffset": 49}, {"referenceID": 12, "context": ", 2015) to compose vector representations of sentences from word-level embeddings (Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 82, "endOffset": 129}, {"referenceID": 15, "context": ", 2015) to compose vector representations of sentences from word-level embeddings (Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 82, "endOffset": 129}], "year": 2015, "abstractText": "In this paper, we consider the task of learning control policies for text-based games. In these games, all interactions in the virtual world are through text and the underlying state is not observed. The resulting language barrier makes such environments challenging for automatic game players. We employ a deep reinforcement learning framework to jointly learn state representations and action policies using game rewards as feedback. This framework enables us to map text descriptions into vector representations that capture the semantics of the game states. We evaluate our approach on two game worlds, comparing against a baseline with a bag-of-words state representation. Our algorithm outperforms the baseline on quest completion by 54% on a newly created world and by 14% on a pre-existing fantasy game.", "creator": "LaTeX with hyperref package"}}}