{"id": "1506.09039", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2015", "title": "Scalable Discrete Sampling as a Multi-Armed Bandit Problem", "abstract": "Drawing a sample from a discrete distribution is one of the building components for Monte Carlo methods. Like other sampling algorithms, discrete sampling also suffers from high computational burden in large-scale inference problems. We study the problem of sampling a discrete random variable with a high degree of dependency that is typical in large-scale Bayesian inference and graphical models, and propose an efficient approximate solution with a subsampling approach. We make a novel connection between the discrete sampling and Multi-Armed Bandits problems with a finite reward population and provide three algorithms with theoretical guarantees. Empirical evaluations show the robustness and efficiency of the approximate algorithms in both synthetic and real-world large-scale problems.", "histories": [["v1", "Tue, 30 Jun 2015 11:20:45 GMT  (300kb,D)", "https://arxiv.org/abs/1506.09039v1", null], ["v2", "Tue, 9 Feb 2016 14:21:05 GMT  (303kb,D)", "http://arxiv.org/abs/1506.09039v2", null], ["v3", "Wed, 27 Apr 2016 21:09:43 GMT  (320kb,D)", "http://arxiv.org/abs/1506.09039v3", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["yutian chen", "zoubin ghahramani"], "accepted": true, "id": "1506.09039"}, "pdf": {"name": "1506.09039.pdf", "metadata": {"source": "META", "title": "Scalable Discrete Sampling as a Multi-Armed Bandit Problem", "authors": ["Yutian Chen", "Zoubin Ghahramani"], "emails": ["YUTIAN.CHEN@ENG.CAM.AC.UK", "ZOUBIN@ENG.CAM.AC.UK"], "sections": [{"heading": "1. Introduction", "text": "It is a ubiquitous and often necessary component for distribution algorithms that relate to different dimensions of \"large scales.\" \"For example, distributed algorithms were used to pitch a model with a large number of discrete variables (Newman et al., 2009; Bratires et al., 2010; Wu et al., 2011), intelligent transition kernels were used for Markov Chain Monte Carlo (MCMC) algorithms to efficiently create a single variable at the 33 rd International Conference on Machine Learning, New York, NY."}, {"heading": "2. Approximate Discrete Sampling", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Discrete Sampling as an Optimization Problem", "text": "The usual procedure for sampling X from a discrete domain.X = {1, 2,., D} is to first normalize p (X) and calculate the CDF (X = x) = \u2211 x i = 1 p (X = i).Then draw a uniform random variable u \u00b2 uniform (0, 1] and find x that satisfies F (x \u2212 1) < u \u2264 F (x).This procedure requires the calculation of the sum of all non-normalized probabilities. For p \u00b2 in the form of equation 1, this is O (ND).An alternative approach is to first draw D i.d. samples from the standard rubber distribution1 \u03b5i \u0445 rubber (0, 1) and then solve the following optimization problem: x = argmax i \u00b2 X log p (i) + \u03b5i. (2) It is shown in Kuzmin & Warmuth (2005) that the distribution follows x (X) p."}, {"heading": "2.2. Approximate Discrete Sampling as a Multi-Armed Bandits Problem", "text": "In a Multi-Armed Bandit (MAB) problem, the i'th bandit is a one-arm slot machine that, when pulled, generates an i.i.d. reward li from a distribution associated with that arm with an unknown mean. The optimal arm identification problem for MABs (Bechhofer, 1958; Paulson, 1964) in the specified confidence situation is to find the arm with the highest mean reward with a confidence, using as few draws as possible. Assuming equation 1, the solution in equation 2 can be be1The Gumbel distribution is used to model the maximum extreme value distribution. If a random variable Z value Exp (1) is applied, then \u2212 log (Z) rubber (0, 1)."}, {"heading": "3. Algorithms for MABs with a Finite Population and Fixed Confidence", "text": "The main difference between our problem and regular MABs is that our rewards are generated from a limited population, while regular MABs accept rewards. Since the exact mean can be obtained by scanning all N values li, n for arm i without substitution, a good algorithm should draw no more than N times for each arm, regardless of the mean between arms. In this section, we present three algorithms whose sample complexity is limited by O (ND) in the worst case and can be very efficient if the mean is large."}, {"heading": "3.1. Notations", "text": "The iteration of an algorithm is indexed by t. We designate the entire index set with [N] = {1, 2,.., N}, the sampled set of reward indices up to the t'th iteration from arm i with N (t) i [N] and the corresponding number of sampled rewards with T (t) i. We define the estimated mean for the i'th arm with \u00b5 (t) i def = 1 | N (t) i | x (t) i | x n (t) i, n (n) N (t) i li, n (n) n, the natural variance (biased) with (\u03c3 (t) i) 2 def = 1 | N (t) i (li, n \u2212 \u00b5 (t) i (t) 2, the deviation of the mean gap between two arms with (t) i, j \u2212 n (t) n (n) if the value for the city (n) is not defined."}, {"heading": "3.2. Adapted lil\u2019UCB", "text": "The lil'UCB algorithm (Jamieson et al., 2014) therefore maintains an upper confidence limit (UCB) of \u00b5i, which is inspired by the law of iterated logarithm (LIL) for each arm. At each iteration, it pulls a single sample from the arm with the highest limit and updates it. The algorithm ends when one arm is sampled much more frequently than all the other arms. We refer to Figure 1 by Jamieson et al. (2014) for details. The temporal complexity for t-iterations is O (log (D) t). Jamieson et al. (2014) showed that lil'UCB has reached the optimal sample complexity when the arm is not."}, {"heading": "3.3. Racing Algorithm for a Finite Population", "text": "If rewards are scanned without substitution, the negative correlation between rewards would generally improve the convergence of \u00b5-i = evenly. Unfortunately, the limit in lil'UCB ignores the negative correlation when T (t) i < N itself with the adjustments. We introduce a new family of racing algorithms (Maron & Moore, 1994) that exploits the finite population setting, as in Alg. 1. Choosing the uncertainty-bound function G distinguishes specific algorithms and two examples are discussed in the following sections. Alg. 1 maintains a set of candidates setD initialized with all arms. Iteration t draws a common mini-batch of m (t) indices w / o substitutes for all surviving arms in D. Then the uncertainty-bound G is used to eliminate suboptimal arms with a given confidence. The algorithm stops when only one arm is left."}, {"heading": "3.3.1. RACING WITH SERFLING CONCENTRATION BOUNDS FOR G", "text": "Serfling (1974) investigated the concentration inequalities of the sample without replacement and obtained an improved algorithm 1 (racing algorithm with a finite reward population input Number of samples D, population size N, minichurge sizes {m (t)} t = 1, confidence level 1 \u2212 \u03b4, uncertainty-bound function G (\u03b4, T, \u03c3, C), sample range Ci (optional). t \u00b2 0, T \u00b2 0, D \u00b2 n (t), N \u00b2, confidence level 1 \u2212 \u03b4 t = 1, while | D | 1 do t \u00b2 t + 1 sample w / o replacement m (t) indicesM [N]\\ N \u2212 N, and Set N \u00b2 M, T \u00b2 T + m (t) Compute li, n, i \u00b2 D, n \u00b2 D, and update \u00b5 \u00b2 i and sp."}, {"heading": "3.3.2. RACING WITH A NORMAL ASSUMPTION FOR G", "text": "The concentration limits often provide a conservative strategy, assuming an arbitrary limited distribution of rewards. If the number of samples drawn is large, the central boundary theorem suggests that \u00b5 (t) follows an approximate Gaussian distribution. Korattikara et al. (2014) made such an estimate and reached a narrower limit. We first present a direct sequence of Prop. 2 in Appx. A of Korattikara et al. (2014).Korollary 4. Allow \u00b5 (t) unit, t = 1, the estimated mean using the non-substitute sample from each finite population with average \u00b5 and unit variance. The common normal random variables \u00b5 and covariance matrix with \u00b5 (t) unit follow a Gaussian random process (\u00b5)."}, {"heading": "1\u2212 \u03b4 Racing-Normal draws no more rewards than", "text": "T \u0445 (\u0445) = D N (N \u2212 1) \u0445 24B2Normal (\u03b4 / D \u2032) + 1 m (1) (10), where dnem def = m2dlog2 n / me \u0441 N \u2265 n, \u0394n \u2264 N. D \u2032 def = D using \u03c3-i and D \u2212 1 using \u03c3-x, i."}, {"heading": "3.4. Variance Reduction for Random Rewards with", "text": "Control variationsThe difficulty of MABs strongly depends on the ratio of the mean gap to the reward noise. To improve the signal-to-noise ratio, we use the control variant technique (Wilson, 1984) to reduce the reward variance. Let's consider a variable whose expectation En \u0445 [N] [hi, n] can be efficiently calculated; the residual reward li, n \u2212 hi, n + En [hi, n] has the same mean as li, n, and the variance decreases when hi, n \u2012 li, n. In the Baiyan inference experiment, where the factor fn (X = i) = p (yn \u2212 y = i) is required, we take a similar approach to Wang et al. (2013) and take the Taylor expansion of li, n by a reference point y asli, n li (y li) + gTi (yn \u2212 y) = p (yn \u2212 y = i)."}, {"heading": "4. Related Work", "text": "The Gumbel Max trick has been exploited for various problems in Kuzmin & Warmuth (2005); Papandreou & Yuille (2011); Maddison et al. (2014); the next work is Maddison et al. (2014), where this trick is extended to draw continuous random variables with a Gumbel process involving adaptive repulsion sampling.Our work is closely related to the problem of optimal arm identification for MABs with a firm confidence. To our knowledge, this is the first work to look at MABs with a finite population.The proposed algorithms tailored under this setting could go beyond the discrete sampling problem; the normal assumption in Sec. 3.2 is similar to the UCB standard in Auer et al. (2002), but the latter assumes a normal distribution for individual rewards and will work poorly if not hold.The Bounds in Sec. 3.3 are based on subampling algorithms in H."}, {"heading": "5. Experiments", "text": "Since this is the first paper to discuss efficient discrete sampling for problem (1), we compare the adapted lil'UCB, Racing-EBS, Racing-Normal only with the exact sampler. We report the result of Racing-Normal in real data experiments only because the speed gains of the other two are marginal."}, {"heading": "5.1. Synthetic Data", "text": "We construct a distribution with D = 10 by sampling N = 105 rewards of li, n for each state from one of the three distributions N (0, 1), Uniform [0, 1], LogNormal (0, 2). We normalize li, n to have a fixed distribution p (X) in Fig. 1 (a) and a reward variance \u03c32 that controls the difficulty. Normal distribution is the ideal setting for racing normal, and uniform distribution is desirable for adapted lil'UCB and Racing-EBS, since the reward limit is close to \u03c3. The logNormal distribution, whose ex. kurtosis \u2248 4000, is difficult for all due to the heavy tail. We use a closely bound C = max {li, n \u2212 li, n \u2032} for racing-EBS. We set the scale parameter of the adapted lil'UCB with C / 2 and other parameters with the heuristic setting in Jamieson et al (2014)."}, {"heading": "5.2. Bayesian ARCH Model Selection", "text": "The discrete sampler is integrated into the Markov chain as a component to sample the hierarchical model.Specifically, we consider a mixture of ARCHs for the yield of stock price series with student-t innovations, each component with a different order q: rt = \u03c3tzt, zt iid-Q: the prior distribution of a candidate model in the set Q. The random variables that suggest the discrete model selection include the discrete model selection q and the continuous parameters ter.We adopt the augmented MCMC algorithm in the set Q. The random variables we use on the discrete model selection q."}, {"heading": "5.3. Author Coreference", "text": "We then examine the performance in a large-scale graphical model follow-up problem. The author's reference problem for a database of scientific essays is to group the authors \"mentions into real people. Singh et al. (2012) addressed this problem with a conditional random field model using pair-wise factors. The common and conditional distributions vary by the respective number of authors. (y-x) Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp-Exp"}, {"heading": "6. Discussion", "text": "We look at the discrete scanning problem with a high degree of dependence and propose three approximate algorithms within the framework of MABs with theoretical warranties. The racing algorithm offers a unifying approach to various subsampling-based Monte Carlo algorithms and also improves the robustness of the original MH algorithm in Korattikara et al. (2014). This is also the first work to discuss MABs by defining a finite reward population. Empirical evaluations show that Racing-Normal achieves a robust and highest speed among all competitors. While adaptive UCB exhibits worse empirical performance compared to Racing-Normal, it exhibits better sample complexity than the number of arms D. It will be a future direction, the limit of Racing-Normal with other MAB algorithms including lil'UCB for better scalability in D. Another important problem is how to loosen up the normal assumptions for racing problems."}, {"heading": "Acknowledgements", "text": "We would like to thank Matt Hoffman for helpful discussions about the link between our work and the MAB problems and all reviewers for their constructive comments. We would like to thank the Alan Turing Institute, Google, Microsoft Research and EPSRC Grant EP / I036575 / 1."}, {"heading": "A. Proofs", "text": "For a discrete state, the total variation is equivalent to half of the L1 distance between two probability vectors. Denote by p (X = i | \u03b5) the distribution of the output of the approximate algorithm, conditioned by the vector of the Gumbel variables. We can call the L1 error of conditional probability as a function of Eq. 2 as a function of Eq. 2 as a function of Eq. (X = i) -Prop. (X = x) -Prop. (X = x) -Prop. (X = x). We can call the L1 error of conditional probability as a function of Eq. (X = i) -Prop. (X = i) -Prop. (X = x)."}, {"heading": "C. Experiment Detailed Setting and Extra Results", "text": "C.More Results of the Synthetic Data ExperimentThe results with the marginal variance estimance q q q q q q for racing are shown in Fig. 5. Racing algorithms (both EBS and Normal) perform more conservative comparisons to the diagrams when using pairs of variance estimates. In Fig. 1, but the relative performance of all algorithms is very similar to Fig. 1. We also provide the results with D = 2 and D = 100 when racing algorithms use pairs of variance estimates in Fig. 6 and 7 respectively in Fig. Racing Normal performs the best results in all situations and the empirical error never exceeds the bound significance provided by 0.05.Notice that the error of the adaptive lil'UCB exceeds the error tolerance in the experiment with D = 100 and li, n \u00b2 uniform [0, 1]. This is because we do not use the recommended heuristic setting of parameters in Jamieson, unfortunately significantly, in 2014."}], "references": [{"title": "Bayesian posterior sampling via stochastic gradient fisher scoring", "author": ["Ahn", "Sungjin", "Korattikara", "Anoop", "Welling", "Max"], "venue": "In Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "Ahn et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ahn et al\\.", "year": 2012}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Auer", "Peter", "Cesa-Bianchi", "Nicolo", "Fischer", "Paul"], "venue": "Machine learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Algorithms for scoring coreference chains", "author": ["Bagga", "Amit", "Baldwin", "Breck"], "venue": "In LREC workshop on linguistics coreference,", "citeRegEx": "Bagga et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Bagga et al\\.", "year": 1998}, {"title": "Concentration inequalities for sampling without replacement", "author": ["Bardenet", "R\u00e9mi", "Maillard", "Odalric-Ambrym"], "venue": "arXiv preprint arXiv:1309.4029,", "citeRegEx": "Bardenet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bardenet et al\\.", "year": 2013}, {"title": "Towards scaling up Markov chain Monte Carlo: an adaptive subsampling approach", "author": ["Bardenet", "R\u00e9mi", "Doucet", "Arnaud", "Holmes", "Chris"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Bardenet et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bardenet et al\\.", "year": 2014}, {"title": "On markov chain monte carlo methods for tall data", "author": ["Bardenet", "R\u00e9mi", "Doucet", "Arnaud", "Holmes", "Chris"], "venue": "arXiv preprint arXiv:1505.02827,", "citeRegEx": "Bardenet et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bardenet et al\\.", "year": 2015}, {"title": "A sequential multiple-decision procedure for selecting the best one of several normal populations with a common unknown variance, and its use with various experimental designs", "author": ["Bechhofer", "Robert E"], "venue": null, "citeRegEx": "Bechhofer and E.,? \\Q1958\\E", "shortCiteRegEx": "Bechhofer and E.", "year": 1958}, {"title": "The tradeoffs of large scale learning", "author": ["L. Bottou", "O. Bousquet"], "venue": "In NIPS,", "citeRegEx": "Bottou and Bousquet,? \\Q2008\\E", "shortCiteRegEx": "Bottou and Bousquet", "year": 2008}, {"title": "Scaling the iHMM: Parallelization versus hadoop", "author": ["S. Bratires", "J. van Gael", "A. Vlachos", "Z. Ghahramani"], "venue": "In Computer and Information Technology (CIT),", "citeRegEx": "Bratires et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bratires et al\\.", "year": 2010}, {"title": "Bayesian model choice via Markov chain Monte Carlo", "author": ["B.P. Carlin", "S. Chib"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "Carlin and Chib,? \\Q1995\\E", "shortCiteRegEx": "Carlin and Chib", "year": 1995}, {"title": "Stochastic gradient hamiltonian monte carlo", "author": ["Chen", "Tianqi", "Fox", "Emily", "Guestrin", "Carlos"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Bayesian sampling using stochastic gradient thermostats", "author": ["Ding", "Nan", "Fang", "Youhan", "Babbush", "Ryan", "Chen", "Changyou", "Skeel", "Robert D", "Neven", "Hartmut"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Ding et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2014}, {"title": "Approximate slice sampling for Bayesian posterior inference", "author": ["DuBois", "Christopher", "Korattikara", "Anoop", "Welling", "Max", "Smyth", "Padhraic"], "venue": "In Proceedings of AISTATS,", "citeRegEx": "DuBois et al\\.,? \\Q2014\\E", "shortCiteRegEx": "DuBois et al\\.", "year": 2014}, {"title": "Incorporating non-local information into information extraction systems by gibbs sampling", "author": ["Finkel", "Jenny Rose", "Grenager", "Trond", "Manning", "Christopher"], "venue": "In Proceedings of the 43rd ACL,", "citeRegEx": "Finkel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Finkel et al\\.", "year": 2005}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["Hoeffding", "Wassily"], "venue": "Journal of the American statistical association,", "citeRegEx": "Hoeffding and Wassily.,? \\Q1963\\E", "shortCiteRegEx": "Hoeffding and Wassily.", "year": 1963}, {"title": "lil\u2019UCB: An optimal exploration algorithm for multi-armed bandits", "author": ["Jamieson", "Kevin", "Malloy", "Matthew", "Nowak", "Robert", "Bubeck", "S\u00e9bastien"], "venue": "In Proceedings of The 27th COLT, pp", "citeRegEx": "Jamieson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jamieson et al\\.", "year": 2014}, {"title": "Slice sampling mixture models", "author": ["Kalli", "Maria", "Griffin", "Jim E", "Walker", "Stephen G"], "venue": "Statistics and computing,", "citeRegEx": "Kalli et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kalli et al\\.", "year": 2011}, {"title": "Austerity in MCMC land: Cutting the Metropolis-Hastings budget", "author": ["Korattikara", "Anoop", "Chen", "Yutian", "Welling", "Max"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "Korattikara et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Korattikara et al\\.", "year": 2014}, {"title": "Optimum follow the leader algorithm", "author": ["Kuzmin", "Dima", "Warmuth", "Manfred K"], "venue": "In Proceedings of the 18th annual conference on Learning Theory,", "citeRegEx": "Kuzmin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Kuzmin et al\\.", "year": 2005}, {"title": "Reducing the sampling complexity of topic models", "author": ["Li", "Aaron", "Ahmed", "Amr", "Ravi", "Sujith", "Smola", "Alex"], "venue": "In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Firefly monte carlo: Exact mcmc with subsets of data", "author": ["Maclaurin", "Dougal", "Adams", "Ryan Prescott"], "venue": "In Twenty-Fourth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Maclaurin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Maclaurin et al\\.", "year": 2015}, {"title": "Hoeffding races: Accelerating model selection search for classification and function approximation", "author": ["Maron", "Oded", "Moore", "Andrew W"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Maron et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Maron et al\\.", "year": 1994}, {"title": "Swiftlink: parallel mcmc linkage analysis using multicore cpu and gpu", "author": ["Medlar", "Alan", "G\u0142owacka", "Dorota", "Stanescu", "Horia", "Bryson", "Kevin", "Kleta", "Robert"], "venue": null, "citeRegEx": "Medlar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Medlar et al\\.", "year": 2013}, {"title": "Sensitivity and convergence of uniformly ergodic markov chains", "author": ["Mitrophanov", "A Yu"], "venue": "Journal of Applied Probability,", "citeRegEx": "Mitrophanov and Yu.,? \\Q2005\\E", "shortCiteRegEx": "Mitrophanov and Yu.", "year": 2005}, {"title": "Distributed algorithms for topic models", "author": ["Newman", "David", "Asuncion", "Arthur", "Smyth", "Padhraic", "Welling", "Max"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Newman et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Newman et al\\.", "year": 2009}, {"title": "Perturb-and-MAP random fields: Using discrete optimization to learn and sample from energy models", "author": ["G. Papandreou", "A. Yuille"], "venue": "In Proceedings of ICCV,", "citeRegEx": "Papandreou and Yuille,? \\Q2011\\E", "shortCiteRegEx": "Papandreou and Yuille", "year": 2011}, {"title": "A sequential procedure for selecting the population with the largest mean from k normal populations", "author": ["Paulson", "Edward"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Paulson and Edward.,? \\Q1964\\E", "shortCiteRegEx": "Paulson and Edward.", "year": 1964}, {"title": "Ergodicity of approximate MCMC chains with applications to large data sets", "author": ["Pillai", "Natesh S", "Smith", "Aaron"], "venue": "arXiv preprint arXiv:1405.0182,", "citeRegEx": "Pillai et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pillai et al\\.", "year": 2014}, {"title": "Bayes and big data: The consensus monte carlo algorithm", "author": ["Scott", "Steven L", "Blocker", "Alexander W", "Bonassi", "Fernando V", "H Chipman", "E George", "R. McCulloch"], "venue": "EFaBBayes 250 conference,", "citeRegEx": "Scott et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Scott et al\\.", "year": 2013}, {"title": "Probability inequalities for the sum in sampling without replacement", "author": ["R.J. Serfling"], "venue": "Ann. Statist., 2(1):39\u201348,", "citeRegEx": "Serfling,? \\Q1974\\E", "shortCiteRegEx": "Serfling", "year": 1974}, {"title": "Monte Carlo MCMC: efficient inference by approximate sampling", "author": ["Singh", "Sameer", "Wick", "Michael", "McCallum", "Andrew"], "venue": "In Proceedings of EMNLP-CoNLL", "citeRegEx": "Singh et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2012}, {"title": "Variance reduction for stochastic gradient optimization", "author": ["Wang", "Chong", "Chen", "Xi", "Smola", "Alex J", "Xing", "Eric P"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Bayesian learning via stochastic gradient Langevin dynamics", "author": ["Welling", "Max", "Teh", "Yee W"], "venue": "In Proceedings of ICML", "citeRegEx": "Welling et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Welling et al\\.", "year": 2011}, {"title": "Variance reduction techniques for digital simulation", "author": ["Wilson", "James R"], "venue": "American Journal of Mathematical and Management Sciences,", "citeRegEx": "Wilson and R.,? \\Q1984\\E", "shortCiteRegEx": "Wilson and R.", "year": 1984}, {"title": "Efficient multicore collaborative filtering", "author": ["Wu", "Yao", "Yan", "Qiang", "Bickson", "Danny", "Low", "Yucheng", "Yang", "Qing"], "venue": "In ACM KDD CUP workshop,", "citeRegEx": "Wu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2011}, {"title": "Distributed context-aware bayesian posterior sampling via expectation propagation", "author": ["M. Xu", "Y.W. Teh", "J. Zhu", "B. Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Xu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "i.i.d. assumption for rewards from each arm was used only in Lemma 3 to provide Chernoff\u2019s bound and Hoeffding\u2019s bound", "author": ["Jamieson"], "venue": "As noted in Sec. 6 of Hoeffding", "citeRegEx": "Jamieson,? \\Q1963\\E", "shortCiteRegEx": "Jamieson", "year": 1963}, {"title": "\u2200i, Theorem 2 in Jamieson et al. (2014) applies to adapted lil\u2019UCB with modification 1 and 2 only", "author": ["N i"], "venue": null, "citeRegEx": "\u2264,? \\Q2014\\E", "shortCiteRegEx": "\u2264", "year": 2014}], "referenceMentions": [{"referenceID": 24, "context": "For example, distributed algorithms have been used to sample a model with a large number of discrete variables (Newman et al., 2009; Bratires et al., 2010; Wu et al., 2011), smart transition kernels were described for Markov chain Monte Carlo (MCMC) algorithms to sample efficiently a single variable", "startOffset": 111, "endOffset": 172}, {"referenceID": 8, "context": "For example, distributed algorithms have been used to sample a model with a large number of discrete variables (Newman et al., 2009; Bratires et al., 2010; Wu et al., 2011), smart transition kernels were described for Markov chain Monte Carlo (MCMC) algorithms to sample efficiently a single variable", "startOffset": 111, "endOffset": 172}, {"referenceID": 34, "context": "For example, distributed algorithms have been used to sample a model with a large number of discrete variables (Newman et al., 2009; Bratires et al., 2010; Wu et al., 2011), smart transition kernels were described for Markov chain Monte Carlo (MCMC) algorithms to sample efficiently a single variable", "startOffset": 111, "endOffset": 172}, {"referenceID": 19, "context": "with a large or even infinite state space (Li et al., 2014; Kalli et al., 2011).", "startOffset": 42, "endOffset": 79}, {"referenceID": 16, "context": "with a large or even infinite state space (Li et al., 2014; Kalli et al., 2011).", "startOffset": 42, "endOffset": 79}, {"referenceID": 28, "context": "A common approach to address the big data problem is divide-and-conquer that uses parallel or distributed computing resources to process data in parallel and then synchronize the results periodically or merely once in the end (Scott et al., 2013; Medlar et al., 2013; Xu et al., 2014).", "startOffset": 226, "endOffset": 284}, {"referenceID": 22, "context": "A common approach to address the big data problem is divide-and-conquer that uses parallel or distributed computing resources to process data in parallel and then synchronize the results periodically or merely once in the end (Scott et al., 2013; Medlar et al., 2013; Xu et al., 2014).", "startOffset": 226, "endOffset": 284}, {"referenceID": 35, "context": "A common approach to address the big data problem is divide-and-conquer that uses parallel or distributed computing resources to process data in parallel and then synchronize the results periodically or merely once in the end (Scott et al., 2013; Medlar et al., 2013; Xu et al., 2014).", "startOffset": 226, "endOffset": 284}, {"referenceID": 0, "context": "The stochastic gradient Langevin dynamics (SGLD) (Welling & Teh, 2011) and its extensions (Ahn et al., 2012; Chen et al., 2014; Ding et al., 2014) introduced efficient proposal distributions based on subsampled data.", "startOffset": 90, "endOffset": 146}, {"referenceID": 10, "context": "The stochastic gradient Langevin dynamics (SGLD) (Welling & Teh, 2011) and its extensions (Ahn et al., 2012; Chen et al., 2014; Ding et al., 2014) introduced efficient proposal distributions based on subsampled data.", "startOffset": 90, "endOffset": 146}, {"referenceID": 11, "context": "The stochastic gradient Langevin dynamics (SGLD) (Welling & Teh, 2011) and its extensions (Ahn et al., 2012; Chen et al., 2014; Ding et al., 2014) introduced efficient proposal distributions based on subsampled data.", "startOffset": 90, "endOffset": 146}, {"referenceID": 17, "context": "Another line of research on approximate subsampled MH algorithms does not ignore the rejection step but controls the error with an approximate rejection step based on a subset of data (Korattikara et al., 2014; Bardenet et al., 2014).", "startOffset": 184, "endOffset": 233}, {"referenceID": 4, "context": "Another line of research on approximate subsampled MH algorithms does not ignore the rejection step but controls the error with an approximate rejection step based on a subset of data (Korattikara et al., 2014; Bardenet et al., 2014).", "startOffset": 184, "endOffset": 233}, {"referenceID": 12, "context": "That idea has also been extended to slice sampling (DuBois et al., 2014) and Gibbs for binary variables (Korattikara et al.", "startOffset": 51, "endOffset": 72}, {"referenceID": 17, "context": ", 2014) and Gibbs for binary variables (Korattikara et al., 2014).", "startOffset": 39, "endOffset": 65}, {"referenceID": 17, "context": "3 provides a unified framework for subsampling-based discrete sampling, MH (Korattikara et al., 2014; Bardenet et al., 2014) and slice sampling (DuBois et al.", "startOffset": 75, "endOffset": 124}, {"referenceID": 4, "context": "3 provides a unified framework for subsampling-based discrete sampling, MH (Korattikara et al., 2014; Bardenet et al., 2014) and slice sampling (DuBois et al.", "startOffset": 75, "endOffset": 124}, {"referenceID": 12, "context": ", 2014) and slice sampling (DuBois et al., 2014) algorithms as discussed in Sec.", "startOffset": 27, "endOffset": 48}, {"referenceID": 15, "context": "The lil\u2019UCB algorithm (Jamieson et al., 2014) maintains an upper confidence bound (UCB) of \u03bci that is inspired by the law of the iterated logarithm (LIL) for every arm.", "startOffset": 22, "endOffset": 45}, {"referenceID": 15, "context": "The lil\u2019UCB algorithm (Jamieson et al., 2014) maintains an upper confidence bound (UCB) of \u03bci that is inspired by the law of the iterated logarithm (LIL) for every arm. At each iteration, it draws a single sample from the arm with the highest bound and updates it. The algorithm terminates when some arm is sampled much more often than all the other arms. We refer readers to Fig. 1 of Jamieson et al. (2014) for details.", "startOffset": 23, "endOffset": 409}, {"referenceID": 15, "context": "The lil\u2019UCB algorithm (Jamieson et al., 2014) maintains an upper confidence bound (UCB) of \u03bci that is inspired by the law of the iterated logarithm (LIL) for every arm. At each iteration, it draws a single sample from the arm with the highest bound and updates it. The algorithm terminates when some arm is sampled much more often than all the other arms. We refer readers to Fig. 1 of Jamieson et al. (2014) for details. The time complexity for t iterations is O(log(D)t). It was shown in Jamieson et al. (2014) that lil\u2019UCB achieved the optimal sample complexity up to constants.", "startOffset": 23, "endOffset": 513}, {"referenceID": 15, "context": "2 of Jamieson et al. (2014) with additional properties as shown in the following proposition with proof in Appx.", "startOffset": 5, "endOffset": 28}, {"referenceID": 15, "context": "Theorem 2 of Jamieson et al. (2014) holds for the adapted lil\u2019UCB algorithm.", "startOffset": 13, "endOffset": 36}, {"referenceID": 15, "context": "Theorem 2 of Jamieson et al. (2014) holds for the adapted lil\u2019UCB algorithm. Moreover T (t) i \u2264 N, \u2200i, t. Therefore, when the algorithm terminates, t = \u2211 i\u2208X T (t) i \u2264 ND. Notice that Thm. 2 of Jamieson et al. (2014) shows that t scales roughly asO(1/\u2206) with \u2206 being the mean gap and therefore t ND when the gap is large.", "startOffset": 13, "endOffset": 217}, {"referenceID": 4, "context": "Bardenet & Maillard (2013) extended the work and provided an empirical Bernstein-Serfling bound that was later used for the subsampling-based MH algorithm (Bardenet et al., 2014): for any \u03b4 \u2208 (0, 1] and any n \u2264 N , with probability 1\u2212 \u03b4, it holds that", "startOffset": 155, "endOffset": 178}, {"referenceID": 17, "context": "Korattikara et al. (2014) made such", "startOffset": 0, "endOffset": 26}, {"referenceID": 17, "context": "A of Korattikara et al. (2014). Corollary 4.", "startOffset": 5, "endOffset": 31}, {"referenceID": 31, "context": "In the Bayesian inference experiment where the factor fn(X = i) = p(yn|X = i), we adopt a similar approach as Wang et al. (2013) and take the Taylor expansion of li,n around a reference point \u0177 as", "startOffset": 110, "endOffset": 129}, {"referenceID": 3, "context": "For algorithms depending on a reward bound C in order to get a tight bound for li,n \u2212 hi,n it requires a more restrictive condition for C as in Bardenet et al. (2015) and we might end up with an even more conservative strategy in general cases.", "startOffset": 144, "endOffset": 167}, {"referenceID": 1, "context": "2 is similar to UCB-Normal in Auer et al. (2002) but the latter assumes a normal distribution for individual rewards and will perform poorly when it does not hold.", "startOffset": 30, "endOffset": 49}, {"referenceID": 3, "context": "3 are based on subsampling-based MH algorithms in Bardenet et al. (2014); Korattikara et al.", "startOffset": 50, "endOffset": 73}, {"referenceID": 3, "context": "3 are based on subsampling-based MH algorithms in Bardenet et al. (2014); Korattikara et al. (2014). The proposed algorithm extends those ideas from MH to discrete sampling.", "startOffset": 50, "endOffset": 100}, {"referenceID": 3, "context": "3 are based on subsampling-based MH algorithms in Bardenet et al. (2014); Korattikara et al. (2014). The proposed algorithm extends those ideas from MH to discrete sampling. In fact, let x and x\u2032 be the current and proposed value in an MH iteration, Racing-EBS and Racing-Normal reduce to the algorithms in Bardenet et al. (2014) and Korattikara et al.", "startOffset": 50, "endOffset": 330}, {"referenceID": 3, "context": "3 are based on subsampling-based MH algorithms in Bardenet et al. (2014); Korattikara et al. (2014). The proposed algorithm extends those ideas from MH to discrete sampling. In fact, let x and x\u2032 be the current and proposed value in an MH iteration, Racing-EBS and Racing-Normal reduce to the algorithms in Bardenet et al. (2014) and Korattikara et al. (2014) respectively if we set", "startOffset": 50, "endOffset": 360}, {"referenceID": 3, "context": "The difference with Bardenet et al. (2014) is that we distribute the error \u03b4 evenly across t in Eq.", "startOffset": 20, "endOffset": 43}, {"referenceID": 3, "context": "The difference with Bardenet et al. (2014) is that we distribute the error \u03b4 evenly across t in Eq. 7 while Bardenet et al. (2014) set \u03b4t = (p \u2212 1)/(p(T ))\u03b4 with p a free parameter.", "startOffset": 20, "endOffset": 131}, {"referenceID": 3, "context": "The difference with Bardenet et al. (2014) is that we distribute the error \u03b4 evenly across t in Eq. 7 while Bardenet et al. (2014) set \u03b4t = (p \u2212 1)/(p(T ))\u03b4 with p a free parameter. The differences with Korattikara et al. (2014) are that we take a doubling schedule for m and replace the t-test with the normal assumption.", "startOffset": 20, "endOffset": 229}, {"referenceID": 3, "context": "The difference with Bardenet et al. (2014) is that we distribute the error \u03b4 evenly across t in Eq. 7 while Bardenet et al. (2014) set \u03b4t = (p \u2212 1)/(p(T ))\u03b4 with p a free parameter. The differences with Korattikara et al. (2014) are that we take a doubling schedule for m and replace the t-test with the normal assumption. We find that our algorithms are more efficient and robust than both original algorithms in practice. Moreover, the binary Gibbs sampling in Appx. F of Korattikara et al. (2014) is also a special case of RacingNormal with D = 2.", "startOffset": 20, "endOffset": 500}, {"referenceID": 3, "context": "The difference with Bardenet et al. (2014) is that we distribute the error \u03b4 evenly across t in Eq. 7 while Bardenet et al. (2014) set \u03b4t = (p \u2212 1)/(p(T ))\u03b4 with p a free parameter. The differences with Korattikara et al. (2014) are that we take a doubling schedule for m and replace the t-test with the normal assumption. We find that our algorithms are more efficient and robust than both original algorithms in practice. Moreover, the binary Gibbs sampling in Appx. F of Korattikara et al. (2014) is also a special case of RacingNormal with D = 2. Therefore, Alg. 1 provides a unifying approach to a family of subsampling-based samplers. The variance reduction technique is similar to the proxies in Bardenet et al. (2015), but the control variate here is a function in the data space while the proxy in the latter is a function in the parameter space.", "startOffset": 20, "endOffset": 726}, {"referenceID": 12, "context": "We set the scale parameter of adapted lil\u2019UCB with C/2 and other parameters with the heuristic setting in Jamieson et al. (2014). Racing uses the pairwise variance estimate.", "startOffset": 106, "endOffset": 129}, {"referenceID": 3, "context": "Surprisingly, Racing-Normal performs robustly regardless of reward distributions with the first mini-batch size m = 50 while it was shown in Bardenet et al. (2014) that the algorithm with the same normal assumption in Korattikara et al.", "startOffset": 141, "endOffset": 164}, {"referenceID": 3, "context": "Surprisingly, Racing-Normal performs robustly regardless of reward distributions with the first mini-batch size m = 50 while it was shown in Bardenet et al. (2014) that the algorithm with the same normal assumption in Korattikara et al. (2014) failed with LogNormal even when m = 500.", "startOffset": 141, "endOffset": 244}, {"referenceID": 30, "context": "Singh et al. (2012) addressed this problem with a conditional random field model with pairwise factors.", "startOffset": 0, "endOffset": 20}, {"referenceID": 13, "context": "We consider the MAP inference problem with fixed \u03b8 using annealed Gibbs sampling (Finkel et al., 2005).", "startOffset": 81, "endOffset": 102}, {"referenceID": 17, "context": "The Racing algorithm provides a unifying approaches to various subsampling-based Monte Carlo algorithms and also improves the robustness of the original MH algorithm in Korattikara et al. (2014). This is also the first work to discuss MABs under the setting of a finite reward population.", "startOffset": 169, "endOffset": 195}], "year": 2016, "abstractText": "Drawing a sample from a discrete distribution is one of the building components for Monte Carlo methods. Like other sampling algorithms, discrete sampling suffers from the high computational burden in large-scale inference problems. We study the problem of sampling a discrete random variable with a high degree of dependency that is typical in large-scale Bayesian inference and graphical models, and propose an efficient approximate solution with a subsampling approach. We make a novel connection between the discrete sampling and Multi-Armed Bandits problems with a finite reward population and provide three algorithms with theoretical guarantees. Empirical evaluations show the robustness and efficiency of the approximate algorithms in both synthetic and real-world large-scale problems.", "creator": "LaTeX with hyperref package"}}}