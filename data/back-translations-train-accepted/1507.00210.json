{"id": "1507.00210", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jul-2015", "title": "Natural Neural Networks", "abstract": "We introduce Natural Neural Networks, a novel family of algorithms that speed up convergence by adapting their internal representation during training to improve conditioning of the Fisher matrix. In particular, we show a specific example that employs a simple and efficient reparametrization of the neural network weights by implicitly whitening the representation obtained at each layer, while preserving the feed-forward computation of the network. Such networks can be trained efficiently via the proposed Projected Natural Gradient Descent algorithm (PRONG), which amortizes the cost of these reparametrizations over many parameter updates and is closely related to the Mirror Descent online learning algorithm. We highlight the benefits of our method on both unsupervised and supervised learning tasks, and showcase its scalability by training on the large-scale ImageNet Challenge dataset.", "histories": [["v1", "Wed, 1 Jul 2015 12:42:01 GMT  (767kb,D)", "http://arxiv.org/abs/1507.00210v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG cs.NE", "authors": ["guillaume desjardins", "karen simonyan", "razvan pascanu", "koray kavukcuoglu"], "accepted": true, "id": "1507.00210"}, "pdf": {"name": "1507.00210.pdf", "metadata": {"source": "CRF", "title": "Natural Neural Networks", "authors": ["Guillaume Desjardins", "Karen Simonyan", "Razvan Pascanu", "Koray Kavukcuoglu"], "emails": ["gdesjardins@google.com", "simonyan@google.com", "razp@google.com", "korayk@google.com"], "sections": [{"heading": "1 Introduction", "text": "While their deep and complex structure provides them with a rich modeling capacity, it also creates complex interdependencies between parameters that can make learning about a stochastic gradient descent (SGD) more difficult. As long as SGD remains the workhorse of deep learning, our ability to extract high-level representations from data can be hampered by difficult optimization problems, such as the performance boost from batch normalization (BN) [7] on the conception architecture [25]. Although its adoption remains limited, the natural gradient [1] seems ideally suited to these difficult optimization problems because it traces the direction of the steepest descent on the probability scale, the natural gradient can be made constant progress over the course of optimization as measured by the kullback conductor (KL) divergence between successive iterates."}, {"heading": "2 The Natural Gradient", "text": "This section provides the necessary background and derives a specific form of FIM, the structure of which will be key to our efficient approach. As we adapt the development of our method to the classification settings, our approach generalizes to regression and density estimates."}, {"heading": "2.1 Overview", "text": "We consider the problem of adapting the parameters \u03b8-RN of a model p (y | x; \u03b8) to an empirical distribution \u03c0 (x, y) under the log loss. We refer to x-X as the observation vector and y-Y as its associated designation. Specifically, this stochastic optimization problem aims at finding the following solution:.....?.?.?.?.?.?.?.?.?.?...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "2.2 Fisher Information Matrix for MLPs", "text": "We begin by deriving the exact shape of the Fisher matrix for a canonical multi-layer hypothesis (MLP) consisting of L layers. We consider the following deep network for binary classification, although our approach generalizes to an arbitrary number of output categories. MLP parameters, which are called \u03b8 = {W1, b1, \u00b7 \u00b7 \u00b7, WL, bL}, are the weights Wi-RNi \u2212 Ni \u2212 1 connecting layers i and i \u2212 1, and the distortions bi-RNi. fi is an elementary non-linear function. Let us define digits as the backward-distributed gradient T (WLhL, bL}, where i \u2212 Ni \u2212 Ni \u2212 1 is the connecting layers i and i \u2212 1, and the distortions bi-RNi. fi is an elementary non-linear function. Let us define digits as the backward-distributed gradient T by the non-linear T."}, {"heading": "3 Projected Natural Gradient Descent", "text": "In this section, we present whitened neural networks (WNN), which perform an approximate lightening of their internal hidden representations. We begin with the presentation of a novel white neural layer, assuming that the network statistics \u00b5i (\u03b8) = E [hi] and \u0102i (\u03b8) = E [hihTi] are fixed, and then show how these layers can be adapted to efficiently track population statistics during training, the resulting learning algorithm is called the Projected Natural Gradient Descent (PRONG)."}, {"heading": "3.1 A Whitened Neural Layer", "text": "The building block of WNN is the following neural layer, hi = fi (ViUi \u2212 1 (hi \u2212 1 \u2212 ci) + di). (6) Compared to Eq. 4, we have introduced an explicit centering parameter ci = \u00b5i, which ensures that the input to the point product has zero mean value in expectation. This corresponds to the centering repair matrix for Deep Boltzmann machines [13]. The weight matrix Ui \u2212 1 \u00b7 RNi \u2212 1 is in fact one per layer ZCA whitening matrix, the rows of which are obtained from an inherent decomposition of surfaces i \u2212 1."}, {"heading": "3.2 Updating the Whitening Coefficients", "text": "Since the whitened model parameters evolve during training, so can the statistics \u00b5i and \u0441i updates. In order for our model to remain well conditioned, the whitened coefficients must be updated at regular intervals, algorithm 1 Projected Natural Gradient Descent 1: Input: Training Set D, initial parameters \u03b8. 2: Hyper-parameters: Repair frequency T, number of samples Ns, regulation term. 3: Ui \u2190 I; ci \u2190 0: repeat 5: if mod (t, T) = 0 dann. amortize cost of lines [6-11] 6: For all layers I make to the canonical parameters Wi = ViUi \u2212 1; bi = di + Wici."}, {"heading": "3.3 Duality and Mirror Descent", "text": "There is an inherent duality between the parameters of our white neural layer and the parameters of the double order of a canonical model. In fact, there are linear projections contained in line 10 of algorithm 1, while P \u2212 1\u03a6 (preservation) of line 7. However, this duality between the two parameters reveals a close connection between PRONG and mirror descent [3]. Mirror descent (MD) is an online learning algorithm that shows the proximal form of gradient descent to the class of Bregman divergences Bell (q, p), where q, p and PRONG descent (MD) is a strictly convex and differentiable function. Replacing the distance of L2 with Belle, mirror descent solves the proximal problem of Eq."}, {"heading": "3.4 Related Work", "text": "This paper extends the recent contributions of [17] in formalizing many commonly used heuristics for the formation of MLPs: the importance of activations and gradients between zero and medium [10, 21], as well as the importance of normalized variances in forward and reverse [10, 21, 6]. More recently, Vatanen et al. [28] have expanded their previous work by introducing a multiplicative constant regarding centered nonlinearity. In contrast, we are introducing a full brightening matrix Ui and focusing on the brightening of network activations rather than normalizing a geometric mean across units and gradient variants. The recently introduced Batch Normalization Scheme (BN) is quite similar to a diagonal version of PRONG, with the main difference being that BN normalizes the variance of activations before non-linearity, as opposed to normalizing the latency by looking at the interesting Kovariance method of the ONG-PRG."}, {"heading": "4 Experiments", "text": "We begin with a series of diagnostic experiments highlighting the effectiveness of our method in improving conditioning, illustrating the effects of hyperparameters T and, controlling the frequency of repair and the size of the trust region. Section 4.2 evaluates PRONG for unattended learning problems where the models are both deeply and fully interconnected, and Section 4.3 goes into large-scale Convolutionary Models of Image Classification."}, {"heading": "4.1 Introspective Experiments", "text": "To gain a better understanding of PRONG's approach, we train a small 3-layer MLP with tanh nonlinearity on a downsampled version of MNIST (10x10) [11], the model size was chosen so that the full Fisher can be traced. Fig. 2 (a-b) shows the FIM of the middle hidden layers before and after the brightening of the model activations (we took the absolute value of the entries to improve visibility). Fig. 2c shows the evolution of the state of the FIM during training, measured as a percentage of its initial value (before the first brightening of the repair measures in the PRONG case). We present such curves for SGD, RMSprop and PRONG. Results clearly show that the repair rate of PRONG nitrization performed improves conditioning (reduction of more than 95%)."}, {"heading": "4.2 Unsupervised Learning", "text": "Following Martens [12], we compare PRONG with the task of minimizing the reconstruction error of an 8-layer auto encoder on the MNIST dataset. The encoder consists of 4 tightly connected sigmoidal layers with a number of hidden units per layer in {1k, 500, 250, 30} and a symmetrical (unbound) decoder. Hyperparameters were selected by network search, based on training errors, with the following grid specifications: training batch size in {32, 64, 128, 256}, learning rates in {10 \u2212 1, 10 \u2212 2, 10 \u2212 3} and impulse term in {0, 0.9}. For RMSprop, we further fine-tuned the moving average coefficient in {0.99, 0.999}, and the regulation term controls the maximum scaling factor in {0.1, 0.01}. For PRONG, we set the natural update time to 100 (we set it to ST = 103), and the regulation term controls the maximum scaling factor in {0.1, 0.01}."}, {"heading": "4.3 Supervised Learning", "text": "In the following [7], we perform whitening only on feature maps: that is, we treat pixels in a given feature map as independent samples. This allows us to implement the whitened neural layer as a sequence of two turns, the first of which passes through a 1x1 whitening filter. PRONG is compared with SGD, RMSprop and batch normalization, with each algorithm accelerated by dynamics, and the results are presented on both the CIFAR-10 [9] and the ImageNet Challenge (ILSVRC12) datasets [20]. In both cases, the learning rates were reduced by a \"waterfall\" annealing schedule that divided the learning rate by 10 when the validation error did not improve after a certain number of evaluations.4"}, {"heading": "4.3.1 CIFAR-10", "text": "The model used in our CIFAR experiments consists of 8 convolutionary layers, each with 3 x 3 receptive fields. 2 x 2 spatial maximum pooling was applied between stacks of two convolutionary layers, except for the last convolutionary layer, which calculates class results and is followed by global maximum pooling and soft-max nonlinearity. 3We note that our implementation of whitening operations was not inspired by the VGG model, as it does not take advantage of GPU acceleration, as opposed to neural network computations. Therefore, the runtime of our method is expected to be improved as we shift the self-decomposed layers to GPU-10."}, {"heading": "4.3.2 ImageNet Challenge Dataset", "text": "Our last set of experiments aims to demonstrate the scalability of our method: so we apply our natural gradient algorithm to the large-scale ILSVRC12 dataset (1.3 million images in 1000 categories) using the Inception architecture [7]. To scale the problems of this size, we paralleled our training loop to split the processing of a single minibatch (of size 256) across several GPUs. Note that PRONG can scale well in this setting, as the estimation of the mean and covariance parameters of each layer is also embarrassingly parallel. Eight GPUs were used to calculate gradients and estimate model statistics, although the one was not parallelized for the brightening itself."}, {"heading": "5 Discussion", "text": "From a theoretical and experimental perspective, we have shown that whitened neural networks can achieve this through simple, scalable and efficient whitening repair. However, they are one of several possible instances of the concept of natural neural networks. In an earlier incarnation of the idea, we have used a similar repair method to include the whitening method of backward propagated gradients 7. We prefer the simpler approach presented in this paper, as we have generally found the deep network alternative to be less stable. Ensuring zero-medium gradients also required the use of skip connections, with lengthy accounting to compensate for the repair of centered nonlinearity. [17] Maintaining soft activations can also provide additional benefits from the point of view of model compression and optimization, due to the fact that whitening is the least significant form of Uihi-linearity that exhibits the most compression."}, {"heading": "Acknowledgments", "text": "We thank Shakir Mohamed for the valuable discussions and feedback in preparing this manuscript, and Philip Thomas, Volodymyr Mnih, Raia Hadsell, Sergey Ioffe and Shane Legg for feedback on the essay."}], "references": [{"title": "Natural gradient works efficiently in learning", "author": ["Shun-ichi Amari"], "venue": "Neural Computation,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Do deep nets really need to be deep", "author": ["Jimmy Ba", "Rich Caruana"], "venue": "In NIPS", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Mirror descent and nonlinear projected subgradient methods for convex optimization", "author": ["Amir Beck", "Marc Teboulle"], "venue": "Oper. Res. Lett.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Proximal Splitting Methods in Signal Processing", "author": ["P.L. Combettes", "J.-C. Pesquet"], "venue": "ArXiv e-prints,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "In JMLR", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In AISTATS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Optimizing neural networks with kronecker-factored approximate curvature", "author": ["Roger Grosse James Martens"], "venue": "In ICML,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky"], "venue": "Master\u2019s thesis, University of Toronto,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Efficient backprop. In Neural Networks, Tricks of the Trade, Lecture Notes in Computer Science LNCS 1524", "author": ["Yann LeCun", "L\u00e9on Bottou", "Genevieve B. Orr", "Klaus-Robert M\u00fcller"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann Lecun", "Lon Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Deep learning via Hessian-free optimization", "author": ["James Martens"], "venue": "In ICML,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Deep boltzmann machines and the centering trick", "author": ["K.-R. M\u00fcller", "G. Montavon"], "venue": "Neural Networks: Tricks of the Trade. Springer,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Revisiting natural gradient for deep networks", "author": ["Razvan Pascanu", "Yoshua Bengio"], "venue": "In ICLR,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Parallel training of deep neural networks with natural gradient and parameter averaging", "author": ["Daniel Povey", "Xiaohui Zhang", "Sanjeev Khudanpur"], "venue": "ICLR workshop,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Deep learning made easier by linear transformations in perceptrons", "author": ["T. Raiko", "H. Valpola", "Y. LeCun"], "venue": "In AISTATS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "The Information Geometry of Mirror Descent", "author": ["G. Raskutti", "S. Mukherjee"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Topmoumoute online natural gradient algorithm", "author": ["Nicolas L. Roux", "Pierre antoine Manzagol", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Accelerated gradient descent by factor-centering decomposition", "author": ["Nicol N. Schraudolph"], "venue": "Technical Report IDSIA-33-98, Istituto Dalle Molle di Studi sull\u2019Intelligenza Artificiale,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "The natural gradient by analogy to signal whitening, and recipes and tricks for its use", "author": ["Jascha Sohl-Dickstein"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Projected natural actorcritic", "author": ["Philip S Thomas", "William C Dabney", "Stephen Giguere", "Sridhar Mahadevan"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Rmsprop: Divide the gradient by a running average of its recent magnitude. coursera: Neural networks for machine", "author": ["Tijmen Tieleman", "Geoffrey Hinton"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Pushing stochastic gradient towards second-order methods \u2013 backpropagation learning with transformations in nonlinearities", "author": ["Tommi Vatanen", "Tapani Raiko", "Harri Valpola", "Yann LeCun"], "venue": "ICONIP,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}], "referenceMentions": [{"referenceID": 6, "context": "As long as SGD remains the workhorse of deep learning, our ability to extract highlevel representations from data may be hindered by difficult optimization, as evidenced by the boost in performance offered by batch normalization (BN) [7] on the Inception architecture [25].", "startOffset": 234, "endOffset": 237}, {"referenceID": 0, "context": "Though its adoption remains limited, the natural gradient [1] appears ideally suited to these difficult optimization issues.", "startOffset": 58, "endOffset": 61}, {"referenceID": 11, "context": "Truncated Newton methods can avoid explicitly forming the FIM in memory [12, 15], but they require an expensive iterative procedure to compute the inverse.", "startOffset": 72, "endOffset": 80}, {"referenceID": 13, "context": "Truncated Newton methods can avoid explicitly forming the FIM in memory [12, 15], but they require an expensive iterative procedure to compute the inverse.", "startOffset": 72, "endOffset": 80}, {"referenceID": 15, "context": "Inspired by recent work on model reparametrizations [17, 13], our approach starts with a simple question: can we devise a neural network architecture whose Fisher is constrained to be identity? This is an important question, as SGD and NGD would be equivalent in the resulting model.", "startOffset": 52, "endOffset": 60}, {"referenceID": 12, "context": "Inspired by recent work on model reparametrizations [17, 13], our approach starts with a simple question: can we devise a neural network architecture whose Fisher is constrained to be identity? This is an important question, as SGD and NGD would be equivalent in the resulting model.", "startOffset": 52, "endOffset": 60}, {"referenceID": 2, "context": "Our second contribution is in unifying many heuristics commonly used for training neural networks, under the roof of the natural gradient, while highlighting an important connection between model reparametrizations and Mirror Descent [3].", "startOffset": 234, "endOffset": 237}, {"referenceID": 3, "context": "An equivalent proximal form of gradient descent [4] reveals the precise nature of \u03b1:", "startOffset": 48, "endOffset": 51}, {"referenceID": 13, "context": "See [15, 14] for a recent overview of the topic.", "startOffset": 4, "endOffset": 12}, {"referenceID": 12, "context": "This is analogous to the centering reparametrization for Deep Boltzmann Machines [13].", "startOffset": 81, "endOffset": 85}, {"referenceID": 5, "context": "amortize cost of lines [6-11] 6: for all layers i do 7: Compute canonical parameters Wi = ViUi\u22121; bi = di +Wici.", "startOffset": 23, "endOffset": 29}, {"referenceID": 6, "context": "amortize cost of lines [6-11] 6: for all layers i do 7: Compute canonical parameters Wi = ViUi\u22121; bi = di +Wici.", "startOffset": 23, "endOffset": 29}, {"referenceID": 7, "context": "amortize cost of lines [6-11] 6: for all layers i do 7: Compute canonical parameters Wi = ViUi\u22121; bi = di +Wici.", "startOffset": 23, "endOffset": 29}, {"referenceID": 8, "context": "amortize cost of lines [6-11] 6: for all layers i do 7: Compute canonical parameters Wi = ViUi\u22121; bi = di +Wici.", "startOffset": 23, "endOffset": 29}, {"referenceID": 9, "context": "amortize cost of lines [6-11] 6: for all layers i do 7: Compute canonical parameters Wi = ViUi\u22121; bi = di +Wici.", "startOffset": 23, "endOffset": 29}, {"referenceID": 10, "context": "amortize cost of lines [6-11] 6: for all layers i do 7: Compute canonical parameters Wi = ViUi\u22121; bi = di +Wici.", "startOffset": 23, "endOffset": 29}, {"referenceID": 6, "context": "Unfortunately, while estimating the mean \u03bci and diag(\u03a3i) could be performed online over a minibatch of samples as in the recent Batch Normalization scheme [7], estimating the full covariance matrix will undoubtedly require a larger number of samples.", "startOffset": 155, "endOffset": 158}, {"referenceID": 23, "context": "While statistics could be accumulated online via an exponential moving average as in RMSprop [27] or K-FAC [8], the cost of the eigendecomposition required for computing the whitening matrix Ui remains cubic in the layer size.", "startOffset": 93, "endOffset": 97}, {"referenceID": 7, "context": "While statistics could be accumulated online via an exponential moving average as in RMSprop [27] or K-FAC [8], the cost of the eigendecomposition required for computing the whitening matrix Ui remains cubic in the layer size.", "startOffset": 107, "endOffset": 110}, {"referenceID": 2, "context": "This duality between \u03b8 and \u03a9 reveals a close connection between PRONG and Mirror Descent [3].", "startOffset": 89, "endOffset": 92}, {"referenceID": 22, "context": "It is well known [26, 18] that the natural gradient is a special case of MD, where the distance generating function 1 is chosen to be \u03c8(\u03b8) = 12\u03b8 F\u03b8.", "startOffset": 17, "endOffset": 25}, {"referenceID": 16, "context": "It is well known [26, 18] that the natural gradient is a special case of MD, where the distance generating function 1 is chosen to be \u03c8(\u03b8) = 12\u03b8 F\u03b8.", "startOffset": 17, "endOffset": 25}, {"referenceID": 15, "context": "This work extends the recent contributions of [17] in formalizing many commonly used heuristics for training MLPs: the importance of zero-mean activations and gradients [10, 21], as well as the importance of normalized variances in the forward and backward passes [10, 21, 6].", "startOffset": 46, "endOffset": 50}, {"referenceID": 9, "context": "This work extends the recent contributions of [17] in formalizing many commonly used heuristics for training MLPs: the importance of zero-mean activations and gradients [10, 21], as well as the importance of normalized variances in the forward and backward passes [10, 21, 6].", "startOffset": 169, "endOffset": 177}, {"referenceID": 18, "context": "This work extends the recent contributions of [17] in formalizing many commonly used heuristics for training MLPs: the importance of zero-mean activations and gradients [10, 21], as well as the importance of normalized variances in the forward and backward passes [10, 21, 6].", "startOffset": 169, "endOffset": 177}, {"referenceID": 9, "context": "This work extends the recent contributions of [17] in formalizing many commonly used heuristics for training MLPs: the importance of zero-mean activations and gradients [10, 21], as well as the importance of normalized variances in the forward and backward passes [10, 21, 6].", "startOffset": 264, "endOffset": 275}, {"referenceID": 18, "context": "This work extends the recent contributions of [17] in formalizing many commonly used heuristics for training MLPs: the importance of zero-mean activations and gradients [10, 21], as well as the importance of normalized variances in the forward and backward passes [10, 21, 6].", "startOffset": 264, "endOffset": 275}, {"referenceID": 5, "context": "This work extends the recent contributions of [17] in formalizing many commonly used heuristics for training MLPs: the importance of zero-mean activations and gradients [10, 21], as well as the importance of normalized variances in the forward and backward passes [10, 21, 6].", "startOffset": 264, "endOffset": 275}, {"referenceID": 24, "context": "[28] extended their previous work [17] by introducing a multiplicative constant \u03b3i to the centered non-linearity.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[28] extended their previous work [17] by introducing a multiplicative constant \u03b3i to the centered non-linearity.", "startOffset": 34, "endOffset": 38}, {"referenceID": 6, "context": "The recently introduced batch normalization (BN) scheme [7] quite closely resembles a diagonal version of PRONG, the main difference being that BN normalizes the variance of activations before the non-linearity, as opposed to normalizing the latent activations by looking at the full covariance.", "startOffset": 56, "endOffset": 59}, {"referenceID": 23, "context": "A diagonal version of PRONG also bares an interesting resemblance to RMSprop [27, 5], in that both normalization terms involve the square root of the FIM.", "startOffset": 77, "endOffset": 84}, {"referenceID": 4, "context": "A diagonal version of PRONG also bares an interesting resemblance to RMSprop [27, 5], in that both normalization terms involve the square root of the FIM.", "startOffset": 77, "endOffset": 84}, {"referenceID": 7, "context": "K-FAC [8] is also closely related to PRONG and was developed concurrently to our method.", "startOffset": 6, "endOffset": 9}, {"referenceID": 17, "context": "the low rank structure of these blocks for efficiency, reminiscent of TONGA[19].", "startOffset": 75, "endOffset": 79}, {"referenceID": 14, "context": "Their method however operates online via low-rank updates to each block, similar to the preconditioning used in the Kaldi speech recognition toolkit [16].", "startOffset": 149, "endOffset": 153}, {"referenceID": 20, "context": "A similar algorithm to PRONG was later found in [23], where it appeared simply as a thought experiment, but with no amortization or recourse for efficiently computing F .", "startOffset": 48, "endOffset": 52}, {"referenceID": 10, "context": "To provide a better understanding of the approximation made by PRONG, we train a small 3-layer MLP with tanh non-linearities, on a downsampled version of MNIST (10x10) [11].", "startOffset": 168, "endOffset": 172}, {"referenceID": 9, "context": "Fig 3b compares the impact of T for models having a proper whitened initialization (solid lines), to models being initialized with a standard \u201cfan-in\u201d initialization (dashed lines) [10].", "startOffset": 181, "endOffset": 185}, {"referenceID": 11, "context": "Following Martens [12], we compare PRONG on the task of minimizing reconstruction error of an 8-layer auto-encoder on the MNIST dataset.", "startOffset": 18, "endOffset": 22}, {"referenceID": 6, "context": "Following [7], we perform whitening across feature maps only: that is we treat pixels in a given feature map as independent samples.", "startOffset": 10, "endOffset": 13}, {"referenceID": 8, "context": "Results are presented on both CIFAR-10 [9] and the ImageNet Challenge (ILSVRC12) datasets [20].", "startOffset": 39, "endOffset": 42}, {"referenceID": 19, "context": "This particular choice of architecture was inspired by the VGG model [22] and held fixed across all experiments.", "startOffset": 69, "endOffset": 73}, {"referenceID": 13, "context": "This could reflect the findings of [15], which showed how NGD can leverage unlabeled data for better generalization: the \u201cunlabeled\u201d data here comes from the extra perturbations in the training set when estimating the whitening matrices.", "startOffset": 35, "endOffset": 39}, {"referenceID": 6, "context": "3M images labelled into 1000 categories) using the Inception architecture [7].", "startOffset": 74, "endOffset": 77}, {"referenceID": 6, "context": "While our top-1 error is higher than reported in [7] (25.", "startOffset": 49, "endOffset": 52}, {"referenceID": 21, "context": "As our main focus is optimization, regularization consisted of a simple L2 weight decay parameter of 10\u22124, with no Dropout [24].", "startOffset": 123, "endOffset": 127}, {"referenceID": 15, "context": "Ensuring zero-mean gradients also required the use of skip-connections, with tedious book-keeping to offset the reparametrization of centered non-linearities [17].", "startOffset": 158, "endOffset": 162}, {"referenceID": 1, "context": "The sharp roll-off in the eigenspectrum of \u03a3i may explain why deep networks are ammenable to compression [2].", "startOffset": 105, "endOffset": 108}, {"referenceID": 21, "context": "Similarly, one could envision spectral versions of Dropout [24] where the dropout probability is a function of the eigenvalues.", "startOffset": 59, "endOffset": 63}], "year": 2015, "abstractText": "We introduce Natural Neural Networks, a novel family of algorithms that speed up convergence by adapting their internal representation during training to improve conditioning of the Fisher matrix. In particular, we show a specific example that employs a simple and efficient reparametrization of the neural network weights by implicitly whitening the representation obtained at each layer, while preserving the feed-forward computation of the network. Such networks can be trained efficiently via the proposed Projected Natural Gradient Descent algorithm (PRONG), which amortizes the cost of these reparametrizations over many parameter updates and is closely related to the Mirror Descent online learning algorithm. We highlight the benefits of our method on both unsupervised and supervised learning tasks, and showcase its scalability by training on the large-scale ImageNet Challenge dataset.", "creator": "LaTeX with hyperref package"}}}