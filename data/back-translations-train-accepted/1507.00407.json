{"id": "1507.00407", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jul-2015", "title": "Fast Convergence of Regularized Learning in Games", "abstract": "We show that natural classes of regularized learning algorithms with a form of recency bias achieve faster convergence rates to approximate efficiency and to correlated equilibria in multiplayer normal form games. When each player in a game uses an algorithm from our class, their individual regret decays at $O(T^{-3/4})$, while the sum of utilities converges to an approximate optimum at $O(T^{-1})$--an improvement upon the worst case $O(T^{-1/2})$ rates. We show a black-box reduction for any algorithm in the class to achieve $O(T^{-1/2})$ rates against an adversary, while maintaining the faster rates against algorithms in the class. Our results extend those of [Rakhlin and Shridharan 2013] and [Daskalakis et al. 2014], who only analyzed two-player zero-sum games for specific algorithms.", "histories": [["v1", "Thu, 2 Jul 2015 01:55:40 GMT  (362kb,D)", "https://arxiv.org/abs/1507.00407v1", null], ["v2", "Tue, 21 Jul 2015 21:58:16 GMT  (120kb,D)", "http://arxiv.org/abs/1507.00407v2", null], ["v3", "Wed, 5 Aug 2015 14:51:56 GMT  (117kb,D)", "http://arxiv.org/abs/1507.00407v3", null], ["v4", "Tue, 8 Dec 2015 17:22:43 GMT  (112kb,D)", "http://arxiv.org/abs/1507.00407v4", null], ["v5", "Thu, 10 Dec 2015 21:52:29 GMT  (112kb,D)", "http://arxiv.org/abs/1507.00407v5", null]], "reviews": [], "SUBJECTS": "cs.GT cs.AI cs.LG", "authors": ["vasilis syrgkanis", "alekh agarwal", "haipeng luo", "robert e schapire"], "accepted": true, "id": "1507.00407"}, "pdf": {"name": "1507.00407.pdf", "metadata": {"source": "CRF", "title": "Fast Convergence of Regularized Learning in Games", "authors": ["Vasilis Syrgkanis", "Alekh Agarwal"], "emails": ["vasy@microsoft.com", "alekha@microsoft.com", "haipengl@cs.princeton.edu", "schapire@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "What happens when players interact with each other in a game, when everyone acts independently and selfishly to maximize their own utilities? If they are wise, we intuitively expect their utilities - both individually and as a group - to become stronger, perhaps even to approach the best possible regret. However, we also expect that the dynamics of their behavior will eventually achieve some kind of equilibrium. Understanding this dynamics is central to game theory and its various uses, including economics, network routing, auction design, and evolutionary biology. It is natural that players in this environment each use a learning algorithm without regret to make their decisions, an approach known as decentralized no-regret dynamics. No-regret algorithms are a powerful game for gaming because their regret limits even in adversarial environments."}, {"heading": "2 Repeated Game Model and Dynamics", "text": "We assume that each player's strategy space is limited and that the probability of strategy x / s is limited. (We assume that each player's strategy space x / s is limited and that the probability of strategy x / s is limited. (We assume that each player's strategy space x / s is limited.) Finally, we leave Ui (w) = Es (s) the expected usefulness of the player i. We consider the setting in which game G is played repeatedly for T-Time steps. At each point in time, each player i adopts a mixed strategy wti (Si)."}, {"heading": "3 Fast Convergence to Approximate Efficiency", "text": "In this section we present our main theoretical results characterizing a class of no-repentance dynamics that leads to a faster convergence in smooth games.We begin with the description of this class.Definition 3 (RVU property) We say that a vanishing repentance algorithm satisfies the repentance limited by variation in utilities (RVU) with the parameters \u03b1 > 0 and 0 < \u00df \u2264 and a pair of dual norms (RVU property) 1, if its regret about any sequence of utilities u1, u2,..., is limited by the parameters > 0 and 0 < \u00df \u2012 wt, ut \u2264 + T = 1, ut \u2212 1, prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - pity - prudence - pity - prudence - prudence - prudence - prudence - prudence - prudence - prudence - pity - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - prudence - pru"}, {"heading": "3.1 Fast Convergence of Social Welfare", "text": "In view of Proposition 2, we only need to understand the development of the sum of players \"remorse in order to achieve convergence rates of social well-being. \u2212 Proposition 4: Our main result in this section limits this sum if each player uses dynamics with the RVU property. \u2212 Theorem 4: Suppose each player's algorithm i fulfills the property of RVU with parameters \u03b1, \u03b2 and such that \u03b2 \u2212 n \u2212 1: 2 and vice versa. \u2212 Theorem 4: The algorithm of each player i: ri (T) \u2264 3: Proof. Since ui (s) \u2264 1: definitions implicitly imply: The totality of satisfaction of satisfaction of satisfaction of satisfaction of satisfaction of satisfaction of satisfaction by satisfaction of satisfaction of satisfaction by satisfaction of satisfaction by satisfaction of satisfaction by satisfaction of satisfaction by satisfaction of satisfaction by the satisfaction of satisfaction of satisfaction by the satisfaction of satisfaction of satisfaction by the satisfaction of satisfaction by the satisfaction of satisfaction by the satisfaction of satisfaction by the satisfaction of satisfaction by the satisfaction of the satisfaction by the satisfaction of the satisfaction by the satisfaction of the satisfaction by the satisfaction by the satisfaction of the satisfaction of the satisfaction by the satisfaction by the satisfaction of the satisfaction by the satisfaction by the satisfaction by the satisfaction of the satisfaction of the satisfaction by the satisfaction by the satisfaction of the satisfaction by the satisfaction of the satisfaction by the satisfaction of the satisfaction by the satisfaction by the satisfaction of the satisfaction by the satisfaction of the satisfaction by the satisfaction of the satisfaction of the satisfaction of the satisfaction of the social well."}, {"heading": "3.1.1 Optimistic Mirror Descent", "text": "The optimistic mirror descent (OMD) algorithm of Rakhlin and Sridharan [17] is parameterized by an adaptive predictor sequence Mti and a regularizer2 R, which is 1-strongly convex-3 in relation to a norm. Let DR denote the Bregman divergence in connection with R. Then, the update rule is defined as follows: Let g0i = Argming rule (Si) R (g) and\u03a6 (u, g) = argmax w (Si) \u03b7, u > \u2212 DR (w, g), then: wti = (M t i, g \u2212 1 i) and g t \u2212 (u t, g \u2212 1 i) Then the following statement for this method can be obtained. Suggestion 5. The OMD algorithm, which uses the step size and Mti = u \u2212 1 i, fulfils the RVU property with constants."}, {"heading": "3.1.2 Optimistic Follow the Regularized Leader", "text": "Next, we look at a different class of algorithms that are called optimized, following the regularized leader position (OFTRL = \u03b2 \u03b2 = f). This algorithm is similar but not equivalent to OMD and is an analog extension of Standard FTRL [13]. This algorithm takes the same parameters as for OMD and is defined as follows: Let us let w0i = argminw \u00b2 (Si) R (w) and: wTi \u2212 argmax w \u00b2 w \u00b2 (Si) w, T \u2212 1 \u00b2 t = 1 uti + M T i \u2212 R (w).We consider three variants of OFTRL with different options in the order Mti \u2212 \u2212 \u2212 argmax w \u00b2 p \u2212 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p p \u00b2 p p p p p p \u00b2 p p p p p p p p p p p \u00b2 p p p p p p p p \u00b2 p p p p p p p \u00b2 p p p p p p p p p \u00b2 p p p p p p p p p p \u00b2 p p p p p p p p \u00b2 p p p p p p \u00b2 p p p p p \u00b2 p p p p p p p p p) p \u00b2 p p p p p p p p p \u00b2 p p p p p p p - the simplest form of OFTRL uses the property Mti = u \u2212 R + M T p (w).We consider three variants of OFTRL with different options in the order Mti \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 argminw \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p p p \u00b2 p p p p \u00b2 p p p p p \u00b2 p p p p p p p p \u00b2 p p p p p p p \u00b2 p p p p p p p \u00b2 p p p p p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p"}, {"heading": "3.2 Fast Convergence of Individual Utilities", "text": "Theorem 11. Let us assume that the players use algorithms to satisfy the RVU property with the parameters \u03b1 > 0, \u03b2 > \u2265 0. If we continue to satisfy the stability property of the RVU \u2212 w + 1 itiststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststst"}, {"heading": "4 Robustness to Adversarial Opponent", "text": "It is natural to wonder if this comes at the cost of the worst case guarantees if some players do not apply algorithms with this feature while still guaranteeing O (1 / 2) remorse for each player, no matter how the opponents play. It is not so obvious how this modification could extend to other procedures, and it seems undesirable to abandon the black box remorse transformations that we have used to obtain Theorem 4. In this section, we present a generic way of transforming an algorithm that satisfies the RVU property in such a way that it maintains rapid convergence in advantageous settings."}, {"heading": "5 Experimental Evaluation", "text": "We analyzed the performance of the optimistic leader with the entropy regulator, who expects for each game the highest effectiveness corresponding to the hedge algorithm [8], which has been modified so that the benefit of the last iteration is counted twice for each strategy; we refer to it as an optimistic hedge. Formally, the probability of the player having his strategy in iteration T is proportionate to exp (-) to exp (-).We examined a simple auction in which n players bid for m-elements. Each player has a value to get at least one point and no additional value for additional items; the benefit of a player is the value for the allocation he has to derive, minus the payment he has to make. The game is defined as follows: At the same time, each player selects one of the m-elements and submits a bid."}, {"heading": "6 Discussion", "text": "We demonstrate a class of no-repentance algorithms that exhibit rapid convergence when pitted against each other while being robust against opposing opponents, which has implications for calculating correlated equilibrium and understanding the behavior of agents in complex multiplayer games. There are a number of interesting questions and instructions for future research that are suggested by our results, including the following: Vanilla hedge convergence rates: The fast rates of our work do not apply to algorithms such as hedge without modification. Is this modification sufficient or necessary? If not, are there counter-examples? In the supplement, we include a sketch that indicates such a counterexample, but also shows fast rates for a worse balance than our optimistic algorithms. Convergence of player strategies: The OFTRL algorithm often generates much more stable trajecting rates, though these large counterparts may not be considered equal to other fast ones."}, {"heading": "A Proof of Proposition 2", "text": "Suggestion 2: In a (\u03bb, \u00b5) smooth game, if each player i suffers ri (T) regret at most, then: 1T T-T-T-T-T-T = 1 W-T-T = 1 + \u00b5 OPT-1 1 + \u00b5 1 T-I-N-ri (T) = 1 \u03c1 OPT-1 + \u00b5 1 T-I-N-ri (T), whereby the factor \u03c1 = (1 + \u00b5) / \u03bb is called the price of total anarchy (POA). Proof: Since each player i-i-ri (T) has regret, we have this: T-T-T = 1 wti, u t-i t-T-T = 1 uti, s-i-ri (T) (6) Summing up all players and the property of smoothness: T-T = 1 W (wt) = T-T-T-T (T) = 1-T-T-T (T)."}, {"heading": "B Proof of Proposition 5", "text": "Proposition 5: The OMD algorithms that players with Mti-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-"}, {"heading": "C Proof of Proposition 7", "text": "Gti gti, i gti, i gti, i gti, i gti, i gti, i gti, i gti, i gti, i gti, i gti, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i gti, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i gti, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i gti, i, i, i, i, i, i, i, i gti, i, i, i, i, i, i, i, i, i, i gti, i, i, i, i gti, i, i, i, i gti, i, i, i, i, i gti, i, i, i, i, i, i, i, i, i, i gti, i, i, i, i, i, i, i, i, i, i, i gti, i, i, i, i, i"}, {"heading": "D Proof of Proposition 9", "text": "Suggestion 9: The OFTRL algorithm with the incremental value and Mti = Nominal value and nominal value = Nominal value and nominal value fulfils the RVU property with the constants Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value = Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 Nominal value \u2212 nominal value \u2212 value \u2212 value \u2212 Nominal value \u2212 nominal value \u2212 value \u2212 value \u2212 Nominal value \u2212 value \u2212 nominal value \u2212 value \u2212 value \u2212 nominal value \u2212 nominal value \u2212 nominal value \u2212"}, {"heading": "E Proof of Proposition 10", "text": "Proposition 10: Proposition 10: Proposition 10: Proposition 10: Proposition 10: Proposition 10: Proposition 10: Proposition 10: Proposition 10: Proposition 10: Proposition 10: Proposition 10: Proposition 10: Proposition 10: Proposition 10: Proposition 10: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: Proposition 1: 1: Proposition 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1:"}, {"heading": "F Proof of Theorem 14", "text": "Theorem 14: Algorithm A. \"\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "G Proof of Corollary 16", "text": "Episode 16: If A's property RVU (T 1 / 4) is fulfilled and also played against itself, and O's (T) against any opponent. Note that algorithm A is executed in every round of A \"with 1 / 4, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 9, 8, 8, 8, 8, 8, 8, 8, 9, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 8, 9, 8, 8, 8, 9, 8, 8, 9, 8, 8, 9, 9, 8, 8, 9, 9, 8, 8, 9, 8, 8, 8, 9, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8"}, {"heading": "H Fast convergence via a first order regret bound for cost-minimization", "text": "In this section, we show how a different regret can lead to a rapid convergence rate for a smooth game. (For some technical reasons, we consider costs instead of benefits in this section.) We use ci: \"S1 +.\" (0, 1) to specify the cost function, and similar to previous sections C (s) = \"s,\" \"s,\" \"s,\" \"s,\" \"s,\" \"s,\" \"s.\" \"We use ci:\" 0, 1), \"s,\" \"s,\" \"s.\" A game is (1) faster, \"s,\" s, \"if there is a strategy profile,\" so that for each strategy profile s., \"\" s. \"\" s., \"\" s. \"\" s., \"s.,\" s., \"\" s., \"s.,\" \"s.,\" s., \"s.,\" s., \"s.\". \"(\"), \"s.,\" s., \"s.,\" s., \".\" s., \"s.,\". \".\" (\"s.\"), \"s.,\" s., \"s.,\" s. \"(\" s., \"s.,\"., \"s.,\" s. \"(\" s., \"s.,\"., \"s.,\" s., \"s.,\". \".\""}, {"heading": "I Extension to continuous strategy space games", "text": "In this section, we extend our findings to continuous strategy games such as \"splittable selfish routing games\" (see e.g. [20]), games in which the price of anarchy has been well studied and quite well motivated by the strategies of the Internet. In these games, we look at the dynamics in which players simply observe the past game of their opponents and not the expected past game. In this setting, we look at the dynamics in which players do not apply mixed strategies, but simply apply online convex optimization algorithms to their continuous strategy spaces."}], "references": [{"title": "Learning, regret minimization, and equilibria", "author": ["A. Blum", "Y. Mansour"], "venue": "Algorithmic Game Theory,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Regret minimization and the price of total anarchy", "author": ["Avrim Blum", "MohammadTaghi Hajiaghayi", "Katrina Ligett", "Aaron Roth"], "venue": "In Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Prediction, Learning, and Games", "author": ["Nicolo Cesa-Bianchi", "Gabor Lugosi"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Near-optimal no-regret algorithms for zero-sum games", "author": ["Constantinos Daskalakis", "Alan Deckelbaum", "Anthony Kim"], "venue": "Games and Economic Behavior,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Internet advertising and the generalized second price auction: Selling billions of dollars worth of keywords", "author": ["Benjamin Edelman", "Michael Ostrovsky", "Michael Schwarz"], "venue": "Working Paper 11765,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "On the convergence of regret minimization dynamics in concave games", "author": ["Eyal Even-dar", "Yishay Mansour", "Uri Nadav"], "venue": "In Proceedings of the Forty-first Annual ACM Symposium on Theory of Computing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Calibrated learning and correlated equilibrium", "author": ["Dean P. Foster", "Rakesh V. Vohra"], "venue": "Games and Economic Behavior,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E Schapire"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Adaptive game playing using multiplicative weights", "author": ["Yoav Freund", "Robert E Schapire"], "venue": "Games and Economic Behavior,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Recency, records and recaps: Learning and nonequilibrium behavior in a simple decision problem", "author": ["Drew Fudenberg", "Alexander Peysakhovich"], "venue": "In Proceedings of the Fifteenth ACM Conference on Economics and Computation, EC", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "A simple adaptive procedure leading to correlated", "author": ["Sergiu Hart", "Andreu Mas-Colell"], "venue": "equilibrium. Econometrica,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "Efficient algorithms for online decision problems", "author": ["Adam Kalai", "Santosh Vempala"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "The weighted majority algorithm", "author": ["Nick Littlestone", "Manfred K Warmuth"], "venue": "Information and computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1994}, {"title": "Problem complexity and method efficiency in optimization", "author": ["AS Nemirovsky", "DB Yudin"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1983}, {"title": "Smooth minimization of non-smooth functions", "author": ["Yu. Nesterov"], "venue": "Mathematical Programming,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Online learning with predictable sequences", "author": ["Alexander Rakhlin", "Karthik Sridharan"], "venue": "COLT", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Optimization, learning, and games with predictable sequences", "author": ["Alexander Rakhlin", "Karthik Sridharan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Intrinsic robustness of the price of anarchy", "author": ["T. Roughgarden"], "venue": "In Proceedings of the 41st annual ACM symposium on Theory of computing,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Local smoothness and the price of anarchy in atomic splittable congestion games", "author": ["Tim Roughgarden", "Florian Schoppmann"], "venue": "In Proceedings of the Twenty-Second Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Online learning and online convex optimization", "author": ["Shai Shalev-Shwartz"], "venue": "Found. Trends Mach. Learn.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}], "referenceMentions": [{"referenceID": 16, "context": "Our results extend those of Rakhlin and Shridharan [18] and Daskalakis et al.", "startOffset": 51, "endOffset": 55}, {"referenceID": 3, "context": "[4], who only analyzed two-player zero-sum games for specific algorithms.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "When played against one another, it can also be shown that the sum of utilities approaches an approximate optimum [2, 19], and the player strategies converge to an equilibrium under appropriate conditions [7, 1, 9], at rates governed by the regret bounds.", "startOffset": 114, "endOffset": 121}, {"referenceID": 17, "context": "When played against one another, it can also be shown that the sum of utilities approaches an approximate optimum [2, 19], and the player strategies converge to an equilibrium under appropriate conditions [7, 1, 9], at rates governed by the regret bounds.", "startOffset": 114, "endOffset": 121}, {"referenceID": 6, "context": "When played against one another, it can also be shown that the sum of utilities approaches an approximate optimum [2, 19], and the player strategies converge to an equilibrium under appropriate conditions [7, 1, 9], at rates governed by the regret bounds.", "startOffset": 205, "endOffset": 214}, {"referenceID": 0, "context": "When played against one another, it can also be shown that the sum of utilities approaches an approximate optimum [2, 19], and the player strategies converge to an equilibrium under appropriate conditions [7, 1, 9], at rates governed by the regret bounds.", "startOffset": 205, "endOffset": 214}, {"referenceID": 8, "context": "When played against one another, it can also be shown that the sum of utilities approaches an approximate optimum [2, 19], and the player strategies converge to an equilibrium under appropriate conditions [7, 1, 9], at rates governed by the regret bounds.", "startOffset": 205, "endOffset": 214}, {"referenceID": 12, "context": "Well-known families of no-regret algorithms include multiplicative-weights [14, 8], Mirror Descent [15], and Follow the Regularized/Perturbed Leader [13].", "startOffset": 75, "endOffset": 82}, {"referenceID": 7, "context": "Well-known families of no-regret algorithms include multiplicative-weights [14, 8], Mirror Descent [15], and Follow the Regularized/Perturbed Leader [13].", "startOffset": 75, "endOffset": 82}, {"referenceID": 13, "context": "Well-known families of no-regret algorithms include multiplicative-weights [14, 8], Mirror Descent [15], and Follow the Regularized/Perturbed Leader [13].", "startOffset": 99, "endOffset": 103}, {"referenceID": 11, "context": "Well-known families of no-regret algorithms include multiplicative-weights [14, 8], Mirror Descent [15], and Follow the Regularized/Perturbed Leader [13].", "startOffset": 149, "endOffset": 153}, {"referenceID": 2, "context": "(See [3, 21] for excellent overviews.", "startOffset": 5, "endOffset": 12}, {"referenceID": 19, "context": "(See [3, 21] for excellent overviews.", "startOffset": 5, "endOffset": 12}, {"referenceID": 3, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "For two-player zero-sum games, they developed a decentralized variant of Nesterov\u2019s accelerated saddle point algorithm [16] and showed that each player\u2019s average regret converges at the remarkable rate ofO(1/T ).", "startOffset": 119, "endOffset": 123}, {"referenceID": 16, "context": "dynamics are somewhat unnatural, in later work, Rakhlin and Sridharan [18] showed surprisingly that the same convergence rate holds for a simple variant of Mirror Descent with the seemingly minor modification that the last utility observation is counted twice.", "startOffset": 70, "endOffset": 74}, {"referenceID": 17, "context": "Concretely, we show a natural class of regularized no-regret algorithms with recency bias that achieve welfare at least (\u03bb/(1 + \u03bc))OPT \u2212 O(1/T ), where \u03bb and \u03bc are parameters in a smoothness condition on the game introduced by Roughgarden [19].", "startOffset": 239, "endOffset": 243}, {"referenceID": 3, "context": "Even for two-person zero-sum games, our results for general games expose a hidden generality and modularity underlying the previous results [4, 18].", "startOffset": 140, "endOffset": 147}, {"referenceID": 16, "context": "Even for two-person zero-sum games, our results for general games expose a hidden generality and modularity underlying the previous results [4, 18].", "startOffset": 140, "endOffset": 147}, {"referenceID": 16, "context": "This covers the Optimistic Mirror Descent of Rakhlin and Sridharan [18] as an example, but also applies to optimistic variants of Follow the Regularized Leader (FTRL), including dependence on arbitrary weighted windows in the history as opposed to just the utility from the last round.", "startOffset": 67, "endOffset": 71}, {"referenceID": 9, "context": "Recency bias is a behavioral pattern commonly observed in game-theoretic environments [10]; as such, our results can be viewed as a partial theoretical justification.", "startOffset": 86, "endOffset": 90}, {"referenceID": 3, "context": "Second, previous approaches in [4, 18] on achieving both faster convergence against similar algorithms while at the same time \u00d5(1/ \u221a T ) regret rates against adversaries were shown via ad-hoc modifications of specific algorithms.", "startOffset": 31, "endOffset": 38}, {"referenceID": 16, "context": "Second, previous approaches in [4, 18] on achieving both faster convergence against similar algorithms while at the same time \u00d5(1/ \u221a T ) regret rates against adversaries were shown via ad-hoc modifications of specific algorithms.", "startOffset": 31, "endOffset": 38}, {"referenceID": 7, "context": "Finally, we simulate a 4-bidder simultaneous auction game, and compare our optimistic algorithms against Hedge [8] in terms of utilities, regrets and convergence to equilibria.", "startOffset": 111, "endOffset": 114}, {"referenceID": 0, "context": "\u00d7 Sn \u2192 [0, 1] that maps a strategy profile s = (s1, .", "startOffset": 7, "endOffset": 13}, {"referenceID": 17, "context": "We next define a class of games first identified by Roughgarden [19] on which we can approximate the optimal welfare using decoupled no-regret dynamics.", "startOffset": 64, "endOffset": 68}, {"referenceID": 17, "context": "Definition 1 (Smooth game [19]).", "startOffset": 26, "endOffset": 30}, {"referenceID": 17, "context": "This proposition is essentially a more explicit version of Roughgarden\u2019s result [19]; we provide a proof in the appendix for completeness.", "startOffset": 80, "endOffset": 84}, {"referenceID": 15, "context": "However, Rakhlin and Sridharan [17] give a modification of Mirror Descent with this property, and we will present a similar variant of FTRL in the sequel.", "startOffset": 31, "endOffset": 35}, {"referenceID": 3, "context": "Also, the theorem does not require that all players use the same no-regret algorithm unlike previous results [4, 18], as long as each player\u2019s algorithm satisfies the RVU property with a common bound on the constants.", "startOffset": 109, "endOffset": 116}, {"referenceID": 16, "context": "Also, the theorem does not require that all players use the same no-regret algorithm unlike previous results [4, 18], as long as each player\u2019s algorithm satisfies the RVU property with a common bound on the constants.", "startOffset": 109, "endOffset": 116}, {"referenceID": 15, "context": "1 Optimistic Mirror Descent The optimistic mirror descent (OMD) algorithm of Rakhlin and Sridharan [17] is parameterized by an adaptive predictor sequence Mi and a regularizer 2 R which is 1-strongly convex3 with respect to a norm \u2016 \u00b7 \u2016.", "startOffset": 99, "endOffset": 103}, {"referenceID": 16, "context": "The proposition follows by further crystallizing the arguments of Rakhlin and Sridaran [18], and we provide a proof in the appendix for completeness.", "startOffset": 87, "endOffset": 91}, {"referenceID": 16, "context": "The above proposition, along with Theorem 4, immediately yields the following corollary, which had been proved by Rakhlin and Sridharan [18] for two-person zero-sum games, and which we here extend to general games.", "startOffset": 136, "endOffset": 140}, {"referenceID": 11, "context": "This algorithm is similar but not equivalent to OMD, and is an analogous extension of standard FTRL [13].", "startOffset": 100, "endOffset": 104}, {"referenceID": 15, "context": "Rakhlin and Sridharan [17] also analyze an FTRL variant, but require a self-concordant barrier for the constraint set as opposed to an arbitrary strongly convex regularizer, and their bound is missing the crucial negative terms of the RVU property which are essential for obtaining Theorem 4.", "startOffset": 22, "endOffset": 26}, {"referenceID": 10, "context": "[11]) to CCE using natural, decoupled no-regret dynamics defined in [4].", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[11]) to CCE using natural, decoupled no-regret dynamics defined in [4].", "startOffset": 68, "endOffset": 71}, {"referenceID": 16, "context": "Rakhlin and Sridharan [18] address this concern by modifying the OMD algorithm with additional smoothing and adaptive step-sizes so as to preserve the fast rates in the favorable case while still guaranteeing O(1/ \u221a T ) regret for each player, no matter how the opponents play.", "startOffset": 22, "endOffset": 26}, {"referenceID": 7, "context": "5 Experimental Evaluation We analyzed the performance of optimistic follow the regularized leader with the entropy regularizer, which corresponds to the Hedge algorithm [8] modified so that the last iteration\u2019s utility for each strategy is double counted; we refer to it as Optimistic Hedge.", "startOffset": 169, "endOffset": 172}, {"referenceID": 0, "context": "The bids are discretized to be any integer in [1, 20].", "startOffset": 46, "endOffset": 53}, {"referenceID": 18, "context": "The bids are discretized to be any integer in [1, 20].", "startOffset": 46, "endOffset": 53}, {"referenceID": 0, "context": "References [1] A.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Avrim Blum, MohammadTaghi Hajiaghayi, Katrina Ligett, and Aaron Roth.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Nicolo Cesa-Bianchi and Gabor Lugosi.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Constantinos Daskalakis, Alan Deckelbaum, and Anthony Kim.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Benjamin Edelman, Michael Ostrovsky, and Michael Schwarz.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Eyal Even-dar, Yishay Mansour, and Uri Nadav.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Dean P.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Yoav Freund and Robert E Schapire.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Yoav Freund and Robert E Schapire.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Drew Fudenberg and Alexander Peysakhovich.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Sergiu Hart and Andreu Mas-Colell.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] Adam Kalai and Santosh Vempala.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] Nick Littlestone and Manfred K Warmuth.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] AS Nemirovsky and DB Yudin.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] Yu.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] Alexander Rakhlin and Karthik Sridharan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] Alexander Rakhlin and Karthik Sridharan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20] Tim Roughgarden and Florian Schoppmann.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21] Shai Shalev-Shwartz.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "We will use the following theorem of [18].", "startOffset": 37, "endOffset": 41}, {"referenceID": 16, "context": "Theorem 17 (Raklin and Sridharan [18]).", "startOffset": 33, "endOffset": 37}, {"referenceID": 16, "context": "Some of the key facts (Equations (9) and (10)) that we use in the following proof appear in [18].", "startOffset": 92, "endOffset": 96}, {"referenceID": 0, "context": "\u00d7 Sn \u2192 [0, 1] to denote the cost function, and similarly to previous sections C(s) = \u2211 i\u2208N ci(s), C(w) = Es\u223cw[C(s)],OPT \u2032 = mins\u2208S1\u00d7.", "startOffset": 7, "endOffset": 13}, {"referenceID": 18, "context": "[20]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Such learning on continuous games has also been studied in more restrictive settings in [6].", "startOffset": 88, "endOffset": 91}], "year": 2015, "abstractText": "We show that natural classes of regularized learning algorithms with a form of recency bias achieve faster convergence rates to approximate efficiency and to coarse correlated equilibria in multiplayer normal form games. When each player in a game uses an algorithm from our class, their individual regret decays at O(T\u22123/4), while the sum of utilities converges to an approximate optimum at O(T\u22121)\u2013an improvement upon the worst case O(T\u22121/2) rates. We show a blackbox reduction for any algorithm in the class to achieve \u00d5(T\u22121/2) rates against an adversary, while maintaining the faster rates against algorithms in the class. Our results extend those of Rakhlin and Shridharan [18] and Daskalakis et al. [4], who only analyzed two-player zero-sum games for specific algorithms.", "creator": "LaTeX with hyperref package"}}}