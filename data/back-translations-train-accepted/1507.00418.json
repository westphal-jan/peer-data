{"id": "1507.00418", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jul-2015", "title": "No-Regret Learning in Bayesian Games", "abstract": "Recent price-of-anarchy analyses of games of complete information suggest that coarse correlated equilibria, which characterize outcomes resulting from no-regret learning dynamics, have near-optimal welfare. This work provides two main technical results that lift this conclusion to games of incomplete information, a.k.a., Bayesian games. First, near-optimal welfare in Bayesian games follows directly from the smoothness-based proof of near-optimal welfare in the same game when the private information is public. Second, no-regret learning dynamics converge to Bayesian coarse correlated equilibrium in these incomplete information games. These results are enabled by interpretation of a Bayesian game as a stochastic game of complete information.", "histories": [["v1", "Thu, 2 Jul 2015 03:27:32 GMT  (50kb)", "http://arxiv.org/abs/1507.00418v1", null], ["v2", "Thu, 19 Nov 2015 23:45:14 GMT  (57kb)", "http://arxiv.org/abs/1507.00418v2", null]], "reviews": [], "SUBJECTS": "cs.GT cs.LG", "authors": ["jason d hartline", "vasilis syrgkanis", "\u00e9va tardos"], "accepted": true, "id": "1507.00418"}, "pdf": {"name": "1507.00418.pdf", "metadata": {"source": "CRF", "title": "No-Regret Learning in Repeated Bayesian Games", "authors": ["Jason Hartline", "Vasilis Syrgkanis \u00c9va Tardos"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 150 7.00 418v 1 [cs.G T] 2J ul2 015Recent price-of-anarchy analyses of games with complete information suggest that roughly correlated balances characterizing the results of learning dynamics without remorse exhibit near-optimal well-being. This work provides two key technical results that draw this conclusion to games with incomplete information, also known as Bayesian games. First, near-optimal well-being in Bayesian games follows directly from the smooth evidence of near-optimal well-being in the same game when private information is public. Second, learning dynamics without remorse converge in these incomplete information games to a roughly correlated balance in Bayesian balance. These results are made possible by interpreting a Bayesian game as a stochastic game with complete information."}, {"heading": "1 Introduction", "text": "In fact, it is such that most people will be able to move to another world, in which they are able to move to another world, in which they are able to change the world, in which they are able to change the world, in which they are able to move, in which they are able to change the world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "2 Preliminaries", "text": "This section describes a general game theory that includes auctions and resource allocation mechanisms. For this general environment, we review the results from the literature to analyse the social well-being resulting from the no-regret learning dynamics in repeated games. the subsequent sections of the paper will generalize this model and these results to Bayesian games, a.k.a., games of the incomplete game form. A general gameM becomes a (implicit) result of an A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-"}, {"heading": "3 Learning in Repeated Bayesian Game", "text": "We look at the following settings of a repeated Bayesian game: there are n equally large populations of players 2. However, we use no regrets for each population q. For each population q, Pi consists of a finite number of players. Each player q has a certain rating Vi (q). We look at the repeated game in which each iteration selects a player q from each population uniformly and independently of other populations. In other words, the value of a randomly selected player from the population Pi is determined by Fi.We look at the repeated game in which a player q from each population is uniformed and selected independently of other populations. The set of selected players then participates in an instance of a mechanism M. We assume that each player uses Qi, no regrets learning rule to play in this repeated game. An equivalent way of looking at the game is that the number of players is fixed, but their ratings change and are drawn identically from the distributions."}, {"heading": "4 Bayesian Coarse Correlated Equilibria", "text": "Note that the game, which is repeated at each step in time, can be considered a Bayesian game among a number of n players, in which the rating of each player is defined only by the player himself and is drawn by a distribution profile. As part of this interpretation, before deciding what action to play, each player learns his own rating vi and nothing else. A player's strategy in this Bayesian game is a mapping si: A Vi from a rating vi \u2212 Vi to an action ai ai Ai. We will use Vi to describe each player's strategy space and nothing else. In an incomplete information game, we want to measure well-being in comparison to Bayvi's expected ex-post optimal social well-being. If the game that is repeated is a complete information game in which no player has any private information, Theorem 3 gives a strong link between no-repentance learning and the notion of correlated equality of the game."}, {"heading": "5 Convergence of Bayesian No-Regret to Bayes-CCE", "text": "For each given sequence of the game of repeated Bayesian games that we have defined in definition 7, we define the following sequence of strategy-value pairs (st, vt), in which s: V \u2192 A: st (v) = 2 (qt), if V (qt) = 2 (qt), up to time step T: o.w. (8) and vt = V (qt). Then we observe that everything that is relevant for calculating the average social well-being of the game for each given time step T, the empirical distribution of (s, v), up to time step T, deno.w. (sT, vT) is a random sample of DT: 1TT = 1 SW (qt) = 1 SW (qt) = E (sT) [sT) [sW (sT)] (vT)], [S, [S, VT), VT (VT), VT (VT), VT (1), VT (1), VT (1)."}, {"heading": "6 Efficiency of Smooth Mechanisms at Bayes Coarse Correlated", "text": "EquilibriaIn this section we define the mechanism Mex, where a player's scope of action depends on a function to a complete function i. (i) It means that each Bayes-CCE of incomplete information setting achieves at least \u03bbmax {1, \u00b5} of the expected optimal well-being. To show this, we will adopt the interpretation of Bayes-CCE that we used in the previous section, as a roughly correlated equilibrium of a more complex normal form game. We can interpret this complex normal form game as the game resulting from a complete information mechanism Mex. The most important theory we will show is that whenever the mechanism M (\u03bb, \u00b5) -smooth is, then the mechanism Mex-smooth is also. Then we will invoke a theorem of [13, 10] showing that every roughly correlated equilibrium of a mechanism in the complete information setting achieves at least two different kinds."}, {"heading": "7 Finite Time Analysis and Convergence Rates", "text": "Theorem 14. Consider in this section the convergence rate to Bayes-CCE and show approximate efficiency results even for finite time, if players are allowed to show some degree of remorse. Theorem 14. Consider the repeated matching game with a (\u03bb, \u00b5) smooth mechanism. Suppose that each player in each of the n populations shows at most remorse for each T \u2265 T 0. Then, for each of these populations, there is a T \u0445 (\u03b4, \u03c1), so that for each T \u2265 min {T 0, T \u0445}, with probability 1 \u2212 \u03c1: 1TT \u0445 t = 1SW (st (vt); vt) \u2265 \u03bbmax {1, \u00b5} Ev [Opt (v)] \u2212 \u03b4 \u2212 \u00b5 (11) In addition, for each T \u0445 (\u043c, \u043c) \u2264 54 \u00b7 n 3 \u00b7 \u00b7 | \u03a3 | V | 2 \u00b7 H3\u0441\u04423 (2)."}, {"heading": "A Proof of Theorem 11", "text": "Imagine that D-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V"}, {"heading": "B Proof of Theorem 14", "text": "For a shorter notation, we will describe the empirical distribution of a Bayesian strategy s and with pT (v | s) = pT (v \u00b7 s) = pT (v \u00b7 s) = pT (v \u00b7 s) = pT (v \u00b7 s) = pT (v \u00b7 s) = pT (v \u00b7 s) = pT (v \u00b7 s) = empirical distribution of values tied to a Bayesian strategy s. The average usefulness of a population i up to a certain time step T, if the conversion to a fixed Bayesian strategy s s s i, can be written as: 1TT (s) = pT (s) = empirical distribution of values tied to a Bayesian strategy s. The average usefulness of a population i up to a certain time step T, if the conversion to a fixed Bayesian strategy s i, can be written as: 1TT (s), we will write t = 1p () = empirical distribution of values from a Bayesian strategy s, irical strategy s."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>Recent price-of-anarchy analyses of games of complete information suggest that coarse correlated<lb>equilibria, which characterize outcomes resulting from no-regret learning dynamics, have near-optimal<lb>welfare. This work provides two main technical results that lift this conclusion to games of incomplete<lb>information, a.k.a., Bayesian games. First, near-optimal welfare in Bayesian games follows directly from<lb>the smoothness-based proof of near-optimal welfare in the same game when the private information<lb>is public. Second, no-regret learning dynamics converge to Bayesian coarse correlated equilibrium in<lb>these incomplete information games. These results are enabled by interpretation of a Bayesian game<lb>as a stochastic game of complete information.", "creator": "LaTeX with hyperref package"}}}