{"id": "1507.01127", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jul-2015", "title": "AutoExtend: Extending Word Embeddings to Embeddings for Synsets and Lexemes", "abstract": "We present \\textit{AutoExtend}, a system to learn embeddings for synsets and lexemes. It is flexible in that it can take any word embeddings as input and does not need an additional training corpus. The synset/lexeme embeddings obtained live in the same vector space as the word embeddings. A sparse tensor formalization guarantees efficiency and parallelizability. We use WordNet as a lexical resource, but AutoExtend can be easily applied to other resources like Freebase. AutoExtend achieves state-of-the-art performance on word similarity and word sense disambiguation tasks.", "histories": [["v1", "Sat, 4 Jul 2015 16:59:30 GMT  (369kb,D)", "http://arxiv.org/abs/1507.01127v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sascha rothe", "hinrich sch\u00fctze"], "accepted": true, "id": "1507.01127"}, "pdf": {"name": "1507.01127.pdf", "metadata": {"source": "CRF", "title": "AutoExtend: Extending Word Embeddings to Embeddings for Synsets and Lexemes", "authors": ["Sascha Rothe"], "emails": ["sascha@cis.lmu.de"], "sections": [{"heading": "1 Introduction", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "2 Model", "text": "We are looking for a model that shows the simplest possible relationship between the three types of WordNet."}, {"heading": "2.1 Learning", "text": "For this purpose, we consider the tensor equation S = E'W as the encoding part of the autoencoder: The synsets are the encoding of the words. We define a corresponding decoding part that decodes the synsets into words as follows: s (j) = [1] l (i, j), w (i) = [1] l (i, j) (10) In analogy to E (i, j), we introduce the diagonal matrix D (j, i) = D (j, i) = D (j, i) s (j) (11) In this case, it is the synthesis that is distributed among its lexemes. We can then rewrite the equation 10 to: s (j) = intercoding matrix D (j) = intercoding D (j, i) s (j), w (i) s (j) s (j, i) s () s () s)."}, {"heading": "2.2 Matrix formalization", "text": "Based on the assumption that each dimension is completely independent of other dimensions, a separate autoencoder for each dimension can be created and trained in parallel. Let W-R-W-N be a matrix in which each line is an embedded word and w (d) = W \u00b7, d be the d-th column of W, i.e. a vector containing the d-th dimension of each word vector. Likewise, we can write S-E-W as: s (d) = E (d) w (d) w (d) w (d) w (d) w (18) withE (d) i, j = 0 if l (i, j) = 0 (syrical), the decryption equation W = D-E (0) and the decryption equation W = D-D (0)."}, {"heading": "2.3 Lexeme embeddings", "text": "The hidden layer S of the autoencoder gives us synset embeddings. The lexeme embeddings are defined at the transition from W to S, or more explicitly by: l (i, j) = E (i, j) w (i) (21) However, there is also a second lexem embedded in AutoExtend at the transition from S to W: l (i, j) = D (j, i) s (22) The alignment of these two representations occurs naturally, so that we impose the following lexeme constraints: argmin E (d), D (j, i), D (i, j) w (i) (i) \u2212 D (j, i) s (j), j (23) This can also be expressed dimensionally. The matrix formulation is expressed by: argmin E (d), D (d), x (d), diag (d), d), (d), (d), (d), (d, d, d, d, d, d, d, d, d, d, (, d, d, d, d, d, d, d, d, (), d, d, (), d, (d, d, d, d, d, d), (d, d, d, d, (d, d), (d, d), (d, d, d), (d, (d, d), (d, d, d), (d), (d, (d), (d), (d), (d), (d), (d, (d), (d), (d), (d), (d, (d), (d, and (d)."}, {"heading": "2.4 WN relations", "text": "Some WordNet synsets contain only a single word (lexem), and the autoencoder learns from synset constraints, i.e., lexemes shared by different synsets (and also words), so it is difficult to learn good embeddings for individual lexemes synsets. To address this problem, we set the constraint that synsets related to WordNet relationships (WN relationships) should have similar embeddings. Table 1 shows relationships we used. WN relationships are entered into a new matrixR-Rr-Rr-Rr-S-S matrix, where r is the number of WN relations. For each relationship stage, i.e. line in R, we set the columns to 1 and \u2212 1 according to the first and second synthesis, respectively. The values of R are not updated during the training. We use a quadratic error function and 0 as the target value. This forces the system to find values related to WD synsets (the WN)."}, {"heading": "2.5 Implementation", "text": "Our training goal is to minimize the sum of the synset constraints (Eq. 20), weighted by \u03b1, the lexem constraints (Eq. 24), weighted by \u03b2, and the WN relation constraints (Eq. 25), weighted by 1 \u2212 \u03b1 \u2212 \u03b2.The training goal cannot be solved analytically, because it is subject to limitations Eq. 8, Eq. 9, Eq. 15 and Eq. 16. We therefore use back propagation. We do not use regulation, as we have found that all learned weights are in [\u2212 2, 2]. AutoExtend is implemented in MATLAB. We perform 1000 iterations of the gradient drop. On an Intel Xeon CPU E7-8857 v2 3.0GHz, iteration on one dimension takes less than one minute, because the gradient calculation ignores zero entries in the matrix."}, {"heading": "2.6 Column normalization", "text": "Our model is based on the premise that a word is the sum of its lexemes (equation 1). From the definition of E (i, j) we deduce that E (d), R | \u00b7 n \u00d7 | W | \u00b7 n is normalized over the first dimension (equation 8). E (d), R | S | \u00d7 | W | is also normalized over the first dimension. In other words, E (d) is a normalized column matrix. Another premise of the model is that a synthesis is the sum of its lexemes. D (d) is therefore also a column normalization. An easy way to do this is to start the calculation with normalized column matrices and normalize them after each iteration as long as the error function is still decreasing. When the error function begins to rise, we normalize the matrices and proceed with a normal gradient decrease. This means that E (d) and D (d) (d) (d) (d) should be normalized as long as there are oxons in the theory."}, {"heading": "3 Data, experiments and evaluation", "text": "We have downloaded 300-dimensional embedding for 3,000,000 words and phrases trained on Google News, a corpus of \u2248 1011 tokens that use word2vec CBOW (Mikolov et al., 2013c). Many words in the word2vec vocabulary are not in WordNet, e.g. inflected forms (cars) and personal terms (Tony Blair). Conversely, many WordNet lemmats are not in the word2vec vocabulary, e.g. 42 (digits have been converted to 0), which results in a number of empty synsets (see Table 2). Note, however, that AutoExend can generate embedding for empty synsets, because in addition to synthesis and lexeme constraints, we use WN relation constraints. We run AutoExtend on word2vec vectors. Since we do not have a proper weighting for the three different embedding types, we are expecting our most important bet results to be used for improving all our bedding types = 0.33."}, {"heading": "3.1 Word Sense Disambiguation", "text": "For WSD, we use the common tasks of Senseval2 (Kilgarriff, 2001) and Senseval-3 (Mihalcea et al., 2004) and a system called IMS (Zhong and Ng, 2010). Senseval-2 contains only 139, Senseval-357 different words. They provide 8,611, respectively 8,022 training instances, and 4,328, respectively 3,944 test instances. For the system, we use the same settings as in the original paper. Pre-processing consists of sentence splitting, tokenization, POS tagging, and lemmatization; the classification is a linear SVM. In our experiments (Table 3), we use the same settings as in the original paper. The relative strengths of feature sets (lines 1-7) and feature set set combinations to determine which combination is best for WSD (lines 8, 12-15).IMS implements three standard sets: part of the speech (surrounding word)."}, {"heading": "3.2 Synset and lexeme similarity", "text": "We use SCWS (Huang et al., 2012) for the evaluation of similarity. SCWS provides not only isolated words and corresponding similarity values, but also context for each word. SCWS is based on WordNet, but the information about which synset a word came from in the context is not available. However, the data set is closest we could find for sensual similarity. Synset and lexeme embeddings are achieved by running AutoExtend. Based on the results of the WSD task, we set \u03b1 = 0.2 and \u03b2 = 0.5. Lexeme embeddings are the natural choice for this task, as human subjects are given two words and a context and then have to assign a similarity value. But for completeness, we also conduct experiments for synsets.For each word, we calculate a context vector c by adding all word vectors of the context, without the test word itself."}, {"heading": "4 Analysis", "text": "We will first consider the effects of the parameters \u03b1, \u03b2 (Section 2.5), which control the weighting of synset constraints vs lexem constraints vs WN relationship constraints. We will examine the effects for three different tasks. WSD alone: Accuracy of IMS (average of Senseval-2 and Senseval-3), when only Sproduct is used as a feature set (line 6 in Table 3). WSD additionally: Accuracy of IMS (average of Senseval-2 and Senseval-3), when S product is used together with the feature sets POS, surrounding word and local collocation (line 14 in Table 3). SCWS: Spearman correlation to SCWS (line 7 in Table 4). For WSD alone (Figure 3, center), the most powerful weights (red), all have high weights for WN relationships and are therefore at the top of the triangle. WN relationships are very important for additional WD weight and more than three of SD alone."}, {"heading": "5 Resources other than WordNet", "text": "AutoExtend is generally applicable to lexical and knowledge resources that have certain properties. While, in this essay, we will only conduct experiments with WordNet, we will briefly discuss other resources. For Freebase (Bollacker et al., 2008), we could replace the synsets with freebase units. Each unit has several aliases, e.g. Barack Obama, President Obama, Obama. The role of words in WordNet would correspond to these aliases in Freebase. This would give us the synset constraint, as well as the lexeme constraint of the system. Relations are given by freebase types; for example, we can add a constraint that entity embeddings should be similar to the type \"Presi-dent of the US.\" To explore multilingual word embeddings, we need the word embeddings of different languages to live in the same vector space."}, {"heading": "6 Related Work", "text": "There has been a resurgence of work on them lately (e.g., Bengio et al. (2003) Mnih and Hinton (2007), Collobert et al. (2011), Mikolov et al. (2013a), Pennington et al. (2014). These models produce only a single embedding for each word. All of them can also be used as input for AutoExtend.There are several approaches to finding embedding for senses, variously called meaning, meaning and multiple words embedding. Protects (1998) creates sense representations by clustering context representations derived from co-occurrence. Representation of a sense is simply the central word of its clusters. Huang et al. (2012) improves this by learning individual prototype embedding before performing literal discrimination on them. Bordes et al. (2011) creates similarity actions for relationships in relation to networks and energy in order to learn."}, {"heading": "7 Conclusion", "text": "We introduced AutoExtend, a flexible method for learning synthesis and lexicon embedding from Word embeddings. It is completely generic and can be used for any other type of embeddings and any other resource that imposes limitations on the relationship between words and other data types of a particular type. Our experimental results show that AutoExtend delivers state-of-the-art performance in terms of word similarity and meaning uniqueness. Along with this work, we will publish AutoExtend to extend word embeddings to other data types, the lexeme and synset embeddings used in the experiments, and the code required to replicate our WSD evaluation2."}, {"heading": "Acknowledgments", "text": "This work was partly funded by the German Research Foundation (DFG SCHU 2246 / 2-2) and we thank Christiane Fellbaum for the discussions leading up to this paper and the anonymous reviewers for their comments."}], "references": [{"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "Rejean Ducharme", "Pascal Vincent"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Knowledge-powered deep learning for word embedding", "author": ["Bian et al.2014] Jiang Bian", "Bin Gao", "Tie-Yan Liu"], "venue": "In Proceedings of ECML PKDD", "citeRegEx": "Bian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bian et al\\.", "year": 2014}, {"title": "A typology of ontology-based semantic measures", "author": ["Mounira Harzallah", "Henri Briand", "Pascale Kuntz"], "venue": "In Proceedings of EMOI - INTEROP", "citeRegEx": "Blanchard et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Blanchard et al\\.", "year": 2005}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of ACM SIGMOD", "citeRegEx": "Bollacker et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Learning structured embeddings of knowledge bases", "author": ["Jason Weston", "Ronan Collobert", "Yoshua Bengio"], "venue": "In Proceedings of AAAI", "citeRegEx": "Bordes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2011}, {"title": "Joint learning of words and meaning representations for opentext semantic parsing", "author": ["Xavier Glorot", "Jason Weston", "Yoshua Bengio"], "venue": "Proceedings of AISTATS", "citeRegEx": "Bordes et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2012}, {"title": "Multimodal distributional semantics", "author": ["Bruni et al.2014] Elia Bruni", "Nam Khanh Tran", "Marco Baroni"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bruni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "A unified model for word sense representation and disambiguation", "author": ["Chen et al.2014] Xinxiong Chen", "Zhiyuan Liu", "Maosong Sun"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of ICML", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "WordNet: An Electronic Lexical Database. Bradford Books", "author": ["Christiane Fellbaum"], "venue": null, "citeRegEx": "Fellbaum.,? \\Q1998\\E", "shortCiteRegEx": "Fellbaum.", "year": 1998}, {"title": "Placing search in context: The concept revisited", "author": ["Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": "Proceedings of WWW", "citeRegEx": "Finkelstein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Learning sense-specific word embeddings by exploiting bilingual resources", "author": ["Jiang Guo", "Wanxiang Che", "Haifeng Wang", "Ting Liu"], "venue": "In Proceedings of Coling,", "citeRegEx": "Guo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Using the structure of a conceptual network in computing semantic relatedness", "author": ["Iryna Gurevych"], "venue": "In IJCNLP", "citeRegEx": "Gurevych.,? \\Q2005\\E", "shortCiteRegEx": "Gurevych.", "year": 2005}, {"title": "Germanet-a lexical-semantic net for german", "author": ["Hamp et al.1997] Birgit Hamp", "Helmut Feldweg"], "venue": "In Proceedings of ACL,", "citeRegEx": "Hamp and Feldweg,? \\Q1997\\E", "shortCiteRegEx": "Hamp and Feldweg", "year": 1997}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Hill et al.2014] Felix Hill", "Roi Reichart", "Anna Korhonen"], "venue": "arXiv preprint arXiv:1408.3456", "citeRegEx": "Hill et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Huang et al.2012] Eric H Huang", "Richard Socher", "Christopher D Manning", "Andrew Y Ng"], "venue": "Proceedings of ACL", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "A convolutional neural network for modelling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": "In Proceedings of ACL", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "English lexical sample task description", "author": ["Adam Kilgarriff"], "venue": "In Proceedings of SENSEVAL-2", "citeRegEx": "Kilgarriff.,? \\Q2001\\E", "shortCiteRegEx": "Kilgarriff.", "year": 2001}, {"title": "Combining local context and wordnet similarity for word sense identification. WordNet: An electronic lexical database, 49(2):265\u2013283", "author": ["Leacock", "Chodorow1998] Claudia Leacock", "Martin Chodorow"], "venue": null, "citeRegEx": "Leacock et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Leacock et al\\.", "year": 1998}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Richard Socher", "Christopher D Manning"], "venue": "In Proceedings of CoNLL", "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "The senseval-3 english lexical sample task", "author": ["Timothy Chklovski", "Adam Kilgarriff"], "venue": "In Proceedings of SENSEVAL-3", "citeRegEx": "Mihalcea et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2004}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Exploiting similarities among languages for machine translation", "author": ["Quoc V Le", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1309.4168", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "Proceedings of NIPS", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Contextual correlates of semantic similarity", "author": ["Miller", "Charles1991] George A Miller", "Walter G Charles"], "venue": "Language and Cognitive Processes,", "citeRegEx": "Miller et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Miller et al\\.", "year": 1991}, {"title": "Three new graphical models for statistical language modelling", "author": ["Mnih", "Hinton2007] Andriy Mnih", "Geoffrey Hinton"], "venue": "In Proceedings of ICML", "citeRegEx": "Mnih et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2007}, {"title": "A scalable hierarchical distributed language model", "author": ["Mnih", "Hinton2009] Andriy Mnih", "Geoffrey E Hinton"], "venue": "In Proceedings of NIPS", "citeRegEx": "Mnih et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2009}, {"title": "Babelnet: Building a very large multilingual semantic network", "author": ["Navigli", "Ponzetto2010] Roberto Navigli", "Simone Paolo Ponzetto"], "venue": "In Proceedings of ACL", "citeRegEx": "Navigli et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Navigli et al\\.", "year": 2010}, {"title": "Efficient non-parametric estimation of multiple embeddings per word in vector space", "author": ["Jeevan Shankar", "Alexandre Passos", "Andrew McCallum"], "venue": "Proceedings of EMNLP", "citeRegEx": "Neelakantan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Multi-prototype vectorspace models of word meaning", "author": ["Reisinger", "Mooney2010] Joseph Reisinger", "Raymond J Mooney"], "venue": "In Proceedings of NAACL", "citeRegEx": "Reisinger et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Reisinger et al\\.", "year": 2010}, {"title": "Contextual correlates of synonymy", "author": ["Rubenstein", "John B Goodenough"], "venue": "Communications of the ACM,", "citeRegEx": "Rubenstein et al\\.,? \\Q1965\\E", "shortCiteRegEx": "Rubenstein et al\\.", "year": 1965}, {"title": "Learning representations by back-propagating errors", "author": ["Geoffrey E Hinton", "Ronald J Williams"], "venue": "Cognitive Modeling,", "citeRegEx": "Rumelhart et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "Automatic word sense discrimination", "author": ["Hinrich Sch\u00fctze"], "venue": "Computational Linguistics,", "citeRegEx": "Sch\u00fctze.,? \\Q1998\\E", "shortCiteRegEx": "Sch\u00fctze.", "year": 1998}, {"title": "A probabilistic model for learning multiprototype word embeddings", "author": ["Tian et al.2014] Fei Tian", "Hanjun Dai", "Jiang Bian", "Bin Gao", "Rui Zhang", "Enhong Chen", "Tie-Yan Liu"], "venue": "In Proceedings of Coling,", "citeRegEx": "Tian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tian et al\\.", "year": 2014}, {"title": "Verbs semantics and lexical selection", "author": ["Wu", "Palmer1994] Zhibiao Wu", "Martha Palmer"], "venue": "In Proceedings of ACL", "citeRegEx": "Wu et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Wu et al\\.", "year": 1994}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["Yu", "Dredze2014] Mo Yu", "Mark Dredze"], "venue": "In Proceedings of ACL", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Automatically creating datasets for measures of semantic relatedness", "author": ["Zesch", "Gurevych2006] Torsten Zesch", "Iryna Gurevych"], "venue": "In Proceedings of the Workshop on Linguistic Distances", "citeRegEx": "Zesch et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zesch et al\\.", "year": 2006}, {"title": "It makes sense: A wide-coverage word sense disambiguation system for free text", "author": ["Zhong", "Ng2010] Zhi Zhong", "Hwee Tou Ng"], "venue": "In Proceedings of ACL, System Demonstrations", "citeRegEx": "Zhong et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhong et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 30, "context": ", 2013c) and GloVe (Pennington et al., 2014).", "startOffset": 19, "endOffset": 44}, {"referenceID": 22, "context": "Examples for word embeddings are SENNA (Collobert and Weston, 2008), the hierarchical log-bilinear model (Mnih and Hinton, 2009), word2vec (Mikolov et al., 2013c) and GloVe (Pennington et al., 2014). However, there are many other resources that are undoubtedly useful in NLP, including lexical resources like WordNet and Wiktionary and knowledge bases like Wikipedia and Freebase. We will simply call these resources in the rest of the paper. Our goal is to enrich these valuable resources with embeddings for those data types that are not words; e.g., we want to enrich WordNet with embeddings for synsets and lexemes. A synset is a set of synonyms that are interchangeable in some context. A lexeme pairs a particular spelling or pronunciation with a particular meaning, i.e., a lexeme is a conjunction of a word and a synset. Our premise is that many NLP applications will benefit if the non-word data types of resources \u2013 e.g., synsets in WordNet \u2013 are also available as embeddings. For example, in machine translation, enriching and improving translation dictionaries (cf. Mikolov et al. (2013b)) would benefit from these embeddings because they would enable us to create an enriched dictionary for word senses.", "startOffset": 140, "endOffset": 1101}, {"referenceID": 10, "context": "We will focus on WordNet (Fellbaum, 1998) in this paper, but our method \u2013 based on a formalization that exploits the constraints of a resource for extending embeddings from words to other data types \u2013 is broadly applicable to other resources including Wikipedia and Freebase.", "startOffset": 25, "endOffset": 41}, {"referenceID": 22, "context": "Another motivation for our formalization stems from the analogy calculus developed by Mikolov et al. (2013a), which can be viewed as a group theory formalization of word relations: we have a set of elements (our vectors) and an operation (addition) satisfying the properties of a mathematical group, in particular, associativity and invertibility.", "startOffset": 86, "endOffset": 109}, {"referenceID": 17, "context": ", Kalchbrenner et al. (2014)).", "startOffset": 2, "endOffset": 29}, {"referenceID": 18, "context": "For WSD we use the shared tasks of Senseval2 (Kilgarriff, 2001) and Senseval-3 (Mihalcea et al.", "startOffset": 45, "endOffset": 63}, {"referenceID": 21, "context": "For WSD we use the shared tasks of Senseval2 (Kilgarriff, 2001) and Senseval-3 (Mihalcea et al., 2004) and a system named IMS (Zhong and Ng, 2010).", "startOffset": 79, "endOffset": 102}, {"referenceID": 16, "context": "We use SCWS (Huang et al., 2012) for the similarity evaluation.", "startOffset": 12, "endOffset": 32}, {"referenceID": 15, "context": "1 Huang et al. (2012) 62.", "startOffset": 2, "endOffset": 22}, {"referenceID": 15, "context": "1 Huang et al. (2012) 62.8\u2020 65.7\u2020 2 Tian et al. (2014) \u2013 65.", "startOffset": 2, "endOffset": 55}, {"referenceID": 15, "context": "1 Huang et al. (2012) 62.8\u2020 65.7\u2020 2 Tian et al. (2014) \u2013 65.4\u2020 3 Neelakantan et al. (2014) 67.", "startOffset": 2, "endOffset": 91}, {"referenceID": 7, "context": "3\u2020 4 Chen et al. (2014) 66.", "startOffset": 5, "endOffset": 24}, {"referenceID": 16, "context": "including (Huang et al., 2012) and (Tian et al.", "startOffset": 10, "endOffset": 30}, {"referenceID": 35, "context": ", 2012) and (Tian et al., 2014).", "startOffset": 12, "endOffset": 31}, {"referenceID": 16, "context": "Huang et al. (2012) for a similar point).", "startOffset": 0, "endOffset": 20}, {"referenceID": 3, "context": "For Freebase (Bollacker et al., 2008), we could replace the synsets with Freebase entities.", "startOffset": 13, "endOffset": 37}, {"referenceID": 0, "context": ", Bengio et al. (2003) Mnih and Hinton (2007), Collobert et al.", "startOffset": 2, "endOffset": 23}, {"referenceID": 0, "context": ", Bengio et al. (2003) Mnih and Hinton (2007), Collobert et al.", "startOffset": 2, "endOffset": 46}, {"referenceID": 0, "context": ", Bengio et al. (2003) Mnih and Hinton (2007), Collobert et al. (2011), Mikolov et al.", "startOffset": 2, "endOffset": 71}, {"referenceID": 0, "context": ", Bengio et al. (2003) Mnih and Hinton (2007), Collobert et al. (2011), Mikolov et al. (2013a), Pennington et al.", "startOffset": 2, "endOffset": 95}, {"referenceID": 0, "context": ", Bengio et al. (2003) Mnih and Hinton (2007), Collobert et al. (2011), Mikolov et al. (2013a), Pennington et al. (2014)).", "startOffset": 2, "endOffset": 121}, {"referenceID": 0, "context": ", Bengio et al. (2003) Mnih and Hinton (2007), Collobert et al. (2011), Mikolov et al. (2013a), Pennington et al. (2014)). These models produce only a single embedding for each word. All of them can be used as input for AutoExtend. There are several approaches to finding embeddings for senses, variously called meaning, sense and multiple word embeddings. Sch\u00fctze (1998) created sense representations by clustering context representations derived from co-occurrence.", "startOffset": 2, "endOffset": 372}, {"referenceID": 0, "context": ", Bengio et al. (2003) Mnih and Hinton (2007), Collobert et al. (2011), Mikolov et al. (2013a), Pennington et al. (2014)). These models produce only a single embedding for each word. All of them can be used as input for AutoExtend. There are several approaches to finding embeddings for senses, variously called meaning, sense and multiple word embeddings. Sch\u00fctze (1998) created sense representations by clustering context representations derived from co-occurrence. The representation of a sense is simply the centroid of its cluster. Huang et al. (2012) improved this by learning single-prototype embeddings before performing word sense discrimination on them.", "startOffset": 2, "endOffset": 557}, {"referenceID": 0, "context": ", Bengio et al. (2003) Mnih and Hinton (2007), Collobert et al. (2011), Mikolov et al. (2013a), Pennington et al. (2014)). These models produce only a single embedding for each word. All of them can be used as input for AutoExtend. There are several approaches to finding embeddings for senses, variously called meaning, sense and multiple word embeddings. Sch\u00fctze (1998) created sense representations by clustering context representations derived from co-occurrence. The representation of a sense is simply the centroid of its cluster. Huang et al. (2012) improved this by learning single-prototype embeddings before performing word sense discrimination on them. Bordes et al. (2011) created similarity measures for relations in WordNet and Freebase to learn entity embeddings.", "startOffset": 2, "endOffset": 685}, {"referenceID": 4, "context": "proposed by Bordes et al. (2012) to create disambiguated meaning embeddings and Neelakantan et al.", "startOffset": 12, "endOffset": 33}, {"referenceID": 4, "context": "proposed by Bordes et al. (2012) to create disambiguated meaning embeddings and Neelakantan et al. (2014) and Tian et al.", "startOffset": 12, "endOffset": 106}, {"referenceID": 4, "context": "proposed by Bordes et al. (2012) to create disambiguated meaning embeddings and Neelakantan et al. (2014) and Tian et al. (2014) extended the Skip-gram model (Mikolov et al.", "startOffset": 12, "endOffset": 129}, {"referenceID": 4, "context": "proposed by Bordes et al. (2012) to create disambiguated meaning embeddings and Neelakantan et al. (2014) and Tian et al. (2014) extended the Skip-gram model (Mikolov et al., 2013a) to learn multiple word embeddings. While these embeddings can correspond to different word senses, there is no clear mapping between them and a lexical resource like WordNet. Chen et al. (2014) also modified word2vec to learn sense embeddings, each corresponding to a WordNet synset.", "startOffset": 12, "endOffset": 376}, {"referenceID": 1, "context": "Bian et al. (2014) used not only semantic, but also morphological and syntactic knowledge to compute more effective word embeddings.", "startOffset": 0, "endOffset": 19}, {"referenceID": 12, "context": "Another interesting approach to create sense specific word embeddings uses bilingual resources (Guo et al., 2014).", "startOffset": 95, "endOffset": 113}, {"referenceID": 11, "context": "Other frequently used datasets are WordSim-353 (Finkelstein et al., 2001) or MEN (Bruni et al.", "startOffset": 47, "endOffset": 73}, {"referenceID": 6, "context": ", 2001) or MEN (Bruni et al., 2014).", "startOffset": 15, "endOffset": 35}, {"referenceID": 2, "context": "Examples include (Wu and Palmer, 1994) and (Leacock and Chodorow, 1998); Blanchard et al. (2005) give a good overview.", "startOffset": 73, "endOffset": 97}], "year": 2015, "abstractText": "We present AutoExtend, a system to learn embeddings for synsets and lexemes. It is flexible in that it can take any word embeddings as input and does not need an additional training corpus. The synset/lexeme embeddings obtained live in the same vector space as the word embeddings. A sparse tensor formalization guarantees efficiency and parallelizability. We use WordNet as a lexical resource, but AutoExtend can be easily applied to other resources like Freebase. AutoExtend achieves state-of-the-art performance on word similarity and word sense disambiguation tasks.", "creator": "LaTeX with hyperref package"}}}