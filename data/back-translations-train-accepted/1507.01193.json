{"id": "1507.01193", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jul-2015", "title": "Dependency Recurrent Neural Language Models for Sentence Completion", "abstract": "Recent work on language modelling has shifted focus from count-based models to neural models. In these works, the words in each sentence are always considered in a left-to-right order. In this paper we show how we can improve the performance of the recurrent neural network (RNN) language model by incorporating the syntactic dependencies of a sentence, which have the effect of bringing relevant contexts closer to the word being predicted. We evaluate our approach on the Microsoft Research Sentence Completion Challenge and show that the dependency RNN proposed improves over the RNN by about 10 points in accuracy. Furthermore, we achieve results comparable with the state-of-the-art models on this task.", "histories": [["v1", "Sun, 5 Jul 2015 11:10:24 GMT  (190kb,D)", "http://arxiv.org/abs/1507.01193v1", "Accepted for publication at ACL 2015"]], "COMMENTS": "Accepted for publication at ACL 2015", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["piotr mirowski", "andreas vlachos"], "accepted": true, "id": "1507.01193"}, "pdf": {"name": "1507.01193.pdf", "metadata": {"source": "CRF", "title": "Dependency Recurrent Neural Language Models for Sentence Completion", "authors": ["Piotr Mirowski", "Andreas Vlachos"], "emails": ["piotr.mirowski@computer.org", "a.vlachos@cs.ucl.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "They are an essential building block in a variety of applications such as machine translation, speech recognition, and grammatical error correction, so the standard method of evaluating a language model was to calculate its perplexity on a large corpus. However, this evaluation assumes that the output of the language model is probabilistic, and it has been observed that perplexity does not always correlate with the downstream task. For these reasons, Zweig and Burges (2012) proposed the Sentence Completion Challenge, in which the task is to select the correct word from five candidates. Performance is evaluated by accuracy (how many sentences have been correctly completed), i.e. both probable and non-probable models (e.g. Roarket al. (2007) Recursive approaches to this task include both neural and linguistic models."}, {"heading": "2 Dependency Recurrent Neural Network", "text": "The hidden representation s (t) for the word in the position of the word in the RNN follows a first order of auto-regressive dynamics (Eq. 1), in which W is capable of the hidden representation of the previous word (t \u2212 1), the hidden representation s (t) for the word in the position of the word in the RNN follows a first order of auto-regressive dynamics (Eq. 1), in which we join the matrix."}, {"heading": "3 Labelled Dependency RNN", "text": "To this end, we have adapted the context-dependent characteristics of the RNN (Mikolov and Branch, 2012) to be used as additional M - dimensional markers f (t). These characteristics require a matrix F that combines markers with word vectors, creating a new dynamic model (Equation 5) in the RNN, and a matrix G that combines markers with output word probabilities. The complete model is as follows: s (t) = f (Ws \u2212 1) + Uw (t) + Ff (t) (5) y (t) = g (Vs (t) + Gf (t) + dh (wtt \u2212 n + 1))) (6) Our training dataset finds the dependence on the model M = 44 unique markers (e.g. nsubj, det or prep)."}, {"heading": "4 Implementation and Dataset", "text": "We modified the feature augmented RNN Toolkit2 and adapted it to deal with tree-structured data. Specifically, instead of running sequentially on the entire training corpus, the RNN will run on all word marks in all rolls of all sentences in all books in the corpus. Specifically, the RNN will be reset at the beginning of each roll of a sentence. In calculating the record probability of a sentence, the contribution of each word token is counted only once (and stored in a hash table specific to that sentence). Once all rolls of a sentence are processed, the record probability of the sentence is the sum of the per-token log probabilities in that hash table. Furthermore, we will run http: / / research.microsoft.com / en-us / projects / rnn / enhanced the RNN library by replacing some large matrix multiplication routines with requests to the CLAS library."}, {"heading": "5 Results", "text": "Table 1 shows the accuracy (validation and test sets) achieved with a simple RNN with 50, 100, 200 and 300-dimensional hidden word representation and 250 frequency-based word classes (word size N = 72846 words occurring at least 5 times in the training corpus). It is noted that adding the direct word context to word combinations (using the additional matrix described in Section 2) allows a leap from poor performance of about 30% accuracy to about 40% accuracy, which is essentially consistent with the 39% accuracy reported for good-turning-gram language models in Zweig et al. (2012). Modelling 4-grams yields even better results, closer to the 45% accuracy reported for RNNs in (Zweig et al., 2012). 4As Table 2 shows, dependence on RNNNN (depRNN) language models allowing a dependence on 5% and 5% word accuracy over NN is likely to improve RNN accuracy."}, {"heading": "6 Discussion", "text": "In fact, we are able to go in search of a solution that enables us, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in the position we are in. \""}, {"heading": "7 Conclusions", "text": "In this paper, we proposed a novel language model, RNN dependence, which includes syntactic dependencies in the RNN formulation. We examined its performance in the task of completing the sentence of the MSR and showed that it improves on RNN by 10 points in accuracy, while achieving results comparable to the state of the art. Further work includes extending the language modeling of the dependency tree to Long Short-Term Memory RNN to handle longer syntactic dependencies."}, {"heading": "Acknowledgements", "text": "We thank our anonymous reviewers for their valuable feedback. PM also thanks Geoffrey Zweig, Daniel Voinea, Francesco Nidito and Davide di Gennaro for sharing the original FeatureAugmented RNN toolkit on the Microsoft Research website and for insights into this code, as well as Bhaskar Mitra, Milad Shokouhi and Andriy Mnih for discussing word embedding and sentence completion."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "Recent work on language modelling has shifted focus from count-based models to neural models. In these works, the words in each sentence are always considered in a left-to-right order. In this paper we show how we can improve the performance of the recurrent neural network (RNN) language model by incorporating the syntactic dependencies of a sentence, which have the effect of bringing relevant contexts closer to the word being predicted. We evaluate our approach on the Microsoft Research Sentence Completion Challenge and show that the dependency RNN proposed improves over the RNN by about 10 points in accuracy. Furthermore, we achieve results comparable with the stateof-the-art models on this task.", "creator": "LaTeX with hyperref package"}}}