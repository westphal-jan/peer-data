{"id": "1507.01526", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jul-2015", "title": "Grid Long Short-Term Memory", "abstract": "This paper introduces Grid Long Short-Term Memory, a network of LSTM cells arranged in a multidimensional grid that can be applied to vectors, sequences or higher dimensional data such as images. The network differs from existing deep LSTM architectures in that the cells are connected between network layers as well as along the spatiotemporal dimensions of the data. It therefore provides a unified way of using LSTM for both deep and sequential computation. We apply the model to algorithmic tasks such as integer addition and determining the parity of random binary vectors. It is able to solve these problems for 15-digit integers and 250-bit vectors respectively. We then give results for three empirical tasks. We find that 2D Grid LSTM achieves 1.47 bits per character on the Wikipedia character prediction benchmark, which is state-of-the-art among neural approaches. We also observe that a two-dimensional translation model based on Grid LSTM outperforms a phrase-based reference system on a Chinese-to-English translation task, and that 3D Grid LSTM yields a near state-of-the-art error rate of 0.32% on MNIST.", "histories": [["v1", "Mon, 6 Jul 2015 16:30:05 GMT  (442kb,D)", "http://arxiv.org/abs/1507.01526v1", "14 pages"], ["v2", "Fri, 20 Nov 2015 17:40:17 GMT  (444kb,D)", "http://arxiv.org/abs/1507.01526v2", "15 pages"], ["v3", "Thu, 7 Jan 2016 18:39:48 GMT  (444kb,D)", "http://arxiv.org/abs/1507.01526v3", "15 pages"]], "COMMENTS": "14 pages", "reviews": [], "SUBJECTS": "cs.NE cs.CL cs.LG", "authors": ["nal kalchbrenner", "ivo danihelka", "alex graves"], "accepted": true, "id": "1507.01526"}, "pdf": {"name": "1507.01526.pdf", "metadata": {"source": "CRF", "title": "Grid Long Short-Term Memory", "authors": ["Nal Kalchbrenner", "Ivo Danihelka", "Google DeepMind", "Alex Graves"], "emails": [], "sections": [{"heading": null, "text": "This paper introduces Grid Long Short-Term Memory, a network of LSTM cells arranged in a multi-dimensional grid that can be applied to vectors, sequences, or higher-dimensional data such as images. It differs from existing deep LSTM architectures in that the cells are interconnected both between network layers and along the spatio-temporal dimensions of the data, providing a unified way to use LSTM for both deep and sequential computations. We find that 2D Grid LSTM achieves 1.47 bits per character on the Wikipedia character prediction standard, which is state-of-the-art among neural approaches. We also find that a two-dimensional translation model is based on Grid LSTM system that exceeds the Chinese reference size STM-T close to the three-dimensional translation rate, which is based on a multi-dimensional grid that can be applied to vectors, or higher dimensional data."}, {"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Background", "text": "We start with the description of the standard LSTM recursive neural network and the stacked and multidimensional LSTM networks derived from it; some aspects of these networks motivate the Grid-LSTM."}, {"heading": "2.1 Long Short-Term Memory", "text": "The LSTM network processes a sequence of input and target pairs (x1, y1),..., (xm, ym). For each pair (xi, yi), the LSTM network takes the new input xi and creates an estimate for the target yi taking into account all previous inputs x1,..., xi. the past inputs x1,..., xi \u2212 1 determine the state of the network, which consists of a hidden vector h-Rd and a memory vector m-Rd."}, {"heading": "2d Grid LSTMStacked LSTM", "text": "Each step is defined as follows [14]: gu = \u03c3 (Wu \u0445 H) gf = \u03c3 (Wf \u0445 H) go = \u03c3 (Where \u0445 H) gc = tanh (Wc \u0445 H) m \u2032 = gf m + gu gch \u2032 = tanh (go m \u2032) (1), where \u03c3 is the logistic sigmoid function, Wu, Wf, Wo, Wc in Rd \u00b7 2d are the recurring weight matrices of the network and H \u0445 R2d is the concatenation of the new input xi, transformed by a projection matrix I, and the previous hidden vector h: H = [I \u0445 xi h] (2) The calculation prints new hidden and storage vectors h \u00b2 and m \u00b2, which include the next state of the network. The estimate for the target is then calculated in relation to the hidden vector h \u00b2 and the hidden vector h \u00b2. We use the functional vector (STvM) as abbreviation for Eq."}, {"heading": "2.2 Stacked LSTM", "text": "A model closely related to the standard LSTM network is Stacked LSTM [14, 35]. Stacked LSTM increases capacity by stacking LSTM layers on top of each other. The hidden vector hi inEq. 1 from the LSTM below is taken as input to the LSTM above instead of I \u043a xi. Stacked LSTM is shown in Figure 2. Note that while the LSTM cells are present along the sequential computation of each LSTM network, they are not present in the vertical computation from one layer to the next."}, {"heading": "2.3 Multidimensional LSTM", "text": "Another related model is multidimensional LSTM [13]. Here the inputs are not arranged in an order, but in a N-dimensional grid, like the two-dimensional grid of pixels in an image. At each input x in the array, the network receives N hidden vectors h1,..., hN and N memory vectors m1,..., mN and calculates a hidden vector h and a memory vector m, which are passed as the next state for each of the N dimensions. The network concatenates the transformed input I \u0445 x and the N hidden vectors h1,..., hN into a vector H and calculates as in Equation 1 gu, go and gc, as well as N forget gates gfi. These gates are then used to calculate the memory vector as follows: m = N \u2211 i gfi mi + gu gc (4) Since the number of paths in a grid can be combined with the size of each dimension and the total number of values of the N can increase the input dimensions for this d."}, {"heading": "3 Architecture", "text": "Grid LSTM uses cells along one or all dimensions, including the depth of the network. In the context of predicting a sequence, the grid LSTM has cells along two dimensions, the time of the sequence itself and the vertical along the depth. To modulate the interaction of the cells in the two dimensions, the grid LSTM proposes a simple mechanism in which the values in the cells cannot grow combinatorial as in Equation 4. In this section, we describe the multidimensional blocks and the way they are combined to form a grid LSTM."}, {"heading": "3.1 Grid LSTM Blocks", "text": "As in the multidimensional LSTM, a N-dimensional block in a grid LSTM receives as input N hidden vectors h1,..., hN andN memory vectors m1,..., mN. In contrast to the multidimensional case, the block N returns hidden vectors h \u2032 1,..., h \u2032 N and N memory vectors m \u2032 1,..., m \u2032 N, which are all different. Calculation is simple and proceeds as follows. First, the model concatenates the hidden input vectors from the N dimensions: H = h1... hN (5) Then the block N computes LSTM (\u00b7, \u00b7, \u00b7), one for each dimension and gets the desired output hidden and memory vectors: (h \u2032 1, m \u2032 1) = LSTM (H, m1, W1)... (h \u2032 N, m \u2032 N) = LSTM (H, mN, WN)."}, {"heading": "3.2 Priority Dimensions", "text": "In a N-dimensional block, the transformations for all dimensions are calculated in parallel, but it can be useful for one dimension to know the outputs of the transformations from the other dimensions, especially if the outgoing vectors from that dimension are used to estimate the target. For example, to prioritize the first dimension of the network, the block first computes the N-1 transformations for the other dimensions, thus obtaining the output-hidden vectors h \u00b2 2,..., h \u2032 N. Then, the block concatenates these output-hidden vectors and the entered hidden vector h1 for the first dimension to a new vector H \u2032 as follows: H \u2032 = h1 h \u2032 2... h \u2032 N (7) The vector is then used in the final transformation to obtain the prioritized output-hidden and memory vectors h \u00b2 1 and m \u00b2 1."}, {"heading": "3.3 Non-LSTM Dimensions", "text": "In grid LSTM networks, which have only a few blocks along a certain dimension in the grid, it can be useful to simply have regular connections along that dimension without the use of cells. This, of course, can be achieved within the block by using for this dimension in Equation 6 a simple transformation with a nonlinear activation function instead of the transformation LSTM (\u00b7, \u00b7, \u00b7). Given a weight matrix V-Rd-Nd, this looks for the first dimension as follows: h \u2032 1 = (V-H) (8), where \u03b1 is a standard nonlinear transmission function or simply the identity. This allows us to see how, modulo the differences in mechanism within the blocks, grid LSTM networks correspond to the models in Section 2. A 2d LSTM that is applied to time sequences with cells in the time dimension, but not in the vertical depth dimension that stacked LSTM corresponds to."}, {"heading": "3.4 Inputs from Multiple Sides", "text": "If we imagine a N-dimensional block as in Fig. 1, we see that N of the sides of the block have input vectors connected to them, and the other N-side output vectors. As the blocks are arranged in a grid, this separation extends to the grid as a whole; each side of the grid has either input or output vectors connected to it. For certain tasks that have inputs of various kinds, a model can take advantage of this separation by projecting any kind of input onto a different side of the grid. The mechanism within the blocks ensures that the hidden and memory vectors from the different sides interact closely without being confused, as is the case in the neural translation model introduced in Fig. 4, in which source and target words are projected onto two different sides of a grid LSTM."}, {"heading": "3.5 Weight Sharing", "text": "The distribution of weight matrices can be specified along each dimension in a grid LSTM, and it can be useful to induce invariance in the calculation along this dimension. As in the translation and image models, if multiple sides of a grid must share weights, capacity can be added to the model by introducing a new dimension into the grid without dividing the weights."}, {"heading": "4 Experiments", "text": "We first report on the results of the three algorithmic tasks and then focus on the three empirical tasks, which include character prediction, translation and numerical image classification."}, {"heading": "4.1 Parity", "text": "In fact, most of them will be able to go in search of a solution that has its origin in the real world, but not in the real world, but in the real world and in the real world in which they live."}, {"heading": "4.2 Addition", "text": "Next, we experiment with 2 LSTM networks to sum up two 15-digit integers; the problem formulation is similar to that in [39], where each number is assigned a digit to the network and the result is also predicted by a digit. Input numbers are separated by separator symbols, and an end-of-results symbol is predicted by the network; these symbols, as well as input and destination pads, are indicated by \u2212. An example is as follows: \u2212 1 2 3-8 9 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 Unlike the work in [39], which assumes 4 to 9 digits for the input integers, we use the digits to 15, we do not use curriculum learning strategies, and we do not re-insert the digits from the partially predicted networks into the network and force the network to remember its partial forecasts."}, {"heading": "4.3 Memorization", "text": "For our third algorithmic task, we analyze the performance of 2-LSTM networks in the midst of the task of storing a random sequence of symbols; the sequences are 20 symbols long and we use a vocabulary of 64 symbols encoded as one-hot vectors, giving the network one symbol per step; the structure is similar to that of the complement above; the network is tasked with reading the entire sequence and leaving the same sequence unchanged: \u2212 \u03b2 \u03b2 \u2212 \u2212 \u03b1 \u03b2 \u03b2 \u03b3 - Since the sequences are generated randomly, there is no correlation between consecutive symbols and the network, the whole sequence must be memorized without compression; we train 2-LSTM and stacked LSTM with either bound or unbound weights on the memory task; all networks have unbound units and have between 1 and 50 layers; we use size 15 mini-batches and optimize the network with Adam and a learning rate of 0.0001."}, {"heading": "4.4 Character-Level Language Modelling", "text": "Next, we test the 2-LSTM network on the Hutter dataset [22], with the goal of successively predicting the next character in the corpus.The dataset consists of 100 million characters. We follow the splitting procedure of [4], in which the last 5 million characters are used for testing.The alphabet consists of a total of 205 characters. We use a bound 2-LSTM with 1000 hidden units and 6 layers of depth.As in Fig. 2 and in the previous tasks, the characters are projected for both the initial hidden input and for the cell vectors.The top Softmax layer is connected to the top hidden and cell vectors.The model has a total of 2000 x 4000 + 205 x 4 x 1000 = 8.82 x 106 parameters. As usual, the goal is to minimize the negative log probability of the character sequence and increase the number of character sequences under the model.The training is performed by processing sequences of 10000 characters in the sequence of the 10000 sequence."}, {"heading": "4.5 Translation", "text": "This year is the highest in the history of the country."}, {"heading": "4.6 MNIST Digit Recognition", "text": "In fact, most of them are able to abide by the rules which they have imposed on themselves, and that they are able to abide by the rules which they have imposed on themselves. (...) It is not so that they abide by the rules. (...) It is not so that they abide by the rules. (...) It is as if they abide by the rules. (...) It is as if they abide by the rules. (...) It is as if they abide by the rules. (...) It is as if they abide by the rules. (...) It is as if they abide by the rules. (...) It is as if they abide by the rules. (...) (...) (...) () (...) () (() () (() (()) (()) ((()) () (()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () ()) () () () () () () () () () () () () () () () ()) () () () () ()) () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () () (() (() () () () (() (() (() ((() (() () () (() () (((() () (() () () (() (() () (((() (() (() () ((() () () (() ((() ((() (() () () ((("}, {"heading": "5 Conclusion", "text": "We have introduced Grid LSTM, a network that uses LSTM cells in all dimensions and modulates versatile interaction in a novel way. We have recognized the advantages of cells over regular connections in solving tasks such as parity, addition and memorization. We have described powerful and flexible methods to apply the model to character prediction, machine translation and image classification, with overall strong performance."}, {"heading": "Acknowledgements", "text": "We thank Koray Kavukcuoglu, Razvan Pascanu, Ilya Sutskever and Oriol Vinyals for helpful comments and discussions."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "translation. CoRR,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Gated feedback recurrent neural networks", "author": ["Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1502.02367,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Multi-column deep neural networks for image classification", "author": ["Dan Claudiu Ciresan", "Ueli Meier", "J\u00fcrgen Schmidhuber"], "venue": "In arXiv:1202.2745v1 [cs.CV],", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Technical Report UCB/EECS-2010-24, EECS Department,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models", "author": ["Chris Dyer", "Adam Lopez", "Juri Ganitkevitch", "Johnathan Weese", "Ferhan Ture", "Phil Blunsom", "Hendra Setiawan", "Vladimir Eidelman", "Philip Resnik"], "venue": "In Proceedings of the Association for Computational Linguistics (ACL),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Fractional max-pooling", "author": ["Benjamin Graham"], "venue": "CoRR, abs/1412.6071,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Spatially-sparse convolutional neural networks", "author": ["Benjamin Graham"], "venue": "CoRR, abs/1409.6070,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Supervised sequence labelling with recurrent neural networks, volume 385", "author": ["A. Graves"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Multi-dimensional recurrent neural networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "J. Schmidhuber"], "venue": "In Proceedings of the 2007 International Conference on Artificial Neural Networks,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. Mohamed", "G. Hinton"], "venue": "In Proc ICASSP", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Offline handwriting recognition with multidimensional recurrent neural networks", "author": ["A. Graves", "J. Schmidhuber"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "CoRR, abs/1308.0850,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "LSTM: A search space odyssey", "author": ["Klaus Greff", "Rupesh Kumar Srivastava", "Jan Koutn\u0131\u0301k", "Bas R. Steunebrink", "J\u00fcrgen Schmidhuber"], "venue": "CoRR, abs/1503.04069,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Untersuchungen zu dynamischen neuronalen Netzen", "author": ["S. Hochreiter"], "venue": "Diploma thesis, Institut fu\u0308r Informatik, Lehrstuhl Prof. Brauer, Technische Universita\u0308t Mu\u0308nchen,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1991}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies", "author": ["S. Hochreiter", "Y. Bengio", "P. Frasconi", "J. Schmidhuber"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2001}, {"title": "Long Short-Term Memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1997}, {"title": "Solving the n-bit parity problem using neural networks", "author": ["Myron E. Hohil", "Derong Liu", "Stanley H. Smith"], "venue": "Neural Networks,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1999}, {"title": "The human knowledge compression", "author": ["Marcus Hutter"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom"], "venue": "Seattle, October", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["Ryan Kiros", "Ruslan Salakhutdinov", "Richard S. Zemel"], "venue": "CoRR, abs/1411.2539,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1998}, {"title": "Deeply-supervised nets", "author": ["Chen-Yu Lee", "Saining Xie", "Patrick Gallagher", "Zhengyou Zhang", "Zhuowen Tu"], "venue": "In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Convolutional kernel networks", "author": ["Julien Mairal", "Piotr Koniusz", "Za\u0131\u0308d Harchaoui", "Cordelia Schmid"], "venue": "Neural Information Processing Systems,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Perceptrons: An Introduction to Computational Geometry", "author": ["Seymour Papert Marvin Minsky"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1972}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E. Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "Best practices for convolutional neural networks applied to visual document analysis", "author": ["Patrice Y. Simard", "David Steinkraus", "John C. Platt"], "venue": "In 7th International Conference on Document Analysis and Recognition (ICDAR", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2003}, {"title": "Generating text with recurrent neural networks", "author": ["I. Sutskever", "J. Martens", "G. Hinton"], "venue": "In ICML,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "arXiv preprint arXiv:1411.4555,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "Renet: A recurrent neural network based alternative to convolutional networks", "author": ["Francesco Visin", "Kyle Kastner", "Kyunghyun Cho", "Matteo Matteucci", "Aaron C. Courville", "Yoshua Bengio"], "venue": "CoRR, abs/1505.00393,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "Regularization of neural networks using dropconnect", "author": ["Li Wan", "Matthew D. Zeiler", "Sixin Zhang", "Yann LeCun", "Rob Fergus"], "venue": "In ICML (3),", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2013}, {"title": "Learning to execute", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "CoRR, abs/1410.4615,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2014}], "referenceMentions": [{"referenceID": 17, "context": "Long Short-Term Memory (LSTM) networks are recurrent neural networks equipped with a special gating mechanism that controls access to memory cells [20].", "startOffset": 147, "endOffset": 151}, {"referenceID": 11, "context": "These properties allow LSTM networks to process data with complex and separated interdependencies and to excel in a range of sequence learning domains such as speech recognition [14], offline hand-writing recognition [15], machine translation [35] and image-to-caption generation [36, 25].", "startOffset": 178, "endOffset": 182}, {"referenceID": 12, "context": "These properties allow LSTM networks to process data with complex and separated interdependencies and to excel in a range of sequence learning domains such as speech recognition [14], offline hand-writing recognition [15], machine translation [35] and image-to-caption generation [36, 25].", "startOffset": 217, "endOffset": 221}, {"referenceID": 30, "context": "These properties allow LSTM networks to process data with complex and separated interdependencies and to excel in a range of sequence learning domains such as speech recognition [14], offline hand-writing recognition [15], machine translation [35] and image-to-caption generation [36, 25].", "startOffset": 243, "endOffset": 247}, {"referenceID": 31, "context": "These properties allow LSTM networks to process data with complex and separated interdependencies and to excel in a range of sequence learning domains such as speech recognition [14], offline hand-writing recognition [15], machine translation [35] and image-to-caption generation [36, 25].", "startOffset": 280, "endOffset": 288}, {"referenceID": 22, "context": "These properties allow LSTM networks to process data with complex and separated interdependencies and to excel in a range of sequence learning domains such as speech recognition [14], offline hand-writing recognition [15], machine translation [35] and image-to-caption generation [36, 25].", "startOffset": 280, "endOffset": 288}, {"referenceID": 15, "context": "Deep networks suffer from exactly the same problems as recurrent networks applied to long sequences: namely that information from past computations rapidly attenuates as it progresses through the chain \u2013 the vanishing gradient problem [18] \u2013 and that each layer cannot dynamically select or ignore its inputs.", "startOffset": 235, "endOffset": 239}, {"referenceID": 27, "context": "One-dimensional Grid LSTM corresponds to a feed-forward network that uses LSTM cells in place of transfer functions such as tanh and ReLU [31].", "startOffset": 138, "endOffset": 142}, {"referenceID": 11, "context": "Grid LSTM with three or more dimensions is analogous to Multidimensional LSTM [14, 35, 13, 12], but differs from it not just by having the cells along the depth dimension, but also by using the proposed mechanism for modulating the N-way interaction that is not prone to the instability present in Multidimesional LSTM.", "startOffset": 78, "endOffset": 94}, {"referenceID": 30, "context": "Grid LSTM with three or more dimensions is analogous to Multidimensional LSTM [14, 35, 13, 12], but differs from it not just by having the cells along the depth dimension, but also by using the proposed mechanism for modulating the N-way interaction that is not prone to the instability present in Multidimesional LSTM.", "startOffset": 78, "endOffset": 94}, {"referenceID": 10, "context": "Grid LSTM with three or more dimensions is analogous to Multidimensional LSTM [14, 35, 13, 12], but differs from it not just by having the cells along the depth dimension, but also by using the proposed mechanism for modulating the N-way interaction that is not prone to the instability present in Multidimesional LSTM.", "startOffset": 78, "endOffset": 94}, {"referenceID": 9, "context": "Grid LSTM with three or more dimensions is analogous to Multidimensional LSTM [14, 35, 13, 12], but differs from it not just by having the cells along the depth dimension, but also by using the proposed mechanism for modulating the N-way interaction that is not prone to the instability present in Multidimesional LSTM.", "startOffset": 78, "endOffset": 94}, {"referenceID": 26, "context": "When trained on the highly non-linear parity function of k-bit strings [30, 6], one-dimensional Grid LSTM networks find solutions for up to k = 250 input bits, whereas feed-forward networks equipped with other transfer functions find solutions only up to k = 30 bits.", "startOffset": 71, "endOffset": 78}, {"referenceID": 34, "context": "We compare the performance of two-dimensional Grid LSTM to Stacked LSTM on computing the addition of two 15-digit integers without curriculum learning and on memorizing sequences of numbers [39].", "startOffset": 190, "endOffset": 194}, {"referenceID": 19, "context": "47 bits-per-character in the 100M characters Wikipedia dataset [22] outperforming other neural networks.", "startOffset": 63, "endOffset": 67}, {"referenceID": 6, "context": "The network outperforms the reference phrase-based CDEC system [8] on the IWSLT BTEC Chinese-to-Ensligh translation task.", "startOffset": 63, "endOffset": 66}, {"referenceID": 11, "context": "each step is defined as follows [14]: g = \u03c3(W \u2217H) g = \u03c3(W \u2217H) g = \u03c3(W \u2217H) g = tanh(W \u2217H) m\u2032 = g m+ g g h\u2032 = tanh(g m\u2032) (1)", "startOffset": 32, "endOffset": 36}, {"referenceID": 16, "context": "Each memory vector is obtained by a linear transformation of the previous memory vector and the gates; this ensures that the forward signals from one step to the other are not repeatedly squashed by a non-linearity such as tanh and that the backward error signals do not decay sharply at each step, an issue known as the vanishing gradient problem [19].", "startOffset": 348, "endOffset": 352}, {"referenceID": 11, "context": "A model that is closely related to the standard LSTM network is Stacked LSTM [14, 35].", "startOffset": 77, "endOffset": 85}, {"referenceID": 30, "context": "A model that is closely related to the standard LSTM network is Stacked LSTM [14, 35].", "startOffset": 77, "endOffset": 85}, {"referenceID": 10, "context": "Another related model is Multidimensional LSTM [13].", "startOffset": 47, "endOffset": 51}, {"referenceID": 18, "context": "Although manually crafted neural networks for the problem have been devised [21], training a generic neural network from a finite number of examples and a generic", "startOffset": 76, "endOffset": 80}, {"referenceID": 26, "context": "random initialization of the weights to successfully learn to compute the parity of k-bit strings for significant values of k is a longstanding problem [30, 6].", "startOffset": 152, "endOffset": 159}, {"referenceID": 5, "context": "06 [7].", "startOffset": 3, "endOffset": 6}, {"referenceID": 34, "context": "The problem formulation is similar to that in [39], where each number is given to the network one digit at a time and the result is also predicted one digit at a time.", "startOffset": 46, "endOffset": 50}, {"referenceID": 34, "context": "Contrary to the work in [39] that uses from 4 to 9 digits for the input integers, we fix the number of digits to 15, we do not use curriculum learning strategies and we do not put digits from the partially predicted output back into the network, forcing the network to remember its partial predictions and making the task more challenging.", "startOffset": 24, "endOffset": 28}, {"referenceID": 21, "context": "001 [24].", "startOffset": 4, "endOffset": 8}, {"referenceID": 13, "context": "BPC Parameters Alphabet Size Test data Stacked LSTM [16] 1.", "startOffset": 52, "endOffset": 56}, {"referenceID": 29, "context": "67 27M 205 last 4MB MRNN [34] 1.", "startOffset": 25, "endOffset": 29}, {"referenceID": 3, "context": "9M 86 last 10MB GFRNN [4] 1.", "startOffset": 22, "endOffset": 25}, {"referenceID": 19, "context": "We next test the 2-LSTM network on the Hutter challenge Wikipedia dataset [22].", "startOffset": 74, "endOffset": 78}, {"referenceID": 3, "context": "We follow the splitting procedure of [4], where the last 5 million characters are used for testing.", "startOffset": 37, "endOffset": 40}, {"referenceID": 6, "context": "Test CDEC [8] 50.", "startOffset": 10, "endOffset": 13}, {"referenceID": 20, "context": "In the neural approach to machine translation one trains a neural network end-to-end to map the source sentence to the target sentence [23, 35, 3].", "startOffset": 135, "endOffset": 146}, {"referenceID": 30, "context": "In the neural approach to machine translation one trains a neural network end-to-end to map the source sentence to the target sentence [23, 35, 3].", "startOffset": 135, "endOffset": 146}, {"referenceID": 2, "context": "In the neural approach to machine translation one trains a neural network end-to-end to map the source sentence to the target sentence [23, 35, 3].", "startOffset": 135, "endOffset": 146}, {"referenceID": 0, "context": "This issue can be alleviated by a soft attention mechanism in the decoder neural network that uses gates to focus on specific parts of the source sentence [1].", "startOffset": 155, "endOffset": 158}, {"referenceID": 0, "context": "Note that, like the attention-based model [1], the two-dimensional translation model has complexity O(nm), where n andm are respectively the length of the source and target; by contrast the recurrent encoderdecoder model only has complexity O(m+ n).", "startOffset": 42, "endOffset": 45}, {"referenceID": 6, "context": "We use as baseline the state-of-the-art hierarchical phrase-based system CDEC [8].", "startOffset": 78, "endOffset": 81}, {"referenceID": 23, "context": "Like in a convolutional neural network [26], the same three-way transform of the 3-LSTM is applied at all parts of the grid, ensuring that the same features can be extracted across all parts of the input image.", "startOffset": 39, "endOffset": 43}, {"referenceID": 9, "context": "The setup has some similarity with the original application of Multidimensional LSTM to images [12] and with the recently described ReNet architecture [37].", "startOffset": 95, "endOffset": 99}, {"referenceID": 32, "context": "The setup has some similarity with the original application of Multidimensional LSTM to images [12] and with the recently described ReNet architecture [37].", "startOffset": 151, "endOffset": 155}, {"referenceID": 33, "context": "[38] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "28 Graham [11] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 4, "context": "[5] 0.", "startOffset": 0, "endOffset": 3}, {"referenceID": 25, "context": "[29] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[27] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[32] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "4 Graham [10] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 32, "context": "[37] 0.", "startOffset": 0, "endOffset": 4}], "year": 2015, "abstractText": "This paper introduces Grid Long Short-Term Memory, a network of LSTM cells arranged in a multidimensional grid that can be applied to vectors, sequences or higher dimensional data such as images. The network differs from existing deep LSTM architectures in that the cells are connected between network layers as well as along the spatiotemporal dimensions of the data. It therefore provides a unified way of using LSTM for both deep and sequential computation. We apply the model to algorithmic tasks such as integer addition and determining the parity of random binary vectors. It is able to solve these problems for 15-digit integers and 250-bit vectors respectively. We then give results for three empirical tasks. We find that 2D Grid LSTM achieves 1.47 bits per character on the Wikipedia character prediction benchmark, which is state-of-the-art among neural approaches. We also observe that a two-dimensional translation model based on Grid LSTM outperforms a phrase-based reference system on a Chinese-to-English translation task, and that 3D Grid LSTM yields a near state-of-the-art error rate of 0.32% on MNIST.", "creator": "LaTeX with hyperref package"}}}