{"id": "1507.01784", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jul-2015", "title": "Rethinking LDA: Moment Matching for Discrete ICA", "abstract": "We consider moment matching techniques for estimation in Latent Dirichlet Allocation (LDA). By drawing explicit links between LDA and discrete versions of independent component analysis (ICA), we first derive a new set of cumulant-based tensors, with an improved sample complexity. Moreover, we reuse standard ICA techniques such as joint diagonalization of tensors to improve over existing methods based on the tensor power method. In an extensive set of experiments on both synthetic and real datasets, we show that our new combination of tensors and orthogonal joint diagonalization techniques outperforms existing moment matching methods.", "histories": [["v1", "Tue, 7 Jul 2015 12:48:30 GMT  (122kb,D)", "http://arxiv.org/abs/1507.01784v1", "27 pages. Under review"], ["v2", "Thu, 5 Nov 2015 20:16:04 GMT  (1318kb)", "http://arxiv.org/abs/1507.01784v2", "30 pages; added plate diagrams and clarifications, changed style, corrected typos, updated figures. in Proceedings of the 29-th Conference on Neural Information Processing Systems (NIPS), 2015"]], "COMMENTS": "27 pages. Under review", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["anastasia podosinnikova", "francis r bach", "simon lacoste-julien"], "accepted": true, "id": "1507.01784"}, "pdf": {"name": "1507.01784.pdf", "metadata": {"source": "CRF", "title": "Rethinking LDA: moment matching for discrete ICA", "authors": ["Anastasia Podosinnikova", "Francis Bach", "Simon Lacoste-Julien"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Thematic models have emerged as flexible and important tools for modeling text corpora. While early work focused on graphical approximate inference techniques such as variation inference [6] or Gibbs sampling [21], sort-based moment matching techniques have recently proven to be strong competitors due to their computational speed and theoretical guarantees [1, 3]. In this paper, we establish explicit links with the literature on independent component analysis (ICA) (e.g. [19] and references therein) by showing a strong relationship between latent dirichlet allocation (LDA) [6] and ICA [24, 25, 18]. Subsequently, we can reuse standard ICA techniques and results and derive new tensors with better sample complexity and new algorithms based on common diagonalization."}, {"heading": "2 Is LDA discrete PCA or discrete ICA?", "text": "So it is convenient to use the \"-th-character\" of the n-th document as an indicator-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector."}, {"heading": "3 Moment matching for topic modeling", "text": "In general, the method of moments is based on the idea of estimating latent parameters of a probability model by comparing theoretical expressions of moments with their sample estimates. In the present work by Anandkumar et al. [1, 3], applications of the moment method to several latent variable models, including LDA, are discussed, leading to computationally fast learning algorithms with theoretical guarantees. Their key ideas for LDA are (a) the construction of the moments of the LDA model with a certain diagonal structure (hereinafter referred to as \"LDA moments\") and (b) the development of algorithms for estimating the model parameters by exploiting this particular diagonal structure. As discussed later, these algorithms are a special type of common diagonalization algorithms based on the example estimates of expressions that contain moments. This paper has a similar high-level structure. In Section 3.1, we present new cumulants for the GP / DICA models, which we call \"cumulative moments\" with each other \"(i.e., the\" GP / ICollars moments \") and\" ICollars with each other structures."}, {"heading": "3.1 Cumulants of the GP and DICA models", "text": "In this section we derive and analyze the novel cumulants of the DICA model. Since the GP model is a special case of the DICA model, all the results of this section extend to the GP model. The first three cumulant tendencies of the random vector x can be defined as the following vector x (x): = E (x), (6), x (x), x (x), x (x), x \u2212 E [x \u2212 E (x)), x \u2212 E (x), (x), (7), cum (x, x): = E (x), x (x \u2212 E), x (x \u2212 E) in which we use the tensor product (see some properties of the cumulants in Appendix B.1). The essential property of the cumulants (which does not apply to moments we use in this paper) is that the random sor product (see some properties of the cumulants in Appendix B.1)."}, {"heading": "4 Diagonalization algorithms", "text": "??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "5 Experiments", "text": "In this section, one compares (a) the results of the study with the results of the study. (b) We compare the results of the study. (a) We compare the results of the study. (c) We compare the results of the study. (c) We compare the results of the study. (c) We compare the results of the study. (c) We compare the results of the study. (c) We compare the results of the study. (c) We compare the results of the study. (c) We compare the results of the study. (c) We compare the results of the study. (c) We compare the results of the study. (c) We compare the results of the study. (c) We compare the results of the study. (c) We compare the results of the study. (c) The results of the study. (c) The data. (c). (d). (d). (D). (D). (D). (D. (D). (D. (D). (D. (D). (D. (D). (D. (D). (D. (D). (D. (D). (D. (D). (D. (D). (D. (D). (D. (D). (D). (D. (D). (D. (D). (D. (D). (D). (D). (D. (D. (D). (D.). (D.). (D.). (D. (D.). (D). (D. (D).). (D. (D. (D). (D. (D). (D). (D.). (D. (D. (D.). (D.). (D.). (D. (D.). (D.). (D.). (D. (D. (D). (D. (D). (D.). (D. (.). (D). (.). (D. (.). (D). (D. (D.). (D. (.). (.). (. (D.)."}, {"heading": "5.1 Comparison of the diagonalization algorithms", "text": "In Figure 1, we compare the diagnostic algorithms on the semi-synthetic AP dataset for K = 50 with the GP model for sampling. We compare the tensor power method (TPM) [3], the spectral algorithm (RK) and the selective diagnostic algorithms (A) with various options for selecting random forecasts. JD (K) records P = K vectors. (RK) and the selective diagnostic algorithms vp = W > 0 (A)."}, {"heading": "5.2 Comparison of the GP/DICA cumulants and the LDA moments", "text": "In Figure 2, for the sample from the GP model (top left), both the GP / DICA cumulants and the LDA moments are well specified, implying that the approximation error is low for both. In the sample from the LDA fix (200) model (top right), the GP / DICA cumulants are already low for N = 10,000 documents, while the convergence for the LDA moments is slower. In the sample from the LDA fix (200) model (top right), the GP / DICA cumulants are incorrectly specified and their approximation error is high, although the estimation error is low due to the faster finite sample convergence. In this case, one reason for the poor performance of the GP / DICA cumulants is the absence of a deviation in the document length. In fact, when documents of two different lengths are mixed by sample from the LDA fix 2 (0.5.20.200) model (bottom left), the GP / DICA cumulant in practice does not change."}, {"heading": "5.3 Real data experiments", "text": "Each data set is divided into 5 training / evaluation pairs, in which the documents for evaluation are selected randomly and not repeatedly between the folds (400 documents are taken in the hand for AP; 600 documents are taken in the hand for KOS) and the model parameters for a different number of topics are learned. Evaluation of the held documents takes place with an average of over 5 folds. In Figure 3, the predictive log probability is represented on the y-axis in bits averaged per token. JD-GP, Spec-GP, JD-LDA and Spec-LDA are compared with varying inference (VI) and with varying inference initialized with the output of JD-GP (VI-JD)."}, {"heading": "6 Conclusion", "text": "In this paper, we have proposed a new set of tensors for a discrete ICA model related to LDA, in which word numbers are directly modeled. These moments make fewer assumptions about distributions and are theoretically and empirically more robust than previously proposed LDA tensors, both on synthetic and real data. Following the ICA literature, we have shown that our common diagonalization method is also more robust. Once the topic matrix has been estimated, it would be interesting to know the unknown distributions of independent topic intensities. This work was partially supported by the MSR-Inria Joint Center."}, {"heading": "A Appendix. The GP model", "text": "To show that the LDA model (2) with the additional assumption that the document length is modeled as a gamma Poisson random variable is equivalent to the GP model (4), we show that - when modeling the document length L as a Poisson random variable with the parameter \u03bb, the counting variables x1, x2,. xM are mutually independent Poisson random variables; - the gamma before the gamma reveals the connection between the Dirichlet random variable and the mutually independent gamma variable x1, x2,. For complexity, we repeat the known result if L-Poisson (\u03bb) and x | L-model Multinomial (L, DTB) (meaning that L-xm with the probability one), then xM."}, {"heading": "B Appendix. The cumulants of the GP and DICA models", "text": "B.1 CumulantsFor a random vector x-RM, the first three cumulative properties of this property are irrelevant (1).The first three cumulative properties of this property are not (1). (xm) = 1 (xm) = 1 (xm) = 1 (xm1, xm2) = 1 (xm1), cum (xm1, xm3) = E (xm1) (xm1) (xm1) (xm2) (xm3) (xm3)), cum (xm1) (xm1) (xm3) (xm3) (xm3) (xm3)) (xm3)) (xm3)) (xm3)) (xm3) (xm3) (xm3), cum (xm3) (xm3)."}, {"heading": "C Appendix. The sketch of the proof for Proposition 3.1", "text": "C.1 Expected square error for the example expectation is E (x) = > n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n (n) n (n (n (n) n (n (n (n) n (n (n (n (n (n (n) n (n (n (n (n (n) n (n (n (n (n (n) n (n (n (n) n (n (n (n) n (n (n (n) n (n (n) n (n (n) n (n (n (n) n (n (n) n (n (n) n (n (n) n (n (n) n (n (n) n (n (n) n (n (n) n) n n (n) n (n (n) n (n (n) n (n (n) n) n) n (n) n (n) n) n n n (n) n n n n (n (n) n (n) n (n n) n (n n n n n) n n (n) n) n n n n) n n n (n n n (n n (n n) n n n n (n) n n n (n (n n n) n n n) n n) n n n n n n n n n (n n) n n n n n n n (n) n n n n (n) n n n n n (n) n n (n (n n n n) n n n (n n) n n n) n n n n n n n n (n n n n n n n n (n (n n"}, {"heading": "D Appendix. The LDA moments", "text": "The LDA moments were only using the first three words 1, 2, 3, 4, 5, 5, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 8, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8"}, {"heading": "E Appendix. Practical aspects and implementation details", "text": "It is not only the calculation of such a matrix, but also the reduction of its dimensionality as WSW > = IK. Let S = U > be an orthogonal intrinsic decomposition of the symmetric matrix S. Let's use the symmetric matrix S. Let's use 1: K to denote the diagonal matrix that contains the largest K values. Let S = U > be an orthogonal intrinsic decomposition of the symmetric matrix S. Let's use 1: K to denote the diagonal matrix that contains the largest K values."}, {"heading": "F Appendix. Supplementary experiments", "text": "F. 1 The LDA moments vs. parameter c0The LDA moments depend on the parameter c0, which is in fact not trivial to set in the unattended environment of topic modeling, especially taking into account the complexity of the evaluation of topic models [31]. In Figure 4, the common diagnostic algorithm is compared with the LDA moment for different values of c0 to the algorithm. Data is generated in a manner similar to Figure 2. The experiment shows that the LDA moments react somewhat sensitively to the choice of c0."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We consider moment matching techniques for estimation in Latent Dirichlet Allocation (LDA). By drawing explicit links between LDA and discrete versions of independent component analysis (ICA), we first derive a new set of cumulant-based tensors, with an improved sample complexity. Moreover, we reuse standard ICA techniques such as joint diagonalization of tensors to improve over existing methods based on the tensor power method. In an extensive set of experiments on both synthetic and real datasets, we show that our new combination of tensors and orthogonal joint diagonalization techniques outperforms existing moment matching methods.", "creator": "LaTeX with hyperref package"}}}