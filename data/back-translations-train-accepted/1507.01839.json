{"id": "1507.01839", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jul-2015", "title": "Dependency-based Convolutional Neural Networks for Sentence Embedding", "abstract": "In sentence modeling and classification, convolutional neural network approaches have recently achieved state-of-the-art results, but all such efforts process word vectors sequentially and neglect long-distance dependencies. To exploit both deep learning and linguistic structures, we propose a tree-based convolutional neural network model which exploit various long-distance relationships between words. Our model improves the sequential baselines on all three sentiment and question classification tasks, and achieves the highest published accuracy on TREC.", "histories": [["v1", "Tue, 7 Jul 2015 15:20:36 GMT  (530kb,D)", "http://arxiv.org/abs/1507.01839v1", "this paper has been accepted by ACL 2015"], ["v2", "Mon, 3 Aug 2015 15:36:45 GMT  (538kb,D)", "http://arxiv.org/abs/1507.01839v2", "this paper has been accepted by ACL 2015"]], "COMMENTS": "this paper has been accepted by ACL 2015", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["mingbo ma", "liang huang", "bowen zhou", "bing xiang"], "accepted": true, "id": "1507.01839"}, "pdf": {"name": "1507.01839.pdf", "metadata": {"source": "CRF", "title": "Tree-based Convolution for Sentence Modeling\u2217", "authors": ["Mingbo Ma", "Liang Huang", "Bing Xiang", "Bowen Zhou"], "emails": ["lhuang@us.ibm.com", "bingxia@us.ibm.com", "zhou@us.ibm.com"], "sections": [{"heading": null, "text": "1 The introduction of Convolutionary Neural Networks (CNNs), which originally played an important role in many phenomena (LeCun et al., 1995), has recently drawn much attention in natural language processing (NLP) to problems such as sequence labeling (Collobert et al., 2011), semantic analysis (Yih et al., 2014), and search terms (Shen et al., 2014). In particular, the recent work on CNN-based sentence modeling (Kalchbrenner et al., 2014) has achieved excellent, often state-of-the-art results on various classification tasks such as entity, subjectivity, and question-type classification. However, despite its celebrated success, it remains a significant limitation from a linguistic perspective: CNNs invented on pixel matrices in image processing are considered only sequential n-grams that follow each other."}], "references": [{"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa."], "venue": "volume 12, pages 2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Mining the peanut gallery: Opinion extraction and semantic classification of product reviews", "author": ["Kushal Dave", "Steve Lawrence", "David M Pennock."], "venue": "Proceedings of the 12th international conference on World Wide Web, pages 519\u2013528. ACM.", "citeRegEx": "Dave et al\\.,? 2003", "shortCiteRegEx": "Dave et al\\.", "year": 2003}, {"title": "Sentiment classification on customer feedback data: noisy data, large feature vectors, and the role of linguistic analysis", "author": ["Michael Gamon."], "venue": "Proceedings of the 20th international conference on Computational Linguistics, page 841.", "citeRegEx": "Gamon.,? 2004", "shortCiteRegEx": "Gamon.", "year": 2004}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "volume abs/1207.0580.", "citeRegEx": "Hinton et al\\.,? 2012", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["Ozan Irsoy", "Claire Cardie."], "venue": "Advances in Neural Information Processing Systems, pages 2096\u20132104.", "citeRegEx": "Irsoy and Cardie.,? 2014", "shortCiteRegEx": "Irsoy and Cardie.", "year": 2014}, {"title": "Questionbank: Creating a corpus of parseannotated questions", "author": ["John Judge", "Aoife Cahill", "Josef van Genabith."], "venue": "Proceedings of the 21st International Conference on Computational Linguistics, pages 497\u2013504.", "citeRegEx": "Judge et al\\.,? 2006", "shortCiteRegEx": "Judge et al\\.", "year": 2006}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 655\u2013665.", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746\u2013 1751, Doha, Qatar, October. Association for Com-", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Efficient thirdorder dependency parsers", "author": ["Terry Koo", "Michael Collins."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1\u201311. Association for Computational Linguistics.", "citeRegEx": "Koo and Collins.,? 2010", "shortCiteRegEx": "Koo and Collins.", "year": 2010}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov"], "venue": null, "citeRegEx": "Le and Mikolov.,? \\Q2014\\E", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Comparison of learning algorithms for handwritten digit", "author": ["Y. LeCun", "L. Jackel", "L. Bottou", "A. Brunot", "C. Cortes", "J. Denker", "H. Drucker", "I. Guyon", "U. Mller", "E. Sckinger", "P. Simard", "V. Vapnik"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1995\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1995}, {"title": "Learning question classifiers", "author": ["Xin Li", "Dan Roth."], "venue": "Proceedings of the 19th International Conference on Computational Linguistics - Volume 1, COLING \u201902, pages 1\u20137. Association for Computational Linguistics.", "citeRegEx": "Li and Roth.,? 2002", "shortCiteRegEx": "Li and Roth.", "year": 2002}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky."], "venue": "Proceedings of 52nd Annual Meeting of the Association for Computa-", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Sentiment classification using word sub-sequences and dependency sub-trees", "author": ["Shotaro Matsumoto", "Hiroya Takamura", "Manabu Okumura."], "venue": "Proceedings of the 9th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Min-", "citeRegEx": "Matsumoto et al\\.,? 2005", "shortCiteRegEx": "Matsumoto et al\\.", "year": 2005}, {"title": "Online learning of approximate dependency parsing algorithms", "author": ["Ryan McDonald", "Fernando Pereira."], "venue": "Proceedings of EACL.", "citeRegEx": "McDonald and Pereira.,? 2006", "shortCiteRegEx": "McDonald and Pereira.", "year": 2006}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["Bo Pang", "Lillian Lee."], "venue": "Proceedings of ACL, pages 115\u2013124.", "citeRegEx": "Pang and Lee.,? 2005", "shortCiteRegEx": "Pang and Lee.", "year": 2005}, {"title": "Learning semantic representations using convolutional neural networks for web search", "author": ["Yelong Shen", "Xiaodong he", "Jianfeng Gao", "Li Deng", "Gregoire Mesnil."], "venue": "WWW 2014, April.", "citeRegEx": "Shen et al\\.,? 2014", "shortCiteRegEx": "Shen et al\\.", "year": 2014}, {"title": "From symbolic to sub-symbolic information in question classification", "author": ["J. Silva", "L. Coheur", "A.C. Mendes", "Andreas Wichert."], "venue": "volume 35.", "citeRegEx": "Silva et al\\.,? 2011", "shortCiteRegEx": "Silva et al\\.", "year": 2011}, {"title": "Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning."], "venue": "Proceedings of EMNLP 2011.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts."], "venue": "Proceedings of EMNLP 2013.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Semantic parsing for single-relation question answering", "author": ["Wen-tau Yih", "Xiaodong He", "Christopher Meek."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 643\u2013648.", "citeRegEx": "Yih et al\\.,? 2014", "shortCiteRegEx": "Yih et al\\.", "year": 2014}, {"title": "Adadelta: An adaptive learning rate method", "author": ["Mattgew Zeiler."], "venue": "arxiv: abs/1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Long short-term memory over tree structures", "author": ["Xiaodan Zhu", "Parinaz Sobhani", "Hongyu Guo."], "venue": "In Proceedings of ICML.", "citeRegEx": "Zhu et al\\.,? 2015", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 10, "context": "Convolutional neural networks (CNNs), originally invented in computer vision (LeCun et al., 1995), has recently attracted much attention in natural language processing (NLP) on problems such as sequence labeling (Collobert et al.", "startOffset": 77, "endOffset": 97}, {"referenceID": 0, "context": ", 1995), has recently attracted much attention in natural language processing (NLP) on problems such as sequence labeling (Collobert et al., 2011), semantic parsing (Yih et al.", "startOffset": 122, "endOffset": 146}, {"referenceID": 20, "context": ", 2011), semantic parsing (Yih et al., 2014), and search query retrieval (Shen et al.", "startOffset": 26, "endOffset": 44}, {"referenceID": 16, "context": ", 2014), and search query retrieval (Shen et al., 2014).", "startOffset": 36, "endOffset": 55}, {"referenceID": 6, "context": "In particular, recent work on CNN-based sentence modeling (Kalchbrenner et al., 2014; Kim, 2014) has achieved excellent, often state-of-the-art, results on various classification tasks such as sentiment, subjectivity, and question-type classification.", "startOffset": 58, "endOffset": 96}, {"referenceID": 7, "context": "In particular, recent work on CNN-based sentence modeling (Kalchbrenner et al., 2014; Kim, 2014) has achieved excellent, often state-of-the-art, results on various classification tasks such as sentiment, subjectivity, and question-type classification.", "startOffset": 58, "endOffset": 96}, {"referenceID": 2, "context": "Indeed, in the sentiment analysis literature, researchers have incorporated long-distance information from syntactic parse trees, but the results are somewhat inconsistent: some reported small improvements (Gamon, 2004; Matsumoto et al., 2005), while some otherwise (Dave et al.", "startOffset": 206, "endOffset": 243}, {"referenceID": 13, "context": "Indeed, in the sentiment analysis literature, researchers have incorporated long-distance information from syntactic parse trees, but the results are somewhat inconsistent: some reported small improvements (Gamon, 2004; Matsumoto et al., 2005), while some otherwise (Dave et al.", "startOffset": 206, "endOffset": 243}, {"referenceID": 1, "context": ", 2005), while some otherwise (Dave et al., 2003; Kudo and Matsumoto, 2004).", "startOffset": 30, "endOffset": 75}, {"referenceID": 7, "context": "Our model is similar to Kim (2014), but while his sequential CNNs put a word in its sequential context, ours considers a word and its parent, grand-parent, great-grandparent, and siblings on the dependency tree.", "startOffset": 24, "endOffset": 35}, {"referenceID": 8, "context": "The original CNN, first proposed by LeCun et al. (1995), applies convolution kernels on a series of continuous areas of given images, and was adapted to NLP by Collobert et al.", "startOffset": 36, "endOffset": 56}, {"referenceID": 0, "context": "(1995), applies convolution kernels on a series of continuous areas of given images, and was adapted to NLP by Collobert et al. (2011). Following Kim (2014), one dimensional convolution operates the convolution kernel in sequential order in Equation 1, where xi \u2208 Rd represents the d dimensional ar X iv :1 50 7.", "startOffset": 111, "endOffset": 135}, {"referenceID": 0, "context": "(1995), applies convolution kernels on a series of continuous areas of given images, and was adapted to NLP by Collobert et al. (2011). Following Kim (2014), one dimensional convolution operates the convolution kernel in sequential order in Equation 1, where xi \u2208 Rd represents the d dimensional ar X iv :1 50 7.", "startOffset": 111, "endOffset": 157}, {"referenceID": 15, "context": "Figure 1 illustrates an example from the Movie Reviews (MR) dataset (Pang and Lee, 2005).", "startOffset": 68, "endOffset": 88}, {"referenceID": 0, "context": "In sequential CNNs, max-over-time pooling (Collobert et al., 2011; Kim, 2014) operates over the feature map to get the maximum activation \u0109 = max c representing the entire feature map.", "startOffset": 42, "endOffset": 77}, {"referenceID": 7, "context": "In sequential CNNs, max-over-time pooling (Collobert et al., 2011; Kim, 2014) operates over the feature map to get the maximum activation \u0109 = max c representing the entire feature map.", "startOffset": 42, "endOffset": 77}, {"referenceID": 3, "context": "Following Kim (2014), we employ random dropout on penultimate layer (Hinton et al., 2012).", "startOffset": 68, "endOffset": 89}, {"referenceID": 21, "context": "Following Kim (2014), training is done through stochastic gradient descent over shuffled mini-batches with the Adadelta update rule (Zeiler, 2012).", "startOffset": 132, "endOffset": 146}, {"referenceID": 0, "context": "In sequential CNNs, max-over-time pooling (Collobert et al., 2011; Kim, 2014) operates over the feature map to get the maximum activation \u0109 = max c representing the entire feature map. Our DTCNNs also pool the maximum activation from feature map to detect the strongest activation over the whole tree (i.e., over the whole sentence). Since the tree no longer defines a sequential \u201ctime\u201d direction, we refer to our pooling as \u201cmax-over-tree\u201d pooling. In order to capture enough variations, we randomly initialize the set of filters to detect different structure patterns. Each filter\u2019s height is the number of words considered and the width is always equal to the dimensionality d of word representation. Each filter will be represented by only one feature after max-over-tree pooling. After a series of convolution with different filter with different heights, multiple features carry different structural information become the final representation of the input sentence. Then, this sentence representation is passed to a fully connected soft-max layer and outputs a distribution over different labels. Neural networks often suffer from overtraining. Following Kim (2014), we employ random dropout on penultimate layer (Hinton et al.", "startOffset": 43, "endOffset": 1173}, {"referenceID": 0, "context": "In sequential CNNs, max-over-time pooling (Collobert et al., 2011; Kim, 2014) operates over the feature map to get the maximum activation \u0109 = max c representing the entire feature map. Our DTCNNs also pool the maximum activation from feature map to detect the strongest activation over the whole tree (i.e., over the whole sentence). Since the tree no longer defines a sequential \u201ctime\u201d direction, we refer to our pooling as \u201cmax-over-tree\u201d pooling. In order to capture enough variations, we randomly initialize the set of filters to detect different structure patterns. Each filter\u2019s height is the number of words considered and the width is always equal to the dimensionality d of word representation. Each filter will be represented by only one feature after max-over-tree pooling. After a series of convolution with different filter with different heights, multiple features carry different structural information become the final representation of the input sentence. Then, this sentence representation is passed to a fully connected soft-max layer and outputs a distribution over different labels. Neural networks often suffer from overtraining. Following Kim (2014), we employ random dropout on penultimate layer (Hinton et al., 2012). in order to prevent co-adaptation of hidden units. In our experiments, we set our drop out rate as 0.5 and learning rate as 0.95 by default. Following Kim (2014), training is done through stochastic gradient descent over shuffled mini-batches with the Adadelta update rule (Zeiler, 2012).", "startOffset": 43, "endOffset": 1405}, {"referenceID": 15, "context": "Figure 1 illustrates an example from the Movie Reviews (MR) dataset (Pang and Lee, 2005).", "startOffset": 68, "endOffset": 88}, {"referenceID": 7, "context": "In sequential CNNs, max-over-time pooling (Collobert et al., 20 1; Kim, 2014) operates over the feature map to get the maximum activation \u0109 = max c representing the entire feature ap.", "startOffset": 42, "endOffset": 77}, {"referenceID": 3, "context": "Following Kim (2014), we employ random dropout on penultimate layer (Hinton et al., 2012).", "startOffset": 68, "endOffset": 89}, {"referenceID": 21, "context": "Following Kim (2014), training is done through stochastic gradient descent over shuffled mini-batches with the Adadelta update rule (Zeiler, 2012).", "startOffset": 132, "endOffset": 146}, {"referenceID": 0, "context": "In sequential CNNs, max-over-time pooling (Collobert et al., 20 1; Kim, 2014) operates over the feature map to get the maximum activation \u0109 = max c representing the entire feature ap. Our DTCNNs also pool the maximum activation from feature map to detect the strongest activation over the whole tree (i.e., over the whole sentence). Since the tree no longer defines a sequential \u201ctime\u201d direction, we refer to our pooling as \u201cmax-over-tree\u201d pooling. In order to capture enough variations, we randomly initialize the set of filters to detect different structure patterns. Each filter\u2019s height is the number of words considered and the width is always equal to the dimensionality d of word representation. Each filter will be represented by only one feature after max-over-tree pooling. After a series of convolution with different filter with different heights, multiple features carry different structural information become the final representation of the input sentence. Then, this sentence representation is passed to a fully connected soft-max layer and outputs a distribution over different labels. Neural networks often suffer from overtraining. Following Kim (2014), we employ random dropout on penultimate layer (Hinton et al.", "startOffset": 43, "endOffset": 1172}, {"referenceID": 0, "context": "In sequential CNNs, max-over-time pooling (Collobert et al., 20 1; Kim, 2014) operates over the feature map to get the maximum activation \u0109 = max c representing the entire feature ap. Our DTCNNs also pool the maximum activation from feature map to detect the strongest activation over the whole tree (i.e., over the whole sentence). Since the tree no longer defines a sequential \u201ctime\u201d direction, we refer to our pooling as \u201cmax-over-tree\u201d pooling. In order to capture enough variations, we randomly initialize the set of filters to detect different structure patterns. Each filter\u2019s height is the number of words considered and the width is always equal to the dimensionality d of word representation. Each filter will be represented by only one feature after max-over-tree pooling. After a series of convolution with different filter with different heights, multiple features carry different structural information become the final representation of the input sentence. Then, this sentence representation is passed to a fully connected soft-max layer and outputs a distribution over different labels. Neural networks often suffer from overtraining. Following Kim (2014), we employ random dropout on penultimate layer (Hinton et al., 2012). in order to prevent co-adaptation of hidden units. In our experiments, we set our drop out rate as 0.5 and learning rate as 0.95 by default. Following Kim (2014), training is done through stochastic gradient descent over shuffled mini-batches with the Adadelta update rule (Zeiler, 2012).", "startOffset": 43, "endOffset": 1404}, {"referenceID": 14, "context": "Inspired by higher-order dependency parsing (McDonald and Pereira, 2006; Koo and Collins, 2010), we also incorporate siblings for a given word in various ways.", "startOffset": 44, "endOffset": 95}, {"referenceID": 8, "context": "Inspired by higher-order dependency parsing (McDonald and Pereira, 2006; Koo and Collins, 2010), we also incorporate siblings for a given word in various ways.", "startOffset": 44, "endOffset": 95}, {"referenceID": 15, "context": "We use three benchmark datasets in two categories: sentiment analysis on both Movie Review (MR) (Pang and Lee, 2005) and Stanford Sentiment Treebank", "startOffset": 96, "endOffset": 116}, {"referenceID": 7, "context": "We implement our DTCNN on top of the open source CNN code by Kim (2014).1 Table 2 summarizes our results in the context of other high-performing efforts in the literature.", "startOffset": 61, "endOffset": 72}, {"referenceID": 19, "context": "com/yoonkim/CNN sentence (SST-1) (Socher et al., 2013) datasets, and question classification on TREC (Li and Roth, 2002).", "startOffset": 33, "endOffset": 54}, {"referenceID": 11, "context": ", 2013) datasets, and question classification on TREC (Li and Roth, 2002).", "startOffset": 54, "endOffset": 73}, {"referenceID": 12, "context": "For all datasets, we first obtain the dependency parse tree from Stanford parser (Manning et al., 2014).", "startOffset": 81, "endOffset": 103}, {"referenceID": 22, "context": "4, and ties with Zhu et al. (2015).", "startOffset": 17, "endOffset": 35}, {"referenceID": 4, "context": "5 which is second only to Irsoy and Cardie (2014). We set batch size to 100 for this task.", "startOffset": 26, "endOffset": 50}, {"referenceID": 7, "context": "We implement our DTCNN on top of the open source CNN code by Kim (2014).1 Table 1 summarizes our results in the context of other high-performing efforts in the literature.", "startOffset": 61, "endOffset": 72}, {"referenceID": 19, "context": "com/yoonkim/CNN sentence and Lee, 2005) and Stanford Sentiment Treebank (SST-1) (Socher et al., 2013) datasets, and question classification on TREC (Li and Roth, 2002).", "startOffset": 80, "endOffset": 101}, {"referenceID": 11, "context": ", 2013) datasets, and question classification on TREC (Li and Roth, 2002).", "startOffset": 54, "endOffset": 73}, {"referenceID": 22, "context": "4, and ties with Zhu et al. (2015).", "startOffset": 17, "endOffset": 35}, {"referenceID": 7, "context": "CNNs CNNs-non-static (Kim, 2014) \u2013 baseline 81.", "startOffset": 21, "endOffset": 32}, {"referenceID": 7, "context": "4\u2217 CNNs-multichannel (Kim, 2014) 81.", "startOffset": 21, "endOffset": 32}, {"referenceID": 6, "context": "0\u2217 Deep CNNs (Kalchbrenner et al., 2014) - 48.", "startOffset": 13, "endOffset": 40}, {"referenceID": 18, "context": "Recursive NNs Recursive Autoencoder (Socher et al., 2011) 77.", "startOffset": 36, "endOffset": 57}, {"referenceID": 19, "context": "2 - Recursive Neural Tensor (Socher et al., 2013) - 45.", "startOffset": 28, "endOffset": 49}, {"referenceID": 4, "context": "7 - Deep Recursive NNs (Irsoy and Cardie, 2014) - 49.", "startOffset": 23, "endOffset": 47}, {"referenceID": 22, "context": "Recurrent NNs LSTM on tree (Zhu et al., 2015) 81.", "startOffset": 27, "endOffset": 45}, {"referenceID": 9, "context": "Other deep learning Paragraph-Vec (Le and Mikolov, 2014) - 48.", "startOffset": 34, "endOffset": 56}, {"referenceID": 17, "context": "Hand-coded rules SVMS (Silva et al., 2011) 95.", "startOffset": 22, "endOffset": 42}, {"referenceID": 7, "context": "\u2217Results generated from Kim (2014)\u2019s implementation.", "startOffset": 24, "endOffset": 35}, {"referenceID": 4, "context": "5 which is second only to Irsoy and Cardie (2014). We set batch size to 100 for this task.", "startOffset": 26, "endOffset": 50}, {"referenceID": 17, "context": "ceeds SVMS (Silva et al., 2011) with 60 handcoded rules.", "startOffset": 11, "endOffset": 31}, {"referenceID": 5, "context": "8 but their parser is trained only on the QuestionBank (Judge et al., 2006) while we used the standard Stanford parser trained on both the Penn Treebank and QuestionBank.", "startOffset": 55, "endOffset": 75}, {"referenceID": 16, "context": "Like ours, Silva et al. (2011) is a tree-based system but it uses constituency trees compared to ours dependency trees.", "startOffset": 11, "endOffset": 31}, {"referenceID": 7, "context": "CNNs CNNs-non-static (Kim, 2014) \u2013 baseline 81.", "startOffset": 21, "endOffset": 32}, {"referenceID": 7, "context": "4\u2217 CNNs-multichannel (Kim, 2014) 81.", "startOffset": 21, "endOffset": 32}, {"referenceID": 6, "context": "0\u2217 Deep CNNs (Kalchbrenner et al., 2014) - 48.", "startOffset": 13, "endOffset": 40}, {"referenceID": 18, "context": "Recursive NNs Recursive Autoencoder (Socher et al., 2011) 77.", "startOffset": 36, "endOffset": 57}, {"referenceID": 19, "context": "2 - Recursive Neural Tensor (Socher et al., 2013) - 45.", "startOffset": 28, "endOffset": 49}, {"referenceID": 4, "context": "7 - Deep Recursive NNs (Irsoy and Cardie, 2014) - 49.", "startOffset": 23, "endOffset": 47}, {"referenceID": 22, "context": "Recurrent NNs LSTM on tree (Zhu et al., 2015) 81.", "startOffset": 27, "endOffset": 45}, {"referenceID": 9, "context": "Other deep learning Paragraph-Vec (Le and Mikolov, 2014) - 48.", "startOffset": 34, "endOffset": 56}, {"referenceID": 17, "context": "Hand-coded rules SVMS (Silva et al., 2011) 95.", "startOffset": 22, "endOffset": 42}, {"referenceID": 7, "context": "\u2217Results generated from Kim (2014)\u2019s implementation.", "startOffset": 24, "endOffset": 35}, {"referenceID": 17, "context": "One thing to note here is that our best result even exceeds SVMS (Silva et al., 2011) with 60 handcoded rules.", "startOffset": 65, "endOffset": 85}, {"referenceID": 5, "context": "8 but their parser is trained only on the QuestionBank (Judge et al., 2006) while we used the standard Stanford parser trained on both the Penn", "startOffset": 55, "endOffset": 75}, {"referenceID": 5, "context": "However, the parser works substantially better on the TREC dataset since all questions are in formal written English, and the training set for Stanford parser5 already includes the QuestionBank (Judge et al., 2006) which includes 2,000 TREC sentences.", "startOffset": 194, "endOffset": 214}, {"referenceID": 5, "context": "However, the pars r works substantially better on the TREC dataset since all questions are in formal ritten English, and the training set for Stanford parser5 already includes the QuestionBank (Judge et al., 2006) which includes 2,000 TREC sentences.", "startOffset": 193, "endOffset": 213}, {"referenceID": 5, "context": "However, the parser works substantially better on the TREC dataset sinc all q estions are in formal written English, and the training set for St nford parser5 already includes the QuestionBank (Judge et al., 2006) which includes 2,000 TREC sentences.", "startOffset": 193, "endOffset": 213}], "year": 2017, "abstractText": "In sentence modeling and classification, convolutional neural network approaches have recently achieved state-of-the-art results, but all such efforts process word vectors sequentially and neglect long-distance dependencies. To exploit both deep learning and linguistic structures, we propose a tree-based convolutional neural network model which exploit various long-distance relationships between words. Our model improves the sequential baselines on all three sentiment and question classification tasks, and achieves the highest published accuracy on TREC.", "creator": "TeX"}}}