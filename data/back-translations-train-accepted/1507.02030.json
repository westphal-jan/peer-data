{"id": "1507.02030", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jul-2015", "title": "Beyond Convexity: Stochastic Quasi-Convex Optimization", "abstract": "Stochastic convex optimization is a basic and well studied primitive in machine learning. It is well known that convex and Lipschitz functions can be minimized efficiently using Stochastic Gradient Descent (SGD). The Normalized Gradient Descent (NGD) algorithm, is an adaptation of Gradient Descent, which updates according to the direction of the gradients, rather than the gradients themselves. In this paper we analyze a stochastic version of NGD and prove its convergence to a global minimum for a wider class of functions: we require the functions to be quasi-convex and locally-Lipschitz. Quasi-convexity broadens the con- cept of unimodality to multidimensions and allows for certain types of saddle points, which are a known hurdle for first-order optimization methods such as gradient decent. Locally-Lipschitz functions are only required to be Lipschitz in a small region around the optimum. This assumption circumvents gradient explosion, which is another known hurdle for gradient descent variants. Interestingly, unlike the vanilla SGD algorithm, the stochastic normalized gradient descent algorithm provably requires a minimal minibatch size.", "histories": [["v1", "Wed, 8 Jul 2015 05:47:42 GMT  (132kb,D)", "https://arxiv.org/abs/1507.02030v1", null], ["v2", "Mon, 26 Oct 2015 08:14:31 GMT  (132kb,D)", "http://arxiv.org/abs/1507.02030v2", null], ["v3", "Wed, 28 Oct 2015 07:00:56 GMT  (132kb,D)", "http://arxiv.org/abs/1507.02030v3", null]], "reviews": [], "SUBJECTS": "cs.LG math.OC", "authors": ["elad hazan", "kfir y levy", "shai shalev-shwartz"], "accepted": true, "id": "1507.02030"}, "pdf": {"name": "1507.02030.pdf", "metadata": {"source": "CRF", "title": "Beyond Convexity: Stochastic Quasi-Convex Optimization", "authors": ["Elad Hazan", "Kfir Y. Levy", "Shai Shalev-Shwartz"], "emails": ["ehazan@cs.princeton.edu.", "kfiryl@tx.technion.ac.il.", "shais@cs.huji.ac.il."], "sections": [{"heading": null, "text": "The Normalized Gradient Descent (NGD) algorithm is an adaptation of Gradient Descent that updates itself according to the direction of the slopes, not the slopes themselves. In this article, we analyze a stochastic version of NGD and prove its convergence to a global minimum for a broader class of functions: We demand that the functions must be quasi-convex and local-Lipschitz. quasi-convexity broadens the concept of uniformity to multidimensional dimensions and allows certain types of saddle points, which are a known hurdle for first-order optimization methods such as slope descent. Local-Lipschitz functions only need to be used in a small region around the optimum, bypassing the gradient explosion, which is another known hurdle for gradient deviation.Interestingly, the Normalized Gradient Descent (SGD) algorithm requires a batch size as opposed to the vanilla minimum algorithm."}, {"heading": "1 Introduction", "text": "The benefits of using Stochastic Gradient Descent (SGD) for learning cannot be overstated."}, {"heading": "1.1 Related Work", "text": "In fact, the fact is that most of them will be able to demonstrate that they are able to achieve their goals and that they are able to achieve their goals."}, {"heading": "2 Definitions and Notations", "text": "[N] denotes the set {1,..., N}. For the sake of simplicity, we always assume throughout the paper that functions are differentiable (but if they are not specified explicitly, we do not assume that they are bound by the norm of gradients). Definition 2.1. (Local lipschitzness and local smoothness) Let us leave z-Rd, G, 0. A function f: K 7 \u2192 R is called (G, z) local lipschitz, if for each x, y-Bd (z,), we have | f (x) \u2212 f: convex (y) | f-blex-y. Likewise, the function f: (\u03b2, z) -local-smooth if for each x-y-Bd (z,) we have an x-x-y function."}, {"heading": "3 Local-Quasi-Convexity", "text": "Quasi-convexity does not fully capture the concept of unimodality in several dimensions. As an example, we take x = (x1, x2) [\u2212 10, 10] 2, and consider the function (x) = (\u2212 10, \u2212 10) \u2212 1 + (1 + e \u2212 x2) \u2212 1. (2) It is natural to consider g as unimodal since it does not acquire a local minimum except for the unique global minimum at x * (\u2212 10, \u2212 10). However, g is not quasi-convex: consider the points x = (log 16, \u2212 log 4), y = (\u2212 log 4, log 16) that belong to the 1,2 sub-level, their average does not belong to the same sub-level, since g (x / 2 + y / 2) = 4 / 3. Quasi-convex functions that always allow us to explore, which means that the gradient always directs us in a global lineage."}, {"heading": "3.1 Generalized Linear Models (GLM)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1.1 The Idealized GLM", "text": "In this setup we have a collection of m samples {(xi, yi)} mi = 1% Bd \u00b7 [0, 1] and an activation function \u03c6: R 7 \u2192 R. The performance of a prediction factor w \u00b2 Rd is guaranteed to be measured by the average square error across all samples. The performance of a prediction factor w \u00b2 Rd is measured by the average square error across all samples. e \u00b2 rrm (w) = 1m m \u00b2 i = 1 (yi \u2212 \u03c6 < w, xi >) 2. (3) In Kalai and Sastry (2009) it is shown that the perception problem w \u00b2 Rd is a private case of GLM regression. The sigmoid function \u03c6 (z) = (1 + e \u2212 z) \u2212 1 is a popular activation function in the field of deep learning."}, {"heading": "3.1.2 The Noisy GLM", "text": "In the noisy GLM setup (see McCullagh and Nelder (1989); Kalai and Sastry (2009)), we can i.i.d. extract samples {(xi, yi)} mi = 1, Bd \u00b7 [0, 1], from an unknown distribution D. We assume that there is a predictor w *, Rd such that E (x, y) \u0445 D [y | x] = \u03c6 < w *, x >, where \u03c6 is an activation function. We are interested in systems that reach a -optimal minimum to E, within poly (1 /) samples and optimization steps. Given m samples of D < w, x >) 2, and it can be shown that w \u00b2 is a global minimum of E. We are interested in systems that reach a -optimal minimum to E, within poly (1 /) samples and optimization steps that require an optimal measure of D."}, {"heading": "4 NGD for Locally-Quasi-Convex Optimization", "text": "Here we present the NGD algorithms and prove the convergence rate of this algorithm for SLQC targets. Our analysis is simple and allows us to extend the convergence rate set out in Nesterov (1984). \u2212 Let us then see that quasi-convex and local-smooth targets achieve a faster convergence rate. \u2212 Let us see that the convergence rate (NGD) is a faster convergence rate even when the gradients are around the minimums outside a small region. \u2212 Let us then see that NGD achieves a faster convergence rate. \u2212 Let us see that the convergence rate (NGD) achieves a faster convergence rate. \u2212 Let us see that the gradients Descent Descent Descent (NGD) Input: # Iterations T, x1 Rd, Learn Rate for t = 1. T doUpdate: xt + 1 = xt \u2212 Sxt \u2212 f (xt) that we converge the convergence rate where we (xt)"}, {"heading": "4.1 Locally-Lipschitz/Smooth Quasi-Convex Optimization", "text": "It can be shown that strict quasi-convexity and (G, / G, x) local lip coating of f implies that f (, G, x) -SLQC-x-Rd-x-x-x-x-x-x-x-x (see Appendix E) is a direct sequence of Theorem 4.1: Correction 4.1. Fix > 0, leave f: Rd 7 \u2192 R and x-x-x-x-x-arg minx-x-Rd f (x). Since f is strictly quasi-convex and (G, x-G) -local-Lipschitz. Then run the NGD algorithm with T-x1 \u2212 x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x."}, {"heading": "5 SNGD for Stochastic SLQC Optimization", "text": "Here we describe the setting of stochastic SLQC optimization. Then we describe our SNGD algorithms, which provide a -optimal solution within Poly (1 /) queries. We also show that the (noisy) GLM problem, which is described in Section 3.1.2, is an instance of stochastic SLQC optimization, which allows us to verifiably solve this problem within Poly (1 /) -samples and optimization steps using SNGD. Algorithm 2 Stochastic Normalized Gradient Descent (SNGD) Input: # Iterations T, x1) -Rd, Learning Rate, minibatch size b for t = 1. T dosample: {i} bi = 1 \u0445 Db, and define, ft (x) = 1b = 1 (x) Update: xt + 1 = xt \u2212 g, where we assume that gt = xxt (xt), g = gt, g = gt, g, g, g, g g, g g, g g, g g, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, g, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, g, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, g, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c"}, {"heading": "5.1 Main Results", "text": "Normalization is of crucial importance to use the advantages of the SLQC premise and to overcome the hurdles of high-level and cliff edges. Following is our main rate: Theorem 5.1., Fix 6.1., G, M, 0. Suppose we run SNGD with T-2-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x"}, {"heading": "5.2 Remaining Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.2.1 Proof of Lemma 5.1", "text": "Proof. Let's assume next that we perform T iterations and the course of ft at xt in these iterations never disappears. Let's consider the update rule of SNGD (algorithm 2), then the standard algebra that we get: T-iterations + 1-x-x-x-x-x-x-x-x-< g-t, xt-x-x-x-x-x > 2. Let's assume that we have ft (xt) \u2212 ft (x-x) > ft (x-x) > x x x x x x x. Let's assume y = x-x-x-x-x-x-t, and note that y \u2212 x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-"}, {"heading": "5.2.2 Proof of Lemma 5.2", "text": "Proof. At each step t the minibatch is scanned after xt and x * are fixed. The random variables ft (xt) (or ft (x \u0445)) are an average of b i.i.d. random variables whose expectation is f (xt) (or f (x \u0445). These random variables are limited, since we assume that t, x, | ft (x) | \u2264 M (see Thm. 5.1). Applied to the b random samples mentioned above, along with the union bound by t [T] and both sequences of random variables, the problem follows."}, {"heading": "5.2.3 Proof of Theorem 5.2", "text": "We need the following problem, the proof of which is given in App. G. Lemma 5.3 (Absorb probabilities = nominal probabilities). Let us {Xt} \u221e t = 1 a Markov chain over states {i} \u221e i = 0, so that 0 is an absorbing state, and the transition distribution elsewhere is as follows: Xt + 1 | {Xt = i} = {i \u2212 1 w.p i + 1 w.p i \u2212 pDefine the absorbing probabilities \u03b1i: = P (t > 0: Xt = 0 | X0 = i), then: \u03b1i = (p 1 \u2212 p) i, \u0432 i \u2265 1Proof. To prove theorem 5.2, let us construct a counter example in one dimension. Let us consider the following distribution D over loss functions: f (x) = {\u2212 0.5 x w.p 1 \u2212 p 1 \u2212 p = i, max."}, {"heading": "6 Experiments", "text": "A better understanding of how to train deep neural networks is one of the biggest challenges in current machine learning and optimization. Since learning NN (Neural Network) architectures essentially requires the solution of a hard non-convex program, we have decided to focus our empirical study on this type of task. As a test case, we train a neural network with a single hidden layer of 100 units over the MNIST dataset. We use a ReLU activation function and minimize square loss. We use a regularization over weights with a parameter of \u03bb = 5 \u00b7 10 \u2212 4. Initially, we were interested in comparing the performance of SNGD with the performance of MSGD (Minibatch Stochastic Gradient Descent) and with a stochastic variant of Nesterov's accelerated gradient method Sutskever et al. (2013), which is considered the most advanced batch method."}, {"heading": "7 Discussion", "text": "We have presented the first detectable gradient-based algorithm for stochastic quasi-convex optimization, which is a first attempt to generalize the well-developed machinery of stochastic convex optimization to the challenging non-convex problems of machine learning, and to better characterize the boundary between NP-hard non-convex optimization and comprehensible cases like the ones we are examining."}, {"heading": "A Equivalence Between Definitions 2.2 and 2.3", "text": "Let us first show that 2.2 \u21d2 2.3proofs for 2.2 \u21d2 2.3. Let x, y, y, y, y so that f (x), f (y), f (p), f (p), f (z), f (z), f (z), f (z), f (z), f (z), f (z), f (z), f (z), f (z), f (z), f (z), f (z), f (z), f (z), f (z), f (z), f (z), f (z), f (z), f (z), f (z), f (z), f (z), f (z), f (z), f (z), f (z), f (z), f (z)."}, {"heading": "B Local Quasi-convexity of g", "text": "Here we show that the function g that appears in Eq.2 is SLQC. Denote x * = (\u2212 10, \u2212 10), leave it [0, 1] and leave x, y so that g (x) \u2212 \u2212 g (x) and i \u2212 x. to prove that it is enough to show that it is gx * > 0, and that < gx, x \u2212 y > 0 (we denote gx: = g (x)). Derivative g at x we have: gx = g (x) = (e \u2212 x1 / (1 + e \u2212 x1) 2, e \u2212 x2 / (1 + e \u2212 x2) 2) 2). And it is clear that we have gx > 0, x [\u2212 10, 10] 2, therefore strictness always applies."}, {"heading": "C Proof of Lemma 3.1", "text": "The proof. Given that it is a (, eW, w) -SLQC for each W (0, W), we will show that the (, eW, w) -SLQC for each W (0, W) -Spot (0, W) -Spot (0, W) -Spot (0, W) -Spot (0, W) -Spot (1, W) -Spot (0, W) -Spot (0, W) -Spot (1, W) -Spot (1, W) -Spot (1, W) -Spot (1, W) -Spot (1, W) -Spot (1, W) -Spot (1, W) -Spot (1, W) -Spot (1, 1, 1, 1, (1, 1, 1, 1, 1, 1, 1, 1 (1) -Spot (1, 1) -Spot (1, 1, 1) -Spot (1, 1, 1) -Spot (1, 1, 1) -Spot (1, 1, 1) -Spot (1, 0, W) -Spot (0, W) -Spot (0, W) -Spot (0, W) -Spot (0, 0, W) -Spot (0, 0, W) -Spot (0, W) -Spot (0, 0, W) -Spot (0, 0, W) -Spot (0, W) -Spot (0, W) -Spot (0, 0, 0, W) -Spot (0, W) -Spot (1, W) -Spot (1, W) -Spot (1, -Spot (1, W) -Spot (1, -Spot (1, 1, -Spot (1, -Spot (1, 1, 1, 1, 1, 1, 1, 1) -Spot (1, 1, -Spot (1, 1, 1, 1, 1) -Spot (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1"}, {"heading": "D Proof of Lemma 3.2", "text": "Since we are in the noisy idealized constellation, namely in the constellation in which we are, and in the constellation in which we are, and in the constellation in which we are. < w = < w = < w = w = w (m), w = p (m), w = p (m), w = p (m), w = p (m), w = p (m), w = p (m), w = p (m), w (m), w = p (m), w (m), w (m), w \u2212 p (m), m (m), i = p = 1 m (m), m (m), i > m (m), u (m), u (m), u (m)."}, {"heading": "E Locally-Lipschitz and Strictly Quasi-Convex are SLQC", "text": "To show that the strictly quasi-convexe function, which is also (G, / G, x) -Lipschitz, is SLQC, we need the following problem: Lemma E.1. Let's assume that f (G, / G, z) is local Lipschitz. Then, for each x with f (x) \u2212 f (z) > B (z, / G) Sf (x) Proof. Let's remember the notation Sf (x) = {y: f (y) \u2264 f (x)}. Through Lipschitzness, we have for each y B (z, / G) f (y) \u2264 f (z) +. Combined with the assumption that f (z) + < f (x) we get this y Sf (x) \u2264 f (x) \u2264 f (x). If f (x) \u2212 f (x) \u2012 then applies, then f (z) + < f (x) that y (x) is quasiltltlt< < < < and < < (4) < < < (4) < < < < and < (4) < < (4) < < < 4) < (4) < < 4) < (4) <"}, {"heading": "F Proof of Theorem 4.2", "text": "The key problem that enables us to achieve faster rates for smooth functioning is as follows: Lemma F. 1. Leave x * a global minimum of f. Also, assume that f (\u03b2, \u221a 2 / \u03b2, x *) -local-smooth. Then, for each x, we get f (x) \u2212 f (x *) > B (x *, \u221a 2 / \u03b2) Sf (x). Proof. Combine the definition of local smoothness (Def. 2.1) with f (x *) = 0, we get f (y) \u2212 f (x *) \u2212 f (x *). Combine this with the assumption that f (x *) + < f (x *) we get this y Sf (x). The proof for the orem 4.2 / \u03b2) yields f (x *)."}, {"heading": "G Proof of Lemma 5.3", "text": "Proof. On the basis of the stationarity and Markov property of the chain, we can write the following recursive equations for the absorption probabilities: \u03b1i = (1 \u2212 p) \u03b1i + 1 + p\u03b1i \u2212 1, \u0445i > 1 (7) \u03b11 = (1 \u2212 p) \u03b12 + p (8) Let us guess a solution of the form, \u03b1i = c0\u03c1 i, where \u03c1 is the decay parameter of the absorption probabilities. By inserting this solution into equation (7), we obtain an equation for \u03c1: (1 \u2212 p) \u03c12 \u2212 \u03c1 + p = 0. And it can be confirmed that the only non-trivial solution \u03c1 = p 1 \u2212 p is, using the latter in equation (8) to obtain c0 = 1, and thus concluding that \u03b1i = (p 1 \u2212 p) i, \u03b4i \u2265 1"}, {"heading": "H A Broader Notion of Local-Quasi-Convexity", "text": "Definition 3.1 describes a rich functional family as described in Section 3.1.1, and 3.1.2. However, it is clear that there is no access to a constant and quasi-convex functionality, such as the zero-one lots or the perception problem. In some cases, such as the perception problem, we can have access to one direction, G: Rd 7 \u2192 Rd. This oracle is a proxy for the gradient that aims to move us towards a global ascent (descent), followed by a broader definition of the local quasi-convex functions: Definition H.1. (Local Quasi-Convex) Let x, z-Rd, > 0. Also, let us leave G: Rd 7 \u2192 Rd. We say that f: Rd 7 \u2192 R is (, z) -Quasi-Convex."}], "references": [{"title": "Exponentially many local minima for single neurons", "author": ["P. Auer", "M. Herbster", "M.K. Warmuth"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Auer et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Auer et al\\.", "year": 1996}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and trends in Machine Learning,", "citeRegEx": "Bengio.,? \\Q2009\\E", "shortCiteRegEx": "Bengio.", "year": 2009}, {"title": "Bifurcations of recurrent neural networks in gradient descent learning", "author": ["K. Doya"], "venue": "IEEE Transactions on neural networks,", "citeRegEx": "Doya.,? \\Q1993\\E", "shortCiteRegEx": "Doya.", "year": 1993}, {"title": "Complexity analysis of an interior cutting plane method for convex feasibility problems", "author": ["J.-L. Goffin", "Z.-Q. Luo", "Y. Ye"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Goffin et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Goffin et al\\.", "year": 1996}, {"title": "The isotron algorithm: High-dimensional isotonic regression", "author": ["A.T. Kalai", "R. Sastry"], "venue": "In COLT,", "citeRegEx": "Kalai and Sastry.,? \\Q2009\\E", "shortCiteRegEx": "Kalai and Sastry.", "year": 2009}, {"title": "Quasiconvex optimization for robust geometric reconstruction", "author": ["Q. Ke", "T. Kanade"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Ke and Kanade.,? \\Q2007\\E", "shortCiteRegEx": "Ke and Kanade.", "year": 2007}, {"title": "A method to find a point of a convex set. Issled", "author": ["R.F. Khabibullin"], "venue": "Prik. Mat.,", "citeRegEx": "Khabibullin.,? \\Q1977\\E", "shortCiteRegEx": "Khabibullin.", "year": 1977}, {"title": "Convergence and efficiency of subgradient methods for quasiconvex minimization", "author": ["K.C. Kiwiel"], "venue": "Mathematical programming,", "citeRegEx": "Kiwiel.,? \\Q2001\\E", "shortCiteRegEx": "Kiwiel.", "year": 2001}, {"title": "On convergence properties of a subgradient method", "author": ["I.V. Konnov"], "venue": "Optimization Methods and Software,", "citeRegEx": "Konnov.,? \\Q2003\\E", "shortCiteRegEx": "Konnov.", "year": 2003}, {"title": "The theory of incentives: the principal-agent model", "author": ["J.-J. Laffont", "D. Martimort"], "venue": "Princeton university press,", "citeRegEx": "Laffont and Martimort.,? \\Q2009\\E", "shortCiteRegEx": "Laffont and Martimort.", "year": 2009}, {"title": "Learning recurrent neural networks with hessian-free optimization", "author": ["J. Martens", "I. Sutskever"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Martens and Sutskever.,? \\Q2011\\E", "shortCiteRegEx": "Martens and Sutskever.", "year": 2011}, {"title": "Generalised linear models", "author": ["P. McCullagh", "J. Nelder"], "venue": "London: Chapman and Hall/CRC,", "citeRegEx": "McCullagh and Nelder.,? \\Q1989\\E", "shortCiteRegEx": "McCullagh and Nelder.", "year": 1989}, {"title": "Minimization methods for nonsmooth convex and quasiconvex functions", "author": ["Y.E. Nesterov"], "venue": "Matekon, 29:519\u2013531,", "citeRegEx": "Nesterov.,? \\Q1984\\E", "shortCiteRegEx": "Nesterov.", "year": 1984}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "In Proceedings of The 30th International Conference on Machine Learning,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "A general method of solving extremum problems", "author": ["B.T. Polyak"], "venue": "Dokl. Akademii Nauk SSSR,", "citeRegEx": "Polyak.,? \\Q1967\\E", "shortCiteRegEx": "Polyak.", "year": 1967}, {"title": "Quasi subgradient algorithms for calculating surrogate constraints. In Analysis and algorithms of optimization problems, pages 203\u2013236", "author": ["J. Sikorski"], "venue": null, "citeRegEx": "Sikorski.,? \\Q1986\\E", "shortCiteRegEx": "Sikorski.", "year": 1986}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Price discrimination and social welfare", "author": ["H.R. Varian"], "venue": "The American Economic Review, pages 870\u2013875,", "citeRegEx": "Varian.,? \\Q1985\\E", "shortCiteRegEx": "Varian.", "year": 1985}, {"title": "Topics in microeconomics: Industrial organization, auctions, and incentives", "author": ["E. Wolfstetter"], "venue": null, "citeRegEx": "Wolfstetter.,? \\Q1999\\E", "shortCiteRegEx": "Wolfstetter.", "year": 1999}, {"title": "The minimization of quasicomplex functionals", "author": ["Y.I. Zabotin", "A. Korablev", "R.F. Khabibullin"], "venue": "Izv. Vyssh. Uch. Zaved. Mat.,", "citeRegEx": "Zabotin et al\\.,? \\Q1972\\E", "shortCiteRegEx": "Zabotin et al\\.", "year": 1972}], "referenceMentions": [{"referenceID": 1, "context": "In particular, SGD is widely used for deep learning Bengio (2009), one of the most interesting fields where stochastic non-convex optimization problems arise.", "startOffset": 52, "endOffset": 66}, {"referenceID": 11, "context": "\u2022 We introduce a new setup: stochastic optimization of locally-quasi-convex functions; and show that this setup captures Generalized Linear Models (GLM) regression, McCullagh and Nelder (1989). For this setup, we devise a stochastic version of NGD (SNGD), and show that it converges within O(1/ ) iterations to an -optimal minimum.", "startOffset": 165, "endOffset": 193}, {"referenceID": 15, "context": "1 Related Work Quasi-convex optimization problems arise in numerous fields, spanning economics Varian (1985); Laffont and Martimort (2009), industrial organization Wolfstetter (1999) , and computer vision Ke and Kanade (2007).", "startOffset": 95, "endOffset": 109}, {"referenceID": 8, "context": "1 Related Work Quasi-convex optimization problems arise in numerous fields, spanning economics Varian (1985); Laffont and Martimort (2009), industrial organization Wolfstetter (1999) , and computer vision Ke and Kanade (2007).", "startOffset": 110, "endOffset": 139}, {"referenceID": 8, "context": "1 Related Work Quasi-convex optimization problems arise in numerous fields, spanning economics Varian (1985); Laffont and Martimort (2009), industrial organization Wolfstetter (1999) , and computer vision Ke and Kanade (2007).", "startOffset": 110, "endOffset": 183}, {"referenceID": 5, "context": "1 Related Work Quasi-convex optimization problems arise in numerous fields, spanning economics Varian (1985); Laffont and Martimort (2009), industrial organization Wolfstetter (1999) , and computer vision Ke and Kanade (2007). It is well known that quasi-convex optimization tasks", "startOffset": 205, "endOffset": 226}, {"referenceID": 2, "context": "can be solved by a series of convex feasibility problems Boyd and Vandenberghe (2004); However, generally solving such feasibility problems may be very costly Goffin et al. (1996). There exists a rich literature concerning quasi-convex optimization in the offline case, Polyak (1967); Zabotin et al.", "startOffset": 159, "endOffset": 180}, {"referenceID": 2, "context": "can be solved by a series of convex feasibility problems Boyd and Vandenberghe (2004); However, generally solving such feasibility problems may be very costly Goffin et al. (1996). There exists a rich literature concerning quasi-convex optimization in the offline case, Polyak (1967); Zabotin et al.", "startOffset": 159, "endOffset": 284}, {"referenceID": 2, "context": "can be solved by a series of convex feasibility problems Boyd and Vandenberghe (2004); However, generally solving such feasibility problems may be very costly Goffin et al. (1996). There exists a rich literature concerning quasi-convex optimization in the offline case, Polyak (1967); Zabotin et al. (1972); Khabibullin (1977); Sikorski (1986).", "startOffset": 159, "endOffset": 307}, {"referenceID": 2, "context": "can be solved by a series of convex feasibility problems Boyd and Vandenberghe (2004); However, generally solving such feasibility problems may be very costly Goffin et al. (1996). There exists a rich literature concerning quasi-convex optimization in the offline case, Polyak (1967); Zabotin et al. (1972); Khabibullin (1977); Sikorski (1986).", "startOffset": 159, "endOffset": 327}, {"referenceID": 2, "context": "can be solved by a series of convex feasibility problems Boyd and Vandenberghe (2004); However, generally solving such feasibility problems may be very costly Goffin et al. (1996). There exists a rich literature concerning quasi-convex optimization in the offline case, Polyak (1967); Zabotin et al. (1972); Khabibullin (1977); Sikorski (1986). A pioneering paper by Nesterov (1984), was the first to suggest an efficient algorithm, namely Normalized Gradient Descent, and prove that this algorithm attains -optimal solution within O(1/ ) iterations given a differentiable quasi-convex objective.", "startOffset": 159, "endOffset": 344}, {"referenceID": 2, "context": "can be solved by a series of convex feasibility problems Boyd and Vandenberghe (2004); However, generally solving such feasibility problems may be very costly Goffin et al. (1996). There exists a rich literature concerning quasi-convex optimization in the offline case, Polyak (1967); Zabotin et al. (1972); Khabibullin (1977); Sikorski (1986). A pioneering paper by Nesterov (1984), was the first to suggest an efficient algorithm, namely Normalized Gradient Descent, and prove that this algorithm attains -optimal solution within O(1/ ) iterations given a differentiable quasi-convex objective.", "startOffset": 159, "endOffset": 383}, {"referenceID": 2, "context": "can be solved by a series of convex feasibility problems Boyd and Vandenberghe (2004); However, generally solving such feasibility problems may be very costly Goffin et al. (1996). There exists a rich literature concerning quasi-convex optimization in the offline case, Polyak (1967); Zabotin et al. (1972); Khabibullin (1977); Sikorski (1986). A pioneering paper by Nesterov (1984), was the first to suggest an efficient algorithm, namely Normalized Gradient Descent, and prove that this algorithm attains -optimal solution within O(1/ ) iterations given a differentiable quasi-convex objective. This work was later extended by Kiwiel (2001), showing that the same result may be achieved assuming upper semi-continuous quasi-convex objectives.", "startOffset": 159, "endOffset": 643}, {"referenceID": 2, "context": "can be solved by a series of convex feasibility problems Boyd and Vandenberghe (2004); However, generally solving such feasibility problems may be very costly Goffin et al. (1996). There exists a rich literature concerning quasi-convex optimization in the offline case, Polyak (1967); Zabotin et al. (1972); Khabibullin (1977); Sikorski (1986). A pioneering paper by Nesterov (1984), was the first to suggest an efficient algorithm, namely Normalized Gradient Descent, and prove that this algorithm attains -optimal solution within O(1/ ) iterations given a differentiable quasi-convex objective. This work was later extended by Kiwiel (2001), showing that the same result may be achieved assuming upper semi-continuous quasi-convex objectives. In Konnov (2003) it was shown how to attain faster rates for quasi-convex optimization, but they assume to know the optimal value of the objective, an assumption that generally does not hold in practice.", "startOffset": 159, "endOffset": 762}, {"referenceID": 2, "context": "can be solved by a series of convex feasibility problems Boyd and Vandenberghe (2004); However, generally solving such feasibility problems may be very costly Goffin et al. (1996). There exists a rich literature concerning quasi-convex optimization in the offline case, Polyak (1967); Zabotin et al. (1972); Khabibullin (1977); Sikorski (1986). A pioneering paper by Nesterov (1984), was the first to suggest an efficient algorithm, namely Normalized Gradient Descent, and prove that this algorithm attains -optimal solution within O(1/ ) iterations given a differentiable quasi-convex objective. This work was later extended by Kiwiel (2001), showing that the same result may be achieved assuming upper semi-continuous quasi-convex objectives. In Konnov (2003) it was shown how to attain faster rates for quasi-convex optimization, but they assume to know the optimal value of the objective, an assumption that generally does not hold in practice. Among the deep learning community there have been several attempts to tackle gradientexplosion/plateaus. Ideas spanning gradient-clipping Pascanu et al. (2013), smart initialization Doya (1993), and more, Martens and Sutskever (2011), have shown to improve training in practice.", "startOffset": 159, "endOffset": 1109}, {"referenceID": 2, "context": "(2013), smart initialization Doya (1993), and more, Martens and Sutskever (2011), have shown to improve training in practice.", "startOffset": 29, "endOffset": 41}, {"referenceID": 2, "context": "(2013), smart initialization Doya (1993), and more, Martens and Sutskever (2011), have shown to improve training in practice.", "startOffset": 29, "endOffset": 81}, {"referenceID": 10, "context": "Later we justify this definition by showing that it captures Generalized Linear Models (GLM) regression, see McCullagh and Nelder (1989); Kalai and Sastry (2009).", "startOffset": 109, "endOffset": 137}, {"referenceID": 4, "context": "Later we justify this definition by showing that it captures Generalized Linear Models (GLM) regression, see McCullagh and Nelder (1989); Kalai and Sastry (2009). Definition 3.", "startOffset": 138, "endOffset": 162}, {"referenceID": 4, "context": "In Kalai and Sastry (2009) it is shown that the Perceptron problem with \u03b3-margin is a private case of GLM regression.", "startOffset": 3, "endOffset": 27}, {"referenceID": 10, "context": "2 The Noisy GLM In the noisy GLM setup (see McCullagh and Nelder (1989); Kalai and Sastry (2009)), we may draw i.", "startOffset": 44, "endOffset": 72}, {"referenceID": 4, "context": "2 The Noisy GLM In the noisy GLM setup (see McCullagh and Nelder (1989); Kalai and Sastry (2009)), we may draw i.", "startOffset": 73, "endOffset": 97}, {"referenceID": 12, "context": "Our analysis is simple, enabling us to extend the convergence rate presented in Nesterov (1984) beyond quasi-convex functions.", "startOffset": 80, "endOffset": 96}, {"referenceID": 0, "context": "Note that in the general case when the objective is a sum of quasi-convex functions, the number of local minima of such objective may grow exponentially with the dimension d, see Auer et al. (1996). This might imply that a general setup where each \u03c8 \u223c D is quasi-convex may be generally hard.", "startOffset": 179, "endOffset": 198}, {"referenceID": 15, "context": "method Sutskever et al. (2013), which is considered to be state-of-the-art.", "startOffset": 7, "endOffset": 31}], "year": 2015, "abstractText": "Stochastic convex optimization is a basic and well studied primitive in machine learning. It is well known that convex and Lipschitz functions can be minimized efficiently using Stochastic Gradient Descent (SGD). The Normalized Gradient Descent (NGD) algorithm, is an adaptation of Gradient Descent, which updates according to the direction of the gradients, rather than the gradients themselves. In this paper we analyze a stochastic version of NGD and prove its convergence to a global minimum for a wider class of functions: we require the functions to be quasi-convex and locally-Lipschitz. Quasi-convexity broadens the concept of unimodality to multidimensions and allows for certain types of saddle points, which are a known hurdle for first-order optimization methods such as gradient descent. Locally-Lipschitz functions are only required to be Lipschitz in a small region around the optimum. This assumption circumvents gradient explosion, which is another known hurdle for gradient descent variants. Interestingly, unlike the vanilla SGD algorithm, the stochastic normalized gradient descent algorithm provably requires a minimal minibatch size.", "creator": "LaTeX with hyperref package"}}}