{"id": "1507.02284", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jul-2015", "title": "The Information Sieve", "abstract": "We introduce a new framework for unsupervised learning of deep representations based on a novel hierarchical decomposition of information. Intuitively, data is passed through a series of progressively fine-grained sieves. Each layer of the sieve recovers a single latent factor that is maximally informative about multivariate dependence in the data. The data is transformed after each pass so that the remaining unexplained information trickles down to the next layer. Ultimately, we are left with a set of latent factors explaining all the dependence in the original data and remainder information consisting of independent noise. We present a practical implementation of this framework for discrete variables and apply it to a variety of tasks including independent component analysis, lossy and lossless compression, and predicting missing values in data.", "histories": [["v1", "Wed, 8 Jul 2015 20:00:42 GMT  (4363kb,D)", "http://arxiv.org/abs/1507.02284v1", "12 pages, 9 figures"], ["v2", "Fri, 29 Apr 2016 20:29:36 GMT  (6820kb,D)", "http://arxiv.org/abs/1507.02284v2", "Appearing in Proceedings of the International Conference on Machine Learning (ICML), 2016. v2: Revised presentation and added link to code. 13 pages, 12 figures"], ["v3", "Thu, 9 Jun 2016 00:12:24 GMT  (6817kb,D)", "http://arxiv.org/abs/1507.02284v3", "Appearing in Proceedings of the International Conference on Machine Learning (ICML), 2016. Updated reference to continuous version:this http URL"]], "COMMENTS": "12 pages, 9 figures", "reviews": [], "SUBJECTS": "stat.ML cs.IT cs.LG math.IT", "authors": ["greg ver steeg", "aram galstyan"], "accepted": true, "id": "1507.02284"}, "pdf": {"name": "1507.02284.pdf", "metadata": {"source": "CRF", "title": "The Information Sieve", "authors": ["Greg Ver Steeg", "Aram Galstyan"], "emails": ["gregv@isi.edu", "galstyan@isi.edu"], "sections": [{"heading": null, "text": "In this context, it is worth mentioning that this is a very complex issue, which is a very complex and complex issue."}, {"heading": "1 Information-theoretic learning background", "text": "Using the standard notation [11], Xi denotes a random variable that holds values in a particular domain and whose instances are denoted in lowercase letters, xi. in this essay, the domain of all variables is considered discrete and finite. We abbreviate multivariate random variables, which are typically abbreviated to p (x). We will index various groups of multivariate random variables with superscripts, Xk as defined in Figure 1. X0 denotes the originally observed variables, and in this case we omit the superscript for readability. Entropy is commonly referred to as H (X) by using EX [log 1 / p (x). We use two log arithms so that the unit of information is bits."}, {"heading": "2 Incremental information decomposition", "text": "We consider a series of probability functions of some input variables, X, to be a \"representation\" of X (TC). We consider a representation with a single latent factor, Y. Then we try to store the information in X that is not broken down by Y into the \"residual information.\" The end result is in Cor. 2.4, which states that we can iteratively repeat this procedure (as in Figure 1 (b))) and TC (X) decays into a sum of contributions from each Yk.Theorem 2.1, the cremental decomposition of information Let Y consider some (deterministic) functions of X1,. Xn and let X consider a probable function of Xi, Y, for each i = 1,., n. Then the following upper and lower limit of TC (X): \u2212 n"}, {"heading": "3 Implementing unsupervised representation learning with the sieve", "text": "This year it has come to the point that it will be able to eren.n"}, {"heading": "4 Independent components as a byproduct of efficient coding", "text": "In fact, most of us will be able to find ourselves in the position in which we find ourselves."}, {"heading": "5 Using lossy compression to mimic generative models on MNIST digits", "text": "The information screen is not a generative probabilistic model. We construct latent factors that are functions of the data in a way that maximizes the (multivariate) information that is preserved. Nevertheless, due to the way the residual information is constructed, we can execute the screen in reverse order to create new examples (that is, we interpret a generative model as lossy compression), we can use this lossy compression interpretation for mixing tasks and generate new examples (that is, we interpret the generative model as lossy compression)."}, {"heading": "6 Lossless compression", "text": "This year it is more than ever before."}, {"heading": "7 Related work", "text": "The principle of explaining correlation has recently been introduced as a goal for learning unattended representation [16, 15]. While previous work has to determine the depth and number of latent factors in representation, residual information allows us to gradually build up representation by learning the depth and number of factors required. In addition to providing a more flexible approach to representation and structuring learning, the information screen is able to solve more general tasks, including lossless and lossless compression and prediction. Another interesting related result showed that the positivity of the quantity TC (X; Y) (the same quantity that appears within our limits) implies that the X share has a common ancestor in each DAG that is consistent with pX (x) [24]. Another line of work on information decomposition focuses on the distinction of synergy and redundancy [25], although these measures are usually impossible to estimate the difference."}, {"heading": "8 Conclusion", "text": "We introduced the information screen, which enables the decomposition of multivariate information that is practicable for high-dimensional data in terms of computational costs and sample complexity. We examined some of the immediate effects of this decomposition. First, we saw that a natural idea of \"residual information\" emerges and that it allows us to gradually extract information. Several different applications were demonstrated and appear promising for in-depth exploration. Finally, the screen provides an exponentially faster method than the most well-known discrete ICA algorithm (though with no guarantees of global optimism). We also showed that the screen defines both lossless and lossless compression schemes. Finally, the information screen proposes a new framework for understanding the problem of uncontrolled learning of deep representation. Among the many definitions from the standard representation, some characteristics stand out, representations are gradually learned and the depth and structure arise in a data-based way."}, {"heading": "A Proof of Theorem 2.1", "text": "We begin by applying a general definition of \"representations\" and remembering a useful theorem in relation to them. (Definition) The random variables Y (1),.., Ym form a representation of X \u2212 \u2212 if the common distribution is factored in, p (x, y) = 1 p (yj | x) p (x), o (x), o (x), o (x), o (x), o (X), o (X), o (X). A representation is fully defined by the domains of variables and conditional probability tables, p (yj | x) p (x). Theorem A.1. Basic decomposition of information [16], if Y is a representation of X and we define Xi (Xi) Yj, o (Y) n (Y) n (Y). (Y: Xi)."}, {"heading": "B An algorithm for perfect reconstruction of remainder information", "text": "The objective is to construct the rest of the information as a probable function of Xi, Y, so that we can comply with the terms of Lemma 2.2, (i) I (X) I (X) I (X) I (X) I (X) I (X) I (X) I (X) I (X) I (X) I (X) I (X) I (X) I (X) I (X) I) I (X) I (X) I (X) I (X) I (X) I) I (X) I (X) I (X) I) I (X) I (I) I (I) I (I) I (I) I (I) I (X) X (X) I (X) I) (X) X (X) I (X) I (X) (X) (X) (X) (X) (X) (X) I (X) (I) (I) (I) (I) I (I) I (X) I (I) I (I) I (X) I (I) I (X) I (I) I (X) I (I) I (X) I (I) I (X) I (I) I (X) I (X) I (X) I (X) I (X) I (X) I (X) I (X) X (X) I (X) X) (X) (X) I (X) (X) X (X) X) (X) (X) (X) (X) (X) (X) (X) (X) I (X) (X) X) (X) X (X) (X) X (X) (X) X (X) (X) X (X) (I (X) X) (X) X) (X) (X) (X) (X) (I (X) X) (X) (X) (X) (X) (I (I (I) (I) (I) (I) (I) (I) (I) (I (I) (I) (I) (I) (I) (I) (I) (I) (I (I) (I) (I) (I)"}], "references": [{"title": "Unsupervised learning", "author": ["Horace Barlow"], "venue": "Neural computation,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1989}, {"title": "Natural image statistics and neural representation", "author": ["Eero Simoncelli", "Bruno Olshausen"], "venue": "Annu. Rev. Neurosci.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Deep learning in neural networks: An overview", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Representation learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": "Pattern Analysis and Machine Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Self-organization in a perceptual network", "author": ["Ralph Linsker"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1988}, {"title": "An information-maximization approach to blind separation and blind deconvolution", "author": ["Anthony J Bell", "Terrence J Sejnowski"], "venue": "Neural computation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}, {"title": "Information theoretical analysis of multivariate correlation", "author": ["Satosi Watanabe"], "venue": "IBM Journal of research and development,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1960}, {"title": "Independent component analysis, a new concept", "author": ["Pierre Comon"], "venue": "Signal processing,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1994}, {"title": "Independent component analysis: algorithms and applications", "author": ["Aapo Hyv\u00e4rinen", "Erkki Oja"], "venue": "Neural networks,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2000}, {"title": "Generalized binary independent component analysis", "author": ["Amichai Painsky", "Saharon Rosset", "Meir Feder"], "venue": "In Information Theory (ISIT),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Demystifying information-theoretic clustering", "author": ["Greg Ver Steeg", "Aram Galstyan", "Fei Sha", "Simon DeDeo"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Deep learning and the information bottleneck principle", "author": ["Naftali Tishby", "Noga Zaslavsky"], "venue": "arXiv preprint arXiv:1503.02406,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Discovering structure in high-dimensional data through correlation explanation", "author": ["Greg Ver Steeg", "Aram Galstyan"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Maximally informative hierarchical representations of highdimensional data", "author": ["Greg Ver Steeg", "Aram Galstyan"], "venue": "In Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Hierarchical clustering using mutual information", "author": ["Alexander Kraskov", "Harald St\u00f6gbauer", "Ralph G Andrzejak", "Peter Grassberger"], "venue": "EPL (Europhysics Letters),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Feature extraction through lococode", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1999}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "A mathematical theory of communication", "author": ["C.E. Shannon"], "venue": "The Bell System Technical Journal,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1948}, {"title": "A universal algorithm for sequential data compression", "author": ["Jacob Ziv", "Abraham Lempel"], "venue": "IEEE Transactions on information theory,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1977}, {"title": "A method for the construction of minimum redundancy codes", "author": ["David A Huffman"], "venue": "Proceedings of the IRE,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1952}, {"title": "Information-theoretic inference of common", "author": ["Bastian Steudel", "Nihat Ay"], "venue": "ancestors. Entropy,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Nonnegative decomposition of multivariate information", "author": ["P.L. Williams", "R.D. Beer"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "The hope of finding a succinct principle that elucidates the brain\u2019s information processing abilities has often kindled interest in information-theoretic ideas [1, 2].", "startOffset": 160, "endOffset": 166}, {"referenceID": 1, "context": "The hope of finding a succinct principle that elucidates the brain\u2019s information processing abilities has often kindled interest in information-theoretic ideas [1, 2].", "startOffset": 160, "endOffset": 166}, {"referenceID": 2, "context": "In machine learning, on the other hand, the past decade has witnessed a shift in focus towards expressive, hierarchical models with tractable update rules, with successes driven by increasingly effective ways to leverage labeled data to learn rich models [3, 4].", "startOffset": 255, "endOffset": 261}, {"referenceID": 3, "context": "In machine learning, on the other hand, the past decade has witnessed a shift in focus towards expressive, hierarchical models with tractable update rules, with successes driven by increasingly effective ways to leverage labeled data to learn rich models [3, 4].", "startOffset": 255, "endOffset": 261}, {"referenceID": 4, "context": "Information-theoretic ideas like the venerable InfoMax principle [5, 6] can be and are applied in both contexts but they do not shed much light on the questions of when and why deep representations are useful for learning.", "startOffset": 65, "endOffset": 71}, {"referenceID": 5, "context": "Information-theoretic ideas like the venerable InfoMax principle [5, 6] can be and are applied in both contexts but they do not shed much light on the questions of when and why deep representations are useful for learning.", "startOffset": 65, "endOffset": 71}, {"referenceID": 6, "context": "The shift in perspective that enables our information decomposition is to focus on how well the learned representation explains multivariate mutual information in the data (a measure originally introduced as \u201ctotal correlation\u201d [7]).", "startOffset": 228, "endOffset": 231}, {"referenceID": 7, "context": "Because we end up with independent factors, one can also view this decomposition as a new way to do independent component analysis (ICA) [8, 9].", "startOffset": 137, "endOffset": 143}, {"referenceID": 8, "context": "Because we end up with independent factors, one can also view this decomposition as a new way to do independent component analysis (ICA) [8, 9].", "startOffset": 137, "endOffset": 143}, {"referenceID": 9, "context": "The implementation we develop here uses only discrete variables and is therefore most relevant for the challenging problem of ICA with discrete variables, which has applications to compression [10].", "startOffset": 193, "endOffset": 197}, {"referenceID": 4, "context": "The \u201cInfoMax\u201d principle [5, 6] suggests that for unsupervised learning we should construct Y \u2019s to maximize their mutual information with X , the data.", "startOffset": 24, "endOffset": 30}, {"referenceID": 5, "context": "The \u201cInfoMax\u201d principle [5, 6] suggests that for unsupervised learning we should construct Y \u2019s to maximize their mutual information with X , the data.", "startOffset": 24, "endOffset": 30}, {"referenceID": 10, "context": "Despite its intuitive appeal, this approach has several potential problems (see [12] for one example).", "startOffset": 80, "endOffset": 84}, {"referenceID": 11, "context": "Here we focus on the fact that the InfoMax principle is not very useful for characterizing \u201cdeep representations\u201d, even though it is often invoked in this context [13].", "startOffset": 163, "endOffset": 167}, {"referenceID": 12, "context": "This follows directly from the data processing inequality (a similar argument appears in [14]).", "startOffset": 89, "endOffset": 93}, {"referenceID": 13, "context": "Instead of maximizing I(X;Y ), the recently introduced principle of total Correlation Explanation (CorEx) [15, 16] suggests to construct Y \u2019s that explain the multivariate dependence in X according to a multivariate measure of mutual information first introduced as \u201ctotal correlation\u201d [7].", "startOffset": 106, "endOffset": 114}, {"referenceID": 14, "context": "Instead of maximizing I(X;Y ), the recently introduced principle of total Correlation Explanation (CorEx) [15, 16] suggests to construct Y \u2019s that explain the multivariate dependence in X according to a multivariate measure of mutual information first introduced as \u201ctotal correlation\u201d [7].", "startOffset": 106, "endOffset": 114}, {"referenceID": 6, "context": "Instead of maximizing I(X;Y ), the recently introduced principle of total Correlation Explanation (CorEx) [15, 16] suggests to construct Y \u2019s that explain the multivariate dependence in X according to a multivariate measure of mutual information first introduced as \u201ctotal correlation\u201d [7].", "startOffset": 286, "endOffset": 289}, {"referenceID": 6, "context": "That TC(X) can be hierarchically decomposed in terms of short and long range dependencies was already appreciated by Watanabe [7] and has been used in applications such as hierarchical clustering [17].", "startOffset": 126, "endOffset": 129}, {"referenceID": 15, "context": "That TC(X) can be hierarchically decomposed in terms of short and long range dependencies was already appreciated by Watanabe [7] and has been used in applications such as hierarchical clustering [17].", "startOffset": 196, "endOffset": 200}, {"referenceID": 14, "context": "The quantities of the form TC(X;Yk) can be estimated and optimized over efficiently [16], despite involving highdimensional variables.", "startOffset": 84, "endOffset": 88}, {"referenceID": 14, "context": "Surprisingly, optimizing this objective over possible functions has a simple, iterative solution procedure that is guaranteed to find a local maximum of the objective in time linear in the number of variables [16].", "startOffset": 209, "endOffset": 213}, {"referenceID": 14, "context": "1 Not only that, but a byproduct of the procedure is to give us a value for the objective TC(X;Yk), which can be estimated even from a small number of samples [16].", "startOffset": 159, "endOffset": 163}, {"referenceID": 8, "context": "Note that PCA fails to recover the sources for this example while ICA (the FastICA algorithm [9]) also succeeds.", "startOffset": 93, "endOffset": 96}, {"referenceID": 0, "context": "Going back at least to Barlow [1], it was recognized that if X is transformed into some other basis, Y , with the Y \u2019s independent (TC(Y ) = 0), then the coding cost in this new basis is H(Y ) = \u2211 j H(Yj), i.", "startOffset": 30, "endOffset": 33}, {"referenceID": 7, "context": "This is exactly the problem of independent component analysis: transform the data into a basis for which TC(Y ) = 0, or is minimized [8, 9].", "startOffset": 133, "endOffset": 139}, {"referenceID": 8, "context": "This is exactly the problem of independent component analysis: transform the data into a basis for which TC(Y ) = 0, or is minimized [8, 9].", "startOffset": 133, "endOffset": 139}, {"referenceID": 16, "context": "That independence could be achieved as a byproduct of efficient coding has been previously considered [19].", "startOffset": 102, "endOffset": 106}, {"referenceID": 9, "context": "Recent state-of-the-art results lower the complexity of this problem to only a single exponential in the number of variables [10].", "startOffset": 125, "endOffset": 129}, {"referenceID": 9, "context": "guarantees a global optimum [10].", "startOffset": 28, "endOffset": 32}, {"referenceID": 7, "context": "The more traditional scenario for ICA is to consider a reconstruction problem where some (typically continuous) and independent source variables are linearly mixed according to some unknown matrix [8, 9].", "startOffset": 197, "endOffset": 203}, {"referenceID": 8, "context": "The more traditional scenario for ICA is to consider a reconstruction problem where some (typically continuous) and independent source variables are linearly mixed according to some unknown matrix [8, 9].", "startOffset": 197, "endOffset": 203}, {"referenceID": 17, "context": "We can use this lossy compression interpretation to mimic tasks traditionally performed by generative models including in-painting and generating new examples (the converse, interpreting a generative model as lossy compression, has also been considered [20]).", "startOffset": 253, "endOffset": 257}, {"referenceID": 18, "context": "each pixel gives us an estimate of how many bits are required for this compression scheme, about 298 bits [21].", "startOffset": 106, "endOffset": 110}, {"referenceID": 19, "context": "Measure Naive Shannon gzip [22] Huffman [23] Sieve Compression ratio 2.", "startOffset": 27, "endOffset": 31}, {"referenceID": 20, "context": "Measure Naive Shannon gzip [22] Huffman [23] Sieve Compression ratio 2.", "startOffset": 40, "endOffset": 44}, {"referenceID": 19, "context": "For comparison we consider two standard compression schemes, gzip, based on Lempel-Ziv coding [22], and Huffman coding [23].", "startOffset": 94, "endOffset": 98}, {"referenceID": 20, "context": "For comparison we consider two standard compression schemes, gzip, based on Lempel-Ziv coding [22], and Huffman coding [23].", "startOffset": 119, "endOffset": 123}, {"referenceID": 14, "context": "The principle of correlation explanation was recently introduced as an objective for unsupervised representation learning [16, 15].", "startOffset": 122, "endOffset": 130}, {"referenceID": 13, "context": "The principle of correlation explanation was recently introduced as an objective for unsupervised representation learning [16, 15].", "startOffset": 122, "endOffset": 130}, {"referenceID": 21, "context": "Another interesting related result showed that positivity of the quantity TC(X;Y ) (the same quantity appearing in our bounds) implies that the X\u2019s share a common ancestor in any DAG consistent with pX(x) [24].", "startOffset": 205, "endOffset": 209}, {"referenceID": 22, "context": "A different line of work about information decomposition focuses on distinguishing synergy and redundancy [25], though these measures are typically impossible to estimate for high-dimensional systems.", "startOffset": 106, "endOffset": 110}, {"referenceID": 14, "context": "The information bottleneck (IB) [26] is another information-theoretic optimization for constructing representations of data that has many mathematical similarities to the CorEx approach [16], with the main difference being that IB focuses on supervised learning while the latter is an unsupervised learning approach.", "startOffset": 186, "endOffset": 190}, {"referenceID": 12, "context": "Recently, the IB principle was used to investigate the value of depth in the context of supervised learning [14].", "startOffset": 108, "endOffset": 112}], "year": 2017, "abstractText": "We introduce a new framework for unsupervised learning of deep representations based on a novel hierarchical decomposition of information. Intuitively, data is passed through a series of progressively fine-grained sieves. Each layer of the sieve recovers a single latent factor that is maximally informative about multivariate dependence in the data. The data is transformed after each pass so that the remaining unexplained information trickles down to the next layer. Ultimately, we are left with a set of latent factors explaining all the dependence in the original data and remainder information consisting of independent noise. We present a practical implementation of this framework for discrete variables and apply it to a variety of tasks including independent component analysis, lossy and lossless compression, and predicting missing values in data. The hope of finding a succinct principle that elucidates the brain\u2019s information processing abilities has often kindled interest in information-theoretic ideas [1, 2]. In machine learning, on the other hand, the past decade has witnessed a shift in focus towards expressive, hierarchical models with tractable update rules, with successes driven by increasingly effective ways to leverage labeled data to learn rich models [3, 4]. Information-theoretic ideas like the venerable InfoMax principle [5, 6] can be and are applied in both contexts but they do not shed much light on the questions of when and why deep representations are useful for learning. We introduce a novel incremental and hierarchical decomposition of information and show that it defines a framework for unsupervised learning of deep representations in which the contribution of each layer can be precisely quantified. Moreover, this scheme automatically determines the structure and depth among hidden units in the representation based only on local learning rules. The shift in perspective that enables our information decomposition is to focus on how well the learned representation explains multivariate mutual information in the data (a measure originally introduced as \u201ctotal correlation\u201d [7]). Intuitively, our approach constructs a hierarchical representation of data by passing it through a sequence of progressively fine-grained sieves. At the first layer of the sieve we learn a factor that explains as much of the dependence in the data as possible. The data is then transformed into the \u201cremainder information\u201d, which has this dependence extracted. The next layer of the sieve looks for the largest source of dependence in the remainder information, and the cycle repeats. At each step, we obtain a successively tighter upper and lower bound on the multivariate information in the data, with convergence between the bounds obtained when the remaining information consists of nothing but independent factors. Because we end up with independent factors, one can also view this decomposition as a new way to do independent component analysis (ICA) [8, 9]. Unlike traditional methods, we do not assume a specific generative model of the data (i.e., that it consists of a linear transformation of independent sources) and we extract independent factors incrementally rather than all at once. The implementation we develop here uses only discrete variables and is therefore most relevant for the challenging problem of ICA with discrete variables, which has applications to compression [10]. 1 ar X iv :1 50 7. 02 28 4v 1 [ st at .M L ] 8 J ul 2 01 5 After introducing some background in Sec. 1, we introduce a new way to iteratively decompose the information in data in Sec. 2, and show how to use these decompositions to define a practical and incremental framework for unsupervised representation learning in Sec. 3. We demonstrate the versatility of this framework by applying it first to independent component analysis (Sec. 4). Next, we use the sieve as a lossy compression to mimic the traditional strengths of generative models including in-painting and generating new samples (Sec. 5). Finally, we cast the sieve as a lossless compression and show that it beats standard compression schemes on a benchmark task (Sec. 6). 1 Information-theoretic learning background Using standard notation [11], capital Xi denotes a random variable taking values in some domain and whose instances are denoted in lowercase, xi. In this paper, the domain of all variables are considered to be discrete and finite. We abbreviate multivariate random variables, X \u2261 X1:n \u2261 X1, . . . , Xn, with an associated probability distribution, pX(X1 = x1, . . . , Xn = xn), which is typically abbreviated to p(x). We will index different groups of multivariate random variables with superscripts, X, as defined in Fig. 1. We let X denote the original observed variables and we often omit the superscript in this case for readability. Entropy is defined in the usual way as H(X) \u2261 EX [log 1/p(x)]. We use base two logarithms so that the unit of information is bits. Higher-order entropies can be constructed in various ways from this standard definition. For instance, the mutual information between two groups of random variables, X and Y can be written as the reduction of uncertainty in one variable, given information about the other, I(X;Y ) = H(X)\u2212H(X|Y ). The \u201cInfoMax\u201d principle [5, 6] suggests that for unsupervised learning we should construct Y \u2019s to maximize their mutual information with X , the data. Despite its intuitive appeal, this approach has several potential problems (see [12] for one example). Here we focus on the fact that the InfoMax principle is not very useful for characterizing \u201cdeep representations\u201d, even though it is often invoked in this context [13]. This follows directly from the data processing inequality (a similar argument appears in [14]). Namely, if we start with X , construct a layer of hidden units Y 1 that are a function of X , and continue adding layers to a stacked representation so that X \u2192 Y 1 \u2192 Y 2 . . . Y , then the information that the Y \u2019s have about X cannot increase after the first layer, I(X;Y ) = I(X;Y ). From the point of view of mutual information, Y 1 is a copy and Y 2 is just a copy of a copy. While a coarse-grained copy might be useful, the InfoMax principle does not quantify how or why. Instead of maximizing I(X;Y ), the recently introduced principle of total Correlation Explanation (CorEx) [15, 16] suggests to construct Y \u2019s that explain the multivariate dependence in X according to a multivariate measure of mutual information first introduced as \u201ctotal correlation\u201d [7]. TC(X) \u2261 DKL ( p(x)|| n \u220f", "creator": "LaTeX with hyperref package"}}}