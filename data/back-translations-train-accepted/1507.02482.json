{"id": "1507.02482", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jul-2015", "title": "Differentially Private Ordinary Least Squares", "abstract": "Linear regression is one of the most prevalent techniques in data analysis. Given a collection of samples composed of features $x$ and a label $y$, linear regression is used to find the best prediction of the label as a linear combination of the features. However, it is also common to use linear regression for its explanatory capabilities rather than label prediction. Ordinary Least Squares (OLS) is often used in statistics to establish a correlation between an attribute (e.g. gender) and a label (e.g. income) in the presence of other features. OLS uses linear regression in order to estimate the correlation between the label and a feature $x_j$ on a given dataset; and then, under the assumption of a certain generative model for the data, OLS outputs an interval that is likely to contain the correlation between $y$ and $x_j$ in the underlying distribution (a confidence interval). When this interval does not intersect the origin, we can reject the null hypothesis as it is likely that $x_j$ has a non-zero correlation with $y$.", "histories": [["v1", "Thu, 9 Jul 2015 12:32:19 GMT  (41kb)", "https://arxiv.org/abs/1507.02482v1", null], ["v2", "Mon, 2 Nov 2015 02:19:03 GMT  (46kb)", "http://arxiv.org/abs/1507.02482v2", null], ["v3", "Wed, 25 Nov 2015 00:24:42 GMT  (46kb)", "http://arxiv.org/abs/1507.02482v3", null], ["v4", "Mon, 21 Aug 2017 21:30:27 GMT  (605kb,D)", "http://arxiv.org/abs/1507.02482v4", null]], "reviews": [], "SUBJECTS": "cs.DS cs.CR cs.LG", "authors": ["or sheffet"], "accepted": true, "id": "1507.02482"}, "pdf": {"name": "1507.02482.pdf", "metadata": {"source": "META", "title": "Differentially Private Ordinary Least Squares", "authors": ["Or Sheffet"], "emails": ["<osheffet@ualberta.ca>."], "sections": [{"heading": "1. Introduction", "text": "It is therefore no surprise that several of the first differentially private algorithms based on data related to the ubiquitous problem of linear regression (Kasiviswanathan et al., 2008; Chaudhuri et al., 2011; Kifer et al., 2012; Bass-1Computing Science Dept., University of Alberta, Edmonton AB, Canada. This work was done when the author was at Harvard University, supported by NSF CNS-123723. Correspondence to: Or Sheffet < osheffet @ ualberta.ca @.ily et al., 2014: All existing work based on different linear regression measurements by showing the distance between linear regressors."}, {"heading": "2. Preliminaries and OLS Background", "text": "This is not the first time that the distribution effects are so large, that the distribution effects are so large, that the distribution effects are so large, that the distribution effects are so large, that the distribution effects are so large, that the distribution effects are so large, that we also use ej and ek to name elements of the natural base (unit length vector in the direction of the coordinate length distribution).p We use the distribution effects of algorithms 1 and 2, and use the distribution effects of the distribution effects to dense distribution effects (which relate to bad events affecting the distribution effects).p The distribution effects of the distribution effects are so large that the distribution effects are such that the distribution effects are large, and the distribution effects are such that the distribution effects are large."}, {"heading": "3. OLS over Projected Data", "text": "In this section we will deal with the problem of linear regression derived from the projected data."}, {"heading": "3.1. Setting the Value of r, Deriving a Bound on n", "text": "Comparing the lower boundary given by Theorem with the boundary of Theorem 2.2, the data-dependent boundary of n (c) + (2) + (2) + (2) + (2) + (2) + (2) + (2) + (2) + (2) + (2) + (2) + (2) + (2) + (2) + (2) + (2) + (2) + (2) + (2) + (2) + (2) + (2) + (2) + (2) + (2) + (2) + (2) + (2) + (2) + (2) + 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+) () (+) (2) () (+) (2) () (+) () (2) () (+) () () () ()) () () () ()) () ()) () () ()) () () ()) () () ()) () () ()) () () ()) () () ()) () () ()) () () () () ()) () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () (() () () () (() () () (() () () (() () () (() () () (() () () (() () () () ((() () () () () () ((() () () (() ()) ((() ())) ((())"}, {"heading": "4. Projected Ridge Regression", "text": "We now turn to the case that our matrix does not overcome the problem of the approximate nature of algorithms 1 and 2. (In this case, the matrix is connected to a d \u00b7 d \u00b7 matrix, which is wId \u00b7 d \u00b2 matrix. (In this case, the matrix is connected to a d \u00b7 d \u00b2 matrix, which is wId \u00b7 d \u00b2 matrix. (Similarly, the matrix is with the default assumption of y = X\u03b2 + e and ei sampled i.d of N (0, 2) we must now introduce an additional notation. We denote the applied matrix and vectors X \u00b2 p and y \u00b2 s \u00b2 Rn, using the default assumption of y = X\u03b2 + e and ei sampled i.d of N (0, 2). And so we use the output of algorithms 1, we solve the linear regression problem derived."}, {"heading": "5. Confidence Intervals for \u201cAnalyze Gauss\u201d", "text": "In this section we analyze the \"Analyze Gauss\" algorithm by Dwork et al (2014). (\u03b2) Algorithm 2 works by adding random noise to ATA, where the noise is symmetrical with each coordinate above the diagonal. (\u03b2) While we add the results of algorithm 2 as X-TX-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X X-X-X-X"}, {"heading": "Acknowledgements", "text": "The author warmly thanks Prof. Salil Vadhan for his tremendous help in designing this paper. The author also thanks Prof. Jelani Nelson and the members of the \"Privacy Tools for Sharing Research Data\" project at Harvard University (in particular James Honaker, Vito D'Orazio, Vishesh Karwa, Prof. Kobbi Nissim and Prof. Gary King) for many helpful discussions and suggestions; and Abhradeep Thakurta for clarifying the similarity between our findings. Finally, the author thanks the anonymous speakers for many helpful suggestions in general and for a reference to (Ullman, 2015) in particular."}, {"heading": "A. Extended Introductory Discussion", "text": "For reasons of space, some details have been omitted from the introductory parts (sections 1,2), which we reproduce in this appendix. We recommend in particular the uninformed reader to think about the extended OLS background that we provide in Appendix A.3."}, {"heading": "A.1. Proof Of Privacy of Algorithm 1", "text": "The proof of the theorem is based on the fact that algorithm 1 is the result of the compilation of the differentiated private propose test release algorithm of (Dwork & Lei, 2009) with the differentiated private analysis of the Johnson-Lindenstrauss transformation of (Sheffet, 2015). More specifically, we use theorem B.1 from (Sheffet, 2015), which states that a matrix A whose total singular values are greater than T (, \u03b4), in the T (, \u03b4) 2 = 2B2 (\u2264 2r ln (4 / \u03b4) + 2 ln (4 / \u03b4), the publication of RA (, Connecticut) -differentiated is private for an r-row matrix R whose entries are sampled i.i.d normal Gaussians. Since we know that all singular values of A \u00b2 are greater than those specified (TB-1)."}, {"heading": "A.2. Omitted Preliminary Details", "text": "Linear algebra and pseudo-inverses. Faced with a matrix M, we call its SVD M = UPS T with U and V as orthonormal matrices and S as a non-negative diagonal matrix whose entries are the singular values of M. We use \u03c3max (M) and \u03c3min (M) to denote the largest and smallest singular value of M. We use M + to denote the Moore-Penrose distribution of M, defined as M + = V, where S \u2212 1 is a matrix with S \u2212 1j, j = 1 / Sj, j for each j s.t. Sj, j > 0.The Gaussian distribution."}, {"heading": "A.3. Detailed Background on Ordinary Least Squares", "text": "For the unfamiliar reader, we give a brief description of the model under which the OLS operates, as well as the limits of trust that can be derived with the OLS. This is by no means an exemplary account of the OLS, and we refer to the existence of a pdimensional vector. (Rao, 1973; Muller & Stewart, 2006).In view of n observations {(xi, yi) ni = 1, where we use a pdimensional vector \u03b2 Rp s.t. for all i, the label yi was derived from yi = \u03b2Txi + ei, where ei N (0, \u03c32) is independent (also known as the homoscedastic Gaussian model).We use the matrix notation, where X denotes the (n \u00b7 p) matrix whose rows are xi, and the use y, e Rn to name the vectors whose i-th entry is yi and ei resp. To simplify the discussion, we assume."}, {"heading": "Here c\u03b1 denotes the number for which\u222b c\u03b1", "text": "\u2212 c\u03b1 PDFTn \u2212 p (x) dx = 1 \u2212 \u03b1. (If we are satisfied with an approximate concentration of Tn \u2212 p with a normal Gaussian value, then we can set such a value. \u2212 p (x) dx = 1 \u2212 \u03b1 (1 / \u03b1). The discussion above shows that we can be satisfied with an approximate Tn \u2212 p with a normal Gaussian value, as if we could show an approximate difference between the two values. (xp) The discussion shows that we have an approximate difference between the two values. (xp) \u2212 p (xp) \u2212 p (xp) - (xp) - a sufficient condition for rejecting the null hypothesis is that we must have a sufficient difference between the two values."}, {"heading": "B. Projecting the Data using Gaussian Johnson-Lindenstrauss Transform", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1. Main Theorem Restated and Further Discussion", "text": "Theory B.TheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheThe"}, {"heading": "B.2. Proof of Theorem 3.1", "text": "We now turn to our analysis of \"X\" and \"P,\" where our goal is to show that the distribution of the t \"values as in Theorem 3.1 (+ 1) and\" X \"(+ 2) (+ 2) (+ 3) and\" X \"(+ 3), (+ 3), (+ 3) and\" X \"(+ 3), (+ 3), (+ 3), (+ 3), (+ 3), (+ 3), (+ 3), (+ 3), (+ 4), (+ 4), (+ 4), (+ 4), (3), (3), (3), (3), (3), (3), (3), (3), (3), (3), (3), (3), (3, (3), (3, (), (3), (3, (3), (), (3, (), (3, (3), (), (3, (), (3), (3, (), (3), (3, (), (3), (3, (), (), (3, (3), (), (3, (), (3), (), (3, (), (), (3, (), (3), (), (), (3, (), (), (3, (), (), (3, (), (), (, (), ((3), (), (3, (), (), (, (, (),), (, (), ((,),), ((((((),),), ((,), (,),), ((((,), ((((,),),), ((, ((((,),),), (((((((,),),),), (, (((((,),),), (((((,),),), (((((,),),), ((((((((,),),),"}, {"heading": "B.3. Proof of Theorem 3.3", "text": "Theorem B.7 (Theorem 3.3 redefined). Fix a positive definitive matrix (Rp \u00b7 p.) Fix parameters \u03b2 Rp and \u03c32 > 0 and a coordinate j \u2212 t \u03b2j = 0. Let X be a matrix whose n lines are sampled i.i.d by N (0p, \u03a3). Let y be a vector s.t. yi so that there are constants C1, C2 and C4 when we have algorithm 1 via [X; y] withparameter r w.p 1 \u2212 vomN (0, 1 / 2). Fix \u03bd (0, 1 / 2) and \u03b1 (0, 1 / 2). Then there are constants C1, C3 and C4 so that we have algorithms 1 via [X; y] withparameter r w.p."}, {"heading": "C. Projected Ridge Regression", "text": "In this section, we deal with the case that our matrix is no longer a single problem, but a single problem that we are able to solve. \"(\" It \"),\" It \"(\" It \"),\" It \"(\" It \"),\" It \"(\" It \"),\" It \"(\" It \"),\" It \"(\" It \"),\" It \"(\" It \"),\" It \"(\" It \"),\" It \"(\" It \"),\" It \"(\" It \"),\" It \"(\" It \"),\" It \"(\" It \"(\"), \"It\" (\"),\" (\") (\"), \"(\") (\") (\") (\") (\") (\") (\") (\") (\") (\")"}, {"heading": "C.1. Running OLS on the Projected Data", "text": "In this section we analyze the projected ridge regression under the premise (for the moment) that e (for the moment) is fixed, that is, we assume that the only source of randomness from the selection of the matrix R = R1 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "C.2. Conditions for Deriving a Confidence Interval for Ridge Regression", "text": "(1), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (, (2), (2), (2), (2), (2), (2), (2), (2), (, (2), (2), (2), (, (2), (2), (2), (2), (2), (2), (2), (2), (2),"}, {"heading": "D. Confidence Intervals for \u201cAnalyze Gauss\u201d Algorithm", "text": "To complete the picture, we now analyze the \"Analyze Gauss\" algorithm by Dwork et al. (Dwork et al., 2014). Algorithm 2 works by adding random noise to ATA, where the noise is symmetrical with each coordinate above the diagonal sampled i.i.d of N (0, 2) with \"2\" (B4 log (1 / 3) 2).20 Using the same notation for a sub-matrix of A as [X; y] as before, with \"X\" Rn \u00b7 p and y \"Rn, we denote the output of algorithm 2 as X\" TX. \""}, {"heading": "E. Experiment: Additional Figures", "text": "To complete our discussion of the experiments we are conducting, we add additional numbers at this point, showing both the t-value approximations we get from both algorithms and the \"high-level decision\" of whether or not we correctly reject the null hypothesis (and with what sign). First, in Figure 2, we show the distribution of the t-value approximation for coordinates we should reject, and then the decision of whether or not we reject on the basis of this t-value - and whether they are right, conservative (we did not reject them when we needed them) or wrong (we rejected them with the wrong sign or if we should not have rejected them) is in Figure 3. As you can see, algorithm 1 has much lower t-values (as expected) and is therefore much more conservative. In fact, it tends not to reject the coordinate 1 of the real data, even to the largest value of n (Figure 3c)."}], "references": [{"title": "Statistical Methods for the Social Sciences", "author": ["A. Agresti", "B. Finlay"], "venue": null, "citeRegEx": "Agresti and Finlay,? \\Q2009\\E", "shortCiteRegEx": "Agresti and Finlay", "year": 2009}, {"title": "Private empirical risk minimization: Efficient algorithms and tight error bounds", "author": ["R. Bassily", "A. Smith", "A. Thakurta"], "venue": "In FOCS,", "citeRegEx": "Bassily et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bassily et al\\.", "year": 2014}, {"title": "The Johnson-Lindenstrauss transform itself preserves differential privacy", "author": ["J. Blocki", "A. Blum", "A. Datta", "O. Sheffet"], "venue": "In FOCS,", "citeRegEx": "Blocki et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Blocki et al\\.", "year": 2012}, {"title": "Convergence rates for differentially private statistical estimation", "author": ["Chaudhuri", "Kamalika", "Hsu", "Daniel J"], "venue": "In ICML,", "citeRegEx": "Chaudhuri et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2012}, {"title": "Differentially private empirical risk minimization", "author": ["Chaudhuri", "Kamalika", "Monteleoni", "Claire", "Sarwate", "Anand D"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Chaudhuri et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2011}, {"title": "Local privacy and statistical minimax rates", "author": ["Duchi", "John C", "Jordan", "Michael I", "Wainwright", "Martin J"], "venue": "In FOCS, pp", "citeRegEx": "Duchi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2013}, {"title": "Differential privacy and robust statistics", "author": ["C. Dwork", "J. Lei"], "venue": "In STOC,", "citeRegEx": "Dwork and Lei,? \\Q2009\\E", "shortCiteRegEx": "Dwork and Lei", "year": 2009}, {"title": "Our data, ourselves: Privacy via distributed noise generation", "author": ["Dwork", "Cynthia", "Kenthapadi", "Krishnaram", "McSherry", "Frank", "Mironov", "Ilya", "Naor", "Moni"], "venue": "In EUROCRYPT,", "citeRegEx": "Dwork et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2006}, {"title": "Calibrating noise to sensitivity in private data analysis", "author": ["Dwork", "Cynthia", "Mcsherry", "Frank", "Nissim", "Kobbi", "Smith", "Adam"], "venue": "In TCC,", "citeRegEx": "Dwork et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2006}, {"title": "Analyze gauss - optimal bounds for privacy preserving principal component analysis", "author": ["Dwork", "Cynthia", "Talwar", "Kunal", "Thakurta", "Abhradeep", "Zhang", "Li"], "venue": "In STOC,", "citeRegEx": "Dwork et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2014}, {"title": "Private false discovery rate control", "author": ["Dwork", "Cynthia", "Su", "Weijie", "Zhang", "Li"], "venue": "CoRR, abs/1511.03803,", "citeRegEx": "Dwork et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2015}, {"title": "Ridge regression: Biased estimation for nonorthogonal problems", "author": ["A.E. Hoerl", "R.W. Kennard"], "venue": "Technometrics, 12:55\u201367,", "citeRegEx": "Hoerl and Kennard,? \\Q1970\\E", "shortCiteRegEx": "Hoerl and Kennard", "year": 1970}, {"title": "What can we learn privately", "author": ["S. Kasiviswanathan", "H. Lee", "K. Nissim", "S. Raskhodnikova", "A. Smith"], "venue": "In FOCS,", "citeRegEx": "Kasiviswanathan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kasiviswanathan et al\\.", "year": 2008}, {"title": "Private convex optimization for empirical risk minimization with applications to high-dimensional regression", "author": ["Kifer", "Daniel", "Smith", "Adam D", "Thakurta", "Abhradeep"], "venue": "In COLT,", "citeRegEx": "Kifer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kifer et al\\.", "year": 2012}, {"title": "Adaptive estimation of a quadratic functional by model selection", "author": ["B. Laurent", "P. Massart"], "venue": "The Annals of Statistics, 28(5),", "citeRegEx": "Laurent and Massart,? \\Q2000\\E", "shortCiteRegEx": "Laurent and Massart", "year": 2000}, {"title": "On lower bounds for the smallest eigenvalue of a hermitian positivedefinite matrix", "author": ["E.M. Ma", "Zarowski", "Christopher J"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Ma et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Ma et al\\.", "year": 1995}, {"title": "Linear Model Theory: Univariate, Multivariate, and Mixed Models", "author": ["Muller", "Keith E", "Stewart", "Paul W"], "venue": null, "citeRegEx": "Muller et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Muller et al\\.", "year": 2006}, {"title": "Randomized sketches of convex programs with sharp guarantees", "author": ["M. Pilanci", "M. Wainwright"], "venue": "In ISIT,", "citeRegEx": "Pilanci and Wainwright,? \\Q2014\\E", "shortCiteRegEx": "Pilanci and Wainwright", "year": 2014}, {"title": "Iterative hessian sketch: Fast and accurate solution approximation for constrained least-squares", "author": ["Pilanci", "Mert", "Wainwright", "Martin J"], "venue": "CoRR, abs/1411.0347,", "citeRegEx": "Pilanci et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pilanci et al\\.", "year": 2014}, {"title": "Linear statistical inference and its applications", "author": ["Rao", "C. Radhakrishna"], "venue": null, "citeRegEx": "Rao and Radhakrishna.,? \\Q1973\\E", "shortCiteRegEx": "Rao and Radhakrishna.", "year": 1973}, {"title": "Differentially private chi-squared hypothesis testing: Goodness of fit and independence testing", "author": ["Rogers", "Ryan M", "Vadhan", "Salil P", "Lim", "Hyun-Woo", "Gaboardi", "Marco"], "venue": "In ICML,", "citeRegEx": "Rogers et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rogers et al\\.", "year": 2016}, {"title": "Smallest singular value of a random rectangular matrix", "author": ["Rudelson", "Mark", "Vershynin", "Roman"], "venue": "Comm. Pure Appl. Math, pp", "citeRegEx": "Rudelson et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Rudelson et al\\.", "year": 2009}, {"title": "Improved approx. algs for large matrices via random projections", "author": ["T. Sarl\u00f3s"], "venue": "In FOCS,", "citeRegEx": "Sarl\u00f3s,? \\Q2006\\E", "shortCiteRegEx": "Sarl\u00f3s", "year": 2006}, {"title": "Private approximations of the 2nd-moment matrix using existing techniques in linear regression", "author": ["O. Sheffet"], "venue": "CoRR, abs/1507.00056,", "citeRegEx": "Sheffet,? \\Q2015\\E", "shortCiteRegEx": "Sheffet", "year": 2015}, {"title": "Privacy-preserving statistical estimation with optimal convergence rates", "author": ["Smith", "Adam D"], "venue": "In STOC, pp", "citeRegEx": "Smith and D.,? \\Q2011\\E", "shortCiteRegEx": "Smith and D.", "year": 2011}, {"title": "Impact of HbA1c measurement on hospital readmission rates: Analysis of 70,000 clinical database patient", "author": ["B. Strack", "J. DeShazo", "C. Gennings", "J. Olmo", "S. Ventura", "K. Cios", "J. Clore"], "venue": "records. BioMed Research International,", "citeRegEx": "Strack et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Strack et al\\.", "year": 2014}, {"title": "Topics in Random Matrix Theory", "author": ["T. Tao"], "venue": "American Mathematical Soc.,", "citeRegEx": "Tao,? \\Q2012\\E", "shortCiteRegEx": "Tao", "year": 2012}, {"title": "Differentially private feature selection via stability arguments, and the robustness of the lasso", "author": ["Thakurta", "Abhradeep", "Smith", "Adam"], "venue": "In COLT,", "citeRegEx": "Thakurta et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Thakurta et al\\.", "year": 2013}, {"title": "Solution of incorrectly formulated problems and the regularization method", "author": ["A.N. Tikhonov"], "venue": "Soviet Math. Dokl.,", "citeRegEx": "Tikhonov,? \\Q1963\\E", "shortCiteRegEx": "Tikhonov", "year": 1963}, {"title": "Privacy-preserving data sharing for genome-wide association studies", "author": ["Uhler", "Caroline", "Slavkovic", "Aleksandra B", "Fienberg", "Stephen E"], "venue": "Journal of Privacy and Confidentiality,", "citeRegEx": "Uhler et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Uhler et al\\.", "year": 2013}, {"title": "Private multiplicative weights beyond linear queries", "author": ["J. Ullman"], "venue": "In PODS,", "citeRegEx": "Ullman,? \\Q2015\\E", "shortCiteRegEx": "Ullman", "year": 2015}, {"title": "Differential privacy for clinical trial data: Preliminary evaluations", "author": ["D. Vu", "A. Slavkovic"], "venue": "In ICDM,", "citeRegEx": "Vu and Slavkovic,? \\Q2009\\E", "shortCiteRegEx": "Vu and Slavkovic", "year": 2009}, {"title": "Differentially private hypothesis testing, revisited", "author": ["Wang", "Yue", "Lee", "Jaewoo", "Kifer", "Daniel"], "venue": "CoRR, abs/1511.03376,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Mixture of gaussian models and bayes error under differential privacy", "author": ["B. Xi", "M. Kantarcioglu", "A. Inan"], "venue": "In CODASPY. ACM,", "citeRegEx": "Xi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Xi et al\\.", "year": 2011}, {"title": "Comparison with Existing Bounds. Sarlos\u2019 work (2006) utilizes the fact that when r, the numbers of rows in R, is large enough", "author": ["n\u2212p"], "venue": null, "citeRegEx": ".,? \\Q2006\\E", "shortCiteRegEx": ".", "year": 2006}], "referenceMentions": [{"referenceID": 9, "context": "First, we show that for wellspread data, the Gaussian Johnson-Lindenstrauss Transform (JLT) gives a very good approximation of t-values; secondly, when JLT approximates Ridge regression (linear regression with l2-regularization) we derive, under certain conditions, confidence intervals using the projected data; lastly, we derive, under different conditions, confidence intervals for the \u201cAnalyze Gauss\u201d algorithm (Dwork et al., 2014).", "startOffset": 415, "endOffset": 435}, {"referenceID": 2, "context": "We emphasize that the novelty of our work does not lie in the differentially-private algorithms, which are, as we discuss next, based on the Johnson-Lindenstrauss Transform (JLT) and on additive Gaussian noise and are already known to be differentially private (Blocki et al., 2012; Dwork et al., 2014).", "startOffset": 261, "endOffset": 302}, {"referenceID": 9, "context": "We emphasize that the novelty of our work does not lie in the differentially-private algorithms, which are, as we discuss next, based on the Johnson-Lindenstrauss Transform (JLT) and on additive Gaussian noise and are already known to be differentially private (Blocki et al., 2012; Dwork et al., 2014).", "startOffset": 261, "endOffset": 302}, {"referenceID": 4, "context": "Since, we do not deal with OLS based on the private single-regression ERM algorithms (Chaudhuri et al., 2011; Bassily et al., 2014) as such inference requires us to use the Fisher-information matrix of the loss function \u2014 but these algorithms do not minimize a private loss-function but rather prove that outputting the minimizer of the perturbed loss-function is private.", "startOffset": 85, "endOffset": 131}, {"referenceID": 1, "context": "Since, we do not deal with OLS based on the private single-regression ERM algorithms (Chaudhuri et al., 2011; Bassily et al., 2014) as such inference requires us to use the Fisher-information matrix of the loss function \u2014 but these algorithms do not minimize a private loss-function but rather prove that outputting the minimizer of the perturbed loss-function is private.", "startOffset": 85, "endOffset": 131}, {"referenceID": 10, "context": ") We leave this approach \u2014 as well as performing private hypothesis testing using a PTR-type algorithm (Dwork & Lei, 2009) (output merely reject / don\u2019t-reject decision without justification), or releasing only relevant tests judging by their p-values (Dwork et al., 2015) \u2014 for future work.", "startOffset": 252, "endOffset": 272}, {"referenceID": 28, "context": "In this case, solving the linear regression problem on the projected A\u2032 approximates the solution for Ridge Regression (Tikhonov, 1963; Hoerl & Kennard, 1970).", "startOffset": 119, "endOffset": 158}, {"referenceID": 9, "context": "In Section 5 we discuss the \u201cAnalyze Gauss\u201d algorithm (Dwork et al., 2014) that outputs a noisy version of a covariance of a given matrix using additive noise rather than multiplicative noise.", "startOffset": 54, "endOffset": 74}, {"referenceID": 33, "context": "Empirical work (Xi et al., 2011) shows that Analyze Gauss\u2019s output might be non-PSD if the input has small singular values, and this results in truly bad regressors.", "startOffset": 15, "endOffset": 32}, {"referenceID": 5, "context": "Some works have already looked at the intersection of differentially privacy and statistics (Dwork & Lei, 2009; Smith, 2011; Chaudhuri & Hsu, 2012; Duchi et al., 2013; Dwork et al., 2015) (especially focusing on robust statistics and rate of convergence).", "startOffset": 92, "endOffset": 187}, {"referenceID": 10, "context": "Some works have already looked at the intersection of differentially privacy and statistics (Dwork & Lei, 2009; Smith, 2011; Chaudhuri & Hsu, 2012; Duchi et al., 2013; Dwork et al., 2015) (especially focusing on robust statistics and rate of convergence).", "startOffset": 92, "endOffset": 187}, {"referenceID": 29, "context": "But only a handful of works studied the significance and power of hypotheses testing under differential privacy, without arguing that the noise introduced by differential privacy vanishes asymptotically (Vu & Slavkovic, 2009; Uhler et al., 2013; Wang et al., 2015; Rogers et al., 2016).", "startOffset": 203, "endOffset": 285}, {"referenceID": 32, "context": "But only a handful of works studied the significance and power of hypotheses testing under differential privacy, without arguing that the noise introduced by differential privacy vanishes asymptotically (Vu & Slavkovic, 2009; Uhler et al., 2013; Wang et al., 2015; Rogers et al., 2016).", "startOffset": 203, "endOffset": 285}, {"referenceID": 20, "context": "But only a handful of works studied the significance and power of hypotheses testing under differential privacy, without arguing that the noise introduced by differential privacy vanishes asymptotically (Vu & Slavkovic, 2009; Uhler et al., 2013; Wang et al., 2015; Rogers et al., 2016).", "startOffset": 203, "endOffset": 285}, {"referenceID": 1, "context": "Observe, overall this result is similar in nature to many other results in differentially private learning (Bassily et al., 2014) which are of the form \u201cwithout privacy, in order to achieve a total loss of \u2264 \u03b7 we have a sample complexity bound of some N\u03b7; and with differential privacy the sample complexity increases to N\u03b7 + \u03a9( \u221a N\u03b7/ ).", "startOffset": 107, "endOffset": 129}, {"referenceID": 28, "context": "Invented in the 60s (Tikhonov, 1963; Hoerl & Kennard, 1970), Ridge Regression is often motivated from the perspective of penalizing linear vectors whose coefficients are too large.", "startOffset": 20, "endOffset": 59}, {"referenceID": 25, "context": "We ran the two algorithms over diabetes dataset collected over ten years (1999-2008) taken from the UCI repository (Strack et al., 2014).", "startOffset": 115, "endOffset": 136}, {"referenceID": 30, "context": "Lastly the author thanks the anonymous referees for many helpful suggestions in general and for a reference to (Ullman, 2015) in particular.", "startOffset": 111, "endOffset": 125}, {"referenceID": 23, "context": "The proof of the theorem is based on the fact the Algorithm 1 is the result of composing the differentially private Propose-Test-Release algorithm of (Dwork & Lei, 2009) with the differentially private analysis of the Johnson-Lindenstrauss transform of (Sheffet, 2015).", "startOffset": 253, "endOffset": 268}, {"referenceID": 23, "context": "1 from (Sheffet, 2015) that states that given a matrix A whose all of its singular values at greater than T ( , \u03b4) where T ( , \u03b4) = 2B (\u221a 2r ln(4/\u03b4) + 2 ln(4/\u03b4) ) , publishing RA is ( , \u03b4)differentially private for a r-row matrix R whose entries sampled are i.", "startOffset": 7, "endOffset": 22}, {"referenceID": 7, "context": "It is known (Dwork et al., 2006b) that if ALG outputs a vector in R such that for any A and A\u2032 it holds that \u2016ALG(A) \u2212 ALG(A)\u20161 \u2264 B, then adding Laplace noise Lap(1/ ) to each coordinate of the output of ALG(A) satisfies -differential privacy. Similarly, (2006b) showed that if for any neighboring A and A\u2032 it holds that \u2016ALG(A)\u2212ALG(A)\u20162 \u2264 \u2206 then adding Gaussian noise N (0,\u2206 \u00b7 2 ln(2/\u03b4) 2 ) to each coordinate of the output of ALG(A) satisfies ( , \u03b4)-differential privacy.", "startOffset": 13, "endOffset": 263}, {"referenceID": 22, "context": "In this setting, Sarlos\u2019 work (Sarl\u00f3s, 2006) (Theorem 12(3)) guarantees that w.", "startOffset": 30, "endOffset": 44}, {"referenceID": 4, "context": ") As mentioned in the introduction, alternative techniques ((Chaudhuri et al., 2011; Bassily et al., 2014; Ullman, 2015)) for finding a DP estimator \u03b2 of the linear regression give a data-independent12 bound of \u2016\u03b2 \u2212 \u03b2\u0302\u2016 = \u00d5(p/ ).", "startOffset": 60, "endOffset": 120}, {"referenceID": 1, "context": ") As mentioned in the introduction, alternative techniques ((Chaudhuri et al., 2011; Bassily et al., 2014; Ullman, 2015)) for finding a DP estimator \u03b2 of the linear regression give a data-independent12 bound of \u2016\u03b2 \u2212 \u03b2\u0302\u2016 = \u00d5(p/ ).", "startOffset": 60, "endOffset": 120}, {"referenceID": 30, "context": ") As mentioned in the introduction, alternative techniques ((Chaudhuri et al., 2011; Bassily et al., 2014; Ullman, 2015)) for finding a DP estimator \u03b2 of the linear regression give a data-independent12 bound of \u2016\u03b2 \u2212 \u03b2\u0302\u2016 = \u00d5(p/ ).", "startOffset": 60, "endOffset": 120}, {"referenceID": 23, "context": "1 from (Sheffet, 2015) gives that w.", "startOffset": 7, "endOffset": 22}, {"referenceID": 28, "context": "Invented in the 60s (Tikhonov, 1963; Hoerl & Kennard, 1970), Ridge Regression is often motivated from the perspective of penalizing linear vectors whose coefficients are too large.", "startOffset": 20, "endOffset": 59}, {"referenceID": 9, "context": "To complete the picture, we now analyze the \u201cAnalyze Gauss\u201d algorithm of Dwork et al (Dwork et al., 2014).", "startOffset": 85, "endOffset": 105}, {"referenceID": 26, "context": "First, we apply to standard results about Gaussian matrices, such as (Tao, 2012) (used also by (Dwork et al.", "startOffset": 69, "endOffset": 80}, {"referenceID": 9, "context": "First, we apply to standard results about Gaussian matrices, such as (Tao, 2012) (used also by (Dwork et al., 2014) in their analysis), to see that w.", "startOffset": 95, "endOffset": 115}], "year": 2017, "abstractText": "Linear regression is one of the most prevalent techniques in machine learning; however, it is also common to use linear regression for its explanatory capabilities rather than label prediction. Ordinary Least Squares (OLS) is often used in statistics to establish a correlation between an attribute (e.g. gender) and a label (e.g. income) in the presence of other (potentially correlated) features. OLS assumes a particular model that randomly generates the data, and derives tvalues \u2014 representing the likelihood of each real value to be the true correlation. Using t-values, OLS can release a confidence interval, which is an interval on the reals that is likely to contain the true correlation; and when this interval does not intersect the origin, we can reject the null hypothesis as it is likely that the true correlation is non-zero. Our work aims at achieving similar guarantees on data under differentially private estimators. First, we show that for wellspread data, the Gaussian Johnson-Lindenstrauss Transform (JLT) gives a very good approximation of t-values; secondly, when JLT approximates Ridge regression (linear regression with l2-regularization) we derive, under certain conditions, confidence intervals using the projected data; lastly, we derive, under different conditions, confidence intervals for the \u201cAnalyze Gauss\u201d algorithm (Dwork et al., 2014).", "creator": "LaTeX with hyperref package"}}}