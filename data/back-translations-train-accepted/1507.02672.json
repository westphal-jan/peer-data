{"id": "1507.02672", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jul-2015", "title": "Semi-supervised Learning with Ladder Networks", "abstract": "We combine supervised learning with unsupervised learning in deep neural networks. The proposed model is trained to simultaneously minimize the sum of supervised and unsupervised cost functions by backpropagation, avoiding the need for layer-wise pretraining. Our work builds on top of the Ladder network proposed by Valpola (2015) which we extend by combining the model with supervision. We show that the resulting model reaches state-of-the-art performance in various tasks: MNIST and CIFAR-10 classification in a semi-supervised setting and permutation invariant MNIST in both semi-supervised and full-labels setting.", "histories": [["v1", "Thu, 9 Jul 2015 19:52:19 GMT  (817kb,D)", "http://arxiv.org/abs/1507.02672v1", null], ["v2", "Tue, 24 Nov 2015 09:22:23 GMT  (823kb,D)", "http://arxiv.org/abs/1507.02672v2", "Revised denoising function, updated results, fixed typos"]], "reviews": [], "SUBJECTS": "cs.NE cs.LG stat.ML", "authors": ["antti rasmus", "mathias berglund", "mikko honkala", "harri valpola", "tapani raiko"], "accepted": true, "id": "1507.02672"}, "pdf": {"name": "1507.02672.pdf", "metadata": {"source": "CRF", "title": "Semi-Supervised Learning with Ladder Network", "authors": ["Antti Rasmus", "Harri Valpola", "Mikko Honkala", "Tapani Raiko"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year, it is more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country."}, {"heading": "2 Derivation and justification", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "3 Implementation of the Model", "text": "The steps to implement the PCB network (Section 3.1) are typically as follows: 1) Take a feedback model for supervised learning, and add as an encoder (Section 3.2), 2) a decoder that can invert mappings at any level of the encoder and support unattended learning (Section 3.3), and 3) train the entire PCB network by minimizing the sum of all cost functions. In this section, we will go through these steps for a fully connected MLP network in detail and briefly outline the changes needed for the Convolutionary Networks, both of which are used in our experiments (Section 4)."}, {"heading": "3.1 General Steps for Implementing the Ladder Network", "text": "Consider the formation of a classification2, or a mapping of input x to output y with targets t, from a training set of pairs {x (n), t (n) | 1 \u2264 n \u2264 N}. Semi-Supervised Learning (Chapelle et al., 2006) examines how auxiliary data without labels {x (n) | N + 1 \u2264 n \u2264 M} can help in the formation of a classifier. It is often the case that labeled data is scarce, while unlabeled data is plentiful, i.e. N M. The PCB network can improve results even without auxiliary data, but the original motivation was to make it possible to take powerful forward classifiers and supplement them with an auxiliary decoder as follows: 1. Train any standard forward neural network. The network type is not limited to standard MLPs, but the approach can be applied to z, e.g. to revolutionary or recursive networks."}, {"heading": "3.2 Fully Connected MLP as Encoder", "text": "As a starting point, we use a fully interconnected MLP network with rectified linear units. This serves two purposes. First, it improves convergence due to reduced covariant shift, as originally proposed by Ioffe and Szegedy (2015). Second, as explained in Section 2, it requires some kind of normalization to prevent the cost of denoization from deviating from the trivial solution where the encoder outputs are just2Here we only consider the case where the output t (n) is a class label, but it is trivial to use the same approach to other regression tasks.constant values as these are the easiest to denoise. Batch normalization serves this purpose, too.Formally, batch normalization for layers l = 1."}, {"heading": "3.3 Decoder for Unsupervised Learning", "text": "This year it has come to the point where it is able to repent the aforementioned brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated"}, {"heading": "3.4 Variations", "text": "It is easy to extend the same approach to other encoders, for example, to Convolutionary Neural Networks (CNN). For the decoder of fully connected networks, we have used vertical mappings whose shape represents an encoder transposition; the same treatment works for convolution operations: in the networks we have tested in this paper, the decoder has convolutions whose parameterization mirrors the encoder and which effectively reverse the flow of information. Since the idea of convolution is to reduce the number of parameters by weight distribution, we have reduced this to the parameters of denoization function g, too.Many convolutional networks use pooling operations with font, that is, they have reduced the spatial characteristics of confrontation."}, {"heading": "4 Experiments", "text": "With the experiments with the MNIST and the CIFAR-10 dataset, we wanted to compare our method with other semi-monitored methods, but also show that we can connect the decoder to both a fully networked MLP network and a Convolutionary Neural Network, both of which are described in Section 3. We also wanted to compare the performance of the simpler BA model (Section 3.4) with the complete PCB network and experimented with only one cost function on the input layer. With CIFAR-10, we tested only the model. We also measured the performance of the monitored base models, which included only the encoder and the monitored cost function. In all cases where we compared these directly with conductor networks, we did our best to optimize the hyperparameters and the regulation of the base models."}, {"heading": "4.1 MNIST dataset", "text": "For the evaluation of semi-supervised learning, we used the standard 10,000 test samples as a pre-set test and randomly divided the standard 60,000 training samples into 10,000 sample validation sets, using M = 50,000 samples as a training set. From the training set, we randomly selected N = 100, 1000 or all labels for the monitored cost.5 All samples were used for the decoder that does not require labels.The validation set was used to evaluate the model structure and hyperparameters. We also balanced the classes to ensure that no particular class was overrepresented. We repeated each training 10 times and varied the random seed used for the splits.After optimizing the hypoparameters, we performed the final tests using all M = 60,000 training samples with 10 different random initializations of the weight matrices and data splits. We trained all 100 models for the 100 epochs followed by only 50 epochs for the final epoch size."}, {"heading": "4.1.1 Fully-connected MLP", "text": "A useful test for general learning algorithms is the permutation invariant MNIST classification task = 0.5 multiplies it with a larger function. Permutation invariance means that the results must be invariant in relation to the permutation of the elements of the input vector. In other words, it is not allowed to use prior information about the spatial arrangement of the input pixels. This excludes, among other things, revolutionary networks and geometric distortions of the input vector. We chose the layer sizes of the baseline model somewhat arbitrarily 784-1000-500-250-250-10. The network is deep enough to demonstrate the scalability of the method, but not yet an overkill for MNIST. The hyperparameters we have set for each model are the noise level that is added to the inputs and each layer, and the densification of cost multipliers (l). We also ran the reviewed baseline model with different levels."}, {"heading": "4.1.2 Convolutional networks", "text": "We tested two convolutional networks for the general MNIST classification task but sent data augmentation as geometric distortions. We focused on the 100-label case since more labels the results are already so good even in the more difficult permutation invariant task.The first network was a linear extension of the fully networked network that is invariant in permutation. We turned the first fully networked layer into a confrontation with 26-by-26 filters, resulting in a 3-by-3 spatial map of the 1000 features. Each of the 9 spatial locations was processed independently of one network with the same structure as the other layer."}, {"heading": "4.2 Convolutional networks on CIFAR-10", "text": "In fact, it is so that most of them are able to abide by the rules which they have imposed on themselves. (...) In fact, it is so that they are able to abide by the rules. (...) It is not so that they abide by the rules. (...) It is not so that they abide by the rules. (...) It is as if they abide by the rules. (...) It is as if they abide by the rules. (...) It is as if they abide by the rules. (...). (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (It is. \"(...).\" (It is. (...). \"(It is. (...).\" (It is. (...). \"(It is.\" (...). \"(It is. (...).\" (It is. (...). \"(It is.\" (it is.). \"(It is. (...).\" (it is. (it is.). (it is. (it.). (it is. (it is.). (it is. (it.). (it is.). (it is. (it.). (it is. (it.). (it is. (it.). (it is. (it.). (it is. (it.). (it is. (it.). (it. (it is.). (it is. (it. (it is.). (it is.). (it is. (it.). (it.). (it is. (it is. (it.). (it.). (it is. (it.). (it is.). (it is. (it is.). (it is. (it is.). (it.). (it is.). (it is. (it is. (it.). (it is.). (it is.). (it is. (it is.). (it is. (it is. (it.). (it is. (it.). (it.). (it. (it is."}, {"heading": "5 Related Work", "text": "This means that they have to deal not only with themselves, but also with themselves. (...) They have to deal with themselves. (...) They have to deal with themselves. (...) They have to deal with themselves. (...) They have to deal with themselves. (...) They have to deal with themselves. (...) They have to deal with themselves. (...) They have to deal with themselves. (...) They have to deal with themselves. (...) They have to deal with themselves. (...) They have to deal with themselves. (...) They have to deal with themselves. (...) They have to deal with themselves. (...) They have to deal with themselves. (...) They have to deal with themselves."}, {"heading": "6 Discussion", "text": "We have shown how a simultaneous unattended learning task achieves CNN and MLP networks that reach the status quo in various semi-supervised learning tasks. In particular, the performance achieved with very small labels is much better than previous results, which show that the unsupervised task does not depend on supervised learning processes. The proposed model is simple and easy to implement as the training is based on complete labels in the permutation. It is fast that the unsupervised task does not depend on supervised learning processes."}, {"heading": "Acknowledgements", "text": "We have received comments and help from a number of colleagues who all deserve to be mentioned, but we would particularly like to thank Yann LeCun, Diederik Kingma, Aaron Courville and Ian Goodfellow for their helpful comments and suggestions. Theano (Bastien et al., 2012; Bergstra et al., 2010) and Blocks (van Merrie \ufffd nboer et al., 2015) were the simulation software for this work."}, {"heading": "A Specification of the convolutional models", "text": "Here we describe two model structures, Conv-Small and Conv-Large, used for the MNIST and CIFAR-10 datasets, respectively, both inspired by ConvPool-CNN-C of Springenberg et al. (2014). Table 4 describes the model architectures and differences between the models in this work and ConvPool-CNN-C. It is noteworthy that this architecture does not use fully interconnected layers, but replaces them with a global middle pool layer just before the Softmax feature. The main differences between our models and ConvPool-CNN-C are the use of Gaussian noise instead of dropouts and the revolutionary normalization per channel according to Ioffe and Szegedy (2015). We also used 2x2 Stride 2 max-pooling instead of 3x3 Stride 2 max-pooling."}, {"heading": "B Formulation of the Denoising Function", "text": "The question of the meaning and purpose of the distribution mechanisms in the EU and in the EU has not yet been answered. (...) The question of the meaning and purpose of the distribution mechanisms in the EU has not yet been answered. (...) The question of the meaning and purpose of the distribution mechanisms has not been answered. (...) The question of the meaning and purpose of the distribution mechanisms has not yet been answered. (...) The question of the meaning and purpose of the distribution mechanisms of the distribution mechanisms has not been answered. (...) The question of the meaning and purpose of the distribution mechanisms of the distribution mechanisms of the distribution mechanisms of the distribution mechanisms of the distribution mechanisms of the distribution mechanisms of the distribution mechanisms of the distribution mechanisms of the distribution mechanisms of the distribution mechanisms of the distribution mechanisms of the distribution mechanisms of the distribution mechanisms of the distribution mechanisms has not yet been answered. (...) The question of the meaning and purpose of the distribution mechanisms of the distribution mechanisms of the distribution mechanisms of the distribution mechanisms of the distribution mechanisms"}], "references": [{"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop.", "citeRegEx": "Bastien et al\\.,? 2012", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "How auto-encoders could provide credit assignment in deep networks via target propagation", "author": ["Y. Bengio"], "venue": "arXiv:1407.7906.", "citeRegEx": "Bengio,? 2014", "shortCiteRegEx": "Bengio", "year": 2014}, {"title": "Generalized denoising auto-encoders as generative models", "author": ["Y. Bengio", "L. Yao", "G. Alain", "P. Vincent"], "venue": "C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 26 (NIPS 2013), pages 899\u2013907.", "citeRegEx": "Bengio et al\\.,? 2013", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. WardeFarley", "Y. Bengio"], "venue": "Proceedings of the Python for Scientific Computing Conference (SciPy 2010). Oral Presentation.", "citeRegEx": "Bergstra et al\\.,? 2010", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["A. Blum", "T. Mitchell"], "venue": "Proc. of the eleventh annual conference on Computational learning theory (COLT \u201998), pages 92\u2013100.", "citeRegEx": "Blum and Mitchell,? 1998", "shortCiteRegEx": "Blum and Mitchell", "year": 1998}, {"title": "Discriminative unsupervised feature learning with convolutional neural networks", "author": ["A. Dosovitskiy", "J.T. Springenberg", "M. Riedmiller", "T. Brox"], "venue": "Advances in Neural Information Processing Systems 27 (NIPS 2014), pages 766\u2013774.", "citeRegEx": "Dosovitskiy et al\\.,? 2014", "shortCiteRegEx": "Dosovitskiy et al\\.", "year": 2014}, {"title": "Large-scale feature learning with spikeand-slab sparse coding", "author": ["I. Goodfellow", "Y. Bengio", "A.C. Courville"], "venue": "Proc. of ICML 2012, pages 1439\u20131446.", "citeRegEx": "Goodfellow et al\\.,? 2012", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2012}, {"title": "Multi-prediction deep Boltzmann machines", "author": ["I. Goodfellow", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems 26 (NIPS 2013), pages 548\u2013 556.", "citeRegEx": "Goodfellow et al\\.,? 2013a", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Explaining and harnessing adversarial examples", "author": ["I. Goodfellow", "J. Shlens", "C. Szegedy"], "venue": "the International Conference on Learning Representations (ICLR 2015). arXiv:1412.6572.", "citeRegEx": "Goodfellow et al\\.,? 2015", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2015}, {"title": "Maxout networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "Proc. of ICML 2013.", "citeRegEx": "Goodfellow et al\\.,? 2013b", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Deep autoregressive networks", "author": ["K. Gregor", "I. Danihelka", "A. Mnih", "C. Blundell", "D. Wierstra"], "venue": "Proc. of ICML 2014, Beijing, China.", "citeRegEx": "Gregor et al\\.,? 2014", "shortCiteRegEx": "Gregor et al\\.", "year": 2014}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, 313(5786), 504\u2013507.", "citeRegEx": "Hinton and Salakhutdinov,? 2006", "shortCiteRegEx": "Hinton and Salakhutdinov", "year": 2006}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv:1502.03167.", "citeRegEx": "Ioffe and Szegedy,? 2015", "shortCiteRegEx": "Ioffe and Szegedy", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "the International Conference on Learning Representations (ICLR 2015), San Diego. arXiv:1412.6980.", "citeRegEx": "Kingma and Ba,? 2015", "shortCiteRegEx": "Kingma and Ba", "year": 2015}, {"title": "Semi-supervised learning with deep generative models", "author": ["D.P. Kingma", "S. Mohamed", "D.J. Rezende", "M. Welling"], "venue": "Advances in Neural Information Processing Systems 27 (NIPS 2014), pages 3581\u20133589.", "citeRegEx": "Kingma et al\\.,? 2014", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks", "author": ["Lee", "D.-H."], "venue": "Workshop on Challenges in Representation Learning, ICML 2013.", "citeRegEx": "Lee and D..H.,? 2013", "shortCiteRegEx": "Lee and D..H.", "year": 2013}, {"title": "Iterative reclassification procedure for constructing an asymptotically optimal rule of allocation in discriminant analysis", "author": ["G. McLachlan"], "venue": "J. American Statistical Association, 70, 365\u2013369.", "citeRegEx": "McLachlan,? 1975", "shortCiteRegEx": "McLachlan", "year": 1975}, {"title": "Distributional smoothing by virtual adversarial examples. arXiv:1507.00677", "author": ["T. Miyato", "S. ichi Maeda", "M. Koyama", "K. Nakae", "S. Ishii"], "venue": null, "citeRegEx": "Miyato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Miyato et al\\.", "year": 2015}, {"title": "Semi-supervised learning using an unsupervised atlas", "author": ["N. Pitelis", "C. Russell", "L. Agapito"], "venue": "Machine Learning and Knowledge Discovery in Databases (ECML PKDD 2014), pages 565\u2013580. Springer.", "citeRegEx": "Pitelis et al\\.,? 2014", "shortCiteRegEx": "Pitelis et al\\.", "year": 2014}, {"title": "Techniques for learning binary stochastic feedforward neural networks", "author": ["T. Raiko", "M. Berglund", "G. Alain", "L. Dinh"], "venue": "ICLR 2015, San Diego.", "citeRegEx": "Raiko et al\\.,? 2015", "shortCiteRegEx": "Raiko et al\\.", "year": 2015}, {"title": "Semi-supervised learning of compact document representations with deep networks", "author": ["M.A. Ranzato", "M. Szummer"], "venue": "Proc. of ICML 2008, pages 792\u2013799. ACM.", "citeRegEx": "Ranzato and Szummer,? 2008", "shortCiteRegEx": "Ranzato and Szummer", "year": 2008}, {"title": "Denoising autoencoder with modulated lateral connections learns invariant representations of natural images", "author": ["A. Rasmus", "T. Raiko", "H. Valpola"], "venue": "arXiv:1412.7210.", "citeRegEx": "Rasmus et al\\.,? 2015a", "shortCiteRegEx": "Rasmus et al\\.", "year": 2015}, {"title": "Lateral connections in denoising autoencoders support supervised learning", "author": ["A. Rasmus", "H. Valpola", "T. Raiko"], "venue": "arXiv:1504.08215.", "citeRegEx": "Rasmus et al\\.,? 2015b", "shortCiteRegEx": "Rasmus et al\\.", "year": 2015}, {"title": "Higher order contractive auto-encoder", "author": ["S. Rifai", "G. Mesnil", "P. Vincent", "X. Muller", "Y. Bengio", "Y. Dauphin", "X. Glorot"], "venue": "ECML PKDD 2011.", "citeRegEx": "Rifai et al\\.,? 2011a", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "The manifold tangent classifier", "author": ["S. Rifai", "Y.N. Dauphin", "P. Vincent", "Y. Bengio", "X. Muller"], "venue": "Advances in Neural Information Processing Systems 24 (NIPS 2011), pages 2294\u2013 2302.", "citeRegEx": "Rifai et al\\.,? 2011b", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Denoising source separation", "author": ["J. S\u00e4rel\u00e4", "H. Valpola"], "venue": "JMLR, 6, 233\u2013272.", "citeRegEx": "S\u00e4rel\u00e4 and Valpola,? 2005", "shortCiteRegEx": "S\u00e4rel\u00e4 and Valpola", "year": 2005}, {"title": "Creating artificial neural networks that generalize", "author": ["J. Sietsma", "R.J. Dow"], "venue": "Neural networks, 4(1), 67\u201379.", "citeRegEx": "Sietsma and Dow,? 1991", "shortCiteRegEx": "Sietsma and Dow", "year": 1991}, {"title": "Striving for simplicity: The all convolutional net", "author": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M.A. Riedmiller"], "venue": "arxiv:1412.6806.", "citeRegEx": "Springenberg et al\\.,? 2014", "shortCiteRegEx": "Springenberg et al\\.", "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "JMLR, 15(1), 1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Rule-injection hints as a means of improving network performance and learning time", "author": ["S.C. Suddarth", "Y. Kergosien"], "venue": "Proceedings of the EURASIP Workshop 1990 on Neural Networks, pages 120\u2013129. Springer.", "citeRegEx": "Suddarth and Kergosien,? 1990", "shortCiteRegEx": "Suddarth and Kergosien", "year": 1990}, {"title": "Partially labeled classification with Markov random walks", "author": ["M. Szummer", "T. Jaakkola"], "venue": "Advances in Neural Information Processing Systems 15 (NIPS 2002), 14, 945\u2013952.", "citeRegEx": "Szummer and Jaakkola,? 2003", "shortCiteRegEx": "Szummer and Jaakkola", "year": 2003}, {"title": "Statistical analysis of finite mixture distributions", "author": ["D. Titterington", "A. Smith", "U. Makov"], "venue": "Wiley Series in Probability and Mathematical Statistics. Wiley.", "citeRegEx": "Titterington et al\\.,? 1985", "shortCiteRegEx": "Titterington et al\\.", "year": 1985}, {"title": "From neural PCA to deep unsupervised learning", "author": ["H. Valpola"], "venue": "Adv. in Independent Component Analysis and Learning Machines, pages 143\u2013171. Elsevier. arXiv:1411.7783.", "citeRegEx": "Valpola,? 2015", "shortCiteRegEx": "Valpola", "year": 2015}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "Manzagol", "P.-A."], "venue": "JMLR, 11, 3371\u20133408.", "citeRegEx": "Vincent et al\\.,? 2010", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Deep learning via semi-supervised embedding", "author": ["J. Weston", "F. Ratle", "H. Mobahi", "R. Collobert"], "venue": "Neural Networks: Tricks of the Trade, pages 639\u2013655. Springer.", "citeRegEx": "Weston et al\\.,? 2012", "shortCiteRegEx": "Weston et al\\.", "year": 2012}, {"title": "Adaptive deconvolutional networks for mid and high level feature learning", "author": ["M.D. Zeiler", "G.W. Taylor", "R. Fergus"], "venue": "ICCV 2011, pages 2018\u20132025. IEEE.", "citeRegEx": "Zeiler et al\\.,? 2011", "shortCiteRegEx": "Zeiler et al\\.", "year": 2011}, {"title": "The value of unlabeled data for classification problems", "author": ["T. Zhang", "F. Oles"], "venue": "Proc. of ICML 2000, pages 1191\u20131198.", "citeRegEx": "Zhang and Oles,? 2000", "shortCiteRegEx": "Zhang and Oles", "year": 2000}, {"title": "Stacked what-where auto-encoders", "author": ["J. Zhao", "M. Mathieu", "R. Goroshin", "Y. Lecun"], "venue": "arXiv:1506.02351.", "citeRegEx": "Zhao et al\\.,? 2015", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}, {"title": "We utilized batch normalization in all layers, including pooling layers. Gaussian noise was also added to all layers, instead of applying dropout in only some of the layers as with ConvPool-CNN-C. B Formulation of the Denoising Function The denoising function g tries to map the clean z to the reconstructed \u1e91", "author": ["Springenberg"], "venue": null, "citeRegEx": "Springenberg,? \\Q2014\\E", "shortCiteRegEx": "Springenberg", "year": 2014}], "referenceMentions": [{"referenceID": 32, "context": "Our work builds on top of the Ladder network proposed by Valpola (2015) which we extend by combining the model with supervision.", "startOffset": 57, "endOffset": 72}, {"referenceID": 5, "context": ", Ranzato and Szummer, 2008) or classification of each input sample into its own class (Dosovitskiy et al., 2014).", "startOffset": 87, "endOffset": 113}, {"referenceID": 27, "context": "Combining an auxiliary task to help train a neural network was proposed by Suddarth and Kergosien (1990). By sharing the hidden representations among more than one task, the network generalizes better.", "startOffset": 75, "endOffset": 105}, {"referenceID": 20, "context": "Although some methods have been able to simultaneously apply both supervised and unsupervised learning (Ranzato and Szummer, 2008; Goodfellow et al., 2013a), often these unsupervised auxiliary tasks are only applied as pre-training, followed by normal supervised learning (e.", "startOffset": 103, "endOffset": 156}, {"referenceID": 7, "context": "Although some methods have been able to simultaneously apply both supervised and unsupervised learning (Ranzato and Szummer, 2008; Goodfellow et al., 2013a), often these unsupervised auxiliary tasks are only applied as pre-training, followed by normal supervised learning (e.", "startOffset": 103, "endOffset": 156}, {"referenceID": 32, "context": "Previously the Ladder network has only been demonstrated in unsupervised learning (Valpola, 2015; Rasmus et al., 2015a) but we now combine it with supervised learning.", "startOffset": 82, "endOffset": 119}, {"referenceID": 21, "context": "Previously the Ladder network has only been demonstrated in unsupervised learning (Valpola, 2015; Rasmus et al., 2015a) but we now combine it with supervised learning.", "startOffset": 82, "endOffset": 119}, {"referenceID": 30, "context": "Our approach follows Valpola (2015) who proposed a Ladder network where the auxiliary task is to denoise representations at every level of the model.", "startOffset": 21, "endOffset": 36}, {"referenceID": 6, "context": "This approach was taken, for instance, by Goodfellow et al. (2013a) with their multi-prediction deep Boltzmann machine.", "startOffset": 42, "endOffset": 68}, {"referenceID": 2, "context": "On the other hand, given a denoising function, one can draw samples from the corresponding distribution by creating a Markov chain that alternates between corruption and denoising (Bengio et al., 2013).", "startOffset": 180, "endOffset": 201}, {"referenceID": 22, "context": "Preliminary results on the full-labeled setting on permutation invariant MNIST task were reported in a short early version of this paper (Rasmus et al., 2015b).", "startOffset": 137, "endOffset": 159}, {"referenceID": 26, "context": "Valpola (2015) proposed the Ladder network where the inference process itself can be learned by using the principle of denoising which has been used in supervised learning (Sietsma and Dow, 1991), denoising autoencoders (dAE) (Vincent et al.", "startOffset": 172, "endOffset": 195}, {"referenceID": 33, "context": "Valpola (2015) proposed the Ladder network where the inference process itself can be learned by using the principle of denoising which has been used in supervised learning (Sietsma and Dow, 1991), denoising autoencoders (dAE) (Vincent et al., 2010) and denoising source separation (DSS) (S\u00e4rel\u00e4 and Valpola, 2005) for complementary tasks.", "startOffset": 226, "endOffset": 248}, {"referenceID": 25, "context": ", 2010) and denoising source separation (DSS) (S\u00e4rel\u00e4 and Valpola, 2005) for complementary tasks.", "startOffset": 46, "endOffset": 72}, {"referenceID": 21, "context": "Rasmus et al. (2015a) showed that such skip connections allow dAEs to focus on abstract invariant features on the higher levels, making the Ladder network a good fit with supervised learning that can select which information is relevant for the task at hand.", "startOffset": 0, "endOffset": 22}, {"referenceID": 12, "context": "We follow Ioffe and Szegedy (2015) to apply batch normalization to each preactivation including the topmost layer in the L-layer network.", "startOffset": 10, "endOffset": 35}, {"referenceID": 12, "context": "We follow Ioffe and Szegedy (2015) to apply batch normalization to each preactivation including the topmost layer in the L-layer network. This serves two purposes. First, it improves convergence due to reduced covariate shift as originally proposed by Ioffe and Szegedy (2015). Second, as explained in Section 2, DSS-type cost functions for all but the input layer require some type of normalization to prevent the denoising cost from encouraging the trivial solution where the encoder outputs just", "startOffset": 10, "endOffset": 277}, {"referenceID": 13, "context": "We used the Adam optimization algorithm (Kingma and Ba, 2015) for weight updates.", "startOffset": 40, "endOffset": 61}, {"referenceID": 34, "context": "Embedding (Weston et al., 2012) 16.", "startOffset": 10, "endOffset": 31}, {"referenceID": 24, "context": "40* MTC (Rifai et al., 2011b) 12.", "startOffset": 8, "endOffset": 29}, {"referenceID": 18, "context": "46 AtlasRBF (Pitelis et al., 2014) 8.", "startOffset": 12, "endOffset": 34}, {"referenceID": 14, "context": "31 DGN (Kingma et al., 2014) 3.", "startOffset": 7, "endOffset": 28}, {"referenceID": 28, "context": "96 DBM, Dropout (Srivastava et al., 2014) 0.", "startOffset": 16, "endOffset": 41}, {"referenceID": 8, "context": "79 Adversarial (Goodfellow et al., 2015) 0.", "startOffset": 15, "endOffset": 40}, {"referenceID": 17, "context": "78 Virtual Adversarial (Miyato et al., 2015) 2.", "startOffset": 23, "endOffset": 44}, {"referenceID": 34, "context": "Test error without data augmentation % with # of used labels 100 all EmbedCNN (Weston et al., 2012) 7.", "startOffset": 78, "endOffset": 99}, {"referenceID": 37, "context": "75 SWWAE (Zhao et al., 2015) 9.", "startOffset": 9, "endOffset": 28}, {"referenceID": 27, "context": "With the second network, which was inspired by ConvPool-CNN-C from Springenberg et al. (2014), we only tested the \u0393-model.", "startOffset": 67, "endOffset": 94}, {"referenceID": 27, "context": "We tested a few model architectures and selected ConvPool-CNN-C by Springenberg et al. (2014). We also evaluated the strided convolutional version by Springenberg et al.", "startOffset": 67, "endOffset": 94}, {"referenceID": 27, "context": "We tested a few model architectures and selected ConvPool-CNN-C by Springenberg et al. (2014). We also evaluated the strided convolutional version by Springenberg et al. (2014), and while it performed well with all labels, we found that the max-pooling version overfitted less with fewer labels, and thus used it.", "startOffset": 67, "endOffset": 177}, {"referenceID": 12, "context": "The main differences to ConvPool-CNN-C are the use of Gaussian noise instead of dropout and the convolutional per-channel batch normalization following Ioffe and Szegedy (2015). While dropout", "startOffset": 152, "endOffset": 177}, {"referenceID": 27, "context": "Test error % with # of used labels 4 000 All All-Convolutional ConvPool-CNN-C (Springenberg et al., 2014) 9.", "startOffset": 78, "endOffset": 105}, {"referenceID": 6, "context": "31 Spike-and-Slab Sparse Coding (Goodfellow et al., 2012) 31.", "startOffset": 32, "endOffset": 57}, {"referenceID": 6, "context": "We applied global contrast normalization and whitening following Goodfellow et al. (2013b), but no data augmentation was used.", "startOffset": 65, "endOffset": 91}, {"referenceID": 16, "context": "Early works in semi-supervised learning (McLachlan, 1975; Titterington et al., 1985) proposed an approach where inputs x are first assigned to clusters, and each cluster has its class label.", "startOffset": 40, "endOffset": 84}, {"referenceID": 31, "context": "Early works in semi-supervised learning (McLachlan, 1975; Titterington et al., 1985) proposed an approach where inputs x are first assigned to clusters, and each cluster has its class label.", "startOffset": 40, "endOffset": 84}, {"referenceID": 30, "context": "Label propagation methods (Szummer and Jaakkola, 2003) estimate P (y | x), but adjust probabilistic labels q(yt) based on the assumption that nearest neighbors are likely to have the same label.", "startOffset": 26, "endOffset": 54}, {"referenceID": 34, "context": "Weston et al. (2012) explored deep versions of label propagation.", "startOffset": 0, "endOffset": 21}, {"referenceID": 4, "context": "Co-training (Blum and Mitchell, 1998) assumes we have multiple views on x, say x = (x,x).", "startOffset": 12, "endOffset": 37}, {"referenceID": 23, "context": "There is an interesting connection between our \u0393-model and the contractive cost used by Rifai et al. (2011a): a linear denoising function \u1e91 i = aiz\u0303 (L) i + bi, where ai and bi are parameters, turns the denoising cost into a stochastic estimate of the contractive cost.", "startOffset": 88, "endOffset": 109}, {"referenceID": 17, "context": "Recently Miyato et al. (2015) achieved impressive results with a regularization method that is similar to the idea of contractive cost.", "startOffset": 9, "endOffset": 30}, {"referenceID": 7, "context": "The Multi-prediction deep Boltzmann machine (MP-DBM) (Goodfellow et al., 2013a) is a way to train a DBM with backpropagation through variational inference.", "startOffset": 53, "endOffset": 79}, {"referenceID": 10, "context": "The Deep AutoRegressive Network (Gregor et al., 2014) is an unsupervised method for learning representations that also uses lateral connections in the hidden representations.", "startOffset": 32, "endOffset": 53}, {"referenceID": 1, "context": "Recently Bengio (2014) proposed target propagation as an alternative to backpropagation.", "startOffset": 9, "endOffset": 23}, {"referenceID": 0, "context": "The software for the simulations for this paper was based on Theano (Bastien et al., 2012; Bergstra et al., 2010) and Blocks (van Merri\u00ebnboer et al.", "startOffset": 68, "endOffset": 113}, {"referenceID": 3, "context": "The software for the simulations for this paper was based on Theano (Bastien et al., 2012; Bergstra et al., 2010) and Blocks (van Merri\u00ebnboer et al.", "startOffset": 68, "endOffset": 113}], "year": 2015, "abstractText": "We combine supervised learning with unsupervised learning in deep neural networks. The proposed model is trained to simultaneously minimize the sum of supervised and unsupervised cost functions by backpropagation, avoiding the need for layer-wise pretraining. Our work builds on top of the Ladder network proposed by Valpola (2015) which we extend by combining the model with supervision. We show that the resulting model reaches state-of-the-art performance in various tasks: MNIST and CIFAR-10 classification in a semi-supervised setting and permutation invariant MNIST in both semi-supervised and full-labels setting.", "creator": "LaTeX with hyperref package"}}}