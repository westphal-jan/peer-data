{"id": "1507.03181", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jul-2015", "title": "A Probabilistic Approach to Knowledge Translation", "abstract": "In this paper, we focus on a novel knowledge reuse scenario where the knowledge in the source schema needs to be translated to a semantically heterogeneous target schema. We refer to this task as \"knowledge translation\" (KT). Unlike data translation and transfer learning, KT does not require any data from the source or target schema. We adopt a probabilistic approach to KT by representing the knowledge in the source schema, the mapping between the source and target schemas, and the resulting knowledge in the target schema all as probability distributions, specially using Markov random fields and Markov logic networks. Given the source knowledge and mappings, we use standard learning and inference algorithms for probabilistic graphical models to find an explicit probability distribution in the target schema that minimizes the Kullback-Leibler divergence from the implicit distribution. This gives us a compact probabilistic model that represents knowledge from the source schema as well as possible, respecting the uncertainty in both the source knowledge and the mapping. In experiments on both propositional and relational domains, we find that the knowledge obtained by KT is comparable to other approaches that require data, demonstrating that knowledge can be reused without data.", "histories": [["v1", "Sun, 12 Jul 2015 03:24:21 GMT  (36kb,D)", "http://arxiv.org/abs/1507.03181v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["shangpu jiang", "daniel lowd", "dejing dou"], "accepted": true, "id": "1507.03181"}, "pdf": {"name": "1507.03181.pdf", "metadata": {"source": "META", "title": "A Probabilistic Approach to Knowledge Translation", "authors": ["Shangpu Jiang", "Dejing Dou"], "emails": ["shangpu@cs.uoregon.edu", "lowd@cs.uoregon.edu", "dou@cs.uoregon.edu"], "sections": [{"heading": "Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "Related Work", "text": "This year it is more than ever before."}, {"heading": "Probabilistic Representations of Knowledge and Mappings", "text": "In order to translate knowledge from one scheme to another, we need a representation of knowledge and mappings between the two schemes. In many cases, knowledge and mappings are uncertain. For example, minedsource knowledge could be a probabilistic model, such as a Bayesian network. Mappings between two schemes can also be uncertain, either because there is no perfect alignment of concepts, or because there is uncertainty about which alignment is the best. Therefore, we propose a probabilistic approach to knowledge translation."}, {"heading": "Representation of Knowledge", "text": "Our approach to knowledge translation requires that the source and target knowledge are probability distributions that are presented as loglinear models. In some cases, the source knowledge derived from the data can already be presented as loglinear models, such as a Bayesian network used to diagnose errors or model Markov's logical network structures in a social network. In other cases, we must convert the knowledge into this representation. For degraded knowledge that is presented as rules, including association rules, rule sets, and decision trees (which can be considered a special case of rule sets), we can construct for each rule a trait whose weight corresponds to the confidence or probability of the rule. The rule weight has a closed solution based on the log probabilities that the rule is correct: wi = log p (fi) 1 \u2212 p (fi) 1 \u2212 u (fi) 1 \u2212 u (fi), where p (fi) is the probability or reliability of the ith rule or the probability of the formula fi (u)."}, {"heading": "Representation of Mappings", "text": "The relationships between heterogeneous schemas can be represented as mapping. We use probabilistic models to represent mappings. In line with the probabilistic representation of knowledge in a database schema, attributes are considered as random variables for non-relational domains, and attributes or relationships are considered as primary random variables for relational domains. Let's call the variables in the source as X = {X1,..., XN} and those in the target as X \u2032 1,..., X \u2032 M}. A mapping is the conditional distribution p (X \u2032 | X). In real cases, a mapping is often represented as a series of source-to-destination correspondences."}, {"heading": "Knowledge Translation", "text": "In this section we formalize the task of knowledge translation (KT) and propose a solution for this task (X). We have the source knowledge as a probabilistic model p (X) = p (X1,... Xn) and a probabilistic mapping P (X). However, our goal is to find a compact probability model in the target scheme (X) = x (X) p (X) p (X) p (X) p (X) p (X) p (X) p (1) p (X) p) p (X) p (X) p) p (X) p (X) p) p (X) p (p) p (X) p (X) p (p) p (X) p (X) p (X) p (p) p (X) p (X) p (X) p (X) p (1) p) p (p) p (1) p) p (p) p (1) p) p (p) p (X) p) p (X) p (X) p (X) p (X) p (X) p (X) p) p (p) p (1) p) p (1) p) p) p (X) p (X) p (X) p (X) p) p (X) p (X) p) p (X) p (X) p (X) p) p (X) p (X) p (X) p (X) p (1) p) p) p (X \"p (X) p) p (X\" p) p."}, {"heading": "Experiments", "text": "To evaluate our methods, we have created two knowledge translation tasks: one in a non-relational area (NBA) and one in a relational area (University). In each knowledge translation task, we have 2 different database schemes as source and target schemes and a data set for each scheme. Entering a knowledge translation system is the source knowledge and mapping between source and target schemes. We obtained the source knowledge (i.e. a probabilistic model in the source) by executing a common learning algorithm on the source data set and creating the probabilistic schema mapping manually. Outputting a knowledge translation system is the target knowledge (i.e. a probabilistic model with respect to the target scheme)."}, {"heading": "Methods and Baselines", "text": "We evaluate three different versions of our probabilistic knowledge transfer approach described in the previous section. All use source knowledge and probabilistic mapping to generate a sampled approximation of the distribution in the target scheme, and all use these samples to learn an explicit distribution in the target scheme. The difference between them is their approach to the knowledge structure. LS-KS (\"learned structure\") learns the structure directly from the samples, which is the most flexible approach. TS-KS (\"translated structure\") uses a heuristic translation of the structure from the source knowledge base. ES-KS (\"empty structure\") is a simple baseline in which the target knowledge base is limited to a marginal distribution. We also compare with several baselines that use additional data. If there is data DS in the source scheme, we can specify the data DS in the source scheme and specify the target data with target data."}, {"heading": "Non-Relational Domain (NBA)", "text": "We collected information about basketball players in the National Basketball Association (NBA) from two websites, the official website of the NBA nba (as a source scheme) and the Yahoo NBA site yahoo (as a target scheme). The schemes of these two records both have the name, size, weight, position and team of each player. In these schemes, the values of the position have a different granularity. Also in nba we discredit the size and weight in 5 equal width ranges. In yahoo, we discredit them into 5 equal frequency ranges (to make the mapping more difficult). The correspondences of these attributes are originally unit of measurement conversion formulas, e.g. h \u2032 = h \u00d7 3701After discrediting these attributes, we calculate the correspondence distribution of the ranges by making a simple assumption that each value range is uniformly distributed, e.g. p (h, 76.5]."}, {"heading": "Relational Domain (University)", "text": "We use the UW-CSE dataset2 and the UO-CIS dataset, which we collected from the Computer and Information Department of the University of Oregon. UW-CSE dataset was introduced by Richardson and Domingos and is widely used in statistical learning research. In this domain we have concepts such as persons, courses and publications; attributes such as PhD student and, of course, authors. The schemas of the two databases differ in their grandeur and attributes."}, {"heading": "Conclusion", "text": "Knowledge translation is an important task in terms of the reuse of knowledge, where the knowledge in the source scheme has to be translated into a semantically heterogeneous target scheme. Unlike data integration and transfer learning, knowledge translation focuses on the scenario that the data may not be available in both the source and the target. We propose a novel probabilistic approach to knowledge translation by combining probabilistic graphical models with schematic mapping. We have implemented an experimental knowledge translation system and evaluated it on two real data sets for different prediction tasks. Results and baseline comparisons show that our approach can achieve comparable accuracy without data. The proposed log-linear models, such as Markov Random Fields and Markov Logic Networks, already cover most common types of knowledge used in data mining."}], "references": [{"title": "Algorithms and software for collaborative discovery from autonomous, semantically heterogeneous, distributed information sources", "author": ["D. Caragea", "J. Zhang", "J. Bao", "J. Pathak", "V. Honavar"], "venue": "Proceedings of the 16th International Conference on Algorithmic Learning Theory,", "citeRegEx": "Caragea et al\\.,? 2005", "shortCiteRegEx": "Caragea et al\\.", "year": 2005}, {"title": "Deep transfer via secondorder Markov logic", "author": ["J. Davis", "P. Domingos"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, ICML \u201909, 217\u2013224.", "citeRegEx": "Davis and Domingos,? 2009", "shortCiteRegEx": "Davis and Domingos", "year": 2009}, {"title": "Semantic translation for rule-based knowledge in data mining", "author": ["D. Dou", "H. Qin", "H. Liu"], "venue": "Proceedings of the 22nd International Conference on Database and Expert Systems Applications, volume Part II of DEXA\u201911, 74\u201389.", "citeRegEx": "Dou et al\\.,? 2011", "shortCiteRegEx": "Dou et al\\.", "year": 2011}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["D. Koller", "N. Friedman"], "venue": "MIT Press.", "citeRegEx": "Koller and Friedman,? 2009", "shortCiteRegEx": "Koller and Friedman", "year": 2009}, {"title": "Data integration: A theoretical perspective", "author": ["M. Lenzerini"], "venue": "Proceedings of the Twenty-first ACM SIGMODSIGACT-SIGART Symposium on Principles of Database Systems, PODS \u201902, 233\u2013246.", "citeRegEx": "Lenzerini,? 2002", "shortCiteRegEx": "Lenzerini", "year": 2002}, {"title": "Improving markov network structure learning using decision trees", "author": ["D. Lowd", "J. Davis"], "venue": "Journal of Machine Learning Research 15(1):501\u2013532.", "citeRegEx": "Lowd and Davis,? 2014", "shortCiteRegEx": "Lowd and Davis", "year": 2014}, {"title": "Mapping and revising Markov logic networks for transfer learning", "author": ["L. Mihalkova", "T. Huynh", "R.J. Mooney"], "venue": "Proceedings of the 22nd National Conference on Artificial Intelligence, volume 1 of AAAI\u201907, 608\u2013614.", "citeRegEx": "Mihalkova et al\\.,? 2007", "shortCiteRegEx": "Mihalkova et al\\.", "year": 2007}, {"title": "Distributed data mining: Algorithms, systems, and applications", "author": ["Park", "B.-H.", "H. Kargupta"], "venue": "Ye, N., ed., The Handbook of Data Mining. Lawrence Erlbaum Associates. 341\u2013358.", "citeRegEx": "Park et al\\.,? 2002", "shortCiteRegEx": "Park et al\\.", "year": 2002}, {"title": "First-order probabilistic inference", "author": ["D. Poole"], "venue": "Proceedings of the 18th International Joint Conference on Artificial Intelligence, IJCAI\u201903, 985\u2013991. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.", "citeRegEx": "Poole,? 2003", "shortCiteRegEx": "Poole", "year": 2003}, {"title": "Sound and efficient inference with probabilistic and deterministic dependencies", "author": ["H. Poon", "P. Domingos"], "venue": "Proceedings of the 21st National Conference on Artificial Intelligence, volume 1 of AAAI\u201906, 458\u2013463.", "citeRegEx": "Poon and Domingos,? 2006", "shortCiteRegEx": "Poon and Domingos", "year": 2006}, {"title": "Markov logic networks", "author": ["M. Richardson", "P. Domingos"], "venue": "Machine Learning 62:107\u2013136.", "citeRegEx": "Richardson and Domingos,? 2006", "shortCiteRegEx": "Richardson and Domingos", "year": 2006}, {"title": "Artificial Intelligence: A Modern Approach", "author": ["S.J. Russell", "P. Norvig"], "venue": "Pearson Education, 2 edition.", "citeRegEx": "Russell and Norvig,? 2003", "shortCiteRegEx": "Russell and Norvig", "year": 2003}, {"title": "Heterogeneous transfer learning for image clustering via the social web", "author": ["Q. Yang", "Y. Chen", "G.-R. Xue", "W. Dai", "Y. Yu"], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the", "citeRegEx": "Yang et al\\.,? 2009", "shortCiteRegEx": "Yang et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 4, "context": ", (Lenzerini 2002)) are the mostly studied areas in semantic integration.", "startOffset": 2, "endOffset": 18}, {"referenceID": 0, "context": "Distributed Data Mining Efforts in distributed data mining (DDM) (see surveys in (Park and Kargupta 2002; Caragea et al. 2005)) have made considerable progress in mining distributed data resources without putting data in a centralized location.", "startOffset": 81, "endOffset": 126}, {"referenceID": 0, "context": "(Caragea et al. 2005) proposes a general DDM framework with two components: one sends statistical queries to local data sources, and the other uses the returned statistics to revise the current partial hypothesis and generate further queries.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Heterogeneous DDM (Caragea et al. 2005) also handles the semantic heterogeneity between the global and local schemas, in particular, those containing attributes with different granularities called Attribute Value Taxonomy (AVT).", "startOffset": 18, "endOffset": 39}, {"referenceID": 12, "context": "The main exceptions are heterogeneous transfer learning (Yang et al. 2009) and relational transfer learning (e.", "startOffset": 56, "endOffset": 74}, {"referenceID": 1, "context": ", TAMAR (Mihalkova, Huynh, and Mooney 2007), deep transfer (Davis and Domingos 2009)).", "startOffset": 59, "endOffset": 84}, {"referenceID": 3, "context": "This can be done by standard variable elimination (Koller and Friedman 2009; Poole 2003) without calculating parameters.", "startOffset": 50, "endOffset": 88}, {"referenceID": 8, "context": "This can be done by standard variable elimination (Koller and Friedman 2009; Poole 2003) without calculating parameters.", "startOffset": 50, "endOffset": 88}, {"referenceID": 11, "context": "For relational case, the merging involves a first-order unification operation (Russell and Norvig 2003; Poole 2003).", "startOffset": 78, "endOffset": 115}, {"referenceID": 8, "context": "For relational case, the merging involves a first-order unification operation (Russell and Norvig 2003; Poole 2003).", "startOffset": 78, "endOffset": 115}, {"referenceID": 5, "context": "For the remaining source dataset, we used the decision tree structure learning (DTSL) (Lowd and Davis 2014) to learn the source knowledge.", "startOffset": 86, "endOffset": 107}, {"referenceID": 10, "context": "The UW-CSE dataset was introduced by Richardson and Domingos (Richardson and Domingos 2006) and is widely used in statistical relational learning research.", "startOffset": 61, "endOffset": 91}, {"referenceID": 9, "context": "We use MC-SAT (Poon and Domingos 2006) as the sampling algorithm for these experiments.", "startOffset": 14, "endOffset": 38}], "year": 2015, "abstractText": "In this paper, we focus on a novel knowledge reuse scenario where the knowledge in the source schema needs to be translated to a semantically heterogeneous target schema. We refer to this task as \u201cknowledge translation\u201d (KT). Unlike data translation and transfer learning, KT does not require any data from the source or target schema. We adopt a probabilistic approach to KT by representing the knowledge in the source schema, the mapping between the source and target schemas, and the resulting knowledge in the target schema all as probability distributions, specially using Markov random fields and Markov logic networks. Given the source knowledge and mappings, we use standard learning and inference algorithms for probabilistic graphical models to find an explicit probability distribution in the target schema that minimizes the Kullback-Leibler divergence from the implicit distribution. This gives us a compact probabilistic model that represents knowledge from the source schema as well as possible, respecting the uncertainty in both the source knowledge and the mapping. In experiments on both propositional and relational domains, we find that the knowledge obtained by KT is comparable to other approaches that require data, demonstrating that knowledge can be reused without data.", "creator": "TeX"}}}