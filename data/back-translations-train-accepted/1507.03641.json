{"id": "1507.03641", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jul-2015", "title": "Neural CRF Parsing", "abstract": "This paper describes a parsing model that combines the exact dynamic programming of CRF parsing with the rich nonlinear featurization of neural net approaches. Our model is structurally a CRF that factors over anchored rule productions, but instead of linear potential functions based on sparse features, we use nonlinear potentials computed via a feedforward neural network. Because potentials are still local to anchored rules, structured inference (CKY) is unchanged from the sparse case. Computing gradients during learning involves backpropagating an error signal formed from standard CRF sufficient statistics (expected rule counts). Using only dense features, our neural CRF already exceeds a strong baseline CRF model (Hall et al., 2014). In combination with sparse features, our system achieves 91.1 F1 on section 23 of the Penn Treebank, and more generally outperforms the best prior single parser results on a range of languages.", "histories": [["v1", "Mon, 13 Jul 2015 22:23:51 GMT  (539kb,D)", "http://arxiv.org/abs/1507.03641v1", "Accepted for publication at ACL 2015"]], "COMMENTS": "Accepted for publication at ACL 2015", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["greg durrett", "dan klein"], "accepted": true, "id": "1507.03641"}, "pdf": {"name": "1507.03641.pdf", "metadata": {"source": "CRF", "title": "Neural CRF Parsing", "authors": ["Greg Durrett", "Dan Klein"], "emails": ["gdurrett@cs.berkeley.edu", "klein@cs.berkeley.edu"], "sections": [{"heading": "1 Introduction", "text": "A key strength of neural approaches is their ability to learn non-linear interactions between underlying characteristics. In the case of unstructured output spaces, this ability has, of course, led to gains in problems ranging from syntax (Chen and Manning, 2014; Belinkov et al., 2014) to lexical semantics (Kalchburner et al., 2014; Kim, 2014). Neural methods are also powerful tools in the case of strukturd1System available at http: / / nlp.cs.berkeley.eduoutput spaces. Here, the work of the past has often relied on recurrent architectures (Henderson, 2003; Socher et al., 2013; I-rsoy and Cardie, 2014) that disseminate information through the structure about real-rated hidden states, but as a result do not allow efficient dynamic programming."}, {"heading": "2 Model", "text": "Figure 1 shows our neural CRF model. The model decomposes via anchored rules and evaluates each of these rules with a potential function; in a standard CRF, these potentials are typically linear functions of sparse indicator characteristics, which are non-linear functions of word embedding. Section 2.1 describes our anchored rule notation, and Section 2.2 talks about how they are evaluated. We then discuss specific choices of our characteristics (Section 2.3) and the grammar used for structured inference (Section 2.4)."}, {"heading": "2.1 Anchored Rules", "text": "As shown in Figure 2, we define an anchored rule as tuples (r, s), where r is an indicator of the identity of the rule and s = (i, j, k) indicates the span (i, k) and slit point j of the rule. 3 A tree T is simply a collection of anchored rules that are bound to form a tree. All of our analysis models are CRFs that decompose via anchored rule productions and establish a probability distribution over trees conditioned to a sentence w as follows: P (T | w) is simply a collection of anchored rules that are bound to form a tree. 2In the course of this work, we will primarily consider two potential functions: linear functions of sparse indicators and nonlinear neural networks over dense, continuous properties. Although other modeling decisions are possible, these two points in the design space reflect common decisions in NP's past, and functions that we do not question linearly."}, {"heading": "2.2 Scoring Anchored Rules", "text": "In this case it is as if it is a pure vector expressing the properties of r (like the identity of the rule) and f (like those of the parents). (It is as if there is a vector of the surface features associated with the words in the sentence.) W is a vector expressing the properties of r (like the identity of the rule) and f (like the identity of the parents). (It is a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a, a vector, a, a vector, a vector, a, a vector, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a"}, {"heading": "2.3 Features", "text": "At the preterminal level, the model takes into account prefixes and suffixes up to the length of 5 of the current word and adjacent words, as well as the identities of the words. In non-terminal productions, we fire indicators at the word6 before and after the beginning, end, and column point of the anchored rule (as shown in Figure 2), as well as at two other span properties, span length, and span shape (an indicator of where capitalized words, numbers, and punctuation occur in the span). In our neural model, we take fw for all productions (preterminal and nonterminal) as the words surrounding the beginning and end of a span and span, as shown in Figure 2; in particular, we look at two words in both directions around each point of interest, meaning that the neural network needs 12 words as input."}, {"heading": "2.4 Grammar Refinements", "text": "A recurring problem in analyzing discriminatory constituencies is the granularity of annotations in basic grammar (Finkel et al., 2008; Petrov and Klein, 2008; Hall et al., 2014). The use of finer-grained symbols in our rules r gives the model greater capacity, but also introduces more parameters in W and 6The model actually uses the longest suffix of each word that occurs at least 100 times in the training, up to the whole word. Removing this abstraction of rare words harms performance. 7The sparse model has not benefited from using this larger neighborhood, so improvements in the neural network are not simply due to taking into account a more lexical context. Increases the ability to overlap. Following Hall et al. (2014) we use grammars with very few annotations: We do not use horizontal marcoding for any experiments, and all of our English experiments with neural programming we use the neural CRV (we do not use the graphical space for other)."}, {"heading": "3 Learning", "text": "To learn the weights for our neural model, we maximize the probability of our D training trees, although our goal is not differentiable everywhere. (H, W) The interaction of parameters and nonlinearity does not make the target convex either. (H, s; H, W) Nevertheless, we can still follow subgradients to optimize this goal, as is standard practice. (H, s; H) The hidden layer activations are the hidden layer activations. The gradient of W takes the standard form of log-linear models. (R, s) T, h (W, s; H) fo (r) fo (R, s) fo (H) fo (H, W) o (H, W) p (R, S) fo (R, S) fo (H)."}, {"heading": "4 Inference", "text": "We can use CKY in the standard method to calculate either the expected anchored rule numbers EP (T | w) [(r, s)] or the Viterbi tree arg maxT P (T | w). We speed up the conclusion by using a rough truncation pass. We follow Hall et al. (2014) and truncate according to an X-bar grammar with forward-facing binarization, excluding any component whose maximum limit probability is less than e \u2212 9. With this truncation, the number of slits and slit points to be considered is greatly reduced; however, we still need to calculate the neural network activations for each remaining slit and slit point, of which there may be thousands for a given set. 8 We can further improve efficiency by pointing out that the same word will appear in the same position in a span / slit combinations."}, {"heading": "5 System Ablations", "text": "The results of this study are presented in Table 4. We compare the different variants of our system along two axes: whether, however, they use standard linear features, nonlinear, dense features from the neural net, or both, and whether any word representations (vectors or clusters) are used. It is also not as if the individual word types (one, two, three, four, five, six, six, eight, eight, eight, eight, eight, eight, nine, eight, nine, eight, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine, nine,"}, {"heading": "6 Design Choices", "text": "The neural design space is large, so we want to analyze the respective design decisions we have made for this system by examining the performance of several variants of the neural network architecture used in our system. Table 2 shows the development results from potential alternative architectural choices that we are now looking at in more detail. Non-linearity g is often discussed in the neural network literature. Our choice g (x) is a traditional nonlinearity that is widely used throughout the history of neural networks (Bengio et al., 2003), a reflected linear unit that is increasingly popular in computer vision (Krizhevsky et al., 2012). g (x) = tanh is a traditional nonlinearity that is widely used in the history of neural networks (Bengio et al., 2003). We found that it is most successful by Chen and Manning (2014). Table 2 compares the performance of these three nonlinears."}, {"heading": "7 Test Results", "text": "We evaluate our system under two conditions: firstly, the English Penn Treebank and secondly, the nine languages used in the SPMRL in 2013 and 2014."}, {"heading": "7.1 Penn Treebank", "text": "Table 4 reports on the results of Section 23 of Penn Treebank (PTB). We focus our comparison on single-parser systems as opposed to anchors, ensembles or self-trained methods (although these are also mentioned in context). First, we compare four parsers trained only on the PTB without auxiliary data: the CRF parser by Hall et al. (2014), the Berkeley parser (Petrov and Klein, 2007), the discriminatory parser by Carreras et al. (2008) and the single TSG parser by Shindo et al. (2012). To our knowledge, the latter two systems are the most powerful in this single-parser data condition; we correspond to their performance with 91.1 F1, although we also use word vectors calculated from unlabelled data. Furthermore, we compare the Shiftreduction parsers by Zhu et al. (2013), which use unlabelled particles Brown as clusters. Our method achieves a performance similar to the larger parser."}, {"heading": "7.2 SPMRL", "text": "We also examine the performance of our parser in other languages, in particular the nine morphologically rich languages used in the SPMRL 2013 / 2014 Shared Task (Seddah et al., 2013; Seddah et al., 2014). We train word vectors using monolingual data distributed with the SPMRL 2014 Shared Task (typically 100M-200M tokens per language) using the Skip-gram approach of word2vec with a window size of 1 (Mikolov et al., 2013).10 Here we use V = 1 in the basic grammar, which has overall proven beneficial. Table 3 shows that our system improves the performance of the parser from Hall et al. (2014) and the top single parser from the Shared Task (Crabbe \u00b2 and Seddah, 2014), with significant improvements in all languages."}, {"heading": "8 Conclusion", "text": "In this paper, we presented a CRF parser that evaluates anchored rule productions with dense input functions calculated from a feedback-forward neural network. As the neural component is modularized, we can easily integrate it into an existing learning and follow-up framework based on dynamic programming of a separate parse diagram. Our combined neural and sparse model provides strong performance in both English and other languages. Our system is publicly available at http: / / nlp.cs.berkeley.edu."}, {"heading": "Acknowledgments", "text": "This work was partially supported by BBN under the DARPA contract HR0011-12-C-0014, by a Facebook grant for the first author and by a Google Faculty Research Award for the second author. Thanks to David Hall for supporting the Epic Parsing Framework and for a preliminary implementation of the neural architecture, to Kush Rastogi for training word vectors on the SPMRL data, to Dan Jurafsky for helpful discussions and to the anonymous reviewers for their insightful comments."}], "references": [{"title": "How much do word embeddings encode about syntax", "author": ["Jacob Andreas", "Dan Klein"], "venue": "In Proceedings of the Association for Computational Linguistics", "citeRegEx": "Andreas and Klein.,? \\Q2014\\E", "shortCiteRegEx": "Andreas and Klein.", "year": 2014}, {"title": "Tailoring Continuous Word Representations for Dependency Parsing", "author": ["Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "Proceedings of the Association for Computational Linguistics.", "citeRegEx": "Bansal et al\\.,? 2014", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "Exploring Compositional Architectures and Word Vector Representations for Prepositional Phrase Attachment", "author": ["Yonatan Belinkov", "Tao Lei", "Regina Barzilay", "Amir Globerson."], "venue": "Transactions of the Association for Computational Linguistics, 2:561\u2013572.", "citeRegEx": "Belinkov et al\\.,? 2014", "shortCiteRegEx": "Belinkov et al\\.", "year": 2014}, {"title": "A Neural Probabilistic Language Model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "Journal of Machine Learning Research, 3:1137\u20131155, March.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Re)ranking Meets Morphosyntax: State-of-the-art Results from the SPMRL 2013 Shared Task", "author": ["Anders Bj\u00f6rkelund", "Ozlem Cetinoglu", "Rich\u00e1rd Farkas", "Thomas Mueller", "Wolfgang Seeker."], "venue": "Proceedings of the Fourth Workshop on Statistical Pars-", "citeRegEx": "Bj\u00f6rkelund et al\\.,? 2013", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2013}, {"title": "Introducing the IMS-Wroc\u0142aw-Szeged-CIS entry at the SPMRL 2014 Shared Task: Reranking and Morpho-syntax", "author": ["Anders Bj\u00f6rkelund", "\u00d6zlem \u00c7etino\u011flu", "Agnieszka Fale\u0144ska", "Rich\u00e1rd Farkas", "Thomas Mueller", "Wolfgang Seeker", "Zsolt Sz\u00e1nt\u00f3"], "venue": null, "citeRegEx": "Bj\u00f6rkelund et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2014}, {"title": "TAG, Dynamic Programming, and the Perceptron for Efficient, Feature-rich Parsing", "author": ["Xavier Carreras", "Michael Collins", "Terry Koo."], "venue": "Proceedings of the Conference on Computational Natural Language Learning.", "citeRegEx": "Carreras et al\\.,? 2008", "shortCiteRegEx": "Carreras et al\\.", "year": 2008}, {"title": "Coarseto-Fine n-Best Parsing and MaxEnt Discriminative Reranking", "author": ["Eugene Charniak", "Mark Johnson."], "venue": "Proceedings of the Association for Computational Linguistics.", "citeRegEx": "Charniak and Johnson.,? 2005", "shortCiteRegEx": "Charniak and Johnson.", "year": 2005}, {"title": "A Fast and Accurate Dependency Parser using Neural Networks", "author": ["Danqi Chen", "Christopher D Manning."], "venue": "Proceedings of Empirical Methods in Natural Language Processing.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Feature Embedding for Dependency Parsing", "author": ["Wenliang Chen", "Yue Zhang", "Min Zhang."], "venue": "Proceedings of the International Conference on Computational Linguistics.", "citeRegEx": "Chen et al\\.,? 2014", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Natural Language Processing (Almost) from Scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research, 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Multilingual Discriminative Shift-Reduce Phrase Structure Parsing for the SPMRL 2014 Shared Task", "author": ["Benoit Crabb\u00e9", "Djam\u00e9 Seddah."], "venue": "Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntac-", "citeRegEx": "Crabb\u00e9 and Seddah.,? 2014", "shortCiteRegEx": "Crabb\u00e9 and Seddah.", "year": 2014}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "Journal of Machine Learning Research, 12:2121\u20132159, July.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Efficient, Feature-based, Conditional Random Field Parsing", "author": ["Jenny Rose Finkel", "Alex Kleeman", "Christopher D. Manning."], "venue": "Proceedings of the Association for Computational Linguistics.", "citeRegEx": "Finkel et al\\.,? 2008", "shortCiteRegEx": "Finkel et al\\.", "year": 2008}, {"title": "Less Grammar, More Features", "author": ["David Hall", "Greg Durrett", "Dan Klein."], "venue": "Proceedings of the Association for Computational Linguistics.", "citeRegEx": "Hall et al\\.,? 2014", "shortCiteRegEx": "Hall et al\\.", "year": 2014}, {"title": "Inducing History Representations for Broad Coverage Statistical Parsing", "author": ["James Henderson."], "venue": "Proceedings of the North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Henderson.,? 2003", "shortCiteRegEx": "Henderson.", "year": 2003}, {"title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "author": ["Sergey Ioffe", "Christian Szegedy."], "venue": "arXiv preprint, arXiv:1502.03167.", "citeRegEx": "Ioffe and Szegedy.,? 2015", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Opinion Mining with Deep Recurrent Neural Networks", "author": ["Ozan \u0130rsoy", "Claire Cardie."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "\u0130rsoy and Cardie.,? 2014", "shortCiteRegEx": "\u0130rsoy and Cardie.", "year": 2014}, {"title": "A Convolutional Neural Network for Modelling Sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "Proceedings of the Association for Computational Linguistics.", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional Neural Networks for Sentence Classification", "author": ["Yoon Kim."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Accurate Unlexicalized Parsing", "author": ["Dan Klein", "Christopher D. Manning."], "venue": "Proceedings of the Association for Computational Linguistics.", "citeRegEx": "Klein and Manning.,? 2003", "shortCiteRegEx": "Klein and Manning.", "year": 2003}, {"title": "Simple Semi-supervised Dependency Parsing", "author": ["Terry Koo", "Xavier Carreras", "Michael Collins."], "venue": "Proceedings of the Association for Computational Linguistics.", "citeRegEx": "Koo et al\\.,? 2008", "shortCiteRegEx": "Koo et al\\.", "year": 2008}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "The insideoutside recursive neural network model for dependency parsing", "author": ["Phong Le", "Willem Zuidema."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Le and Zuidema.,? 2014", "shortCiteRegEx": "Le and Zuidema.", "year": 2014}, {"title": "Low-Rank Tensors for Scoring Dependency Structures", "author": ["Tao Lei", "Yu Xin", "Yuan Zhang", "Regina Barzilay", "Tommi Jaakkola."], "venue": "Proceedings of the Association for Computational Linguistics.", "citeRegEx": "Lei et al\\.,? 2014", "shortCiteRegEx": "Lei et al\\.", "year": 2014}, {"title": "DependencyBased Word Embeddings", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Proceedings of the Association for Computational Linguistics.", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Building a Large Annotated Corpus of English: The Penn Treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz."], "venue": "Computational Linguistics, 19(2).", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Proceedings of the International Conference on Learning Representations.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Improved Inference for Unlexicalized Parsing", "author": ["Slav Petrov", "Dan Klein."], "venue": "Proceedings of the North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Petrov and Klein.,? 2007", "shortCiteRegEx": "Petrov and Klein.", "year": 2007}, {"title": "Sparse Multi-Scale Grammars for Discriminative Latent Variable Parsing", "author": ["Slav Petrov", "Dan Klein."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Petrov and Klein.,? 2008", "shortCiteRegEx": "Petrov and Klein.", "year": 2008}, {"title": "Products of Random Latent Variable Grammars", "author": ["Slav Petrov."], "venue": "Proceedings of the North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Petrov.,? 2010", "shortCiteRegEx": "Petrov.", "year": 2010}, {"title": "Overview of the SPMRL 2013 Shared Task: A Cross-Framework Evaluation of Parsing Morpho", "author": ["Nivre", "Adam Przepi\u00f3rkowski", "Ryan Roth", "Wolfgang Seeker", "Yannick Versley", "Veronika Vincze", "Marcin Woli\u0144ski", "Alina Wr\u00f3blewska"], "venue": null, "citeRegEx": "Nivre et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2013}, {"title": "Introducing the SPMRL 2014 Shared Task on Parsing Morphologically-rich Languages", "author": ["Djam\u00e9 Seddah", "Sandra K\u00fcbler", "Reut Tsarfaty."], "venue": "Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and", "citeRegEx": "Seddah et al\\.,? 2014", "shortCiteRegEx": "Seddah et al\\.", "year": 2014}, {"title": "Bayesian Symbol-refined Tree Substitution Grammars for Syntactic Parsing", "author": ["Hiroyuki Shindo", "Yusuke Miyao", "Akinori Fujino", "Masaaki Nagata."], "venue": "Proceedings of the Association for Computational Linguistics.", "citeRegEx": "Shindo et al\\.,? 2012", "shortCiteRegEx": "Shindo et al\\.", "year": 2012}, {"title": "Parsing With Compositional Vector Grammars", "author": ["Richard Socher", "John Bauer", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Proceedings of the Association for Computational Linguistics.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Learning Distributed Representations for Structured Output Prediction", "author": ["Vivek Srikumar", "Christopher D Manning."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Srikumar and Manning.,? 2014", "shortCiteRegEx": "Srikumar and Manning.", "year": 2014}, {"title": "Neural Networks Leverage Corpus-wide Information for Part-of-speech Tagging", "author": ["Yuta Tsuboi."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Tsuboi.,? 2014", "shortCiteRegEx": "Tsuboi.", "year": 2014}, {"title": "Word Representations: A Simple and General Method for Semi-supervised Learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."], "venue": "Proceedings of the Association for Computational Linguistics.", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Grammar as a Foreign Language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey E. Hinton."], "venue": "CoRR, abs/1412.7449.", "citeRegEx": "Vinyals et al\\.,? 2014", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Effect of Non-linear Deep Architecture in Sequence Labeling", "author": ["Mengqiu Wang", "Christopher D. Manning."], "venue": "Proceedings of the International Joint Conference on Natural Language Processing.", "citeRegEx": "Wang and Manning.,? 2013", "shortCiteRegEx": "Wang and Manning.", "year": 2013}, {"title": "ADADELTA: An Adaptive Learning Rate Method", "author": ["Matthew D. Zeiler."], "venue": "CoRR, abs/1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Fast and Accurate ShiftReduce Constituent Parsing", "author": ["Muhua Zhu", "Yue Zhang", "Wenliang Chen", "Min Zhang", "Jingbo Zhu."], "venue": "Proceedings of the Association for Computational Linguistics.", "citeRegEx": "Zhu et al\\.,? 2013", "shortCiteRegEx": "Zhu et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 14, "context": "Using only dense features, our neural CRF already exceeds a strong baseline CRF model (Hall et al., 2014).", "startOffset": 86, "endOffset": 105}, {"referenceID": 8, "context": "In the case of unstructured output spaces, this capability has led to gains in problems ranging from syntax (Chen and Manning, 2014; Belinkov et al., 2014) to lexical semantics (Kalchbrenner et al.", "startOffset": 108, "endOffset": 155}, {"referenceID": 2, "context": "In the case of unstructured output spaces, this capability has led to gains in problems ranging from syntax (Chen and Manning, 2014; Belinkov et al., 2014) to lexical semantics (Kalchbrenner et al.", "startOffset": 108, "endOffset": 155}, {"referenceID": 18, "context": ", 2014) to lexical semantics (Kalchbrenner et al., 2014; Kim, 2014).", "startOffset": 29, "endOffset": 67}, {"referenceID": 19, "context": ", 2014) to lexical semantics (Kalchbrenner et al., 2014; Kim, 2014).", "startOffset": 29, "endOffset": 67}, {"referenceID": 15, "context": "Here, past work has often relied on recurrent architectures (Henderson, 2003; Socher et al., 2013; \u0130rsoy and Cardie, 2014), which can propagate information through structure via realvalued hidden state, but as a result do not admit efficient dynamic programming (Socher et al.", "startOffset": 60, "endOffset": 122}, {"referenceID": 34, "context": "Here, past work has often relied on recurrent architectures (Henderson, 2003; Socher et al., 2013; \u0130rsoy and Cardie, 2014), which can propagate information through structure via realvalued hidden state, but as a result do not admit efficient dynamic programming (Socher et al.", "startOffset": 60, "endOffset": 122}, {"referenceID": 17, "context": "Here, past work has often relied on recurrent architectures (Henderson, 2003; Socher et al., 2013; \u0130rsoy and Cardie, 2014), which can propagate information through structure via realvalued hidden state, but as a result do not admit efficient dynamic programming (Socher et al.", "startOffset": 60, "endOffset": 122}, {"referenceID": 34, "context": ", 2013; \u0130rsoy and Cardie, 2014), which can propagate information through structure via realvalued hidden state, but as a result do not admit efficient dynamic programming (Socher et al., 2013; Le and Zuidema, 2014).", "startOffset": 171, "endOffset": 214}, {"referenceID": 23, "context": ", 2013; \u0130rsoy and Cardie, 2014), which can propagate information through structure via realvalued hidden state, but as a result do not admit efficient dynamic programming (Socher et al., 2013; Le and Zuidema, 2014).", "startOffset": 171, "endOffset": 214}, {"referenceID": 10, "context": "However, there is a natural marriage of nonlinear induced features and efficient structured inference, as explored by Collobert et al. (2011) for the case of sequence modeling: feedforward neural networks can be used to score local decisions which are then \u201creconciled\u201d in a discrete structured modeling framework, allowing inference via dynamic programming.", "startOffset": 118, "endOffset": 142}, {"referenceID": 15, "context": "Prior work on parsing using neural network models has often sidestepped the problem of structured inference by making sequential decisions (Henderson, 2003; Chen and Manning, 2014; Tsuboi, 2014) or by doing reranking (Socher et al.", "startOffset": 139, "endOffset": 194}, {"referenceID": 8, "context": "Prior work on parsing using neural network models has often sidestepped the problem of structured inference by making sequential decisions (Henderson, 2003; Chen and Manning, 2014; Tsuboi, 2014) or by doing reranking (Socher et al.", "startOffset": 139, "endOffset": 194}, {"referenceID": 36, "context": "Prior work on parsing using neural network models has often sidestepped the problem of structured inference by making sequential decisions (Henderson, 2003; Chen and Manning, 2014; Tsuboi, 2014) or by doing reranking (Socher et al.", "startOffset": 139, "endOffset": 194}, {"referenceID": 34, "context": "Prior work on parsing using neural network models has often sidestepped the problem of structured inference by making sequential decisions (Henderson, 2003; Chen and Manning, 2014; Tsuboi, 2014) or by doing reranking (Socher et al., 2013; Le and Zuidema, 2014); by contrast, our framework permits exact inference via CKY, since the model\u2019s structured interactions are purely discrete and do not involve continuous hidden state.", "startOffset": 217, "endOffset": 260}, {"referenceID": 23, "context": "Prior work on parsing using neural network models has often sidestepped the problem of structured inference by making sequential decisions (Henderson, 2003; Chen and Manning, 2014; Tsuboi, 2014) or by doing reranking (Socher et al., 2013; Le and Zuidema, 2014); by contrast, our framework permits exact inference via CKY, since the model\u2019s structured interactions are purely discrete and do not involve continuous hidden state.", "startOffset": 217, "endOffset": 260}, {"referenceID": 28, "context": "(2013) as well as the Berkeley Parser (Petrov and Klein, 2007) and matching the discriminative parser of Carreras et al.", "startOffset": 38, "endOffset": 62}, {"referenceID": 13, "context": "Using dense learned features alone, the neural CRF model obtains high performance, outperforming the CRF parser of Hall et al. (2014). When sparse indicators are used in addition, the resulting model gets 91.", "startOffset": 115, "endOffset": 134}, {"referenceID": 13, "context": "Using dense learned features alone, the neural CRF model obtains high performance, outperforming the CRF parser of Hall et al. (2014). When sparse indicators are used in addition, the resulting model gets 91.1 F1 on section 23 of the Penn Treebank, outperforming the parser of Socher et al. (2013) as well as the Berkeley Parser (Petrov and Klein, 2007) and matching the discriminative parser of Carreras et al.", "startOffset": 115, "endOffset": 298}, {"referenceID": 6, "context": "(2013) as well as the Berkeley Parser (Petrov and Klein, 2007) and matching the discriminative parser of Carreras et al. (2008). The model also obtains the best single parser results on nine other languages, again outperforming the system of Hall et al.", "startOffset": 105, "endOffset": 128}, {"referenceID": 6, "context": "(2013) as well as the Berkeley Parser (Petrov and Klein, 2007) and matching the discriminative parser of Carreras et al. (2008). The model also obtains the best single parser results on nine other languages, again outperforming the system of Hall et al. (2014).", "startOffset": 105, "endOffset": 261}, {"referenceID": 39, "context": "Although other modeling choices are possible, these two points in the design space reflect common choices in NLP, and past work has suggested that nonlinear functions of indicators or linear functions of dense features may perform less well (Wang and Manning, 2013).", "startOffset": 241, "endOffset": 265}, {"referenceID": 13, "context": "However, surface features can capture useful syntactic cues (Finkel et al., 2008; Hall et al., 2014).", "startOffset": 60, "endOffset": 100}, {"referenceID": 14, "context": "However, surface features can capture useful syntactic cues (Finkel et al., 2008; Hall et al., 2014).", "startOffset": 60, "endOffset": 100}, {"referenceID": 14, "context": "Following Hall et al. (2014), our baseline sparse scoring function takes the following bilinear form:", "startOffset": 10, "endOffset": 29}, {"referenceID": 9, "context": "However, embedding features rather than words has also been shown to be effective (Chen et al., 2014).", "startOffset": 82, "endOffset": 101}, {"referenceID": 14, "context": "We take fs to be the set of features described in Hall et al. (2014). At the preterminal layer, the model considers prefixes and suffixes up to length 5 of the current word and neighboring words, as well as the words\u2019 identities.", "startOffset": 50, "endOffset": 69}, {"referenceID": 1, "context": "7 For our word embeddings v, we use pre-trained word vectors from Bansal et al. (2014). We compare with other sources of word vectors in Section 5.", "startOffset": 66, "endOffset": 87}, {"referenceID": 13, "context": "A recurring issue in discriminative constituency parsing is the granularity of annotation in the base grammar (Finkel et al., 2008; Petrov and Klein, 2008; Hall et al., 2014).", "startOffset": 110, "endOffset": 174}, {"referenceID": 29, "context": "A recurring issue in discriminative constituency parsing is the granularity of annotation in the base grammar (Finkel et al., 2008; Petrov and Klein, 2008; Hall et al., 2014).", "startOffset": 110, "endOffset": 174}, {"referenceID": 14, "context": "A recurring issue in discriminative constituency parsing is the granularity of annotation in the base grammar (Finkel et al., 2008; Petrov and Klein, 2008; Hall et al., 2014).", "startOffset": 110, "endOffset": 174}, {"referenceID": 14, "context": "Following Hall et al. (2014), we use grammars with very little annotation: we use no horizontal Markovization for any of experiments, and all of our English experiments with the neural CRF use no vertical Markovization (V = 0).", "startOffset": 10, "endOffset": 29}, {"referenceID": 40, "context": "Learning uses Adadelta (Zeiler, 2012), which has been employed in past work (Kim, 2014).", "startOffset": 23, "endOffset": 37}, {"referenceID": 19, "context": "Learning uses Adadelta (Zeiler, 2012), which has been employed in past work (Kim, 2014).", "startOffset": 76, "endOffset": 87}, {"referenceID": 12, "context": "We found that Adagrad (Duchi et al., 2011) performed equally well with tuned regularization and step size parameters, but Adadelta worked better out of the box.", "startOffset": 22, "endOffset": 42}, {"referenceID": 12, "context": "We found that Adagrad (Duchi et al., 2011) performed equally well with tuned regularization and step size parameters, but Adadelta worked better out of the box. We set the momentum term \u03c1 = 0.95 (as suggested by Zeiler (2012)) and did not regularize the weights at all.", "startOffset": 23, "endOffset": 226}, {"referenceID": 8, "context": "8 We can improve efficiency further by noting that the same word will appear in the same position in a large number of span/split point combinations, and cache the contribution to the hidden layer caused by that word (Chen and Manning, 2014).", "startOffset": 217, "endOffset": 241}, {"referenceID": 13, "context": "We follow Hall et al. (2014) and prune according to an X-bar grammar with headoutward binarization, ruling out any constituent whose max marginal probability is less than e\u22129.", "startOffset": 10, "endOffset": 29}, {"referenceID": 26, "context": "Table 1 shows results on section 22 (the development set) of the English Penn Treebank (Marcus et al., 1993), computed using evalb.", "startOffset": 87, "endOffset": 108}, {"referenceID": 14, "context": "This is a surprising result: the features in the sparse CRF have been carefully engineered to capture a range of linguistic phenomena (Hall et al., 2014), and there is no guarantee that word vectors will capture the same.", "startOffset": 134, "endOffset": 153}, {"referenceID": 1, "context": "Unlabeled data Much attention has been paid to the choice of word vectors for various NLP tasks, notably whether they capture more syntactic or semantic phenomena (Bansal et al., 2014; Levy and Goldberg, 2014).", "startOffset": 163, "endOffset": 209}, {"referenceID": 25, "context": "Unlabeled data Much attention has been paid to the choice of word vectors for various NLP tasks, notably whether they capture more syntactic or semantic phenomena (Bansal et al., 2014; Levy and Goldberg, 2014).", "startOffset": 163, "endOffset": 209}, {"referenceID": 1, "context": "Unlabeled data Much attention has been paid to the choice of word vectors for various NLP tasks, notably whether they capture more syntactic or semantic phenomena (Bansal et al., 2014; Levy and Goldberg, 2014). We primarily use vectors from Bansal et al. (2014), who train the skipgram model of Mikolov et al.", "startOffset": 164, "endOffset": 262}, {"referenceID": 1, "context": "Unlabeled data Much attention has been paid to the choice of word vectors for various NLP tasks, notably whether they capture more syntactic or semantic phenomena (Bansal et al., 2014; Levy and Goldberg, 2014). We primarily use vectors from Bansal et al. (2014), who train the skipgram model of Mikolov et al. (2013) using contexts from dependency links; a similar approach was also suggested by Levy and Goldberg (2014).", "startOffset": 164, "endOffset": 317}, {"referenceID": 1, "context": "Unlabeled data Much attention has been paid to the choice of word vectors for various NLP tasks, notably whether they capture more syntactic or semantic phenomena (Bansal et al., 2014; Levy and Goldberg, 2014). We primarily use vectors from Bansal et al. (2014), who train the skipgram model of Mikolov et al. (2013) using contexts from dependency links; a similar approach was also suggested by Levy and Goldberg (2014).", "startOffset": 164, "endOffset": 421}, {"referenceID": 9, "context": "This is not the case: line (e) in Table 1 shows the performance of the neural CRF using the Wikipedia-trained word embeddings of Collobert et al. (2011), which do not perform better than the vectors of Bansal et al.", "startOffset": 129, "endOffset": 153}, {"referenceID": 1, "context": "(2011), which do not perform better than the vectors of Bansal et al. (2014).", "startOffset": 56, "endOffset": 77}, {"referenceID": 0, "context": "This result also reinforces the notion that the utility of word vectors does not come primarily from importing information about out-of-vocabulary words (Andreas and Klein, 2014).", "startOffset": 153, "endOffset": 178}, {"referenceID": 21, "context": "Brown clusters have been shown to be effective vehicles in the past (Koo et al., 2008; Turian et al., 2010; Bansal et al., 2014).", "startOffset": 68, "endOffset": 128}, {"referenceID": 37, "context": "Brown clusters have been shown to be effective vehicles in the past (Koo et al., 2008; Turian et al., 2010; Bansal et al., 2014).", "startOffset": 68, "endOffset": 128}, {"referenceID": 1, "context": "Brown clusters have been shown to be effective vehicles in the past (Koo et al., 2008; Turian et al., 2010; Bansal et al., 2014).", "startOffset": 68, "endOffset": 128}, {"referenceID": 1, "context": ", 2010; Bansal et al., 2014). We can incorporate Brown clusters into the baseline CRF model in an analogous way to how embedding features are used in the dense model: surface features are fired on Brown cluster identities (we use prefixes of length 4 and 10) of key words. We use the Brown clusters from Koo et al. (2008), which are trained on the same data as the vectors of Bansal et al.", "startOffset": 8, "endOffset": 322}, {"referenceID": 1, "context": ", 2010; Bansal et al., 2014). We can incorporate Brown clusters into the baseline CRF model in an analogous way to how embedding features are used in the dense model: surface features are fired on Brown cluster identities (we use prefixes of length 4 and 10) of key words. We use the Brown clusters from Koo et al. (2008), which are trained on the same data as the vectors of Bansal et al. (2014). However, Table 1 shows that these features provide no benefit to the baseline model, which suggests either that it is difficult to learn reliable weights for these as sparse features or that different regularities are being captured by the word embeddings.", "startOffset": 8, "endOffset": 397}, {"referenceID": 22, "context": "computer vision (Krizhevsky et al., 2012).", "startOffset": 16, "endOffset": 41}, {"referenceID": 3, "context": "g(x) = tanh(x) is a traditional nonlinearity widely used throughout the history of neural nets (Bengio et al., 2003).", "startOffset": 95, "endOffset": 116}, {"referenceID": 3, "context": "g(x) = tanh(x) is a traditional nonlinearity widely used throughout the history of neural nets (Bengio et al., 2003). g(x) = x3 (cube) was found to be most successful by Chen and Manning (2014).", "startOffset": 96, "endOffset": 194}, {"referenceID": 16, "context": "9 One drawback of tanh as an activation function is that it is easily \u201csaturated\u201d if the input to the unit is too far away from zero, causing the backpropagation of derivatives through that unit to essentially cease; this is known to cause problems for training, requiring special purpose machinery for use in deep networks (Ioffe and Szegedy, 2015).", "startOffset": 324, "endOffset": 349}, {"referenceID": 39, "context": "This agrees with the results of Wang and Manning (2013), who noted that dense features typically benefit from nonlinear modeling.", "startOffset": 32, "endOffset": 56}, {"referenceID": 35, "context": "We can apply the approach of Srikumar and Manning (2014) and multiply the sparse output vector by a dense matrix K, giving the following scoring function (shown in Figure 4b):", "startOffset": 29, "endOffset": 57}, {"referenceID": 24, "context": "work (Lei et al., 2014).", "startOffset": 5, "endOffset": 23}, {"referenceID": 11, "context": "89 Crabb\u00e9 and Seddah (2014) 77.", "startOffset": 3, "endOffset": 28}, {"referenceID": 11, "context": "89 Crabb\u00e9 and Seddah (2014) 77.66 85.35 79.68 77.15 86.19 87.51 79.35 91.60 82.72 83.02 Hall et al. (2014) 78.", "startOffset": 3, "endOffset": 107}, {"referenceID": 14, "context": "Our parser substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabb\u00e9 and Seddah, 2014).", "startOffset": 89, "endOffset": 133}, {"referenceID": 11, "context": "Our parser substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabb\u00e9 and Seddah, 2014).", "startOffset": 89, "endOffset": 133}, {"referenceID": 4, "context": "2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the best published numbers on this dataset (Bj\u00f6rkelund et al., 2013; Bj\u00f6rkelund et al., 2014).", "startOffset": 121, "endOffset": 171}, {"referenceID": 5, "context": "2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the best published numbers on this dataset (Bj\u00f6rkelund et al., 2013; Bj\u00f6rkelund et al., 2014).", "startOffset": 121, "endOffset": 171}, {"referenceID": 6, "context": "1 Carreras et al. (2008) 91.", "startOffset": 2, "endOffset": 25}, {"referenceID": 6, "context": "1 Carreras et al. (2008) 91.1 Shindo et al. (2012) single 91.", "startOffset": 2, "endOffset": 51}, {"referenceID": 28, "context": "We outperform strong baselines such as the Berkeley Parser (Petrov and Klein, 2007) and the CVG Stanford parser (Socher et al.", "startOffset": 59, "endOffset": 83}, {"referenceID": 34, "context": "We outperform strong baselines such as the Berkeley Parser (Petrov and Klein, 2007) and the CVG Stanford parser (Socher et al., 2013) and we match the performance of sophisticated generative (Shindo et al.", "startOffset": 112, "endOffset": 133}, {"referenceID": 33, "context": ", 2013) and we match the performance of sophisticated generative (Shindo et al., 2012) and discriminative (Carreras et al.", "startOffset": 65, "endOffset": 86}, {"referenceID": 6, "context": ", 2012) and discriminative (Carreras et al., 2008) parsers.", "startOffset": 27, "endOffset": 50}, {"referenceID": 28, "context": "(2014), the Berkeley parser (Petrov and Klein, 2007), the discriminative parser of Carreras et al.", "startOffset": 28, "endOffset": 52}, {"referenceID": 13, "context": "four parsers trained only on the PTB with no auxiliary data: the CRF parser of Hall et al. (2014), the Berkeley parser (Petrov and Klein, 2007), the discriminative parser of Carreras et al.", "startOffset": 79, "endOffset": 98}, {"referenceID": 6, "context": "(2014), the Berkeley parser (Petrov and Klein, 2007), the discriminative parser of Carreras et al. (2008), and the single TSG parser of Shindo et al.", "startOffset": 83, "endOffset": 106}, {"referenceID": 6, "context": "(2014), the Berkeley parser (Petrov and Klein, 2007), the discriminative parser of Carreras et al. (2008), and the single TSG parser of Shindo et al. (2012). To our knowledge, the latter two systems are the highest performing in this PTB-only, single parser data condition; we match their performance at 91.", "startOffset": 83, "endOffset": 157}, {"referenceID": 6, "context": "(2014), the Berkeley parser (Petrov and Klein, 2007), the discriminative parser of Carreras et al. (2008), and the single TSG parser of Shindo et al. (2012). To our knowledge, the latter two systems are the highest performing in this PTB-only, single parser data condition; we match their performance at 91.1 F1, though we also use word vectors computed from unlabeled data. We further compare to the shiftreduce parser of Zhu et al. (2013), which uses unlabeled data in the form of Brown clusters.", "startOffset": 83, "endOffset": 441}, {"referenceID": 34, "context": "We also compare to the compositional vector grammar (CVG) parser of Socher et al. (2013) as well as the LSTM-based parser of Vinyals et", "startOffset": 68, "endOffset": 89}, {"referenceID": 20, "context": "The conditions these parsers are operating under are slightly different: the former is a reranker on top of the Stanford Parser (Klein and Manning, 2003) and the latter trains on much larger amounts of data parsed by a product of Berkeley parsers (Petrov, 2010).", "startOffset": 128, "endOffset": 153}, {"referenceID": 30, "context": "The conditions these parsers are operating under are slightly different: the former is a reranker on top of the Stanford Parser (Klein and Manning, 2003) and the latter trains on much larger amounts of data parsed by a product of Berkeley parsers (Petrov, 2010).", "startOffset": 247, "endOffset": 261}, {"referenceID": 20, "context": "The conditions these parsers are operating under are slightly different: the former is a reranker on top of the Stanford Parser (Klein and Manning, 2003) and the latter trains on much larger amounts of data parsed by a product of Berkeley parsers (Petrov, 2010). Regardless, we outperform the CVG parser as well as the single parser results from Vinyals et al. (2014).", "startOffset": 129, "endOffset": 368}, {"referenceID": 32, "context": "We also examine the performance of our parser on other languages, specifically the nine morphologically-rich languages used in the SPMRL 2013/2014 shared tasks (Seddah et al., 2013; Seddah et al., 2014).", "startOffset": 160, "endOffset": 202}, {"referenceID": 27, "context": "(Mikolov et al., 2013).", "startOffset": 0, "endOffset": 22}, {"referenceID": 11, "context": "(2014) as well as the top single parser from the shared task (Crabb\u00e9 and Seddah, 2014), with robust improvements on all languages.", "startOffset": 61, "endOffset": 86}, {"referenceID": 13, "context": "Table 3 shows that our system improves upon the performance of the parser from Hall et al. (2014) as well as the top single parser from the shared task (Crabb\u00e9 and Seddah, 2014), with robust improvements on all languages.", "startOffset": 79, "endOffset": 98}], "year": 2015, "abstractText": "This paper describes a parsing model that combines the exact dynamic programming of CRF parsing with the rich nonlinear featurization of neural net approaches. Our model is structurally a CRF that factors over anchored rule productions, but instead of linear potential functions based on sparse features, we use nonlinear potentials computed via a feedforward neural network. Because potentials are still local to anchored rules, structured inference (CKY) is unchanged from the sparse case. Computing gradients during learning involves backpropagating an error signal formed from standard CRF sufficient statistics (expected rule counts). Using only dense features, our neural CRF already exceeds a strong baseline CRF model (Hall et al., 2014). In combination with sparse features, our system1 achieves 91.1 F1 on section 23 of the Penn Treebank, and more generally outperforms the best prior single parser results on a range of languages.", "creator": "TeX"}}}