{"id": "1507.03867", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jul-2015", "title": "Rich Component Analysis", "abstract": "In many settings, we have multiple data sets (also called views) that capture different and overlapping aspects of the same phenomenon. We are often interested in finding patterns that are unique to one or to a subset of the views. For example, we might have one set of molecular observations and one set of physiological observations on the same group of individuals, and we want to quantify molecular patterns that are uncorrelated with physiology. Despite being a common problem, this is highly challenging when the correlations come from complex distributions. In this paper, we develop the general framework of Rich Component Analysis (RCA) to model settings where the observations from different views are driven by different sets of latent components, and each component can be a complex, high-dimensional distribution. We introduce algorithms based on cumulant extraction that provably learn each of the components without having to model the other components. We show how to integrate RCA with stochastic gradient descent into a meta-algorithm for learning general models, and demonstrate substantial improvement in accuracy on several synthetic and real datasets in both supervised and unsupervised tasks. Our method makes it possible to learn latent variable models when we don't have samples from the true model but only samples after complex perturbations.", "histories": [["v1", "Tue, 14 Jul 2015 14:38:23 GMT  (82kb,D)", "http://arxiv.org/abs/1507.03867v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["rong ge 0001", "james zou"], "accepted": true, "id": "1507.03867"}, "pdf": {"name": "1507.03867.pdf", "metadata": {"source": "CRF", "title": "Rich Component Analysis", "authors": ["Rong Ge", "James Zou"], "emails": ["rongge@microsoft.com", "jazo@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, we are able to go in search of a solution that is capable of finding a solution that meets the needs of the people."}, {"heading": "2 Preliminaries", "text": "In this section, we present the basics of cumulants. \u2212 For more information, refer to Appendix A. Cumulants offer an alternative way to describe the correlations of random variables. \u2212 Unlike moments, cumulants have the nice property that the sum of independent random variables equals the sum of cumulants (e.g. covariants). \u2212 For n variables (X-R), the cumulant is defined as the coefficient of the cumulant generating function logE [etX]. \u2212 We can also define transverse cumulants that are cumulants for different variables (e.g. covariants). (For n variables X1,..., Xn, their transverse cumulants can be calculated using the following formula: X1,..., Xt) = transverse cumulants that are cumulants for different variables (e.g. covariants)."}, {"heading": "3 Rich Component Analysis", "text": "In this section, we show how to use cumulant to untangle complex latent components. RCA's key ideas and applications are captured in the contrasting learning environment when there are two views. Next, we present this model and then show how to extend it to general settings."}, {"heading": "3.1 RCA for contrastive learning", "text": "Let us recall the example in the introduction where we have two views of the data, formally, U = S1 + S2, V = AS2 + S3. (1) Here, S1, S2, S3, Rd are independent random variables that can have complicated distributions; however, A = s1 + s2 is an unknown linear transformation (1). The observations consist of sample pairs (u, v). Each pair is generated by drawing independent samples si \u00b2 Si, i = 1, 2, 3 and adding these samples to obtain u = s1 + s2 and v = As2 + s3. Note that the same s2 show up both in and in the introduction of the correlation between the two views. We are interested in learning characteristics about Si, for example, its maximum probability to learn (MLE) parameters. For concreteness, we focus our discussion on learning S1, although our techniques are also applicable to S2 and S3. We do not have samples from S1."}, {"heading": "3.2 General model of Rich Component Analysis", "text": "The ideas are very similar, but the algorithm is more technical to track all the components. We present the intuition and the main results here and move the details to Appendix B.2. Consider a series of observations U1, U2,.., Uk, Uk, Rd, each is based on a subset of variables S1, S2,., Sp, Rd, the variable Sj appears in a subset of observations. That's as if they were all based on a subset of variables S1, S2, S2,., Sp, Rd, the variable Sj appears in a subset of Qj."}, {"heading": "3.3 Related models", "text": "In ICA, let s = [s1,..., sn] be a vector of latent sources, where si's are one dimensionally independent, non-Gauss's random variables. There is an unknown mixture matrix A and the observations are x = As. In view of many samples x (t), the goal is to uncouple and restore each sample si. In our environment, each si can be a high-dimensional vector with complex correlations. It is not possible in information theory to uncouple and restore the individual samples si. Instead, we aim to learn the distribution si without having explicit samples therefrom. Another related model is the canonical correlation analysis (CCA) [6]. The generative model interpretation of CCA is: There is a common signal z \u00b2 N (0, I), and visual signals z (m)."}, {"heading": "4 Using Cumulants in learning applications", "text": "The cumulative extraction techniques of Section 3 construct unbiased estimates for the cumulants of Si. In this section we show how to use the estimated cumulants / moments to achieve the maximum probability of learning Si. Specifically, we include the discussion of the contrastive learning environment in which we want to learn S1. In general RCA, the method works if L (see definition 3.1) is small or the distributions have a specific relationship between lower and higher cumulants."}, {"heading": "4.1 Method-of-Moments", "text": "The simplest (and most commonly used) example is probably the main component analysis, where we want to find the maximum direction of variance in S1. This is only related to the covariance matrix E [S1S > 1]. RCA removes the covariance based on S2 and constructs an unbiased estimator of E [S1S > 1] from which we can extract the uppermost own space. The next simplest model is the least square regression (LSR). Suppose that the distribution S1 contains samples and labels of E [S1S > 1], and only the samples are corrupted by interference."}, {"heading": "4.2 Approximating Gradients", "text": "There are many machine learning models where it is not clear how to apply the method of moments. Gradient lineage (GD) and stochastic gradient lineage (SGD) are general techniques for parameter estimation in many models. Here, we show how to combine RCA with gradient lineage. The key idea is that the extracted cumulants / moments of S1 form a general gradient estimation. If the gradient of log estimation of log estimation in S1, then we can use the extracted cumulants from RCA to approximate these gradients / moments of S1. Let's consider the general setting in which we have a model D with parameter estimation, and for each probability is L (s1). The maximum probability is that we try to find the parameter that maximizes the probability of the samples observed."}, {"heading": "5 Experiments", "text": "In the experiments, we focus on the contrastive learning setting, in which we obtain observations of U \u03b2 S2 and V = AS2 + S3 and the goal is to estimate the parameters for the S1 distribution. Our approach can also learn the split component S2 as well as the Ising model. The first three settings illustrate the combination of RCA with methodological moments and the last two settings that require RCA with polynomial approximation to stochastic descent. In each setting, we compared the following four algorithms: 1. The standard learning algorithms use the actual samples s1."}, {"heading": "A More Tensor and Cumulant Notations", "text": "In this section, we present the notations and basics for the Tensors and Cumulants. B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7"}, {"heading": "B Details for Section 3", "text": "In this section we prove the equations and algorithms in section 3 actually calculate the desirable quantity (> quantity, and then we give sample complexity limits (B.1 contrastive learning We first prove equation (2) by calculating the correct linear transformation. Lemma B.1 (Lemma 3.1 recalculated). Suppose the unfolding of the cumulative quantity (Quantity of the 4th order) unfolds (Quantity of the 4th order) (Quantity of the 4th order). Equation (2) finds the correct linear transformation in time O (d5). Since U = S1 + S2 + S2 + S3 + S2 + S3, we know that we know the quantity (V, U, U, U, U, U, U, U)."}, {"heading": "C Details for Section 4", "text": "In this section, we will demonstrate the evidence for a strongly convexed function that is responsible for a strongly convexed function. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <"}, {"heading": "D Details for Section 5", "text": "The course of the composite log liquidity in relation to a given Jij is5Jij lcl = ES (2S) S (j) S (j) S (j) S (j) S (j) S (j) S (j) S (j) S (j) S (i) K (i) K (i) K (i) K (i) S (i) S (k) S (i) S (i) S (i) S (i) S (i) S (i) S (i) S (i) S (i) S (i) S) S)."}], "references": [{"title": "Tensor decompositions for learning latent variable models", "author": ["Animashree Anandkumar", "Rong Ge", "Daniel Hsu", "Sham M. Kakade", "Matus Telgarsky"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Simple, efficient, neural algorithms for dictionary learning", "author": ["Sanjeev Arora", "Rong Ge", "Tengyu Ma", "Ankur Moitra"], "venue": "In Proceedings of The 28th Conference on Learning Theory, COLT,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Bounds for mixed cumulants and higher-order covariances of bounded random variables", "author": ["AV Bulinskii"], "venue": "Theory of Probability & Its Applications,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1975}, {"title": "Handbook of Blind Source Separation: Independent component analysis and applications", "author": ["Pierre Comon", "Christian Jutten"], "venue": "Academic press,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Modern factor analysis", "author": ["Harry H Harman"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1976}, {"title": "Relations between two sets of variates", "author": ["Harold Hotelling"], "venue": "Biometrika, pages 321\u2013377,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1936}, {"title": "Learning mixtures of spherical gaussians: moment methods and spectral decompositions", "author": ["Daniel Hsu", "Sham M Kakade"], "venue": "In Proceedings of the 4th conference on Innovations in Theoretical Computer Science,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Tensor decompositions and applications", "author": ["Tamara G Kolda", "Brett W Bader"], "venue": "SIAM review,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Approximation theory and methods", "author": ["Michael James David Powell"], "venue": "Cambridge university press,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1981}, {"title": "Mathematical statistics with Mathematica, volume 1", "author": ["Colin Rose", "Murray D Smith"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Stochastic Convex Optimization", "author": ["Shai Shalev-Shwartz", "Ohad Shamir", "Nathan Srebro", "Karthik Sridharan"], "venue": "In Proceedings of the Conference on Learning Theory (COLT),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "A overview of composite likelihood methods", "author": ["Cristiano Varin", "Nancy Reid", "David Firth"], "venue": "Statistica Sinica,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Contrastive Learning Using Spectral Methods", "author": ["James Zou", "Daniel Hsu", "David Parkes", "Ryan Adam"], "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}], "referenceMentions": [{"referenceID": 3, "context": "3 Related models Independent component analysis (ICA)[4] may appear similar to our model, but it is actually quite different.", "startOffset": 53, "endOffset": 56}, {"referenceID": 5, "context": "Another related model is canonical correlation analysis (CCA)[6].", "startOffset": 61, "endOffset": 64}, {"referenceID": 4, "context": "Factor analysis (FA)[5] also corresponds to a multivariate Gaussian model, and hence does not address the general problem that we solve.", "startOffset": 20, "endOffset": 23}, {"referenceID": 12, "context": "A different notion of contrastive learning was introduced in [14].", "startOffset": 61, "endOffset": 65}, {"referenceID": 0, "context": "Latent Dirichlet Allocation and many others (see [1]).", "startOffset": 49, "endOffset": 52}, {"referenceID": 10, "context": "For convex functions this is known to converge to the optimal solution [12].", "startOffset": 71, "endOffset": 75}, {"referenceID": 8, "context": "Chebyshev polynomials, see more in [10]).", "startOffset": 35, "endOffset": 39}, {"referenceID": 6, "context": "We then use the estimator in [7] to get a low rank tensor whose components correspond to center vectors, and apply alternating minimization (see [9]) to infer \u03bc\u0302 k .", "startOffset": 29, "endOffset": 32}, {"referenceID": 7, "context": "We then use the estimator in [7] to get a low rank tensor whose components correspond to center vectors, and apply alternating minimization (see [9]) to infer \u03bc\u0302 k .", "startOffset": 145, "endOffset": 148}, {"referenceID": 11, "context": "We use composite likelihood to estimate the couplings Jij of S1, which is asymptotically consistent with MLE of the true likelihood [13].", "startOffset": 132, "endOffset": 136}], "year": 2015, "abstractText": "In many settings, we have multiple data sets (also called views) that capture different and overlapping aspects of the same phenomenon. We are often interested in finding patterns that are unique to one or to a subset of the views. For example, we might have one set of molecular observations and one set of physiological observations on the same group of individuals, and we want to quantify molecular patterns that are uncorrelated with physiology. Despite being a common problem, this is highly challenging when the correlations come from complex distributions. In this paper, we develop the general framework of Rich Component Analysis (RCA) to model settings where the observations from different views are driven by different sets of latent components, and each component can be a complex, highdimensional distribution. We introduce algorithms based on cumulant extraction that provably learn each of the components without having to model the other components. We show how to integrate RCA with stochastic gradient descent into a meta-algorithm for learning general models, and demonstrate substantial improvement in accuracy on several synthetic and real datasets in both supervised and unsupervised tasks. Our method makes it possible to learn latent variable models when we don\u2019t have samples from the true model but only samples after complex perturbations.", "creator": "LaTeX with hyperref package"}}}