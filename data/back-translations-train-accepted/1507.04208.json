{"id": "1507.04208", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jul-2015", "title": "Combinatorial Cascading Bandits", "abstract": "We consider learning to maximize reward in combinatorial cascading bandits, a new learning setting that unifies cascading and combinatorial bandits. The unification of these frameworks presents unique challenges in the analysis but allows for modeling a rich set of partial monitoring problems, such as learning to route in a communication network to minimize the probability of losing routed packets and recommending diverse items. We propose CombCascade, a computationally-efficient UCB-like algorithm for solving our problem; and derive gap-dependent and gap-free upper bounds on its regret. Our analysis builds on recent results in stochastic combinatorial semi-bandits but also addresses two novel challenges of our learning setting, a non-linear objective and partial observability. We evaluate CombCascade on two real-world problems and demonstrate that it performs well even when our modeling assumptions are violated. We also demonstrate that our setting requires new learning algorithms.", "histories": [["v1", "Wed, 15 Jul 2015 13:30:46 GMT  (90kb,D)", "https://arxiv.org/abs/1507.04208v1", null], ["v2", "Mon, 2 Nov 2015 19:34:21 GMT  (87kb,D)", "http://arxiv.org/abs/1507.04208v2", "Advances in Neural Information Processing Systems 28"], ["v3", "Tue, 17 Nov 2015 20:27:44 GMT  (87kb,D)", "http://arxiv.org/abs/1507.04208v3", "Advances in Neural Information Processing Systems 28"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["branislav kveton", "zheng wen", "azin ashkan", "csaba szepesv\u00e1ri"], "accepted": true, "id": "1507.04208"}, "pdf": {"name": "1507.04208.pdf", "metadata": {"source": "CRF", "title": "Combinatorial Cascading Bandits", "authors": ["Branislav Kveton"], "emails": ["kveton@adobe.com", "zhengwen@yahoo-inc.com", "azin.ashkan@technicolor.com", "szepesva@cs.ualberta.ca"], "sections": [{"heading": null, "text": "We propose combinatorial cascading bandits, a class of partial monitoring problems in which a learning agent selects at each step a tuple of basic elements that are subject to limitations, and receives a reward if and only if the weights of all selected elements are one; the weights of the elements are binary, stochastical and independently drawn; the agent observes the index of the first selected element whose weight is zero; this observation model arises, for example, in network routing, where the learning agent is only allowed to observe the first link in the routing path that leads down, and blocks the path; we propose an UCB-like algorithm to solve our problems, CombCascade; and demonstrate gap-free upper limits in his n-step repentance; our evidence builds on recent work in stochastic combinatorial half-bandits, but we also address two novel challenges of our attitude, a nonlinear reward function and partial observability, and we also evaluate two problems that work well."}, {"heading": "1 Introduction", "text": "This year, the time has come for us to be able to find a solution that is capable of finding a solution, that is able to find a solution to all the problems, and that is able to find a solution that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, and that is able to find a solution that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution."}, {"heading": "2 Combinatorial Cascading Bandits", "text": "In this section we present our learning problem, its applications and also our proposed algorithm. We discuss the computational complexity of the algorithm and then present the so-called disjunctive variant of our problem. Random variables are denoted by bold letters. The cardinality of group A is | A | and we assume that min \u2205 = + \u221e. Binary number and operation are denoted by B, and the binary number or is B."}, {"heading": "2.1 Setting", "text": "We assume that the realizable learning problem is a realizable problem. (...) We assume that the realizable learning problem is a realizable problem. (...) We assume that the realizable problem is a realizable problem. (...) We assume that the realizable problem is a realizable problem. (...) We assume that the realizable solution is a realizable problem. (...) We assume that the realizable solution is a realizable problem. (...) We abuse our notation and also treat a realizable problem. (...) We assume that the realizable solution is a realizable problem. (...) We assume that the realizable solution is a realizable problem. (...) We assume that the realizable solution is a realizable problem. (...) We assume that the realizable solution is a realizable problem. (...) We assume that the realizable problem is a realizable problem. (...) We assume that the realizable solution is a realizable problem."}, {"heading": "2.3 Disjunctive Objective", "text": "Our reward model is conjuctive, the reward is one if and only if the weights of all selected items are one. A natural alternative is a disjunctive model rt = maxe \"At wt (e) = enhanced \u2212 \u2212 At wt (e), the reward is one if the weight of any item is one in At. This model arises in referral systems where the referrer is rewarded if the user is satisfied with a recommended item. Feedback Ot is the index of the first item in At whose weight is one, as with cascading bandits [10]. Let f = [0, 1] E \u2192 [0, 1] be a reward function defined as f-point (A, w) = 1 \u2212 and then under the assumption of independence in (2), E [f-point (A, w) e = f-point (A, w)."}, {"heading": "3 Analysis", "text": "We refer to the regrets of CombCascade in Section 3.1 for gapless and gapless ceilings and discuss these limits in Section 3.2."}, {"heading": "3.1 Upper Bounds", "text": "We define the sub-optimality gap of the solution A = (a1,.). We define the sub-optimality gap of the solution A = (a1,.)."}, {"heading": "3.2 Discussion", "text": "In Section 3.1 we demonstrate two upper limits of the n-step regretting of the CombCascade: Theorem 1: O (KL (1 / f) (1 / \u0445) log n), Theorem 2: O (\u221a KL (1 / f) n log n), these limits do not depend on the total number of feasible solutions and are polynomial in any other set of interests.The limits are up to O (1 / f) -factors with the upper limits of CombUCB1 in stochastic combinatorial semi-bandites [12].Since CombCascade receives less feedback than CombUCB1, this is rather surprising and unexpected.The upper limits of Kveton et al. [12] are known to be closely related to polylogarithmic factors. We believe that our upper limits are similar to Kveton et al. [12] The upper limit of Kveton et al. [12] is likely to be the largest part of the network that is recommended to be an optimal one in many cases."}, {"heading": "4 Experiments", "text": "We evaluate CombCascade in three experiments. In Section 4.1 we compare it with CombUCB1 [12], a state-of-the-art algorithm for stochastic combinatorial semi-bandits with linear reward function. This experiment shows that CombUCB1 cannot solve all cases of our problem, underlining the need for a new learning algorithm. It also shows the limitations of CombCascade. We evaluate CombCascade on two real problems in Sections 4.2 and 4.3."}, {"heading": "4.1 Synthetic", "text": "In the first experiment we compare CombUCB1 [12] with CombUCB1 [12] on a synthetic problem. This problem is a combinatorial cascading bandit with L = 4 items and E = {(1, 2), (3, 4). CombUCB1 is a popular algorithm for stochastic combinatorial half-bandits with a linear reward function. We approach maxA (A, w) with minA (e). This approach is motivated by the fact that f (A, w) = equeA w (e)."}, {"heading": "4.2 Network Routing", "text": "We are experimenting with six networks from the RocketFuel dataset [17], shown in Figure 2. Our learning problem is formulated as follows: The base E are the links in the network. The practicable links are all paths in the network. At the moment t we are creating a random pair of start and end nodes, and the learning agent selects a routing path between these nodes. The agent's goal is to maximize the probability that all links in the path are up. The feedback is the index of the first link in the path, which is below. The weight of the link e in due time t, wt (e), is an indicator of the link e in due time. We model wt (e) as an independent Bernoulli random variable wt (e), which is optimal in relation to the link."}, {"heading": "4.3 Diverse Recommendations", "text": "In our last experiment, we rated CombCascade for a problem of different recommendations. This problem is driven by streaming services such as Netflix, which often recommend groups of movies, such as \"Popular on Netflix\" and \"Dramas.\" We are experimenting with the MovieLens datasets from March 2015. The datasets contain 138K ratings for 27K movies. The learning problems are formulated as follows: The grounded movies are from our datasets, 75 random animated movies, 25 random movies, and 75 random non-animated movies, and 75 random non-animated movies."}, {"heading": "5 Related Work", "text": "In fact, it is that we are able to assert ourselves, that we are able to comply, that we are able to comply with the rules, and that we are able to comply with the rules that we have set ourselves."}, {"heading": "6 Conclusions", "text": "We propose combinatorial cascading bandits, a class of stochastic partial monitoring problems that can model many practical problems, such as learning a routing path in an unreliable communications network that maximizes the likelihood of parcel delivery, and learning to recommend a list of attractive elements. We propose a practical UCB-like algorithm for our problems, CombCascade, and prove its upper limits of regret. We evaluate CombCascade for two real-world problems and show that it works well even when our modeling assumptions are violated. Our results and analyses apply to all combinatorial approaches and are therefore quite general. The strongest assumption in our work is that the weights of the elements are distributed independently of each other. This assumption is crucial and difficult to eliminate (Section 2.1). Nevertheless, given the characteristics of items, it can be easily loosened along the lines of Wen et al. [19] We leave this to future work."}, {"heading": "A Proof of Theorem 1", "text": "In Appendix A.1 we have attached the regret to the event that our highly probable confidence intervals do not hold. (Appendix A.2) In Appendix A.3 we have the number of times in which each suboptimal prefix can be chosen instead of the optimal solution. (Appendix A.4) In Appendix A.3 we can use the numerical argument of Kveton et al. (12) and our Proof.Let Rt = R (At, wt) the stochastic regret of the CombCascade at the time t in which the solution and the weights of the items are applied at the time t. (Et = {E s.t.)"}, {"heading": "B Proof of Theorem 2", "text": "The key idea is to divide the regret of the CombCascade into two parts, with the gaps \"At most\" larger and larger than. (9) The first term in (9) can be trivially limited as follows: E [n] T = 1 {n) Rt] = E [n] T = 1 \"At1\" = \"At\" T = 0. \"(9) The second term in (9) can be limited in the same way as R (n) in Theorem 1. The only difference is that\" e, \"min\" E \"T = 1\" At1. \"Hence: E [n] T = 1\" E \"T = 1.\" The second term in (9) can be limited in the same way as R (n) in Theorem 1. The only difference is that \"e\", \"min\" E. \""}, {"heading": "C Technical Lemmas", "text": "Lemma 1. Leave A = (a1,.., a | A |) that K is a workable solution and Bk = (a1,., ak) that K is a prefix of k \u2264 | A | items of A. Then k can be set so that K (A, w), K (K), K (K) and K (K), K (K), K (K), K (K), K (K) and K (K). Then we assume that K (K), K (K), K (K) and K (K). Then we choose K (K), K (K), K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K) and K (K)."}], "references": [{"title": "Asymptotically efficient adaptive allocation schemes for controlled i.i.d. processes: Finite parameter space", "author": ["Rajeev Agrawal", "Demosthenis Teneketzis", "Venkatachalam Anantharam"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1989}, {"title": "Analysis of Thompson sampling for the multi-armed bandit problem", "author": ["Shipra Agrawal", "Navin Goyal"], "venue": "In Proceeding of the 25th Annual Conference on Learning Theory,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Paul Fischer"], "venue": "Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "An adaptive algorithm for finite stochastic partial monitoring", "author": ["Gabor Bartok", "Navid Zolghadr", "Csaba Szepesvari"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Combinatorial multi-armed bandit: General framework, results and applications", "author": ["Wei Chen", "Yajun Wang", "Yang Yuan"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Analysis of point-to-point packet delay in an operational network", "author": ["Baek-Young Choi", "Sue Moon", "Zhi-Li Zhang", "Konstantina Papagiannaki", "Christophe Diot"], "venue": "In Proceedings of the 23rd Annual Joint Conference of the IEEE Computer and Communications Societies,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Learning to rank: Regret lower bounds and efficient algorithms", "author": ["Richard Combes", "Stefan Magureanu", "Alexandre Proutiere", "Cyrille Laroche"], "venue": "In Proceedings of the 2015 ACM SIGMET- RICS International Conference on Measurement and Modeling of Computer Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Combinatorial network optimization with unknown variables: Multi-armed bandits with linear rewards and individual observations", "author": ["Yi Gai", "Bhaskar Krishnamachari", "Rahul Jain"], "venue": "IEEE/ACM Transactions on Networking,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "The KL-UCB algorithm for bounded stochastic bandits and beyond", "author": ["Aurelien Garivier", "Olivier Cappe"], "venue": "In Proceeding of the 24th Annual Conference on Learning Theory,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Cascading bandits: Learning to rank in the cascade model", "author": ["Branislav Kveton", "Csaba Szepesvari", "Zheng Wen", "Azin Ashkan"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Matroid bandits: Fast combinatorial optimization with learning", "author": ["Branislav Kveton", "Zheng Wen", "Azin Ashkan", "Hoda Eydgahi", "Brian Eriksson"], "venue": "In Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Tight regret bounds for stochastic combinatorial semi-bandits", "author": ["Branislav Kveton", "Zheng Wen", "Azin Ashkan", "Csaba Szepesvari"], "venue": "In Proceedings of the 18th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Sequential learning for multi-channel wireless network monitoring with channel switching costs", "author": ["Thanh Le", "Csaba Szepesvari", "Rong Zheng"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Combinatorial partial monitoring game with linear feedback and its applications", "author": ["Tian Lin", "Bruno Abrahao", "Robert Kleinberg", "John Lui", "Wei Chen"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Combinatorial Optimization", "author": ["Christos Papadimitriou", "Kenneth Steiglitz"], "venue": "Dover Publications, Mineola, NY,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "Measuring ISP topologies with Rocketfuel", "author": ["Neil Spring", "Ratul Mahajan", "David Wetherall"], "venue": "IEEE / ACM Transactions on Networking,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["William. R. Thompson"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1933}], "referenceMentions": [{"referenceID": 14, "context": "Combinatorial optimization [16] has many real-world applications.", "startOffset": 27, "endOffset": 31}, {"referenceID": 9, "context": "This feedback model was recently proposed in the so-called cascading bandits [10].", "startOffset": 77, "endOffset": 81}, {"referenceID": 7, "context": "Stochastic online learning with combinatorial actions has been previously studied with semi-bandit feedback and a linear reward function [8, 11, 12], and its monotone transformation [5].", "startOffset": 137, "endOffset": 148}, {"referenceID": 10, "context": "Stochastic online learning with combinatorial actions has been previously studied with semi-bandit feedback and a linear reward function [8, 11, 12], and its monotone transformation [5].", "startOffset": 137, "endOffset": 148}, {"referenceID": 11, "context": "Stochastic online learning with combinatorial actions has been previously studied with semi-bandit feedback and a linear reward function [8, 11, 12], and its monotone transformation [5].", "startOffset": 137, "endOffset": 148}, {"referenceID": 4, "context": "Stochastic online learning with combinatorial actions has been previously studied with semi-bandit feedback and a linear reward function [8, 11, 12], and its monotone transformation [5].", "startOffset": 182, "endOffset": 185}, {"referenceID": 2, "context": "Established algorithms for multi-armed bandits, such as UCB1 [3], KL-UCB [9], and Thompson sampling [18, 2]; can be usually easily adapted to stochastic combinatorial semi-bandits.", "startOffset": 61, "endOffset": 64}, {"referenceID": 8, "context": "Established algorithms for multi-armed bandits, such as UCB1 [3], KL-UCB [9], and Thompson sampling [18, 2]; can be usually easily adapted to stochastic combinatorial semi-bandits.", "startOffset": 73, "endOffset": 76}, {"referenceID": 16, "context": "Established algorithms for multi-armed bandits, such as UCB1 [3], KL-UCB [9], and Thompson sampling [18, 2]; can be usually easily adapted to stochastic combinatorial semi-bandits.", "startOffset": 100, "endOffset": 107}, {"referenceID": 1, "context": "Established algorithms for multi-armed bandits, such as UCB1 [3], KL-UCB [9], and Thompson sampling [18, 2]; can be usually easily adapted to stochastic combinatorial semi-bandits.", "startOffset": 100, "endOffset": 107}, {"referenceID": 11, "context": "[12] recently showed this for CombUCB1, a form of UCB1.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12, 10].", "startOffset": 0, "endOffset": 8}, {"referenceID": 9, "context": "[12, 10].", "startOffset": 0, "endOffset": 8}, {"referenceID": 7, "context": "We also show that CombUCB1 [8, 12] cannot solve some instances of our problem, which highlights the need for a new learning algorithm.", "startOffset": 27, "endOffset": 34}, {"referenceID": 11, "context": "We also show that CombUCB1 [8, 12] cannot solve some instances of our problem, which highlights the need for a new learning algorithm.", "startOffset": 27, "endOffset": 34}, {"referenceID": 0, "context": "The key step in our solution and its analysis is that the reward can be expressed as rt = f(At,wt), where f : \u0398\u00d7 [0, 1] \u2192 [0, 1] is a reward function, which is defined as: f(A,w) = \u220f", "startOffset": 113, "endOffset": 119}, {"referenceID": 0, "context": "The key step in our solution and its analysis is that the reward can be expressed as rt = f(At,wt), where f : \u0398\u00d7 [0, 1] \u2192 [0, 1] is a reward function, which is defined as: f(A,w) = \u220f", "startOffset": 122, "endOffset": 128}, {"referenceID": 0, "context": "e\u2208A w(e) , A \u2208 \u0398 , w \u2208 [0, 1] .", "startOffset": 23, "endOffset": 29}, {"referenceID": 9, "context": "[10] and it is critical to our results.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "This assumption is common in the existing network routing models [6].", "startOffset": 65, "endOffset": 68}, {"referenceID": 0, "context": "First, it computes the upper confidence bounds (UCBs) Ut \u2208 [0, 1] on the expected weights of all items in E.", "startOffset": 59, "endOffset": 65}, {"referenceID": 11, "context": "If w0 is unavailable, we can formulate the problem of obtaining w0 as an optimization problem on \u0398 with a linear objective [12].", "startOffset": 123, "endOffset": 127}, {"referenceID": 11, "context": "[12] tracks observed items and adaptively chooses solutions with the maximum number of unobserved items.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "The feedback Ot is the index of the first item in At whose weight is one, as in cascading bandits [10].", "startOffset": 98, "endOffset": 102}, {"referenceID": 0, "context": "Let f\u2228 : \u0398\u00d7 [0, 1]\u2192 [0, 1] be a reward function, which is defined as f\u2228(A,w) = 1\u2212 \u220f e\u2208A(1\u2212 w(e)).", "startOffset": 12, "endOffset": 18}, {"referenceID": 0, "context": "Let f\u2228 : \u0398\u00d7 [0, 1]\u2192 [0, 1] be a reward function, which is defined as f\u2228(A,w) = 1\u2212 \u220f e\u2208A(1\u2212 w(e)).", "startOffset": 20, "endOffset": 26}, {"referenceID": 11, "context": "The main idea is to reduce our analysis to that of CombUCB1 in stochastic combinatorial semi-bandits [12].", "startOffset": 101, "endOffset": 105}, {"referenceID": 11, "context": "[12].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "the upper bounds of CombUCB1 in stochastic combinatorial semi-bandits [12].", "startOffset": 70, "endOffset": 74}, {"referenceID": 11, "context": "[12] are known to be tight up to polylogarithmic factors.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12], where the expected weight of each item is close to 1 and likely to be observed.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "1, we compare it to CombUCB1 [12], a state-of-the-art algorithm for stochastic combinatorial semi-bandits with a linear reward function.", "startOffset": 29, "endOffset": 33}, {"referenceID": 11, "context": "1 Synthetic In the first experiment, we compare CombCascade to CombUCB1 [12] on a synthetic problem.", "startOffset": 72, "endOffset": 76}, {"referenceID": 15, "context": "We experiment with six networks from the RocketFuel dataset [17], which are described in Figure 2a.", "startOffset": 60, "endOffset": 64}, {"referenceID": 9, "context": "[10] to arbitrary combinatorial constraints.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Our work also extends stochastic combinatorial semi-bandits with a linear reward function [8, 11, 12] to the cascade model of feedback.", "startOffset": 90, "endOffset": 101}, {"referenceID": 10, "context": "Our work also extends stochastic combinatorial semi-bandits with a linear reward function [8, 11, 12] to the cascade model of feedback.", "startOffset": 90, "endOffset": 101}, {"referenceID": 11, "context": "Our work also extends stochastic combinatorial semi-bandits with a linear reward function [8, 11, 12] to the cascade model of feedback.", "startOffset": 90, "endOffset": 101}, {"referenceID": 6, "context": "[7].", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "CombUCB1 [12] chooses solutions with the largest sum of the UCBs.", "startOffset": 9, "endOffset": 13}, {"referenceID": 9, "context": "CascadeUCB1 [10] chooses K items out of L with the largest UCBs.", "startOffset": 12, "endOffset": 16}, {"referenceID": 9, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] to achieve linear dependency on K in Theorem 1.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] and Bartok et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] studied partial monitoring problems and proposed learning algorithms for solving them.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4], we get |\u0398| actions and 2 unobserved outcomes; and the learning algorithm reasons over |\u0398| pairs of actions and requires O(2) space.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[15] also studied combinatorial partial monitoring.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] studied stochastic combinatorial semi-bandits with a non-linear reward function, which is a known monotone function of an unknown linear function.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] is semi-bandit, which is more informative than in our work.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[14] studied a network optimization problem where the reward function is a non-linear function of observations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "References [1] Rajeev Agrawal, Demosthenis Teneketzis, and Venkatachalam Anantharam.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Shipra Agrawal and Navin Goyal.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Gabor Bartok, Navid Zolghadr, and Csaba Szepesvari.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Wei Chen, Yajun Wang, and Yang Yuan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Baek-Young Choi, Sue Moon, Zhi-Li Zhang, Konstantina Papagiannaki, and Christophe Diot.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Richard Combes, Stefan Magureanu, Alexandre Proutiere, and Cyrille Laroche.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Yi Gai, Bhaskar Krishnamachari, and Rahul Jain.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Aurelien Garivier and Olivier Cappe.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Branislav Kveton, Csaba Szepesvari, Zheng Wen, and Azin Ashkan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Branislav Kveton, Zheng Wen, Azin Ashkan, Hoda Eydgahi, and Brian Eriksson.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Branislav Kveton, Zheng Wen, Azin Ashkan, and Csaba Szepesvari.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] Thanh Le, Csaba Szepesvari, and Rong Zheng.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] Tian Lin, Bruno Abrahao, Robert Kleinberg, John Lui, and Wei Chen.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] Christos Papadimitriou and Kenneth Steiglitz.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] Neil Spring, Ratul Mahajan, and David Wetherall.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] William.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] and finish our proof.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] It remains to bound R\u0302(n) in (8).", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "The key idea of our proof is to introduce infinitely-many mutually-exclusive events and then bound the number of times that these events happen when a suboptimal prefix is chosen [12].", "startOffset": 179, "endOffset": 183}, {"referenceID": 11, "context": "[12], Gi,t are exhaustive at any time t when (\u03b1i) and (\u03b2i) satisfy: \u221a 6 \u221e \u2211", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11]: [ \u2206e,1 1 \u2206e,1 + Ne \u2211", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12], \u2211\u221e i=1 \u03b1i \u03b2i < 267.", "startOffset": 0, "endOffset": 4}], "year": 2015, "abstractText": "We propose combinatorial cascading bandits, a class of partial monitoring problems where at each step a learning agent chooses a tuple of ground items subject to constraints and receives a reward if and only if the weights of all chosen items are one. The weights of the items are binary, stochastic, and drawn independently of each other. The agent observes the index of the first chosen item whose weight is zero. This observation model arises in network routing, for instance, where the learning agent may only observe the first link in the routing path which is down, and blocks the path. We propose a UCB-like algorithm for solving our problems, CombCascade; and prove gap-dependent and gap-free upper bounds on its n-step regret. Our proofs build on recent work in stochastic combinatorial semi-bandits but also address two novel challenges of our setting, a non-linear reward function and partial observability. We evaluate CombCascade on two real-world problems and show that it performs well even when our modeling assumptions are violated. We also demonstrate that our setting requires a new learning algorithm.", "creator": "LaTeX with hyperref package"}}}