{"id": "1507.04646", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jul-2015", "title": "A Dependency-Based Neural Network for Relation Classification", "abstract": "Previous research on relation classification has verified the effectiveness of using dependency shortest paths or subtrees. In this paper, we further explore how to make full use of the combination of these dependency information. We first propose a new structure, termed augmented dependency path (ADP), which is composed of the shortest dependency path between two entities and the subtrees attached to the shortest path. To exploit the semantic representation behind the ADP structure, we develop dependency-based neural networks (DepNN): a recursive neural network designed to model the subtrees, and a convolutional neural network to capture the most important features on the shortest path. Experiments on the SemEval-2010 dataset show that our proposed method achieves state-of-art results.", "histories": [["v1", "Thu, 16 Jul 2015 16:43:55 GMT  (704kb)", "http://arxiv.org/abs/1507.04646v1", "This preprint is the full version of a short paper accepted in the annual meeting of the Association for Computational Linguistics (ACL) 2015 (Beijing, China)"]], "COMMENTS": "This preprint is the full version of a short paper accepted in the annual meeting of the Association for Computational Linguistics (ACL) 2015 (Beijing, China)", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["yang liu", "furu wei", "sujian li", "heng ji", "ming zhou 0001", "houfeng wang"], "accepted": true, "id": "1507.04646"}, "pdf": {"name": "1507.04646.pdf", "metadata": {"source": "CRF", "title": "A Dependency-Based Neural Network for Relation Classification", "authors": ["Yang Liu", "Furu Wei Sujian Li", "Heng Ji Ming Zhou", "Houfeng Wang"], "emails": ["wanghf}@pku.edu.cn", "mingzhou}@microsoft.com", "jih@rpi.edu"], "sections": [{"heading": null, "text": "ar Xiv: 150 7.04 646v 1 [cs.C L] 16 Jul 2 015"}, {"heading": "1 Introduction", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able, that we are able to hide ourselves, and that we are able, that we will be able, that we will be able, that we will be able, that we will be able."}, {"heading": "2 Problem Definition and Motivation", "text": "The task of relation classification can be defined as follows: Given a set of S with a pair of entities e1 and e2, the task is to identify the semantic relationship between e1 and e2 in accordance with a set of predefined relationship types. According to the official guidance of SemEval-2010 task 8 (Hendrickx et al., 2010), there are 9 relationship types ordered. We list them in Table 1 with their simplified definitions. Ingredients do not stand out as others. Figure 2 shows the relationship between two entities e1 and e2 = screwdrivers."}, {"heading": "3 Dependency-Based Neural Networks", "text": "In this section, we will present how we use neural network techniques and dependency information to explore the semantic connection between two entities. We refer to our architecture of modeling ADP structures as dependency-based neural networks (DepNN). Figure 3 illustrates DepNN using a concrete example. First, we associate each word w and dependency relationship r with a vector representation xw, xr and Rdim. For each word w on the shortest dependence path, we develop an RNN from its leaf words to the root to create a subtree that embeds cw and concatenates cw with xw to serve as a definitive representation. Next, a CNN will model the shortest dependency path based on the representation of its words and relationships. Finally, our framework can represent the semantic connection between two entities taking into account more comprehensive dependency information."}, {"heading": "3.1 Modeling Dependency Subtree", "text": "The goal of modeling dependency subtrees is to find an appropriate representation for the words on the shortest path q. As mentioned above, we assume that each word w can be interpreted by itself and its children on the dependency subtree. Then, for each word w on the subtree, we insert its subtree representation as cLEAF. Subtree representation of a word is derived by transforming the representations of its children's words. During the subtree construction of the subtree, each word with a dependency relationship such as dobj as in Figure 3.Take the ADP in Figure 3 for example, we first compute the representations of a word such as pthe, pthe = [xthe, cLEAF] (1) we shorten the dependency relationship as in Figure 3.Take the ADP in Figure 3, we first calculate the representations of leaves such as pthe, pthe = [xthe, cLEAF] (1)."}, {"heading": "3.2 Modeling Shortest Dependency Path", "text": "To classify the relationship between two entities, let's further explore the semantic representation behind their shortest dependency path, which can be seen as a sequence of words with dependency relationships. Let's take the shortest dependency path in the last subsection for example. The sequence S will be, S: [priests nsubj broke prep-with work] w1 r1 w2 r2 w3As the Convolutionary Neural Network (CNN) is good at capturing the prominent features of a sequence of objects, let's design a CNN to tackle the shortest dependency path. A CNN contains a folding operation over windows of object representations, followed by a pooling operation. As we know, a word w on the shortest path is associated with the representation pw by modeling the subtree. For a dependency relationship r on the shortest path, let's lay its representation as vector xr Rdim. As a sliding window, we apply the sequence we put the window size."}, {"heading": "3.3 Learning", "text": "As with other classification systems for relations, we also include some lexical plane characteristics that prove useful for this task, such as named entity tags and WordNet hypernyms of e1 and e2. We associate them with the ADP representation L to create a combined vector M. We then pass M to a fully connected Softmax plane whose output is the probability distribution y over relation labels.M = [L, LEX] (7) y = Softmax (W2M) (8) We define the ground truth label vector t for each instance as a binary vector. If the instance belongs to the i-th type, only ti 1 and the other dimensions are set to 0. To learn the parameters, we optimize the crossing error between y and t by means of stochastic gradient descent (Bottou, 2004)."}, {"heading": "4 Experiments", "text": "Our experiments are conducted on the basis of the SemEval 2010 dataset (Hendrickx et al., 2010). The training part of the dataset comprises 8000 instances and the test part 2717 instances. Table 2 shows the statistics of the commented relationship types of this dataset. We see that the distribution of relationship types in the test set is similar to that in the training set. The official evaluation parameter is the macro-averaged F1 score (excluding others). We use dependency trees generated by the Stanford parser (Klein and Manning, 2003) with the \"collapsed\" option, which considers a preposition as a kind of dependency relationship. As de Marneffe and Manning (2008) pointed out, this option is more useful for extracting event relationships."}, {"heading": "4.1 Analysis of DepNN", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1.1 Contributions of different components", "text": "In our experiments, twokinds of word embeddings are used for initialization. One is the 50-d embeddings provided by SENNA (Collobert et al., 2011). The second is the 200-d embeddings (Yu et al., 2014) trained on Gigaword with word2vec1. The corresponding hyperparameters are set with 5-fold cross-validation, including window size k, learning rate \u03bb, subtree embeddings' dimension dimc, and hidden layer size. The final settings are given in Table 3.For Evaluation, we first design a relation extraction system (named PATH) which only models the shortest dependency path with a CNN. Based on PATH, We think to incorporate the two types of lexical features including named entity tags (NER) and WordNet delnyms (WN)."}, {"heading": "4.1.2 Intuitive Analysis of Shortest Path", "text": "We take the output vector of the CNN layer as a distributed representation of a dependency path. In this way, we can calculate the cosmic similarity between two arbitrary paths and illustrate some paths with great similarity. Table 5 shows three training cases with different relationship types and their three most similar paths in the test set.From Table 5, we can see that our approach can capture the core meaning of the shortest dependency paths. For example, for the InstrumentAgency relationship, we conclude that the dependency relationships \"nsubj inv,\" \"dobj\" and \"prep with\" in the dependency path can play a major role in the representation and our model can capture these similar paths. In terms of the product-producer relationship, our model focuses on the representation of the structure of \"nsubj inv inv verb1 xcomp verb2 dobj\" and uses some words such as \"pencil\" and \"create\" in the path representation. This is clearer for the messaic relationship that \"point of similarity is well explored.\""}, {"heading": "4.1.3 Influence of Attached Subtree", "text": "By comparing the results of DepNN before and after the addition of the subtree, we find that the influence of this structure differs from different relation types. Table 6 shows the F1 measurements of each relation type before and after the addition of the subtree. We see that the subtree information generally has a positive impact on all relationship types. It is particularly important for the relationship instrument-agency and product-producer. Since we use only the shortest dependency types, these two types of relation types can be easily confused, as they are both based on the dependency types, such as... \"verb prep-by / prep-with / using.\" But after looking at the subtree information, we can better distinguish these two relationship types. Figure 4 lists two instances that can be correctly classified only by adding the subtree. Figure 4a belongs to the producer-production ratio, which is explained by the subtree structure that contains conformations like"}, {"heading": "4.2 Comparison with Baselines", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "5 Related Work", "text": "It is a question of the extent to which the two persons in the past were in fact persons who were able to play by the rules. (...) It is a question of the extent to which they were persons who were able to play by the rules. (...) It is a question of persons who were able to play by the rules. (...) It is a question of persons who were able to play by the rules. (...) It is a question of persons who were able to play by the rules. (...) It is a question of persons who were able to play by the rules. (...) It is a question of persons who were able to play by the rules. (...) It is a question of persons who were able to play by the rules. (...) It is a question of persons who were able to play by the rules. (...). (...) It is a question of self-determination. (...)"}, {"heading": "6 Conclusion", "text": "In this paper, we propose to classify relationships between entities by modelling the extended dependency path in a neural network. In a particular case, we generate its ADP by combining the shortest path between two entities and their associated sub-trees. We present a novel approach, DepNN, to take advantage of the Convolutionary Neural Network and the Recursive Neural Network to model this structure."}], "references": [{"title": "Stochastic learning", "author": ["L\u00e9on Bottou"], "venue": "In Advanced lectures on machine learning,", "citeRegEx": "Bottou.,? \\Q2004\\E", "shortCiteRegEx": "Bottou.", "year": 2004}, {"title": "A Shortest Path Dependency Kernel for Relation Extraction", "author": ["Bunescu", "Mooney2005] Razvan C. Bunescu", "Raymond J. Mooney"], "venue": "In North American Chapter of the Association for Computational Linguistics", "citeRegEx": "Bunescu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bunescu et al\\.", "year": 2005}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Dependency Tree Kernels for Relation Extraction", "author": ["Culotta", "Sorensen2004] Aron Culotta", "Jeffrey S. Sorensen"], "venue": "In Meeting of the Association for Computational Linguistics,", "citeRegEx": "Culotta et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Culotta et al\\.", "year": 2004}, {"title": "The Stanford typed dependencies representation", "author": ["de Marneffe", "Christopher D. Manning"], "venue": "In International Conference on Computational Linguistics", "citeRegEx": "Marneffe et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2008}, {"title": "Exploring various knowledge in relation extraction", "author": ["GuoDong et al.2005] Zhou GuoDong", "Su Jian", "Zhang Jie", "Zhang Min"], "venue": "In Proceedings of the 43rd annual meeting on association for computational linguistics,", "citeRegEx": "GuoDong et al\\.,? \\Q2005\\E", "shortCiteRegEx": "GuoDong et al\\.", "year": 2005}, {"title": "SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations Between Pairs", "author": ["Zornitsa Kozareva", "Preslav Nakov", "Sebastian Pad ok", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz"], "venue": null, "citeRegEx": "Hendrickx et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hendrickx et al\\.", "year": 2010}, {"title": "A neural network for factoid question answering over paragraphs", "author": ["Iyyer et al.2014] Mohit Iyyer", "Jordan Boyd-Graber", "Leonardo Claudino", "Richard Socher", "Hal Daum\u00e9 III"], "venue": "In Proceedings of the 2014 Conference on Empir-", "citeRegEx": "Iyyer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2014}, {"title": "Accurate Unlexicalized Parsing", "author": ["Klein", "Manning2003] Dan Klein", "Christopher D. Manning"], "venue": "In Meeting of the Association for Computational Linguistics,", "citeRegEx": "Klein et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2003}, {"title": "Relation extraction from wikipedia using subtree mining", "author": ["Nguyen et al.2007] Dat PT Nguyen", "Yutaka Matsuo", "Mitsuru Ishizuka"], "venue": "In Proceedings of the National Conference on Artificial Intelligence,", "citeRegEx": "Nguyen et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2007}, {"title": "Utd: Classifying semantic relations by combining lexical and semantic resources", "author": ["Rink", "Harabagiu2010] Bryan Rink", "Sanda Harabagiu"], "venue": "In Proceedings of the 5th International Workshop on Semantic Evaluation,", "citeRegEx": "Rink et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rink et al\\.", "year": 2010}, {"title": "Learning representations by back-propagating errors", "author": ["Geoffrey E Hinton", "Ronald J Williams"], "venue": "Cognitive modeling,", "citeRegEx": "Rumelhart et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Brody Huval", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Grounded compositional semantics for finding and describing images", "author": ["Andrej Karpathy", "Quoc V Le", "Christopher D Manning", "Andrew Y Ng"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Combining linguistic and statistical analysis to extract relations from web documents", "author": ["Georgiana Ifrim", "Gerhard Weikum"], "venue": "In Proceedings of the 12th ACM SIGKDD international conference on Knowl-", "citeRegEx": "Suchanek et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Suchanek et al\\.", "year": 2006}, {"title": "A re-examination of dependency path kernels for relation extraction", "author": ["Mengqiu Wang"], "venue": "In IJCNLP,", "citeRegEx": "Wang.,? \\Q2008\\E", "shortCiteRegEx": "Wang.", "year": 2008}, {"title": "Factor-based compositional embedding models", "author": ["Yu et al.2014] Mo Yu", "Matthew Gormley", "Mark Dredze"], "venue": "In NIPS Workshop on Learning Semantics", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Kernel Methods for Relation Extraction", "author": ["Chinatsu Aone", "Anthony Richardella"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zelenko et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zelenko et al\\.", "year": 2003}, {"title": "Relation classification via convolutional deep neural network", "author": ["Zeng et al.2014] Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao"], "venue": "In Proceedings of COLING", "citeRegEx": "Zeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}, {"title": "A Composite Kernel to Extract Relations between Entities with Both Flat and Structured Features", "author": ["Zhang et al.2006] Min Zhang", "Jie Zhang", "Jian Su", "Guodong Zhou"], "venue": "In Meeting of the Association for Computational Linguistics", "citeRegEx": "Zhang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2006}, {"title": "Exploring Various Knowledge in Relation Extraction", "author": ["Zhou et al.2005] Guodong Zhou", "Jian Su", "Jie Zhang", "Min Zhang"], "venue": "In Meeting of the Association for Computational Linguistics", "citeRegEx": "Zhou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2005}, {"title": "Tree kernel-based relation extraction with contextsensitive structured parse tree information", "author": ["Zhou et al.2007] GuoDong Zhou", "Min Zhang", "Dong Hong Ji", "Qiaoming Zhu"], "venue": null, "citeRegEx": "Zhou et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 14, "context": "For example, Suchanek et al. (2006) carefully selected a set of features from tokenization and dependency parsing, and extended some of them to", "startOffset": 13, "endOffset": 36}, {"referenceID": 9, "context": "Nguyen et al. (2007) developed these ideas by analyzing multiple subtrees with the guidance of pre-extracted keywords.", "startOffset": 0, "endOffset": 21}, {"referenceID": 6, "context": "According to the the official guideline of SemEval-2010 task 8 (Hendrickx et al., 2010), there are 9 ordered relation types.", "startOffset": 63, "endOffset": 87}, {"referenceID": 6, "context": "According to the the official guideline of SemEval-2010 task 8 (Hendrickx et al., 2010), there are 9 ordered relation types. We list them in Table 1 with their simplified definitions. Instances don\u2019t fall in any of these types are labeled as Other. For example, in Figure 2, the relation between two entities e1=thief and e2=screwdriver is Instrument-Agency. Bunescu and Mooney (2005) reported that, for the relation classification task, the shortest dependency path between two entities plays a vital role.", "startOffset": 64, "endOffset": 385}, {"referenceID": 2, "context": "Next, we adopt the widely-used max-over-time pooling operation (Collobert et al., 2011), which can retain the most important features, to obtain the final representation L from the feature map.", "startOffset": 63, "endOffset": 87}, {"referenceID": 0, "context": "To learn the parameters, we optimize the cross-entropy error between y and t using stochastic gradient descent (Bottou, 2004).", "startOffset": 111, "endOffset": 125}, {"referenceID": 11, "context": "Gradients are computed using backpropagation (Rumelhart et al., 1988).", "startOffset": 45, "endOffset": 69}, {"referenceID": 6, "context": "Our experiments are performed on SemEval-2010 dataset (Hendrickx et al., 2010).", "startOffset": 54, "endOffset": 78}, {"referenceID": 6, "context": "Our experiments are performed on SemEval-2010 dataset (Hendrickx et al., 2010). The training part of the dataset includes 8000 instances, and the test part includes 2717 instances. Table 2 shows the statistics of the annotated relation types of this dataset. We can see that the distribution of relation types in the test set is similar to that in the training set. The official evaluation metric is the macro-averaged F1-score (excluding Other). We use dependency trees generated by the Stanford Parser (Klein and Manning, 2003) with the \u201ccollapsed\u201d option, which regards a preposition as a kind of dependency relation. As de Marneffe and Manning (2008) pointed out, this option is more useful for event relation extraction.", "startOffset": 55, "endOffset": 655}, {"referenceID": 2, "context": "One is the 50-d embeddings provided by SENNA (Collobert et al., 2011).", "startOffset": 45, "endOffset": 69}, {"referenceID": 16, "context": "The second is the 200-d embeddings (Yu et al., 2014) trained on Gigaword with word2vec1.", "startOffset": 35, "endOffset": 52}, {"referenceID": 16, "context": "WordNet seems less useful than NER, which conforms to the results of Yu et al. (2014) , since a large number of WordNet hypernyms may cause overfitting.", "startOffset": 69, "endOffset": 86}, {"referenceID": 12, "context": "MV-RNN (Socher et al., 2012): This model associates each word with a matrix.", "startOffset": 7, "endOffset": 28}, {"referenceID": 13, "context": "DT-RNN (Socher et al., 2014) : This model uses an RNN for modeling dependency trees.", "startOffset": 7, "endOffset": 28}, {"referenceID": 7, "context": "The network is trained using the method provided by (Iyyer et al., 2014).", "startOffset": 52, "endOffset": 72}, {"referenceID": 18, "context": "CNN: Zeng et al. (2014) build a convolutional model to learn a sentence representation over the words in a sentence.", "startOffset": 5, "endOffset": 24}, {"referenceID": 16, "context": "FCM (Yu et al., 2014): FCM decomposes a sentence into some substructures and learns substructure embedding from each of them.", "startOffset": 4, "endOffset": 21}, {"referenceID": 20, "context": "These features are combined to form a feature vector employed in a Max Entropy (Kambhatla, ) or an SVM (Zhou et al., 2005; GuoDong et al., 2005) classifier.", "startOffset": 103, "endOffset": 144}, {"referenceID": 5, "context": "These features are combined to form a feature vector employed in a Max Entropy (Kambhatla, ) or an SVM (Zhou et al., 2005; GuoDong et al., 2005) classifier.", "startOffset": 103, "endOffset": 144}, {"referenceID": 16, "context": "Zelenko et al. (2003) designed a tree kernel to compute the structural commonality between shallow parse trees by a weighted sum of the number of common subtrees.", "startOffset": 0, "endOffset": 22}, {"referenceID": 16, "context": "Zelenko et al. (2003) designed a tree kernel to compute the structural commonality between shallow parse trees by a weighted sum of the number of common subtrees. Culotta and Sorensen (2004) transferred this kernel to a dependency tree and attached more information including POS tag, word chunk tag to each node.", "startOffset": 0, "endOffset": 191}, {"referenceID": 16, "context": "Zelenko et al. (2003) designed a tree kernel to compute the structural commonality between shallow parse trees by a weighted sum of the number of common subtrees. Culotta and Sorensen (2004) transferred this kernel to a dependency tree and attached more information including POS tag, word chunk tag to each node. Zhou et al. (2007) proposed a contextsensitive convolution tree kernel that used context information beyond the local tree.", "startOffset": 0, "endOffset": 333}, {"referenceID": 16, "context": "Zelenko et al. (2003) designed a tree kernel to compute the structural commonality between shallow parse trees by a weighted sum of the number of common subtrees. Culotta and Sorensen (2004) transferred this kernel to a dependency tree and attached more information including POS tag, word chunk tag to each node. Zhou et al. (2007) proposed a contextsensitive convolution tree kernel that used context information beyond the local tree. In another view, Bunescu and Mooney (2005) provided an important insight that the shortest path between the two entities concentrates most of the information for identifying the relation between them.", "startOffset": 0, "endOffset": 481}, {"referenceID": 9, "context": "Nguyen et al. (2007) used the dependency subtrees in a different manner by modeling the subtrees between entities and keywords of certain relations.", "startOffset": 0, "endOffset": 21}, {"referenceID": 9, "context": "Nguyen et al. (2007) used the dependency subtrees in a different manner by modeling the subtrees between entities and keywords of certain relations. Zhang et al. (2006) further proposed composite kernels to combine a tree kernel and a feature-based kernel to promote the performance.", "startOffset": 0, "endOffset": 169}, {"referenceID": 12, "context": "Socher et al. (2012) proposed a recursive neural network model by constructing compositional semantics for the minimal constituent of a constituent parse tree including both marked entities.", "startOffset": 0, "endOffset": 21}, {"referenceID": 12, "context": "Socher et al. (2012) proposed a recursive neural network model by constructing compositional semantics for the minimal constituent of a constituent parse tree including both marked entities. Zeng et al. (2014) used a convolutional neural network over the whole sentence combined with some lexical features.", "startOffset": 0, "endOffset": 210}, {"referenceID": 12, "context": "Socher et al. (2012) proposed a recursive neural network model by constructing compositional semantics for the minimal constituent of a constituent parse tree including both marked entities. Zeng et al. (2014) used a convolutional neural network over the whole sentence combined with some lexical features. They also pointed out that the position of each word in the sentence is very important for relation classification and concatenated a special position feature vector with the corresponding word embedding. Yu et al. (2014) proposed the Factor-based Compositional Embedding Model which extracted features from the substructures of a sentence and combined them through a sum-pooling layer.", "startOffset": 0, "endOffset": 529}], "year": 2015, "abstractText": "Previous research on relation classification has verified the effectiveness of using dependency shortest paths or subtrees. In this paper, we further explore how to make full use of the combination of these dependency information. We first propose a new structure, termed augmented dependency path (ADP), which is composed of the shortest dependency path between two entities and the subtrees attached to the shortest path. To exploit the semantic representation behind the ADP structure, we develop dependency-based neural networks (DepNN): a recursive neural network designed to model the subtrees, and a convolutional neural network to capture the most important features on the shortest path. Experiments on the SemEval-2010 dataset show that our proposed method achieves state-of-art results.", "creator": "LaTeX with hyperref package"}}}