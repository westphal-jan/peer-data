{"id": "1507.04808", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jul-2015", "title": "Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models", "abstract": "We consider the task of generative dialogue modeling for movie scripts. To this end, we extend the recently proposed hierarchical recurrent encoder decoder neural network and demonstrate that this model is competitive with state-of-the-art neural language models and backoff n-gram models. We show that its performance can be improved considerably by bootstrapping the learning from a larger question-answer pair corpus and from pretrained word embeddings.", "histories": [["v1", "Fri, 17 Jul 2015 00:21:39 GMT  (336kb)", "http://arxiv.org/abs/1507.04808v1", "11 pages with references; under review at EMNLP 2015"], ["v2", "Wed, 25 Nov 2015 19:49:39 GMT  (214kb,D)", "http://arxiv.org/abs/1507.04808v2", "8 pages with references; will appear in AAAI 2016 (Special Track on Cognitive Systems)"], ["v3", "Wed, 6 Apr 2016 23:20:41 GMT  (215kb,D)", "http://arxiv.org/abs/1507.04808v3", "8 pages with references; Published in AAAI 2016 (Special Track on Cognitive Systems)"]], "COMMENTS": "11 pages with references; under review at EMNLP 2015", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG cs.NE", "authors": ["iulian vlad serban", "alessandro sordoni", "yoshua bengio", "aaron c courville", "joelle pineau"], "accepted": true, "id": "1507.04808"}, "pdf": {"name": "1507.04808.pdf", "metadata": {"source": "CRF", "title": "Hierarchical Neural Network Generative Models for Movie Dialogues", "authors": ["Iulian V. Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 150 7.04 808v 1 [cs.C L] 17 Jul 2 015"}, {"heading": "1 Introduction", "text": "We have the ability to divide the dialog systems into a variety of applications ranging from technical assistance to language learning tools and entertainment (Young et al., 2013; Shawar and Atwell, 2007). Dialogue systems can be divided into goal-oriented systems, such as technical assistance, and non-goal-oriented systems, such as language learning tools or computer games, which use hand-powered dialog systems to view the dialog problem as a partially observable decision-making process (Young et al., 2013; Pieraccini et al., 2009). Unfortunately, most dialog systems use hand-crafted functions for the state and the room for maneuver, requiring either a large annotated task-specific corpus or a horde of human subjects willing to interact with the unfinished system, making it not only expensive and time-consuming to deploy a real dialog system, but also limiting its use to a narrow domain."}, {"heading": "2 Models", "text": "We consider a dialog as a sequence of M expressions D = {U1,.., UM} with two interlocutors. Each Um contains a sequence of M characters, i.e. Um = {wm, 1,.., wm, Nm}, where wm, n is a random variable that takes the values in the vocabulary V and the character at position n. The characters represent both words and dialog files, e.g. the end of a rotation and pause token. A generative model of dialog parameters describes a probability distribution P - governed by parameters that go beyond the set of all possible dialogs of arbitrary lengths."}, {"heading": "2.1 Recurrent Neural Network", "text": "A recurrent neural network (RNN) is referred to as \"f\" (n). (hN) s \"D\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s.\" s \"s\" s \"s\" s. \"s\" s \"s\" s. \"s\" s \"s.\" s \"s\" s \"s.\" s \"s\" s. \"s\" s \"s.\" s \"s\" s \"s.\" s \"s\" s \"s.\" s \"s\" s \"s.\" s \"s\" s \"s.\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s"}, {"heading": "2.2 Hierarchical Recurrent Encoder Decoder", "text": "In fact, it is such that the greater number of people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "2.3 Bidirectional HRED", "text": "In HRED, the representation of the utterance is given by the last hidden state of the encoder RNN. This architecture worked well for web queries, but may be insufficient for dialog expressions that are longer and contain more syntactical articulations than web queries. In case of long expressions, the last state of the encoder RNN may not reflect important information at the beginning of the utterance. Therefore, we propose to extend the HRED architecture by additional representation capabilities on the encoder component. We choose to model the encoder for utterance with a bidirectional RNN, which has proven useful for machine translation. Bidirectional RNNs run two chains: one forward through the utterance marks and one backward, i.e. the reversal of the to-kings in the utterance."}, {"heading": "2.4 Bootstrapping From Word Embeddings", "text": "Therefore, our models can be significantly improved by learning word embeddings from larger corpora. This is advantageous for classifying user intentions (Forgues et al., 2014). We opt to initialize our word embeddings E with Word2Vec2 (Mikolov et al., 2013), which appears on the Google News record con-1Note that the bi-directional RNN always contains an expression behind the RNN.2http: / / code.google.com / p / word2vec / decoder, which contains about 100 billion words. The sheer size of the data set ensures that the embeddings contain rich semantic information about each word."}, {"heading": "2.5 Bootstrapping From Subtitles Q-A", "text": "In order to learn a good initialization point for these other parameters, we can pre-train the model on a large dialogue-free corpus covering similar topics and types of interactions between interlocutors. One such corpus is the Q-A SubTle Corpus, which contains about 5.5M Q-A pairs constructed from movie subtitles. Now, we construct an artificial dialogue record by using each {Q, A} pair as a two-level dialogue D = {U1 = Q, U2 = A}. Since there are two expressions in each example, all model parameters are updated during the training. However, since these examples are short, it may be that the higher-level context RNN is not initialized to a very useful point for the HRED models."}, {"heading": "3 Related Work", "text": "Modelling conversations on microblogging sites using generative probabilistic models was first proposed by Ritter et al. (2011), who consider the response generation problem to be a translation problem where a post has to be translated into an answer. Generating responses was much more difficult than translating between languages, due to the wide range of plausible answers and the lack of focus on words and phrases between the post and the answer. Specifically, they found that the approach of statistical machine translation was superior to the approach of information recovery. In the same way, Shang et al. (2015) proposed using the neural network as an encoder decoder framework for generating responses on the microblogging site Weibo. They also formulated the problem as a conditional generation where a post is given, the model generates an answer. Unfortunately, this architecture scales with the number of dialog turns. One way to view the conversation context was by Sordoni et al."}, {"heading": "4 Dataset", "text": "The MovieTriples dataset was developed by extending and pre-processing the Movie DiC dataset from Banchs et al. (2012) to adapt it to the generative dialog modeling framework. Based on a literature review, we found that the MovieDiC was the largest dataset available that contained all successive statements from movies. Other datasets in the literature include the corpus of Walker et al. (2012a), Roy et al. (2014) and the unpublished Cornell Movie Dialogue Corpus4.Unlike similarly large domain-specific datasets (Uthus and Aha, 2013; Walker et al., 2012b), film scripts cover a wide range of topics, making them ideal for studying semantic understanding of dialogue models. Unlike micro-blogging sites such as Twitter (Ritter et al., 2010), film scripts are rich in dialogues with few participants, making them relatively suitable for long-term modeling."}, {"heading": "4.1 Extraction And Preprocessing", "text": "We added meta-information for each movie extracted via the online API service OMDBAPI5 to the dataset, then edited the dataset to remove duplicate manuscripts, then applied a spell checker based on Wikipedia's most common English spelling errors (6), then implemented a series of simple regular expressions to remove duplicate punctuation and spacing, used the python-based toolkit NLTK (Bird et al., 2009) to perform tokenization and recognition of name units (7), replaced all names and numbers with < person > and < name > tokens, respectively, and replaced numbers with < number > tokens. Using placeholders allows measurement of performance w.r.t. The abstract semantic and syntactic structure of dialogs has been replaced by < person > and < name > tokens."}, {"heading": "4.2 Triples Construction", "text": "The atomic entry of the MovieTriples is a \"triple\" {U1, U2, U3}, i.e. a dialogue with three turns, which takes place between two interlocutors A and B, for which 5 http: / / www.omdbapi.com6Accessed on February 20, 2015: http: / / en.wikipedia.org / wiki / Wikipedia: Lists _ of _ common _ misspellings7NLTK uses a maximum entropy chunker trained at the ACE corpus: http: / / catalog.ldc.upenn.edu / LDC2005T09A makes the first statement U1, B responds with U2 and A finally responds with the last statement U3. This is similar to the previous work (Sordoni et al., 2015b). In contrast to conversations transmitted from the Internet chat (Elsner and Charniak, 2008), extracted from the dialogue, most of the film scenes are reconstructed to form a single dialogue between almost three dimensions, so that the only one word is contained."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Baselines", "text": "We test our models against modern neural networks and non-neural networks. First, we compare our models with established ngram models (Goodman, 2001). To make a comparison to a neural network base, we train an RNN on the concatenation of expressions in each threefold. Furthermore, we report on the results of the recently proposed context-sensitive model (DCGM-I) by Sordoni et al. (2015b)."}, {"heading": "5.2 Evaluation Metrics", "text": "The exact evaluation of a non-targeted dialog system is an open problem (Galley et al., 2015; Pietquin and Hastie, 2013; Schatzmann et al., 2005). There is no well-established method for automatic evaluation, and human-based evaluation is expensive. Nevertheless, word perplexity is a well-established performance metric for probabilistic language models (Bengio et al., 2003; Mikolov et al., 2010) and has already been proposed for generative dialog models (Pietquin and Hastie, 2013): exp (\u2212 1NWN; n = 1Log; P\u03b8; U n 2; U n 3)), (6) for a model with parameters, dataset with N fragments {Un1, U n 2, U n 3} N = synplexicity, and NW the number of tokens in the entire dataset."}, {"heading": "5.3 Training Procedure", "text": "This year it is more than ever before."}, {"heading": "5.4 Empirical Results", "text": "Our results are summarized in Table 2. All neural models beat state-of-the-art n-gram models w.r.t. Both word confusion and word classification errors (comparing the most likely predicted word with the actual one).Without bootstrapping, the RNN model behaves similarly to the more complex DCGM-I and HRED models, which can be explained by the size of the dataset, making it easy to surpass for the HRED and DCGM-I. The last three lines of Table 2 show that bootstrapping the model parameters from a large non-dialog corpus achieves significant gains in both measurements. SubTle bootstrapping is particularly useful as it allows a gain of almost 10 perfection points compared to the HRED model without bootstrapping. We believe this is due to the fact that it trains all model parameters from a large non-dialog corpus, as opposed to the natural bootstrapping model we get from the bootstrapping model, with no differences in bootstrapping."}, {"heading": "5.5 MAP Outputs", "text": "We evaluate the use of beam search for RNNs (Graves, 2012) to approximate the most likely (MAP) utterance U3, since the first two utterances, U1 and U2. MAP results are shown in Table 5.5 for HRED-Bidirectional bootstrapped from SubTle corpus. As shown in the table, the model often produces meaningful answers. However, most of the predictions are general, as I do not know or I am sorry9. We observed the same phenomenon for the RNN model. This appears to be a recurring observation in the literature (Sordoni et al., 2015b; Vinyals and Le, 2015) 10. However, we are the first to emphasize and discuss it in detail. There are several possible explanations for this behavior."}, {"heading": "6 Conclusion and Future work", "text": "The main contributions of this paper are the following: We have shown that a hierarchically recurring network generative model can outperform both n-gram-based models and basic neural network models in predicting the next utterance and dialog actions in a dialog. To this end, we have introduced a novel data set called MovieTriples based on movie scripts, which is suitable for modeling long, open domain dialogs close to human language. In addition to the recurring hierarchical architecture, we found two key ingredients: the use of a large external monologue corpus to initialize word embeddings, and the use of a large related but non-dialog corpus to prepare the recurring network, indicating the need for larger dialog data. Future work should focus on full dialogues, as opposed to three-dimensional dialogues, and model other dialog actions, such as entering or leaving the dialog and performing actions."}], "references": [{"title": "Luke, i am your father: dealing with out-of-domain requests by using movies subtitles", "author": ["David Ameixa", "Luisa Coheur", "Pedro Fialho", "Paulo Quaresma."], "venue": "Intelligent Virtual Agents, pages 13\u201321. Springer.", "citeRegEx": "Ameixa et al\\.,? 2014", "shortCiteRegEx": "Ameixa et al\\.", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "International Conference on Learning Representations (ICLR 2015).", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Iris: a chatoriented dialogue system based on the vector space model", "author": ["Rafael E Banchs", "Haizhou Li."], "venue": "Proceedings of the Association for Computational Linguistics (ACL 2012), System Demonstrations, pages 37\u201342. Association for Computa-", "citeRegEx": "Banchs and Li.,? 2012", "shortCiteRegEx": "Banchs and Li.", "year": 2012}, {"title": "Movie-dic: A movie dialogue corpus for research and development", "author": ["Rafael E. Banchs."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers - Volume 2, Association for Computational Linguis-", "citeRegEx": "Banchs.,? 2012", "shortCiteRegEx": "Banchs.", "year": 2012}, {"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio."], "venue": "Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "Bastien et al\\.,? 2012", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi."], "venue": "Neural Networks, IEEE Transactions on, 5(2):157\u2013166.", "citeRegEx": "Bengio et al\\.,? 1994", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin."], "venue": "Journal of Machine Learning Research, 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Practical recommendations for gradient-based training of deep architectures", "author": ["Yoshua Bengio."], "venue": "Neural Networks: Tricks of the Trade, pages 437\u2013 478. Springer.", "citeRegEx": "Bengio.,? 2012", "shortCiteRegEx": "Bengio.", "year": 2012}, {"title": "Natural Language Processing with Python", "author": ["Steven Bird", "Ewan Klein", "Edward Loper."], "venue": "O\u2019Reilly Media.", "citeRegEx": "Bird et al\\.,? 2009", "shortCiteRegEx": "Bird et al\\.", "year": 2009}, {"title": "Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, chapter On the Properties of Neural Machine Translation: Encoder", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Hierarchical recurrent neural networks for long-term dependencies", "author": ["Salah El Hihi", "Yoshua Bengio."], "venue": "Advances in Neural Information Processing Systems (NIPS), pages 493\u2013499. Citeseer.", "citeRegEx": "Hihi and Bengio.,? 1995", "shortCiteRegEx": "Hihi and Bengio.", "year": 1995}, {"title": "You talking to me? a corpus and algorithm for conversation disentanglement", "author": ["Micha Elsner", "Eugene Charniak."], "venue": "Association for Computational Linguistics (ACL 2008), pages 834\u2013842.", "citeRegEx": "Elsner and Charniak.,? 2008", "shortCiteRegEx": "Elsner and Charniak.", "year": 2008}, {"title": "Spontaneity reloaded: American face-to-face and movie conversation compared", "author": ["Pierfranca Forchini."], "venue": "Corpus Linguistics 2009. Abstracts of the 5th Corpus Linguistics Conference, page 118.", "citeRegEx": "Forchini.,? 2009", "shortCiteRegEx": "Forchini.", "year": 2009}, {"title": "Bootstrapping dialog systems with word embeddings", "author": ["Gabriel Forgues", "Joelle Pineau", "Jean-Marie Larchev\u00eaque", "R\u00e9al Tremblay."], "venue": "Workshop on Modern Machine Learning and Natural Language Processing, Advances in Neural", "citeRegEx": "Forgues et al\\.,? 2014", "shortCiteRegEx": "Forgues et al\\.", "year": 2014}, {"title": "deltableu: A discriminative metric for generation tasks with intrinsically diverse targets", "author": ["Michel Galley", "Chris Brockett", "Alessandro Sordoni", "Yangfeng Ji", "Michael Auli", "Chris Quirk", "Margaret Mitchell", "Jianfeng Gao", "Bill Dolan."], "venue": "CoRR,", "citeRegEx": "Galley et al\\.,? 2015", "shortCiteRegEx": "Galley et al\\.", "year": 2015}, {"title": "Maxout networks", "author": ["Ian J. Goodfellow", "David Warde-Farley", "Mehdi Mirza", "Aaron Courville", "Yoshua Bengio."], "venue": "Proceedings of The 30th International Conference on Machine Learning (ICML 2013), pages 1319\u20131327.", "citeRegEx": "Goodfellow et al\\.,? 2013", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "A bit of progress in language modeling extended version", "author": ["Joshua T. Goodman."], "venue": "Machine Learning and Applied Statistics Group Microsoft Research. Technical Report, MSR-TR-2001-72.", "citeRegEx": "Goodman.,? 2001", "shortCiteRegEx": "Goodman.", "year": 2001}, {"title": "Sequence transduction with recurrent neural networks", "author": ["Alex Graves."], "venue": "Proceedings of the 29th International Conference on Machine Learning (ICML 2012), Representation Learning Workshop.", "citeRegEx": "Graves.,? 2012", "shortCiteRegEx": "Graves.", "year": 2012}, {"title": "Lstm: A search space odyssey", "author": ["Klaus Greff", "Rupesh Kumar Srivastava", "Jan Koutn\u00edk", "Bas R Steunebrink", "J\u00fcrgen Schmidhuber."], "venue": "arXiv preprint arXiv:1503.04069.", "citeRegEx": "Greff et al\\.,? 2015", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "Deep neural network approach for the dialog state tracking challenge", "author": ["Matthew Henderson", "Blaise Thomson", "Steve Young."], "venue": "Proceedings of the SIGDIAL 2013 Conference, pages 467\u2013471.", "citeRegEx": "Henderson et al\\.,? 2013", "shortCiteRegEx": "Henderson et al\\.", "year": 2013}, {"title": "Word-based dialog state tracking with recurrent neural networks", "author": ["Matthew Henderson", "Blaise Thomson", "Steve Young."], "venue": "15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL 2014), page 292.", "citeRegEx": "Henderson et al\\.,? 2014", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "A clockwork rnn", "author": ["Jan Koutn\u00edk", "Klaus Greff", "Faustino Gomez", "J\u00fcrgen Schmidhuber."], "venue": "Proceedings of the International Conference on Machine Learning (ICML 2014).", "citeRegEx": "Koutn\u00edk et al\\.,? 2014", "shortCiteRegEx": "Koutn\u00edk et al\\.", "year": 2014}, {"title": "A stochastic model of human-machine interaction for learning dialog strategies", "author": ["Esther Levin", "Roberto Pieraccini", "Wieland Eckert."], "venue": "Speech and Audio Processing, IEEE Transactions on, 8(1):11\u201323.", "citeRegEx": "Levin et al\\.,? 2000", "shortCiteRegEx": "Levin et al\\.", "year": 2000}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems (NIPS), pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Developing non-goal dialog system based on examples of drama television", "author": ["Lasguido Nio", "Sakriani Sakti", "Graham Neubig", "Tomoki Toda", "Mirna Adriani", "Satoshi Nakamura."], "venue": "Natural Interaction with Robots, Knowbots and Smartphones, pages 355\u2013", "citeRegEx": "Nio et al\\.,? 2014", "shortCiteRegEx": "Nio et al\\.", "year": 2014}, {"title": "Are we there yet? research in commercial spoken dialog systems", "author": ["Roberto Pieraccini", "David Suendermann", "Krishna Dayanidhi", "Jackson Liscombe."], "venue": "12th International Conference on Text, Speech and Dialogue, pages 3\u201313. Springer.", "citeRegEx": "Pieraccini et al\\.,? 2009", "shortCiteRegEx": "Pieraccini et al\\.", "year": 2009}, {"title": "A survey on metrics for the evaluation of user simulations", "author": ["Olivier Pietquin", "Helen Hastie."], "venue": "The knowledge engineering review, 28(01):59\u201373.", "citeRegEx": "Pietquin and Hastie.,? 2013", "shortCiteRegEx": "Pietquin and Hastie.", "year": 2013}, {"title": "Unsupervised modeling of twitter conversations", "author": ["Alan Ritter", "Colin Cherry", "Bill Dolan."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT", "citeRegEx": "Ritter et al\\.,? 2010", "shortCiteRegEx": "Ritter et al\\.", "year": 2010}, {"title": "Data-driven response generation in social media", "author": ["Alan Ritter", "Colin Cherry", "William B Dolan."], "venue": "Proceedings of the Empiricial Methods in Natural Language (EMNLP 2011), pages 583\u2013593. Association for Computational Linguistics.", "citeRegEx": "Ritter et al\\.,? 2011", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "Tvd: a reproducible and multiply aligned tv series dataset", "author": ["Anindya Roy", "Camille Guinaudeau", "Herv\u00e9 Bredin", "Claude Barras."], "venue": "LREC.", "citeRegEx": "Roy et al\\.,? 2014", "shortCiteRegEx": "Roy et al\\.", "year": 2014}, {"title": "Quantitative evaluation of user simulation techniques for spoken dialogue systems", "author": ["Jost Schatzmann", "Kallirroi Georgila", "Steve Young."], "venue": "6th SIGDIAL Workshop on DISCOURSE and DIALOGUE.", "citeRegEx": "Schatzmann et al\\.,? 2005", "shortCiteRegEx": "Schatzmann et al\\.", "year": 2005}, {"title": "Neural responding machine for short-text conversation", "author": ["Lifeng Shang", "Zhengdong Lu", "Hang Li."], "venue": "Association for Computational Linguistics (ACL-IJCNLP 2015). In press.", "citeRegEx": "Shang et al\\.,? 2015", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "Chatbots: are they really useful", "author": ["Bayan Abu Shawar", "Eric Atwell"], "venue": "In LDV Forum,", "citeRegEx": "Shawar and Atwell.,? \\Q2007\\E", "shortCiteRegEx": "Shawar and Atwell.", "year": 2007}, {"title": "Optimizing dialogue management with reinforcement learning: Experiments with the njfun system", "author": ["Satinder Singh", "Diane Litman", "Michael Kearns", "Marilyn Walker."], "venue": "Journal of Artificial Intelligence Research, pages 105\u2013133.", "citeRegEx": "Singh et al\\.,? 2002", "shortCiteRegEx": "Singh et al\\.", "year": 2002}, {"title": "A hierarchical recurrent encoderdecoder for generative context-aware query suggestion", "author": ["Alessandro Sordoni", "Yoshua Bengio", "Hossein Vahabi", "Christina Lioma", "Jakob Grue Simonsen", "JianYun Nie."], "venue": "Proceedings of the 24th ACM International", "citeRegEx": "Sordoni et al\\.,? 2015a", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Meg Mitchell", "JianYun Nie", "Jianfeng Gao", "Bill Dolan."], "venue": "Conference of", "citeRegEx": "Sordoni et al\\.,? 2015b", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."], "venue": "Advances In Neural Information Processing Systems (NIPS 2014), pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "The ubuntu chat corpus for multiparticipant chat analysis", "author": ["David C Uthus", "David W Aha."], "venue": "AAAI Spring Symposium: Analyzing Microtext.", "citeRegEx": "Uthus and Aha.,? 2013", "shortCiteRegEx": "Uthus and Aha.", "year": 2013}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc Le."], "venue": "International Conference on Machine Learning (ICML 2015), Deep Learning Workshop.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "An annotated corpus of film dialogue for learning and characterizing character style", "author": ["Marilyn A Walker", "Grace I Lin", "Jennifer Sawyer."], "venue": "LREC, pages 1373\u20131378.", "citeRegEx": "Walker et al\\.,? 2012a", "shortCiteRegEx": "Walker et al\\.", "year": 2012}, {"title": "A corpus for research on deliberation and debate", "author": ["Marilyn A Walker", "Jean E Fox Tree", "Pranav Anand", "Rob Abbott", "Joseph King."], "venue": "LREC, pages 812\u2013817.", "citeRegEx": "Walker et al\\.,? 2012b", "shortCiteRegEx": "Walker et al\\.", "year": 2012}, {"title": "Pomdp-based statistical spoken dialog systems: A review", "author": ["Steve Young", "Milica Gasic", "Blaise Thomson", "Jason D Williams."], "venue": "IEEE, 101(5):1160\u2013 1179.", "citeRegEx": "Young et al\\.,? 2013", "shortCiteRegEx": "Young et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 44, "context": "Dialogue systems, also known as interactive conversational agents, virtual agents and sometimes chatterbots, are used in a wide set of applications ranging from technical support services to language learning tools and entertainment (Young et al., 2013; Shawar and Atwell, 2007).", "startOffset": 233, "endOffset": 278}, {"referenceID": 35, "context": "Dialogue systems, also known as interactive conversational agents, virtual agents and sometimes chatterbots, are used in a wide set of applications ranging from technical support services to language learning tools and entertainment (Young et al., 2013; Shawar and Atwell, 2007).", "startOffset": 233, "endOffset": 278}, {"referenceID": 44, "context": "Perhaps the most successful approach to goal driven systems has been to view the dialogue problem as a partially observable Markov decision process (POMDP) (Young et al., 2013; Pieraccini et al., 2009).", "startOffset": 156, "endOffset": 201}, {"referenceID": 28, "context": "Perhaps the most successful approach to goal driven systems has been to view the dialogue problem as a partially observable Markov decision process (POMDP) (Young et al., 2013; Pieraccini et al., 2009).", "startOffset": 156, "endOffset": 201}, {"referenceID": 19, "context": "Recent work has tried to push goal driven systems towards learning the observed features themselves with neural network models (Henderson et al., 2013; Henderson et al., 2014), yet such approaches still require large corpora of annotated task-specific simulated conversations.", "startOffset": 127, "endOffset": 175}, {"referenceID": 20, "context": "Recent work has tried to push goal driven systems towards learning the observed features themselves with neural network models (Henderson et al., 2013; Henderson et al., 2014), yet such approaches still require large corpora of annotated task-specific simulated conversations.", "startOffset": 127, "endOffset": 175}, {"referenceID": 31, "context": "On the other end of the spectrum are the nongoal driven systems (Ritter et al., 2011; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014).", "startOffset": 64, "endOffset": 145}, {"referenceID": 2, "context": "On the other end of the spectrum are the nongoal driven systems (Ritter et al., 2011; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014).", "startOffset": 64, "endOffset": 145}, {"referenceID": 0, "context": "On the other end of the spectrum are the nongoal driven systems (Ritter et al., 2011; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014).", "startOffset": 64, "endOffset": 145}, {"referenceID": 27, "context": "On the other end of the spectrum are the nongoal driven systems (Ritter et al., 2011; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014).", "startOffset": 64, "endOffset": 145}, {"referenceID": 39, "context": "(2015) have drawn inspiration from the use of neural networks in natural language modeling and machine translation tasks (Cho et al., 2014b; Sutskever et al., 2014).", "startOffset": 121, "endOffset": 164}, {"referenceID": 44, "context": "corpora which cover conversations on similar topics) then these models can be used to train a user simulator, which can then train the POMDP models discussed earlier (Young et al., 2013; Pietquin and Hastie, 2013; Levin et al., 2000).", "startOffset": 166, "endOffset": 233}, {"referenceID": 29, "context": "corpora which cover conversations on similar topics) then these models can be used to train a user simulator, which can then train the POMDP models discussed earlier (Young et al., 2013; Pietquin and Hastie, 2013; Levin et al., 2000).", "startOffset": 166, "endOffset": 233}, {"referenceID": 24, "context": "corpora which cover conversations on similar topics) then these models can be used to train a user simulator, which can then train the POMDP models discussed earlier (Young et al., 2013; Pietquin and Hastie, 2013; Levin et al., 2000).", "startOffset": 166, "endOffset": 233}, {"referenceID": 0, "context": ", 2011; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014). Most recently Sordoni et al. (2015b) and Shang et al.", "startOffset": 29, "endOffset": 106}, {"referenceID": 0, "context": ", 2011; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014). Most recently Sordoni et al. (2015b) and Shang et al. (2015) have drawn inspiration from the use of neural networks in natural language modeling and machine translation tasks (Cho et al.", "startOffset": 29, "endOffset": 130}, {"referenceID": 12, "context": "They are close to human spoken language (Forchini, 2009), which makes them suitable for bootstrapping goal-driven dialogue systems.", "startOffset": 40, "endOffset": 56}, {"referenceID": 36, "context": "In particular, we adopt the hierarchical recurrent encoder decoder (HRED) proposed by Sordoni et al. (2015a) and demonstrate that it is competitive with all other models in the literature.", "startOffset": 86, "endOffset": 109}, {"referenceID": 25, "context": "By means of such distributed representations, the recurrent neural network (RNN) based language model (Mikolov et al., 2010) has pushed state-of-the-art performance by learning long n-gram contexts while avoiding data sparsity issues.", "startOffset": 102, "endOffset": 124}, {"referenceID": 39, "context": "Overall, RNNs have performed well on a variety of NLP tasks such as machine translation (Cho et al., 2014b; Sutskever et al., 2014; Bahdanau et al., 2015) and information retrieval (Sordoni et al.", "startOffset": 88, "endOffset": 154}, {"referenceID": 1, "context": "Overall, RNNs have performed well on a variety of NLP tasks such as machine translation (Cho et al., 2014b; Sutskever et al., 2014; Bahdanau et al., 2015) and information retrieval (Sordoni et al.", "startOffset": 88, "endOffset": 154}, {"referenceID": 37, "context": ", 2015) and information retrieval (Sordoni et al., 2015a).", "startOffset": 34, "endOffset": 57}, {"referenceID": 4, "context": "Bengio et al. (2003) first proposed to tackle this problem by using a distributed (dense) vector representation of words, also called embeddings.", "startOffset": 0, "endOffset": 21}, {"referenceID": 5, "context": "However, for long sequences, this can be problematic due to either vanishing or exploding gradients (Bengio et al., 1994).", "startOffset": 100, "endOffset": 121}, {"referenceID": 21, "context": "To tackle these issues, gated variants of the transition function f have been proposed, including the LSTM unit (Hochreiter and Schmidhuber, 1997) and GRU unit (Cho et al.", "startOffset": 112, "endOffset": 146}, {"referenceID": 18, "context": "GRUs are competitive with LSTMs in performance, but are computationally cheaper (Greff et al., 2015).", "startOffset": 80, "endOffset": 100}, {"referenceID": 1, "context": "capture, even with complex transition functions such as GRUs (Cho et al., 2014a; Bahdanau et al., 2015).", "startOffset": 61, "endOffset": 103}, {"referenceID": 36, "context": "Our work extends on the hierarchical recurrent encoder decoder architecture (HRED) proposed by Sordoni et al. (2015a) for web query suggestion, which itself builds on the encoder decoder architecture proposed by Cho et al.", "startOffset": 95, "endOffset": 118}, {"referenceID": 9, "context": "(2015a) for web query suggestion, which itself builds on the encoder decoder architecture proposed by Cho et al. (2014b) and is closely related to the architectures proposed by El et al.", "startOffset": 102, "endOffset": 121}, {"referenceID": 9, "context": "(2015a) for web query suggestion, which itself builds on the encoder decoder architecture proposed by Cho et al. (2014b) and is closely related to the architectures proposed by El et al. (1995) and Koutnik et al.", "startOffset": 102, "endOffset": 194}, {"referenceID": 9, "context": "(2015a) for web query suggestion, which itself builds on the encoder decoder architecture proposed by Cho et al. (2014b) and is closely related to the architectures proposed by El et al. (1995) and Koutnik et al. (2014).", "startOffset": 102, "endOffset": 220}, {"referenceID": 15, "context": "To help generalization, it is also possible to use the maxout activation function between the hidden state and the projected word embeddings of the decoder RNN (Goodfellow et al., 2013).", "startOffset": 160, "endOffset": 185}, {"referenceID": 37, "context": "Adapted with permission from Sordoni et al. (2015a).", "startOffset": 29, "endOffset": 52}, {"referenceID": 1, "context": "We choose to model the utterance encoder with a bidirectional RNN, which proved useful to Bahdanau et al. (2015) for machine translation.", "startOffset": 90, "endOffset": 113}, {"referenceID": 39, "context": "The RNN running in reverse will effectively also introduce additional short term dependencies, which has proven useful in similar architectures (Sutskever et al., 2014).", "startOffset": 144, "endOffset": 168}, {"referenceID": 13, "context": "This has been beneficial for classification of user intents (Forgues et al., 2014).", "startOffset": 60, "endOffset": 82}, {"referenceID": 30, "context": "Modeling conversations on micro-blogging websites with generative probabilistic models was first proposed by Ritter et al. (2011). They view the response generation problem as a translation problem, where a post needs to be translated into a response.", "startOffset": 109, "endOffset": 130}, {"referenceID": 30, "context": "Modeling conversations on micro-blogging websites with generative probabilistic models was first proposed by Ritter et al. (2011). They view the response generation problem as a translation problem, where a post needs to be translated into a response. Generating responses was considerably more difficult than translating between languages, which was attributed to the wide range of plausible responses and the lack of alignment on words and phrases between the post and the response. In particular, they found that the statistical machine translation approach was superior to the information retrieval approach. In the same vein, Shang et al. (2015) proposed to use the neural network encoder-decoder framework for generating responses on the micro-blogging website Weibo.", "startOffset": 109, "endOffset": 651}, {"referenceID": 37, "context": "A way to consider the conversation context was proposed by Sordoni et al. (2015b) to generate responses for posts on Twitter.", "startOffset": 59, "endOffset": 82}, {"referenceID": 30, "context": "They then combined their generative model with a machine translation system, and showed that the hybrid system outperformed the machine translation system proposed by Ritter et al. (2011).", "startOffset": 167, "endOffset": 188}, {"referenceID": 2, "context": "To the best of our knowledge, Banchs et al. (2012) were the first to suggest using movie scripts to build dialogue systems.", "startOffset": 30, "endOffset": 51}, {"referenceID": 0, "context": "Using another information retrieval system, Ameixa et al. (2014) used movie subtitles to train a dialogue system.", "startOffset": 44, "endOffset": 65}, {"referenceID": 3, "context": "The MovieTriples dataset has been developed by expanding and preprocessing the Movie-DiC dataset by Banchs et al. (2012) to make it fit the generative dialogue modeling framework3.", "startOffset": 100, "endOffset": 121}, {"referenceID": 3, "context": "The MovieTriples dataset has been developed by expanding and preprocessing the Movie-DiC dataset by Banchs et al. (2012) to make it fit the generative dialogue modeling framework3. Based on a literature review, we found that the MovieDiC was the largest dataset available containing all consecutive utterances from movies. Other datasets in the literature include the corpora by Walker et al. (2012a), Roy et al.", "startOffset": 100, "endOffset": 401}, {"referenceID": 3, "context": "The MovieTriples dataset has been developed by expanding and preprocessing the Movie-DiC dataset by Banchs et al. (2012) to make it fit the generative dialogue modeling framework3. Based on a literature review, we found that the MovieDiC was the largest dataset available containing all consecutive utterances from movies. Other datasets in the literature include the corpora by Walker et al. (2012a), Roy et al. (2014), and the unpublished Cornell Movie Dialogue Corpus4.", "startOffset": 100, "endOffset": 420}, {"referenceID": 30, "context": "Contrary to micro-blogging websites, such as Twitter (Ritter et al., 2010), movie scripts contain long dialogues with few participants.", "startOffset": 53, "endOffset": 74}, {"referenceID": 12, "context": "human spoken conversations (Forchini, 2009).", "startOffset": 27, "endOffset": 43}, {"referenceID": 12, "context": "human spoken conversations (Forchini, 2009). As noted by Forchini (2009): \"movie language can", "startOffset": 28, "endOffset": 73}, {"referenceID": 8, "context": "We used the python-based natural language toolkit NLTK (Bird et al., 2009) to perform tokenization and named-entity recognition7 .", "startOffset": 55, "endOffset": 74}, {"referenceID": 30, "context": "Similar preprocessing has been applied in previous work (Ritter et al., 2010; Nio et al., 2014).", "startOffset": 56, "endOffset": 95}, {"referenceID": 27, "context": "Similar preprocessing has been applied in previous work (Ritter et al., 2010; Nio et al., 2014).", "startOffset": 56, "endOffset": 95}, {"referenceID": 38, "context": "This is similar to previous work (Sordoni et al., 2015b).", "startOffset": 33, "endOffset": 56}, {"referenceID": 11, "context": "Unlike conversations extracted from internet relayed chat (IRC) (Elsner and Charniak, 2008), the majority of movie scenes only contain a single dialogue thread, which means that nearly all extracted triples constitute a continuous dialogue segment between the active speakers.", "startOffset": 64, "endOffset": 91}, {"referenceID": 16, "context": "First, we compare our models to well-established ngram models (Goodman, 2001).", "startOffset": 62, "endOffset": 77}, {"referenceID": 6, "context": "established performance metric (Bengio et al., 2003; Mikolov et al., 2010), and has been suggested for generative dialogue models previously", "startOffset": 31, "endOffset": 74}, {"referenceID": 25, "context": "established performance metric (Bengio et al., 2003; Mikolov et al., 2010), and has been suggested for generative dialogue models previously", "startOffset": 31, "endOffset": 74}, {"referenceID": 29, "context": "(Pietquin and Hastie, 2013):", "startOffset": 0, "endOffset": 27}, {"referenceID": 22, "context": "To train the neural network models, we optimized the log-likelihood of the triples using the recently proposed Adam optimizer (Kingma and Ba, 2014).", "startOffset": 126, "endOffset": 147}, {"referenceID": 4, "context": "Our implementation relies on the opensource Theano library (Bastien et al., 2012).", "startOffset": 59, "endOffset": 81}, {"referenceID": 7, "context": "The best hyperparameters of the models were chosen by early stopping with patience on the validation set perplexity (Bengio, 2012).", "startOffset": 116, "endOffset": 130}, {"referenceID": 17, "context": "We evaluate the use of beam-search for RNNs (Graves, 2012) to approximate the most probable (MAP) utterance U3, given the first two utterances, U1 and U2.", "startOffset": 44, "endOffset": 58}, {"referenceID": 38, "context": "This appears to be a recurring observation in the literature (Sordoni et al., 2015b; Vinyals and Le, 2015)10.", "startOffset": 61, "endOffset": 106}, {"referenceID": 41, "context": "This appears to be a recurring observation in the literature (Sordoni et al., 2015b; Vinyals and Le, 2015)10.", "startOffset": 61, "endOffset": 106}], "year": 2015, "abstractText": "We consider the task of generative dialogue modeling for movie scripts. To this end, we extend the recently proposed hierarchical recurrent encoder decoder neural network and demonstrate that this model is competitive with state-of-the-art neural language models and backoff n-gram models. We show that its performance can be improved considerably by bootstrapping the learning from a larger questionanswer pair corpus and from pretrained word embeddings.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}