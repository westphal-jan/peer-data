{"id": "1507.05952", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jul-2015", "title": "Optimal Testing for Properties of Distributions", "abstract": "Given samples from an unknown distribution $p$, is it possible to distinguish whether $p$ belongs to some class of distributions $\\mathcal{C}$ versus $p$ being far from every distribution in $\\mathcal{C}$? This fundamental question has received tremendous attention in statistics, focusing primarily on asymptotic analysis, and more recently in information theory and theoretical computer science, where the emphasis has been on small sample size and computational complexity. Nevertheless, even for basic properties of distributions such as monotonicity, log-concavity, unimodality, independence, and monotone-hazard rate, the optimal sample complexity is unknown.", "histories": [["v1", "Tue, 21 Jul 2015 19:52:56 GMT  (986kb)", "https://arxiv.org/abs/1507.05952v1", null], ["v2", "Wed, 22 Jul 2015 14:12:33 GMT  (806kb)", "http://arxiv.org/abs/1507.05952v2", null], ["v3", "Tue, 8 Dec 2015 20:00:11 GMT  (77kb)", "http://arxiv.org/abs/1507.05952v3", "31 pages, extended abstract appeared as a spotlight in NIPS 2015"]], "reviews": [], "SUBJECTS": "cs.DS cs.IT cs.LG math.IT math.ST stat.TH", "authors": ["jayadev acharya", "constantinos daskalakis", "gautam kamath"], "accepted": true, "id": "1507.05952"}, "pdf": {"name": "1507.05952.pdf", "metadata": {"source": "CRF", "title": "Optimal Testing for Properties of Distributions", "authors": ["Jayadev Acharya", "Constantinos Daskalakis"], "emails": ["jayadev@csail.mit.edu", "costis@mit.edu", "g@csail.mit.edu"], "sections": [{"heading": null, "text": "ar Xiv: 150 7.05 952v 3 [cs.D S] 8 Dec 2We offer a general approach through which we obtain sample-optimal and computationally efficient testers for all these distribution families. At the core of our approach is an algorithm that solves the following problem: Given samples from an unknown distribution p and a known distribution q, are p and q near at a distance or far in the total variation distance? With this tool, we develop a general test framework that leads to the following results: \u2022 Checking the identity of any distribution via [n] requires that samples (n / \u03b52) be examined, which is optimal for uniform distribution."}, {"heading": "1 Introduction", "text": "The quintessential scientific question is whether an unknown object has a property, i.e. whether a model from a particular class reflects the observed behavior of the object. If the unknown object is a probability distribution to which we have sample access, we are typically asked to distinguish whether p belongs to any class C or whether it is sufficiently distant from it. This question has received enormous attention in the field of statistics (see, for example, [Fis25, LR06]), where samples for important properties such as the one we are looking at here have been proposed. Nevertheless, emphasis has been placed on asymptotic analysis, which characterizes the rates of test statistics below zero hypotheses, as the number of samples tends toward infinity. In contrast, we will examine the following problem in the small sample class:"}, {"heading": "2 Preliminaries", "text": "The total difference between the distributions p and q is defined as half of the total distribution distance. The total difference of the distribution distance is defined as half of the total distribution distance. The difference between the distribution dispositions p and q is defined as the difference between the distribution dispositions p and q over [n]. The difference between the distribution dispositions p and q is defined as the difference between the distribution dispositions p and q over [n]. The difference between the distribution dispositions p and q is defined as [n]. The difference between the two probability dispositions p and q over an ordered set (e.g. R) with cumulative density functions (CDF) Fp and Fq is defined as asdK (p, q) def = sup x."}, {"heading": "3 Overview", "text": "Our algorithm for testing a distribution p can be broken down into three steps.1This definition describes q monotonous, not increasing distributions. By symmetry, identical results apply to monotonous, non-decreasing distributions. Our first step requires a learning algorithm with very specific guarantees. In proper learning, we get sample access to a distribution p \u00b2 C, where C is any class of distributions, and we want to output q \u00b2 C so that p and q are close to each other in the total variation distance. In our setting, we want to output q in such a way that q is close to C in the total variation distance, and p and q are close to each other in the total variation2 of p. From an information theory standpoint, this problem is more difficult than proper learning, as this distance is more restrictive than the total variation distance. Nevertheless, this problem can be considered an explicit learning distance for the structured classes."}, {"heading": "4 A Robust \u03c72-\u21131 Identity Test", "text": "Our main result in the section is Theorem 2. As a direct consequence, we get the following result on testing whether an unknown distribution is close to \u03c72 or far away from a known distribution. In particular, we show the following: Theorem 1. For a known distribution q, there is an algorithm with sample complexity that follows from our main result of this section, which is expressed somewhat more generally for distribution classes. 2We also need the description of effective support for this property. This requirement can be slightly relaxed, as we are given in our results for testing unimodality.Theorem 2. Suppose. (0, 1), a class of probability distributions C, sample access to a distribution p."}, {"heading": "5 Testing Monotonicity", "text": "Let's see how we can test the monotonicity. Let's d-2 and i-2-2-3-3-3-3-3-3-3-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-"}, {"heading": "6 Testing Unimodality", "text": "A striking feature of Birge's result is that the decomposition of the domain for the samples, and thus for the unknown distribution, is unknown. However, such obscene decomposition will not work for the unimodal distribution, because the mode is unknown. Suppose we knew where the mode of the unknown distribution might be, then the problem can be broken down into monotonous functions over two intervals. Therefore, theoretically, one can modify the monotonicity test algorithm by iterating over all possible n modes. In fact, it follows by applying a compound boundary Theorem 4. (follows from Monoton) For \u03b5 > 1 / n1 / 4, an algorithm exists to test the unimodality over [n] with the sample complexity O (\u221a n \u03b52log n).However, this is unsatisfactory, since our lower boundary (and as we will show) is the true complexity of this problem n."}, {"heading": "7 Testing Independence of Random Variables", "text": "Let X def = [n1] \u00b7. \u00b7 [nd], and let us be the class of all product distributions within X. We first limit the 2-distance between product distributions in terms of each coordinate. \u00b7 Tagma 6. Let p = p1 \u00b7 p2.. \u00b7 pd, and q = q1 \u00b7 q2.. \u00b7 qd are two distributions in terms of each coordinate. \u00b7 Then\u03c72 (p, q) = d (pi) = 1 (p, q) \u2212 1.Evidence. By defining 2-distances 2 (p, q) = i Xp2i qi \u2212 1 (3) = d i (pi) 2q2 \u2212 1 (4) = d 1 (p, q2)."}, {"heading": "8 Testing Log-Concavity", "text": "In this section we describe our results for checking the log concavity of distributions. Our main result is as follows: Theorem 7. There is an algorithm for checking log concavity over [n] with sample complexity O (\u221a n\u03b52 + 1\u03b55) and time complexity poly (n] with sample complexity O (n, 1 / \u03b5). Specifically, this implies the following optimal tester for this class: Episode 4. Let us run a very specific type of learning algorithm through > 1 / n1 / 5. Then there is an algorithm for checking log concavity over [n] with sample complexity O (\u221a n / \u03b52).Our algorithm fits into the structure of our general framework. We first perform a very specific type of learning algorithm, the guarantees of which are summarized in the following problem: Lemma 8. Given the situation > 0 and sample access to a distribution variance, there is an algorithm with the following:"}, {"heading": "9 Testing for Monotone Hazard Rate", "text": "In this section, we get our main result for the Monotonous Hazard Rate Test: 3To be more precise, we need the modification of Theorem 7, which is described in Section 4, to deal with the case where the \u03c72 distance guarantees apply only to known effective support. Theorem 8. There is an algorithm for testing the Monotonous Hazard Rate above [n] with sample complexity (n / \u03b52 + log (n / \u03b5) \u03b54) and time complexity poly (n, 1 / \u03b5).This implies the following optimal tester for the class: Episode 6. Let us either set a protocol (n / \u03b5) / n1 / 4. Then there is an algorithm for testing the monotonous Hazard Rate above [n] with sample complexity O (n / \u03b52).We follow the same framework as before by first applying a Khal2 learner with the following guarantees: Lemma 9. Considering sampling > and the following method of distribution (O):"}, {"heading": "10 Lower Bounds", "text": "We show that the example examined by Pan\u03b5inski [Pan08] to prove lower limits in testing uniformity can be used to prove lower limits for the classes we consider. They view a class Q consisting of 2n / 2 distributions as follows. Without loss of universality, they assume that n is even. For each of the 2n / 2 vectors z0z1.... zn / 2 \u2212 1 for i = 2o. (6) Each distribution in Q has a total variation distance c\u03b5 / 2 from Un, requiring equal distribution q \u00b2 Q over [n] as the following reasons. qi = (1 + eg) n for i = 2o + 1 (1 \u2212 eg) n for i = 2o."}, {"heading": "Acknowledgements", "text": "The authors thank Cle'ment Canonne and Jerry Li; the former for some useful notes and suggestions on earlier drafts of this work, and both for helpful discussions and thoughts on the Independence Test."}, {"heading": "A Moments of the Chi-Squared Statistic", "text": "We start with the mean: E [Z] = E [Xi \u2212 mqi) 2 \u2212 Xi mqi] 2 \u2212 Xi mqi] 2 \u2212 mpi mqi [X2i] \u2212 2mqiE [Xi] + m2q2i \u2212 E [Xi] mqi = [X2p2i + mpi \u2212 2m2qipi + m2q2i \u2212 mpi mqi = m [X2i \u2212 qi) 2 qi = m \u00b7 \u03c72 (pA, qA) Next, we analyze the variance. Let us allow: E [Xi] = mpi \u2212 2m2qipi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi = mpi and mqipi \u2212 mpi \u2212 mpi \u2212 mpi = mi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi = mi \u2212 mpi = mi \u2212 mpi \u2212 mpi = m2qi \u2212 mpi \u2212 mpi \u2212 mpi = mi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpqi = \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi = \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi = mi \u2212 mpi \u2212 mpi \u2212 mpi = mi \u2212 mpi \u2212 mpi = mi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi = mpi = mi = mi \u2212 mpi = mi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi = mpi \u2212 mpi \u2212 mpi = mpi \u2212 mpi = mpi = mpi = mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi \u2212 mpi = mi = mpi = mi = mpi = mi = mi"}, {"heading": "B Analysis of our \u03c72-Test Statistic", "text": "In this case, we first prove the key lemmas in the analysis of our \u03c72 test. Evidence of Lemma 2: first case is simply from (1) and property 2 of q.We turn to the latter case. Let us remember that A = \u2264 Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-"}, {"heading": "C Details on Testing Monotonicity", "text": "In this section we show the lemmas necessary for our monotonicity tests. (C1) A structural result for monotonous distributions on the HypergridBirge \"(Bir87) shows that each monotonous distribution results in total variation with an O (log (n) / \u03b5) constant distribution. Furthermore, the intervals over which the results are constant are independent of the distribution p. This result has been extended to the Kullback-Leibler divergence of [AJOS14] to study the compression of monotonant distributions, which have the upper limit of the KL divergence of 2 distances and are then bound to the respective distance. We extend these results to [n] d in bd rectangles as follows. Let {I1,.., Ib} be a division of the [n] into consecutive intervals."}, {"heading": "D Details on testing Unimodality", "text": "Remember that the first step in our algorithm is to estimate the total probability within each of these intervals. In any case, we want to resolve the interval into disjunction intervals, so that the probability of each interval is about O (1 / b), where b is a parameter specified later. In particular, we consider a decomposition of [n] with the following properties: 1. For each element i with a probability of at least 1 / b there is an interval of [n]. There are at most two intervals with p (I) \u2264 1 / 2b.3. Any other interval I meet p (I). [1 2b, 2 b].Let I1,. IL denounces the partition of [n] that corresponds to these intervals. L = O (b).Claim 1. There is an algorithm that takes O (b), samples and results I1,."}, {"heading": "E Learning product distributions in \u03c72 distance", "text": "In this section, we prove lexicon 7. The proof is analogous to the proof for learning monotonous distributions and depends on the following result of [KOPS15]. For m samples from a distribution q over n elements, the Add-1 estimator (Laplace estimator) fulfills q: E [\u03c72 (p, q)] \u2264 n m + 1. Now, we assume that p is a product distribution over X = [n1] \u00b7 \u00b7 \u00b7 \u00b7 [nd]. We simply perform the Add-1 estimation over each coordinate independently and give a distribution q1 \u00b7 \u00b7 qd. Since p is a product distribution, the estimates in each coordinate are independent. Therefore, a simple application of the previous result and the independence of the coordinates implies E [v2 (p, q)] = d-l = 1 (1 + E [v2 (pl, ql)] \u2212 1 \u2264 d-l = 1 (1 + vlm + 1) ex-l (ex-l), ex-l (v2), ex-l (v1-l (v2) v2, v2 (v2), v2 (v2) (v2, v2) (v2) (v2, v2, v2 (v2) (v2)."}, {"heading": "F Details on Testing Log-Concavity", "text": "(1 / 3 / 2) We assume that this distribution gap will only be closed if the distribution gap is closed. (1 / 3 / 2) We assume that if the distribution gap is closed, the distribution gap will be closed. (2 / 3 / 3) We assume that the distribution gap will be closed. (2 / 3) We assume that the distribution gap is small, and otherwise the distribution gap will be closed. (2 / 3 / 3) The distribution gap will either be closed. (2 / 4 / 4) or closed. We first close the distribution gap. (1 / 5) We assume that the distribution gap is closed. (1 / 5 / 5) We assume that the distribution gap is closed. (1 / 5 / 5) We assume that the distribution gap will be closed."}, {"heading": "G Details on MHR testing", "text": "The proof of Lemma 9: As with log concave distributions, MHR distributions cannot be divided into two parts. In the first step, we will achieve this by finding a multiplicative (1 + O) distribution for each element within these intervals. This step is very similar to the decomposition used for unimodal distributions (described in Section D), so we will outline the argument and highlight the key differences. If p-MHRn, there should always be a feasible point indicating that q is near a distribution in MHRn (leveraging)."}, {"heading": "H Details of the Lower Bounds", "text": "In this section we show that dTV (C, Q) \u2265 \u03b5, i.e. implies a lower distribution limit (\u221a n / \u03b52) for testing C.H.1 Monotonous distributions We first consider d = 1 and prove that for appropriately chosen c, any monotonous distribution over [n] implies such a distribution far from all distributions in Q. Let us consider each q distribution as an increase point if qi < qi + 1. Let Rq be the number of increase points of q. For q-Q, (6) implies at least one of four consecutive integrators in Q. For this distribution it is an increase point, and therefore Rq-4. Furthermore, let us note that if i is an increase point, then i + 1 is not an increase point. For each monotonous (decreasing) distribution p, pi-1."}], "references": [{"title": "Testing k-wise and almost k-wise independence", "author": ["Noga Alon", "Alexandr Andoni", "Tali Kaufman", "Kevin Matulef", "Ronitt Rubinfeld", "Ning Xie"], "venue": "In Proceedings of STOC,", "citeRegEx": "Alon et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Alon et al\\.", "year": 2007}, {"title": "Testing monotone continuous distributions on high-dimensional real cubes", "author": ["Michal Adamaszek", "Artur Czumaj", "Christian Sohler"], "venue": "In SODA,", "citeRegEx": "Adamaszek et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Adamaszek et al\\.", "year": 2010}, {"title": "Testing Poisson Binomial Distributions", "author": ["Jayadev Acharya", "Constantinos Daskalakis"], "venue": "In Proceedings of SODA,", "citeRegEx": "Acharya and Daskalakis.,? \\Q2015\\E", "shortCiteRegEx": "Acharya and Daskalakis.", "year": 2015}, {"title": "Competitive classification and closeness testing", "author": ["Jayadev Acharya", "Hirakendu Das", "Ashkan Jafarpour", "Alon Orlitsky", "Shengjun Pan", "Ananda Theertha Suresh"], "venue": "In COLT,", "citeRegEx": "Acharya et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Acharya et al\\.", "year": 2012}, {"title": "Sample-optimal density estimation in nearly-linear time", "author": ["Jayadev Acharya", "Ilias Diakonikolas", "Jerry Li", "Ludwig Schmidt"], "venue": "arXiv preprint arXiv:1506.00671,", "citeRegEx": "Acharya et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Acharya et al\\.", "year": 2015}, {"title": "Efficient compression of monotone and m-modal distributions", "author": ["Jayadev Acharya", "Ashkan Jafarpour", "Alon Orlitsky", "Ananda Theertha Suresh"], "venue": "In Proceedings of the 2014 IEEE International Symposium on Information Theory, ISIT", "citeRegEx": "Acharya et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Acharya et al\\.", "year": 2014}, {"title": "A competitive test for uniformity of monotone distributions", "author": ["Jayadev Acharya", "Ashkan Jafarpour", "Alon Orlitsky", "Ananda Theertha Suresh"], "venue": "In Proceedings of AISTATS,", "citeRegEx": "Acharya et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Acharya et al\\.", "year": 2013}, {"title": "Categorical data analysis", "author": ["Alan Agresti", "Maria Kateri"], "venue": null, "citeRegEx": "Agresti and Kateri.,? \\Q2011\\E", "shortCiteRegEx": "Agresti and Kateri.", "year": 2011}, {"title": "Statistical Inference under Order Restrictions", "author": ["R.E. Barlow", "D.J. Bartholomew", "J.M. Bremner", "H.D. Brunk"], "venue": null, "citeRegEx": "Barlow et al\\.,? \\Q1972\\E", "shortCiteRegEx": "Barlow et al\\.", "year": 1972}, {"title": "Testing random variables for independence and identity", "author": ["Tugkan Batu", "Eldar Fischer", "Lance Fortnow", "Ravi Kumar", "Ronitt Rubinfeld", "Patrick White"], "venue": "In Proceedings of FOCS,", "citeRegEx": "Batu et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Batu et al\\.", "year": 2001}, {"title": "Testing monotonicity of distributions over general partial orders", "author": ["Arnab Bhattacharyya", "Eldar Fischer", "Ronitt Rubinfeld", "Paul Valiant"], "venue": "In ICS,", "citeRegEx": "Bhattacharyya et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bhattacharyya et al\\.", "year": 2011}, {"title": "Estimating a density under order restrictions: Nonasymptotic minimax risk", "author": ["Lucien Birg\u00e9"], "venue": "The Annals of Statistics,", "citeRegEx": "Birg\u00e9.,? \\Q1987\\E", "shortCiteRegEx": "Birg\u00e9.", "year": 1987}, {"title": "Maximum likelihood estimation and confidence bands for a discrete log-concave", "author": ["Fadoua Balabdaoui", "Hanna Jankowski", "Kaspar Rufibach"], "venue": null, "citeRegEx": "Balabdaoui et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Balabdaoui et al\\.", "year": 2011}, {"title": "Sublinear algorithms for testing monotone and unimodal distributions", "author": ["Tu\u011fkan Batu", "Ravi Kumar", "Ronitt Rubinfeld"], "venue": "In Proceedings of the 36th Annual ACM Symposium on the Theory of Computing,", "citeRegEx": "Batu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Batu et al\\.", "year": 2004}, {"title": "Estimation of a k-monotone density: characterizations, consistency and minimax lower bounds", "author": ["Fadoua Balabdaoui", "Jon A. Wellner"], "venue": "Statistica Neerlandica,", "citeRegEx": "Balabdaoui and Wellner.,? \\Q2010\\E", "shortCiteRegEx": "Balabdaoui and Wellner.", "year": 2010}, {"title": "A survey on distribution testing: your data is big, but is it blue", "author": ["Cl\u00e9ment L Canonne"], "venue": "In Electronic Colloquium on Computational Complexity (ECCC),", "citeRegEx": "Canonne.,? \\Q2015\\E", "shortCiteRegEx": "Canonne.", "year": 2015}, {"title": "Testing shape restrictions of discrete distributions", "author": ["Clement Canonne", "Ilias Diakonikolas", "Themis Gouleakis", "Ronitt Rubinfeld"], "venue": "In STACS,", "citeRegEx": "Canonne et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Canonne et al\\.", "year": 2015}, {"title": "Learning mixtures of structured distributions over discrete domains", "author": ["Siu On Chan", "Ilias Diakonikolas", "Rocco A. Servedio", "Xiaorui Sun"], "venue": "In Proceedings of the 24th Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Chan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2013}, {"title": "Efficient density estimation via piecewise polynomial approximation", "author": ["Siu On Chan", "Ilias Diakonikolas", "Rocco A. Servedio", "Xiaorui Sun"], "venue": "In Proceedings of the 46th Annual ACM Symposium on the Theory of Computing,", "citeRegEx": "Chan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2014}, {"title": "Optimal algorithms for testing closeness of discrete distributions", "author": ["Siu-On Chan", "Ilias Diakonikolas", "Gregory Valiant", "Paul Valiant"], "venue": "In Proceedings of SODA,", "citeRegEx": "Chan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2014}, {"title": "Theoretical properties of the log-concave maximum likelihood estimator of a multidimensional density", "author": ["Madeleine Cule", "Richard Samworth"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "Cule and Samworth.,? \\Q2010\\E", "shortCiteRegEx": "Cule and Samworth.", "year": 2010}, {"title": "Asymptotic minimax character of the sample distribution function and of the classical multinomial estimator", "author": ["A. Dvoretzky", "J. Kiefer", "J. Wolfowitz"], "venue": "The Annals of Mathematical Statistics, 27(3):642\u2013669,", "citeRegEx": "Dvoretzky et al\\.,? \\Q1956\\E", "shortCiteRegEx": "Dvoretzky et al\\.", "year": 1956}, {"title": "Statistical Methods for Research Workers", "author": ["Ronald Aylmer Fisher"], "venue": "Oliver and Boyd, Edinburgh,", "citeRegEx": "Fisher.,? \\Q1925\\E", "shortCiteRegEx": "Fisher.", "year": 1925}, {"title": "The art of uninformed decisions: A primer to property testing", "author": ["Eldar Fischer"], "venue": "Science, 75:97\u2013126,", "citeRegEx": "Fischer.,? \\Q2001\\E", "shortCiteRegEx": "Fischer.", "year": 2001}, {"title": "Combinatorial property testing (a survey)", "author": ["Oded Goldreich"], "venue": "American Mathematical Society,", "citeRegEx": "Goldreich.,? \\Q1998\\E", "shortCiteRegEx": "Goldreich.", "year": 1998}, {"title": "On choosing and bounding probability metrics", "author": ["Alison L. Gibbs", "Francis E. Su"], "venue": "International Statistical Review,", "citeRegEx": "Gibbs and Su.,? \\Q2002\\E", "shortCiteRegEx": "Gibbs and Su.", "year": 2002}, {"title": "Testing for monotone increasing hazard rate", "author": ["Peter Hall", "Ingrid Van Keilegom"], "venue": "Annals of Statistics,", "citeRegEx": "Hall and Keilegom.,? \\Q2005\\E", "shortCiteRegEx": "Hall and Keilegom.", "year": 2005}, {"title": "Estimation of a discrete monotone density", "author": ["Hanna K. Jankowski", "Jon A. Wellner"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "Jankowski and Wellner.,? \\Q2009\\E", "shortCiteRegEx": "Jankowski and Wellner.", "year": 2009}, {"title": "On learning distributions from their samples", "author": ["Sudeep Kamath", "Alon Orlitsky", "Dheeraj Pichapati", "Ananda T. Suresh"], "venue": "In COLT,", "citeRegEx": "Kamath et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kamath et al\\.", "year": 2015}, {"title": "Testing statistical hypotheses", "author": ["Erich L Lehmann", "Joseph P Romano"], "venue": "Springer Science & Business Media,", "citeRegEx": "Lehmann and Romano.,? \\Q2006\\E", "shortCiteRegEx": "Lehmann and Romano.", "year": 2006}, {"title": "Testing properties of collections of distributions", "author": ["Reut Levi", "Dana Ron", "Ronitt Rubinfeld"], "venue": "Theory of Computing,", "citeRegEx": "Levi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Levi et al\\.", "year": 2013}, {"title": "The tight constant in the Dvoretzky-Kiefer-Wolfowitz inequality", "author": ["Pascal Massart"], "venue": "The Annals of Probability, 18(3):1269\u20131283,", "citeRegEx": "Massart.,? \\Q1990\\E", "shortCiteRegEx": "Massart.", "year": 1990}, {"title": "A coincidence-based test for uniformity given very sparsely sampled discrete data", "author": ["Liam Paninski"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Paninski.,? \\Q2008\\E", "shortCiteRegEx": "Paninski.", "year": 2008}, {"title": "Property testing: A learning theory perspective", "author": ["Dana Ron"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Ron.,? \\Q2008\\E", "shortCiteRegEx": "Ron.", "year": 2008}, {"title": "The analysis of categorical data from complex sample surveys: chi-squared tests for goodness of fit and independence in two-way tables", "author": ["Jon NK Rao", "Alastair J Scott"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Rao and Scott.,? \\Q1981\\E", "shortCiteRegEx": "Rao and Scott.", "year": 1981}, {"title": "Log-concavity and strong log-concavity: a review", "author": ["Adrien Saumard", "Jon AWellner"], "venue": "Statistics Surveys,", "citeRegEx": "Saumard and AWellner.,? \\Q2014\\E", "shortCiteRegEx": "Saumard and AWellner.", "year": 2014}, {"title": "Estimating the unseen: An n/ log n-sample estimator for entropy and support size, shown optimal via new CLTs", "author": ["Gregory Valiant", "Paul Valiant"], "venue": "In Proceedings of the 43rd Annual ACM Symposium on the Theory of Computing,", "citeRegEx": "Valiant and Valiant.,? \\Q2011\\E", "shortCiteRegEx": "Valiant and Valiant.", "year": 2011}, {"title": "An automatic inequality prover and instance optimal identity testing", "author": ["Gregory Valiant", "Paul Valiant"], "venue": "In FOCS,", "citeRegEx": "Valiant and Valiant.,? \\Q2014\\E", "shortCiteRegEx": "Valiant and Valiant.", "year": 2014}, {"title": "A, the last equality uses (1), and the final inequality substitutes a value m", "author": [], "venue": null, "citeRegEx": "\u2208,? \\Q2000\\E", "shortCiteRegEx": "\u2208", "year": 2000}, {"title": "When dTV(p, C) \u2265 \u03b5, Lemma 2 and m", "author": ["m\u03b5"], "venue": null, "citeRegEx": "m\u03b5.,? \\Q2000\\E", "shortCiteRegEx": "m\u03b5.", "year": 2000}], "referenceMentions": [], "year": 2015, "abstractText": "Given samples from an unknown distribution p, is it possible to distinguish whether p belongs to some class of distributions C versus p being far from every distribution in C? This fundamental question has received tremendous attention in statistics, focusing primarily on asymptotic analysis, and more recently in information theory and theoretical computer science, where the emphasis has been on small sample size and computational complexity. Nevertheless, even for basic properties of distributions such as monotonicity, log-concavity, unimodality, independence, and monotone-hazard rate, the optimal sample complexity is unknown. We provide a general approach via which we obtain sample-optimal and computationally efficient testers for all these distribution families. At the core of our approach is an algorithm which solves the following problem: Given samples from an unknown distribution p, and a known distribution q, are p and q close in \u03c7-distance, or far in total variation distance? With this tool in place, we develop a general testing framework which leads to the following results: \u2022 Testing identity to any distribution over [n] requires \u0398(\u221an/\u03b52) samples. This is optimal for the uniform distribution. This gives an alternate argument for the minimax sample complexity of testing identity (proved in [VV14]). \u2022 For all d \u2265 1 and n sufficiently large, testing whether a discrete distribution over [n] is monotone requires an optimal \u0398(n/\u03b5) samples. The single-dimensional version of our theorem improves a long line of research starting with [BKR04], where the previous best tester required \u03a9( \u221a n log(n)/\u03b5) samples, while the high-dimensional version improves [BFRV11], which requires \u03a9\u0303(n 1 2poly(1\u03b5 )) samples. \u2022 For all d \u2265 1, testing whether a collection of random variables over [n1]\u00d7\u00b7 \u00b7 \u00b7\u00d7[nd] are independent requiresO ((", "creator": "LaTeX with hyperref package"}}}