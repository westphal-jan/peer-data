{"id": "1507.06228", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jul-2015", "title": "Training Very Deep Networks", "abstract": "Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures.", "histories": [["v1", "Wed, 22 Jul 2015 15:29:14 GMT  (444kb,D)", "http://arxiv.org/abs/1507.06228v1", "9 pages, 4 figures. ExtendsarXiv:1505.00387. Project webpage is atthis http URL"], ["v2", "Mon, 23 Nov 2015 16:25:30 GMT  (638kb,D)", "http://arxiv.org/abs/1507.06228v2", "11 pages. ExtendsarXiv:1505.00387. Project webpage is atthis http URLin Advances in Neural Information Processing Systems 2015"]], "COMMENTS": "9 pages, 4 figures. ExtendsarXiv:1505.00387. Project webpage is atthis http URL", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["rupesh kumar srivastava", "klaus greff", "j\u00fcrgen schmidhuber"], "accepted": true, "id": "1507.06228"}, "pdf": {"name": "1507.06228.pdf", "metadata": {"source": "CRF", "title": "Training Very Deep Networks", "authors": ["Rupesh Kumar Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber"], "emails": ["juergen}@idsia.ch"], "sections": [{"heading": null, "text": "Theoretical and empirical evidence suggests that the depth of neural networks is critical to their success. However, education becomes more difficult as depth increases, and the formation of very deep networks remains an open problem. Here, we present a new architecture for overcoming this problem. Our so-called highway networks allow unhindered flow of information across many layers on data highways, inspired by recursive long-term memory networks, and use adaptive gating units to regulate the flow of information. Even with hundreds of layers, highway networks can be trained directly by simple gradient descents, enabling the study of extremely deep and efficient architectures."}, {"heading": "1 Introduction & Previous Work", "text": "This year, it has come to the point where there is only one person who is able to retaliate."}, {"heading": "2 Highway Networks", "text": "Notation We use bold letters for vectors and matrices and italic uppercase letters to denote transformation functions. 0 and 1 denote vectors of zeros and ones, and I denote an identity matrix. \u03c3 (x) is defined as \u03c3 (x) = 11 + e \u2212 x, x x R. The dot operator (\u00b7) is used to denote elementary multiplication. A simple forward-facing neural network typically consists of L layers in which the lth layer (l,..., L}) applies a nonlinear transformation (from WH, l) to its output xl. Thus, x1 is input to the network and yL is the output of the network. Waiving the layer index and the clarity bias, y = H (x, WH) x."}, {"heading": "2.1 Constructing Highway Networks", "text": "As already mentioned, Equation 3 requires that the dimensionality of x, y, H (x, WH) and T (x, WT) must be the same. To change the size of the intermediate representation, you can replace x with x, obtained by suitable sub-sampling or zero-padding x. Another alternative is to use a plane layer (without highways) to change the dimensionality, which is the strategy we are using in this study. Convolutionary highway layers use weight distribution and local receptive fields for both H and T transformations. We have used equally large receptive fields and zero padding for both to ensure that the block state and the gate feature maps match the input size."}, {"heading": "2.2 Training Deep Highway Networks", "text": "We use the transformation gate, which is defined as T (x) = \u03c3 (WTTx + bT), where WT is the weight matrix and bT is the bias vector for the transformation gates. This suggests a simple initialization scheme that can be initialized with a negative value regardless of the nature of H: bT (e.g. -1, -3, etc.) so that the network initially tends to carry behavior. This scheme is strongly inspired by the proposal [28] to distort the gates initially in an LSTM network in order to bridge long-term time dependencies in the early learning process. Note that \u03c3 (x) - (0, 1) - x - R, i.e. the conditions in Equation 4 can never be exactly fulfilled. In our experiments, we found that a negative bias initialization for the transformation gates was sufficient for training to proceed in very deep networks for various zero-mean networks."}, {"heading": "3 Experiments", "text": "For the rest of the experiments, a simpler, commonly used strategy was used, where the learning rate starts at a value \u03bb and decreases by a factor \u03b3 according to a fixed timetable.The value of \u03bb, \u03b3 and the timetable were selected on the basis of the validation results specified in the CIFAR-10 dataset and kept stable for all experiments.All Convolutionary Motorway Networks use the rectified linear activation function [16] to calculate block stateH. To provide a better estimate of the variability of the classification results due to random initialization, we report our results in the Best format (mean \u00b1 std.dev.) on the basis of 5 runs wherever available.Some experiments were conducted with the Caffe library [31].Source code and network configurations used in this essay will be publicly available.3"}, {"heading": "3.1 Optimization", "text": "To support the hypothesis that motorway networks do not suffer from increasing depth, we conducted a series of rigorous optimization experiments and compared them with simple networks with normalized initialization [16, 17]. Although there are other initialization methods, this is the only one we know that has been used recently to achieve good performance on real data using deep networks with up to 19 levels [17]. We trained both networks at the level and on highways with varying depths on the MNIST digital classification dataset. In all networks, the first layer is a fully interconnected level, followed by 9, 19, 49 or 99 fully interconnected levels or motorways. Finally, the network output is generated by a Softmax layer. All networks are thin: each layer has 50 blocks for motorway networks and 71 units for simple networks, giving approximately identical parameter numbers per layer."}, {"heading": "3.2 Pilot Experiments on MNIST Digit Classification", "text": "As a reliability test for the generalization capability of motorway networks, we trained 10-layer Convolutionary Motorway Networks on MNIST using two architectures, each with 9 revolutionary layers followed by a Softmax output. The number of filter cards (width) was set at 16 and 32 for all layers."}, {"heading": "3.3 Experiments on CIFAR-10 and CIFAR-100 Object Recognition", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.3.1 Comparison to Fitnets", "text": "Fitnet training Maxout networks can handle greater depth much better than those with traditional activation functions [18]. However, Romero et. al. [23] recently reported that training on CIFAR-10 through simple backpropogation was only possible for maxout networks with a depth of up to 5 layers if the number of parameters was limited to \u0445 250K and the number of multiplications to \u0445 30M. Similar limitations were observed for higher computing budgets. Training of deeper networks was only possible through the use of a two-step training procedure and the addition of soft targets produced from a pre-trained flat teacher network (indicative training). We found that it was easy to train motorway networks with a number of parameters and operations comparable to those of fitnets in a single step using SGD."}, {"heading": "3.3.2 Comparison to State-of-the-art Methods", "text": "This approach was popularized by Ciresan et. al. [5] and recently expanded by Graham [32]. Since our goal is only to show that deeper networks can be trained without sacrificing the ease of training or generalizability, we only conducted experiments in the more frequent environment of global contrast normalization, small translations, and image mirroring. Following Lin et. al. [33], we replaced the fully connected layer used in the networks in the previous section with a more convective layer with a size one receptive field and a global average pool layer. The hyperparameters from the last section were reused for both CIFAR-10 and CIFAR-100, so it is quite possible to achieve much better results with better architectures / hyperparameters."}, {"heading": "4 Analysis", "text": "The first three columns show bias, mean activity across all training samples, and activity for a single random sample for each transformation gate. Block output for the same single sample is shown in the last column. Transformation location distortions of the two networks have been initialized to -2 and -4, respectively. Interestingly, contrary to our expectations, most distortions have continued to decrease during training. In the CIFAR-100 network, distortions increase with the depth that forms a gradient. Strangely, this gradient correlates with the average activity of the transformation gates as seen in the second column, suggesting that the strong negative distortions at low depths are not used to close the gates, but to make them more selective. This behavior is also suggested by the fact that transformation activity for a single example (3 columns) has the most impact on the NAR layers, but most NAR layers are less pronounced."}, {"heading": "4.1 Routing of Information", "text": "One possible advantage of the highway architecture over hard-wired links is that the network can learn to dynamically adjust the routing of information based on the current input, which raises the question: does this behavior manifest itself in trained networks or do they merely learn a static routing that applies to all inputs equally? A partial answer can be found by looking at the mean transformation activity of the gate (second column) and the single example that transforms the gate outputs (third, obtained by randomly searching through hyperparameters to minimize the best training error achieved by each configuration column) in Figure 2. In particular, in the case of CIFAR-100, most transformation gates are active on average, while for the single example they exhibit a very selective activity. This means that for each sample only a few blocks perform a transformation, but different blocks of different samplers are used. This data-dependent routing mechanism is shown further in each NAR that the differences between the two columns are evident in each of the 3."}, {"heading": "4.2 Layer Importance", "text": "Since we all tend to close transformation gates, each layer copies only the activations of the previous layer first. Does this behavior actually change with the training, or is the final network still essentially equivalent to a network with many fewer layers? To shed light on this problem, we investigated the extent to which the dilution of a single layer affects the overall performance of trained networks from Section 3.1. By watering down, we mean manually setting all transformation gates of a layer to 0, forcing them to simply copy their inputs. For each layer, we rated the network in the full training set, with the gates of that layer closed. The resulting performance as a function of the diluted layer is shown in Figure 4.For MNIST (left), the error increases significantly when one of the early layers is removed, but layers 15 \u2212 45 seem to have virtually no effect on the final performance of that layer. Approximately 60% of the layers do not appear to contribute to the simple behavior, because the result is not likely to contribute to the IST."}, {"heading": "5 Discussion", "text": "Alternative approaches to counteracting the difficulties mentioned in Section 1 with depth often have several limitations. Learning to route information through neural networks using competing interactions has helped to expand their application to demanding problems by improving lending [36], but they still suffer when depth exceeds \u2248 20 even with careful initialization [17]. Effective initialization methods can be difficult to derive for a variety of activation functions due to their specific architecture. Depth monitoring [22] has been shown to impair the performance of deep narrow networks [23]. Very deep highway networks, on the other hand, can be trained directly with simple downward methods due to their specific architecture. \"This property is not based on specific nonlinear transformations, which can be complex revolutionary or recurrent transformations, and deriving a suitable initialization scheme is not essential. The additional parameters required by the gating mechanism help to respond differently to the multiplication of information through the use of skip."}, {"heading": "Acknowledgments", "text": "We would like to thank NVIDIA Corporation for donating GPUs and Sepp Hochreiter and Thomas Unterthiner for their helpful comments and Jan Koutn\u0131k for their help in conducting experiments."}], "references": [{"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "[cs],", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "[cs],", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Flexible, high performance convolutional neural networks for image classification", "author": ["DC Ciresan", "Ueli Meier", "Jonathan Masci", "Luca M Gambardella", "J\u00fcrgen Schmidhuber"], "venue": "In International Joint Conference on Artificial Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Multi-column deep neural networks for image classification", "author": ["Dan Ciresan", "Ueli Meier", "J\u00fcrgen Schmidhuber"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Feature learning in deep neural networks-studies on speech recognition", "author": ["Dong Yu", "Michael L. Seltzer", "Jinyu Li", "Jui-Ting Huang", "Frank Seide"], "venue": "tasks. arXiv preprint arXiv:1301.3605,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Bridging long time lags by weight guessing and \u201clong shortterm memory", "author": ["Sepp Hochreiter", "Jurgen Schmidhuber"], "venue": "Spatiotemporal models in biological and artificial systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1996}, {"title": "Computational limitations of small-depth circuits", "author": ["Johan H\u00e5stad"], "venue": "MIT press,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1987}, {"title": "On the power of small-depth threshold circuits", "author": ["Johan H\u00e5stad", "Mikael Goldmann"], "venue": "Computational Complexity,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1991}, {"title": "On the complexity of neural network classifiers: A comparison between shallow and deep architectures", "author": ["Monica Bianchini", "Franco Scarselli"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "On the number of linear regions of deep neural networks", "author": ["Guido F Montufar", "Razvan Pascanu", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "On the expressive efficiency of sum product networks", "author": ["James Martens", "Venkatesh Medabalimi"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Training deep and recurrent networks with hessian-free optimization", "author": ["James Martens", "Ilya Sutskever"], "venue": "Neural Networks: Tricks of the Trade,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George Dahl", "Geoffrey Hinton"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Yann N Dauphin", "Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Surya Ganguli", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "[cs],", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Compete to compute", "author": ["Rupesh K. Srivastava", "Jonathan Masci", "Sohrob Kazerounian", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Deep learning made easier by linear transformations in perceptrons", "author": ["Tapani Raiko", "Harri Valpola", "Yann LeCun"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "FitNets: Hints for thin deep nets", "author": ["Adriana Romero", "Nicolas Ballas", "Samira Ebrahimi Kahou", "Antoine Chassang", "Carlo Gatta", "Yoshua Bengio"], "venue": "[cs],", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Learning complex, extended sequences using the principle of history compression", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1992}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E. Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural computation,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2006}, {"title": "Untersuchungen zu dynamischen neuronalen Netzen", "author": ["Sepp Hochreiter"], "venue": "Masters thesis, Technische Universita\u0308t Mu\u0308nchen, Mu\u0308nchen,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1991}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1997}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["Felix A. Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins"], "venue": "In ICANN,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1999}, {"title": "Grid long Short-Term memory", "author": ["Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves"], "venue": "[cs],", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "[cs],", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Spatially-sparse convolutional neural networks", "author": ["Benjamin Graham"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "Deep networks with internal selective attention through feedback connections", "author": ["Marijn F Stollenga", "Jonathan Masci", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "In NIPS", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "Striving for simplicity: The all convolutional net", "author": ["Jost Tobias Springenberg", "Alexey Dosovitskiy", "Thomas Brox", "Martin Riedmiller"], "venue": "[cs],", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Understanding locally competitive networks", "author": ["Rupesh Kumar Srivastava", "Jonathan Masci", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "For instance, within just a few years, the top-5 image classification accuracy on the 1000-class ImageNet dataset has increased from \u223c84% [1] to \u223c95% [2, 3] using deeper networks with rather small receptive fields [4, 5].", "startOffset": 138, "endOffset": 141}, {"referenceID": 1, "context": "For instance, within just a few years, the top-5 image classification accuracy on the 1000-class ImageNet dataset has increased from \u223c84% [1] to \u223c95% [2, 3] using deeper networks with rather small receptive fields [4, 5].", "startOffset": 150, "endOffset": 156}, {"referenceID": 2, "context": "For instance, within just a few years, the top-5 image classification accuracy on the 1000-class ImageNet dataset has increased from \u223c84% [1] to \u223c95% [2, 3] using deeper networks with rather small receptive fields [4, 5].", "startOffset": 150, "endOffset": 156}, {"referenceID": 3, "context": "For instance, within just a few years, the top-5 image classification accuracy on the 1000-class ImageNet dataset has increased from \u223c84% [1] to \u223c95% [2, 3] using deeper networks with rather small receptive fields [4, 5].", "startOffset": 214, "endOffset": 220}, {"referenceID": 4, "context": "For instance, within just a few years, the top-5 image classification accuracy on the 1000-class ImageNet dataset has increased from \u223c84% [1] to \u223c95% [2, 3] using deeper networks with rather small receptive fields [4, 5].", "startOffset": 214, "endOffset": 220}, {"referenceID": 5, "context": "Other results on practical machine learning problems have also underscored the superiority of deeper networks [6] in terms of accuracy and/or performance.", "startOffset": 110, "endOffset": 113}, {"referenceID": 6, "context": "But the natural solution for arbitrary n is a recurrent net with only 3 units and 5 weights, reading the input bit string one bit at a time, making a single recurrent hidden unit flip its state whenever a new 1 is observed [7].", "startOffset": 223, "endOffset": 226}, {"referenceID": 7, "context": "Related observations hold for Boolean circuits [8, 9] and modern neural networks [10, 11, 12].", "startOffset": 47, "endOffset": 53}, {"referenceID": 8, "context": "Related observations hold for Boolean circuits [8, 9] and modern neural networks [10, 11, 12].", "startOffset": 47, "endOffset": 53}, {"referenceID": 9, "context": "Related observations hold for Boolean circuits [8, 9] and modern neural networks [10, 11, 12].", "startOffset": 81, "endOffset": 93}, {"referenceID": 10, "context": "Related observations hold for Boolean circuits [8, 9] and modern neural networks [10, 11, 12].", "startOffset": 81, "endOffset": 93}, {"referenceID": 11, "context": "Related observations hold for Boolean circuits [8, 9] and modern neural networks [10, 11, 12].", "startOffset": 81, "endOffset": 93}, {"referenceID": 12, "context": "[13, 14, 15]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 13, "context": "[13, 14, 15]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 14, "context": "[13, 14, 15]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 15, "context": "Well-designed initialization strategies, in particular the normalized variance-preserving initialization for certain activation functions [16, 17], have been widely adopted for training moderately deep networks.", "startOffset": 138, "endOffset": 146}, {"referenceID": 16, "context": "Well-designed initialization strategies, in particular the normalized variance-preserving initialization for certain activation functions [16, 17], have been widely adopted for training moderately deep networks.", "startOffset": 138, "endOffset": 146}, {"referenceID": 17, "context": "Experiments showed [18] that certain activation functions based on local competition [18, 19] may help to train deeper networks.", "startOffset": 85, "endOffset": 93}, {"referenceID": 18, "context": "Skip connections between layers or to output layers (where error is \u201cinjected\u201d) have long been used in neural networks, more recently with the explicit aim to improve the flow of information [20, 21, 2, 22].", "startOffset": 191, "endOffset": 206}, {"referenceID": 19, "context": "Skip connections between layers or to output layers (where error is \u201cinjected\u201d) have long been used in neural networks, more recently with the explicit aim to improve the flow of information [20, 21, 2, 22].", "startOffset": 191, "endOffset": 206}, {"referenceID": 1, "context": "Skip connections between layers or to output layers (where error is \u201cinjected\u201d) have long been used in neural networks, more recently with the explicit aim to improve the flow of information [20, 21, 2, 22].", "startOffset": 191, "endOffset": 206}, {"referenceID": 20, "context": "A related recent technique is based on using soft targets from a shallow teacher network to aid in training deeper student networks in multiple stages [23], similar to the neural history compressor for sequences, where a slowly ticking teacher recurrent net is \u201cdistilled\u201d into a quickly ticking student recurrent net by forcing the latter to predict the hidden units of the former [24].", "startOffset": 151, "endOffset": 155}, {"referenceID": 21, "context": "A related recent technique is based on using soft targets from a shallow teacher network to aid in training deeper student networks in multiple stages [23], similar to the neural history compressor for sequences, where a slowly ticking teacher recurrent net is \u201cdistilled\u201d into a quickly ticking student recurrent net by forcing the latter to predict the hidden units of the former [24].", "startOffset": 382, "endOffset": 386}, {"referenceID": 21, "context": "Finally, deep networks can be trained layer-wise to help in credit assignment [24, 25], although this approach is less attractive compared to direct training.", "startOffset": 78, "endOffset": 86}, {"referenceID": 22, "context": "Finally, deep networks can be trained layer-wise to help in credit assignment [24, 25], although this approach is less attractive compared to direct training.", "startOffset": 78, "endOffset": 86}, {"referenceID": 23, "context": "Training deep networks, however, still faces problems, albeit perhaps less fundamental ones than the problem of vanishing gradients in standard recurrent networks [26].", "startOffset": 163, "endOffset": 167}, {"referenceID": 24, "context": "To overcome this, we take inspiration from Long Short Term Memory (LSTM) recurrent networks [27, 28].", "startOffset": 92, "endOffset": 100}, {"referenceID": 25, "context": "To overcome this, we take inspiration from Long Short Term Memory (LSTM) recurrent networks [27, 28].", "startOffset": 92, "endOffset": 100}, {"referenceID": 20, "context": "Deep networks with limited computational budget (for which a two-stage training procedure mentioned above was recently proposed [23]) can also be directly trained in a single stage when converted to highway networks.", "startOffset": 128, "endOffset": 132}, {"referenceID": 26, "context": "More recently, a similar LSTMinspired model was also proposed [30].", "startOffset": 62, "endOffset": 66}, {"referenceID": 25, "context": "This scheme is strongly inspired by the proposal [28] to initially bias the gates in an LSTM network, to help bridge long-term temporal dependencies early in learning.", "startOffset": 49, "endOffset": 53}, {"referenceID": 20, "context": "[23]) Teacher 5 \u223c9M 90.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[23] using two-stage hint based training.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "All convolutional highway networks utilize the rectified linear activation function [16] to compute the block stateH .", "startOffset": 84, "endOffset": 88}, {"referenceID": 27, "context": "Some experiments were conducted using the Caffe [31] library.", "startOffset": 48, "endOffset": 52}, {"referenceID": 15, "context": "1 Optimization To support the hypothesis that highway networks do not suffer from increasing depth, we conducted a series of rigorous optimization experiments, comparing them to plain networks with normalized initialization [16, 17].", "startOffset": 224, "endOffset": 232}, {"referenceID": 16, "context": "1 Optimization To support the hypothesis that highway networks do not suffer from increasing depth, we conducted a series of rigorous optimization experiments, comparing them to plain networks with normalized initialization [16, 17].", "startOffset": 224, "endOffset": 232}, {"referenceID": 16, "context": "Although there are other initialization methods, this is the only one we are aware of that has been used in recent work to obtain good performance on real world data using deep networks of up to 19 layers [17].", "startOffset": 205, "endOffset": 209}, {"referenceID": 29, "context": "42 dasNet [34] 90.", "startOffset": 10, "endOffset": 14}, {"referenceID": 30, "context": "43 All-CNN [35] 92.", "startOffset": 11, "endOffset": 15}, {"referenceID": 20, "context": "[23] recently reported that training on CIFAR-10 through plain backpropogation was only possible for maxout networks with a depth up to 5 layers when the number of parameters was limited to\u223c250K and the number of multiplications to \u223c30M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] and recently extended by Graham [32].", "startOffset": 0, "endOffset": 3}, {"referenceID": 28, "context": "[5] and recently extended by Graham [32].", "startOffset": 36, "endOffset": 40}, {"referenceID": 31, "context": "Learning to route information through neural networks with the help of competitive interactions has helped to scale up their application to challenging problems by improving credit assignment [36], but they still suffer when depth increases beyond \u224820 even with careful initialization [17].", "startOffset": 192, "endOffset": 196}, {"referenceID": 16, "context": "Learning to route information through neural networks with the help of competitive interactions has helped to scale up their application to challenging problems by improving credit assignment [36], but they still suffer when depth increases beyond \u224820 even with careful initialization [17].", "startOffset": 285, "endOffset": 289}, {"referenceID": 20, "context": "Deep supervision [22] has been shown to hurt performance of deep narrow networks [23].", "startOffset": 81, "endOffset": 85}], "year": 2015, "abstractText": "Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures.", "creator": "LaTeX with hyperref package"}}}