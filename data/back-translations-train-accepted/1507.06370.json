{"id": "1507.06370", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jul-2015", "title": "Sum-of-Squares Lower Bounds for Sparse PCA", "abstract": "This paper establishes a statistical versus computational trade-off for solving a basic high-dimensional machine learning problem via a basic convex relaxation method. Specifically, we consider the {\\em Sparse Principal Component Analysis} (Sparse PCA) problem, and the family of {\\em Sum-of-Squares} (SoS, aka Lasserre/Parillo) convex relaxations. It was well known that in large dimension $p$, a planted $k$-sparse unit vector can be {\\em in principle} detected using only $n \\approx k\\log p$ (Gaussian or Bernoulli) samples, but all {\\em efficient} (polynomial time) algorithms known require $n \\approx k^2 \\log p$ samples. It was also known that this quadratic gap cannot be improved by the the most basic {\\em semi-definite} (SDP, aka spectral) relaxation, equivalent to a degree-2 SoS algorithms. Here we prove that also degree-4 SoS algorithms cannot improve this quadratic gap. This average-case lower bound adds to the small collection of hardness results in machine learning for this powerful family of convex relaxation algorithms. Moreover, our design of moments (or \"pseudo-expectations\") for this lower bound is quite different than previous lower bounds. Establishing lower bounds for higher degree SoS algorithms for remains a challenging problem.", "histories": [["v1", "Thu, 23 Jul 2015 01:50:43 GMT  (479kb)", "https://arxiv.org/abs/1507.06370v1", null], ["v2", "Sun, 18 Oct 2015 05:50:16 GMT  (491kb)", "http://arxiv.org/abs/1507.06370v2", "to appear at NIPS 2015"]], "reviews": [], "SUBJECTS": "cs.LG cs.CC math.ST stat.CO stat.ML stat.TH", "authors": ["tengyu ma", "avi wigderson"], "accepted": true, "id": "1507.06370"}, "pdf": {"name": "1507.06370.pdf", "metadata": {"source": "CRF", "title": "Sum-of-Squares Lower Bounds for Sparse PCA", "authors": ["Tengyu Ma", "Avi Wigderson"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 150 7.06 370v 2 [cs.L G] 18 E"}, {"heading": "1 Introduction", "text": "We start with a general discussion on the tension between sample size and computational efficiency in statistical and learning problems. We then describe the specific model and problem at hand: sumof squares algorithms and the sparse PCA problem. All these are general topics that are considered from different angles, and the references provided provide more information."}, {"heading": "1.1 Statistical vs. computational sample-size", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "1.2 Sum-of-Squares convex relaxations", "text": "Sum-of-squares algorithms (sometimes called Lasserre hierarchy) comprise perhaps the most powerful known algorithmic technique for a variety of optimization problems. It is a family of convex relations, independent of each other around the year 2000 by Lasserre [Las01], Parillo [Par00], and in the (equivalent) context of evidence systems by Grigoriev [Gri01b]. These papers followed a better and better understanding in real algebraic geometry [Art27, Ste74, Sch91, Put93, Nes00] of David Hilbert's famous 17th problem of certifying the non-negativity of a polynomia by writing squares (which explain the name of this method).We briefly describe this important class of algorithms that can be found in the book."}, {"heading": "1.3 Sparse PCA", "text": "It is not as if it is a real problem introduced by Johnstone [Joh01]. One observes n samples from the p-dimensional Gaussian distribution, and it represents the strength of the signal. The task is to find (or estimate) the sparse versions of the problem that allow for multiple sparse instructions / components and general covariance matrix. [Ma13, VL13, VL13] Sparse PCA and its variants have a wide range of applications ranging from signal processing to biology."}, {"heading": "1.4 Our contribution", "text": "We give a direct, unconditional proof of lower limits for calculating Sparse PCA using Grade 4 SoS algorithms, which show that they also need n = 3 (k2) samples to solve the detection problem (Theorem 3.1), which is closely linked to polylogarithmic factors when the strength of signal \u03bb is a constant. In fact, the theorem gives a lower limit for each strength \u03bb, which becomes weaker the greater it gets. Our proof proceeds by constructing the necessary pseudo-moments for the SoS program, which reach too high an objective value (in optimization jargon, we prove an \"integrity gap\" for these programs). As usual in such proofs, there are tensions between the pseudo-moments, which satisfy the limitations of the program and keep them positively semi-defined (PSD). Different from past lower proofs, we construct two different PSD moments, each of which is met by approximately one set of limitations in the program."}, {"heading": "2 Formal description of the model and problem", "text": "Notation: To denote the Euclidean norm of a vector and the spectral norm of a matrix, we use to denote the q norm of a vector, and | \u00b7 | 0 is the number of unequal entries of a vector. We use [m] to denote the set of integers {1,.., m}. We write M 0 if M is a positive semidefinitive matrix. Rn [x] d is used to denote the set of real polynomials with n variables and degrees at most. We drop the subscript n if it is clear from the context. We assume that n, k, p are all sufficiently large. 4, and that n \u2264 p. Through this paper \"with high probability of an event occurring,\" we mean that the probability of error is limited by p \u2212 c for each constant c, since p tends to infinity. We use the asymptotic notation O \u00b7 p (with high probability of an event that occurs as a) p tends to be (that) p (that) as a (the) p tends to be (that) as an infinite p."}, {"heading": "2.1 Sparse PCA estimation and detection problems", "text": "We consider the simplest setting of the sparse PCA, which is referred to in the literature as the single spiked covariance model [Joh01] (Note: Restriction to one specific case preserves our lower limit in all generalizations of this simple model.) In this model, the task is to restore a single sparse vector from noisy samples as follows. \"Hidden data\" is an unknown k-sparse vector v-sparse vector v-Rp with | v | 0 = k and 0 = v = 1. To make the task easier (and thus the lower limit is stronger), we even assume that v has discrete entries, namely that vi \u00b2 {0, \u00b1 1 \u00b0 k} for all i \u00b2 samples X1,. We observe noisy samples X1,."}, {"heading": "2.2 Statistically optimal estimator/detector", "text": "It is well known that the following non-convex program achieves the optimal statistical minimax rate for the estimation problem and the optimal sample complexity for the detection problem. Note that we scale the variables x up by a factor x (the hidden vector now has entries of {0, \u00b1 1}).\u03bbkmax (\u03a3) = 1k \u00b7 max < \u03a3, xxxT > (2.2) subject to simplicity (the hidden vector now has entries of {0, \u00b1 1}. Proposition 2,1 ([AW09], [BR13b], [VL12] informally stated). The non-convex program (2,2) solves the sparse PCA problem statistically optimally if n \u2265 Ck / \u03bb2 log p holds some sufficiently large C. Namely, the following test holds a high probability. If X is generated from Hv, then the optimal solution xopt of the program (2,2) is satisfactory."}, {"heading": "2.3 Sum of Squares (Lasserre) Relaxations", "text": "Here we will briefly introduce the basic ideas of the sum-of-squares (Lasserre) that are used for most squares (Lasserre). For most squares (Lasserre), the relaxation that is used for this work is very important. (Lasserre) We refer to the detailed [Las15, Lau09, BS14] for detailed discussions on the sum of squares algorithms and their application to algorithms design.Let R [x] d denotes the set of all real polynomials that behave like the actual first d moments of a real distribution x1. (Definition 2.2) We start by defining the concept of the pseudo-moment (sometimes referred to as pseudo-expectation).The intuition is that these pseudo-moments behave like the actual first d moments of a real distribution x1. (Definition 2.2) A degree-d pseudo-moment M is a linear one, the pseudo-moment M is a polymer that satisfies all maps [M] (M1)."}, {"heading": "3 Main Results", "text": "To exhaust the sum of the square relaxation frames as described in Section 2.3, we must first convert the statistically optimal estimator / detector (2.2) to the \"polynomial\" program version (2.2). [2] Note: Non-convex economy (2.4) is replaced by the polynomial constraint (3.3), which ensures that each vector has x entries in {0, \u00b1 1}, and thus, together with the constraint (3.2), guarantees that it has exactly k non-zero entries, any absolute value 1. Note that the constraint (3.3) implies other natural constraints that can be added to the program to make it stronger."}, {"heading": "3.1 Lower bounds for detection problem", "text": "Theorem 3.1. There is an absolute constant C and r, so that for 1 \u2264 \u03bb < min {k1 / 4, \u221a n} and each p \u2265 C\u03bbn, k \u2265 C\u03bb7 / 6 \u221a n logr p the following applies. If the data X is drawn from the null hypothesis H0, then with high probability (1 \u2212 p \u2212 10), then the objective value of the square relaxation sum SoS4 (\u03a3) is at least 10\u03bbk. Consequently, algorithm 1 cannot solve the detection problem. To analyze the theorem and understand its consequences, let us first consider the case if it is a constant (which is probably the most interesting regime), and then the theory says that if we only have n \u041a2 samples, degree-4 SOSrelaxation SoS4 is still too high for the randomness of the data."}, {"heading": "3.2 Lower bounds for the estimation problem", "text": "Theorem 3.2. For each constant B there are absolute constants C and r, so that for \u03bb \u2264 B / 2, Bn \u2265 p \u2265 2\u03bbn and o (p) \u2265 k \u2265 C \u221a n logr p the data X is output first as hypothesis Hv (model (2,1)) and then with high probability (1 \u2212 p \u2212 10) about the randomness of the data. Algorithm 1 will output M \u0445 2 so that the results are 1k \u00b7 M \u0445 2 \u2212 vvT \u0445 1 / 5. We find that the result is of the same kind (and probably almost optimal for the estimation problem) as [KNV15] the one for Grade 2 SoS relaxation. The proof is simply derived from the combination of our detection theorem 3.1 and arguments similar [KNV15]. Finally, we address a threshold behavior, namely the slightly minor estimation that the deviation of the log is necessary for our samples (2,0)."}, {"heading": "4 Design of Pseudo-moments", "text": "We start with a sketch of our approach to designing the moments on a very high level. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <"}, {"heading": "4.1 Approximate Pseudo-moments", "text": "In this section, we design a pseudo-moment that fulfills roughly all the constraints; in the next section, we will adjust it locally to obtain one that exactly fulfills all the constraints; we start by designing a (partial) objective 2 moment that yields a large objective value that will later be used for the degree-4 moments; the design is essentially the same as [KNV15], although we are only working with zero hypothesis for now; for the purpose of this section, we propose to the reader to consider X as a uniform value used for simplicity, although we will later see that X fulfills certain pseudorandomness conditions chosen by a variety of natural stochastic models (with a series of normalizations); we define M [x] 2 \u2192 R as follows: M (xixj) that we (xixj) meet a certain pseudo-condition."}, {"heading": "4.2 Exact Pseudo-moments", "text": "Note that M & # 160; only the constraints of & # 246; & # 223; are fulfilled approximately up to some additive errors (which are carefully limited for the purpose of the n & # 246; highest theorem). & # 160; Specifically, we will use M & # 160; (1) = 1 and f & # 252; for all added M & # 160; (2), M & # 160; (2), M (2), M (2), M (2), M (2), M (2), M (2), M (2), M (2), M (2), M (2), M (3), T & # 160;, M (3), M (2), M (2), M (2), M (2), M (2), M (2), M (2), M (2), M (2) and M (2)."}, {"heading": "5 Proof of Theorem 3.1 and Theorem 3.2", "text": "In this section we will prove our main theorems on the basis of the technical results of the previous sections. Before we enter into the proof, we start with the observation that in order to obtain a lower limit of the objective value, it is sufficient to consider the particular case if p = 10\u03bbn. The reason for this is that the objective value of SoS4 increases in p while all other parameters are set. Then we can M \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p. \"p\" p \"p\" p \"p\" p \"p.\" p \"p\" p. \"p\" p. \"p.\""}, {"heading": "6 Analysis of matrices P and Q", "text": "In this section we prove that Lemma 4.4 and 4.5. < < < < < < < < < < < < < < < < < (P1), (P2), (4.4) and (4.5) and the boundaries for Xi and Xi one by one. \u2022 Equation (4.3): In case if i = j, we verify P (x4i), using property (P1) and (P2), P (x4i) = Xp2n3 < Xi, Xi > 4 + i < Xi, X > 4 (X4i), using property (P2), P (X4i), P (x4i) = Xp2n3 < Xi, Xi, Xi, Xi 4 +."}, {"heading": "7 Pseudo-randomness of X", "text": "In this section, we prove that we essentially have variance 1 (which generalizes the standard Bernoulli and the Gaussian distributions) as long as the noise is no longer conditioned by normalization, that we only have good quality if we meet the pseudorandness condition 4.1 (1), then the matrix X variables X1, XTi, XTi, XTi, XTi, XTi, XTi, XTi, XTi, XTi, XTi, XTi, XTi, XTi, XTi, XTi, XTi, XTi, XTi, XTi, XTi XTi, XTi, XTi, XTi, XTi, XTi and Xi, XTi, XTi, XTi Xi Xi Xi, XTi XTi, XTi and XTi, XTi, XTi XTi Xi, XTi Xi and XTi, XTi XTi Xi, XTi XTi Xi Xi and XTi, XTi XTi, XTi XTi Xi Xi, XTi XTi Xi and Xi XTi, XTi XTi XTi Xi, XTi XTi Xi and XTi, XTi XTi Xi Xi, XTi XTi Xi Xi, XTi and Xi XTi XTi Xi, XTi Xi Xi, Xi Xi XTi Xi XTi and Xi Xi, XTi Xi, XTi Xi XTi, Xi XTi Xi and XTi, Xi XTi Xi Xi Xi, Xi Xi Xi XTi Xi Xi and Xi, XTi Xi, XTi, Xi XTi Xi Xi Xi Xi, Xi and Xi Xi Xi Xi Xi, XTi, Xi XTi Xi Xi Xi and Xi, Xi Xi, Xi, X"}, {"heading": "8 Toolbox", "text": "This section contains a collection of known technical results that are useful for establishing the concentration limits of section 7."}, {"heading": "9 Conclusions and future directions", "text": "In this paper, a lower number of examples turns out to be necessary to solve the problem. It remains an interesting problem to expand our low number of examples (or even better to solve the problem with fewer examples)."}], "references": [{"title": "Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays", "author": ["U. Alon", "N. Barkai", "D.A. Notterman", "K. Gish", "S. Ybarra", "D. Mack", "A.J. Levine"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Alon et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Alon et al\\.", "year": 1999}, {"title": "\u00dcber die zerlegung definiter funktionen in quadrate", "author": ["Emil Artin"], "venue": "In Abhandlungen aus dem mathematischen Seminar der Universita\u0308t Hamburg,", "citeRegEx": "Artin.,? \\Q1927\\E", "shortCiteRegEx": "Artin.", "year": 1927}, {"title": "High-dimensional analysis of semidefinite relaxations for sparse principal components", "author": ["Arash A. Amini", "Martin J. Wainwright"], "venue": "Ann. Statist., 37(5B):2877\u20132921,", "citeRegEx": "Amini and Wainwright.,? \\Q2009\\E", "shortCiteRegEx": "Amini and Wainwright.", "year": 2009}, {"title": "Rounding sum-of-squares relaxations", "author": ["Boaz Barak", "Jonathan A. Kelner", "David Steurer"], "venue": "In STOC,", "citeRegEx": "Barak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Barak et al\\.", "year": 2014}, {"title": "Dictionary learning and tensor decomposition via the sum-of-squares method", "author": ["Boaz Barak", "Jonathan A. Kelner", "David Steurer"], "venue": "In Proceedings of the Forty-seventh Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Barak et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Barak et al\\.", "year": 2015}, {"title": "Tensor prediction, rademacher complexity and random 3-xor", "author": ["Boaz Barak", "Ankur Moitra"], "venue": "CoRR, abs/1501.06521,", "citeRegEx": "Barak and Moitra.,? \\Q2015\\E", "shortCiteRegEx": "Barak and Moitra.", "year": 2015}, {"title": "Complexity theoretic lower bounds for sparse principal component detection", "author": ["Quentin Berthet", "Philippe Rigollet"], "venue": "In COLT 2013 - The 26th Annual Conference on Learning Theory, June 12-14,", "citeRegEx": "Berthet and Rigollet.,? \\Q2013\\E", "shortCiteRegEx": "Berthet and Rigollet.", "year": 2013}, {"title": "Optimal detection of sparse principal components in high dimension", "author": ["Quentin Berthet", "Philippe Rigollet"], "venue": "The Annals of Statistics,", "citeRegEx": "Berthet and Rigollet.,? \\Q2013\\E", "shortCiteRegEx": "Berthet and Rigollet.", "year": 2013}, {"title": "Sum-of-squares proofs and the quest toward optimal algorithms", "author": ["Boaz Barak", "David Steurer"], "venue": "In Proceedings of International Congress of Mathematicians (ICM),", "citeRegEx": "Barak and Steurer.,? \\Q2014\\E", "shortCiteRegEx": "Barak and Steurer.", "year": 2014}, {"title": "Adaptive elastic-net sparse principal component analysis for pathway association testing", "author": ["Xi Chen"], "venue": "Statistical Applications in Genetics and Molecular Biology,", "citeRegEx": "Chen.,? \\Q2011\\E", "shortCiteRegEx": "Chen.", "year": 2011}, {"title": "Computational and statistical tradeoffs via convex relaxation", "author": ["Venkat Chandrasekaran", "Michael I. Jordan"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Chandrasekaran and Jordan.,? \\Q2013\\E", "shortCiteRegEx": "Chandrasekaran and Jordan.", "year": 2013}, {"title": "A direct formulation for sparse pca using semidefinite programming", "author": ["Alexandre d\u2019Aspremont", "Laurent El Ghaoui", "Michael I. Jordan", "Gert R.G. Lanckriet"], "venue": "SIAM Review,", "citeRegEx": "d.Aspremont et al\\.,? \\Q2007\\E", "shortCiteRegEx": "d.Aspremont et al\\.", "year": 2007}, {"title": "Computational sample complexity", "author": ["Scott Decatur", "Oded Goldreich", "Dana Ron"], "venue": "In Proceedings of the Tenth Annual Conference on Computational Learning Theory,", "citeRegEx": "Decatur et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Decatur et al\\.", "year": 1997}, {"title": "Minimax estimation via wavelet shrinkage", "author": ["David L. Donoho", "Iain M. Johnstone"], "venue": "Ann. Statist., 26(3):879\u2013921,", "citeRegEx": "Donoho and Johnstone.,? \\Q1998\\E", "shortCiteRegEx": "Donoho and Johnstone.", "year": 1998}, {"title": "More data speeds up training time in learning halfspaces over sparse vectors", "author": ["Amit Daniely", "Nati Linial", "Shai Shalev-Shwartz"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Daniely et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Daniely et al\\.", "year": 2013}, {"title": "Sparse PCA via covariance thresholding", "author": ["Yash Deshpande", "Andrea Montanari"], "venue": "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems", "citeRegEx": "Deshpande and Montanari.,? \\Q2014\\E", "shortCiteRegEx": "Deshpande and Montanari.", "year": 2014}, {"title": "Improved Sum-of-Squares Lower Bounds for Hidden Clique and Hidden Submatrix Problems", "author": ["Y. Deshpande", "A. Montanari"], "venue": "ArXiv e-prints,", "citeRegEx": "Deshpande and Montanari.,? \\Q2015\\E", "shortCiteRegEx": "Deshpande and Montanari.", "year": 2015}, {"title": "De-noising by soft-thresholding", "author": ["D.L. Donoho"], "venue": "IEEE Trans. Inf. Theor.,", "citeRegEx": "Donoho.,? \\Q1995\\E", "shortCiteRegEx": "Donoho.", "year": 1995}, {"title": "Sparse CCA: Adaptive Estimation and Computational Barriers", "author": ["C. Gao", "Z. Ma", "H.H. Zhou"], "venue": "ArXiv e-prints,", "citeRegEx": "Gao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2014}, {"title": "Complexity of positivstellensatz proofs for the knapsack", "author": ["D. Grigoriev"], "venue": "computational complexity,", "citeRegEx": "Grigoriev.,? \\Q2001\\E", "shortCiteRegEx": "Grigoriev.", "year": 2001}, {"title": "Linear lower bound on degrees of positivstellensatz calculus proofs for the parity", "author": ["Dima Grigoriev"], "venue": "Theoretical Computer Science,", "citeRegEx": "Grigoriev.,? \\Q2001\\E", "shortCiteRegEx": "Grigoriev.", "year": 2001}, {"title": "Sos and planted clique: Tight analysis of MPW moments at all degrees and an optimal lower bound at degree four", "author": ["Samuel B. Hopkins", "Pravesh K. Kothari", "Aaron Potechin"], "venue": "CoRR, abs/1507.05230,", "citeRegEx": "Hopkins et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hopkins et al\\.", "year": 2015}, {"title": "On consistency and sparsity for principal components analysis in high dimensions", "author": ["Iain M. Johnstone", "Arthur Yu Lu"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Johnstone and Lu.,? \\Q2009\\E", "shortCiteRegEx": "Johnstone and Lu.", "year": 2009}, {"title": "Structured sparse principal component analysis", "author": ["Rodolphe Jenatton", "Guillaume Obozinski", "Francis R. Bach"], "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Jenatton et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jenatton et al\\.", "year": 2010}, {"title": "On the distribution of the largest eigenvalue in principal components analysis", "author": ["Iain M. Johnstone"], "venue": "Ann. Statist., 29(2):295\u2013327,", "citeRegEx": "Johnstone.,? \\Q2001\\E", "shortCiteRegEx": "Johnstone.", "year": 2001}, {"title": "Function estimation and gaussian sequence models", "author": ["IM Johnstone"], "venue": "Unpublished manuscript,", "citeRegEx": "Johnstone.,? \\Q2002\\E", "shortCiteRegEx": "Johnstone.", "year": 2002}, {"title": "Do semidefinite relaxations solve sparse pca up to the information limit", "author": ["Robert Krauthgamer", "Boaz Nadler", "Dan Vilenchik"], "venue": "The Annals of Statistics,", "citeRegEx": "Krauthgamer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Krauthgamer et al\\.", "year": 2015}, {"title": "Anneaux pr\u00e9ordonn\u00e9s", "author": ["Jean-Louis Krivine"], "venue": "Journal d\u2019analyse mathe\u0301matique,", "citeRegEx": "Krivine.,? \\Q1964\\E", "shortCiteRegEx": "Krivine.", "year": 1964}, {"title": "Global optimization with polynomials and the problem of moments", "author": ["Jean B. Lasserre"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Lasserre.,? \\Q2001\\E", "shortCiteRegEx": "Lasserre.", "year": 2001}, {"title": "An introduction to polynomial and semi-algebraic optimization. Cambridge Texts in Applied Mathematics", "author": ["Jean Bernard Lasserre"], "venue": null, "citeRegEx": "Lasserre.,? \\Q2015\\E", "shortCiteRegEx": "Lasserre.", "year": 2015}, {"title": "Sums of squares, moment matrices and optimization over polynomials", "author": ["Monique Laurent"], "venue": "Emerging Applications of Algebraic Geometry,", "citeRegEx": "Laurent.,? \\Q2009\\E", "shortCiteRegEx": "Laurent.", "year": 2009}, {"title": "Cones of matrices and set-functions and 01 optimization", "author": ["L. Lov\u00e1sz", "A. Schrijver"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Lov\u00e1sz and Schrijver.,? \\Q1991\\E", "shortCiteRegEx": "Lov\u00e1sz and Schrijver.", "year": 1991}, {"title": "Probability in Banach Spaces: isoperimetry and processes, volume 23", "author": ["Michel Ledoux", "Michel Talagrand"], "venue": "Springer Science & Business Media,", "citeRegEx": "Ledoux and Talagrand.,? \\Q2013\\E", "shortCiteRegEx": "Ledoux and Talagrand.", "year": 2013}, {"title": "Sparse principal component analysis and iterative thresholding", "author": ["Zongming Ma"], "venue": "Ann. Statist., 41(2):772\u2013801,", "citeRegEx": "Ma.,? \\Q2013\\E", "shortCiteRegEx": "Ma.", "year": 2013}, {"title": "Sum-of-squares lower bounds for planted clique", "author": ["Raghu Meka", "Aaron Potechin", "Avi Wigderson"], "venue": "CoRR, abs/1503.06447,", "citeRegEx": "Meka et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Meka et al\\.", "year": 2015}, {"title": "Squared functional systems and optimization problems", "author": ["Yurii Nesterov"], "venue": "High Performance Optimization,", "citeRegEx": "Nesterov.,? \\Q2000\\E", "shortCiteRegEx": "Nesterov.", "year": 2000}, {"title": "Structured Semidefinite Programs and Semialgebraic Geometry Methods in Robustness and Optimization", "author": ["Pablo A. Parrilo"], "venue": "PhD thesis, California Institute of Technology,", "citeRegEx": "Parrilo.,? \\Q2000\\E", "shortCiteRegEx": "Parrilo.", "year": 2000}, {"title": "Augmented sparse principal component analysis for high dimensional data", "author": ["Debashis Paul", "Iain M Johnstone"], "venue": "arXiv preprint arXiv:1202.1242,", "citeRegEx": "Paul and Johnstone.,? \\Q2012\\E", "shortCiteRegEx": "Paul and Johnstone.", "year": 2012}, {"title": "Decoupling inequalities for the tail probabilities of multivariate u-statistics", "author": ["Victor H. de la Pena", "S.J. Montgomery-Smith"], "venue": "The Annals of Probability,", "citeRegEx": "Pena and Montgomery.Smith.,? \\Q1995\\E", "shortCiteRegEx": "Pena and Montgomery.Smith.", "year": 1995}, {"title": "Positive polynomials on compact semi-algebraic sets", "author": ["Mihai Putinar"], "venue": "Indiana University Mathematics Journal,", "citeRegEx": "Putinar.,? \\Q1993\\E", "shortCiteRegEx": "Putinar.", "year": 1993}, {"title": "Tight lower bounds for planted clique in the degree-4", "author": ["Prasad Raghavendra", "Tselil Schramm"], "venue": "SOS program. CoRR,", "citeRegEx": "Raghavendra and Schramm.,? \\Q2015\\E", "shortCiteRegEx": "Raghavendra and Schramm.", "year": 2015}, {"title": "A hierarchy of relaxations between the continuous and convex hull representations for zero-one programming problems", "author": ["Hanif D. Sherali", "Warren P. Adams"], "venue": "SIAM Journal on Discrete Mathematics,", "citeRegEx": "Sherali and Adams.,? \\Q1990\\E", "shortCiteRegEx": "Sherali and Adams.", "year": 1990}, {"title": "Linear level lasserre lower bounds for certain k-csps", "author": ["Grant Schoenebeck"], "venue": "In Proceedings of the 2008 49th Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "Schoenebeck.,? \\Q2008\\E", "shortCiteRegEx": "Schoenebeck.", "year": 2008}, {"title": "Computational sample complexity and attribute-efficient learning", "author": ["Rocco A. Servedio"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Servedio.,? \\Q2000\\E", "shortCiteRegEx": "Servedio.", "year": 2000}, {"title": "An approach to obtaining global extremums in polynomial mathematical programming problems", "author": ["N.Z. Shor"], "venue": null, "citeRegEx": "Shor.,? \\Q1987\\E", "shortCiteRegEx": "Shor.", "year": 1987}, {"title": "Weak Convergence and Empirical Processes: With Applications to Statistics (Springer Series in Statistics)", "author": ["Aad van der Vaart", "Jon Wellner"], "venue": null, "citeRegEx": "Vaart and Wellner.,? \\Q2000\\E", "shortCiteRegEx": "Vaart and Wellner.", "year": 2000}, {"title": "Minimax rates of estimation for sparse PCA in high dimensions", "author": ["Vincent Q. Vu", "Jing Lei"], "venue": "In Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Vu and Lei.,? \\Q2012\\E", "shortCiteRegEx": "Vu and Lei.", "year": 2012}, {"title": "Minimax sparse principal subspace estimation in high dimensions", "author": ["Vincent Q. Vu", "Jing Lei"], "venue": "Ann. Statist., 41(6):2905\u20132947,", "citeRegEx": "Vu and Lei.,? \\Q2013\\E", "shortCiteRegEx": "Vu and Lei.", "year": 2013}, {"title": "Statistical Limits of Convex Relaxations", "author": ["Z. Wang", "Q. Gu", "H. Liu"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Tighten after relax: Minimax-optimal sparse PCA in polynomial time", "author": ["Zhaoran Wang", "Huanran Lu", "Han Liu"], "venue": "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Truncated power method for sparse eigenvalue problems", "author": ["Xiao-Tong Yuan", "Tong Zhang"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Yuan and Zhang.,? \\Q2013\\E", "shortCiteRegEx": "Yuan and Zhang.", "year": 2013}], "referenceMentions": [], "year": 2015, "abstractText": "This paper establishes a statistical versus computational trade-off for solving a basic highdimensional machine learning problem via a basic convex relaxation method. Specifically, we consider the Sparse Principal Component Analysis (Sparse PCA) problem, and the family of Sum-of-Squares (SoS, aka Lasserre/Parillo) convex relaxations. It was well known that in large dimension p, a planted k-sparse unit vector can be in principle detected using only n \u2248 k log p (Gaussian or Bernoulli) samples, but all efficient (polynomial time) algorithms known require n \u2248 k samples. It was also known that this quadratic gap cannot be improved by the the most basic semi-definite (SDP, aka spectral) relaxation, equivalent to a degree-2 SoS algorithms. Here we prove that also degree-4 SoS algorithms cannot improve this quadratic gap. This averagecase lower bound adds to the small collection of hardness results in machine learning for this powerful family of convex relaxation algorithms. Moreover, our design of moments (or \u201cpseudoexpectations\u201d) for this lower bound is quite different than previous lower bounds. Establishing lower bounds for higher degree SoS algorithms for remains a challenging problem.", "creator": "LaTeX with hyperref package"}}}