{"id": "1507.06738", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jul-2015", "title": "Linear Contextual Bandits with Knapsacks", "abstract": "We consider the linear contextual bandit problem with global convex constraints and a concave objective function. In each round, the outcome of pulling an arm is a vector, that depends linearly on the context of that arm. The global constraints require the average of these vectors to lie in a certain convex set. The objective is a concave function of this average vector. This problem turns out to be a common generalization of classic linear contextual bandits (linContextual) [Auer 2003], bandits with concave rewards and convex knapsacks (BwCR) [Agrawal, Devanur 2014], and the online stochastic convex programming (OSCP) problem [Agrawal, Devanur 2015]. We present algorithms with near-optimal regret bounds for this problem. Our bounds compare favorably to results on the unstructured version of the problem [Agrawal et al. 2015, Badanidiyuru et al. 2014] where the relation between the contexts and the outcomes could be arbitrary, but the algorithm only competes against a fixed set of policies.", "histories": [["v1", "Fri, 24 Jul 2015 04:24:22 GMT  (36kb)", "https://arxiv.org/abs/1507.06738v1", null], ["v2", "Sat, 9 Jul 2016 06:29:22 GMT  (37kb)", "http://arxiv.org/abs/1507.06738v2", null]], "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["shipra agrawal", "nikhil r devanur"], "accepted": true, "id": "1507.06738"}, "pdf": {"name": "1507.06738.pdf", "metadata": {"source": "CRF", "title": "Linear Contextual Bandits with Knapsacks", "authors": ["Shipra Agrawal", "Nikhil R. Devanur"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 150 7.06 738v 2 [cs.L G] 9J ul2 016"}, {"heading": "1 Introduction", "text": "In the contextual bandit problem [7, 13, 21, 2], policymakers are forced to observe a sequence of contexts (or characteristics); in each round, they have to withdraw from a different context after observing the context for that round; the result of pulling an arm can be used along with the contexts to decide future arms; Contextual bandit problems have found many useful applications such as online referral systems, online advertising, and clinical trials where the decision in each round has to be adapted to the characteristics of the user being served; and the linear contextual bandit problem [1, 7, 16] is a specific case of the contextual bandit problem, where the result is linear in the function that encodes the context; as the contextual bandit problems represent a natural halfway point between supervised learning and reinforcement learning: the use of characteristics to encode contexts and the relationship between these characteristics are frequently inherited by models."}, {"heading": "1.1 Main results", "text": "It is not only a problem, but also a problem to be solved, in fact, that the relationship between contexts and results is arbitrary, and the algorithms compete with an arbitrary set of context-dependent policies that are attainable via an optimization oracle, with an optimization oracle that is regrettable that the relationship between contexts and results is arbitrary, and the algorithms compete with an arbitrary fixed set of context-dependent policies that are attainable via an optimization oracle. (OppTB + 1) The relationship between contexts and results is arbitrary, and the algorithms compete with an arbitrary fixed set of context-dependent policies that are attainable via an optimization oracle. (OppTB + 1) The relationship between contexts and results is arbitrary and the algorithms competing with an arbitrary fixed set of context-dependent policies."}, {"heading": "2 Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Confidence Ellipsoid", "text": "Consider a stochastic process that generates an observation pair (rt, yt) in each round t, so that rt is an unknown linear function of yt plus a certain 0-mean limited noise, i.e. rt = \u00b5. This technique is common in previous work on linear contextual bandits (e.g. in [7, 16, 1]). High reliability of the unknown vector can be achieved by building a confidence ellipsoid around the two regulated minimum square estimates."}, {"heading": "2.2 Online Learning", "text": "The online problem of convex optimization (OCO) considers a T-round game played between a learner and an opponent, with the learner in turn t selecting the choice of the learner and the opponent in previous rounds. The goal of the learner is to minimize remorse, which is defined as the difference between the objective value of the learner and the value of the best individual choice in retrospect: R (T): = max. short-term remorse, defined as the difference between the objective value of the learner and the value of the best individual choice in d + 1 dimensions. The online mirror descent (OMD) algorithm has very fast updating rules per step and offers the following repentance guarantees for this adjustment. Lemma 4. [32] This problem is the simplest unit in d + 1 dimensions."}, {"heading": "3 Algorithm", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Optimistic estimates of unknown parameters", "text": "At the beginning of each round, we use the results and contexts from previous rounds to construct a trust ellipsoid for a column used for a column j of W *. To construct a trust ellipsoid for a column j *, we use the techniques in Section 2.1, while we construct yt = xt (at) and rt = vt (at) j for each J.A in Section 2.1, we leave Mt: = I + 2 = 1 xi (ai) xi (ai) xi, and construct the regularized least square estimates for one column (at) and rt = vt (at) j for each J.A in Section 2.1, we leave Mt: = 1 xi (ai) xi (ai) xi."}, {"heading": "3.2 The core algorithm", "text": "In this section, we will present an algorithm and analysis, assuming that a particular parameter Z (TB) is used as an optimistic estimate. Later, we will show how to use the first T0 rounds to estimate Z, and also the additional regret due to these T0 rounds. We will also define the OMD algorithm for an instance of the online learning problem, using the unit simplex. The vector played by the online learning algorithm in time step t, after observing the context, is the optimistic estimate for each arm then constructed as defined in (7) and (8). Intuitively, it is used here as a multiplier to combine different columns of the weight matrix to obtain an optimistic weight vector for each."}, {"heading": "3.3 Algorithm with Z computation", "text": "In this section we will introduce a modification of the algorithm 1 + 2 + 1 + 2 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 + 1 + 1 + 1 = 1 + 1 = 1 + 1 = 1 + 1 = 1 = 1 + 1 = 1 = 1 + 1 = 1 = 1 + 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 ="}, {"heading": "A Concentration Inequalities", "text": "Lemma 6 (Azuma-Hoeffding inequality): If a supermartingale (Yt; t \u2265 0) corresponding to the filtration Ft meets | Yt \u2212 Yt \u2212 1 | \u2264 ct for a constant ct, for all t = 1,.., T, then for all a \u2265 0, Pr (YT \u2212 Y0 \u2265 a) \u2264 e \u2212 a2 2 2 \u0445 T t = 1 c2 t."}, {"heading": "B Benchmark", "text": "Detection of Lemma 1. For an instance \u03c9 = (Xt, Vt) T t = 1 of the input sequence, let the vector p \u0445 t (\u03c9) p \u0445 K + 1 distribute measures (plus no-op) by the optimal adaptive policy at a time. Then, OPT = E\u03c9 \u0445 DT [\u2211 T = 1 r't p; t (\u03c9)]] (12) Construct a static context-dependent policy as follows: for each X policy [0, 1] m \u00b7 K, defined \u03c0 (X): = 1TT \u0445 t = 1E\u043e [p-T (\u03c9) | Xt = X-T = X]. Intuitively, \u03c0 (X) a (in retrospect) means the probability that the optimal adaptive policy will take action when presented with a context, X-T = 1E\u043c [p-T], p-T (\u03c9), p-T = 4-T (\u03c0), p-T = 4-T-\u043c (T-T)."}, {"heading": "C Hardness of linear AMO", "text": "In this section we show that finding the best linear policy is NP-hard. Input to the problem is for each t [T] and each arm a [K] a context xt (a) i [0, 1] m and a reward rt (a) i [\u2212 1, 1] m. Input to this problem is for some integers n, for each i [n], a vector zi [0, 1] m and yi [1, + 1] m. Input to this problem is a vector. Input to this problem is for some integers n, for each i [n], a vector zi [0, 1] m and yi [1, + 1] m. The output is a vector that maximizes these half-spaces with noise. Input to this problem is a vector. Input to this problem is for some integers n, for each i [n], a vector zi [0, 1] m and yi [1, + 1] m."}, {"heading": "D Confidence ellipsoids", "text": "The proof for corollary 1 is with probability 1 \u2212 \u03b4. The inequality in the second line results from the multiplication of the two factors in the second line. The inequality in the first line is a matrix-norm version of Cauchy-Schwartz (Lemma 7). The inequality in the second line results from the multiplication of the two factors in the second line. For each positive definitive matrix M Rn \u00b7 n and two vectors a, b Rn, | a b | \u2264 a M b M \u2212 1.Evidence. Since M is positively defined, there is a matrix M1 / 2 so that M = M1 / 2M x a 1 / 2. Further, M \u2212 1 b = 1 \u2212 b \u2212 W / 2 that we get a positive definition for the first line."}, {"heading": "E Appendix for Section 3.2", "text": "The proof for theorem 2: We will use R's to denote the main term in the boundary of remorse (T): = O (m) ln (mdT) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T) ln (T (T) ln (T) ln (T) ln (T) ln (T) ln (T (T) ln (T (T) ln) ln (T (T) ln) ln (T ln (T (T) ln) ln (T (T) ln) ln (T ln) ln (T (T (T) ln) ln) ln (T ln) ln (T (T (T (T) ln) ln) ln (T (T (T (T) ln) ln) ln (T ln) ln (T (T (T) ln) ln (T ln) ln (T (T (T) ln) ln) ln (T (T (T) ln) ln (T ln) ln (T (T (T) ln) ln) ln (T (T ln) ln) ln (T"}, {"heading": "F Appendix for Section 3.3", "text": "The proof for Lemma 5: Let us define an \"intermediate sample optimally\" as follows: OPT \u03b3: = maxqT T0 \u0445 T0 i = 1 \u00b5. Xi\u03c0 (Xi)) so that TT0 \u0445 T0 i = 1 W (Xi). We do not actually calculate it, but will use it for the convenience of evidence exposure. The proof includes two stages. Step 1: Bound - OPT - OPT - OPT (OPT). Step 2: Bound - OPT - OPT (OPT). Step 1: OPT - OPT - OPT - OPT (OPT). - OPT - OPT - OPT - PT 1 bound from the work on online stochastic convex programming in 4 []: The proof includes two stages: OPT - OPT - OPT. OPT - OPT - OPT - OPT."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "<lb>We consider the linear contextual bandit problem with resource consumption, in addition to reward<lb>generation. In each round, the outcome of pulling an arm is a reward as well as a vector of resource<lb>consumptions. The expected values of these outcomes depend linearly on the context of that arm. The<lb>budget/capacity constraints require that the total consumption doesn\u2019t exceed the budget for each re-<lb>source. The objective is once again to maximize the total reward. This problem turns out to be a<lb>common generalization of classic linear contextual bandits (linContextual) [7, 16, 1], bandits with knap-<lb>sacks (BwK) [3, 10], and the online stochastic packing problem (OSPP) [4, 19]. We present algorithms<lb>with near-optimal regret bounds for this problem. Our bounds compare favorably to results on the<lb>unstructured version of the problem [5, 11] where the relation between the contexts and the outcomes<lb>could be arbitrary, but the algorithm only competes against a fixed set of policies accessible through<lb>an optimization oracle. We combine techniques from the work on linContextual, BwK and OSPP in a<lb>nontrivial manner while also tackling new difficulties that are not present in any of these special cases.", "creator": "LaTeX with hyperref package"}}}