{"id": "1507.08750", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jul-2015", "title": "Action-Conditional Video Prediction using Deep Networks in Atari Games", "abstract": "Motivated by vision-based reinforcement learning (RL) problems, in particular Atari games from the recent benchmark Aracade Learning Environment (ALE), we consider spatio-temporal prediction problems where future (image-)frames are dependent on control variables or actions as well as previous frames. While not composed of natural scenes, frames in Atari games are high-dimensional in size, can involve tens of objects with one or more objects being controlled by the actions directly and many other objects being influenced indirectly, can involve entry and departure of objects, and can involve deep partial observability. We propose and evaluate two deep neural network architectures that consist of encoding, action-conditional transformation, and decoding layers based on convolutional neural networks and recurrent neural networks. Experimental results show that the proposed architectures are able to generate visually-realistic frames that are also useful for control over approximately 100-step action-conditional futures in some games. To the best of our knowledge, this paper is the first to make and evaluate long-term predictions on high-dimensional video conditioned by control inputs.", "histories": [["v1", "Fri, 31 Jul 2015 04:43:30 GMT  (4950kb,D)", "http://arxiv.org/abs/1507.08750v1", "9 pages (main) and 33 pages (supplementary material)"], ["v2", "Tue, 22 Dec 2015 04:26:54 GMT  (5705kb,D)", "http://arxiv.org/abs/1507.08750v2", "Published at NIPS 2015 (Advances in Neural Information Processing Systems 28)"]], "COMMENTS": "9 pages (main) and 33 pages (supplementary material)", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV", "authors": ["junhyuk oh", "xiaoxiao guo", "honglak lee", "richard l lewis", "satinder p singh"], "accepted": true, "id": "1507.08750"}, "pdf": {"name": "1507.08750.pdf", "metadata": {"source": "CRF", "title": "Action-Conditional Video Prediction using Deep Networks in Atari Games", "authors": ["Junhyuk Oh", "Xiaoxiao Guo"], "emails": ["junhyuk@umich.edu", "guoxiao@umich.edu", "honglak@umich.edu", "rickl@umich.edu", "baveja@umich.edu"], "sections": [{"heading": "1 Introduction", "text": "Over the years, approaches to deep learning (see [6, 21] for the study) have proven to be very successful, as they are highly complex natural scenarios with complex sequences of time. Recent studies tend to focus on modeling simple video data, such as drill balls or small video patches, in which the next frame is highly predictable. In many cases, future frames depend not only on the previous frames, but also on the underlying actions."}, {"heading": "2 Related Work", "text": "The problem of video forecasting has led to a variety of architectural proposals that are taken into account in the literature on deep learning processes. A recurring time-limited Boltzmann machine (RTRBM) [23] was proposed to learn temporal correlations from sequential data by introducing recurring connections into RBM. A structured RTRBM (sRTRBM) [17] scales RTRBM by learning dependency structures between observations and hidden variables from data. More recently, Michalski et al. [16] proposed a gated autoencoder (HGAE) that defines multiplicative interactions between successive frames and mapping units. Assuming time consistencies in high-order mapping units, they suggest that temporal prediction problems can be considered as learning and inferring transformations between successive images. Srivastava et al."}, {"heading": "3 Proposed Architectures and Training Method", "text": "The goal of our architectures is to learn a function f: xt \u2212 k + 1: t, zt \u2192 xt + 1, where xt and zt are the frame and action variables at the time t and xt \u2212 k + 1: t are the frames from time t \u2212 k + 1 to time t, i.e. the last k frames. Figure 1 shows our two architectures, each consisting of coding layers, extracting spatial and temporal characteristics from the input frames (Section 3.1), action-related transformation layers that transform the encoded characteristics into a prediction of the next frame in the high-level feature space by introducing action variables as additional input (Section 3.2) and finally decrypting layers that map the predicted high-level characteristics in pixels (Section 3.3)."}, {"heading": "3.1 Feedforward encoding and Recurrent encoding", "text": "We present two different types of encoding architecture: feedback encoding and recursive encoding, as shown in Figure 1. Feedback encoding, take a fixed history of earlier frames as input concatenated by channels (see Figure 1a), and stacked folding layers extract spatio-temporal features directly from the concatenated frames. The encoded feature vector can henceforth be formulated as follows: henct = CNN (xt \u2212 k + 1: t), (1) where xt \u2212 k + 1: t denotes Rk \u00b7 n \u00b7 m k frames of n \u00b7 m pixel images. CNN is a mapping of raw concatenated pixels to a high-level feature vector that uses multiple convolution layers, each followed by a rectifier-non-linearity (20], and a fully linked layer of temporary images. CNN is a mapping of concatenated pixels to a high-level configuration layer, each of which uses a high-fixed vector to a high-fidelity vector, and a high-fidelity vector of multiple features."}, {"heading": "3.2 Multiplicative Action-Conditional Transformation", "text": "The transformation layer should be able to predict different frames for different agent actions. A simple approach would be to simply concatenate the action into the encoded feature vector and use a fully connected layer to map the predicted feature vector. In this approach, we suggest multiplicative interactions between the encoded feature vector and the control variables as follows: hdect, i = sE j, k Wikjzt, jh enc t, k + bi (3) where the action conditions the transformation, hdect the multiplicative interactions between the encoded feature vector and the control variables as follows: hdect, i = sE, k Wikjzt, jh enc, k + B (3) where henceforth the function is encoded, hdect the function is transformed."}, {"heading": "3.3 Convolutional Decoding", "text": "It was recently shown that a CNN is capable of producing an image that has fully specified properties of the image [1]. Inspired by this idea, we apply this idea to our end-to-end deep architecture for video prediction. In our method, folding filters are used to decipher high-level features encoded and transformed by CNN, not attributes. More specifically, the transformed feature vector hdec is decoded by repeatedly applying 2 x 2 upsampling and a folding layer in pixels."}, {"heading": "3.4 Incremental Training", "text": "Considering the training data D = {(xn1, z n 1),..., (xnTn, z n Tn)} N = 1, the model is trained to minimize the sum of the square loss of K-step predictions as follows: LK (\u03b8) = 12 \u2211 n \u0445 t K \u2211 k = 1 minute prediction x-nt + k \u2212 xnt + k, where x-nt + k is a k-step future forecast. Intuitively, the network is repeatedly unrolled by K-time steps, using its prediction as input for the next time step. The model is trained in several phases based on the increase of K, as proposed by Michalski et al. [16]. In other words, the model is trained to predict short-term future frames and fine-tuned to predict longer-term future frames after the previous phase. A mini-batch gradient propagation with backagation over time (BT) is used to optimize the training sequence (parameter)."}, {"heading": "4 Experiments", "text": "In fact, most of them will be able to play by the rules they have been playing by over the last five years, and they will be able to play by the rules they have been playing by over the last five years."}, {"heading": "4.1 Evaluation of Predicted Frames", "text": "In fact, most of us are able to outdo ourselves, \"he told the German Press Agency.\" But it's not that we have to get involved in such a situation. \"He added,\" It's not that we get involved in such a situation. \"He added,\" It's not that we get involved in such a situation. \"He added,\" But it's not that we get involved in such a situation. \"He added,\" It's not that we get involved in such a situation. \"He added,\" It's not that we get involved in such a situation. \""}, {"heading": "4.2 Evaluating Usefulness of Predictions for Control", "text": "Since squared loss does not measure how meaningful the predictions are for playing the games, we implement an alternative evaluation method that uses the prediction model to replace the game emulator as follows. A DQN controller that takes the last four frames is first trained using real frames and then used to play the games according to = 0.05 \u2212 greedy policy in which the input frames are generated by our prediction model instead of the game emulator. To evaluate how the depth of the predictions affects the quality of the control, we initialize the predictions using the true last frames after each n-step of the prediction for 1 \u2264 n \u2264 100. Note that the DQN controller never sees a true frame, only the results of our prediction models will be shown in Figure 5."}, {"heading": "4.3 Analysis of Learned Representations", "text": "In factor multiplicative interaction, each action is transformed linearly to f-factors (Wzz in equation 4). In figure 7, we present the cosine2The size of trajectory memory is 200 for QBert and 20 for the other games, \u03b4 is 0 for Freeway and 50 for the other, \u03c3 is 100 for all games. We use our feedback encoding architecture to predica.similarity between each pair of action-factor representations after training in Seaquest. \"N\" and \"F\" correspond to \"no-operation\" and \"fire.\" Black arrows and white arrows correspond to movements with or without \"fire.\" It turns out that there are strong positive correlations between actions that have the same motion directions as \"up\" and \"up + fire.\" There are negative correlations between actions that have opposite motion directions such as \"up + right\" and \"down + left.\""}, {"heading": "5 Conclusion", "text": "In this paper, two new deep architectures were presented that predict future frames that depend on action, and that have shown qualitatively and quantitatively that they are capable of predicting visually realistic and controllable frames over 100-step futures on multiple Atari game domains. To our knowledge, this is the first paper that shows good deep predictions in Atari games. As our architectures were domain independent, we expect that they will extend to many vison-based RL problems. In future work, we will learn models that evaluate the performance of our architectures in model-based RL in addition to predicting future frames."}, {"heading": "B Squared Loss", "text": "3.3 3.3 3.7 3.7 3.7 3.7 3.7 3.7 3.7 3.7 3.7 3.7 3.7 3.7 3.7 3.7 3.7 3.7 3.8 4.7 10.7 11.7 11.7 11.9 12.7 12.7 1.10.9 12.7 1.8 3.7 3.7 3.7 3.7 3.7 3.7 3.7 5.7 5.7 5.7 5.7 5.7 5.7 5.7 5.7 5.7 5.7 5.7 5.7 5.7 5.7 5.7 5.7 5.7 5.7 5.7 5.7 5.7 5.7"}, {"heading": "C Correlation between actions", "text": "N \"N\" Freeway N \"N\" Ms Pacman N \"F\" N \"F\" QBert N \"F\" N \"F\"! \"\" # \"$\" Space Invaders!! N! F! N! F! F!!! \"! #! $!%! &! (!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"}, {"heading": "E Prediction video on down-sampled images", "text": "naLinear Step naFf 146 147 148 \u00ed + fire + fire. \"naLinear Step naFf 143 146 147 148 \u00ed + fire + fire.\" naLinear Step naFf Feedforward Recurrent Ground Truth Ac7on 49 51 53 54 56 \u00e9 + fire \u00e9 and the enemies fire (a) Seaquest (49).naLinear Step naFf Feedforward Recurrent Ground Truth Ac7on 49 51 53 54 56 \u00e9 + fire \u00e9 \u00e9 \u00e9 \u00e9 \u00e9 (a) Seaquest (49).naFile The submarine fills the oxygen tank on the surface of the sea (the oxygen level increases).naLinear Step naFace 3 142 144 147 148 147 147 147 147 148 + fire (a)."}, {"heading": "F Prediction video on original images", "text": "The consequences are catastrophic. - The consequences are catastrophic. - The consequences are catastrophic. - The consequences are catastrophic. - The consequences are catastrophic. - The consequences are catastrophic. - The consequences are catastrophic. - The consequences are catastrophic. - The consequences are catastrophic. - The consequences are catastrophic. - The consequences are catastrophic. - The consequences are catastrophic. - The consequences are catastrophic. - The consequences are catastrophic. - The consequences are catastrophic. - The consequences are catastrophic. - The consequences are catastrophic and catastrophic. - The consequences are catastrophic. - The consequences are catastrophic. - The consequences are catastrophic."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "Motivated by vision-based reinforcement learning (RL) problems, in particular Atari games from the recent benchmark Aracade Learning Environment (ALE), we consider spatio-temporal prediction problems where future (image-)frames are dependent on control variables or actions as well as previous frames. While not composed of natural scenes, frames in Atari games are high-dimensional in size, can involve tens of objects with one or more objects being controlled by the actions directly and many other objects being influenced indirectly, can involve entry and departure of objects, and can involve deep partial observability. We propose and evaluate two deep neural network architectures that consist of encoding, actionconditional transformation, and decoding layers based on convolutional neural networks and recurrent neural networks. Experimental results show that the proposed architectures are able to generate visually-realistic frames that are also useful for control over approximately 100-step action-conditional futures in some games. To the best of our knowledge, this paper is the first to make and evaluate long-term predictions on high-dimensional video conditioned by control inputs.", "creator": "LaTeX with hyperref package"}}}