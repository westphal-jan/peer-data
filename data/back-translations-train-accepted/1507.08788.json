{"id": "1507.08788", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jul-2015", "title": "Fast Stochastic Algorithms for SVD and PCA: Convergence Properties and Convexity", "abstract": "We study the convergence properties of the VR-PCA algorithm introduced by \\cite{shamir2015stochastic} for fast computation of leading singular vectors. We prove several new results, including a formal analysis of a block version of the algorithm, and convergence from random initialization. We also make a few observations of independent interest, such as how pre-initializing with just a single exact power iteration can significantly improve the runtime of stochastic methods, and what are the convexity and non-convexity properties of the underlying optimization problem.", "histories": [["v1", "Fri, 31 Jul 2015 07:57:18 GMT  (362kb,D)", "http://arxiv.org/abs/1507.08788v1", "35 pages, 2 figures"]], "COMMENTS": "35 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.LG cs.NA math.NA math.OC stat.ML", "authors": ["ohad shamir"], "accepted": true, "id": "1507.08788"}, "pdf": {"name": "1507.08788.pdf", "metadata": {"source": "CRF", "title": "Fast Stochastic Algorithms for SVD and PCA: Convergence Properties and Convexity", "authors": ["Ohad Shamir"], "emails": ["ohad.shamir@weizmann.ac.il"], "sections": [{"heading": "1 Introduction", "text": "We look at the problem of restoring the top k links vectors of a d \u00b7 n matrix X = (x1,., xn), where k d. \"This is equivalent to restoring the top k eigenvectors of XX >, or equivalent, the optimization problem in W-Rd \u00b7 k: W > W = I-W > (1n), which require a different eigencomponent analysis. (1) This is one of the most basic matrix calculation problems and has numerous applications (such as low-level matrix approximation and main component analysis). For large-scale matrices X, where exact eigendecomposition is not feasible, deterministic approaches based on power iterations or variants of thereof (e.g. the Lanczos method) are available. Alternatively, we can exploit the structure of Eq (1) and apply stochastic iterms where we update each."}, {"heading": "2 Some Preliminaries and Notation", "text": "We consider a d \u00b7 n matrix X composed of n columns (x1,., xn), and letA = 1n XX > = 1n n n \u2211 i = 1 xix > i. Thus, Equation (1) is equivalent to finding the k leading eigenvectors of A. We generally use bold letters to denote vectors, and uppercase letters to denote matrices. We have Tr (\u00b7) denote the trace of a matrix, and we write B 0 to denote the spectral norm. A symmetrical d \u00b7 d matrix B is positive semidefinit.B 0 means that B is positive if infz-Rd z z z z > Bz \u2265 0. A is defined positively if the inequality is severe. After the standard notation, we write B 0 to indicate that A is positive semidefinit.B 0 means that B is double positive, if the function F is strong, then it is a function for certain I and always itv is defined."}, {"heading": "3 The VR-PCA Algorithm and a Block Version", "text": "We start with the renaming back to the algorithmic nomenclature which we have adopted. (...) We start with the renaming back to the basic algorithmic terms (...). (...) We start with the renaming back to the basic algorithmic terms (...). (...) We start with the renaming back to the basic algorithmic terms (...). (...) We start with the renaming back to the basic algorithmic terms (...). (...) We start with the renaming back to the basic algorithmic terms (...). (...) We start with the renaming back to the basic algorithmic nomenclature. (...) We start with the renaming back to the basic nomenclature. (...) We start with the renaming back to the basic nomenclature."}, {"heading": "4 Warm-Start and the Power of a Power Iteration", "text": "In this section, we examine the time it takes to find a starting point for the satisfaction of the conditions of Theorem 1. < < p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"."}, {"heading": "5 Convexity and Non-Convexity of the Rayleigh Quotient", "text": "iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii"}, {"heading": "6 Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Proof of Theorem 1", "text": "Although the evidence structure generally imitates the proof of theorem 1 in [19] for the special case k = 1, it is more complicated and requires several new technical tools. To simplify the presentation of the evidence, we start with the proof of a series of auxiliary terminals in Section 6.1.1 and then proceed to the main proof in Section 6.1. The main proof itself is divided into several steps, each of which represents one or more lemmates.In the course of the evidence, we use the well-known facts that for all matrices B, C, D are dimensions more suitable Tr (B + C) = Tr (B) + Tr (C), Tr (BC) = Tr (CB), Tr (BCD) = Tr (DBC) and Tr (B > B) = Hb2F. Moreover, since Tr is a linear operation, E [Tr (B) = E [Tr (B)]] is for a random matrix B."}, {"heading": "6.1.1 Auxiliary Lemmas", "text": "It is sufficient to prove that for all positive semidefinitive matrices E, G, that Tr (BD) \u2265 Tr (EG) \u2265 0, G = D (in this case, Tr (BC) = Tr (B \u2212 D)) + Tr (BD) \u2265 Tr (B \u2212 D)))), orE = D, G = C (in this case, Tr (BC) = Tr (B \u2212 D) C) + Tr (B \u2212 D) C).Any positive semidefinitive matrixM can be written as productive M1 / 2M1 / 2 for some symmetrical matrix M1 / 2 (known as matrix M1 / 2)."}, {"heading": "6.1.2 Main Proof", "text": "To simplify the technical derivatives, it should be noted that the algorithm remains the same if we divide each number by the number of points and multiply it by r. Da maxi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi (> W) multiplies. Da maxi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi (> V), and with an inherent gap of xi xi xi xi xi xi xi xi xi xi xi xi (+ r), we can easily analyze the algorithm, assuming that maxi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi (and r instead of xi xi xi xi xi xi xi xi xi xi xi (2), in order to obtain a result that applies to the data with squared norm at most r.Part I: Establishing a stochastic Recurrence RelationWe begin by focusing on a single Iteration of the WK, the number of the W2K, and the number of the W2K, and the number of the W2V."}, {"heading": "6.2 Proof of Theorem 2", "text": "The proof relies mainly on the techniques and lemas of Section 6.1, used to prove theorem 1. (As done in Section 6.1, we will assume without loss of generality that r = maxi xi xi \u00b2 2 is at most 1, and then the limit to a general r (see the discussion at the beginning of Section 6.1.2). First, we will extract the following result, which is essentially the first part of Lemma 11 (for k = 1): Lemma 15. Let A, wt be defined as in Algorithm 1, and assume that the discussion [0, 123]. ThenEit [1 \u2212 < v1, wt + 1 > 2 [for k = 1]."}, {"heading": "6.3 Proof of Theorem 4", "text": "For the simplicity of notation (w > Aw), we are the A-group of FA, and simply refer to F > > A-group (w = > 2). < W-group (w): A-group (w), A-group (w), D-group (w), D-group (w), D-group (w), D-group (w), D-group (W), D-group (W), D-group (W), D-group (W), D-group (W), D-group (W), D-group (W), D-group (W), D-group (W), D-group (W), D-group (W), D-group (W), D-group (W), D-group (W), D-group (W), D-group (W), D-group (W), D-group (W), D-group (W), D-group (W), D-group (W), D-group (W), D-group (W), D-group (W), W-group (W), D-group (W), W-group (W), W-group (W), W-group (W), W-group (W), W-group (W-group (W), W-group (W), W-group (W-group (W), W-group (W-group (W), W-group (W), W-group (W-group (W), W-group (W-group (W), W-group (W-group, W-group (W), W-group (W-group, W-group, W-group, W-D-group, W-group, W-D-group, W-group, W-D group (W group, W-D group), W-D group (W-D group, W group, W-D-D group, W group, W-D group, W-D group, W group (W group), W group (W group, W-D-D-D-W group, W group, W group (W group, W group), W-D-"}, {"heading": "Acknowledgments", "text": "This research is supported in part by a Marie Curie CIG Scholarship under the 7th Research Framework Programme, the Intel ICRI-CI Institute and the Israel Science Foundation."}], "references": [{"title": "A lower bound for the optimization of finite sums", "author": ["A. Agarwal", "L. Bottou"], "venue": "ICML", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Stochastic optimization for PCA and PLS", "author": ["R. Arora", "A. Cotter", "K. Livescu", "N. Srebro"], "venue": "2012 50th Annual Allerton Conference on Communication, Control, and Computing", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Stochastic optimization of PCA with capped MSG", "author": ["R. Arora", "A. Cotter", "N. Srebro"], "venue": "NIPS", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "The fast convergence of incremental PCA", "author": ["A. Balsubramani", "S. Dasgupta", "Y. Freund"], "venue": "NIPS", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Convex optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": "Cambridge university press", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Global convergence of stochastic gradient descent for some nonconvex matrix problems", "author": ["C. De Sa", "K. Olukotun", "C. R\u00e9"], "venue": "ICML", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Matrix computations", "author": ["G. H Golub", "C. Van Loan"], "venue": "volume 3. John Hopkins University Press", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["N. Halko", "P. Martinsson", "J. Tropp"], "venue": "SIAM review, 53(2):217\u2013288", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "The noisy power method: A meta algorithm with applications", "author": ["M. Hardt", "E. Price"], "venue": "NIPS", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["W. Hoeffding"], "venue": "Journal of the American statistical association, 58(301):13\u201330", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1963}, {"title": "Matrix analysis", "author": ["R. Horn", "C. Johnson"], "venue": "Cambridge university press", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["R. Johnson", "T. Zhang"], "venue": "NIPS", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "The method of stochastic approximation for the determination of the least eigenvalue of a symmetrical matrix", "author": ["T.P. Krasulina"], "venue": "USSR Computational Mathematics and Mathematical Physics, 9(6):189\u2013195", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1969}, {"title": "Estimating the largest eigenvalue by the power and lanczos algorithms with a random start", "author": ["J. Kuczynski", "H. Wozniakowski"], "venue": "SIAM journal on matrix analysis and applications, 13(4):1094\u20131122", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1992}, {"title": "Stochastic proximal gradient descent with acceleration techniques", "author": ["A. Nitanda"], "venue": "NIPS", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Simplified neuron model as a principal component analyzer", "author": ["E. Oja"], "venue": "Journal of mathematical biology, 15(3):267\u2013273", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1982}, {"title": "Sampling from large matrices: An approach through geometric functional analysis", "author": ["M. Rudelson", "R. Vershynin"], "venue": "Journal of the ACM (JACM), 54(4):21", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "A stochastic PCA and SVD algorithm with an exponential convergence rate", "author": ["O. Shamir"], "venue": "ICML", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Sketching as a tool for numerical linear algebra", "author": ["D. Woodruff"], "venue": "Theoretical Computer Science, 10(1- 2):1\u2013157", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 17, "context": "Abstract We study the convergence properties of the VR-PCA algorithm introduced by [19] for fast computation of leading singular vectors.", "startOffset": 83, "endOffset": 87}, {"referenceID": 6, "context": "the Lanczos method) [8].", "startOffset": 20, "endOffset": 23}, {"referenceID": 12, "context": "Such algorithms have been known for several decades ([14, 17]), and enjoyed renewed interest in recent years, e.", "startOffset": 53, "endOffset": 61}, {"referenceID": 15, "context": "Such algorithms have been known for several decades ([14, 17]), and enjoyed renewed interest in recent years, e.", "startOffset": 53, "endOffset": 61}, {"referenceID": 1, "context": "[2, 4, 3, 10, 6].", "startOffset": 0, "endOffset": 16}, {"referenceID": 3, "context": "[2, 4, 3, 10, 6].", "startOffset": 0, "endOffset": 16}, {"referenceID": 2, "context": "[2, 4, 3, 10, 6].", "startOffset": 0, "endOffset": 16}, {"referenceID": 8, "context": "[2, 4, 3, 10, 6].", "startOffset": 0, "endOffset": 16}, {"referenceID": 5, "context": "[2, 4, 3, 10, 6].", "startOffset": 0, "endOffset": 16}, {"referenceID": 7, "context": "[9, 20].", "startOffset": 0, "endOffset": 7}, {"referenceID": 18, "context": "[9, 20].", "startOffset": 0, "endOffset": 7}, {"referenceID": 17, "context": "Recently, [19] proposed a new practical algorithm, VR-PCA, for solving Eq.", "startOffset": 10, "endOffset": 14}, {"referenceID": 11, "context": "The algorithm is based on a recent variance-reduction technique designed to speed up stochastic algorithms for convex optimization problems ([13]), although the optimization problem in Eq.", "startOffset": 141, "endOffset": 145}, {"referenceID": 17, "context": "See Section 3 for a more detailed description of this algorithm, and [19] for more discussions as well as empirical results.", "startOffset": 69, "endOffset": 73}, {"referenceID": 17, "context": "The results and analysis in [19] left several issues open.", "startOffset": 28, "endOffset": 32}, {"referenceID": 17, "context": "Although [19] proposed a variant of the algorithm for that case, and studied it empirically, no analysis was provided.", "startOffset": 9, "endOffset": 13}, {"referenceID": 17, "context": "We begin by recalling the algorithm of [19] for the k = 1 case (Algorithm 1), and then discuss its generalization for k > 1.", "startOffset": 39, "endOffset": 43}, {"referenceID": 17, "context": "We now turn to provide a formal analysis of Algorithm 2, which directly generalizes the analysis of Algorithm 1 given in [19]: Theorem 1.", "startOffset": 121, "endOffset": 125}, {"referenceID": 17, "context": "This runtime bound is the same3 as that of [19] for k = 1.", "startOffset": 43, "endOffset": 47}, {"referenceID": 17, "context": "We note that although Bt\u22121 appears essential for our analysis, it isn\u2019t clear that using it is necessary in practice: In [19], the suggested block algorithm was Algorithm 2 with Bt\u22121 = I , which seemed to work well in experiments.", "startOffset": 121, "endOffset": 125}, {"referenceID": 17, "context": "However, experimentally the algorithm seems to work well even with random initialization [19].", "startOffset": 89, "endOffset": 93}, {"referenceID": 5, "context": "Moreover, if we are interested in a theoretical guarantee, one simple solution is to warm-start the algorithm with a purely stochastic algorithm for this problem (such as [6, 10, 4]), with runtime guarantees on getting such a W\u03030.", "startOffset": 171, "endOffset": 181}, {"referenceID": 8, "context": "Moreover, if we are interested in a theoretical guarantee, one simple solution is to warm-start the algorithm with a purely stochastic algorithm for this problem (such as [6, 10, 4]), with runtime guarantees on getting such a W\u03030.", "startOffset": 171, "endOffset": 181}, {"referenceID": 3, "context": "Moreover, if we are interested in a theoretical guarantee, one simple solution is to warm-start the algorithm with a purely stochastic algorithm for this problem (such as [6, 10, 4]), with runtime guarantees on getting such a W\u03030.", "startOffset": 171, "endOffset": 181}, {"referenceID": 17, "context": "[19] showed that it\u2019s possible to further improve the runtime for sparse X , replacing d by the average column sparsity ds.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18]) is a relaxation of the standard notion of rank: For any d \u00d7 d matrix A, nrank(A) is at most the rank of A (which in turn is at most d).", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "For example, this holds for [6], although the bound only guarantees the existence of some iteration which produces the desired output.", "startOffset": 28, "endOffset": 31}, {"referenceID": 3, "context": "The guarantee of [4] scale as d/\u03bb, and the guarantee of [10] scales as d/\u03bb in our setting.", "startOffset": 17, "endOffset": 20}, {"referenceID": 8, "context": "The guarantee of [4] scale as d/\u03bb, and the guarantee of [10] scales as d/\u03bb in our setting.", "startOffset": 56, "endOffset": 60}, {"referenceID": 3, "context": "[4, 10, 6]).", "startOffset": 0, "endOffset": 10}, {"referenceID": 8, "context": "[4, 10, 6]).", "startOffset": 0, "endOffset": 10}, {"referenceID": 5, "context": "[4, 10, 6]).", "startOffset": 0, "endOffset": 10}, {"referenceID": 13, "context": "Indeed, when using deterministic methods such as power iterations or the Lanczos method, the dependence on \u03bb in the runtime is only 1/\u03bb or even \u221a 1/\u03bb [15].", "startOffset": 150, "endOffset": 154}, {"referenceID": 11, "context": "[13, 16, 7] in the context of the variance-reduction technique we use).", "startOffset": 0, "endOffset": 11}, {"referenceID": 14, "context": "[13, 16, 7] in the context of the variance-reduction technique we use).", "startOffset": 0, "endOffset": 11}, {"referenceID": 4, "context": "(1) can be \u201ctrivially\u201d convexified, by re-casting it as an equivalent semidefinite program [5].", "startOffset": 91, "endOffset": 94}, {"referenceID": 0, "context": "whereas the best known guarantees on getting an -optimal solution for \u03bb-strongly convex and smooth functions (see [1]) is on the order of", "startOffset": 114, "endOffset": 117}, {"referenceID": 17, "context": "1 Proof of Theorem 1 Although the proof structure generally mimics the proof of Theorem 1 in [19] for the k = 1 special case, it is more intricate and requires several new technical tools.", "startOffset": 93, "endOffset": 97}, {"referenceID": 0, "context": "Furthermore, suppose that for some fixed \u03b1, \u03b3, \u03b4 > 0, it holds with probability 1 that \u2022 For all \u03bd \u2208 [0, 1], B2 + \u03bdZ2 \u03b4I .", "startOffset": 101, "endOffset": 107}, {"referenceID": 0, "context": "f(\u03bd) = Tr ( (B1 + \u03bdZ1)(B2 + \u03bdZ2) \u22121) , \u03bd \u2208 [0, 1].", "startOffset": 43, "endOffset": 49}, {"referenceID": 6, "context": "[8]), and the solution is easily shown to be B = V U> where USV > is the SVD decomposition of C>D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Letting st, st\u22121 denote the vectors of singular values of V > k Wt and V > k Wt\u22121, and noting that they are both in [0, 1]k (as Vk,Wt\u22121,Wt all have orthonormal columns), the left hand side of the inequality in the lemma statement equals", "startOffset": 116, "endOffset": 122}, {"referenceID": 10, "context": "By Weyl\u2019s matrix perturbation theorem6 [12], this is upper bounded by 2k\u2016V > k Wt \u2212 V > k Wt\u22121\u2016sp \u2264 2k\u2016Vk\u2016sp\u2016Wt \u2212Wt\u22121\u2016sp \u2264 2k\u2016Wt \u2212Wt\u22121\u2016sp.", "startOffset": 39, "endOffset": 43}, {"referenceID": 0, "context": "\u2265 sd in [0, 1], and suppose that sk \u2212 sk+1 \u2265 \u03bb for some \u03bb > 0.", "startOffset": 8, "endOffset": 14}, {"referenceID": 0, "context": "\u2022 B2 + \u03bdZ2 3 8I for all \u03bd \u2208 [0, 1]: Recalling the definition of B2, Z2, and the facts that A 0, N>N 0 (by construction), and W>W = I , we have that B2 I .", "startOffset": 28, "endOffset": 34}, {"referenceID": 0, "context": "But each \u03c3i is in [0, 1] (as Vk,W have orthonormal columns), so no \u03c3i can be less than 12 .", "startOffset": 18, "endOffset": 24}, {"referenceID": 9, "context": "Armed with these facts, and using the maximal version of the Hoeffding-Azuma inequality [11], it follows that with probability at least 1\u2212\u03b2, it holds simultaneously for all t = 1, .", "startOffset": 88, "endOffset": 92}, {"referenceID": 0, "context": "Let A,wt be as defined in Algorithm 1, and suppose that \u03b7 \u2208 [ 0, 1 23 ] .", "startOffset": 60, "endOffset": 71}, {"referenceID": 9, "context": "Armed with these facts, and using the maximal version of the Hoeffding-Azuma inequality [11], it follows that with probability at least 1\u2212 \u03b2, it holds simultaneously for all t = 0, 1, .", "startOffset": 88, "endOffset": 92}], "year": 2015, "abstractText": "We study the convergence properties of the VR-PCA algorithm introduced by [19] for fast computation of leading singular vectors. We prove several new results, including a formal analysis of a block version of the algorithm, and convergence from random initialization. We also make a few observations of independent interest, such as how pre-initializing with just a single exact power iteration can significantly improve the runtime of stochastic methods, and what are the convexity and non-convexity properties of the underlying optimization problem.", "creator": "LaTeX with hyperref package"}}}