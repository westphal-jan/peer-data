{"id": "1508.00625", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Aug-2015", "title": "Sparse PCA via Bipartite Matchings", "abstract": "We consider the following multi-component sparse PCA problem: given a set of data points, we seek to extract a small number of sparse components with disjoint supports that jointly capture the maximum possible variance. These components can be computed one by one, repeatedly solving the single-component problem and deflating the input data matrix, but as we show this greedy procedure is suboptimal. We present a novel algorithm for sparse PCA that jointly optimizes multiple disjoint components. The extracted features capture variance that lies within a multiplicative factor arbitrarily close to 1 from the optimal. Our algorithm is combinatorial and computes the desired components by solving multiple instances of the bipartite maximum weight matching problem. Its complexity grows as a low order polynomial in the ambient dimension of the input data matrix, but exponentially in its rank. However, it can be effectively applied on a low-dimensional sketch of the data; this allows us to obtain polynomial-time approximation guarantees via spectral bounds. We evaluate our algorithm on real data-sets and empirically demonstrate that in many cases it outperforms existing, deflation-based approaches.", "histories": [["v1", "Tue, 4 Aug 2015 00:12:35 GMT  (6911kb)", "http://arxiv.org/abs/1508.00625v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.DS cs.LG math.OC", "authors": ["megasthenis asteris", "dimitris s papailiopoulos", "anastasios kyrillidis", "alexandros g dimakis"], "accepted": true, "id": "1508.00625"}, "pdf": {"name": "1508.00625.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Megasthenis Asteris", "Dimitris Papailiopoulos", "Anastasios Kyrillidis", "Alexandros G. Dimakis"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 150 8.00 625v 1 [stat.ML] 4 A"}, {"heading": "1 Introduction", "text": "This year, we are in a position to take the lead, \"he said in an interview with the German Press Agency.\" We have never lost as much time as this year, \"he said,\" but we are not there yet. \""}, {"heading": "2 Sparse PCA through Bipartite Matchings", "text": "In fact, it is a real problem in which most people are able to play by the rules. (...) It is the only way they are able to play by the rules. (...) It is the only way they are able to play by the rules. (...) It is the only way they are able to play by the rules. (...) It is the only way they are able to play by the rules. (...) It is the only way they are able to play by the rules. (...) It is the only way they are able to play by the rules. (...) It is the only way they are able to play by the rules. (...) It is the only way they are able to play by the rules."}, {"heading": "2.1 Sparse Components via Bipartite Matchings", "text": "At the core of algorithm 1 is algorithm 2, a procedure that resolves the restricted maximization into (6). The algorithm splits the maximization into two stages. First, it identifies the support of the optimal solution. Subsequently, we provide a brief description of the problem of maximum match in a weighted two-part diagram. Then, it restores the exact values of the unequal entries in X. Subsequently, we provide a brief description of algorithm 2, which guarantees in Lemma 2.1.Let Iris, supp (X) support the jth column of X, j."}, {"heading": "3 Sparse PCA on Low-Dimensional Sketches", "text": "It is not possible for us to admit to such a problem (S, r). (D, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S"}, {"heading": "4 Related Work", "text": "(...). (...). (...). (...). (...). It is not that we can agree on a solution. (...). It is not that we can agree on a solution. (...). (...). It is not that we can agree on a solution. (...). (...). (...). It is as if we could agree on a solution. (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (). (...). (...). (...). (...). (...). (...). (...). (...). (...). (. (. (). (). (). (). (...). (). (...). (...). (...). (). (...). (...). (...). (). (...). (). (). (...). (...). (). (). (). ()."}, {"heading": "5 Experiments", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "6 Conclusions", "text": "We have presented a novel algorithm for the joint optimization of several sparse and fragmented components with verifiable approximation guarantees. Our algorithm is combinatorial and utilizes interesting connections between the sparse PCA and the bipartite problem of maximum weight fit. It occurs at a time that grows in the ambient dimension of the input data as a low-order polynomial, but exponentially depends on its rank. To reduce this dependence, we can apply the algorithm to a low-dimensional sketch of the input, which leads to an additional error in our theoretical approximation guarantees. Empirical evaluation of our aluation of our algorithm has shown that it exceeds deflation-based approaches in many cases."}, {"heading": "Acknowledgments", "text": "DP is generously supported by NSF grants CCF-1217058 and CCF-1116404 as well as MURI AFOSR grants 556016. This research was supported by NSF grants CCF 1344179, 1344364, 1407278, 1422549 and ARO YIP W911NF-14-1-0258."}, {"heading": "7 On the sub-optimality of deflation \u2013 An example", "text": "Consider the real 4 \u00b7 4 MatrixA = 1 0; 0; 0; 0; 0; 0; 0; 2; 2; 2; 2; 2; 2; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 8; 5; 5; 5; 5; 5; 5; 5; 5; 5; 5; 5; 5; 5; 5; 5; 5; 5; 0; 0; 0; 0; 6; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8; 8"}, {"heading": "8 Construction of Bipartite Graph", "text": "The following algorithm formally outlines the steps to create the bipartite graph G = ({Uj} kj = 1, V, E) with a weight of d \u00b7 k matrix W. Algorithm 4 Generates bipartite graph input Real d \u00b7 k Matrix W output Bipartite G = ({Uj} kj = 1, V, E) {Fig. 1} 1: for j = 1,..., k do 2: Uj \u2190 {u (j) 1,..., u (j) s} 3: End for 4: U \u2190 kj = 1Uj {| U | = k \u00b7 s} 5: V \u2190 {1,...., d} 6: E \u2190 U \u00b7 V 7: for i = 1,.., d do 8: for j = 1,.., k do 9: for each u-Uj do10: w (u, vi) \u2190 W 2ij 11: End for 12: end for 13: end for 13."}, {"heading": "9 Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9.1 Guarantees of Algorithm 2", "text": "It is not the first time that the EU Commission has set out to determine whether or not it is complying with the rules. (...) It is the first time that the EU Commission has laid down the rules for complying with EU law and EU law. (...) It is the second time that the EU has laid down the rules for complying with EU law. (...) It is the second time that the EU has laid down the rules for complying with EU law. (...) It is the third time that compliance with EU law, EU law and freedom of movement has been enacted. (...) It is the third time that compliance with EU law has been enacted. (...) It is the third time that compliance with EU law, EU right to comply with EU law, EU right to comply with EU law, EU right to comply with EU law and freedom of movement has been enacted. (...) It is the third time that compliance with EU right to comply with EU law, EU right to comply with EU law has been enacted. (...) It is the third time that compliance with EU right to comply with EU law has been enacted."}, {"heading": "9.2 Guarantees of Algorithm 1 \u2013 Proof of Theorem 1", "text": "We first do a more general version of theorem 1 for any number of setpoints. (...) Combining this with the guarantees of algorithm 2, we prove theorem 1 Lemma 9,2. (...) Suppose that there is an operator PX (X), so that PX (W), argmaxX (X), argmaxX (S), wj (S), wj (S), wj (S), wj (S), wj (S), wj (S), wj (S), wj (S), wj (S), wj (S), wj (S), wj (S), wj (S), wjs (S), wj."}, {"heading": "9.3 Guarantees of Algorithm 3 \u2013 Proof of Theorem 2", "text": "We prove that 2-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X X-X-X-X X-X-X-X X X-X X X X X X-X X X X-X X X X X X X-X X X X-X X X X X X-X X X X X X X X X-X X X X X X X X X"}, {"heading": "9.4 Proof of Theorem 3", "text": "First, we restate and prove the following Lemma by [31].Lemma 9,4. Let A-Rd \u00b7 d is a positive semidefinite matrix with entry in [\u2212 1, 1 / r], and definieA-Rd d d matrix such that A = VV. Consider a random matrix R Rd r with entries drawn according to a Gaussian distribution N \u2212 1 / r. The proof is based on the Johnson-Lindenstrauss (JL) Lemma [38], according to which for each unit we have norm-vectors x, y Rd e x x x x x x x x x x x x x j j x x x x j x x x j x x x j x x x x j x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x."}, {"heading": "10 Auxiliary Technical Lemmata", "text": "s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 i \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s s s \u00b2 s \u00b2 s \u00b2 s s s \u00b2 s \u00b2 s \u00b2 s \u00b2 s s s \u00b2 s s s \u00b2 s \u00b2 s"}], "references": [{"title": "The varimax criterion for analytic rotation in factor analysis", "author": ["H.F. Kaiser"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1958}, {"title": "Rotation of principal components: choice of normalization constraints", "author": ["I.T. Jolliffe"], "venue": "Journal of Applied Statistics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1995}, {"title": "A modified principal component technique based on the lasso", "author": ["I.T. Jolliffe", "N.T. Trendafilov", "M. Uddin"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Sparse principal component analysis", "author": ["Hui Zou", "Trevor Hastie", "Robert Tibshirani"], "venue": "Journal of computational and graphical statistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Sparse features for pca-like linear regression", "author": ["Christos Boutsidis", "Petros Drineas", "Malik Magdon-Ismail"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "On the worst-case approximability of sparse PCA", "author": ["Siu On Chan", "Dimitris Papailiopoulos", "Aviad Rubinstein"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Generalized power method for sparse principal component analysis", "author": ["M. Journ\u00e9e", "Y. Nesterov", "P. Richt\u00e1rik", "R. Sepulchre"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Spectral bounds for sparse pca", "author": ["B. Moghaddam", "Y. Weiss", "S. Avidan"], "venue": "Exact and greedy algorithms. NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Optimal solutions for sparse principal component analysis", "author": ["Alexandre d\u2019Aspremont", "Francis Bach", "Laurent El Ghaoui"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Sparse pca: Convex relaxations, algorithms and applications", "author": ["Y. Zhang", "A. d\u2019Aspremont", "L.E. Ghaoui"], "venue": "Handbook on Semidefinite, Conic and Polynomial Optimization,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "A direct formulation for sparse pca using semidefinite programming", "author": ["A. d\u2019Aspremont", "L. El Ghaoui", "M.I. Jordan", "G.R.G. Lanckriet"], "venue": "SIAM review,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Truncated power method for sparse eigenvalue problems", "author": ["Xiao-Tong Yuan", "Tong Zhang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Expectation-maximization for sparse and nonnegative pca", "author": ["Christian D. Sigg", "Joachim M. Buhmann"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Sparse pca through low-rank approximations", "author": ["Dimitris Papailiopoulos", "Alexandros Dimakis", "Stavros Korokythakis"], "venue": "In Proceedings of The 30th International Conference on Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "The sparse principal component of a constant-rank matrix", "author": ["Megasthenis Asteris", "Dimitris S. Papailiopoulos", "Georgios N. Karystinos"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "High-dimensional analysis of semidefinite relaxations for sparse principal components", "author": ["Arash Amini", "Martin Wainwright"], "venue": "In Information Theory,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Sparse principal component analysis and iterative thresholding", "author": ["Zongming Ma"], "venue": "The Annals of Statistics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Approximation bounds for sparse principal component analysis", "author": ["A. d\u2019Aspremont", "F. Bach", "L.E. Ghaoui"], "venue": "arXiv preprint arXiv:1205.0121,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Sparse pca: Optimal rates and adaptive estimation", "author": ["T Tony Cai", "Zongming Ma", "Yihong Wu"], "venue": "arXiv preprint arXiv:1211.1309,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Sparse pca via covariance thresholding", "author": ["Yash Deshpande", "Andrea Montanari"], "venue": "arXiv preprint arXiv:1311.5179,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Optimal detection of sparse principal components in high dimension", "author": ["Quentin Berthet", "Philippe Rigollet"], "venue": "Ann. Statist.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Complexity theoretic lower bounds for sparse principal component detection", "author": ["Q. Berthet", "P. Rigollet"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Statistical and computational trade-offs in estimation of sparse principal components", "author": ["Tengyao Wang", "Quentin Berthet", "Richard J. Samworth"], "venue": "arXiv preprint arXiv:1408.5369,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Do semidefinite relaxations solve sparse PCA up to the information limit", "author": ["Robert Krauthgamer", "Boaz Nadler", "Dan Vilenchik"], "venue": "Annals of Probability,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Deflation methods for sparse pca", "author": ["L. Mackey"], "venue": "NIPS, 21:1017\u20131024,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Minimax rates of estimation for sparse pca in high dimensions", "author": ["Vincent Vu", "Jing Lei"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Optimal sparse linear auto-encoders and sparse PCA", "author": ["Malik Magdon-Ismail", "Christos Boutsidis"], "venue": "CoRR, abs/1502.06626,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "On minimum-cost assignments in unbalanced bipartite graphs", "author": ["Lyle Ramshaw", "Robert E Tarjan"], "venue": "HP Labs, Palo Alto, CA, USA, Tech. Rep. HPL-2012-40R1,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["Nathan Halko", "Per-Gunnar Martinsson", "Joel A Tropp"], "venue": "SIAM review,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "The approximate rank of a matrix and its algorithmic applications: approximate rank", "author": ["Noga Alon", "Troy Lee", "Adi Shraibman", "Santosh Vempala"], "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Nonnegative sparse pca with provable guarantees", "author": ["Megasthenis Asteris", "Dimitris Papailiopoulos", "Alexandros Dimakis"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "On consistency and sparsity for principal components analysis in high dimensions", "author": ["Iain M Johnstone", "Arthur Yu Lu"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2009}, {"title": "Fantope projection and selection: A near-optimal convex relaxation of sparse pca", "author": ["Vincent Q Vu", "Juhee Cho", "Jing Lei", "Karl Rohe"], "venue": "In NIPS,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2013}, {"title": "Nonconvex statistical optimization: minimaxoptimal sparse pca in polynomial time", "author": ["Zhaoran Wang", "Huanran Lu", "Han Liu"], "venue": "arXiv preprint arXiv:1408.5352,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Tight convex relaxations for sparse matrix factorization", "author": ["Emile Richard", "Guillaume R Obozinski", "Jean-Philippe Vert"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Sparse PCA is a useful variant that offers higher data interpretability [1, 2, 3], a property that is sometimes desired even at the cost of statistical fidelity [4].", "startOffset": 72, "endOffset": 81}, {"referenceID": 1, "context": "Sparse PCA is a useful variant that offers higher data interpretability [1, 2, 3], a property that is sometimes desired even at the cost of statistical fidelity [4].", "startOffset": 72, "endOffset": 81}, {"referenceID": 2, "context": "Sparse PCA is a useful variant that offers higher data interpretability [1, 2, 3], a property that is sometimes desired even at the cost of statistical fidelity [4].", "startOffset": 72, "endOffset": 81}, {"referenceID": 3, "context": "Sparse PCA is a useful variant that offers higher data interpretability [1, 2, 3], a property that is sometimes desired even at the cost of statistical fidelity [4].", "startOffset": 161, "endOffset": 164}, {"referenceID": 4, "context": "Furthermore, when the obtained features are used in subsequent learning tasks, sparsity potentially leads to better generalization error [5].", "startOffset": 137, "endOffset": 140}, {"referenceID": 5, "context": "The sparsity constraint makes the problem NP-hard and hence computationally intractable in general, and hard to approximate within some small constant [6].", "startOffset": 151, "endOffset": 154}, {"referenceID": 1, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 111, "endOffset": 153}, {"referenceID": 2, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 111, "endOffset": 153}, {"referenceID": 3, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 111, "endOffset": 153}, {"referenceID": 6, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 111, "endOffset": 153}, {"referenceID": 7, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 111, "endOffset": 153}, {"referenceID": 8, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 111, "endOffset": 153}, {"referenceID": 9, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 111, "endOffset": 153}, {"referenceID": 10, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 111, "endOffset": 153}, {"referenceID": 11, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 111, "endOffset": 153}, {"referenceID": 12, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 111, "endOffset": 153}, {"referenceID": 13, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 111, "endOffset": 153}, {"referenceID": 14, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 111, "endOffset": 153}, {"referenceID": 15, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 253, "endOffset": 289}, {"referenceID": 16, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 253, "endOffset": 289}, {"referenceID": 17, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 253, "endOffset": 289}, {"referenceID": 18, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 253, "endOffset": 289}, {"referenceID": 19, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 253, "endOffset": 289}, {"referenceID": 20, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 253, "endOffset": 289}, {"referenceID": 21, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 253, "endOffset": 289}, {"referenceID": 22, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 253, "endOffset": 289}, {"referenceID": 23, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 253, "endOffset": 289}, {"referenceID": 24, "context": "The scarcity is partially attributed to conventional PCA wisdom: multiple components can be computed one-by-one, repeatedly, by solving the single-component sparse PCA problem (1) and deflating the input data to remove information captured by previously extracted components [25].", "startOffset": 275, "endOffset": 279}, {"referenceID": 24, "context": "Different deflation-based approaches can lead to different outputs: extracted components may or may not be orthogonal, while they may have disjoint or overlapping supports [25].", "startOffset": 172, "endOffset": 176}, {"referenceID": 25, "context": "In the statistics literature, where the objective is typically to recover a \u201ctrue\u201d principal subspace, a branch of work has focused on the \u201csubspace row sparsity\u201d [26], an assumption that leads to sparse components all supported on the same set of variables.", "startOffset": 163, "endOffset": 167}, {"referenceID": 26, "context": "While in [27], the authors discuss an alternative perspective on the fundamental objective of the sparse PCA problem.", "startOffset": 9, "endOffset": 13}, {"referenceID": 5, "context": "Despite the unfavorable dependence on the rank, it is unlikely that a substantial improvement can be achieved in general [6].", "startOffset": 121, "endOffset": 124}, {"referenceID": 27, "context": "The running time is dominated by the computation of the matching, which can be done in O ( |E||U |+ |U |2 log |U | ) using a variant of the Hungarian algorithm [29].", "startOffset": 160, "endOffset": 164}, {"referenceID": 28, "context": "Such a sketch can be obtained in several ways, including for example exact or approximate SVD, or online sketching methods [30].", "startOffset": 123, "endOffset": 127}, {"referenceID": 29, "context": "Using the main matrix approximation result of [31], the next theorem establishes that Algorithm 3 can be turned into an additive PTAS.", "startOffset": 46, "endOffset": 50}, {"referenceID": 30, "context": "If A is a the rank-d SVD approximation of A, then\u2014similar to [32]\u2014we can obtain a multiplicative PTAS for sparse PCA, under the assumption of a decaying spectrum (e.", "startOffset": 61, "endOffset": 65}, {"referenceID": 1, "context": "Representative examples range from early heuristics in [2], to the LASSO based techniques in [3], the elastic net l1-regression in [4], l1 and l0 regularized optimization methods such as GPower in [7], a greedy branch-and-bound", "startOffset": 55, "endOffset": 58}, {"referenceID": 2, "context": "Representative examples range from early heuristics in [2], to the LASSO based techniques in [3], the elastic net l1-regression in [4], l1 and l0 regularized optimization methods such as GPower in [7], a greedy branch-and-bound", "startOffset": 93, "endOffset": 96}, {"referenceID": 3, "context": "Representative examples range from early heuristics in [2], to the LASSO based techniques in [3], the elastic net l1-regression in [4], l1 and l0 regularized optimization methods such as GPower in [7], a greedy branch-and-bound", "startOffset": 131, "endOffset": 134}, {"referenceID": 6, "context": "Representative examples range from early heuristics in [2], to the LASSO based techniques in [3], the elastic net l1-regression in [4], l1 and l0 regularized optimization methods such as GPower in [7], a greedy branch-and-bound", "startOffset": 197, "endOffset": 200}, {"referenceID": 7, "context": "technique in [8], or semidefinite programming approaches [9, 10, 11].", "startOffset": 13, "endOffset": 16}, {"referenceID": 8, "context": "technique in [8], or semidefinite programming approaches [9, 10, 11].", "startOffset": 57, "endOffset": 68}, {"referenceID": 9, "context": "technique in [8], or semidefinite programming approaches [9, 10, 11].", "startOffset": 57, "endOffset": 68}, {"referenceID": 10, "context": "technique in [8], or semidefinite programming approaches [9, 10, 11].", "startOffset": 57, "endOffset": 68}, {"referenceID": 12, "context": "The authors of [13] present an approach that uses ideas from an expectation-maximization (EM) formulation of the problem.", "startOffset": 15, "endOffset": 19}, {"referenceID": 11, "context": "More recently, [12] presents a simple and very efficient truncated version of the power iteration (TPower).", "startOffset": 15, "endOffset": 19}, {"referenceID": 14, "context": "Finally, [15] introduces an exact solver for the low-rank case of the problem; this solver was then used on low-rank sketches in the work of [14] (SpanSPCA), that provides conditional approximation guarantees under spectral assumptions on the input data.", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "Finally, [15] introduces an exact solver for the low-rank case of the problem; this solver was then used on low-rank sketches in the work of [14] (SpanSPCA), that provides conditional approximation guarantees under spectral assumptions on the input data.", "startOffset": 141, "endOffset": 145}, {"referenceID": 15, "context": "Parallel to the algorithmic and optimization perspective, there is large line of statistical analysis for sparse PCA that focuses on guarantees pertaining to planted models and the recovery of a \u201ctrue\u201d sparse component [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 219, "endOffset": 255}, {"referenceID": 16, "context": "Parallel to the algorithmic and optimization perspective, there is large line of statistical analysis for sparse PCA that focuses on guarantees pertaining to planted models and the recovery of a \u201ctrue\u201d sparse component [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 219, "endOffset": 255}, {"referenceID": 17, "context": "Parallel to the algorithmic and optimization perspective, there is large line of statistical analysis for sparse PCA that focuses on guarantees pertaining to planted models and the recovery of a \u201ctrue\u201d sparse component [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 219, "endOffset": 255}, {"referenceID": 18, "context": "Parallel to the algorithmic and optimization perspective, there is large line of statistical analysis for sparse PCA that focuses on guarantees pertaining to planted models and the recovery of a \u201ctrue\u201d sparse component [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 219, "endOffset": 255}, {"referenceID": 19, "context": "Parallel to the algorithmic and optimization perspective, there is large line of statistical analysis for sparse PCA that focuses on guarantees pertaining to planted models and the recovery of a \u201ctrue\u201d sparse component [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 219, "endOffset": 255}, {"referenceID": 20, "context": "Parallel to the algorithmic and optimization perspective, there is large line of statistical analysis for sparse PCA that focuses on guarantees pertaining to planted models and the recovery of a \u201ctrue\u201d sparse component [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 219, "endOffset": 255}, {"referenceID": 21, "context": "Parallel to the algorithmic and optimization perspective, there is large line of statistical analysis for sparse PCA that focuses on guarantees pertaining to planted models and the recovery of a \u201ctrue\u201d sparse component [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 219, "endOffset": 255}, {"referenceID": 22, "context": "Parallel to the algorithmic and optimization perspective, there is large line of statistical analysis for sparse PCA that focuses on guarantees pertaining to planted models and the recovery of a \u201ctrue\u201d sparse component [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 219, "endOffset": 255}, {"referenceID": 23, "context": "Parallel to the algorithmic and optimization perspective, there is large line of statistical analysis for sparse PCA that focuses on guarantees pertaining to planted models and the recovery of a \u201ctrue\u201d sparse component [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 219, "endOffset": 255}, {"referenceID": 31, "context": "Non-deflation-based algorithms include extensions of the diagonal thresholding algorithm [33] and iterative thresholding approaches [17], while [34] and [35] propose methods that rely on the \u201crow sparsity for subspaces\u201d assumption of [26].", "startOffset": 89, "endOffset": 93}, {"referenceID": 16, "context": "Non-deflation-based algorithms include extensions of the diagonal thresholding algorithm [33] and iterative thresholding approaches [17], while [34] and [35] propose methods that rely on the \u201crow sparsity for subspaces\u201d assumption of [26].", "startOffset": 132, "endOffset": 136}, {"referenceID": 32, "context": "Non-deflation-based algorithms include extensions of the diagonal thresholding algorithm [33] and iterative thresholding approaches [17], while [34] and [35] propose methods that rely on the \u201crow sparsity for subspaces\u201d assumption of [26].", "startOffset": 144, "endOffset": 148}, {"referenceID": 33, "context": "Non-deflation-based algorithms include extensions of the diagonal thresholding algorithm [33] and iterative thresholding approaches [17], while [34] and [35] propose methods that rely on the \u201crow sparsity for subspaces\u201d assumption of [26].", "startOffset": 153, "endOffset": 157}, {"referenceID": 25, "context": "Non-deflation-based algorithms include extensions of the diagonal thresholding algorithm [33] and iterative thresholding approaches [17], while [34] and [35] propose methods that rely on the \u201crow sparsity for subspaces\u201d assumption of [26].", "startOffset": 234, "endOffset": 238}, {"referenceID": 26, "context": "Magdon-Ismail and Boutsidis [27] discuss the multiple component Sparse PCA problem, propose an alternative objective function and for that problem obtain interesting theoretical guarantees.", "startOffset": 28, "endOffset": 32}, {"referenceID": 34, "context": "Finally, [36] develops a framework for sparse matrix factorizaiton problems, based on a novel atomic norm.", "startOffset": 9, "endOffset": 13}, {"referenceID": 11, "context": "We evaluate our algorithm on a series of real datasets, and compare it to deflation-based approaches for sparse PCA using TPower [12], EM [13], and SpanSPCA [14].", "startOffset": 129, "endOffset": 133}, {"referenceID": 12, "context": "We evaluate our algorithm on a series of real datasets, and compare it to deflation-based approaches for sparse PCA using TPower [12], EM [13], and SpanSPCA [14].", "startOffset": 138, "endOffset": 142}, {"referenceID": 13, "context": "We evaluate our algorithm on a series of real datasets, and compare it to deflation-based approaches for sparse PCA using TPower [12], EM [13], and SpanSPCA [14].", "startOffset": 157, "endOffset": 161}, {"referenceID": 0, "context": "[1] H.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] I.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] I.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Hui Zou, Trevor Hastie, and Robert Tibshirani.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Christos Boutsidis, Petros Drineas, and Malik Magdon-Ismail.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Siu On Chan, Dimitris Papailiopoulos, and Aviad Rubinstein.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] B.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Alexandre d\u2019Aspremont, Francis Bach, and Laurent El Ghaoui.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Xiao-Tong Yuan and Tong Zhang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Christian D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Dimitris Papailiopoulos, Alexandros Dimakis, and Stavros Korokythakis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Megasthenis Asteris, Dimitris S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Arash Amini and Martin Wainwright.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Zongming Ma.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] T Tony Cai, Zongming Ma, and Yihong Wu.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Yash Deshpande and Andrea Montanari.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] Quentin Berthet and Philippe Rigollet.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] Q.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Tengyao Wang, Quentin Berthet, and Richard J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] Robert Krauthgamer, Boaz Nadler, and Dan Vilenchik.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] L.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] Vincent Vu and Jing Lei.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] Malik Magdon-Ismail and Christos Boutsidis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[29] Lyle Ramshaw and Robert E Tarjan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[30] Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[31] Noga Alon, Troy Lee, Adi Shraibman, and Santosh Vempala.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[32] Megasthenis Asteris, Dimitris Papailiopoulos, and Alexandros Dimakis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[33] Iain M Johnstone and Arthur Yu Lu.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[34] Vincent Q Vu, Juhee Cho, Jing Lei, and Karl Rohe.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[35] Zhaoran Wang, Huanran Lu, and Han Liu.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[36] Emile Richard, Guillaume R Obozinski, and Jean-Philippe Vert.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "For the case of unbalanced bipartite graph with |U | = s \u00b7 k < d = |V | the Hungarian algorithm can be modified [29] to compute the maximum weight bipartite matching in time O ( |E||U |+ |U |2 log |U | ) = O ( d \u00b7 (s \u00b7 k)2 ) .", "startOffset": 112, "endOffset": 116}, {"referenceID": 29, "context": "4 Proof of Theorem 3 First, we restate and prove the following Lemma by [31].", "startOffset": 72, "endOffset": 76}], "year": 2015, "abstractText": "We consider the following multi-component sparse PCA problem: given a set of data points, we seek to extract a small number of sparse components with disjoint supports that jointly capture the maximum possible variance. These components can be computed one by one, repeatedly solving the single-component problem and deflating the input data matrix, but as we show this greedy procedure is suboptimal. We present a novel algorithm for sparse PCA that jointly optimizes multiple disjoint components. The extracted features capture variance that lies within a multiplicative factor arbitrarily close to 1 from the optimal. Our algorithm is combinatorial and computes the desired components by solving multiple instances of the bipartite maximum weight matching problem. Its complexity grows as a low order polynomial in the ambient dimension of the input data matrix, but exponentially in its rank. However, it can be effectively applied on a low-dimensional sketch of the data; this allows us to obtain polynomial-time approximation guarantees via spectral bounds. We evaluate our algorithm on real data-sets and empirically demonstrate that in many cases it outperforms existing, deflationbased approaches.", "creator": "LaTeX with hyperref package"}}}