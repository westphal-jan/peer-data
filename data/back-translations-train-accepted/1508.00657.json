{"id": "1508.00657", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Aug-2015", "title": "Improved Transition-based Parsing by Modeling Characters instead of Words with LSTMs", "abstract": "We present extensions to a continuous-state dependency parsing method that makes it applicable to morphologically rich languages. Starting with a high-performance transition-based parser that uses long short-term memory (LSTM) recurrent neural networks to learn representations of the parser state, we replace look-up based word representations with representations constructed based on the orthographic representations of the words, also using LSTMs. This allows statistical sharing across word forms that are similar on the surface. Experiments for morphologically rich languages show that the parsing model benefits from incorporating the character-based encodings of words.", "histories": [["v1", "Tue, 4 Aug 2015 04:36:36 GMT  (146kb,D)", "http://arxiv.org/abs/1508.00657v1", "In Proceedings of EMNLP 2015"], ["v2", "Tue, 11 Aug 2015 17:33:47 GMT  (172kb,D)", "http://arxiv.org/abs/1508.00657v2", "In Proceedings of EMNLP 2015"]], "COMMENTS": "In Proceedings of EMNLP 2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["miguel ballesteros", "chris dyer", "noah a smith"], "accepted": true, "id": "1508.00657"}, "pdf": {"name": "1508.00657.pdf", "metadata": {"source": "CRF", "title": "Improved Transition-Based Parsing by Modeling Characters instead of Words with LSTMs", "authors": ["Miguel Ballesteros", "Chris Dyer", "Noah A. Smith"], "emails": ["chris@marianaslabs.com,", "miguel.ballesteros@upf.edu,", "nasmith@cs.washington.edu"], "sections": [{"heading": "1 Introduction", "text": "This year it has come to the point where it will be able to retaliate, \"he said in an interview with the German Press Agency.\" We have never hesitated so long, \"he said,\" but we have never hesitated so long until we were able to retaliate. \""}, {"heading": "2 Dependency Parser", "text": "We begin by reviewing the parser approach of Dyer et al. (2015), on which our work is based. Like most transition-based parsers, the parser of Dyer et al. can be understood as a sequential manipulation of three data structures: a buffer B initialized with the sequence of words to be analyzed, a stack S with partially created parses, and a list of actions previously performed by the parser. Specifically, the parser implements the arc standard parsing algorithm (Nivre, 2004). At each step t, a transitional measure is applied that modifies these data structures by pushing or cracking words from the stack and the buffer; the operations are shown in Figure 1. In addition to the above discrete transitions, the parser computes a vector representation of the states of B, S, and A; at some point in time, these are denoted by bt, st, and at."}, {"heading": "2.1 Stack LSTMs", "text": "RNNs are functions that read a sequence of vectors incrementally; in time step t, the xt vector is read in and the ht hidden state is calculated using xt and the previous hidden state ht \u2212 1. In principle, this allows information from time steps in the distant past to be retained, but the nonlinear \"squashing\" functions used in the calculation of each ht cause the error signal used in training with backpropagation to disintegrate. LSTMs are a variant of RNNs designed to cope with this \"vanishing gradient\" problem of having an additional memory cell (Hochreiter and Schmidhuber, 1997; Graves, 2013). Past work explains the calculation within an LSTM \u2212 by metaphors of deciding how much of the current input to (it) or to forget (ft)."}, {"heading": "2.2 Composition Functions", "text": "When a REDUCE operation is selected, two tree fragments are bounced from S and combined to form a new tree fragment, which is then bounced back to S. This tree must be embedded as an input vector x. To do this, Dyer et al. (2015) uses a recursive neural network gr (for relation r), which combines the representations of the two sub-trees that have bounced from S (we refer to them by u and v), resulting in a new vector gr (u, v) or gr (v, u) depending on the direction of attachment, the resulting vector embeds the tree fragment in the same space as the words and other tree fragments. This type of composition has been extensively studied in previous work (Socher et al., 2011; Socher et al., 2013b; Hermann and Blunsom, 2013; Socher et al., 2013a); see Dyer et al (2015) for details."}, {"heading": "2.3 Predicting Parser Decisions", "text": "The parser uses a probabilistic model of parser decisions at all times. Leaving A (S, B) the amount of allowed transitions taking into account the stack S and buffer S (i.e. those in which the prerequisites are met; see Figure 1), the probability of the action z-A (S, B) is defined by a log-linear distribution: p (z | pt) = exp (g > z pt + qz) \u2211 z. \"A (S, B) exp (g > z\" pt + qz \") (2) (where gz and qz are parameters associated with each action type z).The parse assumes that the most likely action is always selected from A (S, B).The probabilistic definition allows a parameter estimation for all parameters (W,\" b \"in all three stack LSTMs as well as W, d, g\" and q \") by maximizing the conditional probability of each correct decision."}, {"heading": "2.4 Adding the SWAP Operation", "text": "The parser by Dyer et al. (2015) implemented the most basic version of the arc standard algorithm, which can only generate projective parse trees. To deal with non-projected trees, we also add the SWAP operation, which makes it possible to produce non-projected trees. SWAP operation, first introduced by Nivre (2009), can generate non-projected trees using a transition-based parser. In the parser by Dyer et al. (2015), in addition to oracle transitions, the inclusion of the SWAP operation requires breaking the linearity of the stack by removing tokens that are not located at the top of the stack, but which is easily handled with the LSTM stack. Figure 1 shows how the parser is able to move words from the stack (S) into the buffer (B), thus breaking the linear order of words, which is easily handled with the stack LSTM."}, {"heading": "3 Word Representations", "text": "The main contribution of this paper is the modification of word representation. In this section we present the standard word embedding as in Dyer et al. (2015) and the improvements we have made in creating word embedding based on morphology based on orthographic strings."}, {"heading": "3.1 Baseline: Standard Word Embeddings", "text": "The parser of Dyer et al. generates word representations to represent each input character by concatenating two vectors: a vector representation for each word type (w) and a representation (t) of the POS tag of the token (if used), which is provided as auxiliary input for the parser. 1 A linear map (V) is applied to the resulting vector and, through a component-based ReLU: x = max {0, V [w; t] + b} for words outside the vocabulary, the parser uses a \"UNK\" symbol, which is treated as a separate word during the parser time. This mapping can be displayed schematically, as in Figure 2.1Dyer et al. (2015), including a third input representation learned from a neural language model."}, {"heading": "3.2 Character-based Embeddings of Words", "text": "Our main modification is an alternative to the continuous vector embedding, which represents the words based on bidirectional LSTMs (Ling et al., 2015). When the parser initiates the learning process and fills the buffer with all the words in the sentence, it reads the words character by character from left to right and computes a continuous vector that embeds the string that is the h-vector of the LSTM; we call it \u2192 w. The same process is applied in reverse, embedding a similar continuous vector, starting with the last character and ending with the first (\u2190 w); again, each character is represented by an LSTM cell; then we associate these vectors and a (learned) representation of their vowels to produce the representation w. As shown in \u00a7 3.1, this character is represented with an LSTM cell that is not linear and linear ()."}, {"heading": "4 Experiments", "text": "We applied our parsing model and several variants of it to several parsing tasks and reported on the results below."}, {"heading": "4.1 Data", "text": "To find out whether the character-based representations are able to learn the morphology of words, we applied the parser to morphologically rich languages. To put our results in context with the recent neural network transition-based parsers, we used the parser in the same constellation as Chen and Manning (2014) and Dyer et al. (2015), which means that we also conducted experiments for English and Chinese. We applied our model to the tree banks of the SPMRL Shared Task (Seddah et al., 2013; Seddah and Tsarfaty, 2014): Arabic (Maamouri et al., 2004), Basque (Aduriz et al., 2003), French (Abeille \u0301 et al., 2003), Turkish (Seeker and Kuhn, 2012), Hebrew and Tsarfaty, 2014): Arabic (Sima'an et al., 2001), Hungarian (Sharet al., 2013, Sharet. (Shari., 2010)."}, {"heading": "4.2 Experimental Configurations", "text": "To isolate the improvements offered by the LSTM character encodings, we execute the parser in the following configurations: \u2022 The parser by using only words, as in \u00a7 3.1. [Words] \u2022 The parser by using character-based representations of words with bidirectional LSTMs, as in \u00a7 3.2. [Character-based] \u2022 The parser by using words and POS tags, as in \u00a7 3.1. [Words + POS] \u2022 The parser by using character-based representations of words with bidirectional LSTMs plus POS tags, as in \u00a7 3.2. [Character-based + POS] None of the experimental configurations includes pre-formed word embeddings or additional data resources. All experiments include SWAP transitions, meaning that non-projective trees can be produced in any language.Dimensionality. The full version of our parsing model sets 100 dimensionality states, as we follow the STM-size two."}, {"heading": "4.3 Training Process", "text": "Parameters are randomly initialized (see Dyer et al. (2015) for details) and optimized using stochastic gradient descendence (without minibatches) using derivatives of the negative log probability of the sequence of parsing actions calculated using backpropagation. Training is terminated when the UAS of the learned model no longer improve on the development set and this model is used to parse the test set. No pre-training of any parameters is performed."}, {"heading": "4.4 Results and Discussion", "text": "The results of the study show that most of them are able to move and that they are able to achieve their goals."}, {"heading": "4.4.1 Learned Word Representations", "text": "Figure 4 illustrates an example of learned two-dimensional representations of character-based LSTMs. Clear clusters of past verbs, rounding and other syntactic classes are visible, and the colors in the figure represent the most common POS tags for each word."}, {"heading": "4.4.2 Out-of-Vocabulary Words", "text": "Character-based representation of words is particularly advantageous for words outside the vocabulary (OOV). We tested this specifically by testing the models [Character-based] and [Character-based + POS], in which unknown words are replaced by the string \"UNK\" during the parsing time, rather than having the corresponding character-based representation. Under these conditions, we see major performance failures in languages with high OOV rates: Korean falls to 15.5 LAS (6,708 OOVs of 25,265 chips in the development group); Hungary falls to 5.5 LAS (5,955 OVs of 29,970 chips). Others show a much smaller decline: French (1,252 OVs of 38,603 chips) falls to 0.8 LAS. Table 3 shows all results. Interestingly, this artificially impoverished model is still better than [Words] for all languages (e.g. Korean by 4 LAS)."}, {"heading": "4.4.3 Computational Requirements", "text": "Character-based representations slow down the parser, as the character-based two-dimensional LSTMs need to be compiled for each word of the input set, but at test time these results could be cached. [Words] analyze a sentence in an average of 44 ms, while [Charbased] requires 130 mse.9 Training time is influenced by the same constant and takes several hours to have a competitive model. In terms of memory, [Words] require an average of 300 MB of memory for both training and parsing, while [Character-based] requires 450 MB."}, {"heading": "4.4.4 Comparison with State-of-the-Art", "text": "For all SPMRL languages, we show the results of Ballesteros (2013), who reported results after conducting a careful experiment on automatic morphological character selection. For Turkish, we show the results of Nivre et al. (2006a), which also performed a careful manual morphological character selection. In comparison, our parser performs competitively for all languages that are better in most9We. We use a machine with 32 Intel (R) Xeon (R) CPU E5-2650 at 2,000GHz; the parser runs on a single core of them. As these systems are based on morphological features, we believe that this comparison shows even more strongly that character-based representations are oriented towards morphology."}, {"heading": "5 Related Work", "text": "Character-based representations have been explored in other NLP tasks; for example, dos Santos and Zadrozny (2014) and dos Santos and Guimara (2015) have learned character-level neural representations for POS marking and the recognition of designated units, resulting in a high error reduction in both tasks. Their approach is similar to ours. Many approaches have used character-based models as additional features to improve existing models. Thus, Chrupa\u0142a (2014) has attempted to normalize character-based recurring neural networks in order to integrate tweets. Botha and Blunsom (2014) also show that strains, prefixes and suffixes can be used to learn useful word representations. Our approach learns similar representations by incorporating bidirectional LSTMs. Similarly, Chen et al. (2015) suggests joint learning of character and word embedding in Chinese."}, {"heading": "6 Conclusion", "text": "We have presented several interesting results: First, we add new evidence that character-based representations are useful for NLP tasks. In this paper, we show that they are useful for transitional dependency analysis because they are able to capture morphological information that is insufficient for syntax analysis.The improvements offered by character-based representations using bidirectional LSTMs are strong for agglutinative languages such as Basque, Hungarian, Korean and Turkish, as the inclusion of POS tags is insufficient. This result is important because annotating morphological information is expensive for a tree base.Our model suggests that only dependency annotations and morphological characteristics can be learned from strings. Character-based representations are also a way to overcome the problem outside the vocabulary; without additional resources, they allow the parser to significantly improve performance at high OV rates."}, {"heading": "Acknowledgments", "text": "MB was supported by the European Commission under contract numbers FP7-ICT610411 (Project MULTISENSOR) and H2020RIA-645012 (Project KRISTINA). This research was supported by the U.S. Army Research Laboratory and the U.S. Army Research Office under contract / funding number W911NF-10-1-0533 and NSF IIS-1054319. This work was completed while NS was at the CMU. Thanks to Joakim Nivre, Bernd Bohnet, Fei Liu and Swabha Swayamdipta for useful comments."}], "references": [{"title": "Building a treebank for french", "author": ["Anne Abeill\u00e9", "Lionel Cl\u00e9ment", "Fran\u00e7ois Toussenel."], "venue": "Anne Abeill\u00e9, editor, Treebanks. Kluwer, Dordrecht.", "citeRegEx": "Abeill\u00e9 et al\\.,? 2003", "shortCiteRegEx": "Abeill\u00e9 et al\\.", "year": 2003}, {"title": "Construction of a Basque dependency treebank", "author": ["I. Aduriz", "M.J. Aranzabe", "J.M. Arriola", "A. Atutxa", "A. D\u0131\u0301az de Ilarraza", "A. Garmendia", "M. Oronoz"], "venue": null, "citeRegEx": "Aduriz et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Aduriz et al\\.", "year": 2003}, {"title": "Effective Morphological Feature Selection with MaltOptimizer at the SPMRL 2013 Shared Task", "author": ["Miguel Ballesteros."], "venue": "Proc. of SPMRL-EMNLP.", "citeRegEx": "Ballesteros.,? 2013", "shortCiteRegEx": "Ballesteros.", "year": 2013}, {"title": "Re)ranking Meets Morphosyntax: State-of-the-art Results from the SPMRL 2013 Shared Task", "author": ["Anders Bj\u00f6rkelund", "Ozlem Cetinoglu", "Rich\u00e1rd Farkas", "Thomas Mueller", "Wolfgang Seeker."], "venue": "SPMRL-EMNLP.", "citeRegEx": "Bj\u00f6rkelund et al\\.,? 2013", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2013}, {"title": "Introducing the IMS-Wroc\u0142aw-Szeged-CIS entry at the SPMRL 2014 Shared Task: Reranking and Morpho-syntax", "author": ["Anders Bj\u00f6rkelund", "\u00d6zlem \u00c7etino\u011flu", "Agnieszka Fale\u0144ska", "Rich\u00e1rd Farkas", "Thomas Mueller", "Wolfgang Seeker", "Zsolt Sz\u00e1nt\u00f3"], "venue": null, "citeRegEx": "Bj\u00f6rkelund et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2014}, {"title": "Joint morphological and syntactic analysis for richly inflected languages", "author": ["Bernd Bohnet", "Joakim Nivre", "Igor Boguslavsky", "Richard Farkas", "Filip Ginter", "Jan Hajia."], "venue": "TACL, 1.", "citeRegEx": "Bohnet et al\\.,? 2013", "shortCiteRegEx": "Bohnet et al\\.", "year": 2013}, {"title": "Compositional Morphology for Word Representations and Language Modelling", "author": ["Jan A. Botha", "Phil Blunsom."], "venue": "ICML.", "citeRegEx": "Botha and Blunsom.,? 2014", "shortCiteRegEx": "Botha and Blunsom.", "year": 2014}, {"title": "CoNLLX shared task on multilingual dependency parsing", "author": ["Sabine Buchholz", "Erwin Marsi."], "venue": "Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL), pages 149\u2013164.", "citeRegEx": "Buchholz and Marsi.,? 2006", "shortCiteRegEx": "Buchholz and Marsi.", "year": 2006}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D. Manning."], "venue": "Proc. EMNLP.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Joint learning of character and word embeddings", "author": ["Xinxiong Chen", "Lei Xu", "Zhiyuan Liu", "Maosong Sun", "Huanbo Luan."], "venue": "Proc. IJCAI.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Preparing Korean Data for the Shared Task on Parsing Morphologically Rich Languages", "author": ["Jinho D. Choi."], "venue": "ArXiv e-prints, September.", "citeRegEx": "Choi.,? 2013", "shortCiteRegEx": "Choi.", "year": 2013}, {"title": "Normalizing tweets with edit scripts and recurrent neural embeddings", "author": ["Grzegorz Chrupa\u0142a."], "venue": "Proceedings of ACL.", "citeRegEx": "Chrupa\u0142a.,? 2014", "shortCiteRegEx": "Chrupa\u0142a.", "year": 2014}, {"title": "Joint morphological and syntactic disambiguation", "author": ["Shay B. Cohen", "Noah A. Smith."], "venue": "Proc. EMNLP-CoNLL.", "citeRegEx": "Cohen and Smith.,? 2007", "shortCiteRegEx": "Cohen and Smith.", "year": 2007}, {"title": "Boosting named entity recognition with neural character embeddings", "author": ["Cicero Nogueira dos Santos", "Victor Guimar\u00e3es."], "venue": "Arxiv.", "citeRegEx": "Santos and Guimar\u00e3es.,? 2015", "shortCiteRegEx": "Santos and Guimar\u00e3es.", "year": 2015}, {"title": "Learning character-level representations for part-ofspeech tagging", "author": ["Cicero dos Santos", "Bianca Zadrozny."], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1818\u20131826.", "citeRegEx": "Santos and Zadrozny.,? 2014", "shortCiteRegEx": "Santos and Zadrozny.", "year": 2014}, {"title": "Transitionbased dependency parsing with stack long shortterm memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "Proceedings of ACL.", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Learning precise timing with LSTM recurrent networks", "author": ["Felix A. Gers", "Nicol N. Schraudolph", "J\u00fcrgen Schmidhuber."], "venue": "JMLR, pages 115\u2013143.", "citeRegEx": "Gers et al\\.,? 2002", "shortCiteRegEx": "Gers et al\\.", "year": 2002}, {"title": "Joint hebrew segmentation and parsing using a pcfg-la lattice parser", "author": ["Yoav Goldberg", "Michael Elhadad."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-", "citeRegEx": "Goldberg and Elhadad.,? 2011", "shortCiteRegEx": "Goldberg and Elhadad.", "year": 2011}, {"title": "Training deterministic parsers with non-deterministic oracles", "author": ["Yoav Goldberg", "Joakim Nivre."], "venue": "Transactions of the association for Computational Linguistics, 1:403\u2013414.", "citeRegEx": "Goldberg and Nivre.,? 2013", "shortCiteRegEx": "Goldberg and Nivre.", "year": 2013}, {"title": "A single generative model for joint morphological segmentation and syntactic parsing", "author": ["Yoav Goldberg", "Reut Tsarfaty."], "venue": "ACL, pages 371\u2013379.", "citeRegEx": "Goldberg and Tsarfaty.,? 2008", "shortCiteRegEx": "Goldberg and Tsarfaty.", "year": 2008}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Alex Graves", "Jrgen Schmidhuber."], "venue": "Neural Networks, 18(5-6).", "citeRegEx": "Graves and Schmidhuber.,? 2005", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves."], "venue": "CoRR, abs/1308.0850.", "citeRegEx": "Graves.,? 2013", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "LSTM: A search space odyssey", "author": ["Klaus Greff", "Rupesh Kumar Srivastava", "Jan Koutn\u0131\u0301k", "Bas R. Steunebrink", "J\u00fcrgen Schmidhuber"], "venue": null, "citeRegEx": "Greff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "The role of syntax in vector space models of compositional semantics", "author": ["Karl Moritz Hermann", "Phil Blunsom."], "venue": "Proc. ACL.", "citeRegEx": "Hermann and Blunsom.,? 2013", "shortCiteRegEx": "Hermann and Blunsom.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Dual decomposition for parsing with non-projective head automata", "author": ["Terry Koo", "Alexander M. Rush", "Michael Collins", "Tommi Jaakkola", "David Sontag."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Koo et al\\.,? 2010", "shortCiteRegEx": "Koo et al\\.", "year": 2010}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Tiago Lu\u0131\u0301s", "Lu\u0131\u0301s Marujo", "Ram\u00f3n Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": "In Proc. EMNLP", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "The Penn Arabic Treebank: Building a Large-Scale Annotated Arabic Corpus", "author": ["Mohamed Maamouri", "Ann Bies", "Tim Buckwalter", "Wigdan Mekki."], "venue": "NEMLAR Conference on Arabic Language Resources and Tools.", "citeRegEx": "Maamouri et al\\.,? 2004", "shortCiteRegEx": "Maamouri et al\\.", "year": 2004}, {"title": "Generating typed dependency parses from phrase structure parses", "author": ["Marie-Catherine De Marneffe", "Bill Maccartney", "Christopher D. Manning."], "venue": "Proc. LREC.", "citeRegEx": "Marneffe et al\\.,? 2006", "shortCiteRegEx": "Marneffe et al\\.", "year": 2006}, {"title": "Efficient higher-order crfs for morphological tagging", "author": ["Thomas M\u00fcller", "Helmut Schmid", "Hinrich Sch\u00fctze."], "venue": "Proc. EMNLP.", "citeRegEx": "M\u00fcller et al\\.,? 2013", "shortCiteRegEx": "M\u00fcller et al\\.", "year": 2013}, {"title": "Labeled pseudo-projective dependency parsing with support vector machines", "author": ["Joakim Nivre", "Johan Hall", "Jens Nilsson", "G\u00fclsen Eryi\u011fit", "Svetoslav Marinov."], "venue": "Proceedings of the 10th Conference on Computational Natural Language Learning", "citeRegEx": "Nivre et al\\.,? 2006a", "shortCiteRegEx": "Nivre et al\\.", "year": 2006}, {"title": "Talbanken05: A Swedish treebank with phrase structure and dependency annotation", "author": ["Joakim Nivre", "Jens Nilsson", "Johan Hall."], "venue": "Proceedings of LREC, pages 1392\u20131395, Genoa, Italy.", "citeRegEx": "Nivre et al\\.,? 2006b", "shortCiteRegEx": "Nivre et al\\.", "year": 2006}, {"title": "Incrementality in deterministic dependency parsing", "author": ["Joakim Nivre."], "venue": "Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together.", "citeRegEx": "Nivre.,? 2004", "shortCiteRegEx": "Nivre.", "year": 2004}, {"title": "Non-projective dependency parsing in expected linear time", "author": ["Joakim Nivre."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Vol-", "citeRegEx": "Nivre.,? 2009", "shortCiteRegEx": "Nivre.", "year": 2009}, {"title": "Building a turkish treebank", "author": ["Kemal Oflazer", "Bilge Say", "Dilek Zeynep Hakkani-T\u00fcr", "G\u00f6khan T\u00fcr."], "venue": "Treebanks, pages 261\u2013277. Springer.", "citeRegEx": "Oflazer et al\\.,? 2003", "shortCiteRegEx": "Oflazer et al\\.", "year": 2003}, {"title": "Introducing the spmrl 2014 shared task on parsing morphologically-rich languages", "author": ["Djam\u00e9 Seddah", "Reut Tsarfaty."], "venue": "SPMRL-SANCL 2014.", "citeRegEx": "Seddah and Tsarfaty.,? 2014", "shortCiteRegEx": "Seddah and Tsarfaty.", "year": 2014}, {"title": "Overview of the SPMRL 2013 shared task: cross-framework evaluation of parsing mor", "author": ["Djam\u00e9 Seddah", "Reut Tsarfaty", "Sandra K\u00fcbler", "Marie Candito", "Jinho Choi", "Rich\u00e1rd Farkas", "Jennifer Foster", "Iakes Goenaga", "Koldo Gojenola", "Yoav Goldberg"], "venue": null, "citeRegEx": "Seddah et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Seddah et al\\.", "year": 2013}, {"title": "Making Ellipses Explicit in Dependency Conversion for a German Treebank", "author": ["Wolfgang Seeker", "Jonas Kuhn."], "venue": "Proceedings of the 8th International Conference on Language Resources and Evaluation, pages 3132\u20133139, Istanbul, Turkey. Euro-", "citeRegEx": "Seeker and Kuhn.,? 2012", "shortCiteRegEx": "Seeker and Kuhn.", "year": 2012}, {"title": "Building a Tree-Bank for Modern Hebrew Text", "author": ["Khalil Sima\u2019an", "Alon Itai", "Yoad Winter", "Alon Altman", "Noa Nativ"], "venue": "In Traitement Automatique des Langues", "citeRegEx": "Sima.an et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Sima.an et al\\.", "year": 2001}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Richard Socher", "Eric H. Huang", "Jeffrey Pennington", "Andrew Y. Ng", "Christopher D. Manning."], "venue": "Proc. NIPS.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Richard Socher", "Andrej Karpathy", "Quoc V. Le", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "TACL.", "citeRegEx": "Socher et al\\.,? 2013a", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y. Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts."], "venue": "Proc. EMNLP.", "citeRegEx": "Socher et al\\.,? 2013b", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Transition-based dependency parsing using recursive neural networks", "author": ["Pontus Stenetorp."], "venue": "Proc. NIPS Deep Learning Workshop.", "citeRegEx": "Stenetorp.,? 2013", "shortCiteRegEx": "Stenetorp.", "year": 2013}, {"title": "Towards a bank of constituent parse trees for Polish", "author": ["Marek \u015awidzi\u0144ski", "Marcin Woli\u0144ski."], "venue": "Text, Speech and Dialogue: 13th International Conference (TSD), Lecture Notes in Artificial Intelligence, pages 197\u2014204, Brno, Czech Republic.", "citeRegEx": "\u015awidzi\u0144ski and Woli\u0144ski.,? 2010", "shortCiteRegEx": "\u015awidzi\u0144ski and Woli\u0144ski.", "year": 2010}, {"title": "A latent variable model for generative dependency parsing", "author": ["Ivan. Titov", "James. Henderson."], "venue": "Proceedings of IWPT.", "citeRegEx": "Titov and Henderson.,? 2007", "shortCiteRegEx": "Titov and Henderson.", "year": 2007}, {"title": "Feature-rich part-ofspeech tagging with a cyclic dependency network", "author": ["Kristina Toutanova", "Dan Klein", "Christopher D. Manning", "Yoram Singer."], "venue": "Proc. NAACL.", "citeRegEx": "Toutanova et al\\.,? 2003", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Integrated morphological and syntactic disambiguation for modern hebrew", "author": ["Reut Tsarfaty."], "venue": "Proceedings of the 21st International Conference on computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics:", "citeRegEx": "Tsarfaty.,? 2006", "shortCiteRegEx": "Tsarfaty.", "year": 2006}, {"title": "Hungarian dependency treebank", "author": ["Veronika Vincze", "D\u00f3ra Szauter", "Attila Alm\u00e1si", "Gy\u00f6rgy M\u00f3ra", "Zolt\u00e1n Alexin", "J\u00e1nos Csirik."], "venue": "LREC.", "citeRegEx": "Vincze et al\\.,? 2010", "shortCiteRegEx": "Vincze et al\\.", "year": 2010}, {"title": "Structured training for neural network transition-based parsing", "author": ["David Weiss", "Christopher Alberti", "Michael Collins", "Slav Petrov."], "venue": "Proc. ACL.", "citeRegEx": "Weiss et al\\.,? 2015", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "Joint word segmentation and pos tagging using a single perceptron", "author": ["Yue Zhang", "Stephen Clark."], "venue": "ACL, pages 888\u2013896.", "citeRegEx": "Zhang and Clark.,? 2008a", "shortCiteRegEx": "Zhang and Clark.", "year": 2008}, {"title": "A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing", "author": ["Yue Zhang", "Stephen Clark."], "venue": "Proc. EMNLP.", "citeRegEx": "Zhang and Clark.,? 2008b", "shortCiteRegEx": "Zhang and Clark.", "year": 2008}, {"title": "Chinese parsing exploiting characters", "author": ["Meishan Zhang", "Yue Zhang", "Wanxiang Che", "Ting Liu."], "venue": "ACL (1), pages 125\u2013134.", "citeRegEx": "Zhang et al\\.,? 2013", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}, {"title": "A Neural Probabilistic StructuredPrediction Model for Transition-Based Dependency Parsing", "author": ["Hao Zhou", "Yue Zhang", "Shujian Huang", "Jiajun Chen."], "venue": "ACL.", "citeRegEx": "Zhou et al\\.,? 2015", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 44, "context": "Recently, continuous-state parsers have been proposed, in which the state is embedded as a vector (Titov and Henderson, 2007; Stenetorp, 2013; Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015).", "startOffset": 98, "endOffset": 224}, {"referenceID": 42, "context": "Recently, continuous-state parsers have been proposed, in which the state is embedded as a vector (Titov and Henderson, 2007; Stenetorp, 2013; Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015).", "startOffset": 98, "endOffset": 224}, {"referenceID": 8, "context": "Recently, continuous-state parsers have been proposed, in which the state is embedded as a vector (Titov and Henderson, 2007; Stenetorp, 2013; Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015).", "startOffset": 98, "endOffset": 224}, {"referenceID": 15, "context": "Recently, continuous-state parsers have been proposed, in which the state is embedded as a vector (Titov and Henderson, 2007; Stenetorp, 2013; Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015).", "startOffset": 98, "endOffset": 224}, {"referenceID": 52, "context": "Recently, continuous-state parsers have been proposed, in which the state is embedded as a vector (Titov and Henderson, 2007; Stenetorp, 2013; Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015).", "startOffset": 98, "endOffset": 224}, {"referenceID": 48, "context": "Recently, continuous-state parsers have been proposed, in which the state is embedded as a vector (Titov and Henderson, 2007; Stenetorp, 2013; Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015).", "startOffset": 98, "endOffset": 224}, {"referenceID": 2, "context": "Since morphology provides strong clues for parsing morphologically rich languages (Ballesteros, 2013), our primary extension takes the idea of continuous-state parsing a step farther to be include sensitive to word forms (in past work, continuous-state parsers consistently made use of vector embeddings of words that were independent of the form of the word).", "startOffset": 82, "endOffset": 101}, {"referenceID": 36, "context": "Although this model is provided no supervision in the form of explicit morphological annotation, we find that it gives a large performance increase when parsing morphologically rich languages in the SPMRL datasets (Seddah et al., 2013; Seddah and Tsarfaty, 2014), especially in agglutinative languages and the ones that present extensive case systems.", "startOffset": 214, "endOffset": 262}, {"referenceID": 35, "context": "Although this model is provided no supervision in the form of explicit morphological annotation, we find that it gives a large performance increase when parsing morphologically rich languages in the SPMRL datasets (Seddah et al., 2013; Seddah and Tsarfaty, 2014), especially in agglutinative languages and the ones that present extensive case systems.", "startOffset": 214, "endOffset": 262}, {"referenceID": 15, "context": "Finally, since the Dyer et al. (2015) parser used only a simple arc-standard parsing algorithm, it could not produce nonprojective trees.", "startOffset": 19, "endOffset": 38}, {"referenceID": 33, "context": "a parallel improvement to the parser by including a SWAP operation (Nivre, 2009) (\u00a72.", "startOffset": 67, "endOffset": 80}, {"referenceID": 15, "context": "We begin by reviewing the parsing approach of Dyer et al. (2015) on which our work is based.", "startOffset": 46, "endOffset": 65}, {"referenceID": 32, "context": "In particular the parser implements the arc-standard parsing algorithm (Nivre, 2004).", "startOffset": 71, "endOffset": 84}, {"referenceID": 24, "context": "LSTMs are a variant of RNNs designed to cope with this \u201cvanishing gradient\u201d problem using an extra memory \u201ccell\u201d (Hochreiter and Schmidhuber, 1997; Graves, 2013).", "startOffset": 113, "endOffset": 161}, {"referenceID": 21, "context": "LSTMs are a variant of RNNs designed to cope with this \u201cvanishing gradient\u201d problem using an extra memory \u201ccell\u201d (Hochreiter and Schmidhuber, 1997; Graves, 2013).", "startOffset": 113, "endOffset": 161}, {"referenceID": 16, "context": "This formulation differs slightly from the classic LSTM formulation in that it makes use of \u201cpeephole connections\u201d (Gers et al., 2002) and defines the forget gate so that it sums with the input gate to 1 (Greff et al.", "startOffset": 115, "endOffset": 134}, {"referenceID": 22, "context": ", 2002) and defines the forget gate so that it sums with the input gate to 1 (Greff et al., 2015).", "startOffset": 77, "endOffset": 97}, {"referenceID": 15, "context": "To do this, Dyer et al. (2015) use a recursive neural network gr (for relation r) that composes", "startOffset": 12, "endOffset": 31}, {"referenceID": 15, "context": "Dyer et al. (2015) used the SHIFT and REDUCE operations in their continuous-state parser; we add SWAP.", "startOffset": 0, "endOffset": 19}, {"referenceID": 39, "context": "This kind of composition was thoroughly explored in prior work (Socher et al., 2011; Socher et al., 2013b; Hermann and Blunsom, 2013; Socher et al., 2013a); for details, see Dyer et al.", "startOffset": 63, "endOffset": 155}, {"referenceID": 41, "context": "This kind of composition was thoroughly explored in prior work (Socher et al., 2011; Socher et al., 2013b; Hermann and Blunsom, 2013; Socher et al., 2013a); for details, see Dyer et al.", "startOffset": 63, "endOffset": 155}, {"referenceID": 23, "context": "This kind of composition was thoroughly explored in prior work (Socher et al., 2011; Socher et al., 2013b; Hermann and Blunsom, 2013; Socher et al., 2013a); for details, see Dyer et al.", "startOffset": 63, "endOffset": 155}, {"referenceID": 40, "context": "This kind of composition was thoroughly explored in prior work (Socher et al., 2011; Socher et al., 2013b; Hermann and Blunsom, 2013; Socher et al., 2013a); for details, see Dyer et al.", "startOffset": 63, "endOffset": 155}, {"referenceID": 15, "context": ", 2013a); for details, see Dyer et al. (2015).", "startOffset": 27, "endOffset": 46}, {"referenceID": 15, "context": "In this section, we present the standard word embeddings as in Dyer et al. (2015), and the improvements we made generating word embeddings that are geared towards morphology based on orthographic strings.", "startOffset": 63, "endOffset": 82}, {"referenceID": 26, "context": "Following (Ling et al., 2015), we compute character-based representations which are calculated as continuous-space vector embeddings representing the words based on bidirectional LSTMs (Graves and Schmidhuber, 2005).", "startOffset": 10, "endOffset": 29}, {"referenceID": 20, "context": ", 2015), we compute character-based representations which are calculated as continuous-space vector embeddings representing the words based on bidirectional LSTMs (Graves and Schmidhuber, 2005).", "startOffset": 163, "endOffset": 193}, {"referenceID": 15, "context": "We conjecture that this will give a clear advantage over a single \u201cUNK\u201d token for all the words that the parser does not see during training, as done by Dyer et al. (2015) and other parsers without additional resources.", "startOffset": 153, "endOffset": 172}, {"referenceID": 8, "context": "To put our results in context with the most recent neural network transition-based parsers, we run the parser in the same setup as Chen and Manning (2014) and Dyer et al.", "startOffset": 131, "endOffset": 155}, {"referenceID": 8, "context": "To put our results in context with the most recent neural network transition-based parsers, we run the parser in the same setup as Chen and Manning (2014) and Dyer et al. (2015), which means that we also run experiments for English and Chinese.", "startOffset": 131, "endOffset": 178}, {"referenceID": 36, "context": "SPMRL Shared Task (Seddah et al., 2013; Seddah and Tsarfaty, 2014): Arabic (Maamouri et al.", "startOffset": 18, "endOffset": 66}, {"referenceID": 35, "context": "SPMRL Shared Task (Seddah et al., 2013; Seddah and Tsarfaty, 2014): Arabic (Maamouri et al.", "startOffset": 18, "endOffset": 66}, {"referenceID": 27, "context": ", 2013; Seddah and Tsarfaty, 2014): Arabic (Maamouri et al., 2004), Basque (Aduriz et al.", "startOffset": 43, "endOffset": 66}, {"referenceID": 1, "context": ", 2004), Basque (Aduriz et al., 2003), French (Abeill\u00e9 et al.", "startOffset": 16, "endOffset": 37}, {"referenceID": 0, "context": ", 2003), French (Abeill\u00e9 et al., 2003), German (Seeker and Kuhn, 2012), Hebrew (Sima\u2019an et al.", "startOffset": 16, "endOffset": 38}, {"referenceID": 37, "context": ", 2003), German (Seeker and Kuhn, 2012), Hebrew (Sima\u2019an et al.", "startOffset": 16, "endOffset": 39}, {"referenceID": 38, "context": ", 2003), German (Seeker and Kuhn, 2012), Hebrew (Sima\u2019an et al., 2001), Hungarian (Vincze et al.", "startOffset": 48, "endOffset": 70}, {"referenceID": 47, "context": ", 2001), Hungarian (Vincze et al., 2010), Korean (Choi, 2013), Polish (\u015awidzi\u0144ski and Woli\u0144ski, 2010) and Swedish (Nivre et al.", "startOffset": 19, "endOffset": 40}, {"referenceID": 10, "context": ", 2010), Korean (Choi, 2013), Polish (\u015awidzi\u0144ski and Woli\u0144ski, 2010) and Swedish (Nivre et al.", "startOffset": 16, "endOffset": 28}, {"referenceID": 43, "context": ", 2010), Korean (Choi, 2013), Polish (\u015awidzi\u0144ski and Woli\u0144ski, 2010) and Swedish (Nivre et al.", "startOffset": 37, "endOffset": 68}, {"referenceID": 31, "context": ", 2010), Korean (Choi, 2013), Polish (\u015awidzi\u0144ski and Woli\u0144ski, 2010) and Swedish (Nivre et al., 2006b).", "startOffset": 81, "endOffset": 102}, {"referenceID": 34, "context": "2 We also ran the experiment with the Turkish dependency treebank3 (Oflazer et al., 2003) of", "startOffset": 67, "endOffset": 89}, {"referenceID": 29, "context": "The POS tags were calculated with MarMot tagger (M\u00fcller et al., 2013) by the best performing system of the SPMRL Shared Task (Bj\u00f6rkelund et al.", "startOffset": 48, "endOffset": 69}, {"referenceID": 3, "context": ", 2013) by the best performing system of the SPMRL Shared Task (Bj\u00f6rkelund et al., 2013).", "startOffset": 63, "endOffset": 88}, {"referenceID": 7, "context": "the CoNLL-X Shared Task (Buchholz and Marsi, 2006) and we use gold POS tags when used as it is common with the CoNLL-X data sets.", "startOffset": 24, "endOffset": 50}, {"referenceID": 28, "context": "For English, we used the Stanford Dependency (SD) treebank4 (Marneffe et al., 2006).", "startOffset": 60, "endOffset": 83}, {"referenceID": 7, "context": "the CoNLL-X Shared Task (Buchholz and Marsi, 2006) and we use gold POS tags when used as it is common with the CoNLL-X data sets. Turkish is an agglutinative language. For English, we used the Stanford Dependency (SD) treebank4 (Marneffe et al., 2006).5 For Chinese, we use the Penn Chinese Treebank 5.1 (CTB5) following Zhang and Clark (2008b),6 with gold POS tags.", "startOffset": 25, "endOffset": 345}, {"referenceID": 45, "context": "The POS tags are predicted by using the Stanford Tagger (Toutanova et al., 2003) with an accuracy of 97.", "startOffset": 56, "endOffset": 80}, {"referenceID": 15, "context": "Parameters are initialized randomly (refer to Dyer et al. (2015) for specifics) and optimized using stochastic gradient descent (without minibatches) using derivatives of the negative log likelihood of the sequence of parsing actions computed using backpropagation.", "startOffset": 46, "endOffset": 65}, {"referenceID": 45, "context": "Tsarfaty (2006) and Cohen and Smith (2007) claimed that for Semitic languages, such as Hebrew, determining the correct morphological segmentation is dependent on syntactic context; our character-based representations are capturing the same kind of information and learning it from syntactic context.", "startOffset": 0, "endOffset": 16}, {"referenceID": 12, "context": "Tsarfaty (2006) and Cohen and Smith (2007) claimed that for Semitic languages, such as Hebrew, determining the correct morphological segmentation is dependent on syntactic context; our character-based representations are capturing the same kind of information and learning it from syntactic context.", "startOffset": 20, "endOffset": 43}, {"referenceID": 2, "context": "a transition-based parser, compared with other features such as case, when the POS tags are included (Ballesteros, 2013).", "startOffset": 101, "endOffset": 120}, {"referenceID": 50, "context": "We include greedy transitionbased parsers that, like ours, do not apply a beam search (Zhang and Clark, 2008b) or a dynamic oracle (Goldberg and Nivre, 2013).", "startOffset": 86, "endOffset": 110}, {"referenceID": 18, "context": "We include greedy transitionbased parsers that, like ours, do not apply a beam search (Zhang and Clark, 2008b) or a dynamic oracle (Goldberg and Nivre, 2013).", "startOffset": 131, "endOffset": 157}, {"referenceID": 2, "context": "For all the SPMRL languages we show the results of Ballesteros (2013), who reported results after carrying out a careful automatic morphological feature selection experiment.", "startOffset": 51, "endOffset": 70}, {"referenceID": 2, "context": "For all the SPMRL languages we show the results of Ballesteros (2013), who reported results after carrying out a careful automatic morphological feature selection experiment. For Turkish, we show the results of Nivre et al. (2006a) which also carried out a careful manual morphological feature selection.", "startOffset": 51, "endOffset": 232}, {"referenceID": 2, "context": "90 (Ballesteros, 2013) 88.", "startOffset": 3, "endOffset": 22}, {"referenceID": 3, "context": "21 (Bj\u00f6rkelund et al., 2013) Basque 85.", "startOffset": 3, "endOffset": 28}, {"referenceID": 2, "context": "58 (Ballesteros, 2013) 89.", "startOffset": 3, "endOffset": 22}, {"referenceID": 4, "context": "70 (Bj\u00f6rkelund et al., 2014) French 86.", "startOffset": 3, "endOffset": 28}, {"referenceID": 2, "context": "98 (Ballesteros, 2013) 89.", "startOffset": 3, "endOffset": 22}, {"referenceID": 4, "context": "66 (Bj\u00f6rkelund et al., 2014) German 87.", "startOffset": 3, "endOffset": 28}, {"referenceID": 2, "context": "75 (Ballesteros, 2013) 91.", "startOffset": 3, "endOffset": 22}, {"referenceID": 3, "context": "65 (Bj\u00f6rkelund et al., 2013) Hebrew 80.", "startOffset": 3, "endOffset": 28}, {"referenceID": 2, "context": "01 (Ballesteros, 2013) 87.", "startOffset": 3, "endOffset": 22}, {"referenceID": 4, "context": "65 (Bj\u00f6rkelund et al., 2014) Hungarian 80.", "startOffset": 3, "endOffset": 28}, {"referenceID": 2, "context": "63 (Ballesteros, 2013) 89.", "startOffset": 3, "endOffset": 22}, {"referenceID": 3, "context": "13 (Bj\u00f6rkelund et al., 2013) Korean 88.", "startOffset": 3, "endOffset": 28}, {"referenceID": 2, "context": "06 (Ballesteros, 2013) 89.", "startOffset": 3, "endOffset": 22}, {"referenceID": 4, "context": "27 (Bj\u00f6rkelund et al., 2014) Polish 87.", "startOffset": 3, "endOffset": 28}, {"referenceID": 2, "context": "89 (Ballesteros, 2013) 91.", "startOffset": 3, "endOffset": 22}, {"referenceID": 3, "context": "07 (Bj\u00f6rkelund et al., 2013) Swedish 83.", "startOffset": 3, "endOffset": 28}, {"referenceID": 2, "context": "82 (Ballesteros, 2013) 88.", "startOffset": 3, "endOffset": 22}, {"referenceID": 4, "context": "75 (Bj\u00f6rkelund et al., 2014) Turkish 76.", "startOffset": 3, "endOffset": 28}, {"referenceID": 30, "context": "68 (Nivre et al., 2006a) 77.", "startOffset": 3, "endOffset": 24}, {"referenceID": 25, "context": "55 n/a (Koo et al., 2010) Chinese 85.", "startOffset": 7, "endOffset": 25}, {"referenceID": 15, "context": "70 (Dyer et al., 2015) 87.", "startOffset": 3, "endOffset": 22}, {"referenceID": 15, "context": "70 (Dyer et al., 2015) English 92.", "startOffset": 3, "endOffset": 22}, {"referenceID": 15, "context": "90 (Dyer et al., 2015) 94.", "startOffset": 3, "endOffset": 22}, {"referenceID": 48, "context": "19 (Weiss et al., 2015)", "startOffset": 3, "endOffset": 23}, {"referenceID": 3, "context": "For the SPMRL data sets, the best performing system of the shared task is either (Bj\u00f6rkelund et al., 2013) or (Bj\u00f6rkelund et al.", "startOffset": 81, "endOffset": 106}, {"referenceID": 4, "context": ", 2013) or (Bj\u00f6rkelund et al., 2014), which are consistently better than our system for all languages.", "startOffset": 11, "endOffset": 36}, {"referenceID": 48, "context": "For English, we report (Weiss et al., 2015) and for Chinese, we report (Dyer et al.", "startOffset": 23, "endOffset": 43}, {"referenceID": 15, "context": ", 2015) and for Chinese, we report (Dyer et al., 2015) which is [Words + POS] but with pretrained word embeddings.", "startOffset": 35, "endOffset": 54}, {"referenceID": 3, "context": "For the SPMRL data sets, the best performing system of the shared task is either (Bj\u00f6rkelund et al., 2013) or (Bj\u00f6rkelund et al., 2014), which are consistently better than our system for all languages. Note that the comparison is harsh to our system, which does not use unlabeled data or explicit morphological features nor any combination of different parsers. For Turkish, we report the results of Koo et al. (2010), which only reported unlabeled attachment scores.", "startOffset": 82, "endOffset": 418}, {"referenceID": 12, "context": "Character-based representations have been explored in other NLP tasks; for instance, dos Santos and Zadrozny (2014) and dos Santos and Guimar\u00e3es (2015) learned character level neural representations for POS tagging and named entity recognition, getting a high error reduction in both tasks.", "startOffset": 89, "endOffset": 116}, {"referenceID": 12, "context": "Character-based representations have been explored in other NLP tasks; for instance, dos Santos and Zadrozny (2014) and dos Santos and Guimar\u00e3es (2015) learned character level neural representations for POS tagging and named entity recognition, getting a high error reduction in both tasks.", "startOffset": 124, "endOffset": 152}, {"referenceID": 11, "context": "For instance, Chrupa\u0142a (2014) tried charater-based recurrent neural networks to normalize tweets.", "startOffset": 14, "endOffset": 30}, {"referenceID": 6, "context": "Also, Botha and Blunsom (2014) show that stems, prefixes and suffixes can be used to learn useful word representations.", "startOffset": 6, "endOffset": 31}, {"referenceID": 9, "context": "Similarly, Chen et al. (2015) proposes joint learning of character and word embeddings for Chinese, claiming that characters contain rich internal information.", "startOffset": 11, "endOffset": 30}, {"referenceID": 17, "context": "Goldberg and Tsarfaty (2008) and Goldberg and Elhadad (2011) presented methods for joint segmentation and phrase-structure parsing, by segmenting the words in useful morphologically oriented units.", "startOffset": 33, "endOffset": 61}, {"referenceID": 49, "context": "(2013) presented efforts on phrasestructure Chinese parsing with characters showing that Chinese can be parsed at the character level, and that Chinese word segmentation is useful for predicting the correct POS tags (Zhang and Clark, 2008a).", "startOffset": 216, "endOffset": 240}], "year": 2017, "abstractText": "We present extensions to a continuousstate dependency parsing method that makes it applicable to morphologically rich languages. Starting with a highperformance transition-based parser that uses long short-term memory (LSTM) recurrent neural networks to learn representations of the parser state, we replace lookup based word representations with representations constructed based on the orthographic representations of the words, also using LSTMs. This allows statistical sharing across word forms that are similar on the surface. Experiments for morphologically rich languages show that the parsing model benefits from incorporating the character-based encodings of words.", "creator": "TeX"}}}