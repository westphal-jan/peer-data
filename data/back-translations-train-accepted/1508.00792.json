{"id": "1508.00792", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Aug-2015", "title": "Fixed-point algorithms for learning determinantal point processes", "abstract": "Determinantal point processes (DPPs) offer an elegant tool for encoding probabilities over subsets of a ground set. Discrete DPPs are parametrized by a positive semidefinite matrix (called the DPP kernel), and estimating this kernel is key to learning DPPs from observed data. We consider the task of learning the DPP kernel, and develop for it a surprisingly simple yet effective new algorithm. Our algorithm offers the following benefits over previous approaches: (a) it is much simpler; (b) it yields equally good and sometimes even better local maxima; and (c) it runs an order of magnitude faster on large problems. We present experimental results on both real and simulated data to illustrate the numerical performance of our technique.", "histories": [["v1", "Tue, 4 Aug 2015 14:58:45 GMT  (100kb,D)", "http://arxiv.org/abs/1508.00792v1", "ICML, 2015"], ["v2", "Thu, 8 Oct 2015 20:56:34 GMT  (100kb,D)", "http://arxiv.org/abs/1508.00792v2", "ICML, 2015"]], "COMMENTS": "ICML, 2015", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zelda mariet", "suvrit sra"], "accepted": true, "id": "1508.00792"}, "pdf": {"name": "1508.00792.pdf", "metadata": {"source": "META", "title": "Fixed-point algorithms for learning determinantal point processes", "authors": ["Zelda Mariet", "Suvrit Sra"], "emails": ["ZELDA@CSAIL.MIT.EDU", "SUVRIT@MIT.EDU"], "sections": [{"heading": "1. Introduction", "text": "This year, it will be able to put itself at the top of the group."}, {"heading": "1.1. Background and problem setup", "text": "Without loss of universality, we assume that the basic set of N items {1, 2,.,. Y items (Y items) is useful (Y items), which we denote by Y items. A (discrete) DPP on Y is a probability variable P on 2Y (the set of all subsets of Y), so that for each Y item the probability P (Y items) confirms P (Y items) (LY items); here LY denotes the principal submatrix of the DPP item L, which is induced by indexes in Y. Intuitively, the diagonal input Lii of the kernel matrix L grasps some notions of the meaning of item i, whereas an extra-diagonal input Lij = Lji measures the similarity between item i and j. This intuitive notion provides further motivation for searching for DPs with interlinkages not observed."}, {"heading": "1.2. Learning the DPP Kernel", "text": "The learning task aims to adapt a DPP kernel (either L or equivalent to the marginal K kernel) consistent with a collection of observed subsets. Suppose we obtain as training data n subsets (Y1,.., Yn) of the soil setY. The task is to maximize the probability of these observations. Two equivalent formulations of this maximization task can be taken into account: max. L 0 \u2211 n i = 1 log det (LYi) \u2212 n log det (I + L), (1.3) max. 0 K I \u2211 n i = 1 log (| K \u2212 IY ci). (1.4) We will use formulations (1.3) in this paper. Gillenwater et al. (2014) used (1.4) and used its structure to derive a somewhat complicated EM-like optimization method for optimization. Both (1.3) and (1.4) are not convective and difficult to optimize. For example, the projection method can be applied (1.4) if it is derived from a falsification-oriented problem."}, {"heading": "2. Optimization algorithm", "text": "The method we derive from this has two key components: (i) a fixed-point view that helps us obtain an iteration that meets the crucial positive definition constraint L 0 by construction (2014); and (ii) an implicitly bound optimization analysis that implicitly ensures a monotonic ascendancy; the resulting algorithm is much simpler than the previous EM-style approach by Gillenwater et al. (2014). So, if we write Ui for a suitable N \u00b7 k indicator matrix, we can reduce Ui = U, also known as a compression (U \u0445 denotes the hermitian transpose); we write U \u0432 i LUi interchangeable with LYi, implicitly assuming that suitable indicator matrices Ui are such that we reduce Ui = I | Yi, we will drop the subscript to the identity matrix, with its function clearly defined from the context.3 of the city."}, {"heading": "2.1. Convergence Analysis", "text": "Theorem 2.2 (X # Y) -1 (X # Y) -1 (X # Y) -1 (Lk) -1 (Lk) -1 (Lk) -1 (Lk) -1 (Lk) -1 (Lk) -1 (Lk) -1 (Lk) -1 (Lk) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -2 (S) -1 (S) -1 (S) -2 (S) -2) -2 (S) -2 (S) -2 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -2 (S) -2 (S) -2) -2 (S) -2 (S) -2 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -2 (S) -2 (S) -2 (S) -2 (S (S) -2) -2 (S) -2 (S) -2 (S) -1 (S) -2 (S) -1 (S) -1 (S) -1 (S) -1 (S) -1 (S) -2 (S) -1 (S) -1 (S) -1 (S) -2) -1 ("}, {"heading": "2.2. Iteration cost and convergence speed", "text": "The cost of each iteration of our algorithm is dominated by the calculation of \u2206, which in total costs O (\u2211 n = 1 | Yi | 3 + N3) = O (n\u03ba3 + N3) arithmetic operations, with the cost of O (| Yi | 3) coming from the time required to calculate the reverse L \u2212 1Yi, while the cost of N3 from the calculation (I + L) \u2212 1. In addition, additional costs arise for N3 in the calculation of L \u0445 L.In comparison, each iteration of the method by Gillenwater et al. (2014) costs O (nN\u03ba2 + N3), which is comparable to, albeit slightly larger than O (n\u04413 + N3) than N \u0438. In applications where the sizes of the sample subsets ensure the satisfaction of N, the difference may be more substantial. Furthermore, we do not need implicit / vectorial calculations to implement our algorithmic iteration."}, {"heading": "3. Experimental results", "text": "We are comparing the performance of our algorithm, called Picard Iteration4, with the EM algorithm presented in Gillenwater et al. (2014). We are experimenting with both synthetic 5 and real-world data. For real-world data, we are using the Baby Registry Test, which reports the results (Gillenwater et al., 2014). This data set consists of 111 006 sub-registers describing articles in 13 different categories; this data set was obtained by collecting baby registers from amazon.com, which all contain between 5 and 100 products, and then dividing each register into sub-registers according to which of the 13 categories (such as \"feeding,\" diapers, \"\" toys, \"etc.) each product belongs to the register. (Gillenwater et al., 2014) provides a more detailed description of this data set. These sub-registers are used to learn a DPP that is able to make recommendations for these products: in fact, a DPP is suitable for all of these products as there is potential customer interest."}, {"heading": "3.1. Implementation details", "text": "We measure the convergence by testing the relative change (\u03b5pic = 0.5 \u00b7 \u03b5em) to take into account the fact that the distance between two consecutive log probabilities for the picard iteration tends to be smaller than for the EM.The parameter a for Picard was set at the beginning of each experiment and never changed, as it remains valid during each test.In EM, the step size was initially 4Our nomenclature comes from the usual term for such iterations in fixed point theory (Granas and Dugundji, 2003).5The numbers and tables for the synthetic results have been changed to include some minor corrections: in particular Tables 1 and 2 now show the run time to 99%. The runtimes were originally reported to the final convergence, but incorrectly range from 95%.6Although it was not necessary to use water in our experiments when it becomes necessary for the experiments (1.0) and we can use it until the final convergence."}, {"heading": "3.2. Synthetic tests", "text": "In each experiment sample n training sets from a base DPP of size N, then learn the DPP with EM and the picard iteration respectively required. We initialize the learning process with a random positive definitive matrix L0 (or K0 for EM), which consists of the same distribution as the true DPP kernel. Specifically, we use two matrix distributions to extract the true kernel and the initial matrix values: \u2022 BASIC: We extract the coefficients of a matrix M from the uniform distribution over [0, 2], then return L = MM > conditioned to its positive definition. \u2022 WISHART: We extract L from a wishart distribution with N degrees of freedom and an identity covariance matrix, and rescale it with a factor 1N. Figures 1, 2 and 3 show the log probability as a function of time for different parameter values."}, {"heading": "3.3. Baby registries experiment", "text": "We tested our implementation on all 13 product categories in the baby register dataset, using two different initializations: \u2022 the above-mentioned wishart distribution \u2022 the data-dependent moment matching initialization (MM) described in (Gillenwater et al., 2014). In any case, 70% of the baby registers in the product category were used for training; 30% served as a test. The results shown in Figures 4 and 5 are on average conducted over 5 learning attempts with different starting matrices; parameter a was set at 1.3 for all course variants. Similar to their behavior on synthetic datasets, picard iteration offers significantly shorter runtimes overall when handling large matrices and training sets. As shown in Table 3, the final log probabilities (in the order of magnitude 10 \u2212 2 to 10 \u2212 4) are very close to those achieved by the EM algorithm."}, {"heading": "4. Conclusions and future work", "text": "We approached the problem of the most likely estimation of a DPP nucleus from a different perspective: We analyzed the standard properties of the cost function and used them to obtain a novel fixed point picard iteration. Experiments with both simulated and real data showed that our picard iteration for a number of ground set sizes and sample count is remarkably faster than the best approach to date, while being extremely easy to implement. Especially for large ground set sizes, our experiments show that our picard iteration reduces runtime to a fraction of the previously optimal EM runtime. We presented a theoretical analysis of the convergence properties of picard iteration and found sufficient conditions for its convergence. However, our experiments show that picard iteration converges for a wider range of step sizes (parameters a in iteration and diagrams) than is currently available for our theoretical analysis, particularly because it is a strong part of our future work to develop."}, {"heading": "ACKNOWLEDGMENTS", "text": "Suvrit Sra is partially supported by NSF: IIS-1409802."}, {"heading": "A. Bound on a", "text": "Proposition A.1. Leave L, Ui and \u0394a as defined above; set Z = 1n \u2211 i Ui (U \u0445 i LUi) \u2212 1 U \u0445 i. Define the constant: = max {\u03bbmin (LZ), 1 / \u03bbmax (I + L)}. (A.1) Then define the constant: = max {\u03bbmin (LZ), 1 / \u03bbmax (I + L). Leave Z = 1n \u2211 n i = 1 Ui (U \u0445 i LUi) \u2212 1 U \u0445 i. To ensure that L + aL \u0445 L 0 is displayed as equal, we show L \u2212 1 + a (1 n \u2211 i = 1 Ui (U \u0445 i LUi) \u2212 1) \u2212 1) 0 = \u21d2 I + aL1 / 2 aL (L + I) \u2212 1 \u2212 a (I +.a) I + a (I + Zilla) \u2212 1 (I + \u03bb1) (I) (+ L) (1) (+ L) (1) (1) (\u03bb1) (1) (L) (1)."}], "references": [{"title": "Optimization algorithms on matrix manifolds", "author": ["P.-A. Absil", "R. Mahony", "R. Sepulchre"], "venue": null, "citeRegEx": "Absil et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Absil et al\\.", "year": 2009}, {"title": "Nystr\u00f6m approximation for large-scale Determinantal Point Processes", "author": ["R. Affandi", "A. Kulesza", "E. Fox", "B. Taskar"], "venue": "In Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Affandi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Affandi et al\\.", "year": 2013}, {"title": "Learning the parameters of Determinantal Point Process kernels", "author": ["R. Affandi", "E. Fox", "R. Adams", "B. Taskar"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Affandi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Affandi et al\\.", "year": 2014}, {"title": "Nonlinear Programming", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific, second edition,", "citeRegEx": "Bertsekas.,? \\Q1999\\E", "shortCiteRegEx": "Bertsekas.", "year": 1999}, {"title": "Positive Definite Matrices", "author": ["R. Bhatia"], "venue": null, "citeRegEx": "Bhatia.,? \\Q2007\\E", "shortCiteRegEx": "Bhatia.", "year": 2007}, {"title": "Manopt, a Matlab toolbox for optimization on manifolds", "author": ["N. Boumal", "B. Mishra", "P.-A. Absil", "R. Sepulchre"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Boumal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Boumal et al\\.", "year": 2014}, {"title": "Near-optimal MAP inference for Determinantal Point Processes", "author": ["J. Gillenwater", "A. Kulesza", "B. Taskar"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Gillenwater et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gillenwater et al\\.", "year": 2012}, {"title": "Expectation-Maximization for learning Determinantal Point Processes", "author": ["J. Gillenwater", "A. Kulesza", "E. Fox", "B. Taskar"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Gillenwater et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gillenwater et al\\.", "year": 2014}, {"title": "Determinantal processes and independence", "author": ["J.B. Hough", "M. Krishnapur", "Y. Peres", "B. Vir\u00e1g"], "venue": "Probability Surveys,", "citeRegEx": "Hough et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hough et al\\.", "year": 2006}, {"title": "Near-optimal sensor placements in Gaussian processes: theory, efficient algorithms and empirical studies", "author": ["A. Krause", "A. Singh", "C. Guestrin"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Krause et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2008}, {"title": "Learning with Determinantal Point Processes", "author": ["A. Kulesza"], "venue": "PhD thesis, University of Pennsylvania,", "citeRegEx": "Kulesza.,? \\Q2013\\E", "shortCiteRegEx": "Kulesza.", "year": 2013}, {"title": "k-DPPs: Fixed-size Determinantal Point Processes", "author": ["A. Kulesza", "B. Taskar"], "venue": "In International Conference on Maachine Learning (ICML),", "citeRegEx": "Kulesza and Taskar.,? \\Q2011\\E", "shortCiteRegEx": "Kulesza and Taskar.", "year": 2011}, {"title": "Learning Determinantal Point Processes", "author": ["A. Kulesza", "B. Taskar"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Kulesza and Taskar.,? \\Q2011\\E", "shortCiteRegEx": "Kulesza and Taskar.", "year": 2011}, {"title": "Determinantal Point Processes for machine learning, volume 5", "author": ["A. Kulesza", "B. Taskar"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Kulesza and Taskar.,? \\Q2012\\E", "shortCiteRegEx": "Kulesza and Taskar.", "year": 2012}, {"title": "Learning mixtures of submodular shells with application to document summarization", "author": ["H. Lin", "J. Bilmes"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Lin and Bilmes.,? \\Q2012\\E", "shortCiteRegEx": "Lin and Bilmes.", "year": 2012}, {"title": "Determinantal probability measures", "author": ["R. Lyons"], "venue": "Publications Mathe\u0301matiques de l\u2019Institut des Hautes E\u0301tudes Scientifiques,", "citeRegEx": "Lyons.,? \\Q2003\\E", "shortCiteRegEx": "Lyons.", "year": 2003}, {"title": "The coincidence approach to stochastic point processes", "author": ["O. Macchi"], "venue": "Advances in Applied Probability,", "citeRegEx": "Macchi.,? \\Q1975\\E", "shortCiteRegEx": "Macchi.", "year": 1975}, {"title": "Conic geometric optimization on the manifold of positive definite matrices", "author": ["S. Sra", "R. Hosseini"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Sra and Hosseini.,? \\Q2015\\E", "shortCiteRegEx": "Sra and Hosseini.", "year": 2015}, {"title": "The concave-convex procedure", "author": ["A.L. Yuille", "A. Rangarajan"], "venue": "Neural Comput.,", "citeRegEx": "Yuille and Rangarajan.,? \\Q2003\\E", "shortCiteRegEx": "Yuille and Rangarajan.", "year": 2003}, {"title": "Solving the apparent diversity-accuracy dilemma of recommender systems", "author": ["T. Zhou", "Z. Kuscsik", "J.-G. Liu", "M. Medo", "J.R. Wakeling", "Y.-C. Zhang"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Zhou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 16, "context": "Determinantal point processes (DPPs) arose in statistical mechanics, where they were originally used to model fermions (Macchi, 1975).", "startOffset": 119, "endOffset": 133}, {"referenceID": 10, "context": "Recently, they have witnessed substantial interest in a variety of machine learning applications (Kulesza, 2013; Kulesza and Taskar, 2012).", "startOffset": 97, "endOffset": 138}, {"referenceID": 13, "context": "Recently, they have witnessed substantial interest in a variety of machine learning applications (Kulesza, 2013; Kulesza and Taskar, 2012).", "startOffset": 97, "endOffset": 138}, {"referenceID": 14, "context": ", document summarization (Lin and Bilmes, 2012), object retrieval (Affandi et al.", "startOffset": 25, "endOffset": 47}, {"referenceID": 2, "context": ", document summarization (Lin and Bilmes, 2012), object retrieval (Affandi et al., 2014), recommender systems (Zhou et al.", "startOffset": 66, "endOffset": 88}, {"referenceID": 19, "context": ", 2014), recommender systems (Zhou et al., 2010), and sensor placement (Krause et al.", "startOffset": 29, "endOffset": 48}, {"referenceID": 9, "context": ", 2010), and sensor placement (Krause et al., 2008).", "startOffset": 30, "endOffset": 51}, {"referenceID": 15, "context": "DPPs are also interesting in their own right: they have various combinatorial, probabilistic, and analytic properties, while involving a fascinating set of open problems (Lyons, 2003; Hough et al., 2006; Kulesza, 2013).", "startOffset": 170, "endOffset": 218}, {"referenceID": 8, "context": "DPPs are also interesting in their own right: they have various combinatorial, probabilistic, and analytic properties, while involving a fascinating set of open problems (Lyons, 2003; Hough et al., 2006; Kulesza, 2013).", "startOffset": 170, "endOffset": 218}, {"referenceID": 10, "context": "DPPs are also interesting in their own right: they have various combinatorial, probabilistic, and analytic properties, while involving a fascinating set of open problems (Lyons, 2003; Hough et al., 2006; Kulesza, 2013).", "startOffset": 170, "endOffset": 218}, {"referenceID": 7, "context": "for instance (Gillenwater et al., 2014); (Kulesza and Taskar, 2011b); (Kulesza and Taskar, 2011a); (Affandi et al.", "startOffset": 13, "endOffset": 39}, {"referenceID": 2, "context": ", 2014); (Kulesza and Taskar, 2011b); (Kulesza and Taskar, 2011a); (Affandi et al., 2014); (Affandi et al.", "startOffset": 67, "endOffset": 89}, {"referenceID": 1, "context": ", 2103); (Affandi et al., 2013); (Gillenwater et al.", "startOffset": 9, "endOffset": 31}, {"referenceID": 6, "context": ", 2013); (Gillenwater et al., 2012).", "startOffset": 9, "endOffset": 35}, {"referenceID": 13, "context": "For additional references and material we refer the reader to the survey (Kulesza and Taskar, 2012).", "startOffset": 73, "endOffset": 99}, {"referenceID": 2, "context": ", (Kulesza and Taskar, 2011b;a; Affandi et al., 2014)) learns a full DPP kernel nonparameterically.", "startOffset": 2, "endOffset": 53}, {"referenceID": 0, "context": "Hence one may wonder if instead we could apply more sophisticated manifold optimization techniques (Absil et al., 2009; Boumal et al., 2014).", "startOffset": 99, "endOffset": 140}, {"referenceID": 5, "context": "Hence one may wonder if instead we could apply more sophisticated manifold optimization techniques (Absil et al., 2009; Boumal et al., 2014).", "startOffset": 99, "endOffset": 140}, {"referenceID": 5, "context": ", via the excellent MANOPT toolbox (Boumal et al., 2014), empirically it turns out to be computationally too demanding; the EM strategy of Gillenwater et al.", "startOffset": 35, "endOffset": 56}, {"referenceID": 0, "context": ", 2014); (Kulesza and Taskar, 2011b); (Kulesza and Taskar, 2011a); (Affandi et al., 2014); (Affandi et al., 2103); (Affandi et al., 2013); (Gillenwater et al., 2012). For additional references and material we refer the reader to the survey (Kulesza and Taskar, 2012). Our paper is motivated by the recent work of Gillenwater et al. (2014), who made notable progress on the task of learning a DPP kernel from data.", "startOffset": 68, "endOffset": 339}, {"referenceID": 0, "context": ", 2014); (Kulesza and Taskar, 2011b); (Kulesza and Taskar, 2011a); (Affandi et al., 2014); (Affandi et al., 2103); (Affandi et al., 2013); (Gillenwater et al., 2012). For additional references and material we refer the reader to the survey (Kulesza and Taskar, 2012). Our paper is motivated by the recent work of Gillenwater et al. (2014), who made notable progress on the task of learning a DPP kernel from data. This task is conjectured to be NP-Hard (Kulesza, 2013, Conjecture 4.1). Gillenwater et al. (2014) presented a carefully designed EM-style procedure, which, unlike several previous approaches (e.", "startOffset": 68, "endOffset": 512}, {"referenceID": 0, "context": ", 2014); (Kulesza and Taskar, 2011b); (Kulesza and Taskar, 2011a); (Affandi et al., 2014); (Affandi et al., 2103); (Affandi et al., 2013); (Gillenwater et al., 2012). For additional references and material we refer the reader to the survey (Kulesza and Taskar, 2012). Our paper is motivated by the recent work of Gillenwater et al. (2014), who made notable progress on the task of learning a DPP kernel from data. This task is conjectured to be NP-Hard (Kulesza, 2013, Conjecture 4.1). Gillenwater et al. (2014) presented a carefully designed EM-style procedure, which, unlike several previous approaches (e.g., (Kulesza and Taskar, 2011b;a; Affandi et al., 2014)) learns a full DPP kernel nonparameterically. One main observation of Gillenwater et al. (2014) is that applying projected gradient ascent to the DPP log-likelihood usually results in degenerate estimates (because it involves projection onto the set {X : 0 X I}).", "startOffset": 68, "endOffset": 760}, {"referenceID": 0, "context": "Hence one may wonder if instead we could apply more sophisticated manifold optimization techniques (Absil et al., 2009; Boumal et al., 2014). While this idea is attractive, and indeed applicable, e.g., via the excellent MANOPT toolbox (Boumal et al., 2014), empirically it turns out to be computationally too demanding; the EM strategy of Gillenwater et al. (2014) is more practical.", "startOffset": 100, "endOffset": 365}, {"referenceID": 10, "context": "It can also be shown (Kulesza, 2013) that P(Y ) = |det(K \u2212 IY c)|, where IY c is a partial N \u00d7 N identity matrix with diagonal entries in Y zeroed out.", "startOffset": 21, "endOffset": 36}, {"referenceID": 6, "context": "2) of the DPP probability are useful: Gillenwater et al. (2014) used a formulation in terms of K; we prefer (1.", "startOffset": 38, "endOffset": 64}, {"referenceID": 6, "context": "Gillenwater et al. (2014) used (1.", "startOffset": 0, "endOffset": 26}, {"referenceID": 6, "context": "Gillenwater et al. (2014) used (1.4) and exploited its structure to derive a somewhat intricate EM-style method for optimizing it. Both (1.3) and (1.4) are nonconvex and difficult optimize. For instance, using projected gradient on (1.4) may seem tempting, but projection ends up yielding degenerate (diagonal and rank-deficient) solutions which is undesirable when trying to capture interaction between observations\u2014indeed, this criticism motivated Gillenwater et al. (2014) to derive the EM algorithm.", "startOffset": 0, "endOffset": 476}, {"referenceID": 6, "context": "The resulting algorithm is vastly simpler than the previous EM-style approach of Gillenwater et al. (2014). If |Y | = k, then for a suitable N \u00d7 k indicator matrix U we can write LY = U\u2217LU , which is also known as a compression (U\u2217 denotes the Hermitian transpose).", "startOffset": 81, "endOffset": 107}, {"referenceID": 18, "context": "As in (Yuille and Rangarajan, 2003), we select \u03be by exploiting the convexity of h: as h(S) \u2265 h(R)+\u3008\u2207h(R), S \u2212R\u3009, we simply set \u03be(S,R) := f(S) + h(R) + \u3008\u2207h(R) |S \u2212R\u3009 .", "startOffset": 6, "endOffset": 35}, {"referenceID": 7, "context": "As shown in (Gillenwater et al., 2014), avoiding this step helps learn non-diagonal matrices.", "startOffset": 12, "endOffset": 38}, {"referenceID": 5, "context": "In comparison, each iteration of the method of Gillenwater et al. (2014) costs O(nN\u03ba + N), which is comparable to, though slightly greater than O(n\u03ba +N) as N \u2265 \u03ba.", "startOffset": 47, "endOffset": 73}, {"referenceID": 7, "context": "For real-world data, we use the baby registry test on which results are reported in (Gillenwater et al., 2014).", "startOffset": 84, "endOffset": 110}, {"referenceID": 7, "context": "(Gillenwater et al., 2014) provides a more in-depth description of this dataset.", "startOffset": 0, "endOffset": 26}, {"referenceID": 6, "context": "We compare performance of our algorithm, referred to as Picard iteration4, against the EM algorithm presented in Gillenwater et al. (2014). We experiment on both synthetic5 and real-world data.", "startOffset": 113, "endOffset": 139}, {"referenceID": 7, "context": "set to 1 and halved when necessary, as per the algorithm described in (Gillenwater et al., 2014); we used the code of Gillenwater et al.", "startOffset": 70, "endOffset": 96}, {"referenceID": 6, "context": "set to 1 and halved when necessary, as per the algorithm described in (Gillenwater et al., 2014); we used the code of Gillenwater et al. (2014) for our EM implementation7.", "startOffset": 71, "endOffset": 144}, {"referenceID": 7, "context": "We tested our implementation on all 13 product categories in the baby registry dataset, using two different initializations: \u2022 the aforementioned Wishart distribution \u2022 the data-dependent moment matching initialization (MM) described in (Gillenwater et al., 2014)", "startOffset": 237, "endOffset": 263}], "year": 2017, "abstractText": "Determinantal point processes (DPPs) offer an elegant tool for encoding probabilities over subsets of a ground set. Discrete DPPs are parametrized by a positive semidefinite matrix (called the DPP kernel), and estimating this kernel is key to learning DPPs from observed data. We consider the task of learning the DPP kernel, and develop for it a surprisingly simple yet effective new algorithm. Our algorithm offers the following benefits over previous approaches: (a) it is much simpler; (b) it yields equally good and sometimes even better local maxima; and (c) it runs an order of magnitude faster on large problems. We present experimental results on both real and simulated data to illustrate the numerical performance of our technique.", "creator": "LaTeX with hyperref package"}}}