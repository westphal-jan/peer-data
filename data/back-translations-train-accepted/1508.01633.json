{"id": "1508.01633", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Aug-2015", "title": "Asynchronous Distributed Semi-Stochastic Gradient Optimization", "abstract": "With the recent proliferation of large-scale learning problems, there have been a lot of interest on distributed machine learning algorithms, particularly those that are based on stochastic gradient descent (SGD) and its variants. However, existing algorithms either suffer from slow convergence due to the inherent variance of stochastic gradients, or have a fast linear convergence rate but at the expense of poorer solution quality. In this paper, we combine their merits together by proposing a distributed asynchronous SGD-based algorithm with variance reduction. A constant learning rate can be used, and it is also guaranteed to converge linearly to the optimal solution. Experiments on the Google Cloud Computing Platform demonstrate that the proposed algorithm outperforms state-of-the-art distributed asynchronous algorithms in terms of both wall clock time and solution quality.", "histories": [["v1", "Fri, 7 Aug 2015 07:54:47 GMT  (1368kb)", "http://arxiv.org/abs/1508.01633v1", null], ["v2", "Fri, 4 Dec 2015 06:33:34 GMT  (207kb,D)", "http://arxiv.org/abs/1508.01633v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DC", "authors": ["ruiliang zhang", "shuai zheng 0004", "james t kwok"], "accepted": true, "id": "1508.01633"}, "pdf": {"name": "1508.01633.pdf", "metadata": {"source": "CRF", "title": "Fast Distributed Asynchronous SGD with Variance Reduction", "authors": ["Ruiliang Zhang", "Shuai Zheng", "James T. Kwok"], "emails": [], "sections": [{"heading": null, "text": "In fact, in recent years, there has been a rapid growth of data that has met this challenge by seeking new avenues. (SGD) Data is divided into sub-areas associated with multiple machines, and optimization problems are solved in a way that is often difficult to store and process a large set of data on a single machine. Thus, there is now a growing interest in distributed machine learning algorithms. (SGD) Data is divided into sub-areas associated with multiple machines, and optimization is solved in a distributed way. (SD) The distributed architecture may have a shared memory [2] or distributed memory [3], [5], [5] in this paper, we will focus on the latter."}, {"heading": "II. BACKGROUND", "text": "In this paper, we consider the following optimization problem in wF (w) \u2261 1NN \u2211 i = 1fi (w). (1) In many machine learning applications, w \u0435Rd is the model parameter to be learned, N is the number of training samples and each fi: Rd \u2192 R is a (possibly regulated) loss related to the sample i. Assumption 2.1: Each fi is Li-smooth, i.e. fi (x) \u2264 fi (y) + < fi (y), x \u2212 y > + Li 2 \u0445 x \u2212 y, 2 \u0445 x, y.Assumption 2.2: F is \u00b5-strong convex, i.e. F (x) \u2265 F (y) + < F (y), x \u2212 y > + \u00b52 \u0445 x \u2212 y, y, y, y."}, {"heading": "A. Delayed Proximal Gradient (DPG) [14]", "text": "In iteration t, a worker uses a copy of w delayed by \u03c4t iterations (referred to as wt \u2212 \u03c4t) to calculate the stochastic gradient gt \u2212 \u03c4t = \u0445fi (wt \u2212 \u03c4t). This delayed gradient is used to update the correspondingly delayed parameter copy wt \u2212 \u03c4t to w \u2212 \u03c4t \u2212 \u03c4t \u2212 \u03c4t, where \u03b7 is a constant learning rate. This delayed gradient is then sent to the server, which considers the new iterate wt + 1 as a convex combination of updated wt and wt \u0445t \u2212 \u03c4t: wt + 1 = (1 \u2212 chsw) wt + \u0445t, (2) in which the new iterate wt + 1 is considered. It can be shown that the {wt} sequence is converged linearly to the optimal solution w \u0445 within a tolerance, i.e. E [F (wt) \u2212 \u0445t (1, where the value drops slightly) (1)."}, {"heading": "B. Stochastic Variance Reduced Gradient [13]", "text": "The SGD, although simple and scalable, has a slower convergence rate than the stack-gradient lineage (19). As mentioned in [13], the underlying reason is that the number of steps of the SGD must be reduced in order to control the variance of the gradient. In this thesis, we focus on one of these techniques, namely the stochastic variance reduction gradient (SVRG) [13] (algorithm 1), [15], [18], [19]. In this thesis, we focus on one of these techniques, namely the stochastic variance reduction gradient (SVRG) [13] (algorithm 1). It is advantageous that no additional space is required for the intermediate gradients or dual variables. The algorithm runs in steps that are varied in each case. At the beginning of each step, the WGD gradient: F (w) = 1- Ni (w) (1)."}, {"heading": "III. PROPOSED ALGORITHM", "text": "Both DPG and SVRG allow the use of a constant and thus higher learning rate than is normally used by SGD. However, while DPG is a distributed algorithm, it converges only into a neighborhood of the optimal solution. Better approximation quality comes at the expense of slower convergence. On the other hand, SVRG can converge to the optimal solution, but is designed only for use on individual machines. Its usage and convergence properties in the distributed asynchronous learning environment remain unexplored. In this section, we propose a hybrid of the two that combines the advantages of both. Similar to DPG and SVRG, it also uses a constant learning rate, but with a guaranteed linear convergence rate to the optimal solution in a distributed learning environment."}, {"heading": "A. Update using Delayed Gradients", "text": "Remember that the server update is based on delayed gradients in distributed asynchronous learning. Also, here we replace the SVRG update rule (line 8 in algorithm 1) bywt + 1 = wt \u2212 (EWT) SVRG. (3) If there is no delay (EWT = 0), (3) the change in wt (EWT) SVRG is reduced to standard (EWT) SVRG. The change in wt (3) has two components. The first component, of course, is a variable gradient as in SVRG. However, it is noisier as it is evaluated by the delayed parameter wt \u2212 t."}, {"heading": "B. Distributed Implementation", "text": "The question is how such a development could have occurred, and how such a development could have occurred. (...) The question is to what extent such a development could have occurred. (...) The question is to what extent such a development can occur. (...) The question is to what extent such a development can occur. (...) The question is to what extent such a development can occur. (...) The question is to what extent such a development can occur. (...) The question is to what extent such a development can occur. (...) The question is to what extent such a development must occur. (...) The question is to what extent such a development must occur. (...) The question is to what extent such a development must occur. (...) The question is to what extent such a development must occur."}, {"heading": "C. Convergence Analysis", "text": "The following theory shows a linear convergence of the proposed algorithm. Note that, in contrast to the delayed proximal gradient method (Section II-A), we have here a convergence to the optimal solution of (1), not only within a tolerance of (1). To our knowledge, this is the first such result for distributed asynchronous SGD-based algorithms with a constant learning rate. Theorem 3.1: Let L = max {Li} Ni = 1, and\u03b3 = (1 \u2212 2\u03b7 (\u00b5 > 2\u03b8))) m1 + \u03c4 + L2\u0445\u043c \u2212 L2. In the presence of (0, ul \u00b2 2L2) and m, the sequence {w \u00b2 Ni = 1, and\u03b3S} should be sufficiently large so that the sequence {w \u00b2 L \u00b2 converts the output from the algorithm to 1As L > \u00b5, it is easy to see that both 1 \u2212 2\u043c (l \u00b2 2L2) and m should be sufficiently large, so that the server {w \u00b2 Ni = 1, and\u03b3S} L \u00b2 L \u00b2 L \u00b2 converts the output from the algorithm to 1As L \u00b2 L \u00b2, L \u00b2 L \u00b2 L \u00b2 L and L \u00b2 L \u00b2 L \u00b2 L \u00b2 L should be conforms the output from the algorithm w &lt."}, {"heading": "IV. EXPERIMENTS", "text": "In this section we will consider the problem of \"2-regulated logistical regression\": min w1NN \u2211 i = 1log (1 + exp (\u2212 yix T i w)) + \u03bb 2 \u0394w \u0445 2, where {(xi, yi)} Ni = 1 are the training samples and \u03bb is a regularization parameter (fixed at 0.01). Experiments are being conducted with the epsilon, OCR and DNA data sets (Table I) of the Pascal Large Scale Learning Challenge2. Minibatch size B is set to 128 for Epsilon, 500 for OCR and 50,000 for DNA. We are conducting experiments with the Google Cloud Computing Platform3. Each computing node is a Google Cloud n1 Highmem 2 instance with dual core and 13 GB of memory. Each worker / dispatcher / server is an instance. The system is implemented in C + +, using the ZeroM4 package for communication."}, {"heading": "A. Comparison with Other Distributed Asynchronous SGDbased Algorithms", "text": "The following asynchronous SGD-based algorithms are compared: (i) downpour SGD [2] (referred to as \"downpour\") with the adaptive learning rate in Adagrad [12]; (ii) a variant of downpour SGD (referred to as \"downpour \u03c4\") in which the stalled stochastic gradients have a maximum delay of \u03c4. As shown in [10], [11], this leads to better performance in practice; (iii) delayed proximal gradient [14]; (iv) the proposed algorithm, which will be referred to as \"distributed variance-reduced stochastic gradients decently\" (distr-VRSGD). The number of stages S is set to 50, and the number of iterations m in each stage is calculated as \"N / B.\" For a fair comparison, the other algorithms for mS itterations will be executed."}, {"heading": "B. Different Number of Workers", "text": "In this experiment, we vary the number of workers from 1 to 16. With fewer workers, each worker needs to store and process a larger amount of data. As each computing node in our configuration has 13 GB of memory, we only conduct experiments with the smallest Epsilon data.Figure 3 (a) shows that both computing time and communication time decrease with the number of workers. Note that the most expensive step in the algorithm comes down to gradient valuations.5 At each stage, there are m iterations, each of which includes a mini-batch of size B. This therefore requires a total of O (B) gradient evaluations. At the end of each stage, additional O (N) gradient evaluations are required for the performance evaluation. Therefore, each worker spends O (mB + N) / P time for the calculation. In terms of communication time, it implies that more parameter pull requests, tasks and data can be sent between server and workers simultaneously."}, {"heading": "C. Effect of \u03c4", "text": "In this experiment, we vary between 0 and 32. We fix m and execute the algorithm for a sufficient number of S-steps until an objective target value is reached. Timing results for the three data sets are shown in Figure 4. With increasing \u03c4, more asynchrony is allowed and there is a significant reduction in communication time. On the other hand, let us remember that in convergence analysis, the convergence factor in each step is given by \u03b3 in theorem 3.1. Increasing mass means that more steps are required, and this leads to a higher computing and communication time. Therefore, when progression becomes very large, the computation and communication time increases again. Furthermore, as can be seen, the choice of \u03c4 = P applied in sections IV-A and IV-B is a good compromise."}], "references": [{"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "Proceedings of the International Conference on Computational Statistics, 2010, pp. 177\u2013186. 5 The scheduler and server operations are simple and their computational loads are negligible.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "M. Mao", "A. Senior", "P. Tucker", "K. Yang", "Q.V. Le"], "venue": "Advances in Neural Information Processing Systems, 2012, pp. 1223\u20131231.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Distributed asynchronous online learning for natural language processing", "author": ["K. Gimpel", "D. Das", "N.A. Smith"], "venue": "Proceedings of the Fourteenth Conference on Computational Natural Language Learning, 2010, pp. 213\u2013222.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Hogwild!: A lock-free approach to parallelizing stochastic gradient descent", "author": ["F. Niu", "B. Recht", "C. R\u00e9", "S. Wright"], "venue": "Advances in Neural Information Processing Systems 24, 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Communication-efficient distributed optimization using an approximate Newton-type method", "author": ["O. Shamir", "N. Srebro", "T. Zhang"], "venue": "Proceedings of the 31st International Conference on Machine Learning, 2014, pp. 1000\u20131008.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Asynchronous distributed ADMM for consensus optimization", "author": ["R. Zhang", "J. Kwok"], "venue": "Proceedings of the 31st International Conference on Machine Learning, 2014, pp. 1701\u20131709.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends in Machine Learning, vol. 3, no. 1, pp. 1\u2013122, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Loose synchronization for large-scale networked systems", "author": ["J. Albrecht", "C. Tuttle", "A. Snoeren", "A. Vahdat"], "venue": "Proceedings of the USENIX Annual Technical Conference, 2006, pp. 301\u2013314.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Distributed delayed stochastic optimization", "author": ["A. Agarwal", "J. Duchi"], "venue": "Advances in Neural Information Processing Systems 24, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "More effective distributed ML via a stale synchronous parallel parameter server", "author": ["Q. Ho", "J. Cipar", "H. Cui", "S. Lee", "J. Kim", "P. Gibbons", "G. Gibson", "G. Ganger", "E. Xing"], "venue": "Advances in Neural Information Processing Systems 26, 2013, pp. 1223\u20131231.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Communication efficient distributed machine learning with the parameter server", "author": ["M. Li", "D.G. Andersen", "A.J. Smola", "K. Yu"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 19\u201327.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research, vol. 12, pp. 2121\u20132159, 2011.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["R. Johnson", "T. Zhang"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 315\u2013323.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "A delayed proximal gradient method with linear convergence rate", "author": ["H.R. Feyzmahdavian", "A. Aytekin", "M. Johansson"], "venue": "Proceedings of the International Workshop on Machine Learning for Signal Processing, 2014, pp. 1\u20136.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives", "author": ["A. Defazio", "F. Bach", "S. Lacoste-Julien"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 1646\u20131654.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "A stochastic gradient method with an exponential convergence rate for finite training sets", "author": ["N.L. Roux", "M. Schmidt", "F.R. Bach"], "venue": "Advances in Neural Information Processing Systems, 2012, pp. 2663\u2013 2671.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Stochastic dual coordinate ascent methods for regularized loss", "author": ["S. Shalev-Shwartz", "T. Zhang"], "venue": "Journal of Machine Learning Research, vol. 14, no. 1, pp. 567\u2013599, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "A proximal stochastic gradient method with progressive variance reduction", "author": ["L. Xiao", "T. Zhang"], "venue": "SIAM Journal on Optimization, vol. 24, no. 4, pp. 2057\u20132075, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Optimization with first-order surrogate functions", "author": ["J. Mairal"], "venue": "Proceedings of the 30th International Conference on Machine Learning, 2013.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Sample size selection in optimization methods for machine learning", "author": ["R.H. Byrd", "G.M. Chin", "J. Nocedal", "Y. Wu"], "venue": "Mathematical programming, vol. 134, no. 1, pp. 127\u2013155, 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "A popular approach to address this challenge is by using stochastic gradient descent (SGD) and its variants [1]\u2013[3].", "startOffset": 108, "endOffset": 111}, {"referenceID": 2, "context": "A popular approach to address this challenge is by using stochastic gradient descent (SGD) and its variants [1]\u2013[3].", "startOffset": 112, "endOffset": 115}, {"referenceID": 3, "context": "In general, the distributed architecture may have shared memory [4] or distributed memory [2], [3], [5], [6].", "startOffset": 64, "endOffset": 67}, {"referenceID": 1, "context": "In general, the distributed architecture may have shared memory [4] or distributed memory [2], [3], [5], [6].", "startOffset": 90, "endOffset": 93}, {"referenceID": 2, "context": "In general, the distributed architecture may have shared memory [4] or distributed memory [2], [3], [5], [6].", "startOffset": 95, "endOffset": 98}, {"referenceID": 4, "context": "In general, the distributed architecture may have shared memory [4] or distributed memory [2], [3], [5], [6].", "startOffset": 100, "endOffset": 103}, {"referenceID": 5, "context": "In general, the distributed architecture may have shared memory [4] or distributed memory [2], [3], [5], [6].", "startOffset": 105, "endOffset": 108}, {"referenceID": 1, "context": "Such a distributed computing model has been commonly used in many recent large-scale machine learning implementations [2], [3], [5], [6].", "startOffset": 118, "endOffset": 121}, {"referenceID": 2, "context": "Such a distributed computing model has been commonly used in many recent large-scale machine learning implementations [2], [3], [5], [6].", "startOffset": 123, "endOffset": 126}, {"referenceID": 4, "context": "Such a distributed computing model has been commonly used in many recent large-scale machine learning implementations [2], [3], [5], [6].", "startOffset": 128, "endOffset": 131}, {"referenceID": 5, "context": "Such a distributed computing model has been commonly used in many recent large-scale machine learning implementations [2], [3], [5], [6].", "startOffset": 133, "endOffset": 136}, {"referenceID": 4, "context": "Often, machines in these systems have to run synchronously [5], [7].", "startOffset": 59, "endOffset": 62}, {"referenceID": 6, "context": "Often, machines in these systems have to run synchronously [5], [7].", "startOffset": 64, "endOffset": 67}, {"referenceID": 7, "context": "It also suffers from the straggler problem [8], in which the system can move forward only at the pace of the slowest worker.", "startOffset": 43, "endOffset": 46}, {"referenceID": 1, "context": "To alleviate these problems, asynchronicity is introduced [2], [6], [9]\u2013[11].", "startOffset": 58, "endOffset": 61}, {"referenceID": 5, "context": "To alleviate these problems, asynchronicity is introduced [2], [6], [9]\u2013[11].", "startOffset": 63, "endOffset": 66}, {"referenceID": 8, "context": "To alleviate these problems, asynchronicity is introduced [2], [6], [9]\u2013[11].", "startOffset": 68, "endOffset": 71}, {"referenceID": 10, "context": "To alleviate these problems, asynchronicity is introduced [2], [6], [9]\u2013[11].", "startOffset": 72, "endOffset": 76}, {"referenceID": 1, "context": "One prominent example of asynchronous SGD is the downpour SGD [2].", "startOffset": 62, "endOffset": 65}, {"referenceID": 11, "context": "Using an adaptive learning rate [12], downpour SGD achieves state-of-the-art performance.", "startOffset": 32, "endOffset": 36}, {"referenceID": 0, "context": "However, in order for these algorithms to converge, the learning rate has to decrease not only with the number of iterations (as in standard single-machine SGD [1]), but also with the maximum delay \u03c4 (i.", "startOffset": 160, "endOffset": 163}, {"referenceID": 9, "context": "Unlike [10], note that downpour SGD does not impose constraints on \u03c4 , and no convergence guarantee is provided.", "startOffset": 7, "endOffset": 11}, {"referenceID": 0, "context": "In practice, a decreasing learning rate leads to slower convergence [1], [13].", "startOffset": 68, "endOffset": 71}, {"referenceID": 12, "context": "In practice, a decreasing learning rate leads to slower convergence [1], [13].", "startOffset": 73, "endOffset": 77}, {"referenceID": 13, "context": "Recently, Feyzmahdavian et al [14] proposed the delayed proximal gradient method in which the delayed gradient is used to update an analogously delayed model parameter (but not its current one).", "startOffset": 30, "endOffset": 34}, {"referenceID": 12, "context": "Recently, there has been the flourish development of variance reduction techniques for SGD [13], [15]\u2013[18].", "startOffset": 91, "endOffset": 95}, {"referenceID": 14, "context": "Recently, there has been the flourish development of variance reduction techniques for SGD [13], [15]\u2013[18].", "startOffset": 97, "endOffset": 101}, {"referenceID": 17, "context": "Recently, there has been the flourish development of variance reduction techniques for SGD [13], [15]\u2013[18].", "startOffset": 102, "endOffset": 106}, {"referenceID": 13, "context": "Delayed Proximal Gradient (DPG) [14]", "startOffset": 32, "endOffset": 36}, {"referenceID": 12, "context": "Stochastic Variance Reduced Gradient [13]", "startOffset": 37, "endOffset": 41}, {"referenceID": 18, "context": "The SGD, though simple and scalable, has a slower convergence rate than batch gradient descent [19].", "startOffset": 95, "endOffset": 99}, {"referenceID": 12, "context": "As noted in [13], the underlying reason is that the stepsize of SGD has to be decreasing so as to control the gradient\u2019s variance.", "startOffset": 12, "endOffset": 16}, {"referenceID": 12, "context": "Recently, by observing that the training set is always finite in practice, a number of techniques have been developed that reduce this variance and thus allows the use of a constant stepsize [13], [15], [16], [18], [19].", "startOffset": 191, "endOffset": 195}, {"referenceID": 14, "context": "Recently, by observing that the training set is always finite in practice, a number of techniques have been developed that reduce this variance and thus allows the use of a constant stepsize [13], [15], [16], [18], [19].", "startOffset": 197, "endOffset": 201}, {"referenceID": 15, "context": "Recently, by observing that the training set is always finite in practice, a number of techniques have been developed that reduce this variance and thus allows the use of a constant stepsize [13], [15], [16], [18], [19].", "startOffset": 203, "endOffset": 207}, {"referenceID": 17, "context": "Recently, by observing that the training set is always finite in practice, a number of techniques have been developed that reduce this variance and thus allows the use of a constant stepsize [13], [15], [16], [18], [19].", "startOffset": 209, "endOffset": 213}, {"referenceID": 18, "context": "Recently, by observing that the training set is always finite in practice, a number of techniques have been developed that reduce this variance and thus allows the use of a constant stepsize [13], [15], [16], [18], [19].", "startOffset": 215, "endOffset": 219}, {"referenceID": 12, "context": "In this paper, we focus on one of these techniques, namely the stochastic variance reduction gradient (SVRG) [13] (Algorithm 1).", "startOffset": 109, "endOffset": 113}, {"referenceID": 12, "context": "Algorithm 1 Stochastic variance reduced gradient (SVRG) [13].", "startOffset": 56, "endOffset": 60}, {"referenceID": 19, "context": "Moreover, as in other SGD-based algorithms, we will use mini-batches to reduce the stochastic gradient\u2019s variance [20] and communication cost [2], [3].", "startOffset": 114, "endOffset": 118}, {"referenceID": 1, "context": "Moreover, as in other SGD-based algorithms, we will use mini-batches to reduce the stochastic gradient\u2019s variance [20] and communication cost [2], [3].", "startOffset": 142, "endOffset": 145}, {"referenceID": 2, "context": "Moreover, as in other SGD-based algorithms, we will use mini-batches to reduce the stochastic gradient\u2019s variance [20] and communication cost [2], [3].", "startOffset": 147, "endOffset": 150}, {"referenceID": 12, "context": "In each stage, it first issues m update tasks to the workers, where m is usually a multiple of \u2308N/B\u2309 as in SVRG [13].", "startOffset": 112, "endOffset": 116}, {"referenceID": 1, "context": "The following asynchronous SGD-based algorithms are compared: (i) downpour SGD [2] (denoted \u201cdownpour\u201d) with the adaptive learning rate in Adagrad [12]; (ii) a variant of Downpour SGD (denoted \u201cdownpour-\u03c4\u201d), in which the staled stochastic gradients have a maximum delay of \u03c4 .", "startOffset": 79, "endOffset": 82}, {"referenceID": 11, "context": "The following asynchronous SGD-based algorithms are compared: (i) downpour SGD [2] (denoted \u201cdownpour\u201d) with the adaptive learning rate in Adagrad [12]; (ii) a variant of Downpour SGD (denoted \u201cdownpour-\u03c4\u201d), in which the staled stochastic gradients have a maximum delay of \u03c4 .", "startOffset": 147, "endOffset": 151}, {"referenceID": 9, "context": "As shown in [10], [11], this leads to better performance in practice; (iii) delayed proximal gradient [14]; (iv) the proposed algorithm, which will be called \u201cdistributed variance-reduced stochastic gradient decent\u201d (distr-VRSGD).", "startOffset": 12, "endOffset": 16}, {"referenceID": 10, "context": "As shown in [10], [11], this leads to better performance in practice; (iii) delayed proximal gradient [14]; (iv) the proposed algorithm, which will be called \u201cdistributed variance-reduced stochastic gradient decent\u201d (distr-VRSGD).", "startOffset": 18, "endOffset": 22}, {"referenceID": 13, "context": "As shown in [10], [11], this leads to better performance in practice; (iii) delayed proximal gradient [14]; (iv) the proposed algorithm, which will be called \u201cdistributed variance-reduced stochastic gradient decent\u201d (distr-VRSGD).", "startOffset": 102, "endOffset": 106}], "year": 2015, "abstractText": "With the recent proliferation of large-scale learning problems, there have been a lot of interest on distributed machine learning algorithms, particularly those that are based on stochastic gradient descent (SGD) and its variants. However, existing algorithms either suffer from slow convergence due to the inherent variance of stochastic gradients, or have a fast linear convergence rate but at the expense of poorer solution quality. In this paper, we combine their merits together by proposing a distributed asynchronous SGD-based algorithm with variance reduction. A constant learning rate can be used, and it is also guaranteed to converge linearly to the optimal solution. Experiments on the Google Cloud Computing Platform demonstrate that the proposed algorithm outperforms state-of-the-art distributed asynchronous algorithms in terms of both wall clock time and solution quality.", "creator": "LaTeX with hyperref package"}}}