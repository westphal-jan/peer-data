{"id": "1508.01745", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Aug-2015", "title": "Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems", "abstract": "Natural language generation (NLG) is a critical component of spoken dialogue and it has a significant impact both on usability and perceived quality. Most NLG systems in common use employ rules and heuristics and tend to generate rigid and stylised responses without the natural variation of human language. They are also not easily scaled to systems covering multiple domains and languages. This paper presents a statistical language generator based on a semantically controlled Long Short-term Memory (LSTM) structure. The LSTM generator can learn from unaligned data by jointly optimising sentence planning and surface realisation using a simple cross entropy training criterion, and language variation can be easily achieved by sampling from output candidates. An objective evaluation in two differing test domains showed improved performance compared to previous methods with fewer heuristics. Human judges scored the LSTM system higher on informativeness and naturalness and overall preferred it to the other systems.", "histories": [["v1", "Fri, 7 Aug 2015 16:16:44 GMT  (643kb,D)", "http://arxiv.org/abs/1508.01745v1", "To be appear in EMNLP 2015"], ["v2", "Wed, 26 Aug 2015 17:16:25 GMT  (813kb,D)", "http://arxiv.org/abs/1508.01745v2", "To be appear in EMNLP 2015"]], "COMMENTS": "To be appear in EMNLP 2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["tsung-hsien wen", "milica gasic", "nikola mrksic", "pei-hao su", "david vandyke", "steve j young"], "accepted": true, "id": "1508.01745"}, "pdf": {"name": "1508.01745.pdf", "metadata": {"source": "CRF", "title": "Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems", "authors": ["Tsung-Hsien Wen", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "Pei-Hao Su", "David Vandyke", "Steve Young"], "emails": ["thw28@cam.ac.uk", "mg436@cam.ac.uk", "nm480@cam.ac.uk", "phs26@cam.ac.uk", "djv27@cam.ac.uk", "sjy@cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to play by the rules that they have imposed on themselves, and they are able to play by the rules that they have imposed on themselves."}, {"heading": "2 Related Work", "text": "In fact, most of them are able to play by the rules that they have imposed on themselves, and they are able to play by the rules that they have imposed on themselves."}, {"heading": "3 The Neural Language Generator", "text": "The generation model proposed in this paper is based on a recurring NN architecture (Mikolov et al., 2010), in which a 1-hot coding with a token1 wt is each step t on a recurring hidden level and outputs the probability distribution of the next token wt + 1. Therefore, by scanning input tokens successively from the output distribution of the RNN until a stop sign is gen-1, we use tokens instead of words, because our model works with text for which slot values are replaced by the corresponding slot tokens. We call this procedure delicalization (Karpathy and Fei-Fei, 2014) or a certain restriction is met (Zhang and Lapata, 2014), the network can generate a sequence of tokens that can be lexicalized to form the required expression."}, {"heading": "3.1 Semantic Controlled LSTM cell", "text": "It is a recursive problem that arises in the various fields of application for an LSTM cell (Graves, 2013; Zaremba et al., 2014), the architecture used in this paper is cited by the following equations: \"It is not what it is.\" (1), \"It is what it is.\" (2), \"It is what it is.\" (3), \"It is what it is.\" (4), \"It is what it is.\" (5), \"It is what it is.\" (4), \"It is what it is.\" (4), \"It is what it is.\" (4), \"It is what it is.\" (5), \"It is what it is.\""}, {"heading": "3.2 The Deep Structure", "text": "Deep Neural Networks (DNN) allow increased differentiation by learning multiple layers of features and represent the state of the art for many applications such as speech recognition (Graves et al., 2013b) and natural language processing (Collobert and Weston, 2008).The neural speech generator proposed in this paper can be easily extended to be both spatially and temporally deep by stacking several LSTM cells over the original structure. As shown in Figure 2, jump links are applied to the inputs of all hidden layers as well as between all hidden layers and the outputs (Graves, 2013).This reduces the number of processing steps between the bottom of the network and the top and thus alleviates the problem of the disappearing gradient (Bengio et al., 1994) in a vertical direction. To allow all hidden layer information to affect the reading gate, Equation 7 is modified to stabilize the structure of the network (W1)."}, {"heading": "3.3 Backward LSTM reranking", "text": "A remaining problem in the structure described so far is that the LSTM generator selects words only on the basis of the previous history, while some sentence forms depend on the backward context. Bidirectional networks (Schuster and Paliwal, 1997) have so far proved effective for sequential problems (Graves et al., 2013a; Sundermeyer et al., 2014). However, applying a bidirectional network directly in the SC-LSTM generator is not easy, as the generation process is time-sequential. So, instead of integrating the bidirectional information into a network, we trained another SC-LSTM from the backward context to select the best candidates from the outputs of the forward generator. In our experiments, we also found that the keyword weights Wwr (see equations 7 and 12) of the forward and backward directed networks make the generator less sensitive to random initializations."}, {"heading": "3.4 Training", "text": "The objective function was the transverse entropy error between the predicted word distribution pt and the actual word distribution yt in the training corpus. An L2 regularization term was added to the objective function for all 10 training examples, as proposed in Mikolov et al. (2011b). However, further regularization was required for the Lesegate dynamics, resulting in the following modified cost function for each mini-match (Ignoring standard l2), F (Ignoring standard l2) = \"p\" t log (yt) + \"p\" t \"(yt) +\" p \"t\" (0) dt + 1 \"dt\" (13), where dT is the DA vector for the last word index T, and \"c\" (Ignoring standard l2) = \"p\" t \"t\" (yt) + \"p\" t \"t\" (13)."}, {"heading": "3.5 Decoding", "text": "The decoding process is divided into two phases: (a) overgeneration and (b) reanking. In the overgeneration phase, the forward generator based on the given DA is used to generate utterances by randomly scanning the predicted next word distributions. In the reanking phase, the costs for the backanchor Fb (\u03b8) are calculated. Together with the costs Ff (\u03b8) from the forward generator, the reanchor value R is calculated as follows: R = \u2212 (Ff (\u03b8) + Fb (\u03b8) + \u03bbERR) (14), where \u03bb is a compromise constant, and the slot error rate ERR is calculated by exact match of the slot marks in the candidate statements, ERR = p + qN (15), where N is the total number of slots in the DA, and p, q is the number of missing and redundant slots in the given realization."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Experimental Setup", "text": "The target application for our generational system is a spoken dialog system that provides information about specific locations in San Francisco. To demonstrate the scalability of the proposed method and its performance in different areas, we tested two domains, each talking about restaurants and hotels. There are 8 system dialog types such as Inform to present information about restaurants, confirm that a slot value has been correctly recognized, and refuse to point out that the user's limitations cannot be met. Each domain contains 12 attributes (slots), but some are common to both domains and the others are domain specific. Detailed ontologies for two domains are provided in Table 1. To form a training corpus for each domain, dialogs were collected from an earlier user study (Gas, 2015) of a statistical dialog manager randomly selected and shown to the workers recruited via the Amazon Mechanical Turk (AMT)."}, {"heading": "4.2 Objective Evaluation", "text": "We compared the individual layers of semantically controlled LSTM (sc-lstm) and a deep version with two hidden layers (+ deep) against multiple baselines: the handmade generators (hdc), k-nearest neighbors (kNN), class-based LMs (classlm) as proposed in Oh and LSTM, the heuristic gated RNN as described in Wen et al. (2015) and a similar LSTM variant (rnn w / lstm w), and the same RNN / LSTM, but without gates (rnu & lstm w)."}, {"heading": "4.3 Human Evaluation", "text": "Since automatic metrics do not consistently correspond to human perception (Stent et al., 2005), a human test is required to assess subjective quality by recruiting a group of judges using AMT. For each task, two systems were randomly selected from the four (Classlm, rnn w /, sc-lstm and + deep) to generate expressions from a series of newly sampled restaurant dialogues. To evaluate system performance in the presence of language variations, each system generated 5 different surface realizations for each DA input, and the human judges were asked to rate each of them in terms of informativeness and naturalness (rating of 3) and also asked to indicate a preference between the two. Here, informativeness is defined as whether the expression contains all the information specified in the DA, and naturalness is defined as a preference (preference of lativity) as a preference of M. To reduce the amount of information presented to the judges."}, {"heading": "5 Conclusion and Future Work", "text": "In this paper, we have proposed a neural network-based generator that is capable of generating natural linguistically diverse responses based on a deep, semantically controlled LSTM architecture that we call SC-LSTM. The generator can be trained on non-aligned data by jointly optimizing its typesetting and surface realization components using a simple criterion of cross entropy without heuristics or manual labor. We found that the SCLSTM model achieves the best overall performance based on two objective metrics in two different domains. An evaluation by human judges also confirmed that the SC-LSTM approach is strongly preferred to a variety of existing methods.This work represents a line of research that seeks to model the NLG problem in a unified architecture, with the entire model consistently traceable based on data. We claim that this approach can produce more natural responses that are more similar to colloquial styles that should be found in a human cooperative conversation, with more potential cognitive uses of language coefficients."}, {"heading": "6 Acknowledgements", "text": "Tsung-Hsien Wen and David Vandyke are supported by Toshiba Research Europe Ltd., Cambridge Research Laboratory."}], "references": [{"title": "A simple domain-independent probabilistic approach to generation", "author": ["Angeli et al.2010] Gabor Angeli", "Percy Liang", "Dan Klein"], "venue": "In Proceedings of the 2010 Conference on EMNLP, EMNLP \u201910", "citeRegEx": "Angeli et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Angeli et al\\.", "year": 2010}, {"title": "Theano: new features and speed improvements", "author": ["Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models", "author": ["Anja Belz"], "venue": "Natural Language Engineering", "citeRegEx": "Belz.,? \\Q2008\\E", "shortCiteRegEx": "Belz.", "year": 2008}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio et al.1994] Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David WardeFarley", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th International Conference on Machine Learning", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Incremental on-line adaptation of pomdp-based dialogue managers", "author": ["Ga\u0161i\u0107 et al.2014] Milica Ga\u0161i\u0107", "Dongho Kim", "Pirros Tsiakoulis", "Catherine Breslin", "Matthew Henderson", "Martin Szummer", "Blaise Thomson", "Steve Young"], "venue": null, "citeRegEx": "Ga\u0161i\u0107 et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ga\u0161i\u0107 et al\\.", "year": 2014}, {"title": "Distributed dialogue policies for multi-domain statistical dialogue management", "author": ["Ga\u0161i\u0107 et al.2015] Milica Ga\u0161i\u0107", "Dongho Kim", "Pirros Tsiakoulis", "Steve Young"], "venue": "Proceedings on ICASSP", "citeRegEx": "Ga\u0161i\u0107 et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ga\u0161i\u0107 et al\\.", "year": 2015}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["Graves et al.2009] Alex Graves", "Marcus Liwicki", "Santiago Fern\u00e1ndez", "Roman Bertolami", "Horst Bunke", "J\u00fcrgen Schmidhuber"], "venue": "Pattern Analysis and Machine Intelligence,", "citeRegEx": "Graves et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2009}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves et al.2013a] Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Speech recognition with deep recurrent neural networks. CoRR, abs/1303.5778", "author": ["Graves et al.2013b] Alex Graves", "Abdel-rahman Mohamed", "Geoffrey E. Hinton"], "venue": null, "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Generating sequences with recurrent neural networks. CoRR, abs/1308.0850", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Robust dialog state tracking using delexicalised recurrent neural networks and unsupervised adaptation", "author": ["Blaise Thomson", "Steve Young"], "venue": "In Proceedings of IEEE Spoken Language Technology", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Deep neural networks for acoustic modeling", "author": ["Li Deng", "Dong Yu", "George Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara Sainath", "Brian Kingsbury"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Deep visual-semantic alignments for generating image descriptions. CoRR", "author": ["Karpathy", "Fei-Fei2014] Andrej Karpathy", "Li Fei-Fei"], "venue": null, "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "A statistical nlg framework for aggregated planning and realization", "author": ["Blake Howald", "Frank Schilder"], "venue": "In Proceedings of the 51st Annual Meeting of the ACL. Association", "citeRegEx": "Kondadadi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kondadadi et al\\.", "year": 2013}, {"title": "Where do phrases come from: Some preliminary experiments in connectionist phrase generation. In Natural Language Generation", "author": ["Karen Kukich"], "venue": null, "citeRegEx": "Kukich.,? \\Q1987\\E", "shortCiteRegEx": "Kukich.", "year": 1987}, {"title": "Generation that exploits corpus-based statistical knowledge", "author": ["Langkilde", "Knight1998] Irene Langkilde", "Kevin Knight"], "venue": "In Proceedings of the 36th Annual Meeting of the ACL,", "citeRegEx": "Langkilde et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Langkilde et al\\.", "year": 1998}, {"title": "Controlling user perceptions of linguistic style: Trainable generation of personality traits", "author": ["Mairesse", "Walker2011] Fran\u00e7ois Mairesse", "Marilyn A. Walker"], "venue": "Computer Linguistics", "citeRegEx": "Mairesse et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mairesse et al\\.", "year": 2011}, {"title": "Stochastic language generation in dialogue using factored language models", "author": ["Mairesse", "Young2014] Fran\u00e7ois Mairesse", "Steve Young"], "venue": "Computer Linguistics", "citeRegEx": "Mairesse et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mairesse et al\\.", "year": 2014}, {"title": "Phrase-based statistical language generation using graphical models and active learning", "author": ["Milica Ga\u0161i\u0107", "Filip Jur\u010d\u0131\u0301\u010dek", "Simon Keizer", "Blaise Thomson", "Kai Yu", "Steve Young"], "venue": "In Proceedings of the 48th ACL,", "citeRegEx": "Mairesse et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mairesse et al\\.", "year": 2010}, {"title": "Rhetorical structure theory: Toward a functional theory of text", "author": ["Mann", "Thompson1988] William C. Mann", "Sandra A. Thompson"], "venue": null, "citeRegEx": "Mann et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Mann et al\\.", "year": 1988}, {"title": "Context dependent recurrent neural network language model", "author": ["Mikolov", "Zweig2012] Tom\u00e1\u0161 Mikolov", "Geoffrey Zweig"], "venue": "Proceedings on IEEE SLT workshop", "citeRegEx": "Mikolov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafit", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u00fd", "Sanjeev Khudanpur"], "venue": "Proceedings on InterSpeech", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["Stefan Kombrink", "Luk\u00e1\u0161 Burget", "Jan H. \u010cernock\u00fd", "Sanjeev Khudanpur"], "venue": "In ICASSP,", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Rnnlm - recurrent neural network language modeling toolkit", "author": ["Stefan Kombrink", "Anoop Deoras", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u00fd"], "venue": "Proceedings on ASRU", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Dialogue management using scripts", "author": ["Mirkovic", "Cavedon2011] Danilo Mirkovic", "Lawrence Cavedon"], "venue": "EP Patent 1,891,625", "citeRegEx": "Mirkovic et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mirkovic et al\\.", "year": 2011}, {"title": "Stochastic language generation for spoken dialogue systems", "author": ["Oh", "Rudnicky2000] Alice H. Oh", "Alexander I. Rudnicky"], "venue": "In Proceedings of the 2000 ANLP/NAACL Workshop on Conversational Systems - Volume", "citeRegEx": "Oh et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2000}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th annual meeting on ACL. Association for Computational Linguistics", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher Manning"], "venue": "In Proceedings of the 2014 Conference on EMNLP. Association for Computational Linguistics", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Trainable approaches to surface natural language generation and their application to conversational dialog systems", "author": ["Adwait Ratnaparkhi"], "venue": "Computer Speech and Language", "citeRegEx": "Ratnaparkhi.,? \\Q2002\\E", "shortCiteRegEx": "Ratnaparkhi.", "year": 2002}, {"title": "Natural language generation as planning under uncertainty for spoken dialogue systems", "author": ["Rieser", "Lemon2010] Verena Rieser", "Oliver Lemon"], "venue": "In Empirical Methods in Natural Language Generation", "citeRegEx": "Rieser et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rieser et al\\.", "year": 2010}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Evaluating automatic extraction of rules for sentence plan construction", "author": ["Stent", "Molina2009] Amanda Stent", "Martin Molina"], "venue": "In Proceedings of SIGdial", "citeRegEx": "Stent et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Stent et al\\.", "year": 2009}, {"title": "Trainable sentence planning for complex information presentation in spoken dialog systems", "author": ["Stent et al.2004] Amanda Stent", "Rashmi Prasad", "Marilyn Walker"], "venue": "Proceedings of the Annual Meeting of the ACL", "citeRegEx": "Stent et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Stent et al\\.", "year": 2004}, {"title": "Evaluating evaluation methods for generation in the presence of variation", "author": ["Stent et al.2005] Amanda Stent", "Matthew Marge", "Mohit Singhai"], "venue": "In in Proceedings of CICLing", "citeRegEx": "Stent et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Stent et al\\.", "year": 2005}, {"title": "Translation modeling with bidirectional recurrent neural networks", "author": ["Tamer Alkhouli", "Joern Wuebker", "Hermann Ney"], "venue": "In Proceedings of the 2014 Conference on EMNLP. Association", "citeRegEx": "Sundermeyer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2014}, {"title": "Generating text with recurrent neural networks", "author": ["James Martens", "Geoffrey E. Hinton"], "venue": "In Proceedings of the 28th International Conference on Machine Learning (ICML-11)", "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks. CoRR", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Training a sentence planner for spoken dialogue using boosting", "author": ["Owen C Rambow", "Monica Rogati"], "venue": "Computer Speech and Language", "citeRegEx": "Walker et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Walker et al\\.", "year": 2002}, {"title": "Individual and domain adaptation in sentence planning for dialogue", "author": ["Amanda Stent", "Franois Mairesse", "Rashmi Prasad"], "venue": "Journal of Artificial Intelligence Research (JAIR", "citeRegEx": "Walker et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Walker et al\\.", "year": 2007}, {"title": "Stochastic language generation in dialogue using recurrent neural networks with convolutional sentence rerank", "author": ["Wen et al.2015] Tsung-Hsien Wen", "Milica Ga\u0161i\u0107", "Dongho Kim", "Nikola Mrk\u0161i\u0107", "Pei-Hao Su", "David Vandyke", "Steve Young"], "venue": null, "citeRegEx": "Wen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J Werbos"], "venue": "Proceedings of the IEEE", "citeRegEx": "Werbos.,? \\Q1990\\E", "shortCiteRegEx": "Werbos.", "year": 1990}, {"title": "Spoken language understanding using long shortterm memory neural networks", "author": ["Yao et al.2014] Kaisheng Yao", "Baolin Peng", "Yu Zhang", "Dong Yu", "Geoffrey Zweig", "Yangyang Shi"], "venue": "Proceedings on IEEE SLT workshop. IEEE Institute of Electrical", "citeRegEx": "Yao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2014}, {"title": "Pomdpbased statistical spoken dialog systems: A review", "author": ["Young et al.2013] Steve Young", "Milica Ga\u0161i\u0107", "Blaise Thomson", "Jason D. Williams"], "venue": "Proceedings of the IEEE", "citeRegEx": "Young et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Young et al\\.", "year": 2013}, {"title": "Recurrent neural network regularization. CoRR, abs/1409.2329", "author": ["Ilya Sutskever", "Oriol Vinyals"], "venue": null, "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Chinese poetry generation with recurrent neural networks", "author": ["Zhang", "Lapata2014] Xingxing Zhang", "Mirella Lapata"], "venue": "In Proceedings of the 2014 Conference on EMNLP. Association", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 32, "context": "As noted in Stent et al. (2005), a good generator usually depends on several factors: adequacy, fluency, readability, and variation.", "startOffset": 12, "endOffset": 32}, {"referenceID": 35, "context": "The trainable generator approach exemplified by the HALOGEN (Langkilde and Knight, 1998) and SPaRKy system (Stent et al., 2004) provides", "startOffset": 107, "endOffset": 127}, {"referenceID": 41, "context": "These systems include specific trainable modules within the generation framework to allow the model to adapt to different domains (Walker et al., 2007), or reproduce certain style (Mairesse and Walker, 2011).", "startOffset": 130, "endOffset": 151}, {"referenceID": 42, "context": "More recently, corpus-based methods (Oh and Rudnicky, 2000; Mairesse and Young, 2014; Wen et al., 2015) have received attention as access to data becomes increasingly available.", "startOffset": 36, "endOffset": 103}, {"referenceID": 42, "context": "As suggested in Wen et al. (2015), a backward reranker is introduced in Section 3.", "startOffset": 16, "endOffset": 34}, {"referenceID": 40, "context": "a tree-like or template structure, then surface realisation converts the intermediate structure into the final text (Walker et al., 2002; Stent et al., 2004).", "startOffset": 116, "endOffset": 157}, {"referenceID": 35, "context": "a tree-like or template structure, then surface realisation converts the intermediate structure into the final text (Walker et al., 2002; Stent et al., 2004).", "startOffset": 116, "endOffset": 157}, {"referenceID": 2, "context": "Although statistical sentence planning has been explored previously, for example, generating the most likely context-free derivations given a corpus (Belz, 2008) or maximising the expected reward using reinforcement learning (Rieser and Lemon, 2010), these methods still rely on a pre-existing, handcrafted gener-", "startOffset": 149, "endOffset": 161}, {"referenceID": 21, "context": "Although active learning (Mairesse et al., 2010) was also proposed to allow learning online directly", "startOffset": 25, "endOffset": 48}, {"referenceID": 28, "context": "Ratnaparkhi (2002) later addressed some of the limitations of class-based LMs in the over-generation phase by using a modified generator based on a syntactic dependency tree.", "startOffset": 0, "endOffset": 19}, {"referenceID": 28, "context": "Ratnaparkhi (2002) later addressed some of the limitations of class-based LMs in the over-generation phase by using a modified generator based on a syntactic dependency tree. Mairesse and Young (2014) proposed a phrase-based NLG system based on factored LMs that can learn from a semantically aligned corpus.", "startOffset": 0, "endOffset": 201}, {"referenceID": 0, "context": ", Angeli et al. (2010) train a set of log-linear models to make", "startOffset": 2, "endOffset": 23}, {"referenceID": 16, "context": "Kondadadi et al. (2013) later show that the outputs can be further improved by an SVM reranker making them comparable to human-authored texts.", "startOffset": 0, "endOffset": 24}, {"referenceID": 24, "context": "Recent advances in recurrent neural network-based language models (RNNLM) (Mikolov et al., 2010; Mikolov et al., 2011a) have demonstrated the value of distributed representations and the ability to model arbitrarily long dependencies.", "startOffset": 74, "endOffset": 119}, {"referenceID": 17, "context": "stock reporter system ANA by Kukich (1987) is perhaps the first NN-based generator, although generation was only done at the phrase level.", "startOffset": 29, "endOffset": 43}, {"referenceID": 17, "context": "stock reporter system ANA by Kukich (1987) is perhaps the first NN-based generator, although generation was only done at the phrase level. Recent advances in recurrent neural network-based language models (RNNLM) (Mikolov et al., 2010; Mikolov et al., 2011a) have demonstrated the value of distributed representations and the ability to model arbitrarily long dependencies. Sutskever et al. (2011) describes a simple variant of the RNN that can generate meaningful sentences by learning from a character-level corpus.", "startOffset": 29, "endOffset": 398}, {"referenceID": 42, "context": "A forerunner of the system presented here is described in Wen et al. (2015), in which a forward RNN generator, a CNN reranker, and a backward RNN reranker are trained jointly", "startOffset": 58, "endOffset": 76}, {"referenceID": 3, "context": "Training an RNN with long range dependencies is difficult because of the vanishing gradient problem (Bengio et al., 1994).", "startOffset": 100, "endOffset": 121}, {"referenceID": 3, "context": "Training an RNN with long range dependencies is difficult because of the vanishing gradient problem (Bengio et al., 1994). Hochreiter and Schmidhuber (1997) mitigated this problem by replacing the sigmoid activation in the RNN recurrent con-", "startOffset": 101, "endOffset": 157}, {"referenceID": 8, "context": ", 2013b), handwriting recognition (Graves et al., 2009), spoken language understanding (Yao et al.", "startOffset": 34, "endOffset": 55}, {"referenceID": 44, "context": ", 2009), spoken language understanding (Yao et al., 2014), and machine translation (Sutskever et al.", "startOffset": 39, "endOffset": 57}, {"referenceID": 39, "context": ", 2014), and machine translation (Sutskever et al., 2014).", "startOffset": 33, "endOffset": 57}, {"referenceID": 8, "context": "by Graves et al. (2014) has demonstrated that an NN structure augmented with a carefully designed memory block and differentiable read/write operations can learn to mimic computer programs.", "startOffset": 3, "endOffset": 24}, {"referenceID": 13, "context": "vides a more sophisticated way of exploiting relations between labels and features, therefore making the prediction more accurate (Hinton et al., 2012).", "startOffset": 130, "endOffset": 151}, {"referenceID": 11, "context": "By extending an LSTM network to be both deep in space and time, Graves (2013) shows the resulting network can used to synthesise handwriting indistinguishable from that of a human.", "startOffset": 64, "endOffset": 78}, {"referenceID": 24, "context": "The generation model proposed in this paper is based on a recurrent NN architecture (Mikolov et al., 2010) in which a 1-hot encoding wt of a token1 wt is input at each time step t conditioned on a recurrent hidden layer ht and outputs the probability distribution of the next token wt+1.", "startOffset": 84, "endOffset": 106}, {"referenceID": 11, "context": "Of the various different connectivity designs for an LSTM cell (Graves, 2013; Zaremba et al., 2014), the architecture used in this paper is given by the following equations,", "startOffset": 63, "endOffset": 99}, {"referenceID": 46, "context": "Of the various different connectivity designs for an LSTM cell (Graves, 2013; Zaremba et al., 2014), the architecture used in this paper is given by the following equations,", "startOffset": 63, "endOffset": 99}, {"referenceID": 3, "context": "Although a related work (Karpathy and Fei-Fei, 2014) has suggested that reapplying this auxiliary information to the RNN at every time step can increase performance by mitigating the vanishing gradient problem (Mikolov and Zweig, 2012; Bengio et al., 1994), we have found that such a model also omits and duplicates slot information in the surface realisation.", "startOffset": 210, "endOffset": 256}, {"referenceID": 3, "context": "Although a related work (Karpathy and Fei-Fei, 2014) has suggested that reapplying this auxiliary information to the RNN at every time step can increase performance by mitigating the vanishing gradient problem (Mikolov and Zweig, 2012; Bengio et al., 1994), we have found that such a model also omits and duplicates slot information in the surface realisation. In Wen et al. (2015) simple heuristics are used to turn off slot feature values in the control vector d once the corresponding slot token", "startOffset": 236, "endOffset": 382}, {"referenceID": 11, "context": "(Graves, 2013).", "startOffset": 0, "endOffset": 14}, {"referenceID": 3, "context": "This reduces the number of processing steps between the bottom of the network and the top, and therefore mitigates the vanishing gradient problem (Bengio et al., 1994) in the vertical direction.", "startOffset": 146, "endOffset": 167}, {"referenceID": 33, "context": "the dropout technique (Srivastava et al., 2014) is used to regularise the network.", "startOffset": 22, "endOffset": 47}, {"referenceID": 46, "context": "As suggested in (Zaremba et al., 2014), dropout was only applied to the non-recurrent connections, as shown in the Figure 2.", "startOffset": 16, "endOffset": 38}, {"referenceID": 37, "context": "Previously, bidirectional networks (Schuster and Paliwal, 1997) have been shown to be effective for sequential problems (Graves et al., 2013a; Sundermeyer et al., 2014).", "startOffset": 120, "endOffset": 168}, {"referenceID": 23, "context": "An l2 regularisation term was added to the objective function for every 10 training examples as suggested in Mikolov et al. (2011b). However, further regularisation was required for the reading gate dynamics.", "startOffset": 109, "endOffset": 132}, {"referenceID": 30, "context": "The forward and backward networks were structured to share the same set of word embeddings, initialised with pre-trained word vectors (Pennington et al., 2014).", "startOffset": 134, "endOffset": 159}, {"referenceID": 43, "context": "works were trained with back propagation through time (Werbos, 1990).", "startOffset": 54, "endOffset": 68}, {"referenceID": 7, "context": "ing corpus for each domain, dialogues collected from a previous user trial (Ga\u0161i\u0107 et al., 2015) of a statistical dialogue manager were randomly sampled and shown to workers recruited via the Amazon Mechanical Turk (AMT) service.", "startOffset": 75, "endOffset": 95}, {"referenceID": 4, "context": "library (Bergstra et al., 2010; Bastien et al., 2012), and trained by partitioning each of the collected corpus into a training, validation, and testing set in the ratio 3:1:1.", "startOffset": 8, "endOffset": 53}, {"referenceID": 1, "context": "library (Bergstra et al., 2010; Bastien et al., 2012), and trained by partitioning each of the collected corpus into a training, validation, and testing set in the ratio 3:1:1.", "startOffset": 8, "endOffset": 53}, {"referenceID": 29, "context": "The BLEU-4 metric was used for the objective evaluation (Papineni et al., 2002).", "startOffset": 56, "endOffset": 79}, {"referenceID": 42, "context": "We compared the single layer semantically controlled LSTM (sc-lstm) and a deep version with two hidden layers (+deep) against several baselines: the handcrafted generator (hdc), k-nearest neighbour (kNN), class-based LMs (classlm) as proposed in Oh and Rudnicky (2000), the heuristic gated RNN as described in Wen et al. (2015) and a similar LSTM variant (rnn w/ & lstm w/), and the same RNN/LSTM but without gates (rnn w/o & lstm w/o).", "startOffset": 310, "endOffset": 328}, {"referenceID": 6, "context": "standard generator used for trialling end-to-end dialogue systems (for example (Ga\u0161i\u0107 et al., 2014)).", "startOffset": 79, "endOffset": 99}, {"referenceID": 36, "context": "agree with human perception (Stent et al., 2005), human testing is needed to assess subjective quality.", "startOffset": 28, "endOffset": 48}], "year": 2017, "abstractText": "Natural language generation (NLG) is a critical component of spoken dialogue and it has a significant impact both on usability and perceived quality. Most NLG systems in common use employ rules and heuristics and tend to generate rigid and stylised responses without the natural variation of human language. They are also not easily scaled to systems covering multiple domains and languages. This paper presents a statistical language generator based on a semantically controlled Long Short-term Memory (LSTM) structure. The LSTM generator can learn from unaligned data by jointly optimising sentence planning and surface realisation using a simple cross entropy training criterion, and language variation can be easily achieved by sampling from output candidates. With fewer heuristics, an objective evaluation in two differing test domains showed the proposed method improved performance compared to previous methods. Human judges scored the LSTM system higher on informativeness and naturalness and overall preferred it to the other systems.", "creator": "LaTeX with hyperref package"}}}