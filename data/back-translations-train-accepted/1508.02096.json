{"id": "1508.02096", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Aug-2015", "title": "Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation", "abstract": "We introduce a model for constructing vector representations of words by composing characters using bidirectional LSTMs. Relative to traditional word representation models that have independent vectors for each word type, our model requires only a single vector per character type and a fixed set of parameters for the compositional model. Despite the compactness of this model and, more importantly, the arbitrary nature of the form-function relationship in language, our \"composed\" word representations yield state-of-the-art results in language modeling and part-of-speech tagging. Benefits over traditional baselines are particularly pronounced in morphologically rich languages (e.g., Turkish).", "histories": [["v1", "Sun, 9 Aug 2015 23:41:38 GMT  (96kb,D)", "http://arxiv.org/abs/1508.02096v1", null], ["v2", "Mon, 23 May 2016 20:57:19 GMT  (96kb,D)", "http://arxiv.org/abs/1508.02096v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wang ling", "chris dyer", "alan w black", "isabel trancoso", "ramon fermandez", "silvio amir", "lu\u00eds marujo", "tiago lu\u00eds"], "accepted": true, "id": "1508.02096"}, "pdf": {"name": "1508.02096.pdf", "metadata": {"source": "CRF", "title": "Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation", "authors": ["Wang Ling", "Tiago Lu\u0131\u0301s Lu\u0131\u0301s Marujo", "Ram\u00f3n Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "emails": ["lingwang@cs.cmu.edu", "lmarujo@cs.cmu.edu", "cdyer@cs.cmu.edu", "awb@cs.cmu.edu", "ramon.astudillo@inesc-id.pt", "samir@inesc-id.pt", "tmcl@inesc-id.pt", "isabel.trancoso@inesc-id.pt"], "sections": [{"heading": "1 Introduction", "text": "Of central importance are vector-space models that capture functional (i.e. semantic and syntactic) similarities in terms of geometric locality. However, when word vectors are learned - a practice that is becoming increasingly common - most models assume that each type of word has its own vector representation that can be independent of other model components. This paper argues that this assumption of independence is inherently problematic, especially in morphologically rich languages (e.g. Turkish). In such languages, a reasonable assumption would be that orthographic (formal) similarity is evidence of functional similarity."}, {"heading": "2 Word Vectors and Wordless Word Vectors", "text": "Unlike naive models, in which all the word types of a vocabulary V are equally different from each other, vector space models capture the intuition that words can be different or similar in a variety of dimensions. Learning vector representations of words by treating them as optimizable parameters in different types of language models has proven to be a remarkably effective means of generating vector representations that also work well in other tasks (Collobert et al., 2011; Kalchbrenner et al., 2013; Liu et al., 2014; Chen and Manning, 2014). Formally, such models define a matrix P that contains d parameters for each word in the vocabulary V. For a given word type w-V, a column is selected by right-multiplying P with a single hot vector of length | V-V that is zero in each dimension, except for this element that is compared with the Wotd table."}, {"heading": "2.1 Problem: Independent Parameters", "text": "There are two practical problems with word search tables. First, while they can be pre-trained with large amounts of data to learn semantic and syntactical similarities between words, each vector is independent. That is, although models based on word search tables are often observed to learn that cats, kings, and queens exist in roughly the same linear analogy to each other as cat, king, and queen, the model does not represent the fact that adding an s at the end of the word is evidence of this transformation. This means that word search tables cannot generate representations for previously invisible words, such as frenchification, even when the components, French and ification, are observed in other contexts. Second, even if extensive data is available, it is impossible to actually store vectors for all word types. Since each word type receives a set of d parameters, the total number of parameter searchers is \u00d7 | where the Vocabulary size is."}, {"heading": "2.2 Solution: Compositional Models", "text": "Our solution to these problems is to construct a vector representation of a word by assembling smaller pieces into a representation of the larger form, an idea that has been explored in previous work by assembling morphemes into word representations (Luong et al., 2013; Botha and Blunsom, 2014; Soricut and Och, 2015). Morphemes are an ideal primitive for such a model, since they are by definition the minimal meaning-bearing (or syntax-bearing) units of language. The disadvantage of such approaches is that they depend on a morphological analyzer. In contrast, we want to compose representations of sign to word representation, but the relationship between word forms and their meanings is not trivial (de Saussure, 1916). While there are some compositional relationships, such as morphological processes such as adding -ing or -ly to a stem, have relatively regular effects, with many words with lexical similarities conveying different meanings, such as the course and pairing of words."}, {"heading": "3 C2W Model", "text": "It's the only way in which we're going to be able to determine a type of term name, which we're able to determine a certain type of term name, which is in the way, how they're in the way, how they're in the way, how they're in the way, how they're in the way, how they're in the way, how they're in the way, how they're in the way, how they're in the way, how they're in the way, how they're in the way, how they're in the way, how they're in the way, how they're in the way, how they're in the way, how they're in the way, how they're in the way, how they're in the way, how they're in the way, how they're in the way, how they're in the way, how they're in the way, how they're in the way, how they're in the way they're in the way, how they're in the way they're in the way, how they're in the way they're in the way, how they're in the way they're in the way, how they're in the way they're in the way, how they're in the way they're in the way they're in the way, how they're in the way they're in the way they're in the way they're in the way, how they're in the way they're in the way they're in the way they're in the way they're in the way, how they're in the way they're in the way they're in the way they're in the way they're in the way, how they're in the way they're in the way they're in the way they're in the way they're in the way they're in the way, how they're in the way they're in the way they're in the way they're in the way they're in the way they're in the way they're in the way they're in the way, how they're in the way they're in the way they're in the way they're in the way they're in the way they're in the way they're in the way they're in the way they're in the way, how they're in the way they're in the way they're in the way they're in the way they're in the way they're in the way they're in the way they're in the way they"}, {"heading": "4 Experiments: Language Modeling", "text": "Our proposed model is similar to models used to calculate composite representations of sentences from words (Cho et al., 2014; Li et al., 2015). However, the relationship between the meanings of individual words and the composite meaning of a phrase or sentence is probably more regular than the relationship between the representations of signs and the meaning of a word. Is our model capable of learning such an irregular relationship? We are now investigating this question empirically. Language modeling is a task that occurs in many applications in NLP. An effective LM requires modeling syntactical aspects of language, such as word orders (e.g. \"John is intelligent\" vs. \"John is intelligent\"), but also semantic aspects (e.g. \"John ate fish\" vs. \"Fish ate John\"). Thus, if our C2W model only covers regular aspects of words, such as prefixes and suffixes, the model yields worse results than word search tables."}, {"heading": "4.1 Language Model", "text": "Language modeling amounts to learning a function that calculates the log probability, log p (w), of a sentence w = (w1,.., wn), which can be broken down into the sum of conditional log probabilities, log p (wi | w1,.., wi \u2212 1) according to the chain rule. Our language model calculates vocabulary by projecting word representations w1,.., wi \u2212 1 using a recurring LSTM model (Mikolov et al., 2010; Sundermeyer et al., 2012).The model is illustrated in Figure 2, where we observe at the first level that every word wi is projected into their word representations. This can be done by using word search tables eWwi, in which case we have a regular recurring language model."}, {"heading": "4.2 Experiments", "text": "This year, as never before in the history of a country in which it is a country in which it is a country in which it is a country in which it is a country, a country in which it is a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country"}, {"heading": "5 Experiments: Part-of-speech Tagging", "text": "As a second example of the utility of our model, we turn to POS marking. As morphology is a strong indicator of syntax in many languages, much effort has been put into developing technical characteristics (Nakagawa et al., 2001; Mueller et al., 2013). We now show that some of these characteristics can be learned automatically using our model."}, {"heading": "5.1 Bi-LSTM Tagging Model", "text": "Our tagging model is also new, but very straightforward. It forms a Bi-LSTM over words, as shown in Figure 3. Entering the model is a sequence of features f (w1),.., f (wn). Again, word vectors can be generated using either the C2W model f (wi) = eCwi or word lookup tables f (wi) = eWwi. We also test the use of handmade features, in this case f1 (wi),..., fn (wi). Then, the sequential features f (w1),.., f (wn) are fed into a bi-directional LSTM model and get the forward states sf0,.., s f n and the reverse states sbN + 1,.., s b 0. Thus, the state sfi contains the information of all words from 0 to i and sbi from n to i. The forward states Wbl and the forward states Wbl are combined for each of the + L-L states."}, {"heading": "5.2 Experiments", "text": "In fact, it is the case that most people are able to identify with other people than with other people in other countries. In other countries, it is the case that most people in other countries are not able to identify with others. In other countries, it is the case that most people in other countries are not able to identify with others. In other countries, it is the case that most people in other countries are not able to identify with others. In other countries, it is the case that we withdraw the last 100 sentences from the training datasets and use them for tuning.Setup The POS model requires two sets of hyperparameters. First, words must be converted into continuous representations and the same hyperparameterization in the language."}, {"heading": "5.3 Discussion", "text": "It is important to note at this point that these results do not imply that our model always outperforms existing benchmarks, in fact, the results in most experiments are generally quite similar to existing systems. Even in Turkish, the use of morphological analyzers to extract additional characteristics could lead to similar results. Our work is not aimed at overcoming existing benchmarks, but at showing that much of the feature engineering performed in the benchmarks can be learned automatically from task-specific data. More importantly, we want to show that large-scale word-look tables can be compressed into a lookup table using characters and a compositional model that allows for better scaling of the model with the size of training data. This is a desirable feature of the model, as data is more common in many NLP tasks."}, {"heading": "6 Related Work", "text": "In fact, the fact is that you are able to be in a position without seeing yourself in a position to be in."}, {"heading": "7 Conclusion", "text": "We propose a C2W model that builds word embeddings for words without an explicit word search table. It therefore benefits from sensitivity to lexical aspects within words, since it requires characters as atomic units to derive the embeddings for the word. In POS tagging, our models that use characters alone can still achieve comparable or better results than modern systems without the need for manual development of such lexical features. Although both language modeling and POS tagging greatly benefit from morphological cues, the success of our models in languages with impoverished morphological cues shows that they are able to learn non-compositional aspects of how letters match. The code for the C2W model and our language model and POS tagger implementations are available at https: / / github.com / wlin12 / JNN."}, {"heading": "Acknowledgements", "text": "Wang Ling's doctoral thesis is supported by the FCT Scholarship SFRH / BD / 51157 / 2010. This research was partially supported by the U.S. Army Research Laboratory, the U.S. Army Research Office under Contract / Scholarship number W911NF-10-1-0533 and NSF IIS-1054319 and FCT on the multi-year contract UID / CEC / 50021 / 2013 and scholarship number SFRH / BPD / 68428 / 2010."}], "references": [{"title": "Floresta sint\u00e1(c)tica\u201d: a treebank for Portuguese", "author": ["Afonso et al.2002] Susana Afonso", "Eckhard Bick", "Renato Haber", "Diana Santos"], "venue": "In Proc. LREC", "citeRegEx": "Afonso et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Afonso et al\\.", "year": 2002}, {"title": "The annotation process in the Turkish treebank", "author": ["Kemal Oflazer", "Bilge Say"], "venue": "In In Proc. the 4th International Workshop on Linguistically Interpreted Corpora (LINC)", "citeRegEx": "Atalay et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Atalay et al\\.", "year": 2003}, {"title": "Improved transitionbased parsing by modeling characters instead of words with LSTMs", "author": ["Chris Dyer", "Noah A. Smith"], "venue": "In Proc. EMNLP", "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Compositional morphology for word representations and language modelling", "author": ["Botha", "Blunsom2014] Jan A. Botha", "Phil Blunsom"], "venue": "In Proc. ICML", "citeRegEx": "Botha et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Botha et al\\.", "year": 2014}, {"title": "The TIGER treebank", "author": ["Brants et al.2002] Sabine Brants", "Stefanie Dipper", "Silvia Hansen", "Wolfgang Lezius", "George Smith"], "venue": null, "citeRegEx": "Brants et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Brants et al\\.", "year": 2002}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher D. Manning"], "venue": "In Proc. EMNLP", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Joint learning of character and word embeddings", "author": ["Chen et al.2015] Xinxiong Chen", "Lei Xu", "Zhiyuan Liu", "Maosong Sun", "Huanbo Luan"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning phrase representations using RNN encoder\u2013decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch. JMLR", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["Graves", "Schmidhuber2005] Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": null, "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proc. EMNLP", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Automatic transliteration and backtransliteration by decision tree learning", "author": ["Kang", "Choi2000] Byung-Ju Kang", "Key-Sun Choi"], "venue": null, "citeRegEx": "Kang et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2000}, {"title": "When are tree structures necessary for deep learning of representations? CoRR, abs/1503.00185", "author": ["Li et al.2015] Jiwei Li", "Dan Jurafsky", "Eduard H. Hovy"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Entropy-based pruning for phrase-based machine translation", "author": ["Ling et al.2012] Wang Ling", "Jo\u00e3o Gra\u00e7a", "Isabel Trancoso", "Alan Black"], "venue": "In Proc. EMNLP", "citeRegEx": "Ling et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2012}, {"title": "Paraphrasing 4 microblog normalization", "author": ["Ling et al.2013] Wang Ling", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": "In Proc. EMNLP", "citeRegEx": "Ling et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2013}, {"title": "Two/too simple adaptations of word2vec for syntax problems", "author": ["Ling et al.2015] Wang Ling", "Chris Dyer", "Alan Black", "Isabel Trancoso"], "venue": "In Proc. NAACL", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "A recursive recurrent neural network for statistical machine translation", "author": ["Liu et al.2014] Shujie Liu", "Nan Yang", "Mu Li", "Ming Zhou"], "venue": "In Proc. ACL", "citeRegEx": "Liu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Luong et al.2013] Thang Luong", "Richard Socher", "Christopher Manning"], "venue": "In Proc. CoNLL", "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Partof-speech tagging from 97% to 100%: Is it time for some linguistics", "author": ["Christopher D. Manning"], "venue": "In Proc. CICLing", "citeRegEx": "Manning.,? \\Q2011\\E", "shortCiteRegEx": "Manning.", "year": 2011}, {"title": "Building a large annotated corpus of English: The Penn treebank", "author": ["Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": null, "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Rules, representations, and the English past tense", "author": ["Marslen-Wilson", "Tyler1998] William MarslenWilson", "Lorraine K. Tyler"], "venue": "Trends in Cognitive Science,", "citeRegEx": "Marslen.Wilson et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Marslen.Wilson et al\\.", "year": 1998}, {"title": "CESSECE: A multilingual and multilevel annotated corpus", "author": ["Mariona Taul\u00e9", "Llu\u0131\u0301s M\u00e1rquez", "Manuel Bertran"], "venue": "In Proc. LREC", "citeRegEx": "Mart\u0131\u0301 et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mart\u0131\u0301 et al\\.", "year": 2007}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In Proc. Interspeech", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean"], "venue": "In Proc. NIPS", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Less is more: significance-based ngram selection for smaller, better language models", "author": ["Moore", "Quirk2009] Robert C. Moore", "Chris Quirk"], "venue": "In Proc. EMNLP", "citeRegEx": "Moore et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Moore et al\\.", "year": 2009}, {"title": "Efficient higher-order CRFs for morphological tagging", "author": ["Helmut Schmid", "Hinrich Sch\u00fctze"], "venue": "In Proc. EMNLP", "citeRegEx": "Mueller et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mueller et al\\.", "year": 2013}, {"title": "Unknown word guessing and part-of-speech tagging using support vector machines", "author": ["Taku Kudoh", "Yuji Matsumoto"], "venue": "In In Proc. the Sixth Natural Language Processing Pacific Rim Symposium", "citeRegEx": "Nakagawa et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Nakagawa et al\\.", "year": 2001}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["Santos", "Zadrozny2014] Cicero D. Santos", "Bianca Zadrozny"], "venue": "In Proc. ICML", "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Scalable backoff language models", "author": ["Seymore", "Rosenfeld1996] Kristie Seymore", "Ronald Rosenfeld"], "venue": "In Proc. ICSLP", "citeRegEx": "Seymore et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Seymore et al\\.", "year": 1996}, {"title": "Semisupervised condensed nearest neighbor for part-of-speech tagging", "author": ["Anders S\u00f8gaard"], "venue": "In Proc. ACL", "citeRegEx": "S\u00f8gaard.,? \\Q2011\\E", "shortCiteRegEx": "S\u00f8gaard.", "year": 2011}, {"title": "Unsupervised morphology induction using word embeddings", "author": ["Soricut", "Och2015] Radu Soricut", "Franz Och"], "venue": "In Proc. NAACL", "citeRegEx": "Soricut et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Soricut et al\\.", "year": 2015}, {"title": "Semi-supervised training for the averaged perceptron POS tagger", "author": ["Jan Haji\u010d", "Jan Raab", "Miroslav Spousta"], "venue": "In Proc. EACL", "citeRegEx": "Spoustov\u00e1 et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Spoustov\u00e1 et al\\.", "year": 2009}, {"title": "Entropy-based pruning of backoff language models", "author": ["Andreas Stolcke"], "venue": "In In Proc. DARPA Broadcast News Transcription and Understanding Workshop", "citeRegEx": "Stolcke.,? \\Q1998\\E", "shortCiteRegEx": "Stolcke.", "year": 1998}, {"title": "Structure regularization for structured prediction: Theories and experiments. CoRR, abs/1411.6243", "author": ["Xu Sun"], "venue": null, "citeRegEx": "Sun.,? \\Q2014\\E", "shortCiteRegEx": "Sun.", "year": 2014}, {"title": "LSTM neural networks for language modeling", "author": ["Ralf Schl\u00fcter", "Hermann Ney"], "venue": "In Proc. Interspeech", "citeRegEx": "Sundermeyer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "Gathering and generating paraphrases from twitter with application to normalization", "author": ["Xu et al.2013] Wei Xu", "Alan Ritter", "Ralph Grishman"], "venue": "In Proceedings of the Sixth Workshop on Building and Using Comparable Corpora", "citeRegEx": "Xu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2013}, {"title": "A systematic comparison of phrase table pruning techniques", "author": ["Zens et al.2012] Richard Zens", "Daisy Stanton", "Peng Xu"], "venue": "In Proc. EMNLP", "citeRegEx": "Zens et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zens et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 8, "context": "been found to be a remarkably effective means for generating vector representations that perform well in other tasks (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Liu et al., 2014; Chen and Manning, 2014).", "startOffset": 117, "endOffset": 215}, {"referenceID": 17, "context": "been found to be a remarkably effective means for generating vector representations that perform well in other tasks (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Liu et al., 2014; Chen and Manning, 2014).", "startOffset": 117, "endOffset": 215}, {"referenceID": 23, "context": "Some examples of these include the skip-n-gram and CBOW models of Mikolov et al. (2013).", "startOffset": 66, "endOffset": 88}, {"referenceID": 18, "context": "This idea has been explored in prior work by composing morphemes into representations of words (Luong et al., 2013; Botha and Blunsom, 2014; Soricut and Och, 2015).", "startOffset": 95, "endOffset": 163}, {"referenceID": 7, "context": "from words (Cho et al., 2014; Li et al., 2015).", "startOffset": 11, "endOffset": 46}, {"referenceID": 13, "context": "from words (Cho et al., 2014; Li et al., 2015).", "startOffset": 11, "endOffset": 46}, {"referenceID": 23, "context": ", wi\u22121 using an recurrent LSTM model (Mikolov et al., 2010; Sundermeyer et al., 2012).", "startOffset": 37, "endOffset": 85}, {"referenceID": 35, "context": ", wi\u22121 using an recurrent LSTM model (Mikolov et al., 2010; Sundermeyer et al., 2012).", "startOffset": 37, "endOffset": 85}, {"referenceID": 27, "context": "As morphology is a strong indicator for syntax in many languages, a much effort has been spent engineering features (Nakagawa et al., 2001; Mueller et al., 2013).", "startOffset": 116, "endOffset": 161}, {"referenceID": 26, "context": "As morphology is a strong indicator for syntax in many languages, a much effort has been spent engineering features (Nakagawa et al., 2001; Mueller et al., 2013).", "startOffset": 116, "endOffset": 161}, {"referenceID": 20, "context": "on the Wall Street Journal of the Penn Treebank dataset (Marcus et al., 1993), using the standard splits (sections 1\u201318 for train, 19\u201321 for tuning and 22\u201324 for testing).", "startOffset": 56, "endOffset": 77}, {"referenceID": 22, "context": "We also perform tests on 4 other languages, which we obtained from the CoNLL shared tasks (Mart\u0131\u0301 et al., 2007; Brants et al., 2002; Afonso et al., 2002; Atalay et al., 2003).", "startOffset": 90, "endOffset": 174}, {"referenceID": 4, "context": "We also perform tests on 4 other languages, which we obtained from the CoNLL shared tasks (Mart\u0131\u0301 et al., 2007; Brants et al., 2002; Afonso et al., 2002; Atalay et al., 2003).", "startOffset": 90, "endOffset": 174}, {"referenceID": 0, "context": "We also perform tests on 4 other languages, which we obtained from the CoNLL shared tasks (Mart\u0131\u0301 et al., 2007; Brants et al., 2002; Afonso et al., 2002; Atalay et al., 2003).", "startOffset": 90, "endOffset": 174}, {"referenceID": 1, "context": "We also perform tests on 4 other languages, which we obtained from the CoNLL shared tasks (Mart\u0131\u0301 et al., 2007; Brants et al., 2002; Afonso et al., 2002; Atalay et al., 2003).", "startOffset": 90, "endOffset": 174}, {"referenceID": 19, "context": "Comparison with Benchmarks Most state-ofthe-art POS tagging systems are obtained by either learning or handcrafting good lexical features (Manning, 2011; Sun, 2014) or using ad-", "startOffset": 138, "endOffset": 164}, {"referenceID": 34, "context": "Comparison with Benchmarks Most state-ofthe-art POS tagging systems are obtained by either learning or handcrafting good lexical features (Manning, 2011; Sun, 2014) or using ad-", "startOffset": 138, "endOffset": 164}, {"referenceID": 16, "context": "We train embeddings using English wikipedia with the dataset used in (Ling et al., 2015), and the Structured Skip-n-gram model.", "startOffset": 69, "endOffset": 88}, {"referenceID": 19, "context": "0 (Manning, 2011) yes no 97.", "startOffset": 2, "endOffset": 17}, {"referenceID": 34, "context": "32 structReg (Sun, 2014) yes no 97.", "startOffset": 13, "endOffset": 24}, {"referenceID": 32, "context": "78 Mor\u010de (Spoustov\u00e1 et al., 2009) yes yes 97.", "startOffset": 9, "endOffset": 33}, {"referenceID": 30, "context": "44 SCCN (S\u00f8gaard, 2011) yes yes 97.", "startOffset": 8, "endOffset": 23}, {"referenceID": 6, "context": "This is generally accomplished by either performing a component-wise addition of the embeddings produced by word lookup tables (Chen et al., 2015), and that generated by the additional lexical model, or simply concatenating both representations (Santos and Zadrozny, 2014).", "startOffset": 127, "endOffset": 146}, {"referenceID": 8, "context": "Many models have been proposed, the work in (Collobert et al., 2011) refers that additional features sets Fi can be added to the one-hot representation and multiple lookup tables IFi can be learnt to project each of the feature sets to the same low-dimensional vector ew .", "startOffset": 44, "endOffset": 68}, {"referenceID": 6, "context": "The models proposed in (Santos and Zadrozny, 2014; Chen et al., 2015) allow the model to arbitrary extract meaningful lexical features from words by defining composi-", "startOffset": 23, "endOffset": 69}, {"referenceID": 6, "context": "The work in (Chen et al., 2015) defines a simple compositional model by summing over all characters in a given word, while the work in (Santos and Zadrozny, 2014) defines a convolutional network, which combines", "startOffset": 12, "endOffset": 31}, {"referenceID": 36, "context": "Aside from neural approaches, characterbased models have been applied to address multiple lexically oriented tasks, such as transliteration (Kang and Choi, 2000) and twitter normalization (Xu et al., 2013; Ling et al., 2013).", "startOffset": 188, "endOffset": 224}, {"referenceID": 15, "context": "Aside from neural approaches, characterbased models have been applied to address multiple lexically oriented tasks, such as transliteration (Kang and Choi, 2000) and twitter normalization (Xu et al., 2013; Ling et al., 2013).", "startOffset": 188, "endOffset": 224}, {"referenceID": 33, "context": "In language modeling, it is frequent to prune higher order ngrams that do not encode any additional information (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009).", "startOffset": 112, "endOffset": 179}, {"referenceID": 14, "context": "The same be applied in machine translation (Ling et al., 2012; Zens et al., 2012) by removing longer translation pairs that can be replicated using smaller ones.", "startOffset": 43, "endOffset": 81}, {"referenceID": 37, "context": "The same be applied in machine translation (Ling et al., 2012; Zens et al., 2012) by removing longer translation pairs that can be replicated using smaller ones.", "startOffset": 43, "endOffset": 81}, {"referenceID": 2, "context": "over word models in morphologically rich languages (Ballesteros et al., 2015).", "startOffset": 51, "endOffset": 77}], "year": 2015, "abstractText": "We introduce a model for constructing vector representations of words by composing characters using bidirectional LSTMs. Relative to traditional word representation models that have independent vectors for each word type, our model requires only a single vector per character type and a fixed set of parameters for the compositional model. Despite the compactness of this model and, more importantly, the arbitrary nature of the form\u2013function relationship in language, our \u201ccomposed\u201d word representations yield state-of-the-art results in language modeling and part-of-speech tagging. Benefits over traditional baselines are particularly pronounced in morphologically rich languages (e.g., Turkish).", "creator": "LaTeX with hyperref package"}}}