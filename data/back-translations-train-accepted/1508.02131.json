{"id": "1508.02131", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Aug-2015", "title": "Learning Structural Kernels for Natural Language Processing", "abstract": "Structural kernels are a flexible learning paradigm that has been widely used in Natural Language Processing. However, the problem of model selection in kernel-based methods is usually overlooked. Previous approaches mostly rely on setting default values for kernel hyperparameters or using grid search, which is slow and coarse-grained. In contrast, Bayesian methods allow efficient model selection by maximizing the evidence on the training data through gradient-based methods. In this paper we show how to perform this in the context of structural kernels by using Gaussian Processes. Experimental results on tree kernels show that this procedure results in better prediction performance compared to hyperparameter optimization via grid search. The framework proposed in this paper can be adapted to other structures besides trees, e.g., strings and graphs, thereby extending the utility of kernel-based methods.", "histories": [["v1", "Mon, 10 Aug 2015 05:57:14 GMT  (133kb,D)", "http://arxiv.org/abs/1508.02131v1", "Transactions of the Association for Computational Linguistics, 2015"]], "COMMENTS": "Transactions of the Association for Computational Linguistics, 2015", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["daniel beck", "trevor cohn", "christian hardmeier", "lucia specia"], "accepted": true, "id": "1508.02131"}, "pdf": {"name": "1508.02131.pdf", "metadata": {"source": "CRF", "title": "Learning Structural Kernels for Natural Language Processing", "authors": ["Daniel Beck", "Trevor Cohn", "Christian Hardmeier", "Lucia Specia"], "emails": ["debeck1@sheffield.ac.uk", "t.cohn@unimelb.edu.au", "christian.hardmeier@lingfil.uu.se", "l.specia@sheffield.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to decide for themselves what they want."}, {"heading": "2 Gaussian Process Regression", "text": "Our definition of GPs closely follows that of Rasmussen and Williams (2006). Consider a setting in which we have a dataset X = (x1, y1), (x2, y2), (xn, y2), where xi is a ddimensional input and yi is the corresponding output. Our goal is to infer from this an underlying function f: Rd \u2192 R to explain this data, i.e. f (xi) is the kernel function.In a regression setting, we assume that the response variables are noisy latent function evaluations, i.e., yi = f (xi) is the mean function that is normally the 0 constant, and the X level is the kernel function.We assume that the response variables are a noisy latent function evaluation, i.e."}, {"heading": "3 Tree Kernels", "text": "In this paper, we will focus on Subset Tree Kernels (SSTK), which is presented for the first time by two different tree fragments. \"We must remember that the tree fragments are undefined tree species.\" \"We must focus on them.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"\" We. \"\" \"We.\" \"\" \"We.\" \"\" \".\" \"\" We. \"\" \"\" \"We.\" \"\" \"\" \"We.\" \"\" \"\" \"We.\". \"\" \"\" \"We.\". \".\" \"\" We. \".\" \"\" \"We.\". \"\" \"\" We. \".\" \"\" \"We...\" \"\". \"\" \"We...\" \"\" \"\" \"We......\" \"\" \"\" \"\" We...... \"\" \"\" \"\" \"\" We..... \"\" \"\" \"\" \"\" We...... \"\" \"\" \"\" \"We.......\" \"\" \"\"... \"\" \"We........\" \"\" \"\" \"...\" \"\" \"\" We........... \"\" \"\" \"\"..... \"\" \"\" \".....\" \"\" \"\" We.............. \"\" \"\" \"\"......... \"\" \"\" \"\" \".........\" \"\" \"\" \"\"....................... \"\" \"\" \"\"............... \"\" \"\" \"\" \"...............\" \"\" \"\" \"\" \".......................\" \"\"........... \"\" \"\" \"\"...... \"\"...."}, {"heading": "3.1 Symbol-aware Subset Tree Kernel", "text": "While variation of the SSTK hyperparameters can lead to different weight schemes, they do so in a very coarse way. In some applications, it may be necessary to give more weight to specific fragments or fragments (e.g. that NPs are more important than ADVP in an information extraction setting).The symbol-conscious subset tree kernel (henceforth SASSTK) that we present here allows for finer-grained control of weights by using one \u03bb and one \u03b1 hyperparameter for each non-terminal symbol in the training data.The calculation uses a similar recursive formula to the SSTK, namely: \u2206 (n1, n2) = 0 pr (n1) 6 = pr (n2) \u03bbx pr (n1) = pr (n2)."}, {"heading": "3.2 Kernel Gradients", "text": "In order to enable hyperparameter optimization via a gradient descent, we must provide gradients for the cores. In this section, we derive the gradients for SASST. From Equation 2, we can obtain them in a vectorized way by looking at the gradients of the hyperparameter vectors \u03bb and \u03b1 over the respective function. Let us let k be the number of symbols considered in the model and let us be \u03bb and \u03b1 k-dimensional vectors containing the respective hyperparameters.In the following, we will use the notation \u0394i as an abbreviation for \u2206 (cin1, c i n2) and we will also omit the parameters of gx. We will start with the profile sequence: Wallace and \u03b1 are k-dimensional vectors containing the respective hyperparameters.In the following, we will use the notation \u0445i as an abbreviation for \u0445 (cin1, c i n2) and we will also omit the parameters of gx. We will start with the profile sequence: Wallace element matching: x."}, {"heading": "3.3 Kernel Normalization", "text": "It is common practice for tree kernels to be used to normalize the kernel, which helps reduce the random effect of tree size. Normalization can be achieved by using the following methods, where k kernels are the normalized kernel: k kernels (t1, t2) = k kernels (t1, t2) \u221a k (t1, t1) k (t2, t2).To apply this normalized version in the optimization process, we must also derive gradients for the normalization function. In the following equation, we use kij and k-ij as abbreviations for k kernels (ti, tj) and k kernels (ti, tj)."}, {"heading": "3.4 Other Extensions", "text": "Many other structure cores rely on recursive definitions and dynamic programming for their calculation. Examples include other tree cores such as the Partial Tree Kernel (Moschitti, 2006a) and string cores such as those defined on character diagrams (Lodhi et al., 2002) or word sequences (Cancedda et al., 2003). While in this paper we focus on the SSTK (and our proposed SASSTK), our approach can easily be extended to these other cores as long as all corresponding recursive definitions are differentiable."}, {"heading": "4 Synthetic Data Experiments", "text": "To answer this question, we conduct a series of experiments with synthetic data. We generate this data by using a set of 1000 natural language syntactic trees, in which we specify a random subset of 200 instances for testing purposes and use the remaining 800 instances as training. For each size of the training set, we define a GP across the entire data set, extract a function from it, and use the function output as a response variable for each tree. We try two different GP priorities, one with the SSTK and another with the SASSTK. The above conditions provide a controlled environment to test the modeling capabilities of our approach, as we know the exact distribution where the data comes from. The reasoning behind these experiments is that we need to be able to provide benefits in real-world tasks where the data distribution is not known, our models need to be learnable in this controlled environment as we also use an appropriate quantity.Finally, we also provide a search speed rating between our search approach speed and our natural language."}, {"heading": "4.1 SSTK Prior", "text": "Our first experiments use an SSTK as a core with \u03bb = 0.001, \u03b1 = 1 and \u03c32n = 0.01. After we have received the input trees and their sampled names, we define a new GP model, using only the training data plus the received response variables, this time with an SSTK with randomized hyperparameter values. We then optimize the GP and check if the learned hyperparameters are close to the original, using 10 random restarts to limit the effect of local optimisation. We also use the optimized GP to predict response variables on the test set and measure the root mean square error (RMSE). Our hypothesis is that with a reasonable sample size we can retrieve the original hyperparameter values and achieve low RMSE. For each training set size, we repeat the experiment 20 times. Figure 2 shows the results of these experiments 200, but if we reach the initial values after the small sample size, we will increase the initial values after the hyperparameter size is large enough."}, {"heading": "4.2 SASSTK Prior", "text": "The large number of hyperparameters of the SASSTK makes it more susceptible to optimization and overadjustment problems compared to the SSTK. This raises the question of how much data is needed to justify its use. To answer this question, we are conducting experiments similar to those above for the SSTK, except that we now have a GP with an SASSTK as its core. Instead of freely optimizing all hyperparameters, we are using a simpler version in which we attach the same value for each symbol \u03bb and \u03b1, with the exception of the symbol \"S.\" Effectively, this version has an additional \u03bb and an additional \u03b1 (henceforth \u03bbS and \u03b1S) compared to the SSTK. The previous hyperparameter values of the GP are set to \u03bb = 0.001, \u03bbS = 0.5, \u03b1 = 0.1, \u03b1S = 1 and \u04412n = 0.01. For each training set size, we form two GPs, one using this SASSTK and using the original use of optimized SK, and these SSE random results are explained by SSE using SSE and SSE random SSE restarts."}, {"heading": "4.3 Performance Experiments", "text": "To give an overview of how efficient the gradient-based method is compared to the grid search, we also run a series of experiments that measure the training time of the wall clock vs. RMSE on a test set. For both GP and SVM models, we use the SSTK as the kernel and we use the same synthetic data from the previous experiments 4. However, we run 20 test runs, with the test set having the same 200 instances for all runs and randomly sampling 200 instances from the remaining instances as training data. Figure 4 shows the curves for both GP and SVM models. We can see that the GP curve is achieved by increasing the maximum number of iterations of the gradient-based method (in this case L-BFGS) and the SVM curve is achieved by increasing the granularity of the grid size. We can see that the optimization of the GP model is consistently much faster than the execution of the grid search on the SVM model (although it has a larger number of logarithms)."}, {"heading": "5 NLP Experiments", "text": "Our experiments with NLP data address two regression tasks: Emotion Analysis and Quality Estimation. For both tasks, we use the Stanford parser (Manning et al., 2014) to obtain constituency trees for all sentences. Furthermore, instead of official splits, we perform a 5-fold cross-validation to get more reliable results."}, {"heading": "5.1 Emotion Analysis", "text": "The goal of emotion analysis is to automatically capture emotions in a text (Strapparava and Mihalcea, 2008).This problem is closely linked to the opinion on mining (Pang and Lee, 2008), with similar applications, but it is usually performed on a finer-grained level and involves predicting a series of labels for each text (one for each emotion) instead of a single label.Beck et al. (2014a) uses a multi-task GP for this task with a pocket-of-words representation. In theory it is possible to combine their multi-task models with our tree kernels, but to keep the focus of the experiments on the test tree approaches, here we use independently trained models per emotion.Dataset We use the dataset provided by the \"Affective Text\" -Shared Task in SemEval2007 (Strapparava and Mihalcea, 2007), which is composed of messages 1000."}, {"heading": "5.2 Quality Estimation", "text": "The aim of quality assessment is to provide a high-quality prediction for new, invisible machine translation texts (Blatz et al., 2004; Bojar et al., 2014). Application review results also include filtering machine translated sentences that would require more post-translation work than English translations (Specia et al., 2009), selecting the best translation methods from different MT systems (Specia et al., 2010) or between an MT system and a translation memory (He et al., 2010), and highlighting segments that need revision (Bach et al., 2011). While there are different quality metrics, here we focus on post-editing time prediction. Tree kernels were used before this task (with SVMs) by Hardmeier (2011) and Hardmeier et al. (2012) While their best tree models show kernels combined with a good set of features to expose them."}, {"heading": "5.3 Overfitting", "text": "Both NLP tasks show that GP models with a large number of hyperparameters (SASSTKfull in the case of Emotion Analysis and the free \u03b1 models in the Quality Estimation) outperform the corresponding datasets. While the Bayean formula for marginal probability helps to limit overfitting, it does not completely prevent it. Small datasets or invalid assumptions about the Gaussian distribution of data can still lead to ill-fitting models. Another means of reducing overfitting is a fully Bayesian approach, in which hyperparameters are considered random variables and marginalized (Osborne, 2010); this is a direction of research that we intend to pursue in the future.8"}, {"heading": "5.4 Extensions to Other Tasks", "text": "The GP framework introduced in Section 2 can be extended to non-regression problems by changing the probability function. For example, models for classification (Rasmussen and Williams, 2006, chap. 3), ordinal regression (Chu and Ghahramani, 2005), and structured prediction (Altun et al., 2004; Bratie res et al., 2013) have been proposed in the literature. As the probability is independent of the core, a natural future step is to apply the nuclei and models presented in this paper to various NLP tasks. Against this background, we conducted initial experiments in the analysis of constituencies. 9 The initial results were not conclusive, but we believe that this is due to naive approaches to using classification (1 - best result against all) and regression (using PARSEVAL metrics as a reactive variable)."}, {"heading": "6 Related Work", "text": "In fact, most of us will be able to feel the way they are, to behave as if they were able to survive themselves, and that they would be able to survive themselves if they were not able to survive themselves."}, {"heading": "7 Conclusions", "text": "This paper describes a Bayesian approach to structural kernel learning based on Gaussian processes for simple model selection. Experiments applying our models to synthetic data showed that it is possible to learn structural kernel hyperparameters with a relatively small amount of data. Furthermore, we achieved promising results in two NLP tasks, including Quality Estimation, where we exceeded the state of the art. Finally, we showed how these rich parameters can lead to more interpretable cores. Beyond empirical improvements, an important goal of this work is to present a method that enables new kernel developments by expanding the number of hyperparameters. We focused on the subtree core by proposing an extension and then deriving its gradients. This approach can be applied to any structural core as long as gradients are available. We hope that this work will serve as a starting point for future developments in these research directions."}, {"heading": "A Details on SVM Baselines", "text": "All SVM baselines use the -insensitive loss function. After reaching the best hyperparameter values, the optimization of the net search takes place via a triple cross-validation of the respective training set and the use of RMSE as a measurement quantity to be minimized. After reaching the best hyperparameter values, the SVM is retrained on the complete respective training set on the basis of these values. The specific intervals used in the net search depend on the task. For the execution experiments on synthetic data, we used an interval of [10 \u2212 2, 10] for C (regulation coefficient) and [10 \u2212 8, 1] for \u03bb and [10 \u2212 4, 2] for \u03b1. In each run, we gradually increase the size of the net by adding intermediate values for each interval. We keep a linear scale for the SSTK hyperparameters and a logarithmic scale for C and. As an example, Table 6 shows the resulting grid value for each mesh, including the 4 for each experiment, the value of the mesh is specified for each parameter including the P for all values in the C and a logarithmic scale."}, {"heading": "C / [10\u22122, 10\u22121, 1, 10]", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Gaussian Process Classification for Segmenting and Annotating Sequences", "author": ["Altun et al.2004] Yasemin Altun", "Thomas Hofmann", "Alexander J. Smola"], "venue": "In Proceedings of ICML", "citeRegEx": "Altun et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Altun et al\\.", "year": 2004}, {"title": "Goodness: A Method for Measuring Machine Translation Confidence", "author": ["Bach et al.2011] Nguyen Bach", "Fei Huang", "Yaser AlOnaizan"], "venue": "In Proceedings of ACL,", "citeRegEx": "Bach et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2011}, {"title": "Joint Emotion Analysis via Multi-task Gaussian Processes", "author": ["Beck et al.2014a] Daniel Beck", "Trevor Cohn", "Lucia Specia"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Beck et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Beck et al\\.", "year": 2014}, {"title": "SHEF-Lite 2.0 : Sparse Multi-task Gaussian Processes for Translation Quality Estimation", "author": ["Beck et al.2014b] Daniel Beck", "Kashif Shah", "Lucia Specia"], "venue": "In Proceedings of WMT14,", "citeRegEx": "Beck et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Beck et al\\.", "year": 2014}, {"title": "Random Search for Hyper-Parameter Optimization", "author": ["Bergstra", "Bengio2012] James Bergstra", "Yoshua Bengio"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bergstra et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2012}, {"title": "Confidence estimation for machine translation", "author": ["Blatz et al.2004] John Blatz", "Erin Fitzgerald", "George Foster"], "venue": "In Proceedings of the 20th Conference on Computational Linguistics,", "citeRegEx": "Blatz et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Blatz et al\\.", "year": 2004}, {"title": "Findings of the 2014", "author": ["Bojar et al.2014] Ondej Bojar", "Christian Buck", "Christian Federmann", "Barry Haddow", "Philipp Koehn", "Johannes Leveling", "Christof Monz", "Pavel Pecina", "Matt Post", "Herve Saint-amand", "Radu Soricut", "Lucia Specia", "Ale\u0161 Tamchyna"], "venue": null, "citeRegEx": "Bojar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bojar et al\\.", "year": 2014}, {"title": "Bayesian Structured Prediction using Gaussian Processes", "author": ["Novi Quadrianto", "Zoubin Ghahramani"], "venue": null, "citeRegEx": "Brati\u00e8res et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Brati\u00e8res et al\\.", "year": 2013}, {"title": "LIBSVM : A Library for Support Vector Machines", "author": ["Chang", "Lin2001] Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),", "citeRegEx": "Chang et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2001}, {"title": "Gaussian Processes for Ordinal Regression", "author": ["Chu", "Ghahramani2005] Wei Chu", "Zoubin Ghahramani"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Chu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chu et al\\.", "year": 2005}, {"title": "Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation", "author": ["Cohn", "Specia2013] Trevor Cohn", "Lucia Specia"], "venue": "In Proceedings of ACL,", "citeRegEx": "Cohn et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cohn et al\\.", "year": 2013}, {"title": "Convolution Kernels for Natural Language", "author": ["Collins", "Duffy2001] Michael Collins", "Nigel Duffy"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Collins et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2001}, {"title": "Graph Kernels and Gaussian Processes for Relational Reinforcement Learning", "author": ["Jan Ramon", "Thomas G\u00e4rtner"], "venue": "Machine Learning,", "citeRegEx": "Driessens et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Driessens et al\\.", "year": 2006}, {"title": "Improving Machine Translation Quality Prediction with Syntactic Tree Kernels", "author": ["Christian Hardmeier"], "venue": "In Proceedings of EAMT,", "citeRegEx": "Hardmeier.,? \\Q2011\\E", "shortCiteRegEx": "Hardmeier.", "year": 2011}, {"title": "Convolution Kernels on Discrete Structures", "author": ["David Haussler"], "venue": "Technical report,", "citeRegEx": "Haussler.,? \\Q1999\\E", "shortCiteRegEx": "Haussler.", "year": 1999}, {"title": "Bridging SMT and TM with Translation Recommendation", "author": ["He et al.2010] Yifan He", "Yanjun Ma", "Josef van Genabith", "Andy Way"], "venue": "In Proceedings of ACL,", "citeRegEx": "He et al\\.,? \\Q2010\\E", "shortCiteRegEx": "He et al\\.", "year": 2010}, {"title": "Gaussian Processes for Big Data", "author": ["Nicol\u00f2 Fusi", "Neil D. Lawrence"], "venue": "In Proceedings of UAI,", "citeRegEx": "Hensman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hensman et al\\.", "year": 2013}, {"title": "Gradientbased Optimization of Kernel-Target Alignment for Sequence Kernels Applied to Bacterial Gene Start Detection", "author": ["Igel et al.2007] Christian Igel", "Tobias Glasmachers", "Britta Mersch", "Nico Pfeifer"], "venue": null, "citeRegEx": "Igel et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Igel et al\\.", "year": 2007}, {"title": "Discriminative Reranking of Discourse Parses Using Tree Kernels", "author": ["Joty", "Moschitti2014] Shafiq Joty", "Alessandro Moschitti"], "venue": "In EMNLP,", "citeRegEx": "Joty et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Joty et al\\.", "year": 2014}, {"title": "Gaussian Processes and Reinforcement Learning for Identification and Control of an Autonomous Blimp", "author": ["Ko et al.2007] Jonathan Ko", "Daniel J. Klein", "Dieter Fox", "Dirk Haehnel"], "venue": "In Proceedings of IEEE International Conference on Robotics and Automation,", "citeRegEx": "Ko et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ko et al\\.", "year": 2007}, {"title": "Text Classification using String Kernels", "author": ["Lodhi et al.2002] Huma Lodhi", "Craig Saunders", "John Shawe-Taylor", "Nello Cristianini", "Chris Watkins"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Lodhi et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Lodhi et al\\.", "year": 2002}, {"title": "The Stanford CoreNLP Natural Language Processing Toolkit", "author": ["Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky"], "venue": "In Proceedings of ACL Demo Session,", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Tree Kernels for Semantic Role Labeling", "author": ["Daniele Pighin", "Roberto Basili"], "venue": "Computational Linguistics,", "citeRegEx": "Moschitti et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Moschitti et al\\.", "year": 2008}, {"title": "Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees", "author": ["Alessandro Moschitti"], "venue": "In Proceedings of ECML,", "citeRegEx": "Moschitti.,? \\Q2006\\E", "shortCiteRegEx": "Moschitti.", "year": 2006}, {"title": "Making Tree Kernels practical for Natural Language Learning", "author": ["Alessandro Moschitti"], "venue": "In EACL,", "citeRegEx": "Moschitti.,? \\Q2006\\E", "shortCiteRegEx": "Moschitti.", "year": 2006}, {"title": "Bayesian Gaussian Processes for Sequential Prediction, Optimisation and Quadrature", "author": ["Michael Osborne"], "venue": "Ph.D. thesis,", "citeRegEx": "Osborne.,? \\Q2010\\E", "shortCiteRegEx": "Osborne.", "year": 2010}, {"title": "Scikit-learn: Machine learning in Python", "author": ["Brucher", "Matthieu Perrot", "\u00c9douard Duchesnay."], "venue": "Journal of Machine Learning Research, 12:2825\u20132830.", "citeRegEx": "Brucher et al\\.,? 2011", "shortCiteRegEx": "Brucher et al\\.", "year": 2011}, {"title": "On Reverse Feature Engineering of Syntactic Tree Kernels", "author": ["Pighin", "Moschitti2010] Daniele Pighin", "Alessandro Moschitti"], "venue": "In Proceedings of CONLL,", "citeRegEx": "Pighin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Pighin et al\\.", "year": 2010}, {"title": "Embedding Semantic Similarity in Tree Kernels for Domain Adaptation of Relation Extraction", "author": ["Plank", "Moschitti2013] Barbara Plank", "Alessandro Moschitti"], "venue": "In Proceedings of ACL,", "citeRegEx": "Plank et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Plank et al\\.", "year": 2013}, {"title": "A temporal model of text periodicities using Gaussian Processes", "author": ["Preo\u0163iuc-Pietro", "Cohn2013] Daniel Preo\u0163iuc-Pietro", "Trevor Cohn"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Preo\u0163iuc.Pietro et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Preo\u0163iuc.Pietro et al\\.", "year": 2013}, {"title": "Gaussian processes for machine learning, volume 1", "author": ["Rasmussen", "Christopher K.I. Williams"], "venue": null, "citeRegEx": "Rasmussen et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen et al\\.", "year": 2006}, {"title": "UoW : Multi-task Learning Gaussian Process for Semantic Textual Similarity", "author": ["Rios", "Specia2014] Miguel Rios", "Lucia Specia"], "venue": "In Proceedings of SemEval,", "citeRegEx": "Rios et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rios et al\\.", "year": 2014}, {"title": "GPPS: A Gaussian Process Positioning System for Cellular Networks", "author": ["Marian Grigoras", "Volker Tresp", "Clemens Hoffmann"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Schwaighofer et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Schwaighofer et al\\.", "year": 2004}, {"title": "Learning Depth from Stereo", "author": ["Sinz et al.2004] Fabian H. Sinz", "Joaquin Qui\u00f1onero Candela", "G\u00f6khan H. Bak\u0131r", "Carl E. Rasmussen", "Matthias O. Franz"], "venue": "Pattern Recognition,", "citeRegEx": "Sinz et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Sinz et al\\.", "year": 2004}, {"title": "Estimating the sentence-level quality of machine translation systems", "author": ["Specia et al.2009] Lucia Specia", "Nicola Cancedda", "Marc Dymetman", "Marco Turchi", "Nello Cristianini"], "venue": "In Proceedings of EAMT,", "citeRegEx": "Specia et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Specia et al\\.", "year": 2009}, {"title": "Machine translation evaluation versus quality estimation", "author": ["Specia et al.2010] Lucia Specia", "Dhwaj Raj", "Marco Turchi"], "venue": "Machine Translation,", "citeRegEx": "Specia et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Specia et al\\.", "year": 2010}, {"title": "Learning to identify emotions", "author": ["Rada Mihalcea"], "venue": null, "citeRegEx": "Mihalcea.,? \\Q2008\\E", "shortCiteRegEx": "Mihalcea.", "year": 2008}], "referenceMentions": [{"referenceID": 16, "context": "GPs have been shown to achieve state of the art performance in various regression tasks (Hensman et al., 2013; Cohn and Specia, 2013).", "startOffset": 88, "endOffset": 133}, {"referenceID": 14, "context": "The seminal work on Convolution Kernels by Haussler (1999) defines a broad class of kernels on discrete structures by counting and weighting the number of substructures they share.", "startOffset": 43, "endOffset": 59}, {"referenceID": 23, "context": "See Pighin and Moschitti (2010) for details and a proof on this derivation.", "startOffset": 15, "endOffset": 32}, {"referenceID": 23, "context": "This hyperparameter was introduced by Moschitti (2006b)3 as a way to select between two different tree kernels.", "startOffset": 38, "endOffset": 56}, {"referenceID": 20, "context": "Examples include other tree kernels like the Partial Tree Kernel (Moschitti, 2006a) and string kernels like the ones defined on character ngrams (Lodhi et al., 2002) or word sequences (Cancedda et al.", "startOffset": 145, "endOffset": 165}, {"referenceID": 21, "context": "For both tasks, we use the Stanford parser (Manning et al., 2014) to obtain constituency trees for all sentences.", "startOffset": 43, "endOffset": 65}, {"referenceID": 2, "context": "Beck et al. (2014a) used a multi-task GP for this task with a bag-of-words feature representation.", "startOffset": 0, "endOffset": 20}, {"referenceID": 23, "context": "\u2022 SSTK: the SSTK formulation introduced by Moschitti (2006b);", "startOffset": 43, "endOffset": 61}, {"referenceID": 5, "context": "The goal of Quality Estimation is to provide a quality prediction for new, unseen machine translated texts (Blatz et al., 2004; Bojar et al., 2014).", "startOffset": 107, "endOffset": 147}, {"referenceID": 6, "context": "The goal of Quality Estimation is to provide a quality prediction for new, unseen machine translated texts (Blatz et al., 2004; Bojar et al., 2014).", "startOffset": 107, "endOffset": 147}, {"referenceID": 34, "context": "ples of applications include filtering machine translated sentences that would require more post-editing effort than translation from scratch (Specia et al., 2009), selecting the best translation from different MT systems (Specia et al.", "startOffset": 142, "endOffset": 163}, {"referenceID": 35, "context": ", 2009), selecting the best translation from different MT systems (Specia et al., 2010) or between an MT system and a translation memory (He et al.", "startOffset": 66, "endOffset": 87}, {"referenceID": 15, "context": ", 2010) or between an MT system and a translation memory (He et al., 2010), and highlighting segments that need revision (Bach et al.", "startOffset": 57, "endOffset": 74}, {"referenceID": 1, "context": ", 2010), and highlighting segments that need revision (Bach et al., 2011).", "startOffset": 54, "endOffset": 73}, {"referenceID": 13, "context": "Tree kernels have been used before in this task (with SVMs) by Hardmeier (2011) and Hardmeier et al.", "startOffset": 63, "endOffset": 80}, {"referenceID": 13, "context": "Tree kernels have been used before in this task (with SVMs) by Hardmeier (2011) and Hardmeier et al. (2012). While their best models combine tree kernels with a set of explicit features, they also show good results using only the tree kernels.", "startOffset": 63, "endOffset": 108}, {"referenceID": 6, "context": "\u2022 English-Spanish (en-es): This dataset was used in the WMT14 Quality Estimation shared task (Bojar et al., 2014), containing 858 sentences translated from English into Spanish and post-edited by an expert translator.", "startOffset": 93, "endOffset": 113}, {"referenceID": 25, "context": "Another means of reducing overfitting is by taking a fully Bayesian approach in which hyperparameters are considered as random variables and are marginalized out (Osborne, 2010); this is a research direction we plan to pursue in the future.", "startOffset": 162, "endOffset": 177}, {"referenceID": 0, "context": "3), ordinal regression (Chu and Ghahramani, 2005) and structured prediction (Altun et al., 2004; Brati\u00e8res et al., 2013) were proposed in the literature.", "startOffset": 76, "endOffset": 120}, {"referenceID": 7, "context": "3), ordinal regression (Chu and Ghahramani, 2005) and structured prediction (Altun et al., 2004; Brati\u00e8res et al., 2013) were proposed in the literature.", "startOffset": 76, "endOffset": 120}, {"referenceID": 22, "context": "Other tree kernel applications include Semantic Role Labelling (Moschitti et al., 2008) and Relation Extraction (Plank and Moschitti, 2013).", "startOffset": 63, "endOffset": 87}, {"referenceID": 20, "context": "String kernels were mostly used in Text Classification (Lodhi et al., 2002; Cancedda et al., 2003), while graph kernels have been used for recognizing Textual Entailment (Zanzotto and Dell\u2019Arciprete, 2009).", "startOffset": 55, "endOffset": 98}, {"referenceID": 19, "context": "Gaussian Processes are a major framework in machine learning nowadays: applications include Robotics (Ko et al., 2007), Geolocation (Schwaighofer et al.", "startOffset": 101, "endOffset": 118}, {"referenceID": 32, "context": ", 2007), Geolocation (Schwaighofer et al., 2004) and Computer Vision (Sinz et al.", "startOffset": 21, "endOffset": 48}, {"referenceID": 33, "context": ", 2004) and Computer Vision (Sinz et al., 2004).", "startOffset": 28, "endOffset": 47}, {"referenceID": 14, "context": "An approach similar to ours was proposed by Igel et al. (2007). They combine oligo kernels (a kind of ngram kernel) with MKL, derive their gradients and optimize towards a kernel alignment metric.", "startOffset": 44, "endOffset": 63}, {"referenceID": 14, "context": "An approach similar to ours was proposed by Igel et al. (2007). They combine oligo kernels (a kind of ngram kernel) with MKL, derive their gradients and optimize towards a kernel alignment metric. Compared to our approach, they restrict the length of the n-grams being considered, while we rely on dynamic programming to explore the whole substructure space. Also, their method does not take into account the underlying learning algorithm. Another recent approach proposed for model selection is random search (Bergstra and Bengio, 2012). Like grid search, it has the drawback of not employing gradient information, as it is designed for any kind of hyperparameters (including categorical ones). Structural kernels have been successfully employed in a number of NLP tasks. The original SSTK proposed by Collins and Duffy (2001) was used to rerank the output of syntactic parsers.", "startOffset": 44, "endOffset": 828}, {"referenceID": 2, "context": "Only very recently they have been successfully employed in a few NLP tasks such as translation quality estimation (Cohn and Specia, 2013; Beck et al., 2014b), detection of temporal patterns in text (Preo\u0163iuc-Pietro and Cohn, 2013), semantic similarity (Rios and Specia, 2014) and emotion analysis (Beck et al., 2014a). In terms of feature representations, previous work focused on the vectorial inputs and applied well-known kernels for these inputs, e.g. the RBF kernel. As shown on \u00a75.2, our approach is orthogonal to these previous ones, since kernels can be easily combined in different ways. It is important to note that we are not the first ones to combine GPs with kernels on structured inputs. Driessens et al. (2006) employed a combination of GPs and graph kernels for reinforcement learning.", "startOffset": 138, "endOffset": 726}], "year": 2015, "abstractText": "Structural kernels are a flexible learning paradigm that has been widely used in Natural Language Processing. However, the problem of model selection in kernel-based methods is usually overlooked. Previous approaches mostly rely on setting default values for kernel hyperparameters or using grid search, which is slow and coarse-grained. In contrast, Bayesian methods allow efficient model selection by maximizing the evidence on the training data through gradient-based methods. In this paper we show how to perform this in the context of structural kernels by using Gaussian Processes. Experimental results on tree kernels show that this procedure results in better prediction performance compared to hyperparameter optimization via grid search. The framework proposed in this paper can be adapted to other structures besides trees, e.g., strings and graphs, thereby extending the utility of kernel-based methods.", "creator": "LaTeX with hyperref package"}}}