{"id": "1508.02354", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Aug-2015", "title": "Syntax-Aware Multi-Sense Word Embeddings for Deep Compositional Models of Meaning", "abstract": "Deep compositional models of meaning acting on distributional representations of words in order to produce vectors of larger text constituents are evolving to a popular area of NLP research. We detail a compositional distributional framework based on a rich form of word embeddings that aims at facilitating the interactions between words in the context of a sentence. Embeddings and composition layers are jointly learned against a generic objective that enhances the vectors with syntactic information from the surrounding context. Furthermore, each word is associated with a number of senses, the most plausible of which is selected dynamically during the composition process. We evaluate the produced vectors qualitatively and quantitatively with positive results. At the sentence level, the effectiveness of the framework is demonstrated on the MSRPar task, for which we report results within the state-of-the-art range.", "histories": [["v1", "Mon, 10 Aug 2015 19:04:18 GMT  (205kb,D)", "https://arxiv.org/abs/1508.02354v1", "Accepted for presentation at EMNLP 2015"], ["v2", "Thu, 13 Aug 2015 12:50:43 GMT  (205kb,D)", "http://arxiv.org/abs/1508.02354v2", "Accepted for presentation at EMNLP 2015"]], "COMMENTS": "Accepted for presentation at EMNLP 2015", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.NE", "authors": ["jianpeng cheng 0002", "dimitri kartsaklis"], "accepted": true, "id": "1508.02354"}, "pdf": {"name": "1508.02354.pdf", "metadata": {"source": "CRF", "title": "Syntax-Aware Multi-Sense Word Embeddings for Deep Compositional Models of Meaning", "authors": ["Jianpeng Cheng", "Dimitri Kartsaklis"], "emails": ["jianpeng.cheng@stcatz.oxon.org", "d.kartsaklis@qmul.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Background and related work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Distributional models of meaning", "text": "Distribution models of meaning follow the distribution hypothesis (Harris, 1954), which states that two words that occur in similar contexts have similar meanings. Traditional approaches to constructing a word space are based on simple counting: A word is represented by a number vector (usually smoothed out by the use of a function such as point-by-point mutual information) that shows how often that word coincides with other possible context words in a text corpus. In contrast to these methods, a newer class of distribution models treats word representations as parameters that are directly optimized for a word prediction task (Bengio et al., 2003; Collobert et Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014). Rather than relying on observed random numbers, these models aim to maximize the objective function of a neural network-based architecture; Mikolov et al et al al., 2013; Pennington et al al al., 2014), instead of relying on observed random numbers, these models aim at maximizing the objective function of a skizzen-based architecture; Mikolov et al et al et al, 2013, better depicts the skizzen-based context as a conditional one (for example)."}, {"heading": "2.2 Syntactic awareness", "text": "Since the main purpose of distribution models has so far been to measure the semantic relationship of words, relatively little effort has been made to sensitize word vectors to information about the syntactic role of a word in a sentence. In some cases, the vectors are POStag-specific, so that \"book\" as a noun and \"book\" as a verb are represented by various vectors (Kartsaklis and Sadrzadeh, 2013). Furthermore, word spaces in which the context of a target word is determined by grammatical dependencies (Pado \"and Lapata, 2007) are more effective in capturing syntactic relationships than approaches based on simple word proximities. For word embeddings trained in neural environments, syntactic information is usually not explicitly taken into account, with some notable exceptions. On the lexical level, Levy and Goldberg (2014) will propose an extension of grammatical dependencies."}, {"heading": "2.3 Compositionality in distributional models", "text": "Methods aimed at equipping distribution models of meaning with compositional skills come in many different levels of complexity, from simple elementary vector operators such as addition and multiplication (Mitchell and Lapata, 2008) to category theory (Coecke et al., 2010). In this latter work, relational words (such as verbs or adjectives) are presented as multilinear maps that act on vectors that represent their arguments (nouns and noun phrases). Generally, the above models are flat in the sense that they have no functional parameters and the output is generated by the direct interaction of the inputs; nevertheless, it has been shown that they capture the compositional meaning of sentences to an adequate imbalance."}, {"heading": "2.4 Disambiguation in composition", "text": "Regardless of how they deal with the composition, all models of section 2.3 are based on ambiguous word spaces, in which each meaning of an ambiguous word is fused into a single vector. Especially in cases of homonymy (such as \"bank,\" \"organ,\" etc.), in which the same word is used to describe two or more completely independent concepts, this approach is problematic: the semantic representation of the word becomes the average of all senses, insufficient to express it in a reliable way. In order to solve this problem, an earlier disambiguation step of the word vectors is often introduced, the purpose of which is to find the word representations that best fit the given context before the composition takes place (Reddy et al., 2011; Kartsaklis et al., 2013; Kartsaklis et al., 2013; Sadrzadeh, 2013; Kartsaklis et al., 2014)."}, {"heading": "3 Syntax-based generic training", "text": "We propose a novel architecture for word embedding and a compositional model to use in a single step. Learning takes place in the context of a RecNN (or RNN), and both word embedding and parameters of the composition level are optimized against a general objective function that uses a hinged loss.Figure 1 shows the general form of recursive and recursive neural networks. In architectures of this form, a composition level is applied to each pair of input words x1 and x2 in the following way: p = g (Wx [1: 2] + b) (1), where x [1: 2] denotes the concatenation of the two vectors, g is a non-linear function, and W, b are the parameters of the model. In the case of RecNN, the composition process continues recursively by following a parameter tree until a vector for the whole sentence or phrase is produced."}, {"heading": "4 From words to senses", "text": "We now expand our model to include lexical ambiguity by applying a gated architecture similar to that used in the multi-sense model by Neelakantan et al. (2014), but applying the main idea to the compositional environment described in Section 3.We assume a fixed number of n senses per word.1 Each word is associated with a main vector (obtained, for example, by using an existing vector set or simply applying the process of Section 3 in a separate step), as well as with n vectors designating cluster centricists and an equal number of sense vectors. Both cluster centricists and sense vectors are randomly initialized at the beginning of the process. For each word in a training set, we prepare a context vector by averaging the main vectors of all other words in the same context, this context vector is selected with the cluster centricifiers of weighting by cosmic similarity, the one that corresponds most closely to the current sentence in the context chosen."}, {"heading": "5 Task-specific dynamic disambiguation", "text": "The model of Figure 2 decouples the formation of word vectors and compositional parameters from 1Note that in principle no fixed number of senses need be assumed; Neelakantan et al. (2014), for example, present a version of their model in which new senses are dynamically added when appropriate. Main (ambiguous) vktorssense vectorsgatecompositional layerphrase vector plausibilisibility layercompositional layer phrase vector loyerFigure 2: Training syntax-aware multi sense embeddings in the context of a RecNN.a specific task, and as a result from any task-specific training dataset. Note, however, that by replacing the plausibility layer with a classifier trained for a specific task, you obtain a task-specific network that trains transparent multisense word embeddings and applies dynamic disambiguation on the fly."}, {"heading": "6 A siamese network for paraphrase detection", "text": "We will test the dynamic disambiguation framework of section 5 in a paraphrase recognition task = below, a paraphrase is defined as a reformulation of the meaning of asentence with different words and / or syntax. Thus, the aim of a paraphrase recognition model is to examine two sentences and decide whether they express the same meaning.While the usual method of addressing this problem is to use a classifier based (for example) on the concatenation of the two sentence vectors, in this work we are following a novel perspective: Specifically, we are applying a Siamese architecture (Bromley et al., 1993), a concept widely used in computer vision (Hadsell et al., 2006; Sun et al., 2014) While Siamese networks have also been used in the past for NLP purposes (for example, by Yih et al. (2011))), in order to apply the best of our knowledge, this is the first time that such a setting is applied."}, {"heading": "7 Experiments", "text": "We evaluate the quality of the compositional word vectors and the proposed in-depth compositional framework in the tasks of word similarity or paraphrase recognition."}, {"heading": "7.1 Model pre-training", "text": "In all experiments, word representations and composition models are pre-trained at the British National Corpus (BNC), a universal text corpus containing 6 million sentences of written and spoken English. For comparison, we use two sets of word vectors and composition models, one ambiguous and one multisense (fixation of 3 senses per word).The dimension of the embedding is 300. Like our composition architectures, we use a RecNN and an RNN. In the RecNN case, we put the words together by following the result of an external parser, while in the RNN the composition takes place in order from left to right. To avoid the exploding or disappearing gradient problem (Bengio et al., 1994) for long sentences, we use a long-term short-term memory (LSTM) network (Hochreiter and Schmidhuber, 1997)."}, {"heading": "7.2 Qualitative evaluation of the word vectors", "text": "We compare the results with those of the Skipgram Model (SG) by Mikolov et al. (2013) and the Language Model (CW) by Collobert and Weston (2008). We refer to our model as SAMS (SyntaxAware Multi-Sense). The results in Table 1 clearly show that our model tends to group words that are both semantically and syntactically related; for example, and in contrast to the comparative models that group words only at the semantic level, our model is able to maintain tensions, numbers (singulars and plurals) and inconsistencies; the observed behavior is comparable to that of embedding models with objective functions that are conditioned by grammatical relationships between words; for example, Levy and Goldberg (2014) provide a similar table for their dependency-based extension of the Skip-gram model."}, {"heading": "7.3 Word similarity", "text": "We now proceed to a quantitative evaluation of our embedding on the Stanford Contextual Word Similarity (SCWS) dataset by Huang et al. (2012), which contains 2,003 pairs of words and the contexts in which they occur, so we can use the context information to select the most appropriate sense for each ambiguous word. Similar to Neelakantan et al. (2014), we use three different metrics: globalSim measures the similarity between two ambiguous word vectors; localSim selects a single meaning for each word based on the context and calculates the similarity between the two sense vectors; avgSim presents each word as a weighted average of all senses in the given context and calculates the similarity between the two ambiguous word vectors; localSim shows the similarity between the two weighted meaning vectors and the value-added version of selectors.We calculate and report the correlation between the commonalities embedded and hierarchies (2)."}, {"heading": "7.4 Paraphrase detection", "text": "This year, it has reached the point where it is a reactionary U-turn, capable of retaliating."}, {"heading": "8 Conclusion and future work", "text": "The main contribution of this paper is a profound compositional distribution model that works on linguistically motivated word embeds.4 The effectiveness of syntax-conscious, multi-meaning word vectors and the dynamic compositional distribution framework in which they are used has been demonstrated by suitable tasks at the lexical or sentence level with very positive results. Besides, we also demonstrated the advantages of a Siamese architecture in the context of a paraphrase recognition task. While the architectures tested in this work were limited to a RecNN and an RNN, the ideas we presented are in principle directly applicable to any kind of deep network. In a future step, we intend to test the proposed models on a revolutionary compositional architecture similar to that of Kalchbrenner et al. (2014)."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the three anonymous reviewers for their useful comments as well as Nal Kalchbrenner and Ed Grefenstette for early discussions and suggestions on the paper and Simon S uster for comments on the final draft. Dimitri Kartsaklis would like to thank AFOSR.3Source: ACL Wiki (http: / / www.aclweb.org / aclwiki), August 2015.4Code in Python / Theano and the word embeddings can be found at https: / / github.com / cheng6076."}], "references": [{"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs", "author": ["Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski."], "venue": "context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for", "citeRegEx": "Baroni et al\\.,? 2014", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi."], "venue": "Neural Networks, IEEE Transactions on, 5(2):157\u2013166.", "citeRegEx": "Bengio et al\\.,? 1994", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "The Journal of Machine Learning Research, 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["William Blacoe", "Mirella Lapata."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language", "citeRegEx": "Blacoe and Lapata.,? 2012", "shortCiteRegEx": "Blacoe and Lapata.", "year": 2012}, {"title": "Signature verification using a siamese time delay neural network", "author": ["Jane Bromley", "James W Bentz", "L\u00e9on Bottou", "Isabelle Guyon", "Yann LeCun", "Cliff Moore", "Eduard S\u00e4ckinger", "Roopak Shah."], "venue": "International Journal of Pattern Recognition", "citeRegEx": "Bromley et al\\.,? 1993", "shortCiteRegEx": "Bromley et al\\.", "year": 1993}, {"title": "Investigating the role of prior disambiguation in deep-learning compositional models of meaning", "author": ["Jianpeng Cheng", "Dimitri Kartsaklis", "Edward Grefenstette."], "venue": "2nd Workshop of Learning Semantics, NIPS 2014, Montreal, Canada,", "citeRegEx": "Cheng et al\\.,? 2014", "shortCiteRegEx": "Cheng et al\\.", "year": 2014}, {"title": "Mathematical foundations for a distributional compositional model of meaning", "author": ["B Coecke", "M Sadrzadeh", "S Clark."], "venue": "lambek festschrift. Linguistic Analysis, 36:345\u2013384.", "citeRegEx": "Coecke et al\\.,? 2010", "shortCiteRegEx": "Coecke et al\\.", "year": 2010}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of the 25th international conference on Machine learning, pages 160\u2013167. ACM.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Paraphrase identification as probabilistic quasi-synchronous recognition", "author": ["Dipanjan Das", "Noah A Smith."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Lan-", "citeRegEx": "Das and Smith.,? 2009", "shortCiteRegEx": "Das and Smith.", "year": 2009}, {"title": "Automatically constructing a corpus of sentential paraphrases", "author": ["W.B. Dolan", "C. Brockett."], "venue": "Third International Workshop on Paraphrasing (IWP2005).", "citeRegEx": "Dolan and Brockett.,? 2005", "shortCiteRegEx": "Dolan and Brockett.", "year": 2005}, {"title": "A semantic similarity approach to paraphrase detection", "author": ["Samuel Fernando", "Mark Stevenson."], "venue": "Proceedings of the 11th Annual Research Colloquium of the UK Special Interest Group for Computational Linguistics, pages 45\u201352. Citeseer.", "citeRegEx": "Fernando and Stevenson.,? 2008", "shortCiteRegEx": "Fernando and Stevenson.", "year": 2008}, {"title": "Ppdb: The paraphrase database", "author": ["Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch."], "venue": "HLT-NAACL, pages 758\u2013764.", "citeRegEx": "Ganitkevitch et al\\.,? 2013", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["Raia Hadsell", "Sumit Chopra", "Yann LeCun."], "venue": "Computer vision and pattern recognition, 2006 IEEE computer society conference on, volume 2, pages 1735\u20131742. IEEE.", "citeRegEx": "Hadsell et al\\.,? 2006", "shortCiteRegEx": "Hadsell et al\\.", "year": 2006}, {"title": "Distributional structure", "author": ["Zellig S Harris."], "venue": "Word.", "citeRegEx": "Harris.,? 1954", "shortCiteRegEx": "Harris.", "year": 1954}, {"title": "Jointly learning word representations and composition functions using predicate-argument structures", "author": ["Kazuma Hashimoto", "Pontus Stenetorp", "Makoto Miwa", "Yoshimasa Tsuruoka."], "venue": "Proceedings of the 2014 Conference on Empirical Methods", "citeRegEx": "Hashimoto et al\\.,? 2014", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2014}, {"title": "The role of syntax in vector space models of compositional semantics", "author": ["Karl Moritz Hermann", "Phil Blunsom."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 894\u2013904,", "citeRegEx": "Hermann and Blunsom.,? 2013", "shortCiteRegEx": "Hermann and Blunsom.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Eric H Huang", "Richard Socher", "Christopher D Manning", "Andrew Y Ng."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Semantic similarity of short texts", "author": ["Aminul Islam", "Diana Inkpen."], "venue": "Recent Advances in Natural Language Processing V, 309:227\u2013236.", "citeRegEx": "Islam and Inkpen.,? 2009", "shortCiteRegEx": "Islam and Inkpen.", "year": 2009}, {"title": "Discriminative improvements to distributional sentence similarity", "author": ["Yangfeng Ji", "Jacob Eisenstein."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 891\u2013896, Seattle, Washington, USA, Oc-", "citeRegEx": "Ji and Eisenstein.,? 2013", "shortCiteRegEx": "Ji and Eisenstein.", "year": 2013}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, June.", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Prior disambiguation of word tensors for constructing sentence vectors", "author": ["Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1590\u20131601, Seattle, Wash-", "citeRegEx": "Kartsaklis and Sadrzadeh.,? 2013", "shortCiteRegEx": "Kartsaklis and Sadrzadeh.", "year": 2013}, {"title": "Separating disambiguation from composition in distributional semantics", "author": ["Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh", "Stephen Pulman."], "venue": "Proceedings of 17th Conference on Natural Language Learning (CoNLL), pages 114\u2013123, Sofia, Bulgaria,", "citeRegEx": "Kartsaklis et al\\.,? 2013", "shortCiteRegEx": "Kartsaklis et al\\.", "year": 2013}, {"title": "Resolving lexical ambiguity in tensor regression models of meaning", "author": ["Dimitri Kartsaklis", "Nal Kalchbrenner", "Mehrnoosh Sadrzadeh."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Vol. 2: Short Pa-", "citeRegEx": "Kartsaklis et al\\.,? 2014", "shortCiteRegEx": "Kartsaklis et al\\.", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner."], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324.", "citeRegEx": "LeCun et al\\.,? 1998", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Dependencybased word embeddings", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 302\u2013308, Baltimore, Maryland, June. Association", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Re-examining machine translation metrics for paraphrase identification", "author": ["Nitin Madnani", "Joel Tetreault", "Martin Chodorow."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-", "citeRegEx": "Madnani et al\\.,? 2012", "shortCiteRegEx": "Madnani et al\\.", "year": 2012}, {"title": "Corpus-based and knowledge-based measures of text semantic similarity", "author": ["Rada Mihalcea", "Courtney Corley", "Carlo Strapparava."], "venue": "AAAI, volume 6, pages 775\u2013780.", "citeRegEx": "Mihalcea et al\\.,? 2006", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2006}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Vector-based models of semantic composition", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "Proceedings of ACL-08: HLT, pages 236\u2013244, Columbus, Ohio, June. Association for Computational Linguistics.", "citeRegEx": "Mitchell and Lapata.,? 2008", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2008}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["Andriy Mnih", "Koray Kavukcuoglu."], "venue": "Advances in Neural Information Processing Systems, pages 2265\u20132273.", "citeRegEx": "Mnih and Kavukcuoglu.,? 2013", "shortCiteRegEx": "Mnih and Kavukcuoglu.", "year": 2013}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton."], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 807\u2013814.", "citeRegEx": "Nair and Hinton.,? 2010", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "Efficient nonparametric estimation of multiple embeddings per word in vector space", "author": ["Arvind Neelakantan", "Jeevan Shankar", "Alexandre Passos", "Andrew McCallum."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Neelakantan et al\\.,? 2014", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2014}, {"title": "Dependency-based Construction of Semantic Space Models", "author": ["S. Pad\u00f3", "M. Lapata."], "venue": "Computational Linguistics, 33(2):161\u2013199.", "citeRegEx": "Pad\u00f3 and Lapata.,? 2007", "shortCiteRegEx": "Pad\u00f3 and Lapata.", "year": 2007}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Recursive distributed representations", "author": ["Jordan B Pollack."], "venue": "Artificial Intelligence, 46(1):77\u2013105.", "citeRegEx": "Pollack.,? 1990", "shortCiteRegEx": "Pollack.", "year": 1990}, {"title": "Paraphrase recognition via dissimilarity significance classification", "author": ["Long Qiu", "Min-Yen Kan", "Tat-Seng Chua."], "venue": "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 18\u201326. Association for Compu-", "citeRegEx": "Qiu et al\\.,? 2006", "shortCiteRegEx": "Qiu et al\\.", "year": 2006}, {"title": "Dynamic and static prototype vectors for semantic composition", "author": ["Siva Reddy", "Ioannis P Klapaftis", "Diana McCarthy", "Suresh Manandhar."], "venue": "IJCNLP, pages 705\u2013713.", "citeRegEx": "Reddy et al\\.,? 2011", "shortCiteRegEx": "Reddy et al\\.", "year": 2011}, {"title": "Paraphrase identification with lexicosyntactic graph subsumption", "author": ["Vasile Rus", "Philip M McCarthy", "Mihai C Lintean", "Danielle S McNamara", "Arthur C Graesser."], "venue": "FLAIRS conference, pages 201\u2013206.", "citeRegEx": "Rus et al\\.,? 2008", "shortCiteRegEx": "Rus et al\\.", "year": 2008}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Richard Socher", "Eric H Huang", "Jeffrey Pennin", "Christopher D Manning", "Andrew Y Ng."], "venue": "Advances in Neural Information Processing Systems, pages 801\u2013809.", "citeRegEx": "Socher et al\\.,? 2011a", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff C Lin", "Chris Manning", "Andrew Y Ng."], "venue": "Proceedings of the 28th international conference on machine learning (ICML-11), pages 129\u2013136.", "citeRegEx": "Socher et al\\.,? 2011b", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["R. Socher", "B. Huval", "C. Manning", "Ng. A."], "venue": "Conference on Empirical Methods in Natural Language Processing 2012.", "citeRegEx": "Socher et al\\.,? 2012", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Deep learning face representation by joint identification-verification", "author": ["Yi Sun", "Yuheng Chen", "Xiaogang Wang", "Xiaoou Tang."], "venue": "Advances in Neural Information Processing Systems, pages 1988\u2013 1996.", "citeRegEx": "Sun et al\\.,? 2014", "shortCiteRegEx": "Sun et al\\.", "year": 2014}, {"title": "Using dependency-based features to take the para-farce out of paraphrase", "author": ["Stephen Wan", "Mark Dras", "Robert Dale", "C\u00e9cile Paris."], "venue": "Proceedings of the Australasian Language Technology Workshop, volume 2006.", "citeRegEx": "Wan et al\\.,? 2006", "shortCiteRegEx": "Wan et al\\.", "year": 2006}, {"title": "Learning discriminative projections for text similarity measures", "author": ["Wen-tau Yih", "Kristina Toutanova", "John C. Platt", "Christopher Meek."], "venue": "Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 247\u2013256, Port-", "citeRegEx": "Yih et al\\.,? 2011", "shortCiteRegEx": "Yih et al\\.", "year": 2011}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler."], "venue": "arXiv preprint arXiv:1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 7, "context": "tics of the target word with words belonging to a representative subset of the vocabulary, or by directly optimizing the word vectors against an objective function in some neural-network based architecture (Collobert and Weston, 2008; Mikolov et al., 2013).", "startOffset": 206, "endOffset": 256}, {"referenceID": 29, "context": "tics of the target word with words belonging to a representative subset of the vocabulary, or by directly optimizing the word vectors against an objective function in some neural-network based architecture (Collobert and Weston, 2008; Mikolov et al., 2013).", "startOffset": 206, "endOffset": 256}, {"referenceID": 42, "context": "While the nature and complexity of these compositional models may vary, approaches based on deep-learning architectures have been shown to be especially successful in modelling the meaning of sentences for a variety of tasks (Socher et al., 2012; Kalchbrenner et al., 2014).", "startOffset": 225, "endOffset": 273}, {"referenceID": 20, "context": "While the nature and complexity of these compositional models may vary, approaches based on deep-learning architectures have been shown to be especially successful in modelling the meaning of sentences for a variety of tasks (Socher et al., 2012; Kalchbrenner et al., 2014).", "startOffset": 225, "endOffset": 273}, {"referenceID": 21, "context": "ter performance than their \u201cambiguous\u201d counterparts (Kartsaklis and Sadrzadeh, 2013; Kartsaklis et al., 2014).", "startOffset": 52, "endOffset": 109}, {"referenceID": 23, "context": "ter performance than their \u201cambiguous\u201d counterparts (Kartsaklis and Sadrzadeh, 2013; Kartsaklis et al., 2014).", "startOffset": 52, "endOffset": 109}, {"referenceID": 5, "context": "A first attempt to test these observations in a deep compositional setting has been presented by Cheng et al. (2014) with promising results.", "startOffset": 97, "endOffset": 117}, {"referenceID": 33, "context": "Note that this is in contrast with recent work in multi-sense neural word embeddings (Neelakantan et al., 2014), in which the word senses are learned without any compositional considerations in mind.", "startOffset": 85, "endOffset": 111}, {"referenceID": 7, "context": "Furthermore, we make the word embeddings syntax-aware by introducing a variation of the hinge loss objective function of Collobert and Weston (2008), in which the goal is not only to predict the occurrence of a target word in a context, but to also predict the position of the word within that context.", "startOffset": 121, "endOffset": 149}, {"referenceID": 17, "context": "Indeed, experimental evaluation shows that the produced word embeddings can serve as a high quality general-purpose semantic word space, presenting performance on the Stanford Contextual Word Similarity (SCWS) dataset of Huang et al. (2012) competitive to and even better of the performance of well-established neural word embeddings sets.", "startOffset": 221, "endOffset": 241}, {"referenceID": 9, "context": "In the context of paraphrase detection, we achieve a result very close to the current state-ofthe-art on the Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005).", "startOffset": 146, "endOffset": 172}, {"referenceID": 4, "context": "An interesting aspect at the sideline of the paraphrase detection experiment is that, in contrast to mainstream approaches that mainly rely on simple forms of classifiers, we approach the problem by following a siamese architecture (Bromley et al., 1993).", "startOffset": 232, "endOffset": 254}, {"referenceID": 13, "context": "Distributional models of meaning follow the distributional hypothesis (Harris, 1954), which states that two words that occur in similar contexts have similar meanings.", "startOffset": 70, "endOffset": 84}, {"referenceID": 2, "context": "In contrast to these methods, a recent class of distributional models treat word representations as parameters directly optimized on a word prediction task (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 156, "endOffset": 252}, {"referenceID": 7, "context": "In contrast to these methods, a recent class of distributional models treat word representations as parameters directly optimized on a word prediction task (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 156, "endOffset": 252}, {"referenceID": 29, "context": "In contrast to these methods, a recent class of distributional models treat word representations as parameters directly optimized on a word prediction task (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 156, "endOffset": 252}, {"referenceID": 35, "context": "In contrast to these methods, a recent class of distributional models treat word representations as parameters directly optimized on a word prediction task (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 156, "endOffset": 252}, {"referenceID": 0, "context": "Recent studies have shown that, compared to their co-occurrence counterparts, neural word vectors reflect better the semantic relationships between words (Baroni et al., 2014) and are more effective in compositional settings (Milajevs et al.", "startOffset": 154, "endOffset": 175}, {"referenceID": 0, "context": "In contrast to these methods, a recent class of distributional models treat word representations as parameters directly optimized on a word prediction task (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014). Instead of relying on observed cooccurrence counts, these models aim to maximize the objective function of a neural net-based architecture; Mikolov et al. (2013), for example, compute the conditional probability of observing words in a context around a target word (an approach known as the skip-gram model).", "startOffset": 157, "endOffset": 416}, {"referenceID": 21, "context": "In some cases the vectors are POStag specific, so that \u2018book\u2019 as noun and \u2018book\u2019 as verb are represented by different vectors (Kartsaklis and Sadrzadeh, 2013).", "startOffset": 126, "endOffset": 158}, {"referenceID": 34, "context": "Furthermore, word spaces in which the context of a target word is determined by means of grammatical dependencies (Pad\u00f3 and Lapata, 2007) are more effective in capturing syntactic relations than approaches based on simple word proximity.", "startOffset": 114, "endOffset": 137}, {"referenceID": 25, "context": "At the lexical level, Levy and Goldberg (2014) propose an extension of the skip-gram model", "startOffset": 22, "endOffset": 47}, {"referenceID": 29, "context": "Following a different approach, Mnih and Kavukcuoglu (2013) weight the vector of each context word depending on its distance from the target word.", "startOffset": 32, "endOffset": 60}, {"referenceID": 14, "context": "With regard to compositional settings (discussed in the next section), Hashimoto et al. (2014) use dependencybased word embeddings by employing a hinge loss objective, while Hermann and Blunsom (2013) condition their objectives on the CCG types of the involved words.", "startOffset": 71, "endOffset": 95}, {"referenceID": 14, "context": "With regard to compositional settings (discussed in the next section), Hashimoto et al. (2014) use dependencybased word embeddings by employing a hinge loss objective, while Hermann and Blunsom (2013) condition their objectives on the CCG types of the involved words.", "startOffset": 71, "endOffset": 201}, {"referenceID": 30, "context": "The methods that aim to equip distributional models of meaning with compositional abilities come in many different levels of sophistication, from simple element-wise vector operators such as addition and multiplication (Mitchell and Lapata, 2008) to category theory (Coecke et al.", "startOffset": 219, "endOffset": 246}, {"referenceID": 6, "context": "The methods that aim to equip distributional models of meaning with compositional abilities come in many different levels of sophistication, from simple element-wise vector operators such as addition and multiplication (Mitchell and Lapata, 2008) to category theory (Coecke et al., 2010).", "startOffset": 266, "endOffset": 287}, {"referenceID": 36, "context": "The idea of using neural networks for compositionality in language appeared 25 years ago in a seminal paper by Pollack (1990), and has been recently re-popularized by Socher and colleagues", "startOffset": 111, "endOffset": 126}, {"referenceID": 40, "context": "(Socher et al., 2011a; Socher et al., 2012).", "startOffset": 0, "endOffset": 43}, {"referenceID": 42, "context": "(Socher et al., 2011a; Socher et al., 2012).", "startOffset": 0, "endOffset": 43}, {"referenceID": 41, "context": "(Socher et al., 2011b), where the words get composed by following a parse tree.", "startOffset": 0, "endOffset": 22}, {"referenceID": 28, "context": "A particular variant of the RecNN is the recurrent neural network (RNN), in which a sentence is assumed to be generated by aggregating words in sequence (Mikolov et al., 2010).", "startOffset": 153, "endOffset": 175}, {"referenceID": 20, "context": "Furthermore, some recent work (Kalchbrenner et al., 2014) models the meaning of sentences by utilizing the concept of a convolutional neural network (LeCun et al.", "startOffset": 30, "endOffset": 57}, {"referenceID": 24, "context": ", 2014) models the meaning of sentences by utilizing the concept of a convolutional neural network (LeCun et al., 1998), the main characteristic of which is that it acts on", "startOffset": 99, "endOffset": 119}, {"referenceID": 38, "context": "To address this problem, a prior disambiguation step on the word vectors is often introduced, the purpose of which is to find the word representations that best fit to the given context, before composition takes place (Reddy et al., 2011; Kartsaklis et al., 2013; Kartsaklis and Sadrzadeh, 2013; Kartsaklis et al., 2014).", "startOffset": 218, "endOffset": 320}, {"referenceID": 22, "context": "To address this problem, a prior disambiguation step on the word vectors is often introduced, the purpose of which is to find the word representations that best fit to the given context, before composition takes place (Reddy et al., 2011; Kartsaklis et al., 2013; Kartsaklis and Sadrzadeh, 2013; Kartsaklis et al., 2014).", "startOffset": 218, "endOffset": 320}, {"referenceID": 21, "context": "To address this problem, a prior disambiguation step on the word vectors is often introduced, the purpose of which is to find the word representations that best fit to the given context, before composition takes place (Reddy et al., 2011; Kartsaklis et al., 2013; Kartsaklis and Sadrzadeh, 2013; Kartsaklis et al., 2014).", "startOffset": 218, "endOffset": 320}, {"referenceID": 23, "context": "To address this problem, a prior disambiguation step on the word vectors is often introduced, the purpose of which is to find the word representations that best fit to the given context, before composition takes place (Reddy et al., 2011; Kartsaklis et al., 2013; Kartsaklis and Sadrzadeh, 2013; Kartsaklis et al., 2014).", "startOffset": 218, "endOffset": 320}, {"referenceID": 5, "context": "Furthermore, it has been also found to provide minimal benefits for a RecNN compositional architecture in a number of phrase and sentence similarity tasks (Cheng et al., 2014).", "startOffset": 155, "endOffset": 175}, {"referenceID": 7, "context": "Following Collobert and Weston (2008), we convert the unsupervised learning problem to a supervised one by corrupting training sentences.", "startOffset": 10, "endOffset": 38}, {"referenceID": 33, "context": "We achieve that by applying a gated architecture, similar to the one used in the multi-sense model of Neelakantan et al. (2014), but advancing the main idea to the compositional setting detailed in Section 3.", "startOffset": 102, "endOffset": 128}, {"referenceID": 33, "context": "Note that in principle the fixed number of senses assumption is not necessary; Neelakantan et al. (2014), for example, present a version of their model in which new senses are added dynamically when appropriate.", "startOffset": 79, "endOffset": 105}, {"referenceID": 4, "context": "While the usual way to approach this problem is to utilize a classifier that acts (for example) on the concatenation of the two sentence vectors, in this work we follow a novel perspective: specifically, we apply a siamese architecture (Bromley et al., 1993), a concept that has been extensively used in computer vision (Hadsell et al.", "startOffset": 236, "endOffset": 258}, {"referenceID": 12, "context": ", 1993), a concept that has been extensively used in computer vision (Hadsell et al., 2006; Sun et al., 2014).", "startOffset": 69, "endOffset": 109}, {"referenceID": 43, "context": ", 1993), a concept that has been extensively used in computer vision (Hadsell et al., 2006; Sun et al., 2014).", "startOffset": 69, "endOffset": 109}, {"referenceID": 12, "context": "There are two commonly used cost functions: the first is based on the L2 norm (Hadsell et al., 2006; Sun et al., 2014), while the second on the cosine similarity (Nair and Hinton, 2010; Sun et al.", "startOffset": 78, "endOffset": 118}, {"referenceID": 43, "context": "There are two commonly used cost functions: the first is based on the L2 norm (Hadsell et al., 2006; Sun et al., 2014), while the second on the cosine similarity (Nair and Hinton, 2010; Sun et al.", "startOffset": 78, "endOffset": 118}, {"referenceID": 32, "context": ", 2014), while the second on the cosine similarity (Nair and Hinton, 2010; Sun et al., 2014).", "startOffset": 51, "endOffset": 92}, {"referenceID": 43, "context": ", 2014), while the second on the cosine similarity (Nair and Hinton, 2010; Sun et al., 2014).", "startOffset": 51, "endOffset": 92}, {"referenceID": 4, "context": "While the usual way to approach this problem is to utilize a classifier that acts (for example) on the concatenation of the two sentence vectors, in this work we follow a novel perspective: specifically, we apply a siamese architecture (Bromley et al., 1993), a concept that has been extensively used in computer vision (Hadsell et al., 2006; Sun et al., 2014). While siamese networks have been also used in the past for NLP purposes (for example, by Yih et al. (2011)), to the best of our knowledge this is the first time that such a setting is applied for paraphrase detection.", "startOffset": 237, "endOffset": 469}, {"referenceID": 1, "context": "To avoid the exploding or vanishing gradient problem (Bengio et al., 1994) for long sentences, we employ a long short-term memory (LSTM) network (Hochreiter and Schmidhuber, 1997).", "startOffset": 53, "endOffset": 74}, {"referenceID": 16, "context": ", 1994) for long sentences, we employ a long short-term memory (LSTM) network (Hochreiter and Schmidhuber, 1997).", "startOffset": 78, "endOffset": 112}, {"referenceID": 46, "context": "All parameters are updated with mini-batches by AdaDelta (Zeiler, 2012) gradient descent method (\u03bb = 0.", "startOffset": 57, "endOffset": 71}, {"referenceID": 27, "context": "We compare the results with those produced by the skipgram model (SG) of Mikolov et al. (2013) and the language model (CW) of Collobert and Weston (2008).", "startOffset": 73, "endOffset": 95}, {"referenceID": 7, "context": "(2013) and the language model (CW) of Collobert and Weston (2008). We refer to our model as SAMS (SyntaxAware Multi-Sense).", "startOffset": 38, "endOffset": 66}, {"referenceID": 25, "context": "The observed behaviour is comparable to that of embedding models with objective functions conditioned on grammatical relations between words; Levy and Goldberg (2014), for example, present a similar table for their dependency-based extension of the skip-gram model.", "startOffset": 142, "endOffset": 167}, {"referenceID": 17, "context": "We now proceed to a quantitative evaluation of our embeddings on the Stanford Contextual Word Similarity (SCWS) dataset of Huang et al. (2012). The dataset contains 2,003 pairs of words and the contexts they occur in.", "startOffset": 123, "endOffset": 143}, {"referenceID": 17, "context": "We now proceed to a quantitative evaluation of our embeddings on the Stanford Contextual Word Similarity (SCWS) dataset of Huang et al. (2012). The dataset contains 2,003 pairs of words and the contexts they occur in. We can therefore make use of the contextual information in order to select the most appropriate sense for each ambiguous word. Similarly to Neelakantan et al. (2014), we use three different metrics: globalSim measures the similarity between two ambiguous word vectors; localSim selects a single sense for each word based on the context and computes the similarity between the two sense vectors; avgSim represents each word as a weighted average of all senses in the given context and computes the similarity between the two weighted sense vectors.", "startOffset": 123, "endOffset": 384}, {"referenceID": 29, "context": "In addition to the skipgram and Collobert and Weston models, we also compare against the CBOW model (Mikolov et al., 2013) and the multi-sense skip-gram (MSSG) model of Neelakantan et al.", "startOffset": 100, "endOffset": 122}, {"referenceID": 7, "context": "In addition to the skipgram and Collobert and Weston models, we also compare against the CBOW model (Mikolov et al., 2013) and the multi-sense skip-gram (MSSG) model of Neelakantan et al. (2014).", "startOffset": 32, "endOffset": 195}, {"referenceID": 9, "context": "In the last set of experiments, the proposed compositional distributional framework is evaluated on the Microsoft Research Paraphrase Corpus (MSRPC) (Dolan and Brockett, 2005), which contains 5,800 pairs of sentences.", "startOffset": 149, "endOffset": 175}, {"referenceID": 11, "context": "Therefore, for our training we rely on a much larger external dataset, the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013).", "startOffset": 102, "endOffset": 129}, {"referenceID": 5, "context": "For the deep compositional models, simple prior disambiguation is still helpful with small improvements, a result which is consistent with the findings of Cheng et al. (2014). The small gains of the prior disambiguation models over the ambiguous models clearly show that deep architectures are quite capable of performing this elementary form of sense selection intrinsically, as part of the learning process itself.", "startOffset": 155, "endOffset": 175}, {"referenceID": 43, "context": "Inspired by related work in computer vision (Sun et al., 2014), we attempt to alleviate this problem by introducing an average pooling layer at the sense vector level and adding the resulting vector to the sentence representation.", "startOffset": 44, "endOffset": 62}, {"referenceID": 39, "context": "In terms of feature selection, we follow Socher et al. (2011a) and Blacoe and Lapata (2012) and add the following features: the difference in sentence length, the unigram overlap among the two sentences, features related to numbers (including the presence or absence of numbers from a sentence and whether or not the numbers in the two sentences are the same).", "startOffset": 41, "endOffset": 63}, {"referenceID": 3, "context": "(2011a) and Blacoe and Lapata (2012) and add the following features: the difference in sentence length, the unigram overlap among the two sentences, features related to numbers (including the presence or absence of numbers from a sentence and whether or not the numbers in the two sentences are the same).", "startOffset": 12, "endOffset": 37}, {"referenceID": 22, "context": "Pu bl is he d re su lts Mihalcea et al. (2006) 70.", "startOffset": 24, "endOffset": 47}, {"referenceID": 22, "context": "Pu bl is he d re su lts Mihalcea et al. (2006) 70.3 81.3 Rus et al. (2008) 70.", "startOffset": 24, "endOffset": 75}, {"referenceID": 22, "context": "Pu bl is he d re su lts Mihalcea et al. (2006) 70.3 81.3 Rus et al. (2008) 70.6 80.5 Qiu et al. (2006) 72.", "startOffset": 24, "endOffset": 103}, {"referenceID": 16, "context": "6 Islam and Inkpen (2009) 72.", "startOffset": 2, "endOffset": 26}, {"referenceID": 9, "context": "3 Fernando and Stevenson (2008) 74.", "startOffset": 2, "endOffset": 32}, {"referenceID": 9, "context": "3 Fernando and Stevenson (2008) 74.1 82.4 Wan et al. (2006) 75.", "startOffset": 2, "endOffset": 60}, {"referenceID": 8, "context": "0 Das and Smith (2009) 76.", "startOffset": 2, "endOffset": 23}, {"referenceID": 8, "context": "0 Das and Smith (2009) 76.1 82.7 Socher et al. (2011a) 76.", "startOffset": 2, "endOffset": 55}, {"referenceID": 8, "context": "0 Das and Smith (2009) 76.1 82.7 Socher et al. (2011a) 76.8 83.6 Madnani et al. (2012) 77.", "startOffset": 2, "endOffset": 87}, {"referenceID": 8, "context": "0 Das and Smith (2009) 76.1 82.7 Socher et al. (2011a) 76.8 83.6 Madnani et al. (2012) 77.4 84.1 Ji and Eisenstein (2013) 80.", "startOffset": 2, "endOffset": 122}, {"referenceID": 19, "context": "6 difference in Fscore from the first (Ji and Eisenstein, 2013).", "startOffset": 38, "endOffset": 63}, {"referenceID": 20, "context": "models on a convolutional compositional architecture, similar to that of Kalchbrenner et al. (2014).", "startOffset": 73, "endOffset": 100}], "year": 2015, "abstractText": "Deep compositional models of meaning acting on distributional representations of words in order to produce vectors of larger text constituents are evolving to a popular area of NLP research. We detail a compositional distributional framework based on a rich form of word embeddings that aims at facilitating the interactions between words in the context of a sentence. Embeddings and composition layers are jointly learned against a generic objective that enhances the vectors with syntactic information from the surrounding context. Furthermore, each word is associated with a number of senses, the most plausible of which is selected dynamically during the composition process. We evaluate the produced vectors qualitatively and quantitatively with positive results. At the sentence level, the effectiveness of the framework is demonstrated on the MSRPar task, for which we report results within the state-of-the-art range.", "creator": "TeX"}}}