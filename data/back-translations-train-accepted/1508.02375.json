{"id": "1508.02375", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Aug-2015", "title": "Approximation-Aware Dependency Parsing by Belief Propagation", "abstract": "We show how to train the fast dependency parser of Smith and Eisner (2008) for improved accuracy. This parser can consider higher-order interactions among edges while retaining O(n^3) runtime. It outputs the parse with maximum expected recall -- but for speed, this expectation is taken under a posterior distribution that is constructed only approximately, using loopy belief propagation through structured factors. We show how to adjust the model parameters to compensate for the errors introduced by this approximation, by following the gradient of the actual loss on training data. We find this gradient by back-propagation. That is, we treat the entire parser (approximations and all) as a differentiable circuit, as Stoyanov et al. (2011) and Domke (2010) did for loopy CRFs. The resulting trained parser obtains higher accuracy with fewer iterations of belief propagation than one trained by conditional log-likelihood.", "histories": [["v1", "Mon, 10 Aug 2015 19:48:33 GMT  (143kb,D)", "http://arxiv.org/abs/1508.02375v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["matthew r gormley", "mark dredze", "jason eisner"], "accepted": true, "id": "1508.02375"}, "pdf": {"name": "1508.02375.pdf", "metadata": {"source": "CRF", "title": "Approximation-Aware Dependency Parsing by Belief Propagation", "authors": ["Matthew R. Gormley", "Mark Dredze", "Jason Eisner"], "emails": ["mrg@cs.jhu.edu", "mdredze@cs.jhu.edu", "jason@cs.jhu.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people who are able to determine themselves what they want and what they want must take matters into their own hands. In fact, people are able to determine themselves what they want and what they don't want. In fact, people are able to determine themselves what they want and what they don't want. In fact, people are able to determine themselves what they want and what they don't want. In fact, people are able to determine themselves what they want and what they want. In fact, people are able to determine themselves what they want and what they want. In fact, people are able to determine themselves what they want and what they want."}, {"heading": "2 Dependency Parsing by Belief Propagation", "text": "This section describes the parser we train.Model A factor graph (Frey et al., 1997; Kschischang et al., 2001) is a split graph between factors \u03b1 and variables yi, and defines the factorization of a probability distribution over a series of variables {y1, y2,.}. The factor graph contains edges between each factor \u03b1 and a subset of variables yi, and each factor has a local opinion about the possible mappings to its adjacent variables. Such opinions are given by the potential function of the factor that assigns each configuration to a subset of variables y\u03b1. We define the probability of a given mapplement y y to a product of potential functions: p (y) = 1 Z's potential function. Smith and Eisner (2008) define a factor graph for dependence on a given set: n2 variables."}, {"heading": "3 Approximation-aware Learning", "text": "We aim to minimize the expected loss of true data distribution across sentence pairs / sentence pairs. (...) We aim to find the parameters that minimize a regulated objective function beyond the training sample. (...) We aim to minimize the parameters. \"(...) We aim to minimize the expected losses.\" (...) We have minimized this goal locally by using \"2-regulated AdaGrad with Composite Mirror Descent (Duchi et al., 2011) - a variant of stochastic gradient descent, mini-batches, an adaptive learning rate per dimension and sparse updates from the regulation.6Objective Functions As in Stoyanov et al. (2011), our goal is to minimize the expected loss of true data distribution across sentence pairs / parameters (...)."}, {"heading": "3.1 Inference, Decoding, and Loss as a Feedfoward Circuit", "text": "The back propagation algorithm is often applied to neural networks, where the topology of a feedback circuit is statically specified and can be applied to any input. Therefore, our BP algorithm, decoder, and the loss function similarly define a feedback circuit that calculates our function J. However, the circuit topology is dynamically defined (per sentence x (d)) by \"unrolling\" the calculation into a graph. Figure 2 shows this topology for selecting the objective function. The high-level modules consist of (A) computational potential functions, (B) initializing messages, (C) sending messages, (D) sending messages, (D) calculating beliefs, and (E) decoding and calculating the loss. We zoom into two submodules: The first calculates messages from the PTREE factor efficiently (C.1-C.3); the second version calculates a soft loss function (E.3)."}, {"heading": "3.2 Differentiable Objective Functions", "text": "It is not to be expected that there will be such a loss. (...) It is not to be expected that there will be such a loss. (...) It is not to be expected that there will be such a loss. (...) It is not to be expected that there will be such a loss. (...) It is not to be expected that there will be such a loss. (...) It is not to be expected that there will be such a loss. (...) It is not to be expected that there will be such a loss. (...) It is to be expected that there will be such a loss. (...) It is to be expected that there will be such a loss. (...) It is not to be expected that there will be such a loss. (...) It is not to be possible that there will be such a loss. (...) It is to be such a loss. (...) It has been to be such a loss. (...) It is not to be possible that there will be such a loss."}, {"heading": "3.3 Backpropagation through BP", "text": "The multiplication of faith takes place iteratively by sending messages. We can tag each message with a time stamp t (e.g. m (t) i \u2192 \u03b1) indicating the time step at which it was calculated. Figure 2 (B) shows the messages at the time t = 0, designated as m (0) i \u2192 \u03b1, which are initialized after the uniform distribution. Figure 2 (C) shows the calculation of all subsequent messages via equations. (4) and (5). Messages at the time t are calculated from messages at the time t \u2212 1 or before and the potential functions. After the final iteration T, the beliefs bi (yi), b\u03b1 (y\u03b1) from the final messages m (T) i \u2192 \u03b1 are calculated using equations. (6) and (7) - this is shown in Figure 2 (D). Optionally, we can normalize the messages after each step to avoid an overflow (not shown in the figure) as well as the faith."}, {"heading": "3.4 BP and backpropagation with PTREE", "text": "Stoyanov et al. (2011) assume that BP takes an explicit sum in (5). For the PTREE factor, this corresponds to a sum over all the projective dependence trees (since for each assignment that is not a tree, the sum is PTREE (y) = 0. There are exponentially many such trees. However, Smith and Eisner (2008) point out that the sum for \u03b1 = PTREE has a special structure that can be exploited by dynamic programming. To calculate the factor-variable messages of \u03b1 = PTREE, they first execute the inside-outside algorithm, in which the edge weights are given by the relationship of the messages to PTREE: m (t) i \u2192 a (ON) m (t) i \u2192 a (OFF)."}, {"heading": "3.5 Backpropagation through Inside-Outside on a Hypergraph", "text": "PASS (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (PASS) (P"}, {"heading": "4 Other Learning Settings", "text": "Loss-aware Training with Exact Inference Backpropagating through inference, decoder, and loss need not limited to approximate inference algorithms. Li and Eisner (2009) optimize Baye's risk by drawing exact conclusions from a hypergraph for machine translation. Each of our differentiable loss functions (\u00a7 3.2) can also be associated with exact conclusions. BP is accurate for a first-order parser. However, instead of modules (B), (C), and (D) in Figure 2, we can use a standard dynamic programming algorithm for dependency analysis that is just another instance from the inside to the outside on a hypergraph (\u00a7 3.5). Exact marginals are then routed from the inside to the outside on a decoder / loss module (E). Conditional and Surrogate Log Algorithm for dependency Parsing is simply another instance from the inside to the outside on a hypergraph (\u00a7 3.5) from the inside to the outside on a hypergraph (\u00a7 3.5)."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Setup", "text": "We have taken up the banner that we will be able to change the world, \"he said in an interview with\" Welt am Sonntag. \"\" We have it in our hands to change the world, \"he said,\" but we have it in our hands to change the world. \"\" We have not yet understood why it has come so far. \"\" We have not yet understood why it has come so far. \"\" We have not yet understood why it has come so far. \"\" We have not yet understood. \"\" We have not yet understood. \"\" We have not yet understood. \"\" We have not yet understood. \"\" We have not yet understood. \"\" We have not yet understood. \"\" We have not yet understood. \"\" We have not yet understood. \"\" We have not yet understood. \"\" We have not yet understood. \""}, {"heading": "5.2 Results", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "6 Discussion", "text": "We applied these methods to a basic higher order dependency analysis model because it was the simplest and first instance of structured BP (Smith and Eisner, 2008). In future work, we hope to explore further models with structured factors - especially those that collectively consider multiple linguistic layers (e.g. syntax, semantics, and theme). Another natural extension of this work is the exploration of other types of factors: In this case, we only looked at exponential potential family functions (commonly used in CRFs), but any differentiable function would be appropriate, such as a neural network. Our primary contribution is approach-aware training for structured BP. While our experiments only consider dependency analysis, our approach applies to any constraint factor that amounts to running the indoor-outdoor algorithm on a hypergraph."}, {"heading": "7 Conclusions", "text": "We present differentiated objectives for empirical risk mitigation (a la ERMA) and a novel goal based on the L2 gap between the derived beliefs and the true edge indicator functions. Experiments with the English Penn Treebank and 19 languages from CoNLL-2006 / 2007 show that our estimator is able to train more accurate dependency savers with fewer iterations of faith propagation than conventional conditional log likelihood training by taking approximate values into account. Our code is available in an all-purpose library for structured BP, hypergraphics, and reverse propagandia.10"}], "references": [{"title": "Feature hashing for large scale multitask learning", "author": ["Josh Attenberg", "A Dasgupta", "J Langford", "A Smola", "K Weinberger."], "venue": "ICML.", "citeRegEx": "Attenberg et al\\.,? 2009", "shortCiteRegEx": "Attenberg et al\\.", "year": 2009}, {"title": "A comparison of loopy belief propagation and dual decomposition for integrated CCG supertagging and parsing", "author": ["Michael Auli", "Adam Lopez."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language", "citeRegEx": "Auli and Lopez.,? 2011", "shortCiteRegEx": "Auli and Lopez.", "year": 2011}, {"title": "Structured learning for taxonomy induction with belief propagation", "author": ["Mohit Bansal", "David Burkett", "Gerard de Melo", "Dan Klein."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages", "citeRegEx": "Bansal et al\\.,? 2014", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "Mathematical Statistics: Basic Ideas and Selected Topics", "author": ["P.J. Bickel", "K.A. Doksum."], "venue": "HoldenDay Inc., Oakland, CA, USA.", "citeRegEx": "Bickel and Doksum.,? 1977", "shortCiteRegEx": "Bickel and Doksum.", "year": 1977}, {"title": "CoNLL-X shared task on multilingual dependency parsing", "author": ["Sabine Buchholz", "Erwin Marsi."], "venue": "In Proc. of CoNLL, pages 149\u2013164.", "citeRegEx": "Buchholz and Marsi.,? 2006", "shortCiteRegEx": "Buchholz and Marsi.", "year": 2006}, {"title": "Fast Inference in Phrase Extraction Models with Belief Propagation", "author": ["David Burkett", "Dan Klein."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Burkett and Klein.,? 2012", "shortCiteRegEx": "Burkett and Klein.", "year": 2012}, {"title": "Experiments with a higher-order projective dependency parser", "author": ["Xavier Carreras."], "venue": "Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 957\u2013961.", "citeRegEx": "Carreras.,? 2007", "shortCiteRegEx": "Carreras.", "year": 2007}, {"title": "Implicit differentiation by perturbation", "author": ["J. Domke."], "venue": "Advances in Neural Information Processing Systems, pages 523\u2013531.", "citeRegEx": "Domke.,? 2010", "shortCiteRegEx": "Domke.", "year": 2010}, {"title": "Graphical Models over Multiple Strings", "author": ["Markus Dreyer", "Jason Eisner."], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 101\u2013110. Association for Computational Linguistics.", "citeRegEx": "Dreyer and Eisner.,? 2009", "shortCiteRegEx": "Dreyer and Eisner.", "year": 2009}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "The Journal of Machine Learning Research.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Choosing a variable to clamp", "author": ["Frederik Eaton", "Zoubin Ghahramani."], "venue": "International Conference on Artificial Intelligence and Statistics, pages 145\u2013 152.", "citeRegEx": "Eaton and Ghahramani.,? 2009", "shortCiteRegEx": "Eaton and Ghahramani.", "year": 2009}, {"title": "Parsing with soft and hard constraints on dependency length", "author": ["Jason Eisner", "Noah A. Smith."], "venue": "Proceedings of the International Workshop on Parsing Technologies (IWPT), pages 30\u201341, Vancouver, October.", "citeRegEx": "Eisner and Smith.,? 2005", "shortCiteRegEx": "Eisner and Smith.", "year": 2005}, {"title": "Three new probabilistic models for dependency parsing: An exploration", "author": ["Jason Eisner."], "venue": "Proceedings of the 16th International Conference on Computational Linguistics (COLING-96), pages 340\u2013345, Copenhagen, August.", "citeRegEx": "Eisner.,? 1996", "shortCiteRegEx": "Eisner.", "year": 1996}, {"title": "First- and second-order", "author": ["Zhifei Li", "Jason Eisner"], "venue": null, "citeRegEx": "Li and Eisner.,? \\Q2009\\E", "shortCiteRegEx": "Li and Eisner.", "year": 2009}, {"title": "Turbo Parsers: Depen", "author": ["Mario Figueiredo"], "venue": null, "citeRegEx": "Figueiredo.,? \\Q2010\\E", "shortCiteRegEx": "Figueiredo.", "year": 2010}, {"title": "Grammarless Parsing for Joint Inference", "author": ["Jason Naradowsky", "Tim Vieira", "David Smith."], "venue": "Proceedings of COLING 2012, pages 1995\u20132010, Mumbai, India, December. The COLING 2012 Organizing Committee.", "citeRegEx": "Naradowsky et al\\.,? 2012", "shortCiteRegEx": "Naradowsky et al\\.", "year": 2012}, {"title": "The CoNLL 2007 shared task on dependency parsing", "author": ["Joakim Nivre", "Johan Hall", "Sandra K\u00fcbler", "Ryan McDonald", "Jens Nilsson", "Sebastian Riedel", "Deniz Yuret."], "venue": "Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 915\u2013932.", "citeRegEx": "Nivre et al\\.,? 2007", "shortCiteRegEx": "Nivre et al\\.", "year": 2007}, {"title": "A universal part-of-speech tagset", "author": ["Slav Petrov", "Dipanjan Das", "Ryan McDonald."], "venue": "Proc. of LREC.", "citeRegEx": "Petrov et al\\.,? 2012", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "Relaxed Marginal Inference and its Application to Dependency Parsing", "author": ["Sebastian Riedel", "David A. Smith."], "venue": "Joint Human Language Technology Conference/Annual Meeting of the North American Chapter of the Association for Computational Linguistics", "citeRegEx": "Riedel and Smith.,? 2010", "shortCiteRegEx": "Riedel and Smith.", "year": 2010}, {"title": "Learning internal representations by error propagation", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams."], "venue": "David E. Rumelhart and James L. McClelland, editors, Parallel Distributed Processing: Explorations in the Microstructure of Cognition, volume 1, pages 318\u2013", "citeRegEx": "Rumelhart et al\\.,? 1986", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Vine pruning for efficient multi-pass dependency parsing", "author": ["Alexander M. Rush", "Slav Petrov."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 498\u2013", "citeRegEx": "Rush and Petrov.,? 2012", "shortCiteRegEx": "Rush and Petrov.", "year": 2012}, {"title": "Dependency parsing by belief propagation", "author": ["David A. Smith", "Jason Eisner."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Smith and Eisner.,? 2008", "shortCiteRegEx": "Smith and Eisner.", "year": 2008}, {"title": "Minimum-risk training of approximate CRF-Based NLP systems", "author": ["Veselin Stoyanov", "Jason Eisner."], "venue": "Proceedings of NAACL-HLT.", "citeRegEx": "Stoyanov and Eisner.,? 2012", "shortCiteRegEx": "Stoyanov and Eisner.", "year": 2012}, {"title": "Empirical risk minimization of graphical model parameters given approximate inference, decoding, and model structure", "author": ["Veselin Stoyanov", "Alexander Ropson", "Jason Eisner."], "venue": "Proceedings of the 14th International Conference on Artificial Intel-", "citeRegEx": "Stoyanov et al\\.,? 2011", "shortCiteRegEx": "Stoyanov et al\\.", "year": 2011}, {"title": "Estimating the wrong graphical model: Benefits in the computation-limited setting", "author": ["Martin J. Wainwright."], "venue": "The Journal of Machine Learning Research, 7:1829\u20131859.", "citeRegEx": "Wainwright.,? 2006", "shortCiteRegEx": "Wainwright.", "year": 2006}, {"title": "Statistical dependency analysis with support vector machines", "author": ["Hiroyasu Yamada", "Yuji Matsumoto."], "venue": "Proceedings of IWPT, volume 3.", "citeRegEx": "Yamada and Matsumoto.,? 2003", "shortCiteRegEx": "Yamada and Matsumoto.", "year": 2003}], "referenceMentions": [{"referenceID": 11, "context": "We show how to train the fast dependency parser of Smith and Eisner (2008) for improved accuracy.", "startOffset": 61, "endOffset": 75}, {"referenceID": 11, "context": "We show how to train the fast dependency parser of Smith and Eisner (2008) for improved accuracy. This parser can consider higher-order interactions among edges while retaining O(n) runtime. It outputs the parse with maximum expected recall\u2014but for speed, this expectation is taken under a posterior distribution that is constructed only approximately, using loopy belief propagation through structured factors. We show how to adjust the model parameters to compensate for the errors introduced by this approximation, by following the gradient of the actual loss on training data. We find this gradient by backpropagation. That is, we treat the entire parser (approximations and all) as a differentiable circuit, as Stoyanov et al. (2011) and Domke (2010) did for loopy CRFs.", "startOffset": 61, "endOffset": 739}, {"referenceID": 7, "context": "(2011) and Domke (2010) did for loopy CRFs.", "startOffset": 11, "endOffset": 24}, {"referenceID": 21, "context": "Such parsers are traditionally trained as if the inference had been exact (Smith and Eisner, 2008).", "startOffset": 74, "endOffset": 98}, {"referenceID": 12, "context": "For example, consider the dependency parser we will train in this paper, which is based on the work of Smith and Eisner (2008). Ostensibly, this parser finds the minimum Bayes risk (MBR) parse under a probability distribution defined by a higher-order dependency parsing model.", "startOffset": 113, "endOffset": 127}, {"referenceID": 12, "context": "Stoyanov and Eisner (2012) call this approach ERMA, for \u201cempirical risk minimization under approximations.", "startOffset": 13, "endOffset": 27}, {"referenceID": 12, "context": "Our primary contribution is the application of Stoyanov and Eisner\u2019s learning method in the parsing setting, for which the graphical model involves a global constraint. Smith and Eisner (2008) previously showed how to run BP in this setting (by calling the inside-outside algorithm as a subroutine).", "startOffset": 60, "endOffset": 193}, {"referenceID": 4, "context": "We evaluate our parser on 19 languages from the CoNLL-2006 (Buchholz and Marsi, 2006) and CoNLL-2007 (Nivre et al.", "startOffset": 59, "endOffset": 85}, {"referenceID": 16, "context": "We evaluate our parser on 19 languages from the CoNLL-2006 (Buchholz and Marsi, 2006) and CoNLL-2007 (Nivre et al., 2007) Shared Tasks as well as the English Penn Treebank (Marcus et al.", "startOffset": 101, "endOffset": 121}, {"referenceID": 12, "context": "Smith and Eisner (2008) define a factor graph for dependency parsing of a given n-word sentence: n2 binary variables {y1, y2, .", "startOffset": 10, "endOffset": 24}, {"referenceID": 3, "context": "Decoder To obtain a single parse as output, we use a minimum Bayes risk (MBR) decoder, which attempts to find the tree with minimum expected loss under the model\u2019s distribution (Bickel and Doksum, 1977).", "startOffset": 177, "endOffset": 202}, {"referenceID": 21, "context": "Prior work (Smith and Eisner, 2008; Bansal et al., 2014) used the log-odds ratio log p\u03b8(yi=ON) p\u03b8(yi=OFF) as the edge scores for decoding, but this yields a parse different from the MBR parse.", "startOffset": 11, "endOffset": 56}, {"referenceID": 2, "context": "Prior work (Smith and Eisner, 2008; Bansal et al., 2014) used the log-odds ratio log p\u03b8(yi=ON) p\u03b8(yi=OFF) as the edge scores for decoding, but this yields a parse different from the MBR parse.", "startOffset": 11, "endOffset": 56}, {"referenceID": 12, "context": "See Smith and Eisner (2008) for details.", "startOffset": 14, "endOffset": 28}, {"referenceID": 9, "context": "We locally minimize this objective using `2-regularized AdaGrad with Composite Mirror Descent (Duchi et al., 2011)\u2014a variant of stochastic gradient descent that uses mini-batches, an adaptive learning rate per dimension, and sparse lazy updates from the regularizer.", "startOffset": 94, "endOffset": 114}, {"referenceID": 23, "context": "Objective Functions As in Stoyanov et al. (2011), our aim is to minimize expected loss on the true data distribution over sentence/parse pairs (X,Y ):", "startOffset": 26, "endOffset": 49}, {"referenceID": 6, "context": "How slow is exact inference for dependency parsing? For certain choices of higher-order factors, polynomial time is possible via dynamic programming (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010).", "startOffset": 149, "endOffset": 211}, {"referenceID": 6, "context": ", 2005; Carreras, 2007; Koo and Collins, 2010). However, BP will typically be asymptotically faster (for a fixed number of iterations) and faster in practice. In some other settings, exact inference is NPhard. In particular, non-projective parsing becomes NP-hard with even second-order factors (McDonald and Pereira, 2006). BP can handle this case in polynomial time by replacing the PTREE factor with a TREE factor that allows edges to cross. \u03b8 is initialized to 0 when not otherwise specified. not be differentiable because of the argmax in the definition of h\u03b8 (3). We will address this below by substituting a differentiable softmax. This is the \u201cERMA\u201d method of Stoyanov and Eisner (2012). We will also consider simpler choices of J(\u03b8;x(d),y(d)) that are commonly used in training neural networks.", "startOffset": 8, "endOffset": 695}, {"referenceID": 19, "context": "This yields the same type of \u201cbackpropagation\u201d algorithm that has long been used for training neural networks (Rumelhart et al., 1986).", "startOffset": 110, "endOffset": 134}, {"referenceID": 7, "context": "Another option would be to use the perturbation method of Domke (2010). However, we implemented the gradient computation directly, and we describe it here.", "startOffset": 58, "endOffset": 71}, {"referenceID": 10, "context": "Eaton and Ghahramani (2009) and Stoyanov et al.", "startOffset": 0, "endOffset": 28}, {"referenceID": 10, "context": "Eaton and Ghahramani (2009) and Stoyanov et al. (2011) showed how to backpropagate through the basic BP algorithm, and we reiterate the key details below (\u00a7 3.", "startOffset": 0, "endOffset": 55}, {"referenceID": 23, "context": "This allows us to go beyond Stoyanov et al. (2011) and train structured BP in an approximation-aware and loss-aware fashion.", "startOffset": 28, "endOffset": 51}, {"referenceID": 23, "context": "Stoyanov et al. (2011) postulate that the nonconvexity of empirical risk may make it a difficult function to optimize (even with annealing).", "startOffset": 0, "endOffset": 23}, {"referenceID": 23, "context": "Stoyanov et al. (2011) found mean squared error (MSE) to give a smoother training objective, though still non-convex, and similarly used it to find an initializer for empirical risk.", "startOffset": 0, "endOffset": 23}, {"referenceID": 23, "context": "Explicit formulas can be found in the appendix of Stoyanov et al. (2011).", "startOffset": 50, "endOffset": 73}, {"referenceID": 21, "context": "Stoyanov et al. (2011) assume that BP takes an explicit sum in (5).", "startOffset": 0, "endOffset": 23}, {"referenceID": 12, "context": "However, Smith and Eisner (2008) point out that for \u03b1 = PTREE, the summation has a special structure that can be exploited by dynamic programming.", "startOffset": 19, "endOffset": 33}, {"referenceID": 12, "context": "In the case of dependency parsing, the structure of the hypergraph is given by the dynamic programming algorithm of Eisner (1996). For the forward pass of the inside-outside module, the input variables are the hyperedge weights we\u2200e and the outputs are the marginal probabilities pw(i)\u2200i of each node i in the hypergraph.", "startOffset": 116, "endOffset": 130}, {"referenceID": 12, "context": "Li and Eisner (2009) optimize Bayes risk with exact inference on a hypergraph for machine translation.", "startOffset": 7, "endOffset": 21}, {"referenceID": 21, "context": "Conditional and Surrogate Log-likelihood The standard approach to training is conditional loglikelihood (CLL) maximization (Smith and Eisner, 2008), which does not take inexact inference into account.", "startOffset": 123, "endOffset": 147}, {"referenceID": 24, "context": "The literature refers to this approximation-unaware training method as surrogate likelihood training since it returns the \u201cwrong\u201d model even under the assumption of infinite training data (Wainwright, 2006).", "startOffset": 188, "endOffset": 206}, {"referenceID": 23, "context": "Training a standard factor graph with ERMA and a log-likelihood objective recovers CLL exactly (Stoyanov et al., 2011).", "startOffset": 95, "endOffset": 118}, {"referenceID": 6, "context": ", 2005) and second-order (Carreras, 2007) parsing.", "startOffset": 25, "endOffset": 41}, {"referenceID": 0, "context": "We use feature hashing (Ganchev and Dredze, 2008; Attenberg et al., 2009) and restrict to at most 20 million features.", "startOffset": 23, "endOffset": 73}, {"referenceID": 15, "context": "We add O(n3) secondorder grandparent and arbitrary sibling factors as in Riedel and Smith (2010) and Martins et al.", "startOffset": 73, "endOffset": 97}, {"referenceID": 15, "context": "We add O(n3) secondorder grandparent and arbitrary sibling factors as in Riedel and Smith (2010) and Martins et al. (2010). We use standard feature sets for first-order (McDonald et al.", "startOffset": 73, "endOffset": 123}, {"referenceID": 5, "context": ", 2005) and second-order (Carreras, 2007) parsing. Following Rush and Petrov (2012), we also include a version of each part-of-speech (POS) tag feature, with the coarse POS tags from Petrov et al.", "startOffset": 26, "endOffset": 84}, {"referenceID": 5, "context": ", 2005) and second-order (Carreras, 2007) parsing. Following Rush and Petrov (2012), we also include a version of each part-of-speech (POS) tag feature, with the coarse POS tags from Petrov et al. (2012). We use feature hashing (Ganchev and Dredze, 2008; Attenberg et al.", "startOffset": 26, "endOffset": 204}, {"referenceID": 11, "context": "Pruning To reduce the time spent on feature extraction, we enforce the type-specific dependency length bounds from Eisner and Smith (2005) as used by Rush and Petrov (2012): the maximum allowed dependency length for each tuple (parent tag, child tag, direction) is given by the maximum observed length for that tuple in the training data.", "startOffset": 115, "endOffset": 139}, {"referenceID": 11, "context": "Pruning To reduce the time spent on feature extraction, we enforce the type-specific dependency length bounds from Eisner and Smith (2005) as used by Rush and Petrov (2012): the maximum allowed dependency length for each tuple (parent tag, child tag, direction) is given by the maximum observed length for that tuple in the training data.", "startOffset": 115, "endOffset": 173}, {"referenceID": 11, "context": "Pruning To reduce the time spent on feature extraction, we enforce the type-specific dependency length bounds from Eisner and Smith (2005) as used by Rush and Petrov (2012): the maximum allowed dependency length for each tuple (parent tag, child tag, direction) is given by the maximum observed length for that tuple in the training data. Following Koo and Collins (2010), we train an (exact) first-order model and for each token prune any parents for which the marginal probability is less than 0.", "startOffset": 115, "endOffset": 372}, {"referenceID": 11, "context": "Pruning To reduce the time spent on feature extraction, we enforce the type-specific dependency length bounds from Eisner and Smith (2005) as used by Rush and Petrov (2012): the maximum allowed dependency length for each tuple (parent tag, child tag, direction) is given by the maximum observed length for that tuple in the training data. Following Koo and Collins (2010), we train an (exact) first-order model and for each token prune any parents for which the marginal probability is less than 0.0001 times the maximum parent marginal for that token.8 On a per-token basis, we further restrict to the ten parents with highest marginal probability as in Martins et al. (2009). The pruning model uses a simpler feature set as in Rush and Petrov (2012).", "startOffset": 115, "endOffset": 677}, {"referenceID": 11, "context": "Pruning To reduce the time spent on feature extraction, we enforce the type-specific dependency length bounds from Eisner and Smith (2005) as used by Rush and Petrov (2012): the maximum allowed dependency length for each tuple (parent tag, child tag, direction) is given by the maximum observed length for that tuple in the training data. Following Koo and Collins (2010), we train an (exact) first-order model and for each token prune any parents for which the marginal probability is less than 0.0001 times the maximum parent marginal for that token.8 On a per-token basis, we further restrict to the ten parents with highest marginal probability as in Martins et al. (2009). The pruning model uses a simpler feature set as in Rush and Petrov (2012).", "startOffset": 115, "endOffset": 752}, {"referenceID": 4, "context": "Data We consider 19 languages from the CoNLL2006 (Buchholz and Marsi, 2006) and CoNLL-2007 (Nivre et al.", "startOffset": 49, "endOffset": 75}, {"referenceID": 16, "context": "Data We consider 19 languages from the CoNLL2006 (Buchholz and Marsi, 2006) and CoNLL-2007 (Nivre et al., 2007) Shared Tasks.", "startOffset": 91, "endOffset": 111}, {"referenceID": 4, "context": "Data We consider 19 languages from the CoNLL2006 (Buchholz and Marsi, 2006) and CoNLL-2007 (Nivre et al., 2007) Shared Tasks. We also convert the English Penn Treebank (PTB) (Marcus et al., 1993) to dependencies using the head rules from Yamada and Matsumoto (2003) (PTB-YM).", "startOffset": 50, "endOffset": 266}, {"referenceID": 6, "context": "For CLL only, we projectivize the training trees following (Carreras, 2007) by finding the maximum projective spanning tree under an oracle model which assigns score +1 to edges in the gold tree and 0 to the others.", "startOffset": 59, "endOffset": 75}, {"referenceID": 21, "context": "Our goal is to demonstrate that our approximationaware training method leads to improved parser accuracy as compared with the standard training approach of conditional log-likelihood (CLL) maximization (Smith and Eisner, 2008), which does not take inexact inference into account.", "startOffset": 202, "endOffset": 226}, {"referenceID": 21, "context": "We applied these methods to a basic higher-order dependency parsing model, because that was the simplest and first (Smith and Eisner, 2008) instance of structured BP.", "startOffset": 115, "endOffset": 139}, {"referenceID": 21, "context": "Prior work has used this structured form of BP to do dependency parsing (Smith and Eisner, 2008), CNF grammar parsing (Naradowsky et al.", "startOffset": 72, "endOffset": 96}, {"referenceID": 15, "context": "Prior work has used this structured form of BP to do dependency parsing (Smith and Eisner, 2008), CNF grammar parsing (Naradowsky et al., 2012), TAG (Auli and Lopez, 2011), ITGconstraints for phrase extraction (Burkett and Klein, 2012), and graphical models over strings (Dreyer and Eisner, 2009).", "startOffset": 118, "endOffset": 143}, {"referenceID": 1, "context": ", 2012), TAG (Auli and Lopez, 2011), ITGconstraints for phrase extraction (Burkett and Klein, 2012), and graphical models over strings (Dreyer and Eisner, 2009).", "startOffset": 13, "endOffset": 35}, {"referenceID": 5, "context": ", 2012), TAG (Auli and Lopez, 2011), ITGconstraints for phrase extraction (Burkett and Klein, 2012), and graphical models over strings (Dreyer and Eisner, 2009).", "startOffset": 74, "endOffset": 99}, {"referenceID": 8, "context": ", 2012), TAG (Auli and Lopez, 2011), ITGconstraints for phrase extraction (Burkett and Klein, 2012), and graphical models over strings (Dreyer and Eisner, 2009).", "startOffset": 135, "endOffset": 160}], "year": 2015, "abstractText": "We show how to train the fast dependency parser of Smith and Eisner (2008) for improved accuracy. This parser can consider higher-order interactions among edges while retaining O(n) runtime. It outputs the parse with maximum expected recall\u2014but for speed, this expectation is taken under a posterior distribution that is constructed only approximately, using loopy belief propagation through structured factors. We show how to adjust the model parameters to compensate for the errors introduced by this approximation, by following the gradient of the actual loss on training data. We find this gradient by backpropagation. That is, we treat the entire parser (approximations and all) as a differentiable circuit, as Stoyanov et al. (2011) and Domke (2010) did for loopy CRFs. The resulting trained parser obtains higher accuracy with fewer iterations of belief propagation than one trained by conditional log-likelihood.", "creator": "LaTeX with hyperref package"}}}