{"id": "1508.03398", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Aug-2015", "title": "End-to-end Learning of LDA by Mirror-Descent Back Propagation over a Deep Architecture", "abstract": "We develop a fully discriminative learning approach for supervised Latent Dirichlet Allocation (LDA) model, which maximizes the posterior probability of the prediction variable given the input document. Different from traditional variational learning or Gibbs sampling approaches, the proposed learning method applies (i) the mirror descent algorithm for exact maximum a posterior inference and (ii) back propagation with stochastic gradient descent for model parameter estimation, leading to scalable learning of the model in an end-to-end discriminative manner. As a byproduct, we also apply this technique to develop a new learning method for the traditional unsupervised LDA model. Experimental results on two real-world regression and classification tasks show that the proposed methods significantly outperform the previous supervised/unsupervised LDA learning methods.", "histories": [["v1", "Fri, 14 Aug 2015 01:32:27 GMT  (765kb,D)", "http://arxiv.org/abs/1508.03398v1", "20 pages, 5 figures"], ["v2", "Sun, 1 Nov 2015 08:11:14 GMT  (540kb,D)", "http://arxiv.org/abs/1508.03398v2", "Proc. NIPS 2015"]], "COMMENTS": "20 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jianshu chen", "ji he", "yelong shen", "lin xiao", "xiaodong he", "jianfeng gao", "xinying song", "li deng"], "accepted": true, "id": "1508.03398"}, "pdf": {"name": "1508.03398.pdf", "metadata": {"source": "CRF", "title": "End-to-end Learning of Latent Dirichlet Allocation by Mirror-Descent Back Propagation", "authors": ["Jianshu Chen", "Ji He", "Yelong Shen", "Lin Xiao", "Xiaodong He", "Jianfeng Gao", "Xinying Song", "Li Deng"], "emails": ["jianshuc@microsoft.com", "yeshen@microsoft.com", "lin.xiao@microsoft.com", "xiaohe@microsoft.com", "jfgao@microsoft.com", "xinson@microsoft.com", "deng@microsoft.com", "jvking@uw.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, the thematization of the topic, which can be viewed from a particular topic field, is to be regarded as a latent representation of the document that can be used as a feature for predictive purposes (e.g., sensation analysis), in particular the derived topic distribution is fed into a separate classification or regression model (e.g., logistic regression or linear regression) in order to perform an autonomous learning structure that normally limits the performance of algorithms. To this end, various superordinate topic models have been proposed in order to model the documents together with the label. [3] Variation methods have been used to learn a superordinate LDA model."}, {"heading": "2 Smoothed Supervised LDA Model", "text": "We consider the smoothed LDA model in Figure 1 as the number of topics, N as the number of words in each document, V as the vocabulary size, and D as the number of documents in the corpus. The generative process of the model in Figure 1 can be described as follows: 1. For each document d, select the proportions of the topic according to a Dirichlet distribution: p = Dir (\u03b1), which is a K \u00b7 1 vector consisting of non-negative components.2. Draw each column in a V \u00b7 K matrix \u00b2, independent of an interchangeable Dirichlet distribution: p = Dir (\u03b2) (i.e."}, {"heading": "3 Exact MAP Inference", "text": "First, we will consider the consequence problem in the smoothed LDA model. In the monitored case, the main objective is to derive from it that the words wd, 1: N in each document d, i.e., the calculation (yd | wd, 1: N, \u03a6, U, \u03b1, \u03b3) = \u0432 \u03b8d p (yd | \u03b8d, U, \u03b3) p (\u03b8d | wd, 1: N, \u03b1, \u03b1) d\u03b8d (5) where the probability p (yd, U, \u03b3) is known (e.g., multinomial or Gaussian for classification and regression problems - see Section 2). The main challenge is to evaluate p (\u03b8d | wd, 1: N, \u03b1), i.e. to derive the issue of proportionality of each document, which is also the important consequence problem, i.e. We will represent all multinomial variables d by a uniform vector that has a single component and all others equal, whereby all components are zero."}, {"heading": "3.1 MAP Inference as a Convex Optimization Problem", "text": "Using the Bayean rule p (\u03b8d | wd, 1: N, \u03a6, \u03b1) = p (\u03b8d | \u03b1) p (wd, 1: N | \u03b8d, \u03a6) / p (wd, 1: N | \u03a6, \u03b1) and the fact that p (wd, 1: N | \u0445, \u03b1) is independent of \u03b8d, we obtain the equivalent form of (7) as\u03b8d \u2212 wd, 1: N = arg max \u03b8d. \u2212 PK [ln p (\u03b8d | \u03b1) + ln p (wd, 1: N | \u03b5d, \u03a6)] (8), where PK = {constamp # 160; RK: \u03b8j 0, K = 1 \u03b8j = 1} is the (K \u2212 1) dimensional probability profile simplex vector, p (\u0445d) = vector."}, {"heading": "3.2 Mirror Descent Algorithm for MAP Inference", "text": "An efficient approach to solving the limited optimization problem (10) is the Mirror Descendancy Algorithm (MDA) with the Bregman divergence, which was chosen to generalize the Kullback-Leibler divergence (1, 15, 18). Specifically, f (\u03b8d) means the cost function in (10), then the MDA updates the MAP estimate of the phenomenon x iteratively by: \u03b8d, \"= arg min divergence.\" \u2212 PK [f (\u03b8d, \"\u2212 1) + [\u03b8df (\u03b8d,\" \u2212 1)] T (\u03b8d, \"\u2212 2) of the MDA divergence figure x iteratively by: \u03b8d,\" \u2212 1). \u2212 PK [f (\u03b8d, \"\u2212 1), describes the estimate of the Hadd elements in the\" th iteration, \"Td, denotes the step size of MDA, and denotes the step size of MTDA (1)."}, {"heading": "4 Learning by Mirror-Descent Back Propagation", "text": "We now look at the supervised learning problem (3) and the unsupervised learning problem (4) using the developed MDA-based MAP inference. First, we look at the supervised learning problem. With (13), the discriminatory learning problem (3) can be solved by the descent of stutter gradients (SGD), U [\u2212 ln p (\u03a6 | \u03b2) - D \u00b2 d = 1ln p (yd, L, U, \u03b3) (14), which can be solved by stochastic descent (SGD). Note that the cost function in (14) of U explicitly depends on p (yd | p, L, U, \u03b3), which can be directly calculated from its definition in Sec. 2. On the other hand, the cost function in (14) implicitly depends on the cost function in (l), L. From Figure 2, we observe that d d d d does not only depend on the explicit d (as in the MDA block on the right side of the function of the figure, but implicitly stated in the figure 2)."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Description of Datasets and Baselines", "text": "We evaluated our proposed supervised learning (referred to as BP-sLDA) and unsupervised learning (referred to as BP-LDA) on two real-world datasets. The first dataset we use is a large-scale dataset based on Amazon Movie Reviews (AMR) [13]. The dataset consists of 7.9 million movie reviews (1.48 billion words) from Amazon, written by 889,176 users, to a total of 253,059 movies. For word preprocessing, we removed punctuation and lowercase letters. A vocabulary the size of 5,000 is constructed by selecting the most common words. The same applies to [19], shifting the score of the task so that it has zero mean. The task is formulated as a regression problem in which we try to predict the rating based on the viewer's text. Second, we demonstrate the effectiveness of our algorithm on a multi-domain classification task (SLDA)."}, {"heading": "5.2 Prediction Performance", "text": "We use the prediction results R2 to measure the prediction performance of different models on the AMR regression task. We use the prediction results R2 to measure the prediction performance, defined as: pR2 = 1 \u2212 (* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *"}, {"heading": "5.3 Analysis and Discussion", "text": "Note from Figure 3 (a): If we increase the number of topics, the pR2 score of the BP sLDA dataset will initially improve and then slightly deteriorate after exceeding 20 topics, most likely caused by overadjustment to the small dataset (79K documents), as the BP sLDA models trained to the full 7.9M dataset produce much higher pR2 values (Table 1) than the values of the 79K dataset and continue to improve as the model size (number of topics) increases. Another interesting observation from Figure 3 is that the unattended LDA models (BP-LDA and Gibbs-LDA) are less susceptible to overadjustments. As unmarked data is widely available, a future task is to merge the monitored and unattended parts of the model in order to have a semi-monitored LDA model."}, {"heading": "5.4 Efficiency in Computation Time", "text": "To compare the efficiency of the algorithms, we show the training time (in hours) of different models on the AMR dataset (79K and 7.9M) in Figure 5, which shows that our algorithm scales well when Table 3: pR2 for \u03b1 < 1 case for complete AMR data (7.9M documents and 5K vocabulary size).Number of topics 5 10 20 50 100 BP-sLDA (\u03b1 = 0.5) 0.488 0.548 0.575 0.571 0.574 BP-sLDA (\u03b1 = 0.1) 0.441 0.558 0.572 0.569 0.5700 20 40 60 80 100 020406080100120Number of topics Tr aini ngti me inh our ssLDA (79K) BP \u2212 sLDA (79K) MedLDA (79K) BP \u2212 sLDA (7.9M) Number of training times (in hours) on the AMR datasets."}, {"heading": "6 Conclusion", "text": "We have developed novel learning approaches for both monitored and unmonitored LDA models, using exact MAP conclusions with mirror descent algorithm and reverse propagation. Specifically, the monitored LDA model is trained in a fully discriminatory manner by maximizing the posterior probability of predictive variables in the given input documents. We evaluate the predictive performance of the models using two real regression and classification tasks. Results show that the discriminatory training approach significantly improves the performance of the monitored LDA model compared to previous learning methods. Furthermore, the newly developed inference and learning techniques also improve the performance of the unmonitored LDA model by providing better properties for prediction. Future work will include (i) exploring other optimization algorithms for the MAP inference problem, such as accelerated mirror descension, and (the Netichi) structuring of the LDA, which may be highly structured [the Netichi]"}, {"heading": "B Derivation of the Recursion for Mirror Descent Algorithm", "text": "Firstly, we rewrite the optimization problem (11) asmin dependent d = empirical effectiveness (empirical effectiveness = empirical effectiveness = empirical effectiveness, empirical effectiveness = empirical effectiveness, empirical effectiveness = empirical effectiveness, empirical effectiveness = empirical effectiveness, empirical effectiveness = empirical effectiveness, empirical effectiveness = empirical effectiveness, empirical effectiveness, empirical effectiveness = empirical effectiveness, empirical effectiveness = empirical effectiveness, empirical effectiveness, empirical effectiveness = empirical effectiveness, empirical effectiveness, empirical effectiveness, empirical effectiveness = empirical effectiveness, empirical effectiveness, empirical effectiveness, empirical effectiveness = empirical effectiveness, empirical effectiveness, empirical effectiveness = empirical effectiveness, empirical effectiveness, empirical effectiveness = empirical effectiveness, empirical effectiveness, empirical effectiveness = empirical effectiveness, empirical effectiveness, empirical effectiveness, empirical effectiveness = empirical effectiveness, empirical effectiveness, empirical effectiveness = empirical effectiveness, effectiveness, effectiveness, empirical effectiveness, effectiveness, empirical effectiveness = empirical effectiveness, empirical effectiveness, effectiveness, empirical effectiveness, effectiveness, empirical effectiveness = empirical, effectiveness, effectiveness, empirical effectiveness, effectiveness, empirical, effectiveness, effectiveness = empirical, effectiveness, effectiveness, empirical), empirical (empirical), effectiveness = empirical, effectiveness, effectiveness = empirical, effectiveness, effectiveness, effectiveness, empirical, effectiveness, empirical, effectiveness, effectiveness, effectiveness, empiri"}, {"heading": "D Gradient Formula of BP-sLDA", "text": "In this section, we specify the sequence formula for the supervised learning of BP-sLDA = 42. To achieve this, we must first rewrite the training costs (14). (asJ), (U,), (D), (U,), (35), where Qd (\u00b7) denotes the loss function on the d-th document, defines asQd (U,), (1D), (S), (S), (S), (S), (K), (S), (K), (K), (K), (K), K (K), K (K), K (K), K (K), K (K), (K), (K), K (K), K (K), K (K, S), K (K, S), K (K, S), K (K), K (K, K, K, K (K), K, K (K), K (K, S), K (K), K (K, K (K), K (K), K (K), K (K, K, K (K), K (K), K (K, K, K (K), K (K), K (K, K, K (K), K (K), K (K (K), K (K, K, K (K), K (K), K (K (K, K), K (K, K, K (K), K (K (K), K (K (K), K (K), K (K (K), K (K, K, K (K), K (K (K, K), K (K, K), K (K, K (K, K, K), K (K (K), K (K (K), K (K (K), K (K (K), K (K, K (K), K (K (K, K), K (K, K, K (K), K (K, K (K, K), K (K, K), K (K (K, K (K), K), K (K, K ("}, {"heading": "E Gradient Formula of BP-LDA", "text": "The unattended learning problem (4) can be rewritten to minimize the following cost function (7): J (7) = D (7) = D (8), where Qd (8) is the loss function, which is asQd (8) = \u2212 1D ln p (8), \u2212 ln p (8), + ln p (8), (9), the first term in (70) is already derived in (46): D ln p (8), p (8), wn p (9), wn p (9), wn p (8), wn (8), + ln p (8), we have already derived in (46): D ln p (8), (8), p (8), p (8), p (8), wn), wd (8), wd (8), wd, wd, wd (8), wd, wd, wd, wd, 8, wd, wd, wd (8), wd, wd, wd (8, wd, wd, wd, 8, wd, wd, wd, 8, wd, wd, wd, 8, wd, wd, wd (8, wd, wd, wd, 8, wd, wd, wd, wd, 8, wd, wd, wd, 8, wd, wd, wd, 8, wd, wd, wd, 8, wd, wd, wd, wd (8, wd), wd, wd (8, wd, wd (8, wd), wd, wd (8, wd, wd, wd, wd, wd, wd, 8, wd, wd), wd, wd (8, wd, wd (8, wd, wd), wd, wd, wd (8, wd, wd), wd (8, wd, wd, wd, wd (8), wd, wd, wd"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>We develop a fully discriminative learning approach for supervised Latent Dirich-<lb>let Allocation (LDA) model, which maximizes the posterior probability of the pre-<lb>diction variable given the input document. Different from traditional variational<lb>learning or Gibbs sampling approaches, the proposed learning method applies<lb>(i) the mirror descent algorithm for exact maximum a posterior inference and (ii)<lb>back propagation with stochastic gradient descent for model parameter estimation,<lb>leading to scalable learning of the model in an end-to-end discriminative manner.<lb>As a byproduct, we also apply this technique to develop a new learning method for<lb>the traditional unsupervised LDA model. Experimental results on two real-world<lb>regression and classification tasks show that the proposed methods significantly<lb>outperform the previous supervised/unsupervised LDA learning methods.", "creator": "LaTeX with hyperref package"}}}