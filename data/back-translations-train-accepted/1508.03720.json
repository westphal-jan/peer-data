{"id": "1508.03720", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Aug-2015", "title": "Classifying Relations via Long Short Term Memory Networks along Shortest Dependency Path", "abstract": "Relation classification is an important research arena in the field of natural language processing (NLP). In this paper, we present SDP-LSTM, a novel neural network to classify the relation of two entities in a sentence. Our neural architecture leverages the shortest dependency path (SDP) between two entities; multichannel recurrent neural networks, with long short term memory (LSTM) units, pick up heterogeneous information along the SDP. Our proposed model has several distinct features: (1) The shortest dependency paths retain most relevant information (to relation classification), while eliminating irrelevant words in the sentence. (2) The multichannel LSTM networks allow effective information integration from heterogeneous sources over the dependency paths. (3) A customized dropout strategy regularizes the neural network to alleviate overfitting. We test our model on the SemEval 2010 relation classification task, and achieve an $F_1$-score of 83.7\\%, higher than competing methods in the literature.", "histories": [["v1", "Sat, 15 Aug 2015 11:15:32 GMT  (104kb,D)", "http://arxiv.org/abs/1508.03720v1", "EMNLP '15"]], "COMMENTS": "EMNLP '15", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["xu yan", "lili mou", "ge li", "yunchuan chen", "hao peng", "zhi jin"], "accepted": true, "id": "1508.03720"}, "pdf": {"name": "1508.03720.pdf", "metadata": {"source": "CRF", "title": "Classifying Relations via Long Short Term Memory Networks along Shortest Dependency Paths", "authors": ["Yan Xu", "Lili Mou", "Ge Li", "\u2020\u2217Yunchuan", "Hao Peng", "Zhi Jin"], "emails": ["chenyunchuan11@mails.ucas.ac.cn"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is so that most people are able to identify themselves and understand what they are doing. (...) It is not as if it is a country in which it is a country. \"(...)\" It is as if it is a country. \"(...)\" It is as if it is a country. \"(...)\" It is as if it is a country. \"(...)\" It is a country in which it is a country. \"(...)\" (...) \"It is as if it is a country.\" (...) \"It is a country.\" (...) \"It is a country in which it is a country.\" (...) \"(...)\" (...) \"It is a country in which it is a country.\" (...) \"(...)\" (...) \"It is a country in which it is a country. (...)\" (...) \"(...\" It is a country in which it is a country. \"(...)\" (...) \"It is a country in which it is a country.\" (...) \"(...\" it is a country in which it is a country. \"(...)\" (... \"it is a country in which it is a country in which it is a country.\" (...) \"(...\""}, {"heading": "2 Related Work", "text": "In recent years, the number of people who are able to stay in the US has multiplied, both in the US and in Europe."}, {"heading": "3 The Proposed SDP-LSTM Model", "text": "In this section, we describe our SDP-LSTM model in detail. Section 3.1 describes the overall architecture of our model. Section 3.2 describes the reasons for using SDPs. Four different information channels along the SDP are explained in Section 3.3. Section 3.4 introduces the relapsing neural network with long-term short-term memory that builds on the dependency path. Section 3.5 adjusts a dropout strategy for our network to alleviate overfitting. Finally, we present our training goal in Section 3.6."}, {"heading": "3.1 Overview", "text": "Figure 2 shows the overall architecture of our SDP-LSTM network. First, a set is analyzed for dependence by the Stanford parser; 1 the shortest dependency path (SDP) is extracted as the input of our network. Along the SDP, four different types of information - referred to as channels - are used, including words, POS tags, grammatical relations, and WordNet hypernyms. (See Figure 2a) In each channel, discrete inputs, such as words, are assigned to real vectors, so-called embeddings, which capture the underlying meanings of the inputs. Two recursive neural networks (Figure 2b) collect information along the left and right subpaths of the SDP. (The path is separated by the common ancestor node of two entities.) Finally, units for effective information propagation are used in the recursive networks (Figure 2b)."}, {"heading": "3.2 The Shortest Dependency Path", "text": "The dependency tree is, of course, suitable for classifying relationships because it focuses on the plot and the agents in one sentence (Socher et al., 2014). Furthermore, the shortest route between the units, as described in Section 1, provides the most illuminating information for the relationship between the units. We also observe that the trails separated by the common ancestral node of two units provide strong indications of the directionality of the relationship. Let us take Figure 1 as an example. Two units of water and region have cast their common ancestral node, which separates the SDP into two parts: [water] e1 \u2192 of \u2192 gallons \u2192 poured and poured \u2190 of [region] e2The first subpath captures information from e1, while the second subpath is mainly about e2."}, {"heading": "3.3 Channels", "text": "We call them channels because these sources of information do not interact during recurrent propagation. Detailed channel descriptions are as follows. \u2022 Word representations. Each word in a given sentence is assigned to a real vector by looking up in a word embedding table. \u2022 Word embeddings trained on a large corpus are probably capable of capturing the syntactic and semantic information of the words well (Mikolov et al., 2013b). \u2022 Part-of-speech tags. Since word embeddings are obtained on a generic corpus on a large scale, the information they contain may not match a specific sentence. We treat each word entered with its POS tag, e.g. noun, verb, etc. In our experiment, we only assume a coarse-grained POS category containing 15 different tags. \u2022 The dependence between words makes a difference."}, {"heading": "3.4 Recurrent Neural Network with Long Short Term Memory Units", "text": "Recurrent neural networks are suitable for modeling sequential data by nature, as they maintain an early weight shift to an early stage of weight shift. (The recurrent neural networks are). (The recurrent neural networks are). (The recurrent neural networks are probably an early weight shift. (The recurrent neural networks are). (The recurrent networks are the recurrent networks in the SDP (Figure 2b).) The hidden state, for the T-th word in the sub-path, is a function of its previous state. (The recurrent networks in the SDP (Figure 2b). (The networks in the SDP are). (The networks in the SDP are)."}, {"heading": "3.5 Dropout Strategies", "text": "A good regulatory approach is needed to mitigate the overmatch. Dropout, as recently proposed by Hinton et al. (2012), has been very successful with the feed networks. By arbitrarily omitting feature detectors in the network during training, it can obtain fewer interdependent network units and achieve better performance. However, since there is no consensus on how to drop LSTM units in the literature, we are trying several dropout strategies for our SDP-LSTM network: \u2022 dropout embedding; \u2022 failing internal cells in storage units, including it, gt, ot, ct and ht; and \u2022 dropout of the penultimate layer. As we will see in Section 4.2, canceling LSTM units is harmful to our model, whereas the other two strategies increase performance \u2212 bxt \u00b7 the following equations \u00b7 D \u00b7 droplt \u00b7 (D = 1 Uratot = 1 \u00b7 dropont = 1 \u00b7 dropont = 1 dropont = 1 dropont = 1 dropout = 1 dropout = 1 dropout = 1 dropout = 1 dropout = 1 dropout = 1 dropout = 1."}, {"heading": "3.6 Training Objective", "text": "The SDP-LSTM described above propagates information along a subpath from one entity to the common ancestor node (of the two entities). A maximum pooling layer bundles the states of the recursive network into a fixed vector for each subpath by taking the maximum value in each dimension.Such an architecture applies to all channels, namely words, POS tags, grammatical relations and WordNet hypernyms. The pooling vectors in these channels are concatenated and are added to a completely hidden layer. Finally, we add a softmax output layer for classification. The training goal is the punished cross-entropy error, which is caused by J = \u2212 nc, i = 1 ti log yi + \u03bb (1)."}, {"heading": "4 Experiments", "text": "In this section we present our experiments in detail. Our implementation is based on Mou et al. (2015). Section 4.1 presents the data set; Section 4.2 describes hyperparameter settings. In Section 4.3 we compare the performance of SDP-LSTM with other methods in the literature. We also analyze the effects of different channels in Section 4.4."}, {"heading": "4.1 Dataset", "text": "The SemEval-2010 Task 8 dataset is a widely used benchmark for the classification of relationships (Hendrickx et al., 2010).The dataset contains 8,000 sets for training and 2,717 for testing. We have divided 1 / 10 samples from the training set for validation.The goal contains 19 labels: 9 directed relationships and one undirected Other Classe.Below are two sample sets with directed relationships. \u2022 Cause Effect \u2022 Component Whole \u2022 Content Container \u2022 Entity Target \u2022 Message Topic \u2022 Membership Collection \u2022 Instrument Agency \u2022 Product Producer Below are two sample sets with directed relationships. [People] e1 are back in [Downtown] e2. Financial [Stress] e1 is one of the main causes of [Divorce] e2. The target labels are entity Goal (e1, e2), and Cause Effect (e2).The categories we place in the category \"Others,\" which we do not classify in the \"category 1.\""}, {"heading": "4.2 Hyperparameters and Training Details", "text": "In this subsection, we present hyperparameter tuning for our model. We specify that word embedding must be 200-dimensional; POS, WordNet hyponymy, and grammatical kinship embedding 50-dimensional. Each channel of the LSTM network contains the same number of units as its source embedding (either 200 or 50). The penultimate hidden layer is 100-dimensional. Since it is not possible to perform a complete network search for all hyperparameters, the above values are empirically selected. We add \"2 penalty for weights with coefficient 10 \u2212 5, selected by validation from the sentence {10 \u2212 2, 10 \u2212 3, \u00b7 \u00b7 \u00b7, 10 \u2212 7}. We then validate the proposed exit strategies in Section 3.5. Since network units do not interact with each other in different channels, we include a channel of LSTM networks in this section to assess effectiveness."}, {"heading": "4.3 Results", "text": "Table 4 compares our SDT-LSTM with other methods not included in the official Formula 1 evaluation; the first entry in the table represents the highest performance achieved by traditional feature engineering; Hendrickx et al. (2010) use a variety of handmade features and use SVM for classification; they achieve an F1 score of 82.2%; neural networks are used for the first time in this task in Socher et al. (2012); they build a recursive neural network (RNN) along a constituency tree for classification; they extend the basic RNN with matrix-vector interaction and achieve a F1Score of 82.4%; Zeng et al. (2014) treat one set as sequential data and use the revolutionary neural network (CNN); they also integrate word position information into their model; Santos et al al al al al (2015) design a model that is referred to as CR-CNN; they propose a cost-based function of the other, not the cost-based one."}, {"heading": "4.4 Effect of Different Channels", "text": "This section analyzes how different channels affect our model. We first used Word embedding only as a baseline, then added POS tags, grammatical relations, or WordNet hypernyms; we also combined all these channels into our models. Note that we did not try the last three channels alone, because each of them (e.g. POS) does not contain much information. Table 2 shows that word embedding yields a remarkable performance of 82.35% in SDP-LSTM alone, compared with CNNs 69.7%, RNNNNNNs 74.9-79.1%, and FCM 80.6%. Adding either grammatical relations or WordNet hypernyms surpasses other existing methods (data purification, which are not taken into account here). POS tagging is comparatively less informative, but nevertheless increases the F1 score by 0.63%."}, {"heading": "5 Conclusion", "text": "In this paper, we propose a novel neural network model called SDP-LSTM for the classification of relationships. It learns characteristics for classifying relationships iteratively along the shortest dependency path. Several types of information (word itself, POS tags, grammatical relationships and WordNet hypernyms) are used along the way. In the meantime, we use LSTM units for the dissemination and integration of information over long distances. We demonstrate the effectiveness of SDP-LSTM by evaluating the model for classifying relationships SemEval-2010, thus surpassing existing modern methods (under fair conditions without data cleansing). Our result sheds some light on the task of classifying relationships as follows. \u2022 The shortest dependency path can be a valuable resource for classifying relationships and covers largely sufficient information of course objectives. \u2022 The classification of relationships is a demanding task due to the inherent multiplicity of natural languages and diversity."}, {"heading": "Acknowledgments", "text": "This research is supported by the National Basic Research Programme of China (the 973 Programme) under grant numbers 2015 CB352201 and the National Science Foundation of China under grant numbers 61232015 and 91318301."}], "references": [{"title": "Open information extraction for the web", "author": ["Banko et al.2007] M. Banko", "M.J. Cafarella", "S. Soderland", "M. Broadhead", "O. Etzioni"], "venue": "In IJCAI,", "citeRegEx": "Banko et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Banko et al\\.", "year": 2007}, {"title": "Representation learning: A review and new perspectives", "author": ["Bengio et al.2013] Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "A shortest path dependency kernel for relation extraction", "author": ["Bunescu", "Mooney2005] R.C. Bunescu", "R.J. Mooney"], "venue": "In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,", "citeRegEx": "Bunescu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bunescu et al\\.", "year": 2005}, {"title": "Learning to extract relations from the web using minimal supervision", "author": ["Bunescu", "Mooney2007] R. Bunescu", "R. Mooney"], "venue": "In Annual meeting-association for Computational Linguistics,", "citeRegEx": "Bunescu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bunescu et al\\.", "year": 2007}, {"title": "Deriving local relational surface forms from dependency-based entity embeddings for unsupervised spoken language understanding", "author": ["Chen et al.2014] Yun-Nung Chen", "Dilek Hakkani-Tur", "Gokan Tur"], "venue": "In Spoken Language Technology Workshop", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger", "author": ["Ciaramita", "Altun2006] M. Ciaramita", "Y. Altun"], "venue": "In Proceedings of the", "citeRegEx": "Ciaramita et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ciaramita et al\\.", "year": 2006}, {"title": "Generating typed dependency parses from phrase structure parses", "author": ["B. MacCartney", "C.D. Manning"], "venue": "In Proceedings of LREC,", "citeRegEx": "Marneffe et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2006}, {"title": "Chain based rnn for relation classification", "author": ["Ebrahimi", "Dou2015] J. Ebrahimi", "D. Dou"], "venue": "In HLT-NAACL", "citeRegEx": "Ebrahimi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ebrahimi et al\\.", "year": 2015}, {"title": "RelEx\u2014Relation extraction using dependency parse", "author": ["Fundel et al.2007] K. Fundel", "R. K\u00fcffner", "R. Zimmer"], "venue": "trees. Bioinformatics,", "citeRegEx": "Fundel et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Fundel et al\\.", "year": 2007}, {"title": "Simple customization of recursive neural networks for semantic relation classification", "author": ["M. Miwa", "Y. Tsuruoka", "T. Chikayama"], "venue": "In EMNLP,", "citeRegEx": "Hashimoto et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2013}, {"title": "Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals", "author": ["S.N. Kim"], "venue": "Kozareva", "citeRegEx": "Hendrickx et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hendrickx et al\\.", "year": 2010}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580", "author": ["Hinton et al.2012] G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "The vanishing gradient problem during learning recurrent neural nets and problem solutions. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 6(02):107\u2013116", "author": ["S. Hochreiter"], "venue": null, "citeRegEx": "Hochreiter.,? \\Q1998\\E", "shortCiteRegEx": "Hochreiter.", "year": 1998}, {"title": "Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations", "author": ["N. Kambhatla"], "venue": "In Proceedings of the ACL 2004 on Interactive poster and demonstration sessions,", "citeRegEx": "Kambhatla.,? \\Q2004\\E", "shortCiteRegEx": "Kambhatla.", "year": 2004}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov et al.2013a] T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Mikolov et al.2013b] T. Mikolov", "W.T. Yih", "G. Zweig"], "venue": "In HLT-NAACL", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mintz et al.2009] M. Mintz", "S. Bills", "R. Snow", "D. Jurafsky"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting", "citeRegEx": "Mintz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Discriminative neural sentence modeling by tree-based convolution", "author": ["Mou et al.2015] L. Mou", "H. Peng", "G. Li", "Y. Xu", "L. Zhang", "Z. Jin"], "venue": "arXiv preprint arXiv:1504.01106", "citeRegEx": "Mou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2015}, {"title": "Embedding semantic similarity in tree kernels for domain adaptation of relation extraction", "author": ["Plank", "Moschitti2013] B. Plank", "A. Moschitti"], "venue": "ACL", "citeRegEx": "Plank et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Plank et al\\.", "year": 2013}, {"title": "Classifying relations by ranking with convolutional neural networks", "author": ["B. Xiang", "B. Zhou"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Socher et al.2011] R. Socher", "J. Pennington", "E.H. Huang", "A.Y. Ng", "C.D. Manning"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Socher et al.2012] R. Socher", "B. Huval", "C.D. Manning", "A.Y. Ng"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Grounded compositional semantics for finding and describing images with sentences. Transactions of the Association for Computational Linguistics, 2:207\u2013218", "author": ["Socher et al.2014] R. Socher", "A. Karpathy", "Q.V. Le", "C.D. Manning", "A.Y. Ng"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Medical relation extraction with manifold models", "author": ["Wang", "Fan2014] C. Wang", "J. Fan"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "A re-examination of dependency path kernels for relation extraction", "author": ["M. Wang"], "venue": "In IJCNLP,", "citeRegEx": "Wang.,? \\Q2008\\E", "shortCiteRegEx": "Wang.", "year": 2008}, {"title": "Open information extraction using wikipedia", "author": ["Wu", "Weld2010] F. Wu", "D.S. Weld"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Wu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2010}, {"title": "Learning non-taxonomic relations on demand for ontology extension", "author": ["Y. Xu", "G. Li", "L. Mou", "Y. Lu"], "venue": "International Journal of Software Engineering and Knowledge Engineering,", "citeRegEx": "Xu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Information extraction over structured data: Question answering with freebase", "author": ["Yao", "Van Durme2014] X. Yao", "B. Van Durme"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Yao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2014}, {"title": "Factor-based compositional embedding models", "author": ["M. Yu", "M.R. Gormley", "M. Dredze"], "venue": "In The NIPS 2014 Learning Semantics Workshop,", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Learning to execute. arXiv preprint arXiv:1410.4615", "author": ["Zaremba", "Sutskever2014] W. Zaremba", "I. Sutskever"], "venue": null, "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Kernel methods for relation extraction", "author": ["Zelenko et al.2003] D. Zelenko", "C. Aone", "A. Richardella"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Zelenko et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zelenko et al\\.", "year": 2003}, {"title": "Relation classification via convolutional deep neural network", "author": ["Zeng et al.2014] D. Zeng", "K. Liu", "S. Lai", "G. Zhou", "J. Zhao"], "venue": "In Proceedings of COLING,", "citeRegEx": "Zeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}, {"title": "Exploring various knowledge in relation extraction", "author": ["Zhou et al.2005] G.D. Zhou", "J. Su", "J. Zhang", "M. Zhang"], "venue": "In Proceedings of the 43rd annual meeting on association for computational linguistics,", "citeRegEx": "Zhou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2005}, {"title": "Long short-term memory over tree structures", "author": ["X. Zhu", "P. Sobhani", "H. Guo"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning", "citeRegEx": "Zhu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 26, "context": ", information extraction (Wu and Weld, 2010), question answering (Yao and Van Durme, 2014), medical informatics (Wang and Fan, 2014), ontology learning (Xu et al., 2014), etc.", "startOffset": 152, "endOffset": 169}, {"referenceID": 13, "context": "Traditional relation classification approaches rely largely on feature representation (Kambhatla, 2004), or kernel design (Zelenko et al.", "startOffset": 86, "endOffset": 103}, {"referenceID": 30, "context": "Traditional relation classification approaches rely largely on feature representation (Kambhatla, 2004), or kernel design (Zelenko et al., 2003; Bunescu and Mooney, 2005).", "startOffset": 122, "endOffset": 170}, {"referenceID": 1, "context": "Deep neural networks, emerging recently, provide a way of highly automatic feature learning (Bengio et al., 2013), and have exhibited considerable potential (Zeng et al.", "startOffset": 92, "endOffset": 113}, {"referenceID": 31, "context": ", 2013), and have exhibited considerable potential (Zeng et al., 2014; Santos et al., 2015).", "startOffset": 51, "endOffset": 91}, {"referenceID": 19, "context": ", 2013), and have exhibited considerable potential (Zeng et al., 2014; Santos et al., 2015).", "startOffset": 51, "endOffset": 91}, {"referenceID": 8, "context": "\u2022 Shortest dependency paths are informative (Fundel et al., 2007; Chen et al., 2014).", "startOffset": 44, "endOffset": 84}, {"referenceID": 4, "context": "\u2022 Shortest dependency paths are informative (Fundel et al., 2007; Chen et al., 2014).", "startOffset": 44, "endOffset": 84}, {"referenceID": 32, "context": ", hypernyms versus named-entity tags), and thus it is hard to improve performance in this way (Zhou et al., 2005).", "startOffset": 94, "endOffset": 113}, {"referenceID": 13, "context": "Kambhatla (2004) uses a maximum entropy model to combine these features for relation classification.", "startOffset": 0, "endOffset": 17}, {"referenceID": 29, "context": "Zelenko et al. (2003) compute the similarity of two trees by utilizing their common subtrees.", "startOffset": 0, "endOffset": 22}, {"referenceID": 29, "context": "Zelenko et al. (2003) compute the similarity of two trees by utilizing their common subtrees. Bunescu and Mooney (2005) propose a shortest path dependency kernel for relation classification.", "startOffset": 0, "endOffset": 120}, {"referenceID": 24, "context": "Wang (2008) provides a systematic analysis of several kernels and show that relation extraction can bene-", "startOffset": 0, "endOffset": 12}, {"referenceID": 21, "context": "(2011) propose a recursive neural network (RNN) along sentences\u2019 parse trees for sentiment analysis; such model can also be used to classify relations (Socher et al., 2012).", "startOffset": 151, "endOffset": 172}, {"referenceID": 18, "context": "Socher et al. (2011) propose a recursive neural network (RNN) along sentences\u2019 parse trees for sentiment analysis; such model can also be used to classify relations (Socher et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 9, "context": "Hashimoto et al. (2013) explicitly weight phrases\u2019 importance in RNNs to improve performance.", "startOffset": 0, "endOffset": 24}, {"referenceID": 9, "context": "Hashimoto et al. (2013) explicitly weight phrases\u2019 importance in RNNs to improve performance. Ebrahimi and Dou (2015) rebuild an RNN on the dependency path between two marked entities.", "startOffset": 0, "endOffset": 118}, {"referenceID": 9, "context": "Hashimoto et al. (2013) explicitly weight phrases\u2019 importance in RNNs to improve performance. Ebrahimi and Dou (2015) rebuild an RNN on the dependency path between two marked entities. Zeng et al. (2014) explore convolutional neural networks, by which they utilize sequential information of sentences.", "startOffset": 0, "endOffset": 204}, {"referenceID": 9, "context": "Hashimoto et al. (2013) explicitly weight phrases\u2019 importance in RNNs to improve performance. Ebrahimi and Dou (2015) rebuild an RNN on the dependency path between two marked entities. Zeng et al. (2014) explore convolutional neural networks, by which they utilize sequential information of sentences. Santos et al. (2015) also use the convolutional network; besides, they propose a ranking loss function with data cleaning, and achieve the state-of-the-art result in SemEval-2010 Task 8.", "startOffset": 0, "endOffset": 323}, {"referenceID": 0, "context": "In addition to the above studies, which mainly focus on relation classification approaches and models, other related research trends include information extraction from Web documents in a semi-supervised manner (Bunescu and Mooney, 2007; Banko et al., 2007), dealing with small datasets without enough labels by distant supervision techniques (Mintz et al.", "startOffset": 211, "endOffset": 257}, {"referenceID": 16, "context": ", 2007), dealing with small datasets without enough labels by distant supervision techniques (Mintz et al., 2009), etc.", "startOffset": 93, "endOffset": 113}, {"referenceID": 22, "context": "The dependency parse tree is naturally suitable for relation classification because it focuses on the action and agents in a sentence (Socher et al., 2014).", "startOffset": 134, "endOffset": 155}, {"referenceID": 12, "context": "Long short term memory (LSTM) units are proposed in Hochreiter (1998) to overcome this problem.", "startOffset": 52, "endOffset": 70}, {"referenceID": 12, "context": "Long short term memory (LSTM) units are proposed in Hochreiter (1998) to overcome this problem. The main idea is to introduce an adaptive gating mechanism, which decides the degree to which LSTM units keep the previous state and memorize the extracted features of the current data input. Many LSTM variants have been proposed in the literature. We adopt in our method a variant introduced by Zaremba and Sutskever (2014), also used in Zhu et al.", "startOffset": 52, "endOffset": 421}, {"referenceID": 12, "context": "Long short term memory (LSTM) units are proposed in Hochreiter (1998) to overcome this problem. The main idea is to introduce an adaptive gating mechanism, which decides the degree to which LSTM units keep the previous state and memorize the extracted features of the current data input. Many LSTM variants have been proposed in the literature. We adopt in our method a variant introduced by Zaremba and Sutskever (2014), also used in Zhu et al. (2014). Concretely, the LSTM-based recurrent neural network comprises four components: an input gate it, a forget gate ft, an output gate ot, and a memory cell ct (depicted in Figure 3 and formalized through Equations 1\u20136 as bellow).", "startOffset": 52, "endOffset": 453}, {"referenceID": 11, "context": "Dropout, proposed recently by Hinton et al. (2012), has been very successful on feed-forward networks.", "startOffset": 30, "endOffset": 51}, {"referenceID": 17, "context": "Our implementation is built upon Mou et al. (2015). Section 4.", "startOffset": 33, "endOffset": 51}, {"referenceID": 10, "context": "The SemEval-2010 Task 8 dataset is a widely used benchmark for relation classification (Hendrickx et al., 2010).", "startOffset": 87, "endOffset": 111}, {"referenceID": 10, "context": "Hendrickx et al. (2010) leverage a variety of handcrafted features, and use SVM for classification; they achieve an F1-score of 82.", "startOffset": 0, "endOffset": 24}, {"referenceID": 20, "context": "Neural networks are first used in this task in Socher et al. (2012). They build a recursive neural network (RNN) along a constituency tree for relation classification.", "startOffset": 47, "endOffset": 68}, {"referenceID": 19, "context": "Santos et al. (2015) design a model called CR-CNN; they propose a ranking-based cost function and elaborately diminish the impact of the Other class, which is not counted in the official F1-measure.", "startOffset": 0, "endOffset": 21}], "year": 2015, "abstractText": "Relation classification is an important research arena in the field of natural language processing (NLP). In this paper, we present SDP-LSTM, a novel neural network to classify the relation of two entities in a sentence. Our neural architecture leverages the shortest dependency path (SDP) between two entities; multichannel recurrent neural networks, with long short term memory (LSTM) units, pick up heterogeneous information along the SDP. Our proposed model has several distinct features: (1) The shortest dependency paths retain most relevant information (to relation classification), while eliminating irrelevant words in the sentence. (2) The multichannel LSTM networks allow effective information integration from heterogeneous sources over the dependency paths. (3) A customized dropout strategy regularizes the neural network to alleviate overfitting. We test our model on the SemEval 2010 relation classification task, and achieve an F1-score of 83.7%, higher than competing methods in the literature.", "creator": "LaTeX with hyperref package"}}}