{"id": "1508.03721", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Aug-2015", "title": "A Comparative Study on Regularization Strategies for Embedding-based Neural Networks", "abstract": "This paper aims to compare different regularization strategies to address a common phenomenon, severe overfitting, in embedding-based neural networks for NLP. We chose two widely studied neural models and tasks as our testbed. We tried several frequently applied or newly proposed regularization strategies, including penalizing weights (embeddings excluded), penalizing embeddings, re-embedding words, and dropout. We also emphasized on incremental hyperparameter tuning, and combining different regularizations. The results provide a picture on tuning hyperparameters for neural NLP models.", "histories": [["v1", "Sat, 15 Aug 2015 11:16:39 GMT  (113kb,D)", "http://arxiv.org/abs/1508.03721v1", "EMNLP '15"]], "COMMENTS": "EMNLP '15", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["hao peng", "lili mou", "ge li", "yunchuan chen", "yangyang lu", "zhi jin"], "accepted": true, "id": "1508.03721"}, "pdf": {"name": "1508.03721.pdf", "metadata": {"source": "CRF", "title": "A Comparative Study on Regularization Strategies for Embedding-based Neural Networks", "authors": ["Hao Peng", "Lili Mou", "Ge Li", "Yunchuan Chen", "Yangyang Lu", "Zhi Jin"], "emails": ["doublepower.mou}@gmail.com,{lige,", "zhijin}@sei.pku.edu.cn", "chenyunchuan11@mails.ucas.ac.cn"], "sections": [{"heading": null, "text": "We selected two widely studied neural models and tasks as test beds. We tried several commonly used or newly proposed regulatory strategies, including the punishment of weights (embedding excluded), the punishment of embedding, the reembedding of words and the dropout. We also emphasized the incremental hyperparameter setting and the combination of different regulations. The results provide an image of the tuning hyperparameters for neural NLP models."}, {"heading": "1 Introduction", "text": "In the early years of neural NLP research, the neural networks were used in speech modeling (Bengio et al., 2005; Mnih and Hinton, 2009); in recent years, they have been used in various areas, such as entity recognition (Collobert and Weston, 2008), empiricism analysis (Mou et al., 2015), relativization (Xu et al., 2015), where the neural networks are usually combined with word embedding, which usually assumes unsupervised algorithms."}, {"heading": "2 Tasks, Models, and Setup", "text": "The goal is to classify the relationship between two marked units in each sentence. We refer to recent advances, e.g., Hashimoto et al. (2013), Zeng et al. (2014), and Xu et al. (2015). To make our task and model general, we do not consider an entity that marks information; we do not distinguish in the order of two entities. In total, there are 10 labels, i.e., 9 different relationships plus a default setting. In terms of the neural model, we have applied Collobert's Convolutional Network (CNN)."}, {"heading": "3 Regularization Strategies", "text": "In this section, we describe four regulatory strategies used in our experiment. \u2022 Punishment \"2-norm of weights.\" Let E be the transverse entropy error for classification, and R be a regularization term. The general cost function is J = E + \u03bbR, where \u03bb is the coefficient. In this case, R = VP-2, and the coefficient is called \u03bbW. \u2022 Punishment \"2-norm of embedding. Somebody does not distinguish between embedding or connecting weights for regulation (Tai et al., 2015). However, we would like to analyze their effect separately, since embedding is rarely used. Let's call it embedding; then we have R = VP-2. \u2022 Re-embedding of words (Labutov and Lipson, 2013)."}, {"heading": "4 Individual Regularization Behaviors", "text": "This section compares the behavior of the individual strategies. We first performed both experiments without regulation, achieving accuracies of 54.02 \u00b1 0.84% and 41.47 \u00b1 2.85%, respectively. Then, in Figure 1, we plot learning curves when each regulation strategy is applied individually. However, we report training and validation accuracy through this work. The most important results are the following. \u2022 The punishment \"2-standard of weights helps generate; the effect largely depends on the size of the training set. Experiment I contains 7,000 training samples and the improvement is 6.98%; Experiment II contains more than 150k samples, and the improvement is only 2.07%. Such results are consistent with other machine learning models. \u2022 Penalizing\" 2-standard of embedding unexpectedly contributes to optimization (improves training accuracy). A plausible explanation is that the embedding of terms on a large corpus is trained by unattended methods."}, {"heading": "5 Incremental Hyperparameter Tuning", "text": "The above experiments show that regulation in general contributes to preventing overuse. To achieve the best performance, we have to try different hyperparameters through validation. Unfortunately, the formation of deep neural networks is time-consuming and prevents the complete network search from becoming a practical method. We exclude the dropout strategy because it does not allow hidden units incrementally, and we focus on experiments that are limited in time and space."}, {"heading": "6 Combination of Regularizations", "text": "Furthermore, we are curious about the behavior when different regulation methods are combined. Table 1 shows that the combination of \"2-standard weights and embedding leads to a further 3-4 percent improvement in accuracy when using one of the two. In a certain range of coefficients, weights and embedding complement each other: for one hyperparameter, we can set the other to achieve a result among the highest. Such compensation is also observed in the punishment of\" 2-standard versus dropouts \"(Table 2) - although peak performance is achieved by pure\" 2-regulation, the application of suspensions with a low \"2 penalty also achieves a similar accuracy. The failure rate is not very sensitive, provided it is small."}, {"heading": "7 Discussion", "text": "In this paper, we have systematically compared four regulatory strategies for embedding neural networks in NLP. Based on the experimental results, we answer our research questions as follows: (1) Regularization methods (with the exception of newly embedded words) essentially help generalize, and the punishment of the \"2-standard of embedding unexpectedly helps optimize. The performance of regularization largely depends on the size of the dataset. (2) '2 Penalty mainly acts as a local effect; hyperparameters can be gradually adjusted. (3) The combination of the\" 2-standard of weights and distortions (drop-out and \"2 penalty) further improves generalization; their coefficients are largely complementary within a certain range. These empirical results of regulatory strategies shed some light on the coordination of neural models for NLP."}, {"heading": "Acknowledgments", "text": "This research is supported by the National Basic Research Programme of China (the 973 Programme) under grant number 2015CB352201 and the National Science Foundation of China under grant number 61232015. We would also like to thank Hao Jia and Ran Jia."}], "references": [{"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "The effects of hyperparameters on SGD training of neural networks. arXiv preprint arXiv:1508.02788", "author": ["T. Breuel"], "venue": null, "citeRegEx": "Breuel.,? \\Q2015\\E", "shortCiteRegEx": "Breuel.", "year": 2015}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "J. Weston"], "venue": "In Proceedings of the 25th International Conference on Machine learning", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves et al.2013] Alex Graves", "A. Mohamed", "G. Hinton"], "venue": "In Proceedings of 2013 IEEE International Conference on Acoustics, Speech and Signal Processing", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Simple customization of recursive neural networks for semantic relation classification", "author": ["M. Miwa", "Y. Tsuruoka", "T. Chikayama"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Hashimoto et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2013}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Hu et al.2014] Baotian Hu", "Z. Lu", "H. Li", "Q. Chen"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Hu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["Irsoy", "Cardie2014] Ozan Irsoy", "C. Cardie"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Irsoy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Irsoy et al\\.", "year": 2014}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["I. Sutskever", "G. Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Re-embedding words", "author": ["Labutov", "Lipson2013] Igor Labutov", "H. Lipson"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Labutov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Labutov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["Mnih", "Hinton2009] Andriy Mnih", "G. Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mnih et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2009}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Morin", "Bengio2005] Frederic Morin", "Y. Bengio"], "venue": "In Proceedings of International Conference on Artificial Intelligence and Statistics", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "Tree-based convolution: A new neural architecture for sentence modeling", "author": ["Mou et al.2015] Lili Mou", "Hao Peng", "Ge Li", "Yan Xu", "Lu Zhang", "Zhi Jin"], "venue": "In Proceedings of Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Mou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2015}, {"title": "An empirical study of learning rates in deep neural networks for speech recognition", "author": ["Senior et al.2013] Andrew Senior", "G. Heigold", "M. Ranzato", "K. Yang"], "venue": "In Proceedings of 2013 IEEE International Conference on Acoustics,", "citeRegEx": "Senior et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Senior et al\\.", "year": 2013}, {"title": "Semisupervised recursive autoencoders for predicting sentiment distributions", "author": ["J. Pennington", "E. Huang", "A. Ng", "C. Manning"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["B. Huval", "C. Manning", "A. Ng"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Com-", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Tai et al.2015] Kai Sheng Tai", "R. Socher", "D. Manning"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Classifying relations via long short term memory networks along shortest dependency paths", "author": ["Xu et al.2015] Yan Xu", "Lili Mou", "Ge Li", "Yunchuan Chen", "Hao Peng", "Zhi Jin"], "venue": "In Proceedings of Conference on Empirical Methods in Natural Language", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Relation classification via convolutional deep neural network", "author": ["Zeng et al.2014] Daojian Zeng", "K. Liu", "S. Lai", "G. Zhou", "J. Zhao"], "venue": "In Proceedings of Computational Linguistics", "citeRegEx": "Zeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}, {"title": "Self-adaptive hierarchical sentence model", "author": ["Zhao et al.2015] Han Zhao", "Z. Lu", "P. Poupart"], "venue": "In Proceedings of Intenational Joint Conference in Artificial Intelligence", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "Neural networks have exhibited considerable potential in various fields (Krizhevsky et al., 2012; Graves et al., 2013).", "startOffset": 72, "endOffset": 118}, {"referenceID": 4, "context": "Neural networks have exhibited considerable potential in various fields (Krizhevsky et al., 2012; Graves et al., 2013).", "startOffset": 72, "endOffset": 118}, {"referenceID": 0, "context": "In early years on neural NLP research, neural networks were used in language modeling (Bengio et al., 2003; Morin and Bengio, 2005; Mnih and Hinton, 2009); recently, they have been applied to various supervised tasks, such as named entity recognition (Collobert and Weston, 2008), sentiment analysis (Socher et al.", "startOffset": 86, "endOffset": 154}, {"referenceID": 15, "context": ", 2003; Morin and Bengio, 2005; Mnih and Hinton, 2009); recently, they have been applied to various supervised tasks, such as named entity recognition (Collobert and Weston, 2008), sentiment analysis (Socher et al., 2011; Mou et al., 2015), relation classification (Zeng et al.", "startOffset": 200, "endOffset": 239}, {"referenceID": 13, "context": ", 2003; Morin and Bengio, 2005; Mnih and Hinton, 2009); recently, they have been applied to various supervised tasks, such as named entity recognition (Collobert and Weston, 2008), sentiment analysis (Socher et al., 2011; Mou et al., 2015), relation classification (Zeng et al.", "startOffset": 200, "endOffset": 239}, {"referenceID": 20, "context": ", 2015), relation classification (Zeng et al., 2014; Xu et al., 2015), etc.", "startOffset": 33, "endOffset": 69}, {"referenceID": 19, "context": ", 2015), relation classification (Zeng et al., 2014; Xu et al., 2015), etc.", "startOffset": 33, "endOffset": 69}, {"referenceID": 0, "context": "In early years on neural NLP research, neural networks were used in language modeling (Bengio et al., 2003; Morin and Bengio, 2005; Mnih and Hinton, 2009); recently, they have been applied to various supervised tasks, such as named entity recognition (Collobert and Weston, 2008), sentiment analysis (Socher et al., 2011; Mou et al., 2015), relation classification (Zeng et al., 2014; Xu et al., 2015), etc. In the field of NLP, neural networks are typically combined with word embeddings, which are usually first pretrained by unsupervised algorithms like Mikolov et al. (2013); then they are fed forward to standard neural models, fine-tuned during supervised learning.", "startOffset": 87, "endOffset": 579}, {"referenceID": 2, "context": "consuming, as suggested in Collobert et al. (2011). Therefore, new studies are needed to provide a more complete picture regarding regularization for neural natural language processing.", "startOffset": 27, "endOffset": 51}, {"referenceID": 17, "context": "In this paper, we systematically and quantitatively compared four different regularization strategies, namely penalizing weights, penalizing embeddings, newly proposed word re-embedding (Labutov and Lipson, 2013), and dropout (Srivastava et al., 2014).", "startOffset": 226, "endOffset": 251}, {"referenceID": 5, "context": ", Hashimoto et al. (2013), Zeng et al.", "startOffset": 2, "endOffset": 26}, {"referenceID": 5, "context": ", Hashimoto et al. (2013), Zeng et al. (2014), and Xu et al.", "startOffset": 2, "endOffset": 46}, {"referenceID": 5, "context": ", Hashimoto et al. (2013), Zeng et al. (2014), and Xu et al. (2015). To make our task and model", "startOffset": 2, "endOffset": 68}, {"referenceID": 15, "context": "The dataset is the Stanford sentiment treebank (Socher et al., 2011)2; target labels are strongly/weakly positive/negative, or neutral.", "startOffset": 47, "endOffset": 68}, {"referenceID": 15, "context": "We used the recursive neural network (RNN), which is proposed in Socher et al. (2011), and further developed in Socher et al.", "startOffset": 65, "endOffset": 86}, {"referenceID": 15, "context": "We used the recursive neural network (RNN), which is proposed in Socher et al. (2011), and further developed in Socher et al. (2012); Irsoy and Cardie (2014).", "startOffset": 65, "endOffset": 133}, {"referenceID": 15, "context": "We used the recursive neural network (RNN), which is proposed in Socher et al. (2011), and further developed in Socher et al. (2012); Irsoy and Cardie (2014). RNNs make use of binarized constituency trees, and recursively encode children\u2019s information to their parent\u2019s; the root vector is finally used for sentiment classification.", "startOffset": 65, "endOffset": 158}, {"referenceID": 21, "context": "Such setting has been used in previous work like Zhao et al. (2015). Our embeddings are pretrained on the Wikipedia corpus using Collobert and Weston (2008).", "startOffset": 49, "endOffset": 68}, {"referenceID": 21, "context": "Such setting has been used in previous work like Zhao et al. (2015). Our embeddings are pretrained on the Wikipedia corpus using Collobert and Weston (2008). The learning rate is 0.", "startOffset": 49, "endOffset": 157}, {"referenceID": 14, "context": "fore, we applied power decay (Senior et al., 2013) with power equal to \u22121.", "startOffset": 29, "endOffset": 50}, {"referenceID": 17, "context": "\u2022 Dropout (Srivastava et al., 2014).", "startOffset": 10, "endOffset": 35}, {"referenceID": 6, "context": "Such strategy is sometimes applied in the literature like Hu et al. (2014), but is not favorable as suggested by the experiment.", "startOffset": 58, "endOffset": 75}, {"referenceID": 1, "context": "may not play an important role in training neural networks, if the effect of parameter symmetry is ruled out (Breuel, 2015).", "startOffset": 109, "endOffset": 123}], "year": 2015, "abstractText": "This paper aims to compare different regularization strategies to address a common phenomenon, severe overfitting, in embedding-based neural networks for NLP. We chose two widely studied neural models and tasks as our testbed. We tried several frequently applied or newly proposed regularization strategies, including penalizing weights (embeddings excluded), penalizing embeddings, reembedding words, and dropout. We also emphasized on incremental hyperparameter tuning, and combining different regularizations. The results provide a picture on tuning hyperparameters for neural NLP models.", "creator": "LaTeX with hyperref package"}}}