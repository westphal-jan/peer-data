{"id": "1508.03826", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2015", "title": "A Generative Word Embedding Model and its Low Rank Positive Semidefinite Solution", "abstract": "Most existing word embedding methods can be categorized into Neural Embedding Models and Matrix Factorization (MF)-based methods. However some models are opaque to probabilistic interpretation, and MF-based methods, typically solved using Singular Value Decomposition (SVD), may incur loss of corpus information. In addition, it is desirable to incorporate global latent factors, such as topics, sentiments or writing styles, into the word embedding model. Since generative models provide a principled way to incorporate latent factors, we propose a generative word embedding model, which is easy to interpret, and can serve as a basis of more sophisticated latent factor models. The model inference reduces to a low rank weighted positive semidefinite approximation problem. Its optimization is approached by eigendecomposition on a submatrix, followed by online blockwise regression, which is scalable and avoids the information loss in SVD. In experiments on 7 common benchmark datasets, our vectors are competitive to word2vec, and better than other MF-based methods.", "histories": [["v1", "Sun, 16 Aug 2015 14:12:17 GMT  (146kb,D)", "http://arxiv.org/abs/1508.03826v1", "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP) 2015 2015, 11 pages, 2 figures"]], "COMMENTS": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP) 2015 2015, 11 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CL cs.LG stat.ML", "authors": ["shaohua li", "jun zhu", "chunyan miao"], "accepted": true, "id": "1508.03826"}, "pdf": {"name": "1508.03826.pdf", "metadata": {"source": "CRF", "title": "A Generative Word Embedding Model and its Low Rank Positive Semidefinite Solution", "authors": ["Shaohua Li", "Jun Zhu", "Chunyan Miao"], "emails": ["lish0018@ntu.edu.sg,", "dcszj@tsinghua.edu.cn,", "ascymiao@ntu.edu.sg"], "sections": [{"heading": "1 Introduction", "text": "This year, it is only a matter of time before there is a result in which there is a result."}, {"heading": "2 Notations and Definitions", "text": "Throughout the paper, we always use a bold uppercase letter asS, V to denote a matrix or set, a bold lowercase letter as vwi to denote a vector, a normal uppercase letter as N, W to denote a scalar constant, and a normal lowercase letter as si, wi to denote a scalar variable.Suppose that a vocabulary S = {s1, \u00b7 \u00b7, sW} consists of all words, where W is the word size. We also assume that s1, \u00b7, sW are sorted in descending order of frequency, i.e. s1 is most common, and sW the least common. A document di is a sequence of words di = (wi1, \u00b7, wiLi), wij. A corpus is a collection of M documents D = {d1, \u00b7, dM}."}, {"heading": "3 Link Function of Text", "text": "In this section we formulate the probability of a word sequence depending on its embedding. We start from the linking function of bigrams, which are the building blocks of a long sequence. Subsequently, this linking function is extended to a text window with c context words, as an approximation of the first order of the actual probability."}, {"heading": "3.1 Link Function of Bigrams", "text": "We generalize the linking function of \"word2vec\" and \"GloVe\" to the following: P (si, sj) = exp {v > sjvsi + asisj} P (si) P (sj) (1) The rationale for (1) comes from the idea of the expert product in (Hinton, 2002). Suppose different kinds of semantic / syntactic regularities between si and sj are combined by multiplication. If si and sj are independent, their common probability should be P (si) P (sj) P (sj) (sj). In the presence of correlations, the actual joint probability P (si, sj) is a scaling of the same."}, {"heading": "3.1.1 Gaussian Priors on Embeddings", "text": "If (1) is applied to the regression of empirical bigram probabilities, a practical problem arises: More and more bigrams have zero frequency because the constituent words become rarer. A zero frequency bigram does not necessarily imply a negative correlation between the two constituent words; it could simply result from missing data. However, in this case, (1) v > sjvsi + asisj forces a large negative number even after smoothing, making vsi excessively long. The increasing size of the embeddings is a sign of overmatch. To reduce the overlap of rare words, we assign vsi: P (vsi) a spherical Gaussian number before N (0, 12\u00b5i I), with the hyperparameter \u00b5i increasing when the frequency of si decreases."}, {"heading": "3.1.2 Gaussian Priors on Residuals", "text": "We want v > sjvsi to capture as many correlations between si and sj as possible in (1). The smaller asisj is, the better. To this end, we punish residual asisj with f (si, sj) a2sisj, where f (\u00b7) is a non-negative monotonic transformation called a weighting function. If we leave hij denote P (si, sj), then the total penalty of all residuals is the square of the weighted Frobenius norm of A: \"si,\" sj \"S f (hij) a2 sisj =\" A \"2 f\" (H), where the total penalty of all residuals is the square of the weighted Frobenius norm of A."}, {"heading": "3.1.3 Jelinek-Mercer Smoothing of Bigrams", "text": "As a further measure to reduce the impact of missing data, we use the commonly used JelinekMercer Smoothing (Zhai and Lafferty, 2004) to smooth the empirical conditional probability P (sj | si) by the uniigram probability P (sj). (6) Accordingly, the smoothed bigram empirical joint probability is defined as P (si, sj) = (1 \u2212 0) P (si, sj) + p (si) P (sj). (7) In practice, good results are obtained when the obtained embedding begins with N, which indicates that the smoothing distorts the true bigram distributions."}, {"heading": "3.2 Link Function of a Text Window", "text": "In this section, we will adopt a first-order approach based on information theory and extend the link function to a longer sequence w0, \u00b7 \u00b7, wc \u2212 1, wc.Decomposing a distribution caused by n random variables as a conditional distribution of its subsets deep in information theory.This is a complicated problem because there could be both (pointwise) redundant information and (pointwise) synergistic information between the climate variables (Williams and Beer, 2010), which are both functions of the PMI. Based on an analysis of the complementary roles of these two types of pointwise information, we assume that they are approximately the same and cancel each other when calculating the pointwise interaction information. see Appendix B for a detailed discussion (www2) wwwwwwwc, ww1 wwww2; w0, w1) wPMI (w2; w0) + wPMI (wPMI) wPMwPMI (wPMI > w2 (wPMI = P), w2 wPMI (wPMI = P), w2 (wPMI = P)."}, {"heading": "4 Generative Process and Likelihood", "text": "We assume that the text is derived from a Markov chain of order c \u2212 \u2212 j = = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = p = (1, \u00b7 \u00b7, \u00b7 W), the generation process of the entire corpus results as follows: 1. For each word si, the embedded vsi results from N (0, 12\u00b5i I); 2. For each bigram si, sj, extract the word wij from N (0, 12f (hij))); 3. For each document di, for the j-th word, extract the word wij from S with the probability P (wij | wi, j \u2212 c: wi, j \u2212 1) defined by (8). The above generation process for a document d is derived as a graphic model in Figure 1.Based on this generative process, the probability of a document wij can be derived as follows."}, {"heading": "5 Learning Algorithm", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Learning Objective", "text": "The learning objective is to find embeddings V that maximize the corpus log probability (9), then sort (9) as follows: Log p (D, V, A) = C0 \u2212 logZ (H, \u00b5) \u2212 HQ (H) \u2212 HQ (W) \u2212 HQ (W) \u2212 HQ (S) 2 + W, W \u00b2 i, J = 1 xij (V > asisj). (10) As the corpus size increases, W, W i, J = 1 xij (V > sivsj + asisj) will dominate the parameter before the terms. Then, we can ignore the previous terms when the maximum (10).Max."}, {"heading": "5.2 Learning V as Low Rank PSD Approximation", "text": "OnceG \u043d was estimated by the corpus using (12), we are looking for V that maximizes (13), this is to find the maximum a posteriori (MAP) estimates of V, A that meet V > V + A = G. Applying this limitation to (13), we are considering algorithm 1 BCD algorithm to determine an unregulated Rank-N weighted PSD value. Input: matrixG \u0445, weight matrixW = f (H), iteration number T, rank NRandomly initializeX (0) for the search for an unregulated Rank-N weighted PSD value. Input: matrixG \u0445 X (t \u2212 1) X (PSD) = f (H), rank NRandomly initializeX (0) for the search for an unregulated Rank-N (0)."}, {"heading": "5.3 Online Blockwise Regression of V", "text": "In fact, most of them are able to surpass themselves. (...) Most of them are not able to surpass themselves. (...) Most of them are not able to surpass themselves. (...) Most of them are not able to surpass themselves. (...) Most of them are not able to surpass themselves. (...) Most of them are not able to surpass themselves. (...) Most of them are not able to surpass themselves. \"(...) Most of them are able to surpass themselves. (...) Most of them are not able to surpass themselves. (...) Most of them are not able to surpass themselves.\" (...) Most of them are able to surpass themselves. \""}, {"heading": "6 Experimental Results", "text": "We trained our model together with some state-of-the-art competitors on Wikipedia and evaluated the embedding using 7 common benchmark sets."}, {"heading": "6.1 Experimental Setup", "text": "Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal, Portugal,"}, {"heading": "6.2 Results", "text": "Table 2 shows the results of all tasks. Word2vec performed significantly worse on analogy tasks than other methods, indicating that its performance is unstable. The new embeddings achieved by Sparse systematically deteriorated compared to the old embeddings, which contradicts the claim in (Faruqui et al., 2015). Compared to PSD-Unreg-180K, our method PSD-Reg-180K consistently performs well and performs best on 4 similarity tasks. It performed worse on analogy tasks than word2vec, but still better than other MF-based methods. Compared to PSD-Unreg-180K, it shows that the Tikhonov regulation delivers a performance increase of 1-4% over all tasks."}, {"heading": "7 Conclusions and Future Work", "text": "In this paper, inspired by the link functions of previous work, with the support of information theory, we propose a new link function of a text window parameterized by the embedding of words and the remnants of bigrams. Based on the link function, we establish a generative model of documents. The learning goal is to find a series of embeddings that maximize their posterior probability in view of the corpus. This goal is reduced to a weighted low-level positive-semi-defined approach, subject to Tikhonov regularization. Subsequently, we use a block coordinate descent algorithm, together with a block-based online regression algorithm, to find an approximate solution. In seven benchmark sets, the embeddings learned show competitive and stable performance. In the future work, we will incorporate global latent factors into this generative model, such as themes, sensations or fuel learning, and the complexity of such documents."}, {"heading": "Acknowledgments", "text": "We thank Omer Levy, Thomas Mach, Peilin Zhao, Mingkui Tan, Zhiqiang Xu and Chunlin Wu for their helpful discussions and insights. This research is supported by the National Research Foundation, Office of the Prime Minister, Singapore under the IDM Futures Funding Initiative and managed by the Office of Interactive and Digital Media."}, {"heading": "Appendix A Possible Trap in SVD", "text": "If some of these singular values correspond to negative eigenvalues, undesirable correlations can be recorded: M (1) = (1.4 0.8 0 0.8 2.6 0 0 2), M (2) = (0.2 \u2212 1.6 0 \u2212 1.6 \u2212 2.2 00 0 2) Two corpora derive two PMI matrices: M (1) = (1.4 0.8 0 0.8 2.0 0 0 2), M (2) = (0.2 \u2212 1.6 \u2212 2.2 00 0 2) They have identical left singular matrices and singular values (3, 2, 1), but their eigenvalues are (3, 2, 1) and (\u2212 3, 2, 1) the eigencorrix (0.2). In a rank-2 approximation, the largest two singular values / vectors are retained, and M (1) and their eigenvalues are (2, 1) the eigenvalues (0.2)."}, {"heading": "Appendix B Information Theory", "text": "Redundant information refers to reduced uncertainty by knowing the value of one of the conditioning variables (hence redundant), synergistic information is the reduced uncertainty attributed to knowing all the values of conditioning variables that cannot be reduced by knowing the value of any variable alone (hence synergistic), the mutual information I (y; xi) and the redundant information Rdn (y; x2) and the redundant information Rdn (y; x2) P (y) are defined as: I (y; x1, x2) and the synergistic information P (x2) [min x1, x2 EP (xi | y) [log P (y; xi) = EP (y, y) [log P (y)."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "Most existing word embedding methods<lb>can be categorized into Neural Embedding<lb>Models and Matrix Factorization (MF)-<lb>based methods. However some mod-<lb>els are opaque to probabilistic interpre-<lb>tation, and MF-based methods, typically<lb>solved using Singular Value Decomposi-<lb>tion (SVD), may incur loss of corpus in-<lb>formation. In addition, it is desirable to<lb>incorporate global latent factors, such as<lb>topics, sentiments or writing styles, into<lb>the word embedding model. Since gen-<lb>erative models provide a principled way<lb>to incorporate latent factors, we propose a<lb>generative word embedding model, which<lb>is easy to interpret, and can serve as a<lb>basis of more sophisticated latent factor<lb>models. The model inference reduces to<lb>a low rank weighted positive semidefinite<lb>approximation problem. Its optimization<lb>is approached by eigendecomposition on a<lb>submatrix, followed by online blockwise<lb>regression, which is scalable and avoids<lb>the information loss in SVD. In experi-<lb>ments on 7 common benchmark datasets,<lb>our vectors are competitive to word2vec,<lb>and better than other MF-based methods.", "creator": "LaTeX with hyperref package"}}}