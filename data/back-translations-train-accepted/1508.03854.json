{"id": "1508.03854", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2015", "title": "Online Representation Learning in Recurrent Neural Language Models", "abstract": "We investigate an extension of continuous online learning in recurrent neural network language models. The model keeps a separate vector representation of the current unit of text being processed and adaptively adjusts it after each prediction. The initial experiments give promising results, indicating that the method is able to increase language modelling accuracy, while also decreasing the parameters needed to store the model along with the computation required at each step.", "histories": [["v1", "Sun, 16 Aug 2015 18:27:25 GMT  (368kb,D)", "http://arxiv.org/abs/1508.03854v1", "In Proceedings of EMNLP 2015"]], "COMMENTS": "In Proceedings of EMNLP 2015", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["marek rei"], "accepted": true, "id": "1508.03854"}, "pdf": {"name": "1508.03854.pdf", "metadata": {"source": "CRF", "title": "Online Representation Learning in Recurrent Neural Language Models", "authors": ["Marek Rei"], "emails": ["marek.rei@cl.cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In recent years, neural network models have shown impressive performance in many natural language processing tasks, such as speech recognition (Chorowski et al., 2014; Graves et al., 2013), machine translation (Kalchbrenner et al., 2013; Cho et al., 2014), text classification (Le and Mikolov, 2014; Kalchbrenner et al., 2014), and image description generation (Kiros et al., 2014). One of the main advantages of these methods is the ability to learn smooth vector representations of words, thereby reducing the sparsity problem inherent in any natural language data collection. Language modeling is another task in which neural networks have achieved excellent results (Bengio et al., 2003; Mikolov et al., 2011). Chelba et al. (2014) have recently highlighted several well-known language models by training on very large datasets. They found that we need a recurrent network model with a Maxonal Language System (MR9) for a combination of neuronal research."}, {"heading": "2 RNNLM", "text": "We base our implementation of the RNNLM on Mikolov et al. (2011), shown in Figure 1. The input layer of the network consists of a 1-hot vector representing the previous word in the sequence and the hidden vector from the previous time step. These are multiplied by corresponding weight matrices and the resulting vectors are passed through an activation function to calculate the hidden vec-ar Xiv: 150 8.03 854v 1 [cs.C L] 16 August 201 5tor to the current time step. 1Class-based output architecture is used to avoid the calculation of the softmax across all words in the vocabulary. The probability distributions across words and classes are calculated by multiplying the hidden vector by the corresponding weight matrix and applying the softmax function: hiddent = circular (E \u00b7 putt \u00b7 soc \u00b7 dmax = 1 \u2212 ftmax)."}, {"heading": "3 RNNLM with online learning", "text": "rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf"}, {"heading": "4 Experiments", "text": "In fact, in the USA, in the USA, in the USA, in Europe, in the USA, in Europe, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA"}, {"heading": "5 Conclusion", "text": "We have described a possible extension of the RNNLM, which uses continuous online learning, with a separate vector for the representation of the text unit, such as a sentence currently being processed; the vector starts in a standard state and is continuously updated through back propagation, resulting in a more informative representation; the modified language model achieves less helplessness with more optimal use of parameters; the idea of continuous training and adaptation is natural and also established in biological learning processes, but is not widely used due to the computational complexity; our experiments suggest that by incorporating this active learning component into the neural network model, the system can achieve greater accuracy, while at the same time reducing the parameters required for storing the model and reducing the computing power required."}], "references": [{"title": "A Neural Probabilistic Language Model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin."], "venue": "3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling", "author": ["Ciprian Chelba", "Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson."], "venue": "INTERSPEECH 2014.", "citeRegEx": "Chelba et al\\.,? 2014", "shortCiteRegEx": "Chelba et al\\.", "year": 2014}, {"title": "Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine Translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "June.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "End-to-end Continuous Speech Recognition using Attention-based Recurrent NN : First Results", "author": ["Jan Chorowski", "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Chorowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of the 25th international conference on Machine learning.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Hybrid speech recognition with deep bidirectional LSTM", "author": ["Alex Graves", "Navdeep Jaitly", "Abdel-rahman Mohamed."], "venue": "ASRU 2013.", "citeRegEx": "Graves et al\\.,? 2013", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Recurrent Continuous Translation Models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "EMNLP.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "The 52nd Annual Meeting of the Association for Computational Linguistics. Proceedings of the 52nd Annual Meeting of the", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Multimodal Neural Language Models", "author": ["Ryan Kiros", "Ruslan Salakhutdinov", "Richard Zemel."], "venue": "ICML 2014.", "citeRegEx": "Kiros et al\\.,? 2014", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Distributed Representations of Sentences and Documents", "author": ["Quoc Le", "Tomas Mikolov."], "venue": "ICML, volume 32.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "RNNLM - Recurrent neural network language modeling toolkit", "author": ["Toma\u0161 Mikolov", "Stefan Kombrink", "Anoop Deoras", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u00fd."], "venue": "ASRU 2011 Demo Session.", "citeRegEx": "Mikolov et al\\.,? 2011", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "ICLR Workshop, pages 1\u201312.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A Scalable Hierarchical Distributed Language Model", "author": ["Andriy Mnih", "Geoffrey Hinton."], "venue": "Advances in Neural Information Processing Systems, pages 1\u20138.", "citeRegEx": "Mnih and Hinton.,? 2008", "shortCiteRegEx": "Mnih and Hinton.", "year": 2008}], "referenceMentions": [{"referenceID": 3, "context": "In recent years, neural network models have shown impressive performance on many natural language processing tasks, such as speech recognition (Chorowski et al., 2014; Graves et al., 2013), machine translation (Kalchbrenner and Blunsom, 2013; Cho et al.", "startOffset": 143, "endOffset": 188}, {"referenceID": 5, "context": "In recent years, neural network models have shown impressive performance on many natural language processing tasks, such as speech recognition (Chorowski et al., 2014; Graves et al., 2013), machine translation (Kalchbrenner and Blunsom, 2013; Cho et al.", "startOffset": 143, "endOffset": 188}, {"referenceID": 6, "context": ", 2013), machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014), text classification (Le and Mikolov, 2014; Kalchbrenner et al.", "startOffset": 29, "endOffset": 79}, {"referenceID": 2, "context": ", 2013), machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014), text classification (Le and Mikolov, 2014; Kalchbrenner et al.", "startOffset": 29, "endOffset": 79}, {"referenceID": 9, "context": ", 2014), text classification (Le and Mikolov, 2014; Kalchbrenner et al., 2014) and image description generation (Kiros et al.", "startOffset": 29, "endOffset": 78}, {"referenceID": 7, "context": ", 2014), text classification (Le and Mikolov, 2014; Kalchbrenner et al., 2014) and image description generation (Kiros et al.", "startOffset": 29, "endOffset": 78}, {"referenceID": 8, "context": ", 2014) and image description generation (Kiros et al., 2014).", "startOffset": 41, "endOffset": 61}, {"referenceID": 0, "context": "Language modelling is another task where neural networks have delivered excellent results (Bengio et al., 2003; Mikolov et al., 2011).", "startOffset": 90, "endOffset": 133}, {"referenceID": 10, "context": "Language modelling is another task where neural networks have delivered excellent results (Bengio et al., 2003; Mikolov et al., 2011).", "startOffset": 90, "endOffset": 133}, {"referenceID": 0, "context": "Language modelling is another task where neural networks have delivered excellent results (Bengio et al., 2003; Mikolov et al., 2011). Chelba et al. (2014) have recently benchmarked several well-known language models by training on very large datasets.", "startOffset": 91, "endOffset": 156}, {"referenceID": 4, "context": "The technique is inspired by work on representation learning (Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2013), especially Le and Mikolov (2014) who use a related model to learn representations for text classification.", "startOffset": 61, "endOffset": 134}, {"referenceID": 12, "context": "The technique is inspired by work on representation learning (Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2013), especially Le and Mikolov (2014) who use a related model to learn representations for text classification.", "startOffset": 61, "endOffset": 134}, {"referenceID": 11, "context": "The technique is inspired by work on representation learning (Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2013), especially Le and Mikolov (2014) who use a related model to learn representations for text classification.", "startOffset": 61, "endOffset": 134}, {"referenceID": 4, "context": "The technique is inspired by work on representation learning (Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2013), especially Le and Mikolov (2014) who use a related model to learn representations for text classification.", "startOffset": 62, "endOffset": 169}, {"referenceID": 10, "context": "We base our implementation of the RNNLM on Mikolov et al. (2011), shown in Figure 1.", "startOffset": 43, "endOffset": 65}, {"referenceID": 1, "context": "While the system of Le and Mikolov (2014) uses a basic feedforward language model, we extend the idea to recurrent neural network language models, as they are currently used in state-ofthe-art language modelling systems (Chelba et al., 2014).", "startOffset": 220, "endOffset": 241}, {"referenceID": 8, "context": "While the system of Le and Mikolov (2014) uses a basic feedforward language model, we extend the idea to recurrent neural network language models, as they are currently used in state-ofthe-art language modelling systems (Chelba et al.", "startOffset": 20, "endOffset": 42}], "year": 2015, "abstractText": "We investigate an extension of continuous online learning in recurrent neural network language models. The model keeps a separate vector representation of the current unit of text being processed and adaptively adjusts it after each prediction. The initial experiments give promising results, indicating that the method is able to increase language modelling accuracy, while also decreasing the parameters needed to store the model along with the computation required at each step.", "creator": "TeX"}}}