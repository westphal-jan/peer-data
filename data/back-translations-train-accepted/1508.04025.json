{"id": "1508.04025", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Aug-2015", "title": "Effective Approaches to Attention-based Neural Machine Translation", "abstract": "An attentional mechanism has been used in neural machine translation (NMT) to selectively focus on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. Our attentional NMTs provide a boost of up to 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. For the English to German direction, we have established new state-of-the-art results of 23.0 BLEU for WMT'14 and 25.9 BLEU for WMT'15. Our in-depth analysis sheds light on which architectures are best and we are first to assess attentional models using alignment error rates.", "histories": [["v1", "Mon, 17 Aug 2015 13:43:19 GMT  (91kb)", "https://arxiv.org/abs/1508.04025v1", "EMNLP 2015 camera-ready version"], ["v2", "Tue, 18 Aug 2015 10:27:26 GMT  (91kb)", "http://arxiv.org/abs/1508.04025v2", "EMNLP 2015 camera-ready version"], ["v3", "Wed, 19 Aug 2015 08:14:59 GMT  (92kb)", "http://arxiv.org/abs/1508.04025v3", "11 pages, EMNLP 2015 camera-ready version"], ["v4", "Sat, 29 Aug 2015 09:03:04 GMT  (95kb)", "http://arxiv.org/abs/1508.04025v4", "11 pages, 7 figures, EMNLP 2015 camera-ready version, minor fixes"], ["v5", "Sun, 20 Sep 2015 08:25:52 GMT  (73kb)", "http://arxiv.org/abs/1508.04025v5", "11 pages, 7 figures, EMNLP 2015 camera-ready version, more training details"]], "COMMENTS": "EMNLP 2015 camera-ready version", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["thang luong", "hieu pham", "christopher d manning"], "accepted": true, "id": "1508.04025"}, "pdf": {"name": "1508.04025.pdf", "metadata": {"source": "CRF", "title": "Effective Approaches to Attention-based Neural Machine Translation", "authors": ["Minh-Thang Luong Hieu Pham", "Christopher D. Manning"], "emails": ["lmthang@stanford.edu", "hyhieu@stanford.edu", "manning@stanford.edu"], "sections": [{"heading": null, "text": "ar Xiv: 150 8.04 025v 5 [cs.C L] 20 Sep 2015"}, {"heading": "1 Introduction", "text": "In fact, most people who are able to survive themselves are able to survive themselves, \"he told the German Press Agency in an interview with\" Welt am Sonntag \":\" I don't think we will be able to put the world in order. \""}, {"heading": "2 Neural Machine Translation", "text": "A neural machine translation system is a neural network that leads directly to the conditional probability p (y = x) of translating a source set (x1,.., xn, to a target sentence, y1,.,., However, a basic form of NMT consists of two components: (a) an encoder that calculates a representation for each source set, and (b) a decoder that generates a target word at a given time, and therefore the conditional probability as: log p (y) = 1 log p (yj) that calculates a representation s (y). (1) A natural choice to model such a decomposition in the decoder is to use a recent work by Gregor et al. (2015) that is very similar to our local attention."}, {"heading": "3 Attention-based Models", "text": "Our various attention-based models are divided into two broad categories, global and local, and these classes differ in whether \"attention\" is paid to all source positions or only a few source positions. We illustrate these two model types in Figure 2 and 3, respectively. Common to these two types of models is the fact that at each step t in the decoding phase, both approaches first use the hidden state on the top layer of a stacking LSTM as input, with the goal then to derive a context vector that captures relevant source-side information to help predict the current target yt. While these models differ in the way the context vector is derived, they share the same subsequent steps. In particular, given the hidden target state and the source-side context vector, we use a simple concatenation layer to combine the information from both vectors to create a maximum merciful layer (the merciful state is 5)."}, {"heading": "3.1 Global Attention", "text": "In this model, a variable alignment vector, the size of which corresponds to the number of time steps on the source side, is derived by comparing the current target steps with the respective output layers. Here, the rating is referred to as a content-based function, for which we consider three different alternatives: Score (ht, h, s), Score (h, s), Score (ht, h), Score (h), Score (h), Score (h), Scale (h), Scale (h), Scale (h), Scale (h), Scale (h), Scale (h), Scale (h), Scale (h), Scale (h), Scale (h), Scale (h), Scale (h), Scale (h), Scale (h), Scale (h), Scale (h), Scale (c), c (c), c (h), c (h), c (h), ale (h), c (h), c (h), c (h), ale (h)."}, {"heading": "3.2 Local Attention", "text": "Global attention has the disadvantage of having to deal with all the words on the source page that are relevant to each target word, which is expensive and can potentially lead to a translation of more than $10 billion. (It is about whether it is in fact a way that is limited to a small subset of the source code.) (It is about the question of whether it is a way that refers to the entire source text. (It is about a way that refers to a small subset of the source code.) (It is about the global attention that sets the priorities. \") The hard attention that we put on the other side is selecting a patch of the image that we point to such a difference.\") While we are less expensive, the hard attention model is not differentiated and requires more complicated techniques."}, {"heading": "3.3 Input-feeding Approach", "text": "In our proposed global and local approaches, attention decisions are made independently, which is sub-optimal. Whereas standard MT often maintains a coverage set during the translation process to track which source words have been translated, attentive NMT alignment decisions should be made jointly, taking into account past alignment information. To achieve this, we propose an input approach that links attentive vectors to inputs in the next steps, as illustrated in Figure 4.11. The effects of such linkages are twofold: (a) We hope to make the model fully aware of previous alignment decisions, and (b) we create a very deep network that encompasses inputs both horizontally and vertically. Compared to other work - Bahdanau et al. (2015) use context vectors, similar to our ct, to obtain subsequent hidden states that can also achieve the \"coverage effect.\""}, {"heading": "4 Experiments", "text": "We evaluate the effectiveness of our models for WMT translation tasks between English and German. newstest2013 (3000 sentences) is used as a development set to select our hyperparameters. Translation services are reported in case-sensitive BLEU (Papineni et al., 2002) on newstest2014 (2737 sentences) and newstest2015 (2169 sentences). In the following (Luong et al., 2015) we report on translation quality using two types of BLEU: (a) tokenized12 BLEU to be comparable with existing NMT work, and (b) NIST13 BLEU to be comparable with WMT results."}, {"heading": "4.1 Training Details", "text": "All our models are trained on the basis of WMT '14 training data, which consists of 4.5 million sentence pairs (116M English words, 110M German words). Similar to (Jean et al., 2015), we limit our vocabulary to the 50 most common words for both languages. Words that are not included in these shortlisted vocabulary will subsequently be converted into a universal token. In the formation of our NMT systems, pairs of sentences exceeding 50 words are then filtered out (Bahdanau et al., 2015; Jean et al., 2015) and mixed into mini-batches. Our LSTM models have 4 layers each with 1000 cells and 1000-dimensional embedding. We follow (Sutskever et al., 2014; Luong et al., 2015) in training NMT with similar settings: (a) our parameters will be uniform in [\u2212 0.1, 0.1], (b) we train for 10 epochs."}, {"heading": "4.2 English-German Results", "text": "We compare our NMT systems in the English task with various other systems, including the winning system in the WMT '14 (Buck et al., 2014), a sentence-based system whose language models were trained on a giant monolingual text, the Common Crawl Corpus. For the end-to-end NMT systems, to the best of our knowledge, (Jean et al., 2015) is the only work that experiments with this language pair and currently the SOTA system. We present only the results for some of our attention models and will later analyze the rest in the table."}, {"heading": "4.3 German-English Results", "text": "We are conducting a similar series of experiments for the WMT '15 translation task from German to English. Although our systems have not yet been aligned with the performance of the SOTA system, we are nevertheless showing the effectiveness of our approaches with large and progressive increases in BLEU, as shown in Table 3. The attention mechanism gives us + 2.2 BLEU gain and on top of that we get a further boost of up to + 1.0 BLEU through the input feed approach. Through a better alignment function, the content-based dot product one together with the drop-out gives us a further gain of + 2.7 BLEU. Finally, when using the unknown word translation technique, we capture an additional + 2.1 BLEU, demonstrating the benefit of attention when aligning rare words."}, {"heading": "5 Analysis", "text": "We carry out extensive analyses to better understand our models in terms of learning, the ability to process long sentences, the choice of attention architectures and the quality of alignment."}, {"heading": "5.1 Learning curves", "text": "It is pleasant to see in Figure 5 a clear separation between inattentive and attentive models; the input feed approach and the local attention model also demonstrate their ability to reduce testing costs; the inattentive model with dropout (the blue + curve) learns more slowly than other non-dropout models, but becomes more stable over time in terms of minimizing test errors."}, {"heading": "5.2 Effects of Translating Long Sentences", "text": "We follow (Bahdanau et al., 2015) to group sentences of similar length together and calculate a BLEU score per group. Figure 6 shows that our attention models are more effective in dealing with long sentences than the inattentive ones: the quality does not deteriorate as the sentences get longer. Our best model (the blue + curve) outperforms all other systems in all length buckets."}, {"heading": "5.3 Choices of Attentional Architectures", "text": "We study different attention models (global, local-m, local-p) and different alignment functions (location, dot, general, concat) as described in Section 3. Due to limited resources, we cannot perform all possible combinations. However, the results in Table 4 give us an idea of different choices. The location-based function does not learn good alignments: the global (location) model can only make a small profit if it performs unknown word exchanges compared to other alignment functions. 14 For content-based functions, our implementation focus does not perform well and more analysis should be done to understand the reasons.15 It is interesting to observe that point works well for global attention and general is better for local attention. Among the different models, the local predictive alignment model (localp) is best, both in terms of perplexity and BLEU."}, {"heading": "5.4 Alignment Quality", "text": "While (Bahdanau et al., 2015) attention is focused on a learned ER model.14There is a subtle difference in how we retrieve alignments for the various alignment functions. In a time step in which we get yt -1 as input and then compute ht, at, ct and h-t before predicting yt, the alignment vector at is used as alignment weights for (a) the predicted word yt in the location-based alignment functions and (b) the input word yt \u2212 1 in the content-related functions.15With concat, the perplexities achieved by different models are used as appendices 6.7 (global), 7.1 (local) and 7.1 (local-p).Such high perplexities could be due to the fact that we simplify the matrix Wa to determine the part that coincides with h-alignment models."}, {"heading": "5.5 Sample Translations", "text": "It is interesting to observe the effect of attention models in the correct translation of names such as \"Miranda Kerr\" and \"Roger Dow.\" Although, from a linguistic model perspective, non-attention models lack the direct connections to make correct translations, in the second example we also observed an interesting case that requires the translation of the doubly negated phrase \"not incompatible.\" The attention model correctly produces \"not... incompatible\"; while the non-attention-oriented model generates \"non-verein-16We,\" we associate the 508 sentence pairs with 1M sentence pairs from WMT and operate the Berkeley aligner.bar, \"which means\" not compatible. \"17 The attention model also demonstrates its superiority in the translation of long sentences as in the last example."}, {"heading": "6 Conclusion", "text": "In this article, we propose two simple and effective mechanisms of attention to neural machine translation: the global approach, which always considers all starting positions, and the local approach, which only takes into account a subset of starting positions at the same time. We test the effectiveness of our models in WMT translation tasks between English and German in both directions. Our local attention leads to large increases of up to 5.0 BLEU compared to non-attention-relevant positions. The reference uses a more sophisticated translation of \"incompatible,\" which is \"at odds with something.\" However, both models have failed to translate \"passenger experience.\" Models that incorporate already known techniques such as abort translation. For the English-German translation direction, our ensemble model has established new state-of-the-art results for both WMT '14 and WMT' 15, which exceed existing best systems, supported by NEU models and LET-based models, and LET-based on different BLT models."}, {"heading": "Acknowledgment", "text": "We thank Andrew Ng and his group as well as Stanford Research Computing for letting us use their computer resources. We thank Russell Stewart for helpful discussions about the models. Finally, we thank Quoc Le, Ilya Sutskever, Oriol Vinyals, Richard Socher, Michael Kayser, Jiwei Li, Panupong Pasupat, Kelvin Guu, members of the Stanford NLP Group and the renowned critics for their valuable comments and feedback."}, {"heading": "A Alignment Visualization", "text": "The visualization of the local attention model is much sharper than that of the global model. This contrast corresponds to our expectation that local attention is designed to focus each time on only a subset of words. As we translate from English into German and reverse the original English sentence, the white steps in the words \"reality\" and. \"in the global attention model reveal an interesting access pattern: it tends to refer back to the beginning of the source sequence. Compared to the alignment visualizations in (Bahdanau et al., 2015), our alignment patterns are not as sharp as theirs. Such a difference could possibly be due to the fact that translating from English into German is more difficult than translating into French as in (Bahdanau et al., 2015), which is an interesting point to examine in future work."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau et al.2015] D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In ICLR", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "N-gram counts and language models from the common crawl", "author": ["Buck et al.2014] Christian Buck", "Kenneth Heafield", "Bas van Ooyen"], "venue": null, "citeRegEx": "Buck et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Buck et al\\.", "year": 2014}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Measuring word alignment quality for statistical machine translation", "author": ["Fraser", "Marcu2007] Alexander Fraser", "Daniel Marcu"], "venue": "Computational Linguistics,", "citeRegEx": "Fraser et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Fraser et al\\.", "year": 2007}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["Gregor et al.2015] Karol Gregor", "Ivo Danihelka", "Alex Graves", "Danilo Jimenez Rezende", "Daan Wierstra"], "venue": null, "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["Jean et al.2015] S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": null, "citeRegEx": "Jean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] N. Kalchbrenner", "P. Blunsom"], "venue": "In EMNLP", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Statistical phrase-based translation", "author": ["Koehn et al.2003] Philipp Koehn", "Franz Josef Och", "Daniel Marcu"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Alignment by agreement", "author": ["Liang et al.2006] P. Liang", "B. Taskar", "D. Klein"], "venue": null, "citeRegEx": "Liang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2006}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Luong et al.2015] M.-T. Luong", "I. Sutskever", "Q.V. Le", "O. Vinyals", "W. Zaremba"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Recurrent models of visual attention", "author": ["Mnih et al.2014] Volodymyr Mnih", "Nicolas Heess", "Alex Graves", "Koray Kavukcuoglu"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei jing Zhu"], "venue": null, "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Sequence to sequence learning with neural networks", "author": ["O. Vinyals", "Q.V. Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu et al.2015] Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio"], "venue": "In ICML", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Recurrent neural network regularization", "author": ["Ilya Sutskever", "Oriol Vinyals"], "venue": "In ICLR", "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "Neural Machine Translation (NMT) achieved state-of-the-art performances in large-scale translation tasks such as from English to French (Luong et al., 2015) and English to German (Jean et al.", "startOffset": 136, "endOffset": 156}, {"referenceID": 5, "context": ", 2015) and English to German (Jean et al., 2015).", "startOffset": 30, "endOffset": 49}, {"referenceID": 5, "context": ", 2015) and English to German (Jean et al., 2015). NMT is appealing since it requires minimal domain knowledge and is conceptually simple. The model by Luong et al. (2015) reads through all the source words until the end-ofsentence symbol <eos> is reached.", "startOffset": 31, "endOffset": 172}, {"referenceID": 7, "context": "Lastly, implementing NMT decoders is easy unlike the highly intricate decoders in standard MT (Koehn et al., 2003).", "startOffset": 94, "endOffset": 114}, {"referenceID": 10, "context": ", between image objects and agent actions in the dynamic control problem (Mnih et al., 2014), between speech frames and text in the speech recognition task (?), or between visual features of a picture and its text description in the image caption generation task (Xu et al.", "startOffset": 73, "endOffset": 92}, {"referenceID": 13, "context": ", 2014), between speech frames and text in the speech recognition task (?), or between visual features of a picture and its text description in the image caption generation task (Xu et al., 2015).", "startOffset": 178, "endOffset": 195}, {"referenceID": 0, "context": "In the context of NMT, Bahdanau et al. (2015) has successfully applied such attentional mechanism to jointly translate and align words.", "startOffset": 23, "endOffset": 46}, {"referenceID": 0, "context": "The former approach resembles the model of (Bahdanau et al., 2015) but is simpler architecturally.", "startOffset": 43, "endOffset": 66}, {"referenceID": 13, "context": "The latter can be viewed as an interesting blend between the hard and soft attention models proposed in (Xu et al., 2015): it is computationally less expensive than the global model or the soft attention; at the same time, unlike the hard attention, the local attention is differentiable almost everywhere, making it easier to implement and train.", "startOffset": 104, "endOffset": 121}, {"referenceID": 12, "context": "recurrent neural network (RNN) architecture, which most of the recent NMT work such as (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015) have in common.", "startOffset": 87, "endOffset": 223}, {"referenceID": 2, "context": "recurrent neural network (RNN) architecture, which most of the recent NMT work such as (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015) have in common.", "startOffset": 87, "endOffset": 223}, {"referenceID": 0, "context": "recurrent neural network (RNN) architecture, which most of the recent NMT work such as (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015) have in common.", "startOffset": 87, "endOffset": 223}, {"referenceID": 9, "context": "recurrent neural network (RNN) architecture, which most of the recent NMT work such as (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015) have in common.", "startOffset": 87, "endOffset": 223}, {"referenceID": 5, "context": "recurrent neural network (RNN) architecture, which most of the recent NMT work such as (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015) have in common.", "startOffset": 87, "endOffset": 223}, {"referenceID": 2, "context": "There is a recent work by Gregor et al. (2015), which is very similar to our local attention and applied to the image generation task.", "startOffset": 26, "endOffset": 47}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015) have in common. They, however, differ in terms of which RNN architectures are used for the decoder and how the encoder computes the source sentence representation s. Kalchbrenner and Blunsom (2013) used an RNN with the standard hidden unit for the decoder and a convolutional neural network for encoding the source sentence representation.", "startOffset": 8, "endOffset": 268}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015) have in common. They, however, differ in terms of which RNN architectures are used for the decoder and how the encoder computes the source sentence representation s. Kalchbrenner and Blunsom (2013) used an RNN with the standard hidden unit for the decoder and a convolutional neural network for encoding the source sentence representation. On the other hand, both Sutskever et al. (2014) and Luong et al.", "startOffset": 8, "endOffset": 458}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015) have in common. They, however, differ in terms of which RNN architectures are used for the decoder and how the encoder computes the source sentence representation s. Kalchbrenner and Blunsom (2013) used an RNN with the standard hidden unit for the decoder and a convolutional neural network for encoding the source sentence representation. On the other hand, both Sutskever et al. (2014) and Luong et al. (2015) stacked multiple layers of an RNN with a Long Short-Term Memory (LSTM) hidden unit for both the encoder and the decoder.", "startOffset": 8, "endOffset": 482}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015) have in common. They, however, differ in terms of which RNN architectures are used for the decoder and how the encoder computes the source sentence representation s. Kalchbrenner and Blunsom (2013) used an RNN with the standard hidden unit for the decoder and a convolutional neural network for encoding the source sentence representation. On the other hand, both Sutskever et al. (2014) and Luong et al. (2015) stacked multiple layers of an RNN with a Long Short-Term Memory (LSTM) hidden unit for both the encoder and the decoder. Cho et al. (2014), Bahdanau et al.", "startOffset": 8, "endOffset": 621}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015) have in common. They, however, differ in terms of which RNN architectures are used for the decoder and how the encoder computes the source sentence representation s. Kalchbrenner and Blunsom (2013) used an RNN with the standard hidden unit for the decoder and a convolutional neural network for encoding the source sentence representation. On the other hand, both Sutskever et al. (2014) and Luong et al. (2015) stacked multiple layers of an RNN with a Long Short-Term Memory (LSTM) hidden unit for both the encoder and the decoder. Cho et al. (2014), Bahdanau et al. (2015), and Jean et al.", "startOffset": 8, "endOffset": 645}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015) have in common. They, however, differ in terms of which RNN architectures are used for the decoder and how the encoder computes the source sentence representation s. Kalchbrenner and Blunsom (2013) used an RNN with the standard hidden unit for the decoder and a convolutional neural network for encoding the source sentence representation. On the other hand, both Sutskever et al. (2014) and Luong et al. (2015) stacked multiple layers of an RNN with a Long Short-Term Memory (LSTM) hidden unit for both the encoder and the decoder. Cho et al. (2014), Bahdanau et al. (2015), and Jean et al. (2015) all adopted a different version of the RNN with an LSTM-inspired hidden unit, the gated recurrent unit (GRU), for both components.", "startOffset": 8, "endOffset": 669}, {"referenceID": 12, "context": "In (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Luong et al., 2015), the source representation s is only used once to initialize the decoder hidden state.", "startOffset": 3, "endOffset": 97}, {"referenceID": 2, "context": "In (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Luong et al., 2015), the source representation s is only used once to initialize the decoder hidden state.", "startOffset": 3, "endOffset": 97}, {"referenceID": 9, "context": "In (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Luong et al., 2015), the source representation s is only used once to initialize the decoder hidden state.", "startOffset": 3, "endOffset": 97}, {"referenceID": 0, "context": "On the other hand, in (Bahdanau et al., 2015; Jean et al., 2015) and this work, s, in fact, implies a set of source hidden states which are consulted throughout the entire course of the translation process.", "startOffset": 22, "endOffset": 64}, {"referenceID": 5, "context": "On the other hand, in (Bahdanau et al., 2015; Jean et al., 2015) and this work, s, in fact, implies a set of source hidden states which are consulted throughout the entire course of the translation process.", "startOffset": 22, "endOffset": 64}, {"referenceID": 12, "context": "In this work, following (Sutskever et al., 2014; Luong et al., 2015), we use the stacking LSTM architecture for our NMT systems, as illustrated", "startOffset": 24, "endOffset": 68}, {"referenceID": 9, "context": "In this work, following (Sutskever et al., 2014; Luong et al., 2015), we use the stacking LSTM architecture for our NMT systems, as illustrated", "startOffset": 24, "endOffset": 68}, {"referenceID": 0, "context": "One can provide g with other inputs such as the currently predicted word yj as in (Bahdanau et al., 2015).", "startOffset": 82, "endOffset": 105}, {"referenceID": 14, "context": "We use the LSTM unit defined in (Zaremba et al., 2015).", "startOffset": 32, "endOffset": 54}, {"referenceID": 0, "context": "Comparison to (Bahdanau et al., 2015) \u2013 While our global attention approach is similar in spirit to the model proposed by Bahdanau et al.", "startOffset": 14, "endOffset": 37}, {"referenceID": 0, "context": "Comparison to (Bahdanau et al., 2015) \u2013 While our global attention approach is similar in spirit to the model proposed by Bahdanau et al. (2015), there are several key differences which reflect how we have both simplified and generalized from the original model.", "startOffset": 15, "endOffset": 145}, {"referenceID": 0, "context": "Comparison to (Bahdanau et al., 2015) \u2013 While our global attention approach is similar in spirit to the model proposed by Bahdanau et al. (2015), there are several key differences which reflect how we have both simplified and generalized from the original model. First, we simply use hidden states at the top LSTM layers in both the encoder and decoder as illustrated in Figure 2. Bahdanau et al. (2015), on the other hand, use the concatenation of the forward and backward source hidden states in the bi-directional encoder", "startOffset": 15, "endOffset": 404}, {"referenceID": 0, "context": "On the other hand, at any time t, Bahdanau et al. (2015) build from the previous hidden state ht\u22121 \u2192 at \u2192 ct \u2192 ht, which, in turn, goes through a deep-output and a maxout layer before making predictions.", "startOffset": 34, "endOffset": 57}, {"referenceID": 0, "context": "On the other hand, at any time t, Bahdanau et al. (2015) build from the previous hidden state ht\u22121 \u2192 at \u2192 ct \u2192 ht, which, in turn, goes through a deep-output and a maxout layer before making predictions.7 Lastly, Bahdanau et al. (2015) only experimented with one alignment function, the concat product; whereas we show later that the other alternatives are better.", "startOffset": 34, "endOffset": 236}, {"referenceID": 13, "context": "This model takes inspiration from the tradeoff between the soft and hard attentional models proposed by Xu et al. (2015) to tackle the image caption generation task.", "startOffset": 104, "endOffset": 121}, {"referenceID": 4, "context": "Comparison to (Gregor et al., 2015) \u2013 have proposed a selective attention mechanism, very similar to our local attention, for the image generation task.", "startOffset": 14, "endOffset": 35}, {"referenceID": 0, "context": "Comparison to other work \u2013 Bahdanau et al. (2015) use context vectors, similar to our ct, in building subsequent hidden states, which can also achieve the \u201ccoverage\u201d effect.", "startOffset": 27, "endOffset": 50}, {"referenceID": 11, "context": "Translation performances are reported in case-sensitive BLEU (Papineni et al., 2002) on newstest2014 (2737 sentences) and newstest2015 (2169 sentences).", "startOffset": 61, "endOffset": 84}, {"referenceID": 9, "context": "Following (Luong et al., 2015), we report translation quality using two types of BLEU: (a) tokenized12 BLEU to be comparable with existing NMT work and (b) NIST13 BLEU to be comparable with WMT results.", "startOffset": 10, "endOffset": 30}, {"referenceID": 5, "context": "Similar to (Jean et al., 2015), we limit our vocabularies to be the top 50K most frequent words for both languages.", "startOffset": 11, "endOffset": 30}, {"referenceID": 0, "context": "When training our NMT systems, following (Bahdanau et al., 2015; Jean et al., 2015), we filter out sentence pairs whose lengths exceed 50 words and shuffle mini-batches as we proceed.", "startOffset": 41, "endOffset": 83}, {"referenceID": 5, "context": "When training our NMT systems, following (Bahdanau et al., 2015; Jean et al., 2015), we filter out sentence pairs whose lengths exceed 50 words and shuffle mini-batches as we proceed.", "startOffset": 41, "endOffset": 83}, {"referenceID": 12, "context": "We follow (Sutskever et al., 2014; Luong et al., 2015) in training NMT with similar settings: (a) our parameters are uniformly initialized in [\u22120.", "startOffset": 10, "endOffset": 54}, {"referenceID": 9, "context": "We follow (Sutskever et al., 2014; Luong et al., 2015) in training NMT with similar settings: (a) our parameters are uniformly initialized in [\u22120.", "startOffset": 10, "endOffset": 54}, {"referenceID": 1, "context": "Winning WMT\u201914 system \u2013 phrase-based + large LM (Buck et al., 2014) 20.", "startOffset": 48, "endOffset": 67}, {"referenceID": 5, "context": "RNNsearch (Jean et al., 2015) 16.", "startOffset": 10, "endOffset": 29}, {"referenceID": 5, "context": "5 RNNsearch + unk replace (Jean et al., 2015) 19.", "startOffset": 26, "endOffset": 45}, {"referenceID": 5, "context": "0 RNNsearch + unk replace + large vocab + ensemble 8 models (Jean et al., 2015) 21.", "startOffset": 60, "endOffset": 79}, {"referenceID": 14, "context": "2 for our LSTMs as suggested by (Zaremba et al., 2015).", "startOffset": 32, "endOffset": 54}, {"referenceID": 1, "context": "These include the winning system in WMT\u201914 (Buck et al., 2014), a phrase-based system whose language models were trained on a huge monolingual text, the Common Crawl corpus.", "startOffset": 43, "endOffset": 62}, {"referenceID": 5, "context": "For end-to-end NMT systems, to the best of our knowledge, (Jean et al., 2015) is the only work experimenting with this language pair and currently the SOTA system.", "startOffset": 58, "endOffset": 77}, {"referenceID": 12, "context": "3 BLEU, as proposed in (Sutskever et al., 2014) and (b) using dropout, +1.", "startOffset": 23, "endOffset": 47}, {"referenceID": 9, "context": "It is interesting to observe the trend previously reported in (Luong et al., 2015) that perplexity strongly correlates with translation quality.", "startOffset": 62, "endOffset": 82}, {"referenceID": 0, "context": "8 BLEU, making our model slightly better than the base attentional system of Bahdanau et al. (2015) (row RNNSearch).", "startOffset": 77, "endOffset": 100}, {"referenceID": 9, "context": "The unknown replacement technique proposed in (Luong et al., 2015; Jean et al., 2015) yields another nice gain of +1.", "startOffset": 46, "endOffset": 85}, {"referenceID": 5, "context": "The unknown replacement technique proposed in (Luong et al., 2015; Jean et al., 2015) yields another nice gain of +1.", "startOffset": 46, "endOffset": 85}, {"referenceID": 5, "context": "best system (Jean et al., 2015) by +1.", "startOffset": 12, "endOffset": 31}, {"referenceID": 0, "context": "We follow (Bahdanau et al., 2015) to group sentences of similar lengths together and compute a BLEU score per group.", "startOffset": 10, "endOffset": 33}, {"referenceID": 0, "context": "While (Bahdanau et al., 2015) visualized", "startOffset": 6, "endOffset": 29}, {"referenceID": 8, "context": "Nevertheless, as shown in Table 6, we were able to achieve AER scores comparable to the one-to-many alignments obtained by the Berkeley aligner (Liang et al., 2006).", "startOffset": 144, "endOffset": 164}], "year": 2015, "abstractText": "An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout. Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT\u201915 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.1", "creator": "LaTeX with hyperref package"}}}