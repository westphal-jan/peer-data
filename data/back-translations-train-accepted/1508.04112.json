{"id": "1508.04112", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Aug-2015", "title": "Molding CNNs for text: non-linear, non-consecutive convolutions", "abstract": "The success of deep learning often derives from well-chosen operational building blocks. In this work, we revise the temporal convolution operation in CNNs to better adapt it to text processing. Instead of concatenating word representations, we appeal to tensor algebra and use low-rank n-gram tensors to directly exploit interactions between words already at the convolution stage. Moreover, we extend the n-gram convolution to non-consecutive words to recognize patterns with intervening words. Through a combination of low-rank tensors, and pattern weighting, we can efficiently evaluate the resulting convolution operation via dynamic programming. We test the resulting architecture on standard sentiment classification and news categorization tasks. Our model achieves state-of-the-art performance both in terms of accuracy and training speed. For instance, we obtain 51.4% accuracy on the fine-grained sentiment classification task.", "histories": [["v1", "Mon, 17 Aug 2015 19:02:45 GMT  (523kb,D)", "https://arxiv.org/abs/1508.04112v1", null], ["v2", "Tue, 18 Aug 2015 02:52:40 GMT  (524kb,D)", "http://arxiv.org/abs/1508.04112v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["tao lei", "regina barzilay", "tommi s jaakkola"], "accepted": true, "id": "1508.04112"}, "pdf": {"name": "1508.04112.pdf", "metadata": {"source": "CRF", "title": "Molding CNNs for text: non-linear, non-consecutive convolutions", "authors": ["Tao Lei", "Regina Barzilay", "Tommi Jaakkola"], "emails": ["tommi}@csail.mit.edu"], "sections": [{"heading": "1 Introduction", "text": "We argue that this basic building block can be improved in relation to two important intergrams. First, powerful techniques in a range of NLP tasks such as sentiment classification, question-answering and semantic parsing. As methods, they require limited domain knowledge to achieve respectable performance with increasing data and calculations, but they allow simple architectural and operational variations to adjust them to specific applications to achieve peak performance. In fact, their success often depends on specific architectural and operational selection criteria. 1Our code and data are at https: / / github. com / taolei87 / text _ convennetCNNs for text applications make use of temporal conversion operations or filters. Similar to image processing, they are applied at multiple resolutions, interspersed with non-linear and pooling. The conversion operation itself is a linear mapping via \"n-gram vectors.\""}, {"heading": "2 Related Work", "text": "Deep neural networks have recently brought significant advances in various tasks of natural language processing, such as speech modeling (Bengio et al., 2003; Mikolov et al., 2010), sentiment analysis (Socher et al., 2013; Iyyer et al., 2015; Le and Zuidema, 2015), syntactic parsing (Collobert et al., 2008; Socher et al., 2011a; Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014; Devlin et al., 2014; Sutskever et al., 2014). Models used in these tasks exhibit significant architectural differences ranging from recurring neural networks (Mikolov et al., 2010; Kalchbrenner et al., 2013) to recursive models (Pollack, 1990; Ku \ufffd chler et al., Legate al., 1996) and also evolutionary neural networks (Collobert et al.)."}, {"heading": "3 Background", "text": "The (sequential) n-gram vector ending at position j is simply set to all zeros by concatenating the corresponding word vj = [xj \u2212 n + 1; xj \u2212 n + 2; \u00b7 \u00b7; xj] Out-of-index words. The traditional convolution operator is parameterized by the filter matrix m > Rnd \u00b7 h, which can be thought of as n smaller filter matrices applied to each xi in the vj vector. The operator maps each ngram vj in the input sequence to m > vj-Rh, so that the input sequence x is converted into a sequence of characteristic representations, [m > v1, \u00b7 \u00b7, m > vL]."}, {"heading": "4 Model", "text": "In the second half of the year, with the first three months of the second half in the United States, the second half of the year will begin in the second half in the United States. (...) The second half of the year in the second half of the year will begin in the United States. (...) The third half of the year in the second half of the year will begin in the United States. (...) The second half of the year in the United States will begin in the United States. (...) The third half of the year in the second half of the year will begin in the United States. (...) The third half of the year in the United States will begin in the United States. (...) The third half of the year in the United States. (...) The third half of the year in the United States will begin in the United States. (...) The first half of the year in the United States. (...) The second half of the year in the United States. (...) The third half of the year in the United States. \"(...) The third half of the year in the United States. (...)"}, {"heading": "5 Learning the Model", "text": "Following standard practice, our model is trained on a given training set by minimizing the transverse entropy error. For a single training sequence x and the corresponding gold marking y [0, 1] m, the error is defined as loss (x, y) = m \u2211 l = 1 yl log (y, l), where m is the number of possible output labels. The set of model parameters (e.g. P, \u00b7 \u00b7 \u00b7, O in each layer) is updated by means of stochastic gradient centering using the AdaGrad algorithm (Duchi et al., 2011).Initialization We initialize the matrices P, Q, R from a uniform distribution [\u2212 3 / d, \u221a 3 / d] and similar O \u00b2 U [\u2212 \u221a 3 / h, \u221a 3 / h]. In this way, each set of matrices is a unit vector in expectation, and each rank-1 filter disc also has a uniform variance."}, {"heading": "6 Experimental Setup", "text": "The database consists of 11855 parasitic English sentences, which are used both at the level of origin (i.e. sentence) and at the level of expression on which all three terms are used. We use the binary term model, in which the individual terms are compared with each other. The binary version has 6920 / 872 sentences for training, development and testing. For the task of message categorization, we also evaluate our model at the level of binary classification, the variants of this benchmark, which ignore all neutral sentences. The binary version has 6920 / 872 sentences for training, development and testing.3 The datasets contain 10 different message categories in their entirety, including finance, technology and automobiles, etc. We use 79520 documents for training, 9940 for testing."}, {"heading": "7 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Overall Performance", "text": "Table 1 shows the performance of our model and other basic methods based on the Stanford Sentiment Treebank Benchmark. Our full model achieves the highest accuracy in both development and test kits. Specifically, it achieves 51.2% and 88.6% test accuracy for fine-grained and binary tasks, respectively. Our full model is also many times faster than other high-performance models. As shown in Table 2, our model performance is relatively stable - it remains high with a standard deviation of about 0.5% at different initializations and failure rates. Our full model is also many times faster than other high-performance models. For example, the Convolutionary Multi-Channel Model (CNNMC) runs for over 2400 seconds per training period. In contrast, our full model (with 3 function layers) runs for an average of 28 seconds with only root markers and an average of 445 seconds with all labels. Our results also show that the CNN model, in which our function chart is replaced by traditional linear, performs worse than our model."}, {"heading": "7.2 Hyperparameter Analysis", "text": "We first examine the effects of hyperparameters in our model performance. We use the models that work on fine-grained sentiment classification task with only a root -1 -1 -1 -1 -97 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -2 -1 -1 -1 -2 -1 -1 -1 -1 -2 -2 -2 level functions significantly improves the classification of precision in various hyperparameter settings and initializations. Non-consecutive n-gram functions We also analyze the effect of modeling non-consecutive n-table 1 -1 the film is not good, the film is not good the 0.25 film 0.25 is 0.08 is not -1.91 the good hyperparameter settings and the good properties."}, {"heading": "7.3 Example Predictions", "text": "Figure 5 shows examples of input sets and the corresponding predictions of our model in fine-grained sentiment classification. To see how our model captures the mood in various local contexts, we apply the learned Softmax activation to the extracted characteristics at each position without taking the average. That is, for each index i we get the local mood p = Softmax (W > (1) [i] 0z (2) [i] 0z (3) [i])))). We capture the expected sentiment scores between 2 s = \u2212 2 s \u2022 p (s), where a score of 2 means \"very positive,\" 0 \"neutral\" and -2 means \"very negative.\" As shown in the figure, our model successfully learns negation and double negation. The model also identifies positive and negative segments that occur in the sentence."}, {"heading": "8 Conclusion", "text": "We proposed a feature mapping operator for Convolutionary Neural Networks by modelling n-gram interactions based on the tensor product and evaluating all non-consecutive n-gram vectors, with the associated parameters retained as a low tensor, resulting in efficient feature extraction through dynamic programming."}, {"heading": "Acknowledgments", "text": "We thank Kai Sheng Tai, Mohit Iyyer, and Jordan Boyd-Graber for answering questions about their work. We also thank Yoon Kim, the MIT NLP Group, and the reviewers for their comments. We thank the U.S. Army Research Office under funding number W911NF-10-10533 for their support. The work was developed in collaboration with the Arabic Language Technologies (ALT) group at the Qatar Computing Research Institute (QCRI)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "The Journal of Machine Learning Research, 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio."], "venue": "Proceedings", "citeRegEx": "Bergstra et al\\.,? 2010", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 740\u2013750.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston."], "venue": "International Conference on Machine Learning, ICML.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "The Journal of Machine Learning Research, 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard Schwartz", "John Makhoul."], "venue": "52nd Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Devlin et al\\.,? 2014", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "The Journal of Machine Learning Research, 12:2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Modeling interestingness with deep neural networks", "author": ["Jianfeng Gao", "Patrick Pantel", "Michael Gamon", "Xiaodong He", "Li Deng", "Yelong Shen."], "venue": "In", "citeRegEx": "Gao et al\\.,? 2014", "shortCiteRegEx": "Gao et al\\.", "year": 2014}, {"title": "Improving neural networks by preventing coadaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov."], "venue": "arXiv preprint arXiv:1207.0580.", "citeRegEx": "Hinton et al\\.,? 2012", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["Ozan Irsoy", "Claire Cardie."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Irsoy and Cardie.,? 2014", "shortCiteRegEx": "Irsoy and Cardie.", "year": 2014}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daume III."], "venue": "Association for Computational Linguistics.", "citeRegEx": "Iyyer et al\\.,? 2015", "shortCiteRegEx": "Iyyer et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013), pages 1700\u20131709.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "Proceedings of the 52th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "A unified sentence space for categorical distributional-compositional semantics: Theory and experiments", "author": ["Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh", "Stephen Pulman."], "venue": "In Proceedings of COLING: Posters.", "citeRegEx": "Kartsaklis et al\\.,? 2012", "shortCiteRegEx": "Kartsaklis et al\\.", "year": 2012}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014).", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Inductive learning in symbolic domains using structuredriven recurrent neural networks", "author": ["Andreas K\u00fcchler", "Christoph Goller."], "venue": "KI-96: Advances in Artificial Intelligence, pages 183\u2013197.", "citeRegEx": "K\u00fcchler and Goller.,? 1996", "shortCiteRegEx": "K\u00fcchler and Goller.", "year": 1996}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc Le", "Tomas Mikolov."], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1188\u20131196.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Compositional distributional semantics with long short term memory", "author": ["Phong Le", "Willem Zuidema."], "venue": "Proceedings of Joint Conference on Lexical and Computational Semantics (*SEM).", "citeRegEx": "Le and Zuidema.,? 2015", "shortCiteRegEx": "Le and Zuidema.", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner."], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324, November.", "citeRegEx": "LeCun et al\\.,? 1998", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Low-rank tensors for scoring dependency structures", "author": ["Tao Lei", "Yu Xin", "Yuan Zhang", "Regina Barzilay", "Tommi Jaakkola."], "venue": "Proceedings of the", "citeRegEx": "Lei et al\\.,? 2014", "shortCiteRegEx": "Lei et al\\.", "year": 2014}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "CoRR.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Vector-based models of semantic composition", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "ACL, pages 236\u2013244.", "citeRegEx": "Mitchell and Lapata.,? 2008", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2008}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "volume 12.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Recursive distributed representations", "author": ["Jordan B Pollack."], "venue": "Artificial Intelligence, 46:77\u2013105.", "citeRegEx": "Pollack.,? 1990", "shortCiteRegEx": "Pollack.", "year": 1990}, {"title": "Learning semantic representations using convolutional neural networks for web search", "author": ["Yelong Shen", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Gr\u00e9goire Mesnil."], "venue": "Proceedings of the companion publication of the 23rd international conference on World", "citeRegEx": "Shen et al\\.,? 2014", "shortCiteRegEx": "Shen et al\\.", "year": 2014}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff C. Lin", "Andrew Y. Ng", "Christopher D. Manning."], "venue": "Proceedings of the 26th International Conference on Machine Learning (ICML).", "citeRegEx": "Socher et al\\.,? 2011a", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H Huang", "Andrew Y Ng", "Christopher D Manning."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Socher et al\\.,? 2011b", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts."], "venue": "Proceedings of the 2013 Conference on", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."], "venue": "Advances in Neural Information Processing Systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D Manning."], "venue": "Proceedings of the 53th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL \u201910. Association for Com-", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Semantic parsing for single-relation question answering", "author": ["Wen-tau Yih", "Xiaodong He", "Christopher Meek."], "venue": "Proceedings of ACL.", "citeRegEx": "Yih et al\\.,? 2014", "shortCiteRegEx": "Yih et al\\.", "year": 2014}, {"title": "Text understanding from scratch", "author": ["Xiang Zhang", "Yann LeCun."], "venue": "arXiv preprint arXiv:1502.01710.", "citeRegEx": "Zhang and LeCun.,? 2015", "shortCiteRegEx": "Zhang and LeCun.", "year": 2015}], "referenceMentions": [{"referenceID": 23, "context": "Non-linear interactions within a local context have been shown to improve empirical performance in various tasks (Mitchell and Lapata, 2008; Kartsaklis et al., 2012; Socher et al., 2013).", "startOffset": 113, "endOffset": 186}, {"referenceID": 14, "context": "Non-linear interactions within a local context have been shown to improve empirical performance in various tasks (Mitchell and Lapata, 2008; Kartsaklis et al., 2012; Socher et al., 2013).", "startOffset": 113, "endOffset": 186}, {"referenceID": 29, "context": "Non-linear interactions within a local context have been shown to improve empirical performance in various tasks (Mitchell and Lapata, 2008; Kartsaklis et al., 2012; Socher et al., 2013).", "startOffset": 113, "endOffset": 186}, {"referenceID": 29, "context": "This enables us to directly tap into non-linear interactions between adjacent word feature vectors (Socher et al., 2013; Lei et al., 2014).", "startOffset": 99, "endOffset": 138}, {"referenceID": 20, "context": "This enables us to directly tap into non-linear interactions between adjacent word feature vectors (Socher et al., 2013; Lei et al., 2014).", "startOffset": 99, "endOffset": 138}, {"referenceID": 31, "context": "6% on binary classification, outperforming the best published numbers obtained by a deep recursive model (Tai et al., 2015) and a convolutional model (Kim, 2014).", "startOffset": 105, "endOffset": 123}, {"referenceID": 15, "context": ", 2015) and a convolutional model (Kim, 2014).", "startOffset": 34, "endOffset": 45}, {"referenceID": 1, "context": "Deep neural networks have recently brought about significant advancements in various natural language processing tasks, such as language modeling (Bengio et al., 2003; Mikolov et al., 2010), sentiment analysis (Socher et al.", "startOffset": 146, "endOffset": 189}, {"referenceID": 21, "context": "Deep neural networks have recently brought about significant advancements in various natural language processing tasks, such as language modeling (Bengio et al., 2003; Mikolov et al., 2010), sentiment analysis (Socher et al.", "startOffset": 146, "endOffset": 189}, {"referenceID": 29, "context": ", 2010), sentiment analysis (Socher et al., 2013; Iyyer et al., 2015; Le and Zuidema, 2015), syntactic parsing (Collobert and Weston, 2008; Socher et al.", "startOffset": 28, "endOffset": 91}, {"referenceID": 11, "context": ", 2010), sentiment analysis (Socher et al., 2013; Iyyer et al., 2015; Le and Zuidema, 2015), syntactic parsing (Collobert and Weston, 2008; Socher et al.", "startOffset": 28, "endOffset": 91}, {"referenceID": 18, "context": ", 2010), sentiment analysis (Socher et al., 2013; Iyyer et al., 2015; Le and Zuidema, 2015), syntactic parsing (Collobert and Weston, 2008; Socher et al.", "startOffset": 28, "endOffset": 91}, {"referenceID": 4, "context": ", 2015; Le and Zuidema, 2015), syntactic parsing (Collobert and Weston, 2008; Socher et al., 2011a; Chen and Manning, 2014) and machine translation (Bahdanau et al.", "startOffset": 49, "endOffset": 123}, {"referenceID": 27, "context": ", 2015; Le and Zuidema, 2015), syntactic parsing (Collobert and Weston, 2008; Socher et al., 2011a; Chen and Manning, 2014) and machine translation (Bahdanau et al.", "startOffset": 49, "endOffset": 123}, {"referenceID": 3, "context": ", 2015; Le and Zuidema, 2015), syntactic parsing (Collobert and Weston, 2008; Socher et al., 2011a; Chen and Manning, 2014) and machine translation (Bahdanau et al.", "startOffset": 49, "endOffset": 123}, {"referenceID": 0, "context": ", 2011a; Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014; Devlin et al., 2014; Sutskever et al., 2014).", "startOffset": 57, "endOffset": 125}, {"referenceID": 6, "context": ", 2011a; Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014; Devlin et al., 2014; Sutskever et al., 2014).", "startOffset": 57, "endOffset": 125}, {"referenceID": 30, "context": ", 2011a; Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014; Devlin et al., 2014; Sutskever et al., 2014).", "startOffset": 57, "endOffset": 125}, {"referenceID": 21, "context": "Models applied in these tasks exhibit significant architectural differences, ranging from recurrent neural networks (Mikolov et al., 2010; Kalchbrenner and Blunsom, 2013) to recursive models (Pollack, 1990; K\u00fcchler and Goller, 1996), and including convolutional neural nets (Collobert and Weston, 2008; Collobert et al.", "startOffset": 116, "endOffset": 170}, {"referenceID": 12, "context": "Models applied in these tasks exhibit significant architectural differences, ranging from recurrent neural networks (Mikolov et al., 2010; Kalchbrenner and Blunsom, 2013) to recursive models (Pollack, 1990; K\u00fcchler and Goller, 1996), and including convolutional neural nets (Collobert and Weston, 2008; Collobert et al.", "startOffset": 116, "endOffset": 170}, {"referenceID": 25, "context": ", 2010; Kalchbrenner and Blunsom, 2013) to recursive models (Pollack, 1990; K\u00fcchler and Goller, 1996), and including convolutional neural nets (Collobert and Weston, 2008; Collobert et al.", "startOffset": 60, "endOffset": 101}, {"referenceID": 16, "context": ", 2010; Kalchbrenner and Blunsom, 2013) to recursive models (Pollack, 1990; K\u00fcchler and Goller, 1996), and including convolutional neural nets (Collobert and Weston, 2008; Collobert et al.", "startOffset": 60, "endOffset": 101}, {"referenceID": 4, "context": ", 2010; Kalchbrenner and Blunsom, 2013) to recursive models (Pollack, 1990; K\u00fcchler and Goller, 1996), and including convolutional neural nets (Collobert and Weston, 2008; Collobert et al., 2011; Yih et al., 2014; Shen et al., 2014; Kalchbrenner et al., 2014; Zhang and LeCun, 2015).", "startOffset": 143, "endOffset": 282}, {"referenceID": 5, "context": ", 2010; Kalchbrenner and Blunsom, 2013) to recursive models (Pollack, 1990; K\u00fcchler and Goller, 1996), and including convolutional neural nets (Collobert and Weston, 2008; Collobert et al., 2011; Yih et al., 2014; Shen et al., 2014; Kalchbrenner et al., 2014; Zhang and LeCun, 2015).", "startOffset": 143, "endOffset": 282}, {"referenceID": 33, "context": ", 2010; Kalchbrenner and Blunsom, 2013) to recursive models (Pollack, 1990; K\u00fcchler and Goller, 1996), and including convolutional neural nets (Collobert and Weston, 2008; Collobert et al., 2011; Yih et al., 2014; Shen et al., 2014; Kalchbrenner et al., 2014; Zhang and LeCun, 2015).", "startOffset": 143, "endOffset": 282}, {"referenceID": 26, "context": ", 2010; Kalchbrenner and Blunsom, 2013) to recursive models (Pollack, 1990; K\u00fcchler and Goller, 1996), and including convolutional neural nets (Collobert and Weston, 2008; Collobert et al., 2011; Yih et al., 2014; Shen et al., 2014; Kalchbrenner et al., 2014; Zhang and LeCun, 2015).", "startOffset": 143, "endOffset": 282}, {"referenceID": 13, "context": ", 2010; Kalchbrenner and Blunsom, 2013) to recursive models (Pollack, 1990; K\u00fcchler and Goller, 1996), and including convolutional neural nets (Collobert and Weston, 2008; Collobert et al., 2011; Yih et al., 2014; Shen et al., 2014; Kalchbrenner et al., 2014; Zhang and LeCun, 2015).", "startOffset": 143, "endOffset": 282}, {"referenceID": 34, "context": ", 2010; Kalchbrenner and Blunsom, 2013) to recursive models (Pollack, 1990; K\u00fcchler and Goller, 1996), and including convolutional neural nets (Collobert and Weston, 2008; Collobert et al., 2011; Yih et al., 2014; Shen et al., 2014; Kalchbrenner et al., 2014; Zhang and LeCun, 2015).", "startOffset": 143, "endOffset": 282}, {"referenceID": 19, "context": "Since these models have originally been developed for computer vision (LeCun et al., 1998), their application to NLP tasks introduced a number of modifications.", "startOffset": 70, "endOffset": 90}, {"referenceID": 33, "context": "This variant has been successfully applied to semantic parsing (Yih et al., 2014) and information retrieval (Shen et al.", "startOffset": 63, "endOffset": 81}, {"referenceID": 26, "context": ", 2014) and information retrieval (Shen et al., 2014; Gao et al., 2014).", "startOffset": 34, "endOffset": 71}, {"referenceID": 8, "context": ", 2014) and information retrieval (Shen et al., 2014; Gao et al., 2014).", "startOffset": 34, "endOffset": 71}, {"referenceID": 0, "context": ", 2011a; Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014; Devlin et al., 2014; Sutskever et al., 2014). Models applied in these tasks exhibit significant architectural differences, ranging from recurrent neural networks (Mikolov et al., 2010; Kalchbrenner and Blunsom, 2013) to recursive models (Pollack, 1990; K\u00fcchler and Goller, 1996), and including convolutional neural nets (Collobert and Weston, 2008; Collobert et al., 2011; Yih et al., 2014; Shen et al., 2014; Kalchbrenner et al., 2014; Zhang and LeCun, 2015). Our model most closely relates to the latter. Since these models have originally been developed for computer vision (LeCun et al., 1998), their application to NLP tasks introduced a number of modifications. For instance, Collobert et al. (2011) use the max-over-time pooling operation to aggregate the features over the input sequence.", "startOffset": 58, "endOffset": 787}, {"referenceID": 0, "context": ", 2011a; Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014; Devlin et al., 2014; Sutskever et al., 2014). Models applied in these tasks exhibit significant architectural differences, ranging from recurrent neural networks (Mikolov et al., 2010; Kalchbrenner and Blunsom, 2013) to recursive models (Pollack, 1990; K\u00fcchler and Goller, 1996), and including convolutional neural nets (Collobert and Weston, 2008; Collobert et al., 2011; Yih et al., 2014; Shen et al., 2014; Kalchbrenner et al., 2014; Zhang and LeCun, 2015). Our model most closely relates to the latter. Since these models have originally been developed for computer vision (LeCun et al., 1998), their application to NLP tasks introduced a number of modifications. For instance, Collobert et al. (2011) use the max-over-time pooling operation to aggregate the features over the input sequence. This variant has been successfully applied to semantic parsing (Yih et al., 2014) and information retrieval (Shen et al., 2014; Gao et al., 2014). Kalchbrenner et al. (2014) instead propose (dynamic) k-max pooling operation for modeling sentences.", "startOffset": 58, "endOffset": 1052}, {"referenceID": 0, "context": ", 2011a; Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014; Devlin et al., 2014; Sutskever et al., 2014). Models applied in these tasks exhibit significant architectural differences, ranging from recurrent neural networks (Mikolov et al., 2010; Kalchbrenner and Blunsom, 2013) to recursive models (Pollack, 1990; K\u00fcchler and Goller, 1996), and including convolutional neural nets (Collobert and Weston, 2008; Collobert et al., 2011; Yih et al., 2014; Shen et al., 2014; Kalchbrenner et al., 2014; Zhang and LeCun, 2015). Our model most closely relates to the latter. Since these models have originally been developed for computer vision (LeCun et al., 1998), their application to NLP tasks introduced a number of modifications. For instance, Collobert et al. (2011) use the max-over-time pooling operation to aggregate the features over the input sequence. This variant has been successfully applied to semantic parsing (Yih et al., 2014) and information retrieval (Shen et al., 2014; Gao et al., 2014). Kalchbrenner et al. (2014) instead propose (dynamic) k-max pooling operation for modeling sentences. In addition, Kim (2014) combines CNNs of different filter widths and either static or fine-tuned word vectors.", "startOffset": 58, "endOffset": 1150}, {"referenceID": 7, "context": "descent using AdaGrad algorithm (Duchi et al., 2011).", "startOffset": 32, "endOffset": 52}, {"referenceID": 9, "context": "In addition, we randomly dropout (Hinton et al., 2012) units on the output feature representations z(i) at each level.", "startOffset": 33, "endOffset": 54}, {"referenceID": 29, "context": "For sentiment classification, we use the Stanford Sentiment Treebank benchmark (Socher et al., 2013).", "startOffset": 79, "endOffset": 100}, {"referenceID": 17, "context": "The top block lists recursive neural network models, the second block are convolutional network models and the third block contains other baseline methods, including the paragraph-vector model (Le and Mikolov, 2014), the deep averaging network model (Iyyer et al.", "startOffset": 193, "endOffset": 215}, {"referenceID": 11, "context": "The top block lists recursive neural network models, the second block are convolutional network models and the third block contains other baseline methods, including the paragraph-vector model (Le and Mikolov, 2014), the deep averaging network model (Iyyer et al., 2015) and our implementation of neural bag-of-words.", "startOffset": 250, "endOffset": 270}, {"referenceID": 11, "context": "The training time of baseline methods is taken from (Iyyer et al., 2015) or directly from the authors.", "startOffset": 52, "endOffset": 72}, {"referenceID": 28, "context": "network baselines include standard RNN (Socher et al., 2011b), RNTN with a small core tensor in the composition function (Socher et al.", "startOffset": 39, "endOffset": 61}, {"referenceID": 29, "context": ", 2011b), RNTN with a small core tensor in the composition function (Socher et al., 2013), the deep recursive model DRNN (Irsoy and Cardie, 2014) and the most recent recursive model using long-short-term-memory units RLSTM (Tai et al.", "startOffset": 68, "endOffset": 89}, {"referenceID": 10, "context": ", 2013), the deep recursive model DRNN (Irsoy and Cardie, 2014) and the most recent recursive model using long-short-term-memory units RLSTM (Tai et al.", "startOffset": 39, "endOffset": 63}, {"referenceID": 31, "context": ", 2013), the deep recursive model DRNN (Irsoy and Cardie, 2014) and the most recent recursive model using long-short-term-memory units RLSTM (Tai et al., 2015).", "startOffset": 141, "endOffset": 159}, {"referenceID": 13, "context": "Such convolutional baselines include the dynamic CNN with k-max pooling DCNN (Kalchbrenner et al., 2014) and the convolutional model with multi-channel CNN-MC by Kim (2014).", "startOffset": 77, "endOffset": 104}, {"referenceID": 10, "context": ", 2013), the deep recursive model DRNN (Irsoy and Cardie, 2014) and the most recent recursive model using long-short-term-memory units RLSTM (Tai et al., 2015). These recursive models assume the input sentences are represented as parse trees. As a benefit, they can readily utilize annotations at the phrase level. In contrast, convolutional neural networks are trained on sequence-level, taking the original sequence and its label as training input. Such convolutional baselines include the dynamic CNN with k-max pooling DCNN (Kalchbrenner et al., 2014) and the convolutional model with multi-channel CNN-MC by Kim (2014). To leverage the phrase-level annotations in the Stanford Sentiment Treebank, all phrases and the corresponding labels are added as separate instances when training the sequence models.", "startOffset": 40, "endOffset": 624}, {"referenceID": 32, "context": "Word vectors The word vectors are pre-trained on much larger unannotated corpora to achieve better generalization given limited amount of training data (Turian et al., 2010).", "startOffset": 152, "endOffset": 173}, {"referenceID": 24, "context": "In particular, for the English sentiment classification task, we use the publicly available 300-dimensional GloVe word vectors trained on the Common Crawl with 840B tokens (Pennington et al., 2014).", "startOffset": 172, "endOffset": 197}, {"referenceID": 11, "context": "This choice of word vectors follows most recent work, such as DAN (Iyyer et al., 2015) and RLSTM (Tai et al.", "startOffset": 66, "endOffset": 86}, {"referenceID": 31, "context": ", 2015) and RLSTM (Tai et al., 2015).", "startOffset": 18, "endOffset": 36}, {"referenceID": 22, "context": "Therefore, we run word2vec (Mikolov et al., 2013) to train 200-dimensional word vectors on the 1.", "startOffset": 27, "endOffset": 49}, {"referenceID": 2, "context": "Implementation details The source code is implemented in Python using the Theano library (Bergstra et al., 2010), a flexible linear algebra compiler that can optimize userspecified computations (models) with efficient automatic low-level implementations, including (back-propagated) gradient calculation.", "startOffset": 89, "endOffset": 112}], "year": 2015, "abstractText": "The success of deep learning often derives from well-chosen operational building blocks. In this work, we revise the temporal convolution operation in CNNs to better adapt it to text processing. Instead of concatenating word representations, we appeal to tensor algebra and use low-rank n-gram tensors to directly exploit interactions between words already at the convolution stage. Moreover, we extend the n-gram convolution to non-consecutive words to recognize patterns with intervening words. Through a combination of lowrank tensors, and pattern weighting, we can efficiently evaluate the resulting convolution operation via dynamic programming. We test the resulting architecture on standard sentiment classification and news categorization tasks. Our model achieves state-of-the-art performance both in terms of accuracy and training speed. For instance, we obtain 51.2% accuracy on the fine-grained sentiment classification task.1", "creator": "TeX"}}}