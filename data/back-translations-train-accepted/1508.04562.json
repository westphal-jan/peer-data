{"id": "1508.04562", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Aug-2015", "title": "Fast, Flexible Models for Discovering Topic Correlation across Weakly-Related Collections", "abstract": "Weak topic correlation across document collections with different numbers of topics in individual collections presents challenges for existing cross-collection topic models. This paper introduces two probabilistic topic models, Correlated LDA (C-LDA) and Correlated HDP (C-HDP). These address problems that can arise when analyzing large, asymmetric, and potentially weakly-related collections. Topic correlations in weakly-related collections typically lie in the tail of the topic distribution, where they would be overlooked by models unable to fit large numbers of topics. To efficiently model this long tail for large-scale analysis, our models implement a parallel sampling algorithm based on the Metropolis-Hastings and alias methods (Yuan et al., 2015). The models are first evaluated on synthetic data, generated to simulate various collection-level asymmetries. We then present a case study of modeling over 300k documents in collections of sciences and humanities research from JSTOR.", "histories": [["v1", "Wed, 19 Aug 2015 08:30:37 GMT  (748kb,D)", "http://arxiv.org/abs/1508.04562v1", "EMNLP 2015"]], "COMMENTS": "EMNLP 2015", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["jingwei zhang", "aaron gerow", "jaan altosaar", "james evans", "richard jean so"], "accepted": true, "id": "1508.04562"}, "pdf": {"name": "1508.04562.pdf", "metadata": {"source": "CRF", "title": "Fast, Flexible Models for Discovering Topic Correlation across Weakly-Related Collections", "authors": ["Jingwei Zhang", "Aaron Gerow", "Jaan Altosaar", "James Evans", "Richard Jean So"], "emails": ["jz2541@columbia.edu", "gerow@uchicago.edu", "jevans@uchicago.edu", "altosaar@princeton.edu", "richardjeanso@uchicago.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that you are able to play by the rules."}, {"heading": "2 Related Work", "text": "It is a question of whether it is a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a"}, {"heading": "3 The Models", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Correlated LDA", "text": "It is also possible to introduce a tree structure into the model, which uses a binomial distribution to decide whether a word is drawn from common or non-common specific background information. C-LDA, however, extends to ccLDA to make it more robust in terms of thematic asymmetries between collections (Figure 1a). The crucial extension is that by allowing each collection to define a set of non-common themes in addition to common themes, the model removes an assumption imposed by ccLDA and other inter-collection models that collections have the same number of themes. As a result, C-LDA is suitable for collections without a large share of common themes and can also reduce noise (discussed in Section 2). To achieve this, C-LDA assumes that a document d in collection c has a multinomial distribution of documents with an asymmetric distribution of themes."}, {"heading": "3.2 Correlated HDP", "text": "In order to defuse the C-LDA's demand that c should be designed in such a way that Kc = K \u2205, we introduce a variant of the model, the correlated hierarchical Dirichlet process (C-HDP), which uses a hierarchical Dirichlet process on 3 levels (Teh et al., 2006). The generative process for C-HDP is the same as the C-LDA process shown above, with the difference that we assume here that the subject of a word, z, is generated by a hierarchical Dirichlet process: G0 | \u03b3, H \u0445 DP (Gc | \u03b10, G0) Gd | \u03b11, Gc \u0445 DP (\u03b11, Gc) z | Gd \u0445 Gd, where G0 is a basic quantity for each Dirichlet process at the level, and Gc are basic measures for document-level Dirichlet processes (Figure 1b)."}, {"heading": "4 Inference", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Posterior Inference in C-LDA", "text": "Considering the status assignments of other words, the sampling distribution for word wi is indicated by: p (yi, zi, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i"}, {"heading": "4.2 Posterior Inference in C-HDP", "text": "C-HDP uses the block sampling algorithm described in (Chen et al., 2011), which is based on the metaphor of the Chinese restaurant process. Instead of tracking all assignments (as the samplers in (Teh et al., 2006)), table indicators are used to track only the beginning of new tables, allowing us to adopt the same sampling framework as C-LDA. In the Chinese restaurant process, each dirichlet process is presented in the hierarchical structure as a restaurant with an infinite number of tables, each serving the same dish. New customers can either join a table with existing customers or start a new table. When a new table is selected, a proxy customer is sent to the parent restaurant to determine the dish to be served to that table.In the block sampler, indicators are used to denote a customer who is creating a table (or tables) down to the level u (0 as the root for the document layer 1)."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Model Comparison", "text": "In all experiments, the gamma primary result for \u03b1 in C-LDA was set to (1, 1) and (5, 0.1), (5, 0.1), (0.1, 0.1) for \u03b3, \u03b10, \u03b11 and in C-HDP, respectively. In the hold-out procedure, 20% of the documents were randomly selected as test data. LDA, C-LDA and ccLDA were evaluated for 1,000 iterations and C-HDP and the TII variant of TPYP for 1,500 iterations (unless otherwise noted), all converging to a state in which the perplexity change in ten consecutive iterations was less than 1%. Perplexity was calculated from the marginal probability of a highly held document p (w | O, \u03b1)."}, {"heading": "5.1.1 Topic Correlation", "text": "C-LDA is unique in the amount of freedom it allows when the number of topics for col-1https: / / github.com / iceboal / correlated-ldalections. To evaluate the results of the models presented with different topic correlations in a fair environment, we generated two collections of synthetic data by following the generative process (varying the number of topics) and measuring the models \"perplexities\" against the basic truth parameters. In each experiment, two collections were generated, each containing 50 words, via a vocabulary of 3,000. \u03b2 and \u03b4 were each fixed at 0.01 and 1.0, and it was asymmetrically defined as 1 / (i + \u221a Kc) for i common topics. Fully shared topics The assumptions imposed by ccLDA and TPYP effectively make it a special case of our model in which K-1 = K2 were equal to all the results..."}, {"heading": "5.2 Semantic Coherence", "text": "Semantic coherence is a corpus-based measure of the quality of a topic, defined as the average pairwise similarity of the uppermost n words (Newman et al., 2010a; Mimno et al., 2011). A PMI-based form of coherence that has proven to be the best2TPYP is not comparable to this metric, but its hierarchical structure naturally causes the topics to mix. Proxy human assessments of topic quality are defined for a topic k as follows: C (k) = 2 n (n \u2212 1) n (wi, wj) n (wi, wj) n (wi, wj) n (wi, jlog D (wi, wj) + 1D (wj), with D (\u00b7) calculating the occurrence of the document."}, {"heading": "5.2.1 Inference Efficiency", "text": "To compare the efficiency of the model, we evaluated a sample of 5,036 JSTOR documents (introduced in the next section) with a heroout of 20% and set K = K1 = K2 = 200 on a commodity computer with four cores and 16 GB of memory. Figure 4a shows the overtime and iterations of confusion. The inference algorithm leads to a certain durability, which leads to slower convergence in the first 200 iterations, but this effect is outweighed in both C-LDA and C-HDP by the increased sampling speed. With 8 threads, CLDA not only converges faster, but also leads to lower perplexity, which is probably due to threads introducing additional stochasticity."}, {"heading": "5.3 Performance on JSTOR", "text": "To compare our models with slower models, we evaluated 2,465 JSTOR documents and retained 20% as a test set. We adapt a model with 100 frequent and 50 non-frequent initial themes using C-HDP, which produced 272 root themes after 2,000 iterations. Perplexity values are roughly the same when C-LDA uses the same average number of themes per collection (Figure 4b), except when the number of themes is very asymmetrical. Our model begins to exceed ccLDA after 80 themes. C-HDP does not outperform C-LDA despite the original HDP, which may be due to the fact that the hierarchical structure of C-HDP differs significantly from the typical 2-level HDP."}, {"heading": "5.4 Qualitative Analysis", "text": "Our models are designed to compare collections of texts in a way that is scalable and sensitive to asymmetries. To show that C-LDA can fulfill this role, we adapt a model for the entire JSTOR science and humanities with over 9 million articles. We use the journal Science to explore the less popular scientific topics in the humanities, and \u03b2 = 0.01, g = 1.0."}, {"heading": "6 Discussion", "text": "Like ccLDA and TPYP, our models provide a robust way to explore large and potentially poorly related collections of text without imposing assumptions about the data; they account for asymmetries in the number of topics (defined in C-LDA, fit in CHDP); and they provide an efficient sequencing method that allows them to adjust data with large values for K. This can help to find correlations on less common topics; our primary contribution is the ability of our models to account for asymmetries between arbitrary collections. JSTOR, the world's largest digital collection of humanities research, was an ideal application scenario given the size, asymmetry, and comprehensibility of the humanities collection; as we show, humanities and scientific research exhibit asymmetries in terms of vocabulary and subject structure - asymmetries that would be systematically overlooked using existing models."}, {"heading": "Acknowledgements", "text": "This work includes analysis of private or otherwise limited data provided to James Evans and Eamon Duede of ITHAKA (JSTOR), whose opinions are not represented in this work. Jaan Altosaar thanks for the support of the Natural Sciences and Engineering Research Council of Canada. This work was supported by a Templeton Foundation grant to the Metaknowledge Research Network and a National Science Foundation grant 1158803."}], "references": [{"title": "Topic significance ranking of LDA generative models", "author": ["Loulwah AlSumait", "Daniel Barbar\u00e1", "James Gentle", "Carlotta Domeniconi."], "venue": "Machine Learning and Knowledge Discovery in Databases, pages 67\u201382. Springer.", "citeRegEx": "AlSumait et al\\.,? 2009", "shortCiteRegEx": "AlSumait et al\\.", "year": 2009}, {"title": "Latent Dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan."], "venue": "Journal of Machine Learning Research, 3:993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "A Bayesian view of the Poisson-Dirichlet process", "author": ["Wray Buntine", "Marcus Hutter."], "venue": "arXiv preprint arXiv:1007.0296.", "citeRegEx": "Buntine and Hutter.,? 2010", "shortCiteRegEx": "Buntine and Hutter.", "year": 2010}, {"title": "The Physicist and the Philosopher: Einstein, Bergson, and the Debate that Changed our Understanding of Time", "author": ["Jimena Canales."], "venue": "Princeton University Press, Princeton, NJ.", "citeRegEx": "Canales.,? 2015", "shortCiteRegEx": "Canales.", "year": 2015}, {"title": "Sampling table configurations for the hierarchical Poisson-Dirichlet process", "author": ["Changyou Chen", "Lan Du", "Wray Buntine."], "venue": "Machine Learning and Knowledge Discovery in Databases, pages 296\u2013 311. Springer.", "citeRegEx": "Chen et al\\.,? 2011", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "Differential topic models", "author": ["Changyou Chen", "Wray Buntine", "Nan Ding", "Lexing Xie", "Lan Du."], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence.", "citeRegEx": "Chen et al\\.,? 2014", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Modeling email network content and structure", "author": ["M. Denny", "J. ben Aaron", "H. Wallach", "B. Desmarais."], "venue": "the 72nd Annual Midwest Political Science Association Conference, 2014; the Northeast Political Methodology Meeting, 2014; the 7th Annual Po-", "citeRegEx": "Denny et al\\.,? 2014", "shortCiteRegEx": "Denny et al\\.", "year": 2014}, {"title": "Poincar\u2019s Maps", "author": ["Peter Galison."], "venue": "Norton, New York, NY.", "citeRegEx": "Galison.,? 2003", "shortCiteRegEx": "Galison.", "year": 2003}, {"title": "Probabilistic latent semantic indexing", "author": ["Thomas Hofmann."], "venue": "Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 50\u201357.", "citeRegEx": "Hofmann.,? 1999", "shortCiteRegEx": "Hofmann.", "year": 1999}, {"title": "On the alias method for generating random variables from a discrete distribution", "author": ["Richard A Kronmal", "Arthur V Peterson Jr."], "venue": "The American Statistician, 33(4):214\u2013218.", "citeRegEx": "Kronmal and Jr.,? 1979", "shortCiteRegEx": "Kronmal and Jr.", "year": 1979}, {"title": "Profile predictive inference", "author": ["Alp Kucukelbir", "David M Blei."], "venue": "arXiv preprint arXiv:1411.0292.", "citeRegEx": "Kucukelbir and Blei.,? 2014", "shortCiteRegEx": "Kucukelbir and Blei.", "year": 2014}, {"title": "Reducing the sampling complexity of topic models", "author": ["Aaron Q Li", "Amr Ahmed", "Sujith Ravi", "Alexander J Smola."], "venue": "Proceedings of the 20th ACM SIGKDD international conference on knowledge discovery and data mining, pages 891\u2013900.", "citeRegEx": "Li et al\\.,? 2014", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Accelerating topic model training on a single machine", "author": ["Mian Lu", "Ge Bai", "Qiong Luo", "Jie Tang", "Jiuxin Zhao."], "venue": "Web Technologies and Applications, pages 184\u2013195. Springer.", "citeRegEx": "Lu et al\\.,? 2013", "shortCiteRegEx": "Lu et al\\.", "year": 2013}, {"title": "Fast generation of discrete random variables", "author": ["George Marsaglia", "Wai Wan Tsang", "Jingbo Wang."], "venue": "Journal of Statistical Software, 11:1\u20138.", "citeRegEx": "Marsaglia et al\\.,? 2004", "shortCiteRegEx": "Marsaglia et al\\.", "year": 2004}, {"title": "Optimizing semantic coherence in topic models", "author": ["David Mimno", "Hanna M Wallach", "Edmund Talley", "Miriam Leenders", "Andrew McCallum."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 262\u2013", "citeRegEx": "Mimno et al\\.,? 2011", "shortCiteRegEx": "Mimno et al\\.", "year": 2011}, {"title": "Distributed algorithms for topic models", "author": ["David Newman", "Arthur Asuncion", "Padhraic Smyth", "Max Welling."], "venue": "The Journal of Machine Learning Research, 10:1801\u20131828.", "citeRegEx": "Newman et al\\.,? 2009", "shortCiteRegEx": "Newman et al\\.", "year": 2009}, {"title": "Automatic evaluation of topic coherence", "author": ["David Newman", "Jey Han Lau", "Karl Grieser", "Timothy Baldwin."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "Newman et al\\.,? 2010a", "shortCiteRegEx": "Newman et al\\.", "year": 2010}, {"title": "Evaluating topic models for digital libraries", "author": ["David Newman", "Youn Noh", "Edmund Talley", "Sarvnaz Karimi", "Timothy Baldwin."], "venue": "Proceedings of the 10th Annual Joint Conference on Digital Libraries, JCDL \u201910, pages 215\u2013224. ACM.", "citeRegEx": "Newman et al\\.,? 2010b", "shortCiteRegEx": "Newman et al\\.", "year": 2010}, {"title": "Cross-cultural analysis of blogs and forums with mixed-collection topic models", "author": ["Michael Paul", "Roxana Girju."], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3, pages 1408\u20131417.", "citeRegEx": "Paul and Girju.,? 2009", "shortCiteRegEx": "Paul and Girju.", "year": 2009}, {"title": "An architecture for parallel topic models", "author": ["Alexander Smola", "Shravan Narayanamurthy."], "venue": "Proceedings of the VLDB Endowment, 3(1-2):703\u2013710.", "citeRegEx": "Smola and Narayanamurthy.,? 2010", "shortCiteRegEx": "Smola and Narayanamurthy.", "year": 2010}, {"title": "Hierarchical Dirichlet processes", "author": ["Yee Whye Teh", "Michael I Jordan", "Matthew J Beal", "David M Blei."], "venue": "Journal of the american statistical association, 101(476).", "citeRegEx": "Teh et al\\.,? 2006", "shortCiteRegEx": "Teh et al\\.", "year": 2006}, {"title": "Rethinking LDA: Why priors matter", "author": ["Hanna M Wallach", "David Mimno", "Andrew Mccallum."], "venue": "Advances in Neural Information Processing Systems, pages 1973\u20131981.", "citeRegEx": "Wallach et al\\.,? 2009a", "shortCiteRegEx": "Wallach et al\\.", "year": 2009}, {"title": "Evaluation methods for topic models", "author": ["Hanna M. Wallach", "Iain Murray", "Ruslan Salakhutdinov", "David Mimno."], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, ICML \u201909, pages 1105\u20131112, New York, NY,", "citeRegEx": "Wallach et al\\.,? 2009b", "shortCiteRegEx": "Wallach et al\\.", "year": 2009}, {"title": "Structured topic models for language", "author": ["Hanna M Wallach."], "venue": "Unpublished doctoral dissertation, Univ. of Cambridge.", "citeRegEx": "Wallach.,? 2008", "shortCiteRegEx": "Wallach.", "year": 2008}, {"title": "Topic modeling on historical newspapers", "author": ["Tze-I Yang", "Andrew J Torget", "Rada Mihalcea."], "venue": "Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 96\u2013104.", "citeRegEx": "Yang et al\\.,? 2011", "shortCiteRegEx": "Yang et al\\.", "year": 2011}, {"title": "Efficient methods for topic model inference on streaming document collections", "author": ["Limin Yao", "David Mimno", "Andrew McCallum."], "venue": "Proceedings of the 15th ACM SIGKDD international conference on knowledge discovery and data mining,", "citeRegEx": "Yao et al\\.,? 2009", "shortCiteRegEx": "Yao et al\\.", "year": 2009}, {"title": "Lightlda: Big topic models on modest computer clusters", "author": ["Jinhui Yuan", "Fei Gao", "Qirong Ho", "Wei Dai", "Jinliang Wei", "Xun Zheng", "Eric Po Xing", "Tie-Yan Liu", "Wei-Ying Ma."], "venue": "Proceedings of the 24th International Conference on World Wide Web,", "citeRegEx": "Yuan et al\\.,? 2015", "shortCiteRegEx": "Yuan et al\\.", "year": 2015}, {"title": "A cross-collection mixture model for comparative text mining", "author": ["ChengXiang Zhai", "Atulya Velivelli", "Bei Yu."], "venue": "Proceedings of the tenth ACM SIGKDD international conference on knowledge discovery and data mining, pages 743\u2013748.", "citeRegEx": "Zhai et al\\.,? 2004", "shortCiteRegEx": "Zhai et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 26, "context": "To efficiently model this long tail for large-scale analysis, our models implement a parallel sampling algorithm based on the Metropolis-Hastings and alias methods (Yuan et al., 2015).", "startOffset": 164, "endOffset": 183}, {"referenceID": 8, "context": "(2004), who developed the ccMix model which extended pLSA (Hofmann, 1999).", "startOffset": 58, "endOffset": 73}, {"referenceID": 1, "context": "Later work by Paul and Girju (2009) developed ccLDA, which adopted the hierarchical Bayes framework of Latent Dirichlet Allocation or LDA (Blei et al., 2003).", "startOffset": 138, "endOffset": 157}, {"referenceID": 24, "context": "Using topic models for comparative text mining was introduced by Zhai et al. (2004), who developed the ccMix model which extended pLSA (Hofmann, 1999).", "startOffset": 65, "endOffset": 84}, {"referenceID": 7, "context": "(2004), who developed the ccMix model which extended pLSA (Hofmann, 1999). Later work by Paul and Girju (2009) developed ccLDA, which adopted the hierarchical Bayes framework of Latent Dirichlet Allocation or LDA (Blei et al.", "startOffset": 59, "endOffset": 111}, {"referenceID": 18, "context": "In situations where collections do not share all topics, the results often include junk, mixed, or sparse topics, making them difficult to interpret (Paul and Girju, 2009).", "startOffset": 149, "endOffset": 171}, {"referenceID": 18, "context": "C-LDA and C-HDP extend ccLDA (Paul and Girju, 2009) to accommodate collection-topic level asymmetries, particularly by allowing noncommon topics to appear in each collection.", "startOffset": 29, "endOffset": 51}, {"referenceID": 5, "context": "To demonstrate the effectiveness of our models, we evaluate them on synthetic data and show that they outperform related models such as ccLDA and differential topic models (Chen et al., 2014).", "startOffset": 172, "endOffset": 191}, {"referenceID": 26, "context": "To make this feasible on large datasets such as JSTOR, we employ a parallelized Metropolis-Hastings (Kronmal and Peterson Jr, 1979) and alias-table sampling framework, adapted from LightLDA (Yuan et al., 2015).", "startOffset": 190, "endOffset": 209}, {"referenceID": 6, "context": "While topic models could be fit to separate collections to make post-hoc comparisons (Denny et al., 2014; Yang et al., 2011), our goal is to account for both document-topic asymmetry and topic-word asymmetry \u201cin-model\u201d.", "startOffset": 85, "endOffset": 124}, {"referenceID": 24, "context": "While topic models could be fit to separate collections to make post-hoc comparisons (Denny et al., 2014; Yang et al., 2011), our goal is to account for both document-topic asymmetry and topic-word asymmetry \u201cin-model\u201d.", "startOffset": 85, "endOffset": 124}, {"referenceID": 20, "context": "Prioritizing in-model solutions for document-topic asymmetry has been explored elsewhere, such as in hierarchical Dirichlet processes (HDP), which use an additional level to account for collection variations in document-topic distributions (Teh et al., 2006).", "startOffset": 240, "endOffset": 258}, {"referenceID": 27, "context": "One method designed to model topic-word asymmetry is ccMix (Zhai et al., 2004), which models the generative probability of a word in topic z from collection c as a mixture of shared and collection-specific distributions \u03b8z:", "startOffset": 59, "endOffset": 78}, {"referenceID": 18, "context": "ccLDA extends ccMix to the LDA framework and adds a beta prior over \u03bbc that reduces sensitivity to input parameters (Paul and Girju, 2009).", "startOffset": 116, "endOffset": 138}, {"referenceID": 5, "context": "Another approach, differential topic models (Chen et al., 2014), is based on hierarchical Bayesian models over topic-word distributions.", "startOffset": 44, "endOffset": 63}, {"referenceID": 18, "context": "As (Paul and Girju, 2009) note, ccLDA cannot accommodate a topic if it is not common across collections \u2014 an assumption made by ccMix, ccLDA and the TPYP.", "startOffset": 3, "endOffset": 25}, {"referenceID": 17, "context": "In a situation where a topic is found in only one collection, it would either dominate the shared topic portion (resulting in a noisy, collection-specific portion), or it would appear as a mixed topic, revealing two sets of unrelated words (Newman et al., 2010b).", "startOffset": 240, "endOffset": 262}, {"referenceID": 25, "context": "Due to increased demand for scalable topic model implementations, there has been a proliferation of optimized methods for efficient inference, such as SparseLDA (Yao et al., 2009) and AliasLDA (Li et al.", "startOffset": 161, "endOffset": 179}, {"referenceID": 11, "context": ", 2009) and AliasLDA (Li et al., 2014).", "startOffset": 21, "endOffset": 38}, {"referenceID": 11, "context": ", 2009) and AliasLDA (Li et al., 2014). AliasLDA achieves O(Kd) complexity by using the MetropolisHastings-Walker algorithm and an alias table to sample topic-word distributions in O(1) time. Although this strategy introduces temporal staleness in the updates of sufficient statistics, the lag is overcome by more iterations, and converges significantly faster. A similar technique by Yuan et al. (2015), LightLDA, employs cycle-based Metropolis Hastings mixing with alias tables for both document-topic and topic-word distributions.", "startOffset": 22, "endOffset": 404}, {"referenceID": 21, "context": "However, we prefer the simpler, non-tree version because background topics are unnecessary when using an asymmetric \u03b1 prior (Wallach et al., 2009a).", "startOffset": 124, "endOffset": 147}, {"referenceID": 20, "context": "To alleviate C-LDA\u2019s requirement that \u2203 c such that Kc = K\u2205, we introduce a variant of the model, the correlated hierarchical Dirichlet process (C-HDP), that uses a 3-level hierarchical Dirichlet process (Teh et al., 2006).", "startOffset": 204, "endOffset": 222}, {"referenceID": 19, "context": "We use the parallel schema in (Smola and Narayanamurthy, 2010; Lu et al., 2013) which applies atomic updates to the sufficient statistics to avoid race conditions.", "startOffset": 30, "endOffset": 79}, {"referenceID": 12, "context": "We use the parallel schema in (Smola and Narayanamurthy, 2010; Lu et al., 2013) which applies atomic updates to the sufficient statistics to avoid race conditions.", "startOffset": 30, "endOffset": 79}, {"referenceID": 26, "context": "The key idea behind the optimized sampler is the combination of alias tables and the MetropolisHastings method (MH), adapted from (Yuan et al., 2015; Li et al., 2014).", "startOffset": 130, "endOffset": 166}, {"referenceID": 11, "context": "The key idea behind the optimized sampler is the combination of alias tables and the MetropolisHastings method (MH), adapted from (Yuan et al., 2015; Li et al., 2014).", "startOffset": 130, "endOffset": 166}, {"referenceID": 26, "context": "It is therefore reasonable to use qd and qw as cycle proposals (Yuan et al., 2015), alternating them in each Metropolis-Hastings step.", "startOffset": 63, "endOffset": 82}, {"referenceID": 21, "context": "Lastly, the use of an asymmetric \u03b1 allows CLDA to discover correlations between less dominant topics across collections (Wallach et al., 2009a).", "startOffset": 120, "endOffset": 143}, {"referenceID": 23, "context": "We use Minka\u2019s fixed-point method, with a gamma hyper-prior to optimize \u03b1c for each collection separately (Wallach, 2008).", "startOffset": 106, "endOffset": 121}, {"referenceID": 4, "context": "C-HDP uses the block sampling algorithm described in (Chen et al., 2011), which is based on the Chinese restaurant process metaphor.", "startOffset": 53, "endOffset": 72}, {"referenceID": 20, "context": "Here, rather than tracking all assignments (as the samplers given in (Teh et al., 2006)), table indicators are used to track only the start of new tables, which allows us to adopt the same sampling framework as C-LDA.", "startOffset": 69, "endOffset": 87}, {"referenceID": 2, "context": "Here, Sn t is the Stirling number, the ratios of which can be efficiently precomputed (Buntine and Hutter, 2010).", "startOffset": 86, "endOffset": 112}, {"referenceID": 20, "context": "The concentration parameters \u03b3, \u03b10, and \u03b11 can be sampled using the auxiliary variable method (Teh et al., 2006).", "startOffset": 94, "endOffset": 112}, {"referenceID": 4, "context": "than O(1), and 2) while the document alias table samples z and u simultaneously, after sampling z from the word alias table u must be sampled using tlc/nlz (Chen et al., 2011).", "startOffset": 156, "endOffset": 175}, {"referenceID": 15, "context": "Parallelizing C-HDP requires an additional empirical method of merging new topics between threads (Newman et al., 2009), which is outside of the scope of this work.", "startOffset": 98, "endOffset": 119}, {"referenceID": 22, "context": "Perplexity was calculated from the marginal likelihood of a held-out document p(w|\u03a6, \u03b1), estimated using the \u201cleft-to-right\u201d method (Wallach et al., 2009b).", "startOffset": 132, "endOffset": 155}, {"referenceID": 0, "context": "Because it is difficult to validate real-world data that exhibits different kinds of asymmetry, we use synthetic data generated specifically for our evaluation tasks (AlSumait et al., 2009; Wallach et al., 2009b; Kucukelbir and Blei, 2014).", "startOffset": 166, "endOffset": 239}, {"referenceID": 22, "context": "Because it is difficult to validate real-world data that exhibits different kinds of asymmetry, we use synthetic data generated specifically for our evaluation tasks (AlSumait et al., 2009; Wallach et al., 2009b; Kucukelbir and Blei, 2014).", "startOffset": 166, "endOffset": 239}, {"referenceID": 10, "context": "Because it is difficult to validate real-world data that exhibits different kinds of asymmetry, we use synthetic data generated specifically for our evaluation tasks (AlSumait et al., 2009; Wallach et al., 2009b; Kucukelbir and Blei, 2014).", "startOffset": 166, "endOffset": 239}, {"referenceID": 16, "context": "Semantic coherence is a corpus-based metric of the quality of a topic, defined as the average pairwise similarity of the top n words (Newman et al., 2010a; Mimno et al., 2011).", "startOffset": 133, "endOffset": 175}, {"referenceID": 14, "context": "Semantic coherence is a corpus-based metric of the quality of a topic, defined as the average pairwise similarity of the top n words (Newman et al., 2010a; Mimno et al., 2011).", "startOffset": 133, "endOffset": 175}, {"referenceID": 7, "context": "These results provide evidence about how ideas move between the sciences and humanities \u2014 a phenomenon that constitutes a growing area of research for historians (Galison, 2003; Canales, 2015).", "startOffset": 162, "endOffset": 192}, {"referenceID": 3, "context": "These results provide evidence about how ideas move between the sciences and humanities \u2014 a phenomenon that constitutes a growing area of research for historians (Galison, 2003; Canales, 2015).", "startOffset": 162, "endOffset": 192}], "year": 2015, "abstractText": "Weak topic correlation across document collections with different numbers of topics in individual collections presents challenges for existing cross-collection topic models. This paper introduces two probabilistic topic models, Correlated LDA (C-LDA) and Correlated HDP (CHDP). These address problems that can arise when analyzing large, asymmetric, and potentially weakly-related collections. Topic correlations in weakly-related collections typically lie in the tail of the topic distribution, where they would be overlooked by models unable to fit large numbers of topics. To efficiently model this long tail for large-scale analysis, our models implement a parallel sampling algorithm based on the Metropolis-Hastings and alias methods (Yuan et al., 2015). The models are first evaluated on synthetic data, generated to simulate various collection-level asymmetries. We then present a case study of modeling over 300k documents in collections of sciences and humanities research from JSTOR.", "creator": "LaTeX with hyperref package"}}}