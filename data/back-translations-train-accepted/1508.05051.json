{"id": "1508.05051", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Aug-2015", "title": "Auto-Sizing Neural Networks: With Applications to n-gram Language Models", "abstract": "Neural networks have been shown to improve performance across a range of natural-language tasks. However, designing and training them can be complicated. Frequently, researchers resort to repeated experimentation to pick optimal settings. In this paper, we address the issue of choosing the correct number of units in hidden layers. We introduce a method for automatically adjusting network size by pruning out hidden units through $\\ell_{\\infty,1}$ and $\\ell_{2,1}$ regularization. We apply this method to language modeling and demonstrate its ability to correctly choose the number of hidden units while maintaining perplexity. We also include these models in a machine translation decoder and show that these smaller neural models maintain the significant improvements of their unpruned versions.", "histories": [["v1", "Thu, 20 Aug 2015 17:21:50 GMT  (475kb,D)", "http://arxiv.org/abs/1508.05051v1", "EMNLP 2015"]], "COMMENTS": "EMNLP 2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kenton murray", "david chiang"], "accepted": true, "id": "1508.05051"}, "pdf": {"name": "1508.05051.pdf", "metadata": {"source": "META", "title": "Auto-Sizing Neural Networks: With Applications to n-gram Language Models", "authors": ["Kenton Murray", "David Chiang"], "emails": ["kmurray4@nd.edu", "dchiang@nd.edu"], "sections": [{"heading": "1 Introduction", "text": "Neural networks have proven to be highly effective in many tasks in natural language. For example, neural language models and common language / translation models significantly improve the quality of machine translation (Vaswani et al., 2013; Devlin et al., 2014). However, neural networks can be complicated to design and well trained. Many decisions need to be made, and performance can depend heavily on getting them right. However, the optimal settings are not obvious and can be difficult to find, which often requires extensive network searching, which requires numerous experiments. In this essay, we focus on selecting the size of hidden layers. We introduce a method of automatic cutting of hidden layers by adding a thrift-inducing regulator that encourages units when they are not needed, so that they can be removed from the network. Thus, after training with more units than necessary, a network is produced that correctly cuts hidden layers, saving both time and memory when actually used."}, {"heading": "2 Background", "text": "In machine translation, for example, the language model helps to output fluent translations, and in speech recognition, the language model helps to determine possible utterances clearly. Current language models are typically n-gram models that look at the previous (n \u2212 1) words to predict the n \u2212 th word in a sequence, based on (smoothed) numbers of n \u2212 grams collected from training data. These models are simple but very effective in improving the performance of natural language systems. However, n \u2212 gram models are subject to some limitations, such as data sparseness and memory usage. As an alternative, researchers have begun to investigate the use of neural networks for speech modeling. For modeling n \u2212 grams, the most common approach is the forward-facing network of Bengio et ar X iv: 150 8.05 051v 1 [cs.C L] 20 Aug 201 5al."}, {"heading": "3 Methods", "text": "Our method focuses on the challenge of minimizing the number of units in the hidden layers of a forward-facing neural network (R = R). The networks used for different tasks require different numbers of units, and the layers in a single network also require different numbers of units. Therefore, selecting fewer units can affect the performance of the network, and selecting too many units can lead to over-adjustment. It can also slow down network calculations, which can be a major concern for many applications, such as integrating neural language models into a machine translation decoder. Our method starts with a large number of units in each layer and then trains the network together, truncating individual units if possible. The goal is to end with a trained network that also has the optimal number of units in each layer. We do this by adding a regulator to the objective function, looking at a single layer without default, y = f (Wx) Let's leave the probability of L (negative)."}, {"heading": "4 Optimization", "text": "However, this also means that sparsity-inducing regulators cannot be differentiated at zero, making the application of gradient-based optimization methods more difficult. We have discussed the methods we use extensively elsewhere (Duchi et al., 2008; Duchi and Singer, 2009); these methods are briefly described in this section for the sake of completeness."}, {"heading": "4.1 Proximal gradient method", "text": "Most of the work on learning with regulators, including this work, can be thought of as instances of the proximal gradient method (Parikh and Boyd, 2014).Our objective function can be divided into two parts, a convex and differentiable part (L) and a non-differentiable part (\u03bbR).We calculate new parameter values w with: v (u) w (3) w (arg max w) w (2) l (4) and repeat the parameter values from the previous iteration.We calculate new parameter values w with: v (u) w (3) w (arg max w) w \u2212 v (2) l (4) and repeat until convergence. The first update is just a standard gradient update to L; the second is known as the proximal operator for qR and in many cases has a closed form solution. In the rest of this section we provide some justifications for this solution, we show the 4.2 and 4.3 sections of this solution and the 4.3."}, {"heading": "4.2 \ufffd2 and \ufffd2,1 regularization", "text": "Since the \ufffd 2,1 standard for matrices (1) can be broken down into the \ufffd 2 standard for each row, we can treat each row separately, so let us assume that we have a single line and G (w) = 12\u03b7 \ufffd w \u2212 v \u2212 2 + \u03bb \ufffd w \ufffd + Constant.The minimum is either w = 0 (the tip of the cone) or where the partial derivatives are zero (Figure 3): \u2202 G \u2202 w = 1 \u03b7 (w \u2212 v) + \u03bb w w \ufffd = 0.Sure, w and vmust have the same direction and differ only in size, i.e., w = \u03b1 v \ufffd v \ufffd.If you replace this with the above equation, you get the solution \u03b1 = \ufffd v \ufffd."}, {"heading": "4.3 \ufffd\u221e and \ufffd\u221e,1 regularization", "text": "As shown above, the 1 norm on matrices (2) is divisible into the hi-temporal approach of each row, we can handle each row individually; therefore, we want to minimize the total decrease (w) = 12\u03b7 \"w.\" See Figure 4. If we sort the | xj | + in non-increasing order, the solution can be characterized as follows: for all maximum | xj | until the total decrease reaches a decrease or all xj are zero. See Figure 4. If we sort the | xj | in non-increasing order, it is easy to see how it can be calculated: for 1,... n, see if there is a value that decreases all x1,.. The xp-temporal decrease amounts to a total decrease of the elements. The largest decrease for which this is possible results in the correct solution. But this situation seems similar to another optimization problem, the projection onto the 1-ball, the Duchi et al."}, {"heading": "5 Experiments", "text": "We evaluate our model using the open source NPLM toolkit published by Vaswani et al. (2013) and expand it to use the additional regulators described in this paper.2 We use a 100k vocabulary size and 50-dimensional word embedding. We use two hidden layers of rectified linear units (Nair and Hinton, 2010).2These extensions have contributed to the NPLM project.We train neural language models (LMs) on two natural speech corpora, Europarl v7 English and the AFP part of the English Gigaword 5. After tokenization, Europarl has 56M tokens and Gigaword AFP has 870M tokens. For both companies, we provide a validation set of 5,000 tokens. We train each model for 10 iterations using the training data. Our experiments are divided into three parts. First, we consider the impact of our pruning method on the perplexity of a two-tier validation system, and second, we examine the impact of the validation system."}, {"heading": "5.1 Evaluating perplexity and network size", "text": "The most important results are presented in Table 1. For \u03bb \u2264 0.01, the regulator seems to have little influence: no hidden units are cropped, and the helplessness is not affected; for \u03bb = 1, on the other hand, most hidden units are cropped - apparently too many, because the helplessness is worse; but for \u03bb = 0.1, we see that we are able to crop many hidden units: up to half the first layer, without much impact on helplessness. We have found that this is consistent in all our experiments, with different n-gram size, initial hidden layer size, and vocabulary size. Table 2 shows the same information for 5-gram models trained on the larger Gigaword AFP corpus. These numbers look very similar to those on Europarl: again \u03bb = 0.1 is considered best, and, contrary to expectations, even the final number of units is similar. Table 3 shows the result that this corpus varies the most, although 1 does not depend on size."}, {"heading": "5.2 A closer look at training", "text": "The first question we want to answer is whether the method simply removes units or converges to an optimal number of units. Figure 5 suggests that it is a bit of both: if we start with too many units (900 or 1000), the method converges to the same number, regardless of how many additional units there were originally. But, if we start with a smaller number of units, the method still cuts out about 50 units. Next, we look at the behavior of different regulatory strengths. We found that not only \u03bb = 1 cuts out too many units, but does so already at the first iteration (Figure 6, above), perhaps prematurely. In contrast, the formula \u03bb = 0.1 units gradually cuts out. By plotting these curves together with the confusion (Figure 6, below), we can see that the run of the network corresponds to the model and truncates it at the same time, which then appears inappropriate for the first fit (1 = 0.0x) and for the first fit (1 = 0.x)."}, {"heading": "5.3 Evaluating on machine translation", "text": "We also investigated the impact of our method on statistical machine translation systems. We used the Moses toolkit (Koehn et al., 2007) to build a phrase-based machine translation system with a traditional 5 gram LM that was trained on the target page of our request. We expanded this system to include neural LMs trained on the Europarl data and the Gigaword AFP data. Based on the results of the confusion experiments, we looked at models that were both built with a \u03bb = 0.1 regularizer and without regularization (\u03bb = 0). We built our system with the message comment data set v8. We optimized our model with Neuest13 and rated it with Neuestestestestest14. After standard purification and tokenization, there were 155k parallel sentences in the message comment data set and 3,000 sentences each for the tuning and test sentences EU. Table 5 shows that complementing a neural line to BL2 helps significantly."}, {"heading": "6 Related Work", "text": "Researchers have been exploring the use of neural networks for speech modeling for a long time. Schmidhuber and Heil (1996) proposed a characteristic n-gram model that they used for text compression. Xu and Rudnicky (2000) proposed a word-based probability model that was developed with the help of cross-entropy, but only for the use of noise contrasts. (2003) They defined a probabilistic word n-gram model and showed improvements over conventional smooth speech models. Mnih and Teh favored the formation of log bilinear language models through the use of noise contrasts (NCE et al)."}, {"heading": "7 Conclusion", "text": "We have presented a method for automatically resizing a neural network during training by removing units with the help of a regulator that reduces the input weights of a unit as a group to zero, which allows the unit to be pruned. Thus, during training, we can prune units from our network with minimal impact to withstand perplexity or downstream performance of a machine translation system. Our results showed empirically that choosing a regulation coefficient of 0.1 was robust to initial configuration parameters such as initial network size, vocabulary size, n-gram order, and training corpus. In addition, all the hidden layers of a network can be matched to one setting by a single regulator, reducing the need to perform costly, multi-dimensional network searches to determine optimal sizes. We have the power and effectiveness of this method on a pre-objective neural network for modeling the language, even though it should be dismantled quickly enough to perform both general and multi-dimensional network experiments in other networks."}, {"heading": "Acknowledgments", "text": "We would like to thank Tomer Levinboim, Antonios Anastasopoulos and Ashish Vaswani for their helpful conversations and the reviewers for their support and feedback."}], "references": [{"title": "Optimization with sparsity-inducing penalties", "author": ["Francis Bach", "Rodolphe Jenatton", "Julien Mairal", "Guillaume Obozinski."], "venue": "Foundations and Trends in Machine Learning, 4(1):1\u2013106.", "citeRegEx": "Bach et al\\.,? 2012", "shortCiteRegEx": "Bach et al\\.", "year": 2012}, {"title": "OxLM: A neural language modelling framework for machine translation", "author": ["Paul Baltescu", "Phil Blunsom", "Hieu Hoang."], "venue": "Prague Bulletin of Mathematical Linguistics, 102(1):81\u201392.", "citeRegEx": "Baltescu et al\\.,? 2014", "shortCiteRegEx": "Baltescu et al\\.", "year": 2014}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "J. Machine Learning Research, 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard Schwartz", "John Makhoul."], "venue": "Proc. ACL, pages 1370\u20131380.", "citeRegEx": "Devlin et al\\.,? 2014", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Efficient online and batch learning using forward backward splitting", "author": ["John Duchi", "Yoram Singer."], "venue": "J. Machine Learning Research, 10:2899\u20132934.", "citeRegEx": "Duchi and Singer.,? 2009", "shortCiteRegEx": "Duchi and Singer.", "year": 2009}, {"title": "Efficient projections onto the \ufffd1-ball for learning in high dimensions", "author": ["John Duchi", "Shai Shalev-Shwartz", "Yoram Singer", "Tushar Chandra."], "venue": "Proc. ICML, pages 272\u2013279.", "citeRegEx": "Duchi et al\\.,? 2008", "shortCiteRegEx": "Duchi et al\\.", "year": 2008}, {"title": "Optimal brain damage", "author": ["Yann LeCun", "John S. Denker", "Sara A. Solla", "Richard E. Howard", "Lawrence D. Jackel."], "venue": "Proc. NIPS, volume 2, pages 598\u2013 605.", "citeRegEx": "LeCun et al\\.,? 1989", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "RNNLM recurrent neural network language modeling toolkit", "author": ["Tomas Mikolov", "Stefan Kombrink", "Anoop Deoras", "Lukar Burget", "Jan Cernocky."], "venue": "Proc. ASRU, pages 196\u2013201.", "citeRegEx": "Mikolov et al\\.,? 2011", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["Andriy Mnih", "Yee Whye Teh."], "venue": "Proc. ICML, pages 1751\u20131758.", "citeRegEx": "Mnih and Teh.,? 2012", "shortCiteRegEx": "Mnih and Teh.", "year": 2012}, {"title": "Rectified linear units improve Restricted Boltzmann Machines", "author": ["Vinod Nair", "Geoffrey E Hinton."], "venue": "Proc. ICML, pages 807\u2013814.", "citeRegEx": "Nair and Hinton.,? 2010", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "Simplifying neural networks by soft weight-sharing", "author": ["Steven J. Nowland", "Geoffrey E. Hinton."], "venue": "Neural Computation, 4:473\u2013493.", "citeRegEx": "Nowland and Hinton.,? 1992", "shortCiteRegEx": "Nowland and Hinton.", "year": 1992}, {"title": "Proximal algorithms", "author": ["Neal Parikh", "Stephen Boyd."], "venue": "Foundations and Trends in Optimization, 1(3):127\u2013239.", "citeRegEx": "Parikh and Boyd.,? 2014", "shortCiteRegEx": "Parikh and Boyd.", "year": 2014}, {"title": "An efficient projection for l1,\u221e regularization", "author": ["Ariadna Quattoni", "Xavier Carreras", "Michael Collins", "Trevor Darrell."], "venue": "Proc. ICML, pages 857\u2013 864.", "citeRegEx": "Quattoni et al\\.,? 2009", "shortCiteRegEx": "Quattoni et al\\.", "year": 2009}, {"title": "Sequential neural text compression", "author": ["Jurgen Schmidhuber", "Stefan Heil."], "venue": "IEEE Transactions on Neural Networks, 7:142\u2013146.", "citeRegEx": "Schmidhuber and Heil.,? 1996", "shortCiteRegEx": "Schmidhuber and Heil.", "year": 1996}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "J. Machine Learning Research, 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "From feedforward to recurrent LSTM neural networks for language modeling", "author": ["Martin Sundermeyer", "Hermann Ney", "Ralf Schl\u00fcter."], "venue": "Trans. Audio, Speech, and Language, 23(3):517\u2013529.", "citeRegEx": "Sundermeyer et al\\.,? 2015", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2015}, {"title": "Decoding with large-scale neural language models improves translation", "author": ["Ashish Vaswani", "Yinggong Zhao", "Victoria Fossum", "David Chiang."], "venue": "Proc. EMNLP, pages 1387\u20131392.", "citeRegEx": "Vaswani et al\\.,? 2013", "shortCiteRegEx": "Vaswani et al\\.", "year": 2013}, {"title": "Can artificial neural networks learn language models? In Proc", "author": ["Wei Xu", "Alexander I. Rudnicky."], "venue": "International Conference on Statistical Language Processing, pages M1\u201313.", "citeRegEx": "Xu and Rudnicky.,? 2000", "shortCiteRegEx": "Xu and Rudnicky.", "year": 2000}], "referenceMentions": [{"referenceID": 16, "context": "For example, neural language models and joint language/translation models improve machine translation quality significantly (Vaswani et al., 2013; Devlin et al., 2014).", "startOffset": 124, "endOffset": 167}, {"referenceID": 3, "context": "For example, neural language models and joint language/translation models improve machine translation quality significantly (Vaswani et al., 2013; Devlin et al., 2014).", "startOffset": 124, "endOffset": 167}, {"referenceID": 2, "context": "Using a neural n-gram language model (Bengio et al., 2003), we are able to show that our novel auto-sizing method is able to learn models that are smaller than models trained without the method, while maintaining nearly the same perplexity.", "startOffset": 37, "endOffset": 58}, {"referenceID": 16, "context": "Vaswani et al. (2013) showed that this model, with some improvements, can be used effectively during decoding in machine translation.", "startOffset": 0, "endOffset": 22}, {"referenceID": 2, "context": "Figure 1: Neural probabilistic language model (Bengio et al., 2003), adapted from Vaswani et al.", "startOffset": 46, "endOffset": 67}, {"referenceID": 2, "context": "Figure 1: Neural probabilistic language model (Bengio et al., 2003), adapted from Vaswani et al. (2013).", "startOffset": 47, "endOffset": 104}, {"referenceID": 5, "context": "The methods we use are discussed in detail elsewhere (Duchi et al., 2008; Duchi and Singer, 2009); in this section, we include a short description of these methods for completeness.", "startOffset": 53, "endOffset": 97}, {"referenceID": 4, "context": "The methods we use are discussed in detail elsewhere (Duchi et al., 2008; Duchi and Singer, 2009); in this section, we include a short description of these methods for completeness.", "startOffset": 53, "endOffset": 97}, {"referenceID": 11, "context": "Most work on learning with regularizers, including this work, can be thought of as instances of the proximal gradient method (Parikh and Boyd, 2014).", "startOffset": 125, "endOffset": 148}, {"referenceID": 4, "context": "In fact, the two problems can be solved by nearly identical algorithms, because they are convex conjugates of each other (Duchi and Singer, 2009; Bach et al., 2012).", "startOffset": 121, "endOffset": 164}, {"referenceID": 0, "context": "In fact, the two problems can be solved by nearly identical algorithms, because they are convex conjugates of each other (Duchi and Singer, 2009; Bach et al., 2012).", "startOffset": 121, "endOffset": 164}, {"referenceID": 3, "context": "But this situation seems similar to another optimization problem, projection onto the \ufffd1-ball, which Duchi et al. (2008) solve in linear time without pre-sorting.", "startOffset": 101, "endOffset": 121}, {"referenceID": 5, "context": "At the conclusion of the algorithm, \u03c1 is set to the largest value that passes the test (line 13), and finally the new xj are computed (line 16) \u2013 the only difference from Duchi et al.\u2019s algorithm. This algorithm is asymptotically faster than that of Quattoni et al. (2009). They reformulate \ufffd\u221e,1 regularization as a constrained optimization problem (in which the \ufffd\u221e,1 norm is bounded by \u03bc) and provide a solution inO(n log n) time.", "startOffset": 171, "endOffset": 273}, {"referenceID": 9, "context": "We use two hidden layers of rectified linear units (Nair and Hinton, 2010).", "startOffset": 51, "endOffset": 74}, {"referenceID": 15, "context": "We evaluate our model using the open-source NPLM toolkit released by Vaswani et al. (2013), extending it to use the additional regularizers as described in this paper.", "startOffset": 69, "endOffset": 91}, {"referenceID": 10, "context": "Schmidhuber and Heil (1996) proposed a character n-gram model using neural networks which they used for text compression.", "startOffset": 0, "endOffset": 28}, {"referenceID": 10, "context": "Schmidhuber and Heil (1996) proposed a character n-gram model using neural networks which they used for text compression. Xu and Rudnicky (2000) proposed a word-based probability model using a softmax output layer trained using cross-entropy, but only for bigrams.", "startOffset": 0, "endOffset": 145}, {"referenceID": 1, "context": "Bengio et al. (2003) defined a probabilistic word n-gram model and demonstrated improvements over conventional smoothed language models.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "Bengio et al. (2003) defined a probabilistic word n-gram model and demonstrated improvements over conventional smoothed language models. Mnih and Teh (2012) sped up training of log-bilinear language models through the use of noise-contrastive estimation (NCE).", "startOffset": 0, "endOffset": 157}, {"referenceID": 1, "context": "Bengio et al. (2003) defined a probabilistic word n-gram model and demonstrated improvements over conventional smoothed language models. Mnih and Teh (2012) sped up training of log-bilinear language models through the use of noise-contrastive estimation (NCE). Vaswani et al. (2013) also used NCE to train the architecture of Bengio et al.", "startOffset": 0, "endOffset": 283}, {"referenceID": 1, "context": "Bengio et al. (2003) defined a probabilistic word n-gram model and demonstrated improvements over conventional smoothed language models. Mnih and Teh (2012) sped up training of log-bilinear language models through the use of noise-contrastive estimation (NCE). Vaswani et al. (2013) also used NCE to train the architecture of Bengio et al. (2003), and were able to integrate a largevocabulary language model directly into a machine translation decoder.", "startOffset": 0, "endOffset": 347}, {"referenceID": 1, "context": "Baltescu et al. (2014) describe a similar model, with extensions like a hierarchical softmax (based on Brown clustering) and direct n-gram features.", "startOffset": 0, "endOffset": 23}, {"referenceID": 7, "context": "RNNLM is an open-source implementation of a language model using recurrent neural networks (RNN) where connections between units can form directed cycles (Mikolov et al., 2011).", "startOffset": 154, "endOffset": 176}, {"referenceID": 6, "context": "The \u201cOptimal Brain Damage\u201d (OBD) technique (LeCun et al., 1989) computes a saliency based on the second derivative of the objective function with respect to each parameter.", "startOffset": 43, "endOffset": 63}, {"referenceID": 14, "context": "introduce a method called dropout in which units are directly deactivated at random during training (Srivastava et al., 2014), which induces sparsity in the hidden unit activations.", "startOffset": 100, "endOffset": 125}, {"referenceID": 6, "context": "RNNLM is an open-source implementation of a language model using recurrent neural networks (RNN) where connections between units can form directed cycles (Mikolov et al., 2011). Sundermeyer et al. (2015) use the long-short term memory (LSTM) neural architecture to show a perplexity improvement over the RNNLM toolkit.", "startOffset": 155, "endOffset": 204}, {"referenceID": 6, "context": "The \u201cOptimal Brain Damage\u201d (OBD) technique (LeCun et al., 1989) computes a saliency based on the second derivative of the objective function with respect to each parameter. The parameters are then sorted by saliency, and the lowest-saliency parameters are pruned. The pruning process is separate from the training process, whereas regularization performs training and pruning simultaneously. Regularization in neural networks is also an old idea; for example, Nowland and Hinton (1992) mention both \ufffd2 and \ufffd0 regularization.", "startOffset": 44, "endOffset": 486}], "year": 2015, "abstractText": "Neural networks have been shown to improve performance across a range of natural-language tasks. However, designing and training them can be complicated. Frequently, researchers resort to repeated experimentation to pick optimal settings. In this paper, we address the issue of choosing the correct number of units in hidden layers. We introduce a method for automatically adjusting network size by pruning out hidden units through \ufffd\u221e,1 and \ufffd2,1 regularization. We apply this method to language modeling and demonstrate its ability to correctly choose the number of hidden units while maintaining perplexity. We also include these models in a machine translation decoder and show that these smaller neural models maintain the significant improvements of their unpruned versions.", "creator": "LaTeX with hyperref package"}}}