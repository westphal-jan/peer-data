{"id": "1508.05326", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Aug-2015", "title": "A large annotated corpus for learning natural language inference", "abstract": "Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.", "histories": [["v1", "Fri, 21 Aug 2015 16:17:01 GMT  (270kb,D)", "http://arxiv.org/abs/1508.05326v1", "To appear at EMNLP 2015. The data will be posted shortly before the conference (the week of 14 Sep) atthis http URL"]], "COMMENTS": "To appear at EMNLP 2015. The data will be posted shortly before the conference (the week of 14 Sep) atthis http URL", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["samuel r bowman", "gabor angeli", "christopher potts", "christopher d manning"], "accepted": true, "id": "1508.05326"}, "pdf": {"name": "1508.05326.pdf", "metadata": {"source": "CRF", "title": "A large annotated corpus for learning natural language inference", "authors": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning"], "emails": ["sbowman@stanford.edu", "angeli@stanford.edu", "cgpotts@stanford.edu", "manning@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, it has never been as good as it has been this year."}, {"heading": "2 A new corpus for NLI", "text": "This year, it is more than ever before in the history of the city, in which it has come as far as never before in the history of the city."}, {"heading": "2.1 Data collection", "text": "In this context, it should be noted that this is a very complex and complex matter."}, {"heading": "2.2 Data validation", "text": "In order to measure the quality of our corpus, and to construct maximum useful test and development kits, we performed an additional round of validation for about 10% of our data. This validation phase followed the same basic form as the Turk mechanical labeling task used to label the SICK origin data: We presented workers pairs of sets in five groups and asked them to choose a single label for each pair. We supplied each pair to four annotators that produced five labels per pair, including the label used by the original author. Nevertheless, the instructions were similar to the instructions for the initial data collection shown in Figure 1, and linked them to a similar FAQ. Although we initially used a very restrictive qualification (based on past approval rates) to select workers for the validation task, we still discovered (and deleted) some instances of random testing in an early batch of 30 completed and then completed a complete qualification."}, {"heading": "2.3 The distributed corpus", "text": "Table 1 shows a series of randomly selected validated examples from the development set with their designations. Qualitatively, we find that the data we collect is relatively common sense, and that hypotheses and premises often differ significantly structurally, suggesting that there is room for improvement beyond superficial word alignment models. We also find that the sentences we collect are written fluently and correctly in English, with a mixture of complete sentences and caption fragments in the caption style, although punctuation and capitalization are often omitted. We distribute the corpus under a CreativeCommons Attribution-ShareAlike license, the same license that is used for the Patchr30k source code. It can be downloaded at: nlp.stanford.edu / projects / snli / partition We distribute the corpus under a predetermined license for the Test Patchr30k-like development license, and the Common-like-license for each of these examples is included."}, {"heading": "3 Our data as a platform for evaluation", "text": "The most immediate application for our corpus is the development of models for the NLI task. As it is dramatically larger than any other body of comparable quality, we expect it to be suitable for parameter-rich models such as neural networks that have not yet been competitive in this task. However, our ability to evaluate standard classifier-based NLI models was limited to those designed to scale to the size of the SNLI without modification, so a more complete comparison of approaches will have to wait for future work. In this section, we will examine the performance of three classes of models that are easily scalable: (i) models of a well-known NLI system, the Open Platform Excitement; (ii) variants of a strong but simple function-based classification model that uses both unleashed and lexicalized features; and (iii) distributed representation models, including a baseline model and neural sequence network models."}, {"heading": "3.1 Excitement Open Platform models", "text": "The first class of models comes from the Excitement Open Platform (EOP, Pado \u0301 et al. 2014; Magnini et al. 2014) - an open source platform for RTE research. EOP is a tool for rapid development of NLI systems while sharing components such as common lexical resources and evaluation sets. We evaluate on the basis of two algorithms included in the distribution: a simple edit distance-based algorithm and a classification-based algorithm, complementing the latter both in bare form and with EOP's complete package of lexical resources. Our original goal was to better understand the difficulty of the task of classifying SNLI corpus inferences, rather than necessarily the performance of a state-of-the-art RTE system. We approached this by running the same system on multiple sets of data: our own test set, the SICK test set, the SICK test data set, and the test data set RTK-Gicolet (Standard RTE-3)."}, {"heading": "3.2 Lexicalized Classifier", "text": "Unlike RTE datasets, the size of SNLI supports approaches that make use of rich lexicalized characteristics. We evaluate a simple lexicalized classifier to examine the ability of non-specialized models to exploit these characteristics instead of a broader understanding of language. Our classifier implements 6 types of characteristics; 3 lexicalized and 3 lexicalized characteristics: 1. The BLEU evaluation of the hypothesis with respect to the premise occurs with an n-gram length between 1 and 4.2. The length difference between the hypothesis and the premise as a real feature. 3. The overlap between words in the premise and the hypothesis, both as an absolute number and as a percentage of possible overlaps, both over all words and over only nouns, adjectives and adjectives. 4. An indicator for each unigram and bigrams in the hypothesis is an essential indicator 5. cross and a pair of words in the hypothesis."}, {"heading": "3.3 Sentence embeddings and NLI", "text": "This year, it has come to the point where it will be able to take the lead, \"he said in an interview with the Deutsche Presse-Agentur.\" We have never lost as much time as this year, \"he said.\" But we are not yet to the point where we will be able to do so. \""}, {"heading": "3.4 Analysis and discussion", "text": "This year, the time has come for an agreement to be reached, and it will only take a few days."}, {"heading": "4 Transfer learning with SICK", "text": "To the extent that the successful formation of a neural network model such as our LSTM on SNLI forces this model to encode largely accurate representations of the English scene descriptions and to build a design classifier on these relationships, we should expect that it is easily possible to adapt the trained model for use in other NLI tasks. In this section, we evaluate the creation task of SICK using a simple transfer learning method (Pratt et al., 1991) and achieve competitive results. To perform the transfer, we take the parameters of the LSTM RNN model that was trained on SNLI and use them to initialize a new model that will be trained from this point on only on the training part of SICK. The only newly initialized parameters are aresoftmax layer parameters and the embedding of words that appear in SICK but not in SNLI (populated with GloVe embedding as above)."}, {"heading": "5 Conclusion", "text": "Natural languages are powerful vehicles for thinking, and almost all questions about the meaningfulness of language can be reduced to questions of entanglement and contradiction in context, suggesting that NLI is an ideal testing ground for theories of semantic representation, and that training for NLI tasks can provide rich cross-domain semantic representations. However, to date, due to the limited nature of existing NLI resources, it has not been possible to fully realize this potential, and this paper attempted to address this with a new, large-scale, naturalistic corpus of sentence pairs labeled for entanglement, contradiction, and independence. We used this corpus to evaluate a number of models and found that both simple lexicalized models and neural network models work well, and that the representations learned through a neural network model on our corpus can be used to dramatically improve performance on a standard challenge dataset."}, {"heading": "Acknowledgments", "text": "We are grateful for the support of a Google Faculty Research Award, a gift from Bloomberg L.P., the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under grant number FA875013-2-0040, the National Science Foundation under grant number IIS 1159679, and the Naval Department, Office of Naval Research, under grant number N00014-10-1-0109. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of Google, Bloomberg L.P., DARPA, AFRL NSF, ONR or the U.S. government."}], "references": [{"title": "Recognising textual entailment with logical inference", "author": ["Johan Bos", "Katja Markert."], "venue": "Proc. EMNLP.", "citeRegEx": "Bos and Markert.,? 2005", "shortCiteRegEx": "Bos and Markert.", "year": 2005}, {"title": "Recursive neural networks can learn logical semantics", "author": ["Samuel R. Bowman", "Christopher Potts", "Christopher D. Manning."], "venue": "Proc. of the 3rd Workshop on Continuous Vector Space Models and their Compositionality.", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "VerbOcean: Mining the web for fine-grained semantic verb relations", "author": ["Timothy Chklovski", "Patrick Pantel."], "venue": "Proc. EMNLP.", "citeRegEx": "Chklovski and Pantel.,? 2004", "shortCiteRegEx": "Chklovski and Pantel.", "year": 2004}, {"title": "Entailment, intensionality and text understanding", "author": ["Cleo Condoravdi", "Dick Crouch", "Valeria de Paiva", "Reinhard Stolle", "Daniel G. Bobrow."], "venue": "Proc. of the HLT-NAACL 2003 Workshop on Text Meaning.", "citeRegEx": "Condoravdi et al\\.,? 2003", "shortCiteRegEx": "Condoravdi et al\\.", "year": 2003}, {"title": "The PASCAL recognising textual entailment challenge", "author": ["Ido Dagan", "Oren Glickman", "Bernardo Magnini."], "venue": "Machine learning challenges. Evaluating predictive uncertainty, visual object classification, and recognising tectual entailment, pages 177\u2013", "citeRegEx": "Dagan et al\\.,? 2006", "shortCiteRegEx": "Dagan et al\\.", "year": 2006}, {"title": "Finding contradictions in text", "author": ["Marie-Catherine de Marneffe", "Anna N. Rafferty", "Christopher D. Manning."], "venue": "Proc. ACL.", "citeRegEx": "Marneffe et al\\.,? 2008", "shortCiteRegEx": "Marneffe et al\\.", "year": 2008}, {"title": "Brown corpus manual", "author": ["W. Nelson Francis", "Henry Kucera."], "venue": "Brown University.", "citeRegEx": "Francis and Kucera.,? 1979", "shortCiteRegEx": "Francis and Kucera.", "year": 1979}, {"title": "A natural logic inference system", "author": ["Yaroslav Fyodorov", "Yoad Winter", "Nissim Francez."], "venue": "Proc. of the 2nd Workshop on Inference in Computational Semantics.", "citeRegEx": "Fyodorov et al\\.,? 2000", "shortCiteRegEx": "Fyodorov et al\\.", "year": 2000}, {"title": "The third PASCAL recognizing textual entailment challenge", "author": ["Danilo Giampiccolo", "Bernardo Magnini", "Ido Dagan", "Bill Dolan."], "venue": "Proc. of the ACL-PASCAL workshop on textual entailment and paraphrasing.", "citeRegEx": "Giampiccolo et al\\.,? 2007", "shortCiteRegEx": "Giampiccolo et al\\.", "year": 2007}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Semantic Theory", "author": ["Jerrold J. Katz."], "venue": "Harper & Row, New York.", "citeRegEx": "Katz.,? 1972", "shortCiteRegEx": "Katz.", "year": 1972}, {"title": "Accurate unlexicalized parsing", "author": ["Dan Klein", "Christopher D. Manning."], "venue": "Proc. ACL.", "citeRegEx": "Klein and Manning.,? 2003", "shortCiteRegEx": "Klein and Manning.", "year": 2003}, {"title": "Illinois-LH: A denotational and distributional approach to semantics", "author": ["Alice Lai", "Julia Hockenmaier."], "venue": "Proc. SemEval.", "citeRegEx": "Lai and Hockenmaier.,? 2014", "shortCiteRegEx": "Lai and Hockenmaier.", "year": 2014}, {"title": "On our best behaviour", "author": ["Hector J. Levesque."], "venue": "Proc. AAAI.", "citeRegEx": "Levesque.,? 2013", "shortCiteRegEx": "Levesque.", "year": 2013}, {"title": "Focused entailment graphs for open IE propositions", "author": ["Omer Levy", "Ido Dagan", "Jacob Goldberger."], "venue": "Proc. CoNLL.", "citeRegEx": "Levy et al\\.,? 2014", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "An extended model of natural logic", "author": ["Bill MacCartney", "Christopher D Manning."], "venue": "Proc. of the Eighth International Conference on Computational Semantics.", "citeRegEx": "MacCartney and Manning.,? 2009", "shortCiteRegEx": "MacCartney and Manning.", "year": 2009}, {"title": "The Excitement Open Platform for textual inferences", "author": ["Bernardo Magnini", "Roberto Zanoli", "Ido Dagan", "Kathrin Eichler", "G\u00fcnter Neumann", "Tae-Gil Noh", "Sebastian Pado", "Asher Stern", "Omer Levy."], "venue": "Proc. ACL.", "citeRegEx": "Magnini et al\\.,? 2014", "shortCiteRegEx": "Magnini et al\\.", "year": 2014}, {"title": "SemEval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and tex", "author": ["Marco Marelli", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli"], "venue": null, "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "A SICK cure for the evaluation of compositional distributional semantic models", "author": ["Marco Marelli", "Stefano Menini", "Marco Baroni", "Luisa Bentivogli", "Raffaella Bernardi", "Roberto Zamparelli."], "venue": "Proc. LREC.", "citeRegEx": "Marelli et al\\.,? 2014b", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "WordNet: a lexical database for english", "author": ["George A Miller."], "venue": "Communications of the ACM, 38(11):39\u201341.", "citeRegEx": "Miller.,? 1995", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Design and realization of a modular architecture for textual entailment", "author": ["Sebastian Pad\u00f3", "Tae-Gil Noh", "Asher Stern", "Rui Wang", "Roberto Zanoli."], "venue": "Journal of Natural Language Engineering.", "citeRegEx": "Pad\u00f3 et al\\.,? 2014", "shortCiteRegEx": "Pad\u00f3 et al\\.", "year": 2014}, {"title": "PPDB 2.0: Better paraphrase ranking, finegrained entailment relations, word embeddings, and style classification", "author": ["Ellie Pavlick", "Johan Bos", "Malvina Nissim", "Charley Beller", "Ben Van Durme", "Chris Callison-Burch"], "venue": "In Proc. ACL", "citeRegEx": "Pavlick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pavlick et al\\.", "year": 2015}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "Proc. EMNLP.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Direct transfer of learned information among neural networks", "author": ["Lorien Y Pratt", "Jack Mostow", "Candace A Kamm", "Ace A Kamm."], "venue": "Proc. AAAI.", "citeRegEx": "Pratt et al\\.,? 1991", "shortCiteRegEx": "Pratt et al\\.", "year": 1991}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts."], "venue": "Proc. EMNLP.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "JMLR.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "A brief history of natural logic", "author": ["Johan van Benthem."], "venue": "M. Chakraborty, B. L\u00f6we, M. Nath Mitra, and S. Sarukki, editors, Logic, Navya-Nyaya and Applications: Homage to Bimal Matilal. College Publications.", "citeRegEx": "Benthem.,? 2008", "shortCiteRegEx": "Benthem.", "year": 2008}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["Sida I. Wang", "Christopher D. Manning."], "venue": "Proc. ACL.", "citeRegEx": "Wang and Manning.,? 2012", "shortCiteRegEx": "Wang and Manning.", "year": 2012}, {"title": "Recognizing textual entailment using sentence similarity based on dependency tree skeletons", "author": ["Rui Wang", "G\u00fcnter Neumann."], "venue": "ACL-PASCAL Workshop on Textual Entailment and Paraphrasing.", "citeRegEx": "Wang and Neumann.,? 2007", "shortCiteRegEx": "Wang and Neumann.", "year": 2007}, {"title": "Information structure and non-canonical syntax", "author": ["Gregory Ward", "Betty Birner."], "venue": "Laurence R. Horn and Gregory Ward, editors, Handbook of Pragmatics, pages 153\u2013174. Blackwell, Oxford.", "citeRegEx": "Ward and Birner.,? 2004", "shortCiteRegEx": "Ward and Birner.", "year": 2004}, {"title": "Towards AI-complete question answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov."], "venue": "arXiv:1502.05698.", "citeRegEx": "Weston et al\\.,? 2015a", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Memory networks", "author": ["Jason Weston", "Sumit Chopra", "Antoine Bordes."], "venue": "Proc. ICLR.", "citeRegEx": "Weston et al\\.,? 2015b", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Understanding natural language", "author": ["Terry Winograd."], "venue": "Cognitive Psychology, 3(1):1\u2013191.", "citeRegEx": "Winograd.,? 1972", "shortCiteRegEx": "Winograd.", "year": 1972}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier."], "venue": "TACL, 2:67\u2013", "citeRegEx": "Young et al\\.,? 2014", "shortCiteRegEx": "Young et al\\.", "year": 2014}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "arXiv preprint arXiv:1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 10, "context": "The semantic concepts of entailment and contradiction are central to all aspects of natural language meaning (Katz, 1972; van Benthem, 2008), from the lexicon to the content of entire texts.", "startOffset": 109, "endOffset": 140}, {"referenceID": 7, "context": "Thus, natural language inference (NLI) \u2014 characterizing and using these relations in computational systems (Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009) \u2014 is essential in tasks ranging from information retrieval to semantic parsing to commonsense reasoning.", "startOffset": 107, "endOffset": 228}, {"referenceID": 3, "context": "Thus, natural language inference (NLI) \u2014 characterizing and using these relations in computational systems (Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009) \u2014 is essential in tasks ranging from information retrieval to semantic parsing to commonsense reasoning.", "startOffset": 107, "endOffset": 228}, {"referenceID": 0, "context": "Thus, natural language inference (NLI) \u2014 characterizing and using these relations in computational systems (Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009) \u2014 is essential in tasks ranging from information retrieval to semantic parsing to commonsense reasoning.", "startOffset": 107, "endOffset": 228}, {"referenceID": 4, "context": "Thus, natural language inference (NLI) \u2014 characterizing and using these relations in computational systems (Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009) \u2014 is essential in tasks ranging from information retrieval to semantic parsing to commonsense reasoning.", "startOffset": 107, "endOffset": 228}, {"referenceID": 15, "context": "Thus, natural language inference (NLI) \u2014 characterizing and using these relations in computational systems (Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009) \u2014 is essential in tasks ranging from information retrieval to semantic parsing to commonsense reasoning.", "startOffset": 107, "endOffset": 228}, {"referenceID": 1, "context": "they can be trained to support the full range of logical and commonsense inferences required for NLI (Bowman et al., 2015; Weston et al., 2015b; Weston et al., 2015a).", "startOffset": 101, "endOffset": 166}, {"referenceID": 31, "context": "they can be trained to support the full range of logical and commonsense inferences required for NLI (Bowman et al., 2015; Weston et al., 2015b; Weston et al., 2015a).", "startOffset": 101, "endOffset": 166}, {"referenceID": 30, "context": "they can be trained to support the full range of logical and commonsense inferences required for NLI (Bowman et al., 2015; Weston et al., 2015b; Weston et al., 2015a).", "startOffset": 101, "endOffset": 166}, {"referenceID": 33, "context": "The Denotation Graph entailment set (Young et al., 2014) contains millions of examples of entailments between sentences and artificially constructed short phrases, but it was labeled using fully automatic methods, and is noisy enough that it is probably suitable only as a source of sup-", "startOffset": 36, "endOffset": 56}, {"referenceID": 21, "context": "(2014) introduce a large corpus of semi-automatically annotated entailment examples between subject\u2013verb\u2013 object relation triples, and the second release of the Paraphrase Database (Pavlick et al., 2015) in-", "startOffset": 181, "endOffset": 203}, {"referenceID": 14, "context": "sentence-level entailment, Levy et al. (2014) introduce a large corpus of semi-automatically annotated entailment examples between subject\u2013verb\u2013 object relation triples, and the second release of the Paraphrase Database (Pavlick et al.", "startOffset": 27, "endOffset": 46}, {"referenceID": 18, "context": "determinacy concerning the correct semantic label (de Marneffe et al. 2008 \u00a74.3; Marelli et al. 2014b).", "startOffset": 50, "endOffset": 102}, {"referenceID": 33, "context": "Flickr30k corpus (Young et al., 2014), a collection of approximately 160k captions (corresponding to about 30k images) collected in an earlier crowdsourced effort.", "startOffset": 17, "endOffset": 37}, {"referenceID": 11, "context": "2 (Klein and Manning, 2003), trained on the standard training set as well as on the Brown Corpus (Francis and Kucera 1979), which we found to improve the parse quality of the descriptive sentences and noun phrases found in the descriptions.", "startOffset": 2, "endOffset": 27}, {"referenceID": 16, "context": "The first class of models is from the Excitement Open Platform (EOP, Pad\u00f3 et al. 2014; Magnini et al. 2014)\u2014an open source platform for RTE research.", "startOffset": 63, "endOffset": 107}, {"referenceID": 8, "context": "We approached this by running the same system on several data sets: our own test set, the SICK test data, and the standard RTE-3 test set (Giampiccolo et al., 2007).", "startOffset": 138, "endOffset": 164}, {"referenceID": 19, "context": "In addition to the base classifier-based system distributed with the platform, we train a variant which includes information from WordNet (Miller, 1995) and VerbOcean (Chklovski and Pantel, 2004), and makes", "startOffset": 138, "endOffset": 152}, {"referenceID": 2, "context": "In addition to the base classifier-based system distributed with the platform, we train a variant which includes information from WordNet (Miller, 1995) and VerbOcean (Chklovski and Pantel, 2004), and makes", "startOffset": 167, "endOffset": 195}, {"referenceID": 28, "context": "use of features based on tree patterns and dependency tree skeletons (Wang and Neumann, 2007).", "startOffset": 69, "endOffset": 93}, {"referenceID": 27, "context": "A similar result was shown in Wang and Manning (2012) for bigram features in sentiment analysis.", "startOffset": 30, "endOffset": 54}, {"referenceID": 9, "context": "In addition, we experiment with two simple sequence embedding models: a plain RNN and an LSTM RNN (Hochreiter and Schmidhuber, 1997).", "startOffset": 98, "endOffset": 132}, {"referenceID": 34, "context": "All of the models are randomly initialized using standard techniques and trained using AdaDelta (Zeiler, 2012) minibatch SGD until performance on the development set stops improving.", "startOffset": 96, "endOffset": 110}, {"referenceID": 25, "context": "We applied L2 regularization to all models, manually tuning the strength coefficient \u03bb for each, and additionally applied dropout (Srivastava et al., 2014) to the inputs and outputs of the senSentence model Train Test", "startOffset": 130, "endOffset": 155}, {"referenceID": 32, "context": "Some of these examples have qualities reminiscent of Winograd schemas (Winograd, 1972; Levesque, 2013).", "startOffset": 70, "endOffset": 102}, {"referenceID": 13, "context": "Some of these examples have qualities reminiscent of Winograd schemas (Winograd, 1972; Levesque, 2013).", "startOffset": 70, "endOffset": 102}, {"referenceID": 29, "context": "(Ward and Birner, 2004).", "startOffset": 0, "endOffset": 23}, {"referenceID": 23, "context": "In this section, we evaluate on the SICK entailment task using a simple transfer learning method (Pratt et al., 1991) and achieve competitive results.", "startOffset": 97, "endOffset": 117}, {"referenceID": 12, "context": "6% (Lai and Hockenmaier, 2014) and the 84% level of interannotator agreement, which likely represents an approximate performance ceiling.", "startOffset": 3, "endOffset": 30}], "year": 2015, "abstractText": "Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.", "creator": "LaTeX with hyperref package"}}}