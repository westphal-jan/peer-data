{"id": "1508.06615", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Aug-2015", "title": "Character-Aware Neural Language Models", "abstract": "We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) over characters, whose output is given to a long short-term memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-of-the-art despite having 60% fewer parameters. On languages with rich morphology (Czech, German, French, Spanish, Russian), the model consistently outperforms a Kneser-Ney baseline (by 30-35%) and a word-level LSTM baseline (by 15-25%), again with far fewer parameters. Our results suggest that on many languages, character inputs are sufficient for language modeling.", "histories": [["v1", "Wed, 26 Aug 2015 19:25:34 GMT  (211kb,D)", "http://arxiv.org/abs/1508.06615v1", null], ["v2", "Thu, 17 Sep 2015 23:18:00 GMT  (209kb,D)", "http://arxiv.org/abs/1508.06615v2", null], ["v3", "Fri, 16 Oct 2015 03:18:13 GMT  (209kb,D)", "http://arxiv.org/abs/1508.06615v3", null], ["v4", "Tue, 1 Dec 2015 22:59:24 GMT  (209kb,D)", "http://arxiv.org/abs/1508.06615v4", "AAAI 2016"]], "reviews": [], "SUBJECTS": "cs.CL cs.NE stat.ML", "authors": ["yoon kim", "yacine jernite", "david sontag", "alexander m rush"], "accepted": true, "id": "1508.06615"}, "pdf": {"name": "1508.06615.pdf", "metadata": {"source": "CRF", "title": "Character-Aware Neural Language Models", "authors": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush"], "emails": ["yoonkim@seas.harvard.edu", "srush@seas.harvard.edu", "jernite@cs.nyu.edu", "dsontag@cs.nyu.edu"], "sections": [{"heading": "Introduction", "text": "In fact, it is true that most people are able to survive if they do not feel able to survive. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "Model", "text": "The architecture of our model, shown in Figure 1, is simple: While a conventional NLM uses Word embedding as input, our model takes the output from a single-layer CNN with a maximum amount of time. For notation, we refer to vectors with bold lowercase letters (e.g. xt, b), matrices with bold uppercase letters (e.g. W, Uo), scalars with italic lowercase letters (e.g. x, b), and sets with italic uppercase letters (e.g. V, C)."}, {"heading": "Recurrent Neural Network", "text": "A recurrent neural network (RNN) is a kind of neural network architecture, which is particularly suitable for modelling sequential phenomena. (Wxt + Uht \u2212 1 + b) (1) Here f is an elementary nonlinearity and W Rm \u00b7 n, U Rm \u00b7 m are the parameters of an affine transformation. (Wxt + Uht \u2212 1 + b) In theory, an RNN can link all historical information up to the time t with the hidden state \u2212 n. In practice, however, the learning of protracted dependencies with a vanilla RNN \u00b7 m, b Rm \u00b7 m is the parameters of an affine transformation."}, {"heading": "Recurrent Neural Network Language Model", "text": "A language model specifies a distribution via wt + 1 (of which support is V) in view of the historical sequence w1: t = [w1,.., wt]. A recurring neural network language model (RNN-LM) does this by applying an affine transformation to the hidden layer, followed by a softmax: Pr (wt + 1 = k | w1: t) = exp (ht \u00b7 pk + qk) \u2211 k \u2032 V exp (ht \u00b7 pk \u2032 + qk \u2032) (3), where pk is the k-th column of P-Rm \u00b7 | V | (also referred to as output sequence), 3 and qk is the k-th element of q-R | V |, which reflects the total frequency of k. Likewise, a conventional RNN-LM that normally takes words as input, if wt = j, then the input is embedded in the RNM model."}, {"heading": "Character-level Convolutional Neural Network", "text": "In our model, input in due time is t an output from a character-level revolutionary neural network (CharCNN), which we describe in this section. CNNs (LeCun et al. 1989) have achieved state-of-the-art results on computer vision (Krizhevsky et al. 2012) and have also proved effective for various NLP tasks (Collobert et al. 2011). Architectures used for NLP applications differ in that they contain temporal (one-dimensional) rather than spatial, evolutionary signs. Let C be the vocabulary of characters (e.g. C-50 for English), d be the dimensionality of character embeddings, 4 and Q-Rd | C be the (dense) matrix of character embeddings to be learned. Let C be the vocabulary of characters (e.g. C-50 for English), d be the dimensionality of character embeddings."}, {"heading": "Highway Network", "text": "We could simply replace xk (the word embedding) with yk on each t in the RNN-LM, and as we will show later, this simple model alone performs well (Table 6). Alternatively, we could have a multi-layer perceptron (MLP) over yk to model interactions between characteristics, but we found that this led to much worse performance. Instead, we made improvements by guiding yk through a highway network (HW-Net) recently proposed by Srivastava et al. (2015). While a layer of an ordinary MLP performs an affine transformation followed by a nonlinearity to obtain a new set of characteristics, z = g (Wy + b) (7) applies a layer of an HW network as follows: z = t g (WHy + bH) + (1 \u2212 t) y (8) y (8), where g is a nonlinearity characteristic, where g (WTy + bT) is the transformation form n."}, {"heading": "Experimental Setup", "text": "As is customary in speech modeling, we use Perplexity (PPL) to evaluate the performance of our models. Perplexity of a model over a sequence w1: T = [w1,.., wT] isPPL = T = 1 Pr (wt | w1 \u2212 1 T (9) 6Srivastava et al. (2015) recommend initializing bT to a negative value to mitigate the initial carry behavior. We actually found that this was critical to achieving good performance and initialized bT to \u2212 2.which is just equal to exp [NLL]."}, {"heading": "Optimization", "text": "The training is done by truncated back propagation over time (Werbos 1990; Graves 2013). We propagate 35 time steps with a batch size of 20, with the learning rate initially set to 1.0 and halved if the helplessness after an epoch does not decrease by more than 1.0 on the validation set. We train for 25 epochs and select the most powerful model on the validation set (which typically happened in the last epoch).The parameters of the model 7http: / / www.fit.vutbr.cz / \u0445 imikolov / rnnlm / 8http: / / www.statmt.org / wmt13 / translation-task.html 9http: / / bothameister.github.io / are randomly initialized via a uniform distribution with support [\u2212 0.05, 0.05].For regulation we use dropout (Hinton al. 2012) with the probability that the word will be buried under the layer (to the starting layer)."}, {"heading": "Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "English Penn Treebank", "text": "We train two versions of our model to assess the trade-off between performance and size. Hyperparameters of our small (LSTM-CharCNN-Small) and large (LSTMCharCNN-Large) models are summarized in Table 2. As a further baseline, we also train two comparable LSTM models that use only word embedding (LSTM-Word-Small, LSTM-Word-Large).11 We found that performance is most sensitive to the number of hidden units in the LSTM and the number of filters in the CharCNN, and is largely insensitive to the other hyperparameters. Failure phenomena proved crucial for good performance with a larger model. As shown in Table 3, our large model is on par with the existing state of the art (Zaremba et al. 2014), although it has about 60% fewer parameters than other NLMs of similar size."}, {"heading": "Other Languages", "text": "The results of the study, however, are relatively simple, and the next set of results from Botha and Blunsom (2014) is expected, whose model also takes into account the subword information summed up in the input and output layers. As a comparison between the M-LBL models, the use of LSTM models is emphasized."}, {"heading": "Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Learned Word Representations", "text": "We explore the word representations that CharCNN / HW-Net learned at PTB. Table 5 has the closest neighbors of word representations learned at both the word and character levels. For the character models, we compare the representations obtained before and after the highway layers. However, before the highway layers, the representations appear to be based exclusively on surface shapes - for example, the closest neighbors of yours are yours, young, four youths who are close to you in terms of processing distance. However, the highway layers seem to allow a coding of semantic characteristics that cannot be distinguished solely from orthography. After the highway layers, the closest neighbor of yours is orthographically distinguishable from yours. Another example is while and although - these words are far apart, but the composition model is able to place them close to each other. Model14The difference in parameters is greater for non-Pmodel companies, as the size of the word layer model is more sensitive."}, {"heading": "Highway Layers", "text": "Since the performance differences may be due to the reduction in model size, we are also training a model that feeds yk (i.e. word representation from the CharCNN) through a single-layer multi-layer perceptron (MLP) to use an input into the LSTM. We note that the MLP model performs poor.We believe that highway networks are well suited for working with CNNs by adaptively combining local features recognized by the individual filters. CNNs have already proven successful in sentence / document composition (Kalchburner et al. 2014; Kim 2014; Zhang and LeCun 2015; Lei et al. 2015), and we are convinced that additional benefits can be achieved by using highway layers on existing CNN architectures."}, {"heading": "Further Observations", "text": "We report on some other experiments and observations: \u2022 The current model does not use subword information at the output level because the hidden-to-output is a regular softmax. We tried to inject subword information into the output by having another charCNN (which differs from the input CharCNN) whose output is softmaxed with ht to get Pr (wt + 1 | w1: t). We found that (1) the training was very slow (despite caching strategies) because you have to run the charCNN over all V in each batch, and (2) the model performed poorly (PPL \u2248 1000 on PTB). \u2022 The combination of word embedding with the CharCNN output to form a combined representation of a word (to be used as input to the LSTM) resulted in slightly worse performance. This was surprising as improvements at the level of language tagging (Santos and Santos could represent the input level of the input level of the input level of the input level of the input level of the input level of the input level of the input level) and input level of the input level of the input level of the input level of the input level of the input level of the input (Santos and input level of the input level of the input level of the input level of the input level of the input level of the input level)."}, {"heading": "Related Work", "text": "The models of Neural Language (NLM) encompass a rich family of neural network architectures for modeling languages. Some15We experimented with (1) concatenation, (2) tensor products, (3) mean and (4) adaptive weight schemes in which the model learns the weighting between word embedding and charCNN output. Examples of architectures include thrust (Bengio et al. 2003), recurring (Mikolov et al. 2010), sum product (Cheng et al. 2014), log bilinear (Mnih and Hinton 2007), and revolutionary (Wang et al. 2015) networks.To address the rare word problem, Alexandrescu and Krichhoff (2006) - building on analog n-gram language models of Bilmes and Kirchhoff (2003) - represent a word as a series of common factor embeddings."}, {"heading": "Conclusion", "text": "Although our model is smaller, it significantly outperforms similar models that use words / morphemes as inputs, and the improvements are more pronounced in languages with complex morphology. Analysis of word representations derived from the character composition part of the model also shows that the model is capable of encoding semantically significant features that are not directly apparent from orthography alone. Our work questions the need for word embedding as inputs for neural language modeling. To the extent that language modeling is largely based on grasping the syntactical role of a word, it would be interesting to see whether the architecture presented in this paper is suitable for more semantic tasks - for example, as a coder / decoder in neural machine translation (Cho et al. 2014; Sutskever et al. 2014)."}, {"heading": "Acknowledgments", "text": "We are especially grateful to Jan Botha for providing the non-English pre-processed data sets and the model results in Table 4."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) over characters, whose output is given to a long short-term memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-of-the-art despite having 60% fewer parameters. On languages with rich morphology (Czech, German, French, Spanish, Russian), the model consistently outperforms a Kneser-Ney baseline (by 30\u201335%) and a word-level LSTM baseline (by 15\u201325%), again with far fewer parameters. Our results suggest that on many languages, character inputs are sufficient for language modeling.", "creator": "TeX"}}}