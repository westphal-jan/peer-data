{"id": "1508.06669", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Aug-2015", "title": "Component-Enhanced Chinese Character Embeddings", "abstract": "Distributed word representations are very useful for capturing semantic information and have been successfully applied in a variety of NLP tasks, especially on English. In this work, we innovatively develop two component-enhanced Chinese character embedding models and their bigram extensions. Distinguished from English word embeddings, our models explore the compositions of Chinese characters, which often serve as semantic indictors inherently. The evaluations on both word similarity and text classification demonstrate the effectiveness of our models.", "histories": [["v1", "Wed, 26 Aug 2015 21:25:25 GMT  (150kb)", "http://arxiv.org/abs/1508.06669v1", "6 pages, 2 figures, conference, EMNLP 2015"]], "COMMENTS": "6 pages, 2 figures, conference, EMNLP 2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yanran li", "wenjie li", "fei sun", "sujian li"], "accepted": true, "id": "1508.06669"}, "pdf": {"name": "1508.06669.pdf", "metadata": {"source": "CRF", "title": "Component-Enhanced Chinese Character Embeddings", "authors": ["Yanran Li", "Wenjie Li", "Fei Sun", "Sujian Li"], "emails": ["cswjli}@comp.polyu.edu.hk,", "ofey.sunfei@gmail.com,", "lisujian@pku.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Component-Enhanced Character Embeddings", "text": "In fact, it is the case that most people who are able to identify themselves and understand themselves are able. (...) Most of them are unable to identify themselves. (...) Most of them are unable to identify themselves. (...) Most of them are unable to identify themselves. (...) Most of them are unable to identify themselves. (...) Most of them are unable to identify themselves. (...) Most of them are unable to identify themselves. (...) Most of them are able to identify themselves. (...) Most of them are not able to identify themselves. (...) Most of them are not able to identify themselves. (...) Most of them are unable to identify themselves. (...) Most of them are unable to identify themselves. (...) Most of them are not able to think of themselves. (...) Most of them are not able to think of themselves. (...) Most of them are not able to think of themselves. (...) Most of them are not able to think of themselves. (...) Most of them are not able to think of themselves. (...) Most of them are not able to think of themselves. (...) Most of them are not able to think of themselves."}, {"heading": "3 Evaluations", "text": "We are investigating the quality of the proposed two Chinese characters as well as their related extensions to both intrinsic word similarity. \"We have only two different characters.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"We.\" \"\" \"\" \"\" \"We.\" \"\" \"\" \"\" We. \"\" \"\" \"\" We. \"\" \"\" \"We.\" \"\" \"\" We. \"\" \"\" \"We.\". \"\" \"\" We. \".\" \"\" \"We.\" \"\" \"We.\". \"\" \"We.\". \"\" \"We.\".. \"\" \"We.\". \"\" \"We...\" \"\" \"We...\" \"\" \"We....\" \"\" \"We..\" \"\" We... \"\" \"\" We... \"\" \"We...\" \"\" We.... \"\" \"We...\" \"\" \"We....\" \"\" We..... \"\" \"We....\" \"\" \"\" We..... \"\" \"\" We...... \"\" \"\" \"We.......\" \"\" \"\" We....... \"\" \"\" \"We.......\" \"\" \"\" We....... \"\" \"\" \"We........\" \"\" \"We.....\" \"\" \"\" \"We.......\" \"\" \"\" We............ \"\" \"\" \"We....\" \"\" \"\" \"We.......\" \"\" \"\" We..... \"We......\" We......................... \"\" \"\" \"We...........\" \"\" \"\" \"\" We...."}, {"heading": "4 Conclusions and Future Work", "text": "In this paper, we propose two component-enhanced embedding models of Chinese characters and their extensions to investigate both the internal composition and external contexts of Chinese characters. Experimental results show their advantages in learning rich semantic representations. In the future, we plan to develop embedding models based on the composition of component character and character-word. Both types of compositions will serve in a coordinated way for the distribution representations."}, {"heading": "Acknowledgements", "text": "The work described in this paper was supported by grants from the Research Grants Council of Hong Kong (PolyU 5202 / 12E and PolyU 152094 / 14E), grants from the National Natural Science Foundation of China (61272291 and 61273278), and an internal PolyU scholarship (4-BCB5). Appendix As mentioned in Section 3, we present the complete list of transformations of the variant and original forms of 24 radicals. The meaning columns provide the corresponding meanings of the components on the left. Transform meaning: Transform meaning: \"grass,\" \"hand,\" \"human,\" water, \"knife,\" vehicle, \"\" fire, \"\" silk, \"\" gold, \"old,\" \"wheat,\" \"cattle,\" \"food,\" heart, \"\" nest, \"\" jade, \"\" \"spider,\" \"cloth,\" \"body.\""}], "references": [{"title": "Tailoring continuous word representations for dependency parsing", "author": ["References Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Bansal et al\\.,? 2014", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "Parsing chinese synthetic words with a characterbased dependency model", "author": ["Fei Cheng", "Kevin Duh", "Yuji Matsumoto"], "venue": null, "citeRegEx": "Cheng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2014}, {"title": "HowNet and the Computation of Meaning", "author": ["Zhendong Dong", "Qiang Dong."], "venue": "World Scientific.", "citeRegEx": "Dong and Dong.,? 2006", "shortCiteRegEx": "Dong and Dong.", "year": 2006}, {"title": "Liblinear: A library for large linear classification", "author": ["Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "XiangRui Wang", "Chih-Jen Lin."], "venue": "The Journal of Machine Learning Research, 9:1871\u20131874.", "citeRegEx": "Fan et al\\.,? 2008", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."], "venue": "Proceedings of the 10th international conference on World Wide Web, pages 406\u2013", "citeRegEx": "Finkelstein et al\\.,? 2001", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "A ?radical? approach to reading development in chinese: The role of semantic radicals and phonetic radicals", "author": ["Connie Suk-Han Ho", "Ting-Ting Ng", "Wing-Kin Ng."], "venue": "Journal of Literacy Research, 35(3): 849\u2013878.", "citeRegEx": "Ho et al\\.,? 2003", "shortCiteRegEx": "Ho et al\\.", "year": 2003}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Eric H Huang", "Richard Socher", "Christopher D Manning", "Andrew Y Ng."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Words similarity algorithm based on tongyici cilin in semantic web adaptive learning system [j", "author": ["TIAN Jiu-le", "ZHAO Wei."], "venue": "Journal of Jilin University (Information Science Edition), 6:010.", "citeRegEx": "Jiu.le and Wei.,? 2010", "shortCiteRegEx": "Jiu.le and Wei.", "year": 2010}, {"title": "Dependencybased word embeddings", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, volume 2, pages 302\u2013308.", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."], "venue": "Transactions of the Association for Computational Linguistics, 3:211\u2013225.", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Two/ too simple adaptations of word2vec for syntax problems", "author": ["Wang Ling", "Chris Dyer", "Alan Black", "Isabel Trancoso."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguis-", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Minh-Thang Luong", "Richard Socher", "Christopher D Manning."], "venue": "CoNLL-2013, 104.", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv: 1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Research design and statistical analysis", "author": ["Jerome L Myers", "Arnold Well", "Robert Frederick Lorch."], "venue": "Routledge.", "citeRegEx": "Myers et al\\.,? 2010", "shortCiteRegEx": "Myers et al\\.", "year": 2010}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12:1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Co-learning of word representations and morpheme representations", "author": ["Siyu Qiu", "Qing Cui", "Jiang Bian", "Bin Gao", "Tie-Yan Liu."], "venue": "COLING.", "citeRegEx": "Qiu et al\\.,? 2014", "shortCiteRegEx": "Qiu et al\\.", "year": 2014}, {"title": "Contextual correlates of synonymy", "author": ["Herbert Rubenstein", "John B Goodenough."], "venue": "Communications of the ACM, 8(10):627\u2013633.", "citeRegEx": "Rubenstein and Goodenough.,? 1965", "shortCiteRegEx": "Rubenstein and Goodenough.", "year": 1965}, {"title": "Radical-enhanced chinese character embedding", "author": ["Yaming Sun", "Lei Lin", "Nan Yang", "Zhenzhou Ji", "Xiaolong Wang."], "venue": "Neural Information Processing, pages 279\u2013286. Springer.", "citeRegEx": "Sun et al\\.,? 2014", "shortCiteRegEx": "Sun et al\\.", "year": 2014}, {"title": "Unsupervised multi-domain adaptation with feature embeddings", "author": ["Yi Yang", "Jacob Eisenstein"], "venue": null, "citeRegEx": "Yang and Eisenstein.,? \\Q2015\\E", "shortCiteRegEx": "Yang and Eisenstein.", "year": 2015}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["Mo Yu", "Mark Dredze."], "venue": "Association for Computational Linguistics (ACL), pages 545\u2013550.", "citeRegEx": "Yu and Dredze.,? 2014", "shortCiteRegEx": "Yu and Dredze.", "year": 2014}, {"title": "Chinese parsing exploiting characters", "author": ["Meishan Zhang", "Yue Zhang", "Wanxiang Che", "Ting Liu."], "venue": "ACL (1), pages 125\u2013134.", "citeRegEx": "Zhang et al\\.,? 2013", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 6, "context": "Among the existing approaches (Huang et al., 2012; Levy and Goldberg, 2014; Yang and Eisenstein, 2015), the continuous bag-of-wordsmodel (CBOW) and the continuous skip-gram model (SkipGram) remain the most popular ones that one can use to build word embeddings efficiently (Mikolov et al.", "startOffset": 30, "endOffset": 102}, {"referenceID": 8, "context": "Among the existing approaches (Huang et al., 2012; Levy and Goldberg, 2014; Yang and Eisenstein, 2015), the continuous bag-of-wordsmodel (CBOW) and the continuous skip-gram model (SkipGram) remain the most popular ones that one can use to build word embeddings efficiently (Mikolov et al.", "startOffset": 30, "endOffset": 102}, {"referenceID": 19, "context": "Among the existing approaches (Huang et al., 2012; Levy and Goldberg, 2014; Yang and Eisenstein, 2015), the continuous bag-of-wordsmodel (CBOW) and the continuous skip-gram model (SkipGram) remain the most popular ones that one can use to build word embeddings efficiently (Mikolov et al.", "startOffset": 30, "endOffset": 102}, {"referenceID": 12, "context": ", 2012; Levy and Goldberg, 2014; Yang and Eisenstein, 2015), the continuous bag-of-wordsmodel (CBOW) and the continuous skip-gram model (SkipGram) remain the most popular ones that one can use to build word embeddings efficiently (Mikolov et al., 2013a; Mikolov et al., 2013b).", "startOffset": 230, "endOffset": 276}, {"referenceID": 13, "context": ", 2012; Levy and Goldberg, 2014; Yang and Eisenstein, 2015), the continuous bag-of-wordsmodel (CBOW) and the continuous skip-gram model (SkipGram) remain the most popular ones that one can use to build word embeddings efficiently (Mikolov et al., 2013a; Mikolov et al., 2013b).", "startOffset": 230, "endOffset": 276}, {"referenceID": 8, "context": "The context defined by the window of surrounding words may unavoidably include certain less semantically-relevant words and/or miss the wordswith important and relevantmeanings (Levy and Goldberg, 2014).", "startOffset": 177, "endOffset": 202}, {"referenceID": 8, "context": "To overcome this shortcoming, a line of research deploys the order information of the words in the contexts by either deriving the contexts using dependency relations where the target word participates (Levy and Goldberg, 2014; Yu and Dredze, 2014; Bansal et al., 2014) or directly keeping the order features (Ling et al.", "startOffset": 202, "endOffset": 269}, {"referenceID": 20, "context": "To overcome this shortcoming, a line of research deploys the order information of the words in the contexts by either deriving the contexts using dependency relations where the target word participates (Levy and Goldberg, 2014; Yu and Dredze, 2014; Bansal et al., 2014) or directly keeping the order features (Ling et al.", "startOffset": 202, "endOffset": 269}, {"referenceID": 0, "context": "To overcome this shortcoming, a line of research deploys the order information of the words in the contexts by either deriving the contexts using dependency relations where the target word participates (Levy and Goldberg, 2014; Yu and Dredze, 2014; Bansal et al., 2014) or directly keeping the order features (Ling et al.", "startOffset": 202, "endOffset": 269}, {"referenceID": 10, "context": ", 2014) or directly keeping the order features (Ling et al., 2015).", "startOffset": 47, "endOffset": 66}, {"referenceID": 0, "context": "To overcome this shortcoming, a line of research deploys the order information of the words in the contexts by either deriving the contexts using dependency relations where the target word participates (Levy and Goldberg, 2014; Yu and Dredze, 2014; Bansal et al., 2014) or directly keeping the order features (Ling et al., 2015). As to another line, Luong et al. (2013) captures morphological composition by using neural networks and Qiu et al.", "startOffset": 249, "endOffset": 370}, {"referenceID": 0, "context": "To overcome this shortcoming, a line of research deploys the order information of the words in the contexts by either deriving the contexts using dependency relations where the target word participates (Levy and Goldberg, 2014; Yu and Dredze, 2014; Bansal et al., 2014) or directly keeping the order features (Ling et al., 2015). As to another line, Luong et al. (2013) captures morphological composition by using neural networks and Qiu et al. (2014) introduces the morphological knowledge as both additional input representation and auxiliary supervision to the neural network framework.", "startOffset": 249, "endOffset": 452}, {"referenceID": 0, "context": "To overcome this shortcoming, a line of research deploys the order information of the words in the contexts by either deriving the contexts using dependency relations where the target word participates (Levy and Goldberg, 2014; Yu and Dredze, 2014; Bansal et al., 2014) or directly keeping the order features (Ling et al., 2015). As to another line, Luong et al. (2013) captures morphological composition by using neural networks and Qiu et al. (2014) introduces the morphological knowledge as both additional input representation and auxiliary supervision to the neural network framework. While most previous work focuses on English, there is a little work on Chinese. Zhang et al. (2013) extracts the syntactical morphemes and Cheng et al.", "startOffset": 249, "endOffset": 690}, {"referenceID": 0, "context": "To overcome this shortcoming, a line of research deploys the order information of the words in the contexts by either deriving the contexts using dependency relations where the target word participates (Levy and Goldberg, 2014; Yu and Dredze, 2014; Bansal et al., 2014) or directly keeping the order features (Ling et al., 2015). As to another line, Luong et al. (2013) captures morphological composition by using neural networks and Qiu et al. (2014) introduces the morphological knowledge as both additional input representation and auxiliary supervision to the neural network framework. While most previous work focuses on English, there is a little work on Chinese. Zhang et al. (2013) extracts the syntactical morphemes and Cheng et al. (2014) incorporates the POS tags and dependency relations.", "startOffset": 249, "endOffset": 749}, {"referenceID": 5, "context": "late to Chinese word reading and sentence comprehension (Ho et al., 2003).", "startOffset": 56, "endOffset": 73}, {"referenceID": 18, "context": "While Sun et al. (2014) utilizes radical information in a supervised fashion, we build our models in a holistic unsupervised and bottom-up way.", "startOffset": 6, "endOffset": 24}, {"referenceID": 12, "context": "charCBOW follows the original continuous bag-of-words model (CBOW) proposed by (Mikolov et al., 2013a).", "startOffset": 79, "endOffset": 102}, {"referenceID": 4, "context": "As the widely used public word similarity datasets like WS-353 (Finkelstein et al., 2001), RG-65 (Rubenstein and Goodenough, 1965) are built for English embeddings, .", "startOffset": 63, "endOffset": 89}, {"referenceID": 17, "context": ", 2001), RG-65 (Rubenstein and Goodenough, 1965) are built for English embeddings, .", "startOffset": 15, "endOffset": 48}, {"referenceID": 2, "context": "Two candidate choices are Chinese dictionaries HowNet (Dong and Dong, 2006) and HIT-CIR\u2019s Extended Tongyici Cilin (denoted as E-TC)4.", "startOffset": 54, "endOffset": 75}, {"referenceID": 3, "context": "We train l2-regularized logistic regression classifiers using the LIBLINEAR package (Fan et al., 2008) with the learned embeddings.", "startOffset": 84, "endOffset": 102}, {"referenceID": 14, "context": "In the word similarity evaluation, we compute the Spearman\u2019s rank correlation (Myers et al., 2010) between the similarity scores based on the learned embedding models and the E-TC similarity scores computed by following Jiu-le and Wei (2010).", "startOffset": 78, "endOffset": 98}, {"referenceID": 7, "context": ", 2010) between the similarity scores based on the learned embedding models and the E-TC similarity scores computed by following Jiu-le and Wei (2010). The bi-character embeddings are concatenation of the composite character embeddings.", "startOffset": 129, "endOffset": 151}, {"referenceID": 15, "context": "This result is consistent with the finding in the previous work (Pennington et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015).", "startOffset": 64, "endOffset": 133}, {"referenceID": 8, "context": "This result is consistent with the finding in the previous work (Pennington et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015).", "startOffset": 64, "endOffset": 133}, {"referenceID": 9, "context": "This result is consistent with the finding in the previous work (Pennington et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015).", "startOffset": 64, "endOffset": 133}, {"referenceID": 9, "context": "Although they have a potential of deriving better representations (Levy et al., 2015), they lose some particular information from each unit of input in the average operations.", "startOffset": 66, "endOffset": 85}], "year": 2015, "abstractText": "Distributed word representations are very useful for capturing semantic information and have been successfully applied in a variety of NLP tasks, especially on English. In this work, we innovatively develop two component-enhanced Chinese character embedding models and their bigram extensions. Distinguished from English word embeddings, our models explore the compositions of Chinese characters, which often serve as semantic indictors inherently. The evaluations on both word similarity and text classification demonstrate the effectiveness of our models.", "creator": " XeTeX output 2015.08.26:0516"}}}