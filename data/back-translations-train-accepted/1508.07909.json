{"id": "1508.07909", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2015", "title": "Neural Machine Translation of Rare Words with Subword Units", "abstract": "Neural machine translation (NMT) models typically operate with a fixed vocabulary, so the translation of rare and unknown words is an open problem. Previous work addresses this problem through back-off dictionaries. In this paper, we introduce a simpler and more effective approach, enabling the translation of rare and unknown words by encoding them as sequences of subword units, based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 0.8 and 1.5 BLEU, respectively.", "histories": [["v1", "Mon, 31 Aug 2015 16:37:31 GMT  (209kb)", "http://arxiv.org/abs/1508.07909v1", null], ["v2", "Fri, 27 Nov 2015 15:41:25 GMT  (122kb)", "http://arxiv.org/abs/1508.07909v2", "new results with improved baseline; more references in related work; cuts to fit conference proceedings"], ["v3", "Thu, 17 Mar 2016 14:56:06 GMT  (125kb)", "http://arxiv.org/abs/1508.07909v3", null], ["v4", "Fri, 3 Jun 2016 15:01:02 GMT  (125kb)", "http://arxiv.org/abs/1508.07909v4", "accepted at ACL 2016"], ["v5", "Fri, 10 Jun 2016 14:45:08 GMT  (197kb)", "http://arxiv.org/abs/1508.07909v5", "accepted at ACL 2016; new in this version: figure 3"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["rico sennrich", "barry haddow", "alexandra birch"], "accepted": true, "id": "1508.07909"}, "pdf": {"name": "1508.07909.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["rico.sennrich@ed.ac.uk,", "a.birch@ed.ac.uk,", "bhaddow@inf.ed.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 150 8.07 909v 1 [cs.C L] 31 Aug 201 5"}, {"heading": "1 Introduction", "text": "In fact, the translation of rare words is an open problem. The vocabulary of neural models is usually limited to 30,000-50,000 words, but the translation is an open vocabulary, and the translation models require mechanisms to translate and generate words that never occur in collaboration with Samsung Electronics sp. z o.o. - Samsung R & D processes such as agglutination and compounding. The translation models require mechanisms to translate and generate words that occur during the training."}, {"heading": "2 Neural Machine Translation", "text": "We follow the neural machine translation architecture of Bahdanau et al. (2014), which we briefly summarize here. However, we note that our approach is not specific to this architecture.The neural machine translation system is implemented as an encoder decoder network with recursive neural networks.The encoder is a bidirectional neural network with gated recurrent units (Cho et al., 2014) that reads an input sequence x = (x1,..., xm) and calculates a forward sequence of hidden states (\u2212 \u2192 h1, \u2212 hm) and a reverse sequence (\u2190 \u2212 h1,... \u2212 hm).The hidden states \u2212 hj and \u2190 \u2212 hj are linked to obtain the annotation vector hj. The decoder is a recursive neural network that predicts a target sequence."}, {"heading": "3 Subword Translation", "text": "The main motivation behind this work is that the translation of some words is transparent insofar as they are translatable by a competent translator, even if they are new to him or her, based on a translation of known subtext units such as morphemes or phonemes. Word categories whose translation is potentially transparent include: \u2022 Named units. Names can often be copied between languages that share an alphabet from source text to target text. Transcription or transliteration may be required, especially if the alphabets or syllables differ from each other. Example: Barack Obama (English; German) The translation of some of the words in the translation of the words in the translation of the translation of the words into the translation of the translation of the words into the translation of the text."}, {"heading": "3.1 Related Work", "text": "Segmentation of morphologically complex words such as compounds is widespread, and various algorithms for morpheme segmentation have been studied (Nie\u00dfen and Ney, 2000; Koehn and Knight, 2003; Virpioja et al., 2007). A large proportion of unknown words are names that can be easily copied into the target text if both languages use the same alphabet. If alphabets differ, transliteration is required (Durrani et al., 2014). Character-based translation has been successfully used for closely related languages (Tiedemann et al.)."}, {"heading": "3.2 Byte Pair Encoding (BPE)", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "4 Evaluation", "text": "We aim to answer the following empirical questions: \u2022 Can we improve the translation of rare and invisible words in neural machine translation by representing them by subword units? \u2022 Since the Russian training text also includes words that use the Latin alphabet, we also use the Latin BPE operations. \u2022 Which segmentation in subword units best performs in terms of word size, text size and translation quality? We conduct experiments on data from the common translation task of the 2015 Workshop on Statistical Machine Translation. For English \u2192 German, our training set consists of 4.2 million sentence pairs, or about 100 million tokens. For English \u2192 Russian, the training set consists of 2.6 million sentence pairs, or about 50 million tokens. We tokenize and truecase the data with the scripts provided in Moses (Koehn et al., 2007).We use Groundhog 3 as the implementation of the NT system for all experiments."}, {"heading": "4.1 Subword statistics", "text": "Apart from the quality of the translation, which we will empirically verify, our objectives are efficiency and compactness of the model. The time complexity of the encoder decoder architectures is at least 4 linear to the sequence length and fragmentation harms efficiency. On the other hand, an algorithm that performs only a few splits produces large vocabularies, and our goal is to enable compact networks with small vocabularies and encode unknown words by subword units known to the network. We compare different segmentation algorithms based on the number of tools that affect efficiency, word size, word size and number of unknown tools in a test set. We conduct experiments with different segmentation techniques on the German section of parallel parallel of parallel training data. A simple baseline is the segmentation of words that affect the efficiency, word size and efficiency of words influenced by the compact and model efficiency of words."}, {"heading": "4.2 Translation experiments", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "5 Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Unigram accuracy", "text": "In fact, most of them are able to trump themselves, and they are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...)"}, {"heading": "5.2 Manual Analysis", "text": "Table 6 shows two translation examples for the translation direction English \u2192 German, Table 7 for English \u2192 Russian. The basic system fails with all examples, either by deleting content (health, situation) or by copying source words that should be translated or transcribed. Subword translations of health research institutes show that the subword systems are able to learn translations when they overlap (Research \u2192 Fo | rs | ch | un | g), or when the segmentation does not coincide with morphology limits: Segmentation research | institutes would be linguistically more plausible and easier to adapt to the English research institutes than the segmentation Forsch | ungsinstitut | ten in the BPE-60k system, but nevertheless a correct translation is produced. If the systems have failed to learn a translation that is due to data sparseness (as with asinine translations that are dumb as translations, but that we should not see the translation system as plausible), then some of the translations may be in English."}, {"heading": "6 Conclusion", "text": "The main contribution of this work is that we show that neural network encoder decoder translation systems are able to translate rare and invisible words by presenting them as a sequence of subword units, which is both simpler and more effective than using an external translation system (even a simple dictionary) as a back-off model. NMT is relatively robust in terms of the type of subword unit, and we observe similar gains across the baseline with character bigrams, character trigrams, or variable length units. In terms of efficiency, we achieve the most compact representation of text and vocabulary with a word segmentation technique based on byte-pair encoding. Our analysis shows that not only words outside the vocabulary, but also rare words within the vocabulary are poorly translated by our baseline NMT system."}, {"heading": "Acknowledgments", "text": "The research results presented in this publication were carried out in cooperation with Samsung Electronics Polska sp. z o.o. - Samsung R & D Institute Poland and funded under the European Union's Horizon 2020 research and innovation programme under Funding Agreement 645452 (QT21)."}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR, abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Modeling outof-vocabulary words for robust speech recognition", "author": ["Issam Bazzi", "James R. Glass."], "venue": "Sixth International Conference on Spoken Language Processing, ICSLP 2000 / INTERSPEECH 2000, Beijing, China, October 16-20, 2000, pages", "citeRegEx": "Bazzi and Glass.,? 2000", "shortCiteRegEx": "Bazzi and Glass.", "year": 2000}, {"title": "Compositional Morphology for Word Representations and Language Modelling", "author": ["Jan A. Botha", "Phil Blunsom."], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML), Beijing, China.", "citeRegEx": "Botha and Blunsom.,? 2014", "shortCiteRegEx": "Botha and Blunsom.", "year": 2014}, {"title": "VariableLength Word Encodings for Neural Translation Models", "author": ["Rohan Chitnis", "John DeNero."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Chitnis and DeNero.,? 2015", "shortCiteRegEx": "Chitnis and DeNero.", "year": 2015}, {"title": "Learning Phrase Representations using RNN Encoder\u2013 Decoder for Statistical Machine Translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Pro-", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Unsupervised Discovery of Morphemes", "author": ["Mathias Creutz", "Krista Lagus."], "venue": "Proceedings of the ACL-02 Workshop on Morphological and Phonological Learning, pages 21\u201330. Association for Computational Linguistics.", "citeRegEx": "Creutz and Lagus.,? 2002", "shortCiteRegEx": "Creutz and Lagus.", "year": 2002}, {"title": "Integrating an Unsupervised Transliteration Model into Statistical Machine Translation", "author": ["Nadir Durrani", "Hassan Sajjad", "Hieu Hoang", "Philipp Koehn."], "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational", "citeRegEx": "Durrani et al\\.,? 2014", "shortCiteRegEx": "Durrani et al\\.", "year": 2014}, {"title": "A Simple, Fast, and Effective Reparameterization of IBM Model 2", "author": ["Chris Dyer", "Victor Chahuneau", "Noah A. Smith."], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-", "citeRegEx": "Dyer et al\\.,? 2013", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "A New Algorithm for Data Compression", "author": ["Philip Gage."], "venue": "C Users J., 12(2):23\u201338, February.", "citeRegEx": "Gage.,? 1994", "shortCiteRegEx": "Gage.", "year": 1994}, {"title": "The Edinburgh/JHU Phrase-based Machine Translation Systems for WMT 2015", "author": ["Barry Haddow", "Matthias Huck", "Alexandra Birch", "Nikolay Bogoychev", "Philipp Koehn."], "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation.", "citeRegEx": "Haddow et al\\.,? 2015", "shortCiteRegEx": "Haddow et al\\.", "year": 2015}, {"title": "On Using Very Large Target Vocabulary for Neural Machine Translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the", "citeRegEx": "Jean et al\\.,? 2015a", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Montreal Neural Machine Translation Systems for WMT\u201915", "author": ["S\u00e9bastien Jean", "Orhan Firat", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation.", "citeRegEx": "Jean et al\\.,? 2015b", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Recurrent Continuous Translation Models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Seattle, October. Association for Computational Linguistics.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Empirical Methods for Compound Splitting", "author": ["Philipp Koehn", "Kevin Knight."], "venue": "EACL \u201903: Proceedings of the Tenth Conference on European Chapter of the Association for Computational Linguistics, pages 187\u2013193, Budapest, Hungary. Asso-", "citeRegEx": "Koehn and Knight.,? 2003", "shortCiteRegEx": "Koehn and Knight.", "year": 2003}, {"title": "Moses: Open Source Toolkit for Statistical Machine Translation", "author": ["Constantin", "Evan Herbst."], "venue": "Proceedings of the ACL-2007 Demo and Poster Sessions, pages 177\u2013180, Prague, Czech Republic. Association for Computational Linguistics.", "citeRegEx": "Constantin and Herbst.,? 2007", "shortCiteRegEx": "Constantin and Herbst.", "year": 2007}, {"title": "Word hy-phen-a-tion by com-put-er", "author": ["Franklin M. Liang."], "venue": "Ph.D. thesis, Stanford University, Department of Linguistics, Stanford, CA.", "citeRegEx": "Liang.,? 1983", "shortCiteRegEx": "Liang.", "year": 1983}, {"title": "Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation", "author": ["Wang Ling", "Chris Dyer", "Alan W. Black", "Isabel Trancoso", "Ramon Fermandez", "Silvio Amir", "Luis Marujo", "Tiago Luis."], "venue": "Proceedings of the", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Better Word Representations with Recursive Neural Networks for Morphology", "author": ["Thang Luong", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning, CoNLL", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Addressing the Rare Word Problem in Neural Machine Translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Subword Language Modeling with Neural Networks", "author": ["Tomas Mikolov", "Ilya Sutskever", "Anoop Deoras", "HaiSon Le", "Stefan Kombrink", "Jan Cernock\u00fd."], "venue": "Unpublished.", "citeRegEx": "Mikolov et al\\.,? 2012", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "Improving SMT quality with morpho-syntactic analysis", "author": ["Sonja Nie\u00dfen", "Hermann Ney."], "venue": "18th Int. Conf. on Computational Linguistics, pages 1081\u20131085.", "citeRegEx": "Nie\u00dfen and Ney.,? 2000", "shortCiteRegEx": "Nie\u00dfen and Ney.", "year": 2000}, {"title": "A Joint Dependency Model of Morphological and Syntactic Structure for Statistical Machine Translation", "author": ["Rico Sennrich", "Barry Haddow."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Sennrich and Haddow.,? 2015", "shortCiteRegEx": "Sennrich and Haddow.", "year": 2015}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Character-Based Pivot Translation for Under-Resourced Languages and Domains", "author": ["J\u00f6rg Tiedemann."], "venue": "Proceedings of the 13th Conference of the", "citeRegEx": "Tiedemann.,? 2012", "shortCiteRegEx": "Tiedemann.", "year": 2012}, {"title": "Morphology-Aware Statistical Machine Translation Based on Morphs Induced in an Unsupervised Manner", "author": ["Sami Virpioja", "Jaakko J. V\u00e4yrynen", "Mathias Creutz", "Markus Sadeniemi."], "venue": "Proceedings of the Machine Translation Summit XI, pages", "citeRegEx": "Virpioja et al\\.,? 2007", "shortCiteRegEx": "Virpioja et al\\.", "year": 2007}, {"title": "ADADELTA: An Adaptive Learning Rate Method", "author": ["Matthew D. Zeiler."], "venue": "CoRR, abs/1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 12, "context": "Neural machine translation has recently shown impressive results (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 65, "endOffset": 144}, {"referenceID": 22, "context": "Neural machine translation has recently shown impressive results (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 65, "endOffset": 144}, {"referenceID": 0, "context": "Neural machine translation has recently shown impressive results (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 65, "endOffset": 144}, {"referenceID": 10, "context": "The translation of out-of-vocabulary words is addressed in (Jean et al., 2015a; Luong et al., 2015) through a back-off to a dictionary lookup.", "startOffset": 59, "endOffset": 99}, {"referenceID": 18, "context": "The translation of out-of-vocabulary words is addressed in (Jean et al., 2015a; Luong et al., 2015) through a back-off to a dictionary lookup.", "startOffset": 59, "endOffset": 99}, {"referenceID": 10, "context": "Jean et al. (2015a) recently proposed techniques for efficiently increasing the vocabulary size by keeping only part of the vocabulary in memory during training, and filtering the vocabulary during decoding.", "startOffset": 0, "endOffset": 20}, {"referenceID": 10, "context": "the target text, as done by (Jean et al., 2015a; Luong et al., 2015), is a reasonable strategy for names, but transliteration is often required, especially if alphabets differ.", "startOffset": 28, "endOffset": 68}, {"referenceID": 18, "context": "the target text, as done by (Jean et al., 2015a; Luong et al., 2015), is a reasonable strategy for names, but transliteration is often required, especially if alphabets differ.", "startOffset": 28, "endOffset": 68}, {"referenceID": 10, "context": "Instead of replacing rare words with special tokens which are later used for a backoff translation (Jean et al., 2015a; Luong et al., 2015), we segment them into subword units, allowing the neural networks to learn", "startOffset": 99, "endOffset": 139}, {"referenceID": 18, "context": "Instead of replacing rare words with special tokens which are later used for a backoff translation (Jean et al., 2015a; Luong et al., 2015), we segment them into subword units, allowing the neural networks to learn", "startOffset": 99, "endOffset": 139}, {"referenceID": 8, "context": "We adapt byte pair encoding (Gage, 1994), a simple compression algorithm, to the task of word segmentation.", "startOffset": 28, "endOffset": 40}, {"referenceID": 0, "context": "We follow the neural machine translation architecture by Bahdanau et al. (2014), which we will here briefly summarize.", "startOffset": 57, "endOffset": 80}, {"referenceID": 4, "context": "The encoder is a bidirectional neural network with gated recurrent units (Cho et al., 2014) that reads an input sequence x = (x1, .", "startOffset": 73, "endOffset": 91}, {"referenceID": 0, "context": "A detailed description can be found in (Bahdanau et al., 2014).", "startOffset": 39, "endOffset": 62}, {"referenceID": 23, "context": "Cognates and loanwords with a common origin can differ in regular ways between languages, so that character-level translation rules are sufficient (Tiedemann, 2012).", "startOffset": 147, "endOffset": 164}, {"referenceID": 20, "context": "The segmentation of morphologically complex words such as compounds is widely used, and various algorithm for morpheme segmentation have been investigated (Nie\u00dfen and Ney, 2000; Koehn and Knight, 2003; Virpioja et al., 2007).", "startOffset": 155, "endOffset": 224}, {"referenceID": 13, "context": "The segmentation of morphologically complex words such as compounds is widely used, and various algorithm for morpheme segmentation have been investigated (Nie\u00dfen and Ney, 2000; Koehn and Knight, 2003; Virpioja et al., 2007).", "startOffset": 155, "endOffset": 224}, {"referenceID": 24, "context": "The segmentation of morphologically complex words such as compounds is widely used, and various algorithm for morpheme segmentation have been investigated (Nie\u00dfen and Ney, 2000; Koehn and Knight, 2003; Virpioja et al., 2007).", "startOffset": 155, "endOffset": 224}, {"referenceID": 6, "context": "If alphabets differ, transliteration is required (Durrani et al., 2014).", "startOffset": 49, "endOffset": 71}, {"referenceID": 23, "context": "Character-based translation has been successfully used for closely related languages (Tiedemann, 2012).", "startOffset": 85, "endOffset": 102}, {"referenceID": 15, "context": "We test a slightly more sophisticated syllable segmentation developed for word hyphenation (Liang, 1983).", "startOffset": 91, "endOffset": 104}, {"referenceID": 1, "context": "The best choice of subword units may be task-specific, and for speech recognition, phone-level language models have been used (Bazzi and Glass, 2000).", "startOffset": 126, "endOffset": 149}, {"referenceID": 17, "context": "Various techniques have been proposed to produce fixed-length continuous word vectors based on characters or morphemes (Luong et al., 2013; Botha and Blunsom, 2014; Ling et al., 2015).", "startOffset": 119, "endOffset": 183}, {"referenceID": 2, "context": "Various techniques have been proposed to produce fixed-length continuous word vectors based on characters or morphemes (Luong et al., 2013; Botha and Blunsom, 2014; Ling et al., 2015).", "startOffset": 119, "endOffset": 183}, {"referenceID": 16, "context": "Various techniques have been proposed to produce fixed-length continuous word vectors based on characters or morphemes (Luong et al., 2013; Botha and Blunsom, 2014; Ling et al., 2015).", "startOffset": 119, "endOffset": 183}, {"referenceID": 8, "context": "Byte Pair Encoding (BPE) (Gage, 1994) is a simple data compression technique that iteratively replaces the most frequent pair of bytes in a sequence with a single, unused byte.", "startOffset": 25, "endOffset": 37}, {"referenceID": 3, "context": "coding of words for NMT (Chitnis and DeNero, 2015), is that our symbol sequences are still in-", "startOffset": 24, "endOffset": 50}, {"referenceID": 0, "context": "We use Groundhog3 as the implementation of the NMT system for all experiments (Bahdanau et al., 2014).", "startOffset": 78, "endOffset": 101}, {"referenceID": 0, "context": "We generally follow settings by previous work (Bahdanau et al., 2014; Jean et al., 2015a).", "startOffset": 46, "endOffset": 89}, {"referenceID": 10, "context": "We generally follow settings by previous work (Bahdanau et al., 2014; Jean et al., 2015a).", "startOffset": 46, "endOffset": 89}, {"referenceID": 0, "context": "We generally follow settings by previous work (Bahdanau et al., 2014; Jean et al., 2015a). All networks have a hidden layer size of 1000, and an embedding layer size of 620. Following Jean et al. (2015a), we only keep a shortlist of \u03c4 = 30000 words in memory.", "startOffset": 47, "endOffset": 204}, {"referenceID": 25, "context": "During training, we use Adadelta (Zeiler, 2012), a minibatch size of 80, and reshuffle the training set between epochs.", "startOffset": 33, "endOffset": 47}, {"referenceID": 10, "context": "We train a network for approximately 7 days, then take the last 4 saved models (models being saved every 12 hours), and continue training each with a fixed embedding layer (as suggested by (Jean et al., 2015a)) for 12 hours.", "startOffset": 189, "endOffset": 209}, {"referenceID": 7, "context": "We use a bilingual dictionary based on fast-align (Dyer et al., 2013).", "startOffset": 50, "endOffset": 69}, {"referenceID": 7, "context": "We use a bilingual dictionary based on fast-align (Dyer et al., 2013). For our baseline, this serves to speed up translation, since only a filtered list of candidate translations is stored in memory (like Jean et al. (2015a), we use K = 30000; K \u2032 = 10), and as back-off dictionary for rare words.", "startOffset": 51, "endOffset": 225}, {"referenceID": 13, "context": "5 Compound splitting (Koehn and Knight, 2003) is a popular technique in SMT.", "startOffset": 21, "endOffset": 45}, {"referenceID": 5, "context": "Morfessor (Creutz and Lagus, 2002) implements word segmentation based on minimum description length.", "startOffset": 10, "endOffset": 34}, {"referenceID": 19, "context": "Syllabification has been used to produce subword units for language modelling in (Mikolov et al., 2012), and we employ a word hyphenation algorithm (Liang, 1983).", "startOffset": 81, "endOffset": 103}, {"referenceID": 15, "context": ", 2012), and we employ a word hyphenation algorithm (Liang, 1983).", "startOffset": 52, "endOffset": 65}, {"referenceID": 0, "context": "The attention model of (Bahdanau et al., 2014) has complexity O(m \u00b7 n), m and n being the number of source and target symbols.", "startOffset": 23, "endOffset": 46}, {"referenceID": 13, "context": "\u25b3: (Koehn and Knight, 2003); *: (Creutz and Lagus, 2002); \u22c4: (Liang, 1983).", "startOffset": 3, "endOffset": 27}, {"referenceID": 5, "context": "\u25b3: (Koehn and Knight, 2003); *: (Creutz and Lagus, 2002); \u22c4: (Liang, 1983).", "startOffset": 32, "endOffset": 56}, {"referenceID": 15, "context": "\u25b3: (Koehn and Knight, 2003); *: (Creutz and Lagus, 2002); \u22c4: (Liang, 1983).", "startOffset": 61, "endOffset": 74}, {"referenceID": 10, "context": "7 We attempted to reproduce the results by Jean et al. (2015a), but our baseline is slightly worse8; We speculate that the difference is caused by differences in preprocessing and cleanup, training time, different hyperparameters, and/or ran-", "startOffset": 43, "endOffset": 63}, {"referenceID": 10, "context": "4 BLEU reported by (Jean et al., 2015a).", "startOffset": 19, "endOffset": 39}, {"referenceID": 21, "context": "syntax-based (Sennrich and Haddow, 2015) 22.", "startOffset": 13, "endOffset": 40}, {"referenceID": 19, "context": "The performance of our NMT systems is below our syntax-based baseline by Sennrich and Haddow (2015). We note that Jean et al.", "startOffset": 73, "endOffset": 100}, {"referenceID": 10, "context": "We note that Jean et al. (2015b) report a score of 24.", "startOffset": 13, "endOffset": 33}, {"referenceID": 9, "context": "The quality of the WDict baseline is substantially below the phrase-based state-of-the-art: the English\u2192Russian system by Haddow et al. (2015) outperforms WDict by 4.", "startOffset": 122, "endOffset": 143}, {"referenceID": 9, "context": "phrase-based (Haddow et al., 2015) 29.", "startOffset": 13, "endOffset": 34}], "year": 2015, "abstractText": "Neural machine translation (NMT) models typically operate with a fixed vocabulary, so the translation of rare and unknown words is an open problem. Previous work addresses this problem through back-off dictionaries. In this paper, we introduce a simpler and more effective approach, enabling the translation of rare and unknown words by encoding them as sequences of subword units, based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English\u2192German and English\u2192Russian by 0.8 and 1.5 BLEU, respectively.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}