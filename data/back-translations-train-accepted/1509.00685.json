{"id": "1509.00685", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Sep-2015", "title": "A Neural Attention Model for Abstractive Sentence Summarization", "abstract": "Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines.", "histories": [["v1", "Wed, 2 Sep 2015 13:20:40 GMT  (353kb,D)", "http://arxiv.org/abs/1509.00685v1", "Proceedings of EMNLP 2015"], ["v2", "Thu, 3 Sep 2015 19:55:45 GMT  (352kb,D)", "http://arxiv.org/abs/1509.00685v2", "Proceedings of EMNLP 2015"]], "COMMENTS": "Proceedings of EMNLP 2015", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["alexander m rush", "sumit chopra", "jason weston"], "accepted": true, "id": "1509.00685"}, "pdf": {"name": "1509.00685.pdf", "metadata": {"source": "CRF", "title": "A Neural Attention Model for Sentence Summarization", "authors": ["Alexander M. Rush", "Sumit Chopra"], "emails": ["srush@seas.harvard.edu", "spchopra@fb.com", "jase@fb.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, the fact is that most of them are only a very small group, able to survive on their own, without being able to survive on their own."}, {"heading": "2 Background", "text": "We begin with the definition of the sentence summary task (2013), which requires a fixed problem."}, {"heading": "3 Model", "text": "The distribution of interest, p (yi + 1 | x, yc; \u03b8), is a conditional language model based on the input set x. Previous work on summary and compression has used a noise-channel approach to divide a language model and a conditional summary model and estimate them independently of each other (Banko et al., 2000; Knight and Marcu, 2002; thumb \u0301 III and Marcu, 2002), i.e., arg max y log p (y | x) = arg max y log p (y) p (x | y), where p (y) and p (x | y) are estimated separately."}, {"heading": "3.1 Neural Language Model", "text": "The core of our parameterization is a language model for estimating the contextual probability of the next word. The language model is adapted to a standard, forward-looking neural network language model (NNLM), in particular to the class of NNLMs described by Bengio et al. (2003). The complete model is: p (yi + 1 | yc, x; \u03b8) - exp (Vh + Wenc (x, yc)) - y-c = [Eyi \u2212 C + 1,..., Eyi], h = tanh (Uy-c).The parameters are: p = (E, U, V, W), where E-RD \u00d7 V is a word embedded in the matrix, U-R (CD) \u00d7 H, V-RV \u00d7 H, W-RV \u00d7 H are weight matrices, W-RV \u00d7 H is the size of the word, and h is a hidden layer of the H function, the size of the black box is a three-dimensional context."}, {"heading": "3.2 Encoders", "text": "It is not only the question of whether and to what extent people are able to integrate into the EU, but also the question of how far they want to enter the EU. (...) It is the question of how far they want to enter the EU. (...) It is the question of how far they want to enter the EU. (...) It is the question of how far they want to enter the EU. (...) It is the question of how far they want to enter the EU. (...) It is the question of how far they want to enter the EU. (...) It is the question of how far the EU wants to enter the EU. (...) It is the question of how far they want to enter the EU. (...) It is the question of how far they want to enter the EU. (...) (...) (it is the EU. (...) (it is the EU. () (it is the EU) (it is the EU. () (it is the) (it is the EU. () (it is the) (it is the EU. () (it is the)"}, {"heading": "3.3 Training", "text": "The absence of generation constraints makes it possible to train the model on arbitrary input-output pairs. After defining the local conditional model p (yi + 1 | x, yc; \u03b8), we can estimate the parameters to minimize the negative log probability of a set of summaries. We define this training set as consisting of J-input summary pairs (x (1),.., (x (J), y (J), y (J))). The negative log probability is conveniently converted into a term for each token in the summary: NLL (\u03b8) = \u2212 J-J = 1log p (j) | x-J-J-J-J-J-J-J-J-J = 1 N \u2212 1-J-J-i = 1 log p (j) i + 1 | x (j), yc \u03b8;.We minimize Ncent Lch by using Mini-Batstostic Grade."}, {"heading": "4 Generating Summaries", "text": "Let us now return to the problem of generating summaries. Let us recall from Eq. 4 that our goal is to find a complete hypothesis (y * = arg max y * Y N \u2212 1 \u2211 i = 0 g (yi + 1, x, yc).Unlike phrase-based machine translation, where the inference NP-hard, it is actually comprehensible in theory to calculate y *. Since there is no explicit hard alignment restriction, viterbi decoding can be applied and takes O (NV C) time to find an exact solution. In practice, V is large enough to make this difficult. An alternative approach is to approach the Arg max with a strictly greedy or deterministic decoder. While decoders of this form can produce very poor approximations, they have shown that they are relatively effective and fast for neural MT models (Sutskhl, 2014 and a more excoherent compromise)."}, {"heading": "5 Extension: Extractive Tuning", "text": "While we will see that the attention-based model is effective in generating summaries, it misses an important aspect seen in the human-generated references. Specifically, the abstract model does not have the ability to find extractive word matches when necessary, such as transferring invisible intrinsic terms from the input line. Similar problems have also been observed in the models of neural translation, especially in the translation of rare words (Luong et al., 2014). To solve this problem, we are experimenting with matching a very small set of additional features that compensate for the abstract / extractive bias of the system. We do this by modifying our scoring function so that we directly estimate the likelihood of a summary using a log-linear model, as is common in machine translation: p (y | x;)."}, {"heading": "6 Related Work", "text": "In fact, most of the people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to move, to dance, to dance, to dance, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance"}, {"heading": "7 Experimental Setup", "text": "We are experimenting with our attention-based sentence summary model for the task of headline generation. In this section, we describe the corpora used for this task, the basic methods with which we compare, and the details of implementing our approach."}, {"heading": "7.1 Data Set", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they are able to move, in which they move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they"}, {"heading": "7.2 Baselines", "text": "Due to the diversity of approaches to the problem of sentence summary, we report on a wide range of headline generation baselines. From task DUC-2004, we include the PREFIX baseline, which simply returns the first 75 characters of input as a heading. We also report on the winning system of this common task, TOPIARY (Zajic et al., 2004). TOPIARY fuses a compression system that uses linguistically motivated translations of input (Dorr et al., 2003) with an unmonitored topic recognition algorithm (UTD) that appends key sentences from the full article to the compressed output. Woodsend et al. (2010) (described above) also report the results of input (Dorr et al., 2003) with an unmonitored topic recognition algorithm (UTD) that appends key sentences from the full article to the compressed output."}, {"heading": "7.3 Implementation", "text": "We use a learning rate of 0.05 and divide the learning rate by half if the validation protocol probability for an epoch does not improve. Training is performed with mixed minibatches of size 64, the minibatches are grouped by input length, after each epoch we renormalize the embedding tables (Hinton et al., 2012) and will be openly available along with the data pipeline. Crucially, the training is performed on GPUs and would require intractable or approximate values. Processing 1000 minibatches with D = 200, H = 400 takes 160 seconds, and the best validation accuracy is achieved after 15 days."}, {"heading": "8 Results", "text": "This year, the number of job-related redundancies is many times higher than in previous years."}, {"heading": "9 Conclusion", "text": "We have presented a neural attention-based abstract summary model based on the latest developments in neural machine translation. We combine this probabilistic model with a generation algorithm that generates precise abstract summaries. In a next step, we want to further improve the grammar of the summaries in a data-driven manner and scale this system to generate paragraph-level summaries, both of which pose additional challenges in terms of efficient alignment and consistency in generation."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Headline generation based on statistical translation", "author": ["Michele Banko", "Vibhu O Mittal", "Michael J Witbrock."], "venue": "Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, pages 318\u2013325. Association for Computational", "citeRegEx": "Banko et al\\.,? 2000", "shortCiteRegEx": "Banko et al\\.", "year": 2000}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "The Journal of Machine Learning Research, 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1406.1078.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Global inference for sentence compression: An integer linear programming approach", "author": ["James Clarke", "Mirella Lapata."], "venue": "Journal of Artificial Intelligence Research, pages 399\u2013429.", "citeRegEx": "Clarke and Lapata.,? 2008", "shortCiteRegEx": "Clarke and Lapata.", "year": 2008}, {"title": "Sentence compression beyond word deletion", "author": ["Trevor Cohn", "Mirella Lapata."], "venue": "Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pages 137\u2013144. Association for Computational Linguistics.", "citeRegEx": "Cohn and Lapata.,? 2008", "shortCiteRegEx": "Cohn and Lapata.", "year": 2008}, {"title": "A noisychannel model for document compression", "author": ["Hal Daum\u00e9 III", "Daniel Marcu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 449\u2013456. Association for Computational Linguistics.", "citeRegEx": "III and Marcu.,? 2002", "shortCiteRegEx": "III and Marcu.", "year": 2002}, {"title": "Hedge trimmer: A parse-and-trim approach to headline generation", "author": ["Bonnie Dorr", "David Zajic", "Richard Schwartz."], "venue": "Proceedings of the HLTNAACL 03 on Text summarization workshop-Volume 5, pages 1\u20138. Association for Computational Lin-", "citeRegEx": "Dorr et al\\.,? 2003", "shortCiteRegEx": "Dorr et al\\.", "year": 2003}, {"title": "Overcoming the lack of parallel data in sentence compression", "author": ["Katja Filippova", "Yasemin Altun."], "venue": "EMNLP, pages 1481\u20131491.", "citeRegEx": "Filippova and Altun.,? 2013", "shortCiteRegEx": "Filippova and Altun.", "year": 2013}, {"title": "English gigaword", "author": ["David Graff", "Junbo Kong", "Ke Chen", "Kazuaki Maeda."], "venue": "Linguistic Data Consortium, Philadelphia.", "citeRegEx": "Graff et al\\.,? 2003", "shortCiteRegEx": "Graff et al\\.", "year": 2003}, {"title": "Improving neural networks by preventing coadaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov."], "venue": "arXiv preprint arXiv:1207.0580.", "citeRegEx": "Hinton et al\\.,? 2012", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Using hidden markov modeling to decompose human-written summaries", "author": ["Hongyan Jing."], "venue": "Computational linguistics, 28(4):527\u2013543.", "citeRegEx": "Jing.,? 2002", "shortCiteRegEx": "Jing.", "year": 2002}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "EMNLP, pages 1700\u20131709.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Summarization beyond sentence extraction: A probabilistic approach to sentence compression", "author": ["Kevin Knight", "Daniel Marcu."], "venue": "Artificial Intelligence, 139(1):91\u2013107.", "citeRegEx": "Knight and Marcu.,? 2002", "shortCiteRegEx": "Knight and Marcu.", "year": 2002}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin."], "venue": "Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74\u201381.", "citeRegEx": "Lin.,? 2004", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc V Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "arXiv preprint arXiv:1410.8206.", "citeRegEx": "Luong et al\\.,? 2014", "shortCiteRegEx": "Luong et al\\.", "year": 2014}, {"title": "Introduction to information retrieval, volume 1", "author": ["Christopher D Manning", "Prabhakar Raghavan", "Hinrich Sch\u00fctze."], "venue": "Cambridge university press Cambridge.", "citeRegEx": "Manning et al\\.,? 2008", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "The stanford corenlp natural language processing toolkit", "author": ["Christopher D Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J Bethard", "David McClosky."], "venue": "Proceedings of 52nd Annual Meeting of the Association for Computa-", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Empirical evaluation and combination of advanced language modeling techniques", "author": ["Tomas Mikolov", "Anoop Deoras", "Stefan Kombrink", "Lukas Burget", "Jan Cernock\u1ef3."], "venue": "INTERSPEECH, number s 1, pages 605\u2013608.", "citeRegEx": "Mikolov et al\\.,? 2011", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Frederic Morin", "Yoshua Bengio."], "venue": "Proceedings of the international workshop on artificial intelligence and statistics, pages 246\u2013252. Citeseer.", "citeRegEx": "Morin and Bengio.,? 2005", "shortCiteRegEx": "Morin and Bengio.", "year": 2005}, {"title": "Annotated gigaword", "author": ["Courtney Napoles", "Matthew Gormley", "Benjamin Van Durme."], "venue": "Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, pages 95\u2013100. Association for Compu-", "citeRegEx": "Napoles et al\\.,? 2012", "shortCiteRegEx": "Napoles et al\\.", "year": 2012}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och."], "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 160\u2013167. Association for Computational Linguistics.", "citeRegEx": "Och.,? 2003", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Duc in context", "author": ["Paul Over", "Hoa Dang", "Donna Harman."], "venue": "Information Processing & Management, 43(6):1506\u20131520.", "citeRegEx": "Over et al\\.,? 2007", "shortCiteRegEx": "Over et al\\.", "year": 2007}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."], "venue": "Advances in Neural Information Processing Systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Generation with quasi-synchronous grammar", "author": ["Kristian Woodsend", "Yansong Feng", "Mirella Lapata."], "venue": "Proceedings of the 2010 conference on empirical methods in natural language processing, pages 513\u2013 523. Association for Computational Linguistics.", "citeRegEx": "Woodsend et al\\.,? 2010", "shortCiteRegEx": "Woodsend et al\\.", "year": 2010}, {"title": "Sentence simplification by monolingual machine translation", "author": ["Sander Wubben", "Antal Van Den Bosch", "Emiel Krahmer."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages", "citeRegEx": "Wubben et al\\.,? 2012", "shortCiteRegEx": "Wubben et al\\.", "year": 2012}, {"title": "Z-mert: A fully configurable open source tool for minimum error rate training of machine translation systems", "author": ["Omar Zaidan."], "venue": "The Prague Bulletin of Mathematical Linguistics, 91:79\u201388.", "citeRegEx": "Zaidan.,? 2009", "shortCiteRegEx": "Zaidan.", "year": 2009}, {"title": "Bbn/umd at duc-2004: Topiary", "author": ["David Zajic", "Bonnie Dorr", "Richard Schwartz."], "venue": "Proceedings of the HLT-NAACL 2004 Document Understanding Workshop, Boston, pages 112\u2013119.", "citeRegEx": "Zajic et al\\.,? 2004", "shortCiteRegEx": "Zajic et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 11, "context": "While much work on this task has looked at deletion-based sentence compression techniques (Knight and Marcu (2002), among many others), studies of human summarizers show that it is common to apply various other operations while condensing, such as paraphrasing, generalization, and reordering (Jing, 2002).", "startOffset": 293, "endOffset": 305}, {"referenceID": 7, "context": "Past work has modeled this abstractive summarization problem either using linguistically-inspired constraints (Dorr et al., 2003; Zajic et al., 2004) or with syntactic transformations of the input text (Cohn and Figure 1: Example output of the attention-based summarization (ABS) system.", "startOffset": 110, "endOffset": 149}, {"referenceID": 28, "context": "Past work has modeled this abstractive summarization problem either using linguistically-inspired constraints (Dorr et al., 2003; Zajic et al., 2004) or with syntactic transformations of the input text (Cohn and Figure 1: Example output of the attention-based summarization (ABS) system.", "startOffset": 110, "endOffset": 149}, {"referenceID": 11, "context": "While much work on this task has looked at deletion-based sentence compression techniques (Knight and Marcu (2002), among many others), studies of human summarizers show that it is common to apply various other operations while condensing, such as paraphrasing, generalization, and reordering (Jing, 2002).", "startOffset": 91, "endOffset": 115}, {"referenceID": 0, "context": "Our encoder is modeled off of the attention-based encoder of Bahdanau et al. (2014) in that it learns a latent soft alignment over the input text to help inform the summary (as shown in Figure 1).", "startOffset": 61, "endOffset": 84}, {"referenceID": 11, "context": "Note that the summary generated is abstractive which makes it possible to generalize (russian defense minister to russia) and paraphrase (for combating to against), in addition to compressing (dropping the creation of), see Jing (2002) for a survey of these editing operations.", "startOffset": 224, "endOffset": 236}, {"referenceID": 9, "context": "1 This allows us to train a summarization model for headline-generation on a corpus of article pairs from Gigaword (Graff et al., 2003) consisting of around 4 million articles.", "startOffset": 115, "endOffset": 135}, {"referenceID": 8, "context": "In contrast to a large-scale sentence compression systems like Filippova and Altun (2013) which require monotonic aligned compressions.", "startOffset": 63, "endOffset": 90}, {"referenceID": 1, "context": "Past work on summarization and compression has used a noisy-channel approach to split and independently estimate a language model and a conditional summarization model (Banko et al., 2000; Knight and Marcu, 2002; Daum\u00e9 III and Marcu, 2002), i.", "startOffset": 168, "endOffset": 239}, {"referenceID": 13, "context": "Past work on summarization and compression has used a noisy-channel approach to split and independently estimate a language model and a conditional summarization model (Banko et al., 2000; Knight and Marcu, 2002; Daum\u00e9 III and Marcu, 2002), i.", "startOffset": 168, "endOffset": 239}, {"referenceID": 2, "context": "The language model is adapted from a standard feed-forward neural network language model (NNLM), particularly the class of NNLMs described by Bengio et al. (2003). The full model is:", "startOffset": 142, "endOffset": 163}, {"referenceID": 0, "context": "A similar issue in machine translation inspired Bahdanau et al. (2014) to instead utilize an attention-based contextual encoder that constructs a representation based on the generation context.", "startOffset": 48, "endOffset": 71}, {"referenceID": 0, "context": "To be explicit, compared to Bahdanau et al. (2014) our model uses an NNLM instead of a target-side LSTM, source-side windowed averaging instead of a source-side bidirectional RNN, and a weighted dot-product for alignment instead of an alignment MLP.", "startOffset": 28, "endOffset": 51}, {"referenceID": 24, "context": "While decoders of this form can produce very bad approximations, they have shown to be relatively effective and fast for neural MT models (Sutskever et al., 2014).", "startOffset": 138, "endOffset": 162}, {"referenceID": 16, "context": "Similar issues have also been observed in neural translation models particularly in terms of translating rare words (Luong et al., 2014).", "startOffset": 116, "endOffset": 136}, {"referenceID": 22, "context": "We follow the statistical machine translation setup and use minimumerror rate training (MERT) to tune for the summarization metric on tuning data (Och, 2003).", "startOffset": 146, "endOffset": 157}, {"referenceID": 1, "context": "Our work is similar to early work of Banko et al. (2000) who developed a statistical machine translation-inspired approach for this task using a corpus of headline-article pairs.", "startOffset": 37, "endOffset": 57}, {"referenceID": 23, "context": "This task was standardized around the DUC2003 and DUC-2004 competitions (Over et al., 2007).", "startOffset": 72, "endOffset": 91}, {"referenceID": 28, "context": "The TOPIARY system (Zajic et al., 2004) performed the best in this task, and is described in detail in the next section.", "startOffset": 19, "endOffset": 39}, {"referenceID": 5, "context": "More recently, Cohn and Lapata (2008) give a compression method which allows for more arbitrary transformations.", "startOffset": 15, "endOffset": 38}, {"referenceID": 25, "context": "Woodsend et al. (2010) propose a quasi-synchronous grammar approach utilizing both context-free parses and dependency parses to produce legible summaries.", "startOffset": 0, "endOffset": 23}, {"referenceID": 1, "context": "In addition to Banko et al. (2000) there has been some work using statistical machine translation directly for abstractive summary.", "startOffset": 15, "endOffset": 35}, {"referenceID": 1, "context": "In addition to Banko et al. (2000) there has been some work using statistical machine translation directly for abstractive summary. Wubben et al. (2012) utilize MOSES directly as a method for text simplification.", "startOffset": 15, "endOffset": 153}, {"referenceID": 8, "context": "Recently Filippova and Altun (2013) developed a strictly extractive system that is trained on a relatively large corpora (250K sentences) of articletitle pairs.", "startOffset": 9, "endOffset": 36}, {"referenceID": 2, "context": "The core of our model is a NNLM based on that of Bengio et al. (2003).", "startOffset": 49, "endOffset": 70}, {"referenceID": 12, "context": "Recently, there have been several papers about models for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014).", "startOffset": 78, "endOffset": 152}, {"referenceID": 3, "context": "Recently, there have been several papers about models for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014).", "startOffset": 78, "endOffset": 152}, {"referenceID": 24, "context": "Recently, there have been several papers about models for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014).", "startOffset": 78, "endOffset": 152}, {"referenceID": 0, "context": "Of these our model is most closely related to the attention-based model of Bahdanau et al. (2014), which explicitly finds a soft alignment between the current position and the input source.", "startOffset": 75, "endOffset": 98}, {"referenceID": 23, "context": "The standard sentence summarization evaluation set is associated with the DUC-2003 and DUC2004 shared tasks (Over et al., 2007).", "startOffset": 108, "endOffset": 127}, {"referenceID": 15, "context": "For this shared task, systems were entered and evaluated using several variants of the recalloriented ROUGE metric (Lin, 2004).", "startOffset": 115, "endOffset": 126}, {"referenceID": 9, "context": "For training data for both tasks, we utilize the annotated Gigaword data set (Graff et al., 2003; Napoles et al., 2012), which consists of standard Gigaword, preprocessed with Stanford CoreNLP tools (Manning et al.", "startOffset": 77, "endOffset": 119}, {"referenceID": 21, "context": "For training data for both tasks, we utilize the annotated Gigaword data set (Graff et al., 2003; Napoles et al., 2012), which consists of standard Gigaword, preprocessed with Stanford CoreNLP tools (Manning et al.", "startOffset": 77, "endOffset": 119}, {"referenceID": 18, "context": ", 2012), which consists of standard Gigaword, preprocessed with Stanford CoreNLP tools (Manning et al., 2014).", "startOffset": 87, "endOffset": 109}, {"referenceID": 28, "context": "We also report the winning system on this shared task, TOPIARY (Zajic et al., 2004).", "startOffset": 63, "endOffset": 83}, {"referenceID": 7, "context": "TOPIARY merges a compression system using linguisticallymotivated transformations of the input (Dorr et al., 2003) with an unsupervised topic detection (UTD) algorithm that appends key phrases from the full article onto the compressed output.", "startOffset": 95, "endOffset": 114}, {"referenceID": 7, "context": "TOPIARY merges a compression system using linguisticallymotivated transformations of the input (Dorr et al., 2003) with an unsupervised topic detection (UTD) algorithm that appends key phrases from the full article onto the compressed output. Woodsend et al. (2010) (described above) also report results on the DUC dataset.", "startOffset": 96, "endOffset": 266}, {"referenceID": 4, "context": "The first is a sentence compression baseline COMPRESS (Clarke and Lapata, 2008).", "startOffset": 54, "endOffset": 79}, {"referenceID": 15, "context": "To control for memorizing titles from training, we implement an information retrieval baseline, IR. This baseline indexes the training set, and gives the title for the article with highest BM-25 match to the input (see Manning et al. (2008)).", "startOffset": 90, "endOffset": 241}, {"referenceID": 14, "context": "Finally, we use a phrase-based statistical machine translation system trained on Gigaword to produce summaries, MOSES+ (Koehn et al., 2007).", "startOffset": 119, "endOffset": 139}, {"referenceID": 10, "context": "After each epoch, we renormalize the embedding tables (Hinton et al., 2012).", "startOffset": 54, "endOffset": 75}, {"referenceID": 27, "context": "For this step we use Z-MERT (Zaidan, 2009).", "startOffset": 28, "endOffset": 42}], "year": 2017, "abstractText": "Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines.", "creator": "TeX"}}}