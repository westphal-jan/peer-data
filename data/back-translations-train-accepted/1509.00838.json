{"id": "1509.00838", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Sep-2015", "title": "What to talk about and how? Selective Generation using LSTMs with Coarse-to-Fine Alignment", "abstract": "We present an end-to-end, domain-independent neural encoder-aligner-decoder model for selective generation, i.e., the joint task of content selection and surface realization. Our model first encodes the full set of over-determined database event records (e.g., in weather forecasting and sportscasting) via a memory-based recurrent neural network (LSTM), then utilizes a novel coarse-to-fine (hierarchical), multi-input aligner to identify the small subset of salient records to talk about, and finally employs a decoder to generate free-form descriptions of the aligned, selected records. Our model achieves up to 54% relative improvement over the current state-of-the-art on the benchmark WeatherGov dataset, despite using no specialized features or resources. Using a simple k-nearest neighbor beam helps further. Finally, we also demonstrate the generalizability of our method on the RoboCup dataset, where it gets results that are competitive with state-of-the-art, despite being severely data-starved.", "histories": [["v1", "Wed, 2 Sep 2015 19:52:56 GMT  (205kb,D)", "https://arxiv.org/abs/1509.00838v1", null], ["v2", "Fri, 8 Jan 2016 23:07:32 GMT  (203kb,D)", "http://arxiv.org/abs/1509.00838v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG cs.NE", "authors": ["hongyuan mei", "mohit bansal", "matthew r walter"], "accepted": true, "id": "1509.00838"}, "pdf": {"name": "1509.00838.pdf", "metadata": {"source": "META", "title": "What to talk about and how? Selective Generation using LSTMs with Coarse-to-Fine Alignment", "authors": ["Hongyuan Mei", "Mohit Bansal", "Matthew R. Walter"], "emails": ["hongyuan@ttic.edu", "mbansal@ttic.edu", "mwalter@ttic.edu"], "sections": [{"heading": null, "text": "1 What to talk about and how? Selective generation using LSTMs with coarse to fine alignment"}, {"heading": "1 Introduction", "text": "In fact, most of them will be able to abide by the rules that they have imposed on themselves, and they will be able to understand the rules that they have imposed on themselves, \"he told the Deutsche Presse-Agentur."}, {"heading": "2 Related Work", "text": "In fact, most of them will be able to put themselves centre stage without having to put themselves centre stage."}, {"heading": "3 Task Definition", "text": "We look at the problem of generating a natural language description for a rich world state, specified on the basis of an overdetermined set of records (database), which requires deciding which of the records to discuss (selection of contents) and how to discuss them (surface realization) Training data consists of scenario pairs (r (i), x (i) for i = 1, 2,.., n, with r (i) being the complete set of records and x (i) the natural language description (Fig. 1). At the test date, only the records are given. We evaluate our model in the context of two publicly available selective benchmark generation datasets. WEATHERGOV The weather forecast dataset (see Fig. 1 (a) by Liang et al. (2009) consists of 29,528 scenarios, each with 36 weather records (e.g. temperature, sky ceiling, etc.) paired with a natural language prediction of the year 2001 (Fig. 1 (a) of 28,7)."}, {"heading": "4 The Model", "text": "We formulate a selective generation as a conclusion about a probabilistic model in machine translation (Bahdanal 2014). We formulate a selective generation as a conclusion about a probable model P (x1: T).N (x1: N, x2,.), where we use the generated description with the word T (x0: 1, r1: N).N The aim of the inference is to generate a natural language description for a given set of records. An effective means to perform this generation is to use an encoder-aligner decoder architecture with a recurrent neural network that has proven itself for related problems in machine translation (Bahdanal 2014)."}, {"heading": "5 Experimental Setup", "text": "Datasets We analyze our model on the benchmark WEATHERGOV dataset, and use the data-hungry ROBOCUP datasets to demonstrate the generalizability of the model. Following Angeli et al. (2010), we use WEATHERGOV training, development and test splits of size 25000, 1000 and 3528, respectively. For ROBOCUP, we follow the evaluation methodology of previous work (Chen and Mooney, 2008), performing triple cross-validation, taking us on three games (about 1000 scenarios) and testing on the fourth. Within each split, we keep 10% of the training data, as the development set, to match the early-stop criterion and \u03b3. We then report the average performance (weighted by the number of scenarios) on these four splits."}, {"heading": "6 Results and Analysis", "text": "We analyze the effectiveness of our model using the benchmark data sets WEATHERGOV (as primary model) and ROBOCUP (as generalization) and present several ablations to illustrate the contributions of the primary model components. 2We implement our model in Theano (Bergstra et al., 2010; Bastien et al., 2012) and will make the code publicly available. 3We calculate BLEU based on the publicly available evaluation by Angeli et al. (2010).7"}, {"heading": "6.1 Primary Results (WEATHERGOV)", "text": "In Table 1, our test results are compared with previous methods, including KL12 (Konstas and Lapata, 2012), KL13 (Konstas and Lapata, 2013) and ALK10 (Angeli et al., 2010). Our method achieves the best results of all three measures to date, with relative improvements of 11.94% (F-1), 58.88% (sBLEU) and 36.68% (cBLEU) over the prior art. 6.2 Beam filters with k closest neighbors We considered beam hunting as an alternative to greedy searching in our primary system (Eqn. 1), but this performs worse, similar to previous work on this dataset (Angeli et al., 2010)."}, {"heading": "6.3 Ablation Analysis (WEATHERGOV)", "text": "Next, we present several ablations to analyze the contribution of our model components.4 Aligner ablation First, we evaluate the contribution of our proposed coarse-to-fine aligner by comparing our model with the basic encoder-aligner decoder model introduced by Bahdanau et al. 4. These results are based on our primary model of paragraph 6.1 and the development set (2014).Table 3 shows the results showing that our aligner yields better F-1 and BLEU values compared to a standard alignment. Table 3: Coarse-to-fine aligner ablation (dev-set) method F-1 sBLEU cBLEU Basic 60.35 63.54 74.90 Coarse-to-fine alignment 76.28 65.58 75.78 Encoder ablation Next, we look at the effectiveness of the encoder. Table 4 compares the results with and without the encoder development group and shows that there is a significant gain from the coding of this MR events under the use of the STR coding events."}, {"heading": "6.4 Qualitative Analysis (WEATHERGOV)", "text": "As shown, our model learns to align data sets with their corresponding words (e.g. windDir and \"southeast,\" temperature and \"71,\" wind speed and \"wind 10\" and gusts and gusts up to 30 km / h \"); it also learns the subset of distinctive data sets that can be spoken about (which we perfectly match to the soil-truth description for this example, i.e. a standard BLEU of 100.00); we also see some word-like mismatches, e.g.\" cloudy \"mismatches that are aligned to id-0 temperature and id-10 precipitation, which we attribute to the high correlation between these types of data sets (\" Garbage Collection \"in Liang et al. (2009)); Word Embeddings Training our decoder has the effect of learning embedding for the words in the training set (see the most common ones in HERE)."}, {"heading": "6.5 Out-of-Domain Results (ROBOCUP)", "text": "We use the ROBOCUP dataset to evaluate the domain independence of our model, which is strongly data hungry with only 1000 (approx.) training pairs that are much smaller than is normally necessary to train RNNs. This results in higher variance in the trained model distributions, and we therefore use the standard denocialization method of ensembles (Sutskever et al., 2014; Vinyals et al., 2015b; Zaremba et al., 2014).5 5We use an ensemble of five randomly initialized models. Table 6: ROBOCUP Results Method F-1 sBLEUG CM08 72.00 - 28.70 LJK09 75.70 - - CKM10 79.30 - - ALK10 79.90 - 28.80 KL12 - 24.88 30.90 Our model 81.58 25.28 29.40 Following previous work, we perform two experiments on the ROCUBOP second generation, with the first result of the first BLUP (25.6)."}, {"heading": "7 Conclusion", "text": "We presented an encoder-aligner decoder model for selective generation that does not use specific features, linguistic resources, or generations.6The result of Chen and Mooney (2008) sBLEUG comes from Angeli et al. (2010).9tion templates. Our model uses a bidirectional LSTM-RNN model with a novel coarse-to-fine balance that jointly learns the selection of content and the realization of surfaces. We evaluate our model using the WEATHERGOV benchmark dataset and obtain state-of-the-art selection and generation results. Further improvements are achieved through a k-next beam filter. We also present several model ablations and visualizations to shed the effects of the primary components of our model. In addition, our model generalizes to another data-hungry domain (ROBOCUP), where it achieves results that are competitive or better with the state of the art."}, {"heading": "Acknowledgments", "text": "We thank Gabor Angeli, David Chen and Ioannis Konstas for their helpful comments."}, {"heading": "A Supplementary Material", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}], "references": [{"title": "A simple domain-independent probabilistic approach to generation", "author": ["Gabor Angeli", "Percy Liang", "Dan Klein."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 502\u2013512.", "citeRegEx": "Angeli et al\\.,? 2010", "shortCiteRegEx": "Angeli et al\\.", "year": 2010}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Collective content selection for concept-to-text generation", "author": ["Regina Barzilay", "Mirella Lapata."], "venue": "Proceedings of the Human Language Technology Conference and the Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP), pages", "citeRegEx": "Barzilay and Lapata.,? 2005", "shortCiteRegEx": "Barzilay and Lapata.", "year": 2005}, {"title": "Catching the drift: Probabilistic content models, with applications to generation and summarization", "author": ["Regina Barzilay", "Lillian Lee."], "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics Human", "citeRegEx": "Barzilay and Lee.,? 2004", "shortCiteRegEx": "Barzilay and Lee.", "year": 2004}, {"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio."], "venue": "NIPS Workshop on Deep Learning and Unsupervised Fea-", "citeRegEx": "Bastien et al\\.,? 2012", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models", "author": ["Anja Belz."], "venue": "Natural Language Engineering, 14(04):431\u2013455.", "citeRegEx": "Belz.,? 2008", "shortCiteRegEx": "Belz.", "year": 2008}, {"title": "Theano: a CPU and GPU math expression", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Learning to sportscast: a test of grounded language acquisition", "author": ["David L. Chen", "Raymond J. Mooney."], "venue": "Proceedings of the International Conference on Machine Learning (ICML), pages 128\u2013135.", "citeRegEx": "Chen and Mooney.,? 2008", "shortCiteRegEx": "Chen and Mooney.", "year": 2008}, {"title": "Training a multilingual sportscaster: Using perceptual context to learn language", "author": ["David L. Chen", "Joohyun Kim", "Raymond J. Mooney."], "venue": "Journal of Artificial Intelligence Research, 37:397\u2013435.", "citeRegEx": "Chen et al\\.,? 2010", "shortCiteRegEx": "Chen et al\\.", "year": 2010}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Mohamed Abdel-rahman", "Geoffrey Hinton."], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6645\u20136649.", "citeRegEx": "Graves et al\\.,? 2013", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Deep visualsemantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3128\u20133137.", "citeRegEx": "Karpathy and Fei.Fei.,? 2015", "shortCiteRegEx": "Karpathy and Fei.Fei.", "year": 2015}, {"title": "Generative alignment and semantic parsing for learning from ambiguous supervision", "author": ["Joohyun Kim", "Raymond J Mooney."], "venue": "Proceedings of the International Conference on Computational Linguistics (COLING), pages 543\u2013551.", "citeRegEx": "Kim and Mooney.,? 2010", "shortCiteRegEx": "Kim and Mooney.", "year": 2010}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Unsupervised concept-to-text generation with hypergraphs", "author": ["Ioannis Konstas", "Mirella Lapata."], "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL HLT),", "citeRegEx": "Konstas and Lapata.,? 2012", "shortCiteRegEx": "Konstas and Lapata.", "year": 2012}, {"title": "Inducing document plans for concept-to-text generation", "author": ["Ioannis Konstas", "Mirella Lapata."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1503\u2013 1514.", "citeRegEx": "Konstas and Lapata.,? 2013", "shortCiteRegEx": "Konstas and Lapata.", "year": 2013}, {"title": "Learning semantic correspondences with less supervision", "author": ["Percy Liang", "Michael I. Jordan", "Dan Klein."], "venue": "Proceedings of the Joint Conference of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on", "citeRegEx": "Liang et al\\.,? 2009", "shortCiteRegEx": "Liang et al\\.", "year": 2009}, {"title": "A probabilistic forestto-string model for language generation from typed lambda calculus expressions", "author": ["Wei Lu", "Hwee Tou Ng."], "venue": "Proceedings of the", "citeRegEx": "Lu and Ng.,? 2011", "shortCiteRegEx": "Lu and Ng.", "year": 2011}, {"title": "A generative model for parsing natural language to meaning representations", "author": ["Wei Lu", "Hwee Tou Ng", "Wee Sun Lee", "Luke S Zettlemoyer."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 783\u2013792.", "citeRegEx": "Lu et al\\.,? 2008", "shortCiteRegEx": "Lu et al\\.", "year": 2008}, {"title": "Natural language generation with tree conditional random fields", "author": ["Wei Lu", "Hwee Tou Ng", "Wee Sun Lee."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 400\u2013409.", "citeRegEx": "Lu et al\\.,? 2009", "shortCiteRegEx": "Lu et al\\.", "year": 2009}, {"title": "Listen, attend, and walk: Neural mapping of navigational instructions to action sequences", "author": ["Hongyuan Mei", "Mohit Bansal", "Matthew R. Walter."], "venue": "arXiv preprint arXiv:1506.04089.", "citeRegEx": "Mei et al\\.,? 2015", "shortCiteRegEx": "Mei et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2001", "shortCiteRegEx": "Papineni et al\\.", "year": 2001}, {"title": "How to construct deep recurrent neural networks", "author": ["Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Pascanu et al\\.,? 2014", "shortCiteRegEx": "Pascanu et al\\.", "year": 2014}, {"title": "Stochastic language generation using WIDL-expressions and its application in machine translation and summarization", "author": ["Radu Soricut", "Daniel Marcu."], "venue": "Proceedings of the International Conference on", "citeRegEx": "Soricut and Marcu.,? 2006", "shortCiteRegEx": "Soricut and Marcu.", "year": 2006}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Lee."], "venue": "Advances in Neural Information Processing Systems (NIPS).", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "arXiv preprint arXiv:1412.7449.", "citeRegEx": "Vinyals et al\\.,? 2014", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Order matters: Sequence to sequence for sets", "author": ["Oriol Vinyals", "Samy Bengio", "Manjunath Kudlur."], "venue": "arXiv preprint arXiv:1511.06391.", "citeRegEx": "Vinyals et al\\.,? 2015a", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3156\u20133164.", "citeRegEx": "Vinyals et al\\.,? 2015b", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Generation by inverting a semantic parser that uses statistical machine translation", "author": ["Yuk Wah Wong", "Raymond J Mooney."], "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics Human Language", "citeRegEx": "Wong and Mooney.,? 2007", "shortCiteRegEx": "Wong and Mooney.", "year": 2007}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio."], "venue": "Proceedings of the International Confer-", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."], "venue": "arXiv preprint arXiv:1409.2329.", "citeRegEx": "Zaremba et al\\.,? 2014", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 7, "context": "Previous work has made significant progress on this task (Chen and Mooney, 2008; Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2012).", "startOffset": 57, "endOffset": 149}, {"referenceID": 0, "context": "Previous work has made significant progress on this task (Chen and Mooney, 2008; Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2012).", "startOffset": 57, "endOffset": 149}, {"referenceID": 12, "context": "Previous work has made significant progress on this task (Chen and Mooney, 2008; Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2012).", "startOffset": 57, "endOffset": 149}, {"referenceID": 14, "context": "Previous work has made significant progress on this task (Chen and Mooney, 2008; Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2012).", "startOffset": 57, "endOffset": 149}, {"referenceID": 0, "context": "Further, our memorybased model captures the long-range contextual dependencies among records and descriptions, which are integral to this task (Angeli et al., 2010).", "startOffset": 143, "endOffset": 164}, {"referenceID": 10, "context": "We formulate our model as an encoder-alignerdecoder framework that uses recurrent neural networks with long short-term memory units (LSTMRNNs) (Hochreiter and Schmidhuber, 1997) together with a coarse-to-fine aligner to select and \u201ctranslate\u201d the rich world state into a natural language description.", "startOffset": 143, "endOffset": 177}, {"referenceID": 25, "context": "The use of LSTMs, which have proven effective for similar long-range generation tasks (Sutskever et al., 2014; Vinyals et al., 2015b; Karpathy and FeiFei, 2015), allows our model to capture the longrange contextual dependencies that exist in selective generation.", "startOffset": 86, "endOffset": 160}, {"referenceID": 28, "context": "The use of LSTMs, which have proven effective for similar long-range generation tasks (Sutskever et al., 2014; Vinyals et al., 2015b; Karpathy and FeiFei, 2015), allows our model to capture the longrange contextual dependencies that exist in selective generation.", "startOffset": 86, "endOffset": 160}, {"referenceID": 1, "context": "Further, the introduction of our proposed variation on alignment-based LSTMs (Bahdanau et al., 2014; Xu et al., 2015) enables our model to learn to perform content selection and surface realization jointly, by aligning each generated word to an event record during decoding.", "startOffset": 77, "endOffset": 117}, {"referenceID": 30, "context": "Further, the introduction of our proposed variation on alignment-based LSTMs (Bahdanau et al., 2014; Xu et al., 2015) enables our model to learn to perform content selection and surface realization jointly, by aligning each generated word to an event record during decoding.", "startOffset": 77, "endOffset": 117}, {"referenceID": 2, "context": "With regards to the former, Barzilay and Lee (2004) model the content structure from unannotated documents and apply it to the application of text summarization.", "startOffset": 28, "endOffset": 52}, {"referenceID": 2, "context": "Barzilay and Lapata (2005) treat content selection as a collective classification problem and simultaneously optimize the local label assignment and their pairwise relations.", "startOffset": 0, "endOffset": 27}, {"referenceID": 2, "context": "Barzilay and Lapata (2005) treat content selection as a collective classification problem and simultaneously optimize the local label assignment and their pairwise relations. Liang et al. (2009) address the related task of aligning a set of records to given textual description clauses.", "startOffset": 0, "endOffset": 195}, {"referenceID": 19, "context": "Other effective approaches include the use of tree conditional random fields (Lu et al., 2009) and template extraction within a log-linear framework (Angeli et al.", "startOffset": 77, "endOffset": 94}, {"referenceID": 0, "context": ", 2009) and template extraction within a log-linear framework (Angeli et al., 2010).", "startOffset": 62, "endOffset": 83}, {"referenceID": 19, "context": "Soricut and Marcu (2006) propose a language generation system that uses the WIDL-representation, a formalism used to compactly represent probability distributions over finite sets of strings.", "startOffset": 0, "endOffset": 25}, {"referenceID": 19, "context": "Soricut and Marcu (2006) propose a language generation system that uses the WIDL-representation, a formalism used to compactly represent probability distributions over finite sets of strings. Wong and Mooney (2007) and Lu and Ng (2011) use synchronous context-free grammars to generate natural language sentences from formal meaning representations.", "startOffset": 0, "endOffset": 215}, {"referenceID": 15, "context": "Wong and Mooney (2007) and Lu and Ng (2011) use synchronous context-free grammars to generate natural language sentences from formal meaning representations.", "startOffset": 27, "endOffset": 44}, {"referenceID": 4, "context": "Similarly, Belz (2008) employs probabilistic context-free grammars to perform surface realization.", "startOffset": 11, "endOffset": 23}, {"referenceID": 7, "context": "Chen and Mooney (2008) and Chen et al.", "startOffset": 0, "endOffset": 23}, {"referenceID": 7, "context": "Chen and Mooney (2008) and Chen et al. (2010) learn alignments between comments and their corresponding event records using a translation model for parsing and generation.", "startOffset": 0, "endOffset": 46}, {"referenceID": 7, "context": "Chen and Mooney (2008) and Chen et al. (2010) learn alignments between comments and their corresponding event records using a translation model for parsing and generation. Kim and Mooney (2010) implement a two-stage framework that decides what to discuss using a combination of the methods of Lu et al.", "startOffset": 0, "endOffset": 194}, {"referenceID": 7, "context": "Chen and Mooney (2008) and Chen et al. (2010) learn alignments between comments and their corresponding event records using a translation model for parsing and generation. Kim and Mooney (2010) implement a two-stage framework that decides what to discuss using a combination of the methods of Lu et al. (2008) and Liang et al.", "startOffset": 0, "endOffset": 310}, {"referenceID": 7, "context": "Chen and Mooney (2008) and Chen et al. (2010) learn alignments between comments and their corresponding event records using a translation model for parsing and generation. Kim and Mooney (2010) implement a two-stage framework that decides what to discuss using a combination of the methods of Lu et al. (2008) and Liang et al. (2009), and then produces the text based on the generation system of Wong and Mooney (2007).", "startOffset": 0, "endOffset": 334}, {"referenceID": 7, "context": "Chen and Mooney (2008) and Chen et al. (2010) learn alignments between comments and their corresponding event records using a translation model for parsing and generation. Kim and Mooney (2010) implement a two-stage framework that decides what to discuss using a combination of the methods of Lu et al. (2008) and Liang et al. (2009), and then produces the text based on the generation system of Wong and Mooney (2007).", "startOffset": 0, "endOffset": 419}, {"referenceID": 1, "context": ", machine translation (Bahdanau et al., 2014) and image captioning (Xu et al.", "startOffset": 22, "endOffset": 45}, {"referenceID": 30, "context": ", 2014) and image captioning (Xu et al., 2015).", "startOffset": 29, "endOffset": 46}, {"referenceID": 16, "context": "1(a)) of Liang et al. (2009) consists of 29528 scenarios, each with 36 weather records (e.", "startOffset": 9, "endOffset": 29}, {"referenceID": 7, "context": "ROBOCUP We evaluate our model\u2019s generalizability on the sportscasting dataset of Chen and Mooney (2008), which consists of only 1539 pairs of temporally ordered robot soccer events (e.", "startOffset": 81, "endOffset": 104}, {"referenceID": 1, "context": "An effective means of learning to perform this generation is to use an encoder-aligner-decoder architecture with a recurrent neural network, which has proven effective for related problems in machine translation (Bahdanau et al., 2014) and image captioning (Xu et al.", "startOffset": 212, "endOffset": 235}, {"referenceID": 30, "context": ", 2014) and image captioning (Xu et al., 2015).", "startOffset": 29, "endOffset": 46}, {"referenceID": 27, "context": "We note that it is possible that a different ordering will yield improved performance, since ordering has been shown to be important when operating on sets (Vinyals et al., 2015a).", "startOffset": 156, "endOffset": 179}, {"referenceID": 0, "context": "In order to model the long-range dependencies among the records and descriptions (which is integral to effectively performing selective generation (Angeli et al., 2010; Konstas and Lapata, 2012; Konstas and Lapata, 2013)), our model employs LSTM units as the nonlinear encoder and decoder functions.", "startOffset": 147, "endOffset": 220}, {"referenceID": 14, "context": "In order to model the long-range dependencies among the records and descriptions (which is integral to effectively performing selective generation (Angeli et al., 2010; Konstas and Lapata, 2012; Konstas and Lapata, 2013)), our model employs LSTM units as the nonlinear encoder and decoder functions.", "startOffset": 147, "endOffset": 220}, {"referenceID": 15, "context": "In order to model the long-range dependencies among the records and descriptions (which is integral to effectively performing selective generation (Angeli et al., 2010; Konstas and Lapata, 2012; Konstas and Lapata, 2013)), our model employs LSTM units as the nonlinear encoder and decoder functions.", "startOffset": 147, "endOffset": 220}, {"referenceID": 9, "context": "We adopt an encoder architecture similar to that of Graves et al. (2013) \uf8ec\uf8ec\uf8ed ij fe j oj ge j \uf8f7\uf8f7\uf8f8 = \uf8ec\uf8ec\uf8ed \u03c3 \u03c3", "startOffset": 52, "endOffset": 73}, {"referenceID": 20, "context": "Our model aligns based on multiple abstractions of the input: both the original input record as well as the hidden annotations mj = (r> j ;h > j ) >, an approach that has previously been shown to yield better results than aligning based only on the hidden state (Mei et al., 2015).", "startOffset": 262, "endOffset": 280}, {"referenceID": 1, "context": "Our model performs content selection using an extension of the alignment mechanism proposed by Bahdanau et al. (2014), which allows for selection and generation that is independent of the ordering of the input.", "startOffset": 95, "endOffset": 118}, {"referenceID": 23, "context": "The decoder outputs the conditional probability distribution Px,t = P (xt|x0:t\u22121, r1:N ) over the next word, represented as a deep output layer (Pascanu et al., 2014), \uf8ec\uf8ec\uf8ed it fd t ot gd t \uf8f7\uf8f7\uf8f8 = \uf8ec\uf8ec\uf8ed \u03c3 \u03c3", "startOffset": 144, "endOffset": 166}, {"referenceID": 1, "context": "0 for all j), and the coarse-to-fine alignment reverts to the standard alignment introduced by Bahdanau et al. (2014). Together with the negative loglikelihood of the ground-truth description x1:T , our loss function becomes", "startOffset": 95, "endOffset": 118}, {"referenceID": 0, "context": "Beam search offers a way to perform approximate joint inference \u2014 however, we empirically found that beam search does not perform any better than greedy search on the datasets that we consider, an observation that is shared with previous work (Angeli et al., 2010).", "startOffset": 243, "endOffset": 264}, {"referenceID": 7, "context": "For ROBOCUP, we follow the evaluation methodology of previous work (Chen and Mooney, 2008), performing three-fold cross-validation whereby we train on three games (approximately 1000 scenarios) and test on the fourth.", "startOffset": 67, "endOffset": 90}, {"referenceID": 0, "context": "Following Angeli et al. (2010), we use WEATHERGOV training, development, and test splits of size 25000, 1000, and 3528, respectively.", "startOffset": 10, "endOffset": 31}, {"referenceID": 13, "context": "For each iteration, we randomly sample a mini-batch of 100 scenarios during back-propagation and use Adam (Kingma and Ba, 2015) for optimization.", "startOffset": 106, "endOffset": 127}, {"referenceID": 22, "context": "We evaluate the quality of surface realization using the BLEU score3 (a 4-gram matching-based precision) (Papineni et al., 2001) of the generated description with respect to the human-created reference.", "startOffset": 105, "endOffset": 128}, {"referenceID": 0, "context": "To be comparable to previous results on WEATHERGOV, we also consider a modified BLEU score (cBLEU) that does not penalize numerical deviations of at most five (Angeli et al., 2010) (i.", "startOffset": 159, "endOffset": 180}, {"referenceID": 6, "context": "We implement our model in Theano (Bergstra et al., 2010; Bastien et al., 2012) and will make the code publicly available.", "startOffset": 33, "endOffset": 78}, {"referenceID": 4, "context": "We implement our model in Theano (Bergstra et al., 2010; Bastien et al., 2012) and will make the code publicly available.", "startOffset": 33, "endOffset": 78}, {"referenceID": 0, "context": "We compute BLEU using the publicly available evaluation provided by Angeli et al. (2010).", "startOffset": 68, "endOffset": 89}, {"referenceID": 14, "context": "Table 1 compares our test results against previous methods that include KL12 (Konstas and Lapata, 2012), KL13 (Konstas and Lapata, 2013), and ALK10 (Angeli et al.", "startOffset": 77, "endOffset": 103}, {"referenceID": 15, "context": "Table 1 compares our test results against previous methods that include KL12 (Konstas and Lapata, 2012), KL13 (Konstas and Lapata, 2013), and ALK10 (Angeli et al.", "startOffset": 110, "endOffset": 136}, {"referenceID": 0, "context": "Table 1 compares our test results against previous methods that include KL12 (Konstas and Lapata, 2012), KL13 (Konstas and Lapata, 2013), and ALK10 (Angeli et al., 2010).", "startOffset": 148, "endOffset": 169}, {"referenceID": 0, "context": "We report the performance of content selection and surface realization using F-1 and two BLEU scores (standard sBLEU and the customized cBLEU of Angeli et al. (2010)), respectively (Sec.", "startOffset": 145, "endOffset": 166}, {"referenceID": 0, "context": "1), but this performs worse, similar to what previous work found on this dataset (Angeli et al., 2010).", "startOffset": 81, "endOffset": 102}, {"referenceID": 2, "context": "We attribute this improvement to the LSTMRNN\u2019s ability to capture the relationships that exist among the records, which is known to be essential to selective generation (Barzilay and Lapata, 2005; Angeli et al., 2010).", "startOffset": 169, "endOffset": 217}, {"referenceID": 0, "context": "We attribute this improvement to the LSTMRNN\u2019s ability to capture the relationships that exist among the records, which is known to be essential to selective generation (Barzilay and Lapata, 2005; Angeli et al., 2010).", "startOffset": 169, "endOffset": 217}, {"referenceID": 16, "context": ", \u201ccloudy\u201d misaligns to id-0 temp and id-10 precipChance, which we attribute to the high correlation between these types of records (\u201cgarbage collection\u201d in Liang et al. (2009)).", "startOffset": 157, "endOffset": 177}, {"referenceID": 25, "context": "This results in higher variance in the trained model distributions, and we thus adopt the standard denoising method of ensembles (Sutskever et al., 2014; Vinyals et al., 2015b; Zaremba et al., 2014).", "startOffset": 129, "endOffset": 198}, {"referenceID": 28, "context": "This results in higher variance in the trained model distributions, and we thus adopt the standard denoising method of ensembles (Sutskever et al., 2014; Vinyals et al., 2015b; Zaremba et al., 2014).", "startOffset": 129, "endOffset": 198}, {"referenceID": 31, "context": "This results in higher variance in the trained model distributions, and we thus adopt the standard denoising method of ensembles (Sutskever et al., 2014; Vinyals et al., 2015b; Zaremba et al., 2014).", "startOffset": 129, "endOffset": 198}, {"referenceID": 14, "context": "88 (Konstas and Lapata, 2012).", "startOffset": 3, "endOffset": 29}, {"referenceID": 6, "context": "The Chen and Mooney (2008) sBLEUG result is from Angeli et al.", "startOffset": 4, "endOffset": 27}, {"referenceID": 0, "context": "The Chen and Mooney (2008) sBLEUG result is from Angeli et al. (2010).", "startOffset": 49, "endOffset": 70}, {"referenceID": 0, "context": "We considered beam search as an alternative, but as with previous work on this dataset (Angeli et al., 2010), we found that greedy search still yields better BLEU performance (Table 7).", "startOffset": 87, "endOffset": 108}, {"referenceID": 21, "context": "10 We also consider different ways of using pretrained word embeddings (Mikolov et al., 2013) to bootstrap the quality of our learned embeddings.", "startOffset": 71, "endOffset": 93}, {"referenceID": 26, "context": "As shown previously for other tasks (Vinyals et al., 2014; Vinyals et al., 2015b), we find that the use of pre-trained embeddings results in negligible improvements (on the development set).", "startOffset": 36, "endOffset": 81}, {"referenceID": 28, "context": "As shown previously for other tasks (Vinyals et al., 2014; Vinyals et al., 2015b), we find that the use of pre-trained embeddings results in negligible improvements (on the development set).", "startOffset": 36, "endOffset": 81}], "year": 2016, "abstractText": "We propose an end-to-end, domainindependent neural encoder-aligner-decoder model for selective generation, i.e., the joint task of content selection and surface realization. Our model first encodes a full set of over-determined database event records via an LSTM-based recurrent neural network, then utilizes a novel coarse-to-fine aligner to identify the small subset of salient records to talk about, and finally employs a decoder to generate free-form descriptions of the aligned, selected records. Our model achieves the best selection and generation results reported to-date (with 59% relative improvement in generation) on the benchmark WEATHERGOV dataset, despite using no specialized features or linguistic resources. Using an improved k-nearest neighbor beam filter helps further. We also perform a series of ablations and visualizations to elucidate the contributions of our key model components. Lastly, we evaluate the generalizability of our model on the ROBOCUP dataset, and get results that are competitive with or better than the state-of-the-art, despite being severely data-starved.", "creator": "LaTeX with hyperref package"}}}