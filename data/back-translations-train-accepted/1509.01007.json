{"id": "1509.01007", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Sep-2015", "title": "Encoding Prior Knowledge with Eigenword Embeddings", "abstract": "Canonical correlation analysis (CCA) is a method for reducing the dimension of data represented using two views. It has been previously used to derive word embeddings, where one view indicates a word, and the other view indicates its context. We describe a way to incorporate prior knowledge into CCA, give a theoretical justification for it, and test it by deriving word embeddings and evaluating them on a myriad of datasets.", "histories": [["v1", "Thu, 3 Sep 2015 09:39:36 GMT  (42kb)", "https://arxiv.org/abs/1509.01007v1", null], ["v2", "Tue, 8 Mar 2016 10:54:17 GMT  (49kb)", "http://arxiv.org/abs/1509.01007v2", null], ["v3", "Wed, 27 Jul 2016 12:46:39 GMT  (49kb)", "http://arxiv.org/abs/1509.01007v3", "in Transactions of the Association of Computational Linguistics (TACL), 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["dominique osborne", "shashi narayan", "shay b cohen"], "accepted": true, "id": "1509.01007"}, "pdf": {"name": "1509.01007.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Shashi Narayan"], "emails": ["dominique.osborne.13@uni.strath.ac.uk", "snaraya2@inf.ed.ac.uk", "scohen@inf.ed.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 150 9.01 007v 3 [cs.C L] 27 Jul 2 01"}, {"heading": "1 Introduction", "text": "In recent years, there has been an immense interest in the representation of words as low-dimensional, continuous real vectors, namely word embeddings and word embeddings, which aim to capture lexical and semantic information so that the regularities in vocabulary are topologically represented. Such word embeddings have reached a state in which they rely on many natural language processing processes (NLP), such as syntactic parsing (Socher et al., 2013), word or phrase similarity (Mikolov et al., 2013b), dependence (Bansal et al., 2014), unsupervised learning (Parikh et al. 2014), and others. Since discovering that word embeddings are useful for various NLP tasks, research on one's own life, with a vibrant community in search for better word representations."}, {"heading": "2 Background and Notation", "text": "We assume the existence of a vocabulary that is normally taken from a corpus. This vocabulary is denoted by H = {h1,..., h | H |}. For a square matrix A we denote by diag (A) a diagonal matrix B that has the same dimensions as A, so that Bii = Aii for all i. For a pair of vectors v and v we denote their 2 norms by | | v |, i.e. by | v | = = D = 1 v 2 i. We also denote by vj or [v] j the jte coordinate of v. For a pair of vectors u and v we denote their dot product by < u, v >. We define a word as embedding f from H to Rm for some (relatively small) m. m."}, {"heading": "3 Canonical Correlation Analysis for Deriving Word Embeddings", "text": "A current approach to the derivation of word components, developed by Dhillon et al. (2015), is characterized by the use of two different terms, which are reflected in the different terms. (2015), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016), (2016, (2016), (2016), (2016 (2016), (2016), (2016), (2016), (2016), (2016, (2016), (2016), (2016 (2016), (2016), (2016), (2016, (2016), (2016), 2016, 2016 (2016, 2016, 2016 (2016 (2016), 2016 (2016 (2016), 2016 (2016, 2016, 2016 (2016, 2016, 2016, 2016 (2016), 2016 (2016"}, {"heading": "4 Incorporating Prior Knowledge into Canonical Correlation Analysis", "text": "In this section, we will explain the technique we use to incorporate prior knowledge into the derivation of canonical correlation analysis. The main motivation of our approach is to improve the correlation between the two views by weighing them up using the external source of previous knowledge. Previous knowledge is based on lexical resources such as WordNet, FrameNet and the Paraphrase Database. Our approach follows a similar idea proposed by Koren and Carmel (2003) to improve the visualization of main vectors with main component analysis (PCA). It is also related to laplac manifold regulation (Belkin et al., 2006).An important idea in our derivation is that of a laplac matrix. The laplaker of an undirected weighted graph is a n \u00d7 n matrix, where n is the number of nodes in the chart. It corresponds to D \u2212 A, where A is the adaptability matrix of the graph (if the Aix is otherwise the graph)."}, {"heading": "4.1 Generalization of CCA", "text": "These three lemmas are useful to prove our final approach; the main statute shows that CCA maximizes the distance between the two views in practice, and the view that represents the contexts for a particular word; the distance between the two views is defined in Eq. 2. Lemma 1; LetX and Y are two views of size n and n that represent the word in a uniform representation; the distance between two views is defined in Eq. 2. Lemma 1; LetX and Y are two matrices of size n."}, {"heading": "4.2 From CCA with Dissimilarities to CCA with Similarities", "text": "It is often more convenient to work with similarity measurements between pairs of words. To do this, we must maintain the same formulation as before in Laplasian, where \u2212 Lij now denotes a measure of similarity. Instead of maximizing the goal in Eq.4, we must minimize it. It can be shown that such a mirror formulation can be performed with an algorithm similar to CCA, resulting in a sentence in the style of Sentence 4. To solve this minimization formula, we only need to select the singular vectors associated with the smallest m singular values (instead of the largest). Once we change the CCA algorithm with the Laplaker to select these projections, for example, we can define L on the basis of a similarity graph. The graph is an undirected graph that has | H | nodes for each word in the vocabulary, and there is an edge between a word pair \u2212 whenever the two words are similar to each other."}, {"heading": "4.3 Final Algorithm", "text": "In order to use any laplac matrix with CCA, we need the data to be centered, i.e. the average over all examples of each of the coordinates of the word and the context vectors to be zero. However, such a requirement would make the matrices C and W dense (with many non-zero values) and would require memory maintenance, and would also make the splitting of individual values inefficient. As such, we do not center the data to keep it sparse, and as such, we use a matrix L that is not strictly laplactic, but behaves better in practice."}, {"heading": "5 Experiments", "text": "In this section we describe our experiments."}, {"heading": "5.1 Experimental Setup", "text": "Training Data We used three sets of data, WIKI1, WIKI2 and WIKI5, all at the first 1, 2 and 1We note that other decomposition processes, such as PCA, also require data centering, but in the case of a sparse data matrix, this step will not be practicable.5 billion words from Wikipedia resp. 2 Each set of data is split into chunks of length 13 (window sizes of 6) corresponding to a document. The above laplac L is calculated separately within each document, meaning that \u2212 Lij is only 1 when i and j denote two words appearing in the same document. This is done to make the calculations mathematically feasible. We calculate word embeddings for the most common 200K words. Prior Knowledge Resources We consider three sources of previous knowledge: WordNet (Miller, 1995), the paraphrase Database Ganoz."}, {"heading": "5.2 Baselines", "text": "We compare our word embeddings with existing state-of-the-2We downloaded the data from https: / / dumps. wikimedia.org /, and pre-processing it using the tool available at http: / / mattmahoney.net / dc / textdata. html.3We use the XL subset of the PPDB. 4https: / / github.com / paramveerdhillon / swell. 5Our implementation and the word embeddings that we calculated are available at http: / / cohort.inf.ed.ac. uk / cohort / eigen /.6We also use the square-root transformation as mentioned in Dhillon et al. (2015) which controls the variance in the accumulated from the corpus. See a justification for this conversion in Stratos et al. (2015).word embeddings, such Glove (Pennington et al., 2014), Farang and Hup-et (of the Hub), (we-compare-our-knowledge-of-the-201m."}, {"heading": "5.3 Evaluation Benchmarks", "text": "We evaluated the quality of our own word embeddings on three different levels: word similarity, geographic analogies, and NP analogies, each of which affects the way people perceive them. (WS-353-ALL Dataset al., 2002) consists of 353 pairs of English words with their human similarity. (WS-353-REL) With specific distinctions between them, Agirre et al. (2009) re-annotated WS-353-ALL for similarity (WS-353-SIM) and relatedness (WS-353-REL). SimLex999 Dataset al. (2015) was built to measure how well models detect similarity rather than relatedness or association. The MEN-TR-3000 Dataset al. (Bruni et al., 2014) consists of 3,000 word pairs."}, {"heading": "5.4 Evaluation", "text": "In our first set of experiments, we vary the dimension of the word \"embedding vectors\" using an average of 54.1, while we try to m \"50, 100, 200, 300.\" Our experiments showed that the results are consistently improved as the dimension for all the different datasets increases. The more data is available, the more likely the quality of the word embedding is improved. In fact, for WIKI5, we get an average of 49.4, 57.0 and 54.2 for each of the dimensions. The dimensional improvements are consistent across all our results, so we notice a consistent improvement in accuracy when using more data from Wikipedia."}, {"heading": "6 Related Work", "text": "This article deals with the question of how to modify the target of word vector training algorithms. Yu and Dredze (2014), Xu et al. (2014), Fried and Duh (2015) and Bian et al. (2014), how to use the training target in neural language models by Mikolov et al. (2013) to encourage semantically related word vectors to come closer to each other. Wang et al. (2014), propose a method for the joint embedding of entities (from FreeBase, a large community-curated knowledge base) and words. Chen and de Melo (2015), propose a similar common model to improve word embeddings, but instead of using structured knowledge sources, focuses on the discovery of stronger setic connections in specific text portraits."}, {"heading": "7 Conclusion", "text": "Our method requires a relatively simple modification of the original canonical correlation analysis, where additional counts are added to the matrix on which a singular value degradation is performed. We used our method to derive word embeddings in the style of eigenwords, and tested it on a number of datasets. Our results show several advantages of encoding prior knowledge in eigenword embeddings."}, {"heading": "Acknowledgements", "text": "The authors would like to thank Paramveer Dhillon for his help in implementing the SWELL package, Manaal Faruqui and Sujay Kumar Jauhar for their help and technical support with the retrofitting package and the word embedding evaluation suite, and thank Ankur Parikh for early discussions on this project, which was completed while the lead author was an intern at the University of Edinburgh as part of the Equate Scotland programme, supported by an EPSRC grant (EP / L02411X / 1) and an EU grant H2020 (688139 / H2020-ICT-2015; SUMMA)."}, {"heading": "Appendix A: Proofs", "text": "The proof for Lemma 1 is similar to that which appears in Koren and Carmel (2003) for Lemma = >. The only difference is the use of two views. Note that [X'LY] ij = \u2211 k, k'XkiLkk \u2032 Yk \u2032 j as such, [X'LY] ij = \u2211 k, k '(n\u03b4kk \u2032 \u2212 1) XkiYk \u2032 j = n'XkiYkj \u2212 (n'K = 1Xki) k \"(n'K\" j \"). (n\" L \"iff\" k \"and otherwise, and the second equality is based on the assumption that the data are centered. Proof Lemma 2. Without loss of universality, we assume d\"."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Canonical correlation analysis (CCA) is a method for reducing the dimension of data represented using two views. It has been previously used to derive word embeddings, where one view indicates a word, and the other view indicates its context. We describe a way to incorporate prior knowledge into CCA, give a theoretical justification for it, and test it by deriving word embeddings and evaluating them on a myriad of datasets.", "creator": "LaTeX with hyperref package"}}}