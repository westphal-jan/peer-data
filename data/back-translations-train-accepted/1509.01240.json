{"id": "1509.01240", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Sep-2015", "title": "Train faster, generalize better: Stability of stochastic gradient descent", "abstract": "We show that any model trained by a stochastic gradient method with few iterations has vanishing generalization error. We prove this by showing the method is algorithmically stable in the sense of Bousquet and Elisseeff. Our analysis only employs elementary tools from convex and continuous optimization. Our results apply to both convex and non-convex optimization under standard Lipschitz and smoothness assumptions.", "histories": [["v1", "Thu, 3 Sep 2015 19:53:40 GMT  (31kb)", "http://arxiv.org/abs/1509.01240v1", null], ["v2", "Sun, 7 Feb 2016 17:06:58 GMT  (444kb,D)", "http://arxiv.org/abs/1509.01240v2", null]], "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["moritz hardt", "ben recht", "yoram singer"], "accepted": true, "id": "1509.01240"}, "pdf": {"name": "1509.01240.pdf", "metadata": {"source": "CRF", "title": "Stability of stochastic gradient descent", "authors": ["Moritz Hardt", "Benjamin Recht", "Yoram Singer"], "emails": ["mrtz@google.com", "brecht@berkeley.edu,", "singer@google.com"], "sections": [{"heading": null, "text": "ar Xiv: 150 9.01 240v 1 [cs.L G] 3S epApplying our results to the convex case, we provide new explanations for why several epochs of stochastic gradient descent generalize well in practice. In the non-convex case, we provide a new interpretation of common practices in neural networks and a formal justification for stability-enhancing mechanisms in the training of large, deep models. Conceptually, our results underscore the importance of reducing training time beyond its obvious utility."}, {"heading": "1 Introduction", "text": "The most widely used optimization method in machine learning practice is the stochastic gradient method (SGM). Stochastic gradient methods aim to minimize the empirical risk of a model by repeatedly calculating the gradient of a loss function using a single training example, or a series of few examples, and updating the model parameters accordingly. SGM is scalable, robust, and performs well through many different areas ranging from smooth and strongly convex problems to complex non-convex objects. In short, our results state that: Any model trained with stochastic gradient methods in a reasonable amount of time achieves small generalization errors. Since the training time is inevitably limited in practice, our results help explain the strong generalization performance of stochastic gradient methods observed in practice. More specifically, we will train the generalization error of a model achieved with stochastic gradient methods in a small amount of time."}, {"heading": "1.1 Our contributions", "text": "The reason why we focus on generalizing limits for models descending with stochastic gradients. Let's remember that the generalizing limit is the expected difference between the error that a model has on a training set and the error that has arisen on a new data point. We assume that we have training models with n sampled data points. Our results are based on a fundamental connection between the generalizing error of an algorithm and its stability characteristics. Broadly speaking, an algorithm is stable if it varies only slightly when we modify individual training data. The exact idea of the stability we use is known as a uniform stability due to [4]. It says that a randomized algorithm is stable if all the data in a single element is different, the learned models produce almost the same predictions."}, {"heading": "1.2 Related work", "text": "In fact, it is the case that we will be able to go in search of a solution that is in the position in which we find ourselves."}, {"heading": "2 Stability of randomized iterative algorithms", "text": "Consider the following general setting of supervised learning. It makes sense to consider the expected ertalisation as such. (EA) [EA] examples from a product room X \u00b7 Y. We get a sample S = (z1,.., zn) of n examples drawn from D. Our goal is to find a model w with low population risk, defined as: R [w] sample def = Ez \u00b2 sample D f (w; z) sample, in which f \u00b2 sample [w] sample [w] sample [w] -S = Ez \u00b2 sample stability (w; z) sample, the empirical risk, defined as asRS [w] def = 1nn \u00b2 function (w; z) denotes the loss of the model described by w. Since we cannot directly measure the objective R [w], we instead use an average proxy, the empirical risk, defined as function Rasw \u00b2 [w] nn."}, {"heading": "2.1 Properties of update rules", "text": "The most frequent update is the history rule that we will consider in this manuscript. The following two definitions provide the basis for our analysis of how two different sequences of the update rules differ from each other when iterated from the same starting point. Ultimately, these definitions will be useful when the stability of the stochastic gradient of descent.Definition 2.3. An update of the rule is the definition of how two different sequences of the update rules differ when iterated from the same starting point. These definitions will ultimately be useful when the stability of the stochastic gradient of descent.Definition 2.3."}, {"heading": "3 Stability of Stochastic Gradient Descent", "text": "Given n examples S = (z1,.., zn), where zi = (xi, yi) \u0394X \u00b7 Y, we consider a decomposable objective function f (w) = 1nn \u2211 i = 1f (w; (xi, yi)), where f (w; (xi, yi) indicates the loss of w using the example (xi, yi).The stochastic gradient update for this problem with the learning rate \u03b1t > 0 is given by wt + 1 = wt \u2212 \u043e\u0441wf (wt; zit).The stochastic gradient method (SGM) is the algorithm resulting from the execution of stochastic gradient updates T times where the indices are randomly selected. There are two popular schemes for selecting the example indices, one consisting of selecting them uniformly in {1,.., n} and updating the results in the previous order:"}, {"heading": "3.1 Proof idea: Stability of stochastic gradient method", "text": "To prove that the method of stochastic gradient is stable, we will analyze the results of the algorithm using two sets of data that differ exactly in one place. Note that if the loss function is L-Lipschitz, we have E-f (w;) -f (w; z) | \u2264 LE-w \u2212 w \u00b2 for each w and w \u00b2. Therefore, it is sufficient to analyze how wt and w \u00b2 t differ in the domain as a function of time t. If we remember that wt \u2212 1 is achieved by means of a gradient update, our goal is to bind the ability of wt \u2212 w \u00b2 n recursively and in anticipation as a function of the gradient \u2212 1.There are two cases to consider. In the first case, SGM chooses the index of an example that is identical in S and S \u00b2."}, {"heading": "3.2 Expansion properties of stochastic gradients", "text": "Let us now record some of the core properties of the stochastic gradient update: 1. The gradient update rule is limited, provided that function f meets the following common Lipschitz conditions. - We say that f-Lipschitz applies to all points in the f-Lipschitz domain. - This implies that the gradient updates Gf, \u03b1-L) -Bounded.Proof. Through our Lipschitz premise, it is possible that f-Lipschitz uses the L-Lipschitz updates Gf, \u03b1-L-Bound.Proof."}, {"heading": "3.3 Convex optimization", "text": "Theorem 3.9. Let us assume that the loss function f (\u00b7; z) for T steps is \u03b2-smooth, convex, and L-Lipschitz for each. Let us assume that we perform SGM with step sizes \u03b1t \u2264 2 / \u03b2. Let us then consider the gradient updates G1,.., GT, and G \u2032 1,.., G \u2032 T induced by executing SGM on sample S and S \u2032. Let us let wT and w \u2032 T be two samples of size n that differ in only a single example. Let us consider the gradient updates G1,., GT, and G \u2032 1,."}, {"heading": "3.4 Strongly convex optimization", "text": "In the strongly convex case, our limit does not depend on the number of steps on all.Theorem 3.10. Suppose that the loss function f (\u03b2; z) [0 \u2212 \u2212 1] is so strongly convex, L-Lipschitz and \u03b2-smooth for all z. Suppose that we perform SGM with constant step size \u2264 1 / \u03b2 for T-steps. Let us repeat the argument of completeness. Let S and S be two examples of size that differ only in a single example. Consider the gradient updates G1,., GT and G-1., G-T induced by executing SGM and S-2, respectively. Let wT and w-T denote the corresponding outputs of SGM.Applied Lemma 3.8 with 0,."}, {"heading": "3.5 Non-convex optimization", "text": "In this section, we demonstrate stability for stochastic progression methods that do not require convexity. We still assume that the objective function is smooth and Lipschitz is defined as before. Suppose we use SGM for T-steps with non-monotonous increments. Then, SGM has a uniform stability with 1 / 2 (2cL) -1 (2cL 2) 1 \u03b2c + 1T \u03b2c \u03b2c \u03b2c \u03b2c \u03b2c \u03b2c \u03b2c \u03b2c \u03b2c \u03b2c \u03b2c \u03b21) 1 (2cL) 1 (2cL) 1 (2cL) 1 (2cL) 1) 1 \u03b2c \u03b2c \u03b2c \u03b2c \u03b2c \u03b2c \u03b2c \u03b2c \u03b2c (1) In particular, we leave out constant factors that depend on \u03b2, c, and L."}, {"heading": "4 Stability-inducing operations", "text": "In view of our results, it makes sense to increase the stability of the stuttering method. (...) We show in this section that we are able to improve the results of the SGM. (...) We show that we are able to analyze the results of the SGM. (...) We show that we are able to analyze the results of the SGM. (...) We show that the SGM is able to improve the results of the SGM. (...) It is easy to verify whether the SGM is able to develop a differentiable function. (...) We define the gradient update with the rate as Gf. (...) We define the gradient update with the rate as Gf. (...) It is easy to verify that the above update rule is equivalent to perform a graditional update of the SGM. (...)"}, {"heading": "5 Convex risk minimization", "text": "The main characteristics of our analysis are that we can transform risk estimates into an optimization error and a term of stability, which is how closely we optimize empirical risk or a proxy of empirical risk. By optimizing with stochastic gradients, we will be able to generalize this optimization against how well we do. These results are inspired by the work of Bousquet and Bottou, who provide similar analyses for SGM based on uniform convergence."}, {"heading": "6 Future Work and Open Problems", "text": "Our analysis parts from many previous papers, in which we directly analyze the generalization performance of an algorithm, are relatively straightforward, but not the solution to an optimization problem, building on the toolkit normally used to prove that algorithms converge into objective values. Indeed, this approach may be more powerful than analyzing momentum, as it may be easier to understand how each data point affects a process than to find an optimal solution. It also has the advantage that the generalization limit cannot be reached even if the algorithm finds a unique optimal solution, as is common in non-convex problems. In addition to this broader perspective on learning algorithms, there are many exciting theoretical and empirical instructions that we intend to pursue in future work. We conclude this manuscript by highlighting some of the more concrete open problems and avenues for direct empirical validation. High probability The results are all in this paper."}, {"heading": "Acknowledgements", "text": "The authors thank John Duchi, Vineet Gupta, Kevin Jamieson, Eric Price, Nati Srebro and Oriol Vinyals for their insightful feedback and helpful suggestions."}, {"heading": "A Elementary properties of convex functions", "text": "The evidence for lemmas (v) \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 \u2212 p \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 p \u2212 \u2212 \u2212 p \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 p \u2212"}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "<lb>We show that any model trained by a stochastic gradient method with few iterations has<lb>vanishing generalization error. We prove this by showing the method is algorithmically stable<lb>in the sense of Bousquet and Elisseeff. Our analysis only employs elementary tools from convex<lb>and continuous optimization. Our results apply to both convex and non-convex optimization<lb>under standard Lipschitz and smoothness assumptions.<lb>Applying our results to the convex case, we provide new explanations for why multiple<lb>epochs of stochastic gradient descent generalize well in practice. In the nonconvex case, we<lb>provide a new interpretation of common practices in neural networks, and provide a formal<lb>rationale for stability-promoting mechanisms in training large, deep models. Conceptually, our<lb>findings underscore the importance of reducing training time beyond its obvious benefit.", "creator": "LaTeX with hyperref package"}}}