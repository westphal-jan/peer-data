{"id": "1509.01626", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2015", "title": "Character-level Convolutional Networks for Text Classification", "abstract": "This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.", "histories": [["v1", "Fri, 4 Sep 2015 22:31:53 GMT  (122kb,D)", "http://arxiv.org/abs/1509.01626v1", "An early version of this work entitled \"Text Understanding from Scratch\" was posted in Feb 2015 asarXiv:1502.01710. The present paper has considerably more experimental results and a rewritten introduction"], ["v2", "Thu, 10 Sep 2015 17:12:43 GMT  (122kb,D)", "http://arxiv.org/abs/1509.01626v2", "An early version of this work entitled \"Text Understanding from Scratch\" was posted in Feb 2015 asarXiv:1502.01710. The present paper has considerably more experimental results and a rewritten introduction. Advances in Neural Information Processing Systems 28 (NIPS 2015)"], ["v3", "Mon, 4 Apr 2016 02:34:30 GMT  (106kb,D)", "http://arxiv.org/abs/1509.01626v3", "An early version of this work entitled \"Text Understanding from Scratch\" was posted in Feb 2015 asarXiv:1502.01710. The present paper has considerably more experimental results and a rewritten introduction, Advances in Neural Information Processing Systems 28 (NIPS 2015)"]], "COMMENTS": "An early version of this work entitled \"Text Understanding from Scratch\" was posted in Feb 2015 asarXiv:1502.01710. The present paper has considerably more experimental results and a rewritten introduction", "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["xiang zhang", "junbo jake zhao", "yann lecun"], "accepted": true, "id": "1509.01626"}, "pdf": {"name": "1509.01626.pdf", "metadata": {"source": "CRF", "title": "Character-level Convolutional Networks for Text Classification\u2217", "authors": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "emails": ["yann}@cs.nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "Text classification ranges from developing the best characteristics to selecting the best possible classifiers for machine learning. So far, almost all text classification techniques are based on words in which simple statistics of some ordered word combinations (such as n-grams) usually perform best. On the other hand, many researchers have found Convolutionary Networks (ConvNets) [17] [18] are useful for extracting information from raw signals ranging from computer visions to speech recognition and others. In particular, the time-delayed networks used in the early days of deep learning research are essentially Convolutionary Networks that model sequential data. In this article, we examine the treatment of text as a kind of raw signal at the character level, and the application of temporary (one-dimensional) ConvNets to it."}, {"heading": "2 Character-level Convolutional Networks", "text": "In this section, we present the design of ConvNets at the character level for text classification, which is modular, with gradients determined by backpropagation [27] to perform optimization."}, {"heading": "2.1 Key Modules", "text": "Suppose we have a discrete input function g (x). [1, l] \u2192 R and a discrete core function f (x). The convolution h (y) \u00b7 g (y) [1, b (l \u2212 k + 1) / dc] \u2192 R between f (x) and g (x) with step velocity d is defined as ash (y) \u00b7 g (y \u00b7 d \u2212 x + c), where c = k \u2212 d + such an offset constant. Just as in traditional evolutionary networks in the vision, the module is parameterized (x) by a series of such core functions (i = 1, 2,.,. m and j = 1, 2,."}, {"heading": "2.2 Character quantization", "text": "Our models accept a sequence of encoded characters as input. The encoding takes place by prescribing a size m alphabet for the input language, and then quantifies each character by means of 1-of-m encoding (or \"one-hot\" encoding). Subsequently, the string is converted into a sequence of such m-sized vectors with a fixed length l0. Any character exceeding the length l0 is ignored, and all characters not contained in the alphabet are quantified as all-zero vectors. The character quantization sequence occurs backwards, so that the newest character input is always placed near the start of output, making it easy to link any completely connected layers of characters to the latest reading.The alphabet used in all our models consists of 70 characters, including 26 English letters, 10 digits, 33 other characters and the new character sign. The non-input signs are the most recent reading."}, {"heading": "2.3 Model Design", "text": "We have designed 2 ConvNets - a large and a small one. Both are 9 layers deep with 6 Convolutionary layers and 3 fully connected layers. Figure 1 gives an illustration. Input has, due to our character quantization method, a number of properties corresponding to 70, and the input remarkable length is 1014. It seems that 1014 characters could already capture most of the interesting texts. We also add 2 Dropouts [10] modules between the three completely connected layers to regulate them. They have a failure probability of 0.5. Table 1 lists the configurations for Convolutionary layers, and Table 2 lists the configurations for fully connected (linear) layers. We initialize the weights based on a Gaussian distribution. The mean and default deviation used for initializing the large model is (0, 0.02) and the small model (0, 0.05).For different problems, the inputs may be different based on a Gaussian distribution. The mean and standard deviation used for initializing the large model is (0, 0.02) and the small model (0, 0.05).For different problems, the input length may be connected to the frame (14) before the first layer is complete (0)."}, {"heading": "2.4 Data Augmentation using Thesaurus", "text": "Many researchers have found that appropriate data augmentation techniques are useful to control generalization errors for deep learning models. These techniques usually work well when we find suitable invariant properties that the model should have. In terms of text, it would not make sense to augment the data with signal transformations such as image or speech recognition, as the exact order of characters can form strict syntactic and semantic meanings. Therefore, the best way to augment data would have been to use human reformulations of sentences, but this is unrealistic and expensive due to the large volume of samples in our data sets. As a result, the most natural choice for us in data augmentation is to replace words or phrases with their synonyms. We experimented with data augmentation by using an English thesaurus derived from the myth as a component in our LibreOffice1 project."}, {"heading": "3 Comparison Models", "text": "In order to provide fair comparisons with competitive models, we conducted a series of experiments with both traditional and deep learning methods. We tried our best to select models that can deliver comparable and competitive results, and the results are faithfully reproduced without model selection."}, {"heading": "3.1 Traditional Methods", "text": "We refer to traditional methods such as those that use a handmade feature extractor and a linear classifier. The classifier used is a multinomial logistical regression in all of these models. Wallet and its TFIDF. For each data set, the Wallet Model is constructed by selecting 50,000 most common words from the subset of the training. For the normal Wallet, we use the number of words as characteristics. For the TFIDF version (Term Frequency Inverse Document Frequency) [14], we use the number of words as term frequency. The inverse frequency of the document is the logarithm of the division between the total number of samples and the samples with the word in the subset of the training. Characteristics are normalized by dividing the largest feature value."}, {"heading": "3.2 Deep Learning Methods", "text": "We choose two simple and representative models for comparison, one word-based ConvNet and the other a simple long-term short-term memory (LSTM) [11] recurring neural network model.Word-based ConvNets. Among the large number of recent work on word-based ConvNets for text classification, one of the differences is in the choice of pre-formed or end-to-end learned word representations. We offer comparisons with both the pre-formed word2vec [23] embedding [16] and with reference tables [5]. The embedding size in both cases is 300, in the same way as our Bagof medium model. To ensure a fair comparison, the models are feasible for each case of the same size as our characteristic word2vec [16] and the use of reference tables [5]. Experiments with a thesaurus for data augmentation are also feasible with a short-term network (http / www.5M)."}, {"heading": "3.3 Choice of Alphabet", "text": "For the English alphabet, an obvious choice is to distinguish between uppercase and lowercase letters. We report on experiments on this choice and have observed that it usually (but not always) leads to worse results when such a distinction is made. One possible explanation might be that semantics do not change with different letter cases, so there is an advantage of regularization."}, {"heading": "4 Large-scale Datasets and Results", "text": "In fact, most of them will be able to abide by the rules they have imposed on themselves, and they will be able to abide by the rules they have imposed on themselves. (...) Most of them are able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...)"}, {"heading": "6 Conclusion and Outlook", "text": "This article provides an empirical study of character-level conventional networks for text classification. We compared a large number of traditional and deep learning models with several large datasets. On the one hand, the analysis shows that ConvNet is an effective method at the character level. On the other hand, the performance of our model when comparing depends on many factors, such as the size of the datasets, the curation of the texts and the choice of the alphabet. In the future, we hope to use ConvNets at the character level for a broader range of language processing tasks, especially when structured output is needed."}, {"heading": "Acknowledgement", "text": "We thank NVIDIA Corporation for supporting this research by donating 2 Tesla K40 GPUs that were used for this research, and we thank Amazon.com Inc for supporting an AWS in Education Research Fellowship that was used for this research."}], "references": [{"title": "Experiments with time delay networks and dynamic time warping for speaker independent isolated digit recognition", "author": ["L. Bottou", "F. Fogelman Souli\u00e9", "P. Blanchet", "J. Lienard"], "venue": "Proceedings of EuroSpeech 89, volume 2, pages 537\u2013540, Paris, France", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1989}, {"title": "Learning mid-level features for recognition", "author": ["Y.-L. Boureau", "F. Bach", "Y. LeCun", "J. Ponce"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 2559\u20132566. IEEE", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "A theoretical analysis of feature pooling in visual recognition", "author": ["Y.-L. Boureau", "J. Ponce", "Y. LeCun"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 111\u2013118", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "BigLearn, NIPS Workshop, number EPFL-CONF-192376", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Deep convolutional neural networks for sentiment analysis of short texts", "author": ["C. dos Santos", "M. Gatti"], "venue": "In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Wordnet and wordnets", "author": ["C. Fellbaum"], "venue": "K. Brown, editor, Encyclopedia of Language and Linguistics, pages 665\u2013670, Oxford", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["A. Graves", "J. Schmidhuber"], "venue": "Neural Networks, 18(5):602\u2013610", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "J", "author": ["K. Greff", "R.K. Srivastava"], "venue": "Koutn\u0131\u0301k, B. R. Steunebrink, and J. Schmidhuber. LSTM: A search space odyssey. CoRR, abs/1503.04069", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1997}, {"title": "Text categorization with suport vector machines: Learning with many relevant features", "author": ["T. Joachims"], "venue": "Proceedings of the 10th European Conference on Machine Learning, pages 137\u2013142. Springer-Verlag", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1998}, {"title": "Effective use of word order for text categorization with convolutional neural networks", "author": ["R. Johnson", "T. Zhang"], "venue": "CoRR, abs/1412.1058", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "A statistical interpretation of term specificity and its application in retrieval", "author": ["K.S. Jones"], "venue": "Journal of Documentation, 28(1):11\u201321", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1972}, {"title": "Words versus character n-grams for anti-spam filtering", "author": ["I. Kanaris", "K. Kanaris", "I. Houvardas", "E. Stamatatos"], "venue": "International Journal on Artificial Intelligence Tools, 16(06):1047\u20131067", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural Computation,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1989}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1998}, {"title": "P", "author": ["J. Lehmann", "R. Isele", "M. Jakob", "A. Jentzsch", "D. Kontokostas", "P.N. Mendes", "S. Hellmann", "M. Morsey"], "venue": "van Kleef, S. Auer, and C. Bizer. DBpedia - a large-scale, multilingual knowledge base extracted from wikipedia. Semantic Web Journal", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "In defense of word embedding for generic text representation", "author": ["G. Lev", "B. Klein", "L. Wolf"], "venue": "C. Biemann, S. Handschuh, A. Freitas, F. Meziane, and E. Mtais, editors, Natural Language Processing and Information Systems, volume 9103 of Lecture Notes in Computer Science, pages 35\u201350. Springer International Publishing", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["D.D. Lewis", "Y. Yang", "T.G. Rose", "F. Li"], "venue": "The Journal of Machine Learning Research, 5:361\u2013397", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "Hidden factors and hidden topics: Understanding rating dimensions with review text", "author": ["J. McAuley", "J. Leskovec"], "venue": "Proceedings of the 7th ACM Conference on Recommender Systems, RecSys \u201913, pages 165\u2013172, New York, NY, USA", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 807\u2013814", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "ICML 2013, volume 28 of JMLR Proceedings, pages 1310\u20131318. JMLR.org", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Some methods of speeding up the convergence of iteration methods", "author": ["B. Polyak"], "venue": "{USSR} Computational Mathematics and Mathematical Physics, 4(5):1 \u2013 17", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1964}, {"title": "Learning representations by back-propagating errors", "author": ["D. Rumelhart", "G. Hintont", "R. Williams"], "venue": "Nature, 323(6088):533\u2013536", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1986}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["C.D. Santos", "B. Zadrozny"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1818\u20131826", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "A latent semantic model with convolutional-pooling structure for information retrieval", "author": ["Y. Shen", "X. He", "J. Gao", "L. Deng", "G. Mesnil"], "venue": "Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, pages 101\u2013110. ACM", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G.E. Dahl", "G.E. Hinton"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML-13),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Phoneme recognition using time-delay neural networks", "author": ["A. Waibel", "T. Hanazawa", "G. Hinton", "K. Shikano", "K.J. Lang"], "venue": "Acoustics, Speech and Signal Processing, IEEE Transactions on, 37(3):328\u2013339", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1989}, {"title": "Automatic online news issue construction in web environment", "author": ["C. Wang", "M. Zhang", "S. Ma", "L. Ru"], "venue": "Proceedings of the 17th International Conference on World Wide Web, WWW \u201908, pages 457\u2013466, New York, NY, USA", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 11, "context": "To date, almost all techniques of text classification are based on words, in which simple statistics of some ordered word combinations (such as n-grams) usually perform the best [12].", "startOffset": 178, "endOffset": 182}, {"referenceID": 16, "context": "On the other hand, many researchers have found convolutional networks (ConvNets) [17] [18] are useful in extracting information from raw signals, ranging from computer vision applications to speech recognition and others.", "startOffset": 81, "endOffset": 85}, {"referenceID": 17, "context": "On the other hand, many researchers have found convolutional networks (ConvNets) [17] [18] are useful in extracting information from raw signals, ranging from computer vision applications to speech recognition and others.", "startOffset": 86, "endOffset": 90}, {"referenceID": 0, "context": "In particular, time-delay networks used in the early days of deep learning research are essentially convolutional networks that model sequential data [1] [31].", "startOffset": 150, "endOffset": 153}, {"referenceID": 30, "context": "In particular, time-delay networks used in the early days of deep learning research are essentially convolutional networks that model sequential data [1] [31].", "startOffset": 154, "endOffset": 158}, {"referenceID": 5, "context": "It has been shown that ConvNets can be directly applied to distributed [6] [16] or discrete [13] embedding of words, without any knowledge on the syntactic or semantic structures of a language.", "startOffset": 71, "endOffset": 74}, {"referenceID": 15, "context": "It has been shown that ConvNets can be directly applied to distributed [6] [16] or discrete [13] embedding of words, without any knowledge on the syntactic or semantic structures of a language.", "startOffset": 75, "endOffset": 79}, {"referenceID": 12, "context": "It has been shown that ConvNets can be directly applied to distributed [6] [16] or discrete [13] embedding of words, without any knowledge on the syntactic or semantic structures of a language.", "startOffset": 92, "endOffset": 96}, {"referenceID": 14, "context": "These include using character-level n-grams with linear classifiers [15], and incorporating character-level features to ConvNets [28] [29].", "startOffset": 68, "endOffset": 72}, {"referenceID": 27, "context": "These include using character-level n-grams with linear classifiers [15], and incorporating character-level features to ConvNets [28] [29].", "startOffset": 129, "endOffset": 133}, {"referenceID": 28, "context": "These include using character-level n-grams with linear classifiers [15], and incorporating character-level features to ConvNets [28] [29].", "startOffset": 134, "endOffset": 138}, {"referenceID": 27, "context": "In particular, these ConvNet approaches use words as a basis, in which character-level features extracted at word [28] or word n-gram [29] level form a distributed representation.", "startOffset": 114, "endOffset": 118}, {"referenceID": 28, "context": "In particular, these ConvNet approaches use words as a basis, in which character-level features extracted at word [28] or word n-gram [29] level form a distributed representation.", "startOffset": 134, "endOffset": 138}, {"referenceID": 26, "context": "The design is modular, where the gradients are obtained by back-propagation [27] to perform optimization.", "startOffset": 76, "endOffset": 80}, {"referenceID": 1, "context": "It is the 1-D version of the max-pooling module used in computer vision [2].", "startOffset": 72, "endOffset": 75}, {"referenceID": 2, "context": "The analysis by [3] might shed some light on this.", "startOffset": 16, "endOffset": 19}, {"referenceID": 23, "context": "The non-linearity used in our model is the rectifier or thresholding function h(x) = max{0, x}, which makes our convolutional layers similar to rectified linear units (ReLUs) [24].", "startOffset": 175, "endOffset": 179}, {"referenceID": 25, "context": "The algorithm used is stochastic gradient descent (SGD) with a minibatch of size 128, using momentum [26] [30] 0.", "startOffset": 101, "endOffset": 105}, {"referenceID": 29, "context": "The algorithm used is stochastic gradient descent (SGD) with a minibatch of size 128, using momentum [26] [30] 0.", "startOffset": 106, "endOffset": 110}, {"referenceID": 3, "context": "The implementation is done using Torch 7 [4].", "startOffset": 41, "endOffset": 44}, {"referenceID": 9, "context": "We also insert 2 dropout [10] modules in between the 3 fully-connected layers to regularize.", "startOffset": 25, "endOffset": 29}, {"referenceID": 6, "context": "That thesaurus in turn was obtained from WordNet [7], where every synonym to a word or phrase is ranked by the semantic closeness to the most frequently seen meaning.", "startOffset": 49, "endOffset": 52}, {"referenceID": 13, "context": "For the TFIDF (term-frequency inverse-document-frequency) [14] version, we use the counts as the term-frequency.", "startOffset": 58, "endOffset": 62}, {"referenceID": 22, "context": "We also have an experimental model that uses k-means on word2vec [23] learnt from the training subset of each dataset, and then use these learnt means as representatives of the clustered words.", "startOffset": 65, "endOffset": 69}, {"referenceID": 10, "context": "We choose two simple and representative models for comparison, in which one is word-based ConvNet and the other a simple long-short term memory (LSTM) [11] recurrent neural network model.", "startOffset": 151, "endOffset": 155}, {"referenceID": 22, "context": "We offer comparisons with both using the pretrained word2vec [23] embedding [16] and using lookup tables [5].", "startOffset": 61, "endOffset": 65}, {"referenceID": 15, "context": "We offer comparisons with both using the pretrained word2vec [23] embedding [16] and using lookup tables [5].", "startOffset": 76, "endOffset": 80}, {"referenceID": 4, "context": "We offer comparisons with both using the pretrained word2vec [23] embedding [16] and using lookup tables [5].", "startOffset": 105, "endOffset": 108}, {"referenceID": 10, "context": "We also offer a comparison with a recurrent neural network model, namely long-short term memory (LSTM) [11].", "startOffset": 103, "endOffset": 107}, {"referenceID": 7, "context": "The variant of LSTM we used is the common \u201cvanilla\u201d architecture [8] [9].", "startOffset": 65, "endOffset": 68}, {"referenceID": 8, "context": "The variant of LSTM we used is the common \u201cvanilla\u201d architecture [8] [9].", "startOffset": 69, "endOffset": 72}, {"referenceID": 24, "context": "We also used gradient clipping [25] in which the gradient norm is limited to 5.", "startOffset": 31, "endOffset": 35}, {"referenceID": 20, "context": "However, most open datasets for text classification are quite small, and large-scale datasets are splitted with a significantly smaller training set than testing [21].", "startOffset": 162, "endOffset": 166}, {"referenceID": 31, "context": "This dataset is a combination of the SogouCA and SogouCS news corpora [32], containing in total 2,909,551 news articles in various topic channels.", "startOffset": 70, "endOffset": 74}, {"referenceID": 18, "context": "DBpedia is a crowd-sourced community effort to extract structured information from Wikipedia [19].", "startOffset": 93, "endOffset": 97}, {"referenceID": 21, "context": "We obtained an Amazon review dataset from the Stanford Network Analysis Project (SNAP), which spans 18 years with 34,686,770 reviews from 6,643,669 users on 2,441,053 products [22].", "startOffset": 176, "endOffset": 180}, {"referenceID": 19, "context": "Bag-of-means is a misuse of word2vec [20].", "startOffset": 37, "endOffset": 41}], "year": 2015, "abstractText": "This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several largescale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.", "creator": "LaTeX with hyperref package"}}}