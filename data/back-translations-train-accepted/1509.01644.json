{"id": "1509.01644", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Sep-2015", "title": "Reinforcement Learning with Parameterized Actions", "abstract": "We introduce a model-free algorithm for learning in Markov decision processes with parameterized actions-discrete actions with continuous parameters. At each step the agent must select both which action to use and which parameters to use with this action. This models domains where there are distinct actions which can be adjusted to a particular state. We introduce the Q-PAMDP algorithm for learning in these domains. We show that Q-PAMDP converges to a local optima, and compare different approaches in a robot soccer goal-scoring domain and a platformer domain.", "histories": [["v1", "Sat, 5 Sep 2015 00:17:35 GMT  (160kb,D)", "http://arxiv.org/abs/1509.01644v1", "This paper is pre-submission. It will be submitted to AAAI 2015. It is currently missing experimental results from the second domain"], ["v2", "Tue, 15 Sep 2015 20:44:11 GMT  (583kb,D)", "http://arxiv.org/abs/1509.01644v2", "This paper has been submitted to AAAI 2015"], ["v3", "Tue, 22 Sep 2015 14:48:21 GMT  (583kb,D)", "http://arxiv.org/abs/1509.01644v3", "This paper has been submitted to AAAI 2015"], ["v4", "Thu, 26 Nov 2015 12:00:42 GMT  (577kb,D)", "http://arxiv.org/abs/1509.01644v4", "Accepted for AAAI 2016"]], "COMMENTS": "This paper is pre-submission. It will be submitted to AAAI 2015. It is currently missing experimental results from the second domain", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["warwick masson", "pravesh ranchod", "george konidaris"], "accepted": true, "id": "1509.01644"}, "pdf": {"name": "1509.01644.pdf", "metadata": {"source": "CRF", "title": "Reinforcement Learning with Parameterized Actions", "authors": ["Warwick Masson", "George Konidaris"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "If we use a discrete action space, we lose the ability to take into account differences in the way: All actions must be expressed as a single vector. However, if we only use discrete actions, we are limited to discrete actions or suffer a blow-up in the number of actions to represent a wide range of actions. A parameterized action is a discrete action that must be parameterized by a real vector. Modelling actions in this way introduces structure into the action space by treating different types of continuous actions as different or suffering a blow-up in the number of actions to represent a wide range of actions. A parameterized action is a discrete action parameterized by a real vector."}, {"heading": "2 Background", "text": "A Markov Decision Process (MDP) is a tuple < S, A, P, R, Q, Q >, where S is a set of states, A is a set of measures, P (s, a, s) is the likelihood of transition to the state s \u2032 of states after taking measures a, R (s, a, r) is the likelihood of receiving a reward r for taking measures in the state s, and \u03b3 is a discount factor (Sutton and Barto 1998). We would like to find a policy that selects an action for each state to maximize the expected sum of discounted rewards (the rate of return).The value function V (s) is defined as the expected discounted rate of return and is given by V (s)."}, {"heading": "3 Parameterized Actions Markov Decision Processes", "text": "We look at MDPs where the government's room for manoeuvre is continuous (S'Rn) and the measures are parameterized: there is a finite series of measures (A = {a1, a2,.). We refer to such MDPs as parameterized action MDPs (PAMDPs). Figure 1 describes the different areas for action. For example, a quad-rotor delivery system could have continuous measures for moving around and dropping its payload on one target. We would not want to combine these measures into a single continuous action while experimenting with falling payloads while we are using a discretization in which we consider a fixed parameterization for the different actions."}, {"heading": "4 Theoretical Results", "text": "We assume that we can be selected with the corresponding convergence property if the convergence from Q-LEARN convergence to W-LEARN converges for each given procedure. Next, we show that if P-UPDATE can be optimized locally, this is also the case for some initial parameters. Q-PAMDP (1) is equivalent to this procedure if the convergence from Q-LEARN to W-LEARN is converted locally for each given procedure. Q-LEARN (1) converts to a local optimum. Theorem 4.1 (Convergence to a local optimum). If the procedure of P-UPDATE can be optimized locally, then we optimize this procedure for any objective function. Q-PAMDP (1) converts to a local optimum."}, {"heading": "5 Experiments", "text": "First, let's consider a simplified Robo Cup problem (Kitano et al. 1997), in which a single striker tries to score a goal. Each episode begins with the player at a random position along the left boundary of the field. The player starts with the ball in possession, and the goalkeeper is positioned between the ball and the goal. However, the game takes place in a 2D environment, where the player and the goalkeeper have a position, speed and orientation, and the ball has a position and speed that leads to 14 continuous state variables. An episode ends when the goalkeeper owns the ball or the ball leaves the field. The reward for an action is 0 for a non-terminal state, 50 for a non-terminal state of goal, and \u2212 d for a non-terminal state in which d is the distance of the ball from the goal. The player has two parameterized actions: kick-to (x, y), which shoots the ball toward position (x, y), and shoots the ball along the line."}, {"heading": "6 Related Work", "text": "Hauskrecht et al. (2004) introduced an algorithm for solving factored MDPs with a hybrid discrete continuous action space. Furthermore, they assume that the domain has a compact factored representation, and consider only the planning. Rachelson (2009) came across parameterized actions in the form of an action to wait for a certain period of time in his research on time-dependent continuous time MDPs (TMDPs). He developed XMDPs that represent a parameterized action space (Rachelson 2009). He developed a Bellman operator for this domain, and mentioned in a later paper that the TiMDPpoly algorithms can work with parameterized actions."}, {"heading": "7 Conclusion", "text": "The PAMDP formalism models reinforce learning areas with parameterised actions. PAMDPs enable new types of areas and new approaches for old areas. Parameterised actions give us the adaptability of continuous areas and the use of different actions. They also allow the simple representation of discontinuous strategies without complex parameterisation. Three approaches to model-free learning in PAMDPs were presented: direct optimisation and two variants of the Q-PAMDP algorithm. We have shown that Q-PAMDP (1) converges with a suitable PUPDATE method to a local or global optimisation. QPAMDP (3) with a global optimisation step converges with a local optimisation step. We have examined the performance of the three approaches in the robot football ordomain. The robot football ordomain models the situation in which a striker must manoeuvre a goalkeeper in order to score a goal."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We introduce a model-free algorithm for learning in Markov decision processes with parameterized actions\u2014discrete actions with continuous parameters. At each step the agent must select both which action to use and which parameters to use with this action. This models domains where there are distinct actions which can be adjusted to a particular state. We introduce the Q-PAMDP algorithm for learning in these domains. We show that Q-PAMDP converges to a local optima, and compare different approaches in a robot soccer goal-scoring domain and a platformer domain.", "creator": "LaTeX with hyperref package"}}}