{"id": "1509.01692", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Sep-2015", "title": "Take and Took, Gaggle and Goose, Book and Read: Evaluating the Utility of Vector Differences for Lexical Relation Learning", "abstract": "Recent work on word embeddings has shown that simple vector subtraction over pre-trained embeddings is surprisingly effective at capturing different lexical relations, despite lacking explicit supervision. Prior work has evaluated this intriguing result using a word analogy prediction formulation and hand-selected relations, but the generality of the finding over a broader range of lexical relation types and different learning settings has not been evaluated. In this paper, we carry out such an evaluation in two learning settings: (1) spectral clustering to induce word relations, and (2) supervised learning to classify vector differences into relation types. We find that word embeddings capture a surprising amount of information, and that, under suitable supervised training, vector subtraction generalises well to a broad range of relations, including over unseen lexical items.", "histories": [["v1", "Sat, 5 Sep 2015 11:23:44 GMT  (113kb,D)", "http://arxiv.org/abs/1509.01692v1", null], ["v2", "Fri, 11 Sep 2015 12:20:03 GMT  (113kb,D)", "http://arxiv.org/abs/1509.01692v2", null], ["v3", "Wed, 17 Feb 2016 05:44:33 GMT  (86kb,D)", "http://arxiv.org/abs/1509.01692v3", null], ["v4", "Sat, 13 Aug 2016 17:56:01 GMT  (141kb,D)", "http://arxiv.org/abs/1509.01692v4", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ekaterina vylomova", "laura rimell", "trevor cohn", "timothy baldwin"], "accepted": true, "id": "1509.01692"}, "pdf": {"name": "1509.01692.pdf", "metadata": {"source": "CRF", "title": "Take and Took, Gaggle and Goose, Book and Read: Evaluating the Utility of Vector Differences for Lexical Relation Learning", "authors": [], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, the fact is that most of them are able to survive on their own, in the way they do it: in the way they do it, in the way they do it, in the way they do it, in the way they do it, in the way they do it, in the way they do it."}, {"heading": "2 Background and Related Work", "text": "A lexical relationship is a binary relationship between a pair of words (wi, wj): a common task; for example, the pair (cart, wheel) is in the WHOLE-PART relationship. NLP tasks related to lexical relationship learning include extraction and discovery, relationship patterns, and relational similarity prediction. In relation to this, word pairs that are in a given relationship are also derived from a corpus (e.g. in the form of bookmarks); the relationship may be predefined or, in the Open Information Extraction paradigm (Banko et al., 2007; Weikum and Theobald, 2010), the relationships themselves are learned from the text (e.g. in the form of bookmarks); in relation, the task is to assign a pair of words to the correct relationship, from a predefined set of relationships. Relational similarity involves assessing the degree of degree to which a pair of words (a, c) is fully related to a pair (or another pair)."}, {"heading": "3 General Approach and Resources", "text": "For our purposes, we define the task of lexical relationship learning to take a set of (ordered) word pairs {(wi, wj)} and a set of binary lexical relationships R = {rk} and map each word pair (wi, wj) as follows: (a) (wi, wj) 7 \u2192 rk-R, i.e. the \"closed world\" environment in which we assume that all word pairs can be uniquely classified according to a relationship in R; or (b) (wi, wj) 7 \u2192 rk-R, where it is important that none of the relationships in R apply to the word pair in question, i.e. the \"open world\" setting. Our starting point for lexical relationship learning is the assumption that important information about different types of relationships is implicitly embedded in the offset vectors."}, {"heading": "3.1 Word Embeddings", "text": "We look at four highly successful word embeddings for our experiments: w2v (mikolov et al = i > J = wi), GloVE (Pennington et al., 2014), SENNA (Collobert et al., 2011), and HLBL (Mnih and Hinton, 2009). w2v is designed to predict the context of a word in which the use of different models, linearity, manner of using words for their contexts, dimensionality, and domain of learning datasets (as listed in Tab 1). w2v is designed to predict the context of a word in which the model is used by a skip-gram with the objective: J = 1T - c \u2264 i + cj 6 = iexp (w > i)."}, {"heading": "3.2 Lexical Relations", "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "4 Clustering", "text": "We assume that we are able to capture all lexical relationships equally, we would expect that each individual group of word pairs with high relational similarity can be identified, or that it applies similar clusters of similar offset vectors. (To test these assumptions, we need to close our 18 relationships in the first instance and evaluate the resulting clusters against the lexical resources in the second instance.) As a further motivation, we consider Fig 1, which represents the DIFFVEC space for 10 samples of each class (based on a projection learned via the full dataset)."}, {"heading": "5 Classification", "text": "A natural question is whether we can more accurately characterize lexical relationships based on DIFFVECs by selecting or scaling the embedding dimensions. While some dimensions might encode lexical semantic information, other dimensions encode other information relevant to their training objectives (see \u00a7 3.1), such as domain, syntax, or selective constraints. The former dimensions should be selected (highly weighted) and the latter dimensions ignored. We try to test this hypothesis using a monitored classification, i.e. by learning a discriminatory classifier to distinguish between different relationship types based solely on the DIFFVECs between a word pair, \u2206 i, j. For these experiments, we use the w2v embedding and a subset of relationships for which we have sufficient data for supervised training and evaluation, namely NOColl, BLESSBASSEVERSBASSES, BASSEP2, BASSEVERS2, BASSEVERS2, BASSEVERSBASSESP2."}, {"heading": "5.1 CLOSED-WORLD Classification", "text": "For the CLOSED-WORLD setting, we train and test a multi-class classifier on data sets with < \u2206 i, j, r > pairs, where r is one of our nine relationship types. We use an SVM with a linear core and report the results of a 10-fold cross-validation in Table 4. Most relationships, even the most difficult from our cluster experiment, are classified with high precision and memory. PREFIX was the only exception and achieved a significantly lower memory due to various other semantic relationships that could be expressed by the same prefix type (e.g. (degree, retrograde), (unification, reunion), (input, reentry)). Slightly surprisingly, we found that the linear SVM slightly outperformed a non-linear SVM with an RBF core. Accordingly, the decision surfaces correspond to simple linear transformations of the embedding dimensions."}, {"heading": "5.2 OPEN-WORLD Classification", "text": "We now turn to a more sophisticated evaluation setting: a test set of randomly drawn word pairs, which aims to demonstrate whether a DIFFVEC-based classifier is able to distinguish related word pairs from noise, and can be applied to open data to learn new related word pairs. For these experiments, we train a binary classifier for each conversion type, using 23 of our relationship data for training and 13 for testing. Test data is topped up with an equal amount of noise samples, which are generated as follows: (1) we first try out a seed lexicon by plotting words proportional to their frequency in Wikipedia; 7 (2) we take the Cartesian product using word pairs from the seed lexicon; and (3) finally, we try word pairs consistently from this set. This method generates word pairs that are representative of the frequency profile of our body."}, {"heading": "5.3 OPEN-WORLD Training with Noise", "text": "To address the problem of word pair deflection as valid relationships, we need to retrain the classifier, which has both valid and negative \"deflection patterns.\" The basic intuition behind this approach is to secure cases where correct samples can be found. 7Filtered to words for the dictor samples will force the sequence of word pairs to differ not only from each other, but also from other word pairs. To this end, we need to generate two types of deflection maneuvers that we have chosen."}, {"heading": "6 Conclusions", "text": "This paper is the first to test the generalizability of the vector-difference approach across a wide range of lexical relationships (in raw numbers and diversity). Firstly, clustering has shown us that many types of morphosyntactic and morphosemetric differences are captured by DIFFVECs, but that lexical semantic relationships are less well captured, which is consistent with previous work (Ko \ufffd per et al., 2015). Secondly, we showed that classification via DIFFVECs works extremely well in a closed world, but less well through open data. However, with the introduction of automatically generated negative samples, the results improved significantly. Overall, we therefore conclude that the DIFFVEC approach has impressive benefits across a wide range of lexical relationships, especially under supervised classification."}], "references": [{"title": "Random walks on context spaces: Towards an explanation of the mysteries of semantic word embeddings", "author": ["Sanjeev Arora", "Yuanzhi Li", "Yingyu Liang", "Tengyu Ma", "Andrej Risteski."], "venue": "arXiv.", "citeRegEx": "Arora et al\\.,? 2015", "shortCiteRegEx": "Arora et al\\.", "year": 2015}, {"title": "Open information extraction for the web", "author": ["Michele Banko", "Michael J. Cafarella", "Stephen Soderland", "Matthew Broadhead", "Oren Etzioni."], "venue": "Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI-2007), pages 2670\u20132676, Hyder-", "citeRegEx": "Banko et al\\.,? 2007", "shortCiteRegEx": "Banko et al\\.", "year": 2007}, {"title": "How we blessed distributional semantic evaluation", "author": ["Marco Baroni", "Alessandro Lenci."], "venue": "Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics, GEMS \u201911, pages 1\u201310, Stroudsburg, PA, USA. Association", "citeRegEx": "Baroni and Lenci.,? 2011", "shortCiteRegEx": "Baroni and Lenci.", "year": 2011}, {"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs", "author": ["Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski."], "venue": "context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Baroni et al\\.,? 2014", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Antoine Bordes", "Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko."], "venue": "Advances in Neural Information Processing Systems 25 (NIPS-13), pages 2787\u20132795.", "citeRegEx": "Bordes et al\\.,? 2013", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Multi-relational latent semantic analysis", "author": ["Kai-Wei Chang", "Wen tau Yih", "Christopher Meek."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013), pages 1602\u20131612.", "citeRegEx": "Chang et al\\.,? 2013", "shortCiteRegEx": "Chang et al\\.", "year": 2013}, {"title": "VerbOcean: Mining the web for fine-grained semantic verb relations", "author": ["Timothy Chklovski", "Patrick Pantel."], "venue": "Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP 2002), pages 33\u201340, Barcelona, Spain.", "citeRegEx": "Chklovski and Pantel.,? 2004", "shortCiteRegEx": "Chklovski and Pantel.", "year": 2004}, {"title": "Two decades of unsupervised pos induction: How far have we come", "author": ["Christos Christodoulopoulos", "Sharon Goldwater", "Mark Steedman"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP 2010),", "citeRegEx": "Christodoulopoulos et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Christodoulopoulos et al\\.", "year": 2010}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research, 12:2493\u2013 2537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "From distributional to semantic similarity", "author": ["James Richard Curran."], "venue": "Ph.D. thesis, University of Edinburgh.", "citeRegEx": "Curran.,? 2004", "shortCiteRegEx": "Curran.", "year": 2004}, {"title": "Classification of semantic relationships between nominals using pattern clusters", "author": ["Dmitry Davidov", "Ari Rappoport."], "venue": "Proceedings of the 46th Annual Meeting of the ACL: HLT (ACL 2008), pages 227\u2013235, Columbus, USA.", "citeRegEx": "Davidov and Rappoport.,? 2008", "shortCiteRegEx": "Davidov and Rappoport.", "year": 2008}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Manaal Faruqui", "Jesse Dodge", "Sujay Jauhar", "Chris Dyer", "Ed Hovy", "Noah Smith."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics \u2014 Human", "citeRegEx": "Faruqui et al\\.,? 2015", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "WordNet: An Electronic Lexical Database", "author": ["Christiane Fellbaum", "editor"], "venue": null, "citeRegEx": "Fellbaum and editor.,? \\Q1998\\E", "shortCiteRegEx": "Fellbaum and editor.", "year": 1998}, {"title": "Incorporating both distributional and relational semantics in word representations", "author": ["Daniel Fried", "Kevin Duh."], "venue": "Proceedings of the International Conference on Learning Representations Workshop (ICLR 2015).", "citeRegEx": "Fried and Duh.,? 2015", "shortCiteRegEx": "Fried and Duh.", "year": 2015}, {"title": "Learning semantic hierarchies via word embeddings", "author": ["Ruiji Fu", "Jiang Guo", "Bing Qin", "Wanxiang Che", "Haifeng Wang", "Ting Liu."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014), pages 1199\u20131209.", "citeRegEx": "Fu et al\\.,? 2014", "shortCiteRegEx": "Fu et al\\.", "year": 2014}, {"title": "The distributional inclusion hypotheses and lexical entailment", "author": ["M. Geffet", "I. Dagan."], "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL 2005), Michigan.", "citeRegEx": "Geffet and Dagan.,? 2005", "shortCiteRegEx": "Geffet and Dagan.", "year": 2005}, {"title": "Semeval-2007 task 04: Classification of semantic relations between nominals", "author": ["Roxana Girju", "Preslav Nakov", "Vivi Nastase", "Stan Szpakowicz", "Peter Turney", "Deniz Yuret."], "venue": "Proceedings of the 4th International Workshop on Semantic Evaluations, pages", "citeRegEx": "Girju et al\\.,? 2007", "shortCiteRegEx": "Girju et al\\.", "year": 2007}, {"title": "Automatic acquisition of hyponyms from large text corpora", "author": ["Marti Hearst."], "venue": "Proceedings of the 14th International Conference on Computational Linguistics (COLING \u201992), Nantes, France.", "citeRegEx": "Hearst.,? 1992", "shortCiteRegEx": "Hearst.", "year": 1992}, {"title": "SemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals", "author": ["Iris Hendrickx", "Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid \u00d3 S\u00e9aghdha", "Sebastian Pad\u00f3", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz"], "venue": null, "citeRegEx": "Hendrickx et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hendrickx et al\\.", "year": 2010}, {"title": "Semeval-2012 task 2: Measuring degrees of relational similarity", "author": ["David Jurgens", "Saif Mohammad", "Peter Turney", "Keith Holyoak."], "venue": "Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), pages 356\u2013364, Montr\u00e9al, Canada.", "citeRegEx": "Jurgens et al\\.,? 2012", "shortCiteRegEx": "Jurgens et al\\.", "year": 2012}, {"title": "Deriving adjectival scales from continuous space word", "author": ["Joo-Kyung Kim", "Marie-Catherine de Marneffe"], "venue": null, "citeRegEx": "Kim and Marneffe.,? \\Q2013\\E", "shortCiteRegEx": "Kim and Marneffe.", "year": 2013}, {"title": "Multilingual reliability and \u201csemantic\u201d structure of continuous word spaces", "author": ["Maximilian K\u00f6per", "Christian Scheible", "Sabine Schulte im Walde."], "venue": "Proceedings of the Eleventh International Workshop on Computational Semantics", "citeRegEx": "K\u00f6per et al\\.,? 2015", "shortCiteRegEx": "K\u00f6per et al\\.", "year": 2015}, {"title": "Directional distributional similarity for lexical inference", "author": ["Lili Kotlerman", "Ido Dagan", "Idan Szpektor", "Maayan Zhitomirsky-Geffet."], "venue": "Natural Language Engineering, 16:359\u2013389.", "citeRegEx": "Kotlerman et al\\.,? 2010", "shortCiteRegEx": "Kotlerman et al\\.", "year": 2010}, {"title": "Semantic class learning from the web with hyponym pattern linkage graphs", "author": ["Zornitsa Kozareva", "Ellen Riloff", "Eduard Hovy."], "venue": "Proceedings of the 46th Annual Meeting of the ACL: HLT (ACL 2008), pages 1048\u20131056, Columbus, USA.", "citeRegEx": "Kozareva et al\\.,? 2008", "shortCiteRegEx": "Kozareva et al\\.", "year": 2008}, {"title": "Identifying hypernyms in distributional semantic spaces", "author": ["Alessandro Lenci", "Giulia Benotto."], "venue": "Proceedings of the First Joint Conference on Lexical and Computational Semantics (*SEM 2012), pages 75\u201379, Montr\u00e9al, Canada.", "citeRegEx": "Lenci and Benotto.,? 2012", "shortCiteRegEx": "Lenci and Benotto.", "year": 2012}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Proceedings of the 18th Conference on Natural Language Learning (CoNLL-2014), pages 171\u2013180, Baltimore, USA.", "citeRegEx": "Levy and Goldberg.,? 2014a", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Neural word embeddings as implicit matrix factorization", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Proceedings of NIPS.", "citeRegEx": "Levy and Goldberg.,? 2014b", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Applicative structure in vector space models", "author": ["M\u00e1rton Makrai", "D\u00e1vid Nemeskey", "Andr\u00e1s Kornai."], "venue": "Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality (CVSC), pages 59\u201363, Sofia, Bulgaria.", "citeRegEx": "Makrai et al\\.,? 2013", "shortCiteRegEx": "Makrai et al\\.", "year": 2013}, {"title": "SemEval-2010 Task 14: Word sense induction & disambiguation", "author": ["Suresh Manandhar", "Ioannis Klapaftis", "Dmitriy Dligach", "Sameer Pradhan."], "venue": "Proceedings of the 5th International Workshop on Semantic Evaluation, pages 63\u201368, Uppsala, Sweden.", "citeRegEx": "Manandhar et al\\.,? 2010", "shortCiteRegEx": "Manandhar et al\\.", "year": 2010}, {"title": "Relation guided bootstrapping of semantic lexicons", "author": ["Tara McIntosh", "Lars Yencken", "James R. Curran", "Timothy Baldwin."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL HLT 2011),", "citeRegEx": "McIntosh et al\\.,? 2011", "shortCiteRegEx": "McIntosh et al\\.", "year": 2011}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Proceedings of Workshop at the International Conference on Learning Representations, 2013, Scottsdale, USA.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig."], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language", "citeRegEx": "Mikolov et al\\.,? 2013c", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Composition in distributional models of semantics", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "Cognitive Science, 34(8):1388\u20131429.", "citeRegEx": "Mitchell and Lapata.,? 2010", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2010}, {"title": "A scalable hierarchical distributed language model", "author": ["Andriy Mnih", "Geoffrey E Hinton."], "venue": "Advances in neural information processing systems, pages 1081\u2013 1088.", "citeRegEx": "Mnih and Hinton.,? 2009", "shortCiteRegEx": "Mnih and Hinton.", "year": 2009}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["Andriy Mnih", "Koray Kavukcuoglu."], "venue": "Proceedings of NIPS.", "citeRegEx": "Mnih and Kavukcuoglu.,? 2013", "shortCiteRegEx": "Mnih and Kavukcuoglu.", "year": 2013}, {"title": "Reading between the lines: Overcoming data sparsity for accurate classification of lexical relationships", "author": ["Silvia Nec\u015fulescu", "Sara Mendes", "David Jurgens", "N\u00faria Bel", "Roberto Navigli."], "venue": "Proceedings of the Fourth Joint Conference on Lexical and Computa-", "citeRegEx": "Nec\u015fulescu et al\\.,? 2015", "shortCiteRegEx": "Nec\u015fulescu et al\\.", "year": 2015}, {"title": "Espresso: Leveraging generic patterns for automatically harvesting semantic relations", "author": ["Patrick Pantel", "Marco Pennacchiotti."], "venue": "Proceedings of COLING/ACL 2006, pages 113\u2013120, Sydney, Australia.", "citeRegEx": "Pantel and Pennacchiotti.,? 2006", "shortCiteRegEx": "Pantel and Pennacchiotti.", "year": 2006}, {"title": "Towards terascale semantic acquisition", "author": ["Patrick Pantel", "Deepak Ravichandran", "Eduard Hovy."], "venue": "Proceedings of the 20th International Conference on Computational Linguistics (COLING 2004), pages 771\u2013777, Geneva, Switzerland.", "citeRegEx": "Pantel et al\\.,? 2004", "shortCiteRegEx": "Pantel et al\\.", "year": 2004}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Distributional lexical entailment by topic coherence", "author": ["Laura Rimell."], "venue": "Proceedings of the 14th Conference of the EACL (EACL 2014), Gothenburg, Sweden.", "citeRegEx": "Rimell.,? 2014", "shortCiteRegEx": "Rimell.", "year": 2014}, {"title": "VMeasure: A conditional entropy-based external cluster evaluation measure", "author": ["Andrew Rosenberg", "Julia Hirschberg."], "venue": "Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Lan-", "citeRegEx": "Rosenberg and Hirschberg.,? 2007", "shortCiteRegEx": "Rosenberg and Hirschberg.", "year": 2007}, {"title": "Chasing hypernyms in vector spaces with entropy", "author": ["Enrico Santus", "Alessandro Lenci", "Qin Lu", "Sabine Schulte im Walde."], "venue": "Proceedings of the 14th Conference of the EACL (EACL 2014), pages 38\u201342.", "citeRegEx": "Santus et al\\.,? 2014", "shortCiteRegEx": "Santus et al\\.", "year": 2014}, {"title": "Using lexical and relational similarity to classify semantic relations", "author": ["Diarmuid \u00d3 S\u00e9aghdha", "Ann Copestake."], "venue": "Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 621\u2013629. Association for", "citeRegEx": "S\u00e9aghdha and Copestake.,? 2009", "shortCiteRegEx": "S\u00e9aghdha and Copestake.", "year": 2009}, {"title": "Learning syntactic patterns for automatic hypernym discovery", "author": ["Rion Snow", "Daniel Jurafsky", "Andrew Y. Ng."], "venue": "Advances in Neural Information Processing Systems 17 (NIPS-05), pages 1297\u20131304, Vancouver, Canada.", "citeRegEx": "Snow et al\\.,? 2005", "shortCiteRegEx": "Snow et al\\.", "year": 2005}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Advances in Neural Information Processing Systems 25 (NIPS-13).", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Introduction to Data Mining", "author": ["Pang-Ning Tan", "Michael Steinbach", "Vipin Kumar."], "venue": "Addison Wesley.", "citeRegEx": "Tan et al\\.,? 2006a", "shortCiteRegEx": "Tan et al\\.", "year": 2006}, {"title": "Extending corpus-based identification of light verb constructions using a supervised learning framework", "author": ["Yee Fan Tan", "Min-Yen Kan", "Hang Cui."], "venue": "Proceedings of the EACL 2006 Workshop on Multiword-expressions in a Multilingual Context, pages 49\u2013", "citeRegEx": "Tan et al\\.,? 2006b", "shortCiteRegEx": "Tan et al\\.", "year": 2006}, {"title": "Combining independent modules to solve multiple-choice synonym and analogy problems", "author": ["Peter D. Turney", "Michael L. Littman", "Jeffrey Bigham", "Victor Shnayder."], "venue": "Proceedings of the International Conference on Recent Advances in Natural Language Pro-", "citeRegEx": "Turney et al\\.,? 2003", "shortCiteRegEx": "Turney et al\\.", "year": 2003}, {"title": "Similarity of semantic relations", "author": ["Peter D. Turney."], "venue": "Computational Linguistics, 32(3):379416.", "citeRegEx": "Turney.,? 2006", "shortCiteRegEx": "Turney.", "year": 2006}, {"title": "Distributional semantics beyond words: Supervised learning of analogy and paraphrase", "author": ["Peter D. Turney."], "venue": "Transactions of the Association for Computational Linguistics, 1:353\u2013366.", "citeRegEx": "Turney.,? 2013", "shortCiteRegEx": "Turney.", "year": 2013}, {"title": "Visualizing data using t-SNE", "author": ["Laurens Van der Maaten", "Geoffrey Hinton."], "venue": "Journal of Machine Learning Research, 9(2579-2605):85.", "citeRegEx": "Maaten and Hinton.,? 2008", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "A tutorial on spectral clustering", "author": ["Ulrike Von Luxburg."], "venue": "Statistics and computing, 17(4):395\u2013416.", "citeRegEx": "Luxburg.,? 2007", "shortCiteRegEx": "Luxburg.", "year": 2007}, {"title": "Learning to distinguish hypernyms and co-hyponyms", "author": ["Julie Weeds", "Daoud Clarke", "Jeremy Reffin", "David Weir", "Bill Keller."], "venue": "Proceedings of the 25th International Conference on Computational Linguistics (COLING 2014).", "citeRegEx": "Weeds et al\\.,? 2014", "shortCiteRegEx": "Weeds et al\\.", "year": 2014}, {"title": "From information to knowledge: harvesting entities and relationships from web sources", "author": ["Gerhard Weikum", "Martin Theobald."], "venue": "Proceedings of the Twenty Ninth ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems, pages 65\u2013", "citeRegEx": "Weikum and Theobald.,? 2010", "shortCiteRegEx": "Weikum and Theobald.", "year": 2010}, {"title": "Rcnet: A general framework for incorporating knowledge into word representations", "author": ["Chang Xu", "Yanlong Bai", "Jiang Bian", "Bin Gao", "Gang Wang", "Xiaoguang Liu", "Tie-Yan Liu."], "venue": "Proceedings of the 23rd ACM Conference on Information and Knowledge", "citeRegEx": "Xu et al\\.,? 2014", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Automatic discovery of telic and agentive roles from corpus data", "author": ["Ichiro Yamada", "Timothy Baldwin."], "venue": "Proceedings of the 18th Pacific Asia Conference on Language, Information and Computation (PACLIC 18), pages 115\u2013126, Tokyo, Japan.", "citeRegEx": "Yamada and Baldwin.,? 2004", "shortCiteRegEx": "Yamada and Baldwin.", "year": 2004}, {"title": "Hypernym discovery based on distributional similarity and hierarchical structures", "author": ["Ichiro Yamada", "Kentaro Torisawa", "Jun\u2019ichi Kazama", "Kow Kuroda", "Masaki Murata", "Stijn De Saeger", "Francis Bond", "Asuka Sumida"], "venue": "In Proceedings of the 2009 Conference", "citeRegEx": "Yamada et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yamada et al\\.", "year": 2009}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["Mo Yu", "Mark Dredze."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014), pages 545\u2013550.", "citeRegEx": "Yu and Dredze.,? 2014", "shortCiteRegEx": "Yu and Dredze.", "year": 2014}, {"title": "Combining heterogeneous models for measuring relational similarity", "author": ["A. Zhila", "W.T. Yih", "C. Meek", "G. Zweig", "T. Mikolov."], "venue": "Proceedings of NAACLHLT.", "citeRegEx": "Zhila et al\\.,? 2013", "shortCiteRegEx": "Zhila et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 1, "context": "Accurate relation classification, relational similarity prediction, and wide-coverage and adaptable relation discovery can contribute to numerous NLP applications including paraphrasing and generation, machine translation, and ontology building (Banko et al., 2007; Hendrickx et al., 2010).", "startOffset": 245, "endOffset": 289}, {"referenceID": 18, "context": "Accurate relation classification, relational similarity prediction, and wide-coverage and adaptable relation discovery can contribute to numerous NLP applications including paraphrasing and generation, machine translation, and ontology building (Banko et al., 2007; Hendrickx et al., 2010).", "startOffset": 245, "endOffset": 289}, {"referenceID": 31, "context": "(2013a) and other neural language models have been to shown to perform well on an analogy completion task (Mikolov et al., 2013c; Mikolov et al., 2013b), in the space of relational similarity prediction (Turney, 2006).", "startOffset": 106, "endOffset": 152}, {"referenceID": 48, "context": ", 2013b), in the space of relational similarity prediction (Turney, 2006).", "startOffset": 59, "endOffset": 73}, {"referenceID": 30, "context": "The skip-gram model of Mikolov et al. (2013a) and other neural language models have been to shown to perform well on an analogy completion task (Mikolov et al.", "startOffset": 23, "endOffset": 46}, {"referenceID": 25, "context": "france vector that appears to encode CAPITAL-OF, presumably by cancelling out the features of paris that are France-specific, and retaining the features that distinguish a capital city (Levy and Goldberg, 2014a).", "startOffset": 185, "endOffset": 211}, {"referenceID": 21, "context": "K\u00f6per et al. (2015) found that a neural language model performed poorly on analogies involvar X iv :1 50 9.", "startOffset": 0, "endOffset": 20}, {"referenceID": 14, "context": "There may also be more fine-grained structure in the offsets: Fu et al. (2014) found that vector offsets representing the hypernym relation could be grouped into semantic sub-clusters, as the difference", "startOffset": 62, "endOffset": 79}, {"referenceID": 1, "context": "The relations may be pre-defined or, in the Open Information Extraction paradigm (Banko et al., 2007; Weikum and Theobald, 2010), the relations themselves are also learned from the text (e.", "startOffset": 81, "endOffset": 128}, {"referenceID": 53, "context": "The relations may be pre-defined or, in the Open Information Extraction paradigm (Banko et al., 2007; Weikum and Theobald, 2010), the relations themselves are also learned from the text (e.", "startOffset": 81, "endOffset": 128}, {"referenceID": 16, "context": "Relation learning is an important and long-standing task in NLP and has been the focus of a number of shared tasks (Girju et al., 2007; Hendrickx et al., 2010; Jurgens et al., 2012).", "startOffset": 115, "endOffset": 181}, {"referenceID": 18, "context": "Relation learning is an important and long-standing task in NLP and has been the focus of a number of shared tasks (Girju et al., 2007; Hendrickx et al., 2010; Jurgens et al., 2012).", "startOffset": 115, "endOffset": 181}, {"referenceID": 19, "context": "Relation learning is an important and long-standing task in NLP and has been the focus of a number of shared tasks (Girju et al., 2007; Hendrickx et al., 2010; Jurgens et al., 2012).", "startOffset": 115, "endOffset": 181}, {"referenceID": 36, "context": "WHOLE-PART, but also corpus-specific relations such as CEO-OF-COMPANY (Pantel and Pennacchiotti, 2006).", "startOffset": 70, "endOffset": 102}, {"referenceID": 16, "context": "Some datasets are task-specific, for example focused on paraphrasing the relation holding between nouns in noun-noun compounds (Girju et al., 2007), or analogy questions from the American SAT exam for relational similarity (Turney et", "startOffset": 127, "endOffset": 147}, {"referenceID": 17, "context": "Relation extraction has used pattern-based approaches such as A such as B, either explicitly (Hearst, 1992; Kozareva et al., 2008; McIntosh et al., 2011) or implicitly (Snow et al.", "startOffset": 93, "endOffset": 153}, {"referenceID": 23, "context": "Relation extraction has used pattern-based approaches such as A such as B, either explicitly (Hearst, 1992; Kozareva et al., 2008; McIntosh et al., 2011) or implicitly (Snow et al.", "startOffset": 93, "endOffset": 153}, {"referenceID": 29, "context": "Relation extraction has used pattern-based approaches such as A such as B, either explicitly (Hearst, 1992; Kozareva et al., 2008; McIntosh et al., 2011) or implicitly (Snow et al.", "startOffset": 93, "endOffset": 153}, {"referenceID": 43, "context": ", 2011) or implicitly (Snow et al., 2005), although not all relations are equally amenable to this style of approach (Yamada and Baldwin, 2004).", "startOffset": 22, "endOffset": 41}, {"referenceID": 55, "context": ", 2005), although not all relations are equally amenable to this style of approach (Yamada and Baldwin, 2004).", "startOffset": 83, "endOffset": 109}, {"referenceID": 6, "context": "sification involves supervised classifiers (Chklovski and Pantel, 2004; Snow et al., 2005; Davidov and Rappoport, 2008).", "startOffset": 43, "endOffset": 119}, {"referenceID": 43, "context": "sification involves supervised classifiers (Chklovski and Pantel, 2004; Snow et al., 2005; Davidov and Rappoport, 2008).", "startOffset": 43, "endOffset": 119}, {"referenceID": 10, "context": "sification involves supervised classifiers (Chklovski and Pantel, 2004; Snow et al., 2005; Davidov and Rappoport, 2008).", "startOffset": 43, "endOffset": 119}, {"referenceID": 42, "context": "Relational similarity prediction has also mostly used classification based on lexico-syntactic patterns linking word pairs in text (S\u00e9aghdha and Copestake, 2009; Jurgens et al., 2012; Turney, 2013), or generalised from manually crafted resources such as WordNet (Fellbaum, 1998) using techniques such as Latent Semantic Analysis (Turney, 2006; Chang et al.", "startOffset": 131, "endOffset": 197}, {"referenceID": 19, "context": "Relational similarity prediction has also mostly used classification based on lexico-syntactic patterns linking word pairs in text (S\u00e9aghdha and Copestake, 2009; Jurgens et al., 2012; Turney, 2013), or generalised from manually crafted resources such as WordNet (Fellbaum, 1998) using techniques such as Latent Semantic Analysis (Turney, 2006; Chang et al.", "startOffset": 131, "endOffset": 197}, {"referenceID": 49, "context": "Relational similarity prediction has also mostly used classification based on lexico-syntactic patterns linking word pairs in text (S\u00e9aghdha and Copestake, 2009; Jurgens et al., 2012; Turney, 2013), or generalised from manually crafted resources such as WordNet (Fellbaum, 1998) using techniques such as Latent Semantic Analysis (Turney, 2006; Chang et al.", "startOffset": 131, "endOffset": 197}, {"referenceID": 48, "context": ", 2012; Turney, 2013), or generalised from manually crafted resources such as WordNet (Fellbaum, 1998) using techniques such as Latent Semantic Analysis (Turney, 2006; Chang et al., 2013).", "startOffset": 153, "endOffset": 187}, {"referenceID": 5, "context": ", 2012; Turney, 2013), or generalised from manually crafted resources such as WordNet (Fellbaum, 1998) using techniques such as Latent Semantic Analysis (Turney, 2006; Chang et al., 2013).", "startOffset": 153, "endOffset": 187}, {"referenceID": 32, "context": "Distributional word vectors, while mostly applied to measuring semantic similarity and relatedness (Mitchell and Lapata, 2010),", "startOffset": 99, "endOffset": 126}, {"referenceID": 56, "context": ", 2014) and qualia structure (Yamada et al., 2009).", "startOffset": 29, "endOffset": 50}, {"referenceID": 31, "context": "ral word embeddings (Mikolov et al., 2013c) can be used to model word analogy tasks.", "startOffset": 20, "endOffset": 43}, {"referenceID": 30, "context": "ral word embeddings (Mikolov et al., 2013c) can be used to model word analogy tasks. This has given rise to a series of papers exploring the DIFFVEC idea in different contexts. The original analogy dataset has been used to evaluate neural language models by Mnih and Kavukcuoglu (2013) and also Zhila et al.", "startOffset": 21, "endOffset": 286}, {"referenceID": 30, "context": "ral word embeddings (Mikolov et al., 2013c) can be used to model word analogy tasks. This has given rise to a series of papers exploring the DIFFVEC idea in different contexts. The original analogy dataset has been used to evaluate neural language models by Mnih and Kavukcuoglu (2013) and also Zhila et al. (2013), who combine a neural language model", "startOffset": 21, "endOffset": 315}, {"referenceID": 14, "context": "Fu et al. (2014) similarly use embeddings to predict hypernym relations, but instead of using a single DIFFVEC, they cluster words by topic and show that the hypernym DIFFVEC can be broken", "startOffset": 0, "endOffset": 17}, {"referenceID": 4, "context": "Neural networks have also been developed for joint learning of lexical and relational similarity, making use of the WordNet relation hierarchy (Bordes et al., 2013; Socher et al., 2013; Xu et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2015; Fried and Duh, 2015).", "startOffset": 143, "endOffset": 266}, {"referenceID": 44, "context": "Neural networks have also been developed for joint learning of lexical and relational similarity, making use of the WordNet relation hierarchy (Bordes et al., 2013; Socher et al., 2013; Xu et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2015; Fried and Duh, 2015).", "startOffset": 143, "endOffset": 266}, {"referenceID": 54, "context": "Neural networks have also been developed for joint learning of lexical and relational similarity, making use of the WordNet relation hierarchy (Bordes et al., 2013; Socher et al., 2013; Xu et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2015; Fried and Duh, 2015).", "startOffset": 143, "endOffset": 266}, {"referenceID": 57, "context": "Neural networks have also been developed for joint learning of lexical and relational similarity, making use of the WordNet relation hierarchy (Bordes et al., 2013; Socher et al., 2013; Xu et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2015; Fried and Duh, 2015).", "startOffset": 143, "endOffset": 266}, {"referenceID": 11, "context": "Neural networks have also been developed for joint learning of lexical and relational similarity, making use of the WordNet relation hierarchy (Bordes et al., 2013; Socher et al., 2013; Xu et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2015; Fried and Duh, 2015).", "startOffset": 143, "endOffset": 266}, {"referenceID": 13, "context": "Neural networks have also been developed for joint learning of lexical and relational similarity, making use of the WordNet relation hierarchy (Bordes et al., 2013; Socher et al., 2013; Xu et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2015; Fried and Duh, 2015).", "startOffset": 143, "endOffset": 266}, {"referenceID": 25, "context": "Another strand of work responding to the vector difference approach has analysed the structure of neural embedding models in order to help explain their success on the analogy and other tasks (Levy and Goldberg, 2014a; Levy and Goldberg, 2014b; Arora et al., 2015).", "startOffset": 192, "endOffset": 264}, {"referenceID": 26, "context": "Another strand of work responding to the vector difference approach has analysed the structure of neural embedding models in order to help explain their success on the analogy and other tasks (Levy and Goldberg, 2014a; Levy and Goldberg, 2014b; Arora et al., 2015).", "startOffset": 192, "endOffset": 264}, {"referenceID": 0, "context": "Another strand of work responding to the vector difference approach has analysed the structure of neural embedding models in order to help explain their success on the analogy and other tasks (Levy and Goldberg, 2014a; Levy and Goldberg, 2014b; Arora et al., 2015).", "startOffset": 192, "endOffset": 264}, {"referenceID": 26, "context": "Makrai et al. (2013) divided antonym pairs into semantic classes such as quality, time, gender, and distance, and tested whether the DIFFVECs internal to each antonym class were significantly more correlated than random.", "startOffset": 0, "endOffset": 21}, {"referenceID": 26, "context": "Makrai et al. (2013) divided antonym pairs into semantic classes such as quality, time, gender, and distance, and tested whether the DIFFVECs internal to each antonym class were significantly more correlated than random. They found that for about two-thirds of the antonym classes, the DIFFVECs were significantly correlated. Nec\u015fulescu et al. (2015) trained a classifier on word pairs using word embeddings in order to predict coordinates, hypernyms, and meronyms.", "startOffset": 0, "endOffset": 351}, {"referenceID": 21, "context": "K\u00f6per et al. (2015) undertook a systematic study of morphosyntactic and semantic relations on word embeddings produced with word2vec (\u201cw2v\u201d hereafter; see \u00a73.", "startOffset": 0, "endOffset": 20}, {"referenceID": 21, "context": "K\u00f6per et al. (2015) undertook a systematic study of morphosyntactic and semantic relations on word embeddings produced with word2vec (\u201cw2v\u201d hereafter; see \u00a73.1) for English and German. They tested a variety of relations including word similarity, antonyms, synonyms, hypernyms, and meronyms, in a novel analogy task. Although the set of relations tested by K\u00f6per et al. (2015) is somewhat more constrained than the set we use, there is a good deal of overlap.", "startOffset": 0, "endOffset": 377}, {"referenceID": 30, "context": "well represented in the analogy dataset of Mikolov et al. (2013c)), but including morphosyntactic and morphosemantic relations (see \u00a73.", "startOffset": 43, "endOffset": 66}, {"referenceID": 30, "context": "We consider four highly successful word embedding models in our experiments: w2v (Mikolov et al., 2013a), GloVE (Pennington et al.", "startOffset": 81, "endOffset": 104}, {"referenceID": 38, "context": ", 2013a), GloVE (Pennington et al., 2014), SENNA", "startOffset": 16, "endOffset": 41}, {"referenceID": 8, "context": "(Collobert et al., 2011), and HLBL (Mnih and Hinton, 2009).", "startOffset": 0, "endOffset": 24}, {"referenceID": 33, "context": ", 2011), and HLBL (Mnih and Hinton, 2009).", "startOffset": 18, "endOffset": 41}, {"referenceID": 30, "context": "There is some overlap between our relations and those included in the analogy task of Mikolov et al. (2013c), but we include a much wider range of lexical semantic relations, especially those standardly evaluated in the relation classification literature.", "startOffset": 86, "endOffset": 109}, {"referenceID": 19, "context": "The final dataset consists of 12,943 triples \u3008relation,word1,word2\u3009, comprising 18 relation types, extracted from SemEval\u201912 (Jurgens et al., 2012), BLESS (Baroni et al.", "startOffset": 125, "endOffset": 147}, {"referenceID": 3, "context": ", 2012), BLESS (Baroni et al., 2014), the MSR analogy dataset (Mikolov et al.", "startOffset": 15, "endOffset": 36}, {"referenceID": 31, "context": ", 2014), the MSR analogy dataset (Mikolov et al., 2013c), the dataset of Tan et al.", "startOffset": 33, "endOffset": 56}, {"referenceID": 3, "context": ", 2012), BLESS (Baroni et al., 2014), the MSR analogy dataset (Mikolov et al., 2013c), the dataset of Tan et al. (2006a), Princeton WordNet, and Wiktionary, as listed in Tab 2 and detailed below (wherein we define each relation relative to the directed word pair (x, y)).", "startOffset": 16, "endOffset": 121}, {"referenceID": 19, "context": "Our dataset includes the seven top-level asymmetric lexical semantic relations from SemEval-2012 Task 2 (Jurgens et al., 2012): SEMEVALClass: x names a class that includes entity y; e.", "startOffset": 104, "endOffset": 126}, {"referenceID": 2, "context": "It also includes three lexical semantic relations from BLESS (Baroni and Lenci, 2011): BLESSHyper: x names a noun class that includes entity y; e.", "startOffset": 61, "endOffset": 85}, {"referenceID": 30, "context": "As morphosyntactic paradigm lexical relations, we include four relations from the original Mikolov et al. (2013c) DIFFVEC paper: NOUNSP: y is the plural form (NNS, in Penn tagset terms) of singular noun x (an NN); e.", "startOffset": 91, "endOffset": 114}, {"referenceID": 45, "context": "Relation Pairs Source SEMEVALClass 123 SemEval\u201912 SEMEVALPart 280 SemEval\u201912 SEMEVALAttr 71 SemEval\u201912 SEMEVALCase 255 SemEval\u201912 SEMEVALCause 255 SemEval\u201912 SEMEVALSpace 261 SemEval\u201912 SEMEVALRef 192 SemEval\u201912 BLESSHyper 1095 BLESS BLESSMero 2631 BLESS BLESSEvent 3163 BLESS NOUNSP 100 MSR VERB3 100 MSR VERBPast 100 MSR VERB3Past 100 MSR LVC 58 Tan et al. (2006b) VERBNOUN 3309 WordNet PREFIX 147 Wiktionary NOUNColl 257 Wiktionary", "startOffset": 348, "endOffset": 367}, {"referenceID": 40, "context": "We tune the hyperparameters over development data, selecting the configuration that maximises the V-Measure (Rosenberg and Hirschberg, 2007).", "startOffset": 108, "endOffset": 140}, {"referenceID": 7, "context": "Our use of V-Measure is based on the findings of Christodoulopoulos et al. (2010), who showed for part-of-speech induction that out of seven clustering evaluation measures, V-Measure is the most effective and least sensitive to the number of clusters.", "startOffset": 49, "endOffset": 82}, {"referenceID": 28, "context": "somewhat related lexical semantic clustering task of word sense induction, the best-performing systems in SemEval-2010 Task 4 (Manandhar et al., 2010) achieved a V-Measure of under 0.", "startOffset": 126, "endOffset": 150}, {"referenceID": 37, "context": "As observed by Pennington et al. (2014) and Curran (2004), training based on one-sided context reduces the ability of a model to capture lexical semantic relations in particular.", "startOffset": 15, "endOffset": 40}, {"referenceID": 9, "context": "(2014) and Curran (2004), training based on one-sided context reduces the ability of a model to capture lexical semantic relations in particular.", "startOffset": 11, "endOffset": 25}, {"referenceID": 37, "context": "experiments are presented in Tab 5, in which we report on the combination of the original (CLOSEDWORLD) and random (OPEN-WORLD) test data, noting that recall (R) for OPEN-WORLD takes the form of relative recall (Pantel et al., 2004) over the positively-classified word pairs.", "startOffset": 211, "endOffset": 232}, {"referenceID": 21, "context": "First, clustering showed us that many types of morphosyntactic and morphosemantic differences are captured by DIFFVECs, but that lexical semantic relations are captured less well, consistent with previous work (K\u00f6per et al., 2015).", "startOffset": 210, "endOffset": 230}], "year": 2017, "abstractText": "Recent work on word embeddings has shown that simple vector subtraction over pre-trained embeddings is surprisingly effective at capturing different lexical relations, despite lacking explicit supervision. Prior work has evaluated this intriguing result using a word analogy prediction formulation and hand-selected relations, but the generality of the finding over a broader range of lexical relation types and different learning settings has not been evaluated. In this paper, we carry out such an evaluation in two learning settings: (1) spectral clustering to induce word relations, and (2) supervised learning to classify vector differences into relation types. We find that word embeddings capture a surprising amount of information, and that, under suitable supervised training, vector subtraction generalises well to a broad range of relations, including over unseen lexical items.", "creator": "TeX"}}}