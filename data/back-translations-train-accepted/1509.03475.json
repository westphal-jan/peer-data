{"id": "1509.03475", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Sep-2015", "title": "Hessian-free Optimization for Learning Deep Multidimensional Recurrent Neural Networks", "abstract": "Multidimensional recurrent neural network (MDRNN) has shown a remarkable performance in speech and handwriting recognition. The performance of MDRNN is improved by further increasing its depth, and the difficulty of learning the deeper network is overcome by Hessian-free (HF) optimization. Considering that connectionist temporal classification (CTC) is utilized as an objective of learning MDRNN for sequence labelling, the non-convexity of CTC poses a problem to apply HF to the network. As a solution to this, a convex approximation of CTC is formulated and its relationship with the EM algorithm and the Fisher information matrix is discussed. MDRNN up to the depth of 15 layers is successfully trained using HF, resulting in improved performance for sequence labelling.", "histories": [["v1", "Fri, 11 Sep 2015 12:28:36 GMT  (26kb)", "http://arxiv.org/abs/1509.03475v1", "to appear at NIPS 2015"], ["v2", "Fri, 23 Oct 2015 07:14:04 GMT  (28kb)", "http://arxiv.org/abs/1509.03475v2", "to appear at NIPS 2015"]], "COMMENTS": "to appear at NIPS 2015", "reviews": [], "SUBJECTS": "cs.LG cs.NE stat.ML", "authors": ["minhyung cho", "chandra shekhar dhir", "jaehyung lee"], "accepted": true, "id": "1509.03475"}, "pdf": {"name": "1509.03475.pdf", "metadata": {"source": "CRF", "title": "Hessian-Free Optimization For Learning Deep Multidimensional Recurrent Neural Networks", "authors": ["Minhyung Cho Chandra", "Shekhar Dhir Jaehyung Lee"], "emails": ["mhyung.cho@gmail.com", "shekhardhir@gmail.com", "jaehyung.lee@kaist.ac.kr"], "sections": [{"heading": null, "text": "ar Xiv: 150 9.03 475v 1 [cs.L G] 11 SeMultidimensional recurrent neural network (MDRNN) shows remarkable performance in speech and handwriting recognition. MDRNN's performance is enhanced by further increasing its depth, and the difficulty of learning the deeper network is overcome by RF optimization. Considering that convectionist time classification (CTC) is used as a target for learning MDRNN for sequence marking, the non-convexity of CTC presents a problem in applying RF to the network. As a solution, a convex approximation of CTC is formulated and its relationship to the EM algorithm and the Fisher information matrix is discussed. MDRNN to the depth of 15 layers is successfully trained with RF, resulting in improved sequence marking performance."}, {"heading": "1 Introduction", "text": "Hdimensional Recursive Neural Network (MDRNN) is an efficient architecture for integrating multidimensional contexts into recursive neural networks [1]. End-to-end training of MDRNN in conjunction with Connectionist Temporal Classification (CTC) has shown that the performance of MDRNN in on / off-line handwriting recognition [2, 3] and speech recognition is state-of-the-art [4]. Previous approaches have demonstrated the performance of MDRNN in conjunction with networks with depths of up to 5 layers, which are relatively limited compared to recent advances in feedforward networks [5]. The effectiveness of deeper MDRNs beyond 5 layers was previously unknown. Training a deeper architecture has always been a challenging topic in machine learning. Remarkable breakthroughs have been achieved where deep forward neural networks have been initialized using layered prealgical networks [6]."}, {"heading": "2 Multidimensional recurrent neural networks", "text": "MDRNN is a generalization of RNN to process multi-dimensional data by replacing the single, recurring connection with as many connections as dimensions of the data [1]. The network can access the contextual information from 2N directions, allowing a collective decision to be made based on rich context information. In order to improve its ability to use context information, long-term short-term memory (LSTM) [8] cells are commonly used as hidden units. In addition, stacking MDRNs to build deeper networks further improves performance with increasing depth by reporting state-of-the-art performance using phoneme detection [4]. For sequence labeling, CTC is used as a loss function of MDRNN. The important advantage of using CTC is that no presegmented sequences are required and the entire transcription of the input sample is sufficient."}, {"heading": "2.1 Learning MDRNN", "text": "A d-dimensional MDRNN with M-inputs and K-outputs is considered as a mapping of an input sequence x-dimensional data x-RM \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 Td to an output sequence a-dimensional data (RK) T of length T, whereby input data for M-input neurons is given by vectorizing a d-dimensional data and T1.... Td is the length of the sequence in each dimension. All learnable weights and distortions are concatenated to obtain a parameter vector. In the learning phase with fixed training data, MDRNN is formalized as mapping N: RN \u2192 (RK) T from the parameters \u03b8 to the output sequence a, i.e. a = N (\u03b8). The scalable loss function is defined as L: (RK) T \u2192 R via the output sequence."}, {"heading": "2.2 Notation", "text": "The Jacobin JF of a function F: Rm \u2192 Rn is the n \u00b7 m matrix, in which each element is a partial derivative of an output element in relation to an input element. HF of a scalar function F: Rm \u2192 R is the m \u00b7 m matrix of partial derivatives of second-order output in relation to their inputs. Throughout the work, a symbol is used to denote the transposition of a vector or matrix. In the case of variables, a vector sequence is denoted by bold a, a vector at the time t in a by at and the k element of at by atk."}, {"heading": "3 Hessian-free optimization for MDRNN", "text": "In this section we will discuss two main points to develop the RF optimization for MDRNN. One of these is obtaining a local square approximation for MDRNN, and the other is an efficient calculation of the matrix-vector product used in each iteration of the conjugate gradient (CG). RF minimizes a goal by constructing a local square approximation to the objective function and minimizing the approximate function instead of the original one. Loss function (\u03b8) must be approximated at each point of n-th iteration as follows: Qn (\u03b8) = L (\u03b8n) = L | \u03b8n \u03b4n + 1 \u03b4 n G\u03b4n, (1) whereby a circular-shaped search direction is a parameter of optimization, and G is a local approximation to the curvature of L (\u0421) + a circular-based antiquity of antiquity, i.e."}, {"heading": "3.1 Quadratic approximation of loss function", "text": "The Hessian matrix, HLN, of the objective L (N HL) is written as HLN = J'NHLJN + KT \u2211 i = 1 [JL] iH [N] i, (2) where JN'RKT \u00b7 N, HL'RKT \u00b7 KT and [q] i denote the i-th component of the vector q. An indefinite Hessian matrix is problematic for 2nd order optimization because it defines an infinite local square approximation [13]. For non-linear systems, the Hessian is not necessarily positively semidefined, so the GN matrix is used as the approximation of the Hessian [11, 9].The GGN matrix is achieved by ignoring the second term in equivalence. (2), as given by GL'N = J'NHLJN, as approximation to the HN entrophic function, as the group DRL, the correspondence is perfect."}, {"heading": "3.2 Computation of matrix-vector product for MDRNN", "text": "The product of any vector v by the GGN matrix, Gv = J 'NHLJN v, amounts to the sequential multiplication of v with three matrices. First, the product JN v is a Jacobin time vector and therefore corresponds to the directional derivative of N (\u03b8) along the direction of v. Thus, JN v can be written with a differential operator JN v \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "4 Convex approximation of CTC for application to HF optimization", "text": "Connectioninst temporal classification (CTC) [14] provides an objective function of learning MDRNN for sequence marking. In this section we derive a convex approach to CTC inspired by the GGN approach. First, the non-convex part is separated from the original target by reformulation of the Softmax part. Next, the remaining convex part is approached without modifying its Hessian aspect, making it well adapted to the non-convex part. Finally, the convex approach is achieved by reuniting the convex and non-convex part."}, {"heading": "4.1 Connectionist temporal classification", "text": "The output activations in due time t are normalized with the softmax functionytk = exp (atk) \u2211 k \u2032 exp (a t k \u2032), (5) where ytk is the probability of the label k given in due time.The conditional probability of the path is calculated by multiplying the label sum of probabilities at any time, as given by p (\u03c0 | a) = T \u0442t = 1yt\u03c0t, (6) where \u03c0t is the label observed along the path at the time t. The path sequence of the length T is mapped to a label sequence of the length M \u2264 T by an operator B, which removes the repeated label and then corrects the label of the label."}, {"heading": "4.2 Reformulation of CTC objective function", "text": "By reformulation, the softmax function is defined by the categorical name sequences. By substituting Equation (5) in Equation (6), it follows that by substituting Equation (9) in Equation (7) and defining l = z, p (z | a), it is possible to rewrite. (9) by rewriting Equation (9) in Equation (7) and by defining l = z, p (z | a). (12) by rewriting asp (z | a) = sequence B \u2212 1 (z) exp (b) exp (b)."}, {"heading": "4.3 Convex approximation of CTC loss function", "text": "The GGN approximation of Eq. (13) immediately gives a convex approximation of Hessian for CTC as GLc. (GLc.) Nc = J'NcHLcJNc. Although HLc has the shape of a diagonal matrix plus a rank-1 matrix, i.e. diag (Y) \u2212 Y, the dimension of HLc is in Eq. (12) the resulting formula does not change its Hessian formula as the length of the sequence increases, making the practical calculation of HLc difficult. On the other hand, the distance of the linear team \u2212 fz from Lc (F) to Eq. (12) the resulting formula is Lp (F) = protocol (Z'S exp.) a positive block of HLc.) The GGN matrices of L = Lc."}, {"heading": "4.4 Sufficient condition for the proposed approximation to be exact", "text": "From Equation (2), the condition HLc \u0445 Nc = HLp \u0445 Nc applies. Since JLc is 6 = JLp in general, we consider only the case of H [Nc] i = 0 for all i, which corresponds to the case that Nc is a linear figure. [Nc] i contains a log sum Exp mapping of paths on a label sequence. If z is the label sequence that corresponds to [Nc] i, then [Nc] i = fz (..) for each label B \u2212 1 (z). If the probability of a path-Exp mapping is large enough to ignore all other paths, that is, exp (b\u03c0) for each condition b."}, {"heading": "4.5 Derivation of the proposed approximation from the Fisher information matrix", "text": "The identity of the GGN information matrix and the Fisher information matrix [16] has been shown for the network using the Softmax with cross entropy loss [17, 18], and it follows that the GGN matrix of Equation (13) is identical to the Fisher information matrix. Now, we show that the Fisher information matrix corresponds to the proposed matrix in Equation (14) under the condition in Section 4.4. The Fisher information matrix of MDRNN using CTC is written asF = Ex [J'NEL] p (l | a) [(jump log p (l | a) \u0430 (jump log p (l | a) \u0430 (jump | a)) jN], (15), where a = a (x) is the KT dimensional output of the N network. CTC assumes that the output probabilities in each timestep are independent of those in other periods."}, {"heading": "4.6 EM interpretation of the proposed approximation", "text": "The aim of the Expectation Maximization (EM) algorithm is to find the maximum probability solution for models with latent variables [19]. With an input sequence x and the corresponding target marking sequence z, the probability for z is derived from log p (z | x, \u03b8) = log \u00b2 B \u2212 1 (z) p (\u03c0 | x), where \u03b8 represents the model parameters. For each observation x, we have a corresponding latent variable q, which is a 1-of-k binary vector, where k is the number of all paths mapped to z. The log probability can be written in q as log p (z, q \u2212 x, \u03b8) = x \u00b2 B \u2212 1 (z) q\u03c0 | x, z log p (p).EM algorithm starts with an initial parameter."}, {"heading": "5 Experiments", "text": "In this section we present the experimental results for two different tasks of sequence labeling, offline handwriting recognition and phoneme recognition: The performance of the Hessen-free optimization for MDRNN with the proposed matrix is compared with that of stochastic gradient descend optimization (SGD) with the same settings."}, {"heading": "5.1 Database and preprocessing", "text": "The IFN / ENIT Database [20] is a database of handwritten Arabic words consisting of 32,492 images written by 411 writers; the entire data set consists of 5 subsets (a, b, c, d, e); the 25,955 images corresponding to the subsets (b \u2212 e) are used for the training; the validation set consists of 3,269 images corresponding to the first half of the sorted list in alphabetical order (ae07 001.tif \u2212 ai54 028.tif) in set a. The rest of the images in set a, which is 3,268, are used for the test; the intensity of the pixels is centered and scaled using the mean and standard deviation calculated from the training set; the TIMIT corpus [21] is a benchmark database for evaluating speech recognition performance; the standard training, validation and core data sets are used for the performance assessment, containing one set of 1996 sets each, and 3,626 sets of effective."}, {"heading": "5.2 Experimental setup", "text": "The number of LSTM cells in the extended layer was chosen to make the total number of weights between different networks similar to each other. Detailed architectures are described in Table 1 with Results. For phoneme detection, deep bi-directional LSTM and CTC were assumed in [4] as the basic architecture. Furthermore, the memory cell block [8] in which the cells share the gates was used for efficient information exchange. Each LSTM block was restricted to have 10 memory cells. We found that the use of a large number of pre-input / output gates is beneficial for training deep MDRNs. One possible explanation is that the activation of neurons was deformed exponentially by input / output gates. Setting large preset values for these gates can help transmit information through many layers at the beginning of learning.For this reason, biases of inputs and outputs for the input and output weights were balanced in DR1."}, {"heading": "5.2.1 Parameters", "text": "The Tikhonov damping was used in conjunction with the Levenberg-Marquardt heuristics; the value of the damping parameter \u03bb was initialized to 0.1 and adjusted according to the reduction ratio \u03c1 (multiplied by 0.9 if \u03c1 > 0.75, divided by 0.9 if \u03c1 < 0.25, and otherwise unchanged); the initial search direction for each CG run was set to 0.7 after the CG direction found by the previous HF iteration; to ensure that CG follows the downward direction, we continued to perform at least 5 and a maximum of 30 more CG iterations after finding the first downward direction; we terminated the CG at iteration i before the maximum iteration was reached if the following condition is met: (xi \u2212 5) / cepts (xi \u2212 5)."}, {"heading": "5.3 Results", "text": "The advantage of using RF is more pronounced with increasing depth. Improvements due to a deeper architecture can be seen in a decrease in the error rate from 6.1% to 4.5% with an increase in depth from 3 to 13, Table 2 shows the phoneme error rate (PER) on the core rate for phoneme detection. Improved performance according to depth can be observed for both optimization methods, with the best PER for RF being 18.54% for 15 layers and for SGD 18.46% for 10 layers comparable to that in [4], where the reported results are PER 18.6% from the 3 layers of 3.8 million weights network and PER 18.4% from the 5 layers of 6.8 million weights network. The benefit of a deeper IT network is evident in terms of the number of weight parameters, although this is not the small advantage that the comparison techniques tend to be unsuitable for data revision."}, {"heading": "6 Conclusion", "text": "To apply RF to CTC, a convex approximation of its objective function was investigated. Improvements in performance are seen as the network deepens for both RF and SGD. RF shows significantly better performance in handwriting recognition compared to SGD and comparable performance in speech recognition."}, {"heading": "A R operator to LSTM", "text": "We follow the version of LSTM in [4]. The forward trajectory of LSTM consists in calculating the following functions: it = \u03c3 (Wxixt + Whiht \u2212 1 + Wcict \u2212 1 + bi), ft = \u03c3 (Wxfxt + Whfht \u2212 1 + Wcf ct \u2212 1 + bf), ct = ft \u00b7 ct \u2212 1 + it \u00b7 tanh (Wxcxt + Whcht \u2212 1 + bc), ot = \u03c3 (Wxoxt + Whoht \u2212 1Wcoct + bo), ht = ot \u00b7 tanh (ct), where \u00b7 the elementary vector product is called, \u03c3 is the logistic sigmoid function, x, h \u2212 and c \u2212 are the input and activation vector, and i, o \u2212 and f are the input and output vector, and forget gates (Vht) (Vht) respectively. All gates and cells have the same size as the hidden vector hWh.Whhollow \u2212 Wxt is the Rhiv operator (Rhiv)."}, {"heading": "B Detailed derivation of the proposed approximation from the Fisher information matrix", "text": "The derivative of the negative log probability of Eq (7) is determined by \u2212 logbook p (s) (l) and \u03b2t (s) forward and backward variables (l) forward and backward variables (l) forward and backward variables (l) forward and backward variables (l) forward and backward (l) forward and backward (l) forward and backward (l) forward and backward (l) forward and backward (l) forward and backward (l) forward and backward (l) backward and backward (l) backward (l) forward and forward (l) forward)."}], "references": [{"title": "Supervised sequence labelling with recurrent neural networks, volume 385", "author": ["Alex Graves"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Unconstrained on-line handwriting recognition with recurrent neural networks", "author": ["Alex Graves", "Marcus Liwicki", "Horst Bunke", "J\u00fcrgen Schmidhuber", "Santiago Fern\u00e1ndez"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Offline handwriting recognition with multidimensional recurrent neural networks", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-ranhman Mohamed", "Geoffrey Hinton"], "venue": "In Proceedings of ICASSP,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Fitnets: Hints for thin deep nets", "author": ["Adriana Romero", "Nicolas Ballas", "Samira Ebrahimi Kahou", "Antoine Chassang", "Carlo Gatta", "Yoshua Bengio"], "venue": "CoRR, abs/1412.6550,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Deep learning via Hessian-free optimization", "author": ["James Martens"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Learning recurrent neural networks with Hessian-free optimization", "author": ["James Martens", "Ilya Sutskever"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Fast curvature matrix-vector products for second-order gradient descent", "author": ["Nicol N Schraudolph"], "venue": "Neural Computation,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Fast exact multiplication by the hessian", "author": ["Barak A Pearlmutter"], "venue": "Neural Computation,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1994}, {"title": "Training deep and recurrent networks with Hessian-free optimization", "author": ["James Martens", "Ilya Sutskever"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["Alex Graves", "Santiago Fern\u00e1ndez", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Convex Optimization", "author": ["Stephen Boyd", "Lieven Vandenberghe", "editors"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Natural gradient works efficiently in learning", "author": ["Shun-Ichi Amari"], "venue": "Neural computation,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "Revisiting natural gradient for deep networks", "author": ["Razvan Pascanu", "Yoshua Bengio"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Adaptive natural gradient learning algorithms for various stochastic models", "author": ["Hyeyoung Park", "S-I Amari", "Kenji Fukumizu"], "venue": "Neural Networks,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2000}, {"title": "Pattern Recognition and Machine Learning", "author": ["Christopher M. Bishop", "editor"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "IFN/ENIT-database of handwritten arabic words", "author": ["Mario Pechwitz", "S Snoussi Maddouri", "Volker M\u00e4rgner", "Noureddine Ellouze", "Hamid Amiri"], "venue": "In Proceedings of CIFED,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2002}, {"title": "Sequence transduction with recurrent neural networks", "author": ["Alex Graves"], "venue": "In ICML Representation Learning Workshop,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Speaker-independent phone recognition using hidden markov models", "author": ["Kai-Fu Lee", "Hsiao-Wuen Hon"], "venue": "IEEE Transactions on Acoustics, Speech and Signal Processing,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1989}, {"title": "Practical variational inference for neural networks", "author": ["Alex Graves"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Multidimensional recurrent neural network (MDRNN) is an efficient architecture to build multidimensional context into recurrent neural networks [1].", "startOffset": 144, "endOffset": 147}, {"referenceID": 1, "context": "End-to-end training of MDRNN in conjunction with connectionist temporal classification (CTC) has shown the state-of-the-art performance in on/off-line handwriting recognition [2, 3] and speech recognition [4].", "startOffset": 175, "endOffset": 181}, {"referenceID": 2, "context": "End-to-end training of MDRNN in conjunction with connectionist temporal classification (CTC) has shown the state-of-the-art performance in on/off-line handwriting recognition [2, 3] and speech recognition [4].", "startOffset": 175, "endOffset": 181}, {"referenceID": 3, "context": "End-to-end training of MDRNN in conjunction with connectionist temporal classification (CTC) has shown the state-of-the-art performance in on/off-line handwriting recognition [2, 3] and speech recognition [4].", "startOffset": 205, "endOffset": 208}, {"referenceID": 4, "context": "In previous approaches, the performance of MDRNN has been demonstrated with the networks having up to depth of 5 layers, which are relatively limited compared to the recent progress on feedforward networks [5].", "startOffset": 206, "endOffset": 209}, {"referenceID": 5, "context": "Notable breakthrough was made where deep feedforward neural networks were initialized using layer-wise pre-training [6].", "startOffset": 116, "endOffset": 119}, {"referenceID": 4, "context": "Recently, there has been approaches to add supervision to intermediate layers to train deep networks [5, 7].", "startOffset": 101, "endOffset": 107}, {"referenceID": 6, "context": "To our knowledge, no such pre-training or bootstrapping method has been developed for MDRNN which potentially utilizes LSTM cells [8] as its hidden unit.", "startOffset": 130, "endOffset": 133}, {"referenceID": 7, "context": "Alternatively, HF optimization is an appealing approach to train deep neural networks due to its ability to overcome pathological curvature of the objective function [9].", "startOffset": 166, "endOffset": 169}, {"referenceID": 7, "context": "The recent success of HF to deep feedforward and recurrent neural networks [9, 10] encourages the use of HF to MDRNN.", "startOffset": 75, "endOffset": 82}, {"referenceID": 8, "context": "The recent success of HF to deep feedforward and recurrent neural networks [9, 10] encourages the use of HF to MDRNN.", "startOffset": 75, "endOffset": 82}, {"referenceID": 0, "context": "2 Multidimensional recurrent neural networks MDRNN is a generalization of RNN to process multidimensional data by replacing the single recurrent connection with as many connections as dimensions of the data [1].", "startOffset": 207, "endOffset": 210}, {"referenceID": 6, "context": "To enhance its ability of exploiting context information, long short-term memory (LSTM) [8] cells are usually utilized as hidden units.", "startOffset": 88, "endOffset": 91}, {"referenceID": 3, "context": "In addition, stacking MDRNNs to construct deeper networks further improves the performance as the depth increases, reporting the state-of-the-art performance in phoneme recognition [4].", "startOffset": 181, "endOffset": 184}, {"referenceID": 9, "context": "For neural networks, an efficient way to compute Gv was proposed by [11], extending the work of [12].", "startOffset": 68, "endOffset": 72}, {"referenceID": 10, "context": "For neural networks, an efficient way to compute Gv was proposed by [11], extending the work of [12].", "startOffset": 96, "endOffset": 100}, {"referenceID": 11, "context": "An indefinite Hessian matrix is problematic for 2nd-order optimization because it defines an unbounded local quadratic approximation [13].", "startOffset": 133, "endOffset": 137}, {"referenceID": 9, "context": "For nonlinear systems, the Hessian is not necessarily positive semidefinite, thus the GGN matrix is used as an approximation of the Hessian [11, 9].", "startOffset": 140, "endOffset": 147}, {"referenceID": 7, "context": "For nonlinear systems, the Hessian is not necessarily positive semidefinite, thus the GGN matrix is used as an approximation of the Hessian [11, 9].", "startOffset": 140, "endOffset": 147}, {"referenceID": 11, "context": "In principle, it is best to define L and N in a way that L performs as much of the computation as possible, with the positive semidefiniteness of HL as a minimum requirement [13].", "startOffset": 174, "endOffset": 178}, {"referenceID": 9, "context": "In practice, a nonlinear output layer along with its matching loss function [11], such as the softmax function with cross-entropy loss, is widely used.", "startOffset": 76, "endOffset": 80}, {"referenceID": 10, "context": "Thus, JN v can be written using a differential operator JN v = Rv(N (\u03b8)) [12], and the properties of the operator can be utilized for efficient computation.", "startOffset": 73, "endOffset": 77}, {"referenceID": 12, "context": "Connectioninst temporal classification (CTC) [14] provides an objective function of learning MDRNN for sequence labelling.", "startOffset": 45, "endOffset": 49}, {"referenceID": 9, "context": "The function \u2212 log(y k), corresponding to the softmax with cross-entropy loss, is convex [11].", "startOffset": 89, "endOffset": 93}, {"referenceID": 13, "context": "Whereas log-concavity is closed under multiplication, the sum of log-concave functions is not log-concave in general [15].", "startOffset": 117, "endOffset": 121}, {"referenceID": 9, "context": "loss function matches the softmax output layer [11], the CTC objective is convex except the part which computes fz for each of the label sequences.", "startOffset": 47, "endOffset": 51}, {"referenceID": 14, "context": "5 Derivation of the proposed approximation from the Fisher information matrix The identity of the GGN and the Fisher information matrix [16] has been shown for the network using the softmax with cross-entropy loss [17, 18].", "startOffset": 136, "endOffset": 140}, {"referenceID": 15, "context": "5 Derivation of the proposed approximation from the Fisher information matrix The identity of the GGN and the Fisher information matrix [16] has been shown for the network using the softmax with cross-entropy loss [17, 18].", "startOffset": 214, "endOffset": 222}, {"referenceID": 16, "context": "5 Derivation of the proposed approximation from the Fisher information matrix The identity of the GGN and the Fisher information matrix [16] has been shown for the network using the softmax with cross-entropy loss [17, 18].", "startOffset": 214, "endOffset": 222}, {"referenceID": 0, "context": "CTC assumes output probabilities at each timestep to be independent of those at other timesteps [1], therefore its Fisher information matrix is given as the sum of every timestep.", "startOffset": 96, "endOffset": 99}, {"referenceID": 17, "context": "6 EM interpretation of the proposed approximation The goal of the Expectation-Maximization (EM) algorithm is to find the maximum likelihood solution for models having latent variables [19].", "startOffset": 184, "endOffset": 188}, {"referenceID": 18, "context": "1 Database and preprocessing IFN/ENIT Database [20] is a database of handwritten Arabic words, which consists of 32,492 images written by 411 writers.", "startOffset": 47, "endOffset": 51}, {"referenceID": 2, "context": "2 Experimental setup For handwriting recognition, the basic architecture was adopted from the one proposed in [3].", "startOffset": 110, "endOffset": 113}, {"referenceID": 3, "context": "For phoneme recognition, deep bidirectional LSTM and CTC in [4] was adopted as the basic architecture.", "startOffset": 60, "endOffset": 63}, {"referenceID": 6, "context": "Additionally, the memory cell block [8], in which the cells share the gates, was applied for efficient information sharing.", "startOffset": 36, "endOffset": 39}, {"referenceID": 0, "context": "To map output probabilities to a label sequence, best path decoding [1] was used for Arabic handwriting, and beam search decoding [4, 22] with the beam width of 100 was used for phoneme recognition.", "startOffset": 68, "endOffset": 71}, {"referenceID": 3, "context": "To map output probabilities to a label sequence, best path decoding [1] was used for Arabic handwriting, and beam search decoding [4, 22] with the beam width of 100 was used for phoneme recognition.", "startOffset": 130, "endOffset": 137}, {"referenceID": 19, "context": "To map output probabilities to a label sequence, best path decoding [1] was used for Arabic handwriting, and beam search decoding [4, 22] with the beam width of 100 was used for phoneme recognition.", "startOffset": 130, "endOffset": 137}, {"referenceID": 3, "context": "For phoneme recognition, 61 phoneme labels were used during training and decoding, and then mapped to 39 classes for calculating the phoneme error rate (PER) in Table 2 [4, 23] .", "startOffset": 169, "endOffset": 176}, {"referenceID": 20, "context": "For phoneme recognition, 61 phoneme labels were used during training and decoding, and then mapped to 39 classes for calculating the phoneme error rate (PER) in Table 2 [4, 23] .", "startOffset": 169, "endOffset": 176}, {"referenceID": 21, "context": "For phoneme recognition, the regularization method suggested in [24] was used.", "startOffset": 64, "endOffset": 68}, {"referenceID": 3, "context": "After that, the network was retrained with Gaussian weight noise [4].", "startOffset": 65, "endOffset": 68}, {"referenceID": 7, "context": "1 Parameters For HF optimization, we followed the basic setup described in [9], but different parameters were utilized.", "startOffset": 75, "endOffset": 78}, {"referenceID": 2, "context": "Note that in order to guarantee the convergence, we selected a conservative criteria compared to the reference where the network converged after 85 epochs in handwriting recognition [3] and after 55-150 epochs in phoneme recognition [4].", "startOffset": 182, "endOffset": 185}, {"referenceID": 3, "context": "Note that in order to guarantee the convergence, we selected a conservative criteria compared to the reference where the network converged after 85 epochs in handwriting recognition [3] and after 55-150 epochs in phoneme recognition [4].", "startOffset": 233, "endOffset": 236}, {"referenceID": 3, "context": "46% at 10 layers, which are comparable to the one in [4] where the reported results are PER 18.", "startOffset": 53, "endOffset": 56}], "year": 2017, "abstractText": "Multidimensional recurrent neural network (MDRNN) has shown a remarkable performance in speech and handwriting recognition. The performance of MDRNN is improved by further increasing its depth, and the difficulty of learning the deeper network is overcome by Hessian-free (HF) optimization. Considering that connectionist temporal classification (CTC) is utilized as an objective of learning MDRNN for sequence labelling, the non-convexity of CTC poses a problem to apply HF to the network. As a solution to this, a convex approximation of CTC is formulated and its relationship with the EM algorithm and the Fisher information matrix is discussed. MDRNN up to the depth of 15 layers is successfully trained using HF, resulting in improved performance for sequence labelling.", "creator": "LaTeX with hyperref package"}}}