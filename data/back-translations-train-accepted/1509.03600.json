{"id": "1509.03600", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Sep-2015", "title": "Hardness of Online Sleeping Combinatorial Optimization Problems", "abstract": "We show that several online combinatorial optimization problems that admit efficient no-regret algorithms become computationally hard in the sleeping setting where a subset of actions becomes unavailable in each round. Specifically, we show that the sleeping versions of these problems are at least as hard as PAC learning DNF expressions, a long standing open problem. We show hardness for the sleeping versions of Online Shortest Paths, Online Minimum Spanning Tree, Online k-Subsets, Online k-Truncated Permutations, Online Minimum Cut, and Online Bipartite Matching. The hardness result for the sleeping version of the Online Shortest Paths problem resolves an open problem presented at COLT 2015 (Koolen et al., 2015).", "histories": [["v1", "Fri, 11 Sep 2015 18:27:42 GMT  (207kb,D)", "http://arxiv.org/abs/1509.03600v1", null], ["v2", "Tue, 22 Sep 2015 18:12:37 GMT  (211kb,D)", "http://arxiv.org/abs/1509.03600v2", null], ["v3", "Mon, 19 Dec 2016 22:28:15 GMT  (209kb,D)", "http://arxiv.org/abs/1509.03600v3", "A version of this paper was published in NIPS 2016"]], "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["satyen kale", "chansoo lee", "d\u00e1vid p\u00e1l"], "accepted": true, "id": "1509.03600"}, "pdf": {"name": "1509.03600.pdf", "metadata": {"source": "CRF", "title": "Hardness of Online Sleeping Combinatorial Optimization Problems", "authors": ["Satyen Kale", "Chansoo Lee"], "emails": ["satyen@yahoo-inc.com", "chansool@umich.edu", "dpal@yahoo-inc.com"], "sections": [{"heading": "1 Introduction", "text": "The goal of the learner is to minimize the regret that defines the difference between the total loss of the algorithm and the loss of the best fixed action in retrospect. A particularly well-run instance system is the problem of the online shorts, in which the actions lie between the two fixed elements."}, {"heading": "1.1 Related Work", "text": "The default problem of linear optimization with d actions (expert settings) admits that algorithms with O (d) empala have problems per round and O (\u221a T log d) regret. (D) The default problem of linear optimization with d actions (expert settings) exists only to a limited extent. (D) The regret is known as minimax optimal (CesaBianchi and Lugosi, 2006, Chapter 2).Online combinatorial optimization problems over a number of d elements may have exp (O (d)) actions, and therefore a naive application of the algorithms for the experts who are asleep will lead to exp (O). (D) runtime and O (Td) regret that many problems (Online shortest path, Online spanning tree, Online k subsets, Online k-truncated permutations, Minimum Cut, Online bipartite matching) admit algorithms with 1 poly (per round) and poly (O)."}, {"heading": "1.2 Overview of the Paper", "text": "In Section 2, we define formally combinatorial optimization problems in the area of online sleeping. In Section 3, we present the problem of agnostic online learning of disjunctions and explain that a computationally efficient algorithm with sublinear action-related regret implies computationally efficient algorithms for learning DNF expressions. At the core of the paper is Section 4, where we reduce agnostic online learning of disjunctions to any combinatorial optimization problem in the area of online sleeping that allows instances of decision sets that fulfill two properties that capture the essence of computational hardness. In Section 5, we show that each of the six online sleep optimization problems (Online Shortest Paths, Online Minimum Spanning Tree, Online k Subsets, Online k-Truncated Permutations, Online Minimum Cut and Online Bipartite Matching) allows instances with decision sets that satisfy these two properties and thus determine their arithmetic hardness."}, {"heading": "2 Preliminaries", "text": "In each round, the online learner is required to choose an action Vt & D, while at the same time an opponent chooses a loss function \"t: U \u2192 [1, 1]. The loss of any V & D is defined by an action V & D. (V) The learner suffers a loss\" t \"(Vt) and receives\" t as feedback. The learner's regret regarding an action V & D is defined as if he has defined an action V & D. (V) \"t\" t \"t (V).We define an instance of the sleeping combinations. (V) In this setting, the opponent chooses a series of sleeping elements St & U and reveals them to the learner. (V)\" We define an instance of the sleeping combinations. (V) We define an instance of the sleeping combinations. (V) We define an instance of the sleeping combinations. (V) We define an instance of the sleeping combinations. (V) We define an instance of the sleeping combinations."}, {"heading": "3 Online Agnostic Learning of Disjunctions", "text": "To show that PAC learning DNF expressions reduce to get efficient algorithms for the online sleeping combinatorial optimization problems considered in this paper, we use an intermediate problem, online agnostic learning of disjunctions (Kearns et al., 1994), which in turn is at least as hard as PAC-learning of DNF expressions (Kalai et al., 2012).Online agnostic learning of disjunctions is a repeated game between the opponent and a learning algorithm. In each turn t the opponent selects a vector xt {0, 1} n, the algorithm that we predict a label y: 0, 1} and then the adversary of disjunctions is relevant."}, {"heading": "4 Base Hardness Result", "text": "Definition 1: We now show how to demonstrate the hardness of an online problem of sleeping optimization. (1) An instance of online combination optimization is designated as a hard instance with the parameter n, (1), (1), (1), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2, (2), (2), (2), (2), (2), (2), (), (2), (2), (2, (2), (2), (2), (), (2), (2, (2), (), (2), (2)."}, {"heading": "4.1 Non-negative Losses", "text": "While the ability to assign a negative loss to element 0 in algorithm B can set off a certain alarm, this is only done to keep the exposure clean. It is easy to see that the above hardness result goes through even if we limit the losses of each element to, say, non-negative, in the range [0, 2]. This is achieved by simply adding 1 to the loss of each waking element e in algorithm B. The only difference in the analysis is that (2) now does not (Vt) \u2265 1 [yt 6 = y-t] + n + 1, and (3) to \"t (V) = 1 [yt 6 = \u03c6 (xt)] + n + 1. The additive constant n + 1 cancels out in the calculation of the regret and therefore the calculations continue as before."}, {"heading": "5 Hardness Results", "text": "In this section, we apply Theorem 1 to prove that many combinatorial optimization problems when sleeping online are mathematically difficult. Note that all of these problems allow efficient no-regrets algorithms when not sleeping."}, {"heading": "5.1 Online Shortest Path Problem", "text": "In the online problem with the shortest path, the learner is given a directed graph G = (V, E) and a designated source and sink point s and t, which is determined over time. The principle is the set of edges, i.e. U = E, and the decision sentence D is the set of all paths from s to t. The dormant version of this problem was designated by Koolen et al. (2015) as the online problem with the shortest path, which raised the open question of whether it permits an efficient no-regrets algorithm. We show the following hardness result and solve the open question: Theorem 2. For each n-N, there is a hard instance with the parameter n of the online course problem with d = 3n + 2, and therefore the online sabotaged path problem is as difficult as the PAC learning result DNF expression.Proof: For each given positive integral n, we look at the graph G (n) in Figure 2. It has the 3 + 1, which are exactly written."}, {"heading": "5.2 Online Minimum Spanning Tree Problem", "text": "In fact, most of them are capable of moving, moving, moving, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing, rushing,"}, {"heading": "5.5 Online Bipartite Matching Problem", "text": "In the online matching path problem, the learner is given a fixed two-sided graph G = (V, E). The background specified here is the edge set, i.e. U = E, and the decision set D is the set of maximum matches in G. We prove the following hardness result: Theorem 6. For each n-N number, there is a hard instance with the parameter n of the online matching problem with d = 3n + 2, and therefore its dormant version is as difficult as the PAC learning of DNF expressions. Proof. Consider for each given positive integer n the chart M (n) in Figure 4. It has 3n + 2 edges passed through the elements of the base set U = n i = 1 {(i, 0), (i, 1), (i,?)}, {0, 1}, as required. Now, it must be noted that any maximum match in this graph has exactly n + 1, so that the gravitational property D satisfies."}, {"heading": "5.6 Online Minimum Cut Problem", "text": "In the online minimum intersection problem, the learner receives a fixed two-part graph G = (V, E) with a certain pair of vertices s and t. The basic effect specified here is the set of edges, i.e. U = E, and the decision sentence D is the set of intersections that separates s and t: An intersection is here a set of edges that, when removed from the graph, separates s from t. We prove the following hardness result: Theorem 7. For any positive integer n, there is a hard instance with the parameter n of the online minimum intersection problem with d = 3n + 2, and therefore its dormant version is as heavy as the PAC learning of DNF expressions. Proof. Consider for each given positive integer n the graph C (n) in Figure 5. It has 3n + 2 edges that are required by the elements of the basic set U = n = 1 (i, 0), (1), {n}."}, {"heading": "6 Conclusion", "text": "In this paper, we found that obtaining an efficient no-regret algorithm for dormant versions of several natural combinatorial online optimization problems is as difficult as PAC's efficient learning of DNF expressions, a long-standing problem. Our reduction techniques require only very modest conditions for hard cases of interest to the problem, and are in fact much more flexible than the specific form presented in this paper. We believe that almost any natural combinatorial optimization problem, which includes cases with exponentially many solutions, will be a hard problem in its online sleep variant. In addition, our hardness result via stochastic i.i.d. availability and loss is a rather harmless form of the adversary. This suggests that obtaining sublinear per-action regretting may be a fairly hard goal, and a search (a topic of future work) for suitable simplifications of the regret criterion or limitations of the adversary's power would enable the efficient algorithm to motivate."}], "references": [{"title": "Regret in online combinatorial optimization", "author": ["Jean-Yves Audibert", "Bubeck S\u00e9bastien", "G\u00e1bor Lugosi"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Audibert et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2013}, {"title": "Prediction, Learning and Games", "author": ["Nicol\u00f2 Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E. Schapire"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Freund and Schapire.,? \\Q1997\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1997}, {"title": "Using and combining predictors that specialize", "author": ["Yoav Freund", "Robert E. Schapire", "Yoram Singer", "Warmuth K. Manfred"], "venue": "In Proceedings of the 29th Annual ACM symposium on Theory of Computing,", "citeRegEx": "Freund et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Freund et al\\.", "year": 1997}, {"title": "Efficient algorithms for online decision problems", "author": ["Adam Kalai", "Santosh Vempala"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Kalai and Vempala.,? \\Q2005\\E", "shortCiteRegEx": "Kalai and Vempala.", "year": 2005}, {"title": "Reliable agnostic learning", "author": ["Adam Tauman Kalai", "Varun Kanade", "Yishay Mansour"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Kalai et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kalai et al\\.", "year": 2012}, {"title": "Learning hurdles for sleeping experts", "author": ["Varun Kanade", "Thomas Steinke"], "venue": "ACM Transactions on Computation Theory (TOCT),", "citeRegEx": "Kanade and Steinke.,? \\Q2014\\E", "shortCiteRegEx": "Kanade and Steinke.", "year": 2014}, {"title": "Sleeping experts and bandits with stochastic action availability and adversarial rewards", "author": ["Varun Kanade", "H. Brendan McMahan", "Brent Bryan"], "venue": "In Proceedings of the 12th International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Kanade et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kanade et al\\.", "year": 2009}, {"title": "Toward efficient agnostic learning", "author": ["Michael J. Kearns", "Robert E. Schapire", "Linda M. Sellie"], "venue": "Machine Learning,", "citeRegEx": "Kearns et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 1994}, {"title": "Regret bounds for sleeping experts and bandits", "author": ["Robert Kleinberg", "Alexandru Niculescu-Mizil", "Yogeshwer Sharma"], "venue": "Machine learning,", "citeRegEx": "Kleinberg et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kleinberg et al\\.", "year": 2010}, {"title": "Hedging structured concepts", "author": ["Wouter M. Koolen", "Manfred K. Warmuth", "Jyrki Kivinen"], "venue": "Proceedings of the 23th Conference on Learning Theory (COLT),", "citeRegEx": "Koolen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Koolen et al\\.", "year": 2010}, {"title": "Online sabotaged shortest path", "author": ["Wouter M. Koolen", "Manfred K. Warmuth", "Dmitry Adamskiy"], "venue": "In Proceedings of the 28th Conference on Learning Theory (COLT),", "citeRegEx": "Koolen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Koolen et al\\.", "year": 2015}, {"title": "The weighted majority algorithm", "author": ["Nick Littlestone", "Manfred K. Warmuth"], "venue": "Information and computation,", "citeRegEx": "Littlestone and Warmuth.,? \\Q1994\\E", "shortCiteRegEx": "Littlestone and Warmuth.", "year": 1994}, {"title": "Online combinatorial optimization with stochastic decision sets and adversarial losses", "author": ["Gergely Neu", "Michal Valko"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Neu and Valko.,? \\Q2014\\E", "shortCiteRegEx": "Neu and Valko.", "year": 2014}, {"title": "Path kernels and multiplicative updates", "author": ["Eiji Takimoto", "Manfred K. Warmuth"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Takimoto and Warmuth.,? \\Q2003\\E", "shortCiteRegEx": "Takimoto and Warmuth.", "year": 2003}], "referenceMentions": [{"referenceID": 11, "context": "The hardness result for the sleeping version of the Online Shortest Paths problem resolves an open problem presented at COLT 2015 (Koolen et al., 2015).", "startOffset": 130, "endOffset": 151}, {"referenceID": 10, "context": "The design of a computationally efficient algorithm for Online Sabotaged Shortest Path problem was presented as an open problem at COLT 2015 by Koolen et al. (2015). In this paper, we resolve this open problem and prove that Online Sabotaged Shortest Path problem is computationally hard.", "startOffset": 144, "endOffset": 165}, {"referenceID": 12, "context": "1 Related Work The standard problem of online linear optimization with d actions (Experts setting) admits algorithms with O(d) running time per-round and O( \u221a T log d) regret after T rounds (Littlestone and Warmuth, 1994; Freund and Schapire, 1997); the regret is known to be minimax optimal (CesaBianchi and Lugosi, 2006, Chapter 2).", "startOffset": 190, "endOffset": 248}, {"referenceID": 2, "context": "1 Related Work The standard problem of online linear optimization with d actions (Experts setting) admits algorithms with O(d) running time per-round and O( \u221a T log d) regret after T rounds (Littlestone and Warmuth, 1994; Freund and Schapire, 1997); the regret is known to be minimax optimal (CesaBianchi and Lugosi, 2006, Chapter 2).", "startOffset": 190, "endOffset": 248}, {"referenceID": 14, "context": "Despite this, many problems (Online Shortest Path, Online Spanning Tree, Online k-Subsets, Online k-Truncated Permutations, Online Minimum Cut, Online Bipartite Matching) admit algorithms with1 poly(d) running time per round and O(poly(d) \u221a T ) regret (Takimoto and Warmuth, 2003; Kalai and Vempala, 2005; Koolen et al., 2010; Audibert et al., 2013).", "startOffset": 252, "endOffset": 349}, {"referenceID": 4, "context": "Despite this, many problems (Online Shortest Path, Online Spanning Tree, Online k-Subsets, Online k-Truncated Permutations, Online Minimum Cut, Online Bipartite Matching) admit algorithms with1 poly(d) running time per round and O(poly(d) \u221a T ) regret (Takimoto and Warmuth, 2003; Kalai and Vempala, 2005; Koolen et al., 2010; Audibert et al., 2013).", "startOffset": 252, "endOffset": 349}, {"referenceID": 10, "context": "Despite this, many problems (Online Shortest Path, Online Spanning Tree, Online k-Subsets, Online k-Truncated Permutations, Online Minimum Cut, Online Bipartite Matching) admit algorithms with1 poly(d) running time per round and O(poly(d) \u221a T ) regret (Takimoto and Warmuth, 2003; Kalai and Vempala, 2005; Koolen et al., 2010; Audibert et al., 2013).", "startOffset": 252, "endOffset": 349}, {"referenceID": 0, "context": "Despite this, many problems (Online Shortest Path, Online Spanning Tree, Online k-Subsets, Online k-Truncated Permutations, Online Minimum Cut, Online Bipartite Matching) admit algorithms with1 poly(d) running time per round and O(poly(d) \u221a T ) regret (Takimoto and Warmuth, 2003; Kalai and Vempala, 2005; Koolen et al., 2010; Audibert et al., 2013).", "startOffset": 252, "endOffset": 349}, {"referenceID": 13, "context": "Policy regret is the total difference between the loss of the algorithm and the loss of the best policy, which maps a set of available actions to one of the available actions (Neu and Valko, 2014).", "startOffset": 175, "endOffset": 196}, {"referenceID": 9, "context": "Ranking regret is the total difference between the loss of the algorithm and the loss of a ranking of actions, which corresponds to a policy that chooses in each round the highest-ranked available action (Kleinberg et al., 2010; Kanade and Steinke, 2014; Kanade et al., 2009).", "startOffset": 204, "endOffset": 275}, {"referenceID": 6, "context": "Ranking regret is the total difference between the loss of the algorithm and the loss of a ranking of actions, which corresponds to a policy that chooses in each round the highest-ranked available action (Kleinberg et al., 2010; Kanade and Steinke, 2014; Kanade et al., 2009).", "startOffset": 204, "endOffset": 275}, {"referenceID": 7, "context": "Ranking regret is the total difference between the loss of the algorithm and the loss of a ranking of actions, which corresponds to a policy that chooses in each round the highest-ranked available action (Kleinberg et al., 2010; Kanade and Steinke, 2014; Kanade et al., 2009).", "startOffset": 204, "endOffset": 275}, {"referenceID": 3, "context": "Per-action regret, which we study in this paper, is the difference between the loss of the algorithm and the loss of an action, summed over only the rounds in which the action is available (Freund et al., 1997; Koolen et al., 2015).", "startOffset": 189, "endOffset": 231}, {"referenceID": 11, "context": "Per-action regret, which we study in this paper, is the difference between the loss of the algorithm and the loss of an action, summed over only the rounds in which the action is available (Freund et al., 1997; Koolen et al., 2015).", "startOffset": 189, "endOffset": 231}, {"referenceID": 3, "context": "First, there exists an algorithm with O(d) running time per round that achieves per-action regret of order O( \u221a T log d) (Freund et al., 1997).", "startOffset": 121, "endOffset": 142}, {"referenceID": 0, "context": ", 2010; Audibert et al., 2013). Common to these six problems is that their corresponding off-line problems have a polynomial-time algorithm. In fact, as shown by Kalai and Vempala (2005), a polynomial-time algorithm for an offline combinatorial problem implies the existence of an algorithm for the corresponding online optimization problem with the same per-round running time and O(poly(d) \u221a T ) regret.", "startOffset": 8, "endOffset": 187}, {"referenceID": 0, "context": ", 2010; Audibert et al., 2013). Common to these six problems is that their corresponding off-line problems have a polynomial-time algorithm. In fact, as shown by Kalai and Vempala (2005), a polynomial-time algorithm for an offline combinatorial problem implies the existence of an algorithm for the corresponding online optimization problem with the same per-round running time and O(poly(d) \u221a T ) regret. In studying online sleeping optimization, three different notions of regret have been used: (a) policy regret, (b) ranking regret, and (c) per-action regret. Policy regret is the total difference between the loss of the algorithm and the loss of the best policy, which maps a set of available actions to one of the available actions (Neu and Valko, 2014). Ranking regret is the total difference between the loss of the algorithm and the loss of a ranking of actions, which corresponds to a policy that chooses in each round the highest-ranked available action (Kleinberg et al., 2010; Kanade and Steinke, 2014; Kanade et al., 2009). Per-action regret, which we study in this paper, is the difference between the loss of the algorithm and the loss of an action, summed over only the rounds in which the action is available (Freund et al., 1997; Koolen et al., 2015). Note that policy regret upper bounds ranking regret, and per-action regret is incomparable to either policy or ranking regret. There are several results about the sleeping Experts setting (also known as Specialists setting). First, there exists an algorithm with O(d) running time per round that achieves per-action regret of order O( \u221a T log d) (Freund et al., 1997). Second, Kleinberg et al. (2010) and Kanade and Steinke (2014) show that achieving O(poly(d)T 1\u2212\u03b4) ranking regret is computationally hard, even under the assumption that the action availability and losses are drawn i.", "startOffset": 8, "endOffset": 1673}, {"referenceID": 0, "context": ", 2010; Audibert et al., 2013). Common to these six problems is that their corresponding off-line problems have a polynomial-time algorithm. In fact, as shown by Kalai and Vempala (2005), a polynomial-time algorithm for an offline combinatorial problem implies the existence of an algorithm for the corresponding online optimization problem with the same per-round running time and O(poly(d) \u221a T ) regret. In studying online sleeping optimization, three different notions of regret have been used: (a) policy regret, (b) ranking regret, and (c) per-action regret. Policy regret is the total difference between the loss of the algorithm and the loss of the best policy, which maps a set of available actions to one of the available actions (Neu and Valko, 2014). Ranking regret is the total difference between the loss of the algorithm and the loss of a ranking of actions, which corresponds to a policy that chooses in each round the highest-ranked available action (Kleinberg et al., 2010; Kanade and Steinke, 2014; Kanade et al., 2009). Per-action regret, which we study in this paper, is the difference between the loss of the algorithm and the loss of an action, summed over only the rounds in which the action is available (Freund et al., 1997; Koolen et al., 2015). Note that policy regret upper bounds ranking regret, and per-action regret is incomparable to either policy or ranking regret. There are several results about the sleeping Experts setting (also known as Specialists setting). First, there exists an algorithm with O(d) running time per round that achieves per-action regret of order O( \u221a T log d) (Freund et al., 1997). Second, Kleinberg et al. (2010) and Kanade and Steinke (2014) show that achieving O(poly(d)T 1\u2212\u03b4) ranking regret is computationally hard, even under the assumption that the action availability and losses are drawn i.", "startOffset": 8, "endOffset": 1703}, {"referenceID": 6, "context": "Under these assumptions, for the sleeping Experts setting, Kanade et al. (2009) give an algorithm running in poly(d) time per iteration with policy regret bounded by O( \u221a T log d), and for the general online sleeping combinatorial optimization setting, Neu and Valko (2014) give an algorithm running in poly(d) time per round and with policy regret bounded by O(m \u221a Td log d), where m is an upper bound on the size of each action.", "startOffset": 59, "endOffset": 80}, {"referenceID": 6, "context": "Under these assumptions, for the sleeping Experts setting, Kanade et al. (2009) give an algorithm running in poly(d) time per iteration with policy regret bounded by O( \u221a T log d), and for the general online sleeping combinatorial optimization setting, Neu and Valko (2014) give an algorithm running in poly(d) time per round and with policy regret bounded by O(m \u221a Td log d), where m is an upper bound on the size of each action.", "startOffset": 59, "endOffset": 274}, {"referenceID": 6, "context": "Our reduction technique is closely related to that of Kanade and Steinke (2014), who reduced agnostic learning of disjunctions to ranking regret minimization in the sleeping Experts setting.", "startOffset": 54, "endOffset": 80}, {"referenceID": 6, "context": "By a standard online-to-batch conversion argument (Kanade and Steinke, 2014), online agnostic learning of disjunctions is at least as hard as agnostic improper PAC-learning of disjunctions (Kearns et al.", "startOffset": 50, "endOffset": 76}, {"referenceID": 8, "context": "By a standard online-to-batch conversion argument (Kanade and Steinke, 2014), online agnostic learning of disjunctions is at least as hard as agnostic improper PAC-learning of disjunctions (Kearns et al., 1994), which in turn is at least as hard as PAC-learning of DNF expressions (Kalai et al.", "startOffset": 189, "endOffset": 210}, {"referenceID": 5, "context": ", 1994), which in turn is at least as hard as PAC-learning of DNF expressions (Kalai et al., 2012).", "startOffset": 78, "endOffset": 98}, {"referenceID": 6, "context": "The online-to-batch conversion of online agnostic learning of disjunctions to agnostic improper PAC-learning of disjunctions (Kanade and Steinke, 2014) mentioned above implies that we may assume that the input sequence (xt, yt) for the online problem is drawn i.", "startOffset": 125, "endOffset": 151}, {"referenceID": 10, "context": "The sleeping version of this problem has been called the Online Sabotaged Shortest Path problem by Koolen et al. (2015), who posed the open question of whether it admits an efficient no-regret algorithm.", "startOffset": 99, "endOffset": 120}], "year": 2017, "abstractText": "We show that several online combinatorial optimization problems that admit efficient noregret algorithms become computationally hard in the sleeping setting where a subset of actions becomes unavailable in each round. Specifically, we show that the sleeping versions of these problems are at least as hard as PAC learning DNF expressions, a long standing open problem. We show hardness for the sleeping versions of Online Shortest Paths, Online Minimum Spanning Tree, Online k-Subsets, Online k-Truncated Permutations, Online Minimum Cut, and Online Bipartite Matching. The hardness result for the sleeping version of the Online Shortest Paths problem resolves an open problem presented at COLT 2015 (Koolen et al., 2015).", "creator": "LaTeX with hyperref package"}}}