{"id": "1509.04397", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Sep-2015", "title": "Exponential Family Matrix Completion under Structural Constraints", "abstract": "We consider the matrix completion problem of recovering a structured matrix from noisy and partial measurements. Recent works have proposed tractable estimators with strong statistical guarantees for the case where the underlying matrix is low--rank, and the measurements consist of a subset, either of the exact individual entries, or of the entries perturbed by additive Gaussian noise, which is thus implicitly suited for thin--tailed continuous data. Arguably, common applications of matrix completion require estimators for (a) heterogeneous data--types, such as skewed--continuous, count, binary, etc., (b) for heterogeneous noise models (beyond Gaussian), which capture varied uncertainty in the measurements, and (c) heterogeneous structural constraints beyond low--rank, such as block--sparsity, or a superposition structure of low--rank plus elementwise sparseness, among others. In this paper, we provide a vastly unified framework for generalized matrix completion by considering a matrix completion setting wherein the matrix entries are sampled from any member of the rich family of exponential family distributions; and impose general structural constraints on the underlying matrix, as captured by a general regularizer $\\mathcal{R}(.)$. We propose a simple convex regularized $M$--estimator for the generalized framework, and provide a unified and novel statistical analysis for this general class of estimators. We finally corroborate our theoretical results on simulated datasets.", "histories": [["v1", "Tue, 15 Sep 2015 04:49:57 GMT  (1204kb)", "http://arxiv.org/abs/1509.04397v1", "20 pages, 9 figures"]], "COMMENTS": "20 pages, 9 figures", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["suriya gunasekar", "pradeep ravikumar", "joydeep ghosh"], "accepted": true, "id": "1509.04397"}, "pdf": {"name": "1509.04397.pdf", "metadata": {"source": "CRF", "title": "Exponential Family Matrix Completion under Structural Constraints", "authors": ["Suriya Gunasekar", "Pradeep Ravikumar", "Joydeep Ghosh"], "emails": ["SURIYA@UTEXAS.EDU", "PRADEEPR@CS.UTEXAS.EDU", "GHOSH@ECE.UTEXAS.EDU"], "sections": [{"heading": null, "text": "ar Xiv: 150 9.04 397v 1 [stat.ML] 1 5SE p20 15Keywords: matrix completion, exponential families, high dimensional prediction, approach to low ranks, minimization of nuclear standards"}, {"heading": "1. Introduction", "text": "It is only a matter of time before it is as far as it has ever been, until it is so far."}, {"heading": "1.1 Notations and Preliminaries", "text": "In this subsection, we describe the notations and definitions commonly used throughout the work. Matrices are denoted by uppercase letters, X, VP, M, etc. For a matrix, M \u2212 remains are the jth column and ith line of M, and Mij denotes the (i, j) th input of M. The transpose, track, and rank of a matrix M are denoted by M \u2020, tr (M), and rk (M), respectively, the inner product between two matrices is denoted by < X, Y > = tr (X \u2020 Y) = random, and precedence of a matrix M. For a matrix M, Rm \u00b7 n in precedence r, with singular values."}, {"heading": "2. Exponential Family Matrix Completion", "text": "we specify the underlying target matrix using the examples described above from the natural exponential family (see definition 1): P (Xij | servation ij) = h (Xij) exp. (Xij) exp. (Xij) exp. (Xij) exp. (Xij) exp. (Xij) exp. (Xij) exp. (Xij) exp. (X) exp. (Xij) exp. (Xij) exp. (Xij) exp. (Xij) exp. (Xij) exp. (Xij) exp. (Xij) exp. (Xij) exp.) exp. (Xij exp.) exp. (Xij) exp.) exp. (Xij) exp. (Xij) exp. (Xij) exp. (Xij) exp. exp. (X. exp. exp. (X. exp.) exp. (X. exp. exp. exp. (X.) exp."}, {"heading": "2.1 Applications", "text": "Gaussian (fixed \u03c32) is typically used to model continuous data, x-R, such as measurements with additive errors, affinity data sets. Here, G (\u03b8) = 12\u03c3 2\u03b82.Bernoulli is a popular distribution of choice for modelling binary data, x-0, 1}, with G-1 = log (1 + e\u03b8). Some examples of data suitable for the Bernoulli model include social networks, gene-protein interactions, etc. Binomial (fixed N) is used to model the number of successes in N studies. Here, x-0, 1, 2,...., and G-1 = N log (1 + e\u043a) are used. Some applications include predicting success / failure rates, survey results, etc. Poisson is used to model data x-0, 1, 2,.., as arrival times, per click time, G-through time, etc."}, {"heading": "2.2 Log\u2013likelihood", "text": "Identify the gradient map: g (\u0443), \u0435G (\u0441), ESTATE (STATE), whose mean and variance are given by E [X] = g (\u0432 \u043a), and Var (X) = \u0442 2G (\u043a \u043a). The fennel conjugate of the log partition function G is indicated by: F (X), sup\u0440 < X, \u0432 > \u2212 G (\u0432). A useful consequence of the exponential family is that the negative log probability is convex in natural parameters, as well as by a bijection with a large class of Bregman divergences (definition 2)."}, {"heading": "2.3 Discussion and directions for future work", "text": "We look at the default matrix completion, in which the distribution of the observation matrix X in (2) the entries take place independently of the other entries. The probability of observing a particular entry is high, if a user gives a bad rating, he could vary in a completely different region of the matrix. It would be interesting to extend the analysis of this paper in such a way."}, {"heading": "3. Main Result and Consequences", "text": "It is not just the way in which it is about the question, whether it is about the question, whether it is about the question, whether it is about the question, whether it is about the question, whether it is about the question, whether it is about the question, whether it is about the question, whether it is about the question, whether it is about the question, whether it is about the question, whether it is about the question, whether it is about the question, whether it is about the question, whether it is about the question, whether it is about the question, whether it is about the question, whether it is about the question, whether it is about the question, whether it is about the question, whether it is about the question, whether it is about the question, whether it is about the question, whether it is about the question after the question, whether it is about the question, whether it is about the question after the question, whether it is about the question after the question, whether it is about the question after the question, whether it is about the question after the question, whether it is about the question after the question after the question, whether it is about the question after the question after the question after the question, whether it is after the question after the question after the question after the question after the question after the question, whether it is after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question, whether it is after the question after the question after the question after the question after the question after the question after the question after the question after question after the question after question after question after the question after the question after question after question after the question after the question after question after the question after the question, whether it is after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question, whether it is after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after"}, {"heading": "3.1 M\u2013estimator for Generalized Matrix Completion", "text": "We propose a regularized M estimate as our candidate parameter matrix.The standards regulator R (.) used is a convex substitute for the structural constraints and is assumed to satisfy A 1. For a suitable \u03bb > 0 matrix, the standards regulator R (.) is a convex substitute for the structural constraints and is assumed to satisfy A 1. (6) For the sake of simplicity, we have assumed that the domain of the minimizing function extends over whole or all of Rm \u00b7 n. In cases where this is violated, additional constraints may be imposed to restrict to the domain and the results and analyses in the following section."}, {"heading": "3.2 Main Results", "text": "Without loss of universality, let's assume that m \u2264 n. Let R \u0445 (.) = > supR (X) \u2264 1 < X,. > the dual standard of the regulator R (.). Let's also define the maximum and minimum subspace compatibility constants of R w.r.t of the model subspace M (definition 3). - Next, let's define the following quantity: B (n) and B (B): E [n] mn | R (R) compatible constants of R w.r.t of the model subspace M (definition 3), with the expectation being higher than the random sampling index that was set, and above the Rademacher sequence (i, j)."}, {"heading": "3.3 Corollary", "text": "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *"}, {"heading": "4. Proof", "text": "In this section we will present the most important steps in the evidence of the main results (Section 3.2-3.3). Evidence of intermediate lemmats will be moved to the appendix."}, {"heading": "4.1 Proof of Theorem 1", "text": "The proof of our main theorems includes two key steps: \u2022 We show first that the adoption 1-3, RSC of the form in definition 4 holds the loss function in (6) over a large subset of the solution space. \u2022 If the RSC condition applies, the result results from a few simple calculations; we handle the case in which the RSC does not keep separately. \u2022 Let's have two results of interest.Lemma 1. We define the following subset: V = \"Rm\" n: R (\"M\"). (\"M\"). (\"M\"). (\"M\"). (\"M\") of adoption 1, and \"M.\" (\"M\") is the projection of \"M.\" If the minimizers of (6), and \"R.\" (\"M\"). (\"M\"). (\"M\"). (\"M\")."}, {"heading": "4.2 Proof of Corollary 1", "text": "From the definition of M (7) it follows that M = Span = Span = Span and Row Spaces (U) = Column (V) = Column (V) = Column (V) = Column (V) = Column (V) = Column (V) = Column (V) = Column (V) = Column (V) = Column (V) = Column (V) = Column (V) = Column (V) = Column (V) = Column (V) = Column (V) = Column (V) = Column (V) = Column (V) = Column (V) = Column (V) = Column (V) = Column (V)."}, {"heading": "4.3 Proof of Theorem 2", "text": "This evidence uses symmetry arguments and contractions (Ledoux and Talagrand (1991) Ch.4 & 6. We observe that the symmetry arguments and contractions (Ledoux and Talagrand (1991) Ch.4 & 6. We observe that the symmetry arguments and contractions (Ledoux and Talagrand (1991) Ch.4 & 6. We observe that the symmetry arguments (i, j) and contractions (Ledoux and Talagrand) (Ledoux and Talagrand) (1991), that the symmetry risk arguments (i, j) and contractions (Ledoux and Talagrand (1991) and the symmetry risk arguments (i, j) and contractions (Ledoux and Talagrand) and the symmetry risk arguments (1991) and contractions (Ledoux and Talagrand)."}, {"heading": "5. Experiments", "text": "We offer simulated experiments to confirm our theoretical guarantees, focusing specifically on episode 1, where we consider the specific case where the underlying parameter matrix has a low rank, but the underlying noise model for the matrix elements could be any of the general class of exponential family distributions. We consider three well-known members of an exponential family suitable for different data types, namely Gaussian, Bernoulli, and Binomial, which are popular choices for modeling continuously estimated, binary, and numerically valuable data."}, {"heading": "5.1 Experimental Setup", "text": "We create low-level ground-truth parameter matrices of quantities n = 50, 100, 150, 200} (for simplicity, we consider square matrices, m = n); we set the rank to r = 2 log n. The observation matrices X are then sampled from the various members of the exponential family distribution parameters, selecting a subset of the observation matrix X for each n at random and estimating the error size from (6). Rating: For each member of the exponential family of the observed distributions, we can measure the performance of our M estimator in the parameter space and the observation space using an appropriate error size (X, X), where X is the maximum probability estimate of the restored corridors."}, {"heading": "Acknowledgments", "text": "Pradeep Ravikumar appreciates the support of ARO via W911NF-12-1-0390 and NSF via IIS-1149803, IIS-1320894, DMS-1264033."}, {"heading": "Appendix A. Proofs of Lemma", "text": "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0"}, {"heading": "Appendix B. Additional Experimental Results", "text": "We provide the additional experimental results where we compare the error of the estimation in the parameter space. We first record the results against the percentage of collected entries observed in Section 5. Again, we find that the curves (for different n) align and converge in view of the \"normalized\" sample size (left), which confirms the theoretical results. Note that the curves do not align with the unnormalized sample size (right). Similar to the errors in the observatory with the samples on pages 1 and 1.5 rn, the error parameters fall to a sufficiently small value."}], "references": [{"title": "Strong converse for identification via quantum channels", "author": ["R. Ahlswede", "A. Winter"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Ahlswede and Winter.,? \\Q2002\\E", "shortCiteRegEx": "Ahlswede and Winter.", "year": 2002}, {"title": "Clustering with bregman divergences", "author": ["A. Banerjee", "S. Merugu", "I.S. Dhillon", "J. Ghosh"], "venue": "JMLR, 6:1705\u20131749,", "citeRegEx": "Banerjee et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2005}, {"title": "Matrix completion with noise", "author": ["E.J. Candes", "Y. Plan"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Candes and Plan.,? \\Q2010\\E", "shortCiteRegEx": "Candes and Plan.", "year": 2010}, {"title": "Exact matrix completion via convex optimization", "author": ["E.J. Candes", "B. Recht"], "venue": "Foundations of Computational mathematics,", "citeRegEx": "Candes and Recht.,? \\Q2009\\E", "shortCiteRegEx": "Candes and Recht.", "year": 2009}, {"title": "The power of convex relaxation: near-optimal matrix completion", "author": ["E.J. Candes", "T. Tao"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Candes and Tao.,? \\Q2010\\E", "shortCiteRegEx": "Candes and Tao.", "year": 2010}, {"title": "Robust principal component analysis", "author": ["E.J. Candes", "X. Li", "Y. Ma", "J. Wright"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Candes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Candes et al\\.", "year": 2011}, {"title": "A generalization of principal components analysis to the exponential family", "author": ["M. Collins", "S. Dasgupta", "R.E. Schapire"], "venue": "In NIPS,", "citeRegEx": "Collins et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2001}, {"title": "1-bit matrix completion", "author": ["M.A. Davenport", "Y. Plan", "E. Berg", "M. Wootters"], "venue": "arXiv preprint arXiv:1209.3672,", "citeRegEx": "Davenport et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Davenport et al\\.", "year": 2012}, {"title": "A rank minimization heuristic with application to minimum order system approximation", "author": ["M. Fazel", "H Hindi", "S.P. Boyd"], "venue": "In American Control Conference,", "citeRegEx": "Fazel et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Fazel et al\\.", "year": 2001}, {"title": "Relative expected instantaneous loss bounds", "author": ["J. Forster", "M. Warmuth"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Forster and Warmuth.,? \\Q2002\\E", "shortCiteRegEx": "Forster and Warmuth.", "year": 2002}, {"title": "Generalized\u02c6 2 linear\u02c6 2 models", "author": ["G.J. Gordon"], "venue": "In NIPS, pages 577\u2013584,", "citeRegEx": "Gordon.,? \\Q2002\\E", "shortCiteRegEx": "Gordon.", "year": 2002}, {"title": "Low-rank matrix completion using alternating minimization", "author": ["P. Jain", "P. Netrapalli", "S. Sanghavi"], "venue": "In STOC,", "citeRegEx": "Jain et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2013}, {"title": "Learning exponential families in highdimensions: Strong convexity and sparsity", "author": ["S.M. Kakade", "O. Shamir", "K. Sridharan", "A. Tewari"], "venue": "In AISTATS, JMLR Workshop and Conference Proceedings,", "citeRegEx": "Kakade et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2010}, {"title": "Matrix completion from a few entries", "author": ["R.H. Keshavan", "A. Montanari", "S. Oh"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Keshavan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Keshavan et al\\.", "year": 2010}, {"title": "Matrix completion from noisy entries", "author": ["R.H. Keshavan", "A. Montanari", "S. Oh"], "venue": null, "citeRegEx": "Keshavan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Keshavan et al\\.", "year": 2010}, {"title": "Matrix completion problems", "author": ["M. Laurent"], "venue": "Encyclopedia of Optimization,", "citeRegEx": "Laurent.,? \\Q2009\\E", "shortCiteRegEx": "Laurent.", "year": 2009}, {"title": "Probability in Banach Spaces: isoperimetry and processes, volume 23", "author": ["M. Ledoux", "M. Talagrand"], "venue": null, "citeRegEx": "Ledoux and Talagrand.,? \\Q1991\\E", "shortCiteRegEx": "Ledoux and Talagrand.", "year": 1991}, {"title": "Probabilistic matrix factorization", "author": ["A. Mnih", "R. Salakhutdinov"], "venue": "In NIPS, pages 1257\u20131264,", "citeRegEx": "Mnih and Salakhutdinov.,? \\Q2007\\E", "shortCiteRegEx": "Mnih and Salakhutdinov.", "year": 2007}, {"title": "Bayesian exponential family pca", "author": ["S. Mohamed", "Z. Ghahramani", "K.A. Heller"], "venue": "In NIPS,", "citeRegEx": "Mohamed et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mohamed et al\\.", "year": 2008}, {"title": "Structured Estimation in High-Dimensions", "author": ["S. Negahban"], "venue": "PhD thesis, EECS Department,", "citeRegEx": "Negahban.,? \\Q2012\\E", "shortCiteRegEx": "Negahban.", "year": 2012}, {"title": "Joint support recovery under high-dimensional scaling: Benefits and perils of l1,-regularization", "author": ["S. Negahban", "M.J. Wainwright"], "venue": null, "citeRegEx": "Negahban and Wainwright.,? \\Q2008\\E", "shortCiteRegEx": "Negahban and Wainwright.", "year": 2008}, {"title": "Restricted strong convexity and weighted matrix completion: Optimal bounds with noise", "author": ["S. Negahban", "M.J. Wainwright"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Negahban and Wainwright.,? \\Q2012\\E", "shortCiteRegEx": "Negahban and Wainwright.", "year": 2012}, {"title": "Support union recovery in high-dimensional multivariate regression", "author": ["G. Obozinski", "M.J. Wainwright", "M.I. Jordan"], "venue": "The Annals of Statistics,", "citeRegEx": "Obozinski et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Obozinski et al\\.", "year": 2011}, {"title": "Restricted eigenvalue properties for correlated gaussian designs", "author": ["G. Raskutti", "M. J Wainwright", "B. Yu"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Raskutti et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Raskutti et al\\.", "year": 2010}, {"title": "A simpler approach to matrix completion", "author": ["B. Recht"], "venue": "JMLR, 12:3413\u20133430,", "citeRegEx": "Recht.,? \\Q2011\\E", "shortCiteRegEx": "Recht.", "year": 2011}, {"title": "Bayesian probabilistic matrix factorization using markov chain monte carlo", "author": ["R. Salakhutdinov", "A. Mnih"], "venue": "In ICML,", "citeRegEx": "Salakhutdinov and Mnih.,? \\Q2008\\E", "shortCiteRegEx": "Salakhutdinov and Mnih.", "year": 2008}, {"title": "Maximum-margin matrix factorization", "author": ["N. Srebro", "J. Rennie", "T.S. Jaakkola"], "venue": "In NIPS,", "citeRegEx": "Srebro et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2004}, {"title": "Probabilistic principal component analysis", "author": ["M.E. Tipping", "C.M. Bishop"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Tipping and Bishop.,? \\Q1999\\E", "shortCiteRegEx": "Tipping and Bishop.", "year": 1999}, {"title": "A note on sums of independent random matrices after ahlswede-winter", "author": ["R. Vershynin"], "venue": "Lecture Notes,", "citeRegEx": "Vershynin.,? \\Q2009\\E", "shortCiteRegEx": "Vershynin.", "year": 2009}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["R. Vershynin"], "venue": "arXiv preprint arXiv:1011.3027,", "citeRegEx": "Vershynin.,? \\Q2010\\E", "shortCiteRegEx": "Vershynin.", "year": 2010}, {"title": "Dirty statistical models", "author": ["E. Yang", "P. Ravikumar"], "venue": "In NIPS,", "citeRegEx": "Yang and Ravikumar.,? \\Q2013\\E", "shortCiteRegEx": "Yang and Ravikumar.", "year": 2013}, {"title": "Graphical models via generalized linear models", "author": ["E. Yang", "G. Allen", "Z. Liu", "P. Ravikumar"], "venue": "In NIPS,", "citeRegEx": "Yang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2012}, {"title": "Conditional random fields via univariate exponential families", "author": ["E. Yang", "P. Ravikumar", "G.I. Allen", "Z. Liu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Yang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2013}, {"title": "Then use a peeling argument Raskutti et al. (2010) to derive at the result in Lemma 4", "author": ["E E"], "venue": "BOUNDING EXPECTATION Note", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}], "referenceMentions": [{"referenceID": 2, "context": "In recent years, leveraging developments in sparse estimation and compressed sensing, there has been a surge of work on computationally tractable estimators with strong statistical guarantees, specifically for the setting where a subset of entries of a low\u2013rank matrix are observed either deterministically, or perturbed by additive noise that is Gaussian Candes and Plan (2010), or more generally sub\u2013Gaussian Keshavan et al.", "startOffset": 356, "endOffset": 379}, {"referenceID": 2, "context": "In recent years, leveraging developments in sparse estimation and compressed sensing, there has been a surge of work on computationally tractable estimators with strong statistical guarantees, specifically for the setting where a subset of entries of a low\u2013rank matrix are observed either deterministically, or perturbed by additive noise that is Gaussian Candes and Plan (2010), or more generally sub\u2013Gaussian Keshavan et al. (2010b); Negahban and Wainwright (2012).", "startOffset": 356, "endOffset": 435}, {"referenceID": 2, "context": "In recent years, leveraging developments in sparse estimation and compressed sensing, there has been a surge of work on computationally tractable estimators with strong statistical guarantees, specifically for the setting where a subset of entries of a low\u2013rank matrix are observed either deterministically, or perturbed by additive noise that is Gaussian Candes and Plan (2010), or more generally sub\u2013Gaussian Keshavan et al. (2010b); Negahban and Wainwright (2012). While such a Gaussian noise model is amenable to the subtle statistical analyses required for the ill\u2013posed problem of matrix completion, it is not always practically suitable for all data settings encountered in matrix completion problems.", "startOffset": 356, "endOffset": 467}, {"referenceID": 4, "context": "This, thus gives rise to the second question of whether we can generalize the standard matrix completion estimators and statistical analyses, suited for thin\u2013 tailed continuous data, to more heterogeneous data\u2013types? Note that there has been some recent work for the specific case of binary data by Davenport et al. (2012), but generalizations to other data\u2013types and distributions is largely unexplored.", "startOffset": 299, "endOffset": 323}, {"referenceID": 2, "context": "Aside from the low\u2013rank constraints, further assumptions to eliminate overly \u201cspiky\u201d matrices are required for well\u2013posed recovery under partial measurements Candes and Recht (2009). Early work provided generalization error bounds for various low\u2013rank matrix completion algorithms, including algorithms based on nuclear norm minimization Candes and Recht (2009); Candes and Tao (2010); Candes and Plan (2010); Recht (2011), max\u2013margin matrix factorization Srebro et al.", "startOffset": 158, "endOffset": 182}, {"referenceID": 2, "context": "Aside from the low\u2013rank constraints, further assumptions to eliminate overly \u201cspiky\u201d matrices are required for well\u2013posed recovery under partial measurements Candes and Recht (2009). Early work provided generalization error bounds for various low\u2013rank matrix completion algorithms, including algorithms based on nuclear norm minimization Candes and Recht (2009); Candes and Tao (2010); Candes and Plan (2010); Recht (2011), max\u2013margin matrix factorization Srebro et al.", "startOffset": 158, "endOffset": 362}, {"referenceID": 2, "context": "Aside from the low\u2013rank constraints, further assumptions to eliminate overly \u201cspiky\u201d matrices are required for well\u2013posed recovery under partial measurements Candes and Recht (2009). Early work provided generalization error bounds for various low\u2013rank matrix completion algorithms, including algorithms based on nuclear norm minimization Candes and Recht (2009); Candes and Tao (2010); Candes and Plan (2010); Recht (2011), max\u2013margin matrix factorization Srebro et al.", "startOffset": 158, "endOffset": 385}, {"referenceID": 2, "context": "Early work provided generalization error bounds for various low\u2013rank matrix completion algorithms, including algorithms based on nuclear norm minimization Candes and Recht (2009); Candes and Tao (2010); Candes and Plan (2010); Recht (2011), max\u2013margin matrix factorization Srebro et al.", "startOffset": 203, "endOffset": 226}, {"referenceID": 2, "context": "Early work provided generalization error bounds for various low\u2013rank matrix completion algorithms, including algorithms based on nuclear norm minimization Candes and Recht (2009); Candes and Tao (2010); Candes and Plan (2010); Recht (2011), max\u2013margin matrix factorization Srebro et al.", "startOffset": 203, "endOffset": 240}, {"referenceID": 2, "context": "Early work provided generalization error bounds for various low\u2013rank matrix completion algorithms, including algorithms based on nuclear norm minimization Candes and Recht (2009); Candes and Tao (2010); Candes and Plan (2010); Recht (2011), max\u2013margin matrix factorization Srebro et al. (2004), spectral algorithms Keshavan et al.", "startOffset": 203, "endOffset": 294}, {"referenceID": 2, "context": "Early work provided generalization error bounds for various low\u2013rank matrix completion algorithms, including algorithms based on nuclear norm minimization Candes and Recht (2009); Candes and Tao (2010); Candes and Plan (2010); Recht (2011), max\u2013margin matrix factorization Srebro et al. (2004), spectral algorithms Keshavan et al. (2010a,b), and alternating minimization Jain et al. (2013). These work made stringent matrix incoherence assumptions to avoid \u201cspiky\u201d matrices.", "startOffset": 203, "endOffset": 390}, {"referenceID": 2, "context": "Early work provided generalization error bounds for various low\u2013rank matrix completion algorithms, including algorithms based on nuclear norm minimization Candes and Recht (2009); Candes and Tao (2010); Candes and Plan (2010); Recht (2011), max\u2013margin matrix factorization Srebro et al. (2004), spectral algorithms Keshavan et al. (2010a,b), and alternating minimization Jain et al. (2013). These work made stringent matrix incoherence assumptions to avoid \u201cspiky\u201d matrices. These assumptions have been made less stringent in more recent results Negahban and Wainwright (2012), which moreover extend the guarantees to approximately low\u2013rank matrices.", "startOffset": 203, "endOffset": 577}, {"referenceID": 2, "context": "Early work provided generalization error bounds for various low\u2013rank matrix completion algorithms, including algorithms based on nuclear norm minimization Candes and Recht (2009); Candes and Tao (2010); Candes and Plan (2010); Recht (2011), max\u2013margin matrix factorization Srebro et al. (2004), spectral algorithms Keshavan et al. (2010a,b), and alternating minimization Jain et al. (2013). These work made stringent matrix incoherence assumptions to avoid \u201cspiky\u201d matrices. These assumptions have been made less stringent in more recent results Negahban and Wainwright (2012), which moreover extend the guarantees to approximately low\u2013rank matrices. Such (approximate) low\u2013rank structure is one instance of general structural constraints which are now understood to be necessary for consistent statistical estimation under high\u2013dimensional settings (with very large number of parameters and very few observations). Note that the high\u2013dimensional matrix completion problem is particularly ill\u2013posed, since the measurements are typically both very local (e.g. individual matrix entries), and partial (e.g. covering a decaying fraction of entries of the entire matrix). However, the specific (approximately) low\u2013rank structural constraint imposed in the past work on matrix completion does not capture the rich variety of other qualitatively different structural constraints such as row\u2013sparseness, column\u2013sparseness, or a superposition structure of low\u2013rank plus elementwise sparseness, among others. For instance, in the classical introductory survey on matrix completion Laurent (2009), the authors discuss structural constraints of a contraction matrix, and a Euclidean distance matrix.", "startOffset": 203, "endOffset": 1587}, {"referenceID": 16, "context": "Following a standard approach Negahban (2012), we (a) first showed that the negative log\u2013likelihood of the subset of observed entries satisfies a form of Restricted Strong Convexity (RSC) (Definition 4); and (b) under this RSC condition, our proposed M\u2013estimator satisfies strong statistical guarantees.", "startOffset": 30, "endOffset": 46}, {"referenceID": 2, "context": "A key corollary of our general framework is matrix completion under sub\u2013Gaussian samples and low\u2013rank constraints, where we show that our theorem recovers results comparable to the existing literature Candes and Plan (2010); Keshavan et al.", "startOffset": 201, "endOffset": 224}, {"referenceID": 2, "context": "A key corollary of our general framework is matrix completion under sub\u2013Gaussian samples and low\u2013rank constraints, where we show that our theorem recovers results comparable to the existing literature Candes and Plan (2010); Keshavan et al. (2010b); Negahban and Wainwright (2012).", "startOffset": 201, "endOffset": 249}, {"referenceID": 2, "context": "A key corollary of our general framework is matrix completion under sub\u2013Gaussian samples and low\u2013rank constraints, where we show that our theorem recovers results comparable to the existing literature Candes and Plan (2010); Keshavan et al. (2010b); Negahban and Wainwright (2012). Finally, we corroborate our theoretical findings via simulated experiments.", "startOffset": 201, "endOffset": 281}, {"referenceID": 28, "context": "Further, if X is sub\u2013Gaussian with parameter b and E[X] = 0, then Var(X) \u2264 b2 (Vershynin (2010)).", "startOffset": 79, "endOffset": 96}, {"referenceID": 8, "context": "The following relationship was first noted by Forster and Warmuth (2002), and later established by Banerjee et al.", "startOffset": 46, "endOffset": 73}, {"referenceID": 1, "context": "The following relationship was first noted by Forster and Warmuth (2002), and later established by Banerjee et al. (2005) [Theorem 4]: \u2212 logP (X|\u0398) \u221d BF (X, g(\u0398)), \u2200X \u2208 dom(F).", "startOffset": 99, "endOffset": 122}, {"referenceID": 12, "context": "Kakade et al. (2010) provide a generalization of compressed sensing problem to general exponential family distributions.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "the typical assumptions (restricted isometry or restricted eigenvalues) made in such settings; see (Candes and Recht, 2009) for additional discussion.", "startOffset": 99, "endOffset": 123}, {"referenceID": 3, "context": "the typical assumptions (restricted isometry or restricted eigenvalues) made in such settings; see (Candes and Recht, 2009) for additional discussion. There have been extensions of classical probabilistic PCA Tipping and Bishop (1999) from Gaussian noise models to exponential family distributions Collins et al.", "startOffset": 100, "endOffset": 235}, {"referenceID": 3, "context": "the typical assumptions (restricted isometry or restricted eigenvalues) made in such settings; see (Candes and Recht, 2009) for additional discussion. There have been extensions of classical probabilistic PCA Tipping and Bishop (1999) from Gaussian noise models to exponential family distributions Collins et al. (2001); Mohamed et al.", "startOffset": 100, "endOffset": 320}, {"referenceID": 3, "context": "the typical assumptions (restricted isometry or restricted eigenvalues) made in such settings; see (Candes and Recht, 2009) for additional discussion. There have been extensions of classical probabilistic PCA Tipping and Bishop (1999) from Gaussian noise models to exponential family distributions Collins et al. (2001); Mohamed et al. (2008); Gordon (2002).", "startOffset": 100, "endOffset": 343}, {"referenceID": 3, "context": "the typical assumptions (restricted isometry or restricted eigenvalues) made in such settings; see (Candes and Recht, 2009) for additional discussion. There have been extensions of classical probabilistic PCA Tipping and Bishop (1999) from Gaussian noise models to exponential family distributions Collins et al. (2001); Mohamed et al. (2008); Gordon (2002). There have also been recent extensions of probabilistic graphical model classes, beyond Gaussian and Ising models, to multivariate extensions of exponential family distributions (Yang et al.", "startOffset": 100, "endOffset": 358}, {"referenceID": 3, "context": "the typical assumptions (restricted isometry or restricted eigenvalues) made in such settings; see (Candes and Recht, 2009) for additional discussion. There have been extensions of classical probabilistic PCA Tipping and Bishop (1999) from Gaussian noise models to exponential family distributions Collins et al. (2001); Mohamed et al. (2008); Gordon (2002). There have also been recent extensions of probabilistic graphical model classes, beyond Gaussian and Ising models, to multivariate extensions of exponential family distributions (Yang et al., 2012, 2013). More complicated probabilistic models have also been proposed in the context of collaborative filtering Mnih and Salakhutdinov (2007); Salakhutdinov and Mnih (2008), but these typically involve non\u2013convex optimization, and it is difficult to extend the rigorous statistical analyses of the form in this paper (and in the matrix completion literature) to these models.", "startOffset": 100, "endOffset": 698}, {"referenceID": 3, "context": "the typical assumptions (restricted isometry or restricted eigenvalues) made in such settings; see (Candes and Recht, 2009) for additional discussion. There have been extensions of classical probabilistic PCA Tipping and Bishop (1999) from Gaussian noise models to exponential family distributions Collins et al. (2001); Mohamed et al. (2008); Gordon (2002). There have also been recent extensions of probabilistic graphical model classes, beyond Gaussian and Ising models, to multivariate extensions of exponential family distributions (Yang et al., 2012, 2013). More complicated probabilistic models have also been proposed in the context of collaborative filtering Mnih and Salakhutdinov (2007); Salakhutdinov and Mnih (2008), but these typically involve non\u2013convex optimization, and it is difficult to extend the rigorous statistical analyses of the form in this paper (and in the matrix completion literature) to these models.", "startOffset": 100, "endOffset": 729}, {"referenceID": 19, "context": "To formalize the notion of such structural constraints, we follow (Negahban, 2012), and assume that \u0398\u2217 satisfies \u0398\u2217 \u2208 M \u2286 M \u2282 Rm\u00d7n, for some subspace M \u2286 M, which contains parameter matrices that are structured similar to the target (the corresponding structural constraints such as low rankness, low rankness+sparsity etc); we also allow the flexibility of working with a superset M of the model subspace that is potentially easier to analyze.", "startOffset": 66, "endOffset": 82}, {"referenceID": 19, "context": "We provide some examples of such decomposable regularizers and structural constraint subspaces, and refer to (Negahban, 2012) for more examples and discussion.", "startOffset": 109, "endOffset": 125}, {"referenceID": 8, "context": "The nuclear norm R(\u0398) = \u2016\u0398\u2016\u2217 = \u2211 k \u03c3k, has been shown to be decomposable with respect to these constraint subspaces Fazel et al. (2001). Example 2.", "startOffset": 116, "endOffset": 136}, {"referenceID": 8, "context": "The nuclear norm R(\u0398) = \u2016\u0398\u2016\u2217 = \u2211 k \u03c3k, has been shown to be decomposable with respect to these constraint subspaces Fazel et al. (2001). Example 2. Block sparsity: Another important structural constraint for a matrix is block\u2013 sparsity, where each row is either all zeros or mostly non\u2013zero, and the number of non\u2013zero rows is small. The structural constraint subspaces in this case correspond to a linear span of specific Frobenius\u2013norm\u2013one matrices that are non\u2013zero in a small subset of the rows (dependent on \u0398\u2217); it has been shown that l1/lq (q > 1) norms Negahban and Wainwright (2008); Obozinski et al.", "startOffset": 116, "endOffset": 592}, {"referenceID": 8, "context": "The nuclear norm R(\u0398) = \u2016\u0398\u2016\u2217 = \u2211 k \u03c3k, has been shown to be decomposable with respect to these constraint subspaces Fazel et al. (2001). Example 2. Block sparsity: Another important structural constraint for a matrix is block\u2013 sparsity, where each row is either all zeros or mostly non\u2013zero, and the number of non\u2013zero rows is small. The structural constraint subspaces in this case correspond to a linear span of specific Frobenius\u2013norm\u2013one matrices that are non\u2013zero in a small subset of the rows (dependent on \u0398\u2217); it has been shown that l1/lq (q > 1) norms Negahban and Wainwright (2008); Obozinski et al. (2011) are decomposable with respect to such structural constraint subspaces.", "startOffset": 116, "endOffset": 617}, {"referenceID": 5, "context": "sponding to these consist of the linear span of weighted sum of specific rank\u2013one matrices and sparse matrices with non\u2013zero components on specified positions; and appropriate regularization function decomposable with respect to such structural constraints is the infimum convolution of the weighted nuclear norm with weighted elementwise l1 norm, \u2016M\u20161,1 = \u2211 ij |Mij | Candes et al. (2011); Yang and Ravikumar (2013): R(\u0398) = inf{\u03bb1\u2016S\u20161,1 + \u03bb2\u2016L\u2016\u2217 : \u0398 = S + L}.", "startOffset": 369, "endOffset": 390}, {"referenceID": 5, "context": "sponding to these consist of the linear span of weighted sum of specific rank\u2013one matrices and sparse matrices with non\u2013zero components on specified positions; and appropriate regularization function decomposable with respect to such structural constraints is the infimum convolution of the weighted nuclear norm with weighted elementwise l1 norm, \u2016M\u20161,1 = \u2211 ij |Mij | Candes et al. (2011); Yang and Ravikumar (2013): R(\u0398) = inf{\u03bb1\u2016S\u20161,1 + \u03bb2\u2016L\u2016\u2217 : \u0398 = S + L}.", "startOffset": 369, "endOffset": 417}, {"referenceID": 2, "context": "As Candes and Recht (2009) show with numerous examples, low\u2013rank and presumably other such structural constraints as above, by themselves are not sufficient for accurate recovery, in part due to the infeasibility of recovering overly \u201cspiky\u201d matrices with very few large entries.", "startOffset": 3, "endOffset": 27}, {"referenceID": 2, "context": "Early work Candes and Plan (2010); Keshavan et al.", "startOffset": 11, "endOffset": 34}, {"referenceID": 2, "context": "Early work Candes and Plan (2010); Keshavan et al. (2010a,b), assumed stringent matrix incoherence conditions to preclude such matrices, while more recent work Davenport et al. (2012); Negahban and Wainwright (2012), relax these assumptions to restricting the spikiness ratio, defined as follows: \u03b1sp(\u0398) = \u221a mn\u2016\u0398\u2016max \u2016\u0398\u2016F .", "startOffset": 11, "endOffset": 184}, {"referenceID": 2, "context": "Early work Candes and Plan (2010); Keshavan et al. (2010a,b), assumed stringent matrix incoherence conditions to preclude such matrices, while more recent work Davenport et al. (2012); Negahban and Wainwright (2012), relax these assumptions to restricting the spikiness ratio, defined as follows: \u03b1sp(\u0398) = \u221a mn\u2016\u0398\u2016max \u2016\u0398\u2016F .", "startOffset": 11, "endOffset": 216}, {"referenceID": 19, "context": "Our proof uses elements from Negahban (2012), as well as Negahban and Wainwright (2012) where they analyze the case of low\u2013rank structure and additive noise, and establish a form of restricted strong convexity (RSC) for squared loss over subset of matrix entries (closely relates to the special case, when the exponential family distribution assumed in (2) is Gaussian).", "startOffset": 29, "endOffset": 45}, {"referenceID": 19, "context": "Our proof uses elements from Negahban (2012), as well as Negahban and Wainwright (2012) where they analyze the case of low\u2013rank structure and additive noise, and establish a form of restricted strong convexity (RSC) for squared loss over subset of matrix entries (closely relates to the special case, when the exponential family distribution assumed in (2) is Gaussian).", "startOffset": 29, "endOffset": 88}, {"referenceID": 19, "context": "The proof follows from Lemma 1 of Negahban (2012).", "startOffset": 34, "endOffset": 50}, {"referenceID": 19, "context": "As noted earlier, such an RSC result for the special case of squared loss under low\u2013rank constraints was shown in Negahban and Wainwright (2012). We prove this theorem in Section 4.", "startOffset": 114, "endOffset": 145}, {"referenceID": 19, "context": "Next, we use the following proposition by Negahban and Wainwright (2012), to bound \u03baR(n, |\u03a9|) in Theorem 1.", "startOffset": 42, "endOffset": 73}, {"referenceID": 19, "context": "This follows from Lemma 6 of Negahban and Wainwright (2012), using |\u03a9| > n log n.", "startOffset": 29, "endOffset": 60}, {"referenceID": 28, "context": "5 of Vershynin (2010), there exists a constant c1 such that \u2016Xij \u2212 g(\u0398ij)\u2016\u03c6 \u2264 c1b, \u2200ij.", "startOffset": 5, "endOffset": 22}, {"referenceID": 16, "context": "3 Proof of Theorem 2 This proof uses symmetrization arguments and contractions (Ledoux and Talagrand (1991) Ch.", "startOffset": 80, "endOffset": 108}], "year": 2015, "abstractText": "We consider the matrix completion problem of recovering a structured matrix from noisy and partial measurements. Recent works have proposed tractable estimators with strong statistical guarantees for the case where the underlying matrix is low\u2013rank, and the measurements consist of a subset, either of the exact individual entries, or of the entries perturbed by additive Gaussian noise, which is thus implicitly suited for thin\u2013tailed continuous data. Arguably, common applications of matrix completion require estimators for (a) heterogeneous data\u2013types, such as skewed\u2013continuous, count, binary, etc., (b) for heterogeneous noise models (beyond Gaussian), which capture varied uncertainty in the measurements, and (c) heterogeneous structural constraints beyond low\u2013rank, such as block\u2013sparsity, or a superposition structure of low\u2013rank plus elementwise sparseness, among others. In this paper, we provide a vastly unified framework for generalized matrix completion by considering a matrix completion setting wherein the matrix entries are sampled from any member of the rich family of exponential family distributions; and impose general structural constraints on the underlying matrix, as captured by a decomposable norm regularizer R(.). We propose a simple convex regularized M\u2013estimator for this generalized framework, and provide a unified and novel statistical analysis for this class of estimators. We finally corroborate our theoretical results on simulated datasets.", "creator": "LaTeX with hyperref package"}}}