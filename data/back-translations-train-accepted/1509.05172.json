{"id": "1509.05172", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Sep-2015", "title": "Generalized Emphatic Temporal Difference Learning: Bias-Variance Analysis", "abstract": "We consider the off-policy evaluation problem in Markov decision processes with function approximation. We propose a generalization of the recently introduced \\emph{emphatic temporal differences} (ETD) algorithm \\citep{SuttonMW15}, which encompasses the original ETD($\\lambda$), as well as several other off-policy evaluation algorithms as special cases. We call this framework \\ETD, where our introduced parameter $\\beta$ controls the decay rate of an importance-sampling term. We study conditions under which the projected fixed-point equation underlying \\ETD\\ involves a contraction operator, allowing us to present the first asymptotic error bounds (bias) for \\ETD. Our results show that the original ETD algorithm always involves a contraction operator, and its bias is bounded. Moreover, by controlling $\\beta$, our proposed generalization allows trading-off bias for variance reduction, thereby achieving a lower total error.", "histories": [["v1", "Thu, 17 Sep 2015 09:03:35 GMT  (219kb)", "https://arxiv.org/abs/1509.05172v1", "arXiv admin note: text overlap witharXiv:1508.03411"], ["v2", "Fri, 27 Nov 2015 07:17:55 GMT  (214kb)", "http://arxiv.org/abs/1509.05172v2", "arXiv admin note: text overlap witharXiv:1508.03411"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1508.03411", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["assaf hallak", "aviv tamar", "r\u00e9mi munos", "shie mannor"], "accepted": true, "id": "1509.05172"}, "pdf": {"name": "1509.05172.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 150 9.05 172v 2 [stat.ML] 2 7"}, {"heading": "1 Introduction", "text": "In Reinforcement Learning (RL; Sutton and Barto 1998), the samples are generated by politics - classic is the behavior of politics. In Reinforcement Learning (RL; Sutton and Barto 1998), policy evaluation refers to the problem of the valuation of value function - a mapping of states to their long-term discounted return under a given policy, whereby an exact calculation of value function is often impossible. Instead, an approximate value function is sought both for the evaluation of the quality of a policy and as a partial procedure for policy optimization (a.k.a. approach to dynamic programming; Bertsekas 2012). In this approach, the parameters of value adjustment are adjusted using machine learning methods, which are often based on time differences (TD; Sutton and Barto 1998). The source of the data generated divides policy evaluation into two cases."}, {"heading": "2 Preliminaries", "text": "We consider an MDP M = (S, A, P, R), where S is the state space, A is the space for action, P is the transition probability matrix, R is the reward function, and F is the discount factor.In the face of a target policy based on the states of distribution through measures, our goal is to evaluate the value function: V \u03c0 (s). = E\u03c0 (s), where the state characteristics of the individual states are different, and the use of sampling to find a suitable solution.Let us adopt a behavioral policy that follows the general patterns s0, s1, s1, a1, a1, a1, a1, a1, a1, a1, etc."}, {"heading": "3.1 Numerical Illustration", "text": "We illustrate the meaning of the ETD (\u03b2, \u03b2) bias tied to a numerical example = \u03b2 \u03b2 = 2001. Let us consider the two-state MDP example of Kolter (\u03b2 \u03b2), with transition matrix P = (1 / 2) 1 (where 1 is actually a 1 matrix), discount factor \u03b3 = 0.99, and value function V = [1, 1.05] (with R = (I \u2212 \u03b3P) V. The properties are more than 1, 1.05 + \u03b5], with \u03b5 = 0.001. In this example, we clearly have the function V = [0.5, 0.5]. Behavioral policy is chosen so that d\u00b5 = [p, 1 \u2212 p].In Figure 1 we plot the mean error rate on St."}, {"heading": "4.1 Numerical Illustration", "text": "At these parameter settings, the error of the standard TD is 42.55 (p was selected to be close to a point of infinite bias for these parameters).In Figure 2, we represent the error in the mean square, where the error was determined by executing ETD (0, \u03b2) with a step size \u03b1 = 0.001 for 10,000 iterations and the results averaged over 10,000 different rounds. First, it should be noted that for all \u03b2 the error is two orders of magnitude smaller than that of the standard TD. Therefore, algorithms converging with the standard TD fix point, such as GTD Sutton et al. (2009) are significantly exceeded in this case by ETD (0, \u03b2).Secondly, the dependence of the error on \u03b2 is to be taken into account, which shows the bias against the aforementioned 0.8 point, which is considerably smaller than the error originally determined = 0.8."}, {"heading": "5 Contraction Property for ETD(\u03bb, \u03b2)", "text": "In this case, we define the emphatic weight vector m bym = d \u00b5 (I \u2212 P) \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p"}, {"heading": "6 Conclusion", "text": "In this paper, we unified several non-political TD algorithms under the ETD framework (\u03bb, \u03b2), which flexibly controls the bias and variance of the algorithm by controlling the decay rate of the importance sample ratio. From this perspective, we showed that several methods proposed in the literature represent special cases of this bias selection.Our main contribution is an error analysis of the ETD (\u03bb, \u03b2), which quantifies the bias compromise. In particular, we showed that the recently proposed ETD algorithm by Sutton, Mahmood and White (2015) limited the bias for general behavior and target policy, and that controlling the decay rate in the ETD algorithm (\u03bb, \u03b2) can achieve improved performance by reducing the deviation of the algorithm while maintaining a reasonable bias. Possible future extensions of our work include finite time limits for non-political policy adjustment influences (ETD), as well as broad policy adjustment influences."}, {"heading": "A Proof of Lemma 1", "text": "Now, if there is a state visited by the target policy but not by the behavioral policy, it means that d\u00b5 (s) = 0, and that there are some t that [d'\u00b5P t \u03c0] (s) > 0 and by definition f (s) \u2265 [\u03b2 td'\u00b5P t \u03c0] (s), so that we can next prove the upper limit of what we want. Note that f (s) \u2265 0 and that GP s f (s) = 1 / (1 \u2212 \u03b2). So, if d\u00b5 6 = (1 \u2212 \u03b2) f, then there must be some s so that d\u00b5 (s) < (1 \u2212 \u03b2) f (s) is so far < 1 \u2212 \u03b2. Now that d\u00b5 = d\u043c by definition is d\u00b5 = (1 \u2212 \u03b2) f, and we get that upper limit."}, {"heading": "B Technical Part of Proposition 1", "text": "2, 2, 2, 2, 2, 2, 2, 2, 2, 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, paragraph 2, 2, paragraph 2, paragraph 2, 2, 2, paragraph 2, 2, 2, paragraph 2, paragraph 2, 2, paragraph 2, 2, paragraph 2, 2, 2, paragraph 2, 2, 2, 2, 2, 2, 2, 2,"}], "references": [{"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["L. Baird"], "venue": "ICML.", "citeRegEx": "Baird,? 1995", "shortCiteRegEx": "Baird", "year": 1995}, {"title": "Neuro-Dynamic Programming", "author": ["D. Bertsekas", "J. Tsitsiklis"], "venue": "Athena Scientific.", "citeRegEx": "Bertsekas and Tsitsiklis,? 1996", "shortCiteRegEx": "Bertsekas and Tsitsiklis", "year": 1996}, {"title": "Projected equation methods for approximate solution of large linear systems", "author": ["D. Bertsekas", "H. Yu"], "venue": "Journal of Computational and Applied Mathematics 227(1):27\u201350.", "citeRegEx": "Bertsekas and Yu,? 2009", "shortCiteRegEx": "Bertsekas and Yu", "year": 2009}, {"title": "Dynamic Programming and Optimal Control, Vol II", "author": ["D. Bertsekas"], "venue": "Athena Scientific, 4th edition.", "citeRegEx": "Bertsekas,? 2012", "shortCiteRegEx": "Bertsekas", "year": 2012}, {"title": "The fixed points of off-policy TD", "author": ["J.Z. Kolter"], "venue": "NIPS.", "citeRegEx": "Kolter,? 2011", "shortCiteRegEx": "Kolter", "year": 2011}, {"title": "Finite-sample analysis of proximal gradient td algorithms", "author": ["B. Liu", "J. Liu", "M. Ghavamzadeh", "S. Mahadevan", "M. Petrik"], "venue": "UAI.", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Emphatic TemporalDifference Learning", "author": ["A.R. Mahmood", "H. Yu", "M. White", "R.S. Sutton"], "venue": "arXiv:1507.01569.", "citeRegEx": "Mahmood et al\\.,? 2015", "shortCiteRegEx": "Mahmood et al\\.", "year": 2015}, {"title": "Error bounds for approximate policy iteration", "author": ["R. Munos"], "venue": "ICML.", "citeRegEx": "Munos,? 2003", "shortCiteRegEx": "Munos", "year": 2003}, {"title": "Off-policy temporal-difference learning with function approximation", "author": ["D. Precup", "R.S. Sutton", "S. Dasgupta"], "venue": "ICML.", "citeRegEx": "Precup et al\\.,? 2001", "shortCiteRegEx": "Precup et al\\.", "year": 2001}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A. Barto"], "venue": "Cambridge Univ Press.", "citeRegEx": "Sutton and Barto,? 1998", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Fast gradient-descent methods for temporal-difference learning with linear function approximation", "author": ["R.S. Sutton", "H.R. Maei", "D. Precup", "S. Bhatnagar", "D. Silver", "C. Szepesv\u00e1ri", "E. Wiewiora"], "venue": "ICML.", "citeRegEx": "Sutton et al\\.,? 2009", "shortCiteRegEx": "Sutton et al\\.", "year": 2009}, {"title": "An emphatic approach to the problem of off-policy temporal-difference learning", "author": ["R.S. Sutton", "A.R. Mahmood", "M. White"], "venue": "arXiv:1503.04269.", "citeRegEx": "Sutton et al\\.,? 2015", "shortCiteRegEx": "Sutton et al\\.", "year": 2015}, {"title": "Least squares temporal difference methods: An analysis under general conditions", "author": ["H. Yu"], "venue": "SIAM Journal on Control and Optimization 50(6):3310\u20133343.", "citeRegEx": "Yu,? 2012", "shortCiteRegEx": "Yu", "year": 2012}, {"title": "On convergence of emphatic temporal-difference learning", "author": ["H. Yu"], "venue": "COLT.", "citeRegEx": "Yu,? 2015", "shortCiteRegEx": "Yu", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "1 Introduction In Reinforcement Learning (RL; Sutton and Barto 1998), policy-evaluation refers to the problem of evaluating the value function \u2013 a mapping from states to their longterm discounted return under a given policy, using sampled observations of the system dynamics and reward.", "startOffset": 41, "endOffset": 68}, {"referenceID": 3, "context": "Instead, an approximate value-function is sought using various function-approximation techniques (a.k.a. approximate dynamic-programming; Bertsekas 2012).", "startOffset": 97, "endOffset": 153}, {"referenceID": 9, "context": "In this approach, the parameters of the value-function approximation are tuned using machine-learning inspired methods, often based on temporaldifferences (TD;Sutton and Barto 1998).", "startOffset": 155, "endOffset": 181}, {"referenceID": 1, "context": "the on-policy setting, TD methods are well understood, with classic convergence guarantees and approximation-error bounds, based on a contraction property of the projected Bellman operator underlying TD (Bertsekas and Tsitsiklis, 1996).", "startOffset": 203, "endOffset": 235}, {"referenceID": 0, "context": "For the off-policy case, however, standard TD methods no longer maintain this contraction property, the error bounds do not hold, and these methods might even diverge (Baird, 1995).", "startOffset": 167, "endOffset": 180}, {"referenceID": 13, "context": "Lately, Sutton, Mahmood, and White (2015) proposed the emphatic TD (ETD) algorithm: a modification of the TD idea, which converges off-policy (Yu, 2015), and has a reduced variance compared to IS-TD.", "startOffset": 142, "endOffset": 152}, {"referenceID": 12, "context": "Related Work: In recent years, several different off-policy policy-evaluation algorithms have been studied, such as importance-sampling based least-squares TD (Yu, 2012), and gradient-based TD (Sutton et al.", "startOffset": 159, "endOffset": 169}, {"referenceID": 10, "context": "Related Work: In recent years, several different off-policy policy-evaluation algorithms have been studied, such as importance-sampling based least-squares TD (Yu, 2012), and gradient-based TD (Sutton et al., 2009; Liu et al., 2015).", "startOffset": 193, "endOffset": 232}, {"referenceID": 5, "context": "Related Work: In recent years, several different off-policy policy-evaluation algorithms have been studied, such as importance-sampling based least-squares TD (Yu, 2012), and gradient-based TD (Sutton et al., 2009; Liu et al., 2015).", "startOffset": 193, "endOffset": 232}, {"referenceID": 2, "context": "These algorithms are guaranteed to converge, however, their asymptotic error can be bounded only when the target and behavior policies are similar (Bertsekas and Yu, 2009), or when their induced transition matrices satisfy a certain matrix-inequality suggested by Kolter (2011), which limits the discrepancy between the target and behavior policies.", "startOffset": 147, "endOffset": 171}, {"referenceID": 4, "context": "When these conditions are not satisfied, the error may be arbitrarily large (Kolter, 2011).", "startOffset": 76, "endOffset": 90}, {"referenceID": 0, "context": "For the off-policy case, however, standard TD methods no longer maintain this contraction property, the error bounds do not hold, and these methods might even diverge (Baird, 1995). The standard error-bounds may be shown to hold for an importance-sampling TD method (IS-TD), as proposed by Precup, Sutton, and Dasgupta (2001). However, this method is known to suffer from a high variance of its importance-sampling estimator, limiting its practicality.", "startOffset": 168, "endOffset": 326}, {"referenceID": 0, "context": "For the off-policy case, however, standard TD methods no longer maintain this contraction property, the error bounds do not hold, and these methods might even diverge (Baird, 1995). The standard error-bounds may be shown to hold for an importance-sampling TD method (IS-TD), as proposed by Precup, Sutton, and Dasgupta (2001). However, this method is known to suffer from a high variance of its importance-sampling estimator, limiting its practicality. Lately, Sutton, Mahmood, and White (2015) proposed the emphatic TD (ETD) algorithm: a modification of the TD idea, which converges off-policy (Yu, 2015), and has a reduced variance compared to IS-TD.", "startOffset": 168, "endOffset": 495}, {"referenceID": 0, "context": "For the off-policy case, however, standard TD methods no longer maintain this contraction property, the error bounds do not hold, and these methods might even diverge (Baird, 1995). The standard error-bounds may be shown to hold for an importance-sampling TD method (IS-TD), as proposed by Precup, Sutton, and Dasgupta (2001). However, this method is known to suffer from a high variance of its importance-sampling estimator, limiting its practicality. Lately, Sutton, Mahmood, and White (2015) proposed the emphatic TD (ETD) algorithm: a modification of the TD idea, which converges off-policy (Yu, 2015), and has a reduced variance compared to IS-TD. This variance reduction is achieved by incorporating a certain decay factor over the importance-sampling ratio. However, to the best of our knowledge, there are no results that bound the bias of ETD. Thus, while ETD is assured to converge, it is not known how good its limit actually is. In this paper, we propose the ETD(\u03bb, \u03b2) framework \u2013 a modification of the ETD(\u03bb) algorithm, where the decay rate of the importance-sampling ratio, \u03b2, is a free parameter, and \u03bb is the same bootstrapping parameter employed in TD(\u03bb) and ETD(\u03bb). By varying the decay rate, one can smoothly transition between the IS-TD algorithm, through ETD, to the standard TD algorithm. We investigate the bias of ETD(\u03bb, \u03b2), by studying the conditions under which its underlying projected Bellman operator is a contraction. We show that the original ETD possesses a contraction property, and present the first error bounds for ETD and ETD(\u03bb, \u03b2). In addition, our error bound reveals that the decay rate parameter balances between the bias and variance of the learning procedure. In particular, we show that selecting a decay equal to the discount factor as in the original ETD may be suboptimal in terms of the mean-squared error. The main contributions of this work are therefore a unification of several off-policy TD algorithms under the ETD(\u03bb, \u03b2) framework, and a new error analysis that reveals the bias-variance trade-off between them. Related Work: In recent years, several different off-policy policy-evaluation algorithms have been studied, such as importance-sampling based least-squares TD (Yu, 2012), and gradient-based TD (Sutton et al., 2009; Liu et al., 2015). These algorithms are guaranteed to converge, however, their asymptotic error can be bounded only when the target and behavior policies are similar (Bertsekas and Yu, 2009), or when their induced transition matrices satisfy a certain matrix-inequality suggested by Kolter (2011), which limits the discrepancy between the target and behavior policies.", "startOffset": 168, "endOffset": 2578}, {"referenceID": 9, "context": "Linear temporal difference methods (Sutton and Barto, 1998) approximate the value function by V (s) \u2248 \u03b8\u03c6(s), where \u03c6(s) \u2208 R are state features, and \u03b8 \u2208 R are weights, and use sampling to find a suitable \u03b8.", "startOffset": 35, "endOffset": 59}, {"referenceID": 9, "context": "Linear temporal difference methods (Sutton and Barto, 1998) approximate the value function by V (s) \u2248 \u03b8\u03c6(s), where \u03c6(s) \u2208 R are state features, and \u03b8 \u2208 R are weights, and use sampling to find a suitable \u03b8. Let \u03bc denote a behavior policy that generates the samples s0, a0, s1, a1, . . . according to at \u223c \u03bc(\u00b7|st) and st+1 \u223c P (\u00b7|st, at). We denote by \u03c1t the ratio \u03c0(at|st)/\u03bc(at|st), and we assume, similarly to Sutton, Mahmood, and White (2015), that \u03bc and \u03c0 are such that \u03c1t is well-defined1 for all t.", "startOffset": 36, "endOffset": 444}, {"referenceID": 9, "context": "Linear temporal difference methods (Sutton and Barto, 1998) approximate the value function by V (s) \u2248 \u03b8\u03c6(s), where \u03c6(s) \u2208 R are state features, and \u03b8 \u2208 R are weights, and use sampling to find a suitable \u03b8. Let \u03bc denote a behavior policy that generates the samples s0, a0, s1, a1, . . . according to at \u223c \u03bc(\u00b7|st) and st+1 \u223c P (\u00b7|st, at). We denote by \u03c1t the ratio \u03c0(at|st)/\u03bc(at|st), and we assume, similarly to Sutton, Mahmood, and White (2015), that \u03bc and \u03c0 are such that \u03c1t is well-defined1 for all t. Let T denote the Bellman operator for policy \u03c0, given by T (V ) . = R+ \u03b3PV, where R and P are the reward vector and transition matrix induced by policy \u03c0, and let \u03a6 denote a matrix whose columns are the feature vectors for all states. Let d\u03bc and d\u03c0 denote the stationary distributions over states induced by the policies \u03bc and \u03c0, respectively. For some d \u2208 R satisfying d > 0 element-wise, we denote by \u03a0d a projection to the subspace spanned by \u03c6(s) with respect to the d-weighted Euclideannorm. For \u03bb = 0, the ETD(0, \u03b2) (Sutton, Mahmood, and White, 2015) algorithm seeks to find a good approximation of the value function by iteratively updating the weight vector \u03b8: \u03b8t+1 = \u03b8t + \u03b1Ft\u03c1t(Rt+1 + \u03b3\u03b8 \u22a4 t \u03c6t+1 \u2212 \u03b8 \u22a4 t \u03c6t)\u03c6t Ft = \u03b2\u03c1t\u22121Ft\u22121 + 1, F0 = 1, (1) where Ft is a decaying trace of the importance-sampling ratios, and \u03b2 \u2208 (0, 1) controls the decay rate. Remark 1. The algorithm of Sutton, Mahmood, and White (2015) selects the decay rate equal to the discount factor, i.", "startOffset": 36, "endOffset": 1420}, {"referenceID": 9, "context": "Linear temporal difference methods (Sutton and Barto, 1998) approximate the value function by V (s) \u2248 \u03b8\u03c6(s), where \u03c6(s) \u2208 R are state features, and \u03b8 \u2208 R are weights, and use sampling to find a suitable \u03b8. Let \u03bc denote a behavior policy that generates the samples s0, a0, s1, a1, . . . according to at \u223c \u03bc(\u00b7|st) and st+1 \u223c P (\u00b7|st, at). We denote by \u03c1t the ratio \u03c0(at|st)/\u03bc(at|st), and we assume, similarly to Sutton, Mahmood, and White (2015), that \u03bc and \u03c0 are such that \u03c1t is well-defined1 for all t. Let T denote the Bellman operator for policy \u03c0, given by T (V ) . = R+ \u03b3PV, where R and P are the reward vector and transition matrix induced by policy \u03c0, and let \u03a6 denote a matrix whose columns are the feature vectors for all states. Let d\u03bc and d\u03c0 denote the stationary distributions over states induced by the policies \u03bc and \u03c0, respectively. For some d \u2208 R satisfying d > 0 element-wise, we denote by \u03a0d a projection to the subspace spanned by \u03c6(s) with respect to the d-weighted Euclideannorm. For \u03bb = 0, the ETD(0, \u03b2) (Sutton, Mahmood, and White, 2015) algorithm seeks to find a good approximation of the value function by iteratively updating the weight vector \u03b8: \u03b8t+1 = \u03b8t + \u03b1Ft\u03c1t(Rt+1 + \u03b3\u03b8 \u22a4 t \u03c6t+1 \u2212 \u03b8 \u22a4 t \u03c6t)\u03c6t Ft = \u03b2\u03c1t\u22121Ft\u22121 + 1, F0 = 1, (1) where Ft is a decaying trace of the importance-sampling ratios, and \u03b2 \u2208 (0, 1) controls the decay rate. Remark 1. The algorithm of Sutton, Mahmood, and White (2015) selects the decay rate equal to the discount factor, i.e., \u03b2 = \u03b3. Here, we provide more freedom in choosing the decay rate. As our analysis reveals, the decay rate controls a bias-variance trade-off of ETD, therefore this freedom is important. Moreover, we note that for \u03b2 = 0, we obtain the standard TD in an off-policy setting Yu (2012), and when \u03b2 = 1 we obtain the full importance-sampling TD algorithm Precup, Sutton, and Dasgupta (2001).", "startOffset": 36, "endOffset": 1759}, {"referenceID": 9, "context": "Linear temporal difference methods (Sutton and Barto, 1998) approximate the value function by V (s) \u2248 \u03b8\u03c6(s), where \u03c6(s) \u2208 R are state features, and \u03b8 \u2208 R are weights, and use sampling to find a suitable \u03b8. Let \u03bc denote a behavior policy that generates the samples s0, a0, s1, a1, . . . according to at \u223c \u03bc(\u00b7|st) and st+1 \u223c P (\u00b7|st, at). We denote by \u03c1t the ratio \u03c0(at|st)/\u03bc(at|st), and we assume, similarly to Sutton, Mahmood, and White (2015), that \u03bc and \u03c0 are such that \u03c1t is well-defined1 for all t. Let T denote the Bellman operator for policy \u03c0, given by T (V ) . = R+ \u03b3PV, where R and P are the reward vector and transition matrix induced by policy \u03c0, and let \u03a6 denote a matrix whose columns are the feature vectors for all states. Let d\u03bc and d\u03c0 denote the stationary distributions over states induced by the policies \u03bc and \u03c0, respectively. For some d \u2208 R satisfying d > 0 element-wise, we denote by \u03a0d a projection to the subspace spanned by \u03c6(s) with respect to the d-weighted Euclideannorm. For \u03bb = 0, the ETD(0, \u03b2) (Sutton, Mahmood, and White, 2015) algorithm seeks to find a good approximation of the value function by iteratively updating the weight vector \u03b8: \u03b8t+1 = \u03b8t + \u03b1Ft\u03c1t(Rt+1 + \u03b3\u03b8 \u22a4 t \u03c6t+1 \u2212 \u03b8 \u22a4 t \u03c6t)\u03c6t Ft = \u03b2\u03c1t\u22121Ft\u22121 + 1, F0 = 1, (1) where Ft is a decaying trace of the importance-sampling ratios, and \u03b2 \u2208 (0, 1) controls the decay rate. Remark 1. The algorithm of Sutton, Mahmood, and White (2015) selects the decay rate equal to the discount factor, i.e., \u03b2 = \u03b3. Here, we provide more freedom in choosing the decay rate. As our analysis reveals, the decay rate controls a bias-variance trade-off of ETD, therefore this freedom is important. Moreover, we note that for \u03b2 = 0, we obtain the standard TD in an off-policy setting Yu (2012), and when \u03b2 = 1 we obtain the full importance-sampling TD algorithm Precup, Sutton, and Dasgupta (2001). 1Namely, if \u03bc(a|s) = 0 then \u03c0(a|s) = 0 for all s \u2208 S.", "startOffset": 36, "endOffset": 1863}, {"referenceID": 13, "context": "(2) It can be shown (Sutton, Mahmood, and White, 2015; Yu, 2015), that ETD(0, \u03b2) converges to \u03b8 - a solution of the following projected fixed point equation: V = \u03a0fTV, V \u2208 R .", "startOffset": 20, "endOffset": 64}, {"referenceID": 1, "context": "(3) For the fixed point equation (3), a contraction property of \u03a0fT is important for guaranteeing both a unique solution, and a bias bound (Bertsekas and Tsitsiklis, 1996).", "startOffset": 139, "endOffset": 171}, {"referenceID": 1, "context": "It is well known that T is a \u03b3-contraction with respect to the d\u03c0-weighted Euclidean norm (Bertsekas and Tsitsiklis, 1996), and by definition \u03a0f is a non-expansion in f norm, however, it is not immediate that the composed operator \u03a0fT is a contraction in any norm.", "startOffset": 90, "endOffset": 122}, {"referenceID": 0, "context": "Indeed, for the TD(0) algorithm (Sutton and Barto 1998; corresponding to the \u03b2 = 0 case in our setting), a similar representation as a projected Bellman operator holds, but it may be shown that in the off-policy setting the algorithm might diverge (Baird, 1995).", "startOffset": 248, "endOffset": 261}, {"referenceID": 1, "context": "Since \u03a0f is a non-expansion in the f -weighted norm (Bertsekas and Tsitsiklis, 1996), \u03a0fT is a \u221a \u03b3 \u03b2 (1 \u2212 \u03ba)-contraction as well.", "startOffset": 52, "endOffset": 84}, {"referenceID": 1, "context": "In this case, the contraction modulus in Theorem 1 is \u03b3, similar to the result for on-policy TD Bertsekas and Tsitsiklis (1996). We remark that Kolter (2011) also used a measure of discrepancy between the behavior and the target policy to bound the TD-error.", "startOffset": 96, "endOffset": 128}, {"referenceID": 1, "context": "In this case, the contraction modulus in Theorem 1 is \u03b3, similar to the result for on-policy TD Bertsekas and Tsitsiklis (1996). We remark that Kolter (2011) also used a measure of discrepancy between the behavior and the target policy to bound the TD-error.", "startOffset": 96, "endOffset": 158}, {"referenceID": 1, "context": "In this case, the contraction modulus in Theorem 1 is \u03b3, similar to the result for on-policy TD Bertsekas and Tsitsiklis (1996). We remark that Kolter (2011) also used a measure of discrepancy between the behavior and the target policy to bound the TD-error. However, Kolter (2011) considered the standard TD algorithm, for which a contraction could be guaranteed only for a class of behavior policies that satisfy a certain matrix inequality criterion.", "startOffset": 96, "endOffset": 282}, {"referenceID": 1, "context": "9 of Bertsekas and Tsitsiklis (1996): Corollary 1.", "startOffset": 5, "endOffset": 37}, {"referenceID": 7, "context": "Such an analysis may proceed similarly to that in Munos (2003) for the on-policy case.", "startOffset": 50, "endOffset": 63}, {"referenceID": 4, "context": "Consider the 2-state MDP example of Kolter (2011), with transition matrix P = (1/2)1 (where 1 is an all 1 matrix), discount factor \u03b3 = 0.", "startOffset": 36, "endOffset": 50}, {"referenceID": 10, "context": "This means that algorithms that converge to this fixed point, such as the GTD algorithm (Sutton et al., 2009), are hopeless in such cases.", "startOffset": 88, "endOffset": 109}, {"referenceID": 4, "context": "Note that, as observed by Kolter (2011), for certain behavior policies the bias of standard TD is infinite.", "startOffset": 26, "endOffset": 40}, {"referenceID": 4, "context": "Note that, as observed by Kolter (2011), for certain behavior policies the bias of standard TD is infinite. This means that algorithms that converge to this fixed point, such as the GTD algorithm (Sutton et al., 2009), are hopeless in such cases. The ETD algorithm, on the other hand, has a bounded bias for all behavior policies. 4 The Bias-Variance Trade-Off of ETD(0, \u03b2) From the results in Corollary 1, it is clear that increasing the decay rate \u03b2 decreases the bias bound. Indeed, for the case \u03b2 = 1 we obtain the importance sampling TD algorithm (Precup, Sutton, and Dasgupta, 2001), which is known to have a bias bound similar to on-policy TD. However, as recognized by Precup, Sutton, and Dasgupta (2001) and Sutton, Mahmood, and White (2015), the importance sampling ratio Ft suffers from a high variance, which increases with \u03b2.", "startOffset": 26, "endOffset": 713}, {"referenceID": 4, "context": "Note that, as observed by Kolter (2011), for certain behavior policies the bias of standard TD is infinite. This means that algorithms that converge to this fixed point, such as the GTD algorithm (Sutton et al., 2009), are hopeless in such cases. The ETD algorithm, on the other hand, has a bounded bias for all behavior policies. 4 The Bias-Variance Trade-Off of ETD(0, \u03b2) From the results in Corollary 1, it is clear that increasing the decay rate \u03b2 decreases the bias bound. Indeed, for the case \u03b2 = 1 we obtain the importance sampling TD algorithm (Precup, Sutton, and Dasgupta, 2001), which is known to have a bias bound similar to on-policy TD. However, as recognized by Precup, Sutton, and Dasgupta (2001) and Sutton, Mahmood, and White (2015), the importance sampling ratio Ft suffers from a high variance, which increases with \u03b2.", "startOffset": 26, "endOffset": 751}, {"referenceID": 10, "context": "Thus, algorithms that converge to the standard TD fixed point such as GTD Sutton et al. (2009) are significantly outperformed by ETD(0, \u03b2) in this case.", "startOffset": 74, "endOffset": 95}, {"referenceID": 6, "context": "Mahmood et al. (2015) show that, under suitable step-size conditions, ETD converges to some \u03b8 that is a solution of the projected fixed-point equation: \u03b8\u03a6 = \u03a0mT (\u03b8\u03a6).", "startOffset": 0, "endOffset": 22}, {"referenceID": 6, "context": "Mahmood et al. (2015) show that, under suitable step-size conditions, ETD converges to some \u03b8 that is a solution of the projected fixed-point equation: \u03b8\u03a6 = \u03a0mT (\u03b8\u03a6). In their analysis, however, Mahmood et al. (2015) did not show how well the solution \u03a6\u03b8 approximates V .", "startOffset": 0, "endOffset": 217}, {"referenceID": 3, "context": "However, as is well-known (Bertsekas, 2012), increasing \u03bb also increases the variance of the algorithm, and we therefore obtain a bias-variance trade-off in \u03bb as well as \u03b2.", "startOffset": 26, "endOffset": 43}, {"referenceID": 3, "context": "However, as is well-known (Bertsekas, 2012), increasing \u03bb also increases the variance of the algorithm, and we therefore obtain a bias-variance trade-off in \u03bb as well as \u03b2. Finally, note that for \u03b2 = \u03b3, the contraction modulus equals \u221a \u03b3(1\u2212\u03bb) 1\u2212\u03b3\u03bb , and that for \u03bb = 0 the result is the same as in Theorem 1. 6 Conclusion In this work we unified several off-policy TD algorithms under the ETD(\u03bb, \u03b2) framework, which flexibly manages the bias and variance of the algorithm by controlling the decay-rate of the importance-sampling ratio. From this perspective, we showed that several different methods proposed in the literature are special instances of this biasvariance selection. Our main contribution is an error analysis of ETD(\u03bb, \u03b2) that quantifies the biasvariance trade-off. In particular, we showed that the recently proposed ETD algorithm of Sutton, Mahmood, and White (2015) has bounded bias for general behavior and target policies, and that by controlling the decay-rate in the ETD(\u03bb, \u03b2) algorithm, an improved performance may be obtained by reducing the variance of the algorithm while still maintaining a reasonable bias.", "startOffset": 27, "endOffset": 884}], "year": 2015, "abstractText": "We consider the off-policy evaluation problem in Markov decision processes with function approximation. We propose a generalization of the recently introduced emphatic temporal differences (ETD) algorithm (Sutton, Mahmood, and White, 2015), which encompasses the original ETD(\u03bb), as well as several other off-policy evaluation algorithms as special cases. We call this framework ETD(\u03bb, \u03b2), where our introduced parameter \u03b2 controls the decay rate of an importancesampling term. We study conditions under which the projected fixedpoint equation underlying ETD(\u03bb, \u03b2) involves a contraction operator, allowing us to present the first asymptotic error bounds (bias) for ETD(\u03bb, \u03b2). Our results show that the original ETD algorithm always involves a contraction operator, and its bias is bounded. Moreover, by controlling \u03b2, our proposed generalization allows trading-off bias for variance reduction, thereby achieving a lower total error.", "creator": "LaTeX with hyperref package"}}}