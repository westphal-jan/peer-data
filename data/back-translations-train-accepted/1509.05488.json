{"id": "1509.05488", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2015", "title": "TransG : A Generative Mixture Model for Knowledge Graph Embedding", "abstract": "Recently, knowledge graph embedding, which projects symbolic entities and relations into continuous vector space, has become a new, hot topic in artificial intelligence. This paper addresses a new issue of \\textbf{multiple relation semantics} that a relation may have multiple meanings revealed by the entity pairs associated with the corresponding triples, and proposes a novel Gaussian mixture model for embedding, \\textbf{TransG}. The new model can discover latent semantics for a relation and leverage a mixture of relation component vectors for embedding a fact triple. To the best of our knowledge, this is the first generative model for knowledge graph embedding, which is able to deal with multiple relation semantics. Extensive experiments show that the proposed model achieves substantial improvements against the state-of-the-art baselines.", "histories": [["v1", "Fri, 18 Sep 2015 02:30:17 GMT  (1019kb)", "http://arxiv.org/abs/1509.05488v1", null], ["v2", "Mon, 28 Sep 2015 02:17:33 GMT  (390kb,D)", "http://arxiv.org/abs/1509.05488v2", null], ["v3", "Mon, 21 Dec 2015 01:32:17 GMT  (343kb,D)", "http://arxiv.org/abs/1509.05488v3", null], ["v4", "Sun, 27 Dec 2015 05:46:51 GMT  (339kb,D)", "http://arxiv.org/abs/1509.05488v4", null], ["v5", "Tue, 13 Jun 2017 06:03:20 GMT  (344kb,D)", "http://arxiv.org/abs/1509.05488v5", null], ["v6", "Sat, 17 Jun 2017 03:54:46 GMT  (377kb,D)", "http://arxiv.org/abs/1509.05488v6", null], ["v7", "Fri, 8 Sep 2017 12:55:14 GMT  (394kb,D)", "http://arxiv.org/abs/1509.05488v7", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["han xiao", "minlie huang", "yu hao", "xiaoyan zhu"], "accepted": true, "id": "1509.05488"}, "pdf": {"name": "1509.05488.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 150 9.05 488v 1 [cs.C L] 18 Sep 20"}, {"heading": "Introduction", "text": "In this sense, however, the embedding of knowledge in continuous vector spaces can be proposed. Among various embedding models, there are a number of translation-based models such as TranE (Bordes et al. 2013), TransH et al. 2014) and TransR (Lin et al. 2015). A fact of the knowledge base can usually be represented by a triple (h, r, t), where h, r, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t"}, {"heading": "Related Work", "text": "Preliminary studies are divided into two areas: translation-based embedding methods and the others."}, {"heading": "Translation-Based Embedding Methods", "text": "Existing translation-based embedding methods share the same translation principle h + r \u2248 t and the score function is designed as follows: fr (h, t) = | | hr + r \u2212 tr | | 2 2where hr, tr are entities embedding vectors projected into relationship-specific space. TransE (Bordes et al. 2013) places the entities in the original entity space: hr = h, tr = t. TransH (Wang et al. 2015) projects the entities into a hyperplane to address the problem of complex embedding of relationships: hr = h \u2212 w r hwr, tr \u2212 w r wr. To address the same problem, TransR (Lin et al. 2015) transforms the entity embedding through the same matrix: hr = Mrh, tr = Mrt.Figure 2: Visualization of multiple groups of relations in semantics."}, {"heading": "Other Embedding Methods", "text": "There is a list of other embedding approaches: Structured Embedding (SE). The SE model (Bordes et al. 2011) transforms the entity space with the head and tail-specific matrices. The score function is defined as fr (h, t) = | Mh, rh \u2212 Mt, rt | |. According to (Socher et al. 2013) this model cannot capture the relationship between entities and the Hadamard product. In some recent work (Bordes et al. 2014) the score function is redefined with 3-way tensors instead of with matrices.Single Layer Model (SLM et al. 2014). SLM applies the neural network to the embedding of knowledge graphs. The score function is defined as fr (h, t)."}, {"heading": "Methods", "text": "In this section we first look at current translation-based methods from a generative perspective and then introduce TransG, a generative mixture embedding model for recording multiple relation semantics."}, {"heading": "A Generative Perspective for Embedding", "text": "The score function is defined as: fr (h, t) = | | h + r \u2212 t | | 2 2 (1) Therefore, the generation process could be considered such that the difference vector between tail and head (t \u2212 h) is drawn from a Gaussian distribution with r as mean and an identical matrix E. If we maximize the data probability of the training set, we derive the TransE model. As discussed in \"Related Work,\" different translation-based methods project units into different vector spaces. Obviously, TransH and TransR have a similar probability of generation as (r \u2212 r), (r \u2212 r), (r \u2212 r), (r, r, r, r, r, (r, r, r, (), (r, r, (), (r, r, r, r, (), (r, r, r, r, r, (), (), (r, r, r, r, r, r, r, r, (), (), (), (r, r, r, r, r, r, r, r, (), (), (), (r, r, r, r, r, r, r, (), (), (r, r, r, r, r, r, r, r, r, r, (), (r, r, r, r, r, r), (), (r, (r, r, r, r), (r), (r), (r, (r), (r), (r, r, (r), (r), (r), (r), (r), (r), (r), (r), (r, (r), (r), (r, (r), (r), (r), (r), (r, (r), (r), (r, (r), (r), (r), (r), (r), (r, (r), (r), (r), (r), (r, (r), (r), (r), (r), (r), (r, (r), (r), (r"}, {"heading": "TransG: A Generative Mixture Model for Embedding", "text": "As explained in the \"Introduction,\" only a single translation vector for a relationship can be incompetent to model multiple relation semantics, which means that Eq. (2) should be generalized to a mixture model such as the Gaussian mixture model (Yu 2012). Therefore, our TransG is proposed as follows: t \u2212 h | r \u0445 M = 1\u03c0r, mN (ur, m, E) (6) P {(h, r, t)} = M \u2211 m = 1\u03c0r, me \u2212 h + ur, m \u2212 t | 2 (7), where M indicates how many components should have a relation. ur, m is the m-th component transformation vector of the relationship r. \u03c0r, m is the mixing factor indicating the weight of the m-th component. Note that if one could be treated as a component before Bajesian statistics, TransG represents these coefficients from Figure & red, Figure 1-G specific component for G."}, {"heading": "Perspective from Geometry", "text": "Previous studies have always had geometric explanations, and the same is true for TransG. In previous methods, this geometric principle is generalized to: m \u00b2 (h, r, t) = argmaxm = 1... M (\u03c0r, me \u2212 | h + ur, m \u2212 t | 2) (8) h + ur, m \u00b2 (h, r, t) \u2248 t (9), where m \u00b2 (h, r, t) is the index of the primary component. Although all components contribute to the model, the primary contributes most to the exponential effect (exp (\u00b7). If a triple (h, r, t) component is given, TransG works out the index of the primary component and then translates the six-dimensional unit to the tail."}, {"heading": "Training Algorithm", "text": "The principle of maximum data probability is applied to training. In order to better distinguish the true triples from the false ones, we maximize the ratio of probability of the true triples to those of the wrong ones. \u2212 Above all, too many redundant components can damage the embedding, and therefore we use a term of deficiency regulation for the embedding of vectors. \u2212 Taking into account all other limitations, the final objective function is achieved as follows: min \u2212 \u2211 (h, r, t) that the proposed tasks are initialized by (Glorot and Bengio 2010)."}, {"heading": "Link Prediction", "text": "To measure the performance of the Knowledge Chart in descending order, we then get the precedence of the triple metrics. When one company and one relationship is given, the embedding models predict the other missing entity. More specifically, in this task, we predict that many AI tasks can be improved by link predictions such as Relation (Hoffmann et al. 2011). Evaluation Protocol. We adopt the same protocol used in previous studies. For each triple test (h, r, t), we corrupt it by replacing the tail t (or the head h) and calculate a probable score of this corrupted triangle (h, r, e) (or (e, r, t) with the score function fr (h, e)."}, {"heading": "Triples Classification", "text": "To prove the discrimination between true and false facts, a triple classification task is performed. This is a classic task in knowledge data embedding, which aims to predict whether a given triple number (h, r, t) is correct or not. WN11 and FB13 are the benchmark data sets for this task. Note that the evaluation of the classification requires negative samples, and the data sets have already been created with negative triple numbers.Evaluation protocol. The decision process is very simple as follows: for a triple number (h, r, t), if fr (h, t) is below a threshold, then positive; otherwise negative. Thresholds {\u03c3r} are determined on the validation datas.Implementation. Since all methods use the same data sets, we directly use the results of different methods from the literature. We have tried several settings on the validation datasets to find the best configuration."}, {"heading": "Semantic Component Analysis", "text": "In this subsection, we analyze the number of semantic components for different relationships. As conciseness is taken into account in our approach, we list the number of components on the data set WN18 and FB13 in Fig.4, where only the component with non-zero \u03c0m is counted. Results. As Fig. 4 and Tab. 5 show, all other relationships have more than one semantic component. 2. Different components actually correspond with different semantic components, which justifies the theoretical analysis and effectiveness of TransG. For example, \"profession\" has at least three significant semantics: scientifically as (ElLissitzky, architect), business-related as (EnochPratt, entrepreneur) and writer-related as (Vlad.Gardin, ScreenWriter).3 WN11 and W11 as different W11 relationships."}, {"heading": "Conclusion", "text": "In this article, we address a new topic, the semantics of multiple relations, and propose TransG, a generative mixing embedding model for the embedding of knowledge graphs. TransG can automatically detect the latent semantics of a relationship and use a mixture of relation components for embedding. Extensive experiments show that our method achieves substantial improvements over modern fundamentals. To reproduce our results, experimental codes and data are published in github1."}], "references": [{"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["K. Bollacker", "C. Evans", "P. Paritosh", "T. Sturge", "J. Taylor"], "venue": "Proceedings of the 2008 ACM SIGMOD international conference on Management of data, 1247\u20131250. ACM.", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Learning structured embeddings of knowledge bases", "author": ["A. Bordes", "J. Weston", "R. Collobert", "Y Bengio"], "venue": "In Proceedings of the Twenty-fifth AAAI Conference on Artificial Intelligence", "citeRegEx": "Bordes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2011}, {"title": "Joint learning of words and meaning representations for open-text semantic parsing", "author": ["A. Bordes", "X. Glorot", "J. Weston", "Y. Bengio"], "venue": "International Conference on Artificial Intelligence and Statistics, 127\u2013135.", "citeRegEx": "Bordes et al\\.,? 2012", "shortCiteRegEx": "Bordes et al\\.", "year": 2012}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["A. Bordes", "N. Usunier", "A. Garcia-Duran", "J. Weston", "O. Yakhnenko"], "venue": "Advances in Neural Information Processing Systems, 2787\u20132795.", "citeRegEx": "Bordes et al\\.,? 2013", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "A semantic matching energy function for learning with multirelational data", "author": ["A. Bordes", "X. Glorot", "J. Weston", "Y. Bengio"], "venue": "Machine Learning 94(2):233\u2013259.", "citeRegEx": "Bordes et al\\.,? 2014", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of the 25th international conference on Machine learning, 160\u2013167. ACM.", "citeRegEx": "Collobert and Weston,? 2008", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Transition-based knowledge graph embedding with relational mapping properties", "author": ["M. Fan", "Q. Zhou", "E. Chang", "T.F. Zheng"], "venue": "Proceedings of the 28th Pacific Asia Conference on Language, Information, and Computation, 328\u2013337.", "citeRegEx": "Fan et al\\.,? 2014", "shortCiteRegEx": "Fan et al\\.", "year": 2014}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "International conference on artificial intelligence and statistics, 249\u2013256.", "citeRegEx": "Glorot and Bengio,? 2010", "shortCiteRegEx": "Glorot and Bengio", "year": 2010}, {"title": "Semantically smooth knowledge graph embedding", "author": ["S. Guo", "Q. Wang", "B. Wang", "L. Wang", "L. Guo"], "venue": "Proceedings of ACL.", "citeRegEx": "Guo et al\\.,? 2015", "shortCiteRegEx": "Guo et al\\.", "year": 2015}, {"title": "Knowledge-based weak supervision for information extraction of overlapping relations", "author": ["R. Hoffmann", "C. Zhang", "X. Ling", "L. Zettlemoyer", "D.S. Weld"], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language", "citeRegEx": "Hoffmann et al\\.,? 2011", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2011}, {"title": "A latent factor model for highly multi-relational data", "author": ["R. Jenatton", "N.L. Roux", "A. Bordes", "G.R. Obozinski"], "venue": "Advances in Neural Information Processing Systems, 3167\u20133175.", "citeRegEx": "Jenatton et al\\.,? 2012", "shortCiteRegEx": "Jenatton et al\\.", "year": 2012}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Y. Lin", "Z. Liu", "M. Sun", "Y. Liu", "X. Zhu"], "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence.", "citeRegEx": "Lin et al\\.,? 2015", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Modeling relation paths for representation learning of knowledge bases", "author": ["Y. Lin", "Z. Liu", "M. Sun"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics.", "citeRegEx": "Lin et al\\.,? 2015", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Wordnet: a lexical database for english", "author": ["G.A. Miller"], "venue": "Communications of the ACM 38(11):39\u201341.", "citeRegEx": "Miller,? 1995", "shortCiteRegEx": "Miller", "year": 1995}, {"title": "A threeway model for collective learning on multi-relational data", "author": ["M. Nickel", "V. Tresp", "H.-P. Kriegel"], "venue": "Proceedings of the 28th international conference on machine learning (ICML-11), 809\u2013816.", "citeRegEx": "Nickel et al\\.,? 2011", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "Factorizing yago: scalable machine learning for linked data", "author": ["M. Nickel", "V. Tresp", "H.-P. Kriegel"], "venue": "Proceedings of the 21st international conference on World Wide Web, 271\u2013280. ACM.", "citeRegEx": "Nickel et al\\.,? 2012", "shortCiteRegEx": "Nickel et al\\.", "year": 2012}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["R. Socher", "D. Chen", "C.D. Manning", "A. Ng"], "venue": "Advances in Neural Information Processing Systems, 926\u2013934.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Modelling relational data using bayesian clustered tensor", "author": ["I. Sutskever", "J.B. Tenenbaum", "R. Salakhutdinov"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2009}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Z. Wang", "J. Zhang", "J. Feng", "Z. Chen"], "venue": "Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, 1112\u20131119.", "citeRegEx": "Wang et al\\.,? 2014", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Knowledge base completion using embeddings and rules", "author": ["Q. Wang", "B. Wang", "L. Guo"], "venue": "Proceedings of the 24th International Joint Conference on Artificial Intelligence.", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "A nonlinear kernel gaussian mixture model based inferential monitoring approach for fault detection and diagnosis of chemical processes", "author": ["J. Yu"], "venue": "Chemical Engineering Science 68(1):506\u2013519.", "citeRegEx": "Yu,? 2012", "shortCiteRegEx": "Yu", "year": 2012}], "referenceMentions": [{"referenceID": 13, "context": "Knowledge bases such as Wordnet (Miller 1995) and Freebase (Bollacker et al.", "startOffset": 32, "endOffset": 45}, {"referenceID": 0, "context": "Knowledge bases such as Wordnet (Miller 1995) and Freebase (Bollacker et al. 2008) have been shown very useful to AI tasks including question answering, knowledge inference, and so on.", "startOffset": 59, "endOffset": 82}, {"referenceID": 3, "context": "Among various embedding models, there is a line of translationbased models such as TransE (Bordes et al. 2013), TransH (Wang et al.", "startOffset": 90, "endOffset": 110}, {"referenceID": 18, "context": "2013), TransH (Wang et al. 2014) and TransR (Lin et al.", "startOffset": 14, "endOffset": 32}, {"referenceID": 11, "context": "2014) and TransR (Lin et al. 2015).", "startOffset": 17, "endOffset": 34}, {"referenceID": 3, "context": "1, clustering results on embedding vectors obtained from TransE (Bordes et al. 2013) show that, there are different clusters for a specific relation, and different clusters indicate different latent semantics.", "startOffset": 64, "endOffset": 84}, {"referenceID": 3, "context": "TransE (Bordes et al. 2013), lays the entities in the original entity space: hr = h, tr = t.", "startOffset": 7, "endOffset": 27}, {"referenceID": 18, "context": "TransH (Wang et al. 2014), projects entities into a hyperplane for addressing the issue of complex relation embedding: hr = h\u2212w r hwr, tr = t\u2212w \u22a4 r twr.", "startOffset": 7, "endOffset": 25}, {"referenceID": 11, "context": "To address the same issue, TransR (Lin et al. 2015), transforms the entity embeddings by the same matrix:hr = Mrh, tr = Mrt.", "startOffset": 34, "endOffset": 51}, {"referenceID": 6, "context": "TransM (Fan et al. 2014) leverages the structure of the knowledge graph via pre-calculating the distinct weight for each training triple to enhance embedding.", "startOffset": 7, "endOffset": 24}, {"referenceID": 1, "context": "The SE model (Bordes et al. 2011) transforms the entity space with the head-specific and tail-specific matrices.", "startOffset": 13, "endOffset": 33}, {"referenceID": 16, "context": "According to (Socher et al. 2013), this model cannot capture the relationship between entities.", "startOffset": 13, "endOffset": 33}, {"referenceID": 2, "context": "The SME model (Bordes et al. 2012) (Bordes et al.", "startOffset": 14, "endOffset": 34}, {"referenceID": 4, "context": "2012) (Bordes et al. 2014) can handle the correlations between entities and relations by matrix product and Hadamard product.", "startOffset": 6, "endOffset": 26}, {"referenceID": 4, "context": "In some recent work (Bordes et al. 2014), the score function is re-defined with 3-way tensors instead of matrices.", "startOffset": 20, "endOffset": 40}, {"referenceID": 5, "context": "Collobert had applied a similar method into the language model, (Collobert and Weston 2008).", "startOffset": 64, "endOffset": 91}, {"referenceID": 10, "context": "The LFM (Jenatton et al. 2012), (Sutskever, Tenenbaum, and Salakhutdinov 2009) attempts to capture the second-order correlations between entities by a quadratic form.", "startOffset": 8, "endOffset": 30}, {"referenceID": 16, "context": "The NTN model (Socher et al. 2013) defines a very expressive score function to combine the SLM and LFM: fr(h, t) =", "startOffset": 14, "endOffset": 34}, {"referenceID": 2, "context": "The UM (Bordes et al. 2012) may be a simplified version of TransE without considering any relation-related information.", "startOffset": 7, "endOffset": 27}, {"referenceID": 8, "context": "(Guo et al. 2015) aims at further discovering the geometric structure of the embedding space to make it semantically smooth.", "startOffset": 0, "endOffset": 17}, {"referenceID": 18, "context": "(Wang et al. 2014) jointly embeds knowledge and texts.", "startOffset": 0, "endOffset": 18}, {"referenceID": 20, "context": "(2) should be generalised to a mixture model, such as Gaussian Mixture Model (Yu 2012).", "startOffset": 77, "endOffset": 86}, {"referenceID": 7, "context": "Note that embedding vectors are initialized by (Glorot and Bengio 2010).", "startOffset": 47, "endOffset": 71}, {"referenceID": 3, "context": "Hence, we introduce a similar condition as TransE (Bordes et al. 2013) adopts: the training algorithm will update the embedding vectors only if the below condition is satisfied:", "startOffset": 50, "endOffset": 70}, {"referenceID": 9, "context": "Note that many AI tasks could be enhanced by Link Prediction such as relation extraction (Hoffmann et al. 2011).", "startOffset": 89, "endOffset": 111}, {"referenceID": 3, "context": "As the datasets are the same, we directly reproduce the experimental results of several baselines from the literature, as in (Bordes et al. 2013), (Wang et al.", "startOffset": 125, "endOffset": 145}, {"referenceID": 18, "context": "2013), (Wang et al. 2014) and (Lin et al.", "startOffset": 7, "endOffset": 25}, {"referenceID": 11, "context": "2014) and (Lin et al. 2015).", "startOffset": 10, "endOffset": 27}, {"referenceID": 1, "context": "Metric Mean Rank HITS@10(%) Mean Rank HITS@10(%) Raw Filter Raw Filter Raw Filter Raw Filter SE(Bordes et al. 2011) 1,011 985 68.", "startOffset": 95, "endOffset": 115}, {"referenceID": 2, "context": "8 SME(linear) (Bordes et al. 2012) 545 533 65.", "startOffset": 14, "endOffset": 34}, {"referenceID": 2, "context": "8 SME(bilinear) (Bordes et al. 2012) 526 509 54.", "startOffset": 16, "endOffset": 36}, {"referenceID": 10, "context": "3 LFM (Jenatton et al. 2012) 469 456 71.", "startOffset": 6, "endOffset": 28}, {"referenceID": 3, "context": "1 TransE (Bordes et al. 2013) 263 251 75.", "startOffset": 9, "endOffset": 29}, {"referenceID": 18, "context": "1 TransH (Wang et al. 2014) 401 388 73.", "startOffset": 9, "endOffset": 27}, {"referenceID": 11, "context": "4 TransR (Lin et al. 2015) 238 225 79.", "startOffset": 9, "endOffset": 26}, {"referenceID": 11, "context": "7 CTransR (Lin et al. 2015) 231 218 79.", "startOffset": 10, "endOffset": 27}, {"referenceID": 1, "context": "Relation Category 1-1 1-N N-1 N-N 1-1 1-N N-1 N-N SE(Bordes et al. 2011) 35.", "startOffset": 52, "endOffset": 72}, {"referenceID": 2, "context": "3 SME(linear) (Bordes et al. 2012) 35.", "startOffset": 14, "endOffset": 34}, {"referenceID": 2, "context": "3 SME(bilinear) (Bordes et al. 2012) 30.", "startOffset": 16, "endOffset": 36}, {"referenceID": 3, "context": "8 TransE (Bordes et al. 2013) 43.", "startOffset": 9, "endOffset": 29}, {"referenceID": 18, "context": "0 TransH (Wang et al. 2014) 66.", "startOffset": 9, "endOffset": 27}, {"referenceID": 11, "context": "2 TransR (Lin et al. 2015) 78.", "startOffset": 9, "endOffset": 26}, {"referenceID": 11, "context": "1 CTransR (Lin et al. 2015) 81.", "startOffset": 10, "endOffset": 27}], "year": 2017, "abstractText": "Recently, knowledge graph embedding, which projects symbolic entities and relations into continuous vector space, has become a new, hot topic in artificial intelligence. This paper addresses a new issue of multiple relation semantics that a relation may have multiple meanings revealed by the entity pairs associated with the corresponding triples, and proposes a novel Gaussian mixture model for embedding, TransG. The new model can discover latent semantics for a relation and leverage a mixture of relation component vectors for embedding a fact triple. To the best of our knowledge, this is the first generative model for knowledge graph embedding, which is able to deal with multiple relation semantics. Extensive experiments show that the proposed model achieves substantial improvements against the state-of-the-art baselines.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}