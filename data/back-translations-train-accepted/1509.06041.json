{"id": "1509.06041", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Sep-2015", "title": "Robust Image Sentiment Analysis Using Progressively Trained and Domain Transferred Deep Networks", "abstract": "Sentiment analysis of online user generated content is important for many social media analytics tasks. Researchers have largely relied on textual sentiment analysis to develop systems to predict political elections, measure economic indicators, and so on. Recently, social media users are increasingly using images and videos to express their opinions and share their experiences. Sentiment analysis of such large scale visual content can help better extract user sentiments toward events or topics, such as those in image tweets, so that prediction of sentiment from visual content is complementary to textual sentiment analysis. Motivated by the needs in leveraging large scale yet noisy training data to solve the extremely challenging problem of image sentiment analysis, we employ Convolutional Neural Networks (CNN). We first design a suitable CNN architecture for image sentiment analysis. We obtain half a million training samples by using a baseline sentiment algorithm to label Flickr images. To make use of such noisy machine labeled data, we employ a progressive strategy to fine-tune the deep network. Furthermore, we improve the performance on Twitter images by inducing domain transfer with a small number of manually labeled Twitter images. We have conducted extensive experiments on manually labeled Twitter images. The results show that the proposed CNN can achieve better performance in image sentiment analysis than competing algorithms.", "histories": [["v1", "Sun, 20 Sep 2015 18:36:01 GMT  (5382kb,D)", "http://arxiv.org/abs/1509.06041v1", "9 pages, 5 figures, AAAI 2015"]], "COMMENTS": "9 pages, 5 figures, AAAI 2015", "reviews": [], "SUBJECTS": "cs.CV cs.IR cs.LG", "authors": ["quanzeng you", "jiebo luo", "hailin jin", "jianchao yang"], "accepted": true, "id": "1509.06041"}, "pdf": {"name": "1509.06041.pdf", "metadata": {"source": "CRF", "title": "Robust Image Sentiment Analysis Using Progressively Trained and Domain Transferred Deep Networks", "authors": ["Quanzeng You", "Jiebo Luo", "Hailin Jin", "Jianchao Yang"], "emails": ["jluo}@cs.rochester.edu", "jiayang}@adobe.com"], "sections": [{"heading": "Introduction", "text": "In fact, it is difficult for most people to express their opinions and opinions. Among the many online users who express their opinions, we are particularly interested in the opinions and feelings of people who are interested in certain topics and events. There are many who are interested in the further development of artificial intelligence (www.facebook.com). All rights to the work of users are reserved to predict the revenue of films (Asur and Hubertus), political elections and other events. Tumasjan et al. 2010 and economic indicators (Bollen, Mao and Zeng, Fuehres 2011)."}, {"heading": "Related Work", "text": "In this section, we look at literature that is closely related to our study of visual sentiment analysis, in particular sentiment analysis and Convolutional Neural Networks."}, {"heading": "Sentiment Analysis", "text": "Sentiment analysis is a very challenging task (Liu et al. 2003; Li et al. 2010). Researchers in natural language processing and information gathering have developed different approaches to solving this problem and achieving promising or satisfactory results (Pang and Lee 2008). In the context of social media, there are several additional unique challenges. Firstly, huge amounts of data are available. Secondly, messages on social networks are by nature informal and brief. Thirdly, people use not only text messages, but also images and videos to express themselves. Tumasjan et al. (2010) and Bollen et al. (2011) used predefined dictionaries to measure the sentiment level of tweets. In contrast, the volume or percentage of sentimental words can produce an estimate of the mood of a particular tweet. Davidov et al. (2010) used the weak labels from a large number of tweets and Bollen et levels. Conversely, they manually select hashtags with strong positive smiley and negative feelings II."}, {"heading": "Convolutional Neural Networks", "text": "Convolutional Neural Networks (CNN) are very successful in document recognition (LeCun et al. 1998). CNN typically consists of several revolutionary layers and several completely connected layers, and between the revolutionary layers there may also be a bundling of layers and normalization layers. CNN is a supervised learning algorithm in which parameters of different layers are learned through back propagation. Due to the computational complexity of CNN, it has previously been applied to relatively small images in literature. Lately, thanks to the increasing computing power of the GPU, it is now possible to train a deep revolutionary neural network on a large-area image dataset (Krizhevsky, Sutskever and Hinton 2012). In fact, CNN has been successfully applied in recent years to scene analysis (Grangier, Bottou and Collobert 2009), to feature learning (LeCun, Kavukcuoglu and Farabet 2010), visual recognition (zukavsky-Krikoglu-Sour) and Sour-Sentil."}, {"heading": "Visual Sentiment Analysis", "text": "We propose to develop a suitable Convolutionary Neural Network Architecture for Visual Sentiment Analysis. In addition, we implement a progressive training strategy that uses the training results of the Convolutionary Neural Network to filter out additional (noisy) training data. Details of the proposed framework are described in the following sections: Emotions. Visual Sentiment Analysis with regular CNN has proven to be effective in image classification tasks, such as achieving the state-of-the-art performance in ImageNet Challenge (Krizhevsky, Sutskever, and Hinton 2012). Visual Sentiment Analysis can also be treated as an image classification problem. It may be a much simpler problem than being the image classification of ImageNet (2 classes vs. 1000 classes in ImageNet). However, visual sentiment analysis is quite challenging because sensations or opinions correspond to high abstractions from a given image. This type of high abstraction requires the viewer's knowledge beyond the image analysis."}, {"heading": "Visual Sentiment Analysis with Progressive CNN", "text": "Because the images are poorly labeled, it is possible that the neural network may get stuck in a poor local optimum, which can lead to poor generalizability of the trained neural network. On the other hand, we have found that the neural network is still able to correctly classify a large portion of the training instances. In other words, the neural network has learned to distinguish the training instances with relatively unique sentiment labels. Therefore, we propose to gradually select a substance of the training instances to reduce the impact of noisy training instances. Figure 3 shows the general flow of the proposed progressive CNN (PCNN). First, we train a CNN on Flickr images. Next, we select training samples based on the predictive value of the trained model on the training data itself. Instead of training from the outset, we will select the trained model with these newly selected and potentially cleaner training instances."}, {"heading": "Experiments", "text": "These images are poorly labeled because each image belongs to an adjective pair (ANP). There are a total of 1200 ANPs. According to Plutchik's Wheel of Emotions (Plutchik 1984), each ANP is generated by combining adjectives with strong mood values and nouns from image and video tags (Borth et al. 2013b). These ANPs are then used as queries to collect related images for each ANP. The published SentiBank contains 1200 ANPs with about half a million patch images. We train our revolutionary neural network mainly on this image set. We implement CNN's proposed architecture on the publicly available implementation Caffe (Jia 2013). All of our experiments are evaluated on a Linux X86 64 machine with 32G RAM and two VIDIA GTX Titan GPUs."}, {"heading": "Comparisons of different CNN architectures", "text": "In Table 1, iCONV-jFC shows that there are 1http: / / visual-sentiment-ontology.appspot.com / i Convolutionary layers and j fully connected layers in the architecture.The model in Figure 2 shows slightly better performance in terms of F1 and accuracy than other models. In the following experiments, we focus mainly on evaluating CNN based on the architecture in Figure 2."}, {"heading": "Baselines", "text": "We compare the performance of PCNN with three other baselines or competing algorithms for classifying image mood. Low-level feature-based Siersdorfer et al. (2010) defined both global and local visual characteristics. Specifically, the global color histograms (GCH) consisted of 64-bin RGB histograms. Local color histogram characteristics (LCH) initially divided the image into 16 blocks and used the 64-bin RGB histogram for each block. In addition, they used SIFT characteristics to learn a visual dictionary. Next, they defined bags of visual word characteristics (BoW) for each image. Mid-level feature-based Damian et al. (2013a; 2013b) proposed a framework for building visual mood ontology and SentiBank according to the previously discussed 1200 ANPs. With the trained 1200 ANP detectors, they are able to pre-generate 1200 responses for each test image by generating these ANPs."}, {"heading": "Deep Learning on Flickr Dataset", "text": "The remaining 10% images are our test data set. We train the Convolutionary Neural Network with 300,000 iterations of mini-stacks (each mini-stack contains 256 images) and we use the sampling probability in Eqn. (3) to filter the training images to its training data according to CNN's predictive value. In the fine-tuning stage of PCNN, we perform another 100,000 iterations of mini-stacks using the filtered training data set. Table 2 gives a summary of the data instances in our experiments. Figure 4 shows the filters learned in the first revolutionary layer of CNN and PCNN respectively. There are some differences between 4 (a) and 4 (b). While it is somewhat inconclusive that the neural networks have reached a better local optimum, we can at least randomly determine that the fine-tuning level of CNN and PCNN is the fine-tuning level of data collection."}, {"heading": "Twitter Testing Dataset", "text": "We created a total of 1269 images as test images for our candidates. We used Crowd Intelligence, Amazon Mechanical Turk (AMT), to generate mood labels for these test images in a similar way to CNN (Borth et al. 2013b). We recruited 5 AMT employees for each candidate image. Table 4 shows the statistics of the identification results of Amazon Mechanical Turk. In the table, \"Five agree\" shows that all 5 AMT workers gave the same mood etiquette for a particular image. Only a small portion of the images, 153 of 1269, showed significant discrepancies between Table 5: Performance of different algorithms on the Twitter image set (Acc stands for Accuracy).Algorithms Five Agree At Least Four Agree At Least Three AgreePrecision Recall F1 Precision Recall F1 Precision Recall F1 Acc Precision Recall F1 Acc Recall F1 Precision Recall F1 0,769 0,80,80,805 722 747 747."}, {"heading": "Transfer Learning", "text": "The characteristics we have learned are common features on these half a million images that we have evaluated. Table 5 shows that these generic features also have the ability to predict visual feelings of images from other areas. The question we are asking ourselves is whether we can further improve the performance of visual sentiment analysis on Twitter images by triggering transmission tutorials. In this section, we will conduct experiments to answer this question. Most of the images relate to current trending topics and personal experiences, making the images on Twitter much more diverse in content as well as quality. In this experiment, we will fine tune the pre-trained neural network model in the following ways to achieve transfer learning results. We randomly divide the images into 5 equal partitions. Each time, we use 4 of the 5 partitions to fine-tune our pre-trained model from half a million images."}, {"heading": "Conclusions", "text": "Visual sentiment analysis is a challenging and interesting problem. In this paper, we adopt the recently developed Convolutionary Neural Networks to solve this problem. We have designed a new architecture, as well as new training strategies to overcome the noisy nature of large training samples. Both progressive training and transfer learning, induced by a small number of self-consciously labeled images from the target area, have yielded remarkable improvements; the experimental results suggest that Convolutionary Neural Networks, properly trained, can surpass both classifiers who use predefined low features or visual attributes for the most challenging problem of visual sentiment analysis, as well as the main advantage of using Convolutionary Neural Networks, which can transfer knowledge to other areas that use a much simpler finetuning technique than those in literature, e.g. (Duan et al. 2012).It is important to propose the importance of this work as a multimodal training, in order to replicate the more direct Borth Method for 2013."}, {"heading": "Acknowledgments", "text": "We thank Columbia University's Digital Video and Multimedia (DVMM) Lab for providing half a million Flickr images and their machine-generated labels."}], "references": [{"title": "B", "author": ["S. Asur", "Huberman"], "venue": "A.", "citeRegEx": "Asur and Huberman 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Modeling public mood and emotion: Twitter sentiment and socio-economic phenomena", "author": ["Mao Bollen", "J. Pepe 2011] Bollen", "H. Mao", "A. Pepe"], "venue": null, "citeRegEx": "Bollen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bollen et al\\.", "year": 2011}, {"title": "Twitter mood predicts the stock market", "author": ["Mao Bollen", "J. Zeng 2011] Bollen", "H. Mao", "X. Zeng"], "venue": "Journal of Computational Science", "citeRegEx": "Bollen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bollen et al\\.", "year": 2011}, {"title": "Sentibank: large-scale ontology and classifiers for detecting sentiment and emotions in visual content", "author": ["Borth"], "venue": "In ACM MM,", "citeRegEx": "Borth,? \\Q2013\\E", "shortCiteRegEx": "Borth", "year": 2013}, {"title": "Large-scale visual sentiment ontology and detectors using adjective noun pairs", "author": ["Borth"], "venue": "In ACM MM,", "citeRegEx": "Borth,? \\Q2013\\E", "shortCiteRegEx": "Borth", "year": 2013}, {"title": "Learned-norm pooling for deep neural networks. CoRR abs/1311.1780", "author": ["\u00c7aglar G\u00fcl\u00e7ehre"], "venue": null, "citeRegEx": "G\u00fcl\u00e7ehre,? \\Q2013\\E", "shortCiteRegEx": "G\u00fcl\u00e7ehre", "year": 2013}, {"title": "L", "author": ["D.C. Cire\u015fan", "U. Meier", "J. Masci", "Gambardella"], "venue": "M.; and Schmidhuber, J.", "citeRegEx": "Cire\u015fan et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Enhanced sentiment learning using twitter hashtags and smileys", "author": ["Tsur Davidov", "D. Rappoport 2010] Davidov", "O. Tsur", "A. Rappoport"], "venue": "In ICL,", "citeRegEx": "Davidov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Davidov et al\\.", "year": 2010}, {"title": "Visual event recognition in videos by learning from web data", "author": ["Duan"], "venue": "IEEE PAMI 34(9):1667\u20131680", "citeRegEx": "Duan,? \\Q2012\\E", "shortCiteRegEx": "Duan", "year": 2012}, {"title": "Deep convolutional networks for scene parsing", "author": ["Bottou Grangier", "D. Collobert 2009] Grangier", "L. Bottou", "R. Collobert"], "venue": "In ICML 2009 Deep Learning Workshop,", "citeRegEx": "Grangier et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Grangier et al\\.", "year": 2009}, {"title": "and Eck", "author": ["P. Hamel"], "venue": "D.", "citeRegEx": "Hamel and Eck 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "G", "author": ["Hinton"], "venue": "E.; Osindero, S.; and Teh, Y.-W.", "citeRegEx": "Hinton. Osindero. and Teh 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Unsupervised sentiment analysis with emotional signals. In WWW, 607\u2013618", "author": ["Hu"], "venue": "International World Wide Web Conferences Steering Committee", "citeRegEx": "Hu,? \\Q2013\\E", "shortCiteRegEx": "Hu", "year": 2013}, {"title": "The wisdom of social multimedia: using flickr for prediction and forecast", "author": ["Jin"], "venue": "In ACM MM,", "citeRegEx": "Jin,? \\Q2010\\E", "shortCiteRegEx": "Jin", "year": 2010}, {"title": "J", "author": ["D. Joshi", "R. Datta", "E. Fedorovskaya", "Q.-T. Luong", "Wang"], "venue": "Z.; Li, J.; and Luo, J.", "citeRegEx": "Joshi et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning convolutional feature hierarchies for visual recognition", "author": ["Kavukcuoglu"], "venue": "In NIPS,", "citeRegEx": "Kavukcuoglu,? \\Q2010\\E", "shortCiteRegEx": "Kavukcuoglu", "year": 2010}, {"title": "G", "author": ["A. Krizhevsky", "I. Sutskever", "Hinton"], "venue": "E.", "citeRegEx": "Krizhevsky. Sutskever. and Hinton 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "L", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "Jackel"], "venue": "D.", "citeRegEx": "LeCun et al. 1989", "shortCiteRegEx": null, "year": 1989}, {"title": "P", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "Haffner"], "venue": "1998. Gradient-based learning applied to document recognition. Proceedings of the IEEE 86(11):2278\u2013", "citeRegEx": "LeCun et al. 1998", "shortCiteRegEx": null, "year": 2324}, {"title": "Convolutional networks and applications in vision", "author": ["Kavukcuoglu LeCun", "Y. Farabet 2010] LeCun", "K. Kavukcuoglu", "C. Farabet"], "venue": "In ISCAS,", "citeRegEx": "LeCun et al\\.,? \\Q2010\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2010}, {"title": "S", "author": ["Li, G.", "Hoi"], "venue": "C.; Chang, K.; and Jain, R.", "citeRegEx": "Li et al. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "P", "author": ["B. Liu", "Y. Dai", "X. Li", "W.S. Lee", "Yu"], "venue": "S.", "citeRegEx": "Liu et al. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "A", "author": ["A.L. Maas", "R.E. Daly", "P.T. Pham", "D. Huang", "Ng"], "venue": "Y.; and Potts, C.", "citeRegEx": "Maas et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Towards multimodal sentiment analysis: Harvesting opinions from the web", "author": ["Mihalcea Morency", "L.-P. Doshi 2011] Morency", "R. Mihalcea", "P. Doshi"], "venue": "In ICMI,", "citeRegEx": "Morency et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Morency et al\\.", "year": 2011}, {"title": "From tweets to polls: Linking text sentiment to public opinion time series", "author": ["O\u2019Connor"], "venue": null, "citeRegEx": "O.Connor,? \\Q2010\\E", "shortCiteRegEx": "O.Connor", "year": 2010}, {"title": "and Lee", "author": ["B. Pang"], "venue": "L.", "citeRegEx": "Pang and Lee 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "and Hays", "author": ["G. Patterson"], "venue": "J.", "citeRegEx": "Patterson and Hays 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Analyzing and predicting sentiment of images on the social web", "author": ["Siersdorfer"], "venue": "In ACM MM,", "citeRegEx": "Siersdorfer,? \\Q2010\\E", "shortCiteRegEx": "Siersdorfer", "year": 2010}, {"title": "I", "author": ["A. Tumasjan", "T.O. Sprenger", "P.G. Sandner", "Welpe"], "venue": "M.", "citeRegEx": "Tumasjan et al. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Sentribute: image sentiment analysis from a mid-level perspective", "author": ["Yuan"], "venue": "In Proceedings of the Second International Workshop on Issues of Sentiment Discovery and Opinion Mining,", "citeRegEx": "Yuan,? \\Q2013\\E", "shortCiteRegEx": "Yuan", "year": 2013}, {"title": "P", "author": ["X. Zhang", "H. Fuehres", "Gloor"], "venue": "A.", "citeRegEx": "Zhang. Fuehres. and Gloor 2011", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [], "year": 2015, "abstractText": "Sentiment analysis of online user generated content is important for many social media analytics tasks. Researchers have largely relied on textual sentiment analysis to develop systems to predict political elections, measure economic indicators, and so on. Recently, social media users are increasingly using images and videos to express their opinions and share their experiences. Sentiment analysis of such large scale visual content can help better extract user sentiments toward events or topics, such as those in image tweets, so that prediction of sentiment from visual content is complementary to textual sentiment analysis. Motivated by the needs in leveraging large scale yet noisy training data to solve the extremely challenging problem of image sentiment analysis, we employ Convolutional Neural Networks (CNN). We first design a suitable CNN architecture for image sentiment analysis. We obtain half a million training samples by using a baseline sentiment algorithm to label Flickr images. To make use of such noisy machine labeled data, we employ a progressive strategy to fine-tune the deep network. Furthermore, we improve the performance on Twitter images by inducing domain transfer with a small number of manually labeled Twitter images. We have conducted extensive experiments on manually labeled Twitter images. The results show that the proposed CNN can achieve better performance in image sentiment analysis than competing algorithms.", "creator": "LaTeX with hyperref package"}}}