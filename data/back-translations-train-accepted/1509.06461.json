{"id": "1509.06461", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Sep-2015", "title": "Deep Reinforcement Learning with Double Q-Learning", "abstract": "The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether this harms performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.", "histories": [["v1", "Tue, 22 Sep 2015 04:40:22 GMT  (945kb,D)", "http://arxiv.org/abs/1509.06461v1", null], ["v2", "Fri, 20 Nov 2015 15:20:50 GMT  (1043kb,D)", "http://arxiv.org/abs/1509.06461v2", "AAAI 2016"], ["v3", "Tue, 8 Dec 2015 21:19:16 GMT  (1043kb,D)", "http://arxiv.org/abs/1509.06461v3", "AAAI 2016"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hado van hasselt", "arthur guez", "david silver"], "accepted": true, "id": "1509.06461"}, "pdf": {"name": "1509.06461.pdf", "metadata": {"source": "CRF", "title": "DEEP REINFORCEMENT LEARNING WITH DOUBLE Q-LEARNING", "authors": ["HADO VAN HASSELT"], "emails": [], "sections": [{"heading": null, "text": "The goal of amplification learning (Sutton and Barto, 1998) is to learn good strategies for sequential decision problems by optimizing a cumulative future reward signal. Q-Learning (Watkins, 1989) is one of the most popular amplification learning algorithms, but it is known that it sometimes learns unrealistically high action values because it includes a maximization step over estimated action values that tends to underestimate overestimated values. Previous work attributed overestimates to insufficiently flexible function approximations (Thrun and Schwartz, 1993) and noise (van Hasselt, 2011). In this paper we combine these views and show overestimates when the action values are inaccurate, regardless of the source of the approximation error."}, {"heading": "BACKGROUND", "text": "To solve sequential decision problems, we can learn estimates for the optimal value of each action, defined as the expected sum of future rewards Q = Q = Q, if we take this action and follow the optimal policy thereafter. Within the framework of a given policy, we can derive the true value of an action a in a state s isQ\u03c0 (s, a) \u2261 Q [R1 + \u03b3R2 +. The optimal value is then Q (s, a) = max\u03c0 Q (s, a). An optimal policy can easily be derived from the optimal values by selecting the highest-rated measures in each state. Estimates for the optimal action values can be learned using Q-Learning (Watkins, 1989), a form of time difference learning (Sutton, 1988). Most interesting problems are too large to learn all action values separately."}, {"heading": "OVEROPTIMISM DUE TO ESTIMATION ERRORS", "text": "The overestimates of Q-Learning were first investigated by Thrun and Schwartz (1993), who showed that if the action values contain random errors that are evenly distributed at an interval (Q = Q = 64), then each target is overestimated, up to a certain value of 1 million + 1, with m estimating the number of actions randomly. In addition, Thrun and Schwartz give a concrete example in which these overestimates even lead asymptotically to suboptimal measures, and show the overestimates that manifest themselves in a small toy problem when functional approximation is used. Later, van Hasselt (2010) argued that noise in the environment can lead to overestimates, even when tabular representation and proposed double Q-Learning measures are used as a solution. In this section, we generally show that estimation errors of any kind can cause an upward tilt, regardless of whether these errors are due to ambient noise, functional approximation, or other sources that are important in practice."}, {"heading": "DOUBLE DQN", "text": "In order to test these hypotheses, in the next section we will analyze the performance of the DQN algorithm and compare it with the Double DQN algorithm that we are now constructing. The idea of Double Q Learning is to split the maximum operation in the target into selecting and evaluating the measures. Although not completely decoupled, the target network in the DQN architecture provides a natural candidate for the secondary value function without having to introduce additional networks. Therefore, we propose to evaluate the greedy policies according to the online network, but to use the target network for appreciation. In terms of Double Q Learning and DQN, we refer to the resulting learning algorithm as Double DQN. The update used by Double DQN is the same as for the DQN policy, but replacing the Y DQNt target without comparing QNt."}, {"heading": "EMPIRICAL RESULTS", "text": "In this section we analyze the overestimates of DQN and show that the dual DQN architecture is experimentally improved via DQN, both in terms of value accuracy and political quality. In order to further test the robustness of the approach, we additionally evaluate the algorithms using random approaches generated by experts, as suggested by Nair et al. (2015) Our test bed contains 49 Atari 2600 games that use only the on-screen pixels as input. This is a sophisticated testbed: not only are the inputs high-dimensional, the game visualizations and game mechanics vary substantially between games. Good solutions must rely heavily on learning algorithms - it is practically impracticable by relying only on the domain."}, {"heading": "DISCUSSION", "text": "This paper consists of five papers. First, we have shown why Q-Learning can be over-optimistic on major problems, even if they are deterministic, due to the inherent errors of assessment of learning. Second, by analyzing the estimates of Atari games, we have shown that these overestimates are more frequent and more serious in practice than previously recognized. Third, we have shown that Double Q-Learning can be used successfully on a scale to reduce this over-optimism, resulting in more stable and reliable learning. Fourth, we have proposed a specific implementation called Double DQN, which uses the existing architecture and deep neural network of the DQN algorithm without requiring additional networks or parameters. Finally, we have shown that Double DQN finds better strategies to achieve new state-of-the-art results on the Atari 2600 domain."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Tom Schaul, Volodymyr Mnih, Marc Bellemare, Thomas Degris and Richard Sutton for their helpful comments on earlier versions of this document and the ideas it presented, and everyone at Google DeepMind for technical and non-technical support and for their contribution to creating a constructive research environment."}, {"heading": "APPENDIX", "text": "Theorem 1. Consider a state s in which all applicable optimal action values are equal for Q * (s, a) = {Q =} (s) for some V * (s). Consider a state s in which all applicable optimal action values are equal for Q * (s, a) = {V * (s) for some V * (s, but these are not all zero, so that 1 m \u00b2 a (Qt, a) \u2212 V * (s, a) \u2212 V * (s) 2 = C for some C > 0, where m \u00b2 2 is the number of actions in s. Under these conditions, maxaQt (s, a) \u2265 V * (s) Cm \u2212 1. This lower limit is narrow. Under the same conditions, the lower limit of the absolute error of the double Q learning estimate is zero."}, {"heading": "EXPERIMENTAL DETAILS FOR THE ATARI 2600 DOMAIN", "text": "We selected the 49 games to match the list used by Mnih et al. (2015), see tables below for the full list. Each step of the agent consists of four frames (the last selected action is repeated during these frames) and the reward values (obtained from the Arcade Learning Environment (Bellemare et al., 2013) are truncated between -1 and 1.Network Architecture.The folding network used in the experiment is exactly what Mnih et al. (2015) suggested, we only give details of completeness here. Input to the network consists of an 84x84x4 tensor containing a newly scaled and gray scaled version of the last four frames. The first folding layer folds the input with 32 size 8 filters (Step 4), the second layer has 64 size 4 layers (Step 2), the last folding layer has 64 size 3 filters (Step 1)."}, {"heading": "SUPPLEMENTARY RESULTS IN THE ATARI 2600 DOMAIN", "text": "Figure 4 shows the normalized values for DQN and Double DQN for the evaluation used by Mnih et al. (2015) and as indicated in Table 3 below. The following tables provide further detailed results for our experiments in the Atari area."}], "references": [{"title": "Sample mean based index policies with O(log n) regret for the multi-armed bandit problem", "author": ["R. Agrawal"], "venue": "Advances in Applied Probability,", "citeRegEx": "Agrawal.,? \\Q1995\\E", "shortCiteRegEx": "Agrawal.", "year": 1995}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["L. Baird"], "venue": "In Machine Learning: Proceedings of the Twelfth International Conference,", "citeRegEx": "Baird.,? \\Q1995\\E", "shortCiteRegEx": "Baird.", "year": 1995}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "Bellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "R-max-a general polynomial time algorithm for near-optimal reinforcement learning", "author": ["R.I. Brafman", "M. Tennenholtz"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Brafman and Tennenholtz.,? \\Q2003\\E", "shortCiteRegEx": "Brafman and Tennenholtz.", "year": 2003}, {"title": "Neocognitron: A hierarchical neural network capable of visual pattern recognition", "author": ["K. Fukushima"], "venue": "Neural networks,", "citeRegEx": "Fukushima.,? \\Q1988\\E", "shortCiteRegEx": "Fukushima.", "year": 1988}, {"title": "Reinforcement learning: A survey", "author": ["L.P. Kaelbling", "M.L. Littman", "A.W. Moore"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Kaelbling et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1996}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Self-improving reactive agents based on reinforcement learning, planning and teaching", "author": ["L. Lin"], "venue": "Machine learning,", "citeRegEx": "Lin.,? \\Q1992\\E", "shortCiteRegEx": "Lin.", "year": 1992}, {"title": "Gradient temporal-difference learning algorithms", "author": ["H.R. Maei"], "venue": "PhD thesis, University of Alberta,", "citeRegEx": "Maei.,? \\Q2011\\E", "shortCiteRegEx": "Maei.", "year": 2011}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Neural fitted Q iteration - first experiences with a data efficient neural reinforcement learning method", "author": ["M. Riedmiller"], "venue": "Proceedings of the 16th European Conference on Machine Learning", "citeRegEx": "Riedmiller.,? \\Q2005\\E", "shortCiteRegEx": "Riedmiller.", "year": 2005}, {"title": "Reinforcement learning with factored states and actions", "author": ["B. Sallans", "G.E. Hinton"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Sallans and Hinton.,? \\Q2004\\E", "shortCiteRegEx": "Sallans and Hinton.", "year": 2004}, {"title": "Reinforcement learning in finite MDPs: PAC analysis", "author": ["A.L. Strehl", "L. Li", "M.L. Littman"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Strehl et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2009}, {"title": "Learning to predict by the methods of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine learning,", "citeRegEx": "Sutton.,? \\Q1988\\E", "shortCiteRegEx": "Sutton.", "year": 1988}, {"title": "Integrated architectures for learning, planning, and reacting based on approximating dynamic programming", "author": ["R.S. Sutton"], "venue": "In Proceedings of the seventh international conference on machine learning,", "citeRegEx": "Sutton.,? \\Q1990\\E", "shortCiteRegEx": "Sutton.", "year": 1990}, {"title": "Introduction to reinforcement learning", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "A convergent O(n) algorithm for off-policy temporal-difference learning with linear function approximation", "author": ["R.S. Sutton", "C. Szepesv\u00e1ri", "H.R. Maei"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Sutton et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2008}, {"title": "An emphatic approach to the problem of off-policy temporal-difference learning", "author": ["R.S. Sutton", "A.R. Mahmood", "M. White"], "venue": "arXiv preprint arXiv:1503.04269,", "citeRegEx": "Sutton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2015}, {"title": "The many faces of optimism: a unifying approach", "author": ["I. Szita", "A. L\u0151rincz"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Szita and L\u0151rincz.,? \\Q2008\\E", "shortCiteRegEx": "Szita and L\u0151rincz.", "year": 2008}, {"title": "Temporal difference learning and td-gammon", "author": ["G. Tesauro"], "venue": "Communications of the ACM,", "citeRegEx": "Tesauro.,? \\Q1995\\E", "shortCiteRegEx": "Tesauro.", "year": 1995}, {"title": "Issues in using function approximation for reinforcement learning", "author": ["S. Thrun", "A. Schwartz"], "venue": "Proceedings of the 1993 Connectionist Models Summer School,", "citeRegEx": "Thrun and Schwartz.,? \\Q1993\\E", "shortCiteRegEx": "Thrun and Schwartz.", "year": 1993}, {"title": "An analysis of temporal-difference learning with function approximation", "author": ["J.N. Tsitsiklis", "B. Van Roy"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Tsitsiklis and Roy.,? \\Q1997\\E", "shortCiteRegEx": "Tsitsiklis and Roy.", "year": 1997}, {"title": "Double Q-learning", "author": ["H. van Hasselt"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Hasselt.,? \\Q2010\\E", "shortCiteRegEx": "Hasselt.", "year": 2010}, {"title": "Insights in Reinforcement Learning", "author": ["H. van Hasselt"], "venue": "PhD thesis, Utrecht University,", "citeRegEx": "Hasselt.,? \\Q2011\\E", "shortCiteRegEx": "Hasselt.", "year": 2011}, {"title": "Learning from delayed rewards", "author": ["C.J.C.H. Watkins"], "venue": "PhD thesis,", "citeRegEx": "Watkins.,? \\Q1989\\E", "shortCiteRegEx": "Watkins.", "year": 1989}], "referenceMentions": [{"referenceID": 16, "context": "The goal of reinforcement learning (Sutton and Barto, 1998) is to learn good policies for sequential decision problems, by optimizing a cumulative future reward signal.", "startOffset": 35, "endOffset": 59}, {"referenceID": 25, "context": "Q-learning (Watkins, 1989) is one of the most popular reinforcement learning algorithms, but it is known to sometimes learn unrealistically high action values because it includes a maximization step over estimated action values, which tends to prefer overestimated to underestimated values.", "startOffset": 11, "endOffset": 26}, {"referenceID": 21, "context": "In previous work, overestimations have been attributed to insufficiently flexible function approximation (Thrun and Schwartz, 1993) and noise (van Hasselt, 2010, 2011).", "startOffset": 105, "endOffset": 131}, {"referenceID": 6, "context": "Furthermore, it is known that sometimes it is good to be optimistic: optimism in the face of uncertainty is a well-known exploration technique (Kaelbling et al., 1996).", "startOffset": 143, "endOffset": 167}, {"referenceID": 10, "context": "To test whether overestimations occur in practice and at scale, we investigate the performance of the recent DQN algorithm (Mnih et al., 2015).", "startOffset": 123, "endOffset": 142}, {"referenceID": 6, "context": "Furthermore, it is known that sometimes it is good to be optimistic: optimism in the face of uncertainty is a well-known exploration technique (Kaelbling et al., 1996). If, however, the overestimations are not uniform and not concentrated at states about which we wish to learn more, then they might negatively affect the quality of the resulting policy. Thrun and Schwartz (1993) give specific examples in which this leads to suboptimal policies, even asymptotically.", "startOffset": 144, "endOffset": 381}, {"referenceID": 25, "context": "Estimates for the optimal action values can be learned using Q-learning (Watkins, 1989), a form of temporal difference learning (Sutton, 1988).", "startOffset": 72, "endOffset": 87}, {"referenceID": 14, "context": "Estimates for the optimal action values can be learned using Q-learning (Watkins, 1989), a form of temporal difference learning (Sutton, 1988).", "startOffset": 128, "endOffset": 142}, {"referenceID": 10, "context": "Two important ingredients of the DQN algorithm as proposed by Mnih et al. (2015) are the use of a target network, and the use of experience replay.", "startOffset": 62, "endOffset": 81}, {"referenceID": 8, "context": "For the experience replay (Lin, 1992), observed transitions are stored for some time and sampled uniformly from this memory bank to update the network.", "startOffset": 26, "endOffset": 37}, {"referenceID": 10, "context": "Both the target network and the experience replay dramatically improve the performance of the algorithm (Mnih et al., 2015).", "startOffset": 104, "endOffset": 123}, {"referenceID": 21, "context": "Q-learning\u2019s overestimations were first investigated by Thrun and Schwartz (1993), who showed that if the action values contain random errors uniformly distributed in an interval [\u2212 , ] then each target is overestimated up to \u03b3 m\u22121 m+1 , where m is the number of actions.", "startOffset": 56, "endOffset": 82}, {"referenceID": 21, "context": "Q-learning\u2019s overestimations were first investigated by Thrun and Schwartz (1993), who showed that if the action values contain random errors uniformly distributed in an interval [\u2212 , ] then each target is overestimated up to \u03b3 m\u22121 m+1 , where m is the number of actions. In addition, Thrun and Schwartz give a concrete example in which these overestimations even asymptotically lead to sub-optimal policies, and show the overestimations manifest themselves in a small toy problem when using function approximation. Later van Hasselt (2010) argued that noise in the environment can lead to overestimations even when using tabular representation, and proposed Double Q-learning as a solution.", "startOffset": 56, "endOffset": 541}, {"referenceID": 21, "context": "Q-learning\u2019s overestimations were first investigated by Thrun and Schwartz (1993), who showed that if the action values contain random errors uniformly distributed in an interval [\u2212 , ] then each target is overestimated up to \u03b3 m\u22121 m+1 , where m is the number of actions. In addition, Thrun and Schwartz give a concrete example in which these overestimations even asymptotically lead to sub-optimal policies, and show the overestimations manifest themselves in a small toy problem when using function approximation. Later van Hasselt (2010) argued that noise in the environment can lead to overestimations even when using tabular representation, and proposed Double Q-learning as a solution. In this section we demonstrate more generally that estimation errors of any kind can induce an upward bias, regardless of whether these errors are due to environmental noise, function approximation, non-stationarity, or any other source. This is important, because in practice any method will incur some inaccuracies during learning, simply due to the fact that the true values are initially unknown. The result by Thrun and Schwartz (1993) cited above gives an upper bound to the overestimation for a specific setup, but it is also possible, and potentially more interesting, to derive a lower bound.", "startOffset": 56, "endOffset": 1133}, {"referenceID": 10, "context": "This is important because flexible parametric function approximators are often employed in reinforcement learning (see, e.g., Tesauro 1995; Sallans and Hinton 2004; Riedmiller 2005; Mnih et al. 2015).", "startOffset": 114, "endOffset": 199}, {"referenceID": 15, "context": "The overestimations should not be confused with optimism in the face of uncertainty (Sutton, 1990; Agrawal, 1995; Kaelbling et al., 1996; Auer et al., 2002; Brafman and Tennenholtz, 2003; Szita and L\u0151rincz, 2008; Strehl et al., 2009), where an exploration bonus is given to states or actions with uncertain values.", "startOffset": 84, "endOffset": 233}, {"referenceID": 0, "context": "The overestimations should not be confused with optimism in the face of uncertainty (Sutton, 1990; Agrawal, 1995; Kaelbling et al., 1996; Auer et al., 2002; Brafman and Tennenholtz, 2003; Szita and L\u0151rincz, 2008; Strehl et al., 2009), where an exploration bonus is given to states or actions with uncertain values.", "startOffset": 84, "endOffset": 233}, {"referenceID": 6, "context": "The overestimations should not be confused with optimism in the face of uncertainty (Sutton, 1990; Agrawal, 1995; Kaelbling et al., 1996; Auer et al., 2002; Brafman and Tennenholtz, 2003; Szita and L\u0151rincz, 2008; Strehl et al., 2009), where an exploration bonus is given to states or actions with uncertain values.", "startOffset": 84, "endOffset": 233}, {"referenceID": 1, "context": "The overestimations should not be confused with optimism in the face of uncertainty (Sutton, 1990; Agrawal, 1995; Kaelbling et al., 1996; Auer et al., 2002; Brafman and Tennenholtz, 2003; Szita and L\u0151rincz, 2008; Strehl et al., 2009), where an exploration bonus is given to states or actions with uncertain values.", "startOffset": 84, "endOffset": 233}, {"referenceID": 4, "context": "The overestimations should not be confused with optimism in the face of uncertainty (Sutton, 1990; Agrawal, 1995; Kaelbling et al., 1996; Auer et al., 2002; Brafman and Tennenholtz, 2003; Szita and L\u0151rincz, 2008; Strehl et al., 2009), where an exploration bonus is given to states or actions with uncertain values.", "startOffset": 84, "endOffset": 233}, {"referenceID": 19, "context": "The overestimations should not be confused with optimism in the face of uncertainty (Sutton, 1990; Agrawal, 1995; Kaelbling et al., 1996; Auer et al., 2002; Brafman and Tennenholtz, 2003; Szita and L\u0151rincz, 2008; Strehl et al., 2009), where an exploration bonus is given to states or actions with uncertain values.", "startOffset": 84, "endOffset": 233}, {"referenceID": 13, "context": "The overestimations should not be confused with optimism in the face of uncertainty (Sutton, 1990; Agrawal, 1995; Kaelbling et al., 1996; Auer et al., 2002; Brafman and Tennenholtz, 2003; Szita and L\u0151rincz, 2008; Strehl et al., 2009), where an exploration bonus is given to states or actions with uncertain values.", "startOffset": 84, "endOffset": 233}, {"referenceID": 6, "context": ", Tesauro 1995; Sallans and Hinton 2004; Riedmiller 2005; Mnih et al. 2015). In contrast to van Hasselt (2010) we did not use a statistical argument to find overestimations, the process to obtain Figure 1 is fully deterministic.", "startOffset": 58, "endOffset": 111}, {"referenceID": 6, "context": ", Tesauro 1995; Sallans and Hinton 2004; Riedmiller 2005; Mnih et al. 2015). In contrast to van Hasselt (2010) we did not use a statistical argument to find overestimations, the process to obtain Figure 1 is fully deterministic. In contrast to Thrun and Schwartz (1993), we did not rely on inflexible function approximation with irreducible asymptotic errors; the bottom row shows that a function that is flexible enough to cover all samples leads to high overestimations.", "startOffset": 58, "endOffset": 270}, {"referenceID": 21, "context": "This was already observed by Thrun and Schwartz (1993), who noted that, in contrast to optimism in the face of uncertainty, these overestimations actually can impede learning an optimal policy.", "startOffset": 29, "endOffset": 55}, {"referenceID": 3, "context": "Our testbed contains 49 Atari 2600 games, using the Arcade Learning Environment (Bellemare et al., 2013).", "startOffset": 80, "endOffset": 104}, {"referenceID": 5, "context": "Briefly, the network architecture is a convolutional neural network (Fukushima, 1988; LeCun et al., 1998) with 3 convolution layers and a fully-connected hidden layer (approximately 1.", "startOffset": 68, "endOffset": 105}, {"referenceID": 7, "context": "Briefly, the network architecture is a convolutional neural network (Fukushima, 1988; LeCun et al., 1998) with 3 convolution layers and a fully-connected hidden layer (approximately 1.", "startOffset": 68, "endOffset": 105}, {"referenceID": 3, "context": "Our testbed contains 49 Atari 2600 games, using the Arcade Learning Environment (Bellemare et al., 2013). The goal is for a single algorithm, with a fixed set of hyperparameters, to learn to play each of the games separately from interaction given only the screen pixels as input. This is a demanding testbed: not only are the inputs high-dimensional, the game visuals and game mechanics vary substantially between games. Good solutions must therefore rely heavily on the learning algorithm \u2014 it is not practically feasible to overfit the domain by relying only on tuning. We closely follow the experimental setting and network architecture outlined by Mnih et al. (2015). Briefly, the network architecture is a convolutional neural network (Fukushima, 1988; LeCun et al.", "startOffset": 81, "endOffset": 672}, {"referenceID": 10, "context": "DQN and Double DQN were both trained under the exact conditions described by Mnih et al. (2015). DQN is consistently and sometimes vastly overoptimistic about the value of the current greedy policy, as can be seen by comparing the orange learning curves in the top row of plots to the straight orange lines, which represent the actual discounted value of the best learned policy.", "startOffset": 77, "endOffset": 96}, {"referenceID": 2, "context": "If seen in isolation, one might perhaps have be tempted to think the observed instability is related to inherent instability problems of off-policy learning with function approximation (Baird, 1995; Tsitsiklis and Van Roy, 1997; Sutton et al., 2008; Maei, 2011; Sutton et al., 2015).", "startOffset": 185, "endOffset": 282}, {"referenceID": 17, "context": "If seen in isolation, one might perhaps have be tempted to think the observed instability is related to inherent instability problems of off-policy learning with function approximation (Baird, 1995; Tsitsiklis and Van Roy, 1997; Sutton et al., 2008; Maei, 2011; Sutton et al., 2015).", "startOffset": 185, "endOffset": 282}, {"referenceID": 9, "context": "If seen in isolation, one might perhaps have be tempted to think the observed instability is related to inherent instability problems of off-policy learning with function approximation (Baird, 1995; Tsitsiklis and Van Roy, 1997; Sutton et al., 2008; Maei, 2011; Sutton et al., 2015).", "startOffset": 185, "endOffset": 282}, {"referenceID": 18, "context": "If seen in isolation, one might perhaps have be tempted to think the observed instability is related to inherent instability problems of off-policy learning with function approximation (Baird, 1995; Tsitsiklis and Van Roy, 1997; Sutton et al., 2008; Maei, 2011; Sutton et al., 2015).", "startOffset": 185, "endOffset": 282}, {"referenceID": 10, "context": "As described by Mnih et al. (2015) we start each evaluation episode by executing a special action that does not affect the environment, a so-called no-op action, up to 30 times to provide", "startOffset": 16, "endOffset": 35}, {"referenceID": 10, "context": "The results are obtained by running DQN and Double DQN with 6 different random seeds with the hyper-parameters employed by Mnih et al. (2015). The darker line shows the median over seeds and we average the two extreme values to obtain the shaded area (i.", "startOffset": 123, "endOffset": 142}, {"referenceID": 10, "context": "The \u2018random\u2019 and \u2018human\u2019 scores are the same as used by Mnih et al. (2015), and are given in the appendix.", "startOffset": 56, "endOffset": 75}], "year": 2015, "abstractText": "The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether this harms performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance", "creator": "TeX"}}}