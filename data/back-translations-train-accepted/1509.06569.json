{"id": "1509.06569", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Sep-2015", "title": "Tensorizing Neural Networks", "abstract": "Deep neural networks currently demonstrate state-of-the-art performance in several domains. At the same time, models of this class are very demanding in terms of computational resources. In particular, a large amount of memory is required by commonly used fully-connected layers, making it hard to use the models on low-end devices and stopping the further increase of the model size. In this paper we convert the dense weight matrices of the fully-connected layers to the Tensor Train format such that the number of parameters is reduced by a huge factor and at the same time the expressive power of the layer is preserved. In particular, for the Very Deep VGG networks we report the compression factor of the dense weight matrix of a fully-connected layer up to 200000 times leading to the compression factor of the whole network up to 7 times.", "histories": [["v1", "Tue, 22 Sep 2015 12:31:03 GMT  (21kb)", "http://arxiv.org/abs/1509.06569v1", null], ["v2", "Sun, 20 Dec 2015 11:44:05 GMT  (21kb)", "http://arxiv.org/abs/1509.06569v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["alexander novikov", "dmitry podoprikhin", "anton osokin", "dmitry p vetrov"], "accepted": true, "id": "1509.06569"}, "pdf": {"name": "1509.06569.pdf", "metadata": {"source": "CRF", "title": "Tensorizing Neural Networks", "authors": ["Alexander Novikov", "Dmitry Podoprikhin", "Anton Osokin", "Dmitry Vetrov"], "emails": ["novikov@bayesgroup.ru", "podoprikhin.dmitry@gmail.com", "anton.osokin@inria.fr", "vetrovd@yandex.ru"], "sections": [{"heading": null, "text": "ar Xiv: 150 9.06 569v 1 [cs.L G] 22 SE"}, {"heading": "1 Introduction", "text": "Deep neuron networks are currently evident in many areas of large-scale machine learning, such as computer and speech recognition, word processing, etc. These advances have been made possible by algorithmic advances, large amounts of available data, and the use of millions of images. [17] The need for expensive hardware and long processing times are factors that complicate the application of such models to conventional desktops and portable devices."}, {"heading": "2 Related work", "text": "In fact, the majority of people who are able to move, to move, to move, to move, to move, to move, to move, to move and to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move"}, {"heading": "3 TT-format", "text": "We refer to the onedimensional arrays as vectors, the two-dimensional arrays - matrices, the arrays of higher dimensions - tensors. Bold lowercase letters (e.g. a) - matrix elements, ordinary lowercase letters (e.g. a) = ai) - vector elements, calligraphic uppercase letters (e.g. A) - for tensors and ordinary calligraphic uppercase letters (e.g. A) - matrix elements, calligraphic uppercase letters (e.g. A) - for tensors and ordinary calligraphic uppercase letters (e.g. A)."}, {"heading": "3.1 TT-representations for vectors and matrices", "text": "The direct application of the TT definition to a matrix (2-dimensional tensor) corresponds to the low-level matrix format and the direct TT decomposition of a vector. In order to work efficiently with large vectors and matrices, the TT format is defined for them in a special way. Consider a vector b) in which N = 1 nk. Wecan establish a bijection between the coordinates 1,., N} of b and a d-dimensional vector index \u00b5 (1) = (\u00b5d) of the corresponding tensor B, in which \u00b5k (1) the coordination of the individual elements {1,.,., nk}. The tensor B is then defined by the corresponding vector elements."}, {"heading": "4 TT-layer", "text": "In this section we present the TT layer of a neural network. In short, the TT layer is a completely connected layer with the weight matrix stored in TT format. We refer to a neural network with one or more TT layers as TensorNet. Fully connected layers apply a linear transformation to a N-dimensional input vector x: y = Wx + b, (4) with the W-RM \u00b7 N weight matrix and the b-RM bias vector defining the transformation. A TT layer consists in storing the weights W of the fully connected layer in TT format, allowing hundreds of thousands (or even millions) of hidden units to be used with a moderate number of parameters. To control the number of parameters, one can vary the number of hidden units as well as the TT order of the weight matrix."}, {"heading": "5 Learning", "text": "The method begins with the calculation of the gradient of L w.r.t. The output of the last layer is carried out sequentially through the layers in the reverse order, while the calculation of the gradient w.r.t. uses the parameters and input of the layer, the use of the gradients calculated earlier. Applied to the completely connected layers (4), the method of backpropagation can calculate the gradients in the reverse order. Input x and parameters W and input of the layer that uses the gradients."}, {"heading": "6 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Parameters of the TT-layer", "text": "In this experiment, we will examine the properties of the TT layer and compare various strategies for determining its parameters: dimensions of the tensors representing the input / output of the layer and the TT ranks of the compressed weight matrix. We will run the experiment on the MNIST dataset [13] for the task of handwritten recognition, using as a starting point a neural network with two fully connected layers (1024 hidden units) and a rectified linear unit (ReLU), which has 1.9% error on the test set. For further transformation options, we will reduce the original 28 x 28 images to 32 x 32. We will train several networks that differ in the parameters of each TT layer. The networks contain the following layers: the TT layer with the 1024 \u00d7 1024 weight matrix, the ReLU, the fully connected layer connected to the layer."}, {"heading": "6.2 CIFAR-10", "text": "The CIFAR-10 dataset [10] consists of 32 x 32 3-channel images assigned to 10 different classes: plane, car, bird, cat, deer, dog, frog, horse, ship, truck. The dataset contains 50,000 train images and 10,000 test images. Following [8], we edit the images by subtracting the mean and performing global contrast normalization and ZCA white drawing. As a starting point, we use the CIFAR-10 Quick [20] CNN, which consists of folding, pooling and nonlinearity layers, followed by two fully connected layers of sizes 1024 x 64 and 64 x 10. We fix the voluminous part of the network and replace the fully connected part with a 1024 x N TT layer, followed by ReLU and a N x 10 fully connected layer. With N = 3125 hidden units (unlike 64 in the original network) we achieve the test error of 23.13%, without fine tuning."}, {"heading": "6.2.1 Wide and shallow network", "text": "With a sufficient number of hidden units, even a neural network with two fully connected layers and sigmoid nonlinearity can approach any decision boundary [4]. Traditionally, very wide flat networks are not considered due to high computing and memory requirements and overpass risk. TensorNet can potentially solve both problems. We use a three-layer TensorNet of the following architecture: the TT layer with the weight matrix of size 3 072 x 262 144, ReLU, the TT layer with the weight matrix of size 262 144 x 4 096, ReLU, the fully connected layer with the weight matrix of size 4 096 x 10. We report the test error of 31.47%, which (to the best of our knowledge) is the best result that a non-revolutionary neural network achieves."}, {"heading": "6.3 ImageNet", "text": "In this experiment, we evaluate the TT layers on a large scale task. We look deeply at the 1000 class ImageNet ILSVRC 2012 dataset [17], which consists of 1.2 million training images and 50 000 validation images. We use CNNs vgg-16 and vgg-19 [19] as reference models.Both networks consist of the two parts: the revolutionary and the fully connected parts. In the two networks, the second part consists of 3 fully connected layers with sizes 25088 \u00d7 4096, 4096 and 4096 \u00d7 1000.In each network, we replace the first fully connected layer with the TT layer. To do this, we shape the 25088 dimensional input vectors into the 25088 \u00d7 4096 \u00d7 4096 \u00d7 4096 tensors, the 4,000 layer."}, {"heading": "6.4 Implementation details", "text": "In all experiments we use our MATLAB extension of the MatConvNet Framework3 [21]. For operations related to the TT format we also use the TT-Toolbox4 implemented in MATLAB. The experiments were carried out on a computer with a quad-core Intel Core i5-4460 CPU, 16 GB RAM and a single NVidia Geforce GTX 980 GPU. We report runtime and memory usage during the run-up of the TT layer and the baseline full link layer in Table 3. We train all networks with stochastic gradient descent with momentum (coefficient 0.9). We initialize all parameters of the TT and full link layers with a Gaussian sound and set the L2 regulation (weight 0.0005) on it. The source code of our method is publicly available 5."}, {"heading": "7 Discussion and future work", "text": "To exploit this redundancy, we propose to use the TT decomposition framework on the weight matrix of a fully bonded layer and use the cores of decomposition as parameters of the layer, which allows us to compress the fully bonded layers by up to 200,000 compared to the explicit parameterization without significant error increase. Our experiments show that it is possible to capture complex dependencies within the data using much more compact representations. On the other hand, it will be possible to use much wider layers than were previously available, and the preliminary experiments with the CIFAR-10 dataset show that broad and flat TensorNets achieve promising results (setting up new state-of-the-art for non-revolutionary neural networks). Another attractive feature of the TT layer is the faster inference time (compared to the corresponding fully bonded layer)."}], "references": [{"title": "Experimental determination of precision requirements for back-propagation training of artificial neural networks", "author": ["K. Asanovi", "N. Morgan"], "venue": "International Computer Science Institute, Tech. Rep., 1991.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1991}, {"title": "Do deep nets really need to be deep?", "author": ["J. Ba", "R. Caruana"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Compressing neural networks with the hashing trick", "author": ["W. Chen", "J.T. Wilson", "S. Tyree", "K.Q. Weinberger", "Y. Chen"], "venue": "International Conference on Machine Learning (ICML), 2015, pp. 2285\u20132294.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["G. Cybenko"], "venue": "Mathematics of control, signals and systems, pp. 303\u2013314, 1989.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1989}, {"title": "Predicting parameters in deep learning", "author": ["M. Denil", "B. Shakibi", "L. Dinh", "M. Ranzato", "N. de Freitas"], "venue": "Advances in Neural Information Processing Systems 26 (NIPS), 2013, pp. 2148\u20132156.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["E. Denton", "W. Zaremba", "J. Bruna", "Y. LeCun", "R. Fergus"], "venue": "Advances in Neural Information Processing Systems 27 (NIPS), 2014, pp. 1269\u20131277.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Y. Gong", "L. Liu", "M. Yang", "L. Bourdev"], "venue": "arXiv preprint, no. 1412.6115, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Maxout networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "International Conference on Machine Learning (ICML), 2013, pp. 1319\u20131327.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep learning with limited numerical precision", "author": ["S. Gupta", "A. Agrawal", "K. Gopalakrishnan", "P. Narayanan"], "venue": "International Conference on Machine Learning (ICML), 2015, pp. 1737\u20131746.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Master\u2019s thesis, Computer Science Department, University of Toronto, 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems 25 (NIPS), 2012, pp. 1097\u20131105.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Speeding-up convolutional neural networks using fine-tuned CP-decomposition", "author": ["V. Lebedev", "Y. Ganin", "M. Rakhuba", "I. Oseledets", "V. Lempitsky"], "venue": "International Conference on Learning Representations (ICLR), 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "The MNIST database of handwritten digits", "author": ["Y. LeCun", "C. Cortes", "C.J.C. Burges"], "venue": "1998.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1998}, {"title": "Putting MRFs on a Tensor Train", "author": ["A. Novikov", "A. Rodomanov", "A. Osokin", "D. Vetrov"], "venue": "International Conference on Machine Learning (ICML), 2014, pp. 811\u2013819.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Tensor-Train decomposition", "author": ["I.V. Oseledets"], "venue": "SIAM J. Scientific Computing, vol. 33, no. 5, pp. 2295\u2013 2317, 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature, vol. 323, no. 6088, pp. 533\u2013536, 1986.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1986}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "International Journal of Computer Vision (IJCV), 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Low-rank matrix factorization for deep neural network training with high-dimensional output targets", "author": ["T.N. Sainath", "B. Kingsbury", "V. Sindhwani", "E. Arisoy", "B. Ramabhadran"], "venue": "International Conference of Acoustics, Speech, and Signal Processing (ICASSP), 2013, pp. 6655\u20136659.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "International Conference on Learning Representations (ICLR), 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["J. Snoek", "H. Larochelle", "R.P. Adams"], "venue": "Advances in Neural Information Processing Systems 25 (NIPS), 2012, pp. 2951\u20132959.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "MatConvNet \u2013 convolutional neural networks for MATLAB", "author": ["A. Vedaldi", "K. Lenc"], "venue": "CoRR, vol. abs/1412.4564, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Restructuring of deep neural network acoustic models with singular value decomposition", "author": ["J. Xue", "J. Li", "Y. Gong"], "venue": "Interspeech, 2013, pp. 2365\u20132369.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep fried convnets", "author": ["Z. Yang", "M. Moczulski", "M. Denil", "N. de Freitas", "A. Smola", "L. Song", "Z. Wang"], "venue": "arXiv preprint, no. 1412.7149, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Enabling high-dimensional hierarchical uncertainty quantification by ANOVA and tensor-train decomposition", "author": ["Z. Zhang", "X. Yang", "I.V. Oseledets", "G.E. Karniadakis", "L. Daniel"], "venue": "Computer-Aided Design of Integrated Circuits and Systems, IEEE Transactions on, pp. 63\u201376, 2014. 9", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 14, "context": "In this paper we convert the dense weight matrices of the fully-connected layers to the Tensor Train [15] format such that the number of parameters is reduced by a huge factor and at the same time the expressive power of the layer is preserved.", "startOffset": 101, "endOffset": 105}, {"referenceID": 18, "context": "In particular, for the Very Deep VGG networks [19] we report the compression factor of the dense weight matrix of a fully-connected layer up to 200000 times leading to the compression factor of the whole network up to 7 times.", "startOffset": 46, "endOffset": 50}, {"referenceID": 10, "context": "For example, convolutional neural networks (CNNs) [11, 19] show by a large margin superior performance on the task of image classification.", "startOffset": 50, "endOffset": 58}, {"referenceID": 18, "context": "For example, convolutional neural networks (CNNs) [11, 19] show by a large margin superior performance on the task of image classification.", "startOffset": 50, "endOffset": 58}, {"referenceID": 16, "context": "These models have thousands of nodes and millions of learnable parameters and are trained using millions of images [17] on powerful Graphics Processing Units (GPUs).", "startOffset": 115, "endOffset": 119}, {"referenceID": 14, "context": "We represent the dense weight matrix of the fully-connected layers in a compact multi-linear format, Tensor Train (TT-format) [15].", "startOffset": 126, "endOffset": 130}, {"referenceID": 15, "context": "The resulting layer is compatible with the existing training algorithms for neural networks because all the derivatives required by the back-propagation algorithm [16] can be computed using the properties of the TTformat.", "startOffset": 163, "endOffset": 167}, {"referenceID": 12, "context": "We apply our method to popular network architectures proposed for several datasets of different scales: MNIST [13], CIFAR-10 [10], ImageNet [11].", "startOffset": 110, "endOffset": 114}, {"referenceID": 9, "context": "We apply our method to popular network architectures proposed for several datasets of different scales: MNIST [13], CIFAR-10 [10], ImageNet [11].", "startOffset": 125, "endOffset": 129}, {"referenceID": 10, "context": "We apply our method to popular network architectures proposed for several datasets of different scales: MNIST [13], CIFAR-10 [10], ImageNet [11].", "startOffset": 140, "endOffset": 144}, {"referenceID": 18, "context": "In particular, modern networks reached the memory limit with 89% [19] or even 100% [22] memory occupied by the weights of the fully-connected layers so it is not surprising that numerous attempts have been made to make the fully-connected layers more compact.", "startOffset": 65, "endOffset": 69}, {"referenceID": 21, "context": "In particular, modern networks reached the memory limit with 89% [19] or even 100% [22] memory occupied by the weights of the fully-connected layers so it is not surprising that numerous attempts have been made to make the fully-connected layers more compact.", "startOffset": 83, "endOffset": 87}, {"referenceID": 4, "context": "Recent studies show that the weight matrix of the fully-connected layer is highly redundant and by restricting its matrix rank it is possible to greatly reduce the number of parameters without significant drop in the predictive accuracy [5, 18, 22].", "startOffset": 237, "endOffset": 248}, {"referenceID": 17, "context": "Recent studies show that the weight matrix of the fully-connected layer is highly redundant and by restricting its matrix rank it is possible to greatly reduce the number of parameters without significant drop in the predictive accuracy [5, 18, 22].", "startOffset": 237, "endOffset": 248}, {"referenceID": 21, "context": "Recent studies show that the weight matrix of the fully-connected layer is highly redundant and by restricting its matrix rank it is possible to greatly reduce the number of parameters without significant drop in the predictive accuracy [5, 18, 22].", "startOffset": 237, "endOffset": 248}, {"referenceID": 2, "context": "An alternative approach to the problem of model compression is to tie random subsets of weights using special hashing techniques [3].", "startOffset": 129, "endOffset": 132}, {"referenceID": 0, "context": "Memory consumption can also be reduced by using lower numerical precision [1, 9] or allowing fewer possible carefully chosen parameter values [7].", "startOffset": 74, "endOffset": 80}, {"referenceID": 8, "context": "Memory consumption can also be reduced by using lower numerical precision [1, 9] or allowing fewer possible carefully chosen parameter values [7].", "startOffset": 74, "endOffset": 80}, {"referenceID": 6, "context": "Memory consumption can also be reduced by using lower numerical precision [1, 9] or allowing fewer possible carefully chosen parameter values [7].", "startOffset": 142, "endOffset": 145}, {"referenceID": 14, "context": "Instead of searching for low-rank approximation of the weight matrix we treat it as multi-dimensional tensor and apply the Tensor Train decomposition algorithm [15].", "startOffset": 160, "endOffset": 164}, {"referenceID": 13, "context": "[14, 24].", "startOffset": 0, "endOffset": 8}, {"referenceID": 23, "context": "[14, 24].", "startOffset": 0, "endOffset": 8}, {"referenceID": 1, "context": "A recent work [2] shows that it is possible to construct wide and shallow (i.", "startOffset": 14, "endOffset": 17}, {"referenceID": 5, "context": "Matrix and tensor decompositions were recently used to speed up the inference time of CNNs [6, 12].", "startOffset": 91, "endOffset": 98}, {"referenceID": 11, "context": "Matrix and tensor decompositions were recently used to speed up the inference time of CNNs [6, 12].", "startOffset": 91, "endOffset": 98}, {"referenceID": 11, "context": "[12] used the CP-decomposition to compress a 4-dimensional convolution kernel and then used the properties of the decomposition to speed up the inference time.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "A d-dimensional array (tensor) A is said to be represented in the TT-format [15] if for each dimension k = 1, .", "startOffset": 76, "endOffset": 80}, {"referenceID": 14, "context": "See [15] for a detailed description of all the supported operations.", "startOffset": 4, "endOffset": 8}, {"referenceID": 15, "context": "Neural networks are usually trained with the stochastic gradient descent algorithm where the gradient is computed using the back-propagation procedure [16].", "startOffset": 151, "endOffset": 155}, {"referenceID": 14, "context": "(with the TT-SVD algorithm [15]) and then add this gradient (multiplied by a step size) to the current estimate of the weight matrix: Wk+1 = Wk + \u03b3k \u2202L \u2202W .", "startOffset": 27, "endOffset": 31}, {"referenceID": 12, "context": "We run the experiment on the MNIST dataset [13] for the task of handwritten-digit recognition.", "startOffset": 43, "endOffset": 47}, {"referenceID": 2, "context": "Comparison with HashedNet [3].", "startOffset": 26, "endOffset": 29}, {"referenceID": 2, "context": "[3] report results on the same architecture.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "CIFAR-10 dataset [10] consists of 32 \u00d7 32 3-channel images assigned to 10 different classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck.", "startOffset": 17, "endOffset": 21}, {"referenceID": 7, "context": "Following [8] we preprocess the images by subtracting the mean and performing global contrast normalization and ZCA whitening.", "startOffset": 10, "endOffset": 13}, {"referenceID": 19, "context": "As a baseline we use the CIFAR-10 Quick [20] CNN, which consists of convolutional, pooling and non-linearity layers followed by two fully-connected layers of sizes 1024\u00d7 64 and 64\u00d7 10.", "startOffset": 40, "endOffset": 44}, {"referenceID": 4, "context": "For comparison, in [5] the fully-connected layers in a CIFAR-10 CNN were compressed by the factor of at most 4.", "startOffset": 19, "endOffset": 22}, {"referenceID": 3, "context": "With sufficient amount of hidden units, even a neural network with two fully-connected layers and sigmoid non-linearity can approximate any decision boundary [4].", "startOffset": 158, "endOffset": 161}, {"referenceID": 16, "context": "We consider the 1000-class ImageNet ILSVRC-2012 dataset [17], which consist of 1.", "startOffset": 56, "endOffset": 60}, {"referenceID": 18, "context": "We use deep the CNNs vgg-16 and vgg-19 [19] as the reference models2.", "startOffset": 39, "endOffset": 43}, {"referenceID": 18, "context": "The parameters of the convolutional parts are kept fixed as trained by Simonyan and Zisserman [19].", "startOffset": 94, "endOffset": 98}, {"referenceID": 1, "context": "As a baseline compression method we constrain the matrix rank of the weight matrix of the first fully-connected layer using the approach of [2].", "startOffset": 140, "endOffset": 143}, {"referenceID": 22, "context": "For comparison, consider the results of [23] obtained for the compression of the fully-connected layers of the Krizhevsky-type network [11] with the Fastfood method.", "startOffset": 40, "endOffset": 44}, {"referenceID": 10, "context": "For comparison, consider the results of [23] obtained for the compression of the fully-connected layers of the Krizhevsky-type network [11] with the Fastfood method.", "startOffset": 135, "endOffset": 139}, {"referenceID": 20, "context": "In all experiments we use our MATLAB extension of the MatConvNet framework3 [21].", "startOffset": 76, "endOffset": 80}], "year": 2015, "abstractText": "Deep neural networks currently demonstrate state-of-the-art performance in several domains. At the same time, models of this class are very demanding in terms of computational resources. In particular, a large amount of memory is required by commonly used fully-connected layers, making it hard to use the models on low-end devices and stopping the further increase of the model size. In this paper we convert the dense weight matrices of the fully-connected layers to the Tensor Train [15] format such that the number of parameters is reduced by a huge factor and at the same time the expressive power of the layer is preserved. In particular, for the Very Deep VGG networks [19] we report the compression factor of the dense weight matrix of a fully-connected layer up to 200000 times leading to the compression factor of the whole network up to 7 times.", "creator": "LaTeX with hyperref package"}}}