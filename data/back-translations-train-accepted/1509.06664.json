{"id": "1509.06664", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Sep-2015", "title": "Reasoning about Entailment with Neural Attention", "abstract": "Automatically recognizing entailment relations between pairs of natural language sentences has so far been the dominion of classifiers employing hand engineered features derived from natural language processing pipelines. End-to-end differentiable neural architectures have failed to approach state-of-the-art performance until very recently. In this paper, we propose a neural model that reads two sentences to determine entailment using long short-term memory units. We extend this model with a word-by-word neural attention mechanism that encourages reasoning over entailments of pairs of words and phrases. Furthermore, we present a qualitative analysis of attention weights produced by this model, demonstrating such reasoning capabilities. On a large entailment dataset this model outperforms the previous best neural model and a classifier with engineered features by a substantial margin. It is the first generic end-to-end differentiable system that achieves state-of-the-art accuracy on a textual entailment dataset.", "histories": [["v1", "Tue, 22 Sep 2015 16:08:24 GMT  (1067kb,D)", "http://arxiv.org/abs/1509.06664v1", "9 pages, 10 figures (incl. subfigures)"], ["v2", "Tue, 10 Nov 2015 22:12:52 GMT  (1182kb,D)", "http://arxiv.org/abs/1509.06664v2", "9 pages, 10 figures (incl. subfigures)"], ["v3", "Mon, 18 Jan 2016 17:28:30 GMT  (1075kb,D)", "http://arxiv.org/abs/1509.06664v3", "9 pages, 10 figures (incl. subfigures)"], ["v4", "Tue, 1 Mar 2016 10:32:06 GMT  (1075kb,D)", "http://arxiv.org/abs/1509.06664v4", "ICLR 2016 camera-ready, 9 pages, 10 figures (incl. subfigures)"]], "COMMENTS": "9 pages, 10 figures (incl. subfigures)", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG cs.NE", "authors": ["tim rockt\\\"aschel", "edward grefenstette", "karl moritz hermann", "tom\\'a\\v{s} ko\\v{c}isk\\'y", "phil blunsom"], "accepted": true, "id": "1509.06664"}, "pdf": {"name": "1509.06664.pdf", "metadata": {"source": "CRF", "title": "Reasoning about Entailment with Neural Attention", "authors": ["Tim Rockt\u00e4schel"], "emails": ["t.rocktaschel@cs.ucl.ac.uk", "etg@google.com", "kmh@google.com", "tomas@kocisky.eu", "pblunsom@google.com"], "sections": [{"heading": "1 Introduction", "text": "This year, it has come to the point where it only takes one year to get to the next round."}, {"heading": "2 Methods", "text": "In this section we discuss LSTMs (\u00a7 2.1) and describe how they can be applied to RTE (\u00a7 2.2). We present an extension of an LSTM for RTE with Neural Attention (\u00a7 2.3) and Word-for-Word Attention (\u00a7 2.4). Finally, we show how such attentive models can easily be used for both types: on the hypothesis-based premise and on the premise-based hypothesis (\u00a7 2.5)."}, {"heading": "2.1 LSTMs", "text": "Recurrent neural networks (RNNs) with long-term short-term memory (LSTM) units [Hochreiter and Schmidhuber, 1997] have been successfully applied to a wide range of NLP tasks, such as machine translation [Sutskever et al., 2014, Bahdanau et al., 2014], parsing constituencies [Vinyals et al., 2014], speech modeling [Zaremba et al., 2014] and more recently RTE [Bowman et al., 2015]. LSTMs include memory cells that can store information over a long period of time, as well as three types of gates that control the flow of information in and out of these cells: input gates (Eq. 2), forming gates (Eq. 4) and output gates (Eq. 4). Given an input vector xt at a given time t, the previous output vectors t \u2212 1 and the cell state b \u2212 1, an LSTK with the next size hidden Zk (xk) and \u2212 W (H) are \u2212 t and \u2212 t."}, {"heading": "2.2 Recognizing Textual Entailment with LSTMs", "text": "LSTMs can easily be used for RTE by encoding the premise and hypothesis independently as dense vectors and taking their concatenation as input to an MLP classifier [Bowman et al., 2015]. This shows that LSTMs can learn semantically rich sentence representations that are suitable for determining textual relationships. In contrast to learning sentence representations, we are interested in neural models that read both sentences to determine the relationships, which makes them think about entanglements of words and phrases. Figure 1 shows the high-level structure of this model. The premise (left) is read by an LSTM (A). A second LSTM with different parameters reads a delimiter and the hypothesis (right), but its memory state is initialized with the last cell state of the previous LSTM (c5 in the example)."}, {"heading": "2.3 Attention", "text": "Attentive neural networks have recently shown success in a wide range of tasks ranging from handwriting synthesis to evaluation (Graves, 2013), machine translation (Bahdanau et al., 2014), number classification (Mnih et al., 2014), picture caption (Xu et al., 2015), speech recognition (Chorowski et al., 2015) and sentence summary (Rush et al., 2015), to geometric reasoning (Vinyals et al., 2015). The idea is to allow the model to participate in past output vectors (see Figure 1 B), thereby alleviating the LSTM's cell-state bottleneck. Specifically, an LSTM with attention to RTE et al., 2015]. The idea is to allow the model to participate in past output vectors (see Figure 1 B), thereby sharpening the LSTM's state."}, {"heading": "2.4 Word-by-word Attention", "text": "To determine whether one sentence results in another, it may be a good strategy to test the effects or contradictions of individual concerted pairs of words and phrases. To encourage such behavior, we use neural word-by-word attention similar to Bahdanau et al. [2014], Hermann et al. [2015] and Rush et al. [2015]. The difference is that we do not use attention to generate words, but to obtain a sentence-pair encoding of fine-grained thinking by soft alignment of words and phrases in the premise and hypothesis. In our case, this amounts to visiting the output vectors of the first LSTM, while the second LSTM processes the hypothesis one word at a time, thus generating attention weights over all the output vectors of the premise and the output vectors of the premise for each word xt in the hypothesis (Figure 1 C)."}, {"heading": "2.5 Two-way Attention", "text": "Inspired by bidirectional LSTMs that read a sequence and its reverse to improve coding [Graves and Schmidhuber, 2005], we introduce a mutual attention to RTE. The idea is simply to use the same model that monitors the premise based on the hypothesis to also consider the hypothesis based on the premise by swapping the two sequences."}, {"heading": "3 Experiments", "text": "We are conducting experiments at the Stanford Natural Language Inference Corpus [SNLI, Bowman et al., 2015], which is two orders of magnitude larger than other existing RTE corpus such as Sentences Involving Compositional Knowledge [SICK, Marelli et al., 2014]. In addition, much of the training examples in SICK were heuristically generated from other examples. By contrast, all sentence pairs in the SNLI come from human annotators. Size and quality of the SNLI make it a suitable resource for training neural architectures such as those proposed in this paper. We use ADAM [Kingma and Ba, 2014] for optimization with an initial impulse coefficient of 0.9 and a second impulse coefficient of 0.999.2. For each model, we perform a small grid search for combinations of the initial learning rate through [1E-4, 3E-4, 1E-3], doutrop3 [0.1, 2, best-1E]."}, {"heading": "4 Results and Discussion", "text": "In fact, most people are able to determine for themselves what they want and what they want. It's not that they are able to decide whether they want it or not. It's not that they want it. It's not that they want it. It's that they don't want it. It's that they don't want it. It's that they don't want it. It's that they don't want it. It's that they don't want it. It's that they don't want it. It's that they don't want it. It's that they don't want it. It's that they don't want it."}, {"heading": "4.1 Qualitative Analysis", "text": "It is instructive to analyze which representations the model makes when deciding on the class of an RTE example. Note that interpretations based on attention weights must be taken with caution, as the model is not forced to rely solely on representations received from attention (see Eq. 10 and 14). In the following, the attention patterns of the presented models are visualized and discussed."}, {"heading": "5 Conclusion", "text": "In this paper, we show how the state of the art in the detection of textual problems on a large, humanized and annotated corpus can be improved by general end-to-end differentiation models. Our results show that LSTM-recurrent neural networks that read sequence pairs to produce a definitive representation from which a simple classifier predicts the consequences outperform both a neural baseline and a classifier with handmade features. Moreover, extending these models beyond the premise offers further improvements in the predictive capabilities of the system, leading to a new state-of-the-art precision in detecting consequences on the Stanford Natural Language Inference Corpus.The models presented here are general sequence models that do not require appeals to natural language-specific processing that go beyond tokenization, and are therefore an appropriate target for transfer learning by pre-training the recurring systems to other corpses, by training them on the larger problems, and by training them on the larger ones."}, {"heading": "Acknowledgements", "text": "We thank Nando de Freitas, Samuel Bowman and Jonathan Berant for their helpful comments on the drafts of this work."}], "references": [{"title": "Naturalli: Natural logic inference for common sense reasoning", "author": ["Gabor Angeli", "Christopher D Manning"], "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Angeli and Manning.,? \\Q2014\\E", "shortCiteRegEx": "Angeli and Manning.", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Representing meaning with a combination of logical form and vectors", "author": ["Islam Beltagy", "Stephen Roller", "Pengxiang Cheng", "Katrin Erk", "Raymond J. Mooney"], "venue": "arXiv preprint arXiv:1505.06816,", "citeRegEx": "Beltagy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Beltagy et al\\.", "year": 2015}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D Manning"], "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Attention-based models for speech recognition", "author": ["Jan Chorowski", "Dzmitry Bahdanau", "Dmitriy Serdyuk", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Chorowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2015}, {"title": "The pascal recognising textual entailment challenge. In Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment", "author": ["Ido Dagan", "Oren Glickman", "Bernardo Magnini"], "venue": null, "citeRegEx": "Dagan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dagan et al\\.", "year": 2006}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Graves and Schmidhuber.,? \\Q2005\\E", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Unal-nlp: Combining soft cardinality features for semantic textual similarity, relatedness and entailment", "author": ["Sergio Jimenez", "George Duenas", "Julia Baquero", "Alexander Gelbukh", "Av Juan Dios B\u00e1tiz", "Av Mendiz\u00e1bal"], "venue": "SemEval", "citeRegEx": "Jimenez et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jimenez et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Illinois-lh: A denotational and distributional approach to semantics", "author": ["Alice Lai", "Julia Hockenmaier"], "venue": "SemEval", "citeRegEx": "Lai and Hockenmaier.,? \\Q2014\\E", "shortCiteRegEx": "Lai and Hockenmaier.", "year": 2014}, {"title": "Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment", "author": ["Marco Marelli", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli"], "venue": "In SemEval-2014,", "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Recurrent models of visual attention", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston"], "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Ecnu: One stone two birds: Ensemble of heterogenous measures for semantic relatedness and textual entailment", "author": ["Jiang Zhao", "Tian Tian Zhu", "Man Lan"], "venue": "SemEval", "citeRegEx": "Zhao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": "This task is important since many natural language processing (NLP) problems, such as information extraction, relation extraction, text summarization or machine translation, rely on it explicitly or implicitly and could benefit from more accurate RTE systems [Dagan et al., 2006].", "startOffset": 259, "endOffset": 279}, {"referenceID": 3, "context": "Recently, Bowman et al. [2015] published the Stanford Natural Language Inference (SNLI) corpus accompanied by a neural network with long short-term memory units [LSTM, Hochreiter and Schmidhuber, 1997], which achieves an accuracy of 77.", "startOffset": 10, "endOffset": 31}, {"referenceID": 9, "context": "Recurrent neural networks (RNNs) with long short-term memory (LSTM) units [Hochreiter and Schmidhuber, 1997] have been successfully applied to a wide range of NLP tasks, such as machine translation [Sutskever et al.", "startOffset": 74, "endOffset": 108}, {"referenceID": 18, "context": ", 2014], constituency parsing [Vinyals et al., 2014], language modeling [Zaremba et al.", "startOffset": 30, "endOffset": 52}, {"referenceID": 20, "context": ", 2014], language modeling [Zaremba et al., 2014] and recently RTE [Bowman et al.", "startOffset": 27, "endOffset": 49}, {"referenceID": 3, "context": ", 2014] and recently RTE [Bowman et al., 2015].", "startOffset": 25, "endOffset": 46}, {"referenceID": 3, "context": "LSTMs can readily be used for RTE by independently encoding the premise and hypothesis as dense vectors and taking their concatenation as input to an MLP classifier [Bowman et al., 2015].", "startOffset": 165, "endOffset": 186}, {"referenceID": 14, "context": "We use word2vec vectors [Mikolov et al., 2013] as word representations, which we do not optimize", "startOffset": 24, "endOffset": 46}, {"referenceID": 6, "context": "Attentive neural networks have recently demonstrated success in a wide range of tasks ranging from handwriting synthesis [Graves, 2013], machine translation [Bahdanau et al.", "startOffset": 121, "endOffset": 135}, {"referenceID": 1, "context": "Attentive neural networks have recently demonstrated success in a wide range of tasks ranging from handwriting synthesis [Graves, 2013], machine translation [Bahdanau et al., 2014], digit classification [Mnih et al.", "startOffset": 157, "endOffset": 180}, {"referenceID": 15, "context": ", 2014], digit classification [Mnih et al., 2014], image captioning [Xu et al.", "startOffset": 30, "endOffset": 49}, {"referenceID": 19, "context": ", 2014], image captioning [Xu et al., 2015], speech recognition [Chorowski et al.", "startOffset": 26, "endOffset": 43}, {"referenceID": 4, "context": ", 2015], speech recognition [Chorowski et al., 2015] and sentence summarization [Rush et al.", "startOffset": 28, "endOffset": 52}, {"referenceID": 16, "context": ", 2015] and sentence summarization [Rush et al., 2015], to geometric reasoning [Vinyals et al.", "startOffset": 35, "endOffset": 54}, {"referenceID": 1, "context": "To encourage such behavior we employ neural word-by-word attention similar to Bahdanau et al. [2014], Hermann et al.", "startOffset": 78, "endOffset": 101}, {"referenceID": 1, "context": "To encourage such behavior we employ neural word-by-word attention similar to Bahdanau et al. [2014], Hermann et al. [2015] and Rush et al.", "startOffset": 78, "endOffset": 124}, {"referenceID": 1, "context": "To encourage such behavior we employ neural word-by-word attention similar to Bahdanau et al. [2014], Hermann et al. [2015] and Rush et al. [2015]. The difference is that we do not use attention to generate words, but to obtain a sentence-pair encoding from fine-grained reasoning via soft-alignment of words and phrases in the premise and hypothesis.", "startOffset": 78, "endOffset": 147}, {"referenceID": 7, "context": "Inspired by bidirectional LSTMs that read a sequence and its reverse for improved encoding [Graves and Schmidhuber, 2005], we introduce two-way attention for RTE.", "startOffset": 91, "endOffset": 121}, {"referenceID": 11, "context": "We use ADAM [Kingma and Ba, 2014] for optimization with a first momentum coefficient of 0.", "startOffset": 12, "endOffset": 33}, {"referenceID": 11, "context": "Standard configuration recommended by Kingma and Ba. As in Zaremba et al. [2014], we apply dropout only on the inputs and outputs of the network.", "startOffset": 38, "endOffset": 81}, {"referenceID": 3, "context": "Model k |\u03b8|W+M |\u03b8|M Train Dev Test LSTM [Bowman et al., 2015] 100 \u2248 10M 221k 84.", "startOffset": 40, "endOffset": 61}, {"referenceID": 3, "context": "6 Classifier [Bowman et al., 2015] - - 99.", "startOffset": 13, "endOffset": 34}], "year": 2015, "abstractText": "Automatically recognizing entailment relations between pairs of natural language sentences has so far been the dominion of classifiers employing hand engineered features derived from natural language processing pipelines. End-to-end differentiable neural architectures have failed to approach state-of-the-art performance until very recently. In this paper, we propose a neural model that reads two sentences to determine entailment using long short-term memory units. We extend this model with a word-by-word neural attention mechanism that encourages reasoning over entailments of pairs of words and phrases. Furthermore, we present a qualitative analysis of attention weights produced by this model, demonstrating such reasoning capabilities. On a large entailment dataset this model outperforms the previous best neural model and a classifier with engineered features by a substantial margin. It is the first generic end-to-end differentiable system that achieves state-of-the-art accuracy on a textual entailment dataset.", "creator": "Creator"}}}