{"id": "1509.07087", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Sep-2015", "title": "Deep Temporal Sigmoid Belief Networks for Sequence Modeling", "abstract": "Deep dynamic generative models are developed to learn sequential dependencies in time-series data. The multi-layered model is designed by constructing a hierarchy of temporal sigmoid belief networks (TSBNs), defined as a sequential stack of sigmoid belief networks (SBNs). Each SBN has a contextual hidden state, inherited from the previous SBNs in the sequence, and is used to regulate its hidden bias. Scalable learning and inference algorithms are derived by introducing a recognition model that yields fast sampling from the variational posterior. This recognition model is trained jointly with the generative model, by maximizing its variational lower bound on the log-likelihood. Experimental results on bouncing balls, polyphonic music, motion capture, and text streams show that the proposed approach achieves state-of-the-art predictive performance, and has the capacity to synthesize various sequences.", "histories": [["v1", "Wed, 23 Sep 2015 18:36:42 GMT  (438kb,D)", "http://arxiv.org/abs/1509.07087v1", "to appear in NIPS 2015"]], "COMMENTS": "to appear in NIPS 2015", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["zhe gan", "chunyuan li", "ricardo henao", "david e carlson", "lawrence carin"], "accepted": true, "id": "1509.07087"}, "pdf": {"name": "1509.07087.pdf", "metadata": {"source": "CRF", "title": "Deep Temporal Sigmoid Belief Networks for Sequence Modeling", "authors": ["Zhe Gan", "Chunyuan Li", "Ricardo Henao", "David Carlson"], "emails": ["lcarin}@duke.edu"], "sections": [{"heading": null, "text": "The multi-layered model was designed by building a hierarchy of temporal sigmoid faith networks (TSBNs), defined as a sequential stack of sigmoid faith networks (SBNs). Each SBN has a contextual hidden state inherited from the previous SBNs in the sequence, and is used to regulate its hidden bias. Scalable learning and inference algorithms are derived by introducing a recognition model that allows for rapid backward scanning of variation. This recognition model is trained in conjunction with the generative model by maximizing its variable lower limit on log probability. Experimental results on bouncing balls, polyphonic music, motion capture and text streams show that the proposed approach achieves advanced predictive performance and has the ability to synthesize multiple sequences."}, {"heading": "1 Introduction", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "2 Model Formulation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Sigmoid Belief Networks", "text": "Deep dynamic generative models are considered, based on the Sigmoid Belief Network (SBN) > > b > specitional data is developed by v | 16]. An SBN is a Bayesian network that models a binary visible vector v \u00b2 {0, 1} M, with respect to binary hidden variables h \u00b2 0, 1} J and weights W \u00b2 RM \u00b7 J withp (vm = 1 | h) = \u03c3 (w > mh + cm), p (hj = 1) = \u03c3 (bj), (1), where v = [v1,.,., vM] >, h = [h1,., hJ] >, W = [w1,.,., cM] >, b = b1, [b1] >, and the logistic function, n \u00b2 S1 / (x), p \u2212 vivite,., b and c = [c1,.,.]."}, {"heading": "2.2 Temporal Sigmoid Belief Networks", "text": "The proposed Temporal Sigmoid Faith Network (TSBN) model is a sequence of SBNs arranged in such a way that the biases of the SBN in each given time step depend on the state of the SBNs in the preceding time steps. In particular, we assume that we have a length-T binary visible sequence whose tster time step is called vt-W. The TSBN describes the common probability aspects (V, H) = p (h1) p (v1 | h1) \u00b7 T = 2p (ht \u2212 1, vt \u2212 1) \u00b7 p (vt, vt \u2212 1) \u00b7 p (2) where V = [v1,. vT], H = [h1,. hT] p (hT], and each ht-T], and each ht-T row (0, 1} J represents the hidden state."}, {"heading": "2.3 TSBN Variants", "text": "The above model can easily be extended to real sequence data by replacing (14) with p (vt | ht, vt \u2212 1) = N (\u00b5t, diag (\u03c32t), where \u00b5mt = w > 2mht + w > 4mvt \u2212 1 + cm, log \u03c3 2 mt = (w \u2032 2m) > ht + (w \u2032 4m) > vt \u2212 1 + c \u2032 m, (5) and \u00b5mt and \u03c32mt are elements of \u00b5t and 2 t respectively. W \u2032 2 and W \u2032 4 have the same size of W2 and W4, respectively. Compared to Gaussian TRBM [9], where \u03c3mt is fixed to 1, our formalism uses a diagonal matrix to parameterise the variance structure of the vt.Modeling counter data, introducing an approach to modeling time series data with counter observations by using (14) a hidden matrix to parameterise the modeling count structure using p (vt, vt \u2212 1)."}, {"heading": "2.4 Deep Architecture for Sequence Modeling with TSBNs", "text": "Learning the sequential dependencies with the flat model in (2) - (14) can be restrictive, so we propose two deep architectures to improve its representational power: (i) adding stochastic hidden layers; (ii) adding deterministic hidden layers. The graphical model for the deep TSBN is shown in Figure 4 (c). Specifically, we consider a deep TSBN with hidden layers h (') t for t = 1,... To get a suitable generative model, the top hidden layer h (L) contains stochastic hidden binary units (') and designates the visible layer vt = h (0) t and leave h (L + 1) t for '. \u2212 \u2212 t, if the top hidden layer h (L) contains stochastic hidden layers, the top hidden layer h (L) contains stochastic hidden variables ('). For the hidden layers, J (= 1), the middle process is used when \u2212 t = 1."}, {"heading": "3 Scalable Learning and Inference", "text": "Calculation of the exact backfield using the hidden variables in (2) is not feasible. Approximate Bayesian conclusions such as Gibbs sampling or Bayes variation (VB) inference can be implemented [15, 16]. However, Gibbs sampling is very inefficient as the conditional backfield distribution of the hidden variables is not factored. Indeed, the VB middle field provides a fully factored backfield variation, but this technique widens the gap between the limit to be optimized and the actual log probability, potentially leading to poor adaptation to the data. To allow tractable and scalable inferences and parameter learning without losing the flexibility of the rear variation, we apply the algorithm described in [13] Neural Variational Inference and Learning (NVIL)."}, {"heading": "3.1 Variational Lower Bound Objective", "text": "We are interested in forming the TSBN model, p\u03b8 (V, H), with the parameters q > J (1) described in (2), with the parameters \u03b8 (H | V). We then follow the variation principle to derive a lower limit for the marginal log probability expressed as 1L (V, \u03b8, \u03c6) = Eq\u03c6 (H | V) [log p\u03b8 (V, H) \u2212 log q\u03c6 (H | V)]]. (7) We construct the approximate back log probability (H | V) as the recognition model. In this way, we avoid the need to calculate variation parameters per data point; instead, we calculate a series of parameters that are used for all V. To achieve a quick conclusion, the recognition model is called h\u03c6 (H | V) = q (hh1 | v1) \u00b7 T > J jvt = 1 > jvt (1), vvt = 1, vvt = 1 (vvt)"}, {"heading": "3.2 Parameter Learning", "text": "In order to optimize (7), we use Monte Carlo methods to approximate expectations and stochastic gradient parentage (SGD) for parameter optimization. The gradients can be expressed as follows: (5) \"empirical data\" (V, H, V) \"empirical data\" (V, H, V) \"empirical data\" (11) \"empirical data\" (V, H) \"empirical data\" (H, V) \"empirical data\" (H, V) \"empirical data\" (V) \"empirical data\" (H, V) \"empirical data\" (H, V) \"empirical data\" (V) \"empirical data\" (H, V) \"empirical data\" (empirical data) \"empirical data\" (H, V) \"empirical data\" empirical data \"(empirical data)\" empirical data \"(H, V)\" empirical data \"empirical data\" (H, V) \"empirical data\" empirical data \"empirical data\" (H, V) \"empirical data\" empirical data \"empirical data\" (H, \"empirical data\" empirical data \"empirical data\" (H, V) \"empirical data\" empirical data \"empirical data\" (H, \"empirical data)\" empirical data \"empirical data\" empirical data \"(H,\" empirical data \"empirical data)\" empirical data \"(H,\" empirical data \"empirical data\" (H, \"empirical data)\" empirical data \"empirical data\" (H, \"empirical data\" empirical data \"empirical data)\" empirical data \"(H,\" empirical data \"empirical data\" empirical data) \"empirical data\" (H, \"empirical data\" empirical data"}, {"heading": "3.3 Extension to deep models", "text": "The detection model corresponding to the low TSBN is shown in Figure 4 (d). In Section 2.4, two types of deep architectures are discussed: (i) the calculation of the lower limit and (ii) the calculation of the gradients. The upper hidden layer is stochastical. If the middle hidden layers are also stochastical, the calculation of the lower layer is more involved compared to the flat model, but the gradient evaluation remains simple as in (12). On the other hand, if deterministic middle hidden layers (i.e. recursive neural networks) are used, the lower lens remains the same as a flat model, since the only stochasticity in the generative process is in the upper layer, but the gradients must be calculated recursively by reverse propagation over the time algorithm [22]."}, {"heading": "4 Related Work", "text": "The RBM has been widely used as a building block to learn the sequential dependencies in time series data, e.g. the conditional RBM-related models [7, 23] and the temporal RBM [8]. In order to allow an exact conclusion, the recurring temporal RBM has also been proposed [9] and extended further to learn the dependency structure within the observations [11]. In the paper reported here, we focus on the modelling of sequences based on the SBN [16], which has recently been shown to have the potential to build deep generative models [13, 15, 24]. Our work serves as a further extension of the SBN that can be used to model time series data. Similar ideas have also been considered in [25] and [26]. However, the authors focus on grammatical learning and the use of a feed-forward approach of the VB center field to perform the follow-up."}, {"heading": "5 Experiments", "text": "We present experimental results on four publicly available datasets: the bouncing balls [9], polyphonic music [10], motion detection [7], and the state of the union [30]. To evaluate the performance of the TSBN model, we show sequences generated from the model and indicate the average log probability that the model assigns to a test sequence, and the average squared prediction error per frame. Code is available at https: / / github.com / zhegan27 / TSBN _ code _ NIPS2015.The TSBN model with W3 = 0 and W4 = 0 is called Hidden Markov SBN (HMSBN), the deep TSBN with stochastic hidden layer is called DTSBN-S, and the deep TSBN with deterministic hidden layer is called Sq1."}, {"heading": "5.1 Bouncing balls dataset", "text": "We conducted the first experiment with synthetic videos of 3 bouncing balls, where pixels are evaluated binarily. We followed the procedure in [9] and created 4,000 videos for training and another 200 videos for the test. Each video has a length of 100 cm and a resolution of 30 x 30. The dictionaries we learned using the HMSBN are shown in Figure 2 (left). Compared to previous work [9, 10], our learned basics are more spatially localized. In Table 6, we compare the average square prediction error per frame over the 200 test videos, with recurring temporary RBM (RTRBM) and structured RTRBM (SRTRBM). As we can see, our approach achieves better performance compared to the baseline in the literature. In addition, we find that a high-level TSBN significantly reduces the prediction error, compared to a Supplement One TSBN, due to the use of high-level information, including past use of the TSBN."}, {"heading": "5.2 Motion capture dataset", "text": "In this experiment, we used the CMU motion capture dataset, which consists of measured joint angles for different types of motion. We used the 33 running and walking sequences of subjects 35 (23 walking sequences and 10 running sequences), and we followed the pre-processing process of [11], leaving 58 joint angles. We divided the 33 sequences into training and test sequences: the first had 31 sequences, and the second had 2 sequences (one walking and one running), and we calculated the prediction error over 100 attempts, as shown in Table 7. The TSBN we implemented is of size 100 in each hidden layer and order 1. It is evident that the TSBN-based models significantly improve over the Gaussian (G-) RTRBM and the Spike Slab (SS-) SRTRBM. Another popular motion capture dataset is the MIT dataset."}, {"heading": "5.3 Polyphonic music dataset", "text": "The third experiment is based on four different polyphonic music sequences from Piano [10], i.e., Piano-midi.de (Piano), Nottingham (Nott), MuseData (Muse) and JSB Channels (JSB). Each of these data sets is presented as a collection of 88-dimensional binary sequences covering the entire range of the piano from A0 to C8. The samples generated from the trained HMSBN model are shown in Figure 2 (Medium). As can be seen, various styles of polyphonic music are synthesized, and the corresponding MIDI files are provided as Music 1 and 2 in the Supplementary Material. Our model has the ability to learn basic harmony rules and local temporal coherence, but long-term structure and musical melody remain elusive."}, {"heading": "5.4 State of the Union dataset", "text": "The State of the Union (STU) dataset contains the transcripts of T = 225 US state of New York, from 1790 to 2014. Two tasks are considered, i.e., prediction and dynamic topic modeling. Prediction The prediction task deals with the estimation of the held words. We use the setup in [31]. After removing stopwords and terms that occur less than 7 times in a document or less than 20 times in total, there are 2375 unique words. All the data from the last year are kept-out. For the documents in previous years, we divide the words of each document into 80% / 20%. The model is trained on the 80% share, and the remaining 20% held words are used to test the prediction each year. The words in both held sets are evaluated according to the probability estimated by (6)."}, {"heading": "6 Conclusion", "text": "We introduced the Deep Temporal Sigmoid Belief Networks, an extension of SBN that models temporal dependencies in high-dimensional sequences. To enable scalable conclusions and learning processes, an efficient variation optimization algorithm is being developed. Experimental results from several datasets show that the proposed approach achieves superior predictive results and synthesizes interesting sequences. In this work, we have individually studied the modeling of different data types. An interesting future work is to combine them into a uniform framework for dynamic multimodal learning. Furthermore, we can use high-order optimization methods to accelerate conclusions [32].Recognition This research was partially supported by ARO, DARPA, DOE, NGA and ONR."}, {"heading": "A Outline of the NVIL algorithm", "text": "The outlines of the NVIL algorithm for calculating gradients are shown below (reproduced from [13]). C\u03bb (vt) represents the data-dependent baseline and \u03b1 = 0.8 during the experiments. Calculate algorithm 1 gradient estimates for the model parameters and detection parameters."}, {"heading": "B Learning and Inference Details on TSBN", "text": "For t = 1,., T, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt (vmt = 1, vvt = 1, vvt = 1, vvt \u2212 1, vvt \u2212 1, vvt \u2212 vvt (vvt \u2212 1), vvt (vvt \u2212 1, vvt \u2212 2), vvt (vmt = 1, vmt = 1), vvt (vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt (vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vv = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvt = 1, vvvt = 1, vvt = 1, vvt = 1, vvt = 1, v"}, {"heading": "C Learning and Inference Details on Deep TSBN", "text": "For the simplicity of the notation, we consider a two-layer, deep TSBN here = > J + > J + > J + > J + > J + > J + > J + > K + > K + > K + > K + > K + > K + > K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 kht = 1) = K (w > K + K \u2212 K \u2212 K \u2212 K \u2212 kht = 1) = K (w > 1jzt \u2212 w > 3jht (w > 3jht \u2212 K) p (hkt = 1) = K \u2212 K \u2212 kht \u2212 K \u2212 kht \u2212 kht \u2212 s (w > 1jzt \u2212 1), (w > K \u2212 K \u2212 K \u2212 K \u2212 kht \u2212 K \u2212 kht \u2212 kht = 1), (w > K \u2212 K \u2212 kht = 1)."}, {"heading": "D Additional Results", "text": "D.1 Generated dataThe generated synthetic motion data and polyphonic music data can be downloaded at: https: / / drive.google.com / drive / u / 0 / folders / 0B1HR6m3IZSO _ SWt0aS1oYmlneDQ.D.2 Bouncing balls datasetFurther experimental results are shown in Table 6. AR represents an autoregressive Markov model with no latent variables.D.3 WITH Motion Capture DatasetWe randomly select 10% of the data set as a test set. Quantitative results are shown in Table 7."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "Deep dynamic generative models are developed to learn sequential dependencies<lb>in time-series data. The multi-layered model is designed by constructing a hierar-<lb>chy of temporal sigmoid belief networks (TSBNs), defined as a sequential stack<lb>of sigmoid belief networks (SBNs). Each SBN has a contextual hidden state,<lb>inherited from the previous SBNs in the sequence, and is used to regulate its hid-<lb>den bias. Scalable learning and inference algorithms are derived by introducing<lb>a recognition model that yields fast sampling from the variational posterior. This<lb>recognition model is trained jointly with the generative model, by maximizing its<lb>variational lower bound on the log-likelihood. Experimental results on bouncing<lb>balls, polyphonic music, motion capture, and text streams show that the proposed<lb>approach achieves state-of-the-art predictive performance, and has the capacity to<lb>synthesize various sequences.", "creator": "LaTeX with hyperref package"}}}