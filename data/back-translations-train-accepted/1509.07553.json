{"id": "1509.07553", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Sep-2015", "title": "Linear-Time Learning on Distributions with Approximate Kernel Embeddings", "abstract": "Many interesting machine learning problems are best posed by considering instances that are distributions, or sample sets drawn from distributions. Previous work devoted to machine learning tasks with distributional inputs has done so through pairwise kernel evaluations between pdfs (or sample sets). While such an approach is fine for smaller datasets, the computation of an $N \\times N$ Gram matrix is prohibitive in large datasets. Recent scalable estimators that work over pdfs have done so only with kernels that use Euclidean metrics, like the $L_2$ distance. However, there are a myriad of other useful metrics available, such as total variation, Hellinger distance, and the Jensen-Shannon divergence. This work develops the first random features for pdfs whose dot product approximates kernels using these non-Euclidean metrics, allowing estimators using such kernels to scale to large datasets by working in a primal space, without computing large Gram matrices. We provide an analysis of the approximation error in using our proposed random features and show empirically the quality of our approximation both in estimating a Gram matrix and in solving learning tasks in real-world and synthetic data.", "histories": [["v1", "Thu, 24 Sep 2015 22:26:02 GMT  (593kb,D)", "http://arxiv.org/abs/1509.07553v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["dougal j sutherland", "junier b oliva", "barnab\u00e1s p\u00f3czos", "jeff g schneider"], "accepted": true, "id": "1509.07553"}, "pdf": {"name": "1509.07553.pdf", "metadata": {"source": "CRF", "title": "Linear-time Learning on Distributions with Approximate Kernel Embeddings", "authors": ["Dougal J. Sutherland", "Junier B. Oliva", "Barnab\u00e1s P\u00f3czos", "Jeff Schneider"], "emails": ["dsutherl@cs.cmu.edu", "joliva@cs.cmu.edu", "bapoczos@cs.cmu.edu", "schneide@cs.cmu.edu"], "sections": [{"heading": "Introduction", "text": "While complex static vector instances are useful in a variety of applications, many machine learning problems are more natural by looking at instances that are distributions, or sentences drawn from distributions. Political scientists can learn a function from community demographics to understand percentages of who supports a candidate (Flaxman, Wang, and Smola 2015).The mass of dark matter halos can be derived from the speed of galaxies in a cluster (Ntampaka et al. 2015).Expensive approaches to propagation of expectations can be achieved by learning a just-in-time regression model (Jitkrittum et al. 2015).All of these applications are supported by working directly from sets drawn from the distribution of interests, rather than developing a pro-problem-in-form inherited statistics."}, {"heading": "K( , ) \u2248 z( )Tz( )", "text": "with random characteristics of sample sets \u03c7i iid \u0445 pi, \u03c7j iid \u0445 pj. Large number of instances using primordial space techniques. We provide an approximation of embedding and demonstrate the effectiveness of embedding in both real and synthetic data. To the best of our knowledge, this work offers the first non-discredited embedding of non-L2 cores for probability density functions."}, {"heading": "Related Work", "text": "The two main lines of relevant research are the development of kernels in probability distributions and explicit embedding for scalable kernel learning. (The two main lines of relevant research are the development of kernel distributions on probability distributions and distributions.) Another approach is a distance between distributions, often the L2 distance or Kullback Leibler (KL) divergence, the parametric (Jebara, Kondor 2004) or non-parametric (Sricharan, Wei and Hero 2013). (The distance can then be used in Vasconcelos 2003)."}, {"heading": "Homogeneous Density Distances (HDDs)", "text": "We consider cores on the basis of metrics that we call homogeneous density distances (HDDs): d2 (p, q) = \u0442 [0,1] \"\u0445 (p (x), q (x))) dx, (2), where \u0432 (x, y): R + \u00b7 R + \u2192 R + is a negative kernel, i.e. a square Hilberian metric, and \u0432 (tx, ty) = t\u0445 (x, y) for all t > 0. Table 1 shows a few important instances. Note that we assume that support for distributions is contained within [0, 1].\" We then use these distances in a generalized RBF kernel (1). d is a Hilberian metric (Fuglede 2005), so K is positively defined (Haasdonk and Bahlmann 2004)."}, {"heading": "Embedding HDDs into L2", "text": "Fuglede (2005) points out that \u03ba corresponds to a limited measure \u00b5 (\u03bb) as shown in Table 1, with\u0432 (x, y) = \u0445 R \u2265 0 | x 1 2 + i\u03bb \u2212 y 1 2 + i\u03bb | 2 d\u00b5 (\u03bb). (3) Let Z = \u00b5 (R \u2265 0) and c\u03bb = (\u2212 12 + i\u03bb) / (1 2 + i\u03bb); then\u0432 (x, y) = E\u03bb \u0445 \u00b5Z | g\u03bb (x) \u2212 g\u03bb (y) | 2where g\u03bb (x) (x) = q Zc\u03bb (x 2 + i\u03bb \u2212 1).We can approximate the expectation with an empirical mean. Let us leave \u03bbj iid \u0445 \u00b5Z for j {1,.., M}; then\u0445 (x, y) \u2248 1 M (M = 1 M = 1 | g\u03bbj p), p (x) p (x), p (x), p (x), p), p)."}, {"heading": "Finite Embeddings of L2", "text": "If the densities p and q are smooth, then the metric L2 between the functions p\u03bb and q\u03bb can be approximated by means of projections on the basic functions. Suppose that {i} i-Z is an orthonormal basis for L2 ([0, 1]); then we can construct an orthonorthonormal basis for L2 ([0, 1] ') according to the tensor product: {\u03b1}. \u2212 Z'wo \u03b1 (f)."}, {"heading": "Embedding RBF Kernels into RD", "text": "The A features approach the hard disk (2) in R2M | V |; therefore, the RKS embedding (Rahimi and Recht 2007) is approximated to the A features. RKS embedding is 1 z: Rm \u2192 RD so that for fixed {\u03c9i} D / 2i = 1 iid \u00b2 N (0, \u03c3 \u2212 2Im) and for each x, y Rm: z (x) Tz (y) \u2248 exp (\u2212 12,000 x \u2212 y \u00b2), where (x) = 2 D (spectrum T1 x), cos (\u03c9 T 1 x),...). (7) Thus, we can approximate the hard disk core (1) as: K (p, q) = exp (\u2212 12\u04452 d2 (p, q)) \u2012 exp (p (p (q) \u2012 exp (p)."}, {"heading": "Finite Sample Estimates", "text": "Our final approximation for hard disk cores (8) depends on integrals of the densities p and q. In practice, it is unlikely that we directly observe an input density, but even with a pdf p, the integrals that make up the elements of A (p) are not readily calculable. Therefore, we first estimate the density as p, e.g. with estimation of the core density (KDE) and estimate A (p) asA (p) (p). Remember that the elements of A (p) are: a\u03b1 (p) S (J) = [0.1] \"(t)\" (Hoffman) p \"S (t) dt (9), where j\" 1,.., M, \"S\" (R, I), \u03b1 \"V.\" In lower dimensions, we can approach (9) with simple numerical integration of Monte Carlo."}, {"heading": "Summary and Complexity", "text": "The algorithm for calculating Ni = 1 for a series of distributions {pi} Ni = 1, given sample sets {\u03c7i} Ni = 1, where \u03c7i = {X (i) j [0, 1]'ni j = 1 iid \u0445 pi, so: 1. Draw M scalars \u03bbj iid \u0445 \u00b5Z and D / 2 vectors \u03c9r iid \u0445 N (0, \u03c3 \u2212 2I2M | V |), in O (M | V) time. (b) Calculate A (p \u00b2 i) for each of the N input distributions i: (a) Calculate an estimate of the kernel density from systems i, p \u00b2 i (uj) for each uj in (10), in O (nine) time. (b) Calculate A (p \u00b2 i), using a numerical integration estimate in (10), in O (M | V | ne) time. (c) Get the features A - (N) in this time (N), in this time (P)."}, {"heading": "Theory", "text": "Using two fixed densities p and q, taking into account each source of error: estimation of kernel density (\u03b5KDE); approximation of the smoothness assumptions to p and q: that they are members of a periodic Ho-Lder class (\u03b5tail); that they are limited below and above the RKS embedding (\u03b5RKS); that their kernel density estimates are based on p and q; that they are members of a periodic Ho-Lder class (\u03b2, L\u03b2); that they are below and above the RKS embedding (\u03b5RKS) and that their kernel density estimates are in a higher proportion than the values of p and q: that they are members of a periodic Ho-Lder class."}, {"heading": "Numerical Experiments", "text": "During these experiments, we use M = 5, | V | = 10 '(selected as a rule of thumb; larger values did not improve performance) and use a validation set (10% of the training set) to select bandwidths for KDE and the RBF kernel, as well as model regularization parameters. Except in the scene classification experiments, histogram methods used 10 containers per dimension; performance with other values was no better. The KL estimator used the fourth closest neighbor. We evaluate RBF kernels based on different distances. First, we try our JS, Hellinger and TV embeddings. We compare with L2 cores as in Oliva et al. (2014): exp (\u2212 12,000 p \u2212 q-2) 2): z (~ a (p-a)) Tz (~ a (q-a) and TV embeddings."}, {"heading": "Gram Matrix Estimation", "text": "We compare three different approaches to the estimation of K (pi, pj) = exp (\u2212 12\u03c32 JS (pi, pj)). Each approach uses kernel density estimates p-i. Estimates are compared based on a dataset of N = 50 random GMM distributions. The first approach approaches JS based on empirical estimates of entropies E log p-i. The second approach estimates JS as the euclidean distance of vectors of projection coefficients (5): JSpc (pi, pj) = empirical estimates of entropies E log p-i. The second approach estimates JS as the euclidean distance of vectors of projection coefficients (5): Jentp (pi, pj) = pentrope-i."}, {"heading": "Estimating the Number of Mixture Components", "text": "We will now illustrate the effectiveness of the random features of the hard disk in a regression task using Oliva et al. (2014): Estimate the number of components from a mixture of shortened Gaussians. We generate the distributions as follows: Draw the number of components Yi for the beddbettbettbettbettbettbettx (1,.,., 10}. For each component select an average \u00b5 (i) k \u0445 Unif [\u2212 5, 5] 2 and Kovarianzbettbettbettbetti (and Kovarianzbettbettbettbettbettbetti) (Yi \u0445 Unif) Unif (i,., i and Unif) Unif (Unif) Unif (Unif) Unif (Unif) Unif (i, i and Unif) Unif (Unif) Unif [\u2212 5, 5] Unif and Kovarianzbettbettbettbettbettbettbettbettbetti (Unif) Unif) Unif (Unif) Unif (Unif) Unif (Unif) Unif (Unif) Unif (Unif) Unif (Unif) Unif (Unif) Unif (Unif) Unif (Unif) Unif (Unif (Unif) Unif) Unif (Unif (Unif) Unif (Unif) Unif) Unif (Unif) Unif (Unif) Unif) Unif (Unif) Unif (Unif) Unif (Unif (Unif) Unif) Unif (Unif) Unif (Unif) Unif (Unif) Unif (Unif) Unif (Unif) Unif (Unif) Unif) Unif (Unif) Unif (Unif) Unif (Unif) Unif (Unif) Unif) Unif (Unif) Unif) Unif (Unif () Unif) Unif () Unif) Unif (Unif () Unif) Unif () Unif) Unif (1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5"}, {"heading": "Image Classification", "text": "As another example of the performance of our embedding, we are now trying to classify images based on their distribution of pixel values. We took the classes \"cat\" and \"dog\" from the CIFAR-10 dataset (Krizhevsky and Hinton 2009) and represented each 32 x 32 image by a set of triples (x, y, v), where x and y are the position of each pixel in the image and v the pixel value after conversion in grayscales. Horizontal reflection of the image was also included, so that each sample set \u03c7i R3 was the correct size. This is certainly not the best representation for these images; rather, we want to show that our HDD cores perform well compared to the other options in view of this simple representation. We used the same cores as in an SVM classifier from LIBLINEAR (Fan et al. 2008, for embedding history) or LIBM (Chang and other options)."}, {"heading": "Scene Classification", "text": "The activations are of an n-dimensional distribution, similar to the number of filters; each unit corresponds to an overlapping patch of the original image. (2012) Used are SIFT features of image patches. Wu, Gao, and Lio (2015) set accuracy datasets using a specific ad hoc method of extracting properties from distributions (D3); we compare our more principled alternatives.2 https: / / dougalsutherland / RMSE 35 1.4 1.45 1.55 55ho 1.110 T (D3)."}, {"heading": "Discussion", "text": "This paper presents the first non-linear embedding of density functions for the rapid calculation of disk-based cores, including cores based on the popular overall variant, Hellinger and Jensen-Shanon divergences. While such divergences have shown good empirical results when comparing densities, the non-parametric use of cores with these divergences previously required the calculation of a large N \u00d7 N-gram matrix, which prohibits their use in large datasets. Our embedding allows work in a primordial space using information theory cores. We analyze the approximation error of our embedding and illustrate its quality on multiple synthetic and real datasets."}, {"heading": "Acknowledgements", "text": "This work was partially funded by the NSF scholarship IIS1247658 and the DARPA scholarship FA87501220324. DJS is also supported by a Sandia Campus Executive Program Fellowship."}, {"heading": "Appendices", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Gram Matrix Estimation", "text": "We illustrate the ability of our embedding to approximate the Jensen-Shanon divergence. In the examples below the considered densities we are dealing with mixtures of five equally weighted spherical Gaussians to [0, 1] 2. That is, pi (x) = 15 5 + j = 1 Nt (mij, diag (s2ij)), where mij iid \u00b2 -Unif ([0, 1] 2), sij iid \u00b2 -Unif ([0.05, 0.15] 2) and Nt (m, s) is the distribution of a Gaussian (s2ij) trend to [0, 1] 2 with the mean parameter m and the covariance matrix parameter s. We work with the sample set {[1, 0.15] 2) Ni = {X (i) j (i) j (i) j \u00b2 nj = 1 iid \u00b2 -Jp, n = 2500, N =.We compare three approaches to the pi (K)."}, {"heading": "Proofs", "text": "& & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & & & # 10; & & & # 10; & & & # 10; & & & # 10; & & & & # 10; & & # 10; & # 10; & & # 10; & & & # 10; & & # 10; & # 10; & & & # 10; & & # 10; & & & # 10; & & # 10; & & & # 10; & & # 10; & & # 10; & & # 10; & # 10; & & & # 10; & & # 10; & # 10; & & & # 10; & & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & & & & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & & # 10; & & & # 10; & # 10; & & & # 10; & & # 10; & & # 10; & & & # 10; & & # 10; & & & # 10; & & # 10; & & # 10; & # 10; & & & & # 10; & & & & # 10; & & & & # 10; & & & & # 10; & & # 10; & & & # 10; & & & & & # 10; & & & & & # 10; & &"}], "references": [{"title": "Information theory and an extension of the maximum likelihood principle", "author": ["H. Akiake"], "venue": "2nd Int. Symp. on Inf. Theory.", "citeRegEx": "Akiake,? 1973", "shortCiteRegEx": "Akiake", "year": 1973}, {"title": "Spectral partitioning with indefinite kernels using the Nystr\u00f6m extension", "author": ["S. Belongie", "C. Fowlkes", "F. Chung", "J. Malik"], "venue": "ECCV.", "citeRegEx": "Belongie et al\\.,? 2002", "shortCiteRegEx": "Belongie et al\\.", "year": 2002}, {"title": "LIBSVM: a library for support vector machines", "author": ["Chang", "C.-C.", "Lin", "C.-J."], "venue": "ACM Trans. Intell. Syst. Technol. 2(3):1\u2013", "citeRegEx": "Chang et al\\.,? 2011", "shortCiteRegEx": "Chang et al\\.", "year": 2011}, {"title": "Similarity-based classification: Concepts and algorithms", "author": ["Y. Chen", "E.K. Garcia", "M.R. Gupta", "A. Rahimi", "L. Cazzanti"], "venue": "JMLR 10:747\u2013776.", "citeRegEx": "Chen et al\\.,? 2009", "shortCiteRegEx": "Chen et al\\.", "year": 2009}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin"], "venue": "JMLR 9:1871\u20131874.", "citeRegEx": "Fan et al\\.,? 2008", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Who supported Obama in 2012? Ecological inference through distribution regression", "author": ["S.R. Flaxman", "Y.-x. Wang", "A.J. Smola"], "venue": "KDD, 289\u2013298.", "citeRegEx": "Flaxman et al\\.,? 2015", "shortCiteRegEx": "Flaxman et al\\.", "year": 2015}, {"title": "Spirals in Hilbert space: With an application in information theory", "author": ["B. Fuglede"], "venue": "Exposition. Math. 23(1):23\u201345.", "citeRegEx": "Fuglede,? 2005", "shortCiteRegEx": "Fuglede", "year": 2005}, {"title": "Rates of strong uniform consistency for multivariate kernel density estimators", "author": ["E. Gin\u00e9", "A. Guillou"], "venue": "Ann. Inst. H. Poincar\u00e9 Probab. Statist. 38(6):907\u2013921.", "citeRegEx": "Gin\u00e9 and Guillou,? 2002", "shortCiteRegEx": "Gin\u00e9 and Guillou", "year": 2002}, {"title": "A fast, consistent kernel two-sample test", "author": ["A. Gretton", "K. Fukumizu", "Z. Harchaoui", "B.K. Sriperumbudur"], "venue": "NIPS.", "citeRegEx": "Gretton et al\\.,? 2009", "shortCiteRegEx": "Gretton et al\\.", "year": 2009}, {"title": "Learning with distance substitution kernels", "author": ["B. Haasdonk", "C. Bahlmann"], "venue": "Pattern Recognition: 26th DAGM Symposium, 220\u2013227.", "citeRegEx": "Haasdonk and Bahlmann,? 2004", "shortCiteRegEx": "Haasdonk and Bahlmann", "year": 2004}, {"title": "The No-U-Turn Sampler: Adaptively setting path lengths in Hamiltonian Monte Carlo", "author": ["M.D. Hoffman", "A. Gelman"], "venue": "JMLR 15(1):1593\u20131623.", "citeRegEx": "Hoffman and Gelman,? 2014", "shortCiteRegEx": "Hoffman and Gelman", "year": 2014}, {"title": "Exploiting generative models in discriminative classifiers", "author": ["T. Jaakkola", "D. Haussler"], "venue": "NIPS.", "citeRegEx": "Jaakkola and Haussler,? 1998", "shortCiteRegEx": "Jaakkola and Haussler", "year": 1998}, {"title": "Probability product kernels", "author": ["T. Jebara", "R. Kondor", "A. Howard"], "venue": "JMLR 5:819\u2013844.", "citeRegEx": "Jebara et al\\.,? 2004", "shortCiteRegEx": "Jebara et al\\.", "year": 2004}, {"title": "Kernelbased just-in-time learning for passing expectation propagation messages", "author": ["W. Jitkrittum", "A. Gretton", "N. Heess", "S. Eslami", "B. Lakshminarayanan", "D. Sejdinovic", "Z. Szab\u00f3"], "venue": "UAI.", "citeRegEx": "Jitkrittum et al\\.,? 2015", "shortCiteRegEx": "Jitkrittum et al\\.", "year": 2015}, {"title": "A kernel between sets of vectors", "author": ["R. Kondor", "T. Jebara"], "venue": "ICML.", "citeRegEx": "Kondor and Jebara,? 2003", "shortCiteRegEx": "Kondor and Jebara", "year": 2003}, {"title": "Nonparametric estimation of R\u00e9nyi divergence and friends", "author": ["A. Krishnamurthy", "K. Kandasamy", "B. Poczos", "L. Wasserman"], "venue": "ICML.", "citeRegEx": "Krishnamurthy et al\\.,? 2014", "shortCiteRegEx": "Krishnamurthy et al\\.", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "University of Toronto, Tech. Rep.", "citeRegEx": "Krizhevsky and Hinton,? 2009", "shortCiteRegEx": "Krizhevsky and Hinton", "year": 2009}, {"title": "Sparse nonparametric graphical models", "author": ["J. Lafferty", "H. Liu", "L. Wasserman"], "venue": "Statistical Science 27(4):519\u2013537.", "citeRegEx": "Lafferty et al\\.,? 2012", "shortCiteRegEx": "Lafferty et al\\.", "year": 2012}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "CVPR.", "citeRegEx": "Lazebnik et al\\.,? 2006", "shortCiteRegEx": "Lazebnik et al\\.", "year": 2006}, {"title": "Representing and recognizing the visual appearance of materials using three-dimensional textons", "author": ["T. Leung", "J. Malik"], "venue": "IJCV 43.", "citeRegEx": "Leung and Malik,? 2001", "shortCiteRegEx": "Leung and Malik", "year": 2001}, {"title": "Random Fourier approximations for skewed multiplicative histogram kernels", "author": ["F. Li", "C. Ionescu", "C. Sminchisescu"], "venue": "Pattern Recognition: DAGM, 262\u2013271.", "citeRegEx": "Li et al\\.,? 2010", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "Divergence measures based on the Shannon entropy", "author": ["J. Lin"], "venue": "IEEE Trans. Inf. Theory 37(1):145\u2013151.", "citeRegEx": "Lin,? 1991", "shortCiteRegEx": "Lin", "year": 1991}, {"title": "Towards a learning theory of causation", "author": ["D. Lopez-Paz", "K. Muandet", "B. Sch\u00f6lkopf", "I. Tolstikhin"], "venue": "ICML.", "citeRegEx": "Lopez.Paz et al\\.,? 2015", "shortCiteRegEx": "Lopez.Paz et al\\.", "year": 2015}, {"title": "Max-margin additive classifiers for detection", "author": ["S. Maji", "A.C. Berg"], "venue": "ICCV.", "citeRegEx": "Maji and Berg,? 2009", "shortCiteRegEx": "Maji and Berg", "year": 2009}, {"title": "A KullbackLeibler divergence based kernel for SVM classification in multimedia applications", "author": ["P.J. Moreno", "P.P. Ho", "N. Vasconcelos"], "venue": "NIPS.", "citeRegEx": "Moreno et al\\.,? 2003", "shortCiteRegEx": "Moreno et al\\.", "year": 2003}, {"title": "Learning from distributions via support measure machines", "author": ["K. Muandet", "K. Fukumizu", "F. Dinuzzo", "B. Sch\u00f6lkopf"], "venue": "NIPS.", "citeRegEx": "Muandet et al\\.,? 2012", "shortCiteRegEx": "Muandet et al\\.", "year": 2012}, {"title": "A machine learning approach for dynamical mass measurements of galaxy clusters", "author": ["M. Ntampaka", "H. Trac", "D.J. Sutherland", "N. Battaglia", "B. P\u00f3czos", "J. Schneider"], "venue": "The Astrophysical Journal 803(2):50.", "citeRegEx": "Ntampaka et al\\.,? 2015", "shortCiteRegEx": "Ntampaka et al\\.", "year": 2015}, {"title": "Fast distribution to real regression", "author": ["J.B. Oliva", "W. Neiswanger", "B. P\u00f3czos", "J. Schneider", "E. Xing"], "venue": "AISTATS.", "citeRegEx": "Oliva et al\\.,? 2014", "shortCiteRegEx": "Oliva et al\\.", "year": 2014}, {"title": "Distribution to distribution regression", "author": ["J.B. Oliva", "B. P\u00f3czos", "J. Schneider"], "venue": "ICML.", "citeRegEx": "Oliva et al\\.,? 2013", "shortCiteRegEx": "Oliva et al\\.", "year": 2013}, {"title": "Distribution-free distribution regression", "author": ["B. P\u00f3czos", "A. Rinaldo", "A. Singh", "L. Wasserman"], "venue": "AISTATS.", "citeRegEx": "P\u00f3czos et al\\.,? 2012a", "shortCiteRegEx": "P\u00f3czos et al\\.", "year": 2012}, {"title": "Nonparametric kernel estimators for image classification", "author": ["B. P\u00f3czos", "L. Xiong", "D.J. Sutherland", "J. Schneider"], "venue": "CVPR.", "citeRegEx": "P\u00f3czos et al\\.,? 2012b", "shortCiteRegEx": "P\u00f3czos et al\\.", "year": 2012}, {"title": "Random features for largescale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "NIPS.", "citeRegEx": "Rahimi and Recht,? 2007", "shortCiteRegEx": "Rahimi and Recht", "year": 2007}, {"title": "Estimating the dimension of a model", "author": ["G. Schwarz"], "venue": "Ann. Statist. 6(2):461\u2013464.", "citeRegEx": "Schwarz,? 1978", "shortCiteRegEx": "Schwarz", "year": 1978}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR.", "citeRegEx": "Simonyan and Zisserman,? 2015", "shortCiteRegEx": "Simonyan and Zisserman", "year": 2015}, {"title": "Ensemble estimators for multivariate entropy estimation", "author": ["K. Sricharan", "D. Wei", "Hero, III, A.O."], "venue": "IEEE Trans. Inf. Theory 59:4374\u20134388.", "citeRegEx": "Sricharan et al\\.,? 2013", "shortCiteRegEx": "Sricharan et al\\.", "year": 2013}, {"title": "On the error of random Fourier features", "author": ["D.J. Sutherland", "J. Schneider"], "venue": "UAI.", "citeRegEx": "Sutherland and Schneider,? 2015", "shortCiteRegEx": "Sutherland and Schneider", "year": 2015}, {"title": "Two-stage sampled learning theory on distributions", "author": ["Z. Szab\u00f3", "A. Gretton", "B. P\u00f3czos", "B. Sriperumbudur"], "venue": "AISTATS.", "citeRegEx": "Szab\u00f3 et al\\.,? 2015", "shortCiteRegEx": "Szab\u00f3 et al\\.", "year": 2015}, {"title": "VLFeat: An open and portable library of computer vision algorithms", "author": ["A. Vedaldi", "B. Fulkerson"], "venue": "http://www.vlfeat.org/.", "citeRegEx": "Vedaldi and Fulkerson,? 2008", "shortCiteRegEx": "Vedaldi and Fulkerson", "year": 2008}, {"title": "Efficient additive kernels via explicit feature maps", "author": ["A. Vedaldi", "A. Zisserman"], "venue": "CVPR.", "citeRegEx": "Vedaldi and Zisserman,? 2010", "shortCiteRegEx": "Vedaldi and Zisserman", "year": 2010}, {"title": "Generalized RBF feature maps for efficient detection", "author": ["S. Vempati", "A. Vedaldi", "A. Zisserman", "C.V. Jawahar"], "venue": "British Machine Vision Conference.", "citeRegEx": "Vempati et al\\.,? 2010", "shortCiteRegEx": "Vempati et al\\.", "year": 2010}, {"title": "Divergence estimation for multidimensional densities via k-nearest-neighbor distances", "author": ["Q. Wang", "S.R. Kulkarni", "S. Verd\u00fa"], "venue": "IEEE Trans. Inf. Theory 55(5):2392\u20132405.", "citeRegEx": "Wang et al\\.,? 2009", "shortCiteRegEx": "Wang et al\\.", "year": 2009}, {"title": "Using the Nystr\u00f6m method to speed up kernel machines", "author": ["C.K.I. Williams", "M. Seeger"], "venue": "NIPS.", "citeRegEx": "Williams and Seeger,? 2001", "shortCiteRegEx": "Williams and Seeger", "year": 2001}, {"title": "Visual recognition using directional distribution distance", "author": ["J. Wu", "B.-B. Gao", "G. Liu"], "venue": null, "citeRegEx": "Wu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2015}, {"title": "Learning deep features for scene recognition using Places database", "author": ["B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "NIPS.", "citeRegEx": "Zhou et al\\.,? 2014", "shortCiteRegEx": "Zhou et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 26, "context": "The mass of dark matter halos can be inferred from the velocity of galaxies in a cluster (Ntampaka et al. 2015).", "startOffset": 89, "endOffset": 111}, {"referenceID": 13, "context": "Expensive expectation propagation messages can be sped up by learning a \u201cjust-in-time\u201d regression model (Jitkrittum et al. 2015).", "startOffset": 104, "endOffset": 128}, {"referenceID": 29, "context": "For example, regression from distributional covariates to real or distributional responses is possible via kernel smoothing (P\u00f3czos et al. 2012a; Oliva, P\u00f3czos, and Schneider 2013), and many learning tasks can be solved with RKHS approaches (Muandet et al.", "startOffset": 124, "endOffset": 180}, {"referenceID": 25, "context": "2012a; Oliva, P\u00f3czos, and Schneider 2013), and many learning tasks can be solved with RKHS approaches (Muandet et al. 2012; P\u00f3czos et al. 2012b).", "startOffset": 102, "endOffset": 144}, {"referenceID": 30, "context": "2012a; Oliva, P\u00f3czos, and Schneider 2013), and many learning tasks can be solved with RKHS approaches (Muandet et al. 2012; P\u00f3czos et al. 2012b).", "startOffset": 102, "endOffset": 144}, {"referenceID": 13, "context": "Expensive expectation propagation messages can be sped up by learning a \u201cjust-in-time\u201d regression model (Jitkrittum et al. 2015). All of these applications are aided by working directly over sets drawn from the distribution of interest, rather than having to develop a per-problem ad-hoc set of summary statistics. \u2217These two authors contributed equally. Distributions are inherently infinite-dimensional objects, since in general they require an infinite number of parameters for their exact representation. Hence, it is not immediate how to extend traditional finite vector technique machine learning techniques to distributional instances. However, recent work has provided various approaches for dealing with distributional data in a nonparametric fashion. For example, regression from distributional covariates to real or distributional responses is possible via kernel smoothing (P\u00f3czos et al. 2012a; Oliva, P\u00f3czos, and Schneider 2013), and many learning tasks can be solved with RKHS approaches (Muandet et al. 2012; P\u00f3czos et al. 2012b). A major shortcoming of both approaches is that they require computing N kernel evaluations per prediction, where N is the number of training instances in a dataset. Often, this implies that one must compute a N \u00d7 N Gram matrix of pairwise kernel evaluations. Such approaches fail to scale to datasets where the number of instances N is very large. Another shortcoming of these approaches is that they are often based on Euclidean metrics, either working over a linear kernel, or one based on the L2 distance over distributions. While such kernels are useful in certain applications, better performance can sometimes be obtained by considering non-Euclidean based kernels. To this end, P\u00f3czos et al. (2012b) use a kernel based on R\u00e9nyi divergences; however, this kernel is not positive semi-definite (PSD), leading to even higher computational cost and other practical issues.", "startOffset": 105, "endOffset": 1753}, {"referenceID": 19, "context": "Learning on distributions In computer vision, the popular \u201cbag of words\u201d model (Leung and Malik 2001) represents a distribution by quantizing it onto codewords (usually by running k-means on all, or many, of the input points from all sets), then compares those histograms with some kernel (often exponentiated \u03c7).", "startOffset": 79, "endOffset": 101}, {"referenceID": 11, "context": "Another approach estimates a distance between distributions, often the L2 distance or Kullback-Leibler (KL) divergence, parametrically (Jaakkola and Haussler 1998; Moreno, Ho, and Vasconcelos 2003; Jebara, Kondor, and Howard 2004) or nonparametrically (Sricharan, Wei, and Hero 2013; Krishnamurthy et al.", "startOffset": 135, "endOffset": 230}, {"referenceID": 15, "context": "Another approach estimates a distance between distributions, often the L2 distance or Kullback-Leibler (KL) divergence, parametrically (Jaakkola and Haussler 1998; Moreno, Ho, and Vasconcelos 2003; Jebara, Kondor, and Howard 2004) or nonparametrically (Sricharan, Wei, and Hero 2013; Krishnamurthy et al. 2014).", "startOffset": 252, "endOffset": 310}, {"referenceID": 29, "context": "The distance can then be used in kernel smoothing (P\u00f3czos et al. 2012a; Oliva, P\u00f3czos, and Schneider 2013) or Mercer kernels (Moreno, Ho, and Vasconcelos 2003; Kondor and Jebara 2003; Jebara, Kondor, and Howard 2004; P\u00f3czos et al.", "startOffset": 50, "endOffset": 106}, {"referenceID": 14, "context": "2012a; Oliva, P\u00f3czos, and Schneider 2013) or Mercer kernels (Moreno, Ho, and Vasconcelos 2003; Kondor and Jebara 2003; Jebara, Kondor, and Howard 2004; P\u00f3czos et al. 2012b).", "startOffset": 60, "endOffset": 172}, {"referenceID": 30, "context": "2012a; Oliva, P\u00f3czos, and Schneider 2013) or Mercer kernels (Moreno, Ho, and Vasconcelos 2003; Kondor and Jebara 2003; Jebara, Kondor, and Howard 2004; P\u00f3czos et al. 2012b).", "startOffset": 60, "endOffset": 172}, {"referenceID": 3, "context": "Typical approaches involve eigendecomposing the Gram matrix, which usually costs O(N) computation and also presents challenges for traditional inductive learning, where the test points are not known at training time (Chen et al. 2009).", "startOffset": 216, "endOffset": 234}, {"referenceID": 41, "context": "One way to alleviate the scaling problem is the Nystr\u00f6m extension (Williams and Seeger 2001), in which some columns of the Gram matrix are used to estimate the remainder.", "startOffset": 66, "endOffset": 92}, {"referenceID": 1, "context": "In practice, one frequently must compute many columns, and methods to make the result PSD are known only for mildly-indefinite kernels (Belongie et al. 2002).", "startOffset": 135, "endOffset": 157}, {"referenceID": 8, "context": "The RKHS inner product is known as the mean map kernel (MMK), and the distance the maximum mean discrepancy (MMD) (Gretton et al. 2009; Muandet et al. 2012; Szab\u00f3 et al. 2015).", "startOffset": 114, "endOffset": 175}, {"referenceID": 25, "context": "The RKHS inner product is known as the mean map kernel (MMK), and the distance the maximum mean discrepancy (MMD) (Gretton et al. 2009; Muandet et al. 2012; Szab\u00f3 et al. 2015).", "startOffset": 114, "endOffset": 175}, {"referenceID": 36, "context": "The RKHS inner product is known as the mean map kernel (MMK), and the distance the maximum mean discrepancy (MMD) (Gretton et al. 2009; Muandet et al. 2012; Szab\u00f3 et al. 2015).", "startOffset": 114, "endOffset": 175}, {"referenceID": 31, "context": "Approximate embeddings Recent interest in approximate kernel embeddings was spurred by the \u201crandom kitchen sink\u201d (RKS) embedding (Rahimi and Recht 2007), which approximates shift-invariant kernelsK on R by sampling their Fourier transform.", "startOffset": 129, "endOffset": 152}, {"referenceID": 13, "context": "An embedding for the base kernel k also gives a simple embedding for the mean map kernel (Flaxman, Wang, and Smola 2015; Jitkrittum et al. 2015; Lopez-Paz et al. 2015; Sutherland and Schneider 2015).", "startOffset": 89, "endOffset": 198}, {"referenceID": 22, "context": "An embedding for the base kernel k also gives a simple embedding for the mean map kernel (Flaxman, Wang, and Smola 2015; Jitkrittum et al. 2015; Lopez-Paz et al. 2015; Sutherland and Schneider 2015).", "startOffset": 89, "endOffset": 198}, {"referenceID": 35, "context": "An embedding for the base kernel k also gives a simple embedding for the mean map kernel (Flaxman, Wang, and Smola 2015; Jitkrittum et al. 2015; Lopez-Paz et al. 2015; Sutherland and Schneider 2015).", "startOffset": 89, "endOffset": 198}, {"referenceID": 19, "context": "Approximate embeddings Recent interest in approximate kernel embeddings was spurred by the \u201crandom kitchen sink\u201d (RKS) embedding (Rahimi and Recht 2007), which approximates shift-invariant kernelsK on R by sampling their Fourier transform. A related line of work considers additive kernels, of the form K(x, y) = \u2211` j=1 \u03ba(xj , yj), usually defined on R\u22650 (e.g. histograms). Maji and Berg (2009) construct an embedding for the intersection kernel \u2211` j=1 min(xj , yj) via step functions.", "startOffset": 210, "endOffset": 395}, {"referenceID": 19, "context": "Approximate embeddings Recent interest in approximate kernel embeddings was spurred by the \u201crandom kitchen sink\u201d (RKS) embedding (Rahimi and Recht 2007), which approximates shift-invariant kernelsK on R by sampling their Fourier transform. A related line of work considers additive kernels, of the form K(x, y) = \u2211` j=1 \u03ba(xj , yj), usually defined on R\u22650 (e.g. histograms). Maji and Berg (2009) construct an embedding for the intersection kernel \u2211` j=1 min(xj , yj) via step functions. Vedaldi and Zisserman (2010) consider any homogeneous \u03ba, so that \u03ba(tx, ty) = t \u03ba(x, y), which also allows them to embed histogram kernels such as the additive \u03c7 kernel and Jensen-Shannon divergence.", "startOffset": 210, "endOffset": 515}, {"referenceID": 6, "context": "Their embedding uses the same fundamental result of Fuglede (2005) as ours; we expand to the continuous rather than the discrete case.", "startOffset": 52, "endOffset": 67}, {"referenceID": 6, "context": "Their embedding uses the same fundamental result of Fuglede (2005) as ours; we expand to the continuous rather than the discrete case. Vempati et al. (2010) later apply RKS embeddings to obtain generalized RBF kernels (1).", "startOffset": 52, "endOffset": 157}, {"referenceID": 6, "context": "Their embedding uses the same fundamental result of Fuglede (2005) as ours; we expand to the continuous rather than the discrete case. Vempati et al. (2010) later apply RKS embeddings to obtain generalized RBF kernels (1). For embeddings of kernels on input spaces other than R, the RKS embedding extends naturally to locally compact abelian groups (Li, Ionescu, and Sminchisescu 2010). Oliva et al. (2014) embedded an estimate of the L2 distance between continuous densities via orthonormal basis functions.", "startOffset": 52, "endOffset": 407}, {"referenceID": 6, "context": "d is a Hilbertian metric (Fuglede 2005), so K is positive definite (Haasdonk and Bahlmann 2004).", "startOffset": 25, "endOffset": 39}, {"referenceID": 9, "context": "d is a Hilbertian metric (Fuglede 2005), so K is positive definite (Haasdonk and Bahlmann 2004).", "startOffset": 67, "endOffset": 95}, {"referenceID": 6, "context": "Embedding HDDs into L2 Fuglede (2005) shows that \u03ba corresponds to a bounded measure \u03bc(\u03bb), as in Table 1, with", "startOffset": 23, "endOffset": 38}, {"referenceID": 31, "context": "Embedding RBF Kernels into RD The A features approximate the HDD (2) in R |V |; thus applying the RKS embedding (Rahimi and Recht 2007) to the A features will approximate our generalized RBF kernel (1).", "startOffset": 112, "endOffset": 135}, {"referenceID": 35, "context": "There are two versions of the embedding in common use, but this one is preferred (Sutherland and Schneider 2015).", "startOffset": 81, "endOffset": 112}, {"referenceID": 28, "context": "Taking |V | to be asymptotically O(n), ne = O(D), and M = O(1) for simplicity, this is O(NnD) time, compared to O(Nn log n+N) for the methods of P\u00f3czos et al. (2012b) and O(Nn) for Muandet et al.", "startOffset": 145, "endOffset": 167}, {"referenceID": 25, "context": "(2012b) and O(Nn) for Muandet et al. (2012).", "startOffset": 22, "endOffset": 44}, {"referenceID": 7, "context": "We use a suitable form of kernel density estimation, to obtain a uniform error bound with a rate based on the function C\u22121 (Gin\u00e9 and Guillou 2002).", "startOffset": 123, "endOffset": 146}, {"referenceID": 25, "context": "We also try the MMD distance (Muandet et al. 2012) with approximate kernel embeddings:", "startOffset": 29, "endOffset": 50}, {"referenceID": 21, "context": "First, we try our JS, Hellinger, and TV embeddings. We compare to L2 kernels as in Oliva et al. (2014): exp ( \u2212 1 2\u03c32 \u2016p\u2212 q\u2016 2 2 ) \u2248 z(~a(p\u0302))z(~a(q\u0302)) (L2).", "startOffset": 25, "endOffset": 103}, {"referenceID": 39, "context": "We further compare to RKS with histogram JS embeddings (Vempati et al. 2010) (Hist JS); we also tried \u03c7 embeddings, but their performance was quite similar.", "startOffset": 55, "endOffset": 76}, {"referenceID": 28, "context": "We finally try the full Gram matrix approach of P\u00f3czos et al. (2012b) with the KL estimator of Wang, Kulkarni, and Verd\u00fa (2009) in an RBF kernel (KL), as did Ntampaka et al.", "startOffset": 48, "endOffset": 70}, {"referenceID": 28, "context": "We finally try the full Gram matrix approach of P\u00f3czos et al. (2012b) with the KL estimator of Wang, Kulkarni, and Verd\u00fa (2009) in an RBF kernel (KL), as did Ntampaka et al.", "startOffset": 48, "endOffset": 128}, {"referenceID": 26, "context": "(2012b) with the KL estimator of Wang, Kulkarni, and Verd\u00fa (2009) in an RBF kernel (KL), as did Ntampaka et al. (2015).", "startOffset": 96, "endOffset": 119}, {"referenceID": 27, "context": "Estimating the Number of Mixture Components We will now illustrate the efficacy of HDD random features in a regression task, following Oliva et al. (2014): estimate the number of components from a mixture of truncated Gaussians.", "startOffset": 135, "endOffset": 155}, {"referenceID": 0, "context": "Note that fitting mixtures with EM and selecting a number of components using AIC (Akiake 1973) or BIC (Schwarz 1978) performed much worse than regression; only AIC with |\u03c7i| = 800 outperformed a constant predictor of 5.", "startOffset": 82, "endOffset": 95}, {"referenceID": 32, "context": "Note that fitting mixtures with EM and selecting a number of components using AIC (Akiake 1973) or BIC (Schwarz 1978) performed much worse than regression; only AIC with |\u03c7i| = 800 outperformed a constant predictor of 5.", "startOffset": 103, "endOffset": 117}, {"referenceID": 37, "context": "Note that the histogram embeddings used an optimized C implementation (Vedaldi and Fulkerson 2008), as did the KL kernel2, while the HDD embeddings used a simple Matlab implementation.", "startOffset": 70, "endOffset": 98}, {"referenceID": 16, "context": "We took the \u201ccat\u201d and \u201cdog\u201d classes from the CIFAR-10 dataset (Krizhevsky and Hinton 2009), and represented each 32 \u00d7 32 image by a set of triples (x, y, v), where x and y are the position of each pixel in the image and v the pixel value after converting to grayscale.", "startOffset": 62, "endOffset": 90}, {"referenceID": 21, "context": "Scene Classification Modern computer vision classification systems typically consist of a deep network with several convolutional and pooling layers to extract complex features of input images, followed by one or two fully-connected classification layers. The activations are of shape n\u00d7h\u00d7w, where n is the number of filters; each unit corresponds to an overlapping patch of the original image. We can thus treat the final pooled activations as a sample of size hw from an n-dimensional distribution, similarly to how P\u00f3czos et al. (2012b) and Muandet et al.", "startOffset": 137, "endOffset": 540}, {"referenceID": 21, "context": "Scene Classification Modern computer vision classification systems typically consist of a deep network with several convolutional and pooling layers to extract complex features of input images, followed by one or two fully-connected classification layers. The activations are of shape n\u00d7h\u00d7w, where n is the number of filters; each unit corresponds to an overlapping patch of the original image. We can thus treat the final pooled activations as a sample of size hw from an n-dimensional distribution, similarly to how P\u00f3czos et al. (2012b) and Muandet et al. (2012) used SIFT features from image patches.", "startOffset": 137, "endOffset": 566}, {"referenceID": 21, "context": "Scene Classification Modern computer vision classification systems typically consist of a deep network with several convolutional and pooling layers to extract complex features of input images, followed by one or two fully-connected classification layers. The activations are of shape n\u00d7h\u00d7w, where n is the number of filters; each unit corresponds to an overlapping patch of the original image. We can thus treat the final pooled activations as a sample of size hw from an n-dimensional distribution, similarly to how P\u00f3czos et al. (2012b) and Muandet et al. (2012) used SIFT features from image patches. Wu, Gao, and Liu (2015) set accuracy records on several scene classification datasets with a particular ad-hoc method of extracting features from distributions (D3); we compare to our more principled alternatives.", "startOffset": 137, "endOffset": 629}, {"referenceID": 33, "context": "We consider the Scene-15 dataset (Lazebnik, Schmid, and Ponce 2006), which contains 4 485 natural images in 15 location categories, and follow Wu, Gao, and Liu in extracting features from the last convolutional layer of the imagenet-vgg-verydeep-16 model (Simonyan and Zisserman 2015).", "startOffset": 255, "endOffset": 284}, {"referenceID": 43, "context": "48, which trained on over 7 million external images (Zhou et al. 2014).", "startOffset": 52, "endOffset": 70}], "year": 2015, "abstractText": "Many interesting machine learning problems are best posed by considering instances that are distributions, or sample sets drawn from distributions. Previous work devoted to machine learning tasks with distributional inputs has done so through pairwise kernel evaluations between pdfs (or sample sets). While such an approach is fine for smaller datasets, the computation of an N \u00d7N Gram matrix is prohibitive in large datasets. Recent scalable estimators that work over pdfs have done so only with kernels that use Euclidean metrics, like the L2 distance. However, there are a myriad of other useful metrics available, such as total variation, Hellinger distance, and the Jensen-Shannon divergence. This work develops the first random features for pdfs whose dot product approximates kernels using these non-Euclidean metrics, allowing estimators using such kernels to scale to large datasets by working in a primal space, without computing large Gram matrices. We provide an analysis of the approximation error in using our proposed random features and show empirically the quality of our approximation both in estimating a Gram matrix and in solving learning tasks in real-world and synthetic data. Introduction As machine learning matures, focus has shifted towards datasets with richer, more complex instances. For example, a great deal of effort has been devoted to learning functions on vectors of a large fixed dimension. While complex static vector instances are useful in a myriad of applications, many machine learning problems are more naturally posed by considering instances that are distributions, or sets drawn from distributions. Political scientists can learn a function from community demographics to vote percentages to understand who supports a candidate (Flaxman, Wang, and Smola 2015). The mass of dark matter halos can be inferred from the velocity of galaxies in a cluster (Ntampaka et al. 2015). Expensive expectation propagation messages can be sped up by learning a \u201cjust-in-time\u201d regression model (Jitkrittum et al. 2015). All of these applications are aided by working directly over sets drawn from the distribution of interest, rather than having to develop a per-problem ad-hoc set of summary statistics. \u2217These two authors contributed equally. Distributions are inherently infinite-dimensional objects, since in general they require an infinite number of parameters for their exact representation. Hence, it is not immediate how to extend traditional finite vector technique machine learning techniques to distributional instances. However, recent work has provided various approaches for dealing with distributional data in a nonparametric fashion. For example, regression from distributional covariates to real or distributional responses is possible via kernel smoothing (P\u00f3czos et al. 2012a; Oliva, P\u00f3czos, and Schneider 2013), and many learning tasks can be solved with RKHS approaches (Muandet et al. 2012; P\u00f3czos et al. 2012b). A major shortcoming of both approaches is that they require computing N kernel evaluations per prediction, where N is the number of training instances in a dataset. Often, this implies that one must compute a N \u00d7 N Gram matrix of pairwise kernel evaluations. Such approaches fail to scale to datasets where the number of instances N is very large. Another shortcoming of these approaches is that they are often based on Euclidean metrics, either working over a linear kernel, or one based on the L2 distance over distributions. While such kernels are useful in certain applications, better performance can sometimes be obtained by considering non-Euclidean based kernels. To this end, P\u00f3czos et al. (2012b) use a kernel based on R\u00e9nyi divergences; however, this kernel is not positive semi-definite (PSD), leading to even higher computational cost and other practical issues. This work addresses these major shortcomings by developing an embedding of random features for distributions. The dot product of the random features for two distributions will approximate kernels based on various distances between densities (see Figure 1). With this technique, we can approximate kernels based on total variation, Hellinger, and Jensen-Shannon divergences, among others. Since there is then no need to compute a Gram matrix, one will be able to use these kernels while still scaling to datasets with a", "creator": "TeX"}}}