{"id": "1509.07728", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Sep-2015", "title": "Online Stochastic Linear Optimization under One-bit Feedback", "abstract": "In this paper, we study a special bandit setting of online stochastic linear optimization, where only one-bit of information is revealed to the learner at each round. This problem has found many applications including online advertisement and online recommendation. We assume the binary feedback is a random variable generated from the logit model, and aim to minimize the regret defined by the unknown linear function. Although the existing method for generalized linear bandit can be applied to our problem, the high computational cost makes it impractical for real-world problems. To address this challenge, we develop an efficient online learning algorithm by exploiting particular structures of the observation model. Specifically, we adopt online Newton step to estimate the unknown parameter and derive a tight confidence region based on the exponential concavity of the logistic loss. Our analysis shows that the proposed algorithm achieves a regret bound of $O(d\\sqrt{T})$, which matches the optimal result of stochastic linear bandits.", "histories": [["v1", "Fri, 25 Sep 2015 14:05:09 GMT  (21kb)", "http://arxiv.org/abs/1509.07728v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["lijun zhang 0005", "tianbao yang", "rong jin", "yichi xiao", "zhi-hua zhou"], "accepted": true, "id": "1509.07728"}, "pdf": {"name": "1509.07728.pdf", "metadata": {"source": "CRF", "title": "Online Stochastic Linear Optimization under One-bit Feedback", "authors": ["Lijun Zhang", "Tianbao Yang", "Rong Jin"], "emails": ["zhanglj@lamda.nju.edu.cn", "tianbao-yang@uiowa.edu", "rongjin@cse.msu.edu", "zhouzh@lamda.nju.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 150 9.07 728v 1 [cs \u221a T), which is the optimal result of stochastic linear bandits. Keywords: bandit, online, repentance bound, stochastic linear optimization, logit model"}, {"heading": "1. Introduction", "text": "The question of the manner in which the player must decide which of the two types of slot machines he wants to play is in the basic stochastic manner in which each arm is accepted as a reward drawn from a fixed but unknown distribution. The goal of the player is to minimize the regret, namely the difference between his expected cumulative reward and that of the best single arm in hindsight (Auer et al., 2002). Although MAB is a powerful framework for modeling online decision-making problems, it becomes intolerable when the number of weapons is very large or even infinite."}, {"heading": "2. Related Work", "text": "The stochastic, multi-armed bandits (MAB) (Robbins, 1952) became a canonical formalism for investigating the problem of decision-making under uncertainty. A long series of successive problems has been extensively examined in statistics (Berry and Fristedt, 1985) and computer science (Bubeck and Cesa-Bianchi, 2012)."}, {"heading": "2.1 Stochastic Multi-armed Bandits (MAB)", "text": "In their groundbreaking paper, Lai and Robbins (1985) set an asymptotic lower limit of O (K log T) for the expected cumulative regret over T periods, assuming that the expected rewards of the best and second-best arms are well separated. By using the upper confidence limits (UCB), they construct further strategies that reach the lower limit asymptotically. However, this initial algorithm is quite elaborate, since the UCB's calculation is based on the entire sequence of rewards received to date. To accommodate this limitation, Agrawal (1995) introduces a family of simpler strategies that only need to calculate the sample of rewards, and regret retains the optimal logarithmic behavior. A finite time analysis of the stochastic MAB is performed by Auer et al (2002)."}, {"heading": "2.2 Stochastic Linear Bandits (SLB)", "text": "SLB is first examined by Auer (2002), who sees case D as finite. > Although an elegant UCB algorithm called LinRel is being developed, he does not link his regret to independence problems. Instead, he designs a complicated master algorithm that uses LinRel as a subroutine and reaches a remorse limit of \u00d8 (log | D |) 3 / 2 \u221a Td, where | D | is the number of feasible decisions. In a later paper, Dani et al. (2008a) generalize LinRel easily so that it can be applied in environments where D can be infinite. \u2212 They point to the new algorithm as ConfidenceBall2 and show that it has a limit of \u00d8 (d \u00b0 T) that is not dependent on the cardinality of D. Abbasi-yadkori et al al al al al al al al al al al al al al al al al al al al. (2011) improve the theoretical analysis of ConfidenceBall2 by using tools from the normalized processes."}, {"heading": "2.4 Generalized Linear Bandit (GLB)", "text": "Filippi et al. (2010) extend SLB to the non-linear case, which is based on the framework of the Generalized Linear Model of Statistics. In the so-called GLB model, it is assumed that yt fulfills E [yt | xt] = \u00b5 (x't w *), where \u00b5: R 7 \u2192 R is a specific linkage function. Regret is also defined in the form of \u00b5 (\u00b7) and given by T max x D\u00b5 (x'w *) \u2212 T \u2211 t = 1\u00b5 (x't w *). (5) Note that by setting \u00b5 (x) = exp (x) / [1 + exp (x)], the problem considered in this paper becomes a special case of GLB. An UCB-like algorithm has been proposed for GLB and also reaches a regret limit of \u00d8 (d't T T). Unlike ConfidenceBall2, which constructs a trust region in the parameter space, the algorithm of Filippi et al works (2010 only)."}, {"heading": "2.5 Adversarial Setting", "text": "All of the above results are found in the stochastic environment in which the reward of each arm is generated from an unknown but fixed distribution; a more general environment is the opposing case, in which the reward from each arm can be arbitrarily modified (Bubeck and Cesa-Bianchi, 2012); the best known method for opposing multi-armed bandits is the Exp3 algorithm, which reaches a repentance limit of \u00d8 (\u221a KT) (Auer et al., 2003); the problem of opposing linear bandits has been extensively investigated, and the start-of-the-art repentance bound is \u00d8 (poly (d) \u221a T) (Dani et al., 2008b; Abernethy et al., 2008; Bubeck et al., 2012); for further results see Bubeck and Cesa-Bianchi (2012), Shamir (2013) and references therein."}, {"heading": "2.6 Bandit Learning with One-bit Feedback", "text": "There are several new variants of bandit learning that are also based on one-bit feedback, such as multi-class bandits (Kakade et al., 2008; Chen et al., 2014) and K-armed Dueling bandits (Yue et al., 2009; Ailon et al., 2014). For multi-class bandits, for example, feedback is whether the label predicted is correct or not, and for K-armed Dueling bandits, feedback is the comparison between the rewards of two arms. However, none of them is designed for linear online optimization."}, {"heading": "2.7 One-bit Compressive Sensing (CS)", "text": "Finally, we would like to discuss a closely related work in signal processing - One-Bit Compressive Sensing (CS) (Boufounos and Baraniuk, 2008; Plan and Vershynin, 2013).One-Bit CS aims to recover a sparse W-Z vector from a set of one-bit measurements (Yi) in which Yi is generated from X-Bit, according to certain observational models such as (3).The main difference is that one-bit CS is examined in batch processing with the aim of minimizing recovery error, while our problem is investigated in online setting to minimize regrettableness.3. Online Learning for Logit Model (OL2M) We first describe the proposed algorithm for online stochastic linear optimization in the face of onbit feedback, then compare it with existing methods, then present its theoretical guarantees and finally discuss implementation problems."}, {"heading": "3.1 The Algorithm", "text": "For a positive definition of matrix A-Rd \u00b7 d, the weighted 2-norm is defined by the way in which we define the weighted 2-norm. (1) We further assume that the 2-norm of the 1-norm of the 1-norm of the 1-norm of the 2-norm of the 1-norm of the 2-norm of the 1-norm of the 1-norm of the 1-norm of the 1-norm of the 1-norm of the 2-norm of the 2-norm of the 1-norm of the 2-norm of the 2-norm of the 2-norm of the 2-norm of the 2-norm of the 2-norm of the 2-norm of the 2-norm of the 2-norm of the 2-norm of the 2-norm of the 1-1-1-1 norm of the 1-1-1-1 norm of the 1-1-1-1 norm of the 1-1-1-1 norm of the 1-1-1 norm of the 1-1-1-1 norm of the 1-1-1 norm of the 1-1-1 norm of the 1-1-1 norm of the 1-1-1 norm of the 1-1-1 norm of the 1-1-1 norm of the 1-1-1 norm of the 1-1-1 norm of the 1-1 norm of the 1-1-1 norm of the 1-1-1 norm of the 1-1-1 norm of the 1-1 norm of the 1-1-1 norm of the 1-1-1 norm of the 1-1 norm of the 1-1-1 norm of the 1-1 norm of the 1-1 norm of the 1-1 norm of the 1-1 norm of the 1-1 norm of the 1-1 norm of the 1-1-1-1 norm of the 1 norm of the 1-1 norm of the 1-1-1-1 norm of the 1-1-1 norm of the 1-1-1 norm of the 1-"}, {"heading": "3.2 Theoretical Guarantees", "text": "Theorem 1 (4\u03b2 + 8 3 R) (4R + (4\u03b2 + 8 3 R) (4\u03b2 + 1\u03b2 logdet (Zt + 1) det (Z1)] + max (\u03bb, \u03b7\u03b22) R2, (12) \u03c4t = log (2 2 2 log2 t-t2\u03b4), (13) \u03b2 = 12 (1 + exp (R). (14) The basic idea is to analyze the growth of the log (wt + 1 \u2212 w) 2Zt + 1 by examining the properties of logistic loss (Lemmas 2 and 4) and concentration inequalities for martyrs (Lemma 5)."}, {"heading": "3.3 Implementation Issues", "text": "The main calculation costs of OL2M come from (11), which is NP-hard in general (Dani et al., 2008a). Below, we discuss several strategies for reducing the calculation costs. (11) In this context, we rewrite the optimization problem in (11), that D is the unit ball, (11) could be solved in time O (poly (d). (11) In this context, we can rewrite the optimization problem in (11) as the following problem: D is the unit ball, (11) could be solved in time O (poly (d). (11) In time Zt + 1), there may be an explanation that uses techniques from convex optimization. (11) In time, we can rewrite the optimization problem in (11) as the following problem. (11) In time, we rewrite the optimization problem in (11)."}, {"heading": "4. Analysis", "text": "Here we present the proofs of the most important theorems. The omitted proofs are included in the appendix."}, {"heading": "4.1 Proof of Theorem 1", "text": "We begin with several Lemmas which play a central role in our analyses. \u2212 SR (v2) VS (v2) VS (v2) VS (v2) VS (v2) VS (v2) VS (v2) VS (v2) VS (v2) VS (v2) VS (v2) VS (v2) VS (v2) VS (v2) VS (v2) VS (v2) VS (v2) VS (v2) VS (v2) VS (v2) VS (v2) VS (v2) VS (v2) VS (v2) VS (v2) VS (v2) VS (v2) VS (vS) VS (v2) VS (v2) VS (vS) VS (v2) VS (v2) VS (v2) VS (v2) VS (v2) VS (v2) VS (v2) VS (VS) VS (v2) VS (v2) VS (v2) VS (v2) VS (v2) VS (v2) VS (VS) VS (v2) VS (v2) VS (v2) VS (v2) VS (VS) VS (v2) VS (VS (v2) VS (v2) VS (VS (v2) VS (VS (v2) VS (VS (v2) VS (v2) VS (v2) VS (VS (v2) VS (VS (v2) VS (v2) VS (v2) VS (VS (v2) VS (v2) VS (VS (v2) VS (v2) VS (v2) VS (v2) VS (VS (v2) VS (v2) VS (v2) VS (v2) VS (v2) VS (v2) VS (v2) VS (v2) VS (V"}, {"heading": "4.2 Proof of Lemma 2", "text": "We first show that the one-dimensional logistic loss (x) = log (1 + exp (\u2212 x)) is 12 (1 + exp (R) - strongly convex over the domain [\u2212 R, R]. It is easy to prove that the strong convexity of (\u00b7), (x) = exp (x) (1 + exp (x) implies 2 (1 + exp (R). From the property strongly convex it follows that we have for each a, b) t (R, R) (a) + (a) (b \u2212 a) + \u03b2 2 (b \u2212 a) 2. (22) Note that we have for each w1, w2 t () t t, ytx t t w2 (), ytx t w2 t wyt () and wyt wyt (wyt)."}, {"heading": "4.3 Proof of Lemma 3", "text": "Lemma 3 follows from a more general result given below. \u2212 Lemma 7 Let M be a positive definitive matrix, andy = argmin w \u00b2 W \u00b2 < w, g > + 1 2% w \u2212 x \u2212 2M, where W is a convex quantity. Then, for all w \u00b2 W < x \u2212 w, g > x \u2212 w \u00b2 2 m \u2212 y \u2212 w \u00b2 x \u00b2, w \u00b2 g \u00b2 2M \u2212 1. Proof Since y is the optimal solution to the optimization problem of the first order, we have obtained from the optimal state (Boyd and Vandenberghe, 2004) < g + M (y \u2212 x), w \u2212 y > 0, w \u00b2 W. (23) Due to the above inequality, we have the optimal solution to the optimization problem (Boyd and Vandenberghe, 2004): x \u2212 w \u00b2 m (y \u2212 x), p \u00b2 g \u2212 W \u00b2 \u2212 W (23)."}, {"heading": "4.4 Proof of Lemma 4", "text": "For each W-round we introduce a discrete probability distribution pw over {\u00b1 1}, such as thatpw (i) = 11 + exp (\u2212 ix't w), i = {\u00b1 1}. Then it is easy to check if f-t (w) = \u2212 p-1) pw (i) pw (i) log pw (i).As a result, pw (i) \u2212 f-t (w) = [i) pw (i) pw (i) pw (i) log pw (i) \u2212 p-1) pw (i) log pw (i) = [i) logpw (i) pw (i) pw (i) = DKL (pw), where DKL (\u00b7) is the Kullback Leibler divergence between two distributions (Cover and Thomas, 2006)."}, {"heading": "4.5 Proof of Lemma 5", "text": "We need Bernstein's inequality for Martingales (Cesa-Bianchi and Lugosi, 2006), which is in Appendix D. Form our definition of f-i (\u00b7) in (16), it is clear bi = [\u2264 f-i (wi) \u2212 fi (wi)] (wi-w) (wi-v) is a martyrdom difference order. In addition, the definition of f-i (wi-v) v i (wi-v) i (wi-v) i (wi-v) fi (wi-v) fi (wi-v) i (wi-v) i (wi-v) i (wi-v) i (wi-v) v i (wi-v) fi (wi) fi (wi-v) fi (wi-v-i) (i] wi-fi (i] v-i) (wi-i) (wi-v) (i) wi-i)"}, {"heading": "4.6 Proof of Theorem 3", "text": "The proof is standard and can be found by Dani et al. (2008a) and Abbasi-yadkori et al. (2011). We include it for the sake of completeness. Let us remember that in each round (xt, w, t) = argmax x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x, T x x x x x x x x x x x, T x x x x x x x x x x x x x, T x x x x x x x x x, T x x x x x x x x x x x x, T x x x x x x x x x x x x x x x x x, T x x x x x x x x x x x x x x x x x x x x x x x x x x x x, T x x x x x x x x x x x x x x x x x x, T x x x x x x x x x x x, T x x x x x x x x x x, T x x x x x x x x x x, T, T x x x x x x, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T Z, T Z, T, T Z Z, T, T, T, T, T, T, Z, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, Z, T, T, T, T, T, T, T, T, T, T, T, T, T., Z, T., T, T., T., T."}, {"heading": "5. Conclusions", "text": "In this paper, we look at the problem of linear online optimization using single-bit feedback. Assuming that the binary feedback is generated from the Logit model, we develop a variant of the online Newton step to approximate the unknown vector, and discuss how the confidence region can be constructed theoretically. Considering the confidence region, we select the action that produces the maximum reward in each round. Theoretical analyses show that our algorithm reaches a repentance limit of \u00d8 (d'T. The current algorithm assumes that the one-bit feedback is generated from a Logit model. In contrast, a much broader class of observation models is permissible in the one-bit compression measurement (Plan and Vershynin, 2013) as long as there is a positive correlation between the one-bit output and the real-rated measurement. In the future, we will examine how we can extend our algorithm to other observation models."}, {"heading": "Appendix A. Proof of Lemma 1", "text": "It is easy to check that for each \u2212 R \u2264 a \u2264 b \u2264 R one (b) = \u00b5 (x) + x (x) (1 + exp (x))) (2 \u2264 1 4 (26) Note that for each \u2212 R \u2264 a \u2264 b \u2264 R one (b) = \u00b5 (a) + x) dx (27) Combine (26) with (27) one (1 + exp (R) (b \u2212 a) (b \u2212 \u00b5 (b) \u2212 \u00b5 (a) \u2264 1 4 (b \u2212 a) Letx \u00b2 (D x w) = argmax x w \u00b2 (x w) 1 + exp (x w) 1 + exp (x w) (x w) (x w) (x w) Since \u2212 R \u2264 x \u2212 t \u00b2 w = implicit R, we have 12 (1 + exp (x w) (x w) 1 + exp (x w) (x w) (x p) (x p) (x p) (x p) (x p)."}, {"heading": "Appendix B. Proof of Lemma 6", "text": "2 x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x 2 x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "Appendix C. Proof of Corollary 2", "text": "Remember that Zt + 1 = Z1 + \u03b7\u03b22t \u0445 i = 1xtx tand \u04452 \u2264 1 for all t > 0. From Lemma 10 by Abbasi-yadkori et al. (2011) we have (Zt + 1) \u2264 (\u03bb + \u03b7\u03b2t2d) d. Da det (Z1) = \u03bb d, we have havelog det (Zt + 1) det (Z1) \u2264 d log (1 + throu\u03b2t2\u03b2d)."}, {"heading": "Appendix D. Bernstein\u2019s Inequality for Martingales", "text": "Theorem 4 Let X1,.., Xn be a limited martyrdom difference sequence with respect to filtration F = (Fi) 1 \u2264 i \u2264 n and with | Xi | \u2264 K. LetSi = i \u0445 j = 1Xjbe be be the associated martyrdom. Specify the sum of conditional deviations of \u043f2n = n \u0445 t = 1E [X2t | Ft \u2212 1]. Then for all constants t, \u03bd > 0, Pr [maxi = 1,..., n Si > t and \u04322 n \u2264 \u03bd] \u2264 exp (\u2212 t 22 (\u03bd + Kt / 3)), and therefore Pr [maxi = 1,..., n Si > \u221a 2\u0445t + 23 Kt and \u04322n \u2264 Kong] \u2264 e \u2212."}], "references": [{"title": "Improved algorithms for linear stochastic bandits", "author": ["Yasin Abbasi-yadkori", "D\u00e1vid P\u00e1l", "Csaba Szepesv\u00e1ri"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Abbasi.yadkori et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Abbasi.yadkori et al\\.", "year": 2011}, {"title": "Competing in the dark: An efficient algorithm for bandit linear optimization", "author": ["Jacob Abernethy", "Elad Hazan", "Alexander Rakhlin"], "venue": "In Proceedings of the 21st Annual Conference on Learning,", "citeRegEx": "Abernethy et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2008}, {"title": "Stochastic convex optimization with bandit feedback", "author": ["Alekh Agarwal", "Dean P. Foster", "Daniel Hsu", "Sham M. Kakade", "Alexander Rakhlin"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Agarwal et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2013}, {"title": "Sample mean based index policies with O(log n) regret for the multi-armed bandit problem", "author": ["Rajeev Agrawal"], "venue": "Advances in Applied Probability,", "citeRegEx": "Agrawal.,? \\Q1995\\E", "shortCiteRegEx": "Agrawal.", "year": 1995}, {"title": "Reducing dueling bandits to cardinal bandits", "author": ["Nir Ailon", "Zohar Karnin", "Thorsten Joachims"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Ailon et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ailon et al\\.", "year": 2014}, {"title": "Using confidence bounds for exploitation-exploration trade-offs", "author": ["Peter Auer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Auer.,? \\Q2002\\E", "shortCiteRegEx": "Auer.", "year": 2002}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Paul Fischer"], "venue": "Machine Learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2003}, {"title": "Local rademacher complexities", "author": ["Peter L. Bartlett", "Olivier Bousquet", "Shahar Mendelson"], "venue": "The Annals of Statistics,", "citeRegEx": "Bartlett et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2005}, {"title": "Bandit problems: Sequential Allocation of Experiments", "author": ["Donald A. Berry", "Bert Fristedt"], "venue": "Monographs on Statistics and Applied Probability. Springer Netherlands,", "citeRegEx": "Berry and Fristedt.,? \\Q1985\\E", "shortCiteRegEx": "Berry and Fristedt.", "year": 1985}, {"title": "1-bit compressive sensing", "author": ["Petros T. Boufounos", "Richard G. Baraniuk"], "venue": "In Proceedings of the 42nd Annual Conference on Information Sciences and Systems,", "citeRegEx": "Boufounos and Baraniuk.,? \\Q2008\\E", "shortCiteRegEx": "Boufounos and Baraniuk.", "year": 2008}, {"title": "Convex Optimization", "author": ["Stephen Boyd", "Lieven Vandenberghe"], "venue": null, "citeRegEx": "Boyd and Vandenberghe.,? \\Q2004\\E", "shortCiteRegEx": "Boyd and Vandenberghe.", "year": 2004}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S\u00e9bastien Bubeck", "Nicol\u00f2 Cesa-Bianchi"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bubeck and Cesa.Bianchi.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Cesa.Bianchi.", "year": 2012}, {"title": "Towards minimax policies for online linear optimization with bandit feedback", "author": ["S\u00e9bastien Bubeck", "Nicol\u00f2 Cesa-Bianchi", "Sham M. Kakade"], "venue": "In Proceedings of the 25th Annual Conference on Learning Theory,", "citeRegEx": "Bubeck et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2012}, {"title": "Prediction, Learning, and Games", "author": ["Nicolo Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Boosting with online binary learners for the multiclass bandit problem", "author": ["Shang-Tse Chen", "Hsuan-Tien Lin", "Chi-Jen Lu"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Stochastic linear optimization under bandit feedback", "author": ["Varsha Dani", "Thomas P. Hayes", "Sham M. Kakade"], "venue": "In Proceedings of the 21st Annual Conference on Learning,", "citeRegEx": "Dani et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dani et al\\.", "year": 2008}, {"title": "The price of bandit information for online optimization", "author": ["Varsha Dani", "Thomas P. Hayes", "Sham M. Kakade"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Dani et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dani et al\\.", "year": 2008}, {"title": "Parametric bandits: The generalized linear case", "author": ["Sarah Filippi", "Olivier Cappe", "Aur\u00e9lien Garivier", "Csaba Szepesv\u00e1ri"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Filippi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Filippi et al\\.", "year": 2010}, {"title": "Online convex optimization in the bandit setting: Gradient descent without a gradient", "author": ["Abraham D. Flaxman", "Adam Tauman Kalai", "H. Brendan McMahan"], "venue": "In Proceedings of the 16th Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Flaxman et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Flaxman et al\\.", "year": 2005}, {"title": "The Elements of Statistical Learning", "author": ["Trevor Hastie", "Robert Tibshirani", "Jerome Friedman"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2009}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["Elad Hazan", "Amit Agarwal", "Satyen Kale"], "venue": "Machine Learning,", "citeRegEx": "Hazan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2007}, {"title": "Logistic regression: Tight bounds for stochastic and online optimization", "author": ["Elad Hazan", "Tomer Koren", "Kfir Y. Levy"], "venue": "In Proceedings of The 27th Conference on Learning Theory,", "citeRegEx": "Hazan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2014}, {"title": "Efficient bandit algorithms for online multiclass prediction", "author": ["Sham M. Kakade", "Shai Shalev-Shwartz", "Ambuj Tewari"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Kakade et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2008}, {"title": "Multi-armed bandits in metric spaces", "author": ["Robert Kleinberg", "Aleksandrs Slivkins", "Eli Upfal"], "venue": "In Proceedings of the 40th Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Kleinberg et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kleinberg et al\\.", "year": 2008}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "Herbert Robbins"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "Lai and Robbins.,? \\Q1985\\E", "shortCiteRegEx": "Lai and Robbins.", "year": 1985}, {"title": "Robust 1-bit compressed sensing and sparse logistic regression: A convex programming approach", "author": ["Yaniv Plan", "Roman Vershynin"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Plan and Vershynin.,? \\Q2013\\E", "shortCiteRegEx": "Plan and Vershynin.", "year": 2013}, {"title": "Some aspects of the sequential design of experiments", "author": ["Herbert Robbins"], "venue": "Bulletin of the American Mathematical Society,", "citeRegEx": "Robbins.,? \\Q1952\\E", "shortCiteRegEx": "Robbins.", "year": 1952}, {"title": "On the complexity of bandit and derivative-free stochastic convex optimization", "author": ["Ohad Shamir"], "venue": "In Proceedings of the 26th Conference on Learning Theory,", "citeRegEx": "Shamir.,? \\Q2013\\E", "shortCiteRegEx": "Shamir.", "year": 2013}, {"title": "The K-armed dueling bandits problem", "author": ["Yisong Yue", "Josef Broder", "Robert Kleinberg", "Thorsten Joachims"], "venue": "In Proceedings of the 22nd Annual Conference on Learning Theory,", "citeRegEx": "Yue et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 12, "context": "Introduction Online learning with bandit feedback plays an important role in several industrial domains, such as ad placement, website optimization, and packet routing (Bubeck and Cesa-Bianchi, 2012).", "startOffset": 168, "endOffset": 199}, {"referenceID": 27, "context": "A canonical framework for studying this problem is the multi-armed bandits (MAB), which models the situation that a gambler must choose which of K slot machines to play (Robbins, 1952).", "startOffset": 169, "endOffset": 184}, {"referenceID": 6, "context": "The goal of the gambler is to minimize the regret, namely the difference between his expected cumulative reward and that of the best single arm in hindsight (Auer et al., 2002).", "startOffset": 157, "endOffset": 176}, {"referenceID": 24, "context": "Lipschitz (Kleinberg et al., 2008) and convex (Flaxman et al.", "startOffset": 10, "endOffset": 34}, {"referenceID": 19, "context": ", 2008) and convex (Flaxman et al., 2005; Agarwal et al., 2013).", "startOffset": 19, "endOffset": 63}, {"referenceID": 2, "context": ", 2008) and convex (Flaxman et al., 2005; Agarwal et al., 2013).", "startOffset": 19, "endOffset": 63}, {"referenceID": 5, "context": "Among them, stochastic linear bandits (SLB) has received considerable attentions during the past decade (Auer, 2002; Dani et al., 2008a; Abbasi-yadkori et al., 2011).", "startOffset": 104, "endOffset": 165}, {"referenceID": 0, "context": "Among them, stochastic linear bandits (SLB) has received considerable attentions during the past decade (Auer, 2002; Dani et al., 2008a; Abbasi-yadkori et al., 2011).", "startOffset": 104, "endOffset": 165}, {"referenceID": 20, "context": "Since the feedback is binary-valued, we assume it is generated according to the logit model (Hastie et al., 2009), i.", "startOffset": 92, "endOffset": 113}, {"referenceID": 18, "context": "The observation model in (3) and the nonlinear regret in (4) can be treated as a special case of the Generalized Linear Bandit (GLB) (Filippi et al., 2010).", "startOffset": 133, "endOffset": 155}, {"referenceID": 12, "context": "Similar to previous studies (Bubeck and Cesa-Bianchi, 2012), we follow the principle of \u201coptimism in face of uncertainty\u201d to deal with the exploration-exploitation dilemma.", "startOffset": 28, "endOffset": 59}, {"referenceID": 21, "context": "Based on the exponential concavity of the logistic loss, we propose to use a variant of the online Newton step (Hazan et al., 2007) to find the center of the confidence region and derive its width by a rather technical analysis of the updating rule.", "startOffset": 111, "endOffset": 131}, {"referenceID": 27, "context": "Related Work The stochastic multi-armed bandits (MAB) (Robbins, 1952), has become the canonical formalism for studying the problem of decision-making under uncertainty.", "startOffset": 54, "endOffset": 69}, {"referenceID": 9, "context": "A long line of successive problems have been extensively studied in statistics (Berry and Fristedt, 1985) and computer science (Bubeck and Cesa-Bianchi, 2012).", "startOffset": 79, "endOffset": 105}, {"referenceID": 12, "context": "A long line of successive problems have been extensively studied in statistics (Berry and Fristedt, 1985) and computer science (Bubeck and Cesa-Bianchi, 2012).", "startOffset": 127, "endOffset": 158}, {"referenceID": 21, "context": "1 Stochastic Multi-armed Bandits (MAB) In their seminal paper, Lai and Robbins (1985) establish an asymptotic lower bound of O(K log T ) for the expected cumulative regret over T periods, under the assumption that the expected rewards of the best and second best arms are well-separated.", "startOffset": 63, "endOffset": 86}, {"referenceID": 3, "context": "To address this limitation, Agrawal (1995) introduces a family of simpler policies that only needs to calculate the sample mean of rewards, and the regret retains the optimal logarithmic behavior.", "startOffset": 28, "endOffset": 43}, {"referenceID": 3, "context": "To address this limitation, Agrawal (1995) introduces a family of simpler policies that only needs to calculate the sample mean of rewards, and the regret retains the optimal logarithmic behavior. A finite time analysis of stochastic MAB is conducted by Auer et al. (2002). In particular, they propose a UCB-type algorithm based on the Chernoff-Hoeffding bound, and demonstrate it achieves the optimal logarithmic regret uniformly over time.", "startOffset": 28, "endOffset": 273}, {"referenceID": 4, "context": "2 Stochastic Linear Bandits (SLB) SLB is first studied by Auer (2002), who considers the case D is finite.", "startOffset": 58, "endOffset": 70}, {"referenceID": 4, "context": "2 Stochastic Linear Bandits (SLB) SLB is first studied by Auer (2002), who considers the case D is finite. Although an elegant UCB-type algorithm named LinRel is developed, he fails to bound its regret due to independence issues. Instead, he designs a complicated master algorithm which uses LinRel as a subroutine, and achieves a regret bound of \u00d8((log |D|)3/2 \u221a Td), where |D| is the number of feasible decisions. In a subsequent work, Dani et al. (2008a) generalize LinRel slightly so that it can be applied in settings where D may be infinite.", "startOffset": 58, "endOffset": 458}, {"referenceID": 0, "context": "Later, Abbasi-yadkori et al. (2011) improve the theoretical analysis of ConfidenceBall2 by employing tools from the self-normalized processes.", "startOffset": 7, "endOffset": 36}, {"referenceID": 0, "context": "Based on the self-normalized bound for vector-valued martingales (Abbasi-yadkori et al., 2011), the width of Ct+1 can be characterized by", "startOffset": 65, "endOffset": 94}, {"referenceID": 18, "context": "4 Generalized Linear Bandit (GLB) Filippi et al. (2010) extend SLB to the nonlinear case based on the Generalized Linear Model framework of statistics.", "startOffset": 34, "endOffset": 56}, {"referenceID": 18, "context": "Different from ConfidenceBall2 which constructs a confidence region in the parameter space, the algorithm of Filippi et al. (2010) operates only in the reward space.", "startOffset": 109, "endOffset": 131}, {"referenceID": 12, "context": "A more general setting is the adversarial case, in which the reward from each arm may change arbitrary (Bubeck and Cesa-Bianchi, 2012).", "startOffset": 103, "endOffset": 134}, {"referenceID": 7, "context": "The most well-known method for the adversarial multi-armed bandits is the Exp3 algorithm, which achieves a regret bound of \u00d8( \u221a KT ) (Auer et al., 2003).", "startOffset": 133, "endOffset": 152}, {"referenceID": 1, "context": "The problem of adversarial linear bandits has been extensively studied, and the start-of-the-art regret bound is \u00d8(poly(d) \u221a T ) (Dani et al., 2008b; Abernethy et al., 2008; Bubeck et al., 2012).", "startOffset": 129, "endOffset": 194}, {"referenceID": 13, "context": "The problem of adversarial linear bandits has been extensively studied, and the start-of-the-art regret bound is \u00d8(poly(d) \u221a T ) (Dani et al., 2008b; Abernethy et al., 2008; Bubeck et al., 2012).", "startOffset": 129, "endOffset": 194}, {"referenceID": 1, "context": ", 2008b; Abernethy et al., 2008; Bubeck et al., 2012). For more results, please refer to Bubeck and Cesa-Bianchi (2012), Shamir (2013) and references therein.", "startOffset": 9, "endOffset": 120}, {"referenceID": 1, "context": ", 2008b; Abernethy et al., 2008; Bubeck et al., 2012). For more results, please refer to Bubeck and Cesa-Bianchi (2012), Shamir (2013) and references therein.", "startOffset": 9, "endOffset": 135}, {"referenceID": 23, "context": "6 Bandit Learning with One-bit Feedback There are several new variants of bandit learning that also rely on one one-bit feedback, such as multi-class bandits (Kakade et al., 2008; Chen et al., 2014) and K-armed dueling bandits (Yue et al.", "startOffset": 158, "endOffset": 198}, {"referenceID": 15, "context": "6 Bandit Learning with One-bit Feedback There are several new variants of bandit learning that also rely on one one-bit feedback, such as multi-class bandits (Kakade et al., 2008; Chen et al., 2014) and K-armed dueling bandits (Yue et al.", "startOffset": 158, "endOffset": 198}, {"referenceID": 29, "context": ", 2014) and K-armed dueling bandits (Yue et al., 2009; Ailon et al., 2014).", "startOffset": 36, "endOffset": 74}, {"referenceID": 4, "context": ", 2014) and K-armed dueling bandits (Yue et al., 2009; Ailon et al., 2014).", "startOffset": 36, "endOffset": 74}, {"referenceID": 10, "context": "7 One-bit Compressive Sensing (CS) Finally, we would like to discuss one closely related work in signal processing\u2014one-bit Compressive Sensing (CS) (Boufounos and Baraniuk, 2008; Plan and Vershynin, 2013).", "startOffset": 148, "endOffset": 204}, {"referenceID": 26, "context": "7 One-bit Compressive Sensing (CS) Finally, we would like to discuss one closely related work in signal processing\u2014one-bit Compressive Sensing (CS) (Boufounos and Baraniuk, 2008; Plan and Vershynin, 2013).", "startOffset": 148, "endOffset": 204}, {"referenceID": 22, "context": "is exponentially concave over bounded domain (Hazan et al., 2014), which motives us to apply a variant of the online Newton step (Hazan et al.", "startOffset": 45, "endOffset": 65}, {"referenceID": 21, "context": ", 2014), which motives us to apply a variant of the online Newton step (Hazan et al., 2007).", "startOffset": 71, "endOffset": 91}, {"referenceID": 21, "context": "Although our updating rule is similar to the method in (Hazan et al., 2007), there also exist some differences.", "startOffset": 55, "endOffset": 75}, {"referenceID": 21, "context": "Although our updating rule is similar to the method in (Hazan et al., 2007), there also exist some differences. As indicated by (9), in our case xtx \u22a4 t is used to approximate the Hessian matrix, while in Hazan et al. (2007) \u2207ft(wt)[\u2207ft(wt)\u22a4] is used.", "startOffset": 56, "endOffset": 225}, {"referenceID": 0, "context": "Due to the difference in the updating rule and the observation model, the self-normalized bound for vector-valued martingales (Abbasi-yadkori et al., 2011) can not be applied here.", "startOffset": 126, "endOffset": 155}, {"referenceID": 18, "context": "Although our observation model in (3) can be handled by the Generalized Linear Bandit (GLB) (Filippi et al., 2010), this paper differs from GLB in the following aspects.", "startOffset": 92, "endOffset": 114}, {"referenceID": 16, "context": "3 Implementation Issues The main computational cost of OL2M comes from (11) which is NP-hard in general (Dani et al., 2008a). In the following, we discuss several strategies for reducing the computational cost. Optimization Over Ball As mentioned by Dani et al. (2008a), in the special case that D is the unit ball, (11) could be solved in time O(poly(d)).", "startOffset": 105, "endOffset": 270}, {"referenceID": 11, "context": "The above problem is an optimization problem with a quadratic objective and one quadratic inequality constraint, it is well-known that strong duality holds provided there exists a strictly feasible point (Boyd and Vandenberghe, 2004).", "startOffset": 204, "endOffset": 233}, {"referenceID": 16, "context": "When studying SLB, Dani et al. (2008a) propose to enlarge the confidence region from Ct+1 = { w : \u2016w \u2212wt+1\u2016Zt+1 \u2264 \u221a \u03b3t+1 } to C\u0303t+1 = { w : \u2016w \u2212wt+1\u20161,Zt+1 \u2264 \u221a d\u03b3t+1 } such that the computational cost could be reduced.", "startOffset": 19, "endOffset": 39}, {"referenceID": 15, "context": "Following the arguments in Dani et al. (2008a), it is straightforward to show that the regret is only increased by a factor of \u221a d.", "startOffset": 27, "endOffset": 47}, {"referenceID": 0, "context": "Lazy Updating Abbasi-yadkori et al. (2011) propose a lazy updating strategy which only needs to solve (11) O(log T ) times.", "startOffset": 14, "endOffset": 43}, {"referenceID": 21, "context": "Although the application of online Newton step (Hazan et al., 2007) in Algorithm 1 is motivated from the fact that ft(w) is exponentially concave over bounded domain, our analysis is built upon a related but different property that the logistic loss log(1 + exp(x)) is strongly convex over bounded domain, from which we obtain the following lemma.", "startOffset": 47, "endOffset": 67}, {"referenceID": 21, "context": "Comparing Lemma 2 with Lemma 3 in (Hazan et al., 2007), we can see that the quadratic term in our inequality does not depends on yt.", "startOffset": 34, "endOffset": 54}, {"referenceID": 14, "context": "To this end, we prove the following lemma, which is built up the Bernstein\u2019s inequality for martingales (Cesa-Bianchi and Lugosi, 2006) and the peeling technique (Bartlett et al.", "startOffset": 104, "endOffset": 135}, {"referenceID": 8, "context": "To this end, we prove the following lemma, which is built up the Bernstein\u2019s inequality for martingales (Cesa-Bianchi and Lugosi, 2006) and the peeling technique (Bartlett et al., 2005).", "startOffset": 162, "endOffset": 185}, {"referenceID": 21, "context": "Finally, we show an upper bound for \u2211t i=1 ci, which is a direct consequence of Lemma 12 in Hazan et al. (2007). Lemma 6 We have t \u2211", "startOffset": 92, "endOffset": 112}, {"referenceID": 11, "context": "Proof Since y is the optimal solution to the optimization problem, from the first-order optimality condition (Boyd and Vandenberghe, 2004), we have \u3008\u03b7g +M(y \u2212 x),w \u2212 y\u3009 \u2265 0, \u2200w \u2208 W.", "startOffset": 109, "endOffset": 138}, {"referenceID": 14, "context": "5 Proof of Lemma 5 We need the Bernstein\u2019s inequality for martingales (Cesa-Bianchi and Lugosi, 2006), which is provided in Appendix D.", "startOffset": 70, "endOffset": 101}, {"referenceID": 8, "context": "To address this issue, we make use of the peeling process (Bartlett et al., 2005).", "startOffset": 58, "endOffset": 81}, {"referenceID": 15, "context": "6 Proof of Theorem 3 The proof is standard and can be found from Dani et al. (2008a) and Abbasi-yadkori et al.", "startOffset": 65, "endOffset": 85}, {"referenceID": 0, "context": "(2008a) and Abbasi-yadkori et al. (2011). We include it for the sake of completeness.", "startOffset": 12, "endOffset": 41}, {"referenceID": 0, "context": "To proceed, we need the following results from Lemma 11 in Abbasi-yadkori et al. (2011),", "startOffset": 59, "endOffset": 88}, {"referenceID": 26, "context": "In contrast, a much broader class of observation models are allowed in one-bit compressive sensing (Plan and Vershynin, 2013), as long as there is a positive correlation between the one-bit output and the real-valued measurement.", "startOffset": 99, "endOffset": 125}, {"referenceID": 21, "context": "Proof of Lemma 6 We have \u2016xi\u20162Z\u22121 i+1 = 2 \u03b7\u03b2 \u3008Z\u22121 i+1, Zi+1 \u2212 Zi\u3009 \u2264 2 \u03b7\u03b2 log det(Zi+1) det(Zi) , where the inequality follows from Lemma 12 in Hazan et al. (2007). Thus, we have t \u2211", "startOffset": 143, "endOffset": 163}, {"referenceID": 0, "context": "From Lemma 10 of Abbasi-yadkori et al. (2011), we have det(Zt+1) \u2264 ( \u03bb+ \u03b7\u03b2t 2d )d .", "startOffset": 17, "endOffset": 46}], "year": 2015, "abstractText": "In this paper, we study a special bandit setting of online stochastic linear optimization, where only one-bit of information is revealed to the learner at each round. This problem has found many applications including online advertisement and online recommendation. We assume the binary feedback is a random variable generated from the logit model, and aim to minimize the regret defined by the unknown linear function. Although the existing method for generalized linear bandit can be applied to our problem, the high computational cost makes it impractical for real-world problems. To address this challenge, we develop an efficient online learning algorithm by exploiting particular structures of the observation model. Specifically, we adopt online Newton step to estimate the unknown parameter and derive a tight confidence region based on the exponential concavity of the logistic loss. Our analysis shows that the proposed algorithm achieves a regret bound of \u00d8(d \u221a T ), which matches the optimal result of stochastic linear bandits.", "creator": "LaTeX with hyperref package"}}}