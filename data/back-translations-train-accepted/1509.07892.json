{"id": "1509.07892", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Sep-2015", "title": "Evasion and Hardening of Tree Ensemble Classifiers", "abstract": "Recent work has successfully constructed adversarial \"evading\" instances for differentiable prediction models. However generating adversarial instances for tree ensembles, a piecewise constant class of models, has remained an open problem. In this paper, we construct both exact and approximate evasion algorithms for tree ensembles: for a given instance x we find the \"nearest\" instance x' such that the classifier predictions of x and x' are different. First, we show that finding such instances is practically possible despite tree ensemble models being non-differentiable and the optimal evasion problem being NP-hard.", "histories": [["v1", "Fri, 25 Sep 2015 20:57:35 GMT  (317kb,D)", "http://arxiv.org/abs/1509.07892v1", "9 pages, 9 figures"], ["v2", "Fri, 27 May 2016 01:09:22 GMT  (891kb,D)", "http://arxiv.org/abs/1509.07892v2", "11 pages, 7 figures, Appears in Proceedings of the 33rd International Conference on Machine Learning (ICML), New York, NY, USA, 2016. JMLR: W&amp;CP volume 48"]], "COMMENTS": "9 pages, 9 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CR stat.ML", "authors": ["alex kantchelian", "j d tygar", "anthony d joseph"], "accepted": true, "id": "1509.07892"}, "pdf": {"name": "1509.07892.pdf", "metadata": {"source": "CRF", "title": "Evasion and Hardening of Tree Ensemble Classifiers", "authors": ["Alex Kantchelian", "J. D. Tygar", "Anthony D. Joseph"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "These models can successfully and accurately address additional learning problems, including the classification of audio, video, and natural language, which is possible where previous approaches have failed. However, the existence of counterproductive \"circumvention models\" for the current incarnation of DNNs [1] shows a perhaps surprising fragility: for virtually every instance x that correctly classifies the model, it is possible to find a negligible disorder that is correctly classified despite the current incarnation of DNNs [1], obtains an (sometimes knowingly) inaccurate prediction. The general investigation of the circumvention problem is important for at least two reasons, one of which is conceptual, the other practical. First, we expect a high-performance learning algorithm to be good at generalizing and hard to evade: only a \"large enough\" disorder should be able to change its decision. The existence of small circumvention models shows a defect in the model's ability to intuitively, intuitively, and intuitively bypass the model."}, {"heading": "2 Related Work", "text": "In fact, we see ourselves in a position to put ourselves at the top of society, in the way we do it: in the way we do it, in the way we do it, in the way we do it, in the way we do it, in the way we do it, in the way we do it, in the way we do it, in the way we do it. \""}, {"heading": "3 Evading Tree Ensemble Models", "text": "Let m: X \u2192 Y (model) be a classifier. For a given instance x-X and a given distance function d: X \u00b7 X \u2192 R +, the optimal evasive problem is to minimize x \u2032 Xd (x, x \u2032) subject to m (x) 6 = m (x \u2032) (1) In this paper, we focus on binary classifiers Y = {\u2212 1} defined by an n-dimensional attribute space X-Rn. Setting aside the classifier m, the distance function d is fully specified (1), so we can talk about d-adversarial distances or d-robustness. Since different measurements of instance deformation lead to different solutions, we present results for four representative distances. We briefly describe these and their typical effects on solving (1). \u2022 The L0 distance is ni = 1 Ixi 6 = x \u00b2 i, or the hamming distance encourages the most sparate, most localized deformations."}, {"heading": "3.1 Optimal Evasion Attack", "text": "We represent a reduction of the problem (1) into a mixed sum of indicators defined as conjunctions of the atomic propositions. (2) We represent a reduction of the problem (1) into a mixed integer when m is considered as a binary classification (2). (3) We represent a reduction of the problem (1). (3) We represent a reduction of the problem (1). (3) We are the sum of the individual regression trees (1). (4) We are simply the sum of the individual regression trees (1). (3) The sum of the individual regression trees (2) actually corresponds to the leafs of T, each domain L is defined as the conjunction of most J atoms l. (4) We have a prediction value of vL-R. Let J be the maximum tree depth in the model. (4) Since these domains actually correspond to the leafs of T, each domain L is defined as the conjunction of most J atoms < (4) We have the most atomic projections."}, {"heading": "3.2 Approximate Evasion Attack", "text": "While the above reduction of the problem (1) to a MIP problem (1) has a linear size of the model, the actual solution time can be anywhere from a few seconds to 30 minutes, even for the state of the art that we use. [13] Therefore, we are developing an approximate bypass algorithm to generate a good quality, the instances.Our approximate bypass attack is based on the following iterative descent method: Let's leave the maximum amount of adversarial deformation that we are willing to tolerate. (x) Assuming M (x) < 0. Starting point is the original instance x. (x) We calculate the best dimension of each dimension of the x. (x) The algorithms end when d (x) > B. The critical part of this approach is the search for the best dimension of x. (x) Formally, we want to solve the following problem: maximize (x)."}, {"heading": "4 Results", "text": "In fact, most of them are able to go to another world, to go to another world, to go to another world."}, {"heading": "4.2 Robustification", "text": "We show that it is possible to significantly improve the robustness of the BDT model by retraining the training on enemy instances. We create iterative L0 hostile instances for the current model and for all 11,876 original training instances using the approach outlined in subsection 3.2, with a maximum modification budget of B = 10 pixels. We then train on the original training dataset, which has been expanded to include all contradictory but correctly designated instances, in order to obtain the next iteration model. We keep the model's hyperparameters constant at 1,000 trees with maximum depth 4. We perform this procedure for 60 laps to generate more than 700,000 additional training instances, 60 times more opposing instances than the original instances. We find that our fast attack finds good quality that circumvents the instance in less than a second, while solving the L0 problem can be reduced to optimum in the exact approach."}, {"heading": "5 Conclusion", "text": "Gradient-enhanced trees are both one of the most accurate learning models and the easiest to circumvent, which could have problematic effects when these algorithms are used in harsh environments. On the other hand, gradual retraining can successfully harden augmented trees against L0 evasive attacks. The retraining strategy also improves model accuracy and can therefore be understood as a form of model regulation. In future research, it would be interesting to investigate how the tree groups generated by random forests can be compared with augmented decision trees on the robustness scale. To investigate whether augmented trees can be equipped to withstand all four types of attacks simultaneously is also an interesting open problem. Finally, it would be a significant breakthrough to exploit the regulatory effect of retraining adverse entities to achieve a similar regulatory effect without generating new training instances and overextending already large training sets."}], "references": [{"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "venue": "arXiv preprint,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Second place winner entry for the Higgs Boson Machine Learning Kaggle Challenge. https://github.com/TimSalimans/HiggsML", "author": ["Tim Salimans"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Can machine learning be secure", "author": ["Marco Barreno", "Blaine Nelson", "Russell Sears", "Anthony D. Joseph", "J.D. Tygar"], "venue": "In Proceedings of the 2006 ACM Symposium on Information, Computer and Communications Security, ASIACCS \u201906,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Good word attacks on statistical spam filters", "author": ["Daniel Lowd"], "venue": "In Proceedings of the Second Conference on Email and Anti-Spam (CEAS),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Practical evasion of a learning-based classifier: A case study", "author": ["Nedim Srndic", "Pavel Laskov"], "venue": "In Proceedings of the 2014 IEEE Symposium on Security and Privacy, S&P", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Adversarial learning", "author": ["Daniel Lowd", "Christopher Meek"], "venue": "In Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining, KDD", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Query strategies for evading convex-inducing classifiers", "author": ["Blaine Nelson", "Benjamin I.P. Rubinstein", "Ling Huang", "Anthony D. Joseph", "Steven J. Lee", "Satish Rao", "J.D. Tygar"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Analysis of classifiers robustness to adversarial perturbations", "author": ["Alhussein Fawzi", "Omar Fawzi", "Pascal Frossard"], "venue": "arXiv preprint,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Evasion attacks against machine learning at test time. In Machine Learning and Knowledge Discovery in Databases, volume 8190 of Lecture Notes in Computer Science", "author": ["Battista Biggio", "Igino Corona", "Davide Maiorca", "Blaine Nelson", "Nedim rndi", "Pavel Laskov", "Giorgio Giacinto", "Fabio Roli"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Explaining and harnessing adversarial examples", "author": ["Ian J. Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": "arXiv preprint,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A Nguyen", "J Yosinski", "J Clune"], "venue": "In Computer Vision and Pattern Recognition", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Towards deep neural network architectures robust to adversarial examples", "author": ["Shixiang Gu", "Luca Rigazio"], "venue": "arXiv preprint,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "XGBoost: eXtreme Gradient Boosting", "author": ["Tianqi Chen", "Tong He"], "venue": "https://github. com/dmlc/xgboost", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "LIBLIN- EAR: A library for large linear classification", "author": ["Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Large-margin convex polytope machine", "author": ["Alex Kantchelian", "Michael C. Tschantz", "Ling Huang", "Peter L. Bartlett", "Anthony D. Joseph", "J.D. Tygar"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "On the hardness of evading combinations of linear classifiers", "author": ["David Stevens", "Daniel Lowd"], "venue": "In Proceedings of the 2013 ACM Workshop on Artificial Intelligence and Security, AISec", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Yet, the existence of adversarial \u201cevading\u201d instances for the current incarnation of DNNs [1] shows a perhaps surprising brittleness: for virtually any instance x that the model classifies correctly, it is possible to find a negligible perturbation \u03b4 such that x + \u03b4 evades being correctly classified, that is, receives a (sometimes widly) inaccurate prediction.", "startOffset": 90, "endOffset": 93}, {"referenceID": 1, "context": "Tree sum-ensembles as produced by boosting or bagging are perhaps the most important models from this class as they are often able to achieve competitive performance and enjoy good adoption rates in both industrial and academic contexts [2].", "startOffset": 237, "endOffset": 240}, {"referenceID": 2, "context": "[3], evasion attacks are part of the larger family of exploratory attacks which occur at testing time, for a fixed classifier.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "For example, Lowd [4] describes an attack on linear spam filters, where the spammer finds and appends enough benign-weighted terms to avoid detection.", "startOffset": 18, "endOffset": 21}, {"referenceID": 4, "context": "[5] present and evaluate several concrete evasion attacks for a malware detection system based on linear and RBF-SVM under different adversary power assumptions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] investigate evasion of linear models when the models are kept secret but queries are allowed.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] later extend this analysis to any model that induces a convex decision boundary in a continuous feature space.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] provide theoretical results on the robustness of linear and quadratic models when the models are fully known.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8], we do not limit the amount of information available to the adversary.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] introduce evasion as a constrained optimization problem and proceed to compute L1-evading instances for RBF-SVM using a projected gradient descent method.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] use a quasi-Newtonian method to first demonstrate that L2-small perturbations on image recognition tasks significantly impact state-of-the-art deep learning classifiers.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10], showing that for large-dimensional image data, a single small gradient step on the model score is sufficient to obtain an evading instance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10], we show that retraining tree ensembles significantly strengthens the resulting model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] study a version of evasion where the small \u03b4 constraint is altogether dropped and replaced by a \u201cregularity\u201d constraint on the attack instance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] show some promising results by augmenting DNNs with a pre-filtering step based on a form of contractive auto-encoding.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "So we have n = 784 and X = [0, 1].", "startOffset": 27, "endOffset": 33}, {"referenceID": 12, "context": "We choose to use second order gradient boosting (denoted BDT) in the XGBoost [15] implementation as the tree ensemble learner because of its outstanding performance as an off-the-shelf general purpose learning algorithm.", "startOffset": 77, "endOffset": 81}, {"referenceID": 13, "context": "L2) and RBF-SVM trained using LibLinear [16] and LibSVM [17] respectively.", "startOffset": 40, "endOffset": 44}, {"referenceID": 14, "context": "L2) and RBF-SVM trained using LibLinear [16] and LibSVM [17] respectively.", "startOffset": 56, "endOffset": 60}, {"referenceID": 15, "context": "We use Theano [18] to implement a sigmoidal (tanh) activation network with three hidden layers (denoted NN) in a 784-40-30-20-1 architecture, and train it by gradient descent for a top logistic regression layer, without pre-training nor drop-out.", "startOffset": 14, "endOffset": 18}, {"referenceID": 16, "context": "Polytope Machines [19] (one for each class) and we use the authors\u2019 implementation (CPM).", "startOffset": 18, "endOffset": 22}, {"referenceID": 17, "context": "First, previous work has theoretically considered the evasion robustness of such ensemble of linear classifiers and deemed the problem NP-hard in the general case [20].", "startOffset": 163, "endOffset": 167}, {"referenceID": 9, "context": "On the other end, RBF-SVM is apparently the hardest model to evade, agreeing with results from [10].", "startOffset": 95, "endOffset": 99}], "year": 2017, "abstractText": "Recent work has successfully constructed adversarial \u201cevading\u201d instances for differentiable prediction models. However generating adversarial instances for tree ensembles, a piecewise constant class of models, has remained an open problem. In this paper, we construct both exact and approximate evasion algorithms for tree ensembles: for a given instance x we find the \u201cnearest\u201d instance x\u2032 such that the classifier predictions of x and x\u2032 are different. First, we show that finding such instances is practically possible despite tree ensemble models being nondifferentiable and the optimal evasion problem being NP-hard. In addition, we quantify the susceptibility of such models applied to the task of recognizing handwritten digits by measuring the distance between the original instance and the modified instance under the L0, L1, L2 and L\u221e norms. We also analyze a wide variety of classifiers including linear and RBF-kernel models, maxensemble of linear models, and neural networks for comparison purposes. Our analysis shows that tree ensembles produced by a state-of-the-art gradient boosting method are consistently the least robust models notwithstanding their competitive accuracy. Finally, we show that a sufficient number of retraining rounds with L0-adversarial instances makes the hardened model three times harder to evade. This retraining set also marginally improves classification accuracy, but simultaneously makes the model more susceptible to L1, L2 and L\u221e evasions.", "creator": "LaTeX with hyperref package"}}}