{"id": "1509.07943", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Sep-2015", "title": "Super-Resolution Off the Grid", "abstract": "Super-resolution is the problem of recovering a superposition of point sources using bandlimited measurements, which may be corrupted with noise. This signal processing problem arises in numerous imaging problems, ranging from astronomy to biology to spectroscopy, where it is common to take (coarse) Fourier measurements of an object. Of particular interest is in obtaining estimation procedures which are robust to noise, with the following desirable statistical and computational properties: we seek to use coarse Fourier measurements (bounded by some cutoff frequency); we hope to take a (quantifiably) small number of measurements; we desire our algorithm to run quickly.", "histories": [["v1", "Sat, 26 Sep 2015 03:49:27 GMT  (45kb,D)", "http://arxiv.org/abs/1509.07943v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["qingqing huang", "sham m kakade"], "accepted": true, "id": "1509.07943"}, "pdf": {"name": "1509.07943.pdf", "metadata": {"source": "CRF", "title": "Super-Resolution Off the Grid", "authors": ["Qingqing Huang", "Sham M. Kakade"], "emails": ["qqh@mit.edu", "sham@cs.washington.edu"], "sections": [{"heading": null, "text": "Suppose we have k-point sources in d dimensions, where the points are separated from each other by at least \u2206 (at Euclidean distance). This work provides an algorithm with the following favorable guarantees: \u2022 The algorithm uses Fourier measurements, the frequency of which is limited by O (1 / \u2206) (up to log factors). Previous algorithms require a cut-off frequency that can be as large as N (\u221a d / \u0445). \u2022 The number of measurements performed by and the computational complexity of our algorithm are limited by both the number of points k and the dimension d by a polynomial, without dependence on the separation. In contrast, previous algorithms depended inversely polynomially on the minimal separation and exponentially on the dimension for both of these quantities. Our estimation method itself is simple: We take random band-limited measurements (as opposed to an exponential number of measurements on the hyper network and the concentration of these two quantities)."}, {"heading": "1 Introduction", "text": "We follow the usual mathematical abstraction of this problem (Candes & Fernandez-Granda): \"We assume that a d-dimensional signal x (t) is the weighted sum of the measurement quantities in Rd: x (t) = k = 1 wjp (j), (1) where the measurement quantities, the measurement quantities, the measurement quantities, the measurement quantities, the measurement quantities, the measurement quantities, the measurement quantities, the measurement quantities, the measurement quantities, the measurement quantities, the measurement quantities, the measurement quantities, the measurement quantities, the measurement quantities, the measurement quantities, the measurement quantities, the measurement quantities, the measurement quantities, the measurement quantities, the measurement quantities, the measurement quantities, the quantities, the measurement quantities, the quantities, the measurement quantities, the quantities, the quantities, the quantities, the quantities, the quantities, the measurement quantities, the quantities, the quantities, the quantities, the quantities, the measurement quantities, the quantities, the quantities, the quantities, the quantities, the quantities, the quantities, the quantities, the quantities, the quantities, the quantities, the quantities, the quantities, the quantities, the quantities, the quantities, the quantities, the quantities, the quantities, the quantities, the quantities, the quantities, the quantities, the quantities, the quantities, the quantities, the quantities, the quantities, the"}, {"heading": "1.1 This work", "text": "We are interested in stable recovery methods with the following desirable statistical and computational properties: we aim for rough (low frequency) measurements; we hope to make a (quantifiable) small number of measurements; we want our algorithm to run fast. Informal, our main result is as follows: Theorem 1.3 (informal statement of theorem 3.2). For a fixed error probability, the proposed algorithm achieves a stable recovery with a number of measurements and a computational run time, both of which are in the order of O ((k log (k) + d) 2). Furthermore, the algorithm takes measurements that are limited in frequency by O (1 / \u0445) (ignoring protocol factors). It is noteworthy that our algorithm and analysis are directly related to the multivariate case, with the univariate case being considered a special case. It is important that the number of measurements and the minimum computational run time do not depend on the separation of the separation of the separation of the point time."}, {"heading": "1.2 Comparison to related work", "text": "Table 1 summarizes the comparisons between our algorithm and the existing results; the multidimensional cutoff frequency we refer to in the table is the maximum coordinate input of any measurement frequency s (i.e., the number of measurements can essentially be reduced by the method in Tang et. al. [23] (this is reflected in the table); \"MP\" refers to the matrix-pencil type of methods investigated in [14] and [15] for the univariate case; the number of measurements can be reduced by the method in Tang etal; \"MP\" refers to the type of methods investigated in the matrix-pencil method; and [15] for the univariate case. \"Here, we define the infinity standard separation as\" minj 6 = j \u2032 \u00b5 (j) \u2212 \u00b5 (j \u2032) \u2212 \u00b5, \"the substoodas wrap around the distance on the unit circle\" Cd 1 is a constant. \""}, {"heading": "1.3 Notation", "text": "Let R, C, and Z stand for real, complex, and natural numbers. Let D-Z stand for the set [d] = {1,.., d}. Let S stand for its cardinality. Let PdR stand for the direct sum of sets, namely S1-S2 = (a + b): a-S1, b-S2}. Let PdR stand for the n-th standard base vector in Rd, for n-d). Let PdR, 2 = {x-Rd: o-x-S2 = 1} for the d-sphere of radius R in d-dimensional euclidean space. Specify the condition number of a matrix X-Rm \u00b7 n as Cond2 (X) = maximum (X) / zm (X) / zm (X) (X), where the maximum (X) and minimum individual values of X."}, {"heading": "2 Warm-up", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 1-D case: revisiting the matrix pencil method", "text": "s start with the matrix method (2) f (2) f (2) f (2) f (3) f (2) f (2) f (2) f (3) f (3) f (3) f (3) f (3) f (3) f (1) f (1) f (m) f (m) f (m) f (m) f (m) f (m) f (m) f (m) f (m) f (m) f (m) f (m) f (m) f (m) f (m) f (m) f (m) f (m) f (m) f (m) f (m) f (m) f (m) f (m) f (m) f (m) f (m) f (m) f)."}, {"heading": "2.2 The multivariate case: a toy example", "text": "One could naively expand the matrix-pencil method to higher dimensions by taking measurements from a hyper-grid that is exponentially in dimension d. We are now investigating a toy problem that indicates that the high-dimensional case cannot per se be more difficult than the univariate case. The key ideas are that a suitably sampled number can significantly reduce the number of measurements (compared to using all the grid points). Tang et al [23] made a similar observation for the univariate case. They used a small random subset of measurements (actually still from the grid points) and showed that this contains enough information to restore all the measurements on the grid; the complete measurements were then used for the static restoration of the point sources. Consider the case where the dimension d is k."}, {"heading": "3 Main Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 The algorithm", "text": "The following briefly describes the steps of the algorithm 1 < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <"}, {"heading": "1. Take measurements:", "text": "Let S = {s (1),.., s (m)} be m i.i.d. samples from the Gaussian distribution N (0, R2Id \u00b7 d). Set s (m + n) = en for all n [d] and s (m + n + 1) = 0. Name m \u2032 = m + d + 1. Take more samples v from the unit sphere and set v (1) = v and v (2) = 2v. Construct a tensor F-Cm \u00b2 \u00b7 m \u00b2 x 3: F-n1, n2, n3 = f-c.For j = 1,..., k, set [V-S \u00b2) + v (n3).2. Tensor decomposition: Set (V-S \u2032, D-w) = TensorDecomp (F-N).For j = 1,..."}, {"heading": "4. Set W\u0302 = arg minW\u2208Ck \u2016F\u0302 \u2212 V\u0302S\u2032 \u2297 V\u0302S\u2032 \u2297 V\u0302dDw\u2016F .", "text": "Algorithm 1: General algorithms define with a very high probability the matrix V: \"Cm.\" Since each element of the unit VSVd. \"(VSVd 1,..., 1, (11), where Vd\" Cd. \"is defined in (8). DefineV2 = ei\u03c0 < \u00b5 (1), v (1) >. ei\u03c0 < \u00b5 (k), v\" ei\u03c0 \"(1) >. ei\u03c0 < \u00b5 (2), v (2) > 1. Note that in the exact case (z = 0) the tensor F constructed in (9) allows a rank decomposition: F = VS\" VS. \"(V2Dw), (12) assumes that VS\" has full column rank, then this tensor decomposition is unique up to the column permutation and rescalation with a very high probability."}, {"heading": "3.2 Guarantees", "text": "In this section we will discuss how to select the two parameters m and R and prove that the proposed algorithm does indeed achieve a stable recovery in the presence of measuring noise."}, {"heading": "3.3 Key Lemmas", "text": "In this paragraph we give a brief description and the stability guarantee of the known algorithm of Jennrich ([11, 13]) in case the factor V has a complete column k. Then the decomposition is unique up to column mutation and rescaling, and algorithm 2 finds the factors efficient. Furthermore, the self-decomposition is stable if the factor V is well conditioned and the eigenvalues of FaF are well separated. Lemma 3.5 (Stability of Jennrich's algorithm).Consider the 3rd order tensor F = V (V2Dw)."}, {"heading": "4 Discussions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Numerical results", "text": "We show empirically the performance of the proposed superresolution algorithm in this paragraph. First, we consider a simple instance with dimension d = 2 and minimum separation p = 0.05. Our disturbance analysis of the stability results is limited to low noise, i.e. z is inversely polynmically small in dimensions, and the number of measurements m must be polynmically large in dimensions. However, we believe that this is only the artifact of the raw analysis, rather than being intrinsic to the approach. In the following numerical example, we examine a typical instance of 8 randomly generated 2-D point sources. The minimum separation p = 0.01 and the weights are evenly distributed in [0.1, 1.1] The measurement noise level z is set at 0.1, and we take only 2178 low noise measurements (1 / 2)."}, {"heading": "4.2 Connection with learning GMMs", "text": "One reason we are interested in scaling the algorithm with respect to dimension D is that it naturally leads to an algorithm for learning the Gaussian mixing models (GMMs). < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <; < <; <; <; <; < <; < < < <; < <; < <; < <; <; < < < < <; < < <; <;; < < < <; < < < < <;;; < < < < <;; < <;;; < <; < <; < <; <;;;; < &lt"}, {"heading": "4.3 Open problems", "text": "In a recent paper, Chen & Chi [5] have shown that the sample complexity can be reduced to O (k log4 d) for stable restoration, but the computational complexity is still in the order of O (kd), since the Hankel matrix has the dimension O (kd) and a semi-defined program is used to complete the matrix. It remains an open problem to reduce the sample complexity of our algorithm from O (k2) to the information theoretical limit O (k) while maintaining polynomic scaling of the computational complexity. Recently, Schiebinger et al [22] investigated the problem of learning a mixture of displaced and newly scaled point distribution functions f (s) = ichj wj\u043e (s, \u00b5 (j (j))).This model has the Gaussian mixture as a special case, where the point-distributed function is the Gaussian spread function."}, {"heading": "Acknowledgments", "text": "The authors thank Rong Ge and Ankur Moitra for the very helpful discussions. Sham Kakade recognizes the Washington Research Foundation's support for innovation in data-intensive discoveries."}], "references": [{"title": "Tensor decompositions for learning latent variable models", "author": ["A. Anandkumar", "R. Ge", "D. Hsu", "S.M. Kakade", "M. Telgarsky"], "venue": "The Journal of Machine Learning Research, 15(1):2773\u20132832,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "A method of moments for mixture models and hidden markov models", "author": ["A. Anandkumar", "D. Hsu", "S.M. Kakade"], "venue": "arXiv preprint arXiv:1203.0683,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Super-resolution from noisy data", "author": ["E.J. Cand\u00e8s", "C. Fernandez-Granda"], "venue": "Journal of Fourier Analysis and Applications, 19(6):1229\u20131254,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Towards a mathematical theory of super-resolution", "author": ["E.J. Cand\u00e8s", "C. Fernandez-Granda"], "venue": "Communications on Pure and Applied Mathematics, 67(6):906\u2013956,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Robust spectral compressed sensing via structured matrix completion", "author": ["Y. Chen", "Y. Chi"], "venue": "Information Theory, IEEE Transactions on, 60(10):6576\u20136601,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning mixtures of gaussians", "author": ["S. Dasgupta"], "venue": "Foundations of Computer Science, 1999. 40th Annual Symposium on, pages 634\u2013644. IEEE,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "An elementary proof of a theorem of johnson and lindenstrauss", "author": ["S. Dasgupta", "A. Gupta"], "venue": "Random structures and algorithms, 22(1):60\u201365,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "A two-round variant of em for gaussian mixtures", "author": ["S. Dasgupta", "L.J. Schulman"], "venue": "Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence, pages 152\u2013 159. Morgan Kaufmann Publishers Inc.,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "Superresolution via sparsity constraints", "author": ["D.L. Donoho"], "venue": "SIAM Journal on Mathematical Analysis, 23(5):1309\u20131331,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1992}, {"title": "A Convex-programming Framework for Super-resolution", "author": ["C. Fernandez-Granda"], "venue": "PhD thesis, Stanford University,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Foundations of the parafac procedure: Models and conditions for an \u201dexplanatory\u201d multi-modal factor analysis", "author": ["R.A. Harshman"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1970}, {"title": "Fourier series in control theory", "author": ["V. Komornik", "P. Loreti"], "venue": "Springer Science & Business Media,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "A decomposition for three-way arrays", "author": ["S. Leurgans", "R. Ross", "R. Abel"], "venue": "SIAM Journal on Matrix Analysis and Applications, 14(4):1064\u20131083,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1993}, {"title": "Music for single-snapshot spectral estimation: Stability and superresolution", "author": ["W. Liao", "A. Fannjiang"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "The threshold for super-resolution via extremal functions", "author": ["A. Moitra"], "venue": "arXiv preprint arXiv:1408.1681,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning nonsingular phylogenies and hidden markov models", "author": ["E. Mossel", "S. Roch"], "venue": "Proceedings of the thirty-seventh annual ACM symposium on Theory of computing, pages 366\u2013 375. ACM,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Noise space decomposition method for twodimensional sinusoidal model", "author": ["S. Nandi", "D. Kundu", "R.K. Srivastava"], "venue": "Computational Statistics & Data Analysis, 58:147\u2013161,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Contributions to the mathematical theory of evolution", "author": ["K. Pearson"], "venue": "Philosophical Transactions of the Royal Society of London. A, pages 71\u2013110,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1894}, {"title": "Parameter estimation for nonincreasing exponential sums by pronylike methods", "author": ["D. Potts", "M. Tasche"], "venue": "Linear Algebra and its Applications, 439(4):1024\u20131039,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Controllability and stabilizability theory for linear partial differential equations: recent progress and open questions", "author": ["D.L. Russell"], "venue": "Siam Review, 20(4):639\u2013739,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1978}, {"title": "Learning mixtures of arbitrary gaussians", "author": ["A. Sanjeev", "R. Kannan"], "venue": "Proceedings of the thirty-third annual ACM symposium on Theory of computing, pages 247\u2013257. ACM,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2001}, {"title": "Superresolution without separation", "author": ["G. Schiebinger", "E. Robeva", "B. Recht"], "venue": "arXiv preprint arXiv:1506.03144,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Compressed sensing off the grid", "author": ["G. Tang", "B.N. Bhaskar", "P. Shah", "B. Recht"], "venue": "Information Theory, IEEE Transactions on, 59(11):7465\u20137490,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Max vs min: Independent component analysis with nearly linear sample complexity", "author": ["S.S. Vempala", "Y.F. Xiao"], "venue": "arXiv preprint arXiv:1412.2954,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 3, "context": "We follow the standard mathematical abstraction of this problem (Candes & Fernandez-Granda [4, 3]): consider a d-dimensional signal x(t) modeled as a weighted sum of k Dirac measures in Rd:", "startOffset": 91, "endOffset": 97}, {"referenceID": 2, "context": "We follow the standard mathematical abstraction of this problem (Candes & Fernandez-Granda [4, 3]): consider a d-dimensional signal x(t) modeled as a weighted sum of k Dirac measures in Rd:", "startOffset": 91, "endOffset": 97}, {"referenceID": 0, "context": "We follow the standard normalization to assume that: \u03bc \u2208 [\u22121,+1], |wj | \u2208 [0, 1] \u2200j \u2208 [k].", "startOffset": 74, "endOffset": 80}, {"referenceID": 3, "context": "Our claims hold withut using the \u201cwrap around metric\u201d, as in [4, 3], due to our random sampling.", "startOffset": 61, "endOffset": 67}, {"referenceID": 2, "context": "Our claims hold withut using the \u201cwrap around metric\u201d, as in [4, 3], due to our random sampling.", "startOffset": 61, "endOffset": 67}, {"referenceID": 8, "context": "The terminology of \u201csuper-resolution\u201d is appropriate due to the following remarkable result (in the noiseless case) of Donoho [9]: suppose we want to accurately recover the point sources to an error of \u03b3, where \u03b3 \u2206.", "startOffset": 126, "endOffset": 129}, {"referenceID": 8, "context": "Donoho [9] showed that it suffices to obtain a finite number of measurements, whose frequencies are bounded by O(1/\u2206), in order to achieve exact recovery; thus resolving the point sources far more accurately than that which is naively implied by using frequencies of O(1/\u2206).", "startOffset": 7, "endOffset": 10}, {"referenceID": 3, "context": "Furthermore, the work of Candes & Fernandez-Granda [4, 3] showed that stable recovery, in the univariate case (d = 1), is achievable with a cutoff frequency of O(1/\u2206) using a convex program and a number of measurements whose size is polynomial in the relevant quantities.", "startOffset": 51, "endOffset": 57}, {"referenceID": 2, "context": "Furthermore, the work of Candes & Fernandez-Granda [4, 3] showed that stable recovery, in the univariate case (d = 1), is achievable with a cutoff frequency of O(1/\u2206) using a convex program and a number of measurements whose size is polynomial in the relevant quantities.", "startOffset": 51, "endOffset": 57}, {"referenceID": 2, "context": "\u201cSDP\u201d refers to the semidefinite programming (SDP) based algorithms of Candes & Fernandez-Granda [3, 4]; in the univariate case, the number of measurements can be reduced by the method in Tang et.", "startOffset": 97, "endOffset": 103}, {"referenceID": 3, "context": "\u201cSDP\u201d refers to the semidefinite programming (SDP) based algorithms of Candes & Fernandez-Granda [3, 4]; in the univariate case, the number of measurements can be reduced by the method in Tang et.", "startOffset": 97, "endOffset": 103}, {"referenceID": 22, "context": "[23] (this is reflected in the table).", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "\u201cMP\u201d refers to the matrix pencil type of methods, studied in [14] and [15] for the univariate case.", "startOffset": 61, "endOffset": 65}, {"referenceID": 14, "context": "\u201cMP\u201d refers to the matrix pencil type of methods, studied in [14] and [15] for the univariate case.", "startOffset": 70, "endOffset": 74}, {"referenceID": 18, "context": "3) Furthermore, one could project the multivariate signal to the coordinates and solve multiple univariate problems (such as in [19, 17], which provided only exact recovery results).", "startOffset": 128, "endOffset": 136}, {"referenceID": 16, "context": "3) Furthermore, one could project the multivariate signal to the coordinates and solve multiple univariate problems (such as in [19, 17], which provided only exact recovery results).", "startOffset": 128, "endOffset": 136}, {"referenceID": 2, "context": "SDP approaches: The work in [3, 4, 10] formulates the recovery problem as a total-variation minimization problem; they then show the dual problem can be formulated as an SDP.", "startOffset": 28, "endOffset": 38}, {"referenceID": 3, "context": "SDP approaches: The work in [3, 4, 10] formulates the recovery problem as a total-variation minimization problem; they then show the dual problem can be formulated as an SDP.", "startOffset": 28, "endOffset": 38}, {"referenceID": 9, "context": "SDP approaches: The work in [3, 4, 10] formulates the recovery problem as a total-variation minimization problem; they then show the dual problem can be formulated as an SDP.", "startOffset": 28, "endOffset": 38}, {"referenceID": 19, "context": "For d \u2265 1, Ingham-type theorems (see [20, 12]) suggest that Cd = O( \u221a d).", "startOffset": 37, "endOffset": 45}, {"referenceID": 11, "context": "For d \u2265 1, Ingham-type theorems (see [20, 12]) suggest that Cd = O( \u221a d).", "startOffset": 37, "endOffset": 45}, {"referenceID": 22, "context": "The number of measurements can be reduced by the method in [23] for the d = 1 case, which is noted in the table.", "startOffset": 59, "endOffset": 63}, {"referenceID": 13, "context": "Recently, for the univariate matrix pencil method, Liao & Fannjiang [14] and Moitra [15] provide a stability analysis of the MUSIC algorithm.", "startOffset": 68, "endOffset": 72}, {"referenceID": 14, "context": "Recently, for the univariate matrix pencil method, Liao & Fannjiang [14] and Moitra [15] provide a stability analysis of the MUSIC algorithm.", "startOffset": 84, "endOffset": 88}, {"referenceID": 14, "context": "Moitra [15] studied the optimal relationship between the cutoff frequency and \u2206, showing that if the cutoff frequency is less than 1/\u2206, then stable recovery is not possible with matrix pencil method (with high probability).", "startOffset": 7, "endOffset": 11}, {"referenceID": 0, "context": "Moreover, if the condition number of the factors are upper bounded by a positive constant, then one can compute the unique tensor decomposition V with stability guarantees (See [1] for a review.", "startOffset": 177, "endOffset": 180}, {"referenceID": 13, "context": "1 1-D case: revisiting the matrix pencil method Let us first review the matrix pencil method for the univariate case, which stability was recently rigorously analyzed in Liao & Fannjiang [14] and Moitra [15].", "startOffset": 187, "endOffset": 191}, {"referenceID": 14, "context": "1 1-D case: revisiting the matrix pencil method Let us first review the matrix pencil method for the univariate case, which stability was recently rigorously analyzed in Liao & Fannjiang [14] and Moitra [15].", "startOffset": 203, "endOffset": 207}, {"referenceID": 13, "context": "Furthermore, for m > 1/\u2206, [14, 15] showed that cond2(Vm) is upper bounded by a constant that does not depend on k and m.", "startOffset": 26, "endOffset": 34}, {"referenceID": 14, "context": "Furthermore, for m > 1/\u2206, [14, 15] showed that cond2(Vm) is upper bounded by a constant that does not depend on k and m.", "startOffset": 26, "endOffset": 34}, {"referenceID": 18, "context": "This bound on condition number is also implicitly discussed in [19].", "startOffset": 63, "endOffset": 67}, {"referenceID": 0, "context": "Another way to view the matrix pencil method is that it corresponds to the low rank 3rd order tensor decomposition (see for example [1]).", "startOffset": 132, "endOffset": 135}, {"referenceID": 1, "context": "For m \u2265 k, construct a 3rd order tensor F \u2208 Cm\u00d7m\u00d72 with elements of H0 and H1 defined in (5) as: Fi,i\u2032,j = [Hj\u22121]i,i\u2032 , \u2200j \u2208 [2], i, i\u2032 \u2208 [m].", "startOffset": 125, "endOffset": 128}, {"referenceID": 10, "context": "Given the tensor F , the basic idea of the well-known Jennrich\u2019s algorithm ([11, 13]) for finding the unique low rank tensor decomposition is to consider two random projections v1, v2 \u2208 Rm, and then with high probability the two matrices F (I, I, v1) and F (I, I, v2) admit simultaneous diagonalization.", "startOffset": 76, "endOffset": 84}, {"referenceID": 12, "context": "Given the tensor F , the basic idea of the well-known Jennrich\u2019s algorithm ([11, 13]) for finding the unique low rank tensor decomposition is to consider two random projections v1, v2 \u2208 Rm, and then with high probability the two matrices F (I, I, v1) and F (I, I, v2) admit simultaneous diagonalization.", "startOffset": 76, "endOffset": 84}, {"referenceID": 22, "context": "Tang et al [23] made a similar observation for the univariate case.", "startOffset": 11, "endOffset": 15}, {"referenceID": 2, "context": "First, take d3 number of measurements by evaluating f(s) in the set S3 = {s = en1 + en2 + en3 : [n1, n2, n3] \u2208 [d]\u00d7 [d]\u00d7 [d]}, noting that S3 contains only a subset of d3 points from the grid of [3]d.", "startOffset": 195, "endOffset": 198}, {"referenceID": 1, "context": "Construct the 3rd order tensor F\u0303 \u2208 Cm\u00d7m\u00d73 with noise corrupted measurements f\u0303(s) evaluated at the points in S \u2032 \u2295 S \u2032 \u2295 {v(1), v(2)}, arranged in the following way: F\u0303n1,n2,n3 = f\u0303(s) \u2223\u2223 s=s(n1)+s(n2)+v(n3) ,\u2200n1, n2 \u2208 [m\u2032], n3 \u2208 [2].", "startOffset": 231, "endOffset": 234}, {"referenceID": 0, "context": "Other algorithms, for example tensor power method ([1]) and recursive projection ([24]), which are possibly more stable than Jennrich\u2019s algorithm, can also be applied here.", "startOffset": 51, "endOffset": 54}, {"referenceID": 23, "context": "Other algorithms, for example tensor power method ([1]) and recursive projection ([24]), which are possibly more stable than Jennrich\u2019s algorithm, can also be applied here.", "startOffset": 82, "endOffset": 86}, {"referenceID": 1, "context": "For d > 1, with high probability, all of the 2(m\u2032)2 sampling frequencies in S \u2032\u2295S \u2032\u2295{v(1), v(2)} satisfy that \u2016s(j1) + s(j2) + v3\u2016\u221e \u2264 R\u2032, \u2200j1, j2 \u2208 [m], j3 \u2208 [2], where the per-coordinate cutoff frequency is given by R\u2032 = O(R \u221a logmd).", "startOffset": 158, "endOffset": 161}, {"referenceID": 10, "context": "3 Key Lemmas Stability of tensor decomposition: In this paragraph, we give a brief description and the stability guarantee of the well-known Jennrich\u2019s algorithm ([11, 13]) for low rank 3rd order tensor decomposition.", "startOffset": 163, "endOffset": 171}, {"referenceID": 12, "context": "3 Key Lemmas Stability of tensor decomposition: In this paragraph, we give a brief description and the stability guarantee of the well-known Jennrich\u2019s algorithm ([11, 13]) for low rank 3rd order tensor decomposition.", "startOffset": 163, "endOffset": 171}, {"referenceID": 15, "context": "5) The proof is mostly based on the arguments in [16, 2], we still show the clean arguments here for our case.", "startOffset": 49, "endOffset": 56}, {"referenceID": 1, "context": "5) The proof is mostly based on the arguments in [16, 2], we still show the clean arguments here for our case.", "startOffset": 49, "endOffset": 56}, {"referenceID": 17, "context": "Learning mixture of Gaussians is a fundamental problem in statistics and machine learning, whose study dates back to Pearson[18] in the 1900s, and later arise in numerous areas of applications.", "startOffset": 124, "endOffset": 128}, {"referenceID": 5, "context": "This concentration of distance leads to a line of works of provably learning GMMs in the wellseparated case, started by the seminal work of Dasgupta[6] (spherical and identical \u03a3, \u2206G \u2265 \u03a9(d1/2), complexity poly(d, k)) and followed by works of Dasgupta & Schulman [8] (spherical and identical \u03a3, d log(k), \u2206G \u2265 \u03a9(d1/4), complexity poly(d, k)), Arora & Kannan [21] (general and identical \u03a3, \u2206G \u2265 \u03a9(d1/4) complexity O(kd)).", "startOffset": 148, "endOffset": 151}, {"referenceID": 7, "context": "This concentration of distance leads to a line of works of provably learning GMMs in the wellseparated case, started by the seminal work of Dasgupta[6] (spherical and identical \u03a3, \u2206G \u2265 \u03a9(d1/2), complexity poly(d, k)) and followed by works of Dasgupta & Schulman [8] (spherical and identical \u03a3, d log(k), \u2206G \u2265 \u03a9(d1/4), complexity poly(d, k)), Arora & Kannan [21] (general and identical \u03a3, \u2206G \u2265 \u03a9(d1/4) complexity O(kd)).", "startOffset": 262, "endOffset": 265}, {"referenceID": 20, "context": "This concentration of distance leads to a line of works of provably learning GMMs in the wellseparated case, started by the seminal work of Dasgupta[6] (spherical and identical \u03a3, \u2206G \u2265 \u03a9(d1/2), complexity poly(d, k)) and followed by works of Dasgupta & Schulman [8] (spherical and identical \u03a3, d log(k), \u2206G \u2265 \u03a9(d1/4), complexity poly(d, k)), Arora & Kannan [21] (general and identical \u03a3, \u2206G \u2265 \u03a9(d1/4) complexity O(kd)).", "startOffset": 357, "endOffset": 361}, {"referenceID": 5, "context": "Although this algorithm does not outperform the scaling result in Dasgupta[6], it still sheds light on a different approach of learning GMMs.", "startOffset": 74, "endOffset": 77}, {"referenceID": 4, "context": "3 Open problems In a recent work, Chen & Chi [5] showed that via structured matrix completion, the sample complexity for stable recovery can be reduced to O(k log d).", "startOffset": 45, "endOffset": 48}, {"referenceID": 21, "context": "Recently, Schiebinger et al [22] studied the problem of learning a mixture of shifted and re-scaled point spread functions f(s) = \u2211 j wj\u03c6(s, \u03bc (j)).", "startOffset": 28, "endOffset": 32}], "year": 2015, "abstractText": "Super-resolution is the problem of recovering a superposition of point sources using bandlimited measurements, which may be corrupted with noise. This signal processing problem arises in numerous imaging problems, ranging from astronomy to biology to spectroscopy, where it is common to take (coarse) Fourier measurements of an object. Of particular interest is in obtaining estimation procedures which are robust to noise, with the following desirable statistical and computational properties: we seek to use coarse Fourier measurements (bounded by some cutoff frequency); we hope to take a (quantifiably) small number of measurements; we desire our algorithm to run quickly. Suppose we have k point sources in d dimensions, where the points are separated by at least \u2206 from each other (in Euclidean distance). This work provides an algorithm with the following favorable guarantees: \u2022 The algorithm uses Fourier measurements, whose frequencies are bounded by O(1/\u2206) (up to log factors). Previous algorithms require a cutoff frequency which may be as large as \u03a9( \u221a d/\u2206). \u2022 The number of measurements taken by and the computational complexity of our algorithm are bounded by a polynomial in both the number of points k and the dimension d, with no dependence on the separation \u2206. In contrast, previous algorithms depended inverse polynomially on the minimal separation and exponentially on the dimension for both of these quantities. Our estimation procedure itself is simple: we take random bandlimited measurements (as opposed to taking an exponential number of measurements on the hyper-grid). Furthermore, our analysis and algorithm are elementary (based on concentration bounds for sampling and the singular value decomposition). 1 ar X iv :1 50 9. 07 94 3v 1 [ cs .L G ] 2 6 Se p 20 15", "creator": "LaTeX with hyperref package"}}}