{"id": "1509.08360", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Sep-2015", "title": "Compressive spectral embedding: sidestepping the SVD", "abstract": "Spectral embedding based on the Singular Value Decomposition (SVD) is a widely used \"preprocessing\" step in many learning tasks, typically leading to dimensionality reduction by projecting onto a number of dominant singular vectors and rescaling the coordinate axes (by a predefined function of the singular value). However, the number of such vectors required to capture problem structure grows with problem size, and even partial SVD computation becomes a bottleneck. In this paper, we propose a low-complexity it compressive spectral embedding algorithm, which employs random projections and finite order polynomial expansions to compute approximations to SVD-based embedding. For an m times n matrix with T non-zeros, its time complexity is O((T+m+n)log(m+n)), and the embedding dimension is O(log(m+n)), both of which are independent of the number of singular vectors whose effect we wish to capture. To the best of our knowledge, this is the first work to circumvent this dependence on the number of singular vectors for general SVD-based embeddings. The key to sidestepping the SVD is the observation that, for downstream inference tasks such as clustering and classification, we are only interested in using the resulting embedding to evaluate pairwise similarity metrics derived from the euclidean norm, rather than capturing the effect of the underlying matrix on arbitrary vectors as a partial SVD tries to do. Our numerical results on network datasets demonstrate the efficacy of the proposed method, and motivate further exploration of its application to large-scale inference tasks.", "histories": [["v1", "Mon, 28 Sep 2015 15:32:20 GMT  (27kb)", "http://arxiv.org/abs/1509.08360v1", "NIPS 2015"]], "COMMENTS": "NIPS 2015", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["dinesh ramasamy", "upamanyu madhow"], "accepted": true, "id": "1509.08360"}, "pdf": {"name": "1509.08360.pdf", "metadata": {"source": "CRF", "title": "Compressive spectral embedding: sidestepping the SVD", "authors": ["Dinesh Ramasamy"], "emails": ["dineshr@ece.ucsb.edu", "madhow@ece.ucsb.edu"], "sections": [{"heading": null, "text": "ar Xiv: 150 9.08 360v 1 [stat.ML] 2 8SE p"}, {"heading": "1 Introduction", "text": "This is a first step to reduce dimensionality without actually performing the expensive computationally SVD method. Setting is as follows: The input is represented in the matrix form, which could represent the adjacency matrix or laplacian matrix of a graph."}, {"heading": "2 Related work", "text": "As discussed in Section 3.1, the concept of compression measurements is a key element in our algorithm and is based on the JL problem [10]. The latter, which provides probability guarantees for the approximate conservation of Euclidean geometry for a finite collection of points under random projections, forms the basis for many other applications, such as compression perception [11]. We will now mention a few techniques for exact and approximate SVD calculation before discussing the SVD algorithms as we do. The temporal complexity of the complete SVD is O (mn2) (for m > n)."}, {"heading": "3 Algorithm", "text": "We will first introduce the algorithm for a symmetrical n \u00b7 n matrix S. Later, in Section 3.5, we will show how to deal with a general m \u00b7 n matrix by considering a related (m + n) \u00b7 (m + n) symmetrical matrix. Let us name the eigenvalues of S in descending order and vl their corresponding standard eigenvectors (chosen to be orthogonal in case of repeated eigenvalues). For each function g (x): R 7 \u2192 R, we will use g (S) to denote the n \u00b7 n symmetrical matrix g (S) = \u2211 l = n l = 1 g (\u03bbl) vlv T. We will now develop an O (n log n n) algorithm to calculate a d = O (log n) dimensional embedding that approximates the pair-part distances between the lines of the embedding source vell (vl) [vf \u00b7 2) vl (vl)."}, {"heading": "3.1 Compressive embedding", "text": "Suppose we know f (S). This is an n-dimensional embedding, and similarity queries between two \"vertices\" (we refer to the variables corresponding to the rows of S as vertices, as we would do with matrices derived from graphs) require O (n) operations. However, we can reduce this time to O (log n) by using the JL-Lemma, which informs us that pairs of distances can be roughly captured by compressing projection on d = O (log n) dimensions. Specifically, for d > (4 + 2\u03b2) logn / (2 / 2 \u2212 3 / 3) we can use an n \u00b7 d matrix with i.i.d. equally random entries drawn from {\u00b1 1 / 4 (log n) dimensions. According to the JL-Lemm, pairs of distances between rows of f (S) \u2212 u dimensions are more likely to be at risk."}, {"heading": "3.2 Polynomial approximation of embedding", "text": "The direct calculation of E \u2032 = f (S) \u0445 from the eigenvectors and eigenvalues of S is expensive (O (n3). However, we now observe that the calculation of swaps (S) \u0432 is simple if it is a polynomial. In this case, the calculation of swaps (S) for a polynomial is necessary. Therefore, only LdT + O (Ldn) is required for each of the d columns of the B-pillar. Our strategy is to calculate E \u2032 = f (S) as a result of L-matrix vector products intertwined with vector additions running in parallel. Our strategy is to supplement E \u2032 = f (S) with E-matrix vector products, where f-L (x) is a polynomia of the L-th order (Ldn)."}, {"heading": "3.3 Performance guarantees", "text": "The spectral standard of the \"error matrix\" Z = f (S) \u2212 q (S) = q (S) = p (S) = p (S) \u2212 p (S) \u00b7 p (S) \u2212 p (S) \u00b7 p (S) \u00b7 p (S) \u00b7 p (S) \u00b7 p (S) \u00b7 p (S) \u00b7 p (S) \u2212 p (S) \u00b7 p (S) \u2212 p (S) \u00b7 p (S) \u00b7 p (S) \u00b7 p (S) \u00b7 p (S) \u00b7 p (S) \u00b7 p (S) \u00b7 p (S) \u00b7 p (S) \u00b7 p (S) \u00b7 p (S)."}, {"heading": "3.4 Choosing the polynomial approximation", "text": "We limit attention to matrices that fulfill the matrix Z (f), which means that we can generate (F) values (F) values. (F) We observe that we can trivially center the spectrum of any matrix to fulfill this assumption if we have the following limits: f (x) to f (x) to f (x) = f (x) to f (x) to f (x) = f (x) (x). We have shown that the errors introduced by the polynomial approximation of f (x) to the \"good\" approximation can be summarized in Section 3.3, that the polynomial approximation to the polynomial approximation to the values can be summarized. (F) We have shown that the errors introduced by the polynomial approximation to the polynomial approximation can be summarized in Section 3.3."}, {"heading": "3.5 Embedding general matrices", "text": "We complete the description of the algorithm by generalizing to any m \u00b7 n matrix A (not necessarily symmetrical) in such a way that both values are the same. Consider the following spectral mapping of the rows of A to the rows of Erow = [f (\u03c31) u1 \u00b7 \u00b7 f (\u03c31) and the columns of A to the rows of Ecol = [f (\u03c31) v1 \u00b7 f (\u0441n) vn = Ecol and the rows of Ecol = [f (\u04411) v1 \u00b7 \u00b7 f (\u0441\u0442\u043e\u043b\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u043d\u0438nynynynynynynyny\u043d\u0438\u043d\u0438\u043d\u0438\u0439). The approach is to calculate an approximate d-dimensional embedding of the symmetrix S = [0 AT; A 0]."}, {"heading": "4 Implementation considerations", "text": "To ensure that the eigenvalues of S are within [\u2212 1, 1] of the spectral standard, as we have assumed, we scale the matrix based on its spectral standard (\u0432 S = max | \u03bbl |) and then scale it by a small factor (1.01) for our estimate (typically an upper limit) of the spectral standard using potentiality iteration (20 iterates to 6 logn randomly selected start vectors) and then scale it by a small factor (1.01) for our estimate (typically an upper limit). Polynomial approximation to the order L: The error in the approximation of f (\u03bb) to f-L (\u03bb) according to f-Logn initial vectors, measured by means of which we measure these values (x) \u2212 f-reduction of L (x) \u2212 f-reduction of the polynomic order L-reduction, often corresponding to a reduction in L-order."}, {"heading": "5 Numerical results", "text": "While the proposed approach is particularly useful for large problems in which exact self-determination with values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of appropriation of values of appropriation of appropriation of values of appropriation of appropriation of values of appropriation of appropriation of values of appropriation of appropriation of values of appropriation of appropriation of values of appropriation of appropriation of values of appropriation of appropriation of values of appropriation of values of appropriation of appropriation of values of appropriation of values of appropriation of appropriation of values of appropriation of appropriation of values of appropriation of values of appropriation of appropriation of values of appropriation of appropriation of values of appropriation of values of appropriation of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of values of appropriation of values of appropriation of values of appropriation of values of appropriation of values of values of appropriation of values of appropriation of values of appropriation of values of values of appropriation of values of appropriation of values of values of appropriation of values of appropriation of values of values of appropriation of values of appropriation of values of appropriation of values of values of appropriation of values of appropriation of values of values of appropriation of appropriation of values of values of appropriation of values of appropriation of values of values of values of the"}, {"heading": "6 Conclusion", "text": "We have shown that random projections and polynomial expansions provide a powerful approach to the spectral embedding of large matrices: For a m \u00b7 n matrix A, our O ((T + m + n) log (m + n))) -dimensional compressive embedding, which has been shown to approximate the distances between points in the desired spectral embedding in pairs. Numerical results for several real datasets show that our method provides good approximations for embedding based on partial SVD, while causing much less complexity. Furthermore, our method can also approximate spectral embedding, which depend on the entire SVD, as their complexity does not depend on the number of dominant vectors whose effect we would like to model. An insight into this potential is provided by the example of K-mean-based clustering to estimate thinner sections of the Amazon diagram, where our method delivers a much better performance (based on a partial improvement of the metrical performance of SVD)."}, {"heading": "Acknowledgments", "text": "This work is partially supported by DARPA GRAPHS (BAA-12-01) and Systems on Nanoscale Information fabriCs (SONIC), one of the six SRC STARnet Centers sponsored by MARCO and DARPA. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies."}], "references": [{"title": "Kernel principal component analysis", "author": ["B. Sch\u00f6lkopf", "A. Smola", "K.-R. M\u00fcller"], "venue": "Artificial Neural Networks ICANN\u201997, ser. Lecture Notes in Computer Science, W. Gerstner, A. Germond, M. Hasler, and J.-D. Nicoud, Eds. Springer Berlin Heidelberg, 1997, pp. 583\u2013588. 8", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "Kernel PCA and de-noising in feature spaces", "author": ["S. Mika", "B. Sch\u00f6lkopf", "A.J. Smola", "K.-R. M\u00fcller", "M. Scholz", "G. R\u00e4tsch"], "venue": "Advances in Neural Information Processing Systems, 1999.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1999}, {"title": "A spectral clustering approach to finding communities in graph.", "author": ["S. White", "P. Smyth"], "venue": "in SDM,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Random walks on graphs", "author": ["F. G\u00f6bel", "A.A. Jagers"], "venue": "Stochastic Processes and their Applications, 1974.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1974}, {"title": "Graph spectra and the detectability of community structure in networks", "author": ["R.R. Nadakuditi", "M.E.J. Newman"], "venue": "Physical Review Letters, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Spectral grouping using the Nystr\u00f6m method", "author": ["C. Fowlkes", "S. Belongie", "F. Chung", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 26, no. 2, 2004.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "On the Nystr\u00f6m Method for Approximating a Gram Matrix for Improved Kernel-Based Learning", "author": ["P. Drineas", "M.W. Mahoney"], "venue": "Journal on Machine Learning Resources, 2005.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions", "author": ["N. Halko", "P.G. Martinsson", "J.A. Tropp"], "venue": "SIAM Rev., 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Database-friendly random projections", "author": ["D. Achlioptas"], "venue": "Proceedings of the Twentieth ACM SIGMOD- SIGACT-SIGART Symposium on Principles of Database Systems, ser. PODS \u201901, 2001.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2001}, {"title": "An introduction to compressive sampling", "author": ["E. Candes", "M. Wakin"], "venue": "Signal Processing Magazine, IEEE, March 2008.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Simultaneous iteration for the matrix eigenvalue problem", "author": ["S.F. McCormick", "T. Noe"], "venue": "Linear Algebra and its Applications, vol. 16, no. 1, pp. 43\u201356, 1977.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1977}, {"title": "Improved Nystr\u00f6m Low-rank Approximation and Error Analysis", "author": ["K. Zhang", "I.W. Tsang", "J.T. Kwok"], "venue": "Proceedings of the 25th International Conference on Machine Learning, ser. ICML \u201908. ACM, 2008.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Fast Approximate Spectral Clustering", "author": ["D. Yan", "L. Huang", "M.I. Jordan"], "venue": "Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD \u201909. ACM, 2009.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Making Large-Scale Nystr\u00f6m Approximation Possible.", "author": ["M. Li", "J.T. Kwok", "B.-L. Lu"], "venue": "in ICML,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Ensemble Nystr\u00f6m method", "author": ["S. Kumar", "M. Mohri", "A. Talwalkar"], "venue": "Advances in Neural Information Processing Systems, 2009.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Power iteration clustering", "author": ["F. Lin", "W.W. Cohen"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), 2010.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Scalable methods for graph-based unsupervised and semi-supervised learning", "author": ["F. Lin"], "venue": "Ph.D. dissertation, Carnegie Mellon University, 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "PIC: Parallel power iteration clustering for big data", "author": ["W. Yan", "U. Brahmakshatriya", "Y. Xue", "M. Gilder", "B. Wise"], "venue": "Journal of Parallel and Distributed Computing, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Random walks on graphs: A survey", "author": ["L. Lov\u00e1sz"], "venue": "Combinatorics, Paul erdos is eighty, vol. 2, no. 1, pp. 1\u201346, 1993.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1993}, {"title": "Nearly-linear time algorithms for graph partitioning, graph sparsification, and solving linear systems", "author": ["D.A. Spielman", "S.-H. Teng"], "venue": "Proceedings of the Thirty-sixth Annual ACM Symposium on Theory of Computing, ser. STOC \u201904. New York, NY, USA: ACM, 2004.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Nearly linear time algorithms for preconditioning and solving symmetric, diagonally dominant linear systems", "author": ["D. Spielman", "S. Teng"], "venue": "SIAM Journal on Matrix Analysis and Applications, vol. 35, Jan. 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Graph sparsification by effective resistances", "author": ["D. Spielman", "N. Srivastava"], "venue": "SIAM Journal on Computing, 2011.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Kernel polynomial approximations for densities of states and spectral functions", "author": ["R.N. Silver", "H. Roeder", "A.F. Voter", "J.D. Kress"], "venue": "Journal of Computational Physics, vol. 124, no. 1, pp. 115\u2013130, Mar. 1996.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1996}, {"title": "Efficient estimation of eigenvalue counts in an interval", "author": ["E. Di Napoli", "E. Polizzi", "Y. Saad"], "venue": "arXiv:1308.4275 [cs], Aug. 2013.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Defining and evaluating network communities based on ground-truth", "author": ["J. Yang", "J. Leskovec"], "venue": "2012 IEEE 12th International Conference on Data Mining (ICDM), Dec. 2012.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Community detection in graphs", "author": ["S. Fortunato"], "venue": "Physics Reports, vol. 486, no. 3-5, Feb. 2010. 9", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": ",m} (kernel PCA)[1][2] such as A(p, q) = e 2/2\u03b12 (or) A(p, q) = I(\u2016x(p)\u2212 x(q)\u2016 < \u03b1), 1 \u2264 p, q \u2264 l, (1) where I(\u00b7) denotes the indicator function or matrices derived from K-nearest-neighbor graphs constructed from {x(p)}.", "startOffset": 16, "endOffset": 19}, {"referenceID": 1, "context": ",m} (kernel PCA)[1][2] such as A(p, q) = e 2/2\u03b12 (or) A(p, q) = I(\u2016x(p)\u2212 x(q)\u2016 < \u03b1), 1 \u2264 p, q \u2264 l, (1) where I(\u00b7) denotes the indicator function or matrices derived from K-nearest-neighbor graphs constructed from {x(p)}.", "startOffset": 19, "endOffset": 22}, {"referenceID": 2, "context": "Other important choices include f(x) = constant used to cut graphs [3] and f(x) = 1 /\u221a 1\u2212 x for commute time embedding of graphs [4].", "startOffset": 67, "endOffset": 70}, {"referenceID": 3, "context": "Other important choices include f(x) = constant used to cut graphs [3] and f(x) = 1 /\u221a 1\u2212 x for commute time embedding of graphs [4].", "startOffset": 129, "endOffset": 132}, {"referenceID": 4, "context": ", see [5], which analyzes graph adjacency matrices).", "startOffset": 6, "endOffset": 9}, {"referenceID": 5, "context": "The number of singular vectors k needed to capture the structure of an m\u00d7 n matrix grows with its size, and two bottlenecks emerge as we scale: (a) The computational effort required to extract a large number of singular vectors using conventional iterative methods such as Lanczos or simultaneous iteration or approximate algorithms like Nystrom [6], [7] and Randomized SVD [8] for computation of partial SVD becomes prohibitive (scaling as \u03a9(kT ), where T is the number of non-zeros in A) (b) the resulting k-dimensional embedding becomes unwieldy for use in subsequent inference steps.", "startOffset": 346, "endOffset": 349}, {"referenceID": 6, "context": "The number of singular vectors k needed to capture the structure of an m\u00d7 n matrix grows with its size, and two bottlenecks emerge as we scale: (a) The computational effort required to extract a large number of singular vectors using conventional iterative methods such as Lanczos or simultaneous iteration or approximate algorithms like Nystrom [6], [7] and Randomized SVD [8] for computation of partial SVD becomes prohibitive (scaling as \u03a9(kT ), where T is the number of non-zeros in A) (b) the resulting k-dimensional embedding becomes unwieldy for use in subsequent inference steps.", "startOffset": 351, "endOffset": 354}, {"referenceID": 7, "context": "The number of singular vectors k needed to capture the structure of an m\u00d7 n matrix grows with its size, and two bottlenecks emerge as we scale: (a) The computational effort required to extract a large number of singular vectors using conventional iterative methods such as Lanczos or simultaneous iteration or approximate algorithms like Nystrom [6], [7] and Randomized SVD [8] for computation of partial SVD becomes prohibitive (scaling as \u03a9(kT ), where T is the number of non-zeros in A) (b) the resulting k-dimensional embedding becomes unwieldy for use in subsequent inference steps.", "startOffset": 374, "endOffset": 377}, {"referenceID": 8, "context": "1, the concept of compressive measurements forms a key ingredient in our algorithm, and is based on the JL lemma [10].", "startOffset": 113, "endOffset": 117}, {"referenceID": 9, "context": "The latter, which provides probabilistic guarantees on approximate preservation of the Euclidean geometry for a finite collection of points under random projections, forms the basis for many other applications, such as compressive sensing [11].", "startOffset": 239, "endOffset": 243}, {"referenceID": 10, "context": "complexity of standard iterative eigensolvers such as simultaneous iteration[13] and the Lanczos method scales as \u03a9(kT ) [12], where T denotes the number of non-zeros of A.", "startOffset": 76, "endOffset": 80}, {"referenceID": 5, "context": "A commonly employed approximate eigendecomposition algorithm is the Nystrom method [6], [7] based on random sampling of s columns of A, which has time complexity O(ksn+ s).", "startOffset": 83, "endOffset": 86}, {"referenceID": 6, "context": "A commonly employed approximate eigendecomposition algorithm is the Nystrom method [6], [7] based on random sampling of s columns of A, which has time complexity O(ksn+ s).", "startOffset": 88, "endOffset": 91}, {"referenceID": 11, "context": "These aim to improve accuracy using preprocessing steps such as K-means clustering [14] or random projection trees [15].", "startOffset": 83, "endOffset": 87}, {"referenceID": 12, "context": "These aim to improve accuracy using preprocessing steps such as K-means clustering [14] or random projection trees [15].", "startOffset": 115, "endOffset": 119}, {"referenceID": 13, "context": "Methods to reduce the complexity of the Nystrom algorithm to O(ksn+ k)[16], [17] enable Nystrom sketches that see more columns of A.", "startOffset": 70, "endOffset": 74}, {"referenceID": 14, "context": "Methods to reduce the complexity of the Nystrom algorithm to O(ksn+ k)[16], [17] enable Nystrom sketches that see more columns of A.", "startOffset": 76, "endOffset": 80}, {"referenceID": 7, "context": "Other randomized algorithms, involving iterative computations, include the Randomized SVD [8].", "startOffset": 90, "endOffset": 93}, {"referenceID": 15, "context": "In [18], [19], vertices of a graph are embedded based on diffusion of probability mass in random walks on the graph, using the power iteration run independently on random starting vectors, and stopping \u201cprior to convergence.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "In [18], [19], vertices of a graph are embedded based on diffusion of probability mass in random walks on the graph, using the power iteration run independently on random starting vectors, and stopping \u201cprior to convergence.", "startOffset": 9, "endOffset": 13}, {"referenceID": 17, "context": "A parallel implementation of this algorithm was considered in [20]; similar parallelization directly applies to our algorithm.", "startOffset": 62, "endOffset": 66}, {"referenceID": 3, "context": "Another specific application that falls within our general framework is the commute time embedding on a graph, based on the normalized adjacency matrix and weighing function f(x) = 1/ \u221a 1\u2212 x [4], [21].", "startOffset": 191, "endOffset": 194}, {"referenceID": 18, "context": "Another specific application that falls within our general framework is the commute time embedding on a graph, based on the normalized adjacency matrix and weighing function f(x) = 1/ \u221a 1\u2212 x [4], [21].", "startOffset": 196, "endOffset": 200}, {"referenceID": 19, "context": "Approximate commute time embeddings have been computed using Spielman-Teng solvers [22], [23] and the JL lemma in [24].", "startOffset": 83, "endOffset": 87}, {"referenceID": 20, "context": "Approximate commute time embeddings have been computed using Spielman-Teng solvers [22], [23] and the JL lemma in [24].", "startOffset": 89, "endOffset": 93}, {"referenceID": 21, "context": "Approximate commute time embeddings have been computed using Spielman-Teng solvers [22], [23] and the JL lemma in [24].", "startOffset": 114, "endOffset": 118}, {"referenceID": 22, "context": "It is interesting to note that methods similar to ours have been used in a different context, to estimate the empirical distribution of eigenvalues of a large hermitian matrix [25], [26].", "startOffset": 176, "endOffset": 180}, {"referenceID": 23, "context": "It is interesting to note that methods similar to ours have been used in a different context, to estimate the empirical distribution of eigenvalues of a large hermitian matrix [25], [26].", "startOffset": 182, "endOffset": 186}, {"referenceID": 8, "context": "Applying the JL lemma [10] to the rows of f\u0303L(S), we have that when d > O ( \u01eb logn ) with i.", "startOffset": 22, "endOffset": 26}, {"referenceID": 24, "context": "We consider two real world undirected graphs in [27] for our evaluation, and compute embeddings for the normalized adjacency matrix \u00c3 (= DAD, where D is a diagonal matrix with row sums of the adjacency matrix A; the eigenvalues of \u00c3 lie in [\u22121, 1]) for graphs.", "startOffset": 48, "endOffset": 52}, {"referenceID": 24, "context": "DBLP collaboration network [27] is an undirected graph on n = 317080 vertices with 1049866 edges.", "startOffset": 27, "endOffset": 31}, {"referenceID": 24, "context": "Application to graph clustering for the Amazon co-purchasing network [27] : This is an undirected graph on n = 334863 vertices with 925872 edges.", "startOffset": 69, "endOffset": 73}, {"referenceID": 25, "context": "We consider 25 instances of K-means clustering with K = 200 throughout, reporting the median of a commonly used graph clustering score, modularity [28] (larger values translate to better clustering solutions).", "startOffset": 147, "endOffset": 151}, {"referenceID": 7, "context": "When we replace the exact eigenvector embedding E with approximate eigendecomposition using Randomized SVD [8] (parameters: power iterates q = 5 and excess dimensionality l = 10), the time taken reduces from 10 minutes to 17 seconds, but this comes at the expense of inference quality: median modularity drops to 0.", "startOffset": 107, "endOffset": 110}], "year": 2015, "abstractText": "Spectral embedding based on the Singular Value Decomposition (SVD) is a widely used \u201cpreprocessing\u201d step in many learning tasks, typically leading to dimensionality reduction by projecting onto a number of dominant singular vectors and rescaling the coordinate axes (by a predefined function of the singular value). However, the number of such vectors required to capture problem structure grows with problem size, and even partial SVD computation becomes a bottleneck. In this paper, we propose a low-complexity compressive spectral embedding algorithm, which employs random projections and finite order polynomial expansions to compute approximations to SVD-based embedding. For anm\u00d7n matrix with T non-zeros, its time complexity is O ((T +m+ n) log(m+ n)), and the embedding dimension is O(log(m + n)), both of which are independent of the number of singular vectors whose effect we wish to capture. To the best of our knowledge, this is the first work to circumvent this dependence on the number of singular vectors for general SVD-based embeddings. The key to sidestepping the SVD is the observation that, for downstream inference tasks such as clustering and classification, we are only interested in using the resulting embedding to evaluate pairwise similarity metrics derived from the l2-norm, rather than capturing the effect of the underlying matrix on arbitrary vectors as a partial SVD tries to do. Our numerical results on network datasets demonstrate the efficacy of the proposed method, and motivate further exploration of its application to large-scale inference tasks.", "creator": "LaTeX with hyperref package"}}}