{"id": "1509.08731", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2015", "title": "Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning", "abstract": "The mutual information is a core statistical quantity that has applications in all areas of machine learning, whether this is in training of density models over multiple data modalities, in maximising the efficiency of noisy transmission channels, or when learning behaviour policies for exploration by artificial agents. Most learning algorithms that involve optimisation of the mutual information rely on the Blahut-Arimoto algorithm --- an enumerative algorithm with exponential complexity that is not suitable for modern machine learning applications. This paper provides a new approach for scalable optimisation of the mutual information by merging techniques from variational inference and deep learning. We develop our approach by focusing on the problem of intrinsically-motivated learning, where the mutual information forms the definition of a well-known internal drive known as empowerment. Using a variational lower bound on the mutual information, combined with convolutional networks for handling visual input streams, we develop a stochastic optimisation algorithm that allows for scalable information maximisation and empowerment-based reasoning directly from pixels to actions.", "histories": [["v1", "Tue, 29 Sep 2015 13:04:03 GMT  (5560kb,D)", "http://arxiv.org/abs/1509.08731v1", "Proceedings of the 29th Conference on Neural Information Processing Systems (NIPS 2015)"]], "COMMENTS": "Proceedings of the 29th Conference on Neural Information Processing Systems (NIPS 2015)", "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG", "authors": ["shakir mohamed", "danilo jimenez rezende"], "accepted": true, "id": "1509.08731"}, "pdf": {"name": "1509.08731.pdf", "metadata": {"source": "CRF", "title": "Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning", "authors": ["Shakir Mohamed", "Danilo J. Rezende"], "emails": ["danilor}@google.com"], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2 Intrinsically-motivated Reinforcement Learning", "text": "Consider an online learning system that must model through its incoming data flows and interact with its environment. [23] This perception and action loop is common to many areas, such as active learning, process control, black box optimization, and enhanced learning. An agent receives observations and takes action in the external environment. Importantly, the source and nature of reward signals is not assumed by an oracle in the external environment, but by an internal environment that is part of the agent's decision-making system."}, {"heading": "3 Mutual Information and Empowerment", "text": "Mutual information is a central information theoretical quantity that functions as a general measure of dependence between two random variables x and y, defined as: I (x, y) = Ep (y) p (x) [log (x, y) p (x) p (y) p (y) p (y)]]], (1) where the p (x, y) is a common distribution over the random variables and p (x) and p (y) are the corresponding marginal distribution steps. x and y can be many variables of interest: in composed neuroscience they are the sensory inputs and the spiking population code; in telecommunications they are the input signal to a channel and the transmission received; when learning exploration policy in RL, they are the current state and action at a certain time in the future, respectful. For intrinsic motivation we use an internal reward measure called empowerment."}, {"heading": "4 Scalable Information Maximisation", "text": "The mutual information (MI) as we have described it so far, whether for issues of empowerment, channel capacity or rate distortion, hides two difficult statistical problems. Firstly, the calculation of the MI includes expectations about the unknown probability of transition. This can be done by rewriting the MI in relation to the difference between conditional entropies H (\u00b7) as: I (a, s \u2032 | s) = H (a | s \u2032, s), (3) where H (a | s) = \u2212 E\u03c9 (a | s) [log\u03c9 (a | s) and H (a | s \u2032, s \u2032, s) = \u2212 Ep (s \u2032, s, s). This calculation requires a marginalization by the K-step transition dynamics of the environment p (s \u2032 | a, s)."}, {"heading": "4.1 Variational Information Lower Bound", "text": "The use of the Entropieformel des MI (3) makes it clear that the limitation of the conditional entropy component is sufficient to bind all the mutual information. By using the non-negative property of the KL-Divergence, we obtain the limitation: KL [p (x | y); KL (x | y); KL (x | y); KL (s); H (a | s); H (a); Ep (s); Ep (s); Ep (s); Ep (s); Ep (x | y); Ep (a); Ep (c); c (c); c (c); c); c (c); c (c); c (c); c (c); Ep (c); c); Ep (a); c); Ep (c); Ep (c); Ep (c); Ep (c); Ep (c); Ep (c); Ep (c); Ep (c); Ep); Ep (c); Ep (c); Ep (c); Ep (c); Ep (c); c); c); c (c) (c) (c); c) (c) (c) (c); c) (c) (c); c) (c) (c) (c); c) (c) (c) (c) (c) c))) (c) (c))) (c) (c) (c)) (c) (c))))) (c) (c) (c)) (c) (c) (c) (c)) (c) (c)) (c) (c))) (c)))) (c) (c)) (c)) (c)) (c) (c)) (c)) (3) (3)) (3) (3)."}, {"heading": "4.2 Variational Information Maximisation", "text": "A simple optimization method based on (4) is alternately optimizing the parameters of the distributions q\u0442 (\u00b7) and \u03c9\u03b8 (\u00b7). Barber & Agakov [1] have established the link between this approach and the generalized EM algorithm and refer to it as the IM (information maximization) algorithm, and we follow the same optimization principle. From an optimization perspective, the deviations may differ by adding a restriction to the value of entropy H (a), which leads to the limited optimization problem: E (s) = max., q (s)."}, {"heading": "4.2.1 Maximum Likelihood Decoder", "text": "The first step of the alternating optimization is the optimization of the equation (5) w.r.t. of the decoder q and is a supervised highest probability problem. In view of a series of data of past interactions with the environment, we learn a distribution from the starting or finishing states s, s, up to the action sequences taken a. We parameterize the decoder as an auto-regressive distribution via the K-step action sequence: q\u0443 (a | s, s) = q (a1 | s, s \u2032) K-k = 2 q (ak \u2212 1, s \u2032), (6) We can freely select the distributions q (ak) for each action in the sequence, which we select as categorical distributions, the middle parameters of which are the result of the function f\u0442 (\u00b7) with parameters b. f is a non-linear function, which we specify with the help of a two-layer neural network, which we activate with the help of this log network."}, {"heading": "4.2.2 Estimating the Source Distribution", "text": "In view of a current estimate of the decoder q, the variational solution for the distribution \u03c9 (a | s) is solved by the solution of the functional derivative process \u03b4I\u03c9 (s) / \u03b4\u03c9 (a | s) = 0 with the restriction that \"a\" (a | s) = 1, by \"a\" (a)? (s) = 1Z (s) exp (s, a), whereu (s, a) = Ep (s \u2032 | s, a) [ln qs (a, s \u2032), u (s, s \u00b2), u (s, a), and Z (s) = \"a\" (s, a) is a normalization term. By replacing this optimal distribution with the original objective (5), we find that it can only be expressed in terms of the normalization function Z (s), E (s) = 1\u03b2 logZ (s)."}, {"heading": "4.3 Empowerment-based Behaviour policies", "text": "We can treat the empowerment value E (s) as a government-dependent reward, and then use any standard planning algorithm, such as Q-Learning, political gradients, or the Monte Carlo Search. We use the simplest planning strategy, using a one-step empowerment maximization, which amounts to selecting measures a = arg maxa C (s, a) in which C (s, a) = Ep (s, a) [E (s)]. This policy does not take into account the effects of measures that go beyond the planning horizon. A natural improvement is to use the value classification [25] to allow the agent to take action by maximizing its long-term (potentiallyTr ueM ueM ITr ueM and MIy = \u2212 0.083 + 1 expressive effect x, r2 = 0.903 \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "4.4 Algorithm Summary and Complexity", "text": "The system we have described is a scalable and universally valid algorithm for maximizing mutual information, and we summarize the core components in Figure 2 and Algorithm 1. The state representation mechanism is transformed by converting raw observations x, x to produce the initial and final state. If the raw observations are pixels from the vision, state representation is a conventional neural network [13, 15], while for other observations we use a fully connected neural network - we show the parameters of these models with which we identify."}, {"heading": "5 Results", "text": "We demonstrate the use of empowerment and the effectiveness of varying information maximization in two types of environments. Static environments consist of spaces and labyrinths in different configurations in which there are no objects with which the agent can interact, or other moving objects. The number of states in these environments corresponds to the number of locations in the environment and is therefore still manageable for approaches based on state enumeration. In dynamic environments, aspects of the environment change, such as flowing lava that causes the agent to reset, or a predator that hunts the agent. Mostly, we consider discrete action situations in which the agent has five actions (top, bottom, left, right, nothing). The agent may have other actions, such as picking up a key or dropping a brick. There are no external rewards and the agent must use purely visual (pixel) information. 5. For all these experiments, we used one horizon from K."}, {"heading": "5.1 Effectiveness of the MI Bound", "text": "First, we find that using the lower limit variable information leads to the same behavior as using the exact reciprocal information in a series of static environments. We look at environments that have a maximum of 400 discrete states, and calculate the true reciprocal information using the Blahut-Arimoto algorithm. We calculate the variable information bound in the same environment using pixel information (on 20 x 20 images). To compare the two approaches, we look at the empowerment landscape achieved by calculating the empowerment at any location in the environment, and display it as heatmaps. For action selection, it depends on the position of the maximum empowerment, and by comparing the heatmaps in Figure 3 we see that the empowerment landscape matches between the exact and the variational solution, and that it therefore leads to the same agent behavior.In each image in Figure 3 we show a thermal map of the empowerment for each location in the environment."}, {"heading": "5.2 Dynamic Environments", "text": "Once we have established the usefulness of the Bound and a wider understanding of empowerment, we now examine empowerment behavior in environments with dynamic properties. Even in small environments, the number of states becomes extremely large if there are objects that can be moved, or added and removed from the environment, making enumerative algorithms (such as BA) quickly unfeasible, since we have an exponential explosion in the number of states. We first reproduce an experiment from Salge et al. [21, \u00a7 4.5.3] which takes into account the empowered behavior of an agent in a room environment, a room that is empty, has a moving box, has a set of moving boxes, has a set of moving boxes."}, {"heading": "5.3 Predator-Prey Scenario", "text": "We demonstrate the applicability of our approach to continuous settings by studying a simple 3D physics simulation [27] shown in Figure 7. Here, the agent (blue) is pursued by a predator (red) and randomly reset to a new location in the environment when caught by the predator. Both the agent and the predator are depicted in the environment as balls that roll on a surface with friction, the state being the position, speed and angular momentum of the agent and the predator, and the action is a 2D force vector. As expected, the maximum empowerment is in regions away from the predator, which causes the agent to learn to escape the predator."}, {"heading": "6 Conclusion", "text": "We have developed a new approach to a scalable assessment of mutual information, taking advantage of recent advances in deep learning and variable inferences. We focused specifically on intrinsic motivation with a reward measure known as empowerment, which essentially requires the efficient calculation of mutual information. By using a variable lower limit of mutual information, we have developed a scalable model and an efficient algorithm that extends the applicability of empowerment to high-dimensional problems, while the complexity of our approach is highly advantageous compared to the complexity of the Blahut-Arimoto algorithm, which is currently the standard. The overall system does not require a generative model of the environment to be created, learns only in interactions with the environment, and allows the agent to learn directly from visual information or in continuous governmental spheres. While we have chosen to develop the algorithm in terms of intrinsic motivation, the mutual information has broad applications in other scalable areas that all benefit from an algorithmic."}, {"heading": "A Empowerment as Path-counting", "text": "For the sake of simplicity, we will focus on deterministic and discrete environments. In this context, the transition probability p (s) | s, a) is a delta distribution p (s) | s, a) = \u03b4 (s) \u2212 T (s, a), where T (s, a) is a transition function that begins in state s, performs the action sequence a and delivers the resulting state. The solution equation (2) for the optimal source distribution (a), (a), (a), visit (a), using Blahut Arimoto [4], yields the fixation: (k + 1) (a), (k) (a), (a), for the optimal source distribution (a), (a), visit (a), (a), (s), (b), (s), (a), (c, c, c, c, c, (, c, c, c, c, c, b, b, n, c, b, b, c, b, b, b, b, b, c, c, a, c, a, (), c, c, a, (), c, c, a, c, c, a, (, c, c, c, b, b, b, b, b, b, b, b, b, b, b, n, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, (c, b, b, (c, a), (c, (c, a), (c, a), (c, (), (c, (c, a), (c, 2), (c, 2)."}, {"heading": "B Model-based Empowerment", "text": "The approach we described in the main text was \"model-free\" in the sense that it does not use a model of transition dynamics of the environment. Building accurate transition models can be difficult and much success in RL has been achieved with model-free methods. Ideally, we would like to apply a model-based method, as this allows reflection on task-independent aspects of the world.B.1 Importance Sampling EstimatorThe most-generic model-based empowerment method is to approximate the empowerment that we developed for generic importance-sampling estimator. We assume that a model of the environment p (s) is available, but at this point it is not specified how this model is obtained."}, {"heading": "C Deriving the Blahut-Arimoto Iterations from the Variational Bound", "text": "Here we show that the Blahut-Arimoto algorithm can be derived from the limit of variation (8). The distribution of variation q (a | s \u2032, s), which maximizes the limit (8), is the posterior distribution over the given present and future states, q? (a | s \u2032, s) = p (a | s \u2032, s), p (s \u00b2, s (s \u00b2, s). (16) The Blahut-Arimoto algorithm is achieved by replacing equation (16) into equation (10) and reclassifying the terms: \u03c9t + 1 (a | s \u00b2, s \u00b2 exp (\u03b2Ep (s \u00b2, A) [ln qt (a | s \u00b2, s \u00b2, A), p (s \u00b2, A) [ln pt (a \u00b2, s \u00b2), p (s \u00b2, s \u00b2), p (s \u00b2) (17), p () (s \u00b2, s \u00b2, s \u00b2, A)."}, {"heading": "D Neural Network Description", "text": "D.1 State representation by means of Convolutionary NetworkFor observations that are images, we use a Convolutionary Network (q = q | 1) to obtain a State representation. We use the same Convolutionary Network for all experiments. After each convolution, we apply a corrected nonlinearity. For all experiments, we use a 10-filter for each layer of convolution. The first convolution consists of 4-4 nuclei with a step of 1, and the second convolution consists of 3-3 nuclei with a step of 2. The output of the convolution is guided through a fully connected layer with 100 hidden units, followed by a rectified nonlinearity. This 100-dimensional representation is what constitutes the state representation s used for the variational information maximization components that follow this processing stage. D.2 Parameterization of other networks: We also use neural networks in the parameterization of the decoder distribution qak (component, component) and the distribution component qak."}], "references": [{"title": "Stochastic gradient vb and the variational auto-encoder", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "International Conference on Learning Representations,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Stochastic back-propagation and variational inference in deep latent gaussian models", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Empowerment\u2013an introduction", "author": ["C. Salge", "C. Glackin", "D. Polani"], "venue": "In Guided Self- Organization: Inception,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "The Blahut-Arimoto algorithms", "author": ["R.W. Yeung"], "venue": "In Information Theory and Network Coding,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}], "referenceMentions": [{"referenceID": 3, "context": "The problem of measuring and harnessing dependence between random variables is an inescapable statistical problem that forms the basis of a large number of applications in machine learning, including rate distortion theory [4], information bottleneck methods [26], population coding [1], curiositydriven exploration [24, 19], model selection [3], and intrinsically-motivated reinforcement learning [20].", "startOffset": 223, "endOffset": 226}, {"referenceID": 0, "context": "The problem of measuring and harnessing dependence between random variables is an inescapable statistical problem that forms the basis of a large number of applications in machine learning, including rate distortion theory [4], information bottleneck methods [26], population coding [1], curiositydriven exploration [24, 19], model selection [3], and intrinsically-motivated reinforcement learning [20].", "startOffset": 283, "endOffset": 286}, {"referenceID": 2, "context": "The problem of measuring and harnessing dependence between random variables is an inescapable statistical problem that forms the basis of a large number of applications in machine learning, including rate distortion theory [4], information bottleneck methods [26], population coding [1], curiositydriven exploration [24, 19], model selection [3], and intrinsically-motivated reinforcement learning [20].", "startOffset": 342, "endOffset": 345}, {"referenceID": 0, "context": "1 Variational Information Lower Bound The MI can be made more tractable by deriving a lower bound to it and maximising this instead \u2014 here we present the bound derived by Barber & Agakov [1].", "startOffset": 187, "endOffset": 190}, {"referenceID": 1, "context": "Other lower bounds for the mutual information are also possible: Jaakkola & Jordan [9] present a lower bound by using the convexity bound for the logarithm; Brunel & Nadal [2] use a Gaussian assumption and appeal to the Cramer-Rao lower bound.", "startOffset": 172, "endOffset": 175}, {"referenceID": 0, "context": "Barber & Agakov [1] made the connection between this approach and the generalised EM algorithm and refer to it as the IM (information maximisation) algorithm and we follow the same optimisation principle.", "startOffset": 16, "endOffset": 19}], "year": 2015, "abstractText": "The mutual information is a core statistical quantity that has applications in all areas of machine learning, whether this is in training of density models over multiple data modalities, in maximising the efficiency of noisy transmission channels, or when learning behaviour policies for exploration by artificial agents. Most learning algorithms that involve optimisation of the mutual information rely on the Blahut-Arimoto algorithm \u2014 an enumerative algorithm with exponential complexity that is not suitable for modern machine learning applications. This paper provides a new approach for scalable optimisation of the mutual information by merging techniques from variational inference and deep learning. We develop our approach by focusing on the problem of intrinsically-motivated learning, where the mutual information forms the definition of a well-known internal drive known as empowerment. Using a variational lower bound on the mutual information, combined with convolutional networks for handling visual input streams, we develop a stochastic optimisation algorithm that allows for scalable information maximisation and empowerment-based reasoning directly from pixels to actions.", "creator": "LaTeX with hyperref package"}}}