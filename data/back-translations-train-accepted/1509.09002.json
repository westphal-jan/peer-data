{"id": "1509.09002", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Sep-2015", "title": "Convergence of Stochastic Gradient Descent for PCA", "abstract": "We consider the problem of principal component analysis (PCA) in a streaming stochastic setting, where our goal is to find a direction of approximate maximal variance, based on a stream of i.i.d. data points in $\\mathbb{R}^d$. A simple and computationally cheap algorithm for this is stochastic gradient descent (SGD), which incrementally updates its estimate based on each new data point. However, due to the non-convex nature of the problem, analyzing its performance has been a challenge. In particular, existing guarantees rely on a non-trivial eigengap assumption on the covariance matrix, which is intuitively unnecessary. In this note, we provide (to the best of our knowledge) the first eigengap-free convergence guarantees for SGD in the context of PCA. This also partially resolves an open problem posed in [Hardt and Price, 2014].", "histories": [["v1", "Wed, 30 Sep 2015 03:02:59 GMT  (17kb)", "https://arxiv.org/abs/1509.09002v1", "18 pages"], ["v2", "Mon, 4 Jan 2016 08:25:56 GMT  (21kb)", "http://arxiv.org/abs/1509.09002v2", "Added analysis of the positive eigengap scenario, with new results; Some minor corrections"]], "COMMENTS": "18 pages", "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["ohad shamir"], "accepted": true, "id": "1509.09002"}, "pdf": {"name": "1509.09002.pdf", "metadata": {"source": "CRF", "title": "Convergence of Stochastic Gradient Descent for PCA", "authors": ["Ohad Shamir"], "emails": ["ohad.shamir@weizmann.ac.il"], "sections": [{"heading": null, "text": "ar Xiv: 150 9.09 002v 2 [cs.L G] 4J an2 01"}, {"heading": "1 Introduction", "text": "It is a basic tool in data analysis and visualization, developed on the basis of self-interest, to find the subspace of the largest variance in a given dataset (a set of points in Euclidean space). We focus on a simple, objective situation in which the data x1, x2,.. we assume that there is an approximate underlying distribution, and our goal is to find a direction of approximate maximum variance. This can be written as an optimization problem in which the data x1, x2,. [xx] w, (1) or equivalent, an approximate leading eigenvector of covariance. [xx] The conceptually simplest method for this task, given m sampled points x1,., xm, is to construct the empirical covariance matrix."}, {"heading": "2 Setting", "text": "We use bold letters to denote vectors, and uppercase letters to denote matrices. Faced with a matrix M, we let them denote their spectral norm, and vice versa to denote their Frobenius norm. We now present the formal problem in a somewhat more general way than the PCA problem that was previously considered. Specifically, we investigate the problem of finding a solution w-Rd: E [A] w-Aw, (2) where d > 1 and A is a positive semidefinitive matrix, gain access to a stream of positive semidefinite matrices A-T, where E [A-t] = A (e.g. xtx t in the PCA case). Note that the gradient of Equation (2) at one point w is equal to 2Aw, with an unbiased stochastic estimate that is 2A-walify."}, {"heading": "3 Convergence Without an Eigengap Assumption", "text": "The main results are the following: \"It is a multiplicative guarantor for the suboptimality of Eq (2), since we have a positive numerical constant. The proof and an outline of its main ideas appears in subsection 5.1 below."}, {"heading": "4 Convergence under an Eigengap Assumption", "text": "Although our main interest so far has been the convergence of SGD without eigengap assumptions, in this section we show that our techniques also imply new limits for PCA with eigengap assumptions that are stronger in certain aspects than what was previously known. Furthermore, we consider the same setting as before, but where the ratio s1 \u2212 s2s1, where s2 are the leading individual values of the covariance matrix A, it is assumed that the top two eigenvalues of A have a gap."}, {"heading": "5 Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Proof of Thm. 1", "text": "To simplify things, we assume that we are working in a coordinate system in which A is the diagonal, A = diag (s1,.., sd), in which s1 [s2], and s1 is the eigenvalue corresponding to v, since the algorithm and the theorem conditions are immutable for the choice of the coordinate system. Furthermore, since the objective function in the theorem is an upper limit, we will assume that the objective conditions of the theorem are immutable for the choice of the coordinate system."}, {"heading": "5.2 Proof of Lemma 1", "text": "Definition: \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W."}, {"heading": "5.3 Proof of Thm. 2", "text": "The evidence is very similar to Thm's. \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212"}, {"heading": "Acknowledgments", "text": "This research is supported in part by a Marie Curie CIG Scholarship from the Seventh Framework Programme, the Intel ICRI-CI Institute and the Israel Science Foundation 425 / 13. We thank Ofer Zeitouni for some enlightening discussions."}], "references": [{"title": "Stochastic optimization for PCA and PLS", "author": ["R. Arora", "A. Cotter", "K. Livescu", "N. Srebro"], "venue": "2012 50th Annual Allerton Conference on Communication, Control, and Computing,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Stochastic optimization of PCA with capped MSG", "author": ["R. Arora", "A. Cotter", "N. Srebro"], "venue": "NIPS,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "The fast convergence of incremental PCA", "author": ["A. Balsubramani", "S. Dasgupta", "Y. Freund"], "venue": "NIPS,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Measure concentration lecture notes", "author": ["A. Barvinok"], "venue": "http://www.math.lsa.umich.edu/ \u0303barvinok/total7", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "The tradeoffs of large scale learning", "author": ["Olivier Bousquet", "L\u00e9on Bottou"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Online principal components analysis", "author": ["C. Boutsidis", "D. Garber", "Z. Karnin", "E. Liberty"], "venue": "SODA,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Global convergence of stochastic gradient descent for some nonconvex matrix problems", "author": ["C. De Sa", "K. Olukotun", "C. R\u00e9"], "venue": "ICML,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast and simple pca via convex optimization", "author": ["D. Garber", "E. Hazan"], "venue": "arXiv preprint arXiv:1509.05647,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Online learning of eigenvectors", "author": ["D. Garber", "E. Hazan", "T. Ma"], "venue": "ICML,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "The noisy power method: A meta algorithm with applications", "author": ["M. Hardt", "E. Price"], "venue": "NIPS,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Analysis of a complex of statistical variables into principal components", "author": ["H. Hotelling"], "venue": "Journal of educational psychology, 24(6):417,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1933}, {"title": "Robust shift-and-invert preconditioning: Faster and more sample efficient algorithms for eigenvector computation", "author": ["C. Jin", "S. Kakade", "C. Musco", "P. Netrapalli", "A. Sidford"], "venue": "arXiv preprint arXiv:1510.08896,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Pca with gaussian perturbations", "author": ["W. Kot\u0142owski", "M. Warmuth"], "venue": "arXiv preprint arXiv:1506.04855,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Estimating the largest eigenvalue by the power and lanczos algorithms with a random start", "author": ["J. Kuczynski", "H. Wozniakowski"], "venue": "SIAM journal on matrix analysis and applications, 13(4):1094\u20131122,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1992}, {"title": "Memory limited, streaming PCA", "author": ["I. Mitliagkas", "C. Caramanis", "P. Jain"], "venue": "NIPS,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Stronger approximate singular value decomposition via the block lanczos and power methods", "author": ["C. Musco", "C. Musco"], "venue": "arXiv preprint arXiv:1504.05477,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Online pca with optimal regrets", "author": ["J. Nie", "W. Kot\u0142owski", "M. Warmuth"], "venue": "Algorithmic Learning Theory, pages 98\u2013112. Springer,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Simplified neuron model as a principal component analyzer", "author": ["E. Oja"], "venue": "Journal of mathematical biology, 15(3):267\u2013273,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1982}, {"title": "On stochastic approximation of the eigenvectors and eigenvalues of the expectation of a random matrix", "author": ["E. Oja", "J. Karhunen"], "venue": "Journal of mathematical analysis and applications, 106(1):69\u201384,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1985}, {"title": "Liii", "author": ["K. Pearson"], "venue": "on lines and planes of closest fit to systems of points in space. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 2(11):559\u2013572,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1901}, {"title": "Making gradient descent optimal for strongly convex stochastic optimization", "author": ["A. Rakhlin", "O. Shamir", "K. Sridharan"], "venue": "ICML,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Understanding Machine Learning: From Theory to Algorithms", "author": ["S. Shalev-Shwartz", "S. Ben-David"], "venue": "Cambridge University Press,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Stochastic convex optimization", "author": ["S. Shalev-Shwartz", "O. Shamir", "N. Srebro", "K. Sridharan"], "venue": "COLT,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast stochastic algorithms for svd and pca: Convergence properties and convexity", "author": ["O. Shamir"], "venue": "arXiv preprint arXiv:1507.08788,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "A stochastic PCA and SVD algorithm with an exponential convergence rate", "author": ["O. Shamir"], "venue": "ICML,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes", "author": ["O. Shamir", "T. Zhang"], "venue": "ICML,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "User-friendly tail bounds for sums of random matrices", "author": ["J. Tropp"], "venue": "Foundations of Computational Mathematics, 12(4):389\u2013434,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Online variance minimization", "author": ["M. Warmuth", "D. Kuzmin"], "venue": "Learning theory, pages 514\u2013528. Springer,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2006}, {"title": "Randomized online pca algorithms with regret bounds that are logarithmic in the dimension", "author": ["M. Warmuth", "D. Kuzmin"], "venue": "Journal of Machine Learning Research, 9(10):2287\u20132320,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 9, "context": "This also partially resolves an open problem posed in [10].", "startOffset": 54, "endOffset": 58}, {"referenceID": 19, "context": "1 Introduction Principal component analysis (PCA) [20, 11] is a fundamental tool in data analysis and visualization, designed to find the subspace of largest variance in a given dataset (a set of points in Euclidean space).", "startOffset": 50, "endOffset": 58}, {"referenceID": 10, "context": "1 Introduction Principal component analysis (PCA) [20, 11] is a fundamental tool in data analysis and visualization, designed to find the subspace of largest variance in a given dataset (a set of points in Euclidean space).", "startOffset": 50, "endOffset": 58}, {"referenceID": 13, "context": "Although this doesn\u2019t require computing and storing the matrix explicitly, it still requires multiple passes over the data, whose number may scale with eigengap parameters of the matrix or the target accuracy [14, 16].", "startOffset": 209, "endOffset": 217}, {"referenceID": 15, "context": "Although this doesn\u2019t require computing and storing the matrix explicitly, it still requires multiple passes over the data, whose number may scale with eigengap parameters of the matrix or the target accuracy [14, 16].", "startOffset": 209, "endOffset": 217}, {"referenceID": 24, "context": "Recently, new randomized algorithms for this problem were able to significantly reduce the required number of passes, while maintaining the ability to compute high-accuracy solutions [25, 24, 8, 12].", "startOffset": 183, "endOffset": 198}, {"referenceID": 23, "context": "Recently, new randomized algorithms for this problem were able to significantly reduce the required number of passes, while maintaining the ability to compute high-accuracy solutions [25, 24, 8, 12].", "startOffset": 183, "endOffset": 198}, {"referenceID": 7, "context": "Recently, new randomized algorithms for this problem were able to significantly reduce the required number of passes, while maintaining the ability to compute high-accuracy solutions [25, 24, 8, 12].", "startOffset": 183, "endOffset": 198}, {"referenceID": 11, "context": "Recently, new randomized algorithms for this problem were able to significantly reduce the required number of passes, while maintaining the ability to compute high-accuracy solutions [25, 24, 8, 12].", "startOffset": 183, "endOffset": 198}, {"referenceID": 17, "context": "In the context of PCA, this is also known as Oja\u2019s method [18, 19].", "startOffset": 58, "endOffset": 66}, {"referenceID": 18, "context": "In the context of PCA, this is also known as Oja\u2019s method [18, 19].", "startOffset": 58, "endOffset": 66}, {"referenceID": 4, "context": "In the world of convex stochastic optimization and learning, SGD has another remarkable property: Despite it being a simple, one-pass algorithm, it is essentially (worst-case) statistically optimal, attaining the same statistical estimation error rate as exact empirical risk minimization [5, 23, 22].", "startOffset": 289, "endOffset": 300}, {"referenceID": 22, "context": "In the world of convex stochastic optimization and learning, SGD has another remarkable property: Despite it being a simple, one-pass algorithm, it is essentially (worst-case) statistically optimal, attaining the same statistical estimation error rate as exact empirical risk minimization [5, 23, 22].", "startOffset": 289, "endOffset": 300}, {"referenceID": 21, "context": "In the world of convex stochastic optimization and learning, SGD has another remarkable property: Despite it being a simple, one-pass algorithm, it is essentially (worst-case) statistically optimal, attaining the same statistical estimation error rate as exact empirical risk minimization [5, 23, 22].", "startOffset": 289, "endOffset": 300}, {"referenceID": 0, "context": "The study of SGD (or variants thereof) for PCA has gained interest in recent years, with some notable examples including [1, 3, 2, 15, 10, 7, 12].", "startOffset": 121, "endOffset": 145}, {"referenceID": 2, "context": "The study of SGD (or variants thereof) for PCA has gained interest in recent years, with some notable examples including [1, 3, 2, 15, 10, 7, 12].", "startOffset": 121, "endOffset": 145}, {"referenceID": 1, "context": "The study of SGD (or variants thereof) for PCA has gained interest in recent years, with some notable examples including [1, 3, 2, 15, 10, 7, 12].", "startOffset": 121, "endOffset": 145}, {"referenceID": 14, "context": "The study of SGD (or variants thereof) for PCA has gained interest in recent years, with some notable examples including [1, 3, 2, 15, 10, 7, 12].", "startOffset": 121, "endOffset": 145}, {"referenceID": 9, "context": "The study of SGD (or variants thereof) for PCA has gained interest in recent years, with some notable examples including [1, 3, 2, 15, 10, 7, 12].", "startOffset": 121, "endOffset": 145}, {"referenceID": 6, "context": "The study of SGD (or variants thereof) for PCA has gained interest in recent years, with some notable examples including [1, 3, 2, 15, 10, 7, 12].", "startOffset": 121, "endOffset": 145}, {"referenceID": 11, "context": "The study of SGD (or variants thereof) for PCA has gained interest in recent years, with some notable examples including [1, 3, 2, 15, 10, 7, 12].", "startOffset": 121, "endOffset": 145}, {"referenceID": 6, "context": "For example, [7] require O(d/\u03bb2\u01eb) iterations to ensure with high probability that one of the iterates is \u01eb-optimal.", "startOffset": 13, "endOffset": 16}, {"referenceID": 11, "context": "[12] require O(1/\u03bb2 + 1/\u03bb\u01eb) iterations, provided we begin close enough to an optimal solution.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Getting an eigengap-free analysis has also been posed as an open problem in [10].", "startOffset": 76, "endOffset": 80}, {"referenceID": 27, "context": "We note that while there are quite a few other single-pass, eigengap-free methods for this problem, such as [28, 29, 17, 6, 9, 13], their memory and runtime-per iteration requirements are much higher than SGD, often O(d2) or worse.", "startOffset": 108, "endOffset": 130}, {"referenceID": 28, "context": "We note that while there are quite a few other single-pass, eigengap-free methods for this problem, such as [28, 29, 17, 6, 9, 13], their memory and runtime-per iteration requirements are much higher than SGD, often O(d2) or worse.", "startOffset": 108, "endOffset": 130}, {"referenceID": 16, "context": "We note that while there are quite a few other single-pass, eigengap-free methods for this problem, such as [28, 29, 17, 6, 9, 13], their memory and runtime-per iteration requirements are much higher than SGD, often O(d2) or worse.", "startOffset": 108, "endOffset": 130}, {"referenceID": 5, "context": "We note that while there are quite a few other single-pass, eigengap-free methods for this problem, such as [28, 29, 17, 6, 9, 13], their memory and runtime-per iteration requirements are much higher than SGD, often O(d2) or worse.", "startOffset": 108, "endOffset": 130}, {"referenceID": 8, "context": "We note that while there are quite a few other single-pass, eigengap-free methods for this problem, such as [28, 29, 17, 6, 9, 13], their memory and runtime-per iteration requirements are much higher than SGD, often O(d2) or worse.", "startOffset": 108, "endOffset": 130}, {"referenceID": 12, "context": "We note that while there are quite a few other single-pass, eigengap-free methods for this problem, such as [28, 29, 17, 6, 9, 13], their memory and runtime-per iteration requirements are much higher than SGD, often O(d2) or worse.", "startOffset": 108, "endOffset": 130}, {"referenceID": 23, "context": "Recently, it was shown that a single exact power iteration can improve the starting point of stochastic methods for PCA [24].", "startOffset": 120, "endOffset": 124}, {"referenceID": 2, "context": "[3, 10, 7, 12]), an interesting difference is that the dependence on the eigengap \u03bb is only 1/\u03bb, as opposed to 1/\u03bb2 or worse.", "startOffset": 0, "endOffset": 14}, {"referenceID": 9, "context": "[3, 10, 7, 12]), an interesting difference is that the dependence on the eigengap \u03bb is only 1/\u03bb, as opposed to 1/\u03bb2 or worse.", "startOffset": 0, "endOffset": 14}, {"referenceID": 6, "context": "[3, 10, 7, 12]), an interesting difference is that the dependence on the eigengap \u03bb is only 1/\u03bb, as opposed to 1/\u03bb2 or worse.", "startOffset": 0, "endOffset": 14}, {"referenceID": 11, "context": "[3, 10, 7, 12]), an interesting difference is that the dependence on the eigengap \u03bb is only 1/\u03bb, as opposed to 1/\u03bb2 or worse.", "startOffset": 0, "endOffset": 14}, {"referenceID": 20, "context": "This has an interesting parallel in the analysis of SGD for \u03bb-strongly convex functions, where the suboptimality of wT decays as \u00d5(1/\u03bbT ), although E[\u2016wT \u2212 w\u2217\u20162] can only be bounded by O(1/\u03bb2T ) (compare for instance Lemma 1 in [21] and Theorem 1 in [26]).", "startOffset": 228, "endOffset": 232}, {"referenceID": 25, "context": "This has an interesting parallel in the analysis of SGD for \u03bb-strongly convex functions, where the suboptimality of wT decays as \u00d5(1/\u03bbT ), although E[\u2016wT \u2212 w\u2217\u20162] can only be bounded by O(1/\u03bb2T ) (compare for instance Lemma 1 in [21] and Theorem 1 in [26]).", "startOffset": 250, "endOffset": 254}, {"referenceID": 11, "context": "([12]) proposed another streaming algorithm which does have only 1/\u03bb dependence (at least for sufficiently large T ), and a high probability convergence rate which is even asymptotically optimal in some cases.", "startOffset": 1, "endOffset": 5}, {"referenceID": 11, "context": "Moreover, the algorithm in [12] is different and more complex, whereas our focus here is on the simple and practical SGD algorithm.", "startOffset": 27, "endOffset": 31}, {"referenceID": 0, "context": "For any \u01eb, \u03b7 \u2208 (0, 1), and integer k \u2265 0, max s\u2208[0,1] (1 + \u03b7s)(1\u2212 \u01eb\u2212 s) \u2264 1 + 2 + \u03b7(1 \u2212 \u01eb)) k \u03b7(k + 1) .", "startOffset": 48, "endOffset": 53}, {"referenceID": 0, "context": "Let sc = k(1\u2212\u01eb)\u22121/\u03b7 k+1 denote this critical point, and consider two cases: \u2022 sc / \u2208 [0, 1]: In that case, f has no critical points in the domain, hence is maximized at one of the domain endpoints, with a value of at most max{f(0), f(1)} = max{1\u2212 \u01eb,\u2212\u01eb(1 + \u03b7)} \u2264 1.", "startOffset": 85, "endOffset": 91}, {"referenceID": 0, "context": "\u2022 sc \u2208 [0, 1]: In that case, we must have k(1\u2212 \u01eb)\u2212 1 \u03b7 \u2265 0, and the value of f at sc is", "startOffset": 7, "endOffset": 13}, {"referenceID": 0, "context": "\uf8f8 max s\u2208[0,1] (1 + \u03b7s) (1\u2212 \u01eb\u2212 s), which by the assumptions s1 = 1 and 1 = \u2016w0\u2016 = \u2211d j=1w 2 0,j is at most \u2212 \u01eb p (1 + \u03b7) + max s\u2208[0,1] (1 + \u03b7s) (1\u2212 \u01eb\u2212 s).", "startOffset": 8, "endOffset": 13}, {"referenceID": 0, "context": "\uf8f8 max s\u2208[0,1] (1 + \u03b7s) (1\u2212 \u01eb\u2212 s), which by the assumptions s1 = 1 and 1 = \u2016w0\u2016 = \u2211d j=1w 2 0,j is at most \u2212 \u01eb p (1 + \u03b7) + max s\u2208[0,1] (1 + \u03b7s) (1\u2212 \u01eb\u2212 s).", "startOffset": 128, "endOffset": 133}, {"referenceID": 0, "context": "j=2 (1 + \u03b7sj) 2T (1\u2212 \u01eb\u2212 sj)w 0,j \u2264 \u2212(1 + \u03b7) \u01eb p + max s\u2208[0,1] (1 + \u03b7s) (1\u2212 \u01eb\u2212 s).", "startOffset": 56, "endOffset": 61}, {"referenceID": 0, "context": "k=0 ( (1 + \u03b7) + (\u03b7b) k max s\u2208[0,1] (1 + \u03b7s)2(T\u2212k\u22121)(1\u2212 \u01eb\u2212 s).", "startOffset": 29, "endOffset": 34}, {"referenceID": 0, "context": "Let X be a non-negative random variable such that for some \u03b1, \u03b2 \u2208 [0, 1], we have E[X] \u2265 \u03b1, and for any \u03b4 \u2208 (0, 1], Pr (", "startOffset": 66, "endOffset": 72}, {"referenceID": 26, "context": "[27]), and the fact that \u2016A\u2016 = s1, it follows that with probability at least 1\u2212 \u03b4, \u2206 \u2264 \u2016A\u2016b \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "(see for instance the proof of Lemma 1 in [24], and Corollary 2.", "startOffset": 42, "endOffset": 46}, {"referenceID": 3, "context": "3 in [4]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 0, "context": "Therefore, applying Lemma 5 on RT , with \u03b1 = 1 4p exp((b2+3)T\u03b72) (which is in [0, 1]) and with \u03b2 = \u03b7b \u221a T (which can be verified to be in [0, 1] by the fact that \u03b7 = log(T ) \u03bbT and Lemma 6), we get that Pr (", "startOffset": 78, "endOffset": 84}, {"referenceID": 0, "context": "Therefore, applying Lemma 5 on RT , with \u03b1 = 1 4p exp((b2+3)T\u03b72) (which is in [0, 1]) and with \u03b2 = \u03b7b \u221a T (which can be verified to be in [0, 1] by the fact that \u03b7 = log(T ) \u03bbT and Lemma 6), we get that Pr (", "startOffset": 138, "endOffset": 144}], "year": 2016, "abstractText": "We consider the problem of principal component analysis (PCA) in a streaming stochastic setting, where our goal is to find a direction of approximate maximal variance, based on a stream of i.i.d. data points in R. A simple and computationally cheap algorithm for this is stochastic gradient descent (SGD), which incrementally updates its estimate based on each new data point. However, due to the non-convex nature of the problem, analyzing its performance has been a challenge. In particular, existing guarantees rely on a non-trivial eigengap assumption on the covariance matrix, which is intuitively unnecessary. In this paper, we provide (to the best of our knowledge) the first eigengap-free convergence guarantees for SGD in the context of PCA. This also partially resolves an open problem posed in [10]. Moreover, under an eigengap assumption, we show that the same techniques lead to new SGD convergence guarantees with better dependence on the eigengap.", "creator": "LaTeX with hyperref package"}}}