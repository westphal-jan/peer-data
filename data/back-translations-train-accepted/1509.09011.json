{"id": "1509.09011", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Sep-2015", "title": "Regret Lower Bound and Optimal Algorithm in Finite Stochastic Partial Monitoring", "abstract": "Partial monitoring is a general model for sequential learning with limited feedback formalized as a game between two players. In this game, the learner chooses an action and at the same time the opponent chooses an outcome, then the learner suffers a loss and receives a feedback signal. The goal of the learner is to minimize the total loss. In this paper, we study partial monitoring with finite actions and stochastic outcomes. We derive a logarithmic distribution-dependent regret lower bound that defines the hardness of the problem. Inspired by the DMED algorithm (Honda and Takemura, 2010) for the multi-armed bandit problem, we propose PM-DMED, an algorithm that minimizes the distribution-dependent regret. PM-DMED significantly outperforms state-of-the-art algorithms in numerical experiments. To show the optimality of PM-DMED with respect to the regret bound, we slightly modify the algorithm by introducing a hinge function (PM-DMED-Hinge). Then, we derive an asymptotically optimal regret upper bound of PM-DMED-Hinge that matches the lower bound.", "histories": [["v1", "Wed, 30 Sep 2015 04:36:40 GMT  (1407kb)", "http://arxiv.org/abs/1509.09011v1", "24 pages, to appear in NIPS2015"]], "COMMENTS": "24 pages, to appear in NIPS2015", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["junpei komiyama", "junya honda", "hiroshi nakagawa"], "accepted": true, "id": "1509.09011"}, "pdf": {"name": "1509.09011.pdf", "metadata": {"source": "CRF", "title": "Regret Lower Bound and Optimal Algorithm in Finite Stochastic Partial Monitoring", "authors": ["Junpei Komiyama", "Junya Honda"], "emails": ["junpei@komiyama.info", "honda@stat.t.u-tokyo.ac.jp", "nakagawa@dl.itc.u-tokyo.ac.jp"], "sections": [{"heading": null, "text": "ar Xiv: 150 9.09 011v 1 [stat.ML] Partial monitoring is a general model for sequential learning with limited feedback that is formalized as a game between two players. In this game, the learner selects an action and at the same time the opponent selects a result, then the learner suffers a loss and receives a feedback signal. The goal of the learner is to minimize the overall loss. In this essay, we examine partial monitoring with finite actions and heartbreaking results. We derive a logarithmic distribution-dependent regret lower limit that defines the severity of the problem. Inspired by the DMED algorithm (Honda and Takemura, 2010) for the multi-armed bandit problem, we propose PM-DMED, an algorithm that minimizes distribution-based regret. PM-DMED significantly exceeds the state of the art in numerical experiments. In order to show the optimizability of PM-DMED with respect to the MED function, the introduction of a minor impediment to the MED function."}, {"heading": "1 Introduction", "text": "Partial monitoring is a general framework for sequential decision problems with imperfect feedback. Many classes of problems, including expert prediction [1], multi-arm bandit problem [2], dynamic pricing [3], dark pool problem [4], efficient prediction [5], and linear and convex optimization with full or bandit feedback [6, 7] can be modeled as an example of partial monitoring. Partial monitoring is formalized as a repetitive game played by two players referred to as learners and opponents. In each round, the learner selects an action and at the same time the opponent selects a result. Then, the learner observes a feedback signal from a given set of symbols and suffers a certain loss, both of which are deterministic functions of the selected action and the outcome. The goal of the learner is to find the optimal action that can minimize his / her cumulative loss between the only loss we can take as an alternative to the small loss."}, {"heading": "1.1 Related work", "text": "The aforementioned cerebral consecrated cerebral consecrated cerebral consecrated cerebral consecrated cerebral consecrated tec-eaeSrcnlhirnge\u00e4eaeFnln in the eeirlcehnlcnlrrteeaeVnlrrlln-eaeFnlrrrrrrrrdteeaeeFnlrlrlrln-eaeoiuiuiuueaeaeSrmnlrnlrlrrrlrrlrrlrllllllllllllllhc in the eeirlteeaeaeFnlrteeFnlrrlrlrlrln in the eeaeaeeeFeFnlreFeFnlrrrrrreFeFeFeeFeeeeeeeFeeeeeeeeeFeeeeeeeeFeeeeeeeeeeeeeeFeeeeeeeeeeeeeeeeeeeeeeeReeeeeeeeeeeeeeeeeeeeeSreSreaeaeSreSreeSreaeaeSreSreSreaeSreSreSreeSreSreeSreeeaeSreaeaeSreaeaeaeeeeaeaeeeeSreeeeeeSrlreeeeaeaeaeSrlrlreeeeeeeeeeeeeeeeeeeeeeeReReeeeReeeeeeeeReReReeeeeeReReaeeeReeeeeee"}, {"heading": "2 Problem Setup", "text": "This paper examines the finite stochastic partial monitoring problem with N actions, M results and A symbols. An instance of the partial monitoring game is defined by a loss matrix L = (li, j), RN-M and a feedback matrix H = (hi, j), A-M, where [A], 1, 2,., A-M, at the beginning the learner is informed about L and H. In each round t = 1, 2,., T, a learner selects an action i (t), and at the same time an opponent selects a result j (t), which is a result j (t), [M), the result of which is the learner's 1note, which is a polylog factor.suffers loss li (t), j (t), which he / she cannot observe: the only information the learner receives is the signal hi (t), j (t), j (t)."}, {"heading": "3 Regret Lower Bound", "text": "A good algorithm should work well against any strategy of the opponent. We expand this idea by introducing the concept of strong consistency (= q =)."}, {"heading": "4 PM-DMED Algorithm", "text": "In this section, we describe the partial monitoring on the basis of empirical minimum deviations (PMDMED) suggested by the DMED [17] for solving the multi-armed bandit problem. (D) In contrast to the empirical distributions of symbols under the selection of action, (D) the empirical distribution of symbols under the selection of action is. (D) In contrast to the empirical distribution of symbols under the selection of action. (D) In contrast to the empirical distribution of symbols under the selection of action. (D) In contrast to the empirical distribution of symbols under the selection of action. (T) In the context, we leave open the empirical deviations of q-PM action. (T) D (t) D (t) D (p) -1) deviations from action."}, {"heading": "5 Experiment", "text": "After Barto, the seller sets a price for a product and at the same time the buyer agrees to pay a maximum purchase price. \"The four-state game (Section 4), a three-state game and dynamic pricing (Section 4), is characterized as: L = (1 0 0 1 1 1 1 1 1) and H = (1 2 2 2 2 2 2 2 2 1). The signal matrices of this game are, S1 = (1 0 0 0 1 1), S2 = (0 1 0 1 1 1 0). The signal matrices of this game are, S1 = (1 0 1 0 1), S2 = (0 0 1 0 1 0 1). The pricing that is considered hard in terms of Minimax repentance is a game that represents a repeated auction between a seller (learner) and a buyer (opponent)."}, {"heading": "6 Theoretical Analysis", "text": "i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i."}, {"heading": "Acknowledgements", "text": "The authors gratefully accept the advice of Kentaro Minami and sincerely thank the anonymous reviewers for their useful comments. This work was partially supported by JSPS KAKENHI grant numbers 15J09850 and 26106506."}, {"heading": "A Case in which Condition 2 Does Not Hold", "text": "Figure 3 is an example that does not cover theorem 3: The dashed line is {q: p \u0445 1 = S1q}, which coincides (inadvertently) with a line that forms the convex polytopic of Cc1. In this case, condition 2 does not apply because int (Cc1) and cl (Cc1) coincide, while ccl (S0) and S0 (two star points), which means that a slight modification of p \u0445 changes the set of cells intersecting discontinuously with the dashed line. We exclude these unusual cases for the sake of ease of analysis. The authors consider that it is quite difficult to name the optimal regret without such regularity conditions. In fact, many regularity conditions are assumed in Graves and Lai [22], where a further generalization of the bandit problem is taken into account and the lower limit of regret in relation to LIP conditionality is strongly simplified by this rule, but it remains a rule argument in this essay."}, {"heading": "B Proof: Regret Lower Bound", "text": "In this section, we prove that Lemma 1 and Theorem 2 [E] [E] [E] [E] [E] [E] [E] [E] [E] [E] [E] [E] [E] [E] < S \"p\" int (Cc1) and i \"6 = 1 is the optimal action under the strategy of the adversary p.\" We consider a modified partial surveillance game with the strategy of the adversary as p. \"Note: Let X\" mi \"[A] is the signal of m\" th \"observation of the action i.\" LetK \"Li (n)\" T \"Li (n) [N) = n\" T \"m\" (N)."}, {"heading": "D Optimality of PM-DMED-Hinge", "text": "In this appendix, we assign theorem 3. First, we define distances between distributions. For distributions pi, p, i, PA of symbols, we use the total variation distance pi, p, i, 12A, a, (pi) a, (p, i) a, for distributions p, p, p, PM of results we identify p with the total variation distance p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p"}], "references": [{"title": "The weighted majority algorithm", "author": ["Nick Littlestone", "Manfred K. Warmuth"], "venue": "Inf. Comput.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1994}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "Herbert Robbins"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1985}, {"title": "The value of knowing a demand curve: Bounds on regret for online posted-price auctions", "author": ["Robert D. Kleinberg", "Frank Thomson Leighton"], "venue": "In FOCS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Optimal allocation strategies for the dark pool problem", "author": ["Alekh Agarwal", "Peter L. Bartlett", "Max Dama"], "venue": "In AISTATS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Minimizing regret with label efficient prediction", "author": ["Nicol\u00f2 Cesa-Bianchi", "G\u00e1bor Lugosi", "Gilles Stoltz"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["Martin Zinkevich"], "venue": "In ICML, pages 928\u2013936,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Stochastic linear optimization under bandit feedback", "author": ["Varsha Dani", "Thomas P. Hayes", "Sham M. Kakade"], "venue": "In COLT,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Discrete prediction games with arbitrary feedback and loss", "author": ["Antonio Piccolboni", "Christian Schindelhauer"], "venue": "In COLT,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "Regret minimization under partial monitoring", "author": ["Nicol\u00f2 Cesa-Bianchi", "G\u00e1bor Lugosi", "Gilles Stoltz"], "venue": "Math. Oper. Res.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Minimax regret of finite partial-monitoring games in stochastic environments", "author": ["G\u00e1bor Bart\u00f3k", "D\u00e1vid P\u00e1l", "Csaba Szepesv\u00e1ri"], "venue": "In COLT,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "An adaptive algorithm for finite stochastic partial monitoring", "author": ["G\u00e1bor Bart\u00f3k", "Navid Zolghadr", "Csaba Szepesv\u00e1ri"], "venue": "In ICML,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "A near-optimal algorithm for finite partial-monitoring games against adversarial opponents", "author": ["G\u00e1bor Bart\u00f3k"], "venue": "In COLT,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Efficient partial monitoring with prior information", "author": ["Hastagiri P. Vanchinathan", "G\u00e1bor Bart\u00f3k", "Andreas Krause"], "venue": "In NIPS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Finite-time Analysis of the Multiarmed Bandit Problem", "author": ["Peter Auer", "Nicol\u00f3 Cesa-bianchi", "Paul Fischer"], "venue": "Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2002}, {"title": "The KL-UCB algorithm for bounded stochastic bandits and beyond", "author": ["Aur\u00e9lien Garivier", "Olivier Capp\u00e9"], "venue": "In COLT,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Large deviations techniques and applications", "author": ["Amir Dembo", "Ofer Zeitouni"], "venue": "Applications of mathematics", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "An Asymptotically Optimal Bandit Algorithm for Bounded Support Models", "author": ["Junya Honda", "Akimichi Takemura"], "venue": "In COLT,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "A dual parametrization method for convex semi-infinite programming", "author": ["S. Ito", "Y. Liu", "K.L. Teo"], "venue": "Annals of Operations Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2000}, {"title": "Introduction to sensitivity and stability analysis in nonlinear programming", "author": ["Anthony V. Fiacco"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1983}, {"title": "Asymptotically efficient adaptive choice of control laws in controlled Markov chains", "author": ["T.L. Graves", "T.L. Lai"], "venue": "SIAM J. Contr. and Opt.,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1997}, {"title": "On the complexity of A/B testing", "author": ["Emilie Kaufmann", "Olivier Capp\u00e9", "Aur\u00e9lien Garivier"], "venue": "In COLT,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Bandits Games and Clustering Foundations", "author": ["S\u00e9bastien Bubeck"], "venue": "Theses, Universite\u0301 des Sciences et Technologie de Lille - Lille I,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Point-to-set maps in mathematical programming", "author": ["William W. Hogan"], "venue": "SIAM Review,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1973}], "referenceMentions": [{"referenceID": 0, "context": "Many classes of problems, including prediction with expert advice [1], the multi-armed bandit problem [2], dynamic pricing [3], the dark pool problem [4], label efficient prediction [5], and linear and convex optimization with full or bandit feedback [6, 7] can be modeled as an instance of partial monitoring.", "startOffset": 66, "endOffset": 69}, {"referenceID": 1, "context": "Many classes of problems, including prediction with expert advice [1], the multi-armed bandit problem [2], dynamic pricing [3], the dark pool problem [4], label efficient prediction [5], and linear and convex optimization with full or bandit feedback [6, 7] can be modeled as an instance of partial monitoring.", "startOffset": 102, "endOffset": 105}, {"referenceID": 2, "context": "Many classes of problems, including prediction with expert advice [1], the multi-armed bandit problem [2], dynamic pricing [3], the dark pool problem [4], label efficient prediction [5], and linear and convex optimization with full or bandit feedback [6, 7] can be modeled as an instance of partial monitoring.", "startOffset": 123, "endOffset": 126}, {"referenceID": 3, "context": "Many classes of problems, including prediction with expert advice [1], the multi-armed bandit problem [2], dynamic pricing [3], the dark pool problem [4], label efficient prediction [5], and linear and convex optimization with full or bandit feedback [6, 7] can be modeled as an instance of partial monitoring.", "startOffset": 150, "endOffset": 153}, {"referenceID": 4, "context": "Many classes of problems, including prediction with expert advice [1], the multi-armed bandit problem [2], dynamic pricing [3], the dark pool problem [4], label efficient prediction [5], and linear and convex optimization with full or bandit feedback [6, 7] can be modeled as an instance of partial monitoring.", "startOffset": 182, "endOffset": 185}, {"referenceID": 5, "context": "Many classes of problems, including prediction with expert advice [1], the multi-armed bandit problem [2], dynamic pricing [3], the dark pool problem [4], label efficient prediction [5], and linear and convex optimization with full or bandit feedback [6, 7] can be modeled as an instance of partial monitoring.", "startOffset": 251, "endOffset": 257}, {"referenceID": 6, "context": "Many classes of problems, including prediction with expert advice [1], the multi-armed bandit problem [2], dynamic pricing [3], the dark pool problem [4], label efficient prediction [5], and linear and convex optimization with full or bandit feedback [6, 7] can be modeled as an instance of partial monitoring.", "startOffset": 251, "endOffset": 257}, {"referenceID": 7, "context": "1 Related work The paper by Piccolboni and Schindelhauer [8] is one of the first to study the regret of the finite partial monitoring problem.", "startOffset": 57, "endOffset": 60}, {"referenceID": 8, "context": "[9] to O(T ), who also showed an instance in which the bound is optimal.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] classified the partial monitoring problems into four categories in terms of the minimax regret: a trivial problem with zero regret, an easy problem with \u0398\u0303( \u221a T ) regret1, a hard problem with \u0398(T ) regret, and a hopeless problem with \u0398(T ) regret.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Since then, several algorithms with a \u00d5( \u221a T ) regret bound for easy problems have been proposed [11, 12, 13].", "startOffset": 97, "endOffset": 109}, {"referenceID": 11, "context": "Since then, several algorithms with a \u00d5( \u221a T ) regret bound for easy problems have been proposed [11, 12, 13].", "startOffset": 97, "endOffset": 109}, {"referenceID": 12, "context": "Since then, several algorithms with a \u00d5( \u221a T ) regret bound for easy problems have been proposed [11, 12, 13].", "startOffset": 97, "endOffset": 109}, {"referenceID": 12, "context": "Among them, the Bayes-update Partial Monitoring (BPM) algorithm [13] is state-of-the-art in the sense of empirical performance.", "startOffset": 64, "endOffset": 68}, {"referenceID": 10, "context": "[11], which derivedO(logT ) distribution-dependent regret for easy problems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Upper confidence bound (UCB), the most well-known algorithm for the multi-armed bandits, has a distribution-dependent regret bound [2, 14], and algorithms that minimize the distribution-dependent regret (e.", "startOffset": 131, "endOffset": 138}, {"referenceID": 13, "context": "Upper confidence bound (UCB), the most well-known algorithm for the multi-armed bandits, has a distribution-dependent regret bound [2, 14], and algorithms that minimize the distribution-dependent regret (e.", "startOffset": 131, "endOffset": 138}, {"referenceID": 14, "context": "[15]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10]) in which the number of rows of Si is the number of the different symbols in the i-th row of H .", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "In the context of the multi-armed bandit problem, Lai and Robbins [2] derived the regret lower bound of a strongly consistent algorithm: an algorithm must select each arm i until its number of draws Ni(t) satisfies log t .", "startOffset": 66, "endOffset": 69}, {"referenceID": 15, "context": "Large deviation principle [16] states that, the probability that an opponent with strategy q behaves like p is", "startOffset": 26, "endOffset": 30}, {"referenceID": 1, "context": "Based on the technique used in Lai and Robbins [2], the proof considers a modified game in which another action i 6= 1 is optimal.", "startOffset": 47, "endOffset": 50}, {"referenceID": 12, "context": "[13]) ambiguously define the harshness as the closeness to the boundary of the cells.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "4 PM-DMED Algorithm In this section, we describe the partial monitoring deterministic minimum empirical divergence (PMDMED) algorithm, which is inspired by DMED [17] for solving the multi-armed bandit problem.", "startOffset": 161, "endOffset": 165}, {"referenceID": 0, "context": "Let p\u0302i(t) \u2208 [0, 1] be the empirical distribution of the symbols under the selection of action i.", "startOffset": 13, "endOffset": 19}, {"referenceID": 10, "context": "[11], we compared the performances of algorithms in three different games: the four-state game (Section 4), a three-state game and dynamic pricing.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11], we set N = 5,M = 5, and c = 2.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "We compared Random, FeedExp3 [8], CBP [11] with \u03b1 = 1.", "startOffset": 29, "endOffset": 32}, {"referenceID": 10, "context": "We compared Random, FeedExp3 [8], CBP [11] with \u03b1 = 1.", "startOffset": 38, "endOffset": 42}, {"referenceID": 12, "context": "01, BPM-LEAST, BPM-TS [13], and PM-DMED with c = 1.", "startOffset": 22, "endOffset": 26}, {"referenceID": 12, "context": "Following the optimization of BPM-LEAST [13], we resorted to a finite sample approximation and used the Gurobi LP solver [19] in computing {r\u2217 i }: at each round, we sampled 1,000 points from PM , and relaxed the constraints on the samples.", "startOffset": 40, "endOffset": 44}, {"referenceID": 17, "context": "[20] and references therein).", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Moreover, the coefficient of the leading logarithmic term in the regret bound of the partial monitoring problem is equal to the bound given in Lai and Robbins [2].", "startOffset": 159, "endOffset": 162}, {"referenceID": 18, "context": ", Fiacco [21]).", "startOffset": 9, "endOffset": 13}, {"referenceID": 0, "context": "References [1] Nick Littlestone and Manfred K.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] T.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Robert D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Alekh Agarwal, Peter L.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Nicol\u00f2 Cesa-Bianchi, G\u00e1bor Lugosi, and Gilles Stoltz.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Martin Zinkevich.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Varsha Dani, Thomas P.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Antonio Piccolboni and Christian Schindelhauer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Nicol\u00f2 Cesa-Bianchi, G\u00e1bor Lugosi, and Gilles Stoltz.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] G\u00e1bor Bart\u00f3k, D\u00e1vid P\u00e1l, and Csaba Szepesv\u00e1ri.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] G\u00e1bor Bart\u00f3k, Navid Zolghadr, and Csaba Szepesv\u00e1ri.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] G\u00e1bor Bart\u00f3k.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Hastagiri P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Peter Auer, Nicol\u00f3 Cesa-bianchi, and Paul Fischer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Aur\u00e9lien Garivier and Olivier Capp\u00e9.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Amir Dembo and Ofer Zeitouni.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Junya Honda and Akimichi Takemura.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[21] Anthony V.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[22] T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[23] Emilie Kaufmann, Olivier Capp\u00e9, and Aur\u00e9lien Garivier.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[24] S\u00e9bastien Bubeck.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[25] William W.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "In fact, many regularity conditions are assumed in Graves and Lai [22], where another generalization of the bandit problem is considered and the regret lower bound is expressed in terms of LSIP.", "startOffset": 66, "endOffset": 70}, {"referenceID": 1, "context": "The technique here is mostly inspired from Theorem 1 in Lai and Robbins [2].", "startOffset": 72, "endOffset": 75}, {"referenceID": 20, "context": "[23].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "5 in [24]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 1, "context": "During the proof we also show that C 1 (p , {pi }) is equal to the optimal constant factor of Lai and Robbins [2].", "startOffset": 110, "endOffset": 113}, {"referenceID": 1, "context": "LBTheory is the asymptotic regret lower bound of Lai and Robbins [2].", "startOffset": 65, "endOffset": 68}, {"referenceID": 1, "context": "LB-Theory is the regret lower bound of Lai and Robbins [2], that is, \u2211 i6=1 \u2206i log t d(\u03bci\u2016\u03bc1) .", "startOffset": 55, "endOffset": 58}, {"referenceID": 22, "context": "See Hogan [25] for definitions of terms such as continuity of point-to-set maps.", "startOffset": 10, "endOffset": 14}, {"referenceID": 0, "context": "2 Regret analysis of PM-DMED-Hinge Let p\u0302i,n \u2208 [0, 1] be the empirical distribution of the symbols from the action i when the action i is selected n times.", "startOffset": 47, "endOffset": 53}], "year": 2015, "abstractText": "Partial monitoring is a general model for sequential learning with limited feedback formalized as a game between two players. In this game, the learner chooses an action and at the same time the opponent chooses an outcome, then the learner suffers a loss and receives a feedback signal. The goal of the learner is to minimize the total loss. In this paper, we study partial monitoring with finite actions and stochastic outcomes. We derive a logarithmic distribution-dependent regret lower bound that defines the hardness of the problem. Inspired by the DMED algorithm (Honda and Takemura, 2010) for the multi-armed bandit problem, we propose PM-DMED, an algorithm that minimizes the distribution-dependent regret. PM-DMED significantly outperforms state-of-the-art algorithms in numerical experiments. To show the optimality of PM-DMED with respect to the regret bound, we slightly modify the algorithm by introducing a hinge function (PMDMED-Hinge). Then, we derive an asymptotically optimal regret upper bound of PM-DMED-Hinge that matches the lower bound.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}