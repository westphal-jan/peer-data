{"id": "1509.09292", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Sep-2015", "title": "Convolutional Networks on Graphs for Learning Molecular Fingerprints", "abstract": "Predicting properties of molecules requires functions that take graphs as inputs. Molecular graphs are usually preprocessed using hash-based functions to produce fixed-size fingerprint vectors, which are used as features for making predictions. We introduce a convolutional neural network that operates directly on graphs, allowing end-to-end learning of the feature pipeline. This architecture generalizes standard molecular fingerprints. We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.", "histories": [["v1", "Wed, 30 Sep 2015 18:33:50 GMT  (1247kb,D)", "http://arxiv.org/abs/1509.09292v1", "9 pages, 5 figures. To appear in Neural Information Processing Systems (NIPS)"], ["v2", "Tue, 3 Nov 2015 17:18:32 GMT  (1261kb,D)", "http://arxiv.org/abs/1509.09292v2", "9 pages, 5 figures. To appear in Neural Information Processing Systems (NIPS)"]], "COMMENTS": "9 pages, 5 figures. To appear in Neural Information Processing Systems (NIPS)", "reviews": [], "SUBJECTS": "cs.LG cs.NE stat.ML", "authors": ["david k duvenaud", "dougal maclaurin", "jorge aguilera-iparraguirre", "rafael bombarell", "timothy hirzel", "al\u00e1n aspuru-guzik", "ryan p adams"], "accepted": true, "id": "1509.09292"}, "pdf": {"name": "1509.09292.pdf", "metadata": {"source": "CRF", "title": "Convolutional Networks on Graphs for Learning Molecular Fingerprints", "authors": ["David Duvenaud", "Dougal Maclaurin", "Jorge Aguilera-Iparraguirre", "Rafael G\u00f3mez-Bombarelli", "Timothy Hirzel", "Al\u00e1n Aspuru-Guzik", "Ryan P. Adams"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "One difficulty in this task is that input to the predictor, a molecule, can be of any size and shape. Most machine learning pipelines can only process input of a fixed size. Up-to-date state of the art is the use of fingerprint software to calculate specified characteristics. In this paper, we replace these characteristics as inputs for a fully networked deep neural network or other machine learning method. This formula was followed by [28, 3, 19]. During the training, the molecular fingerprints were treated as fixed. In this paper, we replace the bottom layer of this stack - the fixed molecular fingerprints - with a differentiated neural network whose input represents the original molecule."}, {"heading": "2 Circular fingerprints", "text": "Circular fingerprints [6] are a refinement of the Morgan algorithm [17], which is used to determine which substructures are present in a molecule in a way that is invariant to atomic relabeling. Circular fingerprints generate the characteristics of each layer by applying a fixed hash function to the concatenated characteristics of the neighborhood in the previous layer. The results of these bunnies are then treated as integer indexes, with a 1 written on the fingerprint vector at the index indicated by the characteristic vector at each node in the diagram. Figure 1 (left) shows a sketch of this computational architecture. Ignoring collisions, each index of the fingerprint indicates the presence of a particular substructure. The size of the substructures represented by each index depends on the depth of the mesh.The number of layers is thus referred to as \"fingerprints,\" which are applied throughout the voluminous networks."}, {"heading": "3 Creating a differentiable fingerprint", "text": "The space of possible network architectures is large. In the spirit of starting with a well-known-good configuration, we have opted for an architecture analogous to existing fingerprints. This section describes our replacement for each individual discrete operation in the way it varies in a fragment, no matter how small, will lead to a different fingerprint index applied to each layer of a neural network. We replace hash operation with a single layer of a neural network. With a smooth function, the activations can be similar when the local molecular structure varies. Fingerprint indexing uses indexing to combine each atomic function."}, {"heading": "4 Experiments", "text": "Circular fingerprints can be interpreted as a special case of neural graph fingerprints with large random weights. This is to be expected, because in the limit of large input weights tanh approximate nonlinearity step functions, which resemble in their association a hash function. Also, in the limit of large input weights, the Softmax operator approaches a random argmax operator, which is analogous to an indexing operation.One use for molecular fingerprints is the calculation of distances between molecules. We investigated whether ECFP-based distances were similar to random neural fingerprint-based distances. Figure 3 (left) shows a scatter plot of pairs of distances, which were calculated with either circular or neural fingerprints. Fingerprints had a length of 2048 and were calculated on molecules from the dataset of solubility."}, {"heading": "4.1 Examining learned features", "text": "To show that neural graph fingerprints can be interpreted, we show examples of classes of substructures that activate individual features in a fingerprint. Circular fingerprint features can only be activated by a single fragment of a single radius, except by random collisions. In contrast, neural graph fingerprints can be activated by variations in the same structure, making them more economical and interpretable.Solubility characteristics Figure 4 shows the fragments that activate the most predictive features of a fingerprint at most. Fingerprint network has been trained as an input into a linear model that predicts solubility, as measured in [4].The upper fingerprint has a positive predictive relationship to solubility and is most strongly activated by fragments that contain a hydrophilic R-OH group, a standard indicator of solubility. The lower fingerprint, which is strongly predictive for the non-soluble, is predicted by the two ring characteristics that are insoluble."}, {"heading": "4.2 Predictive Performance", "text": "This year, it has reached the stage where it will be able to put itself at the forefront in order to embark on the path to the future."}, {"heading": "5 Limitations", "text": "In practice, however, the calculation of the neural fingerprint in the depth of deeper fingerprints usually requires several minutes, while the formation of both the fingerprints and the mesh took place in the order of an hour. Limited calculation in each layer costs O (RNF 2). In practice, the formation of neural networks at the top of circulating fingerprints usually requires several minutes, while the formation of both the fingerprints and the mesh at the top took place in the order of an hour. Limited calculation in each layer should be the function that goes from one layer of the network to the next."}, {"heading": "6 Related work", "text": "In fact, most of them will be able to play by the rules."}, {"heading": "7 Conclusion", "text": "By making each operation in the feature pipeline differentiated, we can leverage standard neural network training methods to optimize the parameters of these molecular neural fingerprints to the very end. We demonstrated the interpretability and predictability of these new fingerprints. Data-driven functions have already replaced handmade functions in speech recognition, machine vision, and natural language processing, and performing the same task for virtual screening, drug design, and material design is a natural next step."}, {"heading": "Acknowledgments", "text": "We thank Edward Pyzer-Knapp, Jennifer Wei and the Samsung Advanced Institute of Technology for their support."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Spectral networks and locally connected networks on graphs", "author": ["Joan Bruna", "Wojciech Zaremba", "Arthur Szlam", "Yann LeCun"], "venue": "arXiv preprint arXiv:1312.6203,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Multi-task neural networks for QSAR predictions", "author": ["George E. Dahl", "Navdeep Jaitly", "Ruslan Salakhutdinov"], "venue": "arXiv preprint arXiv:1406.1231,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "ESOL: Estimating aqueous solubility directly from molecular structure", "author": ["John S. Delaney"], "venue": "Journal of Chemical Information and Computer Sciences,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Thousands of chemical starting points for antimalarial lead identification", "author": ["Francisco-Javier Gamo", "Laura M Sanz", "Jaume Vidal", "Cristina de Cozar", "Emilio Alvarez", "Jose-Luis Lavandera", "Dana E Vanderwall", "Darren VS Green", "Vinod Kumar", "Samiul Hasan"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Circular fingerprints: flexible molecular descriptors with applications from physical chemistry to ADME", "author": ["Robert C. Glem", "Andreas Bender", "Catrin H. Arnby", "Lars Carlsson", "Scott Boyer", "James Smith"], "venue": "IDrugs: the investigational drugs journal,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "The Harvard clean energy project: large-scale computational screening and design of organic photovoltaics on the world community", "author": ["Johannes Hachmann", "Roberto Olivares-Amaya", "Sule Atahan-Evrenk", "Carlos Amador-Bedolla", "Roel S S\u00e1nchez-Carrera", "Aryeh Gold-Parker", "Leslie Vogt", "Anna M Brockway", "Al\u00e1n Aspuru-Guzik"], "venue": "grid. The Journal of Physical Chemistry Letters,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Deep unfolding: Model-based inspiration of novel deep architectures", "author": ["John R Hershey", "Jonathan Le Roux", "Felix Weninger"], "venue": "arXiv preprint arXiv:1409.2574,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom"], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Yann LeCun", "Yoshua Bengio"], "venue": "The handbook of brain theory and neural networks,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1995}, {"title": "Deep architectures and deep learning in chemoinformatics: the prediction of aqueous solubility for drug-like molecules", "author": ["Alessandro Lusci", "Gianluca Pollastri", "Pierre Baldi"], "venue": "Journal of chemical information and modeling,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Neural network for graphs: A contextual constructive approach", "author": ["Alessio Micheli"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "The generation of a unique machine description for chemical structure", "author": ["H.L. Morgan"], "venue": "Journal of Chemical Documentation,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1965}, {"title": "Python for scientific computing", "author": ["Travis E Oliphant"], "venue": "Computing in Science & Engineering,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Massively multitask networks for drug", "author": ["Bharath Ramsundar", "Steven Kearnes", "Patrick Riley", "Dale Webster", "David Konerding", "Vijay Pande"], "venue": "discovery. arXiv:1502.02072,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Extended-connectivity fingerprints", "author": ["David Rogers", "Mathew Hahn"], "venue": "Journal of Chemical Information and Modeling,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "The graph neural network model", "author": ["F. Scarselli", "M. Gori", "Ah Chung Tsoi", "M. Hagenbuchner", "G. Monfardini"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Richard Socher", "Eric H Huang", "Jeffrey Pennin", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H Huang", "Andrew Y Ng", "Christopher D Manning"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D Manning"], "venue": "arXiv preprint arXiv:1503.00075,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Toxicity prediction using deep learning", "author": ["Thomas Unterthiner", "Andreas Mayr", "G\u00fcnter Klambauer", "Sepp Hochreiter"], "venue": "arXiv preprint arXiv:1503.01445,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Deep learning as an opportunity in virtual screening", "author": ["Thomas Unterthiner", "Andreas Mayr", "G \u00fcnter Klambauer", "Marvin Steijaert", "J\u00f6rg Wenger", "Hugo Ceulemans", "Sepp Hochreiter"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Regularization of neural networks using dropconnect", "author": ["Li Wan", "Matthew Zeiler", "Sixin Zhang", "Yann L. Cun", "Rob Fergus"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "SMILES, a chemical language and information system", "author": ["David Weininger"], "venue": "Journal of chemical information and computer sciences,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1988}], "referenceMentions": [{"referenceID": 24, "context": "This formula was followed by [28, 3, 19].", "startOffset": 29, "endOffset": 40}, {"referenceID": 2, "context": "This formula was followed by [28, 3, 19].", "startOffset": 29, "endOffset": 40}, {"referenceID": 17, "context": "This formula was followed by [28, 3, 19].", "startOffset": 29, "endOffset": 40}, {"referenceID": 24, "context": "For example, [28] used a fingerprint vector of size 43,000, after having removed rarely-occurring features.", "startOffset": 13, "endOffset": 17}, {"referenceID": 18, "context": "2 Circular fingerprints The state of the art in molecular fingerprints are extended-connectivity circular fingerprints (ECFP) [21].", "startOffset": 126, "endOffset": 130}, {"referenceID": 5, "context": "Circular fingerprints [6] are a refinement of the Morgan algorithm [17], designed to identify which substructures are present in a molecule in a way that is invariant to atom-relabeling.", "startOffset": 22, "endOffset": 25}, {"referenceID": 15, "context": "Circular fingerprints [6] are a refinement of the Morgan algorithm [17], designed to identify which substructures are present in a molecule in a way that is invariant to atom-relabeling.", "startOffset": 67, "endOffset": 71}, {"referenceID": 3, "context": "Fingerprints had length 2048, and were calculated on molecules from the solubility dataset [4].", "startOffset": 91, "endOffset": 94}, {"referenceID": 3, "context": "The fingerprint network was trained as inputs to a linear model predicting solubility, as measured in [4].", "startOffset": 102, "endOffset": 105}, {"referenceID": 23, "context": "[27] constructed similar visualizations, but in a semi-manual way: to determine which toxic fragments activated a given neuron, they searched over a hand-made list of toxic substructures and chose the one most correlated with a given neuron.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "Experimental setup Our pipeline takes as input the SMILES [30] string encoding of each molecule, which is then converted into a graph using RDKit [20].", "startOffset": 58, "endOffset": 62}, {"referenceID": 9, "context": "Training and Architecture Training used batch normalization [11].", "startOffset": 60, "endOffset": 64}, {"referenceID": 25, "context": "We also experimented with dropconnect [29], a variant of dropout in which weights are randomly set to zero instead of hidden units, but found that it led to worse validation error in general.", "startOffset": 38, "endOffset": 42}, {"referenceID": 11, "context": "Each experiment optimized for 10000 minibatches of size 100 using the Adam algorithm [13], a variant of RMSprop that includes momentum.", "startOffset": 85, "endOffset": 89}, {"referenceID": 3, "context": "Dataset Solubility [4] Drug efficacy [5] Photovoltaic efficiency [8] Units log Mol/L EC50 in nM percent Predict mean 4.", "startOffset": 19, "endOffset": 22}, {"referenceID": 4, "context": "Dataset Solubility [4] Drug efficacy [5] Photovoltaic efficiency [8] Units log Mol/L EC50 in nM percent Predict mean 4.", "startOffset": 37, "endOffset": 40}, {"referenceID": 6, "context": "Dataset Solubility [4] Drug efficacy [5] Photovoltaic efficiency [8] Units log Mol/L EC50 in nM percent Predict mean 4.", "startOffset": 65, "endOffset": 68}, {"referenceID": 3, "context": "\u2022 Solubility: The aqueous solubility of 1144 molecules as measured by [4].", "startOffset": 70, "endOffset": 73}, {"referenceID": 4, "context": "falciparum, the parasite that causes malaria, as measured by [5].", "startOffset": 61, "endOffset": 64}, {"referenceID": 6, "context": "\u2022 Organic photovoltaic efficiency: The Harvard Clean Energy Project [8] uses expensive DFT simulations to estimate the photovoltaic efficiency of organic molecules.", "startOffset": 68, "endOffset": 71}, {"referenceID": 0, "context": "Software Automatic differentiation (AD) software packages such as Theano [1] significantly speed up development time by providing gradients automatically, but can only handle limited control structures and indexing.", "startOffset": 73, "endOffset": 76}, {"referenceID": 16, "context": "This package handles standard Numpy [18] code, and can differentiate code containing while loops, branches, and indexing.", "startOffset": 36, "endOffset": 40}, {"referenceID": 19, "context": "However, it may be fruitful to apply multiple layers of nonlinearities between each message-passing step (as in [22]), or to make information preservation easier by adapting the Long Short-Term Memory [10] architecture to pass information upwards.", "startOffset": 112, "endOffset": 116}, {"referenceID": 8, "context": "However, it may be fruitful to apply multiple layers of nonlinearities between each message-passing step (as in [22]), or to make information preservation easier by adapting the Long Short-Term Memory [10] architecture to pass information upwards.", "startOffset": 201, "endOffset": 205}, {"referenceID": 1, "context": "To avoid this problem, [2] proposed a hierarchical clustering of graph substructures.", "startOffset": 23, "endOffset": 26}, {"referenceID": 22, "context": "Techniques from natural language processing [25] might be fruitfully adapted to this domain.", "startOffset": 44, "endOffset": 48}, {"referenceID": 12, "context": "Convolutional neural networks Convolutional neural networks have been used to model images, speech, and time series [14].", "startOffset": 116, "endOffset": 120}, {"referenceID": 10, "context": "More recently, [12] and others have developed a convolutional neural network architecture for modeling sentences of varying length.", "startOffset": 15, "endOffset": 19}, {"referenceID": 13, "context": "Neural fingerprints The most closely related work is [15], who build a neural network having graph-valued inputs.", "startOffset": 53, "endOffset": 57}, {"referenceID": 20, "context": "A recursive neural network [23, 24] is then run from the leaves to the root to produce a fixed-size representation.", "startOffset": 27, "endOffset": 35}, {"referenceID": 21, "context": "A recursive neural network [23, 24] is then run from the leaves to the root to produce a fixed-size representation.", "startOffset": 27, "endOffset": 35}, {"referenceID": 2, "context": "[3] used circular fingerprints as inputs to an ensemble of neural networks, Gaussian processes, and random forests.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[19] used circular fingerprints (of depth 2) as inputs to a multitask neural network, showing that multiple tasks helped performance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Neural networks on fixed graphs [2] introduce convolutional networks on graphs in the regime where the graph structure is fixed, and each training example differs only in having different features at the vertices of the same graph.", "startOffset": 32, "endOffset": 35}, {"referenceID": 19, "context": "Neural networks on input-dependent graphs [22] propose a neural network model for graphs having an interesting training procedure.", "startOffset": 42, "endOffset": 46}, {"referenceID": 14, "context": "[16] also propose a neural network model for graphs with a learning scheme whose inner loop optimizes not the training loss, but rather the correlation between each newly-proposed vector and the training error residual.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Unrolled inference algorithms [9] and others have noted that iterative inference procedures sometimes resemble the feedforward computation of a recurrent neural network.", "startOffset": 30, "endOffset": 33}], "year": 2015, "abstractText": "Predicting properties of molecules requires functions that take graphs as inputs. Molecular graphs are usually preprocessed using hash-based functions to produce fixed-size fingerprint vectors, which are used as features for making predictions. We introduce a convolutional neural network that operates directly on graphs, allowing end-to-end learning of the feature pipeline. This architecture generalizes standard molecular fingerprints. We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.", "creator": "LaTeX with hyperref package"}}}