{"id": "1510.00259", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Oct-2015", "title": "A Generative Model of Words and Relationships from Multiple Sources", "abstract": "Neural Language Models are a powerful tool to meaningfully embed words into semantic vector spaces. However, learning vector space models of language generally relies on the availability of abundant and diverse training examples. In highly specialized domains this requirement may not be met due to difficulties in obtaining a large corpus, or the limited range of expression in average usage. Prior knowledge about entities in the language often exists in a knowledge base or ontology. We propose a generative model which allows for modeling and transfering semantic information in vector spaces by combining diverse data sources. We generalize the concept of co-occurrence from distributional semantics to include other types of relations between entities, evidence for which can come from a knowledge base (such as WordNet or UMLS). Our model defines a probability distribution over triplets consisting of word pairs with relations. Through stochastic maximum likelihood we learn a representation of these words as elements of a vector space and model the relations as affine transformations. We demonstrate the effectiveness of our generative approach by outperforming recent models on a knowledge-base completion task and demonstrating its ability to profit from the use of partially observed or fully unobserved data entries. Our model is capable of operating semi-supervised, where word pairs with no known relation are used as training data. We further demonstrate the usefulness of learning from different data sources with overlapping vocabularies.", "histories": [["v1", "Thu, 1 Oct 2015 14:42:19 GMT  (2404kb)", "https://arxiv.org/abs/1510.00259v1", "7 pages, 5 figures"], ["v2", "Thu, 3 Dec 2015 17:08:28 GMT  (125kb,D)", "http://arxiv.org/abs/1510.00259v2", "8 pages, 5 figures; incorporated feedback from reviewers; to appear in Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence 2016"]], "COMMENTS": "7 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.CL cs.LG stat.ML", "authors": ["stephanie l hyland", "theofanis karaletsos", "gunnar r\u00e4tsch"], "accepted": true, "id": "1510.00259"}, "pdf": {"name": "1510.00259.pdf", "metadata": {"source": "META", "title": "A Generative Model of Words and Relationships from Multiple Sources", "authors": ["Stephanie L. Hylanda", "Theofanis Karaletsosa", "Gunnar R\u00e4tscha"], "emails": ["gunnar}@ratschlab.org"], "sections": [{"heading": null, "text": "In fact, most of them are able to survive on their own."}, {"heading": "Related Work", "text": "The task of finding a continuous representation of elements of language is a specific case of our model with an approximate relationship; in recent years, this has been studied in detail and 2Imatinib is a tyrosine kinase inhibition used in the treatment of chronic myeloid leukemia. Bengio et al. (2003) described a neural architecture to predict the next word in a sequence, using distributed representations to overcome the curse of dimensionality. Since then, much work has been done to predict the surrounding context of a word and apply these distributed language representations, one such model being word2vec by Mikolov et al. (2013), which relies more explicitly on the distribution hypothesis of semantics by trying to predict the surrounding context of a word, either as a set of adjacent words (the Skip-gram model) or as an average of its environment (continuous pocket of words). We note later in the skip version that the model section is a specialized 2p version of language."}, {"heading": "Probabilistic Modelling of Words and Relationships", "text": "We consider a probable distribution via three (S, R, T), where S (S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S"}, {"heading": "Experiments", "text": "The fact is that we are at a time when we are not yet in a position, when we are in a position to assert ourselves, that we are in a position, that we are in a position to assert ourselves, that we are in a position, that we are in a position, that we are in a position, that we are in a position, that we are in a position, that we are in a position, that we are in a position, to put ourselves in a position, that we are in."}, {"heading": "Discussion", "text": "We have presented a probabilistic generative model of words and relationships between them. By estimating the parameters of this model by stochastic gradient descent, we obtain vector and matrix representations of these words and relationships. To make learning tractable, we use persistent contrastive divergence with Gibbs sampling between entity types (S, R, T) to approximate gradients of the partition function. Our model uses an energy function that includes the idealized word2vec model as a special case. By augmentalizing space and viewing relationships as arbitrary affine transformations, we combine the advantages of earlier models. Furthermore, our formulation as a generative model is distinguishable and allows for more flexible use, especially in the missing data, semi- and unattended environments. Motivated by domain settings in which structured or unstructured data can be scarce, we can illustrate the quality of a model that includes both."}, {"heading": "Acknowledgments", "text": "This work was funded by Memorial Hospital and the Sloan Kettering Institute (MSKCC; G.R.). Additional support for S.L.H. came from the Tri-Institutional Training Program in Computational Biology and Medicine."}], "references": [{"title": "A generative model of vector space semantics", "author": ["J. Andreas", "Z. Ghahramani"], "venue": "Association for Computational Linguistics (ACL) 91.", "citeRegEx": "Andreas and Ghahramani,? 2013", "shortCiteRegEx": "Andreas and Ghahramani", "year": 2013}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "J. Mach. Learn. Res. 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "The unified medical language system (umls): integrating biomedical terminology", "author": ["O. Bodenreider"], "venue": "Nucleic Acids Research 32:D267\u2013D270.", "citeRegEx": "Bodenreider,? 2004", "shortCiteRegEx": "Bodenreider", "year": 2004}, {"title": "Learning structured embeddings of knowledge bases", "author": ["A. Bordes", "J. Weston", "R. Collobert", "Y. Bengio"], "venue": "Conference on Artificial Intelligence.", "citeRegEx": "Bordes et al\\.,? 2011", "shortCiteRegEx": "Bordes et al\\.", "year": 2011}, {"title": "Joint learning of words and meaning representations for open-text semantic parsing", "author": ["A. Bordes", "X. Glorot", "J. Weston", "Y. Bengio"], "venue": "International Conference on Artificial Intelligence and Statistics, 127\u2013135.", "citeRegEx": "Bordes et al\\.,? 2012", "shortCiteRegEx": "Bordes et al\\.", "year": 2012}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["A. Bordes", "N. Usunier", "A. Garcia-Duran", "J. Weston", "O. Yakhnenko"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2787\u20132795.", "citeRegEx": "Bordes et al\\.,? 2013", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Multitask Learning", "author": ["R. Caruana"], "venue": "Machine Learning 28(1):41 \u2013 75.", "citeRegEx": "Caruana,? 1997", "shortCiteRegEx": "Caruana", "year": 1997}, {"title": "Regularized multi-task learning", "author": ["T. Evgeniou", "M. Pontil"], "venue": "International Conference on Knowledge Discovery and Data Mining, 109\u2013117.", "citeRegEx": "Evgeniou and Pontil,? 2004", "shortCiteRegEx": "Evgeniou and Pontil", "year": 2004}, {"title": "Transition-based knowledge graph embedding with relational mapping properties", "author": ["M. Fan", "Q. Zhou", "E. Chang", "T.F. Zheng"], "venue": "Proceedings of the 28th Pacific Asia Conference on Language, Information, and Computation, 328\u2013337.", "citeRegEx": "Fan et al\\.,? 2014", "shortCiteRegEx": "Fan et al\\.", "year": 2014}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["M. Faruqui", "J. Dodge", "S.K. Jauhar", "C. Dyer", "E. Hovy", "N.A. Smith"], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL). Association for Computational", "citeRegEx": "Faruqui et al\\.,? 2015", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Incorporating both distributional and relational semantics in word representations", "author": ["D. Fried", "K. Duh"], "venue": "Workshop Contribution at the International Conference on Learning Representations (ICLR), 2015.", "citeRegEx": "Fried and Duh,? 2014", "shortCiteRegEx": "Fried and Duh", "year": 2014}, {"title": "word2vec explained: deriving mikolov et al.\u2019s negative-sampling word-embedding method", "author": ["Y. Goldberg", "O. Levy"], "venue": "arXiv preprint arXiv:1402.3722", "citeRegEx": "Goldberg and Levy,? \\Q2014\\E", "shortCiteRegEx": "Goldberg and Levy", "year": 2014}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural Computation 14(8):1771\u20131800.", "citeRegEx": "Hinton,? 2002", "shortCiteRegEx": "Hinton", "year": 2002}, {"title": "A generative model of words and relationships from multiple sources", "author": ["S.L. Hyland", "T. Karaletsos", "G. R\u00e4tsch"], "venue": "International Workshop on Embeddings and Semantics, 16\u201320.", "citeRegEx": "Hyland et al\\.,? 2015", "shortCiteRegEx": "Hyland et al\\.", "year": 2015}, {"title": "A fast variational approach for learning markov random field language models", "author": ["Y. Jernite", "A.M. Rush", "D. Sontag"], "venue": "International Conference on Machine Learning (ICML).", "citeRegEx": "Jernite et al\\.,? 2015", "shortCiteRegEx": "Jernite et al\\.", "year": 2015}, {"title": "Embedding a semantic network in a word space", "author": ["R. Johansson", "L. Nieto Pi\u00f1a"], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL): Human Language Technologies, 1428\u20131433. Association for Computa-", "citeRegEx": "Johansson and Pi\u00f1a,? 2015", "shortCiteRegEx": "Johansson and Pi\u00f1a", "year": 2015}, {"title": "Semmeddb: a pubmed-scale repository of biomedical semantic predications", "author": ["H. Kilicoglu", "D. Shin", "M. Fiszman", "G. Rosemblat", "T.C. Rindflesch"], "venue": "Bioinformatics 28(23):3158\u20133160.", "citeRegEx": "Kilicoglu et al\\.,? 2012", "shortCiteRegEx": "Kilicoglu et al\\.", "year": 2012}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "Proceedings of the International Conference on Learning Representations (ICLR) 2015.", "citeRegEx": "Kingma and Ba,? 2015", "shortCiteRegEx": "Kingma and Ba", "year": 2015}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Y. Lin", "Z. Liu", "M. Sun", "Y. Liu", "X. Zhu"], "venue": "Proceedings of AAAI.", "citeRegEx": "Lin et al\\.,? 2015", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Exploiting task-oriented resources to learn word embeddings for clinical abbreviation expansion", "author": ["Y. Liu", "T. Ge", "K. Mathews", "H. Ji", "D. McGuinness"], "venue": "Proceedings of BioNLP 15, 92\u201397. Beijing, China: Association for Computational Linguistics.", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems (NIPS), 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "The distributional hypothesis", "author": ["M. Sahlgren"], "venue": "Italian Journal of Linguistics 20(1):33\u201353.", "citeRegEx": "Sahlgren,? 2008", "shortCiteRegEx": "Sahlgren", "year": 2008}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["R. Socher", "D. Chen", "C.D. Manning", "A. Ng"], "venue": "Advances in Neural Information Processing Systems (NIPS), 926\u2013934.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["K. Tai", "R. Socher", "C.D. Manning"], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Training restricted boltzmann machines using approximations to the likelihood gradient", "author": ["T. Tieleman"], "venue": "International Conference on Machine Learning (ICML).", "citeRegEx": "Tieleman,? 2008", "shortCiteRegEx": "Tieleman", "year": 2008}, {"title": "Knowledge graph and text jointly embedding", "author": ["Z. Wang", "J. Zhang", "J. Feng", "Z. Chen"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, 1591\u20131601.", "citeRegEx": "Wang et al\\.,? 2014a", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Z. Wang", "J. Zhang", "J. Feng", "Z. Chen"], "venue": "Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, 1112\u20131119.", "citeRegEx": "Wang et al\\.,? 2014b", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Connecting language and knowledge bases with embedding models for relation extraction", "author": ["J. Weston", "A. Bordes", "O. Yakhnenko", "N. Usunier"], "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP), 1366\u20131371.", "citeRegEx": "Weston et al\\.,? 2013", "shortCiteRegEx": "Weston et al\\.", "year": 2013}, {"title": "Multitask Learning in Computational Biology", "author": ["C. Widmer", "G. R\u00e4tsch"], "venue": "JMLR W&CP. ICML 2011 Unsupervised and Transfer Learning Workshop. 27:207\u2013216.", "citeRegEx": "Widmer and R\u00e4tsch,? 2012", "shortCiteRegEx": "Widmer and R\u00e4tsch", "year": 2012}, {"title": "Rc-net: A general framework for incorporating knowledge into word representations", "author": ["C. Xu", "Y. Bai", "J. Bian", "B. Gao", "G. Wang", "X. Liu", "T.-Y. Liu"], "venue": "Proceedings of the 23rd ACM International Conference on Information and Knowledge Management, 1219\u20131228. ACM.", "citeRegEx": "Xu et al\\.,? 2014", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Embedding entities and relations for learning and inference in knowledge bases", "author": ["B. Yang", "W. Yih", "X. He", "J. Gao", "L. Deng"], "venue": "Proceedings of the International Conference on Learning Representations (ICLR) 2015.", "citeRegEx": "Yang et al\\.,? 2015", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["M. Yu", "M. Dredze"], "venue": "Association for Computational Linguistics (ACL), 545\u2013550.", "citeRegEx": "Yu and Dredze,? 2014", "shortCiteRegEx": "Yu and Dredze", "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": "One paradigm for obtaining this embedding is the neural language model (Bengio et al. 2003), which traditionally draws on local co-occurence statistics from sequences of words (sentences) to obtain an encoding of words as vectors in a space whose geometry respects linguistic and semantic features.", "startOffset": 71, "endOffset": 91}, {"referenceID": 1, "context": "One paradigm for obtaining this embedding is the neural language model (Bengio et al. 2003), which traditionally draws on local co-occurence statistics from sequences of words (sentences) to obtain an encoding of words as vectors in a space whose geometry respects linguistic and semantic features. The core concept behind this procedure is the distributional hypothesis of language; see Sahlgren (2008), that semantics can be inferred by examining the context of a word.", "startOffset": 72, "endOffset": 404}, {"referenceID": 19, "context": "For example, medical text data (Liu et al. 2015) often contains protected health information, necessitating access restrictions and potentially limiting corpus size to that obtainable from a single institution, resulting in a corpus with less than tens of millions of sentences, not billions as in (for example) Google n-grams.", "startOffset": 31, "endOffset": 48}, {"referenceID": 0, "context": "Bengio et al. (2003) described a neural architecture to predict the next word in a sequence, using distributed representations to overcome the curse of dimensionality.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Bengio et al. (2003) described a neural architecture to predict the next word in a sequence, using distributed representations to overcome the curse of dimensionality. Since then, much work has been devoted to obtaining, understanding, and applying these distributed language representations. One such model is word2vec of Mikolov et al. (2013), which more explicitly relies on the distributional hypothesis of semantics by attempting to predict the surrounding context of a word, either as a set of neighbouring words (the skip-gram model) or as an average of its environment (continuous bag of words).", "startOffset": 0, "endOffset": 345}, {"referenceID": 0, "context": "We note that a generative approach to language was also explored by Andreas and Ghahramani (2013), but does not concern relationships.", "startOffset": 68, "endOffset": 98}, {"referenceID": 20, "context": "(2015)) represents relationships as translations, motivated by the tree representation of hierarchical relationships, and observations that linear composition of entities appears to preserve semantic meaning (Mikolov et al. 2013).", "startOffset": 208, "endOffset": 229}, {"referenceID": 3, "context": "Bordes et al. (2011) scored the similarity of entities under a given relationship by their distance after transformation using pairs of relationship-specific matrices.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Bordes et al. (2011) scored the similarity of entities under a given relationship by their distance after transformation using pairs of relationship-specific matrices. Socher et al. (2013) describe a neural network architecture with a more complex scoring function, noting that the previous method does not allow for interactions between entities.", "startOffset": 0, "endOffset": 189}, {"referenceID": 3, "context": "Bordes et al. (2011) scored the similarity of entities under a given relationship by their distance after transformation using pairs of relationship-specific matrices. Socher et al. (2013) describe a neural network architecture with a more complex scoring function, noting that the previous method does not allow for interactions between entities. The TransE model of Bordes et al. (2013) (and extensions such as Wang et al.", "startOffset": 0, "endOffset": 389}, {"referenceID": 3, "context": "Bordes et al. (2011) scored the similarity of entities under a given relationship by their distance after transformation using pairs of relationship-specific matrices. Socher et al. (2013) describe a neural network architecture with a more complex scoring function, noting that the previous method does not allow for interactions between entities. The TransE model of Bordes et al. (2013) (and extensions such as Wang et al. (2014b), Fan et al.", "startOffset": 0, "endOffset": 433}, {"referenceID": 3, "context": "Bordes et al. (2011) scored the similarity of entities under a given relationship by their distance after transformation using pairs of relationship-specific matrices. Socher et al. (2013) describe a neural network architecture with a more complex scoring function, noting that the previous method does not allow for interactions between entities. The TransE model of Bordes et al. (2013) (and extensions such as Wang et al. (2014b), Fan et al. (2014), and Lin et al.", "startOffset": 0, "endOffset": 452}, {"referenceID": 3, "context": "Bordes et al. (2011) scored the similarity of entities under a given relationship by their distance after transformation using pairs of relationship-specific matrices. Socher et al. (2013) describe a neural network architecture with a more complex scoring function, noting that the previous method does not allow for interactions between entities. The TransE model of Bordes et al. (2013) (and extensions such as Wang et al. (2014b), Fan et al. (2014), and Lin et al. (2015)) represents relationships as translations, motivated by the tree representation of hierarchical relationships, and observations that linear composition of entities appears to preserve semantic meaning (Mikolov et al.", "startOffset": 0, "endOffset": 475}, {"referenceID": 3, "context": "Bordes et al. (2011) scored the similarity of entities under a given relationship by their distance after transformation using pairs of relationship-specific matrices. Socher et al. (2013) describe a neural network architecture with a more complex scoring function, noting that the previous method does not allow for interactions between entities. The TransE model of Bordes et al. (2013) (and extensions such as Wang et al. (2014b), Fan et al. (2014), and Lin et al. (2015)) represents relationships as translations, motivated by the tree representation of hierarchical relationships, and observations that linear composition of entities appears to preserve semantic meaning (Mikolov et al. 2013). These approaches are uniquely concerned with relational data however, and do not consider distributional semantics from free text. Faruqui et al. (2015) and Johansson and Nieto Pi\u00f1a (2015) describe methods to modify pre-existing word embeddings to align them with evidence derived from a knowledge base, although their models do not learn representations de novo.", "startOffset": 0, "endOffset": 852}, {"referenceID": 3, "context": "Bordes et al. (2011) scored the similarity of entities under a given relationship by their distance after transformation using pairs of relationship-specific matrices. Socher et al. (2013) describe a neural network architecture with a more complex scoring function, noting that the previous method does not allow for interactions between entities. The TransE model of Bordes et al. (2013) (and extensions such as Wang et al. (2014b), Fan et al. (2014), and Lin et al. (2015)) represents relationships as translations, motivated by the tree representation of hierarchical relationships, and observations that linear composition of entities appears to preserve semantic meaning (Mikolov et al. 2013). These approaches are uniquely concerned with relational data however, and do not consider distributional semantics from free text. Faruqui et al. (2015) and Johansson and Nieto Pi\u00f1a (2015) describe methods to modify pre-existing word embeddings to align them with evidence derived from a knowledge base, although their models do not learn representations de novo.", "startOffset": 0, "endOffset": 888}, {"referenceID": 23, "context": "Similar in spirit to our work is Weston et al. (2013), where entities belonging to a structured database are identified in unstructured (free) text in order to obtain embeddings useful for relation prediction.", "startOffset": 33, "endOffset": 54}, {"referenceID": 10, "context": "This approach is also employed by Fried and Duh (2014), Xu et al.", "startOffset": 34, "endOffset": 55}, {"referenceID": 10, "context": "This approach is also employed by Fried and Duh (2014), Xu et al. (2014), Yu and Dredze (2014), and Wang et al.", "startOffset": 34, "endOffset": 73}, {"referenceID": 10, "context": "This approach is also employed by Fried and Duh (2014), Xu et al. (2014), Yu and Dredze (2014), and Wang et al.", "startOffset": 34, "endOffset": 95}, {"referenceID": 10, "context": "This approach is also employed by Fried and Duh (2014), Xu et al. (2014), Yu and Dredze (2014), and Wang et al. (2014a). In these cases, separate objectives are used to incorporate different data sources, combining (in the case of Xu et al.", "startOffset": 34, "endOffset": 120}, {"referenceID": 10, "context": "This approach is also employed by Fried and Duh (2014), Xu et al. (2014), Yu and Dredze (2014), and Wang et al. (2014a). In these cases, separate objectives are used to incorporate different data sources, combining (in the case of Xu et al. (2014)) the skip-gram objective from Mikolov et al.", "startOffset": 34, "endOffset": 248}, {"referenceID": 10, "context": "This approach is also employed by Fried and Duh (2014), Xu et al. (2014), Yu and Dredze (2014), and Wang et al. (2014a). In these cases, separate objectives are used to incorporate different data sources, combining (in the case of Xu et al. (2014)) the skip-gram objective from Mikolov et al. (2013) and the TransE ob-", "startOffset": 34, "endOffset": 300}, {"referenceID": 3, "context": "jective of Bordes et al. (2013). Our method uses a single energy function over the joint space of word pairs with relationships, combining the \u2018distributional objective\u2019 with that of relational data by considering free-text co-occurrences as another type of relationship.", "startOffset": 11, "endOffset": 32}, {"referenceID": 6, "context": "The motivation for our work is similar in spirit to multitask and transfer learning (for instance, Caruana (1997), Evgeniou and Pontil (2004), or Widmer and R\u00e4tsch (2012)).", "startOffset": 99, "endOffset": 114}, {"referenceID": 6, "context": "The motivation for our work is similar in spirit to multitask and transfer learning (for instance, Caruana (1997), Evgeniou and Pontil (2004), or Widmer and R\u00e4tsch (2012)).", "startOffset": 99, "endOffset": 142}, {"referenceID": 6, "context": "The motivation for our work is similar in spirit to multitask and transfer learning (for instance, Caruana (1997), Evgeniou and Pontil (2004), or Widmer and R\u00e4tsch (2012)).", "startOffset": 99, "endOffset": 171}, {"referenceID": 17, "context": "Following Mikolov et al. (2013), we learn two representations for each word: cs represents word s when it appears as a source, and vt for word t appearing as a target.", "startOffset": 10, "endOffset": 32}, {"referenceID": 17, "context": "Following Mikolov et al. (2013), we learn two representations for each word: cs represents word s when it appears as a source, and vt for word t appearing as a target.3 Relationships act by altering cs through their action on the vector space (cs 7\u2192 GRcs). By allowing GR to be an arbitrary affine transformation, we combine the bilinear form of Socher et al. (2013) with translation operators of Bordes et al.", "startOffset": 10, "endOffset": 367}, {"referenceID": 3, "context": "(2013) with translation operators of Bordes et al. (2013).", "startOffset": 37, "endOffset": 58}, {"referenceID": 21, "context": "Sahlgren (2008)).", "startOffset": 0, "endOffset": 16}, {"referenceID": 20, "context": "we observe that the |R| = 1, GR = I case recovers the original softmax objective described in Mikolov et al. (2013), so the idealised word2vec model is a special case of our model.", "startOffset": 94, "endOffset": 116}, {"referenceID": 24, "context": "In order to circumvent this intractability we resort to techniques used to train Restricted Boltzmann Machines and use stochastic maximum likelihood, also known as persistent contrastive divergence (PCD); (Tieleman 2008).", "startOffset": 205, "endOffset": 220}, {"referenceID": 12, "context": "In contrastive divergence, the gradient of the partition function is estimated using samples drawn from the model distribution seeded at the current training example (Hinton 2002).", "startOffset": 166, "endOffset": 179}, {"referenceID": 17, "context": "We use Adam (Kingma and Ba 2015) to adapt learning rates and improve numerical stability.", "startOffset": 12, "endOffset": 32}, {"referenceID": 22, "context": "Data As structured data, we use the WordNet dataset described by Socher et al. (2013), available at http:// stanford.", "startOffset": 65, "endOffset": 86}, {"referenceID": 22, "context": "WordNet Prediction Task We used our model to solve the basic prediction task described in Socher et al. (2013). In this case, the model must differentiate true and false triples, where false triples are obtained by corrupting the T entry", "startOffset": 90, "endOffset": 111}, {"referenceID": 4, "context": "were a single layer model without an interaction term, a Hadamard model (Bordes et al. 2012) and the model of Bordes et al.", "startOffset": 72, "endOffset": 92}, {"referenceID": 19, "context": "By learning explicit representations of each of the 38,588 entities in WordNet, our approach most closely follows the \u2018Entity Vector\u2019 task in Socher et al. This is to be contrasted with the \u2018Word Vector\u2019 task, where a representation is learned for each word, and entity representations are obtained by averaging their word vectors. We elected not to perform this task because we are not confident that composition into phrases through averaging is well-justified. Using the validation set to select an early stopping point at 66 epochs, we obtain a test set accuracy of 78.2% with an AUROC of 85.6%. The \u2018Neural Tensor Model\u2019 (NTN) described in Socher et al. (2013) achieves an accuracy of around 70% on this task, although we note that the simpler Bilinear model also described in Socher et al.", "startOffset": 142, "endOffset": 666}, {"referenceID": 19, "context": "By learning explicit representations of each of the 38,588 entities in WordNet, our approach most closely follows the \u2018Entity Vector\u2019 task in Socher et al. This is to be contrasted with the \u2018Word Vector\u2019 task, where a representation is learned for each word, and entity representations are obtained by averaging their word vectors. We elected not to perform this task because we are not confident that composition into phrases through averaging is well-justified. Using the validation set to select an early stopping point at 66 epochs, we obtain a test set accuracy of 78.2% with an AUROC of 85.6%. The \u2018Neural Tensor Model\u2019 (NTN) described in Socher et al. (2013) achieves an accuracy of around 70% on this task, although we note that the simpler Bilinear model also described in Socher et al. (2013) achieves 74% and is closer to the energy function we employ.", "startOffset": 142, "endOffset": 803}, {"referenceID": 19, "context": "By learning explicit representations of each of the 38,588 entities in WordNet, our approach most closely follows the \u2018Entity Vector\u2019 task in Socher et al. This is to be contrasted with the \u2018Word Vector\u2019 task, where a representation is learned for each word, and entity representations are obtained by averaging their word vectors. We elected not to perform this task because we are not confident that composition into phrases through averaging is well-justified. Using the validation set to select an early stopping point at 66 epochs, we obtain a test set accuracy of 78.2% with an AUROC of 85.6%. The \u2018Neural Tensor Model\u2019 (NTN) described in Socher et al. (2013) achieves an accuracy of around 70% on this task, although we note that the simpler Bilinear model also described in Socher et al. (2013) achieves 74% and is closer to the energy function we employ. The improved performance exhibited by this simpler Bilinear model was also noted by Yang et al. (2015). Other baselines reported by Socher et al.", "startOffset": 142, "endOffset": 967}, {"referenceID": 3, "context": "were a single layer model without an interaction term, a Hadamard model (Bordes et al. 2012) and the model of Bordes et al. (2011) which learns separate left and right relationship operators for each element of the triple.", "startOffset": 73, "endOffset": 131}, {"referenceID": 3, "context": "were a single layer model without an interaction term, a Hadamard model (Bordes et al. 2012) and the model of Bordes et al. (2011) which learns separate left and right relationship operators for each element of the triple. These were outperformed by the Bilinear and NTN models, see Figure 4 in Socher et al. (2013) for further details.", "startOffset": 73, "endOffset": 316}, {"referenceID": 22, "context": "As a preliminary test of our model, we also considered the FreeBase task described by Socher et al. (2013). Initial testing yielded an accuracy of 85.", "startOffset": 86, "endOffset": 107}, {"referenceID": 22, "context": "Figure 2: Semi-supervised learning improves learned embeddings: We tested the semi-supervised extension of our approach on the entity relationship learning task described in Socher et al. (2013) and previous subsection.", "startOffset": 174, "endOffset": 195}, {"referenceID": 20, "context": "We also trained word2vec (Mikolov et al. 2013) on a much larger Wikipedia-only dataset (4,145,372 sentences) and trained a classifier on its vectors; results are shown as black lines.", "startOffset": 25, "endOffset": 46}, {"referenceID": 16, "context": "Indeed, initial experiments combining clinical text notes with relational data between UMLS concepts from SemMedDB (Kilicoglu et al. 2012) have demonsrated the utility of this combined approach to predict the functional relationship between medical concepts, for example, cisplatin", "startOffset": 115, "endOffset": 138}], "year": 2015, "abstractText": "Neural language models are a powerful tool to embed words into semantic vector spaces. However, learning such models generally relies on the availability of abundant and diverse training examples. In highly specialised domains this requirement may not be met due to difficulties in obtaining a large corpus, or the limited range of expression in average use. Such domains may encode prior knowledge about entities in a knowledge base or ontology. We propose a generative model which integrates evidence from diverse data sources, enabling the sharing of semantic information. We achieve this by generalising the concept of co-occurrence from distributional semantics to include other relationships between entities or words, which we model as affine transformations on the embedding space. We demonstrate the effectiveness of this approach by outperforming recent models on a link prediction task and demonstrating its ability to profit from partially or fully unobserved data training labels. We further demonstrate the usefulness of learning from different data sources with overlapping vocabularies.", "creator": "TeX"}}}