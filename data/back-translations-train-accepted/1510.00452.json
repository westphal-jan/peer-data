{"id": "1510.00452", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Oct-2015", "title": "Optimal Binary Classifier Aggregation for General Losses", "abstract": "We develop a worst-case analysis of aggregation of binary classifier ensembles in a transductive setting, for a broad class of losses including but not limited to all convex surrogates. The result is a family of parameter-free ensemble aggregation algorithms, which are as efficient as linear learning and prediction for convex risk minimization but work without any relaxations whatsoever on many nonconvex losses like the 0-1 loss. The prediction algorithms take a familiar form, applying \"link functions\" to a generalized notion of ensemble margin, but without the assumptions typically made in margin-based learning - all this structure follows from a minimax interpretation of loss minimization.", "histories": [["v1", "Thu, 1 Oct 2015 23:58:46 GMT  (64kb,D)", "http://arxiv.org/abs/1510.00452v1", null], ["v2", "Mon, 5 Oct 2015 06:05:15 GMT  (62kb,D)", "http://arxiv.org/abs/1510.00452v2", null], ["v3", "Sat, 5 Dec 2015 20:28:54 GMT  (63kb,D)", "http://arxiv.org/abs/1510.00452v3", "NIPS 2015, \"Learning from Easy Data\" Workshop"], ["v4", "Fri, 26 Feb 2016 02:04:48 GMT  (65kb,D)", "http://arxiv.org/abs/1510.00452v4", "NIPS 2015, \"Learning Faster from Easy Data II\" Workshop"], ["v5", "Mon, 7 Nov 2016 10:28:36 GMT  (71kb,D)", "http://arxiv.org/abs/1510.00452v5", "NIPS 2016"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["akshay balsubramani", "yoav freund"], "accepted": true, "id": "1510.00452"}, "pdf": {"name": "1510.00452.pdf", "metadata": {"source": "CRF", "title": "Minimax Binary Classifier Aggregation with General Losses", "authors": ["Akshay Balsubramani", "Yoav Freund"], "emails": ["abalsubr@ucsd.edu", "yfreund@ucsd.edu"], "sections": [{"heading": null, "text": "The result is a family of parameter-free ensemble aggregation algorithms that are as efficient as linear learning and predictions to minimize convex risk, but that do not ease many nonconvex losses, such as 0-1 loss. Prediction algorithms take a familiar form and apply \"link functions\" to a general idea of ensemble span, but without the assumptions typically made in margin-based learning - all this structure follows from a minimax interpretation of loss minimization."}, {"heading": "1 Introduction", "text": "Consider a problem with binary classification, in which one tries to find the best predictor, which is divided into two categories."}, {"heading": "1.1 Preliminaries", "text": "Our setting generalizes that of Balsubramani and Freund [BF15a], in which we are given an ensemble H = {h1,..., hp} and unlabeled data x1,..., xn on which we want to make predictions. The predictions of the ensemble on the unlabeled data are called F: F = h1 (x1) h1 (x2) \u00b7 \u00b7 h1 (xn)....... hp (x1) hp (x2) \u00b7 \u00b7 \u00b7 hp (xn). [\u2212 1, 1] p \u00b7 n (1) We use vector notation for the rows and columns of F: hi = (hi (x1), \u00b7 \u00b7, hi (xn) > and xj = (h1 (xj), \u00b7 \u00b7, hp (xj) p \u00b7 n (xj) >."}, {"heading": "1.2 Loss Functions", "text": "At each individual test point with the randomized binary designation zj (\u2212 1, 1], our expected performance in predicting gj in relation to randomizing zj by a loss function (zj, gj) is measured. It shows that '(zj, gj) = (1 + zj2)' (1, gj) + (1 \u2212 zj2) '(\u2212 1, gj): = (1 + zj2)' + (gj) + (1 \u2212 zj2) '\u2212 (gj), where we conveniently assume' + (gj): = '(1, gj) and' (gj): = '(\u2212 1, gj)' (\u00b1 the partial losses after previous work [RW10] '. In this manuscript, we assume that the assumption of' + (\u00b7) and '\u2212 (\u00b7): the assumption of the differentiability of' (\u2212 1, gj), '(\u00b7), the prediction decreases from'."}, {"heading": "2 Evaluation with General Losses", "text": "The idea of [BF15a] is to formulate the ensemble aggregation problem as a two-player zero-sum game between a predictor and an opponent. In this game, the predictor is the first player, the g = (g1; g2;...; gn), a randomized label gj [\u2212 1, 1] for each example {xj} nj = 1. The opponent then sets the labels z [\u2212 1, 1]. Of course, the classifiers are known to perform well on the test data. Accordingly, in this section we assume that the predictor has knowledge of a correlation vector b [0, 1] p, that it has the labels z [p], 1 n n n, j = 1 hi (xj) zj, bi (2) i.e. 1nFz, b."}, {"heading": "2.1 Results", "text": "A few more variables are convenient to define before discussing our main results.The loss-based score function is: [\u2212 1, 1] 7 \u2192 R setpoint (g): = \"\u2212 (g) \u2212\" + (g) (We will also specify the vector setpoint (g) component by component with [\u0432 (g)] j = \"(gj),\" so we can imagine this as a kind of connecting function. From these points of view, we can provide the solution for the game (3), which depends on the optimum of a convexic function. Definition 1 (Potential source): p setpoint (m). Definition 1: p setpoint (m) = \u2212 m setpoint (\u2212 1) if m setpoint is. Definition 1 (Potential source): p setpoint (m)."}, {"heading": "2.1.1 Solution of the Game", "text": "These are used to describe the minimax balance of the game (3), in our main result: \"b > cases.\" Theorem 3: The minimax value of the game (3) ismin g: \"b\" (3) ismin g: \"b\" (3) ismin g: \"b\" (3) ismin g: \"b\" (3) ismin g: \"b\" (4), \"n\" (4), \"n\" (4), \"1 nFz: b\" (3), 1 nFz: b \"(3), 1 nFz: b,\" 2 \"(4), 2\" (4), \"2\" (4), \"(4),\" (4), \"2\" (4), \"2,\" 3 \"(4),\" (4), \"(4),\" (4), \"2,\" (4), \"(4),\" (4), \"2,\" (4), \"(4),\" (4), \"(4),\" 2, \"(4),\" (4), \"(4),\" (4), \"(4),\" (4), \"(4),\" 2, \"(4),\" (4), \"(4), 2,\" (4), \"(4),\" (4), \"(4),\" (4), \"(4),\" (4), \"(4),\" (4), \"(4),\" (2), \"(4),\" (4), \"(4),\" (4), \"(4),\" (2), \"(4),\" (4), 2), \"(4),\" (4), \"(4),\" (4), 2), \"(4), 2), 2), 2,\" (4), \",\" (4), 2, \",\" (4), \",\" (4), \"(4), 2), 2), 2,\", \",\", \",\", \"(4),\", \"(4), 2),\", \"(4),\", 2),"}, {"heading": "2.2 The Ensemble Aggregation Algoritheorem", "text": "Theorem 3 defines a recipe for aggregating the given ensemble predictions on the test set. This can be specified in the form of a learning algorithm and a prediction method. Learn. Minimize the slip function \u03b3 (\u03c3), and find the minimizer \u043c, which achieves V. This is a convex optimization under general conditions (Lemma 5), and if the test examples are i.i.d., it is a sum of n i.i.d. functions. As such, it is easily accessible even for standard first order optimization methods, which require only O (1) test examples at once. In practice, learning uses such methods to roughly minimize such a point by finding some \u03c3A, so that it (\u03c3A) to a certain degree (\u03c3) +. Standard convex optimization methods will do so, because the slip function is Lipschitz, as Lemma 5 shows (combining the prediction that both have an observation)."}, {"heading": "2.3 Discussion and Extensions", "text": "The paper [BF15a] deals with a problem of 0-1 loss minimization, which is known to be strong NP-hard when solved directly, and its formulation in the transductive environment in which the data distribution is known is crucial, giving particular importance to the dual problem, so that the learning problem is based on the always convex Lagrange double function and is therefore understandable. This work generalizes this idea, since the possibly non-convex partial losses are minimized transductively by a simple convex optimization. A similar formal technique, including the use of L-standard constraints to efficiently decompose the optimization across many examples, has been used for another purpose in the \"drifting game\" analysis of the increase ([SF12], Sec. 13.4.1).Our transductive formulation does not include environments or loosening of the loss, an advantage that we consider significant - it circumvents the consistency and we apply Jagnox-limiting methods in the Jnox rule."}, {"heading": "2.3.1 Uniform Convergence Bounds", "text": "Since b is considered a lower limit for ensemble classification losses, the hatching function can be efficiently optimized, which is reflected in Section 2.2 of the Worst-Case Prediction Loss Limit. This can also be extended to explicitly include uniform convergence limits to b affecting the dual (L1) standard of the dual vector. Theorem 6. We have g [\u2212 1,1] n max z [\u2212 1,1] n, [\u2212 1nFz \u2212 z, g) = min \u03c3 Rp \u2212 clinical + 1 j = 1 \u0445 (x > j) + 1 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _"}, {"heading": "2.3.2 Weighted Test Sets, Covariate Shift, and Label Noise", "text": "Although our results here are the binary classification of a uniformly weighted test set, it should be noted that our formulation is a weighted test set with only one modification of the slip function: Theorem 7. For each arbitrary vector r \u2264 0n, min g = max z, [\u2212 1.1] n max z, [\u2212 1.1] n, 1 nFz \u2265 b1n \u00b2 j = 1 rj '(zj, gj) = 1 2 min \u2264 0p \u2212 b > \u03c3 + 1 n \u00b2 j = 1 rj\u044b (x > j \u03c3 rj) Such a weighted classification can be parlayed into algorithms for general verified learning problems via learning reductions [BLZ08]. Assigning weights to the test set for evaluation is equivalent to taking into account the covariable shift in our settings. Furthermore, the weights used in Theorem 7 can be interpreted as alternating box constraints on the evaluation set."}, {"heading": "2.4 Examples of Different Losses", "text": "For more information on Theorem 3, we list some specific cases in which \"+,\" \u2212 are explicitly defined. These losses can be found throughout the literature; for more information, see Reid and Williamson [RW10]. The key functions are for these losses in Table 1 and Figure 1. \u2022 0-1 loss: Here gj is considered a randomized binary prediction; this case was developed in [BF15a]. \u2022 Log loss \u2022 Absolute loss: The absolute loss can be defined as \"abs \u2212 (gj) = 1 + gj and\" abs + (gj) = 1 \u2212 gj (13). The partial losses are the same as for 0-1 loss up to scaling, and therefore all our results are the same. \u2022 Square loss \u2022 Cost-weighted misclassification (quantity) loss: This is defined with a parameter representing the relative cost of false positives versus false negatives."}, {"heading": "3 Constraints on General Losses", "text": "In the previous section, we take advantage of the fact that each hypothesis is a linear constraint on the true terms z, with respect to zero-one loss. In fact, all the losses we look at are linear in z, as in (4): \"(z, g) = 1n n n-one loss = 1 1 2 ['+ (gj) +\" \u2212 1 2-one loss. \"(14) Therefore, any classifier hi can be used to constrain the test terms z, not with the zero-one loss of His predictions, but with some other losses. Therefore, let's remember that hi-one constraints [\u2212 1, 1] n is the vector of test predictions of hi. Let's say that we can have an upper constraint on the loss of hi, i.e.\" (z, hi) one-one loss of hi-one. \"If we define constraints as follows 1 j = hi-n (n = 1)."}, {"heading": "3.1 Matching Objective and Constraint Losses", "text": "Despite this generality, we can gain some intuition about the aggregation method for general losses. To do this in the rest of this section, we only consider the case if each indexer contributes exactly one constraint to the problem, and the losses used for these constraints are all the same as the losses used in the objective function. In other words, we consider the Minimax prediction problem to be V ': = min g [\u2212 1,1] nmax z [\u2212 1,1] n, and the losses are all the same [p]: \"(z, hi) \u2264\" i \"(z, g) = min g\" (\u2212 1,1] nmax z [\u2212 1,1] n, namely [p]: 1nz > (hi) b'i \"(z, g) (17) The matrix F and the slip function of (1) are therefore redefined: F'ij\" ij \"n: = = (hi (xj)."}, {"heading": "3.2 Beating the Best Classifier and the Best Weighted Majority", "text": "In other words, our predictor always has the option of simply selecting the best single classifier i \u0445 [p] and guaranteeing its loss-related \"i.\" Consequently, the loss of our predictor is always at most that of a single classifier, as demonstrated by the following observation. Proposal 9. V \"\u2264\" i for each classifier i [p] and each loss. \"Although the proposal is evident from the fact that we minimize the loss compared to the predicted value, we provide brief proof (in Section 4) by using the definitions of this section to better illuminate how they match. Since the optimization of the slip function involves searching for all points and losses, our algorithm automatically allows that the loss of the majority is limited in the worst case, even with weighted majorities."}, {"heading": "4 Supporting Results and Proofs", "text": "Lemma 11, where (a) the Minimax Theorem (SF12), p.144) the Minimax Theorem Rule (SF12) is applied. (a) Lemma 11, where (a) the Minimax Rule is applied. (c) Lemma 11, where (a) the Minimax Rule is applied. (c) Lemma 11, where (c) the Minimax Rule is applied. (c) Lemma 11, where (c) the Minimax Rule is applied. (c) Lemma 11, where (c) the Minimax Rule is applied. (c) Lemma 11, where (c) the Minimax Rule is applied. (c) Lemma 11, where (c) the Minimax Rule is applied. (c) Lemma 11, where (c) the Minimax Rule is applied. (c) Lemma 11."}], "references": [{"title": "Optimally combining classifiers using unlabeled data", "author": ["Akshay Balsubramani", "Yoav Freund"], "venue": "In Conference on Learning Theory,", "citeRegEx": "Balsubramani and Freund.,? \\Q2015\\E", "shortCiteRegEx": "Balsubramani and Freund.", "year": 2015}, {"title": "Scalable semi-supervised classifier aggregation", "author": ["Akshay Balsubramani", "Yoav Freund"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Balsubramani and Freund.,? \\Q2015\\E", "shortCiteRegEx": "Balsubramani and Freund.", "year": 2015}, {"title": "Convexity, classification, and risk bounds", "author": ["Peter L Bartlett", "Michael I Jordan", "Jon D McAuliffe"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Bartlett et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2006}, {"title": "Machine learning techniques?reductions between prediction quality metrics", "author": ["Alina Beygelzimer", "John Langford", "Bianca Zadrozny"], "venue": "In Performance Modeling and Engineering,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2008}, {"title": "Loss functions for binary class probability estimation and classification: Structure and applications", "author": ["Andreas Buja", "Werner Stuetzle", "Yi Shen"], "venue": null, "citeRegEx": "Buja et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Buja et al\\.", "year": 2005}, {"title": "Composite binary losses", "author": ["Mark D Reid", "Robert C Williamson"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Reid and Williamson.,? \\Q2010\\E", "shortCiteRegEx": "Reid and Williamson.", "year": 2010}, {"title": "Boosting: Foundations and Algorithms", "author": ["Robert E. Schapire", "Yoav Freund"], "venue": null, "citeRegEx": "Schapire and Freund.,? \\Q2012\\E", "shortCiteRegEx": "Schapire and Freund.", "year": 2012}, {"title": "Admissible probability measurement procedures", "author": ["Emir H Shuford Jr.", "Arthur Albert", "H Edward Massengill"], "venue": null, "citeRegEx": "Jr et al\\.,? \\Q1966\\E", "shortCiteRegEx": "Jr et al\\.", "year": 1966}, {"title": "Statistical behavior and consistency of classification methods based on convex risk minimization", "author": ["Tong Zhang"], "venue": "Annals of Statistics,", "citeRegEx": "Zhang.,? \\Q2004\\E", "shortCiteRegEx": "Zhang.", "year": 2004}], "referenceMentions": [], "year": 2017, "abstractText": "We develop a worst-case analysis of aggregation of binary classifier ensembles in a transductive setting, for a broad class of losses including but not limited to all convex surrogates. The result is a family of parameter-free ensemble aggregation algorithms, which are as efficient as linear learning and prediction for convex risk minimization but work without any relaxations whatsoever on many nonconvex losses like the 0-1 loss. The prediction algorithms take a familiar form, applying \u201clink functions\" to a generalized notion of ensemble margin, but without the assumptions typically made in margin-based learning \u2013 all this structure follows from a minimax interpretation of loss minimization.", "creator": "LaTeX with hyperref package"}}}