{"id": "1510.00756", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Oct-2015", "title": "Rapidly Mixing Gibbs Sampling for a Class of Factor Graphs Using Hierarchy Width", "abstract": "Gibbs sampling on factor graphs is a widely used inference technique, which often produces good empirical results. Theoretical guarantees for its performance are weak: even for tree structured graphs, the mixing time of Gibbs may be exponential in the number of variables. To help understand the behavior of Gibbs sampling, we introduce a new (hyper)graph property, called hierarchy width. We show that under suitable conditions on the weights, bounded hierarchy width ensures polynomial mixing time. Our study of hierarchy width is in part motivated by a class of factor graph templates, hierarchical templates, which have bounded hierarchy width---regardless of the data used to instantiate them. We demonstrate a rich application from natural language processing in which Gibbs sampling provably mixes rapidly and achieves accuracy that exceeds human volunteers.", "histories": [["v1", "Fri, 2 Oct 2015 23:14:05 GMT  (66kb)", "http://arxiv.org/abs/1510.00756v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["christopher de sa", "ce zhang", "kunle olukotun", "christopher r\u00e9"], "accepted": true, "id": "1510.00756"}, "pdf": {"name": "1510.00756.pdf", "metadata": {"source": "CRF", "title": "Rapidly Mixing Gibbs Sampling for a Class of Factor Graphs Using Hierarchy Width", "authors": ["Christopher De Sa"], "emails": ["cdesa@stanford.edu,", "czhang@cs.wisc.edu,", "kunle@stanford.edu,", "chrismre@stanford.edu"], "sections": [{"heading": null, "text": "ar Xiv: 151 0.00 756v 1 [cs.L G] 2O ct"}, {"heading": "1 Introduction", "text": "We study conclusions on factor graphs using Gibbs sampling, the de facto Markov Chain Monte Carlo (MCMC) method [8, p. 505]. Specifically, our goal is to quickly calculate the marginal distribution of some query variables using Gibbs sampling, since there is evidence of some other variables and a number of factor weights. We focus on the case where all variables are discrete. In this situation, a Gibbs sampler will randomly update a single variable at each iteration by sampling from its conditional distribution, since the values of all other variables are specified in the model. Many systems - such as Factorie [14], OpenBugs [12], PGibbs [5], DimmWitted [28], and others [15, 22] - use Gibbs sampling for conclusions because it is quick to implement, and often produces high-quality results."}, {"heading": "1.1 Related Work", "text": "The variable elimination algorithm [8] is an exact inference method that runs in polynomial time for diagrams with limited hypertree width [7]. Upscale inference [18] is a way to exploit the structural symmetry of factor graphs instantiated from a template; there are upscale versions of many common algorithms, such as variable elimination [16], belief propagation [21], and Gibbs sampling [26]. It is also possible to use a template for quick compilation: Venugopal et al. [27] reach orders of magnitude of Gibbs sampling speed on MLNs."}, {"heading": "2 Main Result", "text": "In this section, we will describe our most important contribution. We will analyze some simple sample graphics and use them to show that the limited width of a hypertree is not sufficient to guarantee fast mixing of the Gibbs scan. Intuitively, we will define the property of the hierarchical width diagram and prove that Gibbs scan mixes in polynomial time for diagrams with limited hierarchical width. First, we will give some basic definitions. A factor diagram G is a graphical model that consists of a series of variables V and factors and determines a distribution over these variables. (1) If I am a world for G (an assignment of a value to each variable in V), then the energy of the world is defined as an assignment of (I) to (I). (1) The probability of world I is reversed (I) = 1 Z exp (I), where Z is the normalization constant necessary for this distribution."}, {"heading": "2.1 Voting Example", "text": "We begin by considering a simple sample model [20], called a tuning model, which is the character of a particular \"Q\" variable Q = two \"Q\" factors = two \"Q\" variables Q = two \"Q\" variables Q = 1 \"Q\" variables Q = 1 \"Q\" variables Q = 1 \"Q\" variables Q = 1 \"Q\" variables Q = 1 \"Q\" variable Q = 1 \"Q\" variable. \"We consider three versions of this model possible: the first, the tuning model with linear semantics, has energy functions (Q, T) = wQ\" tics \"n\" tics \": 1\" tics \"tics\" n \"tics\": 1 \"tics\" s \"i\": 1 \"Fixi\" n \"i = 1 wFiFi,\" where the tuning model with linear semantics, and w > 0 are constant weights. This model has a factor that connects each choice variable with the query that represents the value of this tuning, and gives an additional factor."}, {"heading": "2.2 Hypertree Width", "text": "In this section, we will describe the commonly used graph property of the hypertree width and show from the example that its limitation is not sufficient to ensure fast Gibbs sampling. Hypertree width is typically used to limit the complexity of dynamic programming algorithms on a graph; in particular, the elimination of variables for exact inferences in polynomial time on factor diagrams with limited hypertree width [8, p. 1000]. The hypertree width of a hypergraph, which we call tw (G), is a generalization of the term acicity; since the definition of the hypertree width is technical, we will instead give the definition of an acyclic hypergraph that is sufficient for our analysis. To apply these terms to factor graphs, we can present a factor graph as a hyper graph that has a hyperedge containing a factor of each one, and a peredge containing a factor of each one."}, {"heading": "2.3 Hierarchy Width", "text": "Since the hypertree width is insufficient, we define a new graph property in the hierarchy hierarchy, which, if limited, ensures fast mixing of Gibbs sampling, which is our main effect factor. (G) The hierarchy width hw (G) of a factor chart G is recursively defined so that each linked factor chart G = < V) and for each linked factor chart G = < V (G) and for each linked factor chart G with associated components G2,.., hw (G) = max i hw (Gi) As a base case, all factor charts G are without factors havehw (< V) = 0. (4) To develop some intuitions about the definition of hierarchy width, we derive the hierarchy of the drawn hierarchy."}, {"heading": "3 Factor Graph Templates", "text": "There are many ways to define templates; here we follow the formulation in Koller and Friedman, each of which is a thing we want to think about, divided into classes.) A factor diagram template, for example, is an abstract model that could be instantiated on a dataset to create a factor graph. (There are many ways to define templates; here we follow the formulation in Koller and Friedman [8, p. 213]. A factor diagram template consists of a series of template variables and template factors. A template variable represents a property of a tuple of zero or more objects of certain classes. For example, we could have an IsPopular template that takes up a single argument of the class Movie."}, {"heading": "3.1 Hierarchical Factor Graphs", "text": "In this section, we will outline a class of templates, hierarchical templates that have hierarchical hierarchical factors. We will focus on models that have hierarchical structures in their template factors; for example, \u03c6 (A (x, y, z), B (x, y, c), Q (x, y))) (7) would have to have a hierarchical structure, while \u03c6 (A (z), B (x, c), Q (x, y))) (8) would not. Armed with this intuition, we will specify the following definitions. Definition 5 (Hierarchy Definition): A template factor \u03c6 has hierarchy depth d if the first d object symbols appearing in each of its terms are the same. We will refer to these symbols as hierarchical symbols."}, {"heading": "4 Experiments", "text": "Synthetic data We constructed a synthetic dataset using an ensemble of Ising model diagrams, each with 360 nodes, 359 edges, and tree width 1, but with different hierarchy widths. These diagrams ranged from the star diagram (as in Figure 1 (a)) to the path diagram; and each had a different hierarchy width. For each diagram, we were able to calculate the exact marginal size of each variable due to the small tree width. We then selected Gibbs samples for each diagram and calculated the error in estimating the boundary of a single randomly selected query variable. Figure 5 (a) shows the result with different weights and hierarchy widths. It shows that even for tree diagrams with the same number of nodes and edges, the mixing time can be variable depending on the hierarchy width of the model."}, {"heading": "5 Conclusion", "text": "This paper showed that for a class of factor chart templates, hierarchical templates, Gibbs samples mix in polynomial time. It also introduced the hierarchical width of graph properties and showed that for charts with limited factor weight and hierarchical width Gibbs samples converge quickly. These results can contribute to a better understanding of the behavior of Gibbs samples for both template and general factor diagrams."}, {"heading": "Acknowledgments", "text": "Thanks to Stefano Ermon and Percy Liang for helpful conversations. The authors thank for the support of: DARPA FA8750-12-2-0335; NSF IIS-1247701; NSF CCF-1111943; DOE 108845; NSF CCF1337375; DARPA FA8750-13-2-0039; NSF IIS-1353606; ONR N000141210041 and N000141310129; NIH U54EB020405; Oracle; NVIDIA; Huawei; SAP Labs; Sloan Research Fellowship; Moore Foundation; American Family Insurance; Google and Toshiba."}, {"heading": "A Proof of Voting Program Rates", "text": "The strategy for the uppermost distribution category is to construct a coupling between the Gibbs samplers and another process that achieves the equilibrium distribution in each step. Total variation distance [10, p. 48] is a distance metric between two probability measures and a probability space, which we will use in the proofs in this section. Total variation distance [10, p. 48] is a distance between two probability measures and a probability space, which is defined as a quantity.The mixing time of a Markov chain is the first time in which the estimated distribution within the total distribution distance [10, p. 55] The maximum difference between the probabilities we have to a single event.Definition 8 (mixing time) is the first time in which the estimated distribution within the actual distribution distance [10, p. 55]."}, {"heading": "B Proof of Theorem 2", "text": "In this section we will prove the main result of the work, theorem 2. First, we will specify some basic factors that we need for the proof in section B.1. Then we will prove the main result in section B.2. Finally, we will prove the transition matrix of a Markov process. Since it is a Markov process, some of these lemmas will have to be redefined from the body of the Markov process. The absolute spectral gap of the Markov process is the value gap where the maximum over all non-dominant eigenvalues of P.Lemma 5 (Disconnected Case) is. Let us leave a disconnected factor with n variables and m connected components G1, G2."}, {"heading": "C Proofs of Other Results", "text": "In this section, we will prove the other results on hierarchy and hierarchy set forth in Section 2.3. (First, we express them in relation to a hypergraphy that has a hyperstructure; this is the way the hyperstructure of T is defined, so it is useful to compare the definition 11 (hierarchy decomposition) that meets the following conditions: (1) for each edge e E, there are some nodes v of T, so that there is a linkage of E and V of T; (2) if for two nodes u and v of T and for some edges e of G, which meet the following conditions: (1) for each edge e E, there are some nodes v of T, so that there is a hyperposition of E; (2) if for two nodes u and v of T and for some edges e of G, both e-hierarchy (u) and e-hierarchy (v), then for each node v, there is a unique pair w and v (T) between the pair w and T (T)."}, {"heading": "D Proofs of Factor Graph Template Results", "text": "In this section, we will prove the results in Section 3.1. First, we will prove Lemma 16.Lemma 16. If G is an instance of a hierarchical factor diagram template G with E template factors, then hw (G) \u2264 E.Lemma 16. If G is an instance of a hierarchical factor diagram template G with E template factors, then hw (G) \u2264 E.Proof. We will prove this result with the term hierarchy decomposition from the previous section. For the instantiated factor graph G, let T be a tree where each node v of T is associated with a specific mapping of the first n-header symbols of the rules. That is, there is a node of the hierarchy decomposition of object symbols (x1,.. xm) - even for m = 0. (Since the rules are hierarchical and therefore the symbols must be contained in the same order.) These mapping are well defined."}], "references": [{"title": "Complexity of inference in graphical models", "author": ["Venkat Chandrasekaran", "Nathan Srebro", "Prahladh Harsha"], "venue": "arXiv preprint arXiv:1206.3240,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Gibbs sampling, exponential families and orthogonal polynomials", "author": ["Persi Diaconis", "Kshitij Khare", "Laurent Saloff-Coste"], "venue": "Statist. Sci.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Gibbs sampling, conjugate priors and coupling", "author": ["Persi Diaconis", "Kshitij Khare", "Laurent Saloff-Coste"], "venue": "Sankhya A,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "A tractable first-order probabilistic logic", "author": ["Pedro Domingos", "William Austin Webb"], "venue": "In AAAI,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Parallel gibbs sampling: From colored fields to thin junction trees", "author": ["Joseph Gonzalez", "Yucheng Low", "Arthur Gretton", "Carlos Guestrin"], "venue": "In AISTATS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Treewidth and hypertree width. Tractability: Practical Approaches to Hard Problems, page", "author": ["Georg Gottlob", "Gianluigi Greco", "Francesco Scarcello"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Loopy belief propagation: Convergence and effects of message errors", "author": ["Alexander T Ihler", "John Iii", "Alan S Willsky"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["Daphne Koller", "Nir Friedman"], "venue": "MIT press,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "The necessity of bounded treewidth for efficient inference in bayesian networks", "author": ["Johan Kwisthout", "Hans L Bodlaender", "Linda C van der Gaag"], "venue": "In ECAI,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Markov chains and mixing times", "author": ["David Asher Levin", "Yuval Peres", "Elizabeth Lee Wilmer"], "venue": "American Mathematical Soc.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Projecting markov random field parameters for fast mixing", "author": ["Xianghang Liu", "Justin Domke"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "The BUGS project: evolution, critique and future directions", "author": ["David Lunn", "David Spiegelhalter", "Andrew Thomas", "Nicky Best"], "venue": "Statistics in medicine,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Tractable hypergraph properties for constraint satisfaction and conjunctive queries", "author": ["D\u00e1niel Marx"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Factorie: Probabilistic programming via imperatively defined factor graphs", "author": ["Andrew McCallum", "Karl Schultz", "Sameer Singh"], "venue": "In NIPS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Distributed inference for latent dirichlet allocation", "author": ["David Newman", "Padhraic Smyth", "Max Welling", "Arthur U Asuncion"], "venue": "In NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Probabilistic modelling, inference and learning using logical theories", "author": ["Kee Siong Ng", "John W Lloyd", "William TB Uther"], "venue": "Annals of Mathematics and Artificial Intelligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "A machine reading system for assembling synthetic Paleontological databases", "author": ["Shanan E Peters", "Ce Zhang", "Miron Livny", "Christopher R\u00e9"], "venue": "PloS ONE,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "First-order probabilistic inference", "author": ["David Poole"], "venue": "In IJCAI,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "Graph minors. ii. algorithmic aspects of tree-width", "author": ["Neil Robertson", "Paul D. Seymour"], "venue": "Journal of algorithms,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1986}, {"title": "Incremental knowledge base construction using deepdive", "author": ["Jaeho Shin", "Sen Wu", "Feiran Wang", "Christopher De Sa", "Ce Zhang", "Christopher R\u00e9"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Lifted first-order belief propagation", "author": ["Parag Singla", "Pedro Domingos"], "venue": "In AAAI,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "An architecture for parallel topic models", "author": ["Alexander Smola", "Shravan Narayanamurthy"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Probabilistic databases", "author": ["Dan Suciu", "Dan Olteanu", "Christopher R\u00e9", "Christoph Koch"], "venue": "Synthesis Lectures on Data Management,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Overview of the english slot filling track at the TAC2014 knowledge base population evaluation", "author": ["Mihai Surdeanu", "Heng Ji"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Training sparse natural image models with a fast gibbs sampler of an extended state space", "author": ["Lucas Theis", "Jascha Sohl-dickstein", "Matthias Bethge"], "venue": "In NIPS,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "On lifting the gibbs sampling algorithm", "author": ["Deepak Venugopal", "Vibhav Gogate"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Just count the satisfied groundings: Scalable local-search and sampling based inference in mlns", "author": ["Deepak Venugopal", "Somdeb Sarkhel", "Vibhav Gogate"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Sparsity: graphs, structures, and algorithms", "author": ["Patrice Ossona de Mendez"], "venue": "Springer Science & Business Media,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Probability Theory: The Coupling Method", "author": ["Frank den Hollander"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}], "referenceMentions": [{"referenceID": 13, "context": "Many systems\u2014 such as Factorie [14], OpenBugs [12], PGibbs [5], DimmWitted [28], and others [15, 22, 25]\u2014use Gibbs sampling for inference because it is fast to run, simple to implement, and often produces high quality empirical results.", "startOffset": 31, "endOffset": 35}, {"referenceID": 11, "context": "Many systems\u2014 such as Factorie [14], OpenBugs [12], PGibbs [5], DimmWitted [28], and others [15, 22, 25]\u2014use Gibbs sampling for inference because it is fast to run, simple to implement, and often produces high quality empirical results.", "startOffset": 46, "endOffset": 50}, {"referenceID": 4, "context": "Many systems\u2014 such as Factorie [14], OpenBugs [12], PGibbs [5], DimmWitted [28], and others [15, 22, 25]\u2014use Gibbs sampling for inference because it is fast to run, simple to implement, and often produces high quality empirical results.", "startOffset": 59, "endOffset": 62}, {"referenceID": 14, "context": "Many systems\u2014 such as Factorie [14], OpenBugs [12], PGibbs [5], DimmWitted [28], and others [15, 22, 25]\u2014use Gibbs sampling for inference because it is fast to run, simple to implement, and often produces high quality empirical results.", "startOffset": 92, "endOffset": 104}, {"referenceID": 21, "context": "Many systems\u2014 such as Factorie [14], OpenBugs [12], PGibbs [5], DimmWitted [28], and others [15, 22, 25]\u2014use Gibbs sampling for inference because it is fast to run, simple to implement, and often produces high quality empirical results.", "startOffset": 92, "endOffset": 104}, {"referenceID": 24, "context": "Many systems\u2014 such as Factorie [14], OpenBugs [12], PGibbs [5], DimmWitted [28], and others [15, 22, 25]\u2014use Gibbs sampling for inference because it is fast to run, simple to implement, and often produces high quality empirical results.", "startOffset": 92, "endOffset": 104}, {"referenceID": 10, "context": "Recent work has outlined conditions under which Gibbs sampling of Markov Random Fields mixes rapidly [11].", "startOffset": 101, "endOffset": 105}, {"referenceID": 1, "context": "Continuous-valued Gibbs sampling over models with exponential-family distributions is also known to mix rapidly [2, 3].", "startOffset": 112, "endOffset": 118}, {"referenceID": 2, "context": "Continuous-valued Gibbs sampling over models with exponential-family distributions is also known to mix rapidly [2, 3].", "startOffset": 112, "endOffset": 118}, {"referenceID": 23, "context": "Each of these celebrated results still leaves a gap: there are many classes of factor graphs on which Gibbs sampling seems to work very well\u2014including as part of systems that have won quality competitions [24]\u2014for which there are no theoretical guarantees of rapid mixing.", "startOffset": 205, "endOffset": 209}, {"referenceID": 0, "context": "In some sense, bounded hypertree width is a necessary and sufficient condition for tractability of inference in graphical models [1, 9].", "startOffset": 129, "endOffset": 135}, {"referenceID": 8, "context": "In some sense, bounded hypertree width is a necessary and sufficient condition for tractability of inference in graphical models [1, 9].", "startOffset": 129, "endOffset": 135}, {"referenceID": 13, "context": "Many state-of-the-art systems use Gibbs sampling on factor graph templates and achieve better results than competitors using other algorithms [14, 27].", "startOffset": 142, "endOffset": 150}, {"referenceID": 26, "context": "Many state-of-the-art systems use Gibbs sampling on factor graph templates and achieve better results than competitors using other algorithms [14, 27].", "startOffset": 142, "endOffset": 150}, {"referenceID": 3, "context": "This is a kind of sampling analog to tractable Markov logic [4] or so-called \u201csafe plans\u201d in probabilistic databases [23].", "startOffset": 60, "endOffset": 63}, {"referenceID": 22, "context": "This is a kind of sampling analog to tractable Markov logic [4] or so-called \u201csafe plans\u201d in probabilistic databases [23].", "startOffset": 117, "endOffset": 121}, {"referenceID": 7, "context": "The variable elimination algorithm [8] is an exact inference method that runs in polynomial time for graphs of bounded hypertree width.", "startOffset": 35, "endOffset": 38}, {"referenceID": 6, "context": "Belief propagation is another widely-used inference algorithm that produces an exact result for trees and, although it does not converge in all cases, converges to a good approximation under known conditions [7].", "startOffset": 208, "endOffset": 211}, {"referenceID": 17, "context": "Lifted inference [18] is one way to take advantage of the structural symmetry of factor graphs that are instantiated from a template; there are lifted versions of many common algorithms, such as variable elimination [16], belief propagation [21], and Gibbs sampling [26].", "startOffset": 17, "endOffset": 21}, {"referenceID": 15, "context": "Lifted inference [18] is one way to take advantage of the structural symmetry of factor graphs that are instantiated from a template; there are lifted versions of many common algorithms, such as variable elimination [16], belief propagation [21], and Gibbs sampling [26].", "startOffset": 216, "endOffset": 220}, {"referenceID": 20, "context": "Lifted inference [18] is one way to take advantage of the structural symmetry of factor graphs that are instantiated from a template; there are lifted versions of many common algorithms, such as variable elimination [16], belief propagation [21], and Gibbs sampling [26].", "startOffset": 241, "endOffset": 245}, {"referenceID": 25, "context": "Lifted inference [18] is one way to take advantage of the structural symmetry of factor graphs that are instantiated from a template; there are lifted versions of many common algorithms, such as variable elimination [16], belief propagation [21], and Gibbs sampling [26].", "startOffset": 266, "endOffset": 270}, {"referenceID": 26, "context": "[27] achieve orders of magnitude of speedup of Gibbs sampling on MLNs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Many algorithms are known to run in polynomial time on graphs of bounded treewidth [19], despite being otherwise NP-hard.", "startOffset": 83, "endOffset": 87}, {"referenceID": 12, "context": "Sometimes, using a stronger or weaker property than treewidth will produce a better result; for example, the submodular width used for constraint satisfaction problems [13].", "startOffset": 168, "endOffset": 172}, {"referenceID": 19, "context": "1 Voting Example We start by considering a simple example model [20], called the voting model, that models the sign of a particular \u201cquery\u201d variable Q \u2208 {\u22121, 1} in the presence of other \u201cvoter\u201d variables Ti \u2208 {0, 1} and Fi \u2208 {0, 1}, for i \u2208 {1, .", "startOffset": 64, "endOffset": 68}, {"referenceID": 5, "context": "Definition 2 (Acyclic Factor Graph [6]).", "startOffset": 35, "endOffset": 38}, {"referenceID": 19, "context": "[20] introduce the notion of a semantic function g, which counts the of energy of instances of the factor template in a non-standard way.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "We focus on three semantic functions in particular [20].", "startOffset": 51, "endOffset": 55}, {"referenceID": 19, "context": "[20] exhibit several classification problems where using logical or ratio semantics gives better F1 scores.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] contain subgraphs that are grounded by hierarchical templates.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "On the same task, this quality is actually higher than professional human volunteers [17].", "startOffset": 85, "endOffset": 89}, {"referenceID": 19, "context": "[20] that can be grounded by a hierarchical template and chose a setting of the weight such that the true marginal was 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "References [1] Venkat Chandrasekaran, Nathan Srebro, and Prahladh Harsha.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Persi Diaconis, Kshitij Khare, and Laurent Saloff-Coste.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Persi Diaconis, Kshitij Khare, and Laurent Saloff-Coste.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Pedro Domingos and William Austin Webb.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Joseph Gonzalez, Yucheng Low, Arthur Gretton, and Carlos Guestrin.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Georg Gottlob, Gianluigi Greco, and Francesco Scarcello.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Alexander T Ihler, John Iii, and Alan S Willsky.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Daphne Koller and Nir Friedman.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Johan Kwisthout, Hans L Bodlaender, and Linda C van der Gaag.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] David Asher Levin, Yuval Peres, and Elizabeth Lee Wilmer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Xianghang Liu and Justin Domke.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] David Lunn, David Spiegelhalter, Andrew Thomas, and Nicky Best.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] D\u00e1niel Marx.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Andrew McCallum, Karl Schultz, and Sameer Singh.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] David Newman, Padhraic Smyth, Max Welling, and Arthur U Asuncion.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Kee Siong Ng, John W Lloyd, and William TB Uther.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Shanan E Peters, Ce Zhang, Miron Livny, and Christopher R\u00e9.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] David Poole.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Neil Robertson and Paul D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Jaeho Shin, Sen Wu, Feiran Wang, Christopher De Sa, Ce Zhang, Feiran Wang, and Christopher R\u00e9.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] Parag Singla and Pedro Domingos.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] Alexander Smola and Shravan Narayanamurthy.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Dan Suciu, Dan Olteanu, Christopher R\u00e9, and Christoph Koch.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] Mihai Surdeanu and Heng Ji.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] Lucas Theis, Jascha Sohl-dickstein, and Matthias Bethge.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] Deepak Venugopal and Vibhav Gogate.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] Deepak Venugopal, Somdeb Sarkhel, and Vibhav Gogate.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "Next, we define a coupling [30].", "startOffset": 27, "endOffset": 31}, {"referenceID": 9, "context": "[10]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Definition 12 (Hypertree Decomposition [6]).", "startOffset": 39, "endOffset": 42}, {"referenceID": 27, "context": "Secondary Literature [29] Patrice Ossona de Mendez et al.", "startOffset": 21, "endOffset": 25}, {"referenceID": 28, "context": "[30] Frank den Hollander.", "startOffset": 0, "endOffset": 4}], "year": 2015, "abstractText": "Gibbs sampling on factor graphs is a widely used inference technique, which often produces good empirical results. Theoretical guarantees for its performance are weak: even for tree structured graphs, the mixing time of Gibbs may be exponential in the number of variables. To help understand the behavior of Gibbs sampling, we introduce a new (hyper)graph property, called hierarchy width. We show that under suitable conditions on the weights, bounded hierarchy width ensures polynomial mixing time. Our study of hierarchy width is in part motivated by a class of factor graph templates, hierarchical templates, which have bounded hierarchy width\u2014regardless of the data used to instantiate them. We demonstrate a rich application from natural language processing in which Gibbs sampling provably mixes rapidly and achieves accuracy that exceeds human volunteers.", "creator": "gnuplot 4.6 patchlevel 5"}}}