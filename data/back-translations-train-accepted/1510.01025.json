{"id": "1510.01025", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Oct-2015", "title": "Quadratic Optimization with Orthogonality Constraints: Explicit Lojasiewicz Exponent and Linear Convergence of Line-Search Methods", "abstract": "A fundamental class of matrix optimization problems that arise in many areas of science and engineering is that of quadratic optimization with orthogonality constraints. Such problems can be solved using line-search methods on the Stiefel manifold, which are known to converge globally under mild conditions. To determine the convergence rate of these methods, we give an explicit estimate of the exponent in a Lojasiewicz inequality for the (non-convex) set of critical points of the aforementioned class of problems. By combining such an estimate with known arguments, we are able to establish the linear convergence of a large class of line-search methods. A key step in our proof is to establish a local error bound for the set of critical points, which may be of independent interest.", "histories": [["v1", "Mon, 5 Oct 2015 04:14:22 GMT  (122kb)", "http://arxiv.org/abs/1510.01025v1", null]], "reviews": [], "SUBJECTS": "math.OC cs.LG cs.NA math.NA", "authors": ["huikang liu", "weijie wu", "anthony man-cho so"], "accepted": true, "id": "1510.01025"}, "pdf": {"name": "1510.01025.pdf", "metadata": {"source": "CRF", "title": "Quadratic Optimization with Orthogonality Constraints: Explicit Lojasiewicz Exponent and Linear Convergence of Line-Search Methods", "authors": ["Huikang Liu", "Weijie Wu", "Anthony Man\u2013Cho"], "emails": ["hkliu@se.cuhk.edu.hk", "wwu@se.cuhk.edu.hk", "manchoso@se.cuhk.edu.hk"], "sections": [{"heading": null, "text": "ar Xiv: 151 0.01 025v 1 [mat h.O C"}, {"heading": "1 Introduction", "text": "It is an important class of matrix optimization problems that have found applications in areas such as combinatorial optimization, data mining, dynamic systems, multivariate statistical analysis, and signal processing, to name but a few (see, for example, [6, 13, 3, 8, 10, 16, 21, 25]). A prototype form of such problems is not applied. (1), where St (m), n), n), where St (n) = {S, X, Rm, n, XTX = In} (with m, n, and n identity matrix.) is the compact Boot manifolds, and A, B, B, Sn are given symmetrical matrices. Despite their simplicity, Problem (1) already has many applications, the best known of which is the department of systems engineering and engineering."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 First-Order Optimality Condition and Descent Directions", "text": "s metric < \u00b7, \u00b7 > given by < X, Y > = tr (XTY). The gradient of F (X) = tr (XTAXB) is for St (X) = 2AXB, and its orthogonal projection on T (X) = {Y (X) + Y TX = 0}. The gradient of F (X) = tr (XTAXB) = 2AXB, and its orthogonal projection on T (X) = (X) T (X) + 12 X (F (XT) = i."}, {"heading": "2.2 Retraction", "text": "Another ingredient in the line search method for optimizing St (m, n) is a retreat: Definition 1 (retreat) A map R: X St (m, n) {X} \u00b7 T (X) \u2192 St (m, n) is called a retreat if for each fixed X St (m, n) and each fixed X T (X) it is assumed that \"7 \u2192 R (X, n) is continuous on T (X), and for all X St (m, n), lim T (X) 0 R (X, n) - (X +) - (X +) - (F) - 0.\" (3) Various smooth retractions on the boot manifold have been suggested in the literature, including polar retraction, QR retraction, QR transformation, and Riemannian exponential mapping."}, {"heading": "2.3 Step Sizes", "text": "To complete the specification of a line search method, we still need to select the step sizes as follows: Definition 2 (Armijo point) Let \u03b3 > 0, \u03b2, c (0, 1) Constants be given. The number \u03b1 = max {\u03b2n\u03b3 | n \u2265 0, F (R (X, \u2212 \u03b2n\u03b3D\u03c1 (X)))) \u2212 F (X) \u2264 \u2212 c\u03b2 n\u03b3 (X) TD\u03c1 (X)} (4) is referred to as an Armijo point at X-St (m, n) with parameters (\u03b3, \u03b2, c). Since the smooth retraction (3) is an approximation of the first order, the left side approaches the derivative of the first order \u2212 \u03b2n\u03b3D\u03c1 if m is large enough. Consequently, the Armijo point exists. We refer the reader to [17] for details.We summarize the line search method in algorithm 1, algorithm 1 of the Linic search for the method of the \u03b2n\u03b3D\u03c1 (X)."}, {"heading": "2.4 Convergence Analysis Framework for the Line-Search Method", "text": "To analyze the convergence properties of Algorithmus 1, we assume the framework for the convergence of algorithm 1 presented in [17]. It has been shown in [17, Corollary 2.9] that algorithm 1 has the following properties: \u2022 (primary ramp) There is a constant large enough for all k - F (Xk + 1) \u2212 F (Xk + 1) F (Xk) F (Xk) F (Xk) X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X"}, {"heading": "3 Proof of Theorem 2", "text": "We now prove Theorem 2, which is the main result of this work. Proof can be divided into four steps."}, {"heading": "3.1 Preliminary Observations", "text": "It is easy to verify that tr (XTAXB) = tr (X-TAXB), where X-TAXUB-St (m, n). Consequently, without loss of generality we can assume that A = Diag (a1,.., am), X-S m and B = Diag (b1,.., bn), where a1 \u2265 a2 \u2265 \u00b7 \u00b7 \u2265 am and b1 \u2265 b2 \u2265 \u00b7 \u00ba bn. With Proposition 1 we can writeX = {X-St (m, n) | AXB \u2212 XBXTAX = 0}. (9) Now it can be confirmed that there are such a2 \u2265 \u00b7 \u00b7 \u00b7 and b1 \u2265 b2 \u2265 \u00b7 \u00b7 \u00b7 \u00b7 b2 \u2264 \u2212 b."}, {"heading": "3.2 Characterizing the Set of Critical Points when B has Full Rank", "text": "In the second half of the last decade, when the US and the EU invaded the EU, the EU Commission put the EU Commission in its place. () The EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU, the EU Commission, the EU, the EU Commission, the EU, the EU Commission, the EU, the EU Commission, the EU, the EU Commission, the EU Commission, the EU, the EU Commission, the EU, the EU Commission, the EU, the Commission, the EU, the EU, the EU Commission, the Commission, the EU, the EU, the EU Commission, the EU, the Commission, the Commission, the EU, the EU, the EU Commission, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the Commission, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the Commission, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the Commission, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the Commission, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the Commission, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the Commission, the EU, the EU, the EU, the EU, the Commission, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the"}, {"heading": "3.3 Estimating the Distance to the Set of Critical Points", "text": "Let X-j (m, n) and h-j (h1,.., PnA) \u00b7 E (h) \u00b7 BlkDiag (Q1,.., QnB) \u00b7 E (Q1,.., Pi-si-si-si-si-si-si-si for i = 1,.., nA; Qj-tj \u2212 1 for j = 1,.., nB). (12) Let (P-1,...., P-nA, Q-nB) \u00b7 E (X-tj \u2212 1 for j = 1,.., nB). (12) Let (P-1,.., nA,.,........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "3.4 Removing the Full Rank Assumption on B", "text": "s consider the case where B does not have the full rank. Without loss of generality, we assume that B = BlkDiag (B, 0), where B = Diag (b1,.., bp), B = BlkDiag (B, 0), B = Diag (b1,..., bp), Sp (n \u2212 p), AX1B = dist (X, X), X = {X = [X1 X2], St (m, n), X1, R, p, X2, R, m (n \u2212 p), AX1B, X \u2212 X1B, X1 AX1, X \u00b2), where X = Xp X2], St (m, p), p), St (m, n), X1, X1, X1 \u2212 B, X1, 1st B, 1st B, \u2212 B, Ast, \u2212 B, Ast, \u2212 B, Ast (X, X \u2212 P), X = dist (X1, X), X (XB), 1st B, 1st B, 1st B, 1st B, 1st B, 1st B, \u2212 B, \u2212 B, \u2212 B, \u2212 B (1), \u2212 B, \u2212 B, Xst, XB, \u2212 B (1, \u2212 B), XB, \u2212 B (1, \u2212 B, \u2212 B, XB, \u2212 B, \u2212 B, \u2212 B, \u2212 B, \u2212 B (1, \u2212 B, \u2212 B, \u2212 B, \u2212 B, \u2212 B = 1, \u2212 B, \u2212 B, \u2212 B, \u2212 B, \u2212 B, \u2212 B, \u2212 B, \u2212 B, \u2212 B, \u2212 B, \u2212 B, \u2212 B, X1, \u2212 B, \u2212 B, \u2212 B, \u2212 B, \u2212 B, \u2212 B, \u2212 B, \u2212 B, X1, (1, \u2212 B, \u2212 B, \u2212 B, \u2212 B, \u2212 B, \u2212 B, \u2212 B, x1, x1, x1, \u2212 B, \u2212 B, xB, xB), x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, 1st,"}, {"heading": "4 Numerical Experiments", "text": "In this section, we perform numerical experiments to examine the convergence rate of the retracted line search algorithm for problem (1) on synthetic datasets. As we will see, the results are consistent with the theoretical analysis in the preceding sections. In particular, we consider the four above-mentioned retractions. First, we generate our diagonal matrices A-Sm and B-Sn, the diagonal elements of which are randomly sampled from the even distribution. We stop the algorithm when F (Xk) \u2212 F (Xk + 1) < 10 \u2212 8. In practical computations, the orthogonal constraint after multiple iterations may be violated. \u2212 The algorithm is stopped when F (Xk) \u2212 F (Xk + 1) < 10 \u2212 8. In figures, the orthogonal constraint after m can be regarded as convergence."}, {"heading": "5 Conclusion", "text": "In this paper, we gave an explicit estimate of the exponent of a Lojasiewicz inequality for the (non-convex) critical points of the problem (1), which was achieved by establishing a local margin of error for the above-mentioned critical points. Together with known arguments, our result implies the linear convergence of a large class of line search methods on the Stiefel manifold. An interesting future direction would be to expand our techniques for analyzing the convergence rates of first-order methods for solving structured non-convex optimization problems."}, {"heading": "A Proof of Proposition 2", "text": "The proof for Proposition 2 is based on the following problem: Lemma 1 The Armijo points {\u03b1k} k = 0 satisfy limk \u00b2 0 \u03b1k \u00b2 D = 0, Proof The Armijo point exists in every step, which guarantees a sufficient reduction. We add the total reduction together and the sum must be finite, since there is a lower limit for the functional value; i.e. all Armijo points have an upper limit. \u2212 Thus, all Armijo points have an upper limit. \u2212 lim \u00b2 D \u00b2 (Xk) \u00b2 limk \u00b2 2 \u00b2 F < + \u221e, which means that lim \u00b2 k \u00b2 k \u00b2 D \u00b2 (Xk) \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s (Xk) s \u00b2 s \u00b2 s \u00b2 s \u00b2 s (Xk) s \u00b2 s \u00b2 s \u00b2 s \u00b2 s (Xk) s \u00b2 s \u00b2 s \u00b2 s (Xk) s \u00b2 s \u00b2 s \u00b2 s \u00b2 s (Xk) s \u00b2 s \u00b2 s \u00b2 s \u00b2 s (Xk) s \u00b2 s \u00b2 s \u00b2 s (Xk) s \u00b2 s \u00b2 s \u00b2 s (Xk) s \u00b2 s \u00b2 s (Xk) s \u00b2 s \u00b2 s \u00b2 s (Xk)"}, {"heading": "B Proof of Proposition 4", "text": "Since both XTAX and B are symmetrical, this means that XTAX and B can be diagonalized simultaneously. In particular, there are orthogonal matrices Qj-tj-1 and diagonal matrices, where j = 1,.., nB, so that the columns of BlkDiag (Q1,.., QnB) are eigenvectors of B, and that XTAX = BlkDiag (QT1-1Q1,..., nB, so that the columns of BlkDiag (Q1,.., QnB) are eigenvectors of B, and that XTAX = BlkDiag (.) n columns of Ossi."}, {"heading": "C Proof of Proposition 5", "text": "With the help of (12) and (13), it can be demonstrated that the following problem (X,..., QnB) has the same value as the problem (X,...,.............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "D Proof of Proposition 6", "text": "Remember that P + J (P + A), Q + BQ (Q + A), B = BlkDiag (Bt1It1 \u2212 t0,.., btnB ItnB \u2212 tnB \u2212 1) and using (13), the calculation of AXB \u2212 XBXTAX \u00b2, B = BlkDiag (Bt1It1 \u2212 t0,.., btnB ItnB \u2212 tnB \u2212 1) and the calculation of AXB \u2212 XBXTAX \u00b2, AXAX \u00b2, B \u2212 AP \u00b2, B \u00b2 \u2212 P \u00b2, Q \u00b2 B \u00b2 B (Q \u00b2), T \u00b2 T \u00b2 T (P \u00b2) TAP \u00b2, X \u00b2 Q \u00b2, Q \u00b2 Q \u00b2, B \u00b2, B \u00b2, B \u00b2, B \u00b2, B \u00b2, B \u00b2, B, B, B, B, B \u00b2 B, B, B, B \u00b2 B, B, B \u00b2 B, B, B \u00b2 B, B \u00b2 B, B, B \u00b2 B, B \u00b2 B, B \u00b2 B, B \u00b2 B, B \u00b2, B \u00b2 B, B \u00b2 B, B \u00b2, B \u00b2, B \u00b2 B, B \u00b2, B \u00b2, B \u00b2, B, B \u00b2, B \u00b2, B \u00b2, B, B \u00b2, B \u00b2, B, B, B \u00b2, B, B, B, B \u00b2, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B."}, {"heading": "E Proof of Proposition 7", "text": "Consider a fixed j = 1,. \u2212 si = 1,. si = 1,. si = 1,. si = 1,. si (x), m (x), m (x), m (x), m (x), m (x), m (x), m (x), m (x), m (x), m (x), m (x), m (x), m (x), m (x), m (x), m (x), m (x), m (x), m (x), m (x), m (x), m (x), m (x), m (x)."}], "references": [{"title": "Steepest Descent Algorithms for Optimization under Unitary Matrix Constraint", "author": ["T.E. Abrudan", "J. Eriksson", "V. Koivunen"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Convergence of the Iterates of Descent Methods for Analytic Cost Functions", "author": ["P.-A. Absil", "R. Mahony", "B. Andrews"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Optimization Algorithms on Matrix Manifolds", "author": ["P.-A. Absil", "R. Mahony", "R. Sepulchre"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Projection\u2013Like Retractions on Matrix Manifolds", "author": ["P.-A. Absil", "J. Malick"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Manifolds of Negative Curvature", "author": ["R.L. Bishop", "B. O\u2019Neill"], "venue": "Transactions of the American Mathematical Society,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1969}, {"title": "Extrema of Sums of Heterogeneous Quadratic Forms", "author": ["M. Bolla", "G. Michaletzky", "G. Tusn\u00e1dy", "M. Ziermann"], "venue": "Linear Algebra and Its Applications,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "A Framework of Constraint Preserving Update Schemes for Optimization on Stiefel Manifold. Accepted for publication", "author": ["B. Jiang", "Y.-H. Dai"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Generalized Power Method for Sparse Principal Component Analysis", "author": ["M. Journ\u00e9e", "Yu. Nesterov", "P. Richt\u00e1rik", "R. Sepulchre"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Empirical Arithmetic Averaging over the Compact Stiefel Manifold", "author": ["T. Kaneko", "S. Fiori", "T. Tanaka"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Trace Optimization and Eigenproblems in Dimension Reduction Methods", "author": ["E. Kokiopoulou", "J. Chen", "Y. Saad"], "venue": "Numerical Linear Algebra with Applications,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "New Fractional Error Bounds for Polynomial Systems with Applications to H\u00f6lderian Stability in Optimization and Spectral Theory of Tensors", "author": ["G. Li", "B.S. Mordukhovich", "T.S.  Pha.m"], "venue": "Accepted for publication in Mathematical Programming, Series A,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Error Bounds and Convergence Analysis of Feasible Descent Methods: A General Approach", "author": ["Z.-Q. Luo", "P. Tseng"], "venue": "Annals of Operations Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1993}, {"title": "Optimization Algorithms Exploiting Unitary Constraints", "author": ["J.H. Manton"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "Convergence to Equilibrium for Discretizations of Gradient", "author": ["B. Merlet", "T.N. Nguyen"], "venue": "Like Flows on Riemannian Manifolds. Differential and Integral Equations,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Introductory Lectures on Convex Optimization: A Basic Course", "author": ["Yu. Nesterov"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Numerical Methods for Large Eigenvalue Problems", "author": ["Y. Saad"], "venue": "Classics in Applied Mathematics. Society for Industrial and Applied Mathematics, Philadelphia, Pennsylvania, revised edition,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Convergence Results for Projected Line\u2013Search Methods on Varieties of Low\u2013Rank Matrices via Lojasiewicz Inequality", "author": ["R. Schneider", "A. Uschmajew"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "A Generalized Solution of the Orthogonal", "author": ["P.H. Sch\u00f6nemann"], "venue": "Procrustes Problem. Psychometrika,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1966}, {"title": "A Stochastic PCA and SVD Algorithm with an Exponential Convergence Rate", "author": ["O. Shamir"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Optimization Techniques on Riemannian Manifolds", "author": ["S.T. Smith"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1994}, {"title": "Moment Inequalities for Sums of Random Matrices and Their Applications in Optimization", "author": ["A.M.-C. So"], "venue": "Mathematical Programming, Series A,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Non\u2013Asymptotic Convergence Analysis of Inexact Gradient Methods for Machine Learning Without Strong Convexity", "author": ["A.M.-C. So"], "venue": "Preprint, available at http://www.se.cuhk.edu.hk/~manchoso/papers/inexact_GM_conv.pdf,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Approximation Accuracy, Gradient Methods, and Error Bound for Structured Convex Optimization", "author": ["P. Tseng"], "venue": "Mathematical Programming, Series B,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "A Feasible Method for Optimization with Orthogonality Constraints", "author": ["Z. Wen", "W. Yin"], "venue": "Mathematical Programming, Series A,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Adaptive Canonical Correlation Analysis Based on Matrix Manifolds", "author": ["F. Yger", "M. Berar", "G. Gasso", "A. Rakotomamonjy"], "venue": "In Proceedings of the 29th International Conference on Machine Learning (ICML", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "l1,p\u2013Norm Regularization: Error Bounds and Convergence Rate Analysis of First\u2013Order Methods", "author": ["Z. Zhou", "Q. Zhang", "A.M.-C. So"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning (ICML 2015),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": ", [6, 13, 3, 8, 10, 16, 21, 25]).", "startOffset": 2, "endOffset": 31}, {"referenceID": 12, "context": ", [6, 13, 3, 8, 10, 16, 21, 25]).", "startOffset": 2, "endOffset": 31}, {"referenceID": 2, "context": ", [6, 13, 3, 8, 10, 16, 21, 25]).", "startOffset": 2, "endOffset": 31}, {"referenceID": 7, "context": ", [6, 13, 3, 8, 10, 16, 21, 25]).", "startOffset": 2, "endOffset": 31}, {"referenceID": 9, "context": ", [6, 13, 3, 8, 10, 16, 21, 25]).", "startOffset": 2, "endOffset": 31}, {"referenceID": 15, "context": ", [6, 13, 3, 8, 10, 16, 21, 25]).", "startOffset": 2, "endOffset": 31}, {"referenceID": 20, "context": ", [6, 13, 3, 8, 10, 16, 21, 25]).", "startOffset": 2, "endOffset": 31}, {"referenceID": 24, "context": ", [6, 13, 3, 8, 10, 16, 21, 25]).", "startOffset": 2, "endOffset": 31}, {"referenceID": 0, "context": ", [1, 3, 4, 24, 7].", "startOffset": 2, "endOffset": 18}, {"referenceID": 2, "context": ", [1, 3, 4, 24, 7].", "startOffset": 2, "endOffset": 18}, {"referenceID": 3, "context": ", [1, 3, 4, 24, 7].", "startOffset": 2, "endOffset": 18}, {"referenceID": 23, "context": ", [1, 3, 4, 24, 7].", "startOffset": 2, "endOffset": 18}, {"referenceID": 6, "context": ", [1, 3, 4, 24, 7].", "startOffset": 2, "endOffset": 18}, {"referenceID": 18, "context": "More recently, Shamir [19] developed a stochastic line-search method for Problem (1) when n = 1, B = In = 1, and A is negative semidefinite.", "startOffset": 22, "endOffset": 26}, {"referenceID": 19, "context": "On another front, Smith [20] showed that when used to optimize a smooth function over a Riemannian manifold, the method of steepest descent will converge linearly to a critical point if the function is strongly convex on the manifold.", "startOffset": 24, "endOffset": 28}, {"referenceID": 4, "context": "In particular, it is known that every smooth function that is convex on a compact Riemannian manifold (such as the Stiefel manifold) is constant [5].", "startOffset": 145, "endOffset": 148}, {"referenceID": 19, "context": "Therefore, one cannot hope to obtain linear convergence results for Problem (1) using the convexity-based approach in [20].", "startOffset": 118, "endOffset": 122}, {"referenceID": 1, "context": ", [2, 14, 17].", "startOffset": 2, "endOffset": 13}, {"referenceID": 13, "context": ", [2, 14, 17].", "startOffset": 2, "endOffset": 13}, {"referenceID": 16, "context": ", [2, 14, 17].", "startOffset": 2, "endOffset": 13}, {"referenceID": 10, "context": ", [11]) and can in principle be applied to Problem (1).", "startOffset": 2, "endOffset": 6}, {"referenceID": 16, "context": "well-established analysis framework in the literature [17], we conclude that a host of line-search methods for solving Problem (1) converge linearly to a critical point.", "startOffset": 54, "endOffset": 58}, {"referenceID": 2, "context": "Thus, it is qualitatively different from those in [3, 19].", "startOffset": 50, "endOffset": 57}, {"referenceID": 18, "context": "Thus, it is qualitatively different from those in [3, 19].", "startOffset": 50, "endOffset": 57}, {"referenceID": 11, "context": "Moreover, although our work is similar in spirit as [12, 23, 22, 26], there is a crucial difference: While the latter deals exclusively with convex optimization problems, the former considers an optimization problem in which neither the objective function nor the constraint is convex.", "startOffset": 52, "endOffset": 68}, {"referenceID": 22, "context": "Moreover, although our work is similar in spirit as [12, 23, 22, 26], there is a crucial difference: While the latter deals exclusively with convex optimization problems, the former considers an optimization problem in which neither the objective function nor the constraint is convex.", "startOffset": 52, "endOffset": 68}, {"referenceID": 21, "context": "Moreover, although our work is similar in spirit as [12, 23, 22, 26], there is a crucial difference: While the latter deals exclusively with convex optimization problems, the former considers an optimization problem in which neither the objective function nor the constraint is convex.", "startOffset": 52, "endOffset": 68}, {"referenceID": 25, "context": "Moreover, although our work is similar in spirit as [12, 23, 22, 26], there is a crucial difference: While the latter deals exclusively with convex optimization problems, the former considers an optimization problem in which neither the objective function nor the constraint is convex.", "startOffset": 52, "endOffset": 68}, {"referenceID": 2, "context": "We refer the reader to [3, 9] for details of these retractions.", "startOffset": 23, "endOffset": 29}, {"referenceID": 8, "context": "We refer the reader to [3, 9] for details of these retractions.", "startOffset": 23, "endOffset": 29}, {"referenceID": 16, "context": "We refer the reader to [17] for details.", "startOffset": 23, "endOffset": 27}, {"referenceID": 16, "context": "4 Convergence Analysis Framework for the Line-Search Method To analyze the convergence properties of Algorithm 1, we adopt the framework introduced in [17].", "startOffset": 151, "endOffset": 155}, {"referenceID": 14, "context": ", [15].", "startOffset": 2, "endOffset": 6}, {"referenceID": 6, "context": "In the numerical experiments, we follow the technique introduced in [7] and use (XTX)\u22121 to control feasibility error.", "startOffset": 68, "endOffset": 71}, {"referenceID": 17, "context": "(15) Problem (15) is an instance of the orthogonal Procrustes problem, whose optimal solution is given by X\u2217 = UV T , where S1 = U\u03a3V T is the singular value decomposition of S1 [18].", "startOffset": 177, "endOffset": 181}, {"referenceID": 17, "context": "Hence, by the result in [18], an optimal solution to Problem (21) is given by P \u2217 i = Hi [ W T i 0 0 Isi\u2212si\u22121\u2212hi ]", "startOffset": 24, "endOffset": 28}], "year": 2015, "abstractText": "A fundamental class of matrix optimization problems that arise in many areas of science and engineering is that of quadratic optimization with orthogonality constraints. Such problems can be solved using line-search methods on the Stiefel manifold, which are known to converge globally under mild conditions. To determine the convergence rate of these methods, we give an explicit estimate of the exponent in a Lojasiewicz inequality for the (non-convex) set of critical points of the aforementioned class of problems. By combining such an estimate with known arguments, we are able to establish the linear convergence of a large class of line-search methods. A key step in our proof is to establish a local error bound for the set of critical points, which may be of independent interest.", "creator": "LaTeX with hyperref package"}}}