{"id": "1510.01431", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2015", "title": "SentiCap: Generating Image Descriptions with Sentiments", "abstract": "The recent progress on image recognition and language modeling is making automatic description of image content a reality. However, stylized, non-factual aspects of the written description are missing from the current systems. One of such styles is descriptions with emotions, which is commonplace in everyday communication, and influences decision-making and interpersonal relationships. We design a system to describe an image with emotions, and present a model that automatically generates captions with positive or negative sentiments. We propose a novel switching recurrent neural network with word-level regularization, which is able to produce emotional image captions using only 2000+ training sentences containing sentiments. We evaluate the captions with different automatic and crowd-sourcing metrics. Our model compares favourably in common quality metrics for image captioning. In 84.6% of cases the generated positive captions were judged as being at least as descriptive as the factual captions, of these positive captions 88% were confirmed by the crowd-sourced workers as having the appropriate sentiment.", "histories": [["v1", "Tue, 6 Oct 2015 04:57:47 GMT  (1189kb,D)", "https://arxiv.org/abs/1510.01431v1", null], ["v2", "Sun, 13 Dec 2015 23:03:23 GMT  (2282kb,D)", "http://arxiv.org/abs/1510.01431v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["alexander patrick mathews", "lexing xie", "xuming he"], "accepted": true, "id": "1510.01431"}, "pdf": {"name": "1510.01431.pdf", "metadata": {"source": "CRF", "title": "SentiCap: Generating Image Descriptions with Sentiments", "authors": ["Alexander Mathews", "Lexing Xie", "Xuming He"], "emails": ["alex.mathews@anu.edu.au,", "lexing.xie@anu.edu.au,", "xuming.he@nicta.com.au"], "sections": [{"heading": "1 Introduction", "text": "The automatic description of an image by creating a coherent set combines two central challenges in artificial intelligence - vision and language. Although this is a difficult problem, the research community has recently made progress in this area, thanks to large labeled data sets, and advances in learning expressive neural network models. In addition to composing a factual description of the objects, the scene and their interactions in an image, there are richer variations in language, often referred to as styles (Crystal and Davy 1969). Let's say emotions, it is such a common phenomenon in our daily communication that more than half of the text accompanying online images contains an emoji (a graphic alphabet for emotions) (Instagram 2015). How well emotions are expressed and understood influences decision-making (Lerner et al. 2015) - from the mundane world (e.g., the creation of a restaurant menu that is attractive) to large (e.g. the election of a political leader in elections)."}, {"heading": "2 Related Work", "text": "Recent advances in visual recognition have led to \"a picture of a thousand words\" being much closer to reality, largely due to advances in Convolutional Neural Networks (CNN) (Simonyan and Zisserman 2015; Szegedy et al. 2015). A related theme that is also evolving rapidly is caption, where most early systems are based on retrieval of similarity by using objects and attributes (Farhadi et al. 2010; Kulkarni et al. 2011; Hodosh, Young, and Hockenmaier 2013; Gupta, Verma, and Jawahar 2012), and by merging sentence fragments such as object action scenes and attributes (Farhadi et al. 2010), subject verb object (Rohrbach et al. 2013), object attribute prepositions (Kulkarni et al al. 2011), or global image properties such as scene and lighting (Nwogu, Zhou, and Brown 2011)."}, {"heading": "3 Describing an Image with Sentiments", "text": "Considering an image I and its Dx-dimensional visual characteristic x-RDx, our goal is to generate a sequence of words (i.e. a caption) Y = {y1, \u00b7 \u00b7 \u00b7, yT} to describe the image with a particular style, such as the expression of sentiment. This is a 1-by-V encoded indicator vector for the tallest word; V is the size of the vocabulary; and T is the length of the caption. We assume that the sentence generation has two underlying mechanisms, one of which focuses on the factual description of the image, while the other describes the image content with sentiments. We formulate such a process of captioning using a changing multimodal language model that generates words in a sentence sequentially. Formally, we introduce a binary sentiment variable st-0, 1} for each word yt to indicate which mechanism is used."}, {"heading": "3.1 Switching RNNs for Sentiment Captions", "text": "We have a common CNN + RNN architecture (Vinyals et al. 2015) in the conditional labeling function (1). (1) The complete model combines two CNN + RNNs running in parallel. (1) This model aims to learn a number of LSTM units well - with only a small dataset of image description with feelings (Section 4), with the help of millions of picture-set pairs describing factual images (Chen et al. 2015). (1) Each RNN model consists of a series of LSTM units - with only a small dataset of image description with feelings (Section 4), with the help of millions of picture-set pairs describing factual images (Chen et al. 2015). Each RNN stream consists of a series of LSTM units."}, {"heading": "3.2 Learning the Switching RNN Model", "text": "One of the key challenges is to create a learning scheme for p (st) x (st) x (p) x (p) x (p) x (p) x (p) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x x (c) x x (c) x (c) x x x (c) x x x x x (c) x x x x x x x x (c) x) x x (c) x x x x (c) x) x x x (c) x (c) x) x x (c) x (c) x) x (c) x x (c) x (c) x) x x x x x (c) x x x x x (c) x x x (c) x) x x x x (c) x x (c) x x x x (c) x x x x (c) x x x (c) x x (c) x x x (c) x (c) x c) x x x (c) x (c) x (c) x c) x (c) x x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x x (c) x x (c) x x (c) x x (c) x (c) x (c) x x (c) x x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x x x (c) x (c) x (c) x x x (c) x x x (c) x"}, {"heading": "4 An Image Caption Dataset with Sentiments", "text": "In order to learn the association between images and captions with sensations, we are building a novel dataset of caption pairs, where the caption describes both an image and the desired mood. We are summarizing the new datasets and the crowd sourcing task to collect image sentiments. Further details of the data acquisition process can be found in suplementary1.There are many ways a photo could evoke emotions. In this work, we are focusing on creating a collection and learning experience from an objective viewer who does not know the background story outside of the photo - a setting that is also used by recent collections of objectively descriptive captions (Chen et al. 2015). We are designing a crowd sourcing task to collect such objectively described emotional captions, which is done in a caption based on objectively descriptive captions."}, {"heading": "5 Experiments", "text": "We implement RNNs with a fixed dynamics of 0.99 and a fixed learning rate of 0.01. We implement RNNs with LSTM units using the Theano package (Bastien et al. 2012). Our implementation of CNN + RNN reproduces caption generation performance in the most recent work (Karpathy and Fei-Fei 2015). Visual input into the circuit RNN is 4096-dimensional feature vector from the second last layer of the Oxford VGG module CNN (Simonyan and Zisserman 2015). These features are embedded in a D = 512-dimensional space. Our word embedding Ey are 512 dimensions and the hidden state h and the memory cell of the LSTM module also have 512 dimensions. The size of our vocabulary for generalization of sentences is 8.7cent actualization, and desktop 811 will train after inclusion of the SD model with additional grading."}, {"heading": "6 Conclusion", "text": "We proposed SentiCap, an alternating RNN model for generating captions with sensations. A novel feature of this model is a specialized word-level supervision scheme to effectively use a small amount of training data with sensations. We also designed a crowdsourcing task to rewrite captions to generate sentimental but descriptive captions. We demonstrate the effectiveness of the proposed model through automatic and crowdsourced evaluations, with the SentiCap model capable of generating an emotional caption for over 90% of the images, and the vast majority of captions generated being judged appropriate by crowdworkers. Future work may include a unified model for positive and negative sensations; models for linguistic styles (including sensations) that go beyond the word level; and generative models for a richer set of emotions such as pride, shame, anger."}, {"heading": "7 Appendix", "text": "This appendix primarily provides additional details about the model and data collection process, which is included to ensure that our results are easily reproducible and to clarify exactly how the data was collected. We will first provide additional details about the LSTM units used by our approach in Section 7.1. Section 7.2 discusses the differences between the mood of the first, second and third person. Section 7.3 includes a discussion of how the ANPs were selected with the mood. Details on rewriting sentences to include ANPs can be found in Section 7.4. Details on validating the rewritten sentences can be found in Section 7.5. Crowdsourcing evaluation of sentences generated is described in Section 7.6."}, {"heading": "7.1 The LSTM unit", "text": "The LSTM units we use are functionally the same as the units of Vinyals et al. (2015), which differs from the LSTM unit used by Xu et al. (2015a) because we do not couple the contextual information to the input of the units. A graphic representation of our LSTM units is shown in Figure 5; for a more complete definition, see Equation 2 in the accompanying paper. In Figure 5, note that only the LSTM unit is shown without the fully connected output layers or word layers."}, {"heading": "7.2 Sentimental descriptions in the first, second, and third person", "text": "There are many ways in which a photo can evoke emotions, which can be described as feelings of the first, second and third person. A mood of the first person is that a photo evokes the emotions of its owner / author / uploader, who then records those feelings for personal organization or communication with others (Ames and Naaman 2007). Like the Flickr photo titled \"This is the best day ever\" 1, see Figure 6. Title1 https: / / www.flickr.com / photos / pixelmama / 7612700314 / and the caption describe a story but not the content of the photo. A mood of the second person is expressed by someone to whom the photo is shared, such as the comments \"awesome\" and \"so sweet\" for the photo. The mood of the third person is expressed by an objective viewer who has information about its visual content but does not know the background story, such as the description of the photo above as \"Dreamy sunset by the sea.\""}, {"heading": "7.3 Customizing Visual Sentibank for captions", "text": "Visual SentiBank (Borth et al. 2013) is a database of adjective-noun pairs (ANPs) commonly used to describe online images. We use their methodology to build up the vocabulary of feelings. We take the title and first sentence of description from the YFCC100M dataset (Thomee et al. 2015), retain entries that are in English, and obtain all ANPs that appear in at least 100 images. We evaluate these ANPs based on the average of SentiWordNet (Esuli and Sebastiani 2006) and SentiStrength (Thelwall et al. 2010), the former being able to detect common lexical variations and the latter designed to evaluate brief informal text. We retain ANPs that contain clear positive or negative feelings, i.e. have an absolute value of 0.1 and above. We then join the Visual SentiBank ANPs together, thereby obtaining 1,027 ANPs with positive or negative emotions."}, {"heading": "7.4 AMT interface for collecting image captions with sentiment", "text": "We went through three design iterations to gather relevant and concise captions with the intended sentimentality. Our first attempt was to invite Amazon Mechanical Turk (AMT) workers to compose captions with either positive or negative moods for an image - leading to overly long, imaginative captions. A typical example is: \"A shitty picture embodies the total clich\u00e9 that the photographer catches himself in the mirror,\" while it also includes an overly bright bathroom with blazing white walls, dark, unattractive wooden cabinets lurking beneath a boring sink and holding an amber bowl that appears completely meaningless, below the mirror, with its clumsy teenage composition of a door showing a framed mirror inside (cheesy, forced perspective) and a somber-looking man with a camera. \"We asked the Turks to place ANPs in an existing caption, which is an uncomfortable part of the picture, or a verbose one of the candidates."}, {"heading": "7.5 AMT interface validating image captions with sentiment", "text": "The AMT validation interface in Figure 8 is designed to determine the impact of adding sentiment to ground truth captions on their writability, and to understand the fraction of images that can reasonably be described with positive or negative sentiment. Each task provides the user with three MSCOCO captions and three positive or negative sentences and asks the user to rate them. Our four-level description scale is based on schemes used by other authors (Hodosh, Young and Hockenmaier 2013; Vinyals et al. 2015)."}, {"heading": "7.6 AMT interface for rating captions with a sentiment", "text": "Each task consists of three different types of evaluations: the most positive, interesting, and descriptive. The most positive and interesting evaluations are done in pairs by comparing a sentence generated from one of the four methods with a sentence generated by CNN + RNN. Description evaluation uses the same four-point scale as the evaluation interface from Section 7.5. There are 5 images to evaluate the tasks; this is crucial due to the way AMT calculates the prices. We found that asking the Turks to evaluate sentences using this method initially led to very poor results, with many Turks choosing random options without reading the sentences. We suspect that in a number of cases bots were used to complete the tasks. Our first solution was the use of more qualified Turks, the so-called master workers, although this resulted in cleaner results, with the smaller number of workers meaning that a large group of tasks took us far too long to complete, or the most workers were using the results instead, with 95% having been very used."}], "references": [{"title": "and Naaman", "author": ["M. Ames"], "venue": "M.", "citeRegEx": "Ames and Naaman 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "I", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "Goodfellow"], "venue": "J.; Bergeron, A.; Bouchard, N.; and Bengio, Y.", "citeRegEx": "Bastien et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Large-scale visual sentiment ontology and detectors using adjective noun pairs. ACMMM", "author": ["Borth"], "venue": null, "citeRegEx": "Borth,? \\Q2013\\E", "shortCiteRegEx": "Borth", "year": 2013}, {"title": "C", "author": ["X. Chen", "Zitnick"], "venue": "L.", "citeRegEx": "Chen and Zitnick 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "C", "author": ["X. Chen", "H. Fang", "T.-Y. Lin", "R. Vedantam", "S. Gupta", "P. Dollar", "Zitnick"], "venue": "L.", "citeRegEx": "Chen et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Davy", "author": ["D. Crystal"], "venue": "D.", "citeRegEx": "Crystal and Davy 1969", "shortCiteRegEx": null, "year": 1969}, {"title": "L", "author": ["Donahue, J.", "Hendricks"], "venue": "A.; Guadarrama, S.; Rohrbach, M.; Venugopalan, S.; Saenko, K.; and Darrell, T.", "citeRegEx": "Donahue et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Sebastiani", "author": ["A. Esuli"], "venue": "F.", "citeRegEx": "Esuli and Sebastiani 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "J", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R.K. Srivastava", "L. Deng", "P. Dollar", "J. Gao", "X. He", "M. Mitchell", "Platt"], "venue": "C.; Lawrence Zitnick, C.; and Zweig, G.", "citeRegEx": "Fang et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "M", "author": ["A. Farhadi", "M. Hejrati", "Sadeghi"], "venue": "A.; Young, P.; Rashtchian, C.; Hockenmaier, J.; and Forsyth, D.", "citeRegEx": "Farhadi et al. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "C", "author": ["A. Gupta", "Y. Verma", "Jawahar"], "venue": "V.", "citeRegEx": "Gupta. Verma. and Jawahar 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "and Schmidhuber", "author": ["S. Hochreiter"], "venue": "J.", "citeRegEx": "Hochreiter and Schmidhuber 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics. JAIR", "author": ["Young Hodosh", "M. Hockenmaier 2013] Hodosh", "P. Young", "J. Hockenmaier"], "venue": null, "citeRegEx": "Hodosh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "J", "author": ["D. Joshi", "R. Datta", "E. Fedorovskaya", "Q.-T. Luong", "Wang"], "venue": "Z.; Li, J.; and Luo, J.", "citeRegEx": "Joshi et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "and Fei-Fei", "author": ["A. Karpathy"], "venue": "L.", "citeRegEx": "Karpathy and Fei.Fei 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Baby talk: Understanding and generating simple image descriptions. CVPR\u201911", "author": ["Kulkarni"], "venue": null, "citeRegEx": "Kulkarni,? \\Q2011\\E", "shortCiteRegEx": "Kulkarni", "year": 2011}, {"title": "T", "author": ["P. Kuznetsova", "V. Ordonez", "Berg"], "venue": "L.; and Choi, Y.", "citeRegEx": "Kuznetsova et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "K", "author": ["J.S. Lerner", "Y. Li", "P. Valdesolo", "Kassam"], "venue": "S.", "citeRegEx": "Lerner et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN). ICLR\u201915", "author": ["Mao"], "venue": null, "citeRegEx": "Mao,? \\Q2015\\E", "shortCiteRegEx": "Mao", "year": 2015}, {"title": "Strategies for training large scale neural network language models. ASRU\u201911", "author": ["Mikolov"], "venue": null, "citeRegEx": "Mikolov,? \\Q2011\\E", "shortCiteRegEx": "Mikolov", "year": 2011}, {"title": "AVA: A large-scale database for aesthetic visual analysis. CVPR\u201912", "author": ["Marchesotti Murray", "N. Perronnin 2012] Murray", "L. Marchesotti", "F. Perronnin"], "venue": null, "citeRegEx": "Murray et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Murray et al\\.", "year": 2012}, {"title": "Dependency Tree-based Sentiment Classification using CRFs with Hidden Variables", "author": ["Inui Nakagawa", "T. Kurohashi 2010] Nakagawa", "K. Inui", "S. Kurohashi"], "venue": null, "citeRegEx": "Nakagawa et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nakagawa et al\\.", "year": 2010}, {"title": "DISCO: Describing Images Using Scene Contexts and Objects. AAAI\u201911", "author": ["Zhou Nwogu", "I. Brown 2011] Nwogu", "Y. Zhou", "C. Brown"], "venue": null, "citeRegEx": "Nwogu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nwogu et al\\.", "year": 2011}, {"title": "and Lee", "author": ["B. Pang"], "venue": "L.", "citeRegEx": "Pang and Lee 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Translating video content to natural language descriptions. ICCV\u201913", "author": ["Rohrbach"], "venue": null, "citeRegEx": "Rohrbach,? \\Q2013\\E", "shortCiteRegEx": "Rohrbach", "year": 2013}, {"title": "An empirical analysis of domain adaptation algorithms for genomic sequence analysis. NIPS\u201908", "author": ["Schweikert"], "venue": null, "citeRegEx": "Schweikert,? \\Q2008\\E", "shortCiteRegEx": "Schweikert", "year": 2008}, {"title": "and Zisserman", "author": ["K. Simonyan"], "venue": "A.", "citeRegEx": "Simonyan and Zisserman 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "R", "author": ["J. Snoek", "H. Larochelle", "Adams"], "venue": "P.", "citeRegEx": "Snoek. Larochelle. and Adams 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "A", "author": ["R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "Ng"], "venue": "Y.; and Potts, C.", "citeRegEx": "Socher et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "G", "author": ["I. Sutskever", "J. Martens", "Hinton"], "venue": "E.", "citeRegEx": "Sutskever. Martens. and Hinton 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "and McDonald", "author": ["O. T\u00e4ckstr\u00f6m"], "venue": "R.", "citeRegEx": "T\u00e4ckstr\u00f6m and McDonald 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Sentiment strength detection in short informal text. JASIST", "author": ["Thelwall"], "venue": null, "citeRegEx": "Thelwall,? \\Q2010\\E", "shortCiteRegEx": "Thelwall", "year": 2010}, {"title": "D", "author": ["Thomee, B.", "Shamma"], "venue": "A.; Friedland, G.; Elizalde, B.; Ni, K.; Poland, D.; Borth, D.; and Li, L.-J.", "citeRegEx": "Thomee et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator. CVPR\u201915", "author": ["Vinyals"], "venue": null, "citeRegEx": "Vinyals,? \\Q2015\\E", "shortCiteRegEx": "Vinyals", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu"], "venue": null, "citeRegEx": "Xu,? \\Q2015\\E", "shortCiteRegEx": "Xu", "year": 2015}, {"title": "Jointly modeling deep video and compositional text to bridge vision and language in a unified framework. AAAI\u201915", "author": ["Xu"], "venue": null, "citeRegEx": "Xu,? \\Q2015\\E", "shortCiteRegEx": "Xu", "year": 2015}], "referenceMentions": [], "year": 2015, "abstractText": "The recent progress on image recognition and language modeling is making automatic description of image content a reality. However, stylized, non-factual aspects of the written description are missing from the current systems. One such style is descriptions with emotions, which is commonplace in everyday communication, and influences decision-making and interpersonal relationships. We design a system to describe an image with emotions, and present a model that automatically generates captions with positive or negative sentiments. We propose a novel switching recurrent neural network with word-level regularization, which is able to produce emotional image captions using only 2000+ training sentences containing sentiments. We evaluate the captions with different automatic and crowd-sourcing metrics. Our model compares favourably in common quality metrics for image captioning. In 84.6% of cases the generated positive captions were judged as being at least as descriptive as the factual captions. Of these positive captions 88% were confirmed by the crowd-sourced workers as having the appropriate sentiment.", "creator": "LaTeX with hyperref package"}}}