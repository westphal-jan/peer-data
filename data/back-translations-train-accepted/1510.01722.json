{"id": "1510.01722", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2015", "title": "Structured Transforms for Small-Footprint Deep Learning", "abstract": "We consider the task of building compact deep learning pipelines suitable for deployment on storage and power constrained mobile devices. We propose a unified framework to learn a broad family of structured parameter matrices that are characterized by the notion of low displacement rank. Our structured transforms admit fast function and gradient evaluation, and span a rich range of parameter sharing configurations whose statistical modeling capacity can be explicitly tuned along a continuum from structured to unstructured. Experimental results show that these transforms can significantly accelerate inference and forward/backward passes during training, and offer superior accuracy-compactness-speed tradeoffs in comparison to a number of existing techniques. In keyword spotting applications in mobile speech recognition, our methods are much more effective than standard linear low-rank bottleneck layers and nearly retain the performance of state of the art models, while providing more than 3.5-fold compression.", "histories": [["v1", "Tue, 6 Oct 2015 19:42:22 GMT  (75kb,D)", "http://arxiv.org/abs/1510.01722v1", "To appear in NIPS 2015; 9 pages"]], "COMMENTS": "To appear in NIPS 2015; 9 pages", "reviews": [], "SUBJECTS": "stat.ML cs.CV cs.LG", "authors": ["vikas sindhwani", "tara n sainath", "sanjiv kumar"], "accepted": true, "id": "1510.01722"}, "pdf": {"name": "1510.01722.pdf", "metadata": {"source": "CRF", "title": "Structured Transforms for Small-Footprint Deep Learning", "authors": ["Vikas Sindhwani", "Tara N. Sainath", "Sanjiv Kumar"], "emails": ["sanjivk}@google.com"], "sections": [{"heading": "1 Introduction", "text": "This is particularly relevant for \"always-on\" mobile applications, such as constantly searching for specific keywords spoken by the user or processing a live video stream onboard a mobile robot. In such settings, models can be hosted on specialized, low-performance digital signal processing components that are even more resource-constrained than the device itself."}, {"heading": "2 Displacement Operators associated with Structured Matrices", "text": "We begin by carefully selecting A and B of a displacement approach and stone displacement operators with certain properties of cement / important units. Unless otherwise stated, we will henceforth proceed from square transformations, i.e., m = n, and discuss rectangular transformations later. Evidence of various assertions can be found in our own supplement [1] or in [18, 19]. The Sylvester displacement operator, which is referred to as the operator, is the stone displacement operator, which is referred to as L = 4A, B [M] = AM \u2212 MB (1), where A \u00b7 Rn \u00b7 n are fixed matrices, which are referred to as operators. Closely related is the stone displacement operator, denoted as L = 4A, B: Rn \u00b7 n 7 \u2192 Rn \u00b7 n n, and defined by, 4A [M] = AMB (2) By carefully selecting A and B one can find displacement operators with certain properties of important cement / cement operators."}, {"heading": "3 Learning Toeplitz-like Structured Transforms", "text": "The question that arises is whether or not these matrices are capable of unfolding in a certain way. \u2022 The question that arises is whether or not these matrices are able to unfold in a certain way. \u2022 The question of whether or not the matrices are able to unfold is whether the matrices are able to unfold. \u2022 The question of whether or not the matrices are able to unfold is whether or not the matrices are able to unfold. \u2022 The question of whether or not the matrices are able to unfold is whether or not the matrices are able to unfold. \u2022 The question of whether or not the matrices are able to unfold themselves. \u2022 The question of whether or not the matrices are able to unfold. \u2022 All the questions that we ask. \u2022 All the questions that we ask. \u2022 All the questions that we ask. \u2022 All the questions that we ask are."}, {"heading": "4 Empirical Studies", "text": "In fact, most people who have chosen such a project have to do it in order to do it."}, {"heading": "5 Perspective", "text": "We have introduced and demonstrated the effectiveness of new concepts of thrift rooted in the theory of structured matrices. Our proposal can be extended to several other structured matrix classes, including block and multi-level Toeplitz-like [12] matrices related to multidimensional folding [21]. We hope that such ideas will lead to new generalizations of revolutionary neural networks. Recognition: We thank Krzysztof Choromanski, Carolina Parada, Rohit Prabhavalkar, Rajat Monga, Baris Sumengen, Kilian Weinberger and Wenlin Chen for their contributions to this work."}], "references": [{"title": "Small-footprint keyword spotting using deep neural networks", "author": ["G. Chen", "C. Parada", "G. Heigold"], "venue": "In ICASSP,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Compressing neural networks with the hashing trick", "author": ["W. Chen", "J.T. Wilson", "S. Tyree", "K.Q. Weinberger", "Y. Chen"], "venue": "In ICML,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Fast neural networks with circulant projections", "author": ["Y. Cheng", "F.X. Xu", "R.S. Feris", "S. Kumar", "A. Choudhary", "S.-F. Chang"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "High-performance neural networks for visual object classification", "author": ["D.C. Ciresan", "U. Meier", "J. Masci", "L.M. Gambardella", "Schmidhuber"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Memory-bounded deep convolutional neural networks", "author": ["M.D. Collins", "P. Kohli"], "venue": "In ICASSP,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Low-precision storage for deep learning", "author": ["M. Courbariaux", "J.-P. David", "Y. Bengio"], "venue": "In ICLR,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Large-scale distributed deep networks", "author": ["J. Dean", "G.S. Corrado", "R. Monga", "K. Chen", "M. Devin", "Q.V. Le", "M.Z. Mao", "M. Ranzato", "A. Senior", "P. Tucker", "K. Yang", "A.Y. Ng"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Predicting parameters in deep learning", "author": ["M. Denil", "B. Shakibi", "L. Dinh", "N. de Freitas"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Toeplitz and circulant matrices: A review", "author": ["R.M. Gray"], "venue": "Foundations and Trends in Communications and Information Theory,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Distilling the knowledge in a neural network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "In NIPS workshop,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Generalized displacement structure for block toeplitz, toeplitz block and toeplitz-derived matrices", "author": ["T. Kailath", "J. Chun"], "venue": "SIAM J. Matrix Anal. Appl.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1994}, {"title": "Displacement ranks of matrices and linear equations", "author": ["T. Kailath", "S.Y. Kung", "M. Morf"], "venue": "Journal of Mathematical Analysis and Applications,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1979}, {"title": "Displacement structure: Theory and applications", "author": ["T. Kailath", "A.H. Sayed"], "venue": "SIAM Review,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1995}, {"title": "An empirical evaluation of deep architectures on problems with many factors of variation", "author": ["H. Larochelle", "D. Erhan", "A.C. Courville", "J. Bergstra", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Fastfood \u2013 approximating kernel expansions in loglinear time", "author": ["Q. Le", "T. Sarlos", "A. Smola"], "venue": "In ICML,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "The mailman algorithm: a note on matrix vector multiplication", "author": ["E. Liberty", "S.W. Zucker"], "venue": "In Information Processing Letters,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Structured Matrices and Polynomials", "author": ["V. Pan"], "venue": "Unified Superfast Algorithms. Springer,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2001}, {"title": "Inversion of displacement operators", "author": ["V. Pan"], "venue": "SIAM Journal of Matrix Analysis and Applications,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2003}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "In NIPS,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Fast multidimensional convolution in low-rank tensor formats via cross approximation", "author": ["M.V. Rakhuba", "I.V. Oseledets"], "venue": "SIAM J. Sci. Comput.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Low-rank matrix factorization for deep neural network training with high-dimensional output targets", "author": ["T. Sainath", "B. Kingsbury", "V. Sindhwani", "E. Arisoy", "B. Ramabhadran"], "venue": "In ICASSP,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Convolutional neural networks for small-footprint keyword spotting", "author": ["T. Sainath", "C. Parada"], "venue": "In Proc. Interspeech,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Improving the speed of neural networks on cpus", "author": ["V. Vanhoucke", "A. Senior", "M.Z. Mao"], "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Deep fried convnets", "author": ["Z. Yang", "M. Moczulski", "M. Denil", "N. de Freitas", "A. Smola", "L. Song", "Z. Wang"], "venue": "In arXiv:1412.7149,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}], "referenceMentions": [{"referenceID": 18, "context": "Non-linear vector-valued transforms of the form, f(x,M) = s(Mx), where s is an elementwise nonlinearity, x is an input vector, and M is an m \u00d7 n matrix of parameters are building blocks of complex deep learning pipelines and non-parametric function estimators arising in randomized kernel methods [20].", "startOffset": 297, "endOffset": 301}, {"referenceID": 20, "context": "A parsimonious structure typically imposed on parameter matrices is that of low-rankness [22].", "startOffset": 89, "endOffset": 93}, {"referenceID": 4, "context": "Another popular structure is that of sparsity [6] typically imposed during optimization via zero-inducing l0 or l1 regularizers.", "startOffset": 46, "endOffset": 49}, {"referenceID": 18, "context": "Other techniques include freezing M to be a random matrix as motivated via approximations to kernel functions [20], storing M in low fixed-precision formats [7, 24], using specific parameter sharing mechanisms [3], or training smaller models on outputs of larger models (\u201cdistillation\u201d) [11].", "startOffset": 110, "endOffset": 114}, {"referenceID": 5, "context": "Other techniques include freezing M to be a random matrix as motivated via approximations to kernel functions [20], storing M in low fixed-precision formats [7, 24], using specific parameter sharing mechanisms [3], or training smaller models on outputs of larger models (\u201cdistillation\u201d) [11].", "startOffset": 157, "endOffset": 164}, {"referenceID": 22, "context": "Other techniques include freezing M to be a random matrix as motivated via approximations to kernel functions [20], storing M in low fixed-precision formats [7, 24], using specific parameter sharing mechanisms [3], or training smaller models on outputs of larger models (\u201cdistillation\u201d) [11].", "startOffset": 157, "endOffset": 164}, {"referenceID": 1, "context": "Other techniques include freezing M to be a random matrix as motivated via approximations to kernel functions [20], storing M in low fixed-precision formats [7, 24], using specific parameter sharing mechanisms [3], or training smaller models on outputs of larger models (\u201cdistillation\u201d) [11].", "startOffset": 210, "endOffset": 213}, {"referenceID": 9, "context": "Other techniques include freezing M to be a random matrix as motivated via approximations to kernel functions [20], storing M in low fixed-precision formats [7, 24], using specific parameter sharing mechanisms [3], or training smaller models on outputs of larger models (\u201cdistillation\u201d) [11].", "startOffset": 287, "endOffset": 291}, {"referenceID": 16, "context": "Below are classes of structured matrices arising pervasively in many contexts [18] with different types of parameter sharing (indicated by the color).", "startOffset": 78, "endOffset": 82}, {"referenceID": 8, "context": "Toeplitz and Hankel matrices are intimately related to one-dimensional discrete convolutions [10], and arise naturally in time series analysis and dynamical systems.", "startOffset": 93, "endOffset": 97}, {"referenceID": 11, "context": "This displacement rank approach, which can be traced back to a seminal 1979 paper [13], greatly unifies algorithm design and complexity analysis for structured matrices [13], [18], [14].", "startOffset": 82, "endOffset": 86}, {"referenceID": 11, "context": "This displacement rank approach, which can be traced back to a seminal 1979 paper [13], greatly unifies algorithm design and complexity analysis for structured matrices [13], [18], [14].", "startOffset": 169, "endOffset": 173}, {"referenceID": 16, "context": "This displacement rank approach, which can be traced back to a seminal 1979 paper [13], greatly unifies algorithm design and complexity analysis for structured matrices [13], [18], [14].", "startOffset": 175, "endOffset": 179}, {"referenceID": 12, "context": "This displacement rank approach, which can be traced back to a seminal 1979 paper [13], greatly unifies algorithm design and complexity analysis for structured matrices [13], [18], [14].", "startOffset": 181, "endOffset": 185}, {"referenceID": 11, "context": "In Section 2, we attempt to give a self-contained overview of the displacement rank approach [13], [18] drawing key results from the relevant literature on structured matrix computations (proved in our supplementary material [1] for completeness).", "startOffset": 93, "endOffset": 97}, {"referenceID": 16, "context": "In Section 2, we attempt to give a self-contained overview of the displacement rank approach [13], [18] drawing key results from the relevant literature on structured matrix computations (proved in our supplementary material [1] for completeness).", "startOffset": 99, "endOffset": 103}, {"referenceID": 16, "context": "Proofs of various assertions can be found in our selfcontained supplementary material [1] or in [18, 19].", "startOffset": 96, "endOffset": 104}, {"referenceID": 17, "context": "Proofs of various assertions can be found in our selfcontained supplementary material [1] or in [18, 19].", "startOffset": 96, "endOffset": 104}, {"referenceID": 16, "context": "Fast numerical linear algebra algorithms extend to such matrices [18].", "startOffset": 65, "endOffset": 69}, {"referenceID": 17, "context": "2 ( [19], Krylov Decomposition).", "startOffset": 4, "endOffset": 8}, {"referenceID": 16, "context": "4 ([18]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "First, from the properties of displacement operators [18], it follows that this class of matrices is very rich from a statistical modeling perspective.", "startOffset": 53, "endOffset": 57}, {"referenceID": 16, "context": "4 [18]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 17, "context": "4 exist for rectangular transforms, see [19].", "startOffset": 40, "endOffset": 44}, {"referenceID": 1, "context": "We exactly replicate the experimental setting from the recent paper on HASHEDNETS [3] which uses several image classification datasets first prepared by [15].", "startOffset": 82, "endOffset": 85}, {"referenceID": 13, "context": "We exactly replicate the experimental setting from the recent paper on HASHEDNETS [3] which uses several image classification datasets first prepared by [15].", "startOffset": 153, "endOffset": 157}, {"referenceID": 1, "context": "Several existing techniques are benchmarked in [3] for compressing a reference single hidden layer model with 1000 hidden nodes.", "startOffset": 47, "endOffset": 50}, {"referenceID": 3, "context": "\u2022 Random Edge Removal (RER) [5] where a fraction of weights are randomly frozen to be zero-valued.", "startOffset": 28, "endOffset": 31}, {"referenceID": 7, "context": "\u2022 Low-rank Decomposition (LRD) [9] \u2022 Neural Network (NN) where the hidden layer size is reduced to satisfy a parameter budget.", "startOffset": 31, "endOffset": 34}, {"referenceID": 9, "context": "\u2022 Dark Knowledge (DK) [11]: A small neural network is trained with respect to both the original labeled data, as well as soft targets generated by a full uncompressed neural network.", "startOffset": 22, "endOffset": 26}, {"referenceID": 1, "context": "\u2022 HashedNets (HN) [3]: This approach uses a low-cost hash function to randomly group connection weights which share the same value.", "startOffset": 18, "endOffset": 21}, {"referenceID": 23, "context": "We also compare with the FASTFOOD approach of [25, 16] where the weight matrix is a product of diagonal parameter matrices and fixed permutation and Walsh-Hadamard matrices, also admittingO(n log n) multiplication and gradient computation time.", "startOffset": 46, "endOffset": 54}, {"referenceID": 14, "context": "We also compare with the FASTFOOD approach of [25, 16] where the weight matrix is a product of diagonal parameter matrices and fixed permutation and Walsh-Hadamard matrices, also admittingO(n log n) multiplication and gradient computation time.", "startOffset": 46, "endOffset": 54}, {"referenceID": 2, "context": "The CIRCULANT Neural Network approach proposed in [4] is a special case of our framework (Theorem 3.", "startOffset": 50, "endOffset": 53}, {"referenceID": 15, "context": "We note in passing that for HASHEDNETS weight matrices whose entries assume only one of B distinct values, the Mailman algorithm [17] can be used for faster matrix-vector multiplication, with complexity O(n log(B)/(log n)), which still is much slower than matrix-vector multiplication time for Toeplitz-like matrices.", "startOffset": 129, "endOffset": 133}, {"referenceID": 9, "context": "Also note that the distillation ideas of [11] are complementary to our approach and can further improve our results.", "startOffset": 41, "endOffset": 45}, {"referenceID": 0, "context": "Specifically, we consider a keyword spotting (KWS) task, where a deep neural network is trained to detect a specific phrase, such as \u201cOk Google\u201d [2].", "startOffset": 145, "endOffset": 148}, {"referenceID": 21, "context": "We refer the reader to [23] for more details about the datasets.", "startOffset": 23, "endOffset": 27}, {"referenceID": 21, "context": "We consider the task of shrinking a large model for this task whose architecture is as follows [23]: the input layer consists of 40 dimensional log-mel filterbanks, stacked with a temporal context of 32, to produce an input of 32 \u00d7 40 whose dimensions are in time and frequency respectively.", "startOffset": 95, "endOffset": 99}, {"referenceID": 6, "context": "We use asynchronous distributed stochastic gradient descent (SGD) in a parameter server framework [8], with 25 worker nodes for optimizing various models.", "startOffset": 98, "endOffset": 101}, {"referenceID": 21, "context": "Results with 11 different models are reported in Figure 4 (left) including the state of the art keyword spotting model developed in [23].", "startOffset": 132, "endOffset": 136}, {"referenceID": 21, "context": "6 times larger reference [23] models.", "startOffset": 25, "endOffset": 29}, {"referenceID": 10, "context": "Our proposal can be extended to various other structured matrix classes, including Block and multi-level Toeplitz-like [12] matrices related to multidimensional convolution [21].", "startOffset": 119, "endOffset": 123}, {"referenceID": 19, "context": "Our proposal can be extended to various other structured matrix classes, including Block and multi-level Toeplitz-like [12] matrices related to multidimensional convolution [21].", "startOffset": 173, "endOffset": 177}], "year": 2015, "abstractText": "We consider the task of building compact deep learning pipelines suitable for deployment on storage and power constrained mobile devices. We propose a unified framework to learn a broad family of structured parameter matrices that are characterized by the notion of low displacement rank. Our structured transforms admit fast function and gradient evaluation, and span a rich range of parameter sharing configurations whose statistical modeling capacity can be explicitly tuned along a continuum from structured to unstructured. Experimental results show that these transforms can significantly accelerate inference and forward/backward passes during training, and offer superior accuracy-compactness-speed tradeoffs in comparison to a number of existing techniques. In keyword spotting applications in mobile speech recognition, our methods are much more effective than standard linear low-rank bottleneck layers and nearly retain the performance of state of the art models, while providing more than 3.5-fold compression.", "creator": "LaTeX with hyperref package"}}}