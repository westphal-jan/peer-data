{"id": "1510.01784", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2015", "title": "VBPR: Visual Bayesian Personalized Ranking from Implicit Feedback", "abstract": "Modern recommender systems model people and items by discovering or `teasing apart' the underlying dimensions that encode the properties of items and users' preferences toward them. Critically, such dimensions are uncovered based on user feedback, often in implicit form (such as purchase histories, browsing logs, etc.); in addition, some recommender systems make use of side information, such as product attributes, temporal information, or review text. However one important feature that is typically ignored by existing personalized recommendation and ranking methods is the visual appearance of the items being considered. In this paper we propose a scalable factorization model to incorporate visual signals into predictors of people's opinions, which we apply to a selection of large, real-world datasets. We make use of visual features extracted from product images using (pre-trained) deep networks, on top of which we learn an additional layer that uncovers the visual dimensions that best explain the variation in people's feedback. This not only leads to significantly more accurate personalized ranking methods, but also helps to alleviate cold start issues, and qualitatively to analyze the visual dimensions that influence people's opinions.", "histories": [["v1", "Tue, 6 Oct 2015 23:46:15 GMT  (4989kb,D)", "http://arxiv.org/abs/1510.01784v1", "AAAI'16"]], "COMMENTS": "AAAI'16", "reviews": [], "SUBJECTS": "cs.IR cs.AI", "authors": ["ruining he", "julian mcauley"], "accepted": true, "id": "1510.01784"}, "pdf": {"name": "1510.01784.pdf", "metadata": {"source": "CRF", "title": "VBPR: Visual Bayesian Personalized Ranking from Implicit Feedback", "authors": ["Ruining He", "Julian McAuley"], "emails": ["r4he@ucsd.edu", "jmcauley@ucsd.edu"], "sections": [{"heading": "Introduction", "text": "In fact, most of them will be able to move to another world, where they will be able to move to another world, where they will be able to move to another world, where they will be able to move to where they are."}, {"heading": "Related Work", "text": "In recent years, it has become clear that the various users are people who are unable to identify themselves. (In recent years, the number of users who are unable to identify themselves has increased significantly.) In recent years, the number of users who are unable to identify themselves has increased significantly. (In recent years, the number of users who are unable to identify themselves has increased significantly.) In recent years, the number of users who are unable to respond to such a challenge has increased.) In addition, the number of users who are unable to identify themselves has increased significantly."}, {"heading": "Problem Formulation", "text": "Here, we focus on scenarios in which the ranking must be learned from the implicit feedback from users (e.g. purchase history). If U and I denote the set of users or articles, each user u is associated with an article set I + u on which u has explicitly expressed positive feedback. In addition, a single image is available for each element i-I. Our goal is to create a personalized ranking for each user u of those elements about which u has not yet given feedback (i.e. I\\ I + u)."}, {"heading": "Preference Predictor", "text": "Our Preference Predictor is built on top of Matrix Factorization (MF), which is the state-of-the-art technology for evaluating predictions and modelling implicit feedback, the basic formulation of which assumes that the following model predicts a user's preference for an item i (Koren and Bell, 2011): x u, i = \u03b1 + \u03b2i + \u03b3 T u \u03b3i, (1) where the user's visual appearance is globally offset, \u03b2u and \u03b2i are user / item terms, and \u03b3u and \u03b3i are K-dimensional vectors describing the user's latent factors u and Item (respectively). The internal product \u03b3Tu \u03b3i then encodes the \"compatibility\" between the user u and the item i, i.e. the extent to which the user's latent \"preferences\" are aligned with the product characteristics. \"Although theoretically latent factors are able to detect all relevant dimensions,\" it is a \"major problem we observe\" (the existence of a cold dimension)."}, {"heading": "Model Learning Using BPR", "text": "Bayesian Personalized Ranking (BPR) is a pair-wise ranking optimization framework that adopts stochastic gradient ascent as a training procedure. A training set DS consists of triples of the form (u, i, j), where u denotes the user together with an element i, through which he has expressed positive feedback, and an unobserved element j: DS = {(u, i, j) | u - u - i \u2212 j - i + u. \"(5) Following the notation in Rendle et al. (2009), the parameter vector and x - uij (n, j) - x - x - x - x - x - is an arbitrary function of the system that defines the relationship between the components of the triple (u, i, j) - j -.\""}, {"heading": "Scalability", "text": "The efficiency of the underlying BPR-MF makes our models similarly scalable. Specifically, BPR-MF requires O (K) to complete the updating of the parameters for each collected triple (u, i, j). In our case, we also need to update the visual parameters, in particular the updating of \u03b8u takesO (D \u00b7 F) = O (D), \u03b2 \u2032 takesO (F) and E takesO (D \u00b7 F) = O (D), where F is the dimension of CNN characteristics (set at 4096 in our case). Therefore, the overall complexity of our model for updating is every triple O (K + D) (i.e. O (K) + O (D \u00b7 F)))), i.e. linear in the number of dimensions."}, {"heading": "Experiments", "text": "In this section, we conduct experiments with multiple real-world data sets that include a variety of settings in which visual appearance should play a role in consumer decision-making."}, {"heading": "Datasets", "text": "The first set of data sets comes from Amazon.com, which was introduced by McAuley et al. (2015). We look at two broad categories in which visual features have already been proven to be meaningful, namely women's and men's apparel. We also look at mobile phones & accessories, where we expect visual features to play a smaller but potentially still significant role. We take users \"rating history as implicit feedback and use one image per article to extract visual characteristics. We also introduce a new data set from Tradesy.com, a retail community for second-hand apparel. It reveals users\" purchasing history and the \"thumbs up\" that we collectively use as positive feedback. Note that in this environment, a recommendation inherently includes cold-start predictions due to the \"unique\" trading characteristics of second-hand stores. To design a meaningful recommendation system for such a data set, it is critical that we incorporate visual information into each data set as we work."}, {"heading": "Visual Features", "text": "For each element i in the above data sets, we collect a product image and extract visual features fi using the Caffe reference model (Jia et al., 2014), which implements the CNN architecture proposed by Krizhevsky, Sutskever and Hinton (2012).The architecture consists of 5 sinuous layers followed by 3 fully bonded layers and was pre-trained on 1.2 million ImageNet images (ILSVRC2010).In our experiments, we take the output of the second fully bonded layer (i.e. FC7) to obtain a visual feature vector fi of F = 4096."}, {"heading": "Evaluation Methodology", "text": "We divide our data into training / validation / test sets by selecting for each user u a random element that will be used for validation Vu and another one for test Tu. All remaining data will be used for the training.The predicted ranking will be evaluated at Tu using the widely used metric AUC curve (Area Under the ROC curve): AUC = 1 | U | \u2211 u 1 | E (u) | \u2211 (i, j) \u0442 E (u) \u03b4 (x-u, i > x-u, j) (9), where the set of assessment pairs for user u is E (u) = {(i, j) | (u, i) \u0445 Tu (u-u, j) / \u0441\u0442\u043e\u0441\u0442\u043e\u0441\u0442\u043e\u0441\u0442\u043e\u0441\u0442\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u0442\u043e\u0441\u0442\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u0442\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e (10)}, (b) the test set will be defined by the user set on which we report the performance for the Validation set (in all cases)."}, {"heading": "Baselines", "text": "Since there are no comparable visually-conscious MF methods, we compare primarily against the state-of-the-art MF models, in addition to a recently proposed content-based methodology. Table 3: AUC on the Test-Set T (# Factors = 20).The best method on each Dataset is boldfaced.Dataset Setting (a) (d) (e) (f) improvementRAND MP IBR MF-MF BPR f against the best method on each Dataset. The best method on each Dataset is boldfaced.Dataset Setting (a) (d) (f).IBR MP IBR MF-MF BPR f against the best method on eAmazon Women All Items 0,4997 0,5772 0,7127 0,7020 0,90,7834 9.6% Cold Start 0,5031 0,3159 0,6673 0,5489 0,5281 0,6813 0.54% Amazon Men All Items 0,490,570,970,972 0,970,970,970,970,970,9185 0,90,90,90,940,940,940,940,90,940,940,940,90,90,940,90,940,90,90,90,90,90,90,90,940,90,90,90,90,90,90,90,940,940,90,90,940,90,90,90,90,90,90,90,90,940,940,90,940,90,90,90,940,90,90,90,90,90,940,90,90,940,90,90,940,90,940,90,90,90,90,90,940,90,90,940,90,90,90,940,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,9"}, {"heading": "Performance", "text": "In fact, most of them will be able to abide by the rules that they have imposed on themselves, and that they will be able to break the rules that they have imposed on themselves. (...) Most of them are able to understand the rules. (...) Most of them are not able to break the rules that they have imposed on themselves. (...) Most of them are not able to comply with the rules. (...) Most of them are not able to comply with the rules. (...) Most of them are not able to comply with the rules. (...) Most of them are not able to comply with the rules. (...) Most of them are not able to comply with the rules. (...) Most of them are not able to comply with the rules. (...)"}, {"heading": "Conclusion & Future Work", "text": "In this paper, we explored the usefulness of visual characteristics for personalized ranking tasks based on implicit feedback data sets. We proposed a scalable method that incorporates visual characteristics from product images into matrix factorization to reveal the \"visual dimensions\" that most influence people's behavior. Our model is trained using the Bayesian Personalized Ranking (BPR) using stochastic gradient ascents. Experimental results from several large real data sets show that we can significantly outperform cutting-edge ranking techniques and mitigate cold-start problems. As part of future work, we will add time dynamics to our model to take into account shifts in fashion tastes over time. We are also interested in investigating the effectiveness of our proposed method in setting explicit feedback."}], "references": [{"title": "Topicmf: Simultaneously exploiting ratings and reviews for recommendation", "author": ["Y. Bao", "H. Fang", "J. Zhang"], "venue": "AAAI.", "citeRegEx": "Bao et al\\.,? 2014", "shortCiteRegEx": "Bao et al\\.", "year": 2014}, {"title": "The bellkor solution to the netflix prize", "author": ["R.M. Bell", "Y. Koren", "C. Volinsky"], "venue": null, "citeRegEx": "Bell et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bell et al\\.", "year": 2007}, {"title": "The netflix prize", "author": ["J. Bennett", "S. Lanning"], "venue": "KDDCup.", "citeRegEx": "Bennett and Lanning,? 2007", "shortCiteRegEx": "Bennett and Lanning", "year": 2007}, {"title": "Context-aware applications: from the laboratory to the marketplace", "author": ["P. Brown", "J. Bovey", "X. Chen"], "venue": "IEEE Wireless Communications.", "citeRegEx": "Brown et al\\.,? 1997", "shortCiteRegEx": "Brown et al\\.", "year": 1997}, {"title": "Google news personalization: scalable online collaborative filtering", "author": ["A. Das", "M. Datar", "A. Garg", "S. Rajaram"], "venue": "WWW.", "citeRegEx": "Das et al\\.,? 2007", "shortCiteRegEx": "Das et al\\.", "year": 2007}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "ICML.", "citeRegEx": "Donahue et al\\.,? 2014", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "Mymedialite: A free recommender system library", "author": ["Z. Gantner", "S. Rendle", "C. Freudenthaler", "L. SchmidtThieme"], "venue": "RecSys.", "citeRegEx": "Gantner et al\\.,? 2011", "shortCiteRegEx": "Gantner et al\\.", "year": 2011}, {"title": "Collaborative filtering for implicit feedback datasets", "author": ["Y. Hu", "Y. Koren", "C. Volinsky"], "venue": "ICDM. IEEE.", "citeRegEx": "Hu et al\\.,? 2008", "shortCiteRegEx": "Hu et al\\.", "year": 2008}, {"title": "Large scale visual recommendations from street fashion images", "author": ["V. Jagadeesh", "R. Piramuthu", "A. Bhardwaj", "W. Di", "N. Sundaresan"], "venue": "SIGKDD.", "citeRegEx": "Jagadeesh et al\\.,? 2014", "shortCiteRegEx": "Jagadeesh et al\\.", "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R.B. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "MM.", "citeRegEx": "Jia et al\\.,? 2014", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Getting the look: clothing recognition and segmentation for automatic product suggestions in everyday photos", "author": ["Y. Kalantidis", "L. Kennedy", "L.-J. Li"], "venue": "ICMR.", "citeRegEx": "Kalantidis et al\\.,? 2013", "shortCiteRegEx": "Kalantidis et al\\.", "year": 2013}, {"title": "Supercharging recommender systems using taxonomies for learning user purchase behavior", "author": ["B. Kanagal", "A. Ahmed", "S. Pandey", "V. Josifovski", "J. Yuan", "L. Garcia-Pueyo"], "venue": "VLDB Endowment.", "citeRegEx": "Kanagal et al\\.,? 2012", "shortCiteRegEx": "Kanagal et al\\.", "year": 2012}, {"title": "Recognizing image style", "author": ["S. Karayev", "M. Trentacoste", "H. Han", "A. Agarwala", "T. Darrell", "A. Hertzmann", "H. Winnemoeller"], "venue": "BMVC.", "citeRegEx": "Karayev et al\\.,? 2014", "shortCiteRegEx": "Karayev et al\\.", "year": 2014}, {"title": "Yahoo! music recommendations: modeling music ratings with temporal dynamics and item taxonomy", "author": ["N. Koenigstein", "G. Dror", "Y. Koren"], "venue": "RecSys.", "citeRegEx": "Koenigstein et al\\.,? 2011", "shortCiteRegEx": "Koenigstein et al\\.", "year": 2011}, {"title": "Advances in collaborative filtering", "author": ["Y. Koren", "R. Bell"], "venue": "Recommender systems handbook. Springer.", "citeRegEx": "Koren and Bell,? 2011", "shortCiteRegEx": "Koren and Bell", "year": 2011}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Multi-relational matrix factorization using bayesian personalized ranking for social network data", "author": ["A. Krohn-Grimberghe", "L. Drumond", "C. Freudenthaler", "L. Schmidt-Thieme"], "venue": "WSDM.", "citeRegEx": "Krohn.Grimberghe et al\\.,? 2012", "shortCiteRegEx": "Krohn.Grimberghe et al\\.", "year": 2012}, {"title": "RAPID: rating pictorial aesthetics using deep learning", "author": ["X. Lu", "Z. Lin", "H. Jin", "J. Yang", "J.Z. Wang"], "venue": "MM.", "citeRegEx": "Lu et al\\.,? 2014", "shortCiteRegEx": "Lu et al\\.", "year": 2014}, {"title": "Content-based collaborative filtering for news topic recommendation", "author": ["Z. Lu", "Z. Dou", "J. Lian", "X. Xie", "Q. Yang"], "venue": "AAAI.", "citeRegEx": "Lu et al\\.,? 2015", "shortCiteRegEx": "Lu et al\\.", "year": 2015}, {"title": "Image-based recommendations on styles and substitutes", "author": ["J.J. McAuley", "C. Targett", "Q. Shi", "A. van den Hengel"], "venue": null, "citeRegEx": "McAuley et al\\.,? \\Q2015\\E", "shortCiteRegEx": "McAuley et al\\.", "year": 2015}, {"title": "Gbpr: Group preference based bayesian personalized ranking for one-class collaborative filtering", "author": ["W. Pan", "L. Chen"], "venue": "IJCAI.", "citeRegEx": "Pan and Chen,? 2013", "shortCiteRegEx": "Pan and Chen", "year": 2013}, {"title": "One-class collaborative filtering", "author": ["R. Pan", "Y. Zhou", "B. Cao", "N.N. Liu", "R. Lukose", "M. Scholz", "Q. Yang"], "venue": "ICDM.", "citeRegEx": "Pan et al\\.,? 2008", "shortCiteRegEx": "Pan et al\\.", "year": 2008}, {"title": "Combining heterogenous social and geographical information for event recommendation", "author": ["Z. Qiao", "P. Zhang", "Y. Cao", "C. Zhou", "L. Guo", "B. Fang"], "venue": "AAAI.", "citeRegEx": "Qiao et al\\.,? 2014", "shortCiteRegEx": "Qiao et al\\.", "year": 2014}, {"title": "CNN features off-the-shelf: An astounding baseline for recognition", "author": ["A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "CVPR Workshops.", "citeRegEx": "Razavian et al\\.,? 2014", "shortCiteRegEx": "Razavian et al\\.", "year": 2014}, {"title": "Bpr: Bayesian personalized ranking from implicit feedback", "author": ["S. Rendle", "C. Freudenthaler", "Z. Gantner", "L. SchmidtThieme"], "venue": "UAI.", "citeRegEx": "Rendle et al\\.,? 2009", "shortCiteRegEx": "Rendle et al\\.", "year": 2009}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M.S. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "CoRR.", "citeRegEx": "Russakovsky et al\\.,? 2014", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2014}, {"title": "Methods and metrics for cold-start recommendations", "author": ["A. Schein", "A. Popescul", "L. Ungar", "D. Pennock"], "venue": "SIGIR.", "citeRegEx": "Schein et al\\.,? 2002", "shortCiteRegEx": "Schein et al\\.", "year": 2002}, {"title": "Neuroaesthetics in fashion: Modeling the perception of fashionability", "author": ["E. Simo-Serra", "S. Fidler", "F. Moreno-Noguer", "R. Urtasun"], "venue": "CVPR.", "citeRegEx": "Simo.Serra et al\\.,? 2014", "shortCiteRegEx": "Simo.Serra et al\\.", "year": 2014}, {"title": "Semrec: A semantic enhancement framework for tag based recommendation", "author": ["G. Xu", "Y. Gu", "P. Dolog", "Y. Zhang", "M. Kitsuregawa"], "venue": "AAAI.", "citeRegEx": "Xu et al\\.,? 2011", "shortCiteRegEx": "Xu et al\\.", "year": 2011}, {"title": "Beyond clicks: dwell time for personalization", "author": ["X. Yi", "L. Hong", "E. Zhong", "N.N. Liu", "S. Rajan"], "venue": "RecSys.", "citeRegEx": "Yi et al\\.,? 2014", "shortCiteRegEx": "Yi et al\\.", "year": 2014}, {"title": "Leveraging social connections to improve personalized ranking for collaborative filtering", "author": ["T. Zhao", "J. McAuley", "I. King"], "venue": "CIKM.", "citeRegEx": "Zhao et al\\.,? 2014", "shortCiteRegEx": "Zhao et al\\.", "year": 2014}, {"title": "Userrec: A user recommendation framework in social tagging systems", "author": ["T.C. Zhou", "H. Ma", "M.R. Lyu", "I. King"], "venue": "AAAI.", "citeRegEx": "Zhou et al\\.,? 2010", "shortCiteRegEx": "Zhou et al\\.", "year": 2010}, {"title": "Social recommendation using low-rank semidefinite program", "author": ["J. Zhu", "H. Ma", "C. Chen", "J. Bu"], "venue": "AAAI.", "citeRegEx": "Zhu et al\\.,? 2011", "shortCiteRegEx": "Zhu et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 2, "context": "Such systems play a central role in helping people discover items of personal interest from huge corpora, ranging from movies and music (Bennett and Lanning, 2007; Koenigstein, Dror, and Koren, 2011), to research articles, news and books (Das et al.", "startOffset": 136, "endOffset": 199}, {"referenceID": 4, "context": "Such systems play a central role in helping people discover items of personal interest from huge corpora, ranging from movies and music (Bennett and Lanning, 2007; Koenigstein, Dror, and Koren, 2011), to research articles, news and books (Das et al., 2007; Lu et al., 2015), to tags and even other users (Xu et al.", "startOffset": 238, "endOffset": 273}, {"referenceID": 18, "context": "Such systems play a central role in helping people discover items of personal interest from huge corpora, ranging from movies and music (Bennett and Lanning, 2007; Koenigstein, Dror, and Koren, 2011), to research articles, news and books (Das et al., 2007; Lu et al., 2015), to tags and even other users (Xu et al.", "startOffset": 238, "endOffset": 273}, {"referenceID": 28, "context": ", 2015), to tags and even other users (Xu et al., 2011; Zhou et al., 2010; Zhu et al., 2011).", "startOffset": 38, "endOffset": 92}, {"referenceID": 31, "context": ", 2015), to tags and even other users (Xu et al., 2011; Zhou et al., 2010; Zhu et al., 2011).", "startOffset": 38, "endOffset": 92}, {"referenceID": 32, "context": ", 2015), to tags and even other users (Xu et al., 2011; Zhou et al., 2010; Zhu et al., 2011).", "startOffset": 38, "endOffset": 92}, {"referenceID": 29, "context": "(Yi et al., 2014).", "startOffset": 0, "endOffset": 17}, {"referenceID": 21, "context": "been proposed to uncover the most relevant latent dimensions in both explicit and implicit feedback settings (Bell, Koren, and Volinsky, 2007; Hu, Koren, and Volinsky, 2008; Pan et al., 2008; Rendle et al., 2009).", "startOffset": 109, "endOffset": 212}, {"referenceID": 24, "context": "been proposed to uncover the most relevant latent dimensions in both explicit and implicit feedback settings (Bell, Koren, and Volinsky, 2007; Hu, Koren, and Volinsky, 2008; Pan et al., 2008; Rendle et al., 2009).", "startOffset": 109, "endOffset": 212}, {"referenceID": 26, "context": "Although a variety of sources of data have been used to build hybrid models to make cold start or context-aware recommendations (Schein et al., 2002), from text (Bao, Fang, and Zhang, 2014), to a user\u2019s physical location (Qiao et al.", "startOffset": 128, "endOffset": 149}, {"referenceID": 22, "context": ", 2002), from text (Bao, Fang, and Zhang, 2014), to a user\u2019s physical location (Qiao et al., 2014), to the season or temperature (Brown, Bovey, and Chen, 1997), here we are interested in incorporating the visual appearance of the items into the preference predictor, a source of data which is typically neglected by existing RSs.", "startOffset": 79, "endOffset": 98}, {"referenceID": 9, "context": "Methodologically we model visual aspects of items by using representations of product images derived from a (pre-trained) deep network (Jia et al., 2014), on top of which we fit an additional layer that uncovers both visual and latent dimensions that are relevant to users\u2019 opinions.", "startOffset": 135, "endOffset": 153}, {"referenceID": 16, "context": "More recently BPR-MF has been extended to accommodate both users\u2019 feedback and their social relations (Krohn-Grimberghe et al., 2012; Pan and Chen, 2013; Zhao, McAuley, and King, 2014).", "startOffset": 102, "endOffset": 184}, {"referenceID": 20, "context": "More recently BPR-MF has been extended to accommodate both users\u2019 feedback and their social relations (Krohn-Grimberghe et al., 2012; Pan and Chen, 2013; Zhao, McAuley, and King, 2014).", "startOffset": 102, "endOffset": 184}, {"referenceID": 11, "context": "Others have developed content-based and hybrid models that make use of a variety of information sources, including text (and context), taxonomies, and user demographics (Bao, Fang, and Zhang, 2014; Kanagal et al., 2012; Lu et al., 2015; Qiao et al., 2014).", "startOffset": 169, "endOffset": 255}, {"referenceID": 18, "context": "Others have developed content-based and hybrid models that make use of a variety of information sources, including text (and context), taxonomies, and user demographics (Bao, Fang, and Zhang, 2014; Kanagal et al., 2012; Lu et al., 2015; Qiao et al., 2014).", "startOffset": 169, "endOffset": 255}, {"referenceID": 22, "context": "Others have developed content-based and hybrid models that make use of a variety of information sources, including text (and context), taxonomies, and user demographics (Bao, Fang, and Zhang, 2014; Kanagal et al., 2012; Lu et al., 2015; Qiao et al., 2014).", "startOffset": 169, "endOffset": 255}, {"referenceID": 25, "context": "Recently, high-level visual features from Deep Convolutional Neural Networks (\u2018Deep CNNs\u2019) have seen successes in tasks like object detection (Russakovsky et al., 2014), photographic style annotations (Karayev et al.", "startOffset": 142, "endOffset": 168}, {"referenceID": 12, "context": ", 2014), photographic style annotations (Karayev et al., 2014), and aesthetic quality categorization (Lu et al.", "startOffset": 40, "endOffset": 62}, {"referenceID": 17, "context": ", 2014), and aesthetic quality categorization (Lu et al., 2014), among others.", "startOffset": 46, "endOffset": 63}, {"referenceID": 5, "context": "ImageNet) can be generalized to extract CNN features for other datasets, and outperform stateof-the-art approaches on these new datasets for different visual tasks (Donahue et al., 2014; Razavian et al., 2014).", "startOffset": 164, "endOffset": 209}, {"referenceID": 23, "context": "ImageNet) can be generalized to extract CNN features for other datasets, and outperform stateof-the-art approaches on these new datasets for different visual tasks (Donahue et al., 2014; Razavian et al., 2014).", "startOffset": 164, "endOffset": 209}, {"referenceID": 2, "context": "Bell, Koren, and Volinsky (2007); Bennett and Lanning (2007); Rendle et al.", "startOffset": 34, "endOffset": 61}, {"referenceID": 2, "context": "Bell, Koren, and Volinsky (2007); Bennett and Lanning (2007); Rendle et al. (2009)).", "startOffset": 34, "endOffset": 83}, {"referenceID": 2, "context": "Bell, Koren, and Volinsky (2007); Bennett and Lanning (2007); Rendle et al. (2009)). When it comes to personalized ranking from implicit feedback, traditional MF approaches are challenged by the ambiguity of interpreting \u2018non-observed\u2019 feedback. In recent years, point-wise and pairwise methods have been successful at adapting MF to address such challenges. Point-wise methods assume non-observed feedback to be inherently negative to some degree. They approximate the task with regression which for each user-item pair predicts its affinity score and then ranks items accordingly. Hu, Koren, and Volinsky (2008) associate different \u2018confidence levels\u2019 to positive and non-observed feedback and then factorize the resulting weighted matrix, while Pan et al.", "startOffset": 34, "endOffset": 614}, {"referenceID": 2, "context": "Bell, Koren, and Volinsky (2007); Bennett and Lanning (2007); Rendle et al. (2009)). When it comes to personalized ranking from implicit feedback, traditional MF approaches are challenged by the ambiguity of interpreting \u2018non-observed\u2019 feedback. In recent years, point-wise and pairwise methods have been successful at adapting MF to address such challenges. Point-wise methods assume non-observed feedback to be inherently negative to some degree. They approximate the task with regression which for each user-item pair predicts its affinity score and then ranks items accordingly. Hu, Koren, and Volinsky (2008) associate different \u2018confidence levels\u2019 to positive and non-observed feedback and then factorize the resulting weighted matrix, while Pan et al. (2008) sample non-observed feedback as negative instances and factorize a similar weighted matrix.", "startOffset": 34, "endOffset": 766}, {"referenceID": 2, "context": "Bell, Koren, and Volinsky (2007); Bennett and Lanning (2007); Rendle et al. (2009)). When it comes to personalized ranking from implicit feedback, traditional MF approaches are challenged by the ambiguity of interpreting \u2018non-observed\u2019 feedback. In recent years, point-wise and pairwise methods have been successful at adapting MF to address such challenges. Point-wise methods assume non-observed feedback to be inherently negative to some degree. They approximate the task with regression which for each user-item pair predicts its affinity score and then ranks items accordingly. Hu, Koren, and Volinsky (2008) associate different \u2018confidence levels\u2019 to positive and non-observed feedback and then factorize the resulting weighted matrix, while Pan et al. (2008) sample non-observed feedback as negative instances and factorize a similar weighted matrix. In contrast to point-wise methods, pairwise methods are based on a weaker but possibly more realistic assumption that positive feedback must only be \u2018more preferable\u2019 than non-observed feedback. Such methods directly optimize the ranking of the feedback and are to our knowledge state-of-the-art for implicit feedback datasets. Rendle et al. (2009) propose a generalized Bayesian Personalized Ranking (BPR) framework and experimentally show that BPRMF (i.", "startOffset": 34, "endOffset": 1207}, {"referenceID": 2, "context": "Bell, Koren, and Volinsky (2007); Bennett and Lanning (2007); Rendle et al. (2009)). When it comes to personalized ranking from implicit feedback, traditional MF approaches are challenged by the ambiguity of interpreting \u2018non-observed\u2019 feedback. In recent years, point-wise and pairwise methods have been successful at adapting MF to address such challenges. Point-wise methods assume non-observed feedback to be inherently negative to some degree. They approximate the task with regression which for each user-item pair predicts its affinity score and then ranks items accordingly. Hu, Koren, and Volinsky (2008) associate different \u2018confidence levels\u2019 to positive and non-observed feedback and then factorize the resulting weighted matrix, while Pan et al. (2008) sample non-observed feedback as negative instances and factorize a similar weighted matrix. In contrast to point-wise methods, pairwise methods are based on a weaker but possibly more realistic assumption that positive feedback must only be \u2018more preferable\u2019 than non-observed feedback. Such methods directly optimize the ranking of the feedback and are to our knowledge state-of-the-art for implicit feedback datasets. Rendle et al. (2009) propose a generalized Bayesian Personalized Ranking (BPR) framework and experimentally show that BPRMF (i.e., with MF as the underlying predictor) outperforms a variety of competitive baselines. More recently BPR-MF has been extended to accommodate both users\u2019 feedback and their social relations (Krohn-Grimberghe et al., 2012; Pan and Chen, 2013; Zhao, McAuley, and King, 2014). Our goal here is complementary as we aim to incorporate visual signals into BPR-MF, which presents a quite different set of challenges compared with other sources of data. Others have developed content-based and hybrid models that make use of a variety of information sources, including text (and context), taxonomies, and user demographics (Bao, Fang, and Zhang, 2014; Kanagal et al., 2012; Lu et al., 2015; Qiao et al., 2014). However, to our knowledge none of these works have incorporated visual signals into models of users\u2019 preferences and uncover visual dimensions as we do here. Exploiting visual signals for the purpose of \u2018in-style\u2019 image retrieval has been previously proposed. For example, Simo-Serra et al. (2014) predict the fashionability of a person in a photograph and suggest subtle improvements.", "startOffset": 34, "endOffset": 2315}, {"referenceID": 2, "context": "Bell, Koren, and Volinsky (2007); Bennett and Lanning (2007); Rendle et al. (2009)). When it comes to personalized ranking from implicit feedback, traditional MF approaches are challenged by the ambiguity of interpreting \u2018non-observed\u2019 feedback. In recent years, point-wise and pairwise methods have been successful at adapting MF to address such challenges. Point-wise methods assume non-observed feedback to be inherently negative to some degree. They approximate the task with regression which for each user-item pair predicts its affinity score and then ranks items accordingly. Hu, Koren, and Volinsky (2008) associate different \u2018confidence levels\u2019 to positive and non-observed feedback and then factorize the resulting weighted matrix, while Pan et al. (2008) sample non-observed feedback as negative instances and factorize a similar weighted matrix. In contrast to point-wise methods, pairwise methods are based on a weaker but possibly more realistic assumption that positive feedback must only be \u2018more preferable\u2019 than non-observed feedback. Such methods directly optimize the ranking of the feedback and are to our knowledge state-of-the-art for implicit feedback datasets. Rendle et al. (2009) propose a generalized Bayesian Personalized Ranking (BPR) framework and experimentally show that BPRMF (i.e., with MF as the underlying predictor) outperforms a variety of competitive baselines. More recently BPR-MF has been extended to accommodate both users\u2019 feedback and their social relations (Krohn-Grimberghe et al., 2012; Pan and Chen, 2013; Zhao, McAuley, and King, 2014). Our goal here is complementary as we aim to incorporate visual signals into BPR-MF, which presents a quite different set of challenges compared with other sources of data. Others have developed content-based and hybrid models that make use of a variety of information sources, including text (and context), taxonomies, and user demographics (Bao, Fang, and Zhang, 2014; Kanagal et al., 2012; Lu et al., 2015; Qiao et al., 2014). However, to our knowledge none of these works have incorporated visual signals into models of users\u2019 preferences and uncover visual dimensions as we do here. Exploiting visual signals for the purpose of \u2018in-style\u2019 image retrieval has been previously proposed. For example, Simo-Serra et al. (2014) predict the fashionability of a person in a photograph and suggest subtle improvements. Jagadeesh et al. (2014) use a street fashion dataset with detailed annotations to identify accessories whose style is consistent with a picture.", "startOffset": 34, "endOffset": 2427}, {"referenceID": 2, "context": "Bell, Koren, and Volinsky (2007); Bennett and Lanning (2007); Rendle et al. (2009)). When it comes to personalized ranking from implicit feedback, traditional MF approaches are challenged by the ambiguity of interpreting \u2018non-observed\u2019 feedback. In recent years, point-wise and pairwise methods have been successful at adapting MF to address such challenges. Point-wise methods assume non-observed feedback to be inherently negative to some degree. They approximate the task with regression which for each user-item pair predicts its affinity score and then ranks items accordingly. Hu, Koren, and Volinsky (2008) associate different \u2018confidence levels\u2019 to positive and non-observed feedback and then factorize the resulting weighted matrix, while Pan et al. (2008) sample non-observed feedback as negative instances and factorize a similar weighted matrix. In contrast to point-wise methods, pairwise methods are based on a weaker but possibly more realistic assumption that positive feedback must only be \u2018more preferable\u2019 than non-observed feedback. Such methods directly optimize the ranking of the feedback and are to our knowledge state-of-the-art for implicit feedback datasets. Rendle et al. (2009) propose a generalized Bayesian Personalized Ranking (BPR) framework and experimentally show that BPRMF (i.e., with MF as the underlying predictor) outperforms a variety of competitive baselines. More recently BPR-MF has been extended to accommodate both users\u2019 feedback and their social relations (Krohn-Grimberghe et al., 2012; Pan and Chen, 2013; Zhao, McAuley, and King, 2014). Our goal here is complementary as we aim to incorporate visual signals into BPR-MF, which presents a quite different set of challenges compared with other sources of data. Others have developed content-based and hybrid models that make use of a variety of information sources, including text (and context), taxonomies, and user demographics (Bao, Fang, and Zhang, 2014; Kanagal et al., 2012; Lu et al., 2015; Qiao et al., 2014). However, to our knowledge none of these works have incorporated visual signals into models of users\u2019 preferences and uncover visual dimensions as we do here. Exploiting visual signals for the purpose of \u2018in-style\u2019 image retrieval has been previously proposed. For example, Simo-Serra et al. (2014) predict the fashionability of a person in a photograph and suggest subtle improvements. Jagadeesh et al. (2014) use a street fashion dataset with detailed annotations to identify accessories whose style is consistent with a picture. Another method was proposed by Kalantidis, Kennedy, and Li (2013), which accepts a query image and uses segmentation to detect clothing classes before retrieving visually similar products from each of the detected classes.", "startOffset": 34, "endOffset": 2614}, {"referenceID": 2, "context": "Bell, Koren, and Volinsky (2007); Bennett and Lanning (2007); Rendle et al. (2009)). When it comes to personalized ranking from implicit feedback, traditional MF approaches are challenged by the ambiguity of interpreting \u2018non-observed\u2019 feedback. In recent years, point-wise and pairwise methods have been successful at adapting MF to address such challenges. Point-wise methods assume non-observed feedback to be inherently negative to some degree. They approximate the task with regression which for each user-item pair predicts its affinity score and then ranks items accordingly. Hu, Koren, and Volinsky (2008) associate different \u2018confidence levels\u2019 to positive and non-observed feedback and then factorize the resulting weighted matrix, while Pan et al. (2008) sample non-observed feedback as negative instances and factorize a similar weighted matrix. In contrast to point-wise methods, pairwise methods are based on a weaker but possibly more realistic assumption that positive feedback must only be \u2018more preferable\u2019 than non-observed feedback. Such methods directly optimize the ranking of the feedback and are to our knowledge state-of-the-art for implicit feedback datasets. Rendle et al. (2009) propose a generalized Bayesian Personalized Ranking (BPR) framework and experimentally show that BPRMF (i.e., with MF as the underlying predictor) outperforms a variety of competitive baselines. More recently BPR-MF has been extended to accommodate both users\u2019 feedback and their social relations (Krohn-Grimberghe et al., 2012; Pan and Chen, 2013; Zhao, McAuley, and King, 2014). Our goal here is complementary as we aim to incorporate visual signals into BPR-MF, which presents a quite different set of challenges compared with other sources of data. Others have developed content-based and hybrid models that make use of a variety of information sources, including text (and context), taxonomies, and user demographics (Bao, Fang, and Zhang, 2014; Kanagal et al., 2012; Lu et al., 2015; Qiao et al., 2014). However, to our knowledge none of these works have incorporated visual signals into models of users\u2019 preferences and uncover visual dimensions as we do here. Exploiting visual signals for the purpose of \u2018in-style\u2019 image retrieval has been previously proposed. For example, Simo-Serra et al. (2014) predict the fashionability of a person in a photograph and suggest subtle improvements. Jagadeesh et al. (2014) use a street fashion dataset with detailed annotations to identify accessories whose style is consistent with a picture. Another method was proposed by Kalantidis, Kennedy, and Li (2013), which accepts a query image and uses segmentation to detect clothing classes before retrieving visually similar products from each of the detected classes. McAuley et al. (2015) use visual features extracted from CNNs and learn a visual similarity metric to identify visually complementary items to a query image.", "startOffset": 34, "endOffset": 2793}, {"referenceID": 14, "context": "Our preference predictor is built on top of Matrix Factorization (MF), which is state-of-the-art for rating prediction as well as modeling implicit feedback, whose basic formulation assumes the following model to predict the preference of a user u toward an item i (Koren and Bell, 2011):", "startOffset": 265, "endOffset": 287}, {"referenceID": 24, "context": "Following the notation in Rendle et al. (2009), \u0398 is the parameter vector and x\u0302uij(\u0398) denotes an arbitrary function of \u0398 that parameterises the relationship between the components of the triple (u, i, j).", "startOffset": 26, "endOffset": 47}, {"referenceID": 19, "context": "com introduced by McAuley et al. (2015). We consider two large categories where visual features have already been demonstrated to be meaningful, namely Women\u2019s and Men\u2019s Clothing.", "startOffset": 18, "endOffset": 40}, {"referenceID": 9, "context": "For each item i in the above datasets, we collect one product image and extract visual features fi using the Caffe reference model (Jia et al., 2014), which implements the CNN architecture proposed by Krizhevsky, Sutskever, and Hinton (2012).", "startOffset": 131, "endOffset": 149}, {"referenceID": 9, "context": "For each item i in the above datasets, we collect one product image and extract visual features fi using the Caffe reference model (Jia et al., 2014), which implements the CNN architecture proposed by Krizhevsky, Sutskever, and Hinton (2012). The architecture has 5 convolutional layers followed by 3 fully-connected layers, and has been pre-trained on 1.", "startOffset": 132, "endOffset": 242}, {"referenceID": 6, "context": "\u2022 MM-MF: A pairwise MF model from Gantner et al. (2011), which is optimized for a hinge ranking loss on xuij and trained using SGA as in BPR-MF.", "startOffset": 34, "endOffset": 56}, {"referenceID": 6, "context": "\u2022 MM-MF: A pairwise MF model from Gantner et al. (2011), which is optimized for a hinge ranking loss on xuij and trained using SGA as in BPR-MF. \u2022 BPR-MF: This pairwise method was introduced by Rendle et al. (2009) and is the state-of-the-art of personalized ranking for implicit feedback datasets.", "startOffset": 34, "endOffset": 215}, {"referenceID": 19, "context": "\u2022 Image-based Recommendation (IBR): Introduced by McAuley et al. (2015), it learns a visual space and retrieves stylistically similar items to a query image.", "startOffset": 50, "endOffset": 72}, {"referenceID": 6, "context": "Most baselines are from MyMediaLite (Gantner et al., 2011).", "startOffset": 36, "endOffset": 58}], "year": 2015, "abstractText": "Modern recommender systems model people and items by discovering or \u2018teasing apart\u2019 the underlying dimensions that encode the properties of items and users\u2019 preferences toward them. Critically, such dimensions are uncovered based on user feedback, often in implicit form (such as purchase histories, browsing logs, etc.); in addition, some recommender systems make use of side information, such as product attributes, temporal information, or review text. However one important feature that is typically ignored by existing personalized recommendation and ranking methods is the visual appearance of the items being considered. In this paper we propose a scalable factorization model to incorporate visual signals into predictors of people\u2019s opinions, which we apply to a selection of large, real-world datasets. We make use of visual features extracted from product images using (pre-trained) deep networks, on top of which we learn an additional layer that uncovers the visual dimensions that best explain the variation in people\u2019s feedback. This not only leads to significantly more accurate personalized ranking methods, but also helps to alleviate cold start issues, and qualitatively to analyze the visual dimensions that influence people\u2019s opinions.", "creator": "LaTeX with hyperref package"}}}