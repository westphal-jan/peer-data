{"id": "1510.02125", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Oct-2015", "title": "Resolving References to Objects in Photographs using the Words-As-Classifiers Model", "abstract": "The recently introduced \"words as classifiers\" model of grounded semantics (Kennington &amp; Schlangen 2015) views words as classifiers on (perceptual) contexts, and composes the meaning of a phrase through 'soft' intersection of the denotations of its component words. The model is trained from instances of referential language use, and was first evaluated with references in a game-playing scenario with a small number of different types of objects. Here, we apply this model to a large set of real-world photographs (SAIAPR TC-12, (Escalante et al. 2010)) that contain objects with a much larger variety of types, and we show that it achieves good performance in a reference resolution task on this data set. We also extend the model to deal with quantification and negation, and evaluate these extensions, with good results. To investigate what the classifiers learn, we introduce 'intensional' and 'denotational' word vectors, and show that they capture meaning similarity in a way that is different from and complementary to word2vec word embeddings.", "histories": [["v1", "Wed, 7 Oct 2015 20:52:22 GMT  (1146kb,D)", "http://arxiv.org/abs/1510.02125v1", "13 pages"], ["v2", "Mon, 21 Mar 2016 00:33:40 GMT  (1747kb,D)", "http://arxiv.org/abs/1510.02125v2", "10 pages; substantial rewrite; added dataset; different feature extraction method; revised results"], ["v3", "Fri, 3 Jun 2016 11:53:31 GMT  (1749kb,D)", "http://arxiv.org/abs/1510.02125v3", "11 pages; as in Proceedings of ACL 2016, Berlin, 2016"]], "COMMENTS": "13 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["david schlangen", "sina zarrie\u00df", "casey kennington"], "accepted": true, "id": "1510.02125"}, "pdf": {"name": "1510.02125.pdf", "metadata": {"source": "CRF", "title": "Resolving References to Objects in Photographs using the Words-As-Classifiers Model", "authors": ["David Schlangen", "Sina Zarrie\u00df", "Casey Kennington"], "emails": ["first.last@uni-bielefeld.de"], "sections": [{"heading": "1 Introduction", "text": "Wittgenstein's (1953) famous dictum \"meaning is use\" (PU 43) is nowadays often seen as a kind of implicit endorsement of corpus-based methods in semantics, and is often quoted in the same paragraph as Firth's (1957) equally famous dictum \"You will know a word from the company that keeps it\" (e.g. in (Turney and Pantel, 2010; Clarke, 2011; Baroni et al., 2013). However, making their implicit connection between these two assertions requires a bit of a conceptual leap, since words are not typically used to support each other, but rather because the person who uses them wants to achieve something. To do so, the linguistic context of a word only testifies indirectly. 1Common linguistic usage is to refer to something. This use is relatively simple to capture along with language, for example, linking the mentions of beings in the text to unique objects in a database."}, {"heading": "2 Related Work", "text": "The first approach is often referred to as \"grounded semantics\" and deals with learning to execute natural language commands, typically in the context of robotics. Perhaps because of this background, the work here often follows a two-step approach, first creating (or giving) a worldwide representation, followed by grounding. For example, Tellex et al. (2011) learn to map commands given to a robot to be executed by it. Commands are based in the sense that their components are mapped to identifiers of objects and trajectories, but the world is symbolically represented by object names and properties, not, as in the experiments described here, derived from computer vision. It is from this area (Krishnamurthy and Kollar, 2013) that we may be closest to our concerns. This approach also combines formal semantics with classifiers working on images."}, {"heading": "3 The \u201cWords-As-Classifiers\u201d Model", "text": "The model of reference that we are using here has the following subcomponents: 33The model was introduced in (Kennington and Snakes, 2015); we are reformulating and expanding it here. The general idea of modeling words as classifiers on perceptual input goes back to (Harnard, 1990) and was most recently taken from a formal perspective (Larsson, 2015).A model of word meanings, where these are (for certain types of words) classifiers on (visual representations of) candidates, the appropriation of judgments (scores) for each candidate, and, by applying them to all candidates, leads to denotations that are in a particular context of words."}, {"heading": "4 Data: Images & Referring Expressions", "text": "We benefited from the availability of a rich corpus built successively through various efforts, based on the IAPR TC-12 Image Retrieeval Benchmark Collection of \"20,000 Still Natural Images taken from locations around the world and consisting of a sorted cross section of Still Natural Images\" (Grubinger et al., 2006), a typical example of an image from the collection can be seen in Figure 2 on the left. This dataset was later created by Escalante et al. (2010) with (among other things) segmentation masks identifying objects in the images (an average of 5 objects per image). Figure 2 (center) gives an example of such segmentation. These segmentations were performed manually and provide tight masking of the objects (and not just rectangular Bounding Boxes). This dataset, also known as \"SAIAPR TC-12\" (for \"segmented and commented IAPR-C12\"), is a vector visual characteristic that is determined by each region being selected from two regions."}, {"heading": "5 Training the Word Classifiers", "text": "In fact, it is so. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. Most people who stand up for the rights of women and men take themselves and their rights and their rights. (...) It is not. (...) It is. They have the same rights and their rights. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (... It is. It is. (...) It is. (... It is. It is. (...) It is. (... It is."}, {"heading": "6 Exp. 1: References to Single Objects", "text": "The task is the following: In the face of an image (of SAIAPR, i.e. with objects that are segmented and represented.5We point out here in passing that a nice consequence of this type of training data creation is that this can in principle work incrementally and online, with each instance of a successful reference value (e.g. a teacher showing an object to a learner; that is, the speaker is also pointed out by other means than verbal) providing both positive and (indirectly) negative evidence that can be used to update the classifiers for the words used, and no recollection of all previous episodes is required. However, we leave the exploration of possible links with the literature on language acquisition to future work. 6For logistical regression as well as for SVM, we used the scikit learning package (Pedregosa et al., 2011).by feature vectors) and an expression (of REFERITGAME) that refers to one of these objects that we are predicting the object."}, {"heading": "6.1 Applying the Model", "text": "To test the model against a given reference expression from the test set, we proceed as described above (Section 3): We test every (known) word from the reference expression on all objects from the image to which it belongs, and obtain a vector of the adequacy assessments (technically, the probability that the classifier will return for that object belongs to the class of the word). We add these vectors and normalize the length, resulting in a vector that specifies the values for all objects, and the intended reference is then identified as the Argmax of that distribution. (See Appendix for a formal description of this process.) Unknown words (for which there are no classifiers) are simply ignored and do not contribute to the distribution; if no words are known, a speaker is randomly selected from the group of candidates."}, {"heading": "6.2 Results", "text": "Figure 2 shows the results of this experiment (over 10 folds of cross-validation; the variations between the folds are so small that we are only showing the average here) in terms of accuracy and mean mutual rank (MRR) and increasingly restrictive selections of the test corpus. The first line shows the results over the entire test set, which is already well above the random baseline (20%) at 59%; the second line shows results when removing expressions for which the model has just guessed because no words were known, this increases the accuracy by one percentage point. More interesting are the other limitations. When expressions containing relational expressions are removed (as they were from the training corpus), the model shows how they cannot be correctly resolved."}, {"heading": "7 Experiment 2: Negation and Quantifiers", "text": "This year it has come to the point where it will be able to retaliate, to retrench, \"he says.\" We, \"he says,\" are able to retrench, \"he says.\" We, \"he says,\" will be able to retrench. \"\" We, \"he says,\" will be able to retrench. \"\" We, \"he says,\" will be able to retrench. \""}, {"heading": "8 Analysis: What do the Models Learn?", "text": "This year it will be able to reetec the aforementioned brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainteeSe."}, {"heading": "9 Conclusions", "text": "In fact, it is not that we are able to position ourselves in a different way, as we have done in the past, namely by doing in the past what we have done in the past, namely by doing in the past what we have done. (...) We have not managed to stay in the present. (...) We have not managed to survive in the present. (...) We have not managed to survive in the present. (...) We have not managed to survive in the present. (...) (...) We have not managed to survive in the future. (...) \"(...) We have not managed to survive in the present. (...)\" We have managed to survive in the present. \"(...)\" We have succeeded. \"(...) We have succeeded.\" (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (... \"We have succeeded. (...) We have succeeded. (...\" We have succeeded. () We have succeeded. (... \"We have succeeded. () We have succeeded. (...\" We have succeeded. () We have succeeded. (... \"We have succeeded. (...\" We have succeeded. () We have succeeded. (... \"We have succeeded. (...\" We have succeeded. (... \"We have succeeded. () We have succeeded. (...\" We have succeeded. () We have succeeded. () We have succeeded. (.... (.... (.... () We have succeeded. (.... () We have succeeded. (. () We have succeeded. (... We have succeeded.) We have succeeded. (.... (. (.... (.... (.) We have succeeded.) We have succeeded.) We have. (.... (. (. (.... (. ("}], "references": [{"title": "Frege in space: A program for compositional distributional semantics", "author": ["Marco Baroni", "Raffaella Bernardi", "Roberto Zamparelli."], "venue": "Linguistic Issues in Language Technologies (LiLT), 9(6):5\u2013110.", "citeRegEx": "Baroni et al\\.,? 2013", "shortCiteRegEx": "Baroni et al\\.", "year": 2013}, {"title": "Multimodal distributional semantics", "author": ["Elia Bruni", "Nam Khanh Tran", "Marco Baroni."], "venue": "Journal of Artificial Intelligence Research, 49:1\u201347.", "citeRegEx": "Bruni et al\\.,? 2014", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "A Context-theoretic Framework for Compositionality in Distributional Semantics", "author": ["Daoud Clarke."], "venue": "Computational Linguistics, 38(1):28\u201348.", "citeRegEx": "Clarke.,? 2011", "shortCiteRegEx": "Clarke.", "year": 2011}, {"title": "Language models for image captioning: The quirks and what works", "author": ["Jacob Devlin", "Hao Cheng", "Hao Fang", "Saurabh Gupta", "Li Deng", "Xiaodong He", "Geoffrey Zweig", "Margaret Mitchell."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Com-", "citeRegEx": "Devlin et al\\.,? 2015", "shortCiteRegEx": "Devlin et al\\.", "year": 2015}, {"title": "The segmented and annotated IAPR TC-12", "author": ["Hugo Jair Escalante", "Carlos a. Hern\u00e1ndez", "Jesus a. Gonzalez", "a. L\u00f3pez-L\u00f3pez", "Manuel Montes", "Eduardo F. Morales", "L. Enrique Sucar", "Luis Villase\u00f1or", "Michael Grubinger"], "venue": null, "citeRegEx": "Escalante et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Escalante et al\\.", "year": 2010}, {"title": "From captions to visual concepts and back", "author": ["Hao Fang", "Saurabh Gupta", "Forrest Iandola", "Rupesh Srivastava", "Li Deng", "Piotr Dollar", "Jianfeng Gao", "Xiaodong He", "Margaret Mitchell", "John Platt", "Lawrence Zitnick", "Geoffrey Zweig."], "venue": "Proceedings of CVPR,", "citeRegEx": "Fang et al\\.,? 2015", "shortCiteRegEx": "Fang et al\\.", "year": 2015}, {"title": "Papers in Linguistics, 1934\u20131951", "author": ["John R. Firth."], "venue": "Oxford University Press, Oxford, UK.", "citeRegEx": "Firth.,? 1957", "shortCiteRegEx": "Firth.", "year": 1957}, {"title": "Facc1: Freebase annotation", "author": ["Evgeniy Gabrilovich", "Michael Ringgaard", "Amarnag Subramanya"], "venue": null, "citeRegEx": "Gabrilovich et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gabrilovich et al\\.", "year": 2013}, {"title": "Logic, Language and Meaning: Intensional Logic and Logical Grammar, volume 2", "author": ["L.T.F. Gamut."], "venue": "Chicago University Press, Chicago.", "citeRegEx": "Gamut.,? 1991", "shortCiteRegEx": "Gamut.", "year": 1991}, {"title": "The IAPR TC-12 benchmark: a new evaluation resource for visual information systems", "author": ["Michael Grubinger", "Paul Clough", "Henning M\u00fcller", "Thomas Deselaers."], "venue": "Proceedings of the International Conference on Language Resources and Evaluation (LREC", "citeRegEx": "Grubinger et al\\.,? 2006", "shortCiteRegEx": "Grubinger et al\\.", "year": 2006}, {"title": "The symbol grounding problem", "author": ["Stevan Harnard."], "venue": "Physica D, 42:335\u2013346.", "citeRegEx": "Harnard.,? 1990", "shortCiteRegEx": "Harnard.", "year": 1990}, {"title": "ReferItGame: Referring to Objects in Photographs of Natural Scenes", "author": ["Sahar Kazemzadeh", "Vicente Ordonez", "Mark Matten", "Tamara L Berg."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2014), pages 787\u2013", "citeRegEx": "Kazemzadeh et al\\.,? 2014", "shortCiteRegEx": "Kazemzadeh et al\\.", "year": 2014}, {"title": "Simple learning and compositional application of perceptually grounded word meanings for incremental reference resolution", "author": ["Casey Kennington", "David Schlangen."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Kennington and Schlangen.,? 2015", "shortCiteRegEx": "Kennington and Schlangen.", "year": 2015}, {"title": "Jointly learning to parse and perceive: Connecting natural language to the physical world", "author": ["Jayant Krishnamurthy", "Thomas Kollar."], "venue": "Transactions of the Association for Computational Linguistics, 1:193\u2013206.", "citeRegEx": "Krishnamurthy and Kollar.,? 2013", "shortCiteRegEx": "Krishnamurthy and Kollar.", "year": 2013}, {"title": "Formal semantics for perceptual classification", "author": ["Staffan Larsson."], "venue": "Journal of logic and computation, 25(2):335\u2013369.", "citeRegEx": "Larsson.,? 2015", "shortCiteRegEx": "Larsson.", "year": 2015}, {"title": "A Joint Model of Language and Perception for Grounded Attribute Learning", "author": ["Cynthia Matuszek", "Nicholas Fitzgerald", "Luke Zettlemoyer", "Liefeng Bo", "Dieter Fox."], "venue": "Proceedings of the International Conference on Machine Learning (ICML 2012).", "citeRegEx": "Matuszek et al\\.,? 2012", "shortCiteRegEx": "Matuszek et al\\.", "year": 2012}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean."], "venue": "Proceedings of NIPS 2013, pages 3111\u20133119, Lake Tahoe, Nevada, USA, December.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Composition in distributional models of semantics", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "Cognitive science, 34(8):1388\u2014-1429, November.", "citeRegEx": "Mitchell and Lapata.,? 2010", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2010}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": null, "citeRegEx": "Pedregosa et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pedregosa et al\\.", "year": 2011}, {"title": "Software Framework for Topic Modelling with Large Corpora", "author": ["Radim \u0158eh\u016f\u0159ek", "Petr Sojka."], "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45\u201350, Valletta, Malta, May. ELRA. http://is.muni.cz/", "citeRegEx": "\u0158eh\u016f\u0159ek and Sojka.,? 2010", "shortCiteRegEx": "\u0158eh\u016f\u0159ek and Sojka.", "year": 2010}, {"title": "Predicting the birth of a spoken word", "author": ["Brandon C Roy", "Michael C Frank", "Philip Decamp", "Matthew Miller", "Deb Roy."], "venue": "PNAS: Psychological and Cognitive Sciences, pages 1\u20136.", "citeRegEx": "Roy et al\\.,? 2015", "shortCiteRegEx": "Roy et al\\.", "year": 2015}, {"title": "Understanding Natural Language Commands for Robotic Navigation and Mobile Manipulation", "author": ["Stefanie Tellex", "Thomas Kollar", "Steven Dickerson", "Matthew R. Walter", "Ashis Gopal Banerjee", "Seth Teller", "Nicholas Roy."], "venue": "AAAI Conference on Artificial Intel-", "citeRegEx": "Tellex et al\\.,? 2011", "shortCiteRegEx": "Tellex et al\\.", "year": 2011}, {"title": "Formal Philosophy: Selected Papers of Richard Montague", "author": ["Richmond H. Thomason", "editor"], "venue": null, "citeRegEx": "Thomason and editor.,? \\Q1974\\E", "shortCiteRegEx": "Thomason and editor.", "year": 1974}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter D. Turney", "Patrick Pantel."], "venue": "Journal of Artificial Intelligence Research, 37:141\u2013 188.", "citeRegEx": "Turney and Pantel.,? 2010", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."], "venue": "Computer Vision and Pattern Recognition.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Tractatus Logicus Philosophicus und Philosophische Untersuchungen, volume 1 of Werkausgabe", "author": ["Ludwig Wittgenstein."], "venue": "Suhrkamp, Frankfurt am Main. this edition 1984.", "citeRegEx": "Wittgenstein.,? 1953", "shortCiteRegEx": "Wittgenstein.", "year": 1953}, {"title": "From Image Descriptions to Visual Denotations: New Similarity Metrics for Semantic Inference over Event Descriptions", "author": ["Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier."], "venue": "Transactions of the Association for Computational Linguistics (TACL),", "citeRegEx": "Young et al\\.,? 2014", "shortCiteRegEx": "Young et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 12, "context": "The recently introduced \u201cwords as classifiers\u201d model of grounded semantics (Kennington and Schlangen, 2015) views words as classifiers on (perceptual) contexts, and composes the meaning of a phrase through \u2018soft\u2019 intersection of the denotations of its component words.", "startOffset": 75, "endOffset": 107}, {"referenceID": 4, "context": "Here, we apply this model to a large set of real-world photographs (SAIAPR TC-12, (Escalante et al., 2010)) that contain objects with a much larger variety of types, and we show that it achieves good performance in a reference resolution task on this data set.", "startOffset": 82, "endOffset": 106}, {"referenceID": 23, "context": ", in (Turney and Pantel, 2010; Clarke, 2011; Baroni et al., 2013)).", "startOffset": 5, "endOffset": 65}, {"referenceID": 2, "context": ", in (Turney and Pantel, 2010; Clarke, 2011; Baroni et al., 2013)).", "startOffset": 5, "endOffset": 65}, {"referenceID": 0, "context": ", in (Turney and Pantel, 2010; Clarke, 2011; Baroni et al., 2013)).", "startOffset": 5, "endOffset": 65}, {"referenceID": 4, "context": "Wittgenstein\u2019s (1953) famous dictum \u201cmeaning is use\u201d (PU 43) is these days often taken as providing some form of implicit endorsement for corpusbased methods in semantics and is often referenced in the same paragraph as Firth\u2019s (1957) equally famous dictum \u201cyou shall know a word by the company it keeps\u201d (e.", "startOffset": 220, "endOffset": 235}, {"referenceID": 7, "context": ", (Gabrilovich et al., 2013)), or by including in the corpus some other representation of", "startOffset": 2, "endOffset": 28}, {"referenceID": 15, "context": "the context in which the reference took place and the objects that were referred to ( (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013; Young et al., 2014).", "startOffset": 86, "endOffset": 182}, {"referenceID": 21, "context": "the context in which the reference took place and the objects that were referred to ( (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013; Young et al., 2014).", "startOffset": 86, "endOffset": 182}, {"referenceID": 13, "context": "the context in which the reference took place and the objects that were referred to ( (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013; Young et al., 2014).", "startOffset": 86, "endOffset": 182}, {"referenceID": 26, "context": "the context in which the reference took place and the objects that were referred to ( (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013; Young et al., 2014).", "startOffset": 86, "endOffset": 182}, {"referenceID": 9, "context": "We use a medium-sized corpus of images (20k) with object segmentations (100k) and referring expressions (120k; (Grubinger et al., 2006; Escalante et al., 2010; Kazemzadeh et al., 2014), respectively; see also Section 4 below) to induce word meanings in the form of object classifiers (\u201chow well does this word fit to this object?\u201d).", "startOffset": 111, "endOffset": 184}, {"referenceID": 4, "context": "We use a medium-sized corpus of images (20k) with object segmentations (100k) and referring expressions (120k; (Grubinger et al., 2006; Escalante et al., 2010; Kazemzadeh et al., 2014), respectively; see also Section 4 below) to induce word meanings in the form of object classifiers (\u201chow well does this word fit to this object?\u201d).", "startOffset": 111, "endOffset": 184}, {"referenceID": 11, "context": "We use a medium-sized corpus of images (20k) with object segmentations (100k) and referring expressions (120k; (Grubinger et al., 2006; Escalante et al., 2010; Kazemzadeh et al., 2014), respectively; see also Section 4 below) to induce word meanings in the form of object classifiers (\u201chow well does this word fit to this object?\u201d).", "startOffset": 111, "endOffset": 184}, {"referenceID": 21, "context": "For example, Tellex et al. (2011) learn to map commands given to a robot to plans to be executed by it.", "startOffset": 13, "endOffset": 34}, {"referenceID": 13, "context": "Of this area, (Krishnamurthy and Kollar, 2013) is", "startOffset": 14, "endOffset": 46}, {"referenceID": 24, "context": "tion a neural language model (typically, an LSTM) on this to produce an output string (Vinyals et al., 2015; Devlin et al., 2015).", "startOffset": 86, "endOffset": 129}, {"referenceID": 3, "context": "tion a neural language model (typically, an LSTM) on this to produce an output string (Vinyals et al., 2015; Devlin et al., 2015).", "startOffset": 86, "endOffset": 129}, {"referenceID": 5, "context": "(Fang et al., 2015) modify this approach somewhat, by using word detectors first to specifically propose words for image regions, out of which the caption is then generated.", "startOffset": 0, "endOffset": 19}, {"referenceID": 23, "context": "Finally, we take from distributional semantics (Turney and Pantel, 2010) the idea that rich word meanings can be learned from records of word uses.", "startOffset": 47, "endOffset": 72}, {"referenceID": 17, "context": "which has been a recent focus of attention in this field (Mitchell and Lapata, 2010; Baroni et al., 2013).", "startOffset": 57, "endOffset": 105}, {"referenceID": 0, "context": "which has been a recent focus of attention in this field (Mitchell and Lapata, 2010; Baroni et al., 2013).", "startOffset": 57, "endOffset": 105}, {"referenceID": 1, "context": ", (Bruni et al., 2014)) has begun to extend the meanings learned from contexts with other words with image contexts.", "startOffset": 2, "endOffset": 22}, {"referenceID": 26, "context": "somewhat different approach is taken by Young et al. (2014), who specify a denotational semantics for certain words as sets of images, the caption of which the word appears in.", "startOffset": 40, "endOffset": 60}, {"referenceID": 12, "context": "The model was introduced in ((Kennington and Schlangen, 2015)); we reformulate and extend it here.", "startOffset": 29, "endOffset": 61}, {"referenceID": 10, "context": "The general idea of modelling words as classifiers on perceptual input goes back at least to (Harnard, 1990) and was most recently developed from a more formal perspective by (Larsson, 2015).", "startOffset": 93, "endOffset": 108}, {"referenceID": 14, "context": "The general idea of modelling words as classifiers on perceptual input goes back at least to (Harnard, 1990) and was most recently developed from a more formal perspective by (Larsson, 2015).", "startOffset": 175, "endOffset": 190}, {"referenceID": 12, "context": "This model (with \u201cthe\u201d as the only quantifier, and without negation) has been evaluated in (Kennington and Schlangen, 2015) on referring expressions in a game-playing scenario with a small number of dif-", "startOffset": 91, "endOffset": 123}, {"referenceID": 9, "context": "locations around the world and comprising an assorted cross-section of still natural images\u201d (Grubinger et al., 2006).", "startOffset": 93, "endOffset": 117}, {"referenceID": 4, "context": "This dataset was later augmented by Escalante et al. (2010) with (among other things) segmentation", "startOffset": 36, "endOffset": 60}, {"referenceID": 9, "context": "IAPR TC12 20k photographs of natural scenes (Grubinger et al., 2006)", "startOffset": 44, "endOffset": 68}, {"referenceID": 4, "context": "5k regions for IAPR TC12 images feature vectors (27 dim) for each region: area, boundary/area, width and height of the region, average and standard deviation in x and y, convexity, average, standard deviation and skewness in both color spaces RGB and CIE-Lab (Escalante et al., 2010)", "startOffset": 259, "endOffset": 283}, {"referenceID": 11, "context": "5k regions (Kazemzadeh et al., 2014)", "startOffset": 11, "endOffset": 36}, {"referenceID": 11, "context": "The final component is provided by Kazemzadeh et al. (2014), who collected a large number of expressions referring to objects (for which segmentations exist) from these images.", "startOffset": 35, "endOffset": 60}, {"referenceID": 12, "context": "(Such constructions were modelled in (Kennington and Schlangen, 2015), in the puzzle game domain.", "startOffset": 37, "endOffset": 69}, {"referenceID": 18, "context": "For both logistic regression and SVM we used the scikit learn package (Pedregosa et al., 2011).", "startOffset": 70, "endOffset": 94}, {"referenceID": 12, "context": "Some relations were modelled in (Kennington and Schlangen, 2015) in the puzzle domain.", "startOffset": 32, "endOffset": 64}, {"referenceID": 16, "context": "For comparison, we also train word2vec representations (Mikolov et al., 2013).", "startOffset": 55, "endOffset": 77}, {"referenceID": 19, "context": "As implemented in gensim (\u0158eh\u016f\u0159ek and Sojka, 2010).", "startOffset": 25, "endOffset": 50}, {"referenceID": 20, "context": "tiveness of contexts of encounter of words as a good predictor of productive use (Roy et al., 2015).", "startOffset": 81, "endOffset": 99}, {"referenceID": 8, "context": "10 Noting a (loose) correspondence to Montague\u2019s (1974) intensional semantics, where the intension of a word is a function from possible worlds to extensions (Gamut, 1991), the intensional meaning of the word w is then defined in (Kennington and Schlangen, 2015) as the classifier itself, a function from a representation of an object to an \u201cappropriateness score\u201d:", "startOffset": 158, "endOffset": 171}, {"referenceID": 12, "context": "10 Noting a (loose) correspondence to Montague\u2019s (1974) intensional semantics, where the intension of a word is a function from possible worlds to extensions (Gamut, 1991), the intensional meaning of the word w is then defined in (Kennington and Schlangen, 2015) as the classifier itself, a function from a representation of an object to an \u201cappropriateness score\u201d:", "startOffset": 230, "endOffset": 262}, {"referenceID": 12, "context": "In (Kennington and Schlangen, 2015) and below, the classifier is a binary logistic regression and the score can be interpreted as a probability, but this is an implementational detail and incidental to the model.", "startOffset": 3, "endOffset": 35}, {"referenceID": 12, "context": "Going beyond (Kennington and Schlangen, 2015), we extend this idea and define the contribution of other determiners similarly as selection functions operating on distributions.", "startOffset": 13, "endOffset": 45}], "year": 2017, "abstractText": "The recently introduced \u201cwords as classifiers\u201d model of grounded semantics (Kennington and Schlangen, 2015) views words as classifiers on (perceptual) contexts, and composes the meaning of a phrase through \u2018soft\u2019 intersection of the denotations of its component words. The model is trained from instances of referential language use, and was first evaluated with references in a game-playing scenario with a small number of different types of objects. Here, we apply this model to a large set of real-world photographs (SAIAPR TC-12, (Escalante et al., 2010)) that contain objects with a much larger variety of types, and we show that it achieves good performance in a reference resolution task on this data set. We also extend the model to deal with quantification and negation, and evaluate these extensions, with good results. To investigate what the classifiers learn, we introduce \u201cintensional\u201d and \u201cdenotational\u201d word vectors, and show that they capture meaning similarity in a way that is different from and complementary to word2vec word embeddings.", "creator": "TeX"}}}