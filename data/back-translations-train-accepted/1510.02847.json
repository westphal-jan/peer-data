{"id": "1510.02847", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Oct-2015", "title": "Active Learning from Weak and Strong Labelers", "abstract": "An active learner is given a hypothesis class, a large set of unlabeled examples and the ability to interactively query labels to an oracle of a subset of these examples; the goal of the learner is to learn a hypothesis in the class that fits the data well by making as few label queries as possible.", "histories": [["v1", "Fri, 9 Oct 2015 23:15:40 GMT  (38kb)", "https://arxiv.org/abs/1510.02847v1", "To appear in NIPS 2015"], ["v2", "Fri, 16 Oct 2015 01:06:27 GMT  (37kb)", "http://arxiv.org/abs/1510.02847v2", "To appear in NIPS 2015"]], "COMMENTS": "To appear in NIPS 2015", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["chicheng zhang", "kamalika chaudhuri"], "accepted": true, "id": "1510.02847"}, "pdf": {"name": "1510.02847.pdf", "metadata": {"source": "CRF", "title": "Active Learning from Weak and Strong Labelers", "authors": ["Chicheng Zhang"], "emails": ["chz038@eng.ucsd.edu", "kamalika@cs.ucsd.edu"], "sections": [{"heading": null, "text": "ar Xiv: 151 0.02 847v 2 [cs.L G] 16 OThis work deals with active learning with labels originating from strong and weak labellers where, in addition to the normal active learning setting, we have an additional weak labeller who can occasionally provide false labels. An example is learning to classify medical images where either expensive labels can be obtained from a doctor (oracle or strong labeller) or cheaper but occasionally false labels can be obtained from a medical residence (weak labeller). Our goal is to learn a classifier with minor errors in the data marked by the oracle, while we use the weak labeller to reduce the number of labelling queries made on these labellers. We provide an active learning algorithm for this setting, determine its statistical consistency and analyze its label complexity to determine when alone it can provide strong label parameters."}, {"heading": "1 Introduction", "text": "In fact, the fact is that most of them are able to assert themselves, that they are able to assert themselves, that they are able to assert themselves, and that they are able to assert themselves to achieve their goals."}, {"heading": "2 Preliminaries", "text": "We start with a general framework for active learning of weak and strong identifiers (PD = 6). In the default setting for active learning, we are mostly equipped with unlabeled data from a distribution space, which we can automatically pull from an input space. (In our setting, we also have access to a weak labeling oracle that we can interactively query.) However, querying W is much cheaper than querying a label yW drawn from a conditional distribution of PW (yW | x), which is not the same as the conditional distribution of PO (yO | x) from O.Let D is data distribution via examples described such as: PD (x, y) = PU (x) PO (x).Our goal is to learn a classification class H that we most likely have."}, {"heading": "3 Algorithm", "text": "It is not as if we make this decision if we differ from this oracle? A plausible approach is to learn a difference that is in a hypothetical class H d to determine if there is a difference when there is a difference. Our first important observation is if the region where E and W differ from each other can be perfectly modeled."}, {"heading": "4 Performance Guarantees", "text": "We now examine the performance of our algorithm, which is measured by the number of label queries measured by the number of label queries. (D) In addition, we need our algorithms to be statistically consistent, which means that the true error of the output classifier should be based on the true error of the best classifier in H on the data distribution. (D) Algorithm 4 in the discount region of the current trust, x): Test if x is set in the discount region of the current trust. (D) if x is set in the discount region of the current trust, 0 otherwise: Train h, CONS-LEARNH ({/ 0, S). 4: Train h, x, x, x, x, x. (X, \u2212 h)."}, {"heading": "4.1 Discussion", "text": "The first terms in (5) and (6) represent the terms we need to learn the target classifier, and the second terms represent the overhead in learning the difference classifiers. (5) We assume that the difference classifiers (5) and (6) the other terms are the difference classifiers. (5) We assume that the difference classifiers are not associated with an asymptotically high overhead in the more realistic agnostic case. In the realizable case, if the second terms are in the same order as the first; therefore, we should apply a simpler difference hypothesis. (5) We believe that the subordination of overhead terms stems from the fact that there is a classifier in H d whose false negative error is very low.Comparing theorem 2 with the corresponding results for DBAL, we observe that instead of the term supr (2)."}, {"heading": "Acknowledgements", "text": "We thank the NSF under IIS 1162581 for supporting the research and Jennifer Dy for introducing the problem of active learning of multiple labels."}, {"heading": "A Notation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Basic Definitions and Notation", "text": "We are assuming that we have a target hypothesis class H of the UK dimension d and a difference hypothesis class H d of the UK dimension d. \"We are assuming that we have a target hypothesis class H of the UK dimension d.\" We are assuming that we have a target hypothesis class H of the UK dimension d. \"We are assuming that we have a target hypothesis class H of the UK dimension d.\" We are assuming that we have a target hypothesis class D (UK class d) with a difference hypothesis of the UK class H (UK class H). We are using the notation D to name the common distribution via examples and labels from O and W."}, {"heading": "A.2 Adaptive Procedure for Estimating Probability Mass", "text": "For the sake of completeness, in Algorithm 5 we describe a standard doubling procedure for estimating the bias of a coin within a constant factor. This procedure is used by Algorithm 2 to estimate the probability mass of the discrepancy region of the current trust using unspecified examples from U.Lemma 1. Let us assume that p > 0 and algorithm 5 are executed with the probability of failure. Let us then consider the eventE = {for all i-N, p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2p-2"}, {"heading": "A.3 Notations on Datasets", "text": "Without loss of generality, we assume that the examples drawn in Algorithm 1 have different values x, as this is likely to happen with probability 1 under mild assumptions. (Algorithm 1 uses a mixture of three types of labeled data to learn a target classification.) To analyze the effects of these three types of labeled data, we need to define the common distribution D via examples and labels of both O and W as follows: PD (x, yO, yW) = PU (x) PO (yO) x), where an example x, the labels of O and W are generated as follows: PD (x, yO, yW), yW (x), yW)."}, {"heading": "A.4 Events", "text": "Remember that all events occur with high probability. \u2212 Definition eventE1k: \u2212 Definition eventE1k: \u2212 Definition eventE1k: \u2212 Definition eventE1k: \u2212 Definition eventE1k: \u2212 Definition eventE1k: \u2212 Definition eventE1k: \u2212 Definition p.P: \u2212 p.P: \u2212 p.P: \u2212 p.P: \u2212 p.P: \u2212 p.P: \u2212 p.P: \u2212 p.P: \u2212 p.P: \u2212 p.P: \u2212 p.P: \u2212 p.P: \u2212 p.P: \u2212 p.P: \u2212 p.P: \u2212 p.P: \u2212 p.P: \u2212 \u2212 p.P: \u2212 p.P: \u2212 p.P: \u2212 p.P: \u2212 p.P: \u2212 p.P: \u2212 p.P: \u2212 p.P: \u2212 p.P: \u2212 p.P: \u2212 p.P: \u2212 p.P: \u2212 p.P: \u2212 p.P: \u2212 p.P: \u2212 p.P.P: \u2212 p.P: \u2212 p.P: \u2212 p.P: \u2212 p.P: \u2212 p.P: \u2212 p.P: \u2212 p.P: \u2212 p.P: \u2212 p.P: \u2212 p.p.P: \u2212 p.P: \u2212 p.P: \u2212 p.p: \u2212 p.p: \u2212 p.p: \u2212 p.p: \u2212 p.p.p: \u2212 p.p: \u2212 p.p.p: \u2212 p.p.p: \u2212 p.p.p: \u2212 p.p.p: \u2212 p.p.p: \u2212 p.p: \u2212 p.p: \u2212 p.p: \u2212 p.p: \u2212 p.p: \u2212 p.p: \u2212 p...p.p....p: \u2212 p.p: \u2212 p.p: \u2212 p.p: \u2212 p.p: \u2212 p.p: \u2212 p.p.................p::: \u2212 p........p: \u2212 p.p: \u2212 p.p: \u2212 p.p: \u2212 p....p: \u2212 p.p: \u2212 p."}, {"heading": "B Proof Outline and Main Lemmas", "text": "The main idea of the proof is the maintenance of the following three invariants on the outputs of algorithm 1 in each epoch. We prove that these invariants are held simultaneously for each epoch with high probability by going beyond the epochs. In the course of epoch, the end of epoch 1 refers to the end of the execution of line 13 of algorithm 1 in iteration k. The end of epoch 0 refers to the end of the execution of line 5 in algorithm 1.Invariant 1 refers to the fact that if we replace the derived epochs and labels obtained from W in line 1 by those obtained from O (i.e. the dataset Sk), the excess errors of the classifiers in H will not decrease much. Invariant 1 (approximately favored bias). Let us leave h each classification in H, and h is another classifier in H with surpluses."}, {"heading": "B.1 Active Label Inference and Identifying the Disagreement Region", "text": "We start by demonstrating some Lmmas via algorithm 4, which detect whether an example k \u2212 k \u2212 k \u2212 k \u2212 k k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k k k k k k k k k k k k k k k k k k k k k k k k k k k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212"}, {"heading": "B.2 Training the Difference Classifier", "text": "There is a numerical constant c1 > 0 so that the following definition applies: PD (PD) 1 and 2 hold at the end of the epoch k \u2212 1 conditioned on event Fk \u2212 1 and the algorithm 2 has inputs of undescribed data distribution U, oracle O, \u03b5 = \u03b5k / 128, hypothesis class H d f, vaststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststst"}, {"heading": "B.3 Adaptive Subsampling", "text": "There is one numeric constant c2 = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "B.4 Putting It Together \u2013 Consistency and Label Complexity", "text": "The proof of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the invariant of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the validation of the vali"}, {"heading": "C Case Study: Linear Classfication under Uniform Distribution over Unit Ball", "text": "We remind the reader of the setting of our example in section 4. H is the class of homogeneous linear separating signs on the D-dimensional unity sphere and H d f is defined as {h-h-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H-H"}, {"heading": "D Performance Guarantees for Learning with Respect to Data labeled by O and W", "text": "An interesting variant of our model is to look at learning data characterized by a mixture of O and W. Let DW be the distribution over described examples determined by U and W, in particular PDW (x, y) = PU (x) PW (y) x. Let D (x) be a mixture of D and DW, specifically D (1 \u2212 \u03b2) D + \u03b2DW, for a specific parameter \u03b2 > 0. Define h \"to be the best classifier in relation to D.\" and name the error of h \"in relation to D.\" Let O \"be the following mixing oracle. In the face of an example x, the labeling yO\" is generated as follows. Flips of a coin with bias \u03b2. \"When a label occurs, it becomes W for the label of x and returns the result; otherwise, O\" is queried and the result returned."}, {"heading": "E Remaining Proofs", "text": "The proof of the fact that it is a random sample of size mk (k) k (k) k (k) k (k) k (k) k (k) k (k) k (k) k (k) k (k) k (k) k (k) k (k) k (k) k (k) k (k) k (k) k (k) k (k) k (k) k (k) k (k) k) k (k) k (k) k (k) k (k) k (k) k) k (k) k (k) k (k) k (k) k (k) k (k) k (k) k (k) k (k) k) k (k) k) k (k) k) k (k) k) k (k) k) k (k) k (k) k (k) k) k (k) k k k (k) k) k n (k) k k k k k k k (k) k) k (k) k) k (k) k (k) k) k (k) k (k) k) k (k) k (k) k) k (k) k n (k) k) k (k) k (k) k) k (k) k (k) k) k (k) k n (k) k) k (k) k (k) k) k (k) k) k (k) k (k) k n (k) k) k (k) k (k) k (k) k) k n (k) k (k) k (k) k) k (k) k n (k) k n (k) k (k) k (k) k n (k) k (k) k) k (k) k (k (k) k) k n (k) k (k) k n (k (k) k) k (k) k (k) k (k) k (k) k (k) k) k n (k) k (k) k (k) k) k (k (k) k (k) k) k (k) k n (k) k (k) k (k) k n (k) k) k (k (k"}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "An active learner is given a hypothesis class, a large set of unlabeled examples and the ability to interactively query labels to an oracle of a subset of these examples; the goal of the learner is to learn a hypothesis in the class that fits the data well by making as few label queries as possible. This work addresses active learning with labels obtained from strong and weak labelers, where in addition to the standard active learning setting, we have an extra weak labeler which may occasionally provide incorrect labels. An example is learning to classify medical images where either expensive labels may be obtained from a physician (oracle or strong labeler), or cheaper but occasionally incorrect labels may be obtained from a medical resident (weak labeler). Our goal is to learn a classifier with low error on data labeled by the oracle, while using the weak labeler to reduce the number of label queries made to this labeler. We provide an active learning algorithm for this setting, establish its statistical consistency, and analyze its label complexity to characterize when it can provide label savings over using the strong labeler alone.", "creator": "LaTeX with hyperref package"}}}