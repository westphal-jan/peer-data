{"id": "1510.02879", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Oct-2015", "title": "Attend, Adapt and Transfer: Attentive Deep Architecture for Adaptive Transfer from multiple sources in the same domain", "abstract": "The ability to transfer knowledge from learnt source tasks to a new target task can be very useful in speeding up the learning process of a Reinforcement Learning agent. This has been receiving a lot of attention, but the application of transfer poses two serious challenges which have not been adequately addressed in the past. First, the agent should be able to avoid negative transfer, which happens when the transfer hampers or slows down the learning instead of speeding it up. Secondly, the agent should be able to do selective transfer which is the ability to select and transfer from different and multiple source tasks for different parts of the state space of the target task. We propose ADAAPT: A Deep Architecture for Adaptive Policy Transfer, which addresses these challenges. We test ADAAPT using two different instantiations: One as ADAAPTive REINFORCE algorithm for direct policy search and another as ADAAPTive Actor-Critic where the actor uses ADAAPT. Empirical evaluations on simulated domains show that ADAAPT can be effectively used for policy transfer from multiple source MDPs sharing the same state and action space.", "histories": [["v1", "Sat, 10 Oct 2015 05:32:24 GMT  (776kb,D)", "http://arxiv.org/abs/1510.02879v1", "7 pages"], ["v2", "Mon, 21 Dec 2015 07:07:51 GMT  (734kb,D)", "http://arxiv.org/abs/1510.02879v2", "10 pages"], ["v3", "Thu, 22 Sep 2016 20:55:43 GMT  (2427kb,D)", "http://arxiv.org/abs/1510.02879v3", "12 pages"], ["v4", "Tue, 27 Dec 2016 22:59:55 GMT  (869kb,D)", "http://arxiv.org/abs/1510.02879v4", "Under review at ICLR 2017; Accepted at NIPS Deep Reinforcement Learning Workshop, Barcelona, 2016"], ["v5", "Tue, 18 Apr 2017 01:05:04 GMT  (1086kb,D)", "http://arxiv.org/abs/1510.02879v5", "Published as a conference paper at ICLR 2017"]], "COMMENTS": "7 pages", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["janarthanan rajendran", "aravind s lakshminarayanan", "mitesh m khapra", "p prasanna", "balaraman ravindran"], "accepted": true, "id": "1510.02879"}, "pdf": {"name": "1510.02879.pdf", "metadata": {"source": "CRF", "title": "ADAAPT: A Deep Architecture for Adaptive Policy Transfer from Multiple Sources", "authors": ["Janarthanan Rajendran", "Prasanna P", "Balaraman Ravindran", "Mitesh M Khapra"], "emails": ["rsdjjana@gmail.com", "pp1403@gmail.com", "ravi@cse.iitm.ac.in", "mikhapra@in.ibm.com"], "sections": [{"heading": "Introduction", "text": "In fact, it is that it will be able to put itself at the top, in the way that it is able to put itself at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said."}, {"heading": "Related Work", "text": "As already mentioned, learning approaches may have to do with the transmission of representations, policies or value functions as initial exploratory measures. For example, (Banerjee and Stone 2007) describe a method of transferring value functions by constructing a game tree. Similarly, (Sorg and Singh 2009) investigate the idea of transferring value functions from a source task and use it as the first estimate of value functions in the target task to limit themselves to the initial exploration. Another method for achieving a transfer is the reuse policy derived from the source task (s) in the target task. Probabilistic policy of reuse, as discussed in (Ferna) ndez and Veloso 2006, provides a useful way for transferring policies and selects a policy based on a similarity metric or a random policy. The policy selection occurs in a priorization of each episode (Atkeson al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al"}, {"heading": "Experiments and Discussion", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}], "references": [{"title": "and Schaal", "author": ["C.G. Atkeson"], "venue": "S.", "citeRegEx": "Atkeson and Schaal 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Cho Bahdanau", "D. Bengio 2014] Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "and Stone", "author": ["B. Banerjee"], "venue": "P.", "citeRegEx": "Banerjee and Stone 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "and Li", "author": ["E. Brunskill"], "venue": "L.", "citeRegEx": "Brunskill and Li 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Mahadevan", "author": ["K. Ferguson"], "venue": "S.", "citeRegEx": "Ferguson and Mahadevan 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "and Veloso", "author": ["F. Fern\u00e1ndez"], "venue": "M.", "citeRegEx": "Fern\u00e1ndez and Veloso 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "and Tsitsiklis", "author": ["V. Konda"], "venue": "J.", "citeRegEx": "Konda and Tsitsiklis 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "A", "author": ["G. Konidaris", "I. Scheidwasser", "Barto"], "venue": "G.", "citeRegEx": "Konidaris. Scheidwasser. and Barto 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "and Restelli", "author": ["A. Lazaric"], "venue": "M.", "citeRegEx": "Lazaric and Restelli 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Dynamic abstraction in reinforcement learning via clustering", "author": ["Mannor"], "venue": "In Proceedings of the twenty-first international conference on Machine learning,", "citeRegEx": "Mannor,? \\Q2004\\E", "shortCiteRegEx": "Mannor", "year": 2004}, {"title": "A", "author": ["S. Niekum", "S. Chitta", "Barto"], "venue": "G.; Marthi, B.; and Osentoski, S.", "citeRegEx": "Niekum et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Singh", "author": ["J. Sorg"], "venue": "S.", "citeRegEx": "Sorg and Singh 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "A", "author": ["R.S. Sutton", "Barto"], "venue": "G.", "citeRegEx": "Sutton and Barto 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "P", "author": ["M.E. Taylor", "Stone"], "venue": "2009. Transfer learning for reinforcement learning domains: A survey. The Journal of Machine Learning Research 10:1633\u2013", "citeRegEx": "Taylor and Stone 2009", "shortCiteRegEx": null, "year": 1685}, {"title": "and Stone", "author": ["M.E. Taylor"], "venue": "P.", "citeRegEx": "Taylor and Stone 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "and Schmid", "author": ["M. Wernsdorfer"], "venue": "U.", "citeRegEx": "Wernsdorfer and Schmid 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "R", "author": ["Williams"], "venue": "J.", "citeRegEx": "Williams 1992", "shortCiteRegEx": null, "year": 1992}], "referenceMentions": [], "year": 2017, "abstractText": "The ability to transfer knowledge from learnt source tasks to a new target task can be very useful in speeding up the learning process of a Reinforcement Learning agent. This has been receiving a lot of attention, but the application of transfer poses two serious challenges which have not been adequately addressed in the past. First, the agent should be able to avoid negative transfer, which happens when the transfer hampers or slows down the learning instead of speeding it up. Secondly, the agent should be able to do selective transfer which is the ability to select and transfer from different and multiple source tasks for different parts of the state space of the target task. We propose ADAAPT: A Deep Architecture for Adaptive Policy Transfer, which addresses these challenges. We test ADAAPT using two different instantiations: One as ADAAPTive REINFORCE algorithm for direct policy search and another as ADAAPTive Actor-Critic where the actor uses ADAAPT. Empirical evaluations on simulated domains show that ADAAPT can be effectively used for policy transfer from multiple source MDPs sharing the same state and action space.", "creator": "LaTeX with hyperref package"}}}