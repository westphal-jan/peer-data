{"id": "1510.03009", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Oct-2015", "title": "Neural Networks with Few Multiplications", "abstract": "For most deep learning algorithms training is notoriously time consuming. Since most of the computation in training neural networks is typically spent on floating point multiplications, we investigate an approach to training that eliminates the need for most of these. Our method consists of two parts: First we stochastically binarize weights to convert multiplications involved in computing hidden states to sign changes. Second, while back-propagating error derivatives, in addition to binarizing the weights, we quantize the representations at each layer to convert the remaining multiplications into binary shifts. Experimental results across 3 popular datasets (MNIST, CIFAR10, SVHN) show that this approach not only does not hurt classification performance but can result in even better performance than standard stochastic gradient descent training, paving the way to fast, hardware-friendly training of neural networks.", "histories": [["v1", "Sun, 11 Oct 2015 04:32:39 GMT  (85kb,D)", "http://arxiv.org/abs/1510.03009v1", "7 pages, 3 figures"], ["v2", "Mon, 9 Nov 2015 20:16:10 GMT  (111kb,D)", "http://arxiv.org/abs/1510.03009v2", "Submitted to ICLR 2016, 8 pages, 3 figures; More descriptions, add pseudo-code, and more experiments"], ["v3", "Fri, 26 Feb 2016 05:24:30 GMT  (111kb,D)", "http://arxiv.org/abs/1510.03009v3", "Published as a conference paper at ICLR 2016. 9 pages, 3 figures"]], "COMMENTS": "7 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["zhouhan lin", "matthieu courbariaux", "roland memisevic", "yoshua bengio"], "accepted": true, "id": "1510.03009"}, "pdf": {"name": "1510.03009.pdf", "metadata": {"source": "CRF", "title": "Neural Networks with Few Multiplications", "authors": ["Zhouhan Lin", "Matthieu Courbariaux"], "emails": ["zhouhan.lin@umontreal.ca", "matthieu.courbariaux@polymtl.ca", "roland.memisevic@umontreal.ca", "yoshua.umontreal@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Training deep neural networks has long been computationally demanding and time-consuming, and for some state-of-the-art architectures, it can take weeks to develop the models [Krizhevsky et al., 2012]. Another problem is that the need for disk space can be enormous, so many Commmon models require 12 gigabytes or more of disk space in speech recognition or machine translation [Gulcehre et al., 2015]. To deal with these problems, it is common to train deep neural networks by relying on GPU or CPU clusters and well-designed parallelization strategies [Le, 2013]. Most of the calculations performed in neural network training are flow complications. In this paper, we focus on eliminating most of these multiplications in order to reduce compression. Based on our previous work [Courbariaux et al., 2015], we treat the multiplications in both versed representations and versed combinations by combining them."}, {"heading": "2 Related work", "text": "In the past, several approaches have been proposed to simplify calculations in neural networks, some of which attempt to limit weight values to an integer force of two to reduce all multiplications as binary shifts [Kwan and Tang, 1993] [Marchesi et al., 1993], which means that multiplied values can no longer be guaranteed; another approach, [Kim and Paris, 2015], introduces a fully Boolean network that simplifies the calculation of test time at an acceptable performance time; the approach still requires a real-rated, full-resolution training phase, so that the benefits of reducing calculations are not applicable to training. [Machado et al., 2015] manages to achieve acceptable accuracy in economical representation by replacing all logical resolution rates with multiplying compounds from 1999."}, {"heading": "3 Binary and ternary connect", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Binary connect revisited", "text": "In [Courbariaux et al., 2015] we have introduced a weight binarization technology that eliminates multiplications in forward gear. We summarize this approach in this subsection and add an extenion to it in the next.Let's consider a neural network layer with N input and M output units. Forward computation is y = h (Wx + b), where W and b are weights and biases, or h is the activation function, and x and y are the input and output layers of the layer. If we choose ReLU as h, there will be no multiplications in the calculation of the activation function, so all multiplications will be in the matrix product Wx."}, {"heading": "3.2 Ternary connect", "text": "The binary connection introduced in the previous subsection allows weights of \u2212 1 or 1. However, in a trained neural network it is common to observe that many learned weights are zero or close to zero. Although the stochastic scanning process would leave the mean of the recorded weights at zero, this suggests that it may be advantageous to explicitly set weights to zero. To bring weights to zero, some adjustments to Equation 1 are required. We divide the interval of [-1, 1], within which the full resolution value w-ij is, into two sub-intervals: [\u2212 1, 0] and (0, 1]. If a weight value w-ij falls into one of them, we capture w-ij to be the two boundary values of this interval, corresponding to their distance from w-ij, i.e., if w-ij > 0: P (Wij = 1) = w-ij; P (Wij = 0 ternj-j = w-1 and ij-w-1)."}, {"heading": "4 Quantized back propagation", "text": "In the first section, we have described how multiplications from forward gear can be eliminated. In this section, we propose a way to eliminate multiplications from reverse gear. Suppose the i level of the network has N input and M output units and considers an error signal that spreads from its output downwards. The updates for weights and distortions would be the outer product of the layer input and the error signal: \u2206 W = \u03b7g) xT (4) \u2206 b = throughs h \u2032 (Wx + b) (5), where the learning rate and the umpteenth input into the layer is quantification. As the propagation takes place through the layers, the error signal also needs to be updated. Its update taking into account the next layer takes the form: GOP = WT; h \u2032 (Wx + b). There are 3 terms that appear repeatedly in the layer."}, {"heading": "5 Experiments", "text": "Our implementation is based on Theano [Bastien et al., 2012]. We experimented with three sets of data: MNIST, CIFAR10 and SVHN. In the following section, we will show the performance these neural networks can achieve with multiplier light. In the following subsections, we will examine some of their properties, such as convergence and robustness, in more detail."}, {"heading": "5.1 General performance", "text": "We tested different variations of our approach and compared the results with [Courbariaux et al., 2015] and complete precision training (Table 1).All models are trained with stochastic gradient descent (SGD) without impulse. We use batch normalization for all models to accelerate learning. At training time, binary (ternary) compounds and quantified backpropagation are used, while at test time we use the learned weights with full resolution for forward propagation. For each data set, all hyperparameters for the different methods are set to the same values, except that the learning rate is adjusted independently for each one."}, {"heading": "5.1.1 MNIST", "text": "The MNIST dataset [LeCun et al., 1998] has 50,000 images for training and 10,000 for testing. All images are grayscale images of 28 x 28 pixels, divided into 10 classes according to the 10 digits. The model we use is a fully interconnected network with 4 layers: 784-1024-1024-1024-10. On the last layer, we use hinge loss as a cost factor. The training set is divided into two parts, one of which is the training set of 40,000 images and the other the validation set of 10,000 images. Our experimental results show that despite the removal of most multiplications, our approach delivers a comparable (indeed slightly higher) performance than the training at full resolution. The performance improvement is probably due to the regulation effect implied by stochastic scanning."}, {"heading": "5.1.2 CIFAR10", "text": "CIFAR10 [Krizhevsky and Hinton, 2009] contains images of 32 x 32 RGB pixels. As with MNIST, we respectfully split the data set into 40,000, 10,000 and 10,000 training, validation and test cases. We apply our approach to this data set in a Convolutionary Network. The network has 6 folding / pooling layers, a fully connected layer and a classification layer. We use the hinge loss for training. Similar to the fully connected network, binary (ternary) connections and quantified backpropagation achieve slightly higher performance than ordinary SGD."}, {"heading": "5.1.3 SVHN", "text": "The Street View House Numbers (SVHN) dataset [Netzer et al., 2011] contains RGB images of house numbers. It contains more than 600,000 images in its extended training set and about 26,000 images in its test set. We remove 6,000 images from the training set for validation. We use 7 layers of folding / pooling, a fully bonded layer and a classification layer. The services we receive match our results on CIFAR10."}, {"heading": "5.2 Convergence", "text": "Taking the Convolutionary Networks on CIFAR10 as a test bed, we now examine learning behavior in detail. Figure 1 shows the performance of the model in terms of test set errors during training. The figure shows that binarization causes the network to converge more slowly than traditional SGDs, but delivers a better optimum after the convergence of the algorithm. Compared to binary connections (red line), quantification in error propagation (yellow line) does not harm model accuracy at all. Furthermore, a ternary link in combination with quantified backpropagation (green line) outperforms all other three approaches."}, {"heading": "5.3 The effect of bit clipping", "text": "The maximum number of bits to move determines the amount of memory required, but also determines the range in which a single weight update can vary. Figure 2 shows the model performance as a function of the maximum bit shifts allowed. This figure shows that the approach to the number of bits used is not very useful. The maximum allowable shift in the figure varies between 2 bits and 10 bits, and the performance remains roughly the same. Even by limiting the bit shifts to 2, the model can still successfully learn. The fact that the performance is not very sensitive to the maximum number of bit shifts allowed suggests that we do not need to redefine the number of bits used to quantize x."}, {"heading": "6 Conclusion and future work", "text": "We proposed a way to eliminate most of the floating-point multiplications used during the training of a feedforward neural network, which could dramatically accelerate the training of neural networks through the use of dedicated hardware implementations. A somewhat surprising fact is that the approach, rather than damaging the accuracy of predictions, tends to improve, probably due to the regularization effect associated with it. Future work guidelines include exploring actual implementations of this approach (for example, through the use of FPGA), finding more efficient ways of binarization, and expanding to recursive neural networks."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["Bastien et al", "F. 2012] Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Pulsed neural networks. chapter Stochastic Bit-stream Neural Networks, pages 337\u2013352", "author": ["Burge et al", "P.S. 1999] Burge", "M.R. van Daalen", "B.J.P. Rising", "J.S. Shawe-Taylor"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q1999\\E", "shortCiteRegEx": "al. et al\\.", "year": 1999}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["Courbariaux et al", "M. 2015] Courbariaux", "Y. Bengio", "David", "J.-P"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "On using monolingual corpora in neural machine translation", "author": ["Gulcehre et al", "C. 2015] Gulcehre", "O. Firat", "K. Xu", "K. Cho", "L. Barrault", "Lin", "H.-C", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1503.03535", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Generating binary sequences for stochastic computing", "author": ["Jeavons et al", "P. 1994] Jeavons", "D.A. Cohen", "J. Shawe-Taylor"], "venue": "Information Theory, IEEE Transactions", "citeRegEx": "al. et al\\.,? \\Q1994\\E", "shortCiteRegEx": "al. et al\\.", "year": 1994}, {"title": "Bitwise neural networks", "author": ["Kim", "Paris", "M. 2015] Kim", "S. Paris"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Hinton", "A. 2009] Krizhevsky", "G. Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105", "author": ["Krizhevsky et al", "A. 2012] Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Multiplierless multilayer feedforward neural network design suitable for continuous input-output mapping", "author": ["Kwan", "Tang", "H.K. 1993] Kwan", "C. Tang"], "venue": null, "citeRegEx": "Kwan et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Kwan et al\\.", "year": 1993}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["V. Q"], "venue": "[Le,", "citeRegEx": "Q.,? \\Q2013\\E", "shortCiteRegEx": "Q.", "year": 2013}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun et al", "Y. 1998] LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "al. et al\\.,? \\Q1998\\E", "shortCiteRegEx": "al. et al\\.", "year": 1998}, {"title": "Computational cost reduction in learned transform classifications. arXiv preprint arXiv:1504.06779", "author": ["Machado et al", "E.L. 2015] Machado", "C.J. Miosso", "R. von Borries", "M. Coutinho", "Berger", "P. d. A", "T. Marques", "R.P. Jacobi"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Fast neural networks without multipliers", "author": ["Marchesi et al", "M. 1993] Marchesi", "G. Orlandi", "F. Piazza", "A. Uncini"], "venue": "Neural Networks, IEEE Transactions", "citeRegEx": "al. et al\\.,? \\Q1993\\E", "shortCiteRegEx": "al. et al\\.", "year": 1993}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Netzer et al", "Y. 2011] Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "In NIPS workshop on deep learning and unsupervised feature learning,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Backpropagation without multiplication", "author": ["Simard", "Graf", "P.Y. 1994] Simard", "H.P. Graf"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Simard et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Simard et al\\.", "year": 1994}, {"title": "Device for generating binary sequences for stochastic computing", "author": ["van Daalen et al", "M. 1993] van Daalen", "P. Jeavons", "J. Shawe-Taylor", "D. Cohen"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q1993\\E", "shortCiteRegEx": "al. et al\\.", "year": 1993}], "referenceMentions": [], "year": 2015, "abstractText": "For most deep learning algorithms training is notoriously time consuming. Since most of the computation in training neural networks is typically spent on floating point multiplications, we investigate an approach to training that eliminates the need for most of these. Our method consists of two parts: First we stochastically binarize weights to convert multiplications involved in computing hidden states to sign changes. Second, while back-propagating error derivatives, in addition to binarizing the weights, we quantize the representations at each layer to convert the remaining multiplications into binary shifts. Experimental results across 3 popular datasets (MNIST, CIFAR10, SVHN) show that this approach not only does not hurt classification performance but can result in even better performance than standard stochastic gradient descent training, paving the way to fast, hardwarefriendly training of neural networks.", "creator": "LaTeX with hyperref package"}}}