{"id": "1510.03055", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Oct-2015", "title": "A Diversity-Promoting Objective Function for Neural Conversation Models", "abstract": "Sequence-to-sequence neural network models for generation of conversational responses tend to generate safe, commonplace responses (e.g., \\textit{I don't know}) regardless of the input. We suggest that the traditional objective function, i.e., the likelihood of output (responses) given input (messages) is unsuited to response generation tasks. Instead we propose using Maximum Mutual Information (MMI) as objective function in neural models. Experimental results demonstrate that the proposed objective function produces more diverse, interesting, and appropriate responses, yielding substantive gains in \\bleu scores on two conversational datasets.", "histories": [["v1", "Sun, 11 Oct 2015 14:04:57 GMT  (27kb)", "http://arxiv.org/abs/1510.03055v1", null], ["v2", "Thu, 7 Jan 2016 06:59:19 GMT  (270kb)", "http://arxiv.org/abs/1510.03055v2", null], ["v3", "Fri, 10 Jun 2016 22:03:28 GMT  (32kb)", "http://arxiv.org/abs/1510.03055v3", "In. Proc of NAACL 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiwei li", "michel galley", "chris brockett", "jianfeng gao", "bill dolan"], "accepted": true, "id": "1510.03055"}, "pdf": {"name": "1510.03055.pdf", "metadata": {"source": "CRF", "title": "A Diversity-Promoting Objective Function for Neural Conversation Models", "authors": ["Jiwei Li", "Michel Galley Chris Brockett", "Jianfeng Gao Bill Dolan"], "emails": ["jiweil@stanford.edu", "mgalley@microsoft.com", "chrisbkt@microsoft.com", "jfgao@microsoft.com", "billdol@microsoft.com"], "sections": [{"heading": null, "text": "ar Xiv: 151 0.03 055v 1 [cs.C L] 11 Sequence-to-sequence models of neural networks for generating conversation responses tend to generate safe, everyday responses regardless of input (e.g., I do not know). We suggest that the traditional objective function, i.e. the probability of input (of responses) is unsuitable for generating responses. Instead, we propose using Maximum Mutual Information (MMI) as an objective function in neural models. Experimental results show that the proposed objective function produces more diverse, interesting, and appropriate responses, resulting in substantial increases in BLEU values across two conversation datasets."}, {"heading": "1 Introduction", "text": "In fact, most people who are in a position to help themselves, to help themselves, to help themselves, to help themselves, to help themselves, to help themselves, to help themselves, to manoeuvre themselves into a situation in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position to solve their problems, in which they are in a position to solve themselves, in which they are in a position, and in which they are in a position to get their problems under control."}, {"heading": "2 Related work", "text": "Previous efforts to integrate statistical methods into dialog systems have typically relied on one of two approaches: the first is stochastic models based on hand-coded rules or templates (Levin et al., 2000; Young et al., 2010; Walker et al., 2003; Pieraccini et al., 2009; Wang et al., 2011), an approach that is both expensive and difficult to expand to open domain scenarios; the second also requires craftsmanship and an attempt to learn generational rules from a minimal number of written rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; 0; Nio et al., 2014; Chen et al., 2013).A third line of investigation - and the one we are adopting in this paper - was first introduced by Ritter et al al al al al al al al al al al al al. (2011), who applied the response-generation task as a statistical machine translation task (SMT-framed messages) in an array of phrases in a language."}, {"heading": "3 Sequence-to-Sequence Models", "text": "Faced with a sequence of inputs X = {x1, x2,..., xnX}, an LSTM associates each time step with an input gate, a memory gate, and an output gate, each designated as it, ft, and ot. We distinguish e and h, where et denotes the vector for an individual text unit (for example, a word or a sentence) at the time t, while ht denotes the vector calculated by the LSTM model at the time t by representing et and ht \u2212 1. ct the cell state vector at the time t, and \u03c3 denotes the sigmoid function. Then, the vector representation ht is denoted for each time step t by: it = (Wi \u00b7 [ht \u2212 1, et]) (1) ft = \u03c3 (Wf \u00b7 [ht \u2212 1, et]) (2) ot = distribution (Where \u00b7 [ht \u2212 1, et] at the time of the sigmoid function."}, {"heading": "4 MMI Models", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Notation", "text": "In the task of generating responses, S should denote a sequence of the input message (source) S = {s1, s2,..., sNS}, where NS denotes the number of words in S. Let T (target) denote a sequence in response to the source message S, where T = {t1, t2,..., tNT, EOS}, NT denotes the length of the answer (terminated by an EOS character), and w denotes a word symbol associated with a K-dimensional unique word embedding ew. V denotes word size."}, {"heading": "4.2 Objective Function", "text": "The standard objective function for sequence sequence sequence models is the log probability of target T of given source S, which results in the optimization problem at test date: T = argmax T {log p (T | S)} (7) As discussed in the introduction, we suspect that this formulation results in generating generic responses because it only specifies for target sources, not vice versa. To fix this, we replace Maximum Mutual Information (MMI) as an objective function. In the MMI, parameters are chosen to maximize (pair-wise) mutual information between source S and target T (pair-wise), since T: log p (S, T) p (T) p (8) Thus, it is avoided favoring responses that have an unconditionally high probability, and instead we tend to the responses that are specific to the given input. The MMI target can be described as follows: 3T = arglog T max T (T) (T max T \u2212 p) p."}, {"heading": "4.3 Practical Considerations", "text": "The answers should be generated either by Equation 9, i.e., p (T | S) - \u03bbp (T) = Equation 10 i.e., (1 \u2212 \u03bb) p (T | S) + \u03bbp (S | T). However, these strategies are difficult to apply directly to decoding, as they can lead to unrammatic answers (under Eq.9) or render decoding unfeasible (under Eq.10). For the rest of this section, we will discuss these problems and explain how we can solve them in practice, since they can lead to ungrammatic answers (T | S) \u2212 \u03bbp (T) The second term (i.e., \u2212 \u03bbp (T) functions as an anti-language model. It punishes not only radio frequency, generic answers, but also flowing answers, and can therefore lead to ungrammatic results. In theory, this problem should not occur if language is less than 1, since ungrammatic judgments are increasingly punished by the first term of the equation."}, {"heading": "4.4 Training", "text": "Recent research has shown that deep LSTMs for SEQ2SEQ tasks work better than single-layer LSTMs (Vinyals et al., 2015; Sutskever et al., 2014; Li et al., 2015). We adopt a deep structure with four LSTM layers for encoding and four LSTM layers for decoding, each of which consists of a different set of parameters. Each LSTM layer consists of 1,000 hidden neurons, and the dimensionality of word embedding is set to 1,000. Further training details are given below, largely following Sutskever et al. (2014). \u2022 LSTM parameters and word embedding are initialized by an even distribution between [-0.08, 0.08]. \u2022 Stochastic gradient decency is implemented with a fixed learning rate of 0.1. \u2022 Batch size is set to 256. \u2022 Grade section is scaled by gradations when a threshold exceeds 1."}, {"heading": "4.5 Decoding", "text": "4.5.1 p (T | S) \u2212 \u03bbU (T) As described in Section 4.3.1, decoding using this model is easy to implement by predicting tokens at each step in time. Furthermore, we have found in our experiments that it is also important to consider the length of the answers when decoding. We therefore combine the loss function linearly with the length punishment, which results in the ultimate score for a specific target T: Score (T) = p (T | S) \u2212 \u03bbU (T) + \u03b3LT (15), with LT denoting the length of the target and \u03b3 the associated weight. We optimize the length weight using the grid search using the N-best lists of response candidates. The N-best lists are created using the decoding process with beam size 200. We determine a maximum length of 20 for candidates generated. The N-best lists are then constructed so that sentences stored with an EOS-best decoding method are then generated as T (T) and T-best (T) decoding method (4.5.2)."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Datasets", "text": "We used an extension of the data set described in Sordoni et al. (2015), which consists of 23 million conversation snippets randomly selected from a collection of 129M context-message-response triples extracted from the Twitter firehose between June and August 2012. For the purposes of our experiments, we limited the context to the turn of phrase in the conversation immediately before the message. In our LSTM models, we used a simple input model in which contexts and messages are linked to form the source code. For coordination and evaluation, we used the development data set (2118 conversations) and the test data set (2114 examples), which was expanded by IR to create a multi-reference set as described by Sordoni et al. (2015). The selection criteria for these two data sets include a component of relevance / interest, with the result that boring answers are explicitly punished."}, {"heading": "5.2 Evaluation", "text": "For parameter optimization and final evaluation, we used BLEU (Papineni et al., 2002), which correlates relatively well with human judgment about the task of response generation (Galley et al., 2015). In the case of Twitter models, we used BLEU with multiple references. Because IMSDB data is too limited to support the extraction of multiple references, only sin-7IMSDB (http: / / www.imsdb.com /) is a relatively small database with approximately 0.4 million records and therefore not suitable for open-domain dialogue training.gle reference BLEU was used in the training and evaluation of OSDb models. We did not follow Vinyals et al. (2015) in using perplexity as a benchmark. Perplexity is probably not a useful metric in our scenario, as our proposed model is designed to move away from the standard SEQ2SEQ model in order to diversify the results by dividing the number of the numbers into the 1 instead."}, {"heading": "5.3 Results", "text": "We first report on the performance on Twitter records in Table 2, along with the results for different models (i.e., MSD and MT + neuronal ranking), which are provided by Sordoni et al. (2015) on the same dataset.Machine translation is the phrase-based MT system described in (Ritter et al., 2011). MT features commonly used in Moses (Koehn et al., 2007), e.g., forward and backward maximum responses to translation probabilities, word and phrase penalties, linear distortions, etc. For more details, refer to Sordoni et al. (2015).MT + neuronal reranking is the phrase system that reranked with neural models. N-best lists are first generated from the MT system. Recurrent neural models generate the scores for N-best candidates that generate input messages."}, {"heading": "6 Conclusions", "text": "We investigated a problem that arose when applying SEQ2SEQ models to the generation of conversation responses: these models tend to generate safe, day-to-day responses (e.g., I do not know), regardless of input. Our analysis suggests that the problem is at least partially due to the use of the traditional objective function, namely the unidirectional probability of output (responses) of given input (messages), which is widely used in Statistical Machine Translation and other machine learning models. To address this problem, we have proposed using Maximum Mutual Information (MMI) as an objective function in neural models to capture not only the dependence of responses on messages, but also the inversion. Experimental results show that the proposed MMI models produce more diverse, interesting and appropriate responses that yield substantial gains in BLEU values in relation to two conversation dates."}], "references": [{"title": "Luke, I am your father: dealing with out-of-domain requests by using movies subtitles", "author": ["David Ameixa", "Luisa Coheur", "Pedro Fialho", "Paulo Quaresma."], "venue": "Intelligent Virtual Agents, pages 13\u201321. Springer.", "citeRegEx": "Ameixa et al\\.,? 2014", "shortCiteRegEx": "Ameixa et al\\.", "year": 2014}, {"title": "Neural machine translation by jointly learning", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Maximum mutual information estimation of hidden Markov model parameters for speech recognition", "author": ["L. Bahl", "P. Brown", "P. de Souza", "R. Mercer."], "venue": "pages 49\u201352.", "citeRegEx": "Bahl et al\\.,? 1986", "shortCiteRegEx": "Bahl et al\\.", "year": 1986}, {"title": "IRIS: a chatoriented dialogue system based on the vector space model", "author": ["Rafael E Banchs", "Haizhou Li."], "venue": "Proc. of the ACL 2012 System Demonstrations, pages 37\u201342. Association for Computational Linguistics.", "citeRegEx": "Banchs and Li.,? 2012", "shortCiteRegEx": "Banchs and Li.", "year": 2012}, {"title": "The Acoustic-modeling Problem in Automatic Speech Recognition", "author": ["Peter F. Brown."], "venue": "Ph.D. thesis, Carnegie Mellon University, Pittsburgh, PA.", "citeRegEx": "Brown.,? 1987", "shortCiteRegEx": "Brown.", "year": 1987}, {"title": "An empirical investigation of sparse log-linear models for improved dialogue act classification", "author": ["Yun-Nung Chen", "Wei Yu Wang", "Alexander Rudnicky."], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 8317\u2013", "citeRegEx": "Chen et al\\.,? 2013", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "\u2206BLEU: A discriminative metric for generation tasks with intrinsically diverse targets", "author": ["Michel Galley", "Chris Brockett", "Alessandro Sordoni", "Yangfeng Ji", "Michael Auli", "Chris Quirk", "Margaret Mitchell", "Jianfeng Gao", "Bill Dolan."], "venue": "Proc. of ACL-", "citeRegEx": "Galley et al\\.,? 2015", "shortCiteRegEx": "Galley et al\\.", "year": 2015}, {"title": "Learning continuous phrase representations for translation modeling", "author": ["Jianfeng Gao", "Xiaodong He", "Wen-tau Yih", "Li Deng."], "venue": "Proc. of ACL, pages 699\u2013709, Baltimore, Maryland. Association for Computational Linguistics.", "citeRegEx": "Gao et al\\.,? 2014", "shortCiteRegEx": "Gao et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Spoken language processing: A guide to theory, algorithm, and system development", "author": ["Xuedong Huang", "Alex Acero", "Hsiao-Wuen Hon", "Raj Foreword By-Reddy."], "venue": "Prentice Hall.", "citeRegEx": "Huang et al\\.,? 2001", "shortCiteRegEx": "Huang et al\\.", "year": 2001}, {"title": "Open source toolkit", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens", "Chris Dyer", "Ondrej Bojar", "Alexandra Constantin", "Evan Herbst"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "A stochastic model of human-machine interaction for learning dialog strategies", "author": ["Esther Levin", "Roberto Pieraccini", "Wieland Eckert."], "venue": "Speech and Audio Processing, IEEE Transactions on, 8(1):11\u201323.", "citeRegEx": "Levin et al\\.,? 2000", "shortCiteRegEx": "Levin et al\\.", "year": 2000}, {"title": "A hierarchical neural autoencoder for paragraphs and documents", "author": ["Jiwei Li", "Minh-Thang Luong", "Dan Jurafsky."], "venue": "arXiv preprint arXiv:1506.01057.", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proc. of ACL-IJCNLP, pages 11\u201319, Beijing, China.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Developing non-goal dialog system based on examples of drama television", "author": ["Lasguido Nio", "Sakriani Sakti", "Graham Neubig", "Tomoki Toda", "Mirna Adriani", "Satoshi Nakamura."], "venue": "Natural Interaction with Robots, Knowbots and Smartphones, pages 355\u2013361.", "citeRegEx": "Nio et al\\.,? 2014", "shortCiteRegEx": "Nio et al\\.", "year": 2014}, {"title": "Stochastic language generation for spoken dialogue systems", "author": ["Alice H Oh", "Alexander I Rudnicky."], "venue": "Proc. of the 2000 ANLP/NAACL Workshop on Conversational systems-Volume 3, pages 27\u201332. Association for Computational Linguistics.", "citeRegEx": "Oh and Rudnicky.,? 2000", "shortCiteRegEx": "Oh and Rudnicky.", "year": 2000}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proc. of ACL, pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Are we there yet? research in commercial spoken dialog systems", "author": ["Roberto Pieraccini", "David Suendermann", "Krishna Dayanidhi", "Jackson Liscombe."], "venue": "Text, Speech and Dialogue, pages 3\u201313. Springer.", "citeRegEx": "Pieraccini et al\\.,? 2009", "shortCiteRegEx": "Pieraccini et al\\.", "year": 2009}, {"title": "Trainable approaches to surface natural language generation and their application to conversational dialog systems", "author": ["Adwait Ratnaparkhi."], "venue": "Computer Speech & Language, 16(3):435\u2013455.", "citeRegEx": "Ratnaparkhi.,? 2002", "shortCiteRegEx": "Ratnaparkhi.", "year": 2002}, {"title": "Unsupervised modeling of twitter conversations", "author": ["Alan Ritter", "Colin Cherry", "Bill Dolan"], "venue": null, "citeRegEx": "Ritter et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2010}, {"title": "Data-driven response generation in social media", "author": ["Alan Ritter", "Colin Cherry", "William Dolan."], "venue": "Proc. of EMNLP, pages 583\u2013593. Association for Computational Linguistics.", "citeRegEx": "Ritter et al\\.,? 2011", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "Hierarchical neural network generative models for movie dialogues", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."], "venue": "arXiv preprint arXiv:1507.04808.", "citeRegEx": "Serban et al\\.,? 2015", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Neural responding machine for short-text conversation", "author": ["Lifeng Shang", "Zhengdong Lu", "Hang Li."], "venue": "ACL-IJCNLP, pages 1577\u20131586.", "citeRegEx": "Shang et al\\.,? 2015", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Meg Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan."], "venue": "Proc. of NAACL-HLT.", "citeRegEx": "Sordoni et al\\.,? 2015", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc Le."], "venue": "Advances in neural information processing systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "News from OPUS \u2013 a collection of multilingual parallel corpora with tools and interfaces", "author": ["J\u00f6rg Tiedemann."], "venue": "Recent advances in natural language processing, volume 5, pages 237\u2013248.", "citeRegEx": "Tiedemann.,? 2009", "shortCiteRegEx": "Tiedemann.", "year": 2009}, {"title": "A neural conversa", "author": ["Oriol Vinyals", "Quoc Le"], "venue": null, "citeRegEx": "Vinyals and Le.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "A trainable generator for recommendations", "author": ["Marilyn A Walker", "Rashmi Prasad", "Amanda Stent"], "venue": null, "citeRegEx": "Walker et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Walker et al\\.", "year": 2003}, {"title": "Improving spoken dialogue", "author": ["William Yang Wang", "Ron Artstein", "Anton Leuski", "David Traum"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Yoshua Bengio"], "venue": null, "citeRegEx": "Bengio.,? \\Q2015\\E", "shortCiteRegEx": "Bengio.", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "Conversational agents are of growing importance in facilitating smooth interaction between humans and their electronic devices, yet traditional hand-crafted dialog (Levin et al., 2000; Young et al., 2010) poses major challenges for scalability and domain adaptation.", "startOffset": 164, "endOffset": 204}, {"referenceID": 19, "context": "Attention has thus turned to learning conversational patterns from data: researchers have begun to explore data-driven generation of conversational responses within the framework of statistical machine translation (SMT), either phrase-based (Ritter et al., 2010), or using neural networks to rerank, or directly in the form of sequence-to-sequence models (SEQ2SEQ) (Sordoni et al.", "startOffset": 241, "endOffset": 262}, {"referenceID": 23, "context": ", 2010), or using neural networks to rerank, or directly in the form of sequence-to-sequence models (SEQ2SEQ) (Sordoni et al., 2015; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2015; Wen et al., 2015).", "startOffset": 110, "endOffset": 213}, {"referenceID": 26, "context": ", 2010), or using neural networks to rerank, or directly in the form of sequence-to-sequence models (SEQ2SEQ) (Sordoni et al., 2015; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2015; Wen et al., 2015).", "startOffset": 110, "endOffset": 213}, {"referenceID": 22, "context": ", 2010), or using neural networks to rerank, or directly in the form of sequence-to-sequence models (SEQ2SEQ) (Sordoni et al., 2015; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2015; Wen et al., 2015).", "startOffset": 110, "endOffset": 213}, {"referenceID": 21, "context": ", 2010), or using neural networks to rerank, or directly in the form of sequence-to-sequence models (SEQ2SEQ) (Sordoni et al., 2015; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2015; Wen et al., 2015).", "startOffset": 110, "endOffset": 213}, {"referenceID": 23, "context": "of scalability and language-independence, together with the capacity to implicitly learn semantic and syntactic relations between pairs, and to capture contextual dependencies (Sordoni et al., 2015) in a way that was not possible with conventional SMT approaches (Ritter et al.", "startOffset": 176, "endOffset": 198}, {"referenceID": 20, "context": ", 2015) in a way that was not possible with conventional SMT approaches (Ritter et al., 2011).", "startOffset": 72, "endOffset": 93}, {"referenceID": 23, "context": "In practice, however, neural conversation models exhibit a tendency to generate trivial or non-committal responses, often involving high-frequency phrases along the lines of I don\u2019t know or I\u2019m OK (Sordoni et al., 2015; Serban et al., 2015; Vinyals and Le, 2015).", "startOffset": 197, "endOffset": 262}, {"referenceID": 21, "context": "In practice, however, neural conversation models exhibit a tendency to generate trivial or non-committal responses, often involving high-frequency phrases along the lines of I don\u2019t know or I\u2019m OK (Sordoni et al., 2015; Serban et al., 2015; Vinyals and Le, 2015).", "startOffset": 197, "endOffset": 262}, {"referenceID": 26, "context": "In practice, however, neural conversation models exhibit a tendency to generate trivial or non-committal responses, often involving high-frequency phrases along the lines of I don\u2019t know or I\u2019m OK (Sordoni et al., 2015; Serban et al., 2015; Vinyals and Le, 2015).", "startOffset": 197, "endOffset": 262}, {"referenceID": 2, "context": "We propose to capture this intuition by using Maximum Mutual Information (MMI), first introduced in speech recognition (Bahl et al., 1986; Brown, 1987), as an optimization objective that measures the mutual dependence between inputs and outputs.", "startOffset": 119, "endOffset": 151}, {"referenceID": 4, "context": "We propose to capture this intuition by using Maximum Mutual Information (MMI), first introduced in speech recognition (Bahl et al., 1986; Brown, 1987), as an optimization objective that measures the mutual dependence between inputs and outputs.", "startOffset": 119, "endOffset": 151}, {"referenceID": 16, "context": "We also find a significant performance boost from the proposed models as measured by BLEU (Papineni et al., 2002).", "startOffset": 90, "endOffset": 113}, {"referenceID": 11, "context": "The first is stochastic models built on top of hand-coded rules or templates (Levin et al., 2000; Young et al., 2010; Walker et al., 2003; Pieraccini et al., 2009; Wang et al., 2011).", "startOffset": 77, "endOffset": 182}, {"referenceID": 27, "context": "The first is stochastic models built on top of hand-coded rules or templates (Levin et al., 2000; Young et al., 2010; Walker et al., 2003; Pieraccini et al., 2009; Wang et al., 2011).", "startOffset": 77, "endOffset": 182}, {"referenceID": 17, "context": "The first is stochastic models built on top of hand-coded rules or templates (Levin et al., 2000; Young et al., 2010; Walker et al., 2003; Pieraccini et al., 2009; Wang et al., 2011).", "startOffset": 77, "endOffset": 182}, {"referenceID": 28, "context": "The first is stochastic models built on top of hand-coded rules or templates (Levin et al., 2000; Young et al., 2010; Walker et al., 2003; Pieraccini et al., 2009; Wang et al., 2011).", "startOffset": 77, "endOffset": 182}, {"referenceID": 15, "context": "The second approach also requires handcrafting, attempting to learn generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; 0; Nio et al., 2014; Chen et al., 2013).", "startOffset": 132, "endOffset": 235}, {"referenceID": 18, "context": "The second approach also requires handcrafting, attempting to learn generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; 0; Nio et al., 2014; Chen et al., 2013).", "startOffset": 132, "endOffset": 235}, {"referenceID": 3, "context": "The second approach also requires handcrafting, attempting to learn generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; 0; Nio et al., 2014; Chen et al., 2013).", "startOffset": 132, "endOffset": 235}, {"referenceID": 14, "context": "The second approach also requires handcrafting, attempting to learn generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; 0; Nio et al., 2014; Chen et al., 2013).", "startOffset": 132, "endOffset": 235}, {"referenceID": 5, "context": "The second approach also requires handcrafting, attempting to learn generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; 0; Nio et al., 2014; Chen et al., 2013).", "startOffset": 132, "endOffset": 235}, {"referenceID": 24, "context": "Inspired by recent progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), other research teams have subsequently attempted to extend these neural techniques to response generation.", "startOffset": 83, "endOffset": 168}, {"referenceID": 7, "context": "Inspired by recent progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), other research teams have subsequently attempted to extend these neural techniques to response generation.", "startOffset": 83, "endOffset": 168}, {"referenceID": 1, "context": "Inspired by recent progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), other research teams have subsequently attempted to extend these neural techniques to response generation.", "startOffset": 83, "endOffset": 168}, {"referenceID": 13, "context": "Inspired by recent progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), other research teams have subsequently attempted to extend these neural techniques to response generation.", "startOffset": 83, "endOffset": 168}, {"referenceID": 21, "context": "There have also been a number of promising attempts to apply direct end-to-end neural encoding-decoding SEQ2SEQ models (Serban et al., 2015; Shang et al., 2015; Vinyals and Le, 2015; Wen et al., 2015).", "startOffset": 119, "endOffset": 200}, {"referenceID": 22, "context": "There have also been a number of promising attempts to apply direct end-to-end neural encoding-decoding SEQ2SEQ models (Serban et al., 2015; Shang et al., 2015; Vinyals and Le, 2015; Wen et al., 2015).", "startOffset": 119, "endOffset": 200}, {"referenceID": 26, "context": "There have also been a number of promising attempts to apply direct end-to-end neural encoding-decoding SEQ2SEQ models (Serban et al., 2015; Shang et al., 2015; Vinyals and Le, 2015; Wen et al., 2015).", "startOffset": 119, "endOffset": 200}, {"referenceID": 16, "context": "A third line of investigation\u2013and the one that we adopt in this paper\u2013was first introduced by Ritter et al. (2011), who framed the response generation task as a statistical machine translation (SMT) problem in which message-response phrase alignments are learned in an unsupervised manner from large volumes of conversational data.", "startOffset": 94, "endOffset": 115}, {"referenceID": 1, "context": ", 2014; Bahdanau et al., 2015; Luong et al., 2015), other research teams have subsequently attempted to extend these neural techniques to response generation. Sordoni et al. (2015) extend the system of Ritter et al.", "startOffset": 8, "endOffset": 181}, {"referenceID": 1, "context": ", 2014; Bahdanau et al., 2015; Luong et al., 2015), other research teams have subsequently attempted to extend these neural techniques to response generation. Sordoni et al. (2015) extend the system of Ritter et al. (2011) by re-ranking the N-best list of a phrasal SMT-based conversation system with a neural language model that incorporates the prior conversational context.", "startOffset": 8, "endOffset": 223}, {"referenceID": 8, "context": "are Long Short-Term Memory (LSTM) neural networks (Hochreiter and Schmidhuber, 1997) that are able to implicitly capture compositionality and longspan dependencies.", "startOffset": 50, "endOffset": 84}, {"referenceID": 9, "context": "Although the MMI optimization criterion has been comprehensively studied for other tasks, such as acoustic modeling in speech recognition (Huang et al., 2001), adapting MMI to SEQ2SEQ training is empirically nontrivial.", "startOffset": 138, "endOffset": 158}, {"referenceID": 24, "context": "Recent research has shown that deep LSTMs work better than single-layer LSTMs for SEQ2SEQ tasks (Vinyals et al., 2015; Sutskever et al., 2014; Li et al., 2015).", "startOffset": 96, "endOffset": 159}, {"referenceID": 12, "context": "Recent research has shown that deep LSTMs work better than single-layer LSTMs for SEQ2SEQ tasks (Vinyals et al., 2015; Sutskever et al., 2014; Li et al., 2015).", "startOffset": 96, "endOffset": 159}, {"referenceID": 12, "context": ", 2014; Li et al., 2015). We adopt a deep structure with four LSTM layers for encoding and four LSTM layers for decoding, each of which consists of a different set of parameters. Each LSTM layer consists of 1,000 hidden neurons, and the dimensionality of word embeddings is set to 1,000. Other training details are given below, broadly following Sutskever et al. (2014). \u2022 LSTM parameters and word embeddings are initialized from a uniform distribution between [-0.", "startOffset": 8, "endOffset": 370}, {"referenceID": 23, "context": "For tuning and evaluation, we used the development dataset (2118 conversations) and the test dataset (2114 examples), augmented using IR to create a multi-reference set, as described by Sordoni et al. (2015). The selection criteria for these two datasets included a component of relevance/interestingness, with the result that dull responses will tend to be penalized in evaluation.", "startOffset": 186, "endOffset": 208}, {"referenceID": 25, "context": "OpenSubtitles dataset In addition to unscripted Twitter conversations, we also used the OpenSubtitles dataset (Tiedemann, 2009), a large, noisy, open-domain dataset containing roughly 60M-70M scripted lines spoken by movie characters.", "startOffset": 110, "endOffset": 127}, {"referenceID": 25, "context": "OpenSubtitles dataset In addition to unscripted Twitter conversations, we also used the OpenSubtitles dataset (Tiedemann, 2009), a large, noisy, open-domain dataset containing roughly 60M-70M scripted lines spoken by movie characters. This dataset does not specify which character speaks each subtitle line, which prevents us from inferring speaker turns. Following Vinyals et al. (2015), we make the simplifying assumption that each line of subtitle constitutes a full speaker turn.", "startOffset": 111, "endOffset": 388}, {"referenceID": 16, "context": "For parameter tuning and final evaluation, we used BLEU (Papineni et al., 2002), which was shown to correlate reasonably well with human judgment on the response generation task (Galley et al.", "startOffset": 56, "endOffset": 79}, {"referenceID": 6, "context": ", 2002), which was shown to correlate reasonably well with human judgment on the response generation task (Galley et al., 2015).", "startOffset": 106, "endOffset": 127}, {"referenceID": 20, "context": "Machine Translation is the phrase-based MT system described in (Ritter et al., 2011).", "startOffset": 63, "endOffset": 84}, {"referenceID": 10, "context": "MT features include commonly used ones in Moses (Koehn et al., 2007), e.", "startOffset": 48, "endOffset": 68}, {"referenceID": 20, "context": ", Machine Translation and MT+neural reranking) reprinted from Sordoni et al. (2015) on the same dataset.", "startOffset": 62, "endOffset": 84}, {"referenceID": 10, "context": "MT features include commonly used ones in Moses (Koehn et al., 2007), e.g., forward and backward maximum likelihood translation probabilities, word and phrase penalties, linear distortion, etc. For more details, refer to Sordoni et al. (2015). MT+neural reranking is the phrase-based MT system, reranked using neural models.", "startOffset": 49, "endOffset": 243}, {"referenceID": 23, "context": "ditional features to score [1-4]-gram matches between context and response and between message and context (context and message match CMM features) are also employed, as in Sordoni et al. (2015). MT+neural reranking achieves a BLEU score of 4.", "startOffset": 173, "endOffset": 195}], "year": 2015, "abstractText": "Sequence-to-sequence neural network models for generation of conversational responses tend to generate safe, commonplace responses (e.g., I don\u2019t know) regardless of the input. We suggest that the traditional objective function, i.e., the likelihood of output (responses) given input (messages) is unsuited to response generation tasks. Instead we propose using Maximum Mutual Information (MMI) as objective function in neural models. Experimental results demonstrate that the proposed objective function produces more diverse, interesting, and appropriate responses, yielding substantive gains in BLEU scores on two conversational datasets.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}