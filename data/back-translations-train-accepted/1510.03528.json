{"id": "1510.03528", "review": {"conference": "icml", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Oct-2015", "title": "$\\ell_1$-regularized Neural Networks are Improperly Learnable in Polynomial Time", "abstract": "We study the improper learning of multi-layer neural networks. Suppose that the neural network to be learned has $k$ hidden layers and that the $\\ell_1$-norm of the incoming weights of any neuron is bounded by $L$. We present a kernel-based method, such that with probability at least $1 - \\delta$, it learns a predictor whose generalization error is at most $\\epsilon$ worse than that of the neural network. The sample complexity and the time complexity of the presented method are polynomial in the input dimension and in $(1/\\epsilon,\\log(1/\\delta),F(k,L))$, where $F(k,L)$ is a function depending on $(k,L)$ and on the activation function, independent of the number of neurons. The algorithm applies to both sigmoid-like activation functions and ReLU-like activation functions. It implies that any sufficiently sparse neural network is learnable in polynomial time.", "histories": [["v1", "Tue, 13 Oct 2015 04:36:09 GMT  (290kb)", "http://arxiv.org/abs/1510.03528v1", "16 pages"]], "COMMENTS": "16 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yuchen zhang", "jason d lee", "michael i jordan"], "accepted": true, "id": "1510.03528"}, "pdf": {"name": "1510.03528.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Yuchen Zhang", "Jason D. Lee", "Michael I. Jordan"], "emails": ["yuczhang@eecs.berkeley.edu", "jasondlee@eecs.berkeley.edu", "jordan@eecs.berkeley.edu"], "sections": [{"heading": null, "text": "ar Xiv: 151 0,03 528v 1 [cs.L G] 13 E"}, {"heading": "1 Introduction", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "1.1 Our contribution", "text": "In this paper, we propose a practical algorithm called the recursive core method for learning multilayered neural networks in the context of improper learning. [20] It shows that there is a nuclear-based method for binary classification with sigmoidal loss that achieves the same generalization error as the best linear classifier. We extend this method to deeper networks. In particular, we assume that the neural network takes d-dimensional input. It has k hidden layers and the incoming weights of each neuron are taught by L. Under these assumptions, the neural predictor whose generalization makes mistakes learns."}, {"heading": "2 Problem Setup", "text": "Let d (p) represent the number of neurons in the p-th layer. Let y (p) i represent the output of the i-th neuron in the p-th layer. Let d (p) represent the number of neurons in the p-th layer. Let y (p) i define the output of the i-th neuron in the p-th layer. Let d (0) = d (k) define the transformation performed by the neural network as follows: y (p) i (p \u2212 1). Let us define the zero-th layer as the input vector so that d (p \u2212 1) j (x) j (k). The transformation performed by the neural network is defined as follows: y (p) i (p \u2212 1)."}, {"heading": "3 Algorithm and Theoretical Result", "text": "Let's start with the recursive definition of a sequence of cores. Let's leave K: RN \u00b7 RN \u00b7 R is a function defined by K (x, y): = 12 \u2212 < x, y >, where both (x, 2 and 2) are limited by one. K is a kernel function, because we can find a mapping of K (x, y) = such that K (x, y) = < x (x), where the function represents an infinite dimensional vector to an infinite dimensional vector. We use xi to use the algorithm 1: Recursive kernel method for learning neural network input: Feature label pairs {(xi, yi)} ni = 1; Loss function k."}, {"heading": "3.1 Algorithm", "text": "Let us suppose that the neural network k has hidden layers. Let Fk represent the reproducing kernel Hilbert Space (RKHS) induced by the kernel K (k) and let Fk, B, Fk be the set of RKHS elements whose norm is limited by B. In view of the training examples {(xi, yi)} ni = 1, we define the predictive power of n: = arg min f, Fk, B1nn, i = 1 (f (xi), yi). According to the repressive theorem, we can represent f, n, byf, n (x) = n, i = 1\u03b1iK (xi, x), wheren, j = 1\u03b1jK (k) (xi, xj), (B2, (5). Calculation of the vector \u03b1 is a convex optimization problem in Rn and can therefore be solved in time poly (n)."}, {"heading": "3.2 Main Result", "text": "If we apply the classical results of learning theory, we can convert the Rademacher complexity of Fk, B by (see, for example, [13]) an upper limit for the generalization loss of the predictor f-n (x) byE [(f-n (x), y) \u2264 arg min f-Fk, B-E [(f (x), y)] + if the sample size is n = (B2 log (1 / \u03b4) / 2). See [20, Theorem 2.2] for proof of this claim. To determine the limit (1), it is sufficient to show that Nk, L, \u0445Fk, B is a constant that depends only on k and L. The following problem defines the claim. See Appendix A for proof. Lemma 1. Suppose that the function f-k exhibits polynomic expansion."}, {"heading": "3.3 Examples", "text": "Our first example is the quadratic activation function: \u03c3sq (x) = x 2.This activation function has been investigated by Livni et al. [18], which relate to a neural network activated by this function as a polynomial network. In theorem 1, when the quadratic activation function is applied, we have H (\u03bb) = 2L\u03bb2. Consequently, we have F (1, L) = 2L2 and more generally F (k, L) \u2264 (2L) \u2264 (2L) --1 \u2212 1 by induction. The sample and the temporal complexity of the algorithms is a polynomial function of (1 / 2), protocol (1 / 3), L) for each constant k.Further, we examine sigmoid-like or ReLU-like activation functions."}, {"heading": "4 Hardness Result", "text": "In Section 3.3, we see that the dependence of time complexity on L is at least exponential for the functions of \u03c3erf and \u03c3sh, but it is polynomial for the quadratic activation. It is therefore natural to wonder whether there is a sigmoid-like or ReLU-like activation function that can translate the time complexity to a polynomial function of L. In this section, we prove that this is impossible under the standard hardness assumptions."}, {"heading": "5 Experiments", "text": "In this section, we compare the proposed algorithm with several baseline algorithms at the MNIST level. As the basic MNIST numbers are relatively easy to classify, we present three variations that pose a greater challenge to the problem. We use the MNIST numbers and three variations thereof. See Figure 2 for the description of this data and several exemplary images. All images are 28 \u00d7 28 in size. For all datasets, we use 10,000 images for validation and 50,000 images for verification. This image is recommended by the source of the data."}, {"heading": "6 Conclusion", "text": "In this paper, we have presented an algorithm and theoretical analysis for the improper learning of multi-layer neural networks. The proposed method, based on a recursively defined core, guarantees to learn the neural network if it has a constant depth and a constant 1-norm. We also present hardness results that show that the time complexity in the 1-layer boundary cannot be polynomial. We compare the algorithm with several basic methods based on the MNIST dataset and its variations. The algorithm learns better predictors than the fully networked multi-layer perceptron, but is surpassed by LeNet5. We consider this method of working as a contribution to ongoing efforts to develop learning algorithms for neural networks that are both theoretically understandable and practically useful."}, {"heading": "A Proof of Lemma 1", "text": "One thinks of any neural network N (0) that we (0) have. (...) Let g (p) i (= > q) i: = = = x (p) j (p) j (p) y (p) j (p) j (p) 1 (p). (...) We assert that g (p) i (p) i (p) i (p) i (= > q) i is a function of the input vector x. (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (.... (...). (...). (.... (...). (...). (...). (...). (.... (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (. (.).). (.). (.). (.). (.). (. (...).). (.). (.).). (. (.).). (. (.). (.).). (. (.). (.). (. (.).). (.).). (..).). (.)........................................................................................"}, {"heading": "B Proof of Proposition 1", "text": "In the case of the RF function, the RF (x) function completes the RF (x) function = 12 + 1 RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-RF-F-RF-RF-RF-RF-RF-RF-RF-RF-F-RF-F-RF-RF-RF-RF-RF-RF-RF-F-RF-F-RF-RF-RF-RF-F-F-RF-RF-RF-RF-F-F-RF-RF-F-RF-RF-RF-RF-F-"}, {"heading": "C Proof of Theorem 3", "text": "We construct a single-layer neural network that encrypts the intersection of T hemispheres. Suppose that the T hemisphere is denoted by gt (x) = w-t x -bt \u2212 bt \u2212 1 / 2. Since both x, wt and bt are integers, we have gt (x) \u2265 1 / 2 if ht (x) = 1, and gt (x) \u2264 -1 / 2 if ht (x) = \u2212 1. We expand x to be (x, 1), then we expand wt to be (wt, bt), and define t t t t t t t (x) = < w; w = x > where x-t: = 1 \u221a d + 1 (x, 1) and w-x: 2\u03bb (wt, bt), where we expand wt (wt, bt), and define t (x)."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "<lb>We study the improper learning of multi-layer neural networks. Suppose that the neural network<lb>to be learned has k hidden layers and that the l1-norm of the incoming weights of any neuron is<lb>bounded by L. We present a kernel-based method, such that with probability at least 1\u2212 \u03b4, it<lb>learns a predictor whose generalization error is at most \u01eb worse than that of the neural network.<lb>The sample complexity and the time complexity of the presented method are polynomial in the<lb>input dimension and in (1/\u01eb, log(1/\u03b4), F (k, L)), where F (k, L) is a function depending on (k, L)<lb>and on the activation function, independent of the number of neurons. The algorithm applies to<lb>both sigmoid-like activation functions and ReLU-like activation functions. It implies that any<lb>sufficiently sparse neural network is learnable in polynomial time.", "creator": "LaTeX with hyperref package"}}}