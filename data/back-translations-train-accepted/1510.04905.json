{"id": "1510.04905", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Oct-2015", "title": "Robust Partially-Compressed Least-Squares", "abstract": "Randomized matrix compression techniques, such as the Johnson-Lindenstrauss transform, have emerged as an effective and practical way for solving large-scale problems efficiently. With a focus on computational efficiency, however, forsaking solutions quality and accuracy becomes the trade-off. In this paper, we investigate compressed least-squares problems and propose new models and algorithms that address the issue of error and noise introduced by compression. While maintaining computational efficiency, our models provide robust solutions that are more accurate--relative to solutions of uncompressed least-squares--than those of classical compressed variants. We introduce tools from robust optimization together with a form of partial compression to improve the error-time trade-offs of compressed least-squares solvers. We develop an efficient solution algorithm for our Robust Partially-Compressed (RPC) model based on a reduction to a one-dimensional search. We also derive the first approximation error bounds for Partially-Compressed least-squares solutions. Empirical results comparing numerous alternatives suggest that robust and partially compressed solutions are effectively insulated against aggressive randomized transforms.", "histories": [["v1", "Fri, 16 Oct 2015 14:59:04 GMT  (450kb,D)", "http://arxiv.org/abs/1510.04905v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["stephen becker", "ban kawas", "marek petrik"], "accepted": true, "id": "1510.04905"}, "pdf": {"name": "1510.04905.pdf", "metadata": {"source": "CRF", "title": "Robust Partially-Compressed Least-Squares", "authors": ["Stephen Becker", "Ban Kawas", "Marek Petrik", "Karthikeyan N. Ramamurthy"], "emails": [], "sections": [{"heading": null, "text": "Randomized matrix compression techniques, such as the Johnson-Lindenstrauss transformation, have proven to be an effective and practical way to efficiently solve major problems. However, with a focus on computational efficiency, forgoing the quality and accuracy of solutions becomes a trade-off. In this paper, we examine compressed least-squares problems and propose new models and algorithms that address the problem of errors and noise caused by compression. While maintaining computational efficiency, our models offer robust solutions that are more accurate than those of classic compressed solutions. We introduce tools of robust optimization along with a form of partial compression to improve the error-time compromise of compressed least-squares solvers. We develop an efficient solution algorithm for our Robust Partially-Compressed (RPC) model, which is based on a reduction based on a one-dimensional search."}, {"heading": "1 Introduction", "text": "We assume that M N and the reference to xLS as a solution to the problem of least compressed quarters is very large."}, {"heading": "2 Robust Partially-Compressed Least-Squares", "text": "In this section, we describe how to integrate robustness into our partially compressed least-squares model (3). As described above, our goal is to improve the quality of the solutions and increase the robustness against noise and error introduced by compression. One way to improve robustness is to use ridge regression, which, when applied to our model (3), gets the following formulation: min x1, 2, - bTAx + \u00b5 x x x x x 2, (5) for some regulation parameters \u00b5. One caveat of using firm regression is that it does not capture the error structure introduced by compression, which is significantly different from the one introduced in the data of the original uncompressed normal least-squares problem, (5) for some regulation parameters \u00b5. One caveat of using firm regression is that it does not capture the error structure introduced by compression."}, {"heading": "2.1 SOCP Formulation of RPC", "text": "While the inner optimization in (6) is a non-convex optimization problem, we show in the following problem that there is a closed solution.Lemma 1. The inner maximization in (6) can be reformulated for each x as follows: max."}, {"heading": "In addition, the maximal value is achieved for \u2206P = \u03c1\u2016Px\u2016\u2016x\u2016Pxx", "text": "T.Proof. The objective function can be performed using the triangular inequality: max. (\u03b2) p + p = p = p = p = p = p = p = p = p = p = p = p = x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "3 Efficient Computation of RPC", "text": "As Section 2 shows, RPC optimization (8) is a second-order convex problem (\u03b2). However, the simple use of conical or SOCP solutions is too slow when applied to large problems. - In this section we describe a faster approach based on a reduction to a one-dimensional search problem. - This section describes a faster approach based on a reduction to a one-dimensional search problem. - In this section, an efficient algorithm 1: Efficient algorithm to solve RPCInput is needed. - In this section, a solution is proposed. - In this section, a faster approach is used. - In this section, a complex solution is needed. - In this section, a complex solution is needed. - In this section, a complex solution is proposed. - In this section, a complex solution is proposed. - In this section, a complex solution is needed. - In this section, a complex solution is found. - In this section, a complex solution is found. - In this section, a complex solution is required."}, {"heading": "4 Approximation Error Bounds", "text": "Compression of the least quadratic problem can significantly speed up the calculation, but it is also important to analyze the quality of the solution of the least quadratic solution. Such an analysis is known for the fully compressed problem of the least quadratic squares (e.g. [11]) and in this section we deduce limits for the partially compressed solution of the least quadratic regression. First, the following simple analysis clarifies the relative compromises in the calculation of the complete or partial projection solutions. Let x? the solution to the complete problem of the least quadratic (3) and z? = b \u2212 Ax? be the residual solution. Remember that ATz? = 0: xCLS is the solution for (2), then: xCLS = (A) \u2212 1ATS = x?"}, {"heading": "5 Empirical Results", "text": "We also examine the computational speed of the algorithms and show that partial compression is essentially as fast as full compression (and therefore sometimes faster than the standard minimum squares), and that robust partial compression is only approximately twice as slow (and asymptotically it is the same cost). Table 3 summarizes the methods we consider for empirical evaluation; all methods were implemented in MATLAB with some of the random prediction codes implemented in C, and multithreaded with the Pthread library; all experiments were performed on the same computer; we consider two methods: 1) use Matlab's least square solution (which is based on LAPACK) and 2)."}, {"heading": "6 Conclusion", "text": "Our experimental results show that the robust partial compression model outperforms both the partially compressed model (with or without regulation) and the fully compressed model. Partial compression alone can also significantly improve the solution quality over full compression. While the partially compressed smallest squares have the same compressed complexity as full compression, the rugged approach brings an additional difficulty in solving the convex opti-mixing problem. By introducing an algorithm based on the search for one-dimensional parameters, even the rugged partially compressed smallest squares can be faster than the ordinary square ones."}, {"heading": "A Minimization of h\u03c4 in Algorithm 1", "text": "In this section we describe how we can efficiently minimize h\u043d (x). Let us remember that h\u043d (\u03b2) is defined in (12). Let us now consider the problem of calculating argminh\u0432 (x) for a fixed solution. In case 2 of (13) there is a solution x 6 = 0 and therefore the function is guaranteed and the optimal conditions can be solved in such a way that \u03b2 = x = 1. Let us leave V DV T = PTP as eigenvalue decomposition of PTP, i.e., Dii = di = 2 i are the squared singular values of P, and V are the correct singular vectors of P = UD T. We make the chadjusted variables to y = V Tx (hence y = x).We define the squared singular values of P, and V are the correct singular vectors of V T. We make the chmatched variables to y = V Tx."}, {"heading": "B Proof of Theorem 2", "text": "The proof. \u2212 The proof uses the stochastic arguments of [11] directly and modifies their deterministic reasoning (Lemma 1). \u2212 If you write shortly x = xPCLS and x? \u2212 xLS. \u2212 From the optimality of x to the partially compressed problem of the smallest squares (3) we have: From the optimality of x? to the equation (1) the gradient is zero at x?, so we have < Ax, Ax? \u2212 b > = 0 for each x, and therefore the use of x = x \u2212 x?, a reordering results in this < A (x \u2212 x?), b > lt; A (x \u2212 x?), A (x \u2212 x?), Ax \u2212 b > = 0 for each x \u2212 x?, a reordering of this results in < A (x \u2212 x?), b > < < A (x \u2212 x?), b > < A (x \u2212 x?), b > < A (x \u2212 x?)"}], "references": [{"title": "Blendenpik: Supercharging lapack\u2019s least-squares solver", "author": ["Haim Avron", "Petar Maymounkov", "Sivan Toledo"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Robust optimization", "author": ["Aharon Ben-Tal", "Laurent El Ghaoui", "Arkadi Nemirovski"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Near-optimal coresets for least-squares regression", "author": ["Christos Boutsidis", "Petros Drineas", "Malik Magdon-Ismail"], "venue": "IEEE transactions on information theory,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Random projections for k-means clustering", "author": ["Christos Boutsidis", "Anastasios Zouzias", "Petros Drineas"], "venue": "In NIPS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Convex Optimization", "author": ["Stephen Boyd", "Lieven Vandenberghe"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Experiments with random projection", "author": ["Sanjoy Dasgupta"], "venue": "In Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2000}, {"title": "Faster least squares approximation", "author": ["Petros Drineas", "Michael W Mahoney", "S Muthukrishnan", "Tam\u00e1s Sarl\u00f3s"], "venue": "Numerische Mathematik,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Robust solutions to least-squares problems with uncertain data", "author": ["Laurent El Ghaoui", "Herv\u00e9 Lebret"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Random projection for high dimensional data clustering: A cluster ensemble approach", "author": ["Xiaoli Zhang Fern", "Carla E Brodley"], "venue": "In ICML,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Randomized algorithms for matrices and data", "author": ["Michael W Mahoney"], "venue": "arXiv preprint arXiv:1104.5557,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Randomized sketches of convex programs with sharp guarantees", "author": ["Mert Pilanci", "Martin J. Wainwright"], "venue": "CoRR, abs/1404.7203,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Clustering by random projections", "author": ["Thierry Urruty", "Chabane Djeraba", "Dan A Simovici"], "venue": "In Advances in Data Mining. Theoretical Aspects and Applications,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}], "referenceMentions": [{"referenceID": 5, "context": "Random projection is a simple and effective dimensionality reduction technique that enables significant speedups in solving large-scale machine learning problems [6, 10].", "startOffset": 162, "endOffset": 169}, {"referenceID": 9, "context": "Random projection is a simple and effective dimensionality reduction technique that enables significant speedups in solving large-scale machine learning problems [6, 10].", "startOffset": 162, "endOffset": 169}, {"referenceID": 10, "context": "It has been successfully used, for example, in classification [11, 13], clustering [4, 9, 12], and least-squares problems [7, 11].", "startOffset": 62, "endOffset": 70}, {"referenceID": 3, "context": "It has been successfully used, for example, in classification [11, 13], clustering [4, 9, 12], and least-squares problems [7, 11].", "startOffset": 83, "endOffset": 93}, {"referenceID": 8, "context": "It has been successfully used, for example, in classification [11, 13], clustering [4, 9, 12], and least-squares problems [7, 11].", "startOffset": 83, "endOffset": 93}, {"referenceID": 11, "context": "It has been successfully used, for example, in classification [11, 13], clustering [4, 9, 12], and least-squares problems [7, 11].", "startOffset": 83, "endOffset": 93}, {"referenceID": 6, "context": "It has been successfully used, for example, in classification [11, 13], clustering [4, 9, 12], and least-squares problems [7, 11].", "startOffset": 122, "endOffset": 129}, {"referenceID": 10, "context": "It has been successfully used, for example, in classification [11, 13], clustering [4, 9, 12], and least-squares problems [7, 11].", "startOffset": 122, "endOffset": 129}, {"referenceID": 6, "context": "The standard approach to doing so proceeds as follows [7].", "startOffset": 54, "endOffset": 57}, {"referenceID": 2, "context": "Then, we solve the fully compressed problem: xCLS def = argmin x 1 2 \u2016\u03a6 (Ax\u2212 b)\u2016 (2) Numerous efficient methods for constructing the compression matrix \u03a6 have been developed; surveys are provided in [3, 7, 10].", "startOffset": 199, "endOffset": 209}, {"referenceID": 6, "context": "Then, we solve the fully compressed problem: xCLS def = argmin x 1 2 \u2016\u03a6 (Ax\u2212 b)\u2016 (2) Numerous efficient methods for constructing the compression matrix \u03a6 have been developed; surveys are provided in [3, 7, 10].", "startOffset": 199, "endOffset": 209}, {"referenceID": 9, "context": "Then, we solve the fully compressed problem: xCLS def = argmin x 1 2 \u2016\u03a6 (Ax\u2212 b)\u2016 (2) Numerous efficient methods for constructing the compression matrix \u03a6 have been developed; surveys are provided in [3, 7, 10].", "startOffset": 199, "endOffset": 209}, {"referenceID": 7, "context": "Described in Section 2, RPC explicitly models errors introduced by compression and is closely related to robust least-squares regression [8].", "startOffset": 137, "endOffset": 140}, {"referenceID": 1, "context": "Leveraging robust optimization techniques makes it possible to reduce the solution error without excessively increasing computational complexity and is a data-driven approach that has been widely used in the last two decades [2].", "startOffset": 225, "endOffset": 228}, {"referenceID": 4, "context": ", [5] [8] Partial Compression new: (3) new: (5) new: (6) Full Compression e.", "startOffset": 2, "endOffset": 5}, {"referenceID": 7, "context": ", [5] [8] Partial Compression new: (3) new: (5) new: (6) Full Compression e.", "startOffset": 6, "endOffset": 9}, {"referenceID": 6, "context": ", [7] e.", "startOffset": 2, "endOffset": 5}, {"referenceID": 4, "context": ", [5] new (but algo.", "startOffset": 2, "endOffset": 5}, {"referenceID": 1, "context": "Robust optimization [2], however, enables us to do exactly that and allows us to explicity model the error structure.", "startOffset": 20, "endOffset": 23}, {"referenceID": 7, "context": "Without compression, it is worth noting that applying robust optimization techniques to the ordinary least-squares problem yields the same solution as applying ridge regression with a datadependent parameter [8].", "startOffset": 208, "endOffset": 211}, {"referenceID": 10, "context": "[11]) and in this section, we derive bounds for the partially-compressed least-squares regression.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "For sub-Gaussian and ROS sketches, we can show that results in [11] can be extended to bound approximation errors for partially-compressed least-squares based on the definition of -optimal above.", "startOffset": 63, "endOffset": 67}, {"referenceID": 0, "context": ", full compression) minx 1 2 \u2016\u03a6 (Ax\u2212 b)\u2016 Regularized Compressed LS minx 1 2 \u2016\u03a6 (Ax\u2212 b)\u2016 + \u03bc 2 \u2016x\u2016 Robust Compressed LS minx max\u2016[\u2206P,\u2206B]\u2016F\u2264\u03c1 1 2 \u2016(\u03a6A+ \u2206P )x\u2212 (\u03a6 + \u2206B)b)\u2016 Partial Compressed LS minx 1 2 \u2016\u03a6Ax\u2016 \u2212 bAx Regularized Partial Compressed LS minx 1 2 \u2016\u03a6Ax\u2016 \u2212 bAx+ \u03bc 2 \u2016x\u2016 Robust Partial Compressed LS minx max\u2016\u2206P\u2016F\u2264\u03c1 1 2 \u2016(\u03a6A+ \u2206P )x\u2016 \u2212 bAx BLENDENPIK minx 1 2 \u2016Ax\u2212 b\u2016 via preconditioned LSQR [1] Table 3: Methods used in the empirical comparison", "startOffset": 396, "endOffset": 399}, {"referenceID": 7, "context": "The robust version is solved following the algorithm outlined in [8] since this can be treated as a robust ordinary least squares problem.", "startOffset": 65, "endOffset": 68}, {"referenceID": 0, "context": "The next dataset represents a set of synthetically generated matrices used in [1].", "startOffset": 78, "endOffset": 81}, {"referenceID": 0, "context": "The timing results on the semi-coherent matrix from [1] are essentially the same as for the incoherent one, hence we do not show them.", "startOffset": 52, "endOffset": 55}], "year": 2015, "abstractText": "Randomized matrix compression techniques, such as the Johnson-Lindenstrauss transform, have emerged as an effective and practical way for solving large-scale problems efficiently. With a focus on computational efficiency, however, forsaking solutions quality and accuracy becomes the trade-off. In this paper, we investigate compressed least-squares problems and propose new models and algorithms that address the issue of error and noise introduced by compression. While maintaining computational efficiency, our models provide robust solutions that are more accurate\u2014relative to solutions of uncompressed least-squares\u2014than those of classical compressed variants. We introduce tools from robust optimization together with a form of partial compression to improve the error-time trade-offs of compressed least-squares solvers. We develop an efficient solution algorithm for our Robust Partially-Compressed (RPC) model based on a reduction to a one-dimensional search. We also derive the first approximation error bounds for Partially-Compressed least-squares solutions. Empirical results comparing numerous alternatives suggest that robust and partially compressed solutions are effectively insulated against aggressive randomized transforms.", "creator": "LaTeX with hyperref package"}}}