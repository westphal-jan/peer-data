{"id": "1510.04935", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Oct-2015", "title": "Holographic Embeddings of Knowledge Graphs", "abstract": "Learning embeddings of entities and relations is an efficient and versatile method to perform machine learning on relational data such as knowledge graphs. In this work, we propose holographic embeddings (HolE) to learn compositional vector space representations of entire knowledge graphs. The proposed method is related to holographic models of associative memory in that it employs circular correlation to create compositional representations. By using correlation as the compositional operator HolE can capture rich interactions but simultaneously remains efficient to compute, easy to train, and scalable to very large datasets. In extensive experiments we show that holographic embeddings are able to outperform state-of-the-art methods for link prediction in knowledge graphs and relational learning benchmark datasets.", "histories": [["v1", "Fri, 16 Oct 2015 16:29:07 GMT  (32kb,D)", "http://arxiv.org/abs/1510.04935v1", null], ["v2", "Mon, 7 Dec 2015 18:05:52 GMT  (29kb,D)", "http://arxiv.org/abs/1510.04935v2", "To appear in AAAI-16"]], "reviews": [], "SUBJECTS": "cs.AI cs.LG stat.ML", "authors": ["maximilian nickel", "lorenzo rosasco", "tomaso a poggio"], "accepted": true, "id": "1510.04935"}, "pdf": {"name": "1510.04935.pdf", "metadata": {"source": "CRF", "title": "Holographic Embeddings of Knowledge Graphs", "authors": ["Maximilian Nickel", "Lorenzo Rosasco", "Tomaso Poggio"], "emails": [], "sections": [{"heading": "Introduction", "text": "Many of the structures that man imposes on the world, such as logical thinking, analogies, or taxonomies, are based on entities, concepts, and their relationships. Therefore, learning from and with relational knowledge representations has long been considered an important task in artificial intelligence (see, for example, Getoor et al.). This work is about learning knowledge graphs (KGs), i.e. knowledge bases that function as substances of binary relationships (BarackObama, Hawaii). This form of knowledge representation can be interpreted as a multigraph, where entities, facts correspond to standardized edges."}, {"heading": "Holographic Embeddings", "text": "In this section, we propose a novel compositional model for KGs. In order to combine the expressiveness of the tensor product with the efficiency and simplicity of TRANSE, we will use the circular correlation of vectors as a compositional operator, i.e., we will present pairs of entities in which we will model the probability of a triple asPr (s, o) action. (9) Due to its links to holographic models of associative memory (which we will discuss in the next section), we refer to eq. (9) as holographic embedding of KGs."}, {"heading": "Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Knowledge Graphs", "text": "To evaluate their performance in linking to knowledge diagrams, we compared HOLE with state-of-the-art models on two commonly used benchmark datasets for this task: WN18 WordNet is a knowledge diagram divided into synonyms and has lexical relationships between instances. WN18 datasets consist of a subset of word folders containing 40,943 relations and 151,442 triplets. FB15k Freebase is a large knowledge diagram that provides general facts about the world (e.g., harvested from Wikipedia, MusicBrainz, etc.) The FB15k datasets consist of a subset of freebase containing 1345 relationship types and 592,213 triplets. For both datasets we used, validation and test splits provided by Bordes et."}, {"heading": "Relational Learning", "text": "In additional experiments, we wanted to test the relational learning capacities of compositional representation using the country datasets of Bouchard et al. (2015), which consist of 244 countries, 22 subregions (e.g., Southern Africa, Western Europe) and 5 regions (Africa, America, Asia, Europe, Oceania). Each country is located in exactly one region and subregion, each subregion is located in exactly one region, and each country may have a number of countries other than neighbors. From the raw data we created, we obtained a relational representation with two predicates: locatedIn (e1, e2) and neighboring countries (e2). The task in the experiment was to predict locatedIn c, r) instances where c ranges across all countries and r across all regions in the data."}, {"heading": "Conclusion and Future Work", "text": "In this work, we proposed HOLE, a compositional vector space model for knowledge graphs based on the circular correlation of vectors. An attractive feature of circular correlation in this context is that it generates representations with fixed width, which means that the compositional representation has the same dimensionality as the representation of its components. In HOLE, we have used this property to create a compositional model that can capture rich relational data interactions, but at the same time be efficiently calculated, easy to train, and remains highly scalable. Experimentally, we have demonstrated that HOLE provides state-of-the-art performance based on a variety of benchmark datasets, and that it can model complex relational patterns while being very sparing in the number of its parameters. In addition, we have highlighted and discussed connections between HOLE and holographic models of associative memory, as John can interpret in this context (this does not only allow memory and associative learning, but also associative learning)."}, {"heading": "Acknowledgments & Reproducibility", "text": "This material is based on work supported by the Center for Brains, Minds and Machines (CBMM) and funded by the NSF STC with the CCF-1231216 Prize. Code for all models used in this paper will be published as an open source library in combination with the code and data for the experiments."}], "references": [{"title": "DBpedia: A Nucleus for a Web of Open Data", "author": ["S. Auer", "C. Bizer", "G. Kobilarov", "J. Lehmann", "R. Cyganiak", "Z. Ives"], "venue": "The Semantic Web, volume 4825 of Lecture Notes in Computer Science. Springer. 722\u2013735.", "citeRegEx": "Auer et al\\.,? 2007", "shortCiteRegEx": "Auer et al\\.", "year": 2007}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["K. Bollacker", "C. Evans", "P. Paritosh", "T. Sturge", "J. Taylor"], "venue": "Proceedings of the 2008 ACM SIGMOD international conference on Management of data, 1247\u20131250. ACM.", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Learning Structured Embeddings of Knowledge Bases", "author": ["A. Bordes", "J. Weston", "R. Collobert", "Y. Bengio"], "venue": "Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence, AAAI\u201911.", "citeRegEx": "Bordes et al\\.,? 2011", "shortCiteRegEx": "Bordes et al\\.", "year": 2011}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["A. Bordes", "N. Usunier", "A. Garcia-Duran", "J. Weston", "O. Yakhnenko"], "venue": "Advances in Neural Information Processing Systems, 2787\u20132795.", "citeRegEx": "Bordes et al\\.,? 2013", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "On approximate reasoning capabilities of low-rank vector spaces", "author": ["G. Bouchard", "S. Singh", "T. Trouillon"], "venue": "2015 AAAI Spring Symposium Series.", "citeRegEx": "Bouchard et al\\.,? 2015", "shortCiteRegEx": "Bouchard et al\\.", "year": 2015}, {"title": "Knowledge Vault: A Web-scale Approach to Probabilistic Knowledge Fusion", "author": ["X. Dong", "E. Gabrilovich", "G. Heitz", "W. Horn", "N. Lao", "K. Murphy", "T. Strohmann", "S. Sun", "W. Zhang"], "venue": "Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discov-", "citeRegEx": "Dong et al\\.,? 2014", "shortCiteRegEx": "Dong et al\\.", "year": 2014}, {"title": "Knowledgebased trust: Estimating the trustworthiness of web sources", "author": ["X.L. Dong", "E. Gabrilovich", "K. Murphy", "V. Dang", "W. Horn", "C. Lugaresi", "S. Sun", "W. Zhang"], "venue": "Proceedings of the VLDB Endowment 8(9):938\u2013949.", "citeRegEx": "Dong et al\\.,? 2015", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research 12:2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Associative holographic memories", "author": ["D. Gabor"], "venue": "IBM Journal of Research and Development 13(2):156\u2013159.", "citeRegEx": "Gabor,? 1969", "shortCiteRegEx": "Gabor", "year": 1969}, {"title": "Structure-mapping: A theoretical framework for analogy", "author": ["D. Gentner"], "venue": "Cognitive science 7(2):155\u2013170.", "citeRegEx": "Gentner,? 1983", "shortCiteRegEx": "Gentner", "year": 1983}, {"title": "Introduction to Statistical Relational Learning", "author": ["L. Getoor", "B. Taskar"], "venue": "The MIT Press.", "citeRegEx": "Getoor and Taskar,? 2007", "shortCiteRegEx": "Getoor and Taskar", "year": 2007}, {"title": "Learning systems of concepts with an infinite relational model", "author": ["C. Kemp", "J.B. Tenenbaum", "T.L. Griffiths", "T. Yamada", "N. Ueda"], "venue": "Proceedings of the 21st National Conference on Artificial Intelligence - Volume 1, AAAI\u201906, 381\u2013388. AAAI Press.", "citeRegEx": "Kemp et al\\.,? 2006", "shortCiteRegEx": "Kemp et al\\.", "year": 2006}, {"title": "Querying factorized probabilistic triple databases", "author": ["D. Krompa\u00df", "M. Nickel", "V. Tresp"], "venue": "The Semantic Web\u2013ISWC 2014. Springer Int. Publishing. 114\u2013129.", "citeRegEx": "Krompa\u00df et al\\.,? 2014", "shortCiteRegEx": "Krompa\u00df et al\\.", "year": 2014}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Y. Lin", "Z. Liu", "M. Sun", "Y. Liu", "X. Zhu"], "venue": "Twenty-Ninth AAAI Conference on Artificial Intelligence.", "citeRegEx": "Lin et al\\.,? 2015", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Vector-based models of semantic composition", "author": ["J. Mitchell", "M. Lapata"], "venue": "Proceedings of ACL-08: HLT, 236\u2013244.", "citeRegEx": "Mitchell and Lapata,? 2008", "shortCiteRegEx": "Mitchell and Lapata", "year": 2008}, {"title": "Inductive logic programming", "author": ["S. Muggleton"], "venue": "New generation computing 8(4):295\u2013318.", "citeRegEx": "Muggleton,? 1991", "shortCiteRegEx": "Muggleton", "year": 1991}, {"title": "A threeway model for collective learning on multi-relational data", "author": ["M. Nickel", "V. Tresp", "H.-P. Kriegel"], "venue": "Proceedings of the 28th International Conference on Machine Learning, ICML \u201911, 809\u2013816.", "citeRegEx": "Nickel et al\\.,? 2011", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "A review of relational machine learning for knowledge graphs", "author": ["M. Nickel", "K. Murphy", "V. Tresp", "E. Gabrilovich"], "venue": "arXiv preprint arXiv:1503.00759. To appear in Proceedings of the IEEE.", "citeRegEx": "Nickel et al\\.,? 2015", "shortCiteRegEx": "Nickel et al\\.", "year": 2015}, {"title": "Holographic reduced representations", "author": ["T. Plate"], "venue": "IEEE Transactions on Neural networks 6(3):623\u2013641.", "citeRegEx": "Plate,? 1995", "shortCiteRegEx": "Plate", "year": 1995}, {"title": "On holographic models of memory", "author": ["T. Poggio"], "venue": "Kybernetik 12(4):237\u2013238.", "citeRegEx": "Poggio,? 1973", "shortCiteRegEx": "Poggio", "year": 1973}, {"title": "Markov logic networks", "author": ["M. Richardson", "P. Domingos"], "venue": "Machine learning 62(1-2):107\u2013136.", "citeRegEx": "Richardson and Domingos,? 2006", "shortCiteRegEx": "Richardson and Domingos", "year": 2006}, {"title": "Some algebraic relations between involutions, convolutions, and correlations, with applications to holographic memories", "author": ["P. Sch\u00f6nemann"], "venue": "Biological cybernetics 56(5-6):367\u2013374.", "citeRegEx": "Sch\u00f6nemann,? 1987", "shortCiteRegEx": "Sch\u00f6nemann", "year": 1987}, {"title": "Tensor product variable binding and the representation of symbolic structures in connectionist systems", "author": ["P. Smolensky"], "venue": "Artificial Intelligence 46(1):159\u2013216.", "citeRegEx": "Smolensky,? 1990", "shortCiteRegEx": "Smolensky", "year": 1990}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["R. Socher", "B. Huval", "C.D. Manning", "A.Y. Ng"], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, 1201\u20131211.", "citeRegEx": "Socher et al\\.,? 2012", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Reasoning With Neural Tensor Networks For Knowledge Base Completion", "author": ["R. Socher", "D. Chen", "C.D. Manning", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems 26. Curran Associates, Inc. 926\u2013934.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Yago: A Core of Semantic Knowledge", "author": ["F.M. Suchanek", "G. Kasneci", "G. Weikum"], "venue": "Proceedings of the 16th International Conference on World Wide Web, 697\u2013706. ACM.", "citeRegEx": "Suchanek et al\\.,? 2007", "shortCiteRegEx": "Suchanek et al\\.", "year": 2007}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Z. Wang", "J. Zhang", "J. Feng", "Z. Chen"], "venue": "TwentyEighth AAAI Conference on Artificial Intelligence.", "citeRegEx": "Wang et al\\.,? 2014", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Infinite hidden relational models", "author": ["Z. Xu", "V. Tresp", "K. Yu", "H. peter Kriegel"], "venue": "Proceedings of the 22nd International Conference on Uncertainity in Artificial Intelligence (UAI", "citeRegEx": "Xu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2006}, {"title": "Embedding entities and relations for learning and inference in knowledge bases", "author": ["B. Yang", "W. Yih", "X. He", "J. Gao", "L. Deng"], "venue": "Proceedings of the International Conference on Learning Representations (ICLR) 2015.", "citeRegEx": "Yang et al\\.,? 2015", "shortCiteRegEx": "Yang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 25, "context": "Modern knowledge graphs such as YAGO (Suchanek et al., 2007), DBpedia (Auer et al.", "startOffset": 37, "endOffset": 60}, {"referenceID": 0, "context": ", 2007), DBpedia (Auer et al., 2007), and Freebase (Bollacker et al.", "startOffset": 17, "endOffset": 36}, {"referenceID": 1, "context": ", 2007), and Freebase (Bollacker et al., 2008) contain billions of facts about millions of entities and have have found important applications in question answering, structured search, and digital assistants.", "startOffset": 22, "endOffset": 46}, {"referenceID": 11, "context": "(2007); Muggleton (1991); Gentner (1983); Kemp et al.", "startOffset": 8, "endOffset": 25}, {"referenceID": 7, "context": "(2007); Muggleton (1991); Gentner (1983); Kemp et al.", "startOffset": 26, "endOffset": 41}, {"referenceID": 7, "context": "(2007); Muggleton (1991); Gentner (1983); Kemp et al. (2006); Xu et al.", "startOffset": 26, "endOffset": 61}, {"referenceID": 7, "context": "(2007); Muggleton (1991); Gentner (1983); Kemp et al. (2006); Xu et al. (2006); Richardson et al.", "startOffset": 26, "endOffset": 79}, {"referenceID": 7, "context": "(2007); Muggleton (1991); Gentner (1983); Kemp et al. (2006); Xu et al. (2006); Richardson et al. (2006)).", "startOffset": 26, "endOffset": 105}, {"referenceID": 11, "context": ", Nickel et al. (2011); Bordes et al.", "startOffset": 2, "endOffset": 23}, {"referenceID": 2, "context": "(2011); Bordes et al. (2013); Krompa\u00df et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 2, "context": "(2011); Bordes et al. (2013); Krompa\u00df et al. (2014)).", "startOffset": 8, "endOffset": 52}, {"referenceID": 2, "context": "(2011); Bordes et al. (2013); Krompa\u00df et al. (2014)). Moreover, embeddings of KGs have been used to support machine reading and to assess the trustworthiness of web sites (Dong et al., 2014, 2015). However, existing embedding models that can capture rich interactions in relational data are often limited in their scalability. Vice versa, models that can be computed efficiently are often considerably less expressive. Here, we approach learning from KGs within the framework of compositional vector space models. We introduce holographic embeddings (HOLE) which use the circular correlation of entity embeddings (vector representations) to create compositional representations of binary relational data. By using correlation as the compositional operator HOLE can capture rich interactions but simultaneously remains efficient to compute, easy to train, and scalable to very large datasets. As we will show experimentally, the proposed approach is able to outperform state-of-the-art embedding models consistently on a variety of benchmark datasets for learning form KGs and relational data. Compositional vector space models have also been considered in cognitive science and natural language processing, e.g., to model symbolic structures, to represent the semantic meaning of phrases, and as models for associative memory (see e.g., Smolensky (1990); Plate (1995); Mitchell et al.", "startOffset": 8, "endOffset": 1354}, {"referenceID": 2, "context": "(2011); Bordes et al. (2013); Krompa\u00df et al. (2014)). Moreover, embeddings of KGs have been used to support machine reading and to assess the trustworthiness of web sites (Dong et al., 2014, 2015). However, existing embedding models that can capture rich interactions in relational data are often limited in their scalability. Vice versa, models that can be computed efficiently are often considerably less expressive. Here, we approach learning from KGs within the framework of compositional vector space models. We introduce holographic embeddings (HOLE) which use the circular correlation of entity embeddings (vector representations) to create compositional representations of binary relational data. By using correlation as the compositional operator HOLE can capture rich interactions but simultaneously remains efficient to compute, easy to train, and scalable to very large datasets. As we will show experimentally, the proposed approach is able to outperform state-of-the-art embedding models consistently on a variety of benchmark datasets for learning form KGs and relational data. Compositional vector space models have also been considered in cognitive science and natural language processing, e.g., to model symbolic structures, to represent the semantic meaning of phrases, and as models for associative memory (see e.g., Smolensky (1990); Plate (1995); Mitchell et al.", "startOffset": 8, "endOffset": 1368}, {"referenceID": 2, "context": "(2011); Bordes et al. (2013); Krompa\u00df et al. (2014)). Moreover, embeddings of KGs have been used to support machine reading and to assess the trustworthiness of web sites (Dong et al., 2014, 2015). However, existing embedding models that can capture rich interactions in relational data are often limited in their scalability. Vice versa, models that can be computed efficiently are often considerably less expressive. Here, we approach learning from KGs within the framework of compositional vector space models. We introduce holographic embeddings (HOLE) which use the circular correlation of entity embeddings (vector representations) to create compositional representations of binary relational data. By using correlation as the compositional operator HOLE can capture rich interactions but simultaneously remains efficient to compute, easy to train, and scalable to very large datasets. As we will show experimentally, the proposed approach is able to outperform state-of-the-art embedding models consistently on a variety of benchmark datasets for learning form KGs and relational data. Compositional vector space models have also been considered in cognitive science and natural language processing, e.g., to model symbolic structures, to represent the semantic meaning of phrases, and as models for associative memory (see e.g., Smolensky (1990); Plate (1995); Mitchell et al. (2008); Socher et al.", "startOffset": 8, "endOffset": 1392}, {"referenceID": 2, "context": "(2011); Bordes et al. (2013); Krompa\u00df et al. (2014)). Moreover, embeddings of KGs have been used to support machine reading and to assess the trustworthiness of web sites (Dong et al., 2014, 2015). However, existing embedding models that can capture rich interactions in relational data are often limited in their scalability. Vice versa, models that can be computed efficiently are often considerably less expressive. Here, we approach learning from KGs within the framework of compositional vector space models. We introduce holographic embeddings (HOLE) which use the circular correlation of entity embeddings (vector representations) to create compositional representations of binary relational data. By using correlation as the compositional operator HOLE can capture rich interactions but simultaneously remains efficient to compute, easy to train, and scalable to very large datasets. As we will show experimentally, the proposed approach is able to outperform state-of-the-art embedding models consistently on a variety of benchmark datasets for learning form KGs and relational data. Compositional vector space models have also been considered in cognitive science and natural language processing, e.g., to model symbolic structures, to represent the semantic meaning of phrases, and as models for associative memory (see e.g., Smolensky (1990); Plate (1995); Mitchell et al. (2008); Socher et al. (2012)).", "startOffset": 8, "endOffset": 1414}, {"referenceID": 4, "context": "For relational data, minimizing the logistic loss has the additional advantage that it can help to find low dimensional embeddings for complex relational patterns (Bouchard et al., 2015).", "startOffset": 163, "endOffset": 186}, {"referenceID": 5, "context": "In this case, negative examples can be generated by heuristics such as the local closed world assumption (Dong et al., 2014).", "startOffset": 105, "endOffset": 124}, {"referenceID": 2, "context": "Here, D+,D\u2212 denote the set of existing and non-existing instances and \u03b3 > 0 specifies the width of the margin (Bordes et al., 2011).", "startOffset": 110, "endOffset": 131}, {"referenceID": 2, "context": "Here, D+,D\u2212 denote the set of existing and non-existing instances and \u03b3 > 0 specifies the width of the margin (Bordes et al., 2011). An important property of compositional models is that the meaning and representation of entities does not vary with regard to their position in the compositional representation (i.e., the i-th entity has the same representation ei as subject and object). Since the representations of all entities and relations are learned jointly in eqs. (2) and (3), this property allows to propagate information between facts, to capture global dependencies in the data, and to enable the desired relational learning effect. For a review of machine learning on knowledge graphs see also Nickel et al. (2015). Existing models for knowledge graphs are based on the following compositional operators:", "startOffset": 111, "endOffset": 727}, {"referenceID": 16, "context": "Compositional models using the tensor product such as RESCAL (Nickel et al., 2011) (also called bilinear model) and the Neural Tensor Network (Socher et al.", "startOffset": 61, "endOffset": 82}, {"referenceID": 24, "context": ", 2011) (also called bilinear model) and the Neural Tensor Network (Socher et al., 2013).", "startOffset": 67, "endOffset": 88}, {"referenceID": 16, "context": "Compositional models using the tensor product such as RESCAL (Nickel et al., 2011) (also called bilinear model) and the Neural Tensor Network (Socher et al., 2013). have shown state-of-the-art performance for learning from KGs. Smolensky (1990) introduced the tensor product as a way to create compositional vector representations.", "startOffset": 62, "endOffset": 245}, {"referenceID": 16, "context": "Compositional models using the tensor product such as RESCAL (Nickel et al., 2011) (also called bilinear model) and the Neural Tensor Network (Socher et al., 2013). have shown state-of-the-art performance for learning from KGs. Smolensky (1990) introduced the tensor product as a way to create compositional vector representations. While the tensor product allows to capture rich interactions, its main problem as a compositional operator lies in the fact that it requires a large number of parameters. Since a \u2297 b explicitly models all pairwise interactions, rp in eq. (1) must be of size d. This can be problematic both in terms of overfitting and computational demands. Since eq. (1) in combination with the tensor product is equivalent to \u03b7spo = es Rpeo, Yang et al. (2015) proposed to use diagonal Rp\u2019s to reduce the number of parameters.", "startOffset": 62, "endOffset": 778}, {"referenceID": 5, "context": "A variant of this compositional operator which also includes a relation embedding in the composite representation has be used in the ER-MLP model of the Knowledge Vault (Dong et al., 2014).", "startOffset": 169, "endOffset": 188}, {"referenceID": 21, "context": "As Socher et al. (2013) noted, the non-linearity \u03c8 provides only weak interactions while leading to a harder optimization problem.", "startOffset": 3, "endOffset": 24}, {"referenceID": 3, "context": "In particular, TRANSE (Bordes et al., 2013) models the score of a fact as the distance between relation-specific translations of entity embeddings: score(Rp(s, o)) = \u2212dist(es + rp, eo) .", "startOffset": 22, "endOffset": 43}, {"referenceID": 25, "context": "Wang et al. (2014) and Lin et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 13, "context": "(2014) and Lin et al. (2015) proposed TRANSH and TRANSR respectively, to improve the performance of TRANSE on 1-to-N, N-to-1, and N-to-N relations.", "startOffset": 11, "endOffset": 29}, {"referenceID": 18, "context": "Figures adapted from Plate (1995).", "startOffset": 21, "endOffset": 34}, {"referenceID": 18, "context": "Equation (12) follows then from the following identities in convolution algebra (Plate, 1995): c>(\u00e3 \u2217 b) = a>(c\u0303 \u2217 b); c>(\u00e3 \u2217 b) = b>(a \u2217 c).", "startOffset": 80, "endOffset": 93}, {"referenceID": 18, "context": "In particular, holographic reduced representations (Plate, 1995) store the association of a with b via the circular convolution", "startOffset": 51, "endOffset": 64}, {"referenceID": 8, "context": ", see Gabor (1969); Poggio (1973)).", "startOffset": 6, "endOffset": 19}, {"referenceID": 8, "context": ", see Gabor (1969); Poggio (1973)).", "startOffset": 6, "endOffset": 34}, {"referenceID": 16, "context": ", Nickel et al. (2015)).", "startOffset": 2, "endOffset": 23}, {"referenceID": 16, "context": "As baseline methods, we used RESCAL (Nickel et al., 2011), TRANSE (Bordes et al.", "startOffset": 36, "endOffset": 57}, {"referenceID": 3, "context": ", 2011), TRANSE (Bordes et al., 2013), TRANSR (Lin et al.", "startOffset": 16, "endOffset": 37}, {"referenceID": 13, "context": ", 2013), TRANSR (Lin et al., 2015), and ERMLP (Dong et al.", "startOffset": 16, "endOffset": 34}, {"referenceID": 5, "context": ", 2015), and ERMLP (Dong et al., 2014).", "startOffset": 19, "endOffset": 38}, {"referenceID": 7, "context": ", SGD with AdaGrad (Duchi et al., 2011) and the ranking loss of eq.", "startOffset": 19, "endOffset": 39}, {"referenceID": 2, "context": "For both datasets we used the fixed training-, validation-, and test-splits provided by Bordes et al. (2013). As baseline methods, we used RESCAL (Nickel et al.", "startOffset": 88, "endOffset": 109}, {"referenceID": 2, "context": "For both datasets we used the fixed training-, validation-, and test-splits provided by Bordes et al. (2013). As baseline methods, we used RESCAL (Nickel et al., 2011), TRANSE (Bordes et al., 2013), TRANSR (Lin et al., 2015), and ERMLP (Dong et al., 2014). To facilitate a fair comparison we reimplemented all models and used the identical optimization method and loss function for training, i.e., SGD with AdaGrad (Duchi et al., 2011) and the ranking loss of eq. (3). This improved the results of TRANSE and RESCAL significantly on both datasets compared to results previously reported by Bordes et al. (2013).2", "startOffset": 88, "endOffset": 611}, {"referenceID": 2, "context": "Following Bordes et al. (2013), we generated negative relation instances for training by corrupting positive triples and used the following evaluation protocol: For each true triple Rp(s, o) in the test set, we replace the subject s with each entity s\u2032 \u2208 E , compute the score for Rp(s, o), and rank all these instances by their scores in decreasing order.", "startOffset": 10, "endOffset": 31}, {"referenceID": 2, "context": "Following Bordes et al. (2013), we generated negative relation instances for training by corrupting positive triples and used the following evaluation protocol: For each true triple Rp(s, o) in the test set, we replace the subject s with each entity s\u2032 \u2208 E , compute the score for Rp(s, o), and rank all these instances by their scores in decreasing order. Since there can exist multiple true triples in the test set for a given predicate-object pair, we remove all instances from the ranking where Rp(s, o) = 1 and s 6= s\u2032, i.e., we consider only the ranking of the test instance among all wrong instances (which corresponds to the \u201cFiltered\u201d setting in Bordes et al. (2013)).", "startOffset": 10, "endOffset": 676}, {"referenceID": 4, "context": "For this purpose, we used the countries dataset of Bouchard et al. (2015), which consists of 244 countries, 22 subregions (e.", "startOffset": 51, "endOffset": 74}], "year": 2017, "abstractText": "Learning embeddings of entities and relations is an efficient and versatile method to perform machine learning on relational data such as knowledge graphs. In this work, we propose holographic embeddings (HOLE) to learn compositional vector space representations of entire knowledge graphs. The proposed method is related to holographic models of associative memory in that it employs circular correlation to create compositional representations. By using correlation as the compositional operator HOLE can capture rich interactions but simultaneously remains efficient to compute, easy to train, and scalable to very large datasets. In extensive experiments we show that holographic embeddings are able to outperform state-ofthe-art methods for link prediction in knowledge graphs and relational learning benchmark datasets.", "creator": "LaTeX with hyperref package"}}}