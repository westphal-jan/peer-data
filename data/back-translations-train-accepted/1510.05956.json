{"id": "1510.05956", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2015", "title": "Optimal Cluster Recovery in the Labeled Stochastic Block Model", "abstract": "We consider the problem of community detection in the labeled Stochastic Block Model (labeled SBM) with a finite number $K$ of communities of sizes linearly growing with the network size $n$. Every pair of nodes is labeled independently at random, and label $\\ell$ appears with probability $p(i,j,\\ell)$ between two nodes in community $i$ and $j$, respectively. One observes a realization of these random labels, and the objective is to reconstruct the communities from this observation. Under mild assumptions on the parameters $p$, we show that under spectral algorithms, the number of misclassified nodes does not exceed $s$ with high probability as $n$ grows large, whenever $\\bar{p}n=\\omega(1)$ (where $\\bar{p}=\\max_{i,j,\\ell\\ge 1}p(i,j,\\ell)$), $s=o(n)$ and $\\frac{n D(p)}{ \\log (n/s)} &gt;1$, where $D(p)$, referred to as the {\\it divergence}, is an appropriately defined function of the parameters $p=(p(i,j,\\ell), i,j, \\ell)$. We further show that $\\frac{n D(p)}{ \\log (n/s)} &gt;1$ is actually necessary to obtain less than $s$ misclassified nodes asymptotically. This establishes the optimality of spectral algorithms, i.e., when $\\bar{p}n=\\omega(1)$ and $nD(p)=\\omega(1)$, no algorithm can perform better in terms of expected misclassified nodes than spectral algorithms.", "histories": [["v1", "Tue, 20 Oct 2015 16:47:27 GMT  (210kb)", "https://arxiv.org/abs/1510.05956v1", "16 pages"], ["v2", "Mon, 26 Oct 2015 01:18:59 GMT  (247kb)", "http://arxiv.org/abs/1510.05956v2", "20 pages"], ["v3", "Mon, 2 Nov 2015 23:50:11 GMT  (392kb)", "http://arxiv.org/abs/1510.05956v3", "32 pages"], ["v4", "Wed, 4 Nov 2015 14:03:41 GMT  (392kb)", "http://arxiv.org/abs/1510.05956v4", "32 pages. arXiv admin note: text overlap witharXiv:1412.7335"], ["v5", "Mon, 21 Dec 2015 01:23:31 GMT  (399kb)", "http://arxiv.org/abs/1510.05956v5", "33 pages. arXiv admin note: text overlap witharXiv:1412.7335"], ["v6", "Sat, 21 May 2016 19:41:08 GMT  (399kb)", "http://arxiv.org/abs/1510.05956v6", "arXiv admin note: text overlap witharXiv:1412.7335"]], "COMMENTS": "16 pages", "reviews": [], "SUBJECTS": "math.PR cs.LG cs.SI stat.ML", "authors": ["se-young yun", "alexandre prouti\u00e8re"], "accepted": true, "id": "1510.05956"}, "pdf": {"name": "1510.05956.pdf", "metadata": {"source": "CRF", "title": "Optimal Cluster Recovery in the Labeled Stochastic Block Model", "authors": ["Se-Young Yun", "Alexandre Proutiere"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 151 0.05 956v 6 [mat h.PR] Clustering under the SBM and its extensions has recently attracted a lot of attention. Most existing work aims to characterize the parameter set in such a way that it is possible to correlate clusters either positively with the true clusters, or with a vanishing proportion of misclassified items, or exactly with the true clusters. We find the parameter set such that there is a cluster algorithm with at most s misclassified items on average below the general LSBM and for each s = o (n) that solves an open problem raised in [2]. We are further developing an algorithm based on simple spectral methods that achieves this basic performance limit within O (npolylog (n) calculations and without a priori knowledge of the model parameters."}, {"heading": "1 Introduction", "text": "We assume that a wide range of disciplines, including social sciences, biology, computer science and statistical physics, apply items and items derived from the observed pairwise similarities between items, most commonly represented by a graph whose recesses are items and edges known to divide similar items. The stochastic block model (SBM), introduced three decades ago, represents a natural performance benchmark for item capture, and has since been extensively studied. In the SBM, items V = {1, n} are divided into non-overlapping clusters V1,.,., VK, which must be recovered from an observed realization of a random graph. In the latter, an edge between two items belonging to clusters Vi and Vj is present with probability."}, {"heading": "2 Related Work and Applications", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Related work", "text": "The results are categorized depending on the desired performance. First, we consider the concept of detectability, the lowest level of performance, which requires that the extracted clusters are positively correlated only with the true clusters. Second, we consider the results of asymptotically accurate recovery, stating that the percentage of misclassified items disappears as n large. Third, we present existing results in terms of exact cluster recovery, which means that no item is accurately correlated with the true clusters. Finally, we report on current work, whose goal is, like ours, to characterize the optimal cluster recovery rate. Detectability and sufficient conditions for detectability were studied for the binary symmetrical SBM (i.e., L = 1, K = 2, p (1, 1, 1, 1) = frugal, and p (1, 2, 2, 2, 2, 1, 1, 1, 1 p = p =, p, p =, p, p = 1, 1, 1, 1, 1 p =, 1 p =, p =, p = 1 p, 1, 1 p =, 1 p =, 1 p =, 1 p =, p =, p = 1."}, {"heading": "2.2 Applications", "text": "In all examples, f (n) is a function such as f (n) = \u03c9 (1), and a, b are fixed real numbers, so a > b = b.The binary SBM. Consider the binary SBM, where the average item degree (f (n) and the average item degree (f) is n (n) and is represented by an LSBM with the parameters L = 1, K = 2, \u03b1 = (1 \u2212 1). Theorems 1 and 2 yield the optimal number of incorrectly classified vertices as n exp (\u2212 g (\u2212 1), a)."}, {"heading": "3 Fundamental Limits: Change of Measures through Coupling", "text": "In this section we will explain the construction of the proof of the Q question, which is based on a suitable argument of scale change, which is often used to identify upper limits of performance in online stochastic optimization problems [15]. In the following, we will refer to the corresponding probability measurement (or expectation). In our argument of scale change, we will construct a second stochastic model (whose corresponding probability measurement and expectation are each pump and Ep), using a change in measurements from pump to pump, we will refer to the expected number of incorrectly classified items Ep (s)."}, {"heading": "4 The Spectral Partition Algorithm and its Optimality", "text": "The first part of the algorithm can be interpreted as initialization for its second part, and consists in the application of a spectral decomposition of n + n random matrix A, which is constructed by the observed labels. More precisely: A = 1 wB = 1 wB = 1 wB = 1 wB = 1 wB = 1 wB = 1 wB = 1 wB = 1 wB = 1 wB = 1 wB = 1 wB = 1 wB = 1 wB = 1 wB = 1 wB = 1 wB = 1 wB, wB = 1 wB = 1 wB = 1 wB, wB = 1 wB = 1 wB = 1 wB, wB = 1 wB = 1 wB, wB = 1 wB = 1 wB = 1 wB, wB = 1 wB = 1 wB = 1 wB = 1 wB, wB = 1 wB = 1 wB = 1 wB = 1, wB = 1 wB = 1, wB = 1 wB = 1 wB = 1, wB = 1 wB = 1 wB = 1"}, {"heading": "A The SP Algorithm", "text": "In this section we present the spectral partition (SP) algorithm. SP's main pseudo-code is represented in algorithm V. The SP algorithm consists of two parts. In the first part, which corresponds to lines 1-4 in the pseudo-code, we apply a spectral decomposition of the matrix. The second part of the SP algorithm, which corresponds to lines 5 and 6 in algorithm 1, consists in improving the clusters first identified and then applying the spectral decomposition algorithm, whose pseudo-code is represented in algorithm 2. The second part of the SP algorithm, which corresponds to lines 5 and 6 in algorithm 1, consists in improving the clusters identified in the first step."}, {"heading": "B Properties of the divergence D(\u03b1, p) and related quantities", "text": "In this section we substantiate the two assertions of section 2 as well as further results of divergence D (\u03b1, p), which will be important for the proof of the theorems. \u2212 Algorithm 2 Spectral decomposition input: A \u00b2, p \u00b2 1. Iterative power method with singular value threshold (initialization) n \u00b2, k \u00b2 0 and U \u00b2 0n \u00b2 1, while the iterative power method (iterative power method) Ut \u00b2, k \u00b2 2 log \u00b2 (n), p \u00b2 U0 (orthonoralization) U \u00b2 k \u00b2, Ut \u00b2 1: k \u2212 1 (U \u00b2 1: k \u2212 1Ut), Ut \u00b2 n \u00b2 (iterative power method) Ut \u00b2 (iterative power method) 1: k \u00b2, p \u00b2 2 (emulation of the k \u00b2 singular value) Ut \u00b2, Ut \u00b2 1: k \u2212 1 (U \u00b2 -1Ut), p \u00b2 -2K \u00b2 of the singular value."}, {"heading": "B.1 Proof of Claim 4", "text": "DL (p (i), p (j) is the minimum objective function of k = k = k = k = y = y (y = 1 = 1 x optimization problem: min y (PK) \u00b7 p (l + 1) k (l + 1) k (k = 1 x) log (y = 1 x) p () p (k) p (k), k \u2212 p (\u2212 k) p \u2212 p (k,) p () p () 1 \u2212 p () p (LLLLLLLLLLLL), k () p (k) p () p (k) p (L)), p (i () p () p \u2212 p () p () p (,) p (k () p () p (LLLLLLLLLLLLLLLLLL) = 1 p = 1 p = 1 p (i,) p (i,) p (i () p () p () p () p () p (p () p (k) (k) (p) (k) (k) (k) (p) (p)) (p) (p)) (k) (k)) (p) (p)) (p)) (k) (p) (p))"}, {"heading": "B.2 Proof of Claim 5", "text": "When p = o (1), for all i 6 = j, \u03b1i = 1K, p (i, i,) = p (), and p (i, j,) = q (), from claim 4, DL + (\u03b1, p (i), p (j)) = max. (0.1) K k = 1L k (((((1 \u2212) p (i, k,)) + p (j, k,) \u2212 p (i, k) \u2212 p () 1 \u2212 p (i, k,) 1 \u2212 p (j, k,)."}, {"heading": "B.3 Other properties", "text": "Lemma 7 Let (i, j) = argmini, j DL + (q), p (j), p (j), p (j), p (j), p (p), p (p), p (p), p (j), p (j), k (p), k (k), k (k), k (k), k (k), k (k), k (k), k (k), k (k), k (k), k (k), k (k), k (k), k (k), p (k), p (k), k (k), p (k), k (k), k (k), k (k), k (k), k (k), k (k), k (k), k (k), k (k), k (k), k (k), k (k), k (k), k (k), k (k), k (k), k (k, k), k (k), k (k), k (k), k (k), k (k), k (k), k (k), k (k), k (k), k (k), k (k (k), k (k), k (k), k (k (k), k (k), k (k), k (k (k), k (k), k (k (k), k (k), k (k (k), k (k (k), k (k), k (k (k, k), k (k, k (k, k), k (k (k), k (k), k (k (k, k, k), k (k, k (k, k, k, k), k (k (k, k), k (k, k), k (k (k), k (k, k), k (k (k, k, k), k (k), k (k (k, k, k), k (k, k, k), k (k (k), k (k, k, k, k"}, {"heading": "C Proof of Theorem 1", "text": "The proof is a reasonable change of scale. The originality of the proof stems from the fact that the change in measurements is achieved by a forward-looking coupling reasoning. [17] In the following, we point out that all observed random indicators are defined by the parameters, and that the nodes to the various clusters are calculated according to the distribution. [17] The labels between two nodes are then generated by means of distributions. [17] The proof consists in the construction of a perturbed stochastic model that generates the labels, the labels that are generated under the labels. [17] We denote the labels (resp) = E [17] the probability measurement (resp. expectation) under the perturbed stochastic model that generates the labels that are generated under the labels."}, {"heading": "D Performance of the SP Algorithm \u2013 Proof of Theorem 2", "text": "Notations. We use the standard matrix standard: \"A\" = \"A\" = \"A\" = \"A\" = \"A\" = \"A\" = \"V\" = \"V\" (V). We define by \"M\" the expectation of the matrix of \"A,\" i.e., \"M\" u, \"\" V \"=\" L \"and\" V. \"We define by\" M \"the expectation of the matrix of\" A, \"i.e.,\" M \"u,\" \"V\" = \"L\" (i, j,), if u \"V\" and \"V\" Vj. \"We define the matrix\" L \"= 1\" M, \"i.e. the matrix formed by the lines and columns of\" R, \"their indices in\" H. \"Therefore, we can define the\" A, \"M\" and \"M,\" where \"L\" is the items generated after the trimming process \"(line 3)."}, {"heading": "D.1 Preliminary lemmas", "text": "Lemma 10 for each v (V) and c (4) for each v (V), we haveP (V) for each v (V), we haveP (V), we haveP (V), we haveP (V), we haveP (V), we haveP (V), we haveP (V), we haveP (V), we haveP (V), we haveP (V), we haveP (V), we haveP (V), we haveP (V), we haveP (V), we haveP (V), we haveP (V), we haveV (V), we haveP (V), we haveP (V), we (V), we (P)."}, {"heading": "D.2 Part 1 of the SP algorithm \u2013 Proof of Theorem 6", "text": "Man recalls A-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-"}, {"heading": "D.3 Proof of Theorem 2", "text": "Von Tschernoff bound, this means with high probability that | Vk-k-k-k-k-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-"}, {"heading": "D.3.1 Proof of Proposition 13 \u2013 Size of V \\H", "text": "We calculate the number of items that are satisfactory (H1), (H2), and (H3) that are satisfactory (H3), and (H3) that are satisfactory (H3). (33) Number of items that are satisfactory (H2): We will prove that if v is satisfactory (H1), v is satisfactory (H2), as well as with probability (H2), also with probability (H1). (\u2212 exp (\u2212 exp) Number of items that are satisfactory (H2). (34) To achieve this goal, we must first determine that we are unsatisfactory (H2), v satisfactory (H2), that we are unsatisfactory (H2), p)."}, {"heading": "D.3.2 Proof of Proposition 14", "text": "Recall that in the case of loss of generality we assume that the number of incorrectly classified items in H after the t-th step E (t) = (t) k (k))) (t) k (k))) (n (n) k (n) k (n) k (n) k (n) k (n) k (n) k (n) k (n) e (n) e (n) k (n) e (n) k (n) e (n) k (n) e (n) e (n) e (n) e (n) e (n) e (n) n (n) k (n) k (n) k (n) k (n) k (n) k (n) n (n) k (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n), n (n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n (n) n (n) n (n) n) n (n (n) n (n), n (n (n (n) n) n) n (n (n (n) n (n) n) n (n) n (n) n (n (n) n (n), n (n (n) n (n (n) n) n (n) n) n (n) n (n) n (n (n), n (n (n) n (n) n (n) n) n (n) n (n (n) n (n (n (n) n), n (n (n) n) n (n (n) n) p (e (e (e (e (e (e) n) n) n) p (e (e (e (e e e) n) n) p (e (e"}, {"heading": "E Proof of Theorem 3", "text": "We obtain the positive result by setting theorem 2 to s = 12. If lim infn \u2192 \u221e nD (\u03b1, p) log (n) \u2265 1, SP algorithm cluster exactly with high probability, it is sufficient to show the negative result. We prove the negative part by contradiction. Let us consider a maximum a posteriori (MAP) estimate with complete parameter information. If we look at labeled information A, the MAP estimates the clusters as follows: (S-k) k = 1,..., k = arg max (Sk) k = 1,.., KP {(Sk) k = 1,.., K | \u03b1, p, K, A}. (47) Let us let \u03b5MAP denote the number of incorrectly classified nodes by the MAP estimate. From the definition of the MAP estimate it follows that for each string algorithm \u03c0, P {\u03b5MAP, K, A}. (48) Let us specify the number of incorrectly classified nodes by the MAP estimate."}, {"heading": "1 \u2264 k \u2264 K , \u2211L\u2113=1 xk,\u2113 = \u0398(np\u0304) for all k, and", "text": "k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = v = x, using the K + 1 matrix whose (k, + 1) element e (v, e (p) = x \u2212 nD (1 + o (1) if v Vi Vi and D = L = Vi e ("}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "We consider the problem of community detection or clustering in the labeled Stochastic Block Model (LSBM) with a finite number K of clusters of sizes linearly growing with the global population of items n. Every pair of items is labeled independently at random, and label l appears with probability p(i, j, l) between two items in clusters indexed by i and j, respectively. The objective is to reconstruct the clusters from the observation of these random labels. Clustering under the SBM and their extensions has attracted much attention recently. Most existing work aimed at characterizing the set of parameters such that it is possible to infer clusters either positively correlated with the true clusters, or with a vanishing proportion of misclassified items, or exactly matching the true clusters. We find the set of parameters such that there exists a clustering algorithm with at most s misclassified items in average under the general LSBM and for any s = o(n), which solves one open problem raised in [2]. We further develop an algorithm, based on simple spectral methods, that achieves this fundamental performance limit within O(npolylog(n)) computations and without the a-priori knowledge of the model parameters.", "creator": "LaTeX with hyperref package"}}}