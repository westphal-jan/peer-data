{"id": "1510.07389", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Oct-2015", "title": "The Human Kernel", "abstract": "Bayesian nonparametric models, such as Gaussian processes, provide a compelling framework for automatic statistical modelling: these models have a high degree of flexibility, and automatically calibrated complexity. However, automating human expertise remains elusive, for example, Gaussian processes with standard kernels struggle on function extrapolation problems that are trivial for human learners. In this paper, we create function extrapolation problems and acquire human responses, and then design a kernel learning framework to reverse engineer the inductive biases of human learners across a set of behavioral experiments. We use the learned kernels to gain psychological insights and to extrapolate in human-like ways that go beyond traditional stationary and polynomial kernels. Finally, we investigate Occam's razor in human and Gaussian process based function learning.", "histories": [["v1", "Mon, 26 Oct 2015 07:39:47 GMT  (328kb,D)", "https://arxiv.org/abs/1510.07389v1", "11 pages, 5 figures. To appear in Neural Information Processing Systems (NIPS) 2015"], ["v2", "Tue, 27 Oct 2015 18:21:11 GMT  (328kb,D)", "http://arxiv.org/abs/1510.07389v2", "11 pages, 5 figures. To appear in Neural Information Processing Systems (NIPS) 2015"], ["v3", "Thu, 3 Dec 2015 18:07:35 GMT  (328kb,D)", "http://arxiv.org/abs/1510.07389v3", "11 pages, 5 figures. To appear in Neural Information Processing Systems (NIPS) 2015. Version 2: Figure 2 (i)-(n) now displays the second set of progressive function learning experiments"]], "COMMENTS": "11 pages, 5 figures. To appear in Neural Information Processing Systems (NIPS) 2015", "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["andrew gordon wilson", "christoph dann", "christopher g lucas", "eric p xing"], "accepted": true, "id": "1510.07389"}, "pdf": {"name": "1510.07389.pdf", "metadata": {"source": "CRF", "title": "The Human Kernel", "authors": ["Andrew Gordon Wilson", "Christoph Dann", "Christopher G. Lucas", "Eric P. Xing"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2 Related Work", "text": "In the past, efforts to understand human functioning have focused on rules-based relationships (e.g., polynomial or power functions) [15, 16], or on interpolation based on similarity (17, 18]. Griffiths et al. [19] were the first to note that a Gaussian process can be used to unify these two perspectives; they introduced a GP model with a mixture of RBF and polynomial cores to reflect the human ability to identify arbitrary smooth functions, while applying this model to a standard set of assessment tasks that compares predictions of simple functions, and interpolar cores to capture the rate of human error. [20] expanded this model to take into account a broader range of phenomena that we use to obtain an infinite mix of evaluations."}, {"heading": "3 The Human Kernel", "text": "Rules-based and associative theories of human function learning can be unified as part of a Gaussian process framework. Gaussian processes actually contain a wide range of probability models and have the nonparametric flexibility to produce an infinite number of consistent (zero training errors) fits for each dataset. Furthermore, the support and inductive distortions of a Gaussian process are enveloped by a covariant core. Our goal is to learn Gaussian covariance cores from predictions made by humans in functional learning experiments to gain a better understanding of human learning and inspire new machine learning models with improved extrapolation performance and minimal human intervention."}, {"heading": "3.1 Problem Setup", "text": "A (human) learner gains access to data y for training input X and makes predictions y * for test input X. We assume that predictions y * are samples from the posterior distribution of the learner about possible functions. Results show that human inferences and judgments in a wide range of perceptual and decision tasks resemble posterior samples [27, 28, 29]. We assume that for a given X and y we can obtain several y-drawings."}, {"heading": "3.2 Kernel Learning", "text": "In the common Gaussian process applications, one has access to a single realization of data y >, and performs kernel learning by optimizing the marginal probability of data relating to the hyperparametrix covariance function, as described in the supplementary material. However, with only a single realization of data, we are severely limited in our ability to learn an expressive core function - which requires us to make strong assumptions, such as RBF covariances, in order to extract any useful information from the data. One can see this by simulating N data points from a GP with a known kernel, and then the empirical estimation of the known covariance matrix K. The empirical estimate, in most cases, will not look like K., perhaps surprisingly, if we even have a small number of multiple drawings from a GP, we can see a wide range of coance matrices K using the empirical estimator Y > / M - y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y."}, {"heading": "4 Human Experiments", "text": "We would like to discover nuclei that capture human inductive biases for learning functions and extrapolate them from complex or ambiguous training data. Let's start by testing the consistency of our kernel learning procedure in Section 4.1. In Section 4.2, we will examine progressive functional learning. In fact, human participants will have a different representation (e.g. learned nucleus) for different observed data, and examine how these representations gradually adjust with new information to shed light on our previous biases. In Section 4.3, we will learn human nuclei to extrapolate tasks that are difficult for Gaussian processes with standard nuclei. In Section 4.4, we will examine model selection in learning human function. All human participants were recruited with Amazon's Mechanical Turk and saw experimental materials described in the supplement with demonstrations on http: / / functionlearning.com /. When considering stationary basic truth nuclei, we will use a spectral learning blend; otherwise, we will use a spectral learning for the kernel."}, {"heading": "4.1 Reconstructing Ground Truth Kernels", "text": "We use simulations with a known basic truth to test the consistency of our kernel learning procedure, and the effects of multiple posterior drawings by converging to a kernel used for prediction. We sample 20 data points from a GP with an RBF kernel (the appendix describes GPs), kRBF (x, x) = exp (\u2212 0.5 | x \u2212 x \u2032 | / '2), at random input locations. Building on this data, we sample several posterior drawings y (1),.. y (W) \u0445, each containing 20 data points, from a GP with a spectral mixing kernel [14] with two components (the prediction kernel). The prediction kernel was deliberately not trained to match the data kernel. To reconstruct the prediction kernel frequency, we learn the parameters of a randomly initialized spectral mix with five components."}, {"heading": "4.2 Progressive Function Learning", "text": "We asked the human to extrapolate the training data into two sets of 5 functions, each derived from GPs with known nuclei. Students extrapolated these problems in sequence and thus had the opportunity to learn more about the underlying nucleus in each sentence. To learn further test functions, we repeated the first function at the end of the experiment by requesting the extrapolation assessments because they provided more information about the inductive distortions than about the interpolation, and placed difficulties for the conventional Gaussian process nuclei in space. The functions observed were shown in the form of black in Figure 2, the human reactions in blue, and the true extrapolations in black. In the first two lines, the black functions were drawn with a rational square nucleus."}, {"heading": "4.3 Discovering Unconventional Kernels", "text": "The experiments mentioned in this section follow the same general procedure described in Section 4.2. In this case, 40 people were asked to extrapolate from two individual sets of training in mutual order: a saw tooth function (Figure 3 (b)), with training data presented as turned black lines. These types of functions are notoriously difficult for the standard column process cores [12], due to sharp discontinuities and non-stationary behavior. In Figures 3 (b), 3 (c), we use agglomerative clusters to process human responses in three categories shown, green, and blue."}, {"heading": "4.4 Human Occam\u2019s Razor", "text": "This year it is so far that it will be able to use the mentionlcihsrcsrteeS rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for"}, {"heading": "5 Discussion", "text": "We have shown that (1) human learners have systematic expectations of smooth functioning that differ from the inductive distortions inherent in the RBF cores used in earlier models of functional learning; (2) that it is possible to extract cores that reproduce qualitative characteristics of human inductive distortions, including variable saw-tooth and step patterns; (3) that human learners prefer smoother or simpler functions, even compared to GP models that tend to overestimate complexity; and (4) that it is possible to build models that extrapolate in a human way that go beyond traditional stationary and polynomial cores; and we have focused on human extrapolation from noise-free non-parametric relationships; this approach complements previous work that emphasized simple parametric functions and the role of noise [e.g. 25], but kernel learning could also be applied in these environments."}, {"heading": "Acknowledgments", "text": "We thank Tom Minka for helpful conversations."}], "references": [{"title": "A logical calculus of the ideas immanent in nervous activity", "author": ["W.S. McCulloch", "W. Pitts"], "venue": "Bulletin of mathematical biology,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1943}, {"title": "Pattern Recognition and Machine Learning", "author": ["Christopher M. Bishop"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Bayesian brain: probabilistic approaches to neural coding", "author": ["K. Doya", "S. Ishii", "A. Pouget", "R.P.N. Rao"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Probabilistic machine learning and artificial", "author": ["Zoubin Ghahramani"], "venue": "intelligence. Nature,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "An internal model for sensorimotor integration", "author": ["Daniel M Wolpert", "Zoubin Ghahramani", "Michael I Jordan"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1995}, {"title": "Perception as Bayesian inference", "author": ["David C Knill", "Whitman Richards"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1996}, {"title": "Bayesian spiking neurons i: inference", "author": ["Sophie Deneve"], "venue": "Neural computation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Optimal predictions in everyday cognition", "author": ["Thomas L Griffiths", "Joshua B Tenenbaum"], "venue": "Psychological Science,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "How to grow a mind: Statistics, structure, and abstraction", "author": ["J.B. Tenenbaum", "C. Kemp", "T.L. Griffiths", "N.D. Goodman"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Covariance kernels for fast automatic pattern discovery and extrapolation with Gaussian processes", "author": ["Andrew Gordon Wilson"], "venue": "PhD thesis, University of Cambridge,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Bayesian Learning for Neural Networks", "author": ["R.M. Neal"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1996}, {"title": "Gaussian processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Exploiting generative models in discriminative classifiers", "author": ["Tommi Jaakkola", "David Haussler"], "venue": "Advances in neural information processing systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Gaussian process kernels for pattern discovery and extrapolation", "author": ["Andrew Gordon Wilson", "Ryan Prescott Adams"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Functional learning: The learning of continuous functional mappings relating stimulus and response continua", "author": ["J Douglas Carroll"], "venue": "ETS Research Bulletin Series,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1963}, {"title": "Function learning: Induction of continuous stimulusresponse relations", "author": ["Kyunghee Koh", "David E Meyer"], "venue": "Journal of Experimental Psychology: Learning, Memory, and Cognition,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1991}, {"title": "Extrapolation: The sine qua non for abstraction in function learning", "author": ["Edward L DeLosh", "Jerome R Busemeyer", "Mark A McDaniel"], "venue": "Journal of Experimental Psychology: Learning, Memory, and Cognition,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1997}, {"title": "Learning functional relations based on experience with input-output pairs by humans and artificial neural networks", "author": ["Jerome R Busemeyer", "Eunhee Byun", "Edward L Delosh", "Mark A McDaniel"], "venue": "Concepts and Categories,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1997}, {"title": "Modeling human function learning with gaussian processes", "author": ["Thomas L Griffiths", "Chris Lucas", "Joseph Williams", "Michael L Kalish"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "A rational model of function learning", "author": ["Christopher G Lucas", "Thomas L Griffiths", "Joseph J Williams", "Michael L Kalish"], "venue": "Psychonomic bulletin & review,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Infinite mixtures of Gaussian process experts. In Advances in neural information processing systems", "author": ["C.E. Rasmussen", "Z. Ghahramani"], "venue": "14: proceedings of the 2002 conference,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2002}, {"title": "Superspace extrapolation reveals inductive biases in function learning", "author": ["Christopher G Lucas", "Douglas Sterling", "Charles Kemp"], "venue": "In Cognitive Science Society,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "The conceptual basis of function learning and extrapolation: Comparison of rule-based and associative-based models", "author": ["Mark A Mcdaniel", "Jerome R Busemeyer"], "venue": "Psychonomic bulletin & review,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}, {"title": "Iterated learning: Intergenerational knowledge transmission reveals inductive biases", "author": ["Michael L Kalish", "Thomas L Griffiths", "Stephan Lewandowsky"], "venue": "Psychonomic Bulletin & Review,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}, {"title": "Simplicity bias in the estimation of causal functions", "author": ["Daniel R Little", "Richard M Shiffrin"], "venue": "In Proceedings of the 31st Annual Conference of the Cognitive Science Society,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Simplicity and goodness-of-fit in explanation: The case of intuitive curve-fitting", "author": ["Samuel GB Johnson", "Andy Jin", "Frank C Keil"], "venue": "In Proceedings of the 36th Annual Conference of the Cognitive Science Society,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Multistability and perceptual inference", "author": ["Samuel J Gershman", "Edward Vul", "Joshua B Tenenbaum"], "venue": "Neural computation,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Bridging levels of analysis for probabilistic models of cognition", "author": ["Thomas L Griffiths", "Edward Vul", "Adam N Sanborn"], "venue": "Current Directions in Psychological Science,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "One and done? optimal decisions from very few samples", "author": ["Edward Vul", "Noah Goodman", "Thomas L Griffiths", "Joshua B Tenenbaum"], "venue": "Cognitive science,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Fast kernel learning for multidimensional pattern extrapolation", "author": ["Andrew Gordon Wilson", "Elad Gilboa", "Arye Nehorai", "John P. Cunningham"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Information theory, inference, and learning algorithms", "author": ["David JC MacKay"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2003}, {"title": "Occam\u2019s razor", "author": ["Carl Edward Rasmussen", "Zoubin Ghahramani"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2001}, {"title": "Computation with infinite neural networks", "author": ["Christopher KI Williams"], "venue": "Neural Computation,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1998}, {"title": "A process over all stationary kernels", "author": ["Andrew Gordon Wilson"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Therefore it is not surprising that early machine learning efforts, such as the perceptron, have been neurally inspired [1].", "startOffset": 120, "endOffset": 123}, {"referenceID": 1, "context": "In recent years, probabilistic modelling has become a cornerstone of machine learning approaches [2, 3, 4], with applications in neural processing [5, 6, 3, 7] and human learning [8, 9].", "startOffset": 97, "endOffset": 106}, {"referenceID": 2, "context": "In recent years, probabilistic modelling has become a cornerstone of machine learning approaches [2, 3, 4], with applications in neural processing [5, 6, 3, 7] and human learning [8, 9].", "startOffset": 97, "endOffset": 106}, {"referenceID": 3, "context": "In recent years, probabilistic modelling has become a cornerstone of machine learning approaches [2, 3, 4], with applications in neural processing [5, 6, 3, 7] and human learning [8, 9].", "startOffset": 97, "endOffset": 106}, {"referenceID": 4, "context": "In recent years, probabilistic modelling has become a cornerstone of machine learning approaches [2, 3, 4], with applications in neural processing [5, 6, 3, 7] and human learning [8, 9].", "startOffset": 147, "endOffset": 159}, {"referenceID": 5, "context": "In recent years, probabilistic modelling has become a cornerstone of machine learning approaches [2, 3, 4], with applications in neural processing [5, 6, 3, 7] and human learning [8, 9].", "startOffset": 147, "endOffset": 159}, {"referenceID": 2, "context": "In recent years, probabilistic modelling has become a cornerstone of machine learning approaches [2, 3, 4], with applications in neural processing [5, 6, 3, 7] and human learning [8, 9].", "startOffset": 147, "endOffset": 159}, {"referenceID": 6, "context": "In recent years, probabilistic modelling has become a cornerstone of machine learning approaches [2, 3, 4], with applications in neural processing [5, 6, 3, 7] and human learning [8, 9].", "startOffset": 147, "endOffset": 159}, {"referenceID": 7, "context": "In recent years, probabilistic modelling has become a cornerstone of machine learning approaches [2, 3, 4], with applications in neural processing [5, 6, 3, 7] and human learning [8, 9].", "startOffset": 179, "endOffset": 185}, {"referenceID": 8, "context": "In recent years, probabilistic modelling has become a cornerstone of machine learning approaches [2, 3, 4], with applications in neural processing [5, 6, 3, 7] and human learning [8, 9].", "startOffset": 179, "endOffset": 185}, {"referenceID": 9, "context": "For example, if we are performing character recognition, we would want our support to contain a large collection of potential characters, accounting even for rare writing styles, and our inductive biases to reasonably reflect the probability of encountering each character [10, 11].", "startOffset": 273, "endOffset": 281}, {"referenceID": 10, "context": "For example, if we are performing character recognition, we would want our support to contain a large collection of potential characters, accounting even for rare writing styles, and our inductive biases to reasonably reflect the probability of encountering each character [10, 11].", "startOffset": 273, "endOffset": 281}, {"referenceID": 11, "context": ", all polynomial models), splines, and infinite neural networks, can all exactly be represented as a Gaussian process with a particular kernel function [12, 11, 10].", "startOffset": 152, "endOffset": 164}, {"referenceID": 10, "context": ", all polynomial models), splines, and infinite neural networks, can all exactly be represented as a Gaussian process with a particular kernel function [12, 11, 10].", "startOffset": 152, "endOffset": 164}, {"referenceID": 9, "context": ", all polynomial models), splines, and infinite neural networks, can all exactly be represented as a Gaussian process with a particular kernel function [12, 11, 10].", "startOffset": 152, "endOffset": 164}, {"referenceID": 12, "context": "Moreover, the Fisher kernel provides a mechanism to reformulate probabilistic generative models as kernel methods [13].", "startOffset": 114, "endOffset": 118}, {"referenceID": 13, "context": "Our framework leverages spectral mixture kernels [14] and non-parametric estimates.", "startOffset": 49, "endOffset": 53}, {"referenceID": 9, "context": "Since state of the art machine learning methods perform conspicuously poorly on a number of extrapolation problems which would be easy for humans [10], such efforts have the potential to help automate machine learning and improve performance on a wide range of tasks \u2013 including settings which are difficult for humans to process (e.", "startOffset": 146, "endOffset": 150}, {"referenceID": 11, "context": "In the supplement, we provide background on Gaussian processes [12], which we recommend as a review.", "startOffset": 63, "endOffset": 67}, {"referenceID": 14, "context": ", polynomial or power-law functions) [15, 16], or interpolation based on similarity learning [17, 18].", "startOffset": 37, "endOffset": 45}, {"referenceID": 15, "context": ", polynomial or power-law functions) [15, 16], or interpolation based on similarity learning [17, 18].", "startOffset": 37, "endOffset": 45}, {"referenceID": 16, "context": ", polynomial or power-law functions) [15, 16], or interpolation based on similarity learning [17, 18].", "startOffset": 93, "endOffset": 101}, {"referenceID": 17, "context": ", polynomial or power-law functions) [15, 16], or interpolation based on similarity learning [17, 18].", "startOffset": 93, "endOffset": 101}, {"referenceID": 18, "context": "[19] were the first to note that a Gaussian process framework can be used to unify these two perspectives.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] extended this model to accommodate a wider range of phenomena, using an infinite mixture of Gaussian process experts [21], and Lucas et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[20] extended this model to accommodate a wider range of phenomena, using an infinite mixture of Gaussian process experts [21], and Lucas et al.", "startOffset": 122, "endOffset": 126}, {"referenceID": 21, "context": "[22] used this model to shed new light on human predictions given sparse data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "The evaluation corpora using recent reviews [23, 19] are limited to a small set of parametric forms, i.", "startOffset": 44, "endOffset": 52}, {"referenceID": 18, "context": "The evaluation corpora using recent reviews [23, 19] are limited to a small set of parametric forms, i.", "startOffset": 44, "endOffset": 52}, {"referenceID": 23, "context": "Other projects have collected richer and more detailed data sets [24, 25], but we are only aware of", "startOffset": 65, "endOffset": 73}, {"referenceID": 24, "context": "Other projects have collected richer and more detailed data sets [24, 25], but we are only aware of", "startOffset": 65, "endOffset": 73}, {"referenceID": 23, "context": "Thus it is unsurprising that participants tend to revert to the prior mode that arises in almost all function learning experiments: linear functions, especially with slope-1 and intercept-0 [24, 25] (but see [26]).", "startOffset": 190, "endOffset": 198}, {"referenceID": 24, "context": "Thus it is unsurprising that participants tend to revert to the prior mode that arises in almost all function learning experiments: linear functions, especially with slope-1 and intercept-0 [24, 25] (but see [26]).", "startOffset": 190, "endOffset": 198}, {"referenceID": 25, "context": "Thus it is unsurprising that participants tend to revert to the prior mode that arises in almost all function learning experiments: linear functions, especially with slope-1 and intercept-0 [24, 25] (but see [26]).", "startOffset": 208, "endOffset": 212}, {"referenceID": 26, "context": "We assume the predictions y\u2217 are samples from the learner\u2019s posterior distribution over possible functions, following results showing that human inferences and judgments resemble posterior samples across a wide range of perceptual and decision-making tasks [27, 28, 29].", "startOffset": 257, "endOffset": 269}, {"referenceID": 27, "context": "We assume the predictions y\u2217 are samples from the learner\u2019s posterior distribution over possible functions, following results showing that human inferences and judgments resemble posterior samples across a wide range of perceptual and decision-making tasks [27, 28, 29].", "startOffset": 257, "endOffset": 269}, {"referenceID": 28, "context": "We assume the predictions y\u2217 are samples from the learner\u2019s posterior distribution over possible functions, following results showing that human inferences and judgments resemble posterior samples across a wide range of perceptual and decision-making tasks [27, 28, 29].", "startOffset": 257, "endOffset": 269}, {"referenceID": 13, "context": "For this approach, we choose the recent spectral mixture kernels of Wilson and Adams [14], which can model a wide range of stationary covariances, and are intended to help automate kernel selection.", "startOffset": 85, "endOffset": 89}, {"referenceID": 13, "context": ",y (W ) \u2217 , each containing 20 datapoints, from a GP with a spectral mixture kernel [14] with two components (the prediction kernel).", "startOffset": 84, "endOffset": 88}, {"referenceID": 13, "context": "We asked for extrapolation judgments because they provide more information about inductive biases than interpolation, and pose difficulties for conventional Gaussian process kernels [14, 10, 30].", "startOffset": 182, "endOffset": 194}, {"referenceID": 9, "context": "We asked for extrapolation judgments because they provide more information about inductive biases than interpolation, and pose difficulties for conventional Gaussian process kernels [14, 10, 30].", "startOffset": 182, "endOffset": 194}, {"referenceID": 29, "context": "We asked for extrapolation judgments because they provide more information about inductive biases than interpolation, and pose difficulties for conventional Gaussian process kernels [14, 10, 30].", "startOffset": 182, "endOffset": 194}, {"referenceID": 11, "context": "with a rational quadratic (RQ) kernel [12] (for heavy tailed correlations), and there are 20 human participants.", "startOffset": 38, "endOffset": 42}, {"referenceID": 11, "context": "These types of functions are notoriously difficult for standard Gaussian process kernels [12], due to sharp discontinuities and non-stationary behaviour.", "startOffset": 89, "endOffset": 93}, {"referenceID": 30, "context": "For example, MacKay [31] argues that Occam\u2019s razor is automatically embodied by the marginal likelihood in performing Bayesian inference: indeed, in our number sequence example, marginal likelihood computations show that 27 is millions of times more probable than 149.", "startOffset": 20, "endOffset": 24}, {"referenceID": 31, "context": "For example, the marginal likelihood of a Gaussian process (supplement) separates into automatically calibrated model fit and model complexity terms, sometimes referred to as automatic Occam\u2019s razor [32].", "startOffset": 199, "endOffset": 203}, {"referenceID": 23, "context": "In particular, iterated learning (IL) experiments [24] provide a way to draw samples that reflect human learners\u2019 a priori expectations.", "startOffset": 50, "endOffset": 54}, {"referenceID": 24, "context": "Our approach, following Little and Shiffrin [25], instead presents learners with plots of functions.", "startOffset": 44, "endOffset": 48}], "year": 2015, "abstractText": "Bayesian nonparametric models, such as Gaussian processes, provide a compelling framework for automatic statistical modelling: these models have a high degree of flexibility, and automatically calibrated complexity. However, automating human expertise remains elusive; for example, Gaussian processes with standard kernels struggle on function extrapolation problems that are trivial for human learners. In this paper, we create function extrapolation problems and acquire human responses, and then design a kernel learning framework to reverse engineer the inductive biases of human learners across a set of behavioral experiments. We use the learned kernels to gain psychological insights and to extrapolate in humanlike ways that go beyond traditional stationary and polynomial kernels. Finally, we investigate Occam\u2019s razor in human and Gaussian process based function learning.", "creator": "LaTeX with hyperref package"}}}