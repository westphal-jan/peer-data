{"id": "1510.08108", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Oct-2015", "title": "Online Learning with Gaussian Payoffs and Side Observations", "abstract": "We consider a sequential learning problem with Gaussian payoffs and side information: after selecting an action $i$, the learner receives information about the payoff of every action $j$ in the form of Gaussian observations whose mean is the same as the mean payoff, but the variance depends on the pair $(i,j)$ (and may be infinite). The setup allows a more refined information transfer from one action to another than previous partial monitoring setups, including the recently introduced graph-structured feedback case. For the first time in the literature, we provide non-asymptotic problem-dependent lower bounds on the regret of any algorithm, which recover existing asymptotic problem-dependent lower bounds and finite-time minimax lower bounds available in the literature. We also provide algorithms that achieve the problem-dependent lower bound (up to some universal constant factor) or the minimax lower bounds (up to logarithmic factors).", "histories": [["v1", "Tue, 27 Oct 2015 21:59:33 GMT  (40kb)", "http://arxiv.org/abs/1510.08108v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["yifan wu", "andr\u00e1s gy\u00f6rgy", "csaba szepesv\u00e1ri"], "accepted": true, "id": "1510.08108"}, "pdf": {"name": "1510.08108.pdf", "metadata": {"source": "CRF", "title": "Online Learning with Gaussian Payoffs and Side Observations", "authors": ["Yifan Wu", "Csaba Szepesv\u00e1ri"], "emails": ["ywu12@ualberta.ca", "szepesva@ualberta.ca", "a.gyorgy@imperial.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 151 0.08 108v 1 [stat.ML] 2 7"}, {"heading": "1 Introduction", "text": "Online learning in stochastic environments is a sequential decision problem in which at each step a learner selects an action from a given finite amount, observes some random feedback and receives a random payout. Multiple feedback models have been considered in the literature: the simplest is the complete information case in which the learner observes the payout of all possible actions at the end of each round. A popular exception is the case of bandit feedback, in which the learner observes only his own payout and receives no information about the payout of other actions [1]. Recently, several papers considered a more sophisticated constellation called graphically structured feedback, which interpolates between the complete information and the bandit case: here the feedback structure is described by a (directed) graph, and the selection of an action reveals the payout of all actions associated with the selected action, including the chosen action itself. This problem, which is motivated, for example, by social networks, is comprehensively investigated in both cases."}, {"heading": "2 Problem Formulation", "text": "Formally, we consider an online learning problem with Gaussian payouts and side observations: Suppose a learner has to choose between K actions in each round. When selecting an action, the learner receives a random payout and also some side observations that correspond to other actions. Specifically, each action i-K = {1,.., K} is associated with some parameter \u03b8i, and the payout variable Yt, i to action i in round t is normally random variable with medium effects and variance \u03c32ii, while the learner observes a K-dimensional Gaussian random vector Xt, i whose jest coordinate is a normal random variable with medium effect and variance \u03c32ij (we assume that the coordinates of Xt and variance \u03c32ij) and the coordinates of Xt, i, i are independent of each other."}, {"heading": "2.1 Notation", "text": "Let us specify the number of games over all actions performed by any algorithm in T-rounds. Let us also specify the number of games over different expected payout vectors, but the variance matrix is determined. Therefore, an environment can be specified using \u03b8; often we will explicitly specify the dependence of different sizes on it: The probability and expectation functions under environmental conditions are designated by Pr (;.) or E (;.), respectively. Furthermore, ij (.) should be the annual best action (ties are arbitrarily broken, i.e., the probability and expectation functions under environmental conditions are designated by Pr (.) and E (.)."}, {"heading": "3 Lower Bounds", "text": "The goal of this section is to derive generic, problem-dependent lower limits for regret, which are also able to provide minimax lower limits. Hardness in deriving such limits is that the mute algorithm that always selects i1 (\u03b8) achieves zero regrets (the regret of this algorithm is linear for each of these algorithms with i1 (\u03b8) 6 = i1 (\u03b8)))), so that it is generally not possible to give a lower limit for a single instance. In deriving asymptotic lower limits, this is circumvented by taking into account only consistent algorithms whose regrets are subpolynomial for each problem [10]. However, this asymptotic notion of consistency is not applicable to problems with a finite horizon. Following [11], for each problem we create a family of related problems (by perturbing the mean payouts), so that we assume that the regret of an algorithm will be \"too small\" for some of the problems."}, {"heading": "3.1 A General Finite Time Lower Bound", "text": "First, we derive a general lower limit. \u03b8 is a lower limit for each \u03b8 and a smaller problem for each q. (CNT) Define f (\u03b8, q, 2001) asf (\u03b8, q, 2001) = inf q. \"(a) < a, d (\u03b8) > s.t. (a) log q (a) q\" (a). (K) Ii (\u03b8, 2001). (Xt, i \"). (ai) ai, where Ii (\u03b8, 2001) is the KL divergence between Xt, i (2001) and Xt, i (2001) is, i.\" (Ii). (XT, 2001). (XT). (XT, i \"). (Xt, i\"). (Xt, i \"). (Xt, i.\") ai, i. \") ai, where Ii.\" (KA). (KJ \") is the KL divergence between Xt, i.\" (KA)."}, {"heading": "3.2 A Relaxed Lower Bound", "text": "Now we present a relaxed but more interpretable version of the lower limit of finite time of theorem 1, which is demonstrably in line with the asymptotic lower limit (1). The idea of deriving the lower limit is this: Instead of ensuring that the algorithm performs well in the most hostile environment, we look at a series of \"bad\" environments and make sure that the algorithm performs well where each \"bad\" environment is the most counterproductive, disrupting only one coordinate plane of the phenomenon. However, in order to reach lower limits in finite time, we need to be more careful than in the case of asymptotic lower limits, because for every suboptimal action i, if Affi is very close to Agrei1, E [Ni (T) is not necessarily small for a good algorithm."}, {"heading": "3.2.1 Formulation", "text": "We begin with the definition of a subset of CRT, the series of \"reasonable\" values for E [N (T); \u03b8 (V). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F (F). (F). (F). (F). (F (F). (F). (F). (F). (F (F). (F).). (F). (F (F). (F). (F). (F).). (F).). (F. (F.). (.).). (F.).). (F.).). (F. (F.).). ("}, {"heading": "3.2.3 Comparison to Minimax Bounds", "text": "Now we will show that the finite time lower limit dependent on us reproduces the minimax repentance limits of [2] and [5], except for the generalized complete case of information. The minimax limits depend on the following notion of observability: An action i is highly observable if either i-Si or [K]\\ {i} if it is not highly observable, but it exists j so that i-Sj (note that we have already adopted the latter condition for all i). Let W (\u03a3) be the group of all poorly observable actions. It is said that it is highly observable if W (\u03a3) =. It is poorly observable if W (\u03a3) 6 =. Next, we will define two key qualities introduced by [2] and [5] that characterize the hardness of a problem instance with feedback structure."}, {"heading": "4 Algorithms", "text": "In this section, we present two algorithms and their finite time analysis for the uniform variance version of our problem (where \u03c3ij is either \u03c3 or \u221e): the upper limit for the first algorithm corresponds to the asymptotic lower limit in (1) to constants; the second algorithm reaches the minimax lower limits of sequence 4 to logarithmic factors and O (log3 / 2 T) problem-dependent regret. In the problem-dependent upper limits of both algorithms, we assume that the optimal action is unique, i.e. di2 (\u03b8) (\u03b8) > 0."}, {"heading": "4.1 An Asymptotically Optimal Algorithm", "text": "Let us interpret the main idea that comes from [12] so that the exploration can be advanced across all measures. (...) Let c (...) = argminc (...), c (...), c (...), d (...), d (...), d (...), d (...), d (...), d (...), d (...), c (...), c (...), c (...), c (...), c (...), c (...), c (...), c (...), c (...), c (...), c (...), c (...), c (...), c (...), c (...), c (...), c (...), c (...), c (...), c (...), c (...), c (...), c (...), c (...), c (...), c (...), c (...), c (...), c (...), c, c (...), c (...), c (...), c (...), c (...), c (...), c (..., c (...), c (...), c (...), c (...), c (..., c (...), c (...), c (...), c (..., c (...), c (...), c (..., c (...), c (..., c (...), c (...), c (..., c (...), c (...), c (..., c (...), c (...), c (..., c (...), c (...), c (..., c (..., c (...), c (...), c (..., c (...), c (...), c (..., c (..., c (...), c (...), c (..., c (..., c (...), c (...), c (..., c (...), c (...), c (..., c (..., c (...), c (...), c (...), c (..., c (..., c (...), c (...), c"}, {"heading": "4.2 A Minimax Optimal Algorithm", "text": "A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A,"}, {"heading": "5 Conclusions and Open Problems", "text": "We considered a novel system of partial monitoring with Gaussian secondary observations that generalizes the recently introduced setting of graph-structured feedbacks and allows for a more precise quantification of observed information from one action to the next. We also provided an algorithm that implies the asymptotic problem-dependent lower limit (up to some universal constants) and another algorithm that reaches the Minimax limits under both weak and strong observation factors. However, we think this is only the beginning. For example, we currently do not have an algorithm that reaches both the problem-based and the Minimax lower limit simultaneously. Moreover, our upper limits correspond only to the graph-structured feedback case. It is of great interest to go beyond the weak / strong observation in describing the problem to determine the variances between measures that can represent an optimal adjustment to costs."}, {"heading": "Acknowledgments", "text": "This work was supported by Alberta Innovates Technology Futures through the Alberta Ingenuity Centre for Machine Learning (AICML) and NSERC. During this work, A. Gyo \ufffd rgy worked in the Department of Computing Science at the University of Alberta."}, {"heading": "A Proofs for Section 3", "text": "The proof of equality (see, for example, Lemma 15), we obtainq (a) = Pr (n) (n); Lemai (a) for each a) CNT. For each a) CNT that applies a standard change of dimensional equality (see, for example, Lemai a); Lemai (b) for each a) CNT that applies a standard change of dimensional equality (see, for example, Lemma 15), we obtainq (a)."}, {"heading": "B Proofs for Section 4.1", "text": "B.1 proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of the theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof of theorem 6proof"}, {"heading": "C Proofs for Section 4.2", "text": "C.1 Proof Theorem 8Proof Theorem 8: For each r > 0, we define eventsUr = {2, Ur). (AWr). (AWr). (AWr). (AWr). (AWr). (AWr). (AWr). (AWr). (AWr). (AWr). (AWr). (AWr). (AWr). (AWr). (r). (r). (r). (r). (r). (r). (T). (Ar). (1). (AWr). (AWr). (AWr). (AWr). (AWr)."}], "references": [{"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S. Bubeck", "N. Cesa-Bianchi"], "venue": "Foundations and Trends in Machine Learning, 5(1):1\u2013122,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "From bandits to experts: on the value of side-observations", "author": ["S. Mannor", "O. Shamir"], "venue": "Advances in Neural Information Processing Systems 24 (NIPS), pages 684\u2013692,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Online learning with feedback graphs: beyond bandits", "author": ["Noga Alon", "Nicol\u00f2 Cesa-Bianchi", "Ofer Dekel", "Tomer Koren"], "venue": "In Proceedings of The 28th Conference on Learning Theory (COLT),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Leveraging side observations in stochastic bandits", "author": ["St\u00e9phane Caron", "Branislav Kveton", "Marc Lelarge", "Smriti Bhagat"], "venue": "In Proceedings of the 28th Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Stochastic bandits with side observations on networks", "author": ["Swapna Buccapatnam", "Atilla Eryilmaz", "Ness B. Shroff"], "venue": "SIGMETRICS Perform. Eval. Rev.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": "Cambridge University Press, Cambridge,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Partial monitoring \u2013 classification, regret bounds, and algorithms", "author": ["G. Bart\u00f3k", "D. Foster", "D. P\u00e1l", "A. Rakhlin", "Cs. Szepesv\u00e1ri"], "venue": "Mathematics of Operations Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Asymptotically efficient adaptive choice of control laws incontrolled markov chains", "author": ["Todd L Graves", "Tze Leung Lai"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "Toward minimax off-policy value estimation", "author": ["Lihong Li", "R\u00e9mi Munos", "Csaba Szepesv\u00e1ri"], "venue": "In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Lipschitz bandits: Regret lower bounds and optimal algorithms", "author": ["Stefan Magureanu", "Richard Combes", "Alexandre Proutiere"], "venue": "In Proceedings of The 27th Conference on Learning Theory (COLT),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "On the complexity of best arm identification in multi-armed bandit models", "author": ["E. Kaufmann", "O. Capp\u00e9", "A. Garivier"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "A popular setup is the case of bandit feedback, where the learner only observes its own payoff and receives no information about the payoff of other actions [1].", "startOffset": 157, "endOffset": 160}, {"referenceID": 1, "context": "This problem, motivated for example by social networks, has been studied extensively in both the adversarial [2, 3, 4, 5] and the stochastic cases [6, 7].", "startOffset": 109, "endOffset": 121}, {"referenceID": 2, "context": "This problem, motivated for example by social networks, has been studied extensively in both the adversarial [2, 3, 4, 5] and the stochastic cases [6, 7].", "startOffset": 109, "endOffset": 121}, {"referenceID": 3, "context": "This problem, motivated for example by social networks, has been studied extensively in both the adversarial [2, 3, 4, 5] and the stochastic cases [6, 7].", "startOffset": 147, "endOffset": 153}, {"referenceID": 4, "context": "This problem, motivated for example by social networks, has been studied extensively in both the adversarial [2, 3, 4, 5] and the stochastic cases [6, 7].", "startOffset": 147, "endOffset": 153}, {"referenceID": 2, "context": "Removing this self-loop assumption leads to the so-called partial monitoring case [5].", "startOffset": 82, "endOffset": 85}, {"referenceID": 5, "context": "In the absolutely general partial monitoring setup the learner receives some general feedback that depends on its choice (and the environment), with some arbitrary (but known) dependence [8, 9].", "startOffset": 187, "endOffset": 193}, {"referenceID": 6, "context": "In the absolutely general partial monitoring setup the learner receives some general feedback that depends on its choice (and the environment), with some arbitrary (but known) dependence [8, 9].", "startOffset": 187, "endOffset": 193}, {"referenceID": 5, "context": "While the partial monitoring setup covers all other problems, its analysis has concentrated on the finite case where both the set of actions and the set of feedback signals are finite [8, 9], which is in contrast to the standard full information and bandit settings where the feedback is typically assumed to be real-valued.", "startOffset": 184, "endOffset": 190}, {"referenceID": 6, "context": "While the partial monitoring setup covers all other problems, its analysis has concentrated on the finite case where both the set of actions and the set of feedback signals are finite [8, 9], which is in contrast to the standard full information and bandit settings where the feedback is typically assumed to be real-valued.", "startOffset": 184, "endOffset": 190}, {"referenceID": 2, "context": "The only exception to this case is the work of [5], which considers graph-structured feedback without the self-loop assumption.", "startOffset": 47, "endOffset": 50}, {"referenceID": 7, "context": ", [10, 7]), or finite-time minimax bounds (e.", "startOffset": 2, "endOffset": 9}, {"referenceID": 4, "context": ", [10, 7]), or finite-time minimax bounds (e.", "startOffset": 2, "endOffset": 9}, {"referenceID": 6, "context": ", [9, 3, 5]).", "startOffset": 2, "endOffset": 11}, {"referenceID": 2, "context": ", [9, 3, 5]).", "startOffset": 2, "endOffset": 11}, {"referenceID": 6, "context": "Regarding the minimax regret, the hardness (\u0398\u0303(T ) or \u0398\u0303(T ) regret) of partial monitoring problems is characterized by their global/local observability property [9] or, in case of the graph-structured feedback model, by their strong/weak observability property [5].", "startOffset": 162, "endOffset": 165}, {"referenceID": 2, "context": "Regarding the minimax regret, the hardness (\u0398\u0303(T ) or \u0398\u0303(T ) regret) of partial monitoring problems is characterized by their global/local observability property [9] or, in case of the graph-structured feedback model, by their strong/weak observability property [5].", "startOffset": 262, "endOffset": 265}, {"referenceID": 3, "context": "Earlier results for the stochastic graph-structured feedback problems [6, 7] provided only asymptotic problem-dependent lower bounds and performance bounds that did not match the asymptotic lower bounds or the minimax rate up to constant factors.", "startOffset": 70, "endOffset": 76}, {"referenceID": 4, "context": "Earlier results for the stochastic graph-structured feedback problems [6, 7] provided only asymptotic problem-dependent lower bounds and performance bounds that did not match the asymptotic lower bounds or the minimax rate up to constant factors.", "startOffset": 70, "endOffset": 76}, {"referenceID": 2, "context": "Partial monitoring with feedback graphs [5]: Each action i \u2208 [K] is associated with an observation set Si \u2282 [K] such that \u03c3ij = \u03c3j if j \u2208 Si and \u03c3ij = \u221e otherwise.", "startOffset": 40, "endOffset": 43}, {"referenceID": 7, "context": "When deriving asymptotic lower bounds, this is circumvented by only considering consistent algorithms whose regret is sub-polynomial for any problem [10].", "startOffset": 149, "endOffset": 153}, {"referenceID": 8, "context": "Therefore, following [11], for any problem we create a family of related problems (by perturbing the mean payoffs) such that if the regret of an algorithm is \u201ctoo small\u201d in one of the problems than it will be \u201clarge\u201d in another one.", "startOffset": 21, "endOffset": 25}, {"referenceID": 7, "context": "The result presented below is an easy consequence of [10], hence its proof is omitted.", "startOffset": 53, "endOffset": 57}, {"referenceID": 3, "context": "Similar bounds have been provided in [6, 7] for graph-structured feedback with self-observability (under non-Gaussian assumptions on the payoffs).", "startOffset": 37, "endOffset": 43}, {"referenceID": 4, "context": "Similar bounds have been provided in [6, 7] for graph-structured feedback with self-observability (under non-Gaussian assumptions on the payoffs).", "startOffset": 37, "endOffset": 43}, {"referenceID": 1, "context": "3 Comparison to Minimax Bounds Now we will show that our \u03b8-dependent finite-time lower bound reproduces the minimax regret bounds of [2] and [5], except for the generalized full information case.", "startOffset": 133, "endOffset": 136}, {"referenceID": 2, "context": "3 Comparison to Minimax Bounds Now we will show that our \u03b8-dependent finite-time lower bound reproduces the minimax regret bounds of [2] and [5], except for the generalized full information case.", "startOffset": 141, "endOffset": 144}, {"referenceID": 1, "context": "Next we will define two key qualities introduced by [2] and [5] that characterize the hardness of a problem instance with feedback structure \u03a3: A set A \u2282 [K] is called an independent set if for any i \u2208 A, Si \u2229 A \u2282 {i}.", "startOffset": 52, "endOffset": 55}, {"referenceID": 2, "context": "Next we will define two key qualities introduced by [2] and [5] that characterize the hardness of a problem instance with feedback structure \u03a3: A set A \u2282 [K] is called an independent set if for any i \u2208 A, Si \u2229 A \u2282 {i}.", "startOffset": 60, "endOffset": 63}, {"referenceID": 9, "context": "The main idea, coming from [12], is that by forcing exploration over all actions the solution c(\u03b8) of the linear program can be well approximated while", "startOffset": 27, "endOffset": 31}, {"referenceID": 9, "context": "One advantage of our algorithm compared to that of [12] is that we use a sublinear exploration schedule \u03b2(n) instead of a constant rate \u03b2(n) = \u03b2n.", "startOffset": 51, "endOffset": 55}, {"referenceID": 9, "context": "This resolves the problem that, to achieve asymptotically optimal performance, some parameter of the algorithm needs to be chosen according to dmin(\u03b8) as in [12].", "startOffset": 157, "endOffset": 161}, {"referenceID": 3, "context": "Also note that the algorithms presented in [6, 7] do not achieve this asymptotic bound.", "startOffset": 43, "endOffset": 49}, {"referenceID": 4, "context": "Also note that the algorithms presented in [6, 7] do not achieve this asymptotic bound.", "startOffset": 43, "endOffset": 49}, {"referenceID": 4, "context": "Note that Algortihm 2 is similar to the UCB-LP algorithm of [7], which admits a better problem-dependent upper bound (although does not achieve it with optimal problem-dependent constants), but it does not achieve the minimax bound even under strong observability.", "startOffset": 60, "endOffset": 63}, {"referenceID": 0, "context": "References [1] S.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] S.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[5] Noga Alon, Nicol\u00f2 Cesa-Bianchi, Ofer Dekel, and Tomer Koren.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[6] St\u00e9phane Caron, Branislav Kveton, Marc Lelarge, and Smriti Bhagat.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[7] Swapna Buccapatnam, Atilla Eryilmaz, and Ness B.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[8] N.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[9] G.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[10] Todd L Graves and Tze Leung Lai.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[11] Lihong Li, R\u00e9mi Munos, and Csaba Szepesv\u00e1ri.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[12] Stefan Magureanu, Richard Combes, and Alexandre Proutiere.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[13] E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "where for x, y \u2208 [0, 1], KL(x, y) = x log(x/y) + (1 \u2212 x) log((1 \u2212 x)/(1 \u2212 y)) denotes the binary KL-divergence.", "startOffset": 17, "endOffset": 23}, {"referenceID": 2, "context": "The idea of constructing the worst \u03b8 comes from the proof of Theorem 7 in [5] which based on the following graph-theoretic lemma: Lemma 15 (Restated from Lemma 8 in [5]).", "startOffset": 74, "endOffset": 77}, {"referenceID": 2, "context": "The idea of constructing the worst \u03b8 comes from the proof of Theorem 7 in [5] which based on the following graph-theoretic lemma: Lemma 15 (Restated from Lemma 8 in [5]).", "startOffset": 165, "endOffset": 168}], "year": 2015, "abstractText": "We consider a sequential learning problem with Gaussian payoffs and side information: after selecting an action i, the learner receives information about the payoff of every action j in the form of Gaussian observations whose mean is the same as the mean payoff, but the variance depends on the pair (i, j) (and may be infinite). The setup allows a more refined information transfer from one action to another than previous partial monitoring setups, including the recently introduced graph-structured feedback case. For the first time in the literature, we provide non-asymptotic problem-dependent lower bounds on the regret of any algorithm, which recover existing asymptotic problem-dependent lower bounds and finitetime minimax lower bounds available in the literature. We also provide algorithms that achieve the problem-dependent lower bound (up to some universal constant factor) or the minimax lower bounds (up to logarithmic factors).", "creator": "LaTeX with hyperref package"}}}