{"id": "1510.08906", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Oct-2015", "title": "Sample Complexity of Episodic Fixed-Horizon Reinforcement Learning", "abstract": "Recently, there has been significant progress in understanding reinforcement learning in discounted infinite-horizon Markov decision processes (MDPs) by deriving tight sample complexity bounds. However, in many real-world applications, an interactive learning agent operates for a fixed or bounded period of time, for example tutoring students for exams or handling customer service requests. Such scenarios can often be better treated as episodic fixed-horizon MDPs, for which only looser bounds on the sample complexity exist. A natural notion of sample complexity in this setting is the number of episodes required to guarantee a certain performance with high probability (PAC guarantee). In this paper, we derive an upper PAC bound $\\tilde O(\\frac{|\\mathcal S|^2 |\\mathcal A| H^2}{\\epsilon^2} \\ln\\frac 1 \\delta)$ and a lower PAC bound $\\tilde \\Omega(\\frac{|\\mathcal S| |\\mathcal A| H^2}{\\epsilon^2} \\ln \\frac 1 {\\delta + c})$ that match up to log-terms and an additional linear dependency on the number of states $|\\mathcal S|$. The lower bound is the first of its kind for this setting. Our upper bound leverages Bernstein's inequality to improve on previous bounds for episodic finite-horizon MDPs which have a time-horizon dependency of at least $H^3$.", "histories": [["v1", "Thu, 29 Oct 2015 21:14:42 GMT  (64kb,D)", "https://arxiv.org/abs/1510.08906v1", "26 pages, appears in Neural Information Processing Systems (NIPS) 2015"], ["v2", "Tue, 19 Jan 2016 16:36:03 GMT  (65kb,D)", "http://arxiv.org/abs/1510.08906v2", "28 pages, appeared in Neural Information Processing Systems (NIPS) 2015, updated version with fixed typos and modified Lemma 1"], ["v3", "Wed, 11 May 2016 15:27:28 GMT  (65kb,D)", "http://arxiv.org/abs/1510.08906v3", "28 pages, appeared in Neural Information Processing Systems (NIPS) 2015, updated version with fixed typos and modified Lemma 1 and Lemma C.5"]], "COMMENTS": "26 pages, appears in Neural Information Processing Systems (NIPS) 2015", "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG", "authors": ["christoph dann", "emma brunskill"], "accepted": true, "id": "1510.08906"}, "pdf": {"name": "1510.08906.pdf", "metadata": {"source": "CRF", "title": "Sample Complexity of Episodic Fixed-Horizon Reinforcement Learning", "authors": ["Christoph Dann", "Emma Brunskill"], "emails": ["cdann@cdann.net", "ebrun@cs.cmu.edu"], "sections": [{"heading": null, "text": "The lower limit is the first of its kind. Our upper limit uses Bernstein's inequality to improve on previous limits for episodic end-time horizon development programs that have a temporal and horizontal dependence on at least H3."}, {"heading": "1 Introduction and Motivation", "text": "In fact, it is the case that it is a matter of a way in which people are able to determine themselves what they want and what they want. (...) In fact, it is the case that people are able to determine themselves. (...) In fact, it is the case that people are able to determine themselves. (...) It is not the case that people are able to determine themselves what they want. (...) It is the case that people are able to determine themselves what they want. (...) It is as if they are able to determine themselves what they want. (...)"}, {"heading": "2 Problem Setting and Notation", "text": "We look at the quality of a policy based on the expected reward of an entire episode. (S, A, p, p0, H) We look at the reward function as a possible reward. (S, r, p, p0, H) We look at the reward function as a possible reward. (S) We look at the reward function as a possible reward. (S) We look at the reward function as a possible reward. (S, r, p, p0, H) We look at the reward function as a possible reward. (S) We look at the reward function as possibly time-dependent and we look at values in relation to the reward function. (S) We look at the reward function as possibly time-dependent and we look at values in relation to the reward function. (S, r, p, p, 0) We look at the reward function as possibly time-dependent and we look at values in relation to the reward function. (S, r) We look at the reward function as possibly time-dependent and we look at values in relation to the reward function."}, {"heading": "3 Upper PAC-Bound", "text": "We will prove later that UCFH is an optimistic approach to balance and exploitation. Generally, the algorithm operates in phases consisting of optimistic planning, policy execution, and model updating, each lasting several episodes. Phases are indexed by k. As the agent acts and observes (s, r, s) tuples in the environment, UCFH maintains a trust set over the possible transition parameters for each pair of state shareholders that is consistent with the observed transition phases."}, {"heading": "3.1 PAC Analysis", "text": "We assume that each episode begins in a fixed initial state s0. This assumption is not decisive and can easily be removed by additional notational effort. (Theorem 1) We assume that each episode begins in a fixed initial state s0. (This assumption is not decisive and can easily be replaced by additional notational effort. (Theorem 1) We assume that each episode begins in a fixed initial state s0. (This assumption is not decisive and can easily be replaced by additional notational effort. (H2C) The first condition in the first condition in the second condition is actually not necessary for the theoretical results to be held. (Itcan be removed and all 6 / \u03b41 can be replaced by 4 / \u03b41.) The second condition in the second condition in the third condition. (H2C)"}, {"heading": "4 Lower PAC Bound", "text": "Theorem 2, positive constants c1, c2, \u03b40, 0 exist in such a way that for each \u03b4 (0, \u03b40) and for each algorithm A, which gives a PAC guarantee for (, \u03b4) and a deterministic policy, there is, however, a fixed horizon episodic MDP Mhard withE [nA] \u2265 c1 (H \u2212 2) 2 (A \u2212 1) 2 ln (c2\u03b4 + c3) = (S \u00b7 A | H22 ln (c2\u03b4 + c3)) (3), where nA is the number of episodes until the algorithm is the policy (, \u03b4) -precise. The constants can be set to \u03b40 = e \u2212 4 80 vs. 1 5000, 0 = H \u2212 4 vs. H \u2212 H / 35000, c2 vs. H / 4 and c3 = e \u2212 bandits."}, {"heading": "5 Related Work on Fixed-Horizon Sample Complexity Bounds", "text": "We are not aware of any lower sample complexity limits that go beyond the results of multi-armed bandits that are directly applicable to our environment. Our upper limit in Theorem 1 improves existing results by at least a factor of H. We briefly review these existing results in the sequence. Cockade [4, Chapter 8] shows upper and lower PAC limits for a similar setting in which the agent interacts indefinitely with the environment, but the interactions are divided into segments of equal length and the agent is evaluated by the expected sum of rewards until the end of each segment. Tied states are no more than O limits in which the agent interacts with the environment - suboptimal. Strehl et al. [1] improves the state dependence of these limits for their delayed Q learning algorithms at the O level."}, {"heading": "6 Conclusion", "text": "We have shown upper and lower limits of sample complexity of episodic fixed-horizon RL, which are closely related to log factors in time horizon H, accuracy, number of actions | A | and up to an additive constant in failure probability \u03b4. These limits improve existing results by a factor of at least H. One might hope to reduce dependence of the upper limit of | S | by an analysis similar to Mormax [7] for discounted MDPs having linear in | S | sample complexity, at the expense of additional dependencies on H. Our proposed UCFH algorithm, which compares our 6For comparison, we adjust existing limits to our attitude. While the limit originally given by Cakade [4] has only H3, an additional H3 comes through \u2212 3 due to varying normalization of rewards."}, {"heading": "Appendices", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A Fixed-Horizon Extended Value Iteration 12", "text": "B UCFH runtime and space complexity 13"}, {"heading": "C Detailed Proofs for the Upper PAC Bound 13", "text": "......................................................................................................................................................................................................"}, {"heading": "A Fixed-Horizon Extended Value Iteration", "text": "We want to find a politics of trust, which is a dynamic politics of trust. We want to find a politics of trust, which is a dynamic politics of trust. We want to find a politics of trust, which is a dynamic politics of trust. We want to find a dynamic politics of trust. We want a politics of trust. We want a politics of trust. We want a politics of trust. We want a politics of trust. We want a politics of trust. We want a politics of trust. We want a politics of trust. We want a politics of trust. We want a politics of trust. We want a politics of trust. We want a politics of trust. We want a politics of trust. We want a politics of trust. We want a politics of trust. We want a politics of trust. We want a politics of trust. We want a politics of trust. We want a politics of trust. We want a politics of trust. We want a politics of trust. We want a politics of trust."}, {"heading": "C Detailed Proofs for the Upper PAC Bound", "text": "C.1 The total number of updates is limited by Umax = | S \u00b7 A | log2 | S | H wmin.Proof. First of all, it should be noted that n (s, a) never decreases and no updates take place once n (s, a) \u2265 | S | mH for all (s, a). With each update, the n (s, a) of exactly one (s, a) pair increases by max {mwmin, n (s, a)}. For one (s, a) pair, such updates can only occur log2 (| S | mH) \u2212 log2 (mwmin) times. Therefore, the total number of updates is limited to | S \u00d7 A | log2 | S | mH wminm."}, {"heading": "C.2 Proof of Lemma 1 \u2013 Capturing the true MDP", "text": "For a single (s, a) pair, s \"S\" S (s, a) and k \"S\" S \"S (s, a) we can treat the event that is the successor state of s if we consider the plot a as a random variable with the probability p (s) 2nand of Bernstein's inequality (s\" s, a) -p (s \"s,\" a) -p (s \"s, a) -p (s\" s, a) ln (6 / 1 \"s\" s) 2nand of Bernstein's inequality p (s \"s,\" a) -p (s \"s\" s \"s\" s, \"b\" s \"s\" s \"s,\" a \"s\" s \"s\" s) ln (6 / 1 \"s\" s \"s\") 2nand of Bernstein's inequality p (s \"s\" s, \"a\" s \"s\" s \"s\" s. \"s\" s \"s (s). (s\" s \"s\" s \"s\" s)."}, {"heading": "Proof.", "text": "Vi, j, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p"}, {"heading": "Proof.", "text": "(1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1)) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1 (1) (1) (1) (1) (1) (1 (1) (1) (1 (1) (1) (1) (1 (1) (1) (1) (1 (1) (1 (1) (1 (1) (1 (1) (1) (1 (1) (1) (1) (1 (1 (1) (1) (1) (1 (1) (1) (1) (1 (1) (1) (1 (1 (1) (1) (1) (1"}, {"heading": "Proof of Lemma 4.", "text": "Vi: j (s) = E (j + 1: j (st) \u2212 Vi + 1: j (si + 1) + Vi + 1: j (si + 1) + 1: j (si + 1) + Vi + 1: j (si + 1) + 2E [(j + 1) + ri (si) \u2212 Vi: j (si) \u2212 Vi + 1: j (si) \u2212 Vi + 1: j (si + 1) \u2212 Vi + 1: j (si + 1)) 2 | si = s (j + 2E [(j + 1) + 1rt (st) \u2212 Vi + 1: j (si + 1))) (Vi + 1: j (si) \u2212 s (Vi + 0: j (si + 1) \u2212 si (si) \u2212 V (si) = si (si) = s (si) + E [(Vi + 1) + n: i (si) + i (si))."}, {"heading": "C.4.3 Proof of Lemma 3", "text": "The detection of Lemma 3. The recursive limit of Lemma C. 7 \u2206 d = greater than the large limit of 33\u00ba C is therefore not necessary."}, {"heading": "C.5 Proof of Theorem 1", "text": "\u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 and \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2"}, {"heading": "D Proof of the Lower PAC Bound", "text": "The proof of theory 2: We look at the class of MDPs that is generated in Figure 1. The MDPs essentially consist of n parallel, multi-armed bandits. For each bandit there are m + 1 = = A | possible instances, which we call Ii = 0. The other hypotheses Ii = j for j = 1.. m correspond with i (a) = I {a = a0} / 2 + I {a = aj} / 2, that is, only the plot a0 has a slight bias. The other hypotheses Ii = j for j = 1. m correspond with i (a) = 1 = a0} \u2032 / 2 + I {a = aj} \u2032. We use I = (I1) to define the instance of the entire MDP. We define Gi = {s for the entire MDP.I: We define the definition of the reward i: i (i) = aIi}, the event that is generated in politics."}], "references": [{"title": "PAC Model-Free Reinforcement Learning", "author": ["Alexander L. Strehl", "Lihong Li", "Eric Wiewiora", "John Langford", "Michael L. Littman"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Finite-Sample Convergence Rates for Q-Learning and Indirect Algorithms", "author": ["Michael J Kearns", "Satinder P Singh"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "R-MAX \u2013 A General Polynomail Time Algorithm for Near-Optimal Reinforcement Learning", "author": ["Ronen I Brafman", "Moshe Tennenholtz"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "On the Sample Complexity of Reinforcement Learning", "author": ["Sham M. Kakade"], "venue": "PhD thesis,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Online Regret Bounds for a New Reinforcement Learning Algorithm", "author": ["Peter Auer", "Ronald Ortner"], "venue": "In Proceedings 1st Austrian Cognitive Vision Workshop,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "PAC bounds for discounted MDPs", "author": ["Tor Lattimore", "Marcus Hutter"], "venue": "In International Conference on Algorithmic Learning Theory,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Model-based reinforcement learning with nearly tight exploration complexity bounds", "author": ["Istv\u00e0n Szita", "Csaba Szepesv\u00e1ri"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "On the Sample Complexity of Reinforcement Learning with a Generative Model", "author": ["Mohammad Gheshlaghi Azar", "R\u00e9mi Munos", "Hilbert J. Kappen"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Near-Bayesian exploration in polynomial time", "author": ["J Zico Kolter", "Andrew Y Ng"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Efficient reinforcement learning", "author": ["Claude-Nicolas Fiechter"], "venue": "In Conference on Learning Theory,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1994}, {"title": "Expected Mistake Bound Model for On-Line Reinforcement Learning", "author": ["Claude-Nicolas Fiechter"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1997}, {"title": "Efficient PAC learning for episodic tasks with acyclic state spaces", "author": ["Spyros Reveliotis", "Theologos Bountourelis"], "venue": "Discrete Event Dynamic Systems: Theory and Applications,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Incremental Model-based Learners With Formal Learning-Time Guarantees", "author": ["Alexander L Strehl", "Lihong Li", "Michael L Littman"], "venue": "In Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Reinforcement Learning in Finite MDPs : PAC Analysis", "author": ["Alexander L Strehl", "Lihong Li", "Michael L Littman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Near-optimal Regret Bounds for Reinforcement Learning", "author": ["Thomas Jaksch", "Ronald Ortner", "Peter Auer"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "An analysis of model-based Interval Estimation for Markov Decision Processes", "author": ["Alexander L. Strehl", "Michael L. Littman"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "The Variance of Markov Decision Processes", "author": ["Matthew J Sobel"], "venue": "Journal of Applied Probability,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1982}, {"title": "Empirical Bernstein Bounds and Sample-Variance Penalization", "author": ["Andreas Maurer", "Massimiliano Pontil"], "venue": "In Conference on Learning Theory,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "The Sample Complexity of Exploration in the Multi-Armed Bandit Problem", "author": ["Shie Mannor", "John N Tsitsiklis"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Near-optimal Regret Bounds for Reinforcement Learning", "author": ["Thomas Jaksch", "Ronald Ortner", "Peter Auer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "We formalize this as the sample complexity of reinforcement learning [1], which is the number of time steps on which the algorithm may select an action whose value is not near-optimal.", "startOffset": 69, "endOffset": 72}, {"referenceID": 1, "context": "RL algorithms with a sample complexity that is a polynomial function of the domain parameters are referred to as Probably Approximately Correct (PAC) [2, 3, 4, 1].", "startOffset": 150, "endOffset": 162}, {"referenceID": 2, "context": "RL algorithms with a sample complexity that is a polynomial function of the domain parameters are referred to as Probably Approximately Correct (PAC) [2, 3, 4, 1].", "startOffset": 150, "endOffset": 162}, {"referenceID": 3, "context": "RL algorithms with a sample complexity that is a polynomial function of the domain parameters are referred to as Probably Approximately Correct (PAC) [2, 3, 4, 1].", "startOffset": 150, "endOffset": 162}, {"referenceID": 0, "context": "RL algorithms with a sample complexity that is a polynomial function of the domain parameters are referred to as Probably Approximately Correct (PAC) [2, 3, 4, 1].", "startOffset": 150, "endOffset": 162}, {"referenceID": 7, "context": "1 It does not require additional structure such as assuming access to a generative model [8] or that the state transitions are sparse or acyclic [6].", "startOffset": 89, "endOffset": 92}, {"referenceID": 5, "context": "1 It does not require additional structure such as assuming access to a generative model [8] or that the state transitions are sparse or acyclic [6].", "startOffset": 145, "endOffset": 148}, {"referenceID": 3, "context": "The limited prior research on upper bound PAC results for finite horizon MDPs has focused on different settings, such as partitioning a longer trajectory into fixed length segments [4, 1], or considering a sliding time window [9].", "startOffset": 181, "endOffset": 187}, {"referenceID": 0, "context": "The limited prior research on upper bound PAC results for finite horizon MDPs has focused on different settings, such as partitioning a longer trajectory into fixed length segments [4, 1], or considering a sliding time window [9].", "startOffset": 181, "endOffset": 187}, {"referenceID": 8, "context": "The limited prior research on upper bound PAC results for finite horizon MDPs has focused on different settings, such as partitioning a longer trajectory into fixed length segments [4, 1], or considering a sliding time window [9].", "startOffset": 226, "endOffset": 229}, {"referenceID": 9, "context": "2 Fiechter [10, 11] and Reveliotis and Bountourelis [12] do tackle a closely related setting, but find a dependence that is at least H.", "startOffset": 11, "endOffset": 19}, {"referenceID": 10, "context": "2 Fiechter [10, 11] and Reveliotis and Bountourelis [12] do tackle a closely related setting, but find a dependence that is at least H.", "startOffset": 11, "endOffset": 19}, {"referenceID": 11, "context": "2 Fiechter [10, 11] and Reveliotis and Bountourelis [12] do tackle a closely related setting, but find a dependence that is at least H.", "startOffset": 52, "endOffset": 56}, {"referenceID": 5, "context": "Our work builds on recent work [6, 8] on PAC infinite horizon discounted RL that offers much tighter upper and lower sample complexity bounds than was previously known.", "startOffset": 31, "endOffset": 37}, {"referenceID": 7, "context": "Our work builds on recent work [6, 8] on PAC infinite horizon discounted RL that offers much tighter upper and lower sample complexity bounds than was previously known.", "startOffset": 31, "endOffset": 37}, {"referenceID": 0, "context": "The reward function r is possibly time-dependent and takes values in [0, 1].", "startOffset": 69, "endOffset": 75}, {"referenceID": 4, "context": "1 Previous works [5] have shown that the complexity of learning state transitions usually dominates learning reward functions.", "startOffset": 17, "endOffset": 20}, {"referenceID": 5, "context": "We therefore follow existing sample complexity analyses [6, 7] and assume known rewards for simplicity.", "startOffset": 56, "endOffset": 62}, {"referenceID": 6, "context": "We therefore follow existing sample complexity analyses [6, 7] and assume known rewards for simplicity.", "startOffset": 56, "endOffset": 62}, {"referenceID": 2, "context": "Like many other PAC RL algorithms [3, 13, 14, 15], UCFH uses an optimism under uncertainty approach to balance exploration and exploitation.", "startOffset": 34, "endOffset": 49}, {"referenceID": 12, "context": "Like many other PAC RL algorithms [3, 13, 14, 15], UCFH uses an optimism under uncertainty approach to balance exploration and exploitation.", "startOffset": 34, "endOffset": 49}, {"referenceID": 13, "context": "Like many other PAC RL algorithms [3, 13, 14, 15], UCFH uses an optimism under uncertainty approach to balance exploration and exploitation.", "startOffset": 34, "endOffset": 49}, {"referenceID": 14, "context": "Like many other PAC RL algorithms [3, 13, 14, 15], UCFH uses an optimism under uncertainty approach to balance exploration and exploitation.", "startOffset": 34, "endOffset": 49}, {"referenceID": 4, "context": "Specifically, we use a finite horizon variant of extended value iteration (EVI) [5, 14].", "startOffset": 80, "endOffset": 87}, {"referenceID": 13, "context": "Specifically, we use a finite horizon variant of extended value iteration (EVI) [5, 14].", "startOffset": 80, "endOffset": 87}, {"referenceID": 5, "context": "UCFH is inspired by the infinite-horizon UCRL-\u03b3 algorithm by Lattimore and Hutter [6] but has several important differences.", "startOffset": 82, "endOffset": 85}, {"referenceID": 0, "context": "H, s\u2032 \u2208 S(s, a) p\u0303t(s \u2032|s, a) \u2208 ConfidenceSet(p\u0302(s\u2032|s, a), n(s, a)) } ; M\u0303k, \u03c0 k := FixedHorizonEVI(Mk); /* Execute policy */ repeat SampleEpisode(\u03c0) ; // from M using \u03c0 until there is a (s, a) \u2208 S \u00d7A with v(s, a) \u2265 max{mwmin, n(s, a)} and n(s, a) < |S|mH; /* Update model statistics for one (s, a)-pair with condition above */ n(s, a) := n(s, a) + v(s, a); n(s, a, s\u2032) := n(s, a, s\u2032) + v(s, a, s\u2032) \u2200s\u2032 \u2208 S(s, a); v(s, a) := v(s, a, s\u2032) := 0 \u2200s\u2032 \u2208 S(s, a); k := k + 1 Procedure SampleEpisode(\u03c0) s0 \u223c p0; for t = 0 to H \u2212 1 do at := \u03c0t+1(st) and st+1 \u223c p(\u00b7|st, at); v(st, at) := v(st, at) + 1 and v(st, at, st+1) := v(st, at, st+1) + 1; Function ConfidenceSet(p, n) P := { p\u2032 \u2208 [0, 1] :if n > 1 : \u2223\u2223\u2223\u221ap\u2032(1\u2212 p\u2032)\u2212\u221ap(1\u2212 p)\u2223\u2223\u2223 \u2264\u221a2 ln(6/\u03b41) n\u2212 1 , (1)", "startOffset": 677, "endOffset": 683}, {"referenceID": 5, "context": "Unlike the confidence intervals used by Lattimore and Hutter [6], we not only include conditions based on Hoeffding\u2019s inequality5 and Bernstein\u2019s inequality (Eq.", "startOffset": 61, "endOffset": 64}, {"referenceID": 2, "context": "Brafman and Tennenholtz [3], Strehl and Littman [16]).", "startOffset": 24, "endOffset": 27}, {"referenceID": 15, "context": "Brafman and Tennenholtz [3], Strehl and Littman [16]).", "startOffset": 48, "endOffset": 52}, {"referenceID": 5, "context": "The general proof strategy is closest to the one of UCRL-\u03b3 [6] and the obtained bounds are similar if we replace the time horizon H with the equivalent in the discounted case 1/(1\u2212 \u03b3).", "startOffset": 59, "endOffset": 62}, {"referenceID": 5, "context": "\u2022 A central quantity in the analysis by Lattimore and Hutter [6] is the local variance of the value function.", "startOffset": 61, "endOffset": 64}, {"referenceID": 5, "context": "The key insight for the almost tight bounds of Lattimore and Hutter [6] and Azar et al.", "startOffset": 68, "endOffset": 71}, {"referenceID": 7, "context": "[8] is to leverage the fact that these local variances satisfy a Bellman equation [17] and so the discounted sum of local variances can be bounded by O((1\u2212\u03b3)\u22122) instead of O((1\u2212\u03b3)\u22123).", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[8] is to leverage the fact that these local variances satisfy a Bellman equation [17] and so the discounted sum of local variances can be bounded by O((1\u2212\u03b3)\u22122) instead of O((1\u2212\u03b3)\u22123).", "startOffset": 82, "endOffset": 86}, {"referenceID": 5, "context": "\u2022 Lattimore and Hutter [6] assumed there are only two possible successor states (i.", "startOffset": 23, "endOffset": 26}, {"referenceID": 5, "context": "We therefore avoid assuming C = 2 which makes UCFH directly applicable to generic MDPs with C > 2 without the impractical transformation argument used by Lattimore and Hutter [6].", "startOffset": 175, "endOffset": 178}, {"referenceID": 2, "context": "Many PAC RL sample complexity proofs [3, 4, 13, 14] only have a binary notion of \u201cknownness\u201d, distinguishing between known (transition probability estimated sufficiently accurately) and unknown (s, a)-pairs.", "startOffset": 37, "endOffset": 51}, {"referenceID": 3, "context": "Many PAC RL sample complexity proofs [3, 4, 13, 14] only have a binary notion of \u201cknownness\u201d, distinguishing between known (transition probability estimated sufficiently accurately) and unknown (s, a)-pairs.", "startOffset": 37, "endOffset": 51}, {"referenceID": 12, "context": "Many PAC RL sample complexity proofs [3, 4, 13, 14] only have a binary notion of \u201cknownness\u201d, distinguishing between known (transition probability estimated sufficiently accurately) and unknown (s, a)-pairs.", "startOffset": 37, "endOffset": 51}, {"referenceID": 13, "context": "Many PAC RL sample complexity proofs [3, 4, 13, 14] only have a binary notion of \u201cknownness\u201d, distinguishing between known (transition probability estimated sufficiently accurately) and unknown (s, a)-pairs.", "startOffset": 37, "endOffset": 51}, {"referenceID": 5, "context": "However, as recently shown by Lattimore and Hutter [6] for the infinite horizon setting, it is possible to obtain much tighter sample complexity results by using a more fine grained categorization.", "startOffset": 51, "endOffset": 54}, {"referenceID": 17, "context": "By combining Hoeffding\u2019s inequality, Bernstein\u2019s inequality and the concentration result on empirical standard deviations by Maurer and Pontil [18] with the union bound, we get that p(s\u2032|s, a) \u2208 P with probability at least 1 \u2212 \u03b41 for a single phase k, fixed s, a \u2208 S \u00d7 A and fixed s\u2032 \u2208 S(s, a).", "startOffset": 143, "endOffset": 147}, {"referenceID": 18, "context": "The ranges of possible \u03b4 and are of similar order than in other state-of-the-art lower bounds for multi-armed bandits [19] and discounted MDPs [14, 6].", "startOffset": 118, "endOffset": 122}, {"referenceID": 13, "context": "The ranges of possible \u03b4 and are of similar order than in other state-of-the-art lower bounds for multi-armed bandits [19] and discounted MDPs [14, 6].", "startOffset": 143, "endOffset": 150}, {"referenceID": 5, "context": "The ranges of possible \u03b4 and are of similar order than in other state-of-the-art lower bounds for multi-armed bandits [19] and discounted MDPs [14, 6].", "startOffset": 143, "endOffset": 150}, {"referenceID": 18, "context": "They are mostly determined by the bandit result by Mannor and Tsitsiklis [19] we build on.", "startOffset": 73, "endOffset": 77}, {"referenceID": 13, "context": "Similar MDPs that essentially solve multiple of such multi-armed bandits have been used to prove lower sample-complexity bounds for discounted MDPs [14, 6].", "startOffset": 148, "endOffset": 155}, {"referenceID": 5, "context": "Similar MDPs that essentially solve multiple of such multi-armed bandits have been used to prove lower sample-complexity bounds for discounted MDPs [14, 6].", "startOffset": 148, "endOffset": 155}, {"referenceID": 3, "context": "However, the analysis in the infinite horizon case as well as for the sliding-window fixed-horizon optimality criterion considered by Kakade [4] is significantly simpler.", "startOffset": 141, "endOffset": 144}, {"referenceID": 0, "context": "[1] improves the state-dependency of these bounds for their delayed Q-learning algorithm to \u00d5 ( |S||A|H 4 ln 1 \u03b4 ) .", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Kolter and Ng [9] use an interesting sliding-window criterion, but prove bounds for a Bayesian setting instead of PAC.", "startOffset": 14, "endOffset": 17}, {"referenceID": 19, "context": "[20] yields a PAC-bound on the number of episodes of at least \u00d5 ( |S||A|H 2 ln 1 \u03b4 ) even if one ignores the reset after H time steps.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Similar to us, Fiechter [10] uses the value of initial states as optimality-criterion, but defines the value w.", "startOffset": 24, "endOffset": 28}, {"referenceID": 4, "context": "Auer and Ortner [5] investigate the same setting as we and propose a UCB-type algorithm that has noregret, which translates into a basic PAC bound of order \u00d5 ( |S||A|H 3 ln 1 \u03b4 ) episodes.", "startOffset": 16, "endOffset": 19}, {"referenceID": 11, "context": "Reveliotis and Bountourelis [12] also consider the episodic undiscounted fixed-horizon setting and present an efficient algorithm in cases where the transition graph is acyclic and the agent knows for each state a policy that visits this state with a known minimum probability q.", "startOffset": 28, "endOffset": 32}, {"referenceID": 6, "context": "One might hope to reduce the dependency of the upper bound on |S| to be linear by an analysis similar to Mormax [7] for discounted MDPs which has sample complexity linear in |S| at the penalty of additional dependencies on H .", "startOffset": 112, "endOffset": 115}, {"referenceID": 3, "context": "While the original bound stated by Kakade [4] only has H, an additional H comes in through \u22123 due to different normalization of rewards.", "startOffset": 42, "endOffset": 45}, {"referenceID": 0, "context": "References [1] Alexander L.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Michael J Kearns and Satinder P Singh.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Ronen I Brafman and Moshe Tennenholtz.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Sham M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Peter Auer and Ronald Ortner.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Tor Lattimore and Marcus Hutter.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Istv\u00e0n Szita and Csaba Szepesv\u00e1ri.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Mohammad Gheshlaghi Azar, R\u00e9mi Munos, and Hilbert J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] J Zico Kolter and Andrew Y Ng.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Claude-Nicolas Fiechter.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Claude-Nicolas Fiechter.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Spyros Reveliotis and Theologos Bountourelis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Alexander L Strehl, Lihong Li, and Michael L Littman.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Alexander L Strehl, Lihong Li, and Michael L Littman.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Thomas Jaksch, Ronald Ortner, and Peter Auer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Alexander L.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Matthew J Sobel.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Andreas Maurer and Massimiliano Pontil.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Shie Mannor and John N Tsitsiklis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Thomas Jaksch, Ronald Ortner, and Peter Auer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "We can find such a policy by dynamic programming similar to extended value iteration [16, 5].", "startOffset": 85, "endOffset": 92}, {"referenceID": 4, "context": "We can find such a policy by dynamic programming similar to extended value iteration [16, 5].", "startOffset": 85, "endOffset": 92}, {"referenceID": 0, "context": "The feasible set is defined as Ps,a := {p \u2208 [0, 1]|S(s,a)| | \u2016p\u20161 = 1,\u2200s\u2032 \u2208 S(s, a) : p(s\u2032) \u2208 conv(ConfidenceSet(p\u0302(s\u2032|s, a), n(s, a)))}.", "startOffset": 44, "endOffset": 50}, {"referenceID": 15, "context": "The inner max can be solved efficiently by enumeration and the outer maximum similar to extended value iteration [16].", "startOffset": 113, "endOffset": 117}, {"referenceID": 15, "context": "See Appendix A of Strehl and Littman [16] for details.", "startOffset": 37, "endOffset": 41}, {"referenceID": 17, "context": "Using both inequalities of Theorem 10 by Maurer and Pontil [18]8, we have", "startOffset": 59, "endOffset": 63}, {"referenceID": 17, "context": "The empirical variance denoted by Vn(X) by Maurer and Pontil [18] is p\u0303(s\u2032|s, a)(1 \u2212 p\u0303(s\u2032|s, a)) in our case and EVn is the true variance which amounts to p(s\u2032|s, a)(1\u2212 p(s\u2032|s, a)) for us.", "startOffset": 61, "endOffset": 65}, {"referenceID": 0, "context": "Assume p, p\u0302, p\u0303 \u2208 [0, 1] satisfy p \u2208 P and p\u0303 \u2208 conv(P) where", "startOffset": 19, "endOffset": 25}, {"referenceID": 0, "context": "P := { p\u2032 \u2208 [0, 1] :|p\u0302\u2212 p\u2032| \u2264 \u221a ln(6/\u03b41) 2n ,", "startOffset": 12, "endOffset": 18}, {"referenceID": 0, "context": "We have P = P1 \u2229 P2 with P1 = { p\u2032 \u2208 [0, 1] :|p\u0302\u2212 p\u2032| \u2264 \u221a ln(6/\u03b41) 2n ,", "startOffset": 37, "endOffset": 43}, {"referenceID": 0, "context": "Note that the last condition of P1 is equivalent to \u221a p\u0302(1\u2212 p\u0302) \u2264 \u221a p\u2032(1\u2212 p\u2032) + \u221a 2 ln(6/\u03b41) n\u22121 as p\u2032 \u2208 [0, 1].", "startOffset": 105, "endOffset": 111}, {"referenceID": 18, "context": "Since allGi are independent of each other and \u2032 = 16 e (H \u2212 2)\u03b7 \u2264 16(H \u2212 2)e \u03b7 (H \u2212 2)64e4\u03b7 = 1 4 we can apply Theorem 1 by Mannor and Tsitsiklis [19] in cases where \u03b4i \u2264 1 \u03b7 (1\u2212 \u03c6+ \u03b4\u03c6) \u2264 1 \u03b7 (1\u2212 \u03c6+ \u03b4) \u2264 1 8e4 + \u03b4 \u03b7 \u2264 2 8e4 .", "startOffset": 146, "endOffset": 150}, {"referenceID": 0, "context": "\u03b4n\u2208[0,1] n \u2211", "startOffset": 3, "endOffset": 8}, {"referenceID": 0, "context": "with c \u2208 [0, 1] and \u03b7(1\u2212 ln c) \u2264 1 has optimal solution \u03b41 = \u00b7 \u00b7 \u00b7 = \u03b4n = c.", "startOffset": 9, "endOffset": 15}], "year": 2016, "abstractText": "Recently, there has been significant progress in understanding reinforcement learning in discounted infinite-horizon Markov decision processes (MDPs) by deriving tight sample complexity bounds. However, in many real-world applications, an interactive learning agent operates for a fixed or bounded period of time, for example tutoring students for exams or handling customer service requests. Such scenarios can often be better treated as episodic fixed-horizon MDPs, for which only looser bounds on the sample complexity exist. A natural notion of sample complexity in this setting is the number of episodes required to guarantee a certain performance with high probability (PAC guarantee). In this paper, we derive an upper PAC bound \u00d5( |S| |A|H 2 ln 1 \u03b4 ) and a lower PAC bound \u03a9\u0303( |S||A|H 2 ln 1 \u03b4+c ) that match up to log-terms and an additional linear dependency on the number of states |S|. The lower bound is the first of its kind for this setting. Our upper bound leverages Bernstein\u2019s inequality to improve on previous bounds for episodic finitehorizon MDPs which have a time-horizon dependency of at least H.", "creator": "LaTeX with hyperref package"}}}