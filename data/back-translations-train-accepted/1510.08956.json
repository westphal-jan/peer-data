{"id": "1510.08956", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2015", "title": "Principal Differences Analysis: Interpretable Characterization of Differences between Distributions", "abstract": "We introduce principal differences analysis (PDA) for analyzing differences between high-dimensional distributions. The method operates by finding the projection that maximizes the Wasserstein divergence between the resulting univariate populations. Relying on the Cramer-Wold device, it requires no assumptions about the form of the underlying distributions, nor the nature of their inter-class differences. A sparse variant of the method is introduced to identify features responsible for the differences. We provide algorithms for both the original minimax formulation as well as its semidefinite relaxation. In addition to deriving some convergence results, we illustrate how the approach may be applied to identify differences between cell populations in the somatosensory cortex and hippocampus as manifested by single cell RNA-seq. Our broader framework extends beyond the specific choice of Wasserstein divergence.", "histories": [["v1", "Fri, 30 Oct 2015 03:06:00 GMT  (293kb,D)", "http://arxiv.org/abs/1510.08956v1", "Advances in Neural Information Processing Systems 28 (NIPS 2015)"]], "COMMENTS": "Advances in Neural Information Processing Systems 28 (NIPS 2015)", "reviews": [], "SUBJECTS": "stat.ML cs.LG stat.ME", "authors": ["jonas mueller", "tommi s jaakkola"], "accepted": true, "id": "1510.08956"}, "pdf": {"name": "1510.08956.pdf", "metadata": {"source": "CRF", "title": "Principal Differences Analysis: Interpretable Characterization of Differences between Distributions", "authors": ["Jonas Mueller", "Tommi Jaakkola"], "emails": ["jonasmueller@csail.mit.edu", "tommi@csail.mit.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to go in search of a solution."}, {"heading": "2 Related Work", "text": "The problem of characterizing differences between populations, including feature selection, has been extensively studied [2, 12, 13, 5, 1]. We limit our discussion to projection-based methods that are closest to our approach as a family of methods. Among the most common methods for multivariate two-class data are (sparse) linear discriminant analysis (LDA) [2] and the logistic lasso [12]. Although these methods can be interpreted, they seek specific differences (e.g. covariance-rescaled average differences) or operate under strict assumptions (e.g. log-linear model). In contrast, SPARDA (with a positively-defined divergence) aims to find characteristics that characterize a priori unspecified differences between general multivariate distributions. Perhaps the most similar to our general approach is the direction-projection-permutation permutation (DiProPerm) of the Wejial et 5 test method, which is first distributed along the normal [5] SVF."}, {"heading": "3 General Framework for Principal Differences Analysis", "text": "For a given divergence, we measure D between two univariate random variables, we find the projection p\u03b2, which defines additional characteristics about efficiency. Furthermore, the projection p\u03b2, the solvesmax \u03b2PB, | | \u03b2 | 0\u010fkDp\u03b2T pXpnq, \u03b2T pY pmqq (((1), where B: \"t\u03b2 P Rd: | \u03b2 | | | 2\" is the practicable quantity, | \u03b2 | | 0 \"k is the austerity restriction, and \u03b2T pXpnq denotes the observed random variable following the empirical distribution of the n-samples of \u03b2TX. Instead of imposing a hard cardinality restriction, we can spare differentiation by adding a penalty term 1\" \u00df | | 0 \"or its natural relativization, the\" 1 shrinkage used in Lasso [12], LDA [2] and PCA, [14] sparse."}, {"heading": "4 PDA using the Wasserstein Distance", "text": "In the rest of the paper, we focus on the squared L2 Waterstone distance (a.k.\u03b2.), i.e. a complex problem (a.k.a Kantorovich, Mallows, Dudley, or earth-mover distance), defined by asDpX, Y q \"min PXY | | | 2 s.t. pX, Y q\" PXY, X \"PX, Y\" PY (2), where minimization occurs over all common distributions via pX, Y q with the given marginal PX and PY. Intuitively interpreted as the workload required to transform one distribution into the other, D offers a natural dissimilarity between populations, integrating both the proportion of individuals that are different and the magnitude of these differences. While component analysis is based on the Waterstone distance, this divergence is limited to [19], it has been successfully used in many other applications [20]."}, {"heading": "4.1 Semidefinite Relaxation", "text": "\"We have to ask ourselves what we can do if we want to do it.\" \"We have to do what we do.\" \"We have to do it.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" We. \"\" We. \"\" We. \"\" We. \"\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\""}, {"heading": "4.2 Tightening after relaxation", "text": "It is unreasonable to expect that our semidefinitive relaxation is always tight. Therefore, we can sometimes refine \u03b2 \u03b2 \u03b2 q procedures, which we obtain through the RELAX algorithm, using it as a starting point in the original non-convex optimization. We provide a sparsity-limited tightening procedure for the application of the projected gradient ascent for the original non-convex objective solution Jp\u03b2q \"minMPM \u03b2TWMM \u03b2TWM\u03b2, where \u03b2\" s is now forced to lie in BXSk and Sk: \"t\u03b2 P Rd:\" 0 \"0.\" The sparsity level k is fixed on the basis of the relaxed solution (k \"), p\u03b2S\" s os \"s\" s \"s\" s \"s\" s \"s.\""}, {"heading": "4.3 Properties of semidefinite relaxation", "text": "We conclude the algorithmic discussion by highlighting the basic conditions under which our PDA relaxation is tight. Suppose n, m \u04418, each of (i) - (iii) implies that the B \u02da maximizing (5) is almost rank one or equivalent to B \u02da \"r\u00df\" (see Supplementary Information \u00a7 S4 for Intuition), for example, when the tightening process initiated at r\u03b2 produces a global maximum of the PDA target. (i) There is a direction in which the projected Waterstone distance between X and Y is almost as large as the total Waterstone distance in Rd."}, {"heading": "5 Theoretical Results", "text": "\"In this section, we characterize statistical properties of an empirical divergence-maximizing projection.\" \"We assume that such a global maximum forecast for severe nonconvex problems cannot be achieved.\" \"We make the following simplistic assumptions: (A1) n\" m \"(A2) n\" m \"n.\" (A3) X. \"X\" c \"represents universal constants that change from line to line. All proofs are referred to the supplementary information \u00a7 S3.\" We make the following simplifying assumptions: (A1) n \"m\" n \"m\" m \"m\" (A2) X. \""}, {"heading": "6 Experiments", "text": "The synthetic MADELON data used in the NIPS selection challenge consists of dots (n \"m\" 1000, d \"500) that have 5 characteristics scattered over the recesses of a fivedimensional hypercube (so that interactions between the characteristics must be taken into account to distinguish the two classes), 15 characteristics that are noisy linear combinations of the original five and 480 useless characteristics. While the focus of the challenge was on extracting characteristics that are useful for classifiers, we focus on interpretative models."}, {"heading": "7 Conclusion", "text": "While we focused on algorithms for PDA & SPARDA that are tailored to the Waterstone distance, different divergences may be more appropriate for certain applications.Further theoretical studies of the SPARDA framework are of interest, especially in the high-dimensional d \"Opnq environment, where a rich theory for compressed capture and sparse PCA was derived using ideas such as limited isometry or prickly covariance [15].A natural question is then what analog properties of PX, PY theoretically guarantee the strong empirical performance of SPARDA observed in our high-dimensional applications. Finally, we also envisage extensions of the methods presented here that apply multiple projections successively or adapt the approach to the non-pair comparison of multiple populations."}, {"heading": "Acknowledgements", "text": "This research was supported by NIH grant T32HG004947."}, {"heading": "S1 Details of simulation study 10", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "S2 Single cell gene expression in cortex vs. hippocampus 11", "text": "S2.1 Identification of genes whose interactions differ between the cortex and hippocampus cells 11"}, {"heading": "S3 Proofs and Auxiliary Lemmas 15", "text": "S3.1 Proof of theorem 1..........................................................................................................................."}, {"heading": "S4 Derivation of semidefinite relaxation properties 17", "text": "List of FigureS1 Enriched biological processes in annotations of top SPARDA genes....... 13S2 Cellular Snca expression in somatosensory cortex vs. hippocampus........ 14List of tablets S1 Top genes found by SPARDA........................."}, {"heading": "S1 Details of simulation study", "text": "To illustrate the cost function in Figure 1a, we draw n \"m\" 1000 points from the mean zero distributions with the following respective covariance matrices: 1 0.2 0.4 0.2 1 ff.Y \"1 '0.9 0' 0.9 0 ff.Due to the large sample quantities, the empirical distributions accurately represent the underlying populations, and thus the projection generated by the tightening process (in green) is considerably worse than the projection captured by the RELAX algorithm (in red) with respect to the actual divergence. Note that only dimensions 2 and 3 of the projection space are plotted in Figure\" b. \""}, {"heading": "S2 Single cell gene expression in cortex vs. hippocampus", "text": "In fact, it is in such a way that we are able to move to another world, in which we can move to another world, in which we are able to explore another world, in which we are able to explore another world, in which we are able to explore another world, in which we are able to explore another world, in which we are able to explore another world, in which we create another world, in which we create another world, in which we create another world, in which we create another world, in which we are able to move to another world, in which we are able to explore another world, in which we are able to move to explore another world, in which we create another world, in which we create another world, in which we create another world, in which we move to a world, in which we move to a world, in which we move to a world, in which we move to a world, in which we move to a world, in which we move to a world, in which we move to a world, in which we move to a world, in which we move to a world, in which we move to a world, in which we move to a world, in which we move to a world, in which we move to a world, in which we move to a world, in which we move to a world, in which we move to a world in which we move to a world, in which we move to a world, in which we move to a world, in which we move to a world in which we move to a world, in which we move to a world, in which we move to a world in which we move to a world, in which we move to a world in which we move to a world, a world in which we move to a world in which we move to a world, a world in which we move to a world in which we move to a world in which we move to a world in which we move to a world we move to a world, a world in which we move to a world in which we move to a world we move to a world, a world in which we move to a world in which we move to a world, a world in which we move to a world in which we move to a world in which we move to a world we move to a world, a world in which we move to a world in which we move in which we move to a world we move to a world"}, {"heading": "S3 Proofs and Auxiliary Lemmas", "text": "In this section, we use C to name absolute constants whose value may vary from line to line. \"F\" is defined as cdf of a random variable, and the corresponding quantile function is F \"1ppq.\" (A3) \"The quantile function corresponds to the unique inversion of all projected cdf.\" \"Empirical divergence is used to represent the empirical versions of all quantities, and it is recalled that D denotes the square Waterstone distance.\" (S3.1) \"Proof Theorem 1Proof.\" Since empirical divergence is the empirical divergence of PrpDpp\u03b2T pnqq. \""}, {"heading": "S4 Derivation of semidefinite relaxation properties", "text": "Here we provide some intuitive arguments for the conclusions in Abs. 4.3, regarding some conditions under which our semi-definitive relaxation A 48-41E-41E-41E-41E-41E-41E-41E-41E-41E-41E-41E-41E-41E-41E-41E-41E-41E-41E-41E-41E-41E-41E-41E-41E-41E-41E-41E-41E-41E-41E-41E-41E-41E-41E-41E-41E-41E-41E-41E-41E-41E-41E-2E-2E-2E-2E-2E-2E-2E-2E-2E-2E-2E-2E-2E-2E-2E-2E-2E-2E-2E-2E-2E-2E-2E-2E-2E-2E-2E-2E-2E-2D-2D-2D-2D-2D-2D-2D-2D-2D-2D-2D-2D-2D-2E-2D-2D-2E-2D-E-2D-2D-2D-E-E-2D-2D-2D-E-2D-E-2D-E-E-E-2D-E-2D-4E-4E-4E-E-4E-4E-4E-4E-4E-4E-4E-4E-4E-4E-4E-E-4E-E-4E-4E-E-E-4E-4E-E-4E-4E-E-4E-4E-4E-4E-E-E-4E-4E-4E-4E-E-E-4E-4E-E-E"}], "references": [{"title": "A More Powerful Two-Sample Test in High Dimensions using Random Projection", "author": ["M Lopes", "L Jacob", "M Wainwright"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "On Choosing and Bounding Probability Metrics", "author": ["AL Gibbs", "FE Su"], "venue": "International Statistical Review", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Direction-Projection-Permutation for High Dimensional Hypothesis Tests", "author": ["S Wei", "C Lee", "L Wichers", "JS Marron"], "venue": "Journal of Computational and Graphical Statistics", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "An exact distribution-free test comparing two multivariate distributions based on adjacency", "author": ["PR Rosenbaum"], "venue": "Journal of the Royal Statistical Society Series B", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Testing for equal distributions in high dimension", "author": ["G Szekely", "M Rizzo"], "venue": "InterStat", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "A (2012) A Kernel Two-Sample Test", "author": ["A Gretton", "KM Borgwardt", "MJ Rasch", "B Scholkopf", "Smola"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "A sharp form of the Cramer\u2013Wold theorem", "author": ["JA Cuesta-Albertos", "R Fraiman", "T Ransford"], "venue": "Journal of Theoretical Probability", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "On the maximum of covariance estimators", "author": ["M Jirak"], "venue": "Journal of Multivariate Analysis", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R Tibshirani"], "venue": "Journal of the Royal Statistical Society Series B : 267\u2013288", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1996}, {"title": "Feature Selection via Concave Minimization and Support", "author": ["PS Bradley", "OL Mangasarian"], "venue": "Vector Machines", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "A direct formulation for sparse PCA using semidefinite programming", "author": ["A D\u2019Aspremont", "L El Ghaoui", "MI Jordan", "GR Lanckriet"], "venue": "SIAM Review : 434\u2013448", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "High-dimensional analysis of semidefinite relaxations for sparse principal components", "author": ["AA Amini", "MJ Wainwright"], "venue": "The Annals of Statistics", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Permutation Tests: A Practical Guide to Resampling Methods for Testing Hypotheses. Spring-Verlag", "author": ["P Good"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1994}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["J Duchi", "E Hazan", "Y Singer"], "venue": "Journal of Machine Learning Research", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Optimization Algorithms in Machine Learning", "author": ["SJ Wright"], "venue": "NIPS Tutorial", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Nonnegative Matrix Factorization with Earth Mover\u2019s Distance Metric for Image Analysis", "author": ["R Sandler", "M Lindenbaum"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "The Earth Mover\u2019s distance is the Mallows distance: some insights from statistics", "author": ["E Levina", "P Bickel"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2001}, {"title": "Tighten after Relax: Minimax-Optimal Sparse PCA in Polynomial Time", "author": ["Z Wang", "H Lu", "H Liu"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Network Optimization: Continuous and Discrete Models", "author": ["DP Bertsekas"], "venue": "Athena Scientific", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Dual coordinate step methods for linear network flow problems", "author": ["DP Bertsekas", "J Eckstein"], "venue": "Mathematical Programming", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1988}, {"title": "Incremental gradient, subgradient, and proximal methods for convex optimization: A survey. In: Optimization for Machine Learning", "author": ["DP Bertsekas"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems", "author": ["A Beck", "M Teboulle"], "venue": "SIAM Journal on Imaging Sciences", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Feature Extraction: Foundations and Applications", "author": ["I Guyon", "S Gunn", "M Nikravesh", "LA Zadeh"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "Sparse Principal Component Analysis", "author": ["H Zou", "T Hastie", "R Tibshirani"], "venue": "Journal of Computational and Graphical Statistics", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2005}, {"title": "The details in the distributions: why and how to study phenotypic variability", "author": ["KA Geiler-Samerotte", "CR Bauer", "S Li", "N Ziv", "D Gresham"], "venue": "Current opinion in biotechnology", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "The Earth Mover\u2019s distance is the Mallows distance: some insights from statistics", "author": ["E Levina", "P Bickel"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2001}, {"title": "An exact distribution-free test comparing two multivariate distributions based on adjacency", "author": ["PR Rosenbaum"], "venue": "Journal of the Royal Statistical Society Series B", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2005}, {"title": "A (2012) A Kernel Two-Sample Test", "author": ["A Gretton", "KM Borgwardt", "MJ Rasch", "B Scholkopf", "Smola"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "Testing for equal distributions in high dimension", "author": ["G Szekely", "M Rizzo"], "venue": "InterStat", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2004}, {"title": "Direction-Projection-Permutation for High Dimensional Hypothesis Tests", "author": ["S Wei", "C Lee", "L Wichers", "JS Marron"], "venue": "Journal of Computational and Graphical Statistics", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Cell types in the mouse cortex and hippocampus revealed by single-cell RNA-seq", "author": ["A Zeisel", "AB Munoz-Manchado", "S Codeluppi", "P Lonnerberg", "G La Manno"], "venue": "Science", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}, {"title": "The dynamics and regulators of cell fate decisions are revealed by pseudotemporal ordering of single cells", "author": ["C Trapnell", "D Cacchiarelli", "J Grimsby", "P Pokharel", "S Li"], "venue": "Nature Biotechnology", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "limma powers differential expression analyses for RNA-sequencing and microarray studies", "author": ["M Ritchie", "B Phipson", "D Wu", "Y Hu", "CW Law"], "venue": "Nucleic Acids Research", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "Parkinson\u2019s disease: from monogenic forms to genetic susceptibility factors", "author": ["S Lesage", "A Brice"], "venue": "Human Molecular Genetics", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2009}, {"title": "The genetic contributions of SNCA and LRRK2 genes to Lewy Body pathology in Alzheimer\u2019s disease", "author": ["C Linnertz", "MW Lutz", "JF Ervin", "J Allen", "NR Miller"], "venue": "Human molecular genetics", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2014}, {"title": "Distinct subtypes of cholecystokinin (CCK)-containing interneurons of the basolateral amygdala identified using a CCK promoter-specific lentivirus", "author": ["AM Jasnow", "KJ Ressler", "SE Hammack", "JP Chhatwal", "DG Rainnie"], "venue": "Journal of neurophysiology", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2009}, {"title": "Neuronal Basic HelixLoopHelix Proteins Neurod2/6 Regulate Cortical Commissure Formation before Midline Interactions", "author": ["I Bormuth", "K Yan", "T Yonemasu", "M Gummert", "M Zhang"], "venue": "Journal of Neuroscience", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2013}, {"title": "Increased glycogen synthase kinase-3beta mRNA level in the hippocampus of patients with major depression: a study using the stanley neuropathology consortium integrative database", "author": ["DH Oh", "YC Park", "SH Kim"], "venue": "Psychiatry investigation", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2010}, {"title": "Transcriptome Sequencing Revealed Significant Alteration of Cortical Promoter Usage and Splicing in Schizophrenia", "author": ["JQ Wu", "X Wang", "NJ Beveridge", "PA Tooney", "RJ Scott"], "venue": "PLoS ONE", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2012}, {"title": "Neuronal subtype specification in the cerebral cortex", "author": ["BJ Molyneaux", "P Arlotta", "JRL Menezes", "JD Macklis"], "venue": "Nat Rev Neurosci", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2007}, {"title": "Genetic Ablation of Nrf2/Antioxidant Response Pathway in Alexander Disease Mice Reduces Hippocampal Gliosis but Does Not Impact Survival", "author": ["TL Hagemann", "EM Jobe", "A Messing"], "venue": "PLoS ONE", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2012}, {"title": "Developmental expression of the SRF co-activator MAL in brain: role in regulating dendritic morphology", "author": ["J Shiota", "M Ishikawa", "H Sakagami", "M Tsuda", "JM Baraban"], "venue": "Journal of Neurochemistry", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2006}, {"title": "Identifying Tmem59 related gene regulatory network of mouse neural stem cell from a compendium of expression profiles", "author": ["L Zhang", "X Ju", "Y Cheng", "X Guo", "T Wen"], "venue": "BMC Systems Biology", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2011}, {"title": "WT1 and its transcriptional cofactor BASP1 redirect the differentiation pathway of an established blood cell line", "author": ["S Goodfellow", "M Rebello", "E Toska", "L Zeef", "S Rudd"], "venue": "Biochemical Journal", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2011}, {"title": "Pathways and genes differentially expressed in the motor cortex of patients with sporadic amyotrophic lateral sclerosis", "author": ["CW Lederer", "A Torrisi", "M Pantelidou", "N Santama", "S Cavallaro"], "venue": "BMC genomics", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2007}, {"title": "Corticospinal Motor Neurons Are Susceptible to Increased ER Stress and Display Profound Degeneration in the Absence of UCHL1 Function. Cerebral Cortex", "author": ["JH Jara", "B Gen\u00e7", "GA Cox", "MC Bohn", "RP Roos"], "venue": null, "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2015}, {"title": "Cholecystokinin from the entorhinal cortex enables neural plasticity in the auditory cortex", "author": ["X Li", "K Yu", "Z Zhang", "W Sun", "Z Yang"], "venue": "Cell Res", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2014}, {"title": "Cortical sources of CRF, NKB, and CCK and their effects on pyramidal cells in the neocortex. Cerebral cortex", "author": ["T Gallopin", "H Geoffroy", "J Rossier", "B Lambolez"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2006}, {"title": "On the maximum of covariance estimators", "author": ["M Jirak"], "venue": "Journal of Multivariate Analysis", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2011}, {"title": "On Choosing and Bounding Probability Metrics", "author": ["AL Gibbs", "FE Su"], "venue": "International Statistical Review", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "Many two-sample analyses have focused on characterizing limited differences such as mean shifts [1, 2].", "startOffset": 96, "endOffset": 102}, {"referenceID": 1, "context": "), the Kolmogorov distance, or the Wasserstein metric [4].", "startOffset": 54, "endOffset": 57}, {"referenceID": 2, "context": "Unfortunately, this simplicity vanishes as the dimensionality d grows, and complex test-statistics have been designed to address some of the difficulties that appear in high-dimensional settings [5, 6, 7, 8].", "startOffset": 195, "endOffset": 207}, {"referenceID": 3, "context": "Unfortunately, this simplicity vanishes as the dimensionality d grows, and complex test-statistics have been designed to address some of the difficulties that appear in high-dimensional settings [5, 6, 7, 8].", "startOffset": 195, "endOffset": 207}, {"referenceID": 4, "context": "Unfortunately, this simplicity vanishes as the dimensionality d grows, and complex test-statistics have been designed to address some of the difficulties that appear in high-dimensional settings [5, 6, 7, 8].", "startOffset": 195, "endOffset": 207}, {"referenceID": 5, "context": "Unfortunately, this simplicity vanishes as the dimensionality d grows, and complex test-statistics have been designed to address some of the difficulties that appear in high-dimensional settings [5, 6, 7, 8].", "startOffset": 195, "endOffset": 207}, {"referenceID": 6, "context": "This reduction is justified by the Cramer-Wold device, which ensures that PX \u2030 PY if and only if there exists a direction along which the univariate linearly projected distributions differ [9, 10, 11].", "startOffset": 189, "endOffset": 200}, {"referenceID": 7, "context": "This reduction is justified by the Cramer-Wold device, which ensures that PX \u2030 PY if and only if there exists a direction along which the univariate linearly projected distributions differ [9, 10, 11].", "startOffset": 189, "endOffset": 200}, {"referenceID": 8, "context": "2 Related Work The problem of characterizing differences between populations, including feature selection, has received a great deal of study [2, 12, 13, 5, 1].", "startOffset": 142, "endOffset": 159}, {"referenceID": 9, "context": "2 Related Work The problem of characterizing differences between populations, including feature selection, has received a great deal of study [2, 12, 13, 5, 1].", "startOffset": 142, "endOffset": 159}, {"referenceID": 2, "context": "2 Related Work The problem of characterizing differences between populations, including feature selection, has received a great deal of study [2, 12, 13, 5, 1].", "startOffset": 142, "endOffset": 159}, {"referenceID": 0, "context": "2 Related Work The problem of characterizing differences between populations, including feature selection, has received a great deal of study [2, 12, 13, 5, 1].", "startOffset": 142, "endOffset": 159}, {"referenceID": 8, "context": "For multivariate two-class data, the most widely adopted methods include (sparse) linear discriminant analysis (LDA) [2] and the logistic lasso [12].", "startOffset": 144, "endOffset": 148}, {"referenceID": 2, "context": "[5], in which the data is first projected along the normal to the separating hyperplane (found using linear SVM, distance weighted discrimination, or the centroid method) followed by a univariate two-sample test on the projected data.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "The projections could also be chosen at random [1].", "startOffset": 47, "endOffset": 50}, {"referenceID": 9, "context": "We note that by restricting the divergence measure in our technique, methods such as the (sparse) linear support vector machine [13] could be viewed as special cases.", "startOffset": 128, "endOffset": 132}, {"referenceID": 8, "context": "Instead of imposing a hard cardinality constraint ||\u03b2||0 \u010f k, we may instead penalize by adding a penalty term1  \u0301\u03bb||\u03b2||0 or its natural relaxation, the `1 shrinkage used in Lasso [12], sparse LDA [2], and sparse PCA [14, 15].", "startOffset": 180, "endOffset": 184}, {"referenceID": 10, "context": "Instead of imposing a hard cardinality constraint ||\u03b2||0 \u010f k, we may instead penalize by adding a penalty term1  \u0301\u03bb||\u03b2||0 or its natural relaxation, the `1 shrinkage used in Lasso [12], sparse LDA [2], and sparse PCA [14, 15].", "startOffset": 217, "endOffset": 225}, {"referenceID": 11, "context": "Instead of imposing a hard cardinality constraint ||\u03b2||0 \u010f k, we may instead penalize by adding a penalty term1  \u0301\u03bb||\u03b2||0 or its natural relaxation, the `1 shrinkage used in Lasso [12], sparse LDA [2], and sparse PCA [14, 15].", "startOffset": 217, "endOffset": 225}, {"referenceID": 2, "context": "[5, 16]) with statistic Dpp \u03b2 p Xpnq, p \u03b2 p Y pmqq.", "startOffset": 0, "endOffset": 7}, {"referenceID": 12, "context": "[5, 16]) with statistic Dpp \u03b2 p Xpnq, p \u03b2 p Y pmqq.", "startOffset": 0, "endOffset": 7}, {"referenceID": 13, "context": "Gaussian), the unregularized PDA objective (without shrinkage) is a smooth function of \u03b2, and thus amenable to the projected gradient method (or its accelerated variants [17, 18]).", "startOffset": 170, "endOffset": 178}, {"referenceID": 14, "context": "Gaussian), the unregularized PDA objective (without shrinkage) is a smooth function of \u03b2, and thus amenable to the projected gradient method (or its accelerated variants [17, 18]).", "startOffset": 170, "endOffset": 178}, {"referenceID": 15, "context": "While component analysis based on the Wasserstein distance has been limited to [19], this divergence has been successfully used in many other applications [20].", "startOffset": 79, "endOffset": 83}, {"referenceID": 16, "context": "While component analysis based on the Wasserstein distance has been limited to [19], this divergence has been successfully used in many other applications [20].", "startOffset": 155, "endOffset": 159}, {"referenceID": 16, "context": "(3) where M is the set of all n \u02c6 m nonnegative matching matrices with fixed row sums \u201c 1{n and column sums \u201c 1{m (see [20] for details), WM :\u201c \u0159 i,jrZij b ZijsMij , and Zij :\u201c xpiq  \u0301 ypjq.", "startOffset": 119, "endOffset": 123}, {"referenceID": 10, "context": "Similarly, for the sparse variant without minizing over M , the problem would be solvable as sparse PCA [14, 15, 21].", "startOffset": 104, "endOffset": 116}, {"referenceID": 11, "context": "Similarly, for the sparse variant without minizing over M , the problem would be solvable as sparse PCA [14, 15, 21].", "startOffset": 104, "endOffset": 116}, {"referenceID": 17, "context": "Similarly, for the sparse variant without minizing over M , the problem would be solvable as sparse PCA [14, 15, 21].", "startOffset": 104, "endOffset": 116}, {"referenceID": 17, "context": "We propose a two-step procedure similar to \u201ctighten after relax\u201d framework used to attain minimax-optimal rates in sparse PCA [21].", "startOffset": 126, "endOffset": 130}, {"referenceID": 10, "context": "Otherwise, we can truncateB \u030a as in [14], treating the dominant eigenvector as an approximate solution to the original problem (3).", "startOffset": 36, "endOffset": 40}, {"referenceID": 10, "context": "To obtain a relaxation for the sparse version where k \u0103 d (SPARDA), we follow [14] closely.", "startOffset": 78, "endOffset": 82}, {"referenceID": 10, "context": "By selecting \u03bb as the optimal Lagrange multiplier for this `1 constraint, we can obtain an equivalent penalized reformulation parameterized by \u03bb rather than k [14].", "startOffset": 159, "endOffset": 163}, {"referenceID": 18, "context": "[22, 23]) min MPM tr pWMBq \u201c 1 m max u,v n \u00ff", "startOffset": 0, "endOffset": 8}, {"referenceID": 19, "context": "[22, 23]) min MPM tr pWMBq \u201c 1 m max u,v n \u00ff", "startOffset": 0, "endOffset": 8}, {"referenceID": 20, "context": "For scaling to large samples, one may alternatively employ incremental supergradient directions [24] where Step 4 would be replaced by drawing random pi, jq pairs.", "startOffset": 96, "endOffset": 100}, {"referenceID": 20, "context": "15) in [24].", "startOffset": 7, "endOffset": 11}, {"referenceID": 20, "context": "2 of [24], the RELAX algorithm (as well as its incremental variant) is guaranteed to approach the optimal solution of the dual which also solves (6), provided we employ sufficiently large T and small step-sizes.", "startOffset": 5, "endOffset": 9}, {"referenceID": 16, "context": "The gradient is directly derivable from expression (3) where the nonzeroMij are determined by appropriately matching empirical quantiles (represented by sorted indices) since the univariate Wasserstein distance is simply the L2 distance between quantile functions [20].", "startOffset": 264, "endOffset": 268}, {"referenceID": 14, "context": "momentum techniques, adaptive learning rates, or FISTA and other variants of Nesterov\u2019s method [18, 17, 25]).", "startOffset": 95, "endOffset": 107}, {"referenceID": 13, "context": "momentum techniques, adaptive learning rates, or FISTA and other variants of Nesterov\u2019s method [18, 17, 25]).", "startOffset": 95, "endOffset": 107}, {"referenceID": 21, "context": "momentum techniques, adaptive learning rates, or FISTA and other variants of Nesterov\u2019s method [18, 17, 25]).", "startOffset": 95, "endOffset": 107}, {"referenceID": 7, "context": "[11]): TapX,Y q :\u201c |Prp|X1| \u010f a, .", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "The synthetic MADELON dataset used in the NIPS 2003 feature selection challenge consists of points (n \u201c m \u201c 1000, d \u201c 500) which have 5 features scattered on the vertices of a fivedimensional hypercube (so that interactions between features must be considered in order to distinguish the two classes), 15 features that are noisy linear combinations of the original five, and 480 useless features [26].", "startOffset": 396, "endOffset": 400}, {"referenceID": 23, "context": "Figure 1b demonstrates how well SPARDA (red), the top sparse principal component (black) [27], sparse LDA (green) [2], and the logistic lasso (blue) [12] are able to identify the 20 relevant features over different settings of their respective regularization parameters (which determine the cardinality of the vector returned by each method).", "startOffset": 89, "endOffset": 93}, {"referenceID": 8, "context": "Figure 1b demonstrates how well SPARDA (red), the top sparse principal component (black) [27], sparse LDA (green) [2], and the logistic lasso (blue) [12] are able to identify the 20 relevant features over different settings of their respective regularization parameters (which determine the cardinality of the vector returned by each method).", "startOffset": 149, "endOffset": 153}, {"referenceID": 22, "context": "The red asterisk indicates the SPARDA result with \u03bb automatically selected via our crossvalidation procedure (without information of the underlying features\u2019 importance), and the black asterisk indicates the best reported result in the challenge [26].", "startOffset": 246, "endOffset": 250}, {"referenceID": 22, "context": "Despite being class-agnostic, PCA was successfully utilized by numerous challenge participants [26], and we find that the sparse PCA performs on par with logistic regression and LDA.", "startOffset": 95, "endOffset": 99}, {"referenceID": 22, "context": "Similarly, the challengewinning Bayesian SVM with Automatic Relevance Determination [26] only selects 8 of the 20 relevant features.", "startOffset": 84, "endOffset": 88}, {"referenceID": 5, "context": "Figure 1c depicts (average) p-values produced by SPARDA (red), PDA (purple), the overall Wasserstein distance in R (black), Maximum Mean Discrepancy [8] (green), and DiProPerm [5] (blue) in two-sample synthetically controlled problems where PX \u2030 PY and the underlying differences have varying degrees of sparsity.", "startOffset": 149, "endOffset": 152}, {"referenceID": 2, "context": "Figure 1c depicts (average) p-values produced by SPARDA (red), PDA (purple), the overall Wasserstein distance in R (black), Maximum Mean Discrepancy [8] (green), and DiProPerm [5] (blue) in two-sample synthetically controlled problems where PX \u2030 PY and the underlying differences have varying degrees of sparsity.", "startOffset": 176, "endOffset": 179}, {"referenceID": 12, "context": "As we evaluate the significance of each method\u2019s statistic via permutation testing, all the tests are guaranteed to exactly control Type I error [16], and we thus only compare their respective power in determining PX \u2030 PY setting.", "startOffset": 145, "endOffset": 149}, {"referenceID": 2, "context": "This experiment also demonstrate that the unregularized PDA retains greater power than DiProPerm, a similar projection-based method [5].", "startOffset": 132, "endOffset": 135}, {"referenceID": 24, "context": "Recent technological advances allow complete transcriptome profiling in thousands of individual cells with the goal of fine molecular characterization of cell populations (beyond the crude averagetissue-level expression measure that is currently standard) [28].", "startOffset": 256, "endOffset": 260}, {"referenceID": 24, "context": "These types of important changes cannot be detected by standard differential expression analyses which consider each gene in isolation or require gene-sets to be explicitly identified as features [28].", "startOffset": 196, "endOffset": 200}, {"referenceID": 11, "context": "Here, rich theory has been derived for compressed sensing and sparse PCA by leveraging ideas such as restricted isometry or spiked covariance [15].", "startOffset": 142, "endOffset": 146}, {"referenceID": 0, "context": "References [1] Lopes M, Jacob L, Wainwright M (2011) A More Powerful Two-Sample Test in High Dimensions using Random Projection.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[4] Gibbs AL, Su FE (2002) On Choosing and Bounding Probability Metrics.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[5] Wei S, Lee C, Wichers L, Marron JS (2015) Direction-Projection-Permutation for High Dimensional Hypothesis Tests.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[6] Rosenbaum PR (2005) An exact distribution-free test comparing two multivariate distributions based on adjacency.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[7] Szekely G, Rizzo M (2004) Testing for equal distributions in high dimension.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[8] Gretton A, Borgwardt KM, Rasch MJ, Scholkopf B, Smola A (2012) A Kernel Two-Sample Test.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[10] Cuesta-Albertos JA, Fraiman R, Ransford T (2007) A sharp form of the Cramer\u2013Wold theorem.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[11] Jirak M (2011) On the maximum of covariance estimators.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[12] Tibshirani R (1996) Regression shrinkage and selection via the lasso.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[13] Bradley PS, Mangasarian OL (1998) Feature Selection via Concave Minimization and Support Vector Machines.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[14] D\u2019Aspremont A, El Ghaoui L, Jordan MI, Lanckriet GR (2007) A direct formulation for sparse PCA using semidefinite programming.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[15] Amini AA, Wainwright MJ (2009) High-dimensional analysis of semidefinite relaxations for sparse principal components.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[16] Good P (1994) Permutation Tests: A Practical Guide to Resampling Methods for Testing Hypotheses.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[17] Duchi J, Hazan E, Singer Y (2011) Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[18] Wright SJ (2010) Optimization Algorithms in Machine Learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[19] Sandler R, Lindenbaum M (2011) Nonnegative Matrix Factorization with Earth Mover\u2019s Distance Metric for Image Analysis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[20] Levina E, Bickel P (2001) The Earth Mover\u2019s distance is the Mallows distance: some insights from statistics.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[21] Wang Z, Lu H, Liu H (2014) Tighten after Relax: Minimax-Optimal Sparse PCA in Polynomial Time.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[22] Bertsekas DP (1998) Network Optimization: Continuous and Discrete Models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[23] Bertsekas DP, Eckstein J (1988) Dual coordinate step methods for linear network flow problems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[24] Bertsekas DP (2011) Incremental gradient, subgradient, and proximal methods for convex optimization: A survey.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[25] Beck A, Teboulle M (2009) A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[26] Guyon I, Gunn S, Nikravesh M, Zadeh LA (2006) Feature Extraction: Foundations and Applications.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[27] Zou H, Hastie T, Tibshirani R (2005) Sparse Principal Component Analysis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[28] Geiler-Samerotte KA, Bauer CR, Li S, Ziv N, Gresham D, et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "The overall Wasserstein distance in the ambient space is computed by solving a transportation problem [20], and we note the similarity between this statistic and the cross-match test [6].", "startOffset": 102, "endOffset": 106}, {"referenceID": 3, "context": "The overall Wasserstein distance in the ambient space is computed by solving a transportation problem [20], and we note the similarity between this statistic and the cross-match test [6].", "startOffset": 183, "endOffset": 186}, {"referenceID": 5, "context": "A popular kernel method for testing high-dimensional distribution equality, the mean map discrepancy, is computed using the Gaussian kernel with bandwidth parameter chosen by the \u201cmedian trick\u201d [8] (this is very similar to the energy test of [7]).", "startOffset": 194, "endOffset": 197}, {"referenceID": 4, "context": "A popular kernel method for testing high-dimensional distribution equality, the mean map discrepancy, is computed using the Gaussian kernel with bandwidth parameter chosen by the \u201cmedian trick\u201d [8] (this is very similar to the energy test of [7]).", "startOffset": 242, "endOffset": 245}, {"referenceID": 2, "context": "Finally, we also compute the DiProPerm statistic, employing the the DWD-t variant recommended for testing general equality of distributions [5].", "startOffset": 140, "endOffset": 143}, {"referenceID": 31, "context": "Following [36], we represent gene expression by log-transformed FPKM computed from the sequencing read counts2, so values are directly comparable between genes.", "startOffset": 10, "endOffset": 14}, {"referenceID": 31, "context": "Because expression measurements from individual cells are poorer in quality than transcriptome profiles obtained in aggregate across tissue samples (due to a drastically reduced amount of available RNA), it is important to filter out poorly measured genes and we retain a set of 10,305 genes that are measured with sufficient accuracy for informative analysis [36].", "startOffset": 360, "endOffset": 364}, {"referenceID": 32, "context": "For comparison, we also run LIMMA, a standard method for differential expression analysis which tests for marginal mean-differences on a gene-by-gene basis [37].", "startOffset": 156, "endOffset": 160}, {"referenceID": 33, "context": "One particularly relevant gene in this data is Snca, a presynaptic signaling and membrane trafficking gene whose defects are implicated in both Parkinson and Alzheimer\u2019s disease [38, 39].", "startOffset": 178, "endOffset": 186}, {"referenceID": 34, "context": "One particularly relevant gene in this data is Snca, a presynaptic signaling and membrane trafficking gene whose defects are implicated in both Parkinson and Alzheimer\u2019s disease [38, 39].", "startOffset": 178, "endOffset": 186}, {"referenceID": 35, "context": "0593 Primary distinguishing gene between distinct interneuron classes identified in the cortex and hippocampus [40] Neurod6 0.", "startOffset": 111, "endOffset": 115}, {"referenceID": 36, "context": "the hippocampal region [41] Stmn3 0.", "startOffset": 23, "endOffset": 27}, {"referenceID": 37, "context": "0573 Up-expressed in hippocampus of patients with depressive disorders [42] Plp1 0.", "startOffset": 71, "endOffset": 75}, {"referenceID": 38, "context": "0570 An oligodendrocyte- and myelin-related gene which exhibits cortical differential expression in schizophrenia [43] Crym 0.", "startOffset": 114, "endOffset": 118}, {"referenceID": 39, "context": "0550 Plays a role in neuronal specification [44] Spink8 0.", "startOffset": 44, "endOffset": 48}, {"referenceID": 40, "context": "0500 Stress induction leads to reduced expression in the mouse hippocampus [45] Mal 0.", "startOffset": 75, "endOffset": 79}, {"referenceID": 41, "context": "0494 Regulates dendritic morphology and is expressed at lower levels in cortex than in hippocampus [46] Tspan13 0.", "startOffset": 99, "endOffset": 103}, {"referenceID": 42, "context": "Knock out in the hippocampus results in drastic expression changes of many other genes [47] Basp1 0.", "startOffset": 87, "endOffset": 91}, {"referenceID": 43, "context": "1171 Transcriptional cofactor which can divert the differentiation of cells to a neuronal-like morphology [48] Snhg1 0.", "startOffset": 106, "endOffset": 110}, {"referenceID": 44, "context": "1145 Promoter of neurodifferentiation and axonal/dendritic maintenance [49] Uchl1 0.", "startOffset": 71, "endOffset": 75}, {"referenceID": 45, "context": "1137 Loss of function leads to profound degeneration of motor neurons [50].", "startOffset": 70, "endOffset": 74}, {"referenceID": 46, "context": "1131 Targets pyramidal neurons and enables neocortical plasticity allowing for example the auditory cortex to detect light stimuli [51, 52] Table S2: Genes with the greatest weight in the projection p \u03b2 produced by SPARDA analysis of the marginally normalized expression data.", "startOffset": 131, "endOffset": 139}, {"referenceID": 47, "context": "1131 Targets pyramidal neurons and enables neocortical plasticity allowing for example the auditory cortex to detect light stimuli [51, 52] Table S2: Genes with the greatest weight in the projection p \u03b2 produced by SPARDA analysis of the marginally normalized expression data.", "startOffset": 131, "endOffset": 139}, {"referenceID": 7, "context": "Our proof relies primarily on a quantitative form of the Cramer-Wold result presented in [11].", "startOffset": 89, "endOffset": 93}, {"referenceID": 7, "context": "1 [11] in its contrapositive form: If D a \u011b 0 such that TapX,Y q \u0105 hpgp\u2206qq, then D\u03b2 P B such that sup zPR \u02c7 \u02c7 \u02c7 \u02c7 Pr ` \u03b2X \u010f z \u0306 \u0301 Pr ` \u03b2Y \u010f z \u0306 \u02c7 \u02c7 \u02c7 \u02c7 \u0105 gpC\u2206q (12) Subsequently we leverage a number of well-characterized relationships between different probability metrics (cf.", "startOffset": 2, "endOffset": 6}, {"referenceID": 1, "context": "[4]) to lower bound the projected (squared) Wasserstein distance (of the underlying random variables).", "startOffset": 0, "endOffset": 3}, {"referenceID": 25, "context": "Additional References for the Supplementary Information [30] Levina E, Bickel P (2001) The Earth Mover\u2019s distance is the Mallows distance: some insights from statistics.", "startOffset": 56, "endOffset": 60}, {"referenceID": 26, "context": "[31] Rosenbaum PR (2005) An exact distribution-free test comparing two multivariate distributions based on adjacency.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[32] Gretton A, Borgwardt KM, Rasch MJ, Scholkopf B, Smola A (2012) A Kernel Two-Sample Test.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[33] Szekely G, Rizzo M (2004) Testing for equal distributions in high dimension.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[34] Wei S, Lee C, Wichers L, Marron JS (2015) Direction-Projection-Permutation for High Dimensional Hypothesis Tests.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[35] Zeisel A, Munoz-Manchado AB, Codeluppi S, Lonnerberg P, La Manno G, et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[36] Trapnell C, Cacchiarelli D, Grimsby J, Pokharel P, Li S, et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[37] Ritchie M, Phipson B, Wu D, Hu Y, Law CW, et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[38] Lesage S, Brice A (2009) Parkinson\u2019s disease: from monogenic forms to genetic susceptibility factors.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[39] Linnertz C, Lutz MW, Ervin JF, Allen J, Miller NR, et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[40] Jasnow AM, Ressler KJ, Hammack SE, Chhatwal JP, Rainnie DG (2009) Distinct subtypes of cholecystokinin (CCK)-containing interneurons of the basolateral amygdala identified using a CCK promoter-specific lentivirus.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[41] Bormuth I, Yan K, Yonemasu T, Gummert M, Zhang M, et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[42] Oh DH, Park YC, Kim SH (2010) Increased glycogen synthase kinase-3beta mRNA level in the hippocampus of patients with major depression: a study using the stanley neuropathology consortium integrative database.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[43] Wu JQ, Wang X, Beveridge NJ, Tooney PA, Scott RJ, et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[44] Molyneaux BJ, Arlotta P, Menezes JRL, Macklis JD (2007) Neuronal subtype specification in the cerebral cortex.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[45] Hagemann TL, Jobe EM, Messing A (2012) Genetic Ablation of Nrf2/Antioxidant Response Pathway in Alexander Disease Mice Reduces Hippocampal Gliosis but Does Not Impact Survival.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "[46] Shiota J, Ishikawa M, Sakagami H, Tsuda M, Baraban JM, et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[47] Zhang L, Ju X, Cheng Y, Guo X, Wen T (2011) Identifying Tmem59 related gene regulatory network of mouse neural stem cell from a compendium of expression profiles.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "[48] Goodfellow S, Rebello M, Toska E, Zeef L, Rudd S, et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "[49] Lederer CW, Torrisi A, Pantelidou M, Santama N, Cavallaro S (2007) Pathways and genes differentially expressed in the motor cortex of patients with sporadic amyotrophic lateral sclerosis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "[50] Jara JH, Gen\u00e7 B, Cox GA, Bohn MC, Roos RP, et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "[51] Li X, Yu K, Zhang Z, Sun W, Yang Z, et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "[52] Gallopin T, Geoffroy H, Rossier J, Lambolez B (2006) Cortical sources of CRF, NKB, and CCK and their effects on pyramidal cells in the neocortex.", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "[55] Jirak M (2011) On the maximum of covariance estimators.", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "[56] Gibbs AL, Su FE (2002) On Choosing and Bounding Probability Metrics.", "startOffset": 0, "endOffset": 4}], "year": 2015, "abstractText": "We introduce principal differences analysis (PDA) for analyzing differences between high-dimensional distributions. The method operates by finding the projection that maximizes the Wasserstein divergence between the resulting univariate populations. Relying on the Cramer-Wold device, it requires no assumptions about the form of the underlying distributions, nor the nature of their inter-class differences. A sparse variant of the method is introduced to identify features responsible for the differences. We provide algorithms for both the original minimax formulation as well as its semidefinite relaxation. In addition to deriving some convergence results, we illustrate how the approach may be applied to identify differences between cell populations in the somatosensory cortex and hippocampus as manifested by single cell RNA-seq. Our broader framework extends beyond the specific choice of Wasserstein divergence.", "creator": "LaTeX with hyperref package"}}}