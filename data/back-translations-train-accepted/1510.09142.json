{"id": "1510.09142", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2015", "title": "Learning Continuous Control Policies by Stochastic Value Gradients", "abstract": "We present a unified framework for learning continuous control policies using backpropagation. It supports stochastic control by treating stochasticity in the Bellman equation as a deterministic function of exogenous noise. The product is a spectrum of general policy gradient algorithms that range from model-free methods with value functions to model-based methods without value functions. We use learned models but only require observations from the environment in- stead of observations from model-predicted trajectories, minimizing the impact of compounded model errors. We apply these algorithms first to a toy stochastic control problem and then to several physics-based control problems in simulation. One of these variants, SVG(1), shows the effectiveness of learning models, value functions, and policies simultaneously in continuous domains.", "histories": [["v1", "Fri, 30 Oct 2015 16:07:51 GMT  (764kb,D)", "http://arxiv.org/abs/1510.09142v1", "13 pages, NIPS 2015"]], "COMMENTS": "13 pages, NIPS 2015", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["nicolas heess", "gregory wayne", "david silver", "timothy p lillicrap", "tom erez", "yuval tassa"], "accepted": true, "id": "1510.09142"}, "pdf": {"name": "1510.09142.pdf", "metadata": {"source": "CRF", "title": "Learning Continuous Control Policies by Stochastic Value Gradients", "authors": ["Nicolas Heess", "Greg Wayne", "David Silver", "Timothy Lillicrap", "Yuval Tassa", "Tom Erez"], "emails": ["etom}@google.com"], "sections": [{"heading": "1 Introduction", "text": "It is only a matter of time before that happens, that it happens."}, {"heading": "2 Background", "text": "We look at discrete temporal Markov decision-making processes (MDPs) with continuous states and actions and designate the state and action at the time step t according to st-RNA or an-RNA. The MDP has an initial state distribution s0-p0 (\u00b7), a transition distribution st + 1-p (\u00b7 | st, at) and a (potentially time-changing) reward function rt = r (st, at).1 We view time-invariant stochastic politics as a p problem, i.e. J-horizontal problems as E problems parameterized by \u03b8. The goal of policy optimization is to find policy parameters that maximize the expected sum of future rewards. We optimize either finite horizon or infinite horizon problems, i.e. J problems) = E problems T = 0-trt-trt, which is relevant to the state."}, {"heading": "3 Deterministic value gradients", "text": "The Bellman deterministic equation takes the form V (s) = r (s, a) + \u03b3V (f (s, a) for a deterministic model s (s, a) and deterministic politics a = \u03c0 (s). The differentiation of the equation in relation to the state and politics gives an expression for the value gradient Vs = rs + ra\u03c0s + \u03b3V \"s\" (fs + fa\u03c0s), (3) V\u03b8 = ra\u03c0\u03b8 + \u03b3V \"s\" fa\u03c0\u03b8 + \u03b3V. \"(4) In Equation 4, the term \u03b3V\" \u03b8 results because the total gradient contains contributions from subsequent time steps (full derivation in Appendix A.) For a purely model-based formalism, these equations are used as a pair of coupled recursions which, starting from the termination of a trajectory, run backwards to calculate the value gradient in relation to the problem."}, {"heading": "4 Stochastic value gradients", "text": "One limitation of the gradient calculation in Eqs 3 and 4 is that the model and policy must be deterministic. In addition, the accuracy of the policy gradient V\u03b8 is highly sensitive to modeling errors. We are introducing two crucial changes: First, in Section 4.1, we are transforming the Bellman stochastic equation (Eq.2) to allow for the backpropagation of value information in a stochastic environment, which also allows us to calculate gradients along real paths, not using a model, making the approach robust to model errors, leading to our first algorithm, \"SVG,\" described in Section 4.2. Second, in Section 4.3, we will show how value function critics can be integrated into this framework, leading to the algorithms \"SVG (1)\" and \"SVG (0),\" which extend the Bellman recursion by 1 and 0 steps respectively."}, {"heading": "4.1 Differentiating the stochastic Bellman equation", "text": "To do this, we use a concept called \"re-parameterization,\" which allows us to calculate derivatives of deterministic and stochastic models in the same way. A very simple example of re-parameterization is to write a conditional Gaussian density that allows us to calculate derivatives of deterministic and stochastic models in the same way (x). A very simple example of re-parameterization is that one produces a conditional Gaussian density p (y) = N (y), 2 (x)) as a function (x) + 3 (x) + 3 (x), where there is a conditional density of n (0, 1). From this point of view, one produces samples procedurally by first sampling, then deterministically constructs y. Here, we look at conditional densities whose samples are produced by a deterministic function of an input noise variable and other conditional variables: y (x) where a distribution is fixed."}, {"heading": "4.2 SVG(\u221e)", "text": "SVG (s, a, s) calculates value gradients by backward recursions to finite horizon trajectories. After each episode we train the model, f, followed by the policy, \u03c0. For this purpose, in Algorithm 1 we provide a pseudocode, but discuss further implementation details in Section 5 and in the experiments. Algorithm 1 SVG (s, a, r, s) 1: Given empty experience database D 2: Gived empty experience database D 2: Gived empty experience database D 2: Gived goals for 7: Gived goals 3: for t = 0 to T 4: Control a = finite horizon) 9: v \u00b2 s = 0 (finite horizons) 10: Gived goals 10: Given goals (s, a, r, s) 5: Gived goals for 7: Gived goals 3: for t = 0 to T 4: Control a = finite horizon) 9: v \u00b2 s = 0 (finite horizons) 10: Gived goals 10: Gived goals 10: Gived goals (s, r, s, c, c, c, c, c, c, c, c, c, c, c, c, c, c, goals 10, c, c, c, c, goals, c, c, c, c, horizontal, c, c, c, c, c, c, c, c, c, c, c, c, goals 10, c, c, c, c, c, c, goals, c, c, c, c, c, c, goals (horizontal, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, goals 10, c, c, c, c, c, c, c, c, goals, c, c, c, c, c, c, c, c, c, goals, c, c, c, (horizontal, c, c)."}, {"heading": "4.3 SVG(1) and SVG(0)", "text": "Within our framework, we can learn a parametric estimate of the expected value V (s; \u03bd) (critic) with parameters \u03bd. Deriving the critical value with respect to the state, V-s, can also be used instead of the sample gradient estimate given in eq. (9) The critic can reduce the variance of the gradient estimates because V-S approximates the expectation of future rewards, while eq. (9) only one a3In the finite horizon formulation, the gradient calculation begins at the end of the orbit, for which the only terms remaining in eq are the vTs with respect to rTs + rTa \u03c0Ts. In contrast to the overall derivation of the value function with respect to the political parameters, the v0 sample estimate is given, which is a sound estimate for the terms remaining in eq."}, {"heading": "5 Model and value learning", "text": "In our work, we have parameterized the models as neural networks. Our framework supports non-linear state and action-dependent noise, remarkable properties of biological actuators. This can be described, for example, by the parametric form f (s, a, B) = \u00b5 (s, a) + 3 (s, a). Model learning amounts to a purely supervised problem based on observed state transitions. Our model and policy training are done jointly. There is no \"motor\" period used to identify the model. As new transitions are observed, the model is trained first, followed by the value function (for SVG (1), followed by politics. To ensure that the model does not forget information about state transitions, we maintain an experience database and Kull stacks of examples from the database for each model update. Additionally, we model the status change through s (s, a, empris) and determine that we have differentiation models available."}, {"heading": "6 Experiments", "text": "We tested the SVG algorithms in two test series. In the first test series (Section 6.1), we tested whether the evaluation of gradients on real environmental gradients and value function ap-4Note that \u03c0 is a function of the state and sound variable. In our second test series (Section 6.2), we show that SVG (1) can be applied to several complicated, multi-dimensional physical environments in which contact dynamics (Figure 1) play a role in the MuJoCo simulator [28]. Below, we briefly summarize the most important properties of each environment: Further details of the simulations can be found in Appendix D. In all cases, we use generic, 2 hidden neural networks with tanh activation functions to represent models, value functions and guidelines. A video montage is available at https: / / youtu.be / PYdL7bcn _ cM."}, {"heading": "6.1 Analyzing SVG", "text": "To demonstrate the difficulty of planning with a stochastic model, we first present a very simple control problem, for which SVG (\u221e) easily learns a control policy, but for which an otherwise identical planner also fails completely. Our example is based on a problem due to [16]. The policy directly controls the speed of a point-mass \"hand\" on a 2D level. By means of a spring coupling, the hand exerts a force on a ball mass; the ball additionally experiences a gravitational force and random forces (Gaussian noise). The goal is to bring hand and ball into one of two randomly selected target configurations with a relevant reward, which is provided only in the final phase.With simulation time step 0.01s, this requires control and backpropagation of the distal reward along a path of 1,000 steps. As this experiment has a non-stationary, time-dependent value function, this problem favors the gradients over methods that use this function."}, {"heading": "6.2 SVG in complex environments", "text": "In a second set of experiments, we showed that SVG (1) -ER can be applied to several challenging physical control problems with stochastic, nonlinear and discontinuous dynamics due to contacts. Reacher is an arm that is stationed within a walled box with 6 state dimensions and 3 action dimensions and the (x, y) coordinates of a target location, giving a total of 8 state dimensions. In 4-Target Reacher, the side was placed randomly at one of the four corners of the field, and the arm in a random configuration at the beginning of each experiment. In Moving-Target Reacher, the side moved at a randomized speed and went into the box with reflections on the walls. The solution to this latter problem implies the policy has generalized over the entire work space. Gripper helps the Reacher arm with a manipulator that can grab a ball in a randomized position and return it to a specific location."}, {"heading": "7 Related work", "text": "Writing noise variables as exogenous inputs into the system to enable direct differentiation in relation to the system state (Eq.7) is a well-known tool in control theory [10, 7], where the model is given analytically; the idea of using a model to optimize a parametric policy around real trajectories is heuristically presented in [17] and [1] for deterministic strategies and models; even at the boundary of deterministic strategies and models, the recursions derived in Algorithm 1 are reduced to those of [2]. Werbos defines an action-critical algorithm called Heuristic Dynamic Programming, which uses a deterministic model to roll one step forward to create a state forecast that is evaluated by a value function [31]. Deisenroth et al. have used Gaussian process models to calculate political gradients that are sensitive to model uncertainty, [6] which have parametric and non-paraligned models."}, {"heading": "8 Discussion", "text": "We have shown that two potential problems with value gradient methods, their dependence on planning and confinement to deterministic models, can be exorcised and extend their relevance to reinforcement learning. We have experimentally demonstrated that the SVG framework can train neural network strategies in a robust manner to solve interesting continuous control problems, and the framework includes algorithm variants that go beyond those tested in this paper, such as those that combine a value function with k-steps of redistribution through a model (SVG (k). Extending SVG (1) with experiential replay yielded the best results, and a similar extension could be applied to any SVG (k). Furthermore, we have not used sophisticated generative models of stochastic dynamics, but it could easily be done by providing ample room for growth.We thank Arthur Guez, Danilo Rezende, Hvan Riedt, John Riedler, Mohamed Hunt for his discussions."}, {"heading": "A Derivation of recursive gradient of the deterministic value function", "text": "The use of derivatives in Eq.4 is subtle, so here we expand the logic. First, we point out that a change in the political parameters affects immediate action as well as any future state and any future action. Thus, the total derivative d dhabi can be increased by + + + 0 dst d\u03b8 by + 0 dst d\u03b8 st = (da0d\u03b8 by 0 + ds1 dhabi by s1) + [\u2211 t more than 1 dms by + 0 dms by 0 dms. The operator obeys the recursive formula \"thabi by + 0 dst dms.\" Let us now define the operator \"t\u03b8,\" [dms by 0 + dms by 0]. The operator obeys the recursive formula \"t\u03b8\" by + 1 dms by 1 (dst + 1 dms + 1 dms). (dst + 1 dms + 1 dms)."}, {"heading": "B Gradient calculation for noise models", "text": "The evaluation of the jakobin terms in Equations 9 and 10 may require knowledge of the noise variables \u03b7 and \u0438. However, equations 11 and 12 arise from an application of the Bayes rule. Formally, we reverse the process of pre-sampling, where samples are generated from the joint distribution p (a, s) and p (s). Instead of these, we first sample p (a, c), c), c), c), c), s (c), s (c), c), c), c (c), c), c), c), c), c), c (c), c), c, c), c, c), c, c, c, c, c), c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c), c, c, c, c), c, c), c, c), c, c), c, c), c, c), c, c), c, c), c, c), c, c), c, c), c, c), c), c, c), c, c), c, c), c), c, c), c, c), c), c, c), c), c), c, c), c, c), c), c, c), c), c, c), c, c), c), c, c), c, c), c), c, c), c, c), c), c), c), c, c, c), c), c), c, c), c), c), c, c), c), c), c), c, c), c), c), c), c, c), (c), c), (c), c), c), (c), c), and (c), c), c), c), c), and (c), c), and), c), (c),"}, {"heading": "C Model and value learning", "text": "We found that the models had the lowest prediction error per step when the components of p and p were calculated by parallel subnets (p, p, p, p, p, p, p); (p, p, p, p); (p, p, p, p, p). (p, p, p, p). (This was due to the different scaling of the dynamic range of state dimensions.) In the experiments in this essay, the application components were parameterized as constant distortions per application dimension.) Application 3 SVG (0) with Replay 1: Given experience database D 2: for t = 0 to 3: Given experience database D 2: for t, p, p, p, p, p, p: Apply, p: Apply, p: Apply, Apply:, Apply: p:: Apply, Apply: p::, Apply: p:, Apply:::"}, {"heading": "D Experimental details", "text": "For a direct comparison, we performed SVG (0), SVG (1), SVG (3), SVG (3) and DPG without experience replay for the policy updates because replay for SVG (3) is not possible. (All algorithms use replay for the value function updates.) We implemented SVG (1) -ER such that the policy updates are performed at the end of each episode, after updating the value function, rather than after each step of interacting with the environment. (For K-steps of replay for policy, we perform K-gradient steps according to lines 10-14 in algorithm 2, pulling new data from the databaseD at each step. In some cases, we found it helpful to apply an additional regulator that penalizes It. [DKL \u2212 1] During each step of the replay in which we perform the policy at the beginning of the update and the policy after k \u2212 1 of the replay."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "We present a unified framework for learning continuous control policies using<lb>backpropagation. It supports stochastic control by treating stochasticity in the<lb>Bellman equation as a deterministic function of exogenous noise. The product<lb>is a spectrum of general policy gradient algorithms that range from model-free<lb>methods with value functions to model-based methods without value functions.<lb>We use learned models but only require observations from the environment in-<lb>stead of observations from model-predicted trajectories, minimizing the impact<lb>of compounded model errors. We apply these algorithms first to a toy stochastic<lb>control problem and then to several physics-based control problems in simulation.<lb>One of these variants, SVG(1), shows the effectiveness of learning models, value<lb>functions, and policies simultaneously in continuous domains.", "creator": "LaTeX with hyperref package"}}}