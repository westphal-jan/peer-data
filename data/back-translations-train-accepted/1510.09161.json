{"id": "1510.09161", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2015", "title": "Streaming, Distributed Variational Inference for Bayesian Nonparametrics", "abstract": "This paper presents a methodology for creating streaming, distributed inference algorithms for Bayesian nonparametric (BNP) models. In the proposed framework, processing nodes receive a sequence of data minibatches, compute a variational posterior for each, and make asynchronous streaming updates to a central model. In contrast to previous algorithms, the proposed framework is truly streaming, distributed, asynchronous, learning-rate-free, and truncation-free. The key challenge in developing the framework, arising from the fact that BNP models do not impose an inherent ordering on their components, is finding the correspondence between minibatch and central BNP posterior components before performing each update. To address this, the paper develops a combinatorial optimization problem over component correspondences, and provides an efficient solution technique. The paper concludes with an application of the methodology to the DP mixture model, with experimental results demonstrating its practical scalability and performance.", "histories": [["v1", "Fri, 30 Oct 2015 17:04:33 GMT  (751kb,D)", "http://arxiv.org/abs/1510.09161v1", "This paper was presented at NIPS 2015. Please use the following BibTeX citation: @inproceedings{Campbell15_NIPS, Author = {Trevor Campbell and Julian Straub and John W. {Fisher III} and Jonathan P. How}, Title = {Streaming, Distributed Variational Inference for Bayesian Nonparametrics}, Booktitle = {Advances in Neural Information Processing Systems (NIPS)}, Year = {2015}}"]], "COMMENTS": "This paper was presented at NIPS 2015. Please use the following BibTeX citation: @inproceedings{Campbell15_NIPS, Author = {Trevor Campbell and Julian Straub and John W. {Fisher III} and Jonathan P. How}, Title = {Streaming, Distributed Variational Inference for Bayesian Nonparametrics}, Booktitle = {Advances in Neural Information Processing Systems (NIPS)}, Year = {2015}}", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["trevor campbell", "julian straub", "john w fisher iii", "jonathan p how"], "accepted": true, "id": "1510.09161"}, "pdf": {"name": "1510.09161.pdf", "metadata": {"source": "CRF", "title": "Streaming, Distributed Variational Inference for Bayesian Nonparametrics", "authors": ["Trevor Campbell", "Julian Straub", "John W. Fisher"], "emails": ["jstraub@csail.", "fisher@csail.", "jhow@}mit.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "2 Streaming, distributed Bayesian nonparametric inference", "text": "The proposed framework, motivated by a posterior decomposition discussed in Section 2.1, includes a collection of processing nodes with asynchronous access to a central variation posterior approximation (represented for a single node in Figure 1), which is made available to each processing node as a sequence of minibatches. If a processing node receives a minibatch of data, it receives the central posterior variation (Figure 1a) and calculates this as a previous minibatch variation posterior approximation (Figure 1b). Finally, when the minibatch NP inference is complete, the node performs a component identification between the posterior minibatch and the current central posterior, taking into account possible changes by other processing nodes (Figure 1c). Finally, it inserts the posterior minibatch model into the posterior central variation model (Figure 1d). In the following sections, we will use the modelling framework [DP-3] as the technical guide for many of the DP-1NP."}, {"heading": "2.1 Posterior decomposition", "text": "Consider a DP mixing model [3], with cluster parameters, mappings z, and observed data y. For each asynchronous update performed by each processing node, the dataset is divided into three subsets y = yo-yi for analysis. When the processing node receives a minibatch of data, it will query the central processing node for the intermediate posterior p. (3) The central processing node for the intermediate posterior p (3) -yi-yo-yo component for the intermediate posterior p (4), which began asynchronous updates from other processing nodes since minibatch inference. (4) Each subset yr, r, i, m} has Nr observations {yrj} Nrj = 1, and each variable zrj-N assign yrj to cluster paramezrj."}, {"heading": "2.2 Variational component identification", "text": "Suppose we have the following field exponential family before and approximate variation density in the minibatch decomposition (1), p (2), p (3), p (3), p (3), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5), p (5, p (5), p (5), p (5), p (5), p (5, p (5), p (5), p (5, p (5), p (5), p (5, p (5), p (5), p (5, p (5), p (5), p (5, p (5), p (5), p (5, p (5), p (5), p (5, p (5), p (5), p (5, p (5), p (5, p (5), p (5), p (5, p (5), p (5), p (5, p (5), p (5), p (5, p (5), p (5, p (5), p (5, p (5), p (5), p (5, p (5), p (5, p (5), p (5), p (5), p (5), p"}, {"heading": "2.3 Updating the central posterior", "text": "In order to update the central posterior node, the node first locks and unlocks the central posterior node via (5). The locking prevents other nodes from unlocking (5) or modifying the central posterior node, but does not prevent other nodes from reading the central posterior node, obtaining minibatches or performing conclusions; the synthetic experiment in Section 4 shows that in practice this does not result in significant time penalties. Then, the processing node transfers the problem? and its minibatch variation behind? (k) to the central processing node where the product replacement (1) is used to find the updated central variation behind q in (3), with the parameters K = max {Ki, maxk, [Km]."}, {"heading": "3 Application to the Dirichlet process mixture model", "text": "The expectation in the target of (5) is typically not calculated in a closed form; therefore, a suitable lower limit can be used in its place. This section represents such a limit for the dirichlet process and discusses the application of the proposed inference framework to the dirichlet process mixing model using the developed limit. Crucially, the lower limit decomposes in such a way that the optimization (5) becomes a bipartite matching problem with maximum weight. Such problems can be solved in polynomial time [22] by the Hungarian algorithm, resulting in a traceable step in the identification of components within the proposed streaming framework."}, {"heading": "3.1 Regularization lower bound", "text": "For the dirichlet process with the concentration parameters \u03b1 > 0, p (zi, zm, zo), the executable partition is Probability Function (EPPF) [23] p (zi, zm, zo), p (zm, zo), p (nk), p (nk), p (nk), p (nk), p (nk), p (nk), p (nk), p (nk), p (nk), p (nk), p (nk), p (nk), p (nk), p (nk), p (np), p (nk), p (np), p (nk), p (np), p (nk), k (k), k (k), k (k), nk (k), nk (nk), p (nk), nk (nk), p (nk), nk (nk), p (nk), p (p, nk), p (nk), p (nk), p (nk), nk (k), k (k), k (k), nk (k), p (nk), nk (nk, nk), p (nk, nk (nk), nk (nk), p (nk), p (nk (nk), p (nk), p (nk (nk, nk, nk), p (nk (nk), nk (nk), p (nk (nk, nk), p (nk (nk), nk (nk), p (nk (nk, nk), nk (nk), p (nk (nk, nk), p (nk (nk), p (nk), p (nk (nk, nk (nk), nk (nk), p (nk (nk, nk), nk (nk), p (nk (nk, nk), nk (nk"}, {"heading": "3.2 Solving the component identification optimization", "text": "Considering that both the regularization (9) and component concordance in target (5) are resolved as the sum of terms for each k [Ki + K \u00b2 m], the target can be rewritten using a matrix of matching results R + R (Ki + K \u00b2 m) \u00b7 (Ki + K \u00b2 m) and the selector variables X \u00b2 (Ki + K \u00b2 m) \u00b7 (Ki + K \u00b2 m). Specifying Xkj = 1 indicates that component k in the posterior minibatch is concordant with component j in the posterior intermediate (i.e. Ki + K \u00b2 m), with a score Rkj defined by (6) and (10) asRkj = A (\u03b7 ij + mk \u2212 mk) components in component j) + (1 \u2212 it is the optimization of component j + s sp."}, {"heading": "4 Experiments", "text": "In this section, the proposed inference frame on the DP Gaussian mixture was evaluated with a normal-inverse wishart (NIW), while batch tests were performed with 12,000 collected data. We compare the stream-based, distributed procedures coupled with standard variable inferences [24] (SDA-DP) to five state-of-the-art inference algorithms: memoized online variable inference (moVB) [13], stochastic online variant inferencing (SVA) [7] with cluster creation threshold 10 \u2212 1 and prune / merge threshold 10 \u2212 3, subcluster splits MCMC (SC) [14] and batch variant inferences (Batch) [24]. Priors were set manually and all methods were initialized. Methods that allowed multiple passes to do through the data (e.g. moVB, SVI)."}, {"heading": "5 Conclusions", "text": "This paper presented a streamed, distributed, asynchronous inference algorithm for non-parametric Bayesian models, focusing on the combinatorial problem of adapting minibatch components to central rear components during asynchronous updates. Key contributions include optimization of component identification based on minibatch decomposition of rear components, traceable limitation to the target for the Dirichlet process mix, and experiments demonstrating the efficiency of the methodology on large-scale datasets. While this paper focused on the DP mixture as a key example, it is not limited to this model - exploring the application of the proposed methodology to other BNP models is a potential area of research for the future."}, {"heading": "Acknowledgments", "text": "This work was supported by the Office of Naval Research under the ONR MURI grant N000141110688."}], "references": [{"title": "Bayesian Analysis of Finite Mixture Distributions", "author": ["Agostino Nobile"], "venue": "PhD thesis,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1994}, {"title": "A simple example of Dirichlet process mixture inconsistency for the number of components", "author": ["Jeffrey W. Miller", "Matthew T. Harrison"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Dirichlet processes. In Encyclopedia of Machine Learning", "author": ["Yee Whye Teh"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Infinite latent feature models and the Indian buffet process", "author": ["Thomas L. Griffiths", "Zoubin Ghahramani"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Streaming variational Bayes", "author": ["Tamara Broderick", "Nicholas Boyd", "Andre Wibisono", "Ashia C. Wilson", "Michael I. Jordan"], "venue": "In Advances in Neural Information Procesing Systems", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Approximate decentralized Bayesian inference", "author": ["Trevor Campbell", "Jonathan P. How"], "venue": "In Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Online learning of nonparametric mixture models via sequential variational approximation", "author": ["Dahua Lin"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "A sequential algorithm for fast fitting of Dirichlet process mixture models", "author": ["Xiaole Zhang", "David J. Nott", "Christopher Yau", "Ajay Jasra"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Stochastic variational inference", "author": ["Matt Hoffman", "David Blei", "Chong Wang", "John Paisley"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Online variational inference for the hierarchical Dirichlet process", "author": ["Chong Wang", "John Paisley", "David M. Blei"], "venue": "In Proceedings of the 11th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Truly nonparametric online variational inference for hierarchical Dirichlet processes", "author": ["Michael Bryant", "Erik Sudderth"], "venue": "In Advances in Neural Information Proecssing Systems", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Truncation-free stochastic variational inference for Bayesian nonparametric models", "author": ["Chong Wang", "David Blei"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Memoized online variational inference for Dirichlet process mixture models", "author": ["Michael Hughes", "Erik Sudderth"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Parallel sampling of DP mixture models using sub-clusters splits", "author": ["Jason Chang", "John Fisher III"], "venue": "In Advances in Neural Information Procesing Systems", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Asymptotically exact, embarassingly parallel MCMC", "author": ["Willie Neiswanger", "Chong Wang", "Eric P. Xing"], "venue": "In Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Particle learning for general mixtures", "author": ["Carlos M. Carvalho", "Hedibert F. Lopes", "Nicholas G. Polson", "Matt A. Taddy"], "venue": "Bayesian Analysis,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Dealing with label switching in mixture models", "author": ["Matthew Stephens"], "venue": "Journal of the Royal Statistical Society: Series B,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}, {"title": "Markov chain Monte Carlo methods and the label switching problem in Bayesian mixture modeling", "author": ["Ajay Jasra", "Chris Holmes", "David Stephens"], "venue": "Statistical Science,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Hierarchical Dirichlet processes", "author": ["Yee Whye Teh", "Michael I. Jordan", "Matthew J. Beal", "David M. Blei"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Accelerated sampling for the Indian buffet process", "author": ["Finale Doshi-Velez", "Zoubin Ghahramani"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Parallel Markov chain Monte Carlo for Pitman-Yor mixture models", "author": ["Avinava Dubey", "Sinead Williamson", "Eric Xing"], "venue": "In Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Theoretical improvements in algorithmic efficiency for network flow problems", "author": ["Jack Edmonds", "Richard Karp"], "venue": "Journal of the Association for Computing Machinery,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1972}, {"title": "Exchangeable and partially exchangeable random partitions", "author": ["Jim Pitman"], "venue": "Probability Theory and Related Fields,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1995}, {"title": "Variational inference for Dirichlet process mixtures", "author": ["David M. Blei", "Michael I. Jordan"], "venue": "Bayesian Analysis,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "While their fixed, parametric cousins can be used to infer model complexity for datasets with known magnitude a priori [1, 2], such priors are silent with respect to notions of model complexity growth in streaming data settings.", "startOffset": 119, "endOffset": 125}, {"referenceID": 1, "context": "While their fixed, parametric cousins can be used to infer model complexity for datasets with known magnitude a priori [1, 2], such priors are silent with respect to notions of model complexity growth in streaming data settings.", "startOffset": 119, "endOffset": 125}, {"referenceID": 2, "context": "For example, labels from the Chinese Restaurant process [3] are rendered i.", "startOffset": 56, "endOffset": 59}, {"referenceID": 3, "context": "by conditioning on the underlying Dirichlet process (DP) random measure, and feature assignments from the Indian Buffet process [4] are rendered i.", "startOffset": 128, "endOffset": 131}, {"referenceID": 4, "context": "However, previous work has only addressed these two settings in concert for parametric models [5, 6], and only recently has each been addressed individually for BNPs.", "startOffset": 94, "endOffset": 100}, {"referenceID": 5, "context": "However, previous work has only addressed these two settings in concert for parametric models [5, 6], and only recently has each been addressed individually for BNPs.", "startOffset": 94, "endOffset": 100}, {"referenceID": 6, "context": "In the streaming setting, [7] and [8] developed streaming inference for DP mixture models using sequential variational approximation.", "startOffset": 26, "endOffset": 29}, {"referenceID": 7, "context": "In the streaming setting, [7] and [8] developed streaming inference for DP mixture models using sequential variational approximation.", "startOffset": 34, "endOffset": 37}, {"referenceID": 8, "context": "Stochastic variational inference [9] and related methods [10\u201313] are often considered streaming algorithms, but their performance depends on the choice of a learning rate and on the dataset having known, fixed size a priori [5].", "startOffset": 33, "endOffset": 36}, {"referenceID": 9, "context": "Stochastic variational inference [9] and related methods [10\u201313] are often considered streaming algorithms, but their performance depends on the choice of a learning rate and on the dataset having known, fixed size a priori [5].", "startOffset": 57, "endOffset": 64}, {"referenceID": 10, "context": "Stochastic variational inference [9] and related methods [10\u201313] are often considered streaming algorithms, but their performance depends on the choice of a learning rate and on the dataset having known, fixed size a priori [5].", "startOffset": 57, "endOffset": 64}, {"referenceID": 11, "context": "Stochastic variational inference [9] and related methods [10\u201313] are often considered streaming algorithms, but their performance depends on the choice of a learning rate and on the dataset having known, fixed size a priori [5].", "startOffset": 57, "endOffset": 64}, {"referenceID": 12, "context": "Stochastic variational inference [9] and related methods [10\u201313] are often considered streaming algorithms, but their performance depends on the choice of a learning rate and on the dataset having known, fixed size a priori [5].", "startOffset": 57, "endOffset": 64}, {"referenceID": 4, "context": "Stochastic variational inference [9] and related methods [10\u201313] are often considered streaming algorithms, but their performance depends on the choice of a learning rate and on the dataset having known, fixed size a priori [5].", "startOffset": 224, "endOffset": 227}, {"referenceID": 13, "context": "Outside of variational approaches, which are the focus of the present paper, there exist exact parallelized MCMC methods for BNPs [14, 15]; the tradeoff in using such methods is that they provide samples from the posterior rather than the distribution itself, and results regarding assessing", "startOffset": 130, "endOffset": 138}, {"referenceID": 14, "context": "Outside of variational approaches, which are the focus of the present paper, there exist exact parallelized MCMC methods for BNPs [14, 15]; the tradeoff in using such methods is that they provide samples from the posterior rather than the distribution itself, and results regarding assessing", "startOffset": 130, "endOffset": 138}, {"referenceID": 15, "context": "Sequential particle filters for inference have also been developed [16], but these suffer issues with particle degeneracy and exponential forgetting.", "startOffset": 67, "endOffset": 71}, {"referenceID": 16, "context": "This issue has been studied before in the MCMC literature, where it is known as the \u201clabel switching problem\u201d, but past solution techniques are generally model-specific and restricted to use on very simple mixture models [17, 18].", "startOffset": 221, "endOffset": 229}, {"referenceID": 17, "context": "This issue has been studied before in the MCMC literature, where it is known as the \u201clabel switching problem\u201d, but past solution techniques are generally model-specific and restricted to use on very simple mixture models [17, 18].", "startOffset": 221, "endOffset": 229}, {"referenceID": 2, "context": "In the following sections, we use the DP mixture [3] as a guiding example for the technical development of the inference framework.", "startOffset": 49, "endOffset": 52}, {"referenceID": 18, "context": "However, it is emphasized that the material in this paper generalizes to many other BNP models, such as the hierarchical DP (HDP) topic model [19], BP latent feature model [20], and Pitman-Yor (PY) mixture [21] (see the supplement for further details).", "startOffset": 142, "endOffset": 146}, {"referenceID": 19, "context": "However, it is emphasized that the material in this paper generalizes to many other BNP models, such as the hierarchical DP (HDP) topic model [19], BP latent feature model [20], and Pitman-Yor (PY) mixture [21] (see the supplement for further details).", "startOffset": 172, "endOffset": 176}, {"referenceID": 20, "context": "However, it is emphasized that the material in this paper generalizes to many other BNP models, such as the hierarchical DP (HDP) topic model [19], BP latent feature model [20], and Pitman-Yor (PY) mixture [21] (see the supplement for further details).", "startOffset": 206, "endOffset": 210}, {"referenceID": 2, "context": "Consider a DP mixture model [3], with cluster parameters \u03b8, assignments z, and observed data y.", "startOffset": 28, "endOffset": 31}, {"referenceID": 5, "context": "Typical mean-field variational techniques introduce an artificial ordering of the parameters in the posterior, thereby breaking symmetry that is crucial to combining posteriors correctly using density multiplication [6].", "startOffset": 216, "endOffset": 219}, {"referenceID": 4, "context": "Unknown model size: While previous posterior merging procedures required a 1-to-1 matching between the components of the minibatch posterior and central posterior [5, 6], Bayesian nonparametric posteriors break this assumption.", "startOffset": 163, "endOffset": 169}, {"referenceID": 5, "context": "Unknown model size: While previous posterior merging procedures required a 1-to-1 matching between the components of the minibatch posterior and central posterior [5, 6], Bayesian nonparametric posteriors break this assumption.", "startOffset": 163, "endOffset": 169}, {"referenceID": 9, "context": "For example, the optimization applies to the hierarchical Dirichlet process topic model [10] with topic word distributions \u03b8 and local-toglobal topic correspondences z, and to the beta process latent feature model [4] with features \u03b8 and This is equivalent to the KL-divergence regularization \u2212KL [ \u03b6o(zo)\u03b6i(zi)\u03b6 \u03c3 m(zm) \u2223\u2223\u2223\u2223\u2223\u2223 p(zi, zm, zo)].", "startOffset": 88, "endOffset": 92}, {"referenceID": 3, "context": "For example, the optimization applies to the hierarchical Dirichlet process topic model [10] with topic word distributions \u03b8 and local-toglobal topic correspondences z, and to the beta process latent feature model [4] with features \u03b8 and This is equivalent to the KL-divergence regularization \u2212KL [ \u03b6o(zo)\u03b6i(zi)\u03b6 \u03c3 m(zm) \u2223\u2223\u2223\u2223\u2223\u2223 p(zi, zm, zo)].", "startOffset": 214, "endOffset": 217}, {"referenceID": 21, "context": "Such problems are solvable in polynomial time [22] by the Hungarian algorithm, leading to a tractable component identification step in the proposed streaming, distributed framework.", "startOffset": 46, "endOffset": 50}, {"referenceID": 22, "context": "1 Regularization lower bound For the Dirichlet process with concentration parameter \u03b1 > 0, p(zi, zm, zo) is the Exchangeable Partition Probability Function (EPPF) [23] p(zi, zm, zo) \u221d \u03b1|K|\u22121 \u220f", "startOffset": 163, "endOffset": 167}, {"referenceID": 23, "context": "We compare the streaming, distributed procedure coupled with standard variational inference [24] (SDA-DP) to five state-of-the-art inference algorithms: memoized online variational inference (moVB) [13], stochastic online variational inference (SVI) [9] with learning rate (t+10)\u2212 1 2 , sequential variational approximation (SVA) [7] with cluster creation threshold 10\u22121 and prune/merge threshold 10\u22123, subcluster splits MCMC (SC) [14], and batch variational inference (Batch) [24].", "startOffset": 92, "endOffset": 96}, {"referenceID": 12, "context": "We compare the streaming, distributed procedure coupled with standard variational inference [24] (SDA-DP) to five state-of-the-art inference algorithms: memoized online variational inference (moVB) [13], stochastic online variational inference (SVI) [9] with learning rate (t+10)\u2212 1 2 , sequential variational approximation (SVA) [7] with cluster creation threshold 10\u22121 and prune/merge threshold 10\u22123, subcluster splits MCMC (SC) [14], and batch variational inference (Batch) [24].", "startOffset": 198, "endOffset": 202}, {"referenceID": 8, "context": "We compare the streaming, distributed procedure coupled with standard variational inference [24] (SDA-DP) to five state-of-the-art inference algorithms: memoized online variational inference (moVB) [13], stochastic online variational inference (SVI) [9] with learning rate (t+10)\u2212 1 2 , sequential variational approximation (SVA) [7] with cluster creation threshold 10\u22121 and prune/merge threshold 10\u22123, subcluster splits MCMC (SC) [14], and batch variational inference (Batch) [24].", "startOffset": 250, "endOffset": 253}, {"referenceID": 6, "context": "We compare the streaming, distributed procedure coupled with standard variational inference [24] (SDA-DP) to five state-of-the-art inference algorithms: memoized online variational inference (moVB) [13], stochastic online variational inference (SVI) [9] with learning rate (t+10)\u2212 1 2 , sequential variational approximation (SVA) [7] with cluster creation threshold 10\u22121 and prune/merge threshold 10\u22123, subcluster splits MCMC (SC) [14], and batch variational inference (Batch) [24].", "startOffset": 330, "endOffset": 333}, {"referenceID": 13, "context": "We compare the streaming, distributed procedure coupled with standard variational inference [24] (SDA-DP) to five state-of-the-art inference algorithms: memoized online variational inference (moVB) [13], stochastic online variational inference (SVI) [9] with learning rate (t+10)\u2212 1 2 , sequential variational approximation (SVA) [7] with cluster creation threshold 10\u22121 and prune/merge threshold 10\u22123, subcluster splits MCMC (SC) [14], and batch variational inference (Batch) [24].", "startOffset": 431, "endOffset": 435}, {"referenceID": 23, "context": "We compare the streaming, distributed procedure coupled with standard variational inference [24] (SDA-DP) to five state-of-the-art inference algorithms: memoized online variational inference (moVB) [13], stochastic online variational inference (SVI) [9] with learning rate (t+10)\u2212 1 2 , sequential variational approximation (SVA) [7] with cluster creation threshold 10\u22121 and prune/merge threshold 10\u22123, subcluster splits MCMC (SC) [14], and batch variational inference (Batch) [24].", "startOffset": 477, "endOffset": 481}], "year": 2015, "abstractText": "This paper presents a methodology for creating streaming, distributed inference algorithms for Bayesian nonparametric (BNP) models. In the proposed framework, processing nodes receive a sequence of data minibatches, compute a variational posterior for each, and make asynchronous streaming updates to a central model. In contrast to previous algorithms, the proposed framework is truly streaming, distributed, asynchronous, learning-rate-free, and truncation-free. The key challenge in developing the framework, arising from the fact that BNP models do not impose an inherent ordering on their components, is finding the correspondence between minibatch and central BNP posterior components before performing each update. To address this, the paper develops a combinatorial optimization problem over component correspondences, and provides an efficient solution technique. The paper concludes with an application of the methodology to the DP mixture model, with experimental results demonstrating its practical scalability and performance.", "creator": "LaTeX with hyperref package"}}}