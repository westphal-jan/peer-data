{"id": "1511.00041", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2015", "title": "Learning Causal Graphs with Small Interventions", "abstract": "We consider the problem of learning causal networks with interventions, when each intervention is limited in size under Pearl's Structural Equation Model with independent errors (SEM-IE). The objective is to minimize the number of experiments to discover the causal directions of all the edges in a causal graph. Previous work has focused on the use of separating systems for complete graphs for this task. We prove that any deterministic adaptive algorithm needs to be a separating system in order to learn complete graphs in the worst case. In addition, we present a novel separating system construction, whose size is close to optimal and is arguably simpler than previous work in combinatorics. We also develop a novel information theoretic lower bound on the number of interventions that applies in full generality, including for randomized adaptive learning algorithms.", "histories": [["v1", "Fri, 30 Oct 2015 22:24:13 GMT  (61kb)", "http://arxiv.org/abs/1511.00041v1", "Accepted to NIPS 2015"]], "COMMENTS": "Accepted to NIPS 2015", "reviews": [], "SUBJECTS": "cs.AI cs.IT cs.LG math.IT stat.ML", "authors": ["karthikeyan shanmugam", "murat kocaoglu", "alexandros g dimakis", "sriram vishwanath"], "accepted": true, "id": "1511.00041"}, "pdf": {"name": "1511.00041.pdf", "metadata": {"source": "CRF", "title": "Learning Causal Graphs with Small Interventions", "authors": ["Karthikeyan Shanmugam", "Murat Kocaoglu", "Alexandros G. Dimakis", "Sriram Vishwanath"], "emails": ["karthiksh@utexas.edu,", "mkocaoglu@utexas.edu"], "sections": [{"heading": null, "text": "ar Xiv: 151 1.00 041v 1 [cs.A I] 3 0O ctFor general chord graphs, at worst, we derive lower limits for the number of interventions. Based on observations of induced trees, we specify a new deterministic adaptive algorithm to fully learn directions on each chord skeleton. At worst, our achievable scheme is an \u03b1 approximation algorithm where \u03b1 is the number of graphs independence. We also show that there are graph classes where the sufficient number of experiments is close to the lower limit. At the other extreme, there are graph classes where the required number of experiments differs multiplicatively \u03b1 from our lower limit. In simulations, our algorithm almost always works very close to the lower limit, while the approach on separation systems for random chord graphs is significantly worse."}, {"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Background and Terminology", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Essential graphs", "text": "A causal DAG D = (V, E) is a directed acyclic graph, in which V = (x1, x2).xn] is a set of random variables, and (x, y) is a directed edge, if x is a direct cause of y. We assume Pearl's structural equation model with independent errors (SEM-IE) in this work. Variables in S'V cause xi if xi = f (xj) j'S, where ey is a random variable that is independent of all other variables. The causal relationships of D imply a series of conditional independence (CI) relationships between the variables. A conditional independence relationship is the following form: Given set X and set Y are conditional independent subsets of variables X, Y, Z, Z."}, {"heading": "2.2 Interventions and Active Learning", "text": "In fact, it is so that most of them will be able to abide by the rules that they have given themselves. (D) D \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \""}, {"heading": "2.3 Problem Definition", "text": "We are interested in the following question: 3Skeleton of a DAG is the undirected graph we get when directed edges are converted into undirected edges. 4An induced subgraph on X, Y, Z is an immorality if X and Y are separated from each other. 5Graph Union of two DAGs D1 = (V, E1) and D2 = (V, E2) with the same skeleton is a partially directed graph D = (V, E) where (va, vb).E is undirected if the edges (va, vb) in E1 and E2 have different directions, and directed as va \u2192 vb if the edges are directed in E1 and E2."}, {"heading": "3 Complete Graphs", "text": "In this section we will consider the case in which the skeleton with which we begin, i.e. E (D), is an undirected complete graph (referred to as Kn). It is known that at each stage of (1) starting from E (D) the rules R1, R3 and R4 do not apply. Furthermore, the underlying DAG D is a directed clique. The directed clique is characterized by an order \u03c3 to [1: n] in such a way that in the partial graph \u03c3 (i + 1) induced by \u03c3 (i).... \u03c3 (n), \u03c3 (i) has no incidental edges. Let D for some ordered \u03c3 be designated by ~ Kn (\u03c3). Let [1: n] denote the set {1, 2... n}. For our first result regarding adaptive and non-adaptive algorithms we need the following results on a separation system."}, {"heading": "3.1 Separating System", "text": "Definition 1. [10,18] A (n, k) separating system on an n element set [1: n] is a group of subsets S = {S1, S2.... Sm} in such a way that | Si | \u2264 k and for each pair i, j there is a subset S, so that either i S, j / S or j S, i / S. If a pair i, j meets the above condition with respect to S, then S should separate the pair i, j. Here we consider the case when k < n / 2In [10] there is a (n, k) separating system along with a lower limit to | S |. In [18] Wegener gave a simpler argument for the lower limit and also provided a narrower upper limit than that in [10]. In this work we give a different construction below the size of the separating system."}, {"heading": "3.2 Adaptive algorithms: Equivalence to a Separating System", "text": "Consider a non-adaptive algorithm that designs a series of interventions I, each of which is at most k large, in order to discover ~ Kn (\u03c3). At worst, I must be a separation system over all \u03c3. This is already known. Now, we prove the need for a separation system for deterministic adaptive algorithms at worst. Theorem 2. There should be an adaptive deterministic algorithm A that designs the series of interventions I in such a way that the final graph EI (D) = ~ Kn (\u03c3) learns for each basic truth emanating from the initial skeleton E (D) = Kn. Then there is a cycle so that A designs an I that is a separation system.The above theorem is independent of the individual intervention magnitude. Therefore, we have the following theorem, which is a direct consequence of the first skeleton E (D) = Kn: Theorem 2. \u2212 At worst, each adaptive or non-adaptive algorithm is at the lowest level of the algorithms I."}, {"heading": "3.3 Randomized Adaptive Algorithms", "text": "Theorem 4. In order to fully identify a complete causal DAG ~ Kn (\u03c3) of n variables using size-k interventions, n2k interventions are necessary. Furthermore, the total number of variables accessed is at least n2. The lower limit in Theorem 4 is information theoretical. We now specify a randomized algorithm that requires O (nk log k) experiments in anticipation. We offer a simple generalization of [8] in which the authors have given a randomized algorithm for an unlimited intervention size. Theorem 5. Let E (D) Kn and the experiment size k = nr for some 0 < r < 1. Then there is a randomized adaptive algorithm that designs an I so that EI (D) = D is likely to be polynomial in n and | I | = O (log in expectation)."}, {"heading": "4 General Chordal Graphs", "text": "In this section, we will turn to interventions on a general DAG G. After the initial stages in (1), E (G) is a chain graph with chord chain components. There is no further immorality in the entire diagram. In this work, we will focus on one of the chord chain components. Therefore, the DAG D we are working on is regarded as a directed graph without immorality, whose skeleton E (D) from E (D) is recovered by interventions of size at most k to (1)."}, {"heading": "4.1 Bounds for Chordal skeletons", "text": "We offer a lower limit for both adaptive and non-adaptive deterministic schemes for a chord skeleton E (D). Let us specify the color number of the given chord diagram. Since chord diagrams are perfect, it is the same as the clique number. Theorem 6. For a chord skeleton E (D), in the worst case across all DAGs D (with skeleton E (D) and without immorality), if each intervention is at most k large, then | I (D) k can be applied logically (E (D) k logically (E (D)) for each adaptive and non-adaptive algorithm with EI (D) = D. Upper limit: The separation system based on the algorithm of section 3 can be applied to the vertices in the chord skeleton E (D) and it is possible to find all adjustments."}, {"heading": "4.2 Two extreme counter examples", "text": "Theorem 7. There are chord skeletons, so that for each algorithm with intervention size limitation k the number of necessary interventions amounts to at least \u03b1 (B \u2212 1) 2k, where A and B are respectively the number of independence and the chromatic number. There are chord graph classes, so that it is sufficient."}, {"heading": "4.3 An Improved Algorithm using Meek Rules", "text": "In this section, we design an adaptive deterministic algorithm that anticipates the use of the Meek rule R1, along with the idea of a separation system. We evaluate this experimentally using random chord graphs. First, we make a few observations about learning contiguous directed trees T from the skeleton E (T) (undirected trees are chord sequences) that exhibit no immorality, with each intervention being of size k = 1. Since the tree does not have a cycle, the Meek rules R2-R4 do not apply. Lemma 2. Each node in a directed tree without immorality has at most one incidental edge. There is a root node without incidental edges, and the intervention on this node alone identifies the entire tree by repeating rule R1.Lemma 3. If each intervention in I is of size at most 1, learning all directions on a directed tree T without immorality can be performed with at most one combination."}, {"heading": "4.3.1 Description of the algorithm", "text": "The key motivation behind the algorithm is that a pair of color classes is a forest (Lemma 4) (v). Choosing the right node for intervention leaves only a small subtree unlearned, as in the proof of Lemma 3. In the following steps, suitable nodes could be selected in the remaining subtrees until all edges are learned. To intervene in the actual graph, an intervention diagram must be selected that corresponds to Si. We would like to intervene in a node of color c. Now, let us consider a node v of color c. (v, c) as follows: For each color c), let us consider the induced forest F (c, c)."}, {"heading": "5 Simulations", "text": "We simulate our new heuristic system, namely algorithm 1, on randomly generated chord curves and compare it to a na\u00efve algorithm that follows the intervention sets given by our (n, k) separation system, as in Theorem 1. Both algorithms apply R0 and Meek rules after each intervention according to (1). We record the following lower limits: a) Information Theory System constructions for the maximum clique size according to (n) Max Clique Sep. Sys. Entropic LB, which is the chromatic number based on the lower limit of Theorem 6. In addition, we use two known (n, k) separation system constructions for the maximum clique size according to \"references\": The best known (k) separation system according to the Max. Clique Sep. Sys label. Accessible LB and our new simpler separation system according to (Theorem i) is the construction according to (Theorem x)."}, {"heading": "6 Conclusions", "text": "We have proposed lower and upper limits for the number of interventions required in the worst case for different classes of algorithms when the causal graph skeleton is complete. We have proposed lower and upper limits for the minimum number of interventions required in the worst case for general graphs. We have characterized two extreme graph classes in such a way that the minimum number of interventions in one class is close to the lower limit and in the other class is close to the upper limit. In the case of chord chains, we have proposed an algorithm that combines ideas for complete graphs with those when the skeleton is a forest, by applying Meek rules. Empirically, our algorithm behaves on randomly generated chord graphs close to the lower limit and exceeds the previous state of the art. Possible future work involves observing a narrower lower limit for diagrams that would possibly ensure an approximation of our algorithm."}, {"heading": "Acknowledgments", "text": "We would like to thank the authors for their support with grants: NSF CCF 1344179, 1344364, 1407278, 1422549 and an ARO YIP Award (W911NF-14-1-0258). We would also like to thank Frederick Eberhardt for helpful discussions."}, {"heading": "6.1 Proof of Lemma 1", "text": "We describe a string markup procedure as follows: \"A.\" - \"A.\" - \"A.\" - \"A.\" - \"A.\" - \"A.\" - \"A.\" - \"A.\" - \"A.\" - \"A.\" - \"A.\" - \"A.\" - \"A.\" - \"A.\" - \"A.\" - \"A.\" - \"A.\" - \"A.\" - \"A.\" - \"A.\" - \"A.\" - \"A.\" - \"A.\" - \"A.\" - \"A.\" - \"A.\" - \"A.\" - \"A.\" - \"A.\" - \"A.\" - \"A.\" - \"A.\" - \"A.\" - \"A.\" - \"-\" A. \"-\" A. \"-\" - \"A.\" - \"-\" A. \"-\""}, {"heading": "6.2 Proof of Theorem 1", "text": "In Lemma 1, the place has at most n'n / k occurrences of the symbol j. Let us now consider the pair of different elements p, q [1: n]. Since they are uniquely labeled (Lemma 1), there is at least one letter i in their string labels where they differ. Suppose the unambiguous ith letters are a, b, a, 6 = b and say a 6 = 0 without loss of universality. Then, the separation criterion is clearly fulfilled by the subset Si, a. This confirms the assertion."}, {"heading": "6.3 Proof of Theorem 2", "text": "We construct a worst-case \u03c3 (m \u2212 1) i inductively. Before each step m, the adaptive algorithm deterministically discloses that i is always based on E {I1, I2... Im \u2212 1} (Kn). Therefore, we will show a partial sequence \u03c3 (m \u2212 1) to satisfy the observations as much as possible. We will ensure that after each m we select according to the algorithm, more details about the system on n elements.Before each step m, we can reveal that each vertex i with a subset C (m \u2212 1) i: m] gives such a possibility to apply the rule R2. This would ensure that I have a separation system on n elements.Before each step m, let each vertex i with a subset C (m \u2212 1) i: m] such that C (m \u2212 1) i: {p: i = {p: i, p \u2264 m \u2212 1} (C) contains one of these indices."}, {"heading": "6.4 Proof of Lemma 2", "text": "The proof is a direct obvious consequence of cyclicality, the non-existence of immorality and the definition of rule R1."}, {"heading": "6.5 Proof of Lemma 3", "text": "Suppose the root node is unknown to the algorithm. Each tree has a single vertex delimiter that divides the tree into components, each of which is no larger than 23n [11]. If it is a root node, we choose this vertex delimiter a1 (it can be found in each node by removing each node and determining the remaining components). If it is a root node, we will stop here. Otherwise, its parent node p1 (if it is not) will be identified according to rule R0. Consider component trees T1, T2... Tk, which are created by removing the node a1. Let us leave T1 p1 in place. All directions in all other trees are known after repeatedly applying R1 to the original tree after applying R0. Instructions in T1 will not be known. For the next step, E (T1) is the new skeleton, which has no immorality. Again, we find the best vertex on the original tree will not be known according to instructions in T1."}, {"heading": "6.6 Proof of Lemma 4", "text": "The graph induced by two color classes in any graph is a two-part graph, and two-part graphs have no odd induced cycles. Since the graph and any induced subgraph are chord-shaped, this implies that the induced graph has no cycle on a pair of color classes, as the theorem proves."}, {"heading": "6.7 Proof of Theorem 4", "text": "We define a family of partial order \u03c3 (p) as follows: group i, i + 1 in Ci. The order between i and i + 1 is not revealed, but all edges between Ci and Cj for each j > i are directed from Ci to Cj. Now, one must design a series of interventions in such a way that exactly one node under each Ci is intervened at least once, because if neither i nor i + 1 are intervened in Ci, the direction between i and i + 1 cannot be determined by applying rule R2 to all other directions in the rest of the diagram. Since the size of each intervention is at most k and at least n / 2 nodes must be covered by intervention sets, the number of interventions required is at least n2k."}, {"heading": "6.8 Proof of Theorem 5", "text": "This divides the problem of learning a clique of size n into learning cliques of size k. Then we can apply the clique learning algorithm in [8] as a black box to each of the nk blocks: Each block is learned with the probability k \u2212 c after the c log k experiments in expectation. For k = cnr we select c > 1 / r \u2212 1. Then the union bound by n / k blocks yields a probability polynomial in n. Since each block performs O (log k) experiments, we need nkO (log k) experiments."}, {"heading": "6.9 Proof of Theorem 6", "text": "We need the following definitions and some results before proving the theory.Definition 2. A perfect elimination order \u03c3p = {v1, v2... vn} on the corners of an undirected chord diagram G is such that for all i, the induced neighborhood of vi on the subgraph formed by {v1, v2... vi \u2212 1 is a clique.Lemma 5. ([6]) If all directions in the chord diagram are ordered according to a perfect elimination (edges only go from corners lower in order), then there is no immorality. We make the following observation: Let the instructions in a diagram D orient themselves according to an order on the corners. If a clique comes first in order, then the knowledge of edges in order is higher in order."}, {"heading": "6.10 Proof of Theorem 7", "text": "Example with a workable solution with | I \u2212 n of the lower limit: Consider a graph G, which can be divided into a clique of size and an independent group \u03b1. Such graphs are called splitgraphs and as n \u2212 n \u00b2 n, the fraction of the splitgraphs to chord graphs tends to 1. If E (D) = G is a splitgraph skeleton, it is sufficient to intervene only at the nodes in the clique and therefore the number of interventions required for the clique is. It is certainly possible to orient the edges in such a way that immoralities are avoided, since the graph is chordal. Example with | I \u2212 n \u2212 n, which must be close to the upper limit: We construct a connected chord skeleton with independent chord sizes \u03b1 and click sizes (also the coloring of the number) in such a way that immoralities must be avoided, because the graph is different."}, {"heading": "6.12 Proof of Theorem 8", "text": "We provide the following justifications for the correctness of algorithm 1.1. In line 4 of the algorithm, if Meek rules and R0 are applied after each intervention, the intermediate diagram G with unlearned edges will always be a disjunct union of chord components (see (1) and therefore a chord diagram. 2. The number of unlearned edges before and after the main loop in algorithm 1 decreases by at least one. Each edge in E is invading two colors and one of the colors is always selected for processing because we apply a separation system to the colors. Therefore, a node belonging to an edge has a positive score and is encroached on it. The edge direction is learned by rule R0. Therefore, the algorithm ends. 3. It identifies the correct ~ G because each edge is closed after intervention by applying rules R0 and Meek rules, as in (1), both of which are corrected."}], "references": [{"title": "A characterization of markov equivalence classes for acyclic digraphs", "author": ["Steen A. Andersson", "David Madigan", "Michael D. Perlman"], "venue": "The Annals of Statistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "Causation and Intervention", "author": ["Frederick Eberhardt"], "venue": "(Ph.D. Thesis),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Characterization and greedy learning of interventional markov equivalence classes of directed acyclic graphs", "author": ["Alain Hauser", "Peter B\u00fchlmann"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Two optimal strategies for active learning of causal networks from interventional data", "author": ["Alain Hauser", "Peter B\u00fchlmann"], "venue": "In Proceedings of Sixth European Workshop on Probabilistic Graphical Models,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Two optimal strategies for active learning of causal models from interventional data", "author": ["Alain Hauser", "Peter B\u00fchlmann"], "venue": "International Journal of Approximate Reasoning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Nonlinear causal discovery with additive noise models", "author": ["Patrik O Hoyer", "Dominik Janzing", "Joris Mooij", "Jonas Peters", "Bernhard Sch\u00f6lkopf"], "venue": "In Proceedings of NIPS", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Randomized experimental design for causal graph discovery", "author": ["Huining Hu", "Zhentao Li", "Adrian Vetta"], "venue": "In Proceedings of NIPS", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Experiment selection for causal discovery", "author": ["Antti Hyttinen", "Frederick Eberhardt", "Patrik Hoyer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "On separating systems of a finite set", "author": ["Gyula Katona"], "venue": "Journal of Combinatorial Theory,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1966}, {"title": "A separator theorem for planar graphs", "author": ["Richard J Lipton", "Robert Endre Tarjan"], "venue": "SIAM Journal on Applied Mathematics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1979}, {"title": "Causal inference and causal explanation with background knowledge", "author": ["Christopher Meek"], "venue": "In Proceedings of the eleventh international conference on uncertainty in artificial intelligence,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1995}, {"title": "Strong completeness and faithfulness in bayesian networks", "author": ["Christopher Meek"], "venue": "In Proceedings of the eleventh international conference on uncertainty in artificial intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1995}, {"title": "Causality: Models, Reasoning and Inference", "author": ["Judea Pearl"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "A linear non-gaussian acyclic model for causal discovery", "author": ["S Shimizu", "P. O Hoyer", "A Hyvarinen", "A. J Kerminen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Causation, Prediction, and Search", "author": ["Peter Spirtes", "Clark Glymour", "Richard Scheines"], "venue": "A Bradford Book,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "An algorithm for deciding if a set of observed independencies has a causal explanation", "author": ["Thomas Verma", "Judea Pearl"], "venue": "In Proceedings of the Eighth international conference on uncertainty in artificial intelligence,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1992}, {"title": "On separating systems whose elements are sets of at most k elements", "author": ["Ingo Wegener"], "venue": "Discrete Mathematics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1979}], "referenceMentions": [{"referenceID": 4, "context": "[2, 6, 8, 9, 14]).", "startOffset": 0, "endOffset": 16}, {"referenceID": 6, "context": "[2, 6, 8, 9, 14]).", "startOffset": 0, "endOffset": 16}, {"referenceID": 7, "context": "[2, 6, 8, 9, 14]).", "startOffset": 0, "endOffset": 16}, {"referenceID": 12, "context": "[2, 6, 8, 9, 14]).", "startOffset": 0, "endOffset": 16}, {"referenceID": 5, "context": "Given two causally related variables X and Y , it is not possible to infer whether X causes Y or Y causes X from random samples, unless certain assumptions are made on the distribution of E and/or on f [7, 15].", "startOffset": 202, "endOffset": 209}, {"referenceID": 13, "context": "Given two causally related variables X and Y , it is not possible to infer whether X causes Y or Y causes X from random samples, unless certain assumptions are made on the distribution of E and/or on f [7, 15].", "startOffset": 202, "endOffset": 209}, {"referenceID": 12, "context": "This process is different than conditioning as explained in detail in [14].", "startOffset": 70, "endOffset": 74}, {"referenceID": 4, "context": "[6] developed an efficient algorithm that minimizes this number in the worst case.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Unfortunately, the interventions obtained in [6] can involve up to n/2 variables.", "startOffset": 45, "endOffset": 48}, {"referenceID": 1, "context": "Furthermore [3] shows that the set of interventions to fully identify a causal DAG must satisfy a specific set of combinatorial conditions called a separating system1, when the intervention size is not constrained or is 1.", "startOffset": 12, "endOffset": 15}, {"referenceID": 7, "context": "In [9], with the assumption that the same holds true for any intervention size, Hyttinen et al.", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "One open problem is: If the learning algorithm is adaptive after each intervention, is a separating system still needed or can one do better? It was believed that adaptivity does not help in the worst case [3] and that one still needs a separating system.", "startOffset": 206, "endOffset": 209}, {"referenceID": 6, "context": "Recently [8] showed that randomized adaptive algorithms need only log log n interventions with high probability for the unbounded case.", "startOffset": 9, "endOffset": 12}, {"referenceID": 12, "context": "We adopt Pearl\u2019s structural equation model with independent errors (SEM-IE) in this work (see [14] for more details).", "startOffset": 94, "endOffset": 98}, {"referenceID": 14, "context": "All the CI relations that are learned statistically through observations can also be inferred from the Bayesian network using a graphical criterion called the d-separation [16] assuming that the distribution is faithful to the graph 2.", "startOffset": 172, "endOffset": 176}, {"referenceID": 11, "context": "Faithfulness is a widely accepted assumption, since it is known that only a measure zero set of distributions are not faithful [13].", "startOffset": 127, "endOffset": 131}, {"referenceID": 0, "context": "E(D) is always a chain graph with chordal6 chain components 7 [1].", "startOffset": 62, "endOffset": 65}, {"referenceID": 14, "context": "The d-separation criterion can be used to identify the skeleton and all the immoralities of the underlying causal DAG [16].", "startOffset": 118, "endOffset": 122}, {"referenceID": 15, "context": "Meek derived 3 local rules (Meek rules), introduced in [17], to be recursively applied to identify every such additional edge (see Theorem 3 of [12]).", "startOffset": 55, "endOffset": 59}, {"referenceID": 10, "context": "Meek derived 3 local rules (Meek rules), introduced in [17], to be recursively applied to identify every such additional edge (see Theorem 3 of [12]).", "startOffset": 144, "endOffset": 148}, {"referenceID": 12, "context": "This operation, and how it affects the joint distribution is formalized by the do operator by Pearl [14].", "startOffset": 100, "endOffset": 104}, {"referenceID": 14, "context": "parents of S [16], and the remaining adjacent edges in the original skeleton are declared to be the children.", "startOffset": 13, "endOffset": 17}, {"referenceID": 15, "context": "Then, 4 local Meek rules (introduced in [17]) are repeatedly applied to the original DAG D with the new directions learnt from the cut to learn more till no more directed edges can be identified.", "startOffset": 40, "endOffset": 44}, {"referenceID": 2, "context": "The concepts of essential graphs and Markov equivalence classes are extended in [4] to incorporate the role of interventions: Let I = {I1, I2, .", "startOffset": 80, "endOffset": 83}, {"referenceID": 3, "context": "E(D) is a chain graph with undirected chordal components and it is known that interventions on one chain components do not affect the discovery process of directed edges in the other components [5].", "startOffset": 194, "endOffset": 197}, {"referenceID": 8, "context": "[10,18] An (n, k)-separating system on an n element set [1 : n] is a set of subsets S = {S1, S2 .", "startOffset": 0, "endOffset": 7}, {"referenceID": 16, "context": "[10,18] An (n, k)-separating system on an n element set [1 : n] is a set of subsets S = {S1, S2 .", "startOffset": 0, "endOffset": 7}, {"referenceID": 8, "context": "In [10], Katona gave an (n, k)-separating system together with a lower bound on |S|.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "In [18], Wegener gave a simpler argument for the lower bound and also provided a tighter upper bound than the one in [10].", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "In [18], Wegener gave a simpler argument for the lower bound and also provided a tighter upper bound than the one in [10].", "startOffset": 117, "endOffset": 121}, {"referenceID": 8, "context": "By Theorem 2, we need a separating system in the worst case and the lower and upper bounds are from [10, 18].", "startOffset": 100, "endOffset": 108}, {"referenceID": 16, "context": "By Theorem 2, we need a separating system in the worst case and the lower and upper bounds are from [10, 18].", "startOffset": 100, "endOffset": 108}, {"referenceID": 6, "context": "We provide a straightforward generalization of [8], where the authors gave a randomized algorithm for unbounded intervention size.", "startOffset": 47, "endOffset": 50}], "year": 2015, "abstractText": "We consider the problem of learning causal networks with interventions, when each intervention is limited in size under Pearl\u2019s Structural Equation Model with independent errors (SEM-IE). The objective is to minimize the number of experiments to discover the causal directions of all the edges in a causal graph. Previous work has focused on the use of separating systems for complete graphs for this task. We prove that any deterministic adaptive algorithm needs to be a separating system in order to learn complete graphs in the worst case. In addition, we present a novel separating system construction, whose size is close to optimal and is arguably simpler than previous work in combinatorics. We also develop a novel information theoretic lower bound on the number of interventions that applies in full generality, including for randomized adaptive learning algorithms. For general chordal graphs, we derive worst case lower bounds on the number of interventions. Building on observations about induced trees, we give a new deterministic adaptive algorithm to learn directions on any chordal skeleton completely. In the worst case, our achievable scheme is an \u03b1-approximation algorithm where \u03b1 is the independence number of the graph. We also show that there exist graph classes for which the sufficient number of experiments is close to the lower bound. In the other extreme, there are graph classes for which the required number of experiments is multiplicatively \u03b1 away from our lower bound. In simulations, our algorithm almost always performs very close to the lower bound, while the approach based on separating systems for complete graphs is significantly worse for random chordal graphs.", "creator": "LaTeX with hyperref package"}}}