{"id": "1511.00048", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2015", "title": "The Pareto Regret Frontier for Bandits", "abstract": "Given a multi-armed bandit problem it may be desirable to achieve a smaller-than-usual worst-case regret for some special actions. I show that the price for such unbalanced worst-case regret guarantees is rather high. Specifically, if an algorithm enjoys a worst-case regret of B with respect to some action, then there must exist another action for which the worst-case regret is at least {\\Omega}(nK/B), where n is the horizon and K the number of actions. I also give upper bounds in both the stochastic and adversarial settings showing that this result cannot be improved. For the stochastic case the pareto regret frontier is characterised exactly up to constant factors.", "histories": [["v1", "Fri, 30 Oct 2015 23:30:30 GMT  (42kb)", "http://arxiv.org/abs/1511.00048v1", "14 pages. To appear at NIPS 2015"]], "COMMENTS": "14 pages. To appear at NIPS 2015", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tor lattimore"], "accepted": true, "id": "1511.00048"}, "pdf": {"name": "1511.00048.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["tor.lattimore@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 151 1.00 048v 1 [cs.L G] 3"}, {"heading": "1 Introduction", "text": "The multi-armed bandit is the simplest category of problems that present a particularly small problem when the regret so far emerges. At each step, the learner selects one of the actions and receives a reward for the action chosen. A learner's performance is measured in terms of regret, which is the (expected) difference between the rewards he actually received and would have received (in expectation) by selecting the optimal action and asking what can be achieved if some actions are given special treatment. Focusing on the worst-case criterion for unlimited armed bandits has treated all actions uniformly and aimed at regret, which does not depend on which action is optimal. I take a different approach and ask what can be achieved if some actions are given. Whether it is possible to achieve an improved worst-case regret for some actions or not, and what the cost of the remaining actions is. Such results can be useful in a variety of cases."}, {"heading": "2 Preliminaries", "text": "I use the same notation as Bubeck and Cesa-Bianchi [2012]. Define Ti (t) as the number of times action i was selected after the time steps t and \u00b5-i, s as the empirical estimate of \u00b5i from the first scans. This means that \u00b5-i, Ti (t-1) is the empirical estimate of \u00b5i at the beginning of the tallest round. (3) This result is probably known, but a proof is included in Appendix E. The optimal arm is i-i = argmaxi \u00b5i with ties broken in some arbitrary way. The optimal reward is GDP = maxi \u00b5i."}, {"heading": "3 Understanding the Frontier", "text": "Before we prove the main proposition, I will briefly describe the characteristics of the limit of regret. First, I note that if Bi = \u221a n (K \u2212 1) for all i, thenBi = \u221a n (K \u2212 1) for all i, thenBi = \u221a n (K \u2212 1) for all i, thenBi = \u221a n (K \u2212 1) for all i, thenBi = \u221a n / (K \u2212 1) for all i = (K \u2212 1) for all i = inBj (K \u2212 1). This particular B is observed up to constant factors by MOSS [Audibert and Bubeck, 2009] and OC-UCB [Lattimore, 2015] but not by UCB [Auer et al., 2002], the Rucbi (K) is the limit of regret (n).Of course, the uniform choice of B \u2212 n for the first arm is not the only option."}, {"heading": "4 Lower Bounds", "text": "Theorem 1. Let's assume that N (0, 1) is queried from a standard Gaussian. Let \u03c0 be any strategy, then 8 (R\u03c01 > n / 8, then the result is trivial. Let's say R 1 \u2264 n / 8. Let c = 4 and define details (if not, then just rearrange the actions). If R\u03c01 > n / 8, then the result is trivial. \u2212 \u2212 Let's assume that R 1 \u2264 n / 8. Let c = 4 and definie\u03b5k = min {1 2, cR\u03b5k n} \u2264 1 (Define K vectors \u00b51,. \u2212 Let's assume that the RK is applied after (\u00b5k) n \u2212 Nif j = k = 6 = 1 \u2212 nullity. Therefore, the optimal action for the bandit with means \u03b5k (1\u03b5k) is kk."}, {"heading": "5 Upper Bounds", "text": "The algorithm is a generalization MOSS [Audibert and Bubeck, 2009] with two modifications. First, the width of the confidence limits is distorted in a non-uniform way, and second, the upper confidence limits are shifted. Functionally, the new algorithm is identical to MOSS in the special case that Bi is uniform. Definition protocol + (x) = max {0, log (x)} 1: Input: n and B1,., BK 2: ni = n2 / B2i for all i 3: for t: 1,., n do4: Es = argmax i: 0, Ti (t \u2212 1) + Lemji (t \u2212 1) log: 1) log + (ni Ti (t \u2212 1) log: i (t \u2212 1)))."}, {"heading": "On Logarithmic Regret", "text": "In a recent technical report, I have shown empirically that MOSS suffers from sub-optimal problem-related problems that cannot be taken into account with respect to the minimal gap (Lattimore, 2015). Specifically, the order-optimal asymptotic regret may be significantly smaller. Specifically: UCB von Auer et al. [2002] satisfiesRucb\u00b5, i \u00b2 O (\u2211 i > 01 \u0445 i logn), (7), which can be much smaller for unequal gaps than Eq. (6) and asymptotically optimal [Lai and Robbins, 1985]. The problem is that MOSS only researches enough to receive minimax regret, but sometimes observes minimax regret, even if a more conservative algorithm would perform better."}, {"heading": "A Note on Constants", "text": "The constants in the statement of Theorem 2 can be improved by carefully aligning all the results, but the proof would grow significantly and I would not expect a corresponding boost in practical performance. In fact, the opposite is the case, since the \"weak\" boundaries used in the proof would spread to the algorithm. Also, note that the 4 in the square root of the unbalanced MOSS algorithm is conditioned by the fact that I do not accept rewards in [0, 1] for which the variance is at most 1 / 4. It is possible to replace the 4 with 2 + \u03b5 for each \u03b5 > 0 by changing the base in the peeling argument in the evidence of Lemma 4, as done by Bubeck [2010] and others. I compare experimental results with MOSS and unbalanced MOSS in two simple simulated examples, both with horizon n = 5000. Each data point is an empirical average of 104 i.i.i., so the error results are too small to see."}, {"heading": "6 Discussion", "text": "If an algorithm has a low worst-case regret for a particular action, then the worst-case regret of the remaining actions is necessarily much greater than the well-known uniform worst-case limit of \u0432 (\u221a Kn). This unfortunate result is in stark contrast to the expert environment, for which there are algorithms that suffer a constant regret in relation to a single expert at no cost to the rest. Surprisingly, the best attainable (non-uniform) worst-case limits are almost entirely determined by the value of the least worst-case regret until a permutation. There are some interesting open questions. Especially in the adverse environment, I am not sure if the upper or lower limit is narrow (or none of them). It would also be nice to know if the constant factors can be determined exactly asymptotically, but until now this has not even been done in the uniform case."}, {"heading": "Acknowledgements", "text": "I am indebted to the very careful reviewers who have made many suggestions for improvement for this work. Thank you!"}, {"heading": "B Proof of Theorem 7", "text": "Remember that the proof of the UCB depends on ETi (n) as well as (1) and (2i) logn (1) being dependent on each other. Unbalanced UCB works just like UCB, but with deferred rewards. Consequently, for unbalanced UCB we have ETi (n) and (1) logn (n), where (i) and (i) logn (n) logn (n) logn (n) logn (n) logn (n) O (n) logn (n) logn (n) logn (n).For i / A we have logn (n) and (n) logn (n) logn (n) logn (n)."}, {"heading": "C KL Techniques", "text": "Let us prove the assertion that E \u03c0 \u00b5kTk (n) \u2212 E\u03c0\u00b51Tk (n) \u2264 n\u03b5k \u221a E\u03c0\u00b51Tk (n).The result follows the same lines as the proof of the lower limits given by Auer et al. [1995]. Let {Ft} nt = 1 be a filtration in which Ft contains information about rewards and actions up to the time step. So gIt, t and 1 {It = i} are measurable with respect to Ft. Let P1 and Pk be the measurements for F caused by bandit problems \u00b51 and \u00b5k respectively. Note that Tk (n) is a Fn-measurable random variable, limited in [0, n]."}, {"heading": "D Adversarial Bandits", "text": "In the opposing setting I get something similar. First, I introduce a new notation. Let gi, t = i = i = i = i = i = i = i = i (0, 1) is the gain / reward from the choice of the action i in the time step. This is arbitrarily accepted by the opponent with gi, t possibly even depending on the actions of the learner up to the time stamp. The most obvious change of the Exp3-\u03b3 algorithm, which prefers the previous action and adapts the learning rate accordingly. \u2212 The algorithm accepts as input the previous action esa [0, 1] K [n = 1gi, t \u2212 gIt, t]. I make the most obvious change of the Exp3-TA algorithm, which prefers the previous action and adapts the learning rate accordingly. \u2212 The algorithm accepts as input the previous action esa [0, 1] K [n], which must be determined the i-i = 1, and the learning rate thereafter."}, {"heading": "E Concentration", "text": "Theorem 10. Let X1, X2,.., Xn be independent and 1-sub-Gaussian, then P't \u2264 n: 1 t: 1 t: 1 t: 0 t: 0 t: 0 t: 0 t: 0 tXs \u2264 exp (\u2212 \u03b5 22n).Proof. Since Xi is 1-sub-Gaussian, it fulfills by definition E [exp (\u03bbXi)] \u2264 exp: 1 t: 2 / 2. Now X1, X2,. are independent and zero mean, therefore it is a sub-martyingale by convexity of the exponential function exp: t = 1 Xs = 1 Xs. Therefore, if \u03b5 > 0, then by the maximum inequality of doob: t: \u2212 t \u2264 s = 1Xs = inf: 0 t: 0 p: 0 (\u03b5 > p: 2) p: 0 p: 0 (\u03b5 > p: 2)."}], "references": [{"title": "Further optimal regret bounds for thompson sampling", "author": ["Shipra Agrawal", "Navin Goyal"], "venue": "In Proceedings of International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Agrawal and Goyal.,? \\Q2012\\E", "shortCiteRegEx": "Agrawal and Goyal.", "year": 2012}, {"title": "Analysis of thompson sampling for the multi-armed bandit problem", "author": ["Shipra Agrawal", "Navin Goyal"], "venue": "In Proceedings of Conference on Learning Theory (COLT),", "citeRegEx": "Agrawal and Goyal.,? \\Q2012\\E", "shortCiteRegEx": "Agrawal and Goyal.", "year": 2012}, {"title": "Minimax policies for adversarial and stochastic bandits", "author": ["Jean-Yves Audibert", "S\u00e9bastien Bubeck"], "venue": "In COLT,", "citeRegEx": "Audibert and Bubeck.,? \\Q2009\\E", "shortCiteRegEx": "Audibert and Bubeck.", "year": 2009}, {"title": "Gambling in a rigged casino: The adversarial multi-armed bandit problem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Yoav Freund", "Robert E Schapire"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Auer et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Auer et al\\.", "year": 1995}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f3 Cesa-Bianchi", "Paul Fischer"], "venue": "Machine Learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Concentration Inequalities: A Nonasymptotic Theory of Independence", "author": ["Stephane Boucheron", "Gabor Lugosi", "Pascal Massart"], "venue": "OUP Oxford,", "citeRegEx": "Boucheron et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Boucheron et al\\.", "year": 2013}, {"title": "Bandits games and clustering foundations", "author": ["S\u00e9bastien Bubeck"], "venue": "PhD thesis, Universite\u0301 des Sciences et Technologie de Lille-Lille I,", "citeRegEx": "Bubeck.,? \\Q2010\\E", "shortCiteRegEx": "Bubeck.", "year": 2010}, {"title": "Regret Analysis of Stochastic and Nonstochastic Multiarmed Bandit Problems. Foundations and Trends in Machine Learning", "author": ["S\u00e9bastien Bubeck", "Nicol\u00f2 Cesa-Bianchi"], "venue": "Now Publishers Incorporated,", "citeRegEx": "Bubeck and Cesa.Bianchi.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Cesa.Bianchi.", "year": 2012}, {"title": "Kullback\u2013Leibler upper confidence bounds for optimal sequential allocation", "author": ["Olivier Capp\u00e9", "Aur\u00e9lien Garivier", "Odalric-Ambrym Maillard", "R\u00e9mi Munos", "Gilles Stoltz"], "venue": "The Annals of Statistics,", "citeRegEx": "Capp\u00e9 et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Capp\u00e9 et al\\.", "year": 2013}, {"title": "Prediction, learning, and games", "author": ["Nicolo Cesa-Bianchi"], "venue": null, "citeRegEx": "Cesa.Bianchi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi.", "year": 2006}, {"title": "Regret to the best vs. regret to the average", "author": ["Eyal Even-Dar", "Michael Kearns", "Yishay Mansour", "Jennifer Wortman"], "venue": "Machine Learning,", "citeRegEx": "Even.Dar et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2008}, {"title": "Adaptive online prediction by following the perturbed leader", "author": ["Marcus Hutter", "Jan Poland"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Hutter and Poland.,? \\Q2005\\E", "shortCiteRegEx": "Hutter and Poland.", "year": 2005}, {"title": "Prediction strategies without loss", "author": ["Michael Kapralov", "Rina Panigrahy"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kapralov and Panigrahy.,? \\Q2011\\E", "shortCiteRegEx": "Kapralov and Panigrahy.", "year": 2011}, {"title": "The pareto regret frontier", "author": ["Wouter M Koolen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Koolen.,? \\Q2013\\E", "shortCiteRegEx": "Koolen.", "year": 2013}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["Tze Leung Lai", "Herbert Robbins"], "venue": "Advances in applied mathematics,", "citeRegEx": "Lai and Robbins.,? \\Q1985\\E", "shortCiteRegEx": "Lai and Robbins.", "year": 1985}, {"title": "Optimally confident UCB : Improved regret for finite-armed bandits", "author": ["Tor Lattimore"], "venue": "Technical report,", "citeRegEx": "Lattimore.,? \\Q2015\\E", "shortCiteRegEx": "Lattimore.", "year": 2015}, {"title": "On the prior sensitivity of thompson sampling", "author": ["Che-Yu Liu", "Lihong Li"], "venue": "arXiv preprint arXiv:1506.03378,", "citeRegEx": "Liu and Li.,? \\Q2015\\E", "shortCiteRegEx": "Liu and Li.", "year": 2015}, {"title": "Exploiting easy data in online optimization", "author": ["Amir Sani", "Gergely Neu", "Alessandro Lazaric"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sani et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sani et al\\.", "year": 2014}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["William Thompson"], "venue": null, "citeRegEx": "Thompson.,? \\Q1933\\E", "shortCiteRegEx": "Thompson.", "year": 1933}, {"title": "Concentration The following straight-forward concentration inequality is presumably well known and the proof of an almost identical result is available by Boucheron et al", "author": ["E required"], "venue": null, "citeRegEx": "required.,? \\Q2013\\E", "shortCiteRegEx": "required.", "year": 2013}], "referenceMentions": [{"referenceID": 9, "context": "The uniform regret is obtained by choosing \u03c1i = 1/K , which leads to the well-known O( \u221a n logK) bound achieved by the exponential weighting algorithm [Cesa-Bianchi, 2006].", "startOffset": 151, "endOffset": 171}, {"referenceID": 9, "context": "The earliest work seems to be by Hutter and Poland [2005] where it is shown that the learner can assign a prior weight to each action and pays a worst-case regret of O( \u221a\u2212n log \u03c1i) for expert i where \u03c1i is the prior belief in expert i and n is the horizon.", "startOffset": 33, "endOffset": 58}, {"referenceID": 9, "context": "The uniform regret is obtained by choosing \u03c1i = 1/K , which leads to the well-known O( \u221a n logK) bound achieved by the exponential weighting algorithm [Cesa-Bianchi, 2006]. The consequence of this is that an algorithm can enjoy a constant regret with respect to a single action while suffering minimally on the remainder. The problem was studied in more detail by Koolen [2013] where (remarkably) the author was able to exactly describe the pareto regret frontier when K = 2.", "startOffset": 152, "endOffset": 378}, {"referenceID": 9, "context": "The uniform regret is obtained by choosing \u03c1i = 1/K , which leads to the well-known O( \u221a n logK) bound achieved by the exponential weighting algorithm [Cesa-Bianchi, 2006]. The consequence of this is that an algorithm can enjoy a constant regret with respect to a single action while suffering minimally on the remainder. The problem was studied in more detail by Koolen [2013] where (remarkably) the author was able to exactly describe the pareto regret frontier when K = 2. Other related work (also in the experts setting) is where the objective is to obtain an improved regret against a mixture of available experts/actions [Even-Dar et al., 2008, Kapralov and Panigrahy, 2011]. In a similar vain, Sani et al. [2014] showed that algorithms for prediction with expert advice 1", "startOffset": 152, "endOffset": 720}, {"referenceID": 16, "context": "In the bandit setting I am only aware of the work by Liu and Li [2015] who study the effect of the prior on the regret of Thompson sampling in a special case.", "startOffset": 53, "endOffset": 71}, {"referenceID": 7, "context": "Although I was not able to derive a matching upper bound in this case, a simple modification of the Exp-\u03b3 algorithm [Bubeck and Cesa-Bianchi, 2012] leads to an algorithm with R 1 \u2264 B1 and R k .", "startOffset": 116, "endOffset": 147}, {"referenceID": 6, "context": "2 Preliminaries I use the same notation as Bubeck and Cesa-Bianchi [2012]. Define Ti(t) to be the number of times action i has been chosen after time step t and \u03bc\u0302i,s to be the empirical estimate of \u03bci from the first s times action i was sampled.", "startOffset": 43, "endOffset": 74}, {"referenceID": 2, "context": "This particular B is witnessed up to constant factors by MOSS [Audibert and Bubeck, 2009] and OC-UCB [Lattimore, 2015], but not UCB [Auer et al.", "startOffset": 62, "endOffset": 89}, {"referenceID": 15, "context": "This particular B is witnessed up to constant factors by MOSS [Audibert and Bubeck, 2009] and OC-UCB [Lattimore, 2015], but not UCB [Auer et al.", "startOffset": 101, "endOffset": 118}, {"referenceID": 4, "context": "This particular B is witnessed up to constant factors by MOSS [Audibert and Bubeck, 2009] and OC-UCB [Lattimore, 2015], but not UCB [Auer et al., 2002], which suffers Rucb i \u2208 \u03a9( \u221a nK logn).", "startOffset": 132, "endOffset": 151}, {"referenceID": 3, "context": "E\u03bc1Tk(n) , where (a) follows from standard entropy inequalities and a similar argument as used by Auer et al. [1995] (details given in Appendix C), (b) since k 6= 1 and E\u03bc1T1(n) + E\u03bc1Tk(n) \u2264 n, and (c) by Eq.", "startOffset": 98, "endOffset": 117}, {"referenceID": 2, "context": "The algorithm is a generalisation MOSS [Audibert and Bubeck, 2009] with two modifications.", "startOffset": 39, "endOffset": 66}, {"referenceID": 15, "context": "On Logarithmic Regret In a recent technical report I demonstrated empirically that MOSS suffers sub-optimal problemdependent regret in terms of the minimum gap [Lattimore, 2015].", "startOffset": 160, "endOffset": 177}, {"referenceID": 2, "context": "The above proof may be simplified in the special case that B is uniform where we recover the minimax regret of MOSS, but with perhaps a simpler proof than was given originally by Audibert and Bubeck [2009]. On Logarithmic Regret In a recent technical report I demonstrated empirically that MOSS suffers sub-optimal problemdependent regret in terms of the minimum gap [Lattimore, 2015].", "startOffset": 179, "endOffset": 206}, {"referenceID": 3, "context": "Specifically, UCB by Auer et al. [2002] satisfies R \u03bc,i \u2208 O (", "startOffset": 21, "endOffset": 40}, {"referenceID": 14, "context": "(6) and is asymptotically order-optimal [Lai and Robbins, 1985].", "startOffset": 40, "endOffset": 63}, {"referenceID": 8, "context": ", which includes (for example) KL-UCB [Capp\u00e9 et al., 2013] and Thompson sampling (see analysis by Agrawal and Goyal [2012a,b] and original paper by Thompson [1933]), but not OC-UCB [Lattimore, 2015] or MOSS [Audibert and Bubeck, 2009].", "startOffset": 38, "endOffset": 58}, {"referenceID": 15, "context": ", 2013] and Thompson sampling (see analysis by Agrawal and Goyal [2012a,b] and original paper by Thompson [1933]), but not OC-UCB [Lattimore, 2015] or MOSS [Audibert and Bubeck, 2009].", "startOffset": 130, "endOffset": 147}, {"referenceID": 2, "context": ", 2013] and Thompson sampling (see analysis by Agrawal and Goyal [2012a,b] and original paper by Thompson [1933]), but not OC-UCB [Lattimore, 2015] or MOSS [Audibert and Bubeck, 2009].", "startOffset": 156, "endOffset": 183}, {"referenceID": 0, "context": ", 2013] and Thompson sampling (see analysis by Agrawal and Goyal [2012a,b] and original paper by Thompson [1933]), but not OC-UCB [Lattimore, 2015] or MOSS [Audibert and Bubeck, 2009].", "startOffset": 47, "endOffset": 113}, {"referenceID": 0, "context": ", 2013] and Thompson sampling (see analysis by Agrawal and Goyal [2012a,b] and original paper by Thompson [1933]), but not OC-UCB [Lattimore, 2015] or MOSS [Audibert and Bubeck, 2009]. A Note on Constants The constants in the statement of Theorem 2 can be improved by carefully tuning all thresh-holds, but the proof would grow significantly and I would not expect a corresponding boost in practical performance. In fact, the reverse is true, since the \u201cweak\u201d bounds used in the proof would propagate to the algorithm. Also note that the 4 appearing in the square root of the unbalanced MOSS algorithm is due to the fact that I am not assuming rewards are bounded in [0, 1] for which the variance is at most 1/4. It is possible to replace the 4 with 2+ \u03b5 for any \u03b5 > 0 by changing the base in the peeling argument in the proof of Lemma 4 as was done by Bubeck [2010] and others.", "startOffset": 47, "endOffset": 867}], "year": 2015, "abstractText": "Given a multi-armed bandit problem it may be desirable to achieve a smaller-than-usual worst-case regret for some special actions. I show that the price for such unbalanced worst-case regret guarantees is rather high. Specifically, if an algorithm enjoys a worst-case regret of B with respect to some action, then there must exist another action for which the worst-case regret is at least \u03a9(nK/B), where n is the horizon and K the number of actions. I also give upper bounds in both the stochastic and adversarial settings showing that this result cannot be improved. For the stochastic case the pareto regret frontier is characterised exactly up to constant factors.", "creator": "LaTeX with hyperref package"}}}