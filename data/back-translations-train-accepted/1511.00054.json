{"id": "1511.00054", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2015", "title": "Gaussian Process Random Fields", "abstract": "Gaussian processes have been successful in both supervised and unsupervised machine learning tasks, but their computational complexity has constrained practical applications. We introduce a new approximation for large-scale Gaussian processes, the Gaussian Process Random Field (GPRF), in which local GPs are coupled via pairwise potentials. The GPRF likelihood is a simple, tractable, and parallelizeable approximation to the full GP marginal likelihood, enabling latent variable modeling and hyperparameter selection on large datasets. We demonstrate its effectiveness on synthetic spatial data as well as a real-world application to seismic event location.", "histories": [["v1", "Sat, 31 Oct 2015 01:02:14 GMT  (3253kb,D)", "http://arxiv.org/abs/1511.00054v1", "Advances in Neural Information Processing Systems (NIPS), 2015"]], "COMMENTS": "Advances in Neural Information Processing Systems (NIPS), 2015", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["david a moore", "stuart j russell"], "accepted": true, "id": "1511.00054"}, "pdf": {"name": "1511.00054.pdf", "metadata": {"source": "CRF", "title": "Gaussian Process Random Fields", "authors": ["David A. Moore"], "emails": ["russell}@cs.berkeley.edu"], "sections": [{"heading": null, "text": "Gaussian processes have been successful in both supervised and unsupervised machine learning tasks, but their computational complexity limited their practical application. We introduce a new approximation for large-scale Gaussian processes, the Gaussian Process Random Field (GPRF), in which local GPs are coupled via paired potentials. GPRF probability is a simple, traceable, and parallelizable approximation to the full GP boundary probability, enabling latent variable modeling and hyperparameter selection on large datasets. We demonstrate its effectiveness on synthetic spatial data as well as a real-world application to seismic event sites."}, {"heading": "1 Introduction", "text": "In regression and classification, we are asked to predict the results; in contrast, we get a set of results in latent variable modeling and are asked to reconstruct the inputs that could have produced them. Gaussian processes (GPs) are a flexible class of probability distributions to functions that allow us to approach functional learning problems from an appealing principle-driven and clean Bayesian perspective. Unfortunately, the temporal complexity of the exact GP inference is O (n3), where n is the number of data points. This makes exact GP calculations for real datasets with n > 10000.Many approaches have been suggested to avoid this constraint. A particularly simple approach is to divide the input space into smaller blocks, with a single large GP being replaced by a variety of local models."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Gaussian processes", "text": "Gaussian processes [3] are distributions of real functions. Gaussian processes are parameterized by an average function \u00b5\u03b8 (x), which is typically assumed to be \u00b5 (x) = 0 without loss of generality, and a covariance function (sometimes referred to as kernel) k\u03b8 (x, x) with hyperparameters \u03b8. A common choice is the squared exponential covariance, kSE (x, x) = \u03c32f exp (\u2212 12 x \u2212 x \u2032 2), with hyperparameters \u03c32f and \"specification of each of a previous variance and correlation longitudinal scale. We say that a random function f (x) \u2212 a Gaussian process is distributed if the vector of the function values f = f (X) is a multivariate function Y (X) and\" specification of each of a previous variance and correlation longitudinal scale."}, {"heading": "2.2 Scalability and approximate inference", "text": "In fact, most of them are able to survive on their own."}, {"heading": "2.3 Markov Random Fields", "text": "We remember some basic theories about Markov random fields (MRFs), also known as undirected graphical models [18]. A pair-wise MRF consists of an undirected graph (V, E), along with node potentials (i, yj) and edge potentials (2), which define an energy function on a random vector y, E (y) = \u2211 i-V-i (yi) + conservation (i, j), E-ij (yj), (2) where y is divided into components yi, which are identified with nodes in the graph. This energy, in turn, defines a probability density, the \"Gibbs distribution,\" indicated by p (y) = 1Z-exp (\u2212 E (y)), where Z = exp (\u2212 E (z)) dz is a normalizing constant. Gauss random fields are the special case of arbitrary FMRs multiplied by \u2212 p."}, {"heading": "3 Gaussian Process Random Fields", "text": "We consider a vector of n real values1 Observations y \u0445 N (0, Ky) modeled by a GP, where Ky is implicitly a function of input locations X and hyperparameters \u03b8. Unless otherwise stated, all probabilities p (yi), p (yi, yj), etc. relate to marginals of this complete GP. We would like to perform a gradient-based optimization based on the marginal probability (1) with respect to X and / or \u03b8, but assume that the cost of doing so is directly unsustainable. To continue, we assume a partition y = (y1, y2,.., yM) of the observations in M blocks at most m, with an implicit corresponding partition X = (X1, X2,., XM) of the (perhaps not observed) inputs. \u2212 The source of this partition is not a focal point of the current work; we could imagine that the blocks have Jiicspic or Jinoj points, or that we each have the corresponding Jinoy 1 or Jinoy Jinoy binations."}, {"heading": "3.1 The GPRF Objective", "text": "It is not as if we are in a position to see the full GPS and the full GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GPS / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / GSP / / GSP / GSP / GSP / GSP / / GSP / GSP / GSP / / GSP / GSP / GR / / GSP / / GSP / / / GSP / GSP / / / GSP / / / GSP / GSP / GSP / / GSR /"}, {"heading": "3.2 Predictive equivalence to the BCM", "text": "We have introduced qGPRF as a replacement model for the training set (X, y); however, it is natural to extend the GPRF to make predictions at a number of test points X * by adding the function values f * = f (X *) as anM + 1. Block with an edge to each of the training blocks. The resulting prediction distribution, pGPRF (f *, y *), qGPRF (f *, y) = p (f *) M * i = 1 p (yi, f *) p (yi) p (f *) M * i = 1 p (yi). (9) exactly corresponds to the prediction of the Bayesian Committee Machine (BCM) [2]. This motivates the GPRF as a natural extension of the training set (X *, y *)."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Uniform Input Distribution", "text": "This year it has come to the point where it will be able to put itself at the top, \"he said in an interview with the Deutsche Presse-Agentur.\" We have never lost so much time, \"he said.\" But we are not yet in a position to put ourselves in a position, \"he told the Deutsche Presse-Agentur.\" We have never waited so long to be able to retaliate, \"he said."}, {"heading": "4.2 Seismic event location", "text": "s crust. Nearby events tend to generate similar waveforms; we can model this spatial correlation as a Gaussian process. Previous information about the event locations is available from traditional travel-time-based location systems [26], which generate an independent Gaussian uncertainty ellipse for each event. Considering background noise and conducting common alignments of arrival times exceeds the scope of this paper. To specifically focus on the ability to generate GP-LVM inferences, we used real event locations, but generated synthetic waveforms by using a sampling of a 50-output GP using a Mate core."}, {"heading": "5 Conclusions and Future Work", "text": "Gaussian random field is a tractable and effective surrogate for GP marginal probability. It has the flavor of approximate inference methods such as loopy belief propagation, but can be precisely analyzed in terms of a deterministic approach to inverse covariance, and offers a new training time interpretation of the Bayesian Committee Machine. It is easy to implement and can be directly paralleled. One direction for future work is to find partitions for which a GPRF performs well, such as partitions that induce a block-like tree structure. A possibly related question is when the GPRF target defines a normalizable probability distribution (beyond the case of an exact tree structure) and under what circumstances it represents a good approximation of the exact GP probability. This assessment in this paper focuses on spatial data, with both local GPs and the BCM method successfully applied to high-dimensional regression problems while PRF applied to sufficient spatial data for PRF."}, {"heading": "Acknowledgements", "text": "We thank the anonymous reviewers for their helpful suggestions. This work was supported by the DTRA fellowship # HDTRA-11110026 and computing resources donated by Microsoft Research as part of an Azure for Research fellowship."}, {"heading": "A Block tree structure", "text": "It is easy to see that the GPRF target is exactly when the structure J induced by the true precision matrix is a tree with respect to our chosen partition of y. For each selection of the root node yroot, the tree structure implies that we can write the true GP distribution as a product of the parent conditional distributions, p (y) = p (yroot) = 6 = root p (yi | y\u03c0 (i)) where \u03c0 (i) is the (unique) structure of the node i in relation to our chosen root. Then, the extension of the conditional distribution series (y) = p (yroot) = root p (yi) p (yroot) p (yroot (yroot) p (i6) = root p (yroot).i6 = root p (yi) p (yi) p (yp) p (ip) p (ip) p (ip) p (ip), yp (ip) yp (ip), yp (ip) yp (ip), yp (ip (ip) yp (ip) yp (ip), yp (ip (ip) yp (ip) yp (ip), yp (ip (ip) yp (ip) yp (ip), yp (ip (ip) yp (ip) yp (ip (ip), yp (ip) yp (ip (ip) yp (ip) yp (ip (ip) yp (ip), yp (ip) yp (ip (ip) yp (ip), yp (ip) yp (ip (ip) yp (ip (ip), yp (ip (ip) yp (ip) yp (ip (ip), yp (ip) yp (ip (ip) yp (ip), yp (ip (ip) ip (ip (ip) ip (ip) ip (ip) ip (ip (ip) ip (ip (i"}, {"heading": "B Approximation to the true Gaussian", "text": "In this section, we prove theorem 1 from the main text, which shows that qGPRF is an unnormalized Gaussian density with a certain precision matrix. For each pair of blocks (i, j), we define the local precision matrix Q (ij) as the inversion of the marginal covariance, Q (ij) = (Q (ij) 11 Q (ij) 12Q (ij) 21 Q (ij) 22) = (Kii Kij Kji Kjj) \u2212 1, The notation Q (ij) is used to distinguish these local precision matrices from the blocks Jij of the global precision matrix. Font qGPRF \u2212 implicit densities, log qGPRF (y) = \u2212 12 M Implications i = 1 (1 \u2212 Ei YY YY YY YY YY) that the local precision matrices are different from the blocks of the global precision matrix."}, {"heading": "C Approximate normalization", "text": "In this section we prove theorem 2 from the main text: Theorem 2. The objective qGPRF is approximately normalized in the sense that the optimum value of the beta-free energy [19], FB (b) = \u2211 i-V (yi, yi) (1 \u2212 | Ei |) ln b (yi, yj) ln b (yi) ln (yi, yi, yj) ln b (yi, yj)))) logZ, (11) the approximation to the normalizing constant found by loopy-induced propagation is exactly zero. Furthermore, this optimum is achieved by taking the pseudomarginals bi, bij, bij the true GP marginals pi-marginals."}], "references": [{"title": "Gaussian process latent variable models for visualisation of high dimensional data", "author": ["Neil D Lawrence"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "A Bayesian committee machine", "author": ["Volker Tresp"], "venue": "Neural Computation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Gaussian Processes for Machine Learning", "author": ["Carl Rasmussen", "Chris Williams"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Bayesian Gaussian process latent variable model", "author": ["Michalis K Titsias", "Neil D Lawrence"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Variational Inference for Latent Variables and Uncertain Inputs in Gaussian Processes", "author": ["Andreas C. Damianou", "Michalis K. Titsias", "Neil D. Lawrence"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "A unifying view of sparse approximate Gaussian process regression", "author": ["Joaquin Qui\u00f1onero-Candela", "Carl Edward Rasmussen"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Learning for larger datasets with the Gaussian process latent variable model", "author": ["Neil D Lawrence"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Variational learning of inducing variables in sparse Gaussian processes", "author": ["Michalis K Titsias"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Model learning with local Gaussian process regression", "author": ["Duy Nguyen-Tuong", "Matthias Seeger", "Jan Peters"], "venue": "Advanced Robotics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Domain decomposition approach for fast Gaussian process regression of large spatial data sets", "author": ["Chiwoo Park", "Jianhua Z Huang", "Yu Ding"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "A framework for evaluating approximation methods for Gaussian process regression", "author": ["Krzysztof Chalupka", "Christopher KI Williams", "Iain Murray"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Distributed Gaussian Processes", "author": ["Marc Peter Deisenroth", "Jun Wei Ng"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Infinite mixtures of Gaussian process experts", "author": ["Carl Edward Rasmussen", "Zoubin Ghahramani"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "Fast allocation of Gaussian process experts", "author": ["Trung Nguyen", "Edwin Bonilla"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Local and global sparse Gaussian process approximations", "author": ["Edward Snelson", "Zoubin Ghahramani"], "venue": "In Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Modelling local and global phenomena with sparse Gaussian processes", "author": ["Jarno Vanhatalo", "Aki Vehtari"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Gaussian process latent random field", "author": ["Guoqiang Zhong", "Wu-Jun Li", "Dit-Yan Yeung", "Xinwen Hou", "Cheng-Lin Liu"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Probabilistic graphical models: Principles and techniques", "author": ["Daphne Koller", "Nir Friedman"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Bethe free energy, Kikuchi approximations, and belief propagation algorithms", "author": ["Jonathan S Yedidia", "William T Freeman", "Yair Weiss"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2001}, {"title": "Loopy belief propagation for approximate inference: An empirical study", "author": ["Kevin P Murphy", "Yair Weiss", "Michael I Jordan"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}, {"title": "Statistical analysis of non-lattice data", "author": ["Julian Besag"], "venue": "The Statistician,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1975}, {"title": "WiFi-SLAM using Gaussian process latent variable models", "author": ["Brian Ferris", "Dieter Fox", "Neil D Lawrence"], "venue": "In International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Gaussian processes for big data", "author": ["James Hensman", "Nicolo Fusi", "Neil D Lawrence"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Distributed variational inference in sparse Gaussian process regression and latent variable models", "author": ["Yarin Gal", "Mark van der Wilk", "Carl Rasmussen"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "The task of approximating the marginal likelihood is motivated by unsupervised applications such as the GP latent variable model [1], but examining the predictions made by our model also yields a novel interpretation of the Bayesian Committee Machine [2].", "startOffset": 129, "endOffset": 132}, {"referenceID": 1, "context": "The task of approximating the marginal likelihood is motivated by unsupervised applications such as the GP latent variable model [1], but examining the predictions made by our model also yields a novel interpretation of the Bayesian Committee Machine [2].", "startOffset": 251, "endOffset": 254}, {"referenceID": 2, "context": "1 Gaussian processes Gaussian processes [3] are distributions on real-valued functions.", "startOffset": 40, "endOffset": 43}, {"referenceID": 2, "context": "The most common application of GPs is to Bayesian regression [3], in which we attempt to predict the function values f\u2217 at test points X\u2217 via the conditional distribution given the training data, p(f\u2217|y;X,X\u2217, \u03b8).", "startOffset": 61, "endOffset": 64}, {"referenceID": 0, "context": "This setting is known as the Gaussian Process Latent Variable Model (GPLVM) [1]; it uses GPs as a model for unsupervised learning and nonlinear dimensionality reduction.", "startOffset": 76, "endOffset": 79}, {"referenceID": 3, "context": "though some recent work [4, 5] attempts to recover an approximate posterior on X by maximizing a variational bound.", "startOffset": 24, "endOffset": 30}, {"referenceID": 4, "context": "though some recent work [4, 5] attempts to recover an approximate posterior on X by maximizing a variational bound.", "startOffset": 24, "endOffset": 30}, {"referenceID": 5, "context": "These points can be chosen by maximizing the marginal likelihood in a surrogate model [6, 7] or by minimizing the KL divergence between the approximate and exact GP posteriors [8].", "startOffset": 86, "endOffset": 92}, {"referenceID": 6, "context": "These points can be chosen by maximizing the marginal likelihood in a surrogate model [6, 7] or by minimizing the KL divergence between the approximate and exact GP posteriors [8].", "startOffset": 86, "endOffset": 92}, {"referenceID": 7, "context": "These points can be chosen by maximizing the marginal likelihood in a surrogate model [6, 7] or by minimizing the KL divergence between the approximate and exact GP posteriors [8].", "startOffset": 176, "endOffset": 179}, {"referenceID": 2, "context": "A separate class of approximations, so-called \u201clocal\u201d GP methods [3, 9, 10], involves partitioning the inputs into blocks of m points each, then modeling each block with an independent Gaussian process.", "startOffset": 65, "endOffset": 75}, {"referenceID": 8, "context": "A separate class of approximations, so-called \u201clocal\u201d GP methods [3, 9, 10], involves partitioning the inputs into blocks of m points each, then modeling each block with an independent Gaussian process.", "startOffset": 65, "endOffset": 75}, {"referenceID": 9, "context": "A separate class of approximations, so-called \u201clocal\u201d GP methods [3, 9, 10], involves partitioning the inputs into blocks of m points each, then modeling each block with an independent Gaussian process.", "startOffset": 65, "endOffset": 75}, {"referenceID": 10, "context": "Nonetheless, local GPs sometimes work very well in practice, achieving results comparable to more sophisticated methods in a fraction of the time [11].", "startOffset": 146, "endOffset": 150}, {"referenceID": 1, "context": "The Bayesian Committee Machine (BCM) [2] attempts to improve on independent local GPs by averaging the predictions of multiple GP experts.", "startOffset": 37, "endOffset": 40}, {"referenceID": 11, "context": "The BCM can yield high-quality predictions that avoid the pitfalls of local GPs (Figure 1c), while maintaining scalability to very large datasets [12].", "startOffset": 146, "endOffset": 150}, {"referenceID": 12, "context": "Mixture-of-experts models [13, 14] extend the local GP concept in a different direction: instead of deterministically assigning points to GP models based on their spatial locations, they treat the assignments as unobserved random variables and do inference over them.", "startOffset": 26, "endOffset": 34}, {"referenceID": 13, "context": "Mixture-of-experts models [13, 14] extend the local GP concept in a different direction: instead of deterministically assigning points to GP models based on their spatial locations, they treat the assignments as unobserved random variables and do inference over them.", "startOffset": 26, "endOffset": 34}, {"referenceID": 14, "context": "The PIC approximation [15] blends a global inducing-point model with local block-diagonal covariances, thus capturing a mix of global and local structure, though with the same boundary discontinuities as in \u201cvanilla\u201d local GPs.", "startOffset": 22, "endOffset": 26}, {"referenceID": 15, "context": "A related approach is the use of covariance functions with compact support [16] to capture local variation in concert with global inducing points.", "startOffset": 75, "endOffset": 79}, {"referenceID": 10, "context": "[11] surveys and compares several approximate GP regression methods on synthetic and real-world datasets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Finally, we note here the similar title of [17], which is in fact orthogonal to the present work: they use a random field as a prior on input locations, whereas this paper defines a random field decomposition of the GP model itself, which may be combined with any prior on X .", "startOffset": 43, "endOffset": 47}, {"referenceID": 17, "context": "We recall some basic theory regarding Markov random fields (MRFs), also known as undirected graphical models [18].", "startOffset": 109, "endOffset": 113}, {"referenceID": 18, "context": "It can be interpreted as a \u201cBethe-type\u201d approximation [19], in which a joint density is approximated via overlapping pairwise marginals.", "startOffset": 54, "endOffset": 58}, {"referenceID": 19, "context": ") In general this will not be the case, but in the spirit of loopy belief propagation [20], we consider the tree-structured case as an approximation for the general setting.", "startOffset": 86, "endOffset": 90}, {"referenceID": 18, "context": "The objective qGPRF is approximately normalized in the sense that the optimal value of the Bethe free energy [19],", "startOffset": 109, "endOffset": 113}, {"referenceID": 1, "context": "corresponds exactly to the prediction of the Bayesian Committee Machine (BCM) [2].", "startOffset": 78, "endOffset": 81}, {"referenceID": 20, "context": "2 A similar derivation shows that the conditional distribution of any block yi given all other blocks yj 6=i also takes the form of a BCM prediction, suggesting the possibility of pseudolikelihood training [21], i.", "startOffset": 206, "endOffset": 210}, {"referenceID": 21, "context": "1 Uniform Input Distribution We first consider a 2D synthetic dataset intended to simulate spatial location tasks such as WiFiSLAM [22] or seismic event location (below), in which we observe high-dimensional measurements but have only noisy information regarding the locations at which those measurements were taken.", "startOffset": 131, "endOffset": 135}, {"referenceID": 6, "context": "For comparison, we also evaluate the Sparse GP-LVM, implemented in GPy [23], which uses the FITC approximation to the marginal likelihood [7].", "startOffset": 138, "endOffset": 141}, {"referenceID": 3, "context": "(We also considered the Bayesian GP-LVM [4], but found it to be more resource-intensive with no meaningful difference in results on this problem.", "startOffset": 40, "endOffset": 43}, {"referenceID": 22, "context": "Recently, more sophisticated inducing-point methods have claimed scalability to very large datasets [24, 25], but they do so with m \u2264 1000; we expect that they would hit the same fundamental scaling constraints for problems that inherently require many inducing points.", "startOffset": 100, "endOffset": 108}, {"referenceID": 23, "context": "Recently, more sophisticated inducing-point methods have claimed scalability to very large datasets [24, 25], but they do so with m \u2264 1000; we expect that they would hit the same fundamental scaling constraints for problems that inherently require many inducing points.", "startOffset": 100, "endOffset": 108}, {"referenceID": 2, "context": "To focus specifically on the ability to approximate GP-LVM inference, we used real event locations but generated synthetic waveforms by sampling from a 50-output GP using a Mat\u00e9rn kernel [3] with \u03bd = 3/2 and a lengthscale of 40km.", "startOffset": 187, "endOffset": 190}, {"referenceID": 10, "context": "This evaluation in this paper focuses on spatial data; however, both local GPs and the BCM have been successfully applied to high-dimensional regression problems [11, 12], so exploring the effectiveness of the GPRF for dimensionality reduction tasks would also be interesting.", "startOffset": 162, "endOffset": 170}, {"referenceID": 11, "context": "This evaluation in this paper focuses on spatial data; however, both local GPs and the BCM have been successfully applied to high-dimensional regression problems [11, 12], so exploring the effectiveness of the GPRF for dimensionality reduction tasks would also be interesting.", "startOffset": 162, "endOffset": 170}], "year": 2015, "abstractText": "Gaussian processes have been successful in both supervised and unsupervised machine learning tasks, but their computational complexity has constrained practical applications. We introduce a new approximation for large-scale Gaussian processes, the Gaussian Process Random Field (GPRF), in which local GPs are coupled via pairwise potentials. The GPRF likelihood is a simple, tractable, and parallelizeable approximation to the full GP marginal likelihood, enabling latent variable modeling and hyperparameter selection on large datasets. We demonstrate its effectiveness on synthetic spatial data as well as a real-world application to seismic event location.", "creator": "LaTeX with hyperref package"}}}