{"id": "1511.00060", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2015", "title": "Top-down Tree Long Short-Term Memory Networks", "abstract": "In this paper we develop a recurrent neural network (TreeRNN), which is designed to predict a tree rather than a linear sequence as is the case in conventional recurrent neural networks. Our model defines the probability of a sentence by estimating the generation probability of its dependency tree. We construct the tree incrementally by generating the left and right dependents of a node whose probability is computed using recurrent neural networks with shared hidden layers. Application of our model to two language modeling tasks shows that it outperforms or performs on par with related models.", "histories": [["v1", "Sat, 31 Oct 2015 02:05:28 GMT  (140kb,D)", "http://arxiv.org/abs/1511.00060v1", null], ["v2", "Thu, 7 Jan 2016 00:48:42 GMT  (108kb,D)", "http://arxiv.org/abs/1511.00060v2", null], ["v3", "Sun, 3 Apr 2016 23:30:17 GMT  (109kb,D)", "http://arxiv.org/abs/1511.00060v3", "to appear in NAACL 2016; code available atthis https URL"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["xingxing zhang", "liang lu", "mirella lapata"], "accepted": true, "id": "1511.00060"}, "pdf": {"name": "1511.00060.pdf", "metadata": {"source": "CRF", "title": "Tree Recurrent Neural Networks with Application to Language Modeling", "authors": ["Xingxing Zhang", "Liang Lu", "Mirella Lapata"], "emails": ["x.zhang@ed.ac.uk,", "liang.lu@ed.ac.uk,", "mlap@inf.ed.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to deal with the product of conditional probabilities, namely the number of possible terms they use. A simple and effective strategy is to limit the chain reaction to the individual terms. However, simplification reduces the number of free words that are tested in practice in order to control the combinatorial growth in the number of possible stories. Literature offers many examples of how these limitations can be overcome."}, {"heading": "2 Related Work", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "3 Recurrent Neural Network Language Model", "text": "In this section we briefly describe RNNLMs and then proceed with the introduction of our model. Let S = w1, w2,.., wm denote a linear sequence of words. We estimate their probability as follows: P (S) = m, i = 2 P (wi | w1: i \u2212 1) (1) Each term in Equation (1) corresponds to the output of a recurring neural network in one step (Mikolov et al., 2010). Let e (wt) enter the input layer at a time t (where e (wt) is the only hot vector of wt and | V | is the vocabulary size), let e (wt) enter the output layer and yt enter the output layer."}, {"heading": "4 Tree Recurrent Neural Network Language Model", "text": "We will try to model the probability of a set by estimating the generational probability of its dependency tree. We will first present our model when using unmarked dependencies, and then show how it extends to marked dependencies (Section 4.3).We assume that a dependency tree is first built in width, starting from the ROOT node, the only node at level zero. For each node at each level, we first create left dependencies (from the nearest to the farthest) and then right dependencies (again from the nearest to the farthest).Figure 1 illustrates the width traverse of the dependency tree for the set A little girl climbs into a beautiful wooden playhouse. As you can see, we first visit climbing, then its left dependencies (is girl) and then its right dependencies (in,.).Figure 1 illustrates the width-first traverse of the dependency tree for the set A little girl climbs into a beautiful playhouse."}, {"heading": "4.1 Dependency Path", "text": "A dependency path can generally be described as a path between ROOT and w, consisting of the words and edges that connect them. To represent a dependency path, we define four types of edges. Let w0 be a node in a dependency tree, and w1, w2,.., wn its left dependencies. (As shown in Figure 2, LEFT edge is the edge between w0 and its first left dependency referred to as (w0, w1). Assume wk (with 1 < k \u2264 n) is not a first left dependency on w0."}, {"heading": "4.2 Generation with four RNNs", "text": "In a manner analogous to Equation (1), we estimate the probability P (w | D (w))) (Equation (3)) with RNNs. While it is straightforward to model a word sequence in an RNN, we try to model the dependencies (w) that are a sub-tree, or more precisely a sequence of < words, edge type > tuples. To do this, we use four RNNNs (GEN-L, GEN-R, GEN-NX-R) 1, each corresponding to the four types of edges (LEFT, RIGHT, NX-LEFT, and NXRIGHT). For each < word, edge type > tuples, we select an RNN by edge type, we feed the word to the RNN and generate / forecast its dependence."}, {"heading": "4.3 Labeled Dependency Model (LTreeRNN)", "text": "The model in the previous section does not consider dependency labels. Labels provide valuable information about the meaning of sentences for at least two reasons. First, they can potentially distinguish between dependency trees with otherwise very similar structures. To give an example, there are 28 unique in-depth dependency labels for the word mind in one of our records. The most common labels are POBJ, DOBJ, NSUB, ROOT, and CCOMP, which indicate that mind is often used as a verb in the corpus, but without explicit label information, it would be difficult to distinguish between the verb and the noun uses of the mind. Second, trees with different structures may be considered similar if they have similar dependency labels. To model the intuition above, we modify TreeRNN to consider dependency labels (in addition to words)."}, {"heading": "4.4 Model Training", "text": "Calculation of the full softness of the production layer can be very expensive if a large vocabulary is used (e.g. more than 30K words). (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.). (.) (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (. (.). (.). (.). (. (.). (.). (.). (. (.). (.). (.). (. (.). (.). (.). (.). (.). (.). (.). (.). (.). (.).). (.). (.). (.).). (.). (.). (.).). (.).). (.).). (.).). (.).). (.).). (.).). (.).).). (.). (.).).). (.). (.).). (.).). (.).). (.). (.).).). (.). (.). (.).). (.).).). (.). (.).). (.). (.).). (.).).). (.).). (.). (.).). (.).). (.).).).). (.).). (.).).). (.).).). (.).).)"}, {"heading": "4.5 Tree Generation", "text": "This year, the number of working women retiring has tripled and the number of working women retiring has tripled."}, {"heading": "5 Experiments", "text": "In this section we present details on how our models were trained and experimental results on two evaluation tasks commonly used to evaluate the performance of language models. Specifically, in Section 5.2 we perform perplexity experiments on the well-known Penn Treebank (PTB; Marcus et al. (1993) and the APNews dataset (Bengio et al., 2003). Section 5.3 presents our results on the Microsoft Research (MSR) Sentence Completion Challenge dataset (Branch and Burges, 2012)."}, {"heading": "5.1 Training Details", "text": "In all our experiments, we trained labeled and unlabeled versions of TreeRNN with stochastic gradient descent without impulse on an Nvidia GTX 980 graphics card. We used a mini batch size of 50 to 100. We initialized all parameters of our model with the even distribution between \u2212 0.2 to 0.2. We used an initial learning rate of 0.3 or 0.1 for all experiments and validated the model per epoch on the validation theorem. In cases where there was no significant improvement in log probability, we divided the learning rate by 2 or 1.5 per epoch until there was no improvement in log probability again. It is known that RNNNs suffer from problems with exploding gradients; as a result, we changed the gradient g when the gradient norm | | g | > 5 and g = 5g | (Pascanu et al, 2014; Sutskal, 100, if the word is hidden)."}, {"heading": "5.2 Perplexity Evaluation", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "5.3 MSR Sentence Completion Challenge", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "5.4 Tree Generation Evaluation", "text": "In this section we will show the ability to generate dependency trees."}, {"heading": "6 Conclusions", "text": "In this paper, we have proposed a recurrent neural network model that is designed to predict a tree rather than a linear sequence. Experimental results on two language modeling tasks suggest that our model delivers a performance superior to traditional RNNs and competes with a number of neural language models, some of which have been developed specifically for the present modeling tasks. Our model's ability to generate dependence trees holds promise for both text generation applications and tasks performed via structural inputs. Although our experiments have focused exclusively on dependence trees, there is nothing inherently inherent in our formulation that forbids their application to other types of tree structures (e.g. tree composition or even taxonomies). Recently, RNNs with Long Short Term Memory (LSTM) units (Hochreiter and Schmidhudel) have been applied al inherently superior to a variety of sequencing tasks (e.g. sequencing tasks in 2014 and 2014)."}, {"heading": "Appendix A", "text": "Tables 4 and 5 show perplexity (on the validation set) for RNN, TreeRNN and LTreeRNN with different hidden unit sizes. The results are published on the PTB and APNews datasets."}, {"heading": "Appendix B", "text": "In this section we show how to calculate the number of parameters for the RNN and TreeRNN. We used the RNNLM toolkit (Mikolov et al., 2011c) with word classes that speed up the calculation when working with large vocabulary. The number of parameters in the RNN is: NRNNpara. = r \u00b7 | V | + r2 + r \u00d7 Nclass + r \u00b7 | V | = 2 \u00d7 r \u00d7 | V | + r2 + r \u00d7 Nclass, where r is the hidden unit size, | V | is the vocabulary size, and Nclass is the number of classes for the output vocabulary (Nclass = 100 in all our experiments). The number of parameters in TreeRNNN is: NTreeRNpara. = s \u00d7 | V | + 4 \u00d7 s \u00d7 r + 4 \u00d7 r \u00d7 | V | \u2264 4,5 \u00d7 r \u00d7 | V | V | + 6 \u00d7 r2s, where the NEparK and NE22K is."}], "references": [{"title": "Joint Language and Translation Modeling with Recurrent Neural Networks", "author": ["Auli et al.2013] Michael Auli", "Michel Galley", "Chris Quirk", "Geoffrey Zweig"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language", "citeRegEx": "Auli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Auli et al\\.", "year": 2013}, {"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Large language models in machine translation", "author": ["Ashok C. Popat", "Peng Xu", "Franz J. Och", "Jeffrey Dean"], "venue": "In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Brants et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Brants et al\\.", "year": 2007}, {"title": "Structured language modeling", "author": ["Chelba", "Jelinek2000] Ciprian Chelba", "Frederick Jelinek"], "venue": "Computer Speech and Language,", "citeRegEx": "Chelba et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2000}, {"title": "Structure and performance of a dependency language model", "author": ["David Engle", "Frederick Jelinek", "Victor Jimenez", "Sanjeev Khudanpur", "Lidia Mangu", "Harry Printz", "Eric Ristad", "Ronald Rosenfeld", "Andreas Stolcke"], "venue": null, "citeRegEx": "Chelba et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 1997}, {"title": "Recurrent neural network language model training with noise contrastive estimation for speech recognition", "author": ["Chen et al.2015] X Chen", "X Liu", "MJF Gales", "PC Woodland"], "venue": "IEEE International Conference on Accoustics,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman"], "venue": "Cognitive science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Using a connectionist model in a syntactical based language model", "author": ["Emami et al.2003] Ahmad Emami", "Peng Xu", "Frederick Jelinek"], "venue": "In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "Emami et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Emami et al\\.", "year": 2003}, {"title": "Sentence compression by deletion with lstms", "author": ["Enrique Alfonseca", "Carlos A. Colmenares", "Lukasz Kaiser", "Oriol Vinyals"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language", "citeRegEx": "Filippova et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Filippova et al\\.", "year": 2015}, {"title": "Dependency language models for sentence completion", "author": ["Gubbins", "Vlachos2013] Joseph Gubbins", "Andreas Vlachos"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Gubbins et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gubbins et al\\.", "year": 2013}, {"title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics", "author": ["Gutmann", "Hyv\u00e4rinen2012] Michael U Gutmann", "Aapo Hyv\u00e4rinen"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Gutmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gutmann et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Vanishing gradient problem during learning recurrent neural nets and problem solutions", "author": ["Sepp Hochreiter"], "venue": "International Journal of Uncertainty, Fuzziness and Knowledge-based Systems,", "citeRegEx": "Hochreiter.,? \\Q1998\\E", "shortCiteRegEx": "Hochreiter.", "year": 1998}, {"title": "Recurrent Continuous Translation Models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Multimodal neural language models", "author": ["Kiros et al.2014] Ryan Kiros", "Ruslan Salakhutdinov", "Rich Zemel"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "A cache based natural language model for speech recognition", "author": ["Kuhn", "de Mori1990] Roland Kuhn", "Renato de Mori"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Kuhn et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Kuhn et al\\.", "year": 1990}, {"title": "The stanford corenlp natural language processing toolkit", "author": ["Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J Bethard", "David McClosky"], "venue": "In Proceedings of 52nd Annual Meeting of the Association", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Building a large annotated corpus of english: the penn treebank", "author": ["Marcus et al.1993] Mitch Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": null, "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Recurrent Neural Network based Language Model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In Proceedings of INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Empirical evaluation and combination", "author": ["Anoop Deoras", "Stefan Kombrink", "Lukas Burget", "Jan Cernock\u1ef3"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Strategies for Training Large Scale Neural Network Language Models", "author": ["Anoop Deoras", "Daniel Povey", "Lukas Burget", "Jan Cernocky"], "venue": "In Proceedings of ASRU", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "2011c. Extensions of Recurrent Neural Network Language Model", "author": ["Stefan Kombrink", "Lukas Burget", "JH Cernocky", "Sanjeev Khudanpur"], "venue": "In Proceedings of the 2011 IEEE International Conference on Acoustics,", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Efficient estimation of word representations in vector space", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "In Proceedings of the 2013 International Conference on Learning Representations,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning longer memory in recurrent neural networks. arXiv preprint arXiv:1412.7753", "author": ["Armand Joulin", "Sumit Chopra", "Michael Mathieu", "Marc\u2019Aurelio Ranzato"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2014}, {"title": "Statistical Language Models based on", "author": ["Tomas Mikolov"], "venue": "Neural Networks. Ph.D. thesis, Brno University of Technology", "citeRegEx": "Mikolov.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov.", "year": 2012}, {"title": "Three new graphical models for statistical language modelling", "author": ["Mnih", "Hinton2007] Andriy Mnih", "Geoffrey Hinton"], "venue": "In Proceedings of the 24th International Conference on Machine Learning,", "citeRegEx": "Mnih et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2007}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["Mnih", "Teh2012] Andriy Mnih", "Yee Whye Teh"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Mnih et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2012}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Tomas Mikolov", "Yoshua Bengio"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Recursive distributed representations", "author": ["Jordan B. Pollack"], "venue": "Artificial Intelligence,", "citeRegEx": "Pollack.,? \\Q1990\\E", "shortCiteRegEx": "Pollack.", "year": 1990}, {"title": "Robust Probabilistic Predictive Syntactic Processing: Motivations, Models, and Applications", "author": ["Brian Edward Roark"], "venue": "Ph.D. thesis,", "citeRegEx": "Roark.,? \\Q2001\\E", "shortCiteRegEx": "Roark.", "year": 2001}, {"title": "A maximum entropy approach to adaptive statistical language modeling", "author": ["Roni Rosenfeld"], "venue": "Computer Speech and Language,", "citeRegEx": "Rosenfeld.,? \\Q1996\\E", "shortCiteRegEx": "Rosenfeld.", "year": 1996}, {"title": "Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition", "author": ["Sak et al.2014] Ha\u015fim Sak", "Andrew Senior", "Fran\u00e7oise Beaufays"], "venue": "arXiv preprint arXiv:1402.1128", "citeRegEx": "Sak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sak et al\\.", "year": 2014}, {"title": "Bidirectional recurrent neural networks", "author": ["Schuster", "Paliwal1997] Mike Schuster", "Kuldip K Paliwal"], "venue": "Signal Processing, IEEE Transactions", "citeRegEx": "Schuster et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Schuster et al\\.", "year": 1997}, {"title": "Modelling and optimizing on syntactic n-grams for statistical machine translation. Transactions of the Association for Computational Linguistics, 3:169\u2013182", "author": ["Rico Sennrich"], "venue": null, "citeRegEx": "Sennrich.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich.", "year": 2015}, {"title": "A new string-to-dependency machine translation algorithm with a target dependency language model", "author": ["Shen et al.2008] Libin Shen", "Jinxi Xu", "Ralph Weischedel"], "venue": "In Proceedings of ACL-08: HLT,", "citeRegEx": "Shen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2008}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Eric H. Huang", "Jeffrey Pennington", "Christopher D. Manning", "Andrew Ng"], "venue": "In Advances in Neural Information Processing Sys-", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning"], "venue": "In Proceedings of the 2011 Conference", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Srilm-an extensible language modeling toolkit", "author": ["Andreas Stolcke"], "venue": "In Proceedings of the 7th International Conference on Spoken Language Processing,", "citeRegEx": "Stolcke,? \\Q2002\\E", "shortCiteRegEx": "Stolcke", "year": 2002}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075v3", "author": ["Tai et al.2015] Kai Sheng Tai", "Richard Socher", "Christopher D. Manning"], "venue": null, "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Decoding with large-scale neural language models improves translation", "author": ["Yinggong Zhao", "Victoria Fossum", "David Chiang"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language", "citeRegEx": "Vaswani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Vaswani et al\\.", "year": 2013}, {"title": "Show and tell: A neural image caption generator", "author": ["Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Recurrent neural networks for language understanding", "author": ["Yao et al.2013] Kaisheng Yao", "Geoffrey Zweig", "MeiYuh Hwang", "Yangyang Shi", "Dong Yu"], "venue": "In Interspeech,", "citeRegEx": "Yao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2013}, {"title": "Chinese poetry generation with recurrent neural networks", "author": ["Zhang", "Lapata2014] Xingxing Zhang", "Mirella Lapata"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Structured language models for statistical machine translation", "author": ["Ying Zhang"], "venue": null, "citeRegEx": "Zhang.,? \\Q2009\\E", "shortCiteRegEx": "Zhang.", "year": 2009}, {"title": "A challenge set for advancing language modeling", "author": ["Zweig", "Burges2012] Geoffrey Zweig", "Chris J.C. Burges"], "venue": "In Proceedings of the NAACL-HLT", "citeRegEx": "Zweig et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zweig et al\\.", "year": 2012}, {"title": "Computational approaches to sentence completion", "author": ["Zweig et al.2012] Geoffrey Zweig", "John C. Platt", "Christopher Meek", "Christopher J.C. Burges", "Ainur Yessenalina", "Qiang Liu"], "venue": "In Proceedings of the 50th Annual Meeting of the Association", "citeRegEx": "Zweig et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zweig et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 32, "context": "The literature offers many examples of how to overcome this limitation, such as cache language models (Kuhn and de Mori, 1990), trigger models (Rosenfeld, 1996), and notably structured language models (Chelba et al.", "startOffset": 143, "endOffset": 160}, {"referenceID": 4, "context": "The literature offers many examples of how to overcome this limitation, such as cache language models (Kuhn and de Mori, 1990), trigger models (Rosenfeld, 1996), and notably structured language models (Chelba et al., 1997; Chelba and Jelinek, 2000; Roark, 2001).", "startOffset": 201, "endOffset": 261}, {"referenceID": 31, "context": "The literature offers many examples of how to overcome this limitation, such as cache language models (Kuhn and de Mori, 1990), trigger models (Rosenfeld, 1996), and notably structured language models (Chelba et al., 1997; Chelba and Jelinek, 2000; Roark, 2001).", "startOffset": 201, "endOffset": 261}, {"referenceID": 1, "context": "Previous approaches have mainly employed feed-forward (Bengio et al., 2003; Mnih and Hinton, 2007) and recurrent neural networks (Mikolov et al.", "startOffset": 54, "endOffset": 98}, {"referenceID": 19, "context": ", 2003; Mnih and Hinton, 2007) and recurrent neural networks (Mikolov et al., 2010; Mikolov et al., 2011c; Mikolov et al., 2011b; Auli et al., 2013; Mikolov, 2012) in order to map the feature vectors of the context words to the distribution for the next word.", "startOffset": 61, "endOffset": 163}, {"referenceID": 0, "context": ", 2003; Mnih and Hinton, 2007) and recurrent neural networks (Mikolov et al., 2010; Mikolov et al., 2011c; Mikolov et al., 2011b; Auli et al., 2013; Mikolov, 2012) in order to map the feature vectors of the context words to the distribution for the next word.", "startOffset": 61, "endOffset": 163}, {"referenceID": 26, "context": ", 2003; Mnih and Hinton, 2007) and recurrent neural networks (Mikolov et al., 2010; Mikolov et al., 2011c; Mikolov et al., 2011b; Auli et al., 2013; Mikolov, 2012) in order to map the feature vectors of the context words to the distribution for the next word.", "startOffset": 61, "endOffset": 163}, {"referenceID": 6, "context": "Despite superior performance in a many applications ranging from machine translation (Cho et al., 2014), to speech recognition (Mikolov et al.", "startOffset": 85, "endOffset": 103}, {"referenceID": 5, "context": ", 2014), to speech recognition (Mikolov et al., 2011c; Chen et al., 2015), image description generation (Vinyals et al.", "startOffset": 31, "endOffset": 73}, {"referenceID": 43, "context": ", 2015), image description generation (Vinyals et al., 2015), and language understanding (Yao et al.", "startOffset": 38, "endOffset": 60}, {"referenceID": 44, "context": ", 2015), and language understanding (Yao et al., 2013), standard neural language models essentially predict sequences of words.", "startOffset": 36, "endOffset": 54}, {"referenceID": 40, "context": ", performing machine translation (Sutskever et al., 2014; Cho et al., 2014) or image description (Kiros et al.", "startOffset": 33, "endOffset": 75}, {"referenceID": 6, "context": ", performing machine translation (Sutskever et al., 2014; Cho et al., 2014) or image description (Kiros et al.", "startOffset": 33, "endOffset": 75}, {"referenceID": 15, "context": ", 2014) or image description (Kiros et al., 2014; Vinyals et al., 2015).", "startOffset": 29, "endOffset": 71}, {"referenceID": 43, "context": ", 2014) or image description (Kiros et al., 2014; Vinyals et al., 2015).", "startOffset": 29, "endOffset": 71}, {"referenceID": 13, "context": "However, in practice it has proven difficult due to gradient exploding and gradient vanishing problems (Bengio et al., 1994; Hochreiter, 1998).", "startOffset": 103, "endOffset": 142}, {"referenceID": 30, "context": "Recursive Neural Networks (Pollack, 1990) are a related class of models which operate on structured inputs.", "startOffset": 26, "endOffset": 41}, {"referenceID": 41, "context": "The recently proposed tree-structured long shortterm memory network model (Tai et al., 2015) models sentential meaning whilst taking syntactic structure into account.", "startOffset": 74, "endOffset": 92}, {"referenceID": 36, "context": "Other work develops dependency-based language models for specific applications such as machine translation (Shen et al., 2008; Zhang, 2009; Sennrich, 2015) or sentence completion (Gubbins and Vlachos, 2013).", "startOffset": 107, "endOffset": 155}, {"referenceID": 46, "context": "Other work develops dependency-based language models for specific applications such as machine translation (Shen et al., 2008; Zhang, 2009; Sennrich, 2015) or sentence completion (Gubbins and Vlachos, 2013).", "startOffset": 107, "endOffset": 155}, {"referenceID": 35, "context": "Other work develops dependency-based language models for specific applications such as machine translation (Shen et al., 2008; Zhang, 2009; Sennrich, 2015) or sentence completion (Gubbins and Vlachos, 2013).", "startOffset": 107, "endOffset": 155}, {"referenceID": 1, "context": "(2003) and Sennrich (2015) estimate the parameters of a structured language model using feed-forward neural networks (Bengio et al., 2003).", "startOffset": 117, "endOffset": 138}, {"referenceID": 7, "context": "Emami et al. (2003) and Sennrich (2015) estimate the parameters of a structured language model using feed-forward neural networks (Bengio et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 7, "context": "Emami et al. (2003) and Sennrich (2015) estimate the parameters of a structured language model using feed-forward neural networks (Bengio et al.", "startOffset": 0, "endOffset": 40}, {"referenceID": 19, "context": "Each term in Equation (1) corresponds to the output of a recurrent neural network at one time step (Mikolov et al., 2010).", "startOffset": 99, "endOffset": 121}, {"referenceID": 7, "context": "Our current formulation uses basic recurrent neural networks (Elman, 1990) as a backbone.", "startOffset": 61, "endOffset": 74}, {"referenceID": 42, "context": "For largescale experiments, we employ NCE which does not require repeated summations over the whole vocabulary and has been previously shown to work well for neural language models (Mnih and Teh, 2012; Vaswani et al., 2013).", "startOffset": 181, "endOffset": 223}, {"referenceID": 19, "context": "Class-based methods (Mikolov et al., 2011c) and noise-contrastive estimation (NCE; Gutmann and Hyv\u00e4rinen (2012)) are often used to speedup training time.", "startOffset": 21, "endOffset": 112}, {"referenceID": 42, "context": "frequent than real words (Mnih and Teh, 2012; Vaswani et al., 2013).", "startOffset": 25, "endOffset": 67}, {"referenceID": 42, "context": "5 rather than learn it (Mnih and Teh, 2012; Vaswani et al., 2013; Chen et al., 2015), and use 20 negative samples.", "startOffset": 23, "endOffset": 84}, {"referenceID": 5, "context": "5 rather than learn it (Mnih and Teh, 2012; Vaswani et al., 2013; Chen et al., 2015), and use 20 negative samples.", "startOffset": 23, "endOffset": 84}, {"referenceID": 18, "context": "As in Mikolov et al. (2013b), we use smoothed unigram frequencies (exponentiating by 0.", "startOffset": 6, "endOffset": 29}, {"referenceID": 15, "context": "Previous work has shown that it is relatively straightforward to jointly train neural language models with other types of neural networks to perform tasks involving natural language generation such as translating sentences and generating descriptions for images (Kalchbrenner and Blunsom, 2013; Kiros et al., 2014; Zhang and Lapata, 2014; Sutskever et al., 2014; Cho et al., 2014; Vinyals et al., 2015).", "startOffset": 262, "endOffset": 402}, {"referenceID": 40, "context": "Previous work has shown that it is relatively straightforward to jointly train neural language models with other types of neural networks to perform tasks involving natural language generation such as translating sentences and generating descriptions for images (Kalchbrenner and Blunsom, 2013; Kiros et al., 2014; Zhang and Lapata, 2014; Sutskever et al., 2014; Cho et al., 2014; Vinyals et al., 2015).", "startOffset": 262, "endOffset": 402}, {"referenceID": 6, "context": "Previous work has shown that it is relatively straightforward to jointly train neural language models with other types of neural networks to perform tasks involving natural language generation such as translating sentences and generating descriptions for images (Kalchbrenner and Blunsom, 2013; Kiros et al., 2014; Zhang and Lapata, 2014; Sutskever et al., 2014; Cho et al., 2014; Vinyals et al., 2015).", "startOffset": 262, "endOffset": 402}, {"referenceID": 43, "context": "Previous work has shown that it is relatively straightforward to jointly train neural language models with other types of neural networks to perform tasks involving natural language generation such as translating sentences and generating descriptions for images (Kalchbrenner and Blunsom, 2013; Kiros et al., 2014; Zhang and Lapata, 2014; Sutskever et al., 2014; Cho et al., 2014; Vinyals et al., 2015).", "startOffset": 262, "endOffset": 402}, {"referenceID": 1, "context": "(1993)) and the APNews dataset (Bengio et al., 2003).", "startOffset": 31, "endOffset": 52}, {"referenceID": 17, "context": "2 we perform perplexity experiments on the well-known Penn Treebank (PTB; Marcus et al. (1993)) and the APNews dataset (Bengio et al.", "startOffset": 74, "endOffset": 95}, {"referenceID": 29, "context": "It is well known that RNNs suffer from problems of exploding gradients; as a result, we rescaled the gradient g when the gradient norm ||g|| > 5 and set g = 5g ||g|| (Pascanu et al., 2013; Sutskever et al., 2014).", "startOffset": 166, "endOffset": 212}, {"referenceID": 40, "context": "It is well known that RNNs suffer from problems of exploding gradients; as a result, we rescaled the gradient g when the gradient norm ||g|| > 5 and set g = 5g ||g|| (Pascanu et al., 2013; Sutskever et al., 2014).", "startOffset": 166, "endOffset": 212}, {"referenceID": 17, "context": "Specifically, we used the Stanford CoreNLP toolkit (Manning et al., 2014) to convert gold PTB phrase structure trees to dependencies.", "startOffset": 51, "endOffset": 73}, {"referenceID": 1, "context": "With regard to the APNews corpus (Bengio et al., 2003), the training set contains 14M words (sampled from the APW 1996 partition of the English Gigaword3), the validation set contains 1M words (sampled from APW 199501 to 199506) and the test set contains 1M words (sampled from APW 199507 to 199512).", "startOffset": 33, "endOffset": 54}, {"referenceID": 26, "context": "We generally observe that when models have access to more data, larger hidden units improve performance (Mikolov, 2012).", "startOffset": 104, "endOffset": 119}, {"referenceID": 31, "context": "Secondly, as noted in other syntax-based language modeling work (Chelba and Jelinek, 2000; Roark, 2001; Chelba et al., 1997), in a tree-based model the context used to predict a word is more informative compared to a word-based model.", "startOffset": 64, "endOffset": 124}, {"referenceID": 4, "context": "Secondly, as noted in other syntax-based language modeling work (Chelba and Jelinek, 2000; Roark, 2001; Chelba et al., 1997), in a tree-based model the context used to predict a word is more informative compared to a word-based model.", "startOffset": 64, "endOffset": 124}, {"referenceID": 8, "context": "Although not strictly comparable, Emami et al. (2003), obtain a perplexity of 131 on PTB using a structured language model (Chelba and Jelinek, 2000) and feed-forward neural networks for parameter estimation.", "startOffset": 34, "endOffset": 54}, {"referenceID": 17, "context": "After removing the project Gutenberg headers and footers from the files, we tokenized and parse the dataset into dependency trees with the Stanford Core NLP toolkit (Manning et al., 2014).", "startOffset": 165, "endOffset": 187}, {"referenceID": 47, "context": "Several techniques have been already benchmarked on the MSR sentence completion dataset (Zweig and Burges, 2012; Zweig et al., 2012; Mikolov, 2012; Mnih and Teh, 2012).", "startOffset": 88, "endOffset": 167}, {"referenceID": 26, "context": "Several techniques have been already benchmarked on the MSR sentence completion dataset (Zweig and Burges, 2012; Zweig et al., 2012; Mikolov, 2012; Mnih and Teh, 2012).", "startOffset": 88, "endOffset": 167}, {"referenceID": 26, "context": "These include a modified Kneser-Ney 5-gram model (Mikolov, 2012), LSA (Zweig and Burges, 2012), and two variants of the structured language model presented in Gubbins and Vlachos (2013).", "startOffset": 49, "endOffset": 64}, {"referenceID": 26, "context": "These include a modified Kneser-Ney 5-gram model (Mikolov, 2012), LSA (Zweig and Burges, 2012), and two variants of the structured language model presented in Gubbins and Vlachos (2013). The LSA-based approach performs dimensionality reduction on the training data to obtain a 300-dimensional representation of each word.", "startOffset": 50, "endOffset": 186}, {"referenceID": 26, "context": "These include a modified Kneser-Ney 5-gram model (Mikolov, 2012), LSA (Zweig and Burges, 2012), and two variants of the structured language model presented in Gubbins and Vlachos (2013). The LSA-based approach performs dimensionality reduction on the training data to obtain a 300-dimensional representation of each word. To decide which option to select, the average similarity of the candidate to every other word in the sentence is computed and the word with the greatest overall similarity is selected. The models presented in Gubbins and Vlachos (2013) use", "startOffset": 50, "endOffset": 558}, {"referenceID": 2, "context": "maximum likelihood to estimate the probability of words given labeled and unlabeled dependency paths (LDepNgram and UDepNgram in Table 2) in combination with backoff smoothing (Brants et al., 2007).", "startOffset": 176, "endOffset": 197}, {"referenceID": 19, "context": "The comparison includes Mikolov et al.\u2019s (2011a) recurrent neural network language model (RNN) and a variant which is jointly trained with a maximum entropy model with n-gram features (RNNME).", "startOffset": 24, "endOffset": 49}, {"referenceID": 19, "context": "The comparison includes Mikolov et al.\u2019s (2011a) recurrent neural network language model (RNN) and a variant which is jointly trained with a maximum entropy model with n-gram features (RNNME). The log-bilinear model (LBL; Mnih and Teh (2012)) first predicts the representation of the next word by linearly combining the representations of the context words; the distribution for the next word is computed based on the similarity between the predicted representation and the representations of all words in the vocabulary.", "startOffset": 24, "endOffset": 242}, {"referenceID": 33, "context": "ory (LSTM) units (Hochreiter and Schmidhuber, 1997) have been applied to a variety of sequence modeling and prediction tasks demonstrating results superior to RNNs (Sak et al., 2014; Mikolov et al., 2014).", "startOffset": 164, "endOffset": 204}, {"referenceID": 25, "context": "ory (LSTM) units (Hochreiter and Schmidhuber, 1997) have been applied to a variety of sequence modeling and prediction tasks demonstrating results superior to RNNs (Sak et al., 2014; Mikolov et al., 2014).", "startOffset": 164, "endOffset": 204}, {"referenceID": 9, "context": "Finally, we plan to use our model in a variety of applications such as sentence compression and simplification (Filippova et al., 2015), image description generation (Kiros et al.", "startOffset": 111, "endOffset": 135}, {"referenceID": 15, "context": ", 2015), image description generation (Kiros et al., 2014; Vinyals et al., 2015), and notably machine translation (Sutskever et al.", "startOffset": 38, "endOffset": 80}, {"referenceID": 43, "context": ", 2015), image description generation (Kiros et al., 2014; Vinyals et al., 2015), and notably machine translation (Sutskever et al.", "startOffset": 38, "endOffset": 80}, {"referenceID": 40, "context": ", 2015), and notably machine translation (Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 41, "endOffset": 83}, {"referenceID": 6, "context": ", 2015), and notably machine translation (Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 41, "endOffset": 83}], "year": 2015, "abstractText": "In this paper we develop a recurrent neural network (TreeRNN), which is designed to predict a tree rather than a linear sequence as is the case in conventional recurrent neural networks. Our model defines the probability of a sentence by estimating the generation probability of its dependency tree. We construct the tree incrementally by generating the left and right dependents of a node whose probability is computed using recurrent neural networks with shared hidden layers. Application of our model to two language modeling tasks shows that it outperforms or performs on par with related models.", "creator": "LaTeX with hyperref package"}}}