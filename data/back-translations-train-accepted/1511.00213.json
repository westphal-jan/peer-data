{"id": "1511.00213", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Nov-2015", "title": "Large-scale probabilistic predictors with and without guarantees of validity", "abstract": "This paper studies theoretically and empirically a method of turning machine-learning algorithms into probabilistic predictors that automatically enjoys a property of validity (perfect calibration) and is computationally efficient. The price to pay for perfect calibration is that these probabilistic predictors produce imprecise (in practice, almost precise for large data sets) probabilities. When these imprecise probabilities are merged into precise probabilities, the resulting predictors, while losing the theoretical property of perfect calibration, are consistently more accurate than the existing methods in empirical studies.", "histories": [["v1", "Sun, 1 Nov 2015 07:16:04 GMT  (118kb,D)", "https://arxiv.org/abs/1511.00213v1", "26 pages, 5 figures, to appear in Advances in Neural Information Processing Systems 28 (NIPS 2015)"], ["v2", "Fri, 13 Nov 2015 09:28:34 GMT  (193kb,D)", "http://arxiv.org/abs/1511.00213v2", "38 pages, 14 figures, to appear in Advances in Neural Information Processing Systems 28 (NIPS 2015). As compared with the previous version (v1), the MATLAB code (the 5 files with extension .m) and results of new empirical studies have been added"]], "COMMENTS": "26 pages, 5 figures, to appear in Advances in Neural Information Processing Systems 28 (NIPS 2015)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["vladimir vovk", "ivan petej", "valentina fedorova"], "accepted": true, "id": "1511.00213"}, "pdf": {"name": "1511.00213.pdf", "metadata": {"source": "CRF", "title": "Large-scale probabilistic prediction with and without validity guarantees", "authors": ["Vladimir Vovk", "Ivan Petej", "Valentina Fedorova"], "emails": [], "sections": [{"heading": null, "text": "The conference version of this paper will be published in Advances in Neural Information Processing Systems 28, 2015."}, {"heading": "Contents", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 Introduction 1", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 Inductive Venn\u2013Abers predictors (IVAPs) 2", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 Cross Venn\u2013Abers predictors (CVAPs) 10", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 Making probability predictions out of multiprobability ones 10", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 Comparison with other calibration methods 11", "text": "5.1 Platts method....................................... 11 5.2 Isotonic regression.........................."}, {"heading": "6 Empirical studies 15", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7 Conclusion 34", "text": "References 34"}, {"heading": "1 Introduction", "text": "They are based on the method of isotonic regression [1] and are driven by the observation that isotonic regression is more susceptible to data overmatching when it is scarce. Advantage of Venn-Abers predictors is that they represent a specific case of Venn-Abers predictors."}, {"heading": "2 Inductive Venn\u2013Abers predictors (IVAPs)", "text": "In this thesis we consider data sequences (usually loosely referred to as sets) consisting of observations z = (x, y), each observation consisting of an object x and a label y. We consider only random results. We obtain a training set whose size is referred to as l.This section introduces inductive Venn-Abers predictors. Our main concern is how to implement them efficiently, but as functions, an IVAP is defined in terms of a scoring algorithm (see the last paragraph of the previous section) as follows: \u2022 Divide the training set of size l into two subsets, the correct training set of size m and the calibration set of size k, so that l = m + k. \u2022 Train the scoring algorithm on the correct training set. \u2022 Find scores s1,., sk the calibration objects x1,. \u2022 When a new test object x arrives, we compile its scores."}, {"heading": "Computational details of IVAPs", "text": "The corners of a GCM are the points on the GCM where the inclination of the GCM changes. It is clear that the corners belong to the CSD, and we also add the extreme points (P0 and Pk) in the case of (1)) of the CSD to the list of vertices. We will only explain in detail how F 1 is calculated; the calculation of F 0 is analogous and is explained only briefly. First, we will explain how to calculate F 11. Extend the CSD as defined above (in the case where s1,.., yk and y1,., are the calibration results and labels) by adding the point P \u2212 1 (\u2212 1)."}, {"heading": "11: Push(Pi\u22121, S\u2032)", "text": "The active corner is initially at P-1 = (\u2212 1); the corners to the left of the active corner are irrelevant and are ignored (not in S-11).The active interval is always between the first coordination of top (S-1) and the first coordination of next-to-top (S-1).At each corner it is irrelevant whether the main loop 3-11 we perform the calculation of F-11, i.e., f1 (s) for the situation in which it goes between s-1 and s-1."}, {"heading": "3 Cross Venn\u2013Abers predictors (CVAPs)", "text": "A CVAP is merely a combination of K IVAPs, where K is the parameter of the algorithm. It is described as algorithm 7, where IVAP (A, B, x) stands for the output of IVAP applied to A as the proper training set, B as the calibration set and x as the test object, and GM for the geometric mean (so that GM (p1) is the geometric mean of p11....., pK1 and GM (1 \u2212 p0) is the geometric mean of 1 \u2212 p10,.., 1 \u2212 pK0). The folds should be approximately the same size, and normally the training set is randomly divided into folds (although in section 6 we choose contiguous folds to facilitate reproducibility). One way to obtain a random mapping of the training observations to folds (see line 1) is to proceed from a regular array in which the first L1-last step is grouped (K) to assign the following the last observations < L1."}, {"heading": "4 Making probability predictions out of multiprobability ones", "text": "In CVAP (algorithm 7) we merge the K multiprobability predictions of K IVAPs. In this section we design a minimax path to merge, essentially according to [19]. For the log loss function, the result is particularly simple, GM (p1) / (GM (1 \u2212 p0) + GM (p1).It is noteworthy that the probability interval (1 \u2212 GM (1 \u2212 p0), GM (p1)))) (formally a pair of numbers is narrower than the corresponding interval for the arithmetic mean; this is due to the fact that a geometric mean never exceeds the corresponding arithmetic mean and that we always have p0 < p.Let us check that GM (p1) + GM (p1 \u2212 p0) + GM (p1) + GM (p1) + GM (p1) + GM (p1)."}, {"heading": "5 Comparison with other calibration methods", "text": "The two alternative calibration methods we consider in this paper are Platts [13] and isotonic regression [20]."}, {"heading": "5.1 Platt\u2019s method", "text": "Platt's [13] method uses sigmoidsg (s): = 11 + exp (As + B), where A < 0 and B are parameters to calibrate the results. Platt discusses two approaches: \u2022 run the scoring algorithm and adjust the parameters A and B to complete training, \u2022 or run the scoring algorithm to a subset (called the proper training in this work) and adjust A and B to the rest (calibration set). Platt recommends the second approach, in particular that he is interested in SVM, and for SVM, the results for training clusters tend by \u00b1 1. (In fact, this is also true for the calibration results, as discussed below.) Platt's recommended method of matching A and B is \u2212 k (ti log pi + (1 \u2212 ti) log (1 \u2212 pi) log (1 \u2212 pi)))), (5) where we, in the simplest case, ti:"}, {"heading": "5.2 Isotonic regression", "text": "There are two standard applications of isotonic regression: we can train the scoring algorithm based on what we call a proper training session, and then use the results of the observations in a disjointed calibration (also called validation) specified for calibrating the scores of test objects (as in [3]); alternatively, we can train the scoring algorithm on the full training set and also use the full training set for calibration (it appears that this has happened in [20]), but in both cases we can expect an infinite log loss when the test set becomes large enough. Suppose that we have fixed proper training and calibration sets (not necessarily resolved so that both of the above cases are covered), so that the score s (X) of a random object X is below the smallest score of the calibration objects with positive probability; we also assume that the labels are focused on a random distribution with zero probability."}, {"heading": "6 Empirical studies", "text": "In fact, it is such that it is a matter of a way in which people are able to move, in which they are able, in which they are able, in which they are able, in which they are able, in which they are able, in which they are able, in which they are able, in which they are able, in which they are able, in which they are able, in which they are able, in which they are able, in which they are able, in which they are living, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they are able"}, {"heading": "Adult: log loss", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Adult: Brier loss", "text": "We then use the trained algorithm to generate the values for the calibration and the test sets, which allows us to calculate probability forecasts using Platt's method, isotonic regression, IVAP and CVAP. All values except SVM are already in the range [0, 1] and can be used as probability forecasts. Most parameters are set to their default values, and the only parameters to be optimized are C (circumcision of confidence) for J48 and J48, R (ridge) for logistic regression, L (learning rate) and M (impulse) for neural networks (multilayer perceptron) and C (complexity constant) for SVM (SMO, with linear core); naive bayes do not include parameters for logistic regression. Note that none of these parameters is \"hyperparameter\" as they do not provide the flexibility of the pre-dimensioning function we directly control for the calculation rule."}, {"heading": "Additional experimental results", "text": "Figures 3 and 4 show our results for the covertype dataset (available from the UCI repository and also known as the forest).In converting this multilateral classification problem into the binary we follow [3]: the largest class as 1 and the rest as 0, and the values throughout the column are still unstable. Similar results in Insurance, Spambase and the remaining 25, 000 as a test set. CVAP results are still at the lower end of the spectrum and the column."}, {"heading": "Covertype: log loss", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Covertype: Brier loss", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Insurance: log loss", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Insurance: Brier loss", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Bank Marketing: log loss", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Bank Marketing: Brier loss", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Spambase: log loss", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Spambase: Brier loss", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Statlog (German Credit): log loss", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Statlog (German Credit): Brier loss", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Adult: log loss", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Adult: Brier loss", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7 Conclusion", "text": "This paper presents two new computationally efficient probabilistic prediction algorithms, IVAP, which can be considered a regulated form of calibration method based on isotonic regression, and CVAP, which builds on IVAP and uses the idea of cross-validation. While IVAPs are automatically perfectly calibrated, the advantage of CVAPs lies in their good empirical performance. In this paper, the higher and lower probabilities generated by IVAPs and CVAPs are not empirically examined, whereas the distance between them provides information on the reliability of the merged probability prediction."}, {"heading": "Acknowledgments", "text": "We would like to thank the conference reviewers for numerous helpful comments and observations, Vladimir Vapnik for his ideas on the use of synergies between different learning algorithms, and the participants of the conference Machine Learning: Prospects and Applications (October 2015, Berlin) for their questions and comments. EPSRC (funding programme EP / K033344 / 1) and AFOSR (funding programme \"Semantic Completions\") supported the first author in part, and the second and third authors are grateful to their home institutions for funding their trips to Montre \u0301 al to participate in NIPS 2015."}], "references": [{"title": "An empirical distribution function for sampling with incomplete information", "author": ["Miriam Ayer", "H. Daniel Brunk", "George M. Ewing", "W.T. Reid", "Edward Silverman"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1955}, {"title": "Statistical Inference under Order Restrictions: The Theory and Application of Isotonic Regression", "author": ["Richard E. Barlow", "D.J. Bartholomew", "J.M. Bremner", "H. Daniel Brunk"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1972}, {"title": "An empirical comparison of supervised learning algorithms", "author": ["Rich Caruana", "Alexandru Niculescu-Mizil"], "venue": "In Proceedings of the Twenty Third International Conference on Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Introduction to Algorithms", "author": ["Thomas H. Cormen", "Charles E. Leiserson", "Ronald L. Rivest", "Clifford Stein"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "An efficient algorithm for determining the convex hull of a finite planar set", "author": ["Ronald L. Graham"], "venue": "Information Processing Letters,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1972}, {"title": "The WEKA data mining software: an update", "author": ["Mark Hall", "Eibe Frank", "Geoffrey Holmes", "Bernhard Pfahringer", "Peter Reutemann", "Ian H. Witten"], "venue": "SIGKDD Explorations,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Smooth isotonic regression: a new method to calibrate predictive models", "author": ["Xiaoqian Jiang", "Melanie Osl", "Jihoon Kim", "Lucila Ohno-Machado"], "venue": "AMIA Summits on Translational Science Proceedings,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Reliable probability estimates based on support vector machines for large multiclass datasets", "author": ["Antonis Lambrou", "Harris Papadopoulos", "Ilia Nouretdinov", "Alex Gammerman"], "venue": "Proceedings of the AIAI 2012 Workshop on Conformal Prediction and its Applications,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "The Min-Max algorithm and isotonic regression", "author": ["Chu-In Charles Lee"], "venue": "Annals of Statistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1983}, {"title": "A new vector partition of the probability score", "author": ["Allan H. Murphy"], "venue": "Journal of Applied Meteorology,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1973}, {"title": "Probabilities for SV machines", "author": ["John C. Platt"], "venue": "Advances in Large Margin Classifiers,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2000}, {"title": "On the influence of the kernel on the consistency of support vector machines", "author": ["Ingo Steinwart"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2001}, {"title": "Support Vector Machines", "author": ["Ingo Steinwart", "Andreas Christmann"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Intelligent learning: Similarity control and knowledge transfer", "author": ["Vladimir N. Vapnik"], "venue": "Talk at the 2015 Yandex School of Data Analysis Conference Machine Learning: Prospects and Applications,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "The fundamental nature of the log loss function", "author": ["Vladimir Vovk"], "venue": "editors, Fields of Logic and Computation II: Essays Dedicated to Yuri Gurevich on the Occasion of His 75th Birthday,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Algorithmic Learning in a Random World", "author": ["Vladimir Vovk", "Alex Gammerman", "Glenn Shafer"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Venn\u2013Abers predictors, On-line Compression Modelling project (New Series), http://alrw.net", "author": ["Vladimir Vovk", "Ivan Petej"], "venue": "Working Paper", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Obtaining calibrated probability estimates from decision trees and naive Bayesian classifiers", "author": ["Bianca Zadrozny", "Charles Elkan"], "venue": "Proceedings of the Eighteenth International Conference on Machine Learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2001}, {"title": "Statistical behavior and consistency of classification methods based on convex risk minimization", "author": ["Tong Zhang"], "venue": "Annals of Statistics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}], "referenceMentions": [{"referenceID": 16, "context": "Prediction algorithms studied in this paper belong to the class of Venn\u2013Abers predictors, introduced in [19].", "startOffset": 104, "endOffset": 108}, {"referenceID": 0, "context": "They are based on the method of isotonic regression [1] and prompted by the observation that when applied in machine learning the method of isotonic regression often produces miscalibrated probability predictions (see, e.", "startOffset": 52, "endOffset": 55}, {"referenceID": 6, "context": ", [8, 9]); it has also been reported ([3], Section 1) that isotonic regression is more prone to overfitting than Platt\u2019s scaling [13] when data is scarce.", "startOffset": 2, "endOffset": 8}, {"referenceID": 7, "context": ", [8, 9]); it has also been reported ([3], Section 1) that isotonic regression is more prone to overfitting than Platt\u2019s scaling [13] when data is scarce.", "startOffset": 2, "endOffset": 8}, {"referenceID": 2, "context": ", [8, 9]); it has also been reported ([3], Section 1) that isotonic regression is more prone to overfitting than Platt\u2019s scaling [13] when data is scarce.", "startOffset": 38, "endOffset": 41}, {"referenceID": 10, "context": ", [8, 9]); it has also been reported ([3], Section 1) that isotonic regression is more prone to overfitting than Platt\u2019s scaling [13] when data is scarce.", "startOffset": 129, "endOffset": 133}, {"referenceID": 15, "context": "The advantage of Venn\u2013Abers predictors is that they are a special case of Venn predictors ([18], Chapter 6), and so ([18], Theorem 6.", "startOffset": 91, "endOffset": 95}, {"referenceID": 15, "context": "The advantage of Venn\u2013Abers predictors is that they are a special case of Venn predictors ([18], Chapter 6), and so ([18], Theorem 6.", "startOffset": 117, "endOffset": 121}, {"referenceID": 17, "context": "They can be considered to be a regularized version of the procedure used by [20], which helps them resist overfitting.", "startOffset": 76, "endOffset": 80}, {"referenceID": 15, "context": "The main desiderata for Venn (and related conformal, [18], Chapter 2) predictors are validity, predictive efficiency, and computational efficiency.", "startOffset": 53, "endOffset": 57}, {"referenceID": 0, "context": "As precise probabilistic predictors, IVAPs and CVAPs are ways of converting the scores for test objects into numbers in the range [0, 1] that", "startOffset": 130, "endOffset": 136}, {"referenceID": 10, "context": "In Section 5 we discuss two existing calibration methods, Platt\u2019s [13] and the method [20] based on isotonic regression, and compare them with IVAPs and CVAPs theoretically.", "startOffset": 66, "endOffset": 70}, {"referenceID": 17, "context": "In Section 5 we discuss two existing calibration methods, Platt\u2019s [13] and the method [20] based on isotonic regression, and compare them with IVAPs and CVAPs theoretically.", "startOffset": 86, "endOffset": 90}, {"referenceID": 16, "context": "First we state formally the property of validity of IVAPs (adapting the approach of [19] to IVAPs).", "startOffset": 84, "endOffset": 88}, {"referenceID": 0, "context": "A random variable P taking values in [0, 1] is perfectly calibrated (as a predictor) for a random variable Y taking values in {0, 1} if E(Y | P ) = P a.", "startOffset": 37, "endOffset": 43}, {"referenceID": 1, "context": "Proofs of both statements rely on the geometric representation of isotonic regression as the slope of the GCM (greatest convex minorant) of the CSD (cumulative sum diagram): see [2], pages 9\u201313 (especially Theorem 1.", "startOffset": 178, "endOffset": 181}, {"referenceID": 17, "context": ", (sk, yk) is defined to be the slope of the GCM between \u2211i\u22121 j=1 wj and \u2211i j=1 wj ; the values at other s are somewhat arbitrary (namely, the value at s \u2208 (si, si+1) can be set to anything between the left and right slopes of the GCM at \u2211i j=1 wj) but are never needed in this paper (unlike in the standard use of isotonic regression in machine learning, [20]): e.", "startOffset": 356, "endOffset": 360}, {"referenceID": 1, "context": ", [2], Section 2.", "startOffset": 2, "endOffset": 5}, {"referenceID": 8, "context": "3 (although one of the algorithms described in that section, the Minimax Order Algorithm, was later shown to be defective [10, 12]).", "startOffset": 122, "endOffset": 130}, {"referenceID": 13, "context": ") The importance of partially ordered scores stems from the fact that they enable us to benefit from a possible \u201csynergy\u201d between two or more prediction algorithms [16].", "startOffset": 164, "endOffset": 168}, {"referenceID": 13, "context": "Preliminary results reported in [16] in a related context suggest that the resulting predictor can outperform predictors based on the individual scalar scores.", "startOffset": 32, "endOffset": 36}, {"referenceID": 4, "context": "Algorithm 1, which operates with a stack S (initially empty), computes the corners; it is a trivial modification of Graham\u2019s scan ([6]; [4], Section 33.", "startOffset": 131, "endOffset": 134}, {"referenceID": 3, "context": "Algorithm 1, which operates with a stack S (initially empty), computes the corners; it is a trivial modification of Graham\u2019s scan ([6]; [4], Section 33.", "startOffset": 136, "endOffset": 139}, {"referenceID": 3, "context": ", [4], Section 33.", "startOffset": 2, "endOffset": 5}, {"referenceID": 3, "context": "In the case of Algorithm 1, see [4], Section 33.", "startOffset": 32, "endOffset": 35}, {"referenceID": 3, "context": "[4], Exercise 2.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "O(log k) ([4], Chapter 13).", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "Remember that the procedure Randomize-in-Place ([4], Section 5.", "startOffset": 48, "endOffset": 51}, {"referenceID": 16, "context": "In this section we design a minimax way for merging them, essentially following [19].", "startOffset": 80, "endOffset": 84}, {"referenceID": 16, "context": "This expression is more natural than it looks: see [19], the discussion after (11); notice that it reduces to arithmetic mean when p0 = p1.", "startOffset": 51, "endOffset": 55}, {"referenceID": 10, "context": "The two alternative calibration methods that we consider in this paper are Platt\u2019s [13] and isotonic regression [20].", "startOffset": 83, "endOffset": 87}, {"referenceID": 17, "context": "The two alternative calibration methods that we consider in this paper are Platt\u2019s [13] and isotonic regression [20].", "startOffset": 112, "endOffset": 116}, {"referenceID": 10, "context": "1 Platt\u2019s method Platt\u2019s [13] method uses sigmoids", "startOffset": 25, "endOffset": 29}, {"referenceID": 18, "context": "Zhang [21] (Section 3.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": "3) observes that in the case of SVM and universal [14] kernels the scores tend to cluster around \u00b11 at \u201cnon-trivial\u201d objects, i.", "startOffset": 50, "endOffset": 54}, {"referenceID": 12, "context": "where \u03c6(v) := (1\u2212 v) and H is a universal RKHS ([15], Definition 4.", "startOffset": 48, "endOffset": 52}, {"referenceID": 18, "context": "4 in [21] for a natural class of universal kernels related to neural networks.", "startOffset": 5, "endOffset": 9}, {"referenceID": 12, "context": "1 in [15].", "startOffset": 5, "endOffset": 9}, {"referenceID": 0, "context": "For illustration, suppose \u03b7 := \u03b7(X) is distributed uniformly in [0, 1].", "startOffset": 64, "endOffset": 70}, {"referenceID": 2, "context": "2 Isotonic regression There are two standard uses of isotonic regression: we can train the scoring algorithm using what we call a proper training set, and then use the scores of the observations in a disjoint calibration (also called validation) set for calibrating the scores of test objects (as in [3]); alternatively, we can train the scoring algorithm on the full training set and also use the full training set for calibration", "startOffset": 300, "endOffset": 303}, {"referenceID": 17, "context": "(it appears that this was done in [20]).", "startOffset": 34, "endOffset": 38}, {"referenceID": 14, "context": ", [17]) that we use in our empirical studies is the log loss", "startOffset": 2, "endOffset": 6}, {"referenceID": 0, "context": "where log is binary logarithm, p \u2208 [0, 1] is a probability prediction, and y \u2208 {0, 1} is the true label.", "startOffset": 35, "endOffset": 41}, {"referenceID": 9, "context": ", [11] for the most standard decomposition of the latter into the sum of the calibration error and refinement error).", "startOffset": 2, "endOffset": 6}, {"referenceID": 10, "context": ", IVAPs whose outputs are replaced by probability predictions, as explained in Section 4) and CVAPs with Platt\u2019s method [13] and the standard method [20] based on isotonic regression; the latter two will be referred to as \u201cPlatt\u201d and \u201cIsotonic\u201d in our tables and figures.", "startOffset": 120, "endOffset": 124}, {"referenceID": 17, "context": ", IVAPs whose outputs are replaced by probability predictions, as explained in Section 4) and CVAPs with Platt\u2019s method [13] and the standard method [20] based on isotonic regression; the latter two will be referred to as \u201cPlatt\u201d and \u201cIsotonic\u201d in our tables and figures.", "startOffset": 149, "endOffset": 153}, {"referenceID": 16, "context": ") We use the same underlying algorithms as in [19], namely J48 decision trees (abbreviated to \u201cJ48\u201d), J48 decision trees with bagging (\u201cJ48 bagging\u201d), logistic regression (sometimes abbreviated to \u201clogistic\u201d), naive Bayes, neural networks, and support vector machines (SVM), as implemented in Weka [7] (University of Waikato, New Zealand).", "startOffset": 46, "endOffset": 50}, {"referenceID": 5, "context": ") We use the same underlying algorithms as in [19], namely J48 decision trees (abbreviated to \u201cJ48\u201d), J48 decision trees with bagging (\u201cJ48 bagging\u201d), logistic regression (sometimes abbreviated to \u201clogistic\u201d), naive Bayes, neural networks, and support vector machines (SVM), as implemented in Weka [7] (University of Waikato, New Zealand).", "startOffset": 298, "endOffset": 301}, {"referenceID": 0, "context": "The underlying algorithms (except for SVM) produce scores in the interval [0, 1], which can be used directly as probability predictions (referred to as \u201cUnderlying\u201d in our tables and figures) or can be calibrated using the methods of [13, 20] or the methods proposed in this paper (\u201cIVAP\u201d or \u201cCVAP\u201d in the tables and figures).", "startOffset": 74, "endOffset": 80}, {"referenceID": 10, "context": "The underlying algorithms (except for SVM) produce scores in the interval [0, 1], which can be used directly as probability predictions (referred to as \u201cUnderlying\u201d in our tables and figures) or can be calibrated using the methods of [13, 20] or the methods proposed in this paper (\u201cIVAP\u201d or \u201cCVAP\u201d in the tables and figures).", "startOffset": 234, "endOffset": 242}, {"referenceID": 17, "context": "The underlying algorithms (except for SVM) produce scores in the interval [0, 1], which can be used directly as probability predictions (referred to as \u201cUnderlying\u201d in our tables and figures) or can be calibrated using the methods of [13, 20] or the methods proposed in this paper (\u201cIVAP\u201d or \u201cCVAP\u201d in the tables and figures).", "startOffset": 234, "endOffset": 242}, {"referenceID": 10, "context": "We start our empirical studies with the adult data set available from the UCI repository [5] (this is the main data set used in [13] and one of the data sets used in [20]); however, as we will see later, the picture that we observe is typical for other data sets as well.", "startOffset": 128, "endOffset": 132}, {"referenceID": 17, "context": "We start our empirical studies with the adult data set available from the UCI repository [5] (this is the main data set used in [13] and one of the data sets used in [20]); however, as we will see later, the picture that we observe is typical for other data sets as well.", "startOffset": 166, "endOffset": 170}, {"referenceID": 16, "context": "(In our official definition of IVAP we require that the last two sets be disjoint, but in this section we continue to refer to IVAPs modified in this way simply as IVAPs; in [19], such prediction algorithms were referred to as SVAPs, simplified Venn\u2013Abers predictors.", "startOffset": 174, "endOffset": 178}, {"referenceID": 2, "context": ") Using the full training set as both the proper training set and calibration set might appear naive (and is never used in the extensive empirical study [3]), but it often leads to good empirical results on larger data sets.", "startOffset": 153, "endOffset": 156}, {"referenceID": 2, "context": "In Tables 1 and 2 we apply the four calibration methods and six underlying algorithms to a much smaller training set, namely to the first 5, 000 observations of the adult data set as the new training set, following [3]; the first 4, 000 training observations are used as the proper training set, the following 1, 000 training observations as the calibration set, and all other observations (the remaining training and all test observations) are used as the new test set.", "startOffset": 215, "endOffset": 218}, {"referenceID": 16, "context": "Results for nine very small data sets are given in Tables 1 and 2 of [19], where the results for IVAP (with the full training set used as both proper training and calibration sets, labelled \u201cSVA\u201d in the tables in [19]) are consistently (in 52 cases out of the 54 using Brier loss) better, usually significantly better, than for isotonic regression (referred to as DIR in the tables in [19]).", "startOffset": 69, "endOffset": 73}, {"referenceID": 16, "context": "Results for nine very small data sets are given in Tables 1 and 2 of [19], where the results for IVAP (with the full training set used as both proper training and calibration sets, labelled \u201cSVA\u201d in the tables in [19]) are consistently (in 52 cases out of the 54 using Brier loss) better, usually significantly better, than for isotonic regression (referred to as DIR in the tables in [19]).", "startOffset": 213, "endOffset": 217}, {"referenceID": 16, "context": "Results for nine very small data sets are given in Tables 1 and 2 of [19], where the results for IVAP (with the full training set used as both proper training and calibration sets, labelled \u201cSVA\u201d in the tables in [19]) are consistently (in 52 cases out of the 54 using Brier loss) better, usually significantly better, than for isotonic regression (referred to as DIR in the tables in [19]).", "startOffset": 385, "endOffset": 389}, {"referenceID": 0, "context": "All the scores apart from SVM are already in the [0, 1] range and can be used as probability predictions.", "startOffset": 49, "endOffset": 55}, {"referenceID": 2, "context": "In converting this multiclass classification problem to binary we follow [3]: treat the largest class as 1 and the rest as 0, and only consider a random and randomly permuted subset consisting of 30, 000 observations; the first 5000 of those observations are used as the training set and the remaining 25, 000 as the test set.", "startOffset": 73, "endOffset": 76}], "year": 2015, "abstractText": "This paper studies theoretically and empirically a method of turning machinelearning algorithms into probabilistic predictors that automatically enjoys a property of validity (perfect calibration) and is computationally efficient. The price to pay for perfect calibration is that these probabilistic predictors produce imprecise (in practice, almost precise for large data sets) probabilities. When these imprecise probabilities are merged into precise probabilities, the resulting predictors, while losing the theoretical property of perfect calibration, are consistently more accurate than the existing methods in empirical studies. The conference version of this paper is to appear in Advances in Neural Information Processing Systems 28, 2015.", "creator": "LaTeX with hyperref package"}}}