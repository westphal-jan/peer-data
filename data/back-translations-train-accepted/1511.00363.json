{"id": "1511.00363", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Nov-2015", "title": "BinaryConnect: Training Deep Neural Networks with binary weights during propagations", "abstract": "Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide range of tasks, with the best results obtained with large training sets and large models. In the past, GPUs enabled these breakthroughs because of their greater computational speed. In the future, faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices. As a result, there is much interest in research and development of dedicated hardware for Deep Learning (DL). Binary weights, i.e., weights which are constrained to only two possible values (e.g. -1 or 1), would bring great benefits to specialized DL hardware by replacing many multiply-accumulate operations by simple accumulations, as multipliers are the most space and power-hungry components of the digital implementation of neural networks. We introduce BinaryConnect, a method which consists in training a DNN with binary weights during the forward and backward propagations, while retaining precision of the stored weights in which gradients are accumulated. Like other dropout schemes, we show that BinaryConnect acts as regularizer and we obtain near state-of-the-art results with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.", "histories": [["v1", "Mon, 2 Nov 2015 02:50:05 GMT  (1190kb,D)", "http://arxiv.org/abs/1511.00363v1", "Accepted at NIPS 2015, 9 pages, 3 figures"], ["v2", "Thu, 12 Nov 2015 23:31:09 GMT  (1190kb,D)", "http://arxiv.org/abs/1511.00363v2", "Accepted at NIPS 2015, 9 pages, 3 figures"], ["v3", "Mon, 18 Apr 2016 13:11:45 GMT  (1190kb,D)", "http://arxiv.org/abs/1511.00363v3", "Accepted at NIPS 2015, 9 pages, 3 figures"]], "COMMENTS": "Accepted at NIPS 2015, 9 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["matthieu courbariaux", "yoshua bengio", "jean-pierre david"], "accepted": true, "id": "1511.00363"}, "pdf": {"name": "1511.00363.pdf", "metadata": {"source": "CRF", "title": "BinaryConnect: Training Deep Neural Networks with binary weights during propagations", "authors": ["Matthieu Courbariaux", "Yoshua Bengio"], "emails": ["matthieu.courbariaux@polymtl.ca", "yoshua.bengio@gmail.com", "jean-pierre.david@polymtl.ca"], "sections": [{"heading": "1 Introduction", "text": "It is time for the world to move to another world where people are the centre of attention."}, {"heading": "2 BinaryConnect", "text": "In this section, we give a more detailed insight into BinaryConnect by considering which two values we choose, how we discredit, how we train, and how we draw conclusions."}, {"heading": "2.1 +1 or \u22121", "text": "The application of a DNN essentially consists of coils and matrix multiplications. Thus, the most important arithmetic operation of DL is the multiplier accumulation process. Artificial neurons are basically multipliers that calculate weighted sums of their inputs. BinaryConnect limits the weights during propagation to either + 1 or \u2212 1. As a result, many multiplier accumulation processes are replaced by simple additions (and subtractions), which is a huge gain, since fixed point addition is much cheaper in terms of both area and energy than fixed point multiplier accumulators [22]."}, {"heading": "2.2 Deterministic vs stochastic binarization", "text": "(1) 1https: / / github.com / MatthieuCourbariaux / BinaryConnectWhere wb is the binary weight and w is the real weight. Although this is a deterministic operation, averaging this discretization over the many input weights of a hidden unit could compensate for the loss of information. An alternative that allows a finer and more correct mean process is stochastic binarization: wb = {+ 1 with probability p = \u03c3 (w), \u2212 1 with probability 1 \u2212 p. (2) where the hard sigmoid function is the \"hard sigmoid\" function: \u03c3 (x) = Clip (x + 12, 0, 1) = max (0, min (1, + 12)."}, {"heading": "2.3 Propagations vs updates", "text": "We look at the different steps of backward propagation with SGD udpates and whether or not it makes sense to discredit the weight differences between the individual stages. \u2212 Do we look at the DNN input factors? \u2212 Do we look at the individual layers, which range from the top layer to the first hidden layer. This step is called the backward propagation or backward phase of backward propagation. \u2212 Let's look at the gradients w.r.t. Each layer is updated from the top layer to the first hidden layer. This step is called the backward propagation or backward phase of backward propagation. \u2212 Let's look at the gradients w.r.t. Each layer is then updated with its calculated gradients and its previous values. This step is called Parameter update.Algorithm 1 SGD Training with BinaryConnect."}, {"heading": "2.4 Clipping", "text": "Since the binarization operation is not affected by variations in the real weights w if its size exceeds the binary values \u00b1 1, and since it is a common practice to bind weights (usually the weight vector) to regulate them, we have decided to fix the real weights within the [\u2212 1,1] interval immediately after the weight updates, according to algorithm 1. Otherwise, the real weights would become very large without having any influence on the binary weights."}, {"heading": "2.5 A few more tricks", "text": "We use batch normalization (BN) [26] in all our experiments, not only because it speeds up training by reducing internal covariate shift, but also because it reduces the overall impact of the weight scale. Furthermore, we use the ADAM learning rule [27] in all our CNN experiments. Last but not least, we scale weight learning rates according to the weight initialization coefficients of [25] when optimizing with ADAM, and the squares of these coefficients when optimizing with SGD or Nesterov dynamics [28]. Table 1 illustrates the effectiveness of these tricks."}, {"heading": "2.6 Test-Time Inference", "text": "We considered three sensible alternatives: 1. Use the resulting binary weights wb (which is most useful in the deterministic form of BinaryConnect).2. Use the real weights w, i.e. binarization only helps to achieve faster training, but not faster test time results. 3. In the stochastic case, many different networks can be covered by scanning one wb for each weight according to Equation 2. The overall results of these networks can then be obtained by averaging the results of individual networks. We use the first method with the deterministic form of BinaryConnect. As for the stochastic form of BinaryConnect, we focused on the training advantage and used the second method in the experiments, i.e., inference with the deterministic form of BinaryConnect, which removes the real-time weights. \""}, {"heading": "3 Benchmark results", "text": "In this section, we show that BinaryConnect acts as a regulator and that BinaryConnect provides us with almost the latest results on the permutation invariants MNIST, CIFAR-10 and SVHN."}, {"heading": "3.1 Permutation-invariant MNIST", "text": "MNIST is a benchmark image classification dataset [33]. It consists of a training set of 60000 and a test set of 10000 28 x 28 grayscale images, digits from 0 to 9. Permutation invariance means that the model does not need to know the image (2-D) structure of the data (in other words, CNNs are forbidden), and we do not use data augmentation, pre-processing or unattended pre-training. The MLP we train on MNIST consists of 3 hidden layers of 1024 rectifier linear units (ReLU) [34, 24, 3] and an L2 SVM output layer (L2-SVM has proven to be better than Softmax on multiple classification benchmarks [30, 32])."}, {"heading": "3.2 CIFAR-10", "text": "CIFAR-10 is a benchmark image classification dataset. It consists of a training set of 50,000 and a test set of 10,000 32 x 32 color images representing airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships and trucks. We process the data using global contrast normalization and ZCA whitening. We do not use data magnification (which can really be a game changer for this dataset [35]). The architecture of our CNN is: (2 x 128C3) \u2212 MP2 \u2212 (2 x 256C3) \u2212 MP2 \u2212 (2 x 512C3) \u2212 MP2 \u2212 (2 x 1024FC) \u2212 10SVM (5) Where C3 is a 3 x 3 ReLU revolution layer, MP2 is a 2 x 2 max pooling layer, FC is a fully connected layer and SVM is an L2 SVM output layer. This architecture is strongly inspired by validation rate [36] associated with a training speed."}, {"heading": "3.3 SVHN", "text": "SVHN is a benchmark data set for image classification. It consists of a training set of 604K and a test set of 26K 32 x 32 color images, the digits between 0 and 9. We follow the same procedure we used for CIFAR-10, with a few notable exceptions: We use half of the hidden units and we train for 200 epochs instead of 500 (because SVHN is a fairly large data set). The results are listed in Table 2."}, {"heading": "4 Related works", "text": "The formation of DNNs with binary weights has been the subject of very recent work [37, 38, 39, 40]. Although we share the same goal, our approaches are quite different. [37, 38] train their DNN not with back propagation (BP), but with a variant called Expectation Backpropagation (EBP). EBP is based on Expectation Propagation (EP) [41], a varied Bayes method used to draw conclusions in probable graphical models. Let's compare their method with ours: \u2022 It optimizes the weights posterior distribution (which is non-binary). In this respect, our method is quite similar in that we maintain a real version of the weights. \u2022 It binarizes both the neuron issues and the weights, which is more hardware-friendly than just the binarization of the weights. \u2022 It provides good classification accuracy for fully connected networks (on MNIST), but not (yet) for forward-weights, with three neural networks."}, {"heading": "5 Conclusion and future works", "text": "We have shown that it is possible to train DNNs with BinaryConnect on the permutation-invariant MNIST, CIFAR-10, and SVHN datasets and achieve near-state-of-the-art results, and the impact of such a method on specialized hardware deployments of deep networks could be significant by eliminating the need for about 2 / 3 of the multiplications, potentially allowing a 3-fold acceleration at training time. With the deterministic version of BinaryConnect, the impact on test time could be even more important by eliminating the total multiplications and reducing the memory requirements of deep networks by a factor of at least 16 (from 16-bit single-float precision to single-bit precision)."}, {"heading": "6 Acknowledgments", "text": "We thank the reviewers for their many constructive comments. We also thank Roland Memisevic for helpful discussions. We thank the developers of Theano [42, 43], a Python library that enabled us to easily develop fast and optimized code for GPU. We also thank the developers of Pylearn2 [44] and Lasagne, two deep learning libraries based on Theano. We are also grateful for the support of NSERC, the Canada Research Chairs, Compute Canada and CIFAR."}], "references": [{"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["Geoffrey Hinton", "Li Deng", "George E. Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara Sainath", "Brian Kingsbury"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Deep convolutional neural networks for LVCSR", "author": ["Tara Sainath", "Abdel rahman Mohamed", "Brian Kingsbury", "Bhuvana Ramabhadran"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "In NIPS\u20192012", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "Technical report,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard Schwartz", "John Makhoul"], "venue": "In Proc. ACL\u20192014,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "In NIPS\u20192014,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In ICLR\u20192015,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Large-scale deep unsupervised learning using graphics processors", "author": ["Rajat Raina", "Anand Madhavan", "Andrew Y. Ng"], "venue": "In ICML\u20192009,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G.S Corrado", "R. Monga", "K. Chen", "M. Devin", "Q.V. Le", "M.Z. Mao", "M.A. Ranzato", "A. Senior", "P. Tucker", "K. Yang", "A.Y. Ng"], "venue": "In NIPS\u20192012,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "A highly scalable restricted Boltzmann machine FPGA implementation", "author": ["Sang Kyun Kim", "Lawrence C McAfee", "Peter Leonard McMahon", "Kunle Olukotun"], "venue": "In Field Programmable Logic and Applications,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Diannao: A small-footprint high-throughput accelerator for ubiquitous machine-learning", "author": ["Tianshi Chen", "Zidong Du", "Ninghui Sun", "Jia Wang", "Chengyong Wu", "Yunji Chen", "Olivier Temam"], "venue": "In Proceedings of the 19th international conference on Architectural support for programming languages and operating systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Dadiannao: A machine-learning supercomputer", "author": ["Yunji Chen", "Tao Luo", "Shaoli Liu", "Shijin Zhang", "Liqiang He", "Jia Wang", "Ling Li", "Tianshi Chen", "Zhiwei Xu", "Ninghui Sun"], "venue": "In Microarchitecture (MICRO),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Rounding methods for neural networks with low resolution synaptic weights", "author": ["Lorenz K Muller", "Giacomo Indiveri"], "venue": "arXiv preprint arXiv:1504.05767,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Deep learning with limited numerical precision", "author": ["Suyog Gupta", "Ankur Agrawal", "Kailash Gopalakrishnan", "Pritish Narayanan"], "venue": "In ICML\u20192015,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Low precision arithmetic for deep learning", "author": ["Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David"], "venue": "In Arxiv:1412.7024,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Hippocampal spine head sizes are highly precise", "author": ["Thomas M Bartol", "Cailey Bromer", "Justin P Kinney", "Michael A Chirillo", "Jennifer N Bourne", "Kristen M Harris", "Terrence J Sejnowski"], "venue": "bioRxiv,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Practical variational inference for neural networks", "author": ["Alex Graves"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Improving neural networks with dropout", "author": ["Nitish Srivastava"], "venue": "Master\u2019s thesis, U. Toronto,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Regularization of neural networks using dropconnect", "author": ["Li Wan", "Matthew Zeiler", "Sixin Zhang", "Yann LeCun", "Rob Fergus"], "venue": "In ICML\u20192013,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Hardware complexity of modular multiplication and exponentiation", "author": ["J.P. David", "K. Kalach", "N. Tittley"], "venue": "Computers, IEEE Transactions on,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Large Scale Machine Learning", "author": ["R. Collobert"], "venue": "PhD thesis, Universite\u0301 de Paris VI,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2004}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In AISTATS\u20192011,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In AISTATS\u20192010,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "A method for unconstrained convex minimization problem with the rate of convergence o(1/k)", "author": ["Yu Nesterov"], "venue": "Doklady AN SSSR (translated as Soviet. Math. Docl.),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1983}, {"title": "Deep learning using linear support vector machines", "author": ["Yichuan Tang"], "venue": "Workshop on Challenges in Representation Learning,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "Leon Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1998}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In ICML\u20192010,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2010}, {"title": "Spatially-sparse convolutional neural networks", "author": ["Benjamin Graham"], "venue": "arXiv preprint arXiv:1409.6070,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "In ICLR,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Expectation backpropagation: Parameter-free training of multilayer neural networks with continuous or discrete weights", "author": ["Daniel Soudry", "Itay Hubara", "Ron Meir"], "venue": "In NIPS\u20192014,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}, {"title": "Training binary multilayer neural networks for image classification using expectation backpropgation", "author": ["Zhiyong Cheng", "Daniel Soudry", "Zexi Mao", "Zhenzhong Lan"], "venue": "arXiv preprint arXiv:1503.03562,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Fixed-point feedforward deep neural network design using weights+", "author": ["Kyuyeon Hwang", "Wonyong Sung"], "venue": "In Signal Processing Systems (SiPS),", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2014}, {"title": "X1000 real-time phoneme recognition vlsi using feed-forward deep neural networks", "author": ["Jonghong Kim", "Kyuyeon Hwang", "Wonyong Sung"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Expectation propagation for approximate bayesian inference", "author": ["Thomas P Minka"], "venue": "In UAI\u20192001,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2001}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2010}, {"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2012}, {"title": "Pylearn2: a machine learning research", "author": ["Ian J. Goodfellow", "David Warde-Farley", "Pascal Lamblin", "Vincent Dumoulin", "Mehdi Mirza", "Razvan Pascanu", "James Bergstra", "Fr\u00e9d\u00e9ric Bastien", "Yoshua Bengio"], "venue": "library. arXiv preprint arXiv:1308.4214,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Deep Neural Networks (DNN) have substantially pushed the state-of-the-art in a wide range of tasks, especially in speech recognition [1, 2] and computer vision, notably object recognition from images [3, 4].", "startOffset": 133, "endOffset": 139}, {"referenceID": 1, "context": "Deep Neural Networks (DNN) have substantially pushed the state-of-the-art in a wide range of tasks, especially in speech recognition [1, 2] and computer vision, notably object recognition from images [3, 4].", "startOffset": 133, "endOffset": 139}, {"referenceID": 2, "context": "Deep Neural Networks (DNN) have substantially pushed the state-of-the-art in a wide range of tasks, especially in speech recognition [1, 2] and computer vision, notably object recognition from images [3, 4].", "startOffset": 200, "endOffset": 206}, {"referenceID": 3, "context": "Deep Neural Networks (DNN) have substantially pushed the state-of-the-art in a wide range of tasks, especially in speech recognition [1, 2] and computer vision, notably object recognition from images [3, 4].", "startOffset": 200, "endOffset": 206}, {"referenceID": 4, "context": "More recently, deep learning is making important strides in natural language processing, especially statistical machine translation [5, 6, 7].", "startOffset": 132, "endOffset": 141}, {"referenceID": 5, "context": "More recently, deep learning is making important strides in natural language processing, especially statistical machine translation [5, 6, 7].", "startOffset": 132, "endOffset": 141}, {"referenceID": 6, "context": "More recently, deep learning is making important strides in natural language processing, especially statistical machine translation [5, 6, 7].", "startOffset": 132, "endOffset": 141}, {"referenceID": 7, "context": "Interestingly, one of the key factors that enabled this major progress has been the advent of Graphics Processing Units (GPUs), with speed-ups on the order of 10 to 30-fold, starting with [8], and similar improvements with distributed training [9, 10].", "startOffset": 188, "endOffset": 191}, {"referenceID": 8, "context": "Interestingly, one of the key factors that enabled this major progress has been the advent of Graphics Processing Units (GPUs), with speed-ups on the order of 10 to 30-fold, starting with [8], and similar improvements with distributed training [9, 10].", "startOffset": 244, "endOffset": 251}, {"referenceID": 9, "context": "Interestingly, one of the key factors that enabled this major progress has been the advent of Graphics Processing Units (GPUs), with speed-ups on the order of 10 to 30-fold, starting with [8], and similar improvements with distributed training [9, 10].", "startOffset": 244, "endOffset": 251}, {"referenceID": 10, "context": "This along, with the drive to put deep learning systems on low-power devices (unlike GPUs) is greatly increasing the interest in research and development of specialized hardware for deep networks [11, 12, 13].", "startOffset": 196, "endOffset": 208}, {"referenceID": 11, "context": "This along, with the drive to put deep learning systems on low-power devices (unlike GPUs) is greatly increasing the interest in research and development of specialized hardware for deep networks [11, 12, 13].", "startOffset": 196, "endOffset": 208}, {"referenceID": 12, "context": "This along, with the drive to put deep learning systems on low-power devices (unlike GPUs) is greatly increasing the interest in research and development of specialized hardware for deep networks [11, 12, 13].", "startOffset": 196, "endOffset": 208}, {"referenceID": 13, "context": "[14] and [15] show that randomized or stochastic rounding can be used to provide unbiased discretization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[14] and [15] show that randomized or stochastic rounding can be used to provide unbiased discretization.", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "[14] have shown that SGD requires weights with a precision of at least 6 to 8 bits and [16] successfully train DNNs with 12 bits dynamic fixed-point computation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[14] have shown that SGD requires weights with a precision of at least 6 to 8 bits and [16] successfully train DNNs with 12 bits dynamic fixed-point computation.", "startOffset": 87, "endOffset": 91}, {"referenceID": 16, "context": "Besides, the estimated precision of the brain synapses varies between 6 and 12 bits [17].", "startOffset": 84, "endOffset": 88}, {"referenceID": 17, "context": "Noisy weights actually provide a form of regularization which can help to generalize better, as previously shown with variational weight noise [18], Dropout [19, 20] and DropConnect [21], which add noise to the activations or to the weights.", "startOffset": 143, "endOffset": 147}, {"referenceID": 18, "context": "Noisy weights actually provide a form of regularization which can help to generalize better, as previously shown with variational weight noise [18], Dropout [19, 20] and DropConnect [21], which add noise to the activations or to the weights.", "startOffset": 157, "endOffset": 165}, {"referenceID": 19, "context": "Noisy weights actually provide a form of regularization which can help to generalize better, as previously shown with variational weight noise [18], Dropout [19, 20] and DropConnect [21], which add noise to the activations or to the weights.", "startOffset": 157, "endOffset": 165}, {"referenceID": 20, "context": "Noisy weights actually provide a form of regularization which can help to generalize better, as previously shown with variational weight noise [18], Dropout [19, 20] and DropConnect [21], which add noise to the activations or to the weights.", "startOffset": 182, "endOffset": 186}, {"referenceID": 20, "context": "For instance, DropConnect [21], which is closest to BinaryConnect, is a very efficient regularizer that randomly substitutes half of the weights with zeros during propagations.", "startOffset": 26, "endOffset": 30}, {"referenceID": 21, "context": "This is a huge gain, as fixed-point adders are much less expensive both in terms of area and energy than fixed-point multiply-accumulators [22].", "startOffset": 139, "endOffset": 143}, {"referenceID": 22, "context": "It is similar to the \u201chard tanh\u201d non-linearity introduced by [23].", "startOffset": 61, "endOffset": 65}, {"referenceID": 23, "context": "It is also piece-wise linear and corresponds to a bounded form of the rectifier [24].", "startOffset": 80, "endOffset": 84}, {"referenceID": 20, "context": "An interesting analogy to understand BinaryConnect is the DropConnect algorithm [21].", "startOffset": 80, "endOffset": 84}, {"referenceID": 24, "context": "Table 1: Test error rates of a (small) CNN trained on CIFAR-10 depending on optimization method and on whether the learning rate is scaled with the weights initialization coefficients from [25].", "startOffset": 189, "endOffset": 193}, {"referenceID": 25, "context": "We use Batch Normalization (BN) [26] in all of our experiments, not only because it accelerates the training by reducing internal covariate shift, but also because it reduces the overall impact of the weights scale.", "startOffset": 32, "endOffset": 36}, {"referenceID": 26, "context": "Moreover, we use the ADAM learning rule [27] in all of our CNN experiments.", "startOffset": 40, "endOffset": 44}, {"referenceID": 24, "context": "Last but not least, we scale the weights learning rates respectively with the weights initialization coefficients from [25] when optimizing with ADAM, and with the squares of those coefficients when optimizing with SGD or Nesterov momentum [28].", "startOffset": 119, "endOffset": 123}, {"referenceID": 27, "context": "Last but not least, we scale the weights learning rates respectively with the weights initialization coefficients from [25] when optimizing with ADAM, and with the squares of those coefficients when optimizing with SGD or Nesterov momentum [28].", "startOffset": 240, "endOffset": 244}, {"referenceID": 28, "context": "47% Deep L2-SVM [30] 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 20, "context": "35% DropConnect [21] 1.", "startOffset": 16, "endOffset": 20}, {"referenceID": 29, "context": "MNIST is a benchmark image classification dataset [33].", "startOffset": 50, "endOffset": 54}, {"referenceID": 30, "context": "The MLP we train on MNIST consists in 3 hidden layers of 1024 Rectifier Linear Units (ReLU) [34, 24, 3] and a L2-SVM output layer (L2-SVM has been shown to perform better than Softmax on several classification benchmarks [30, 32]).", "startOffset": 92, "endOffset": 103}, {"referenceID": 23, "context": "The MLP we train on MNIST consists in 3 hidden layers of 1024 Rectifier Linear Units (ReLU) [34, 24, 3] and a L2-SVM output layer (L2-SVM has been shown to perform better than Softmax on several classification benchmarks [30, 32]).", "startOffset": 92, "endOffset": 103}, {"referenceID": 2, "context": "The MLP we train on MNIST consists in 3 hidden layers of 1024 Rectifier Linear Units (ReLU) [34, 24, 3] and a L2-SVM output layer (L2-SVM has been shown to perform better than Softmax on several classification benchmarks [30, 32]).", "startOffset": 92, "endOffset": 103}, {"referenceID": 28, "context": "The MLP we train on MNIST consists in 3 hidden layers of 1024 Rectifier Linear Units (ReLU) [34, 24, 3] and a L2-SVM output layer (L2-SVM has been shown to perform better than Softmax on several classification benchmarks [30, 32]).", "startOffset": 221, "endOffset": 229}, {"referenceID": 31, "context": "We do not use any data-augmentation (which can really be a game changer for this dataset [35]).", "startOffset": 89, "endOffset": 93}, {"referenceID": 32, "context": "This architecture is greatly inspired from VGG [36].", "startOffset": 47, "endOffset": 51}, {"referenceID": 33, "context": "Training DNNs with binary weights has been the subject of very recent works [37, 38, 39, 40].", "startOffset": 76, "endOffset": 92}, {"referenceID": 34, "context": "Training DNNs with binary weights has been the subject of very recent works [37, 38, 39, 40].", "startOffset": 76, "endOffset": 92}, {"referenceID": 35, "context": "Training DNNs with binary weights has been the subject of very recent works [37, 38, 39, 40].", "startOffset": 76, "endOffset": 92}, {"referenceID": 36, "context": "Training DNNs with binary weights has been the subject of very recent works [37, 38, 39, 40].", "startOffset": 76, "endOffset": 92}, {"referenceID": 33, "context": "[37, 38] do not train their DNN with Backpropagation (BP) but with a variant called Expectation Backpropagation (EBP).", "startOffset": 0, "endOffset": 8}, {"referenceID": 34, "context": "[37, 38] do not train their DNN with Backpropagation (BP) but with a variant called Expectation Backpropagation (EBP).", "startOffset": 0, "endOffset": 8}, {"referenceID": 37, "context": "EBP is based on Expectation Propagation (EP) [41], which is a variational Bayes method used to do inference in probabilistic graphical models.", "startOffset": 45, "endOffset": 49}, {"referenceID": 35, "context": "[39, 40] retrain neural networks with ternary weights during forward and backward propagations, i.", "startOffset": 0, "endOffset": 8}, {"referenceID": 36, "context": "[39, 40] retrain neural networks with ternary weights during forward and backward propagations, i.", "startOffset": 0, "endOffset": 8}, {"referenceID": 38, "context": "We thank the developers of Theano [42, 43], a Python library which allowed us to easily develop a fast and optimized code for GPU.", "startOffset": 34, "endOffset": 42}, {"referenceID": 39, "context": "We thank the developers of Theano [42, 43], a Python library which allowed us to easily develop a fast and optimized code for GPU.", "startOffset": 34, "endOffset": 42}, {"referenceID": 40, "context": "We also thank the developers of Pylearn2 [44] and Lasagne, two Deep Learning libraries built on the top of Theano.", "startOffset": 41, "endOffset": 45}], "year": 2016, "abstractText": "Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide range of tasks, with the best results obtained with large training sets and large models. In the past, GPUs enabled these breakthroughs because of their greater computational speed. In the future, faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices. As a result, there is much interest in research and development of dedicated hardware for Deep Learning (DL). Binary weights, i.e., weights which are constrained to only two possible values (e.g. -1 or 1), would bring great benefits to specialized DL hardware by replacing many multiply-accumulate operations by simple accumulations, as multipliers are the most space and powerhungry components of the digital implementation of neural networks. We introduce BinaryConnect, a method which consists in training a DNN with binary weights during the forward and backward propagations, while retaining precision of the stored weights in which gradients are accumulated. Like other dropout schemes, we show that BinaryConnect acts as regularizer and we obtain near state-of-the-art results with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.", "creator": "LaTeX with hyperref package"}}}