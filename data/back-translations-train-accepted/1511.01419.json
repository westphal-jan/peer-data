{"id": "1511.01419", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2015", "title": "Train and Test Tightness of LP Relaxations in Structured Prediction", "abstract": "Structured prediction applications often involve complex inference problems that require the use of approximate methods. Approximations based on linear programming (LP) relaxations have proved particularly successful in this setting, with both theoretical and empirical support. Despite the general intractability of inference, it has been observed that in many real-world applications the LP relaxation is often tight. In this work we propose a theoretical explanation to this striking observation. In particular, we show that learning with LP relaxed inference encourages tightness of training instances. We complement this result with a generalization bound showing that tightness generalizes from train to test data.", "histories": [["v1", "Wed, 4 Nov 2015 18:13:35 GMT  (129kb,D)", "https://arxiv.org/abs/1511.01419v1", null], ["v2", "Fri, 6 Nov 2015 12:04:24 GMT  (128kb,D)", "http://arxiv.org/abs/1511.01419v2", null], ["v3", "Wed, 27 Apr 2016 02:58:33 GMT  (143kb,D)", "http://arxiv.org/abs/1511.01419v3", "To appear in ICML 2016"]], "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG", "authors": ["ofer meshi", "mehrdad mahdavi", "adrian weller", "david sontag"], "accepted": true, "id": "1511.01419"}, "pdf": {"name": "1511.01419.pdf", "metadata": {"source": "CRF", "title": "Train and Test Tightness of LP Relaxations in Structured Prediction", "authors": ["Ofer Meshi", "Mehrdad Mahdavi", "Adrian Weller", "David Sontag"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Many applications of machine learning can be formulated as predictive problems over structured output spaces (Bakir et al., 2007; Nowozin et al., 2014).In such problems, output variables are commonly predicted to account for interdependencies between them, such as high-order correlations or structural constraints (e.g. tree linkages or overvoltages).Unfortunately, the improved meaningfulness of these models is associated with computational effort, and in fact, accurate predictions and learning processes generally become difficult. \u2020 Toyota Technological Institute at the University of Cambridge; New York University of Xiv: 151 1.01 419v 3 [stat.ML] 2 7Despite this intractability, efficient predictions achieve very good results in practice. In particular, one type of approximation that is proving effective in many applications."}, {"heading": "2 Related Work", "text": "Many structural prediction problems can be presented as ILPs (Roth and Yih, 2005; Martins et al., 2009a; Rush et al., 2010). Although in their entirety (Roth, 1996; Shimony, 1994) they still present various effective approaches, they have been able to comprehensively investigate both the search for and the search for solutions (Daume \u0301 III et al., 2009; Zhang et al., 2014), as well as natural LP relativization in recent years and contain either the structure of the model or its score function. For example, pair-wise LP relativization is known to be narrow for tree-structural models and for structural predictions!"}, {"heading": "3 Background", "text": "In this section, we will look at the formulation of the structured prediction problem, its LP relaxation and the associated learning problem. Consider a prediction task where the goal is to turn a real rated input vector x into a discrete output vector y = (y1,.,.) A popular model class for this task is based on linear classifiers. In this context, the prediction is made using a linear discrimination rule: y (x; w) = argmaxy (x, y), where there is a function for mapping input-output pairs to feature vectors, and w \"Rd\" is the corresponding weight vector. Since the output space is often huge (exponentially in n), it will usually be intractible to all sorts of outputs. In many applications, the score function has a specific structure."}, {"heading": "4 Analysis", "text": "In this section, we present our main results and propose a theoretical justification for the observed tightness of LP relaxations, which are used for conclusions in models learned by structured prediction on both training and sustained data. To this end, we cite two complementary arguments: in Section 4.1, we argue that optimizing the relaxed training target of Equation (2) also has the effect of promoting the tightness of training instances; in Section 4.2, we show that tightness generalizes from tensile to test data."}, {"heading": "4.1 Tightness at Training", "text": "First of all, we show that the relaxed training goal in Eq. (2), although designed to achieve a high accuracy > q, also induces the narrowness of the LP relaxation. To simplify the notation, we focus on a single training instance and drop the integrality gap. Let us call the solutions to the relaxed and integral integrality gaps: \u00b5L-argmax \u00b5. (3) Let us now consider the following separation: \u03b8 > (\u00b5L \u2212 T) relaxed-sharge = (\u00b5T) relaxed-sharge. (3) Relaxed-sharge is the relaxation gap. (3) This equality means that the difference in the results between the relaxed optimum and the ground truth (relaxed-sharge) can be written as the sum of the integrality gap."}, {"heading": "4.2 Generalization of Tightness", "text": "Our argument in Section 4.1 refers only to the density of tensile instances. However, the empirical evidence discussed above refers to test data. To bridge this gap, we show in this section that tensile density leads to a particular instance. To this end, we consider the discrete composition of the wells of local polytopes ML (without their convex hull), denote the sets of integrality and non-integrality (i.e., fractional) wells (i.e. MI-MF =), and MI-MF consists of all wells of the ML. The wells of the wells are without loss of generality, since linear programs always have a wells that are optimal."}, {"heading": "4.2.1 \u03b3-tight relaxations", "text": "D D D D D D D D D D D D D D D D D D D D D D D D D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D. D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D. D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D.\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D."}, {"heading": "5 Experiments", "text": "In this section, we present some numerical results to support our theoretical analysis. We perform experiments for both a multi-level classification task and an image segmentation task. In all of our experiments, we have the block coordination Frank Wolfe algorithm for structured integration (Lacoste-Julien et al., 2013), with which we apply the LP solver.7 in its entirety as a standard L2 regulator, chosen via a superior validation gap, in which we assume the experimental classification of Finley and Joachims (2008). In this context, we are represented by binary variables, the model consists of singleton and pairdic factors forming a fully connected graph across the labels, and the task loss is the normalized hammering distance. 2 shows relaxed and exact training initations for the \"yeast\" dataset (14 labels)."}, {"heading": "6 Conclusion", "text": "In this paper, we propose an explanation for the narrowness of LP relaxations observed in many structured predictive applications. Our analysis is based on a careful examination of the integrity gap and its relationship to the training target. It shows how training with LP relaxations, although designed with accuracy, also induces the narrowness of relaxation. Our derivative suggests that accurate training can sometimes have the opposite effect by increasing the integrity gap. To explain the narrowness of the test cases, we show that tightness is generalized from one test case to another. Compared with the generalization of Kulesza and Pereira (2007), our bound only looks at the narrowness of the instance by ignoring label errors."}, {"heading": "A \u03b3-Tight LP Relaxations", "text": "In this section, we provide complete derivatives for the results in Section 4.2.1. We use the results in Weller et al. (2016) (some of which are redefined here for completeness). We start by defining a model in minimal representation that will be convenient for each pair. Mapping between the over-complete vector and the minimum vector is as follows. For singleton factors, we have: \u00b5i = (1 \u2212 2) for the pair-wise factors we have: \u00b5ij = (1 +) for the over-complete vector \u00b5 and the minimum vector. (For singleton factors we have: \u00b5i = (1 \u2212 2) for the pair-wise factors we have: \u00b5ij = (1 +) for the over-complete vector \u00b5 and the minimum vector."}, {"heading": "B Additional Experimental Results", "text": "In this section, we present additional experimental results for the \"Scene\" dataset. Specifically, we inject random Gaussian noise into the input characteristics to reduce the signal in the singleton values and increase the role of the pair-by-pair interactions, which complicates the problem because the prediction has to take global information.In Fig. 5, we observe that with accurate training, the exact loss is minimized, which reduces the exact hinge because it is limited by the loss (center, top) at the top. As the exact hinge (and the relaxed loss) also increases during training, this leads to a large integrity gap and less tight spacing. In contrast, during relaxed training, the relaxed loss is minimized, which causes the relaxation hinge to decrease. As the exact hinge is limited by the relaxation hinge at the top, it also decreases during training, but both hinge elements remain close to each other and very closely."}], "references": [{"title": "Predicting Structured Data", "author": ["G.H. Bakir", "T. Hofmann", "B. Sch\u00f6lkopf", "A.J. Smola", "B. Taskar", "S.V.N. Vishwanathan"], "venue": null, "citeRegEx": "Bakir et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bakir et al\\.", "year": 2007}, {"title": "On cuts and matchings in planar graphs", "author": ["F. Barahona"], "venue": "Mathematical Programming,", "citeRegEx": "Barahona.,? \\Q1993\\E", "shortCiteRegEx": "Barahona.", "year": 1993}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["P.L. Bartlett", "S. Mendelson"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bartlett and Mendelson.,? \\Q2002\\E", "shortCiteRegEx": "Bartlett and Mendelson.", "year": 2002}, {"title": "Combining top-down and bottom-up segmentation", "author": ["E. Borenstein", "E. Sharon", "S. Ullman"], "venue": "In CVPR,", "citeRegEx": "Borenstein et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Borenstein et al\\.", "year": 2004}, {"title": "A linear programming formulation and approximation algorithms for the metric labeling problem", "author": ["C. Chekuri", "S. Khanna", "J. Naor", "L. Zosin"], "venue": "SIAM J. on Discrete Mathematics,", "citeRegEx": "Chekuri et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Chekuri et al\\.", "year": 2004}, {"title": "Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms", "author": ["M. Collins"], "venue": "In EMNLP,", "citeRegEx": "Collins.,? \\Q2002\\E", "shortCiteRegEx": "Collins.", "year": 2002}, {"title": "Search-based structured prediction", "author": ["H. Daum\u00e9 III", "J. Langford", "D. Marcu"], "venue": "Machine Learning,", "citeRegEx": "III et al\\.,? \\Q2009\\E", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "Learning graphical model parameters with approximate marginal inference", "author": ["J. Domke"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Domke.,? \\Q2013\\E", "shortCiteRegEx": "Domke.", "year": 2013}, {"title": "Training structural SVMs when exact inference is intractable", "author": ["T. Finley", "T. Joachims"], "venue": "In Proceedings of the 25th International Conference on Machine learning,", "citeRegEx": "Finley and Joachims.,? \\Q2008\\E", "shortCiteRegEx": "Finley and Joachims.", "year": 2008}, {"title": "How hard is inference for structured prediction", "author": ["A. Globerson", "T. Roughgarden", "D. Sontag", "C. Yildirim"], "venue": "In ICML,", "citeRegEx": "Globerson et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Globerson et al\\.", "year": 2015}, {"title": "Dual decomposition for parsing with non-projective head automata", "author": ["T. Koo", "A.M. Rush", "M. Collins", "T. Jaakkola", "D. Sontag"], "venue": "In EMNLP,", "citeRegEx": "Koo et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Koo et al\\.", "year": 2010}, {"title": "The partial constraint satisfaction problem: Facets and lifting theorems", "author": ["A. Koster", "S. van Hoesel", "A. Kolen"], "venue": "Operations Research Letters,", "citeRegEx": "Koster et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Koster et al\\.", "year": 1998}, {"title": "Structured learning with approximate inference", "author": ["A. Kulesza", "F. Pereira"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Kulesza and Pereira.,? \\Q2007\\E", "shortCiteRegEx": "Kulesza and Pereira.", "year": 2007}, {"title": "An analysis of convex relaxations for MAP estimation of discrete MRFs", "author": ["M.P. Kumar", "V. Kolmogorov", "P.H.S. Torr"], "venue": null, "citeRegEx": "Kumar et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2009}, {"title": "Block-coordinate FrankWolfe optimization for structural SVMs", "author": ["S. Lacoste-Julien", "M. Jaggi", "M. Schmidt", "P. Pletscher"], "venue": "In ICML,", "citeRegEx": "Lacoste.Julien et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lacoste.Julien et al\\.", "year": 2013}, {"title": "Bilulinial stable instances of max cut and minimum multiway cut", "author": ["K. Makarychev", "Y. Makarychev", "A. Vijayaraghavan"], "venue": "Proc. 22nd Symposium on Discrete Algorithms (SODA),", "citeRegEx": "Makarychev et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Makarychev et al\\.", "year": 2014}, {"title": "Concise integer linear programming formulations for dependency parsing", "author": ["A. Martins", "N. Smith", "E.P. Xing"], "venue": "In ACL,", "citeRegEx": "Martins et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Martins et al\\.", "year": 2009}, {"title": "Polyhedral outer approximations with application to natural language parsing", "author": ["A. Martins", "N. Smith", "E.P. Xing"], "venue": "In Proceedings of the 26th International Conference on Machine Learning,", "citeRegEx": "Martins et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Martins et al\\.", "year": 2009}, {"title": "Learning max-margin tree predictors", "author": ["O. Meshi", "E. Eban", "G. Elidan", "A. Globerson"], "venue": "In UAI,", "citeRegEx": "Meshi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Meshi et al\\.", "year": 2013}, {"title": "Advanced Structured Prediction", "author": ["S. Nowozin", "P.V. Gehler", "J. Jancsary", "C. Lampert"], "venue": null, "citeRegEx": "Nowozin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nowozin et al\\.", "year": 2014}, {"title": "On the hardness of approximate reasoning", "author": ["D. Roth"], "venue": "Artificial Intelligence,", "citeRegEx": "Roth.,? \\Q1996\\E", "shortCiteRegEx": "Roth.", "year": 1996}, {"title": "Integer linear programming inference for conditional random fields", "author": ["D. Roth", "W. Yih"], "venue": "In ICML,", "citeRegEx": "Roth and Yih.,? \\Q2005\\E", "shortCiteRegEx": "Roth and Yih.", "year": 2005}, {"title": "On dual decomposition and linear programming relaxations for natural language processing", "author": ["A.M. Rush", "D. Sontag", "M. Collins", "T. Jaakkola"], "venue": "In EMNLP,", "citeRegEx": "Rush et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2010}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter"], "venue": "Mathematical programming,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2011}, {"title": "A hierarchy of relaxations between the continuous and convex hull representations for zero-one programming problems", "author": ["H.D. Sherali", "W.P. Adams"], "venue": "SIAM J. on Disc. Math.,", "citeRegEx": "Sherali and Adams.,? \\Q1990\\E", "shortCiteRegEx": "Sherali and Adams.", "year": 1990}, {"title": "Finding the MAPs for belief networks is NP-hard", "author": ["Y. Shimony"], "venue": "Aritifical Intelligence,", "citeRegEx": "Shimony.,? \\Q1994\\E", "shortCiteRegEx": "Shimony.", "year": 1994}, {"title": "New outer bounds on the marginal polytope", "author": ["D. Sontag", "T. Jaakkola"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Sontag and Jaakkola.,? \\Q2008\\E", "shortCiteRegEx": "Sontag and Jaakkola.", "year": 2008}, {"title": "Max-margin Markov networks", "author": ["B. Taskar", "C. Guestrin", "D. Koller"], "venue": "In Advances in Neural Information Processing Systems. MIT Press,", "citeRegEx": "Taskar et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2003}, {"title": "Learning associative Markov networks", "author": ["B. Taskar", "V. Chatalbashev", "D. Koller"], "venue": "In Proc. ICML. ACM Press,", "citeRegEx": "Taskar et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2004}, {"title": "\u017divn\u00fd. The power of linear programming for valued CSPs", "author": ["S.J. Thapper"], "venue": "In FOCS,", "citeRegEx": "Thapper,? \\Q2012\\E", "shortCiteRegEx": "Thapper", "year": 2012}, {"title": "Support vector machine learning for interdependent and structured output spaces", "author": ["I. Tsochantaridis", "T. Hofmann", "T. Joachims", "Y. Altun"], "venue": "In ICML,", "citeRegEx": "Tsochantaridis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2004}, {"title": "Graphical Models, Exponential Families, and Variational Inference", "author": ["M. Wainwright", "M.I. Jordan"], "venue": "Now Publishers Inc.,", "citeRegEx": "Wainwright and Jordan.,? \\Q2008\\E", "shortCiteRegEx": "Wainwright and Jordan.", "year": 2008}, {"title": "MAP estimation via agreement on trees: message-passing and linear programming", "author": ["M. Wainwright", "T. Jaakkola", "A. Willsky"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Wainwright et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wainwright et al\\.", "year": 2005}, {"title": "Structured Prediction Cascades", "author": ["D. Weiss", "B. Taskar"], "venue": "In AISTATS,", "citeRegEx": "Weiss and Taskar.,? \\Q2010\\E", "shortCiteRegEx": "Weiss and Taskar.", "year": 2010}, {"title": "Bethe and related pairwise entropy approximations", "author": ["A. Weller"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Weller.,? \\Q2015\\E", "shortCiteRegEx": "Weller.", "year": 2015}, {"title": "Tightness of LP relaxations for almost balanced models", "author": ["A. Weller", "M. Rowland", "D. Sontag"], "venue": "In AISTATS,", "citeRegEx": "Weller et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Weller et al\\.", "year": 2016}, {"title": "Greed is good if randomized: New inference for dependency parsing", "author": ["Y. Zhang", "T. Lei", "R. Barzilay", "T. Jaakkola"], "venue": "In EMNLP,", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Many applications of machine learning can be formulated as prediction problems over structured output spaces (Bakir et al., 2007; Nowozin et al., 2014).", "startOffset": 109, "endOffset": 151}, {"referenceID": 19, "context": "Many applications of machine learning can be formulated as prediction problems over structured output spaces (Bakir et al., 2007; Nowozin et al., 2014).", "startOffset": 109, "endOffset": 151}, {"referenceID": 21, "context": "Many structured prediction problems can be represented as ILPs (Roth and Yih, 2005; Martins et al., 2009a; Rush et al., 2010).", "startOffset": 63, "endOffset": 125}, {"referenceID": 22, "context": "Many structured prediction problems can be represented as ILPs (Roth and Yih, 2005; Martins et al., 2009a; Rush et al., 2010).", "startOffset": 63, "endOffset": 125}, {"referenceID": 20, "context": "Despite being NPhard in general (Roth, 1996; Shimony, 1994), various effective approximations have been proposed.", "startOffset": 32, "endOffset": 59}, {"referenceID": 25, "context": "Despite being NPhard in general (Roth, 1996; Shimony, 1994), various effective approximations have been proposed.", "startOffset": 32, "endOffset": 59}, {"referenceID": 36, "context": "Those include both search-based methods (Daum\u00e9 III et al., 2009; Zhang et al., 2014), and natural LP relaxations to the hard ILP (Schlesinger, 1976; Koster et al.", "startOffset": 40, "endOffset": 84}, {"referenceID": 11, "context": ", 2014), and natural LP relaxations to the hard ILP (Schlesinger, 1976; Koster et al., 1998; Chekuri et al., 2004; Wainwright et al., 2005).", "startOffset": 52, "endOffset": 139}, {"referenceID": 4, "context": ", 2014), and natural LP relaxations to the hard ILP (Schlesinger, 1976; Koster et al., 1998; Chekuri et al., 2004; Wainwright et al., 2005).", "startOffset": 52, "endOffset": 139}, {"referenceID": 32, "context": ", 2014), and natural LP relaxations to the hard ILP (Schlesinger, 1976; Koster et al., 1998; Chekuri et al., 2004; Wainwright et al., 2005).", "startOffset": 52, "endOffset": 139}, {"referenceID": 5, "context": "\u201808, Rush & Collins \u201811) Figure 1: Percentage of in egral olutions for dependency parsing from Koo et al. (2010).", "startOffset": 12, "endOffset": 113}, {"referenceID": 1, "context": ", Wainwright and Jordan, 2008; Thapper and \u017divn\u00fd, 2012), and the cycle relaxation (equivalently, the second-level of the SheraliAdams hierarchy) is known to be tight both for planar Ising models with no external field (Barahona, 1993) and for almost balanced models (Weller et al.", "startOffset": 218, "endOffset": 234}, {"referenceID": 35, "context": ", Wainwright and Jordan, 2008; Thapper and \u017divn\u00fd, 2012), and the cycle relaxation (equivalently, the second-level of the SheraliAdams hierarchy) is known to be tight both for planar Ising models with no external field (Barahona, 1993) and for almost balanced models (Weller et al., 2016).", "startOffset": 266, "endOffset": 287}, {"referenceID": 8, "context": "However, the sufficient conditions mentioned above are by no means necessary, and indeed, many score functions that are useful in practice do not satisfy them but still produce integral solutions (Roth and Yih, 2004; Sontag et al., 2008; Finley and Joachims, 2008; Martins et al., 2009b; Koo et al., 2010).", "startOffset": 196, "endOffset": 305}, {"referenceID": 10, "context": "However, the sufficient conditions mentioned above are by no means necessary, and indeed, many score functions that are useful in practice do not satisfy them but still produce integral solutions (Roth and Yih, 2004; Sontag et al., 2008; Finley and Joachims, 2008; Martins et al., 2009b; Koo et al., 2010).", "startOffset": 196, "endOffset": 305}, {"referenceID": 1, "context": ", Wainwright and Jordan, 2008; Thapper and \u017divn\u00fd, 2012), and the cycle relaxation (equivalently, the second-level of the SheraliAdams hierarchy) is known to be tight both for planar Ising models with no external field (Barahona, 1993) and for almost balanced models (Weller et al., 2016). To facilitate efficient prediction, one could restrict the model class to be tractable. For example, Taskar et al. (2004) learn supermodular scores, and Meshi et al.", "startOffset": 219, "endOffset": 411}, {"referenceID": 1, "context": ", Wainwright and Jordan, 2008; Thapper and \u017divn\u00fd, 2012), and the cycle relaxation (equivalently, the second-level of the SheraliAdams hierarchy) is known to be tight both for planar Ising models with no external field (Barahona, 1993) and for almost balanced models (Weller et al., 2016). To facilitate efficient prediction, one could restrict the model class to be tractable. For example, Taskar et al. (2004) learn supermodular scores, and Meshi et al. (2013) learn tree structures.", "startOffset": 219, "endOffset": 462}, {"referenceID": 1, "context": ", Wainwright and Jordan, 2008; Thapper and \u017divn\u00fd, 2012), and the cycle relaxation (equivalently, the second-level of the SheraliAdams hierarchy) is known to be tight both for planar Ising models with no external field (Barahona, 1993) and for almost balanced models (Weller et al., 2016). To facilitate efficient prediction, one could restrict the model class to be tractable. For example, Taskar et al. (2004) learn supermodular scores, and Meshi et al. (2013) learn tree structures. However, the sufficient conditions mentioned above are by no means necessary, and indeed, many score functions that are useful in practice do not satisfy them but still produce integral solutions (Roth and Yih, 2004; Sontag et al., 2008; Finley and Joachims, 2008; Martins et al., 2009b; Koo et al., 2010). For example, Martins et al. (2009b) showed that predictors that are learned with LP relaxation yield integral LPs on 92.", "startOffset": 219, "endOffset": 828}, {"referenceID": 1, "context": ", Wainwright and Jordan, 2008; Thapper and \u017divn\u00fd, 2012), and the cycle relaxation (equivalently, the second-level of the SheraliAdams hierarchy) is known to be tight both for planar Ising models with no external field (Barahona, 1993) and for almost balanced models (Weller et al., 2016). To facilitate efficient prediction, one could restrict the model class to be tractable. For example, Taskar et al. (2004) learn supermodular scores, and Meshi et al. (2013) learn tree structures. However, the sufficient conditions mentioned above are by no means necessary, and indeed, many score functions that are useful in practice do not satisfy them but still produce integral solutions (Roth and Yih, 2004; Sontag et al., 2008; Finley and Joachims, 2008; Martins et al., 2009b; Koo et al., 2010). For example, Martins et al. (2009b) showed that predictors that are learned with LP relaxation yield integral LPs on 92.88% of the test data on a dependency parsing problem (see Table 2 therein). Koo et al. (2010) observed a similar behavior for dependency parsing on a number of languages, as can be seen in Fig.", "startOffset": 219, "endOffset": 1006}, {"referenceID": 1, "context": ", Wainwright and Jordan, 2008; Thapper and \u017divn\u00fd, 2012), and the cycle relaxation (equivalently, the second-level of the SheraliAdams hierarchy) is known to be tight both for planar Ising models with no external field (Barahona, 1993) and for almost balanced models (Weller et al., 2016). To facilitate efficient prediction, one could restrict the model class to be tractable. For example, Taskar et al. (2004) learn supermodular scores, and Meshi et al. (2013) learn tree structures. However, the sufficient conditions mentioned above are by no means necessary, and indeed, many score functions that are useful in practice do not satisfy them but still produce integral solutions (Roth and Yih, 2004; Sontag et al., 2008; Finley and Joachims, 2008; Martins et al., 2009b; Koo et al., 2010). For example, Martins et al. (2009b) showed that predictors that are learned with LP relaxation yield integral LPs on 92.88% of the test data on a dependency parsing problem (see Table 2 therein). Koo et al. (2010) observed a similar behavior for dependency parsing on a number of languages, as can be seen in Fig. 1 (kindly provided by the authors). The same phenomenon has been observed for a multi-label classification task, where test integrality reached 100% (Finley and Joachims, 2008, Table 3). Learning structured output predictors from labeled data was proposed in various forms by Collins (2002); Taskar et al.", "startOffset": 219, "endOffset": 1397}, {"referenceID": 1, "context": ", Wainwright and Jordan, 2008; Thapper and \u017divn\u00fd, 2012), and the cycle relaxation (equivalently, the second-level of the SheraliAdams hierarchy) is known to be tight both for planar Ising models with no external field (Barahona, 1993) and for almost balanced models (Weller et al., 2016). To facilitate efficient prediction, one could restrict the model class to be tractable. For example, Taskar et al. (2004) learn supermodular scores, and Meshi et al. (2013) learn tree structures. However, the sufficient conditions mentioned above are by no means necessary, and indeed, many score functions that are useful in practice do not satisfy them but still produce integral solutions (Roth and Yih, 2004; Sontag et al., 2008; Finley and Joachims, 2008; Martins et al., 2009b; Koo et al., 2010). For example, Martins et al. (2009b) showed that predictors that are learned with LP relaxation yield integral LPs on 92.88% of the test data on a dependency parsing problem (see Table 2 therein). Koo et al. (2010) observed a similar behavior for dependency parsing on a number of languages, as can be seen in Fig. 1 (kindly provided by the authors). The same phenomenon has been observed for a multi-label classification task, where test integrality reached 100% (Finley and Joachims, 2008, Table 3). Learning structured output predictors from labeled data was proposed in various forms by Collins (2002); Taskar et al. (2003); Tsochantaridis et al.", "startOffset": 219, "endOffset": 1419}, {"referenceID": 1, "context": ", Wainwright and Jordan, 2008; Thapper and \u017divn\u00fd, 2012), and the cycle relaxation (equivalently, the second-level of the SheraliAdams hierarchy) is known to be tight both for planar Ising models with no external field (Barahona, 1993) and for almost balanced models (Weller et al., 2016). To facilitate efficient prediction, one could restrict the model class to be tractable. For example, Taskar et al. (2004) learn supermodular scores, and Meshi et al. (2013) learn tree structures. However, the sufficient conditions mentioned above are by no means necessary, and indeed, many score functions that are useful in practice do not satisfy them but still produce integral solutions (Roth and Yih, 2004; Sontag et al., 2008; Finley and Joachims, 2008; Martins et al., 2009b; Koo et al., 2010). For example, Martins et al. (2009b) showed that predictors that are learned with LP relaxation yield integral LPs on 92.88% of the test data on a dependency parsing problem (see Table 2 therein). Koo et al. (2010) observed a similar behavior for dependency parsing on a number of languages, as can be seen in Fig. 1 (kindly provided by the authors). The same phenomenon has been observed for a multi-label classification task, where test integrality reached 100% (Finley and Joachims, 2008, Table 3). Learning structured output predictors from labeled data was proposed in various forms by Collins (2002); Taskar et al. (2003); Tsochantaridis et al. (2004). These formulations generalize training methods for binary classifiers, such as the Perceptron algorithm and support vector machines (SVMs), to the case of structured outputs.", "startOffset": 219, "endOffset": 1449}, {"referenceID": 21, "context": "A common approach, introduced right at the inception of structured SVMs by Taskar et al. (2003), is to use LP relaxations for this purpose.", "startOffset": 75, "endOffset": 96}, {"referenceID": 10, "context": "The most closely related work to ours is Kulesza and Pereira (2007), which showed that not all approximations are equally good, and that it is important to match the inference algorithms used at train and test time.", "startOffset": 41, "endOffset": 68}, {"referenceID": 10, "context": "The most closely related work to ours is Kulesza and Pereira (2007), which showed that not all approximations are equally good, and that it is important to match the inference algorithms used at train and test time. The authors defined the concept of algorithmic separability which refers to the setting when an approximate inference algorithm achieves zero loss on a data set. The authors studied the use of LP relaxations for structured learning, giving generalization bounds for the true risk of LP-based prediction. However, since the generalization bounds in Kulesza and Pereira (2007) are focused on prediction accuracy, the only settings in which tightness on test instances can be guaranteed are when the training data is algorithmically separable, which is seldom the case in real-world structured prediction tasks (the models are far from perfect).", "startOffset": 41, "endOffset": 591}, {"referenceID": 10, "context": "The most closely related work to ours is Kulesza and Pereira (2007), which showed that not all approximations are equally good, and that it is important to match the inference algorithms used at train and test time. The authors defined the concept of algorithmic separability which refers to the setting when an approximate inference algorithm achieves zero loss on a data set. The authors studied the use of LP relaxations for structured learning, giving generalization bounds for the true risk of LP-based prediction. However, since the generalization bounds in Kulesza and Pereira (2007) are focused on prediction accuracy, the only settings in which tightness on test instances can be guaranteed are when the training data is algorithmically separable, which is seldom the case in real-world structured prediction tasks (the models are far from perfect). Our paper\u2019s main result (Theorem 4.1), on the other hand, guarantees that the expected fraction of test instances for which a LP relaxation is integral is close to that which was estimated on training data. This then allows us to talk about the generalization of computation. For example, suppose one uses LP relaxation-based algorithms that iteratively tighten the relaxation, such as Sontag and Jaakkola (2008); Sontag et al.", "startOffset": 41, "endOffset": 1272}, {"referenceID": 10, "context": "The most closely related work to ours is Kulesza and Pereira (2007), which showed that not all approximations are equally good, and that it is important to match the inference algorithms used at train and test time. The authors defined the concept of algorithmic separability which refers to the setting when an approximate inference algorithm achieves zero loss on a data set. The authors studied the use of LP relaxations for structured learning, giving generalization bounds for the true risk of LP-based prediction. However, since the generalization bounds in Kulesza and Pereira (2007) are focused on prediction accuracy, the only settings in which tightness on test instances can be guaranteed are when the training data is algorithmically separable, which is seldom the case in real-world structured prediction tasks (the models are far from perfect). Our paper\u2019s main result (Theorem 4.1), on the other hand, guarantees that the expected fraction of test instances for which a LP relaxation is integral is close to that which was estimated on training data. This then allows us to talk about the generalization of computation. For example, suppose one uses LP relaxation-based algorithms that iteratively tighten the relaxation, such as Sontag and Jaakkola (2008); Sontag et al. (2008), and observes that 20% of the instances in the training data are integral using the pairwise relaxation and that after tightening using cycle constraints the remaining 80% are now integral too.", "startOffset": 41, "endOffset": 1294}, {"referenceID": 8, "context": "Finley and Joachims (2008) also studied the effect of various approximate inference methods in the context of structured prediction.", "startOffset": 0, "endOffset": 27}, {"referenceID": 8, "context": "Finley and Joachims (2008) also studied the effect of various approximate inference methods in the context of structured prediction. Their theoretical and empirical results also support the superiority of LP relaxations in this setting. Martins et al. (2009b) established conditions which guarantee algorithmic separability for LP relaxed training, and derived risk bounds for a learning algorithm which uses a combination of exact and relaxed inference.", "startOffset": 0, "endOffset": 260}, {"referenceID": 8, "context": "Finley and Joachims (2008) also studied the effect of various approximate inference methods in the context of structured prediction. Their theoretical and empirical results also support the superiority of LP relaxations in this setting. Martins et al. (2009b) established conditions which guarantee algorithmic separability for LP relaxed training, and derived risk bounds for a learning algorithm which uses a combination of exact and relaxed inference. Finally, recently Globerson et al. (2015) studied the performance of structured predictors for 2D grid graphs with binary labels from an informationtheoretic point of view.", "startOffset": 0, "endOffset": 497}, {"referenceID": 31, "context": "ML is known as the local marginal polytope (Wainwright and Jordan, 2008).", "startOffset": 43, "endOffset": 72}, {"referenceID": 24, "context": "This relaxation is the first level of the Sherali-Adams hierarchy (Sherali and Adams, 1990), which provides successively tighter LP relaxations of an ILP.", "startOffset": 66, "endOffset": 91}, {"referenceID": 27, "context": "In the structured SVM (SSVM) framework (Taskar et al., 2003; Tsochantaridis et al., 2004), the empirical risk is upper bounded by a convex surrogate called the structured hinge loss, which yields the training objective:", "startOffset": 39, "endOffset": 89}, {"referenceID": 30, "context": "In the structured SVM (SSVM) framework (Taskar et al., 2003; Tsochantaridis et al., 2004), the empirical risk is upper bounded by a convex surrogate called the structured hinge loss, which yields the training objective:", "startOffset": 39, "endOffset": 89}, {"referenceID": 27, "context": "Fortunately, as in prediction, LP relaxation can be applied to the structured loss (Taskar et al., 2003; Kulesza and Pereira, 2007), which yields the relaxed training objective:", "startOffset": 83, "endOffset": 131}, {"referenceID": 12, "context": "Fortunately, as in prediction, LP relaxation can be applied to the structured loss (Taskar et al., 2003; Kulesza and Pereira, 2007), which yields the relaxed training objective:", "startOffset": 83, "endOffset": 131}, {"referenceID": 8, "context": "Finley and Joachims (2008) explained tightness of LP relaxations by noting that fractional solutions always incur a loss during training.", "startOffset": 0, "endOffset": 27}, {"referenceID": 2, "context": "Our proof relies on the following general result from Bartlett and Mendelson (2002). Theorem 4.", "startOffset": 54, "endOffset": 84}, {"referenceID": 2, "context": "Our proof relies on the following general result from Bartlett and Mendelson (2002). Theorem 4.2 (Bartlett and Mendelson (2002), Theorem 8).", "startOffset": 54, "endOffset": 128}, {"referenceID": 2, "context": "Theorem 12 in Bartlett and Mendelson (2002) states that if \u03c6\u0303 is Lipschitz with constant L and satisfies \u03c6\u0303(0) = 0, then RM(\u03c6\u0303 \u25e6 f) \u2264 2LRM(F).", "startOffset": 14, "endOffset": 44}, {"referenceID": 2, "context": "Theorem 12 in Bartlett and Mendelson (2002) states that if \u03c6\u0303 is Lipschitz with constant L and satisfies \u03c6\u0303(0) = 0, then RM(\u03c6\u0303 \u25e6 f) \u2264 2LRM(F). In addition, Weiss and Taskar (2010) show that RM(F) = O( qBR\u0302 M ).", "startOffset": 14, "endOffset": 180}, {"referenceID": 23, "context": "Finally, we point out that when using an L2 regularizer at training, we can actually drop the assumption \u2016w\u20162 \u2264 B and instead use a bound on the norm of the optimal solution (as in the analysis of Shalev-Shwartz et al. (2011)).", "startOffset": 197, "endOffset": 226}, {"referenceID": 14, "context": "For training we have implemented the blockcoordinate Frank-Wolfe algorithm for structured SVM (Lacoste-Julien et al., 2013), using GLPK as the LP solver.", "startOffset": 94, "endOffset": 123}, {"referenceID": 8, "context": "Multi-label classification For multi-label classification we adopt the experimental setting of Finley and Joachims (2008). In this setting labels are represented by binary variables, the model consists of singleton and pairwise factors forming a fully connected graph over the labels, and the task loss is the normalized Hamming distance.", "startOffset": 95, "endOffset": 122}, {"referenceID": 3, "context": "Image segmentation Finally, we conduct experiments on a foregroundbackground segmentation problem using the Weizmann Horse dataset (Borenstein et al., 2004).", "startOffset": 131, "endOffset": 156}, {"referenceID": 3, "context": "Image segmentation Finally, we conduct experiments on a foregroundbackground segmentation problem using the Weizmann Horse dataset (Borenstein et al., 2004). The data consists of 328 images, of which we use the first 50 for training and the rest for testing. Here a binary output variable is assigned to each pixel, and there are \u223c 58K variables per image on average. We extract singleton and pairwise features as described in Domke (2013). Fig.", "startOffset": 132, "endOffset": 440}, {"referenceID": 15, "context": ", supermodular potentials or stable instances (Makarychev et al., 2014)) for which the LP relaxation", "startOffset": 46, "endOffset": 71}, {"referenceID": 12, "context": "Compared to the generalization bound of Kulesza and Pereira (2007), our bound only considers the tightness of the instance, ignoring label errors.", "startOffset": 40, "endOffset": 67}, {"referenceID": 13, "context": "1 holds for other convex relaxations which have been proposed for structured prediction, such as semi-definite programming relaxations (Kumar et al., 2009).", "startOffset": 135, "endOffset": 155}, {"referenceID": 12, "context": "In contrast, in Kulesza and Pereira (2007)\u2019s bound, tightness on test instances can only be guaranteed when the training data is algorithmically separable (i.", "startOffset": 16, "endOffset": 43}, {"referenceID": 34, "context": "We make extensive use of the results in Weller et al. (2016) (some of which are restated here for completeness).", "startOffset": 40, "endOffset": 61}, {"referenceID": 31, "context": "If all edges are attractive, then the LP relaxation is known to be tight (Wainwright and Jordan, 2008).", "startOffset": 73, "endOffset": 102}, {"referenceID": 31, "context": "In the sequel we will make use of the known fact that all vertices of the local polytope are half-integral (take values in {0, 1 2 , 1}) (Wainwright and Jordan, 2008).", "startOffset": 137, "endOffset": 166}, {"referenceID": 34, "context": "Weller et al. (2016) define for a given variable i the function F i L(z), which returns for every 0 \u2264 z \u2264 1 the constrained optimum: F i L(z) = max \u03b7\u2208L \u03b7i=z f(\u03b7)", "startOffset": 0, "endOffset": 21}, {"referenceID": 34, "context": "The flip-set, if exists, is easy to find by making a single pass over the graph (see Weller (2015) for more details).", "startOffset": 85, "endOffset": 99}], "year": 2016, "abstractText": "Structured prediction is used in areas such as computer vision and natural language processing to predict structured outputs such as segmentations or parse trees. In these settings, prediction is performed by MAP inference or, equivalently, by solving an integer linear program. Because of the complex scoring functions required to obtain accurate predictions, both learning and inference typically require the use of approximate solvers. We propose a theoretical explanation to the striking observation that approximations based on linear programming (LP) relaxations are often tight on real-world instances. In particular, we show that learning with LP relaxed inference encourages integrality of training instances, and that tightness generalizes from train to test data.", "creator": "LaTeX with hyperref package"}}}