{"id": "1511.01432", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2015", "title": "Semi-supervised Sequence Learning", "abstract": "We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a \"pretraining\" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.", "histories": [["v1", "Wed, 4 Nov 2015 18:48:36 GMT  (21kb)", "http://arxiv.org/abs/1511.01432v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["andrew m dai", "quoc v le"], "accepted": true, "id": "1511.01432"}, "pdf": {"name": "1511.01432.pdf", "metadata": {"source": "CRF", "title": "Semi-supervised Sequence Learning", "authors": ["Andrew M. Dai"], "emails": ["adai@google.com", "qvl@google.com"], "sections": [{"heading": null, "text": "ar Xiv: 151 1,01 432v 1 [cs.L G] 4N ov"}, {"heading": "1 Introduction", "text": "In fact, it is that we are able to hide, and that we are able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be in the position we are, we will be in."}, {"heading": "2 Sequence autoencoders and recurrent language models", "text": "Our approach to sequence autocoding is inspired by the work in sequence learning (also known as seq2seq) by Sutskever et al. [31], which has been successfully used for machine translation [20, 10], text analysis [32], captions [34], video analysis [30], speech recognition [4, 3] and conversation modeling [27, 33]. Key to their approach is the use of a recursive network as an encoder that is read into a hidden state in an input sequence, which is input to a decoded network that predicts the output sequence. The sequence autocoder is similar to the above concept except that it is an unattended learning model that can reconstruct the input sequence itself."}, {"heading": "3 Overview of methods", "text": "Our LSTM implementation is standard and has input, form and output gates [6, 7]. We compare basic LSTMs with LSTMs initialized using the sequence autoencoder method. If LSTMs are initialized using a sequence autoencoder, the methods in our experiments are called SA-LSTMs. If LSTMs are initialized using a language model, the method is called LM-LSTMs. In most of our experiments, our output layer predicts the document label from the LSTM output in the last time step. We are also experimenting with the approach of applying the label in each time step and increasing the weights of the prediction targets linearly from 0 to 1 [24]. In this way, we can inject gradient gradients into earlier steps in the recursive networks."}, {"heading": "4 Experiments", "text": "In our experiments with LSTMs, we follow the basic recipes described in [31] by truncating cell outputs and histories; the benchmarks for understanding texts are tasks that make all datasets publicly available; the tasks are sensory analysis (IMDB and Rotten Tomatoes) and text classification (20 newsgroups and DBpedia); commonly used methods for these datasets, such as bag bags or n-grams, typically ignore long-term ordering information (e.g. modifiers and their objects can be separated by many disjointed words); thus, one would expect recurring methods to execute the information well. However, due to the difficulty in optimizing these networks, recurring models are not the method of document classification; in our experiments with the autocode autocoder sequence, we will reproduce the full document."}, {"heading": "4.1 Sentiment analysis experiments with IMDB", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "4.2 Sentiment analysis experiments with Rotten Tomatoes and the positive effects of additional unlabeled data", "text": "This year, it has come to the point that it has never been as far as this year."}, {"heading": "4.3 Text classification experiments with 20 newsgroups", "text": "The experiments have so far been carried out with data sets in which the number of tokens in a document is relatively small, a few hundred words. Our question is whether it is possible to use SALSTMs for tasks with a significant number of words. To this end, we are conducting the next experiments with the data set of the 20 newsgroups [16].4 There are 11,293 documents in the training set and 7,528 in the test set. We are using 15% of the training documents as a validation set. Each document is an email with an average length of 267 words and a maximum length of 11,925 words. Attachments, PGP keys, duplicates and blank messages are removed. As the documents in the newsgroup are long, it was previously thought unlikely that recurring networks would learn anything from the data set. The best methods are often simple word-of-words. We repeat the same experiments with LSTMs and SA-LSTMs on this data set."}, {"heading": "4.4 Character-level document classification experiments with DBpedia", "text": "The dataset of attention is the DBpedia4http: / / qwone.com / \u02dc jason / 20Newsgroups / dataset [19], which was also used to compare revolutionary neural networks in Zhang and LeCun [38]. DBpedia had no duplication or problems to contain from the outset, so we are comparing experimental results on this dataset. DBpedia is a crowdsourcing effort to extract information from Wikipedia and categorize it into an ontology.For this experiment, we are following the same procedure proposed in Zhang and LeCun [38]. The task is to classify DBpedia abstracts into one of 14 categories after reading character-by-character input. The dataset is divided into 560,000 training examples and 70,000 test examples. A DBpedia document has an average length of 300 characters, while the maximum length of all documents we divide at this level is not."}, {"heading": "4.5 Object classification experiments with CIFAR-10", "text": "In these experiments, we try to see if our pre-training methods extend to non-textual data. To do this, we train an LSTM on the CIFAR 10 image dataset, which consists of 60,000 32 x 32 color images divided into 10 classes. Input in each timeframe of the LSTM consists of a whole series of pixels and we predict the class of the image after reading the last line. We use the same method as in [15] to perform the data magnification. We also train an LSTM to predict the next line (we call this LM-LSTM) and an LSTM to encode the image in rows (SA-LSTM). The loss function during unattended learning is the Euclidean L2 distance between the predicted and the target line. We then specify this in the classification task and present the classification results in Table 8. While the results are not achieved by the STM, the learning results are not contrasted by the STM, the STM is not in the conventional situation."}, {"heading": "5 Discussion", "text": "In this paper, we demonstrated that it is possible to use recurring LSTM networks for NLP tasks such as document classification, and that a language model or sequence autoencoder can help stabilize learning in recurring LSTM networks. On five benchmarks we have tested, LSTMs can reach or exceed the performance level of all previous baselines. Acknowledgements: We thank Oriol Vinyals, Ilya Sutskever, Greg Corrado, Vijay Vasudevan, Manjunath Kudlur, Rajat Monga, Matthieu Devin, and the Google Brain team for their help."}], "references": [{"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["R.K. Ando", "T. Zhang"], "venue": "J. Mach. Learn. Res., 6:1817\u20131853, December", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "Datasets for single-label text categorization", "author": ["A. Cardoso-Cachopo"], "venue": "http://web.ist.utl.pt/acardoso/datasets/,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Listen, attend and spell", "author": ["William Chan", "Navdeep Jaitly", "Quoc V Le", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1508.01211,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "End-to-end continuous speech recognition using attention-based recurrent nn: First results", "author": ["J. Chorowski", "D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.1602,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Stochastic ratio matching of RBMs for sparse high-dimensional inputs", "author": ["Y. Dauphin", "Y. Bengio"], "venue": "NIPS,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["F.A. Gers", "J. Schmidhuber", "F. Cummins"], "venue": "Neural Computation,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "LSTM: A search space odyssey", "author": ["K. Greff", "R.K. Srivastava", "J. Koutn\u0131\u0301k", "B.R. Steunebrink", "J. Schmidhuber"], "venue": "In ICML,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies", "author": ["S. Hochreiter", "Y. Bengio", "P. Frasconi", "J. Schmidhuber"], "venue": "A Field Guide to Dynamical Recurrent Neural Networks,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S. Jean", "K. Cho", "R. Memisevic", "Y. Bengio"], "venue": "ICML,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Effective use of word order for text categorization with convolutional neural networks", "author": ["R. Johnson", "T. Zhang"], "venue": "NAACL,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "EMNLP,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Skipthought vectors", "author": ["R. Kiros", "Y. Zhu", "R. Salakhutdinov", "R.S. Zemel", "A. Torralba", "R. Urtasun", "S. Fidler"], "venue": "NIPS,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional deep belief networks on CIFAR-10", "author": ["A. Krizhevsky"], "venue": "Technical report, University of Toronto,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Newsweeder: Learning to filter netnews", "author": ["K. Lang"], "venue": "ICML,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1995}, {"title": "Learning algorithms for the classification restricted boltzmann machine", "author": ["H. Larochelle", "M. Mandel", "R. Pascanu", "Y. Bengio"], "venue": "JMLR,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "ICML,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "DBpedia \u2013 a large-scale, multilingual knowledge base extracted from wikipedia", "author": ["J. Lehmann", "R. Isele", "M. Jakob", "A. Jentzsch", "D. Kontokostas", "P.N. Mendes", "S. Hellmann", "M. Morsey", "P. van Kleef", "S. Auer"], "venue": "Semantic Web,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["T. Luong", "I. Sutskever", "Q.V. Le", "O. Vinyals", "W. Zaremba"], "venue": "arXiv preprint arXiv:1410.8206,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning word vectors for sentiment analysis", "author": ["A.L. Maas", "R.E. Daly", "P.T. Pham", "D. Huang", "A.Y. Ng", "C. Potts"], "venue": "ACL,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Hidden factors and hidden topics: understanding rating dimensions with review text", "author": ["J. McAuley", "J. Leskovec"], "venue": "RecSys, pages 165\u2013172. ACM,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "INTERSPEECH,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Beyond short snippets: Deep networks for video classification", "author": ["J.Y.H. Ng", "M.J. Hausknecht", "S. Vijayanarasimhan", "O. Vinyals", "R. Monga", "G. Toderici"], "venue": "CVPR,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["B. Pang", "L. Lee"], "venue": "ACL,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning representations by back-propagating errors", "author": ["D. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1986}, {"title": "Neural responding machine for short-text conversation", "author": ["L. Shang", "Z. Lu", "H. Li"], "venue": "EMNLP,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["R. Socher", "B. Huval", "C.D. Manning", "A.Y. Ng"], "venue": "EMNLP,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "A.Y. Ng", "C. Potts"], "venue": "EMNLP,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised learning of video representations using LSTMs", "author": ["N. Srivastava", "E. Mansimov", "R. Salakhutdinov"], "venue": "ICML,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "NIPS,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Grammar as a foreign language", "author": ["O. Vinyals", "L. Kaiser", "T. Koo", "S. Petrov", "I. Sutskever", "G. Hinton"], "venue": "NIPS,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "A neural conversational model", "author": ["O. Vinyals", "Q.V. Le"], "venue": "ICML Deep Learning Workshop,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CVPR,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["S.I. Wang", "C.D. Manning"], "venue": "ACL,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Beyond regression: New tools for prediction and analysis in the behavioral sciences", "author": ["P.J. Werbos"], "venue": "PhD thesis, Harvard,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1974}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Character-level convolutional networks for text classification", "author": ["X. Zhang", "Y. LeCun"], "venue": "NIPS,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 35, "context": "Recurrent neural networks (RNNs) are powerful tools for modeling sequential data, yet training them by back-propagation through time [36, 26] can be difficult [8].", "startOffset": 133, "endOffset": 141}, {"referenceID": 25, "context": "Recurrent neural networks (RNNs) are powerful tools for modeling sequential data, yet training them by back-propagation through time [36, 26] can be difficult [8].", "startOffset": 133, "endOffset": 141}, {"referenceID": 7, "context": "Recurrent neural networks (RNNs) are powerful tools for modeling sequential data, yet training them by back-propagation through time [36, 26] can be difficult [8].", "startOffset": 159, "endOffset": 162}, {"referenceID": 8, "context": "On a number of document classification tasks, we find that it is possible to train Long Short-Term Memory recurrent networks (LSTM RNNs) [9] to achieve good performance with careful tuning of hyperparameters.", "startOffset": 137, "endOffset": 140}, {"referenceID": 15, "context": "In our experiments on document classification with 20 Newsgroups [16] and DBpedia [19], and sentiment analysis with IMDB [21] and Rotten Tomatoes [25], LSTMs pretrained by recurrent language models or sequence autoencoders are usually better than LSTMs initialized randomly.", "startOffset": 65, "endOffset": 69}, {"referenceID": 18, "context": "In our experiments on document classification with 20 Newsgroups [16] and DBpedia [19], and sentiment analysis with IMDB [21] and Rotten Tomatoes [25], LSTMs pretrained by recurrent language models or sequence autoencoders are usually better than LSTMs initialized randomly.", "startOffset": 82, "endOffset": 86}, {"referenceID": 20, "context": "In our experiments on document classification with 20 Newsgroups [16] and DBpedia [19], and sentiment analysis with IMDB [21] and Rotten Tomatoes [25], LSTMs pretrained by recurrent language models or sequence autoencoders are usually better than LSTMs initialized randomly.", "startOffset": 121, "endOffset": 125}, {"referenceID": 24, "context": "In our experiments on document classification with 20 Newsgroups [16] and DBpedia [19], and sentiment analysis with IMDB [21] and Rotten Tomatoes [25], LSTMs pretrained by recurrent language models or sequence autoencoders are usually better than LSTMs initialized randomly.", "startOffset": 146, "endOffset": 150}, {"referenceID": 0, "context": "We believe our semi-supervised approach (as also argued by [1]) has some advantages over other unsupervised sequence learning methods, e.", "startOffset": 59, "endOffset": 62}, {"referenceID": 17, "context": ", Paragraph Vectors [18], because it can allow for easy fine-tuning.", "startOffset": 20, "endOffset": 24}, {"referenceID": 12, "context": "Our semi-supervised learning approach is related to Skip-Thought vectors [13], with two differences.", "startOffset": 73, "endOffset": 77}, {"referenceID": 30, "context": "[31], which has been successfully used for machine translation [20, 10], text parsing [32], image captioning [34], video analysis [30], speech recognition [4, 3] and conversational modeling [27, 33].", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[31], which has been successfully used for machine translation [20, 10], text parsing [32], image captioning [34], video analysis [30], speech recognition [4, 3] and conversational modeling [27, 33].", "startOffset": 63, "endOffset": 71}, {"referenceID": 9, "context": "[31], which has been successfully used for machine translation [20, 10], text parsing [32], image captioning [34], video analysis [30], speech recognition [4, 3] and conversational modeling [27, 33].", "startOffset": 63, "endOffset": 71}, {"referenceID": 31, "context": "[31], which has been successfully used for machine translation [20, 10], text parsing [32], image captioning [34], video analysis [30], speech recognition [4, 3] and conversational modeling [27, 33].", "startOffset": 86, "endOffset": 90}, {"referenceID": 33, "context": "[31], which has been successfully used for machine translation [20, 10], text parsing [32], image captioning [34], video analysis [30], speech recognition [4, 3] and conversational modeling [27, 33].", "startOffset": 109, "endOffset": 113}, {"referenceID": 29, "context": "[31], which has been successfully used for machine translation [20, 10], text parsing [32], image captioning [34], video analysis [30], speech recognition [4, 3] and conversational modeling [27, 33].", "startOffset": 130, "endOffset": 134}, {"referenceID": 3, "context": "[31], which has been successfully used for machine translation [20, 10], text parsing [32], image captioning [34], video analysis [30], speech recognition [4, 3] and conversational modeling [27, 33].", "startOffset": 155, "endOffset": 161}, {"referenceID": 2, "context": "[31], which has been successfully used for machine translation [20, 10], text parsing [32], image captioning [34], video analysis [30], speech recognition [4, 3] and conversational modeling [27, 33].", "startOffset": 155, "endOffset": 161}, {"referenceID": 26, "context": "[31], which has been successfully used for machine translation [20, 10], text parsing [32], image captioning [34], video analysis [30], speech recognition [4, 3] and conversational modeling [27, 33].", "startOffset": 190, "endOffset": 198}, {"referenceID": 32, "context": "[31], which has been successfully used for machine translation [20, 10], text parsing [32], image captioning [34], video analysis [30], speech recognition [4, 3] and conversational modeling [27, 33].", "startOffset": 190, "endOffset": 198}, {"referenceID": 22, "context": "We also find that recurrent language models [23] can be used as a pretraining method for LSTMs.", "startOffset": 44, "endOffset": 48}, {"referenceID": 8, "context": "In our experiments, we use LSTM recurrent networks [9] because they are generally better than RNNs.", "startOffset": 51, "endOffset": 54}, {"referenceID": 5, "context": "Our LSTM implementation is standard and has input, forget, and output gates [6, 7].", "startOffset": 76, "endOffset": 82}, {"referenceID": 6, "context": "Our LSTM implementation is standard and has input, forget, and output gates [6, 7].", "startOffset": 76, "endOffset": 82}, {"referenceID": 23, "context": "We also experiment with the approach of putting the label at every timestep and linearly increasing the weights of the prediction objectives from 0 to 1 [24].", "startOffset": 153, "endOffset": 157}, {"referenceID": 30, "context": "In our experiments with LSTMs, we follow the basic recipes as described in [31] by clipping the cell outputs and gradients.", "startOffset": 75, "endOffset": 79}, {"referenceID": 20, "context": "[21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "The previous baselines are bag-of-words, ConvNets [12] or Paragraph Vectors [18].", "startOffset": 50, "endOffset": 54}, {"referenceID": 17, "context": "The previous baselines are bag-of-words, ConvNets [12] or Paragraph Vectors [18].", "startOffset": 76, "endOffset": 80}, {"referenceID": 36, "context": "With random embedding dimension dropout [37] and random word dropout (not published previously), we are able to reach performance of around 86.", "startOffset": 40, "endOffset": 44}, {"referenceID": 20, "context": "Full+Unlabeled+BoW [21] 11.", "startOffset": 19, "endOffset": 23}, {"referenceID": 20, "context": "11% WRRBM + BoW (bnc) [21] 10.", "startOffset": 22, "endOffset": 26}, {"referenceID": 34, "context": "77% NBSVM-bi (Na\u0131\u0308ve Bayes SVM with bigrams) [35] 8.", "startOffset": 45, "endOffset": 49}, {"referenceID": 10, "context": "78% seq2-bown-CNN (ConvNet with dynamic pooling) [11] 7.", "startOffset": 49, "endOffset": 53}, {"referenceID": 17, "context": "67% Paragraph Vectors [18] 7.", "startOffset": 22, "endOffset": 26}, {"referenceID": 24, "context": "The benchmark of focus in this experiment is the Rotten Tomatoes dataset [25].", "startOffset": 73, "endOffset": 77}, {"referenceID": 21, "context": "To better the performance, we add unlabeled data from the IMDB dataset in the previous experiment and Amazon movie reviews [22] to the autoencoder training stage.", "startOffset": 123, "endOffset": 127}, {"referenceID": 27, "context": "MV-RNN [28] 21.", "startOffset": 7, "endOffset": 11}, {"referenceID": 34, "context": "0% NBSVM-bi [35] 20.", "startOffset": 12, "endOffset": 16}, {"referenceID": 11, "context": "6% CNN-rand [12] 23.", "startOffset": 12, "endOffset": 16}, {"referenceID": 11, "context": "5% CNN-non-static (ConvNet with vectors from word2vec Google News) [12] 18.", "startOffset": 67, "endOffset": 71}, {"referenceID": 28, "context": "[29], a reason of why the methods are not perfect yet is the lack of labeled training data, they proposed to use more labeled data by labeling an addition of 215,154 phrases created by the Stanford Parser.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "We compare our method to their reported results [29] on sentence-level classification.", "startOffset": 48, "endOffset": 52}, {"referenceID": 28, "context": "NB [29] 18.", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "2% SVM [29] 20.", "startOffset": 7, "endOffset": 11}, {"referenceID": 28, "context": "6% BiNB [29] 16.", "startOffset": 8, "endOffset": 12}, {"referenceID": 28, "context": "9% VecAvg [29] 19.", "startOffset": 10, "endOffset": 14}, {"referenceID": 28, "context": "9% RNN [29] 17.", "startOffset": 7, "endOffset": 11}, {"referenceID": 28, "context": "6% MV-RNN [29] 17.", "startOffset": 10, "endOffset": 14}, {"referenceID": 28, "context": "1% RNTN [29] 14.", "startOffset": 8, "endOffset": 12}, {"referenceID": 15, "context": "For that purpose, we carry out the next experiments on the 20 newsgroups dataset [16].", "startOffset": 81, "endOffset": 85}, {"referenceID": 16, "context": "Hybrid Class RBM [17] 23.", "startOffset": 17, "endOffset": 21}, {"referenceID": 4, "context": "8% RBM-MLP [5] 20.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "5% SVM + Bag-of-words [2] 17.", "startOffset": 22, "endOffset": 25}, {"referenceID": 1, "context": "1% Na\u0131\u0308ve Bayes [2] 19.", "startOffset": 16, "endOffset": 19}, {"referenceID": 18, "context": "dataset [19], which was also used to benchmark convolutional neural nets in Zhang and LeCun [38].", "startOffset": 8, "endOffset": 12}, {"referenceID": 37, "context": "dataset [19], which was also used to benchmark convolutional neural nets in Zhang and LeCun [38].", "startOffset": 92, "endOffset": 96}, {"referenceID": 37, "context": "For this experiment, we follow the same procedure suggested in Zhang and LeCun [38].", "startOffset": 79, "endOffset": 83}, {"referenceID": 14, "context": "We use the same method as in [15] to perform data augmentation.", "startOffset": 29, "endOffset": 33}, {"referenceID": 13, "context": "While we do not achieve the results attained by state of the art convolutional networks, our 2-layer pretrained LM-LSTM is able to exceed the results of the baseline convolutional DBN model [14] despite not using any convolutions and outperforms the non pre-trained LSTM.", "startOffset": 190, "endOffset": 194}, {"referenceID": 13, "context": "Convolution DBNs [14] 21.", "startOffset": 17, "endOffset": 21}], "year": 2015, "abstractText": "We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a \u201cpretraining\u201d step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.", "creator": "LaTeX with hyperref package"}}}