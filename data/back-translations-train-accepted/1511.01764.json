{"id": "1511.01764", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Nov-2015", "title": "Discrete R\\'enyi Classifiers", "abstract": "Consider the binary classification problem of predicting a target variable $Y$ from a discrete feature vector $X = (X_1,...,X_d)$. When the probability distribution $\\mathbb{P}(X,Y)$ is known, the optimal classifier, leading to the minimum misclassification rate, is given by the Maximum A-posteriori Probability decision rule. However, estimating the complete joint distribution $\\mathbb{P}(X,Y)$ is computationally and statistically impossible for large values of $d$. An alternative approach is to first estimate some low order marginals of $\\mathbb{P}(X,Y)$ and then design the classifier based on the estimated low order marginals. This approach is also helpful when the complete training data instances are not available due to privacy concerns. In this work, we consider the problem of finding the optimum classifier based on some estimated low order marginals of $(X,Y)$. We prove that for a given set of marginals, the minimum Hirschfeld-Gebelein-Renyi (HGR) correlation principle introduced in [1] leads to a randomized classification rule which is shown to have a misclassification rate no larger than twice the misclassification rate of the optimal classifier. Then, under a separability condition, we show that the proposed algorithm is equivalent to a randomized linear regression approach. In addition, this method naturally results in a robust feature selection method selecting a subset of features having the maximum worst case HGR correlation with the target variable. Our theoretical upper-bound is similar to the recent Discrete Chebyshev Classifier (DCC) approach [2], while the proposed algorithm has significant computational advantages since it only requires solving a least square optimization problem. Finally, we numerically compare our proposed algorithm with the DCC classifier and show that the proposed algorithm results in better misclassification rate over various datasets.", "histories": [["v1", "Thu, 5 Nov 2015 14:47:04 GMT  (19kb)", "http://arxiv.org/abs/1511.01764v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["meisam razaviyayn", "farzan farnia", "david tse"], "accepted": true, "id": "1511.01764"}, "pdf": {"name": "1511.01764.pdf", "metadata": {"source": "CRF", "title": "Discrete Re\u0301nyi Classifiers", "authors": ["Meisam Razaviyayn", "Farzan Farnia", "David Tse"], "emails": ["meisamr@stanford.edu", "farnia@stanford.edu", "dntse@stanford.edu"], "sections": [{"heading": null, "text": "ar Xiv: 151 1.01 764v 1 [cs.L G] 5N ov2 01"}, {"heading": "1 Introduction", "text": "Statistical classification, a core task in many modern data processing and prediction problems, is the problem of predicting markings for a particular trait vector based on a set of training data that contain characteristics of characteristics and their associated labels. (Xn, Y n) of a probability distribution P (X, Y), the target characterization tests for a given test point say X = xtest.However, many modern classification problems are based on high-dimensional categorical characteristics. For example, in genome-wide association studies (GWAS), the classification task is predicted based on observations of SNPs in the genome. In this problem, the function is vector X = (X1, Xd) categorical decision with Xi {0, 1, 2}.What is the optimal classification that leads to a minimal misclassification?"}, {"heading": "2 Problem Formulation", "text": "Consider the problem of binary classification with d discrete features X1, X2,.., Xd, X and a target variable Y, {0, 1}. Suppose that X, {1, 2,.. \u00b7 m} and the data points (X, Y) assume an underlying probability distribution P, X, Y (x, y). If the common probability distribution P, (x, y) is known, the optimal classifier is given by the maximum estimator of a posterior probability (X, Y), i.e., the y rule MAP (x), the argmaxy rule {0,1} P, (Y = y = x). However, the common probability distribution P, y) is often not known in practice. To use the qterial decision, one should first estimate the P rule (x, y) on the basis of the training data."}, {"heading": "3 Worst Case Error Minimization", "text": "In this section, we propose a replacement objective for (2), which results in a decision rule with a misclassification rate not greater than twice the optimal decision rule. Later, we show that the proposed replacement objective is associated with the minimum HGR principle. (2) Let's start with an optimization problem over real estimated variables. (2) Note that any probability distribution PX, Y (\u00b7, \u00b7) is connected by a probability vector p = [px, y] vector p = [x, y] vector p = -vector p = [px, y] vector p = [x, y] vector p = [x, y] vector [x] vector qm = [qx- R2m d] with px, y [P = px] vector [x] vector [x, y] vector [x] vector."}, {"heading": "4 Connection to Hirschfeld-Gebelein-Re\u0301nyi Correlation", "text": "This principle states that the distribution with the maximum (Shannon) entropy under these constraints is a correct representative of the class. To extend this rule to the classification problem, the authors propose in [8] to maximize the distribution that conditions the target entropy to traits, or to minimize equally mutual information between target and traits. Unfortunately, this approach does not lead to a computationally efficient approach to model adaptation, and there is no guarantee of the failure rate of the resulting classifier. Here, we examine an alternative approach of the minimal HGR correlation [1]. This principle suggests to select the distribution in C to minimize the HGR correlation between the target variables and traits. The HGR correlation coefficient between the two random objects X and Y, first introduced by Hirschfeld and Gebelein, 13 and then to define the HGR correlation between the target variables and traits."}, {"heading": "5 Computing Re\u0301nyi Classifier Based on Pairwise Marginals", "text": "In many practical problems, the number of the characteristics d = k is large and therefore it is only mathematically comprehensible (= 1 = 1 = 1 = 1 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 1 = 2 = 1 = 1 = 1 = 1 = 1 = 1 = 2 = 2 = 2 = 2 = 1 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 1 = 2 = 2 = 2 = 2 = 2 = 2 = 1 = 1 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 1 = 1 = 1 = 1 = 2 = 1 = 2 = 2 = 2 = 1 = 1 = 1 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 1 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 1 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 1 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 1 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 1 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 1 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 ="}, {"heading": "6 Robust Re\u0301nyi Feature Selection", "text": "The task of feature selection for classification purposes is to select a subset of features for use in model fitting in the prediction. (Shannon) Mutual information, which is a measure of dependence between two random variables, is used as a target for feature selection in many recent work [19, 20]. In this work, the idea is to select a small subset of features with maximum dependence on the target variable Y. (In other words, the task is to find a subset of variables, the S (XS) and I (XS) with the following optimization probleSMI, argmax S {1, d} I (XS; Y), where XS (XS) and I (XS) characterize the mutual information between the random variables XS and Y. Almost all existing approaches to the solution approaches are based on heuristic approaches and greedy nature, which aim to find a suboptimal solution out of 12."}, {"heading": "7 Numerical Results", "text": "The results are compared with five different benchmarks in [2]: Discrete Chebyshev Classifier [2], greedy DCC [2], Tree Augmented Naive Bayes [3], Minimax Probabilistic Machine [4], and support vector machines (SVM). In addition to the classifiers that only use paired marginals, we also use higher order marginals compared to the marginals in [3]. These classifiers are achieved by defining the new feature variables (Xi, Xj) as in Section 5. Since the number of features in this scenario is large, we combine our Re \u0301 nyi classifiers with the proposed group lasso feature selection. In other words, we first select a number of subscripts."}], "references": [{"title": "Minimum HGR correlation principle: From marginals to joint distribution", "author": ["F. Farnia", "M. Razaviyayn", "S. Kannan", "D. Tse"], "venue": "arXiv preprint arXiv:1504.06010,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Discrete chebyshev classifiers", "author": ["E. Eban", "E. Mezuman", "A. Globerson"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Bayesian network classifiers", "author": ["N. Friedman", "D. Geiger", "M. Goldszmidt"], "venue": "Machine learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "A robust minimax approach to classification", "author": ["C. Bhattacharyya", "M.I. Jordan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "An introduction to variational methods for graphical models", "author": ["M.I. Jordan", "Z. Ghahramani", "T.S. Jaakkola", "L.K. Saul"], "venue": "Machine learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Marginals-to-models reducibility", "author": ["T. Roughgarden", "M. Kearns"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Information theory and statistical mechanics", "author": ["E.T. Jaynes"], "venue": "Physical review,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1957}, {"title": "The minimum information principle for discriminative learning", "author": ["A. Globerson", "N. Tishby"], "venue": "In Proceedings of the 20th conference on Uncertainty in artificial intelligence,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "On general minimax theorems", "author": ["M. Sion"], "venue": "Pacific J. Math,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1958}, {"title": "The complexity of three-way statistical tables", "author": ["J. De Loera", "S. Onn"], "venue": "SIAM Journal on Computing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Moment problems and semidefinite optimization", "author": ["D. Bertsimas", "J. Sethuraman"], "venue": "In Handbook of semidefinite programming,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "A connection between correlation and contingency", "author": ["H.O. Hirschfeld"], "venue": "In Mathematical Proceedings of the Cambridge Philosophical Society,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1935}, {"title": "Das statistische problem der korrelation als variations-und eigenwertproblem und sein zusammenhang mit der ausgleichsrechnung", "author": ["H. Gebelein"], "venue": "ZAMM-Journal of Applied Mathematics and Mechanics/Zeitschrift fu\u0308r Angewandte Mathematik und Mechanik,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1941}, {"title": "On measures of dependence", "author": ["A. R\u00e9nyi"], "venue": "Acta mathematica hungarica,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1959}, {"title": "On maximal correlation, hypercontractivity, and the data processing inequality studied by Erkip and Cover", "author": ["V. Anantharam", "A. Gohari", "S. Kamath", "C. Nair"], "venue": "arXiv preprint arXiv:1304.6133,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Lectures on stochastic programming: modeling and theory, volume", "author": ["A. Shapiro", "D. Dentcheva", "A. Ruszczy\u0144ski"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Monte carlo sampling methods", "author": ["A. Shapiro"], "venue": "Handbooks in operations research and management science,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "On the complexity of linear prediction: Risk bounds, margin bounds, and regularization", "author": ["S.M. Kakade", "K. Sridharan", "A. Tewari"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Feature selection based on mutual information criteria of maxdependency, max-relevance, and min-redundancy", "author": ["H. Peng", "F. Long", "C. Ding"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Using mutual information for selecting features in supervised neural net learning", "author": ["R. Battiti"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1994}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends R  \u00a9 in Machine Learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "We prove that for a given set of marginals, the minimum Hirschfeld-Gebelein-R\u00e9nyi (HGR) correlation principle introduced in [1] leads to a randomized classification rule which is shown to have a misclassification rate no larger than twice the misclassification rate of the optimal classifier.", "startOffset": 124, "endOffset": 127}, {"referenceID": 1, "context": "Our theoretical upper-bound is similar to the recent Discrete Chebyshev Classifier (DCC) approach [2], while the proposed algorithm has significant computational advantages since it only requires solving a least square optimization problem.", "startOffset": 98, "endOffset": 101}, {"referenceID": 1, "context": "This approach, which is the sprit of various machine learning and statistical methods [2\u20136], is also useful when the complete data instances are not available due to privacy concerns in applications such as medical informatics.", "startOffset": 86, "endOffset": 91}, {"referenceID": 2, "context": "This approach, which is the sprit of various machine learning and statistical methods [2\u20136], is also useful when the complete data instances are not available due to privacy concerns in applications such as medical informatics.", "startOffset": 86, "endOffset": 91}, {"referenceID": 3, "context": "This approach, which is the sprit of various machine learning and statistical methods [2\u20136], is also useful when the complete data instances are not available due to privacy concerns in applications such as medical informatics.", "startOffset": 86, "endOffset": 91}, {"referenceID": 4, "context": "This approach, which is the sprit of various machine learning and statistical methods [2\u20136], is also useful when the complete data instances are not available due to privacy concerns in applications such as medical informatics.", "startOffset": 86, "endOffset": 91}, {"referenceID": 5, "context": "This approach, which is the sprit of various machine learning and statistical methods [2\u20136], is also useful when the complete data instances are not available due to privacy concerns in applications such as medical informatics.", "startOffset": 86, "endOffset": 91}, {"referenceID": 0, "context": "Then a surrogate objective function, which is obtained by the minimum HGR correlation principle [1], is used to propose a randomized classification rule.", "startOffset": 96, "endOffset": 99}, {"referenceID": 1, "context": "Finally, we conclude by presenting numerical experiments comparing the proposed classifier with discrete Chebyshev classifier [2], Tree Augmented Naive Bayes [3], and Minimax Probabilistic Machine [4].", "startOffset": 126, "endOffset": 129}, {"referenceID": 2, "context": "Finally, we conclude by presenting numerical experiments comparing the proposed classifier with discrete Chebyshev classifier [2], Tree Augmented Naive Bayes [3], and Minimax Probabilistic Machine [4].", "startOffset": 158, "endOffset": 161}, {"referenceID": 3, "context": "Finally, we conclude by presenting numerical experiments comparing the proposed classifier with discrete Chebyshev classifier [2], Tree Augmented Naive Bayes [3], and Minimax Probabilistic Machine [4].", "startOffset": 197, "endOffset": 200}, {"referenceID": 6, "context": "For example, the maximum entropy principle [7], which is the spirit of the variational method in graphical models [5] and tree augmented naive Bayes [3], is based on the idea of fixing the marginal distributions and fitting a probabilistic model which maximizes the Shannon entropy.", "startOffset": 43, "endOffset": 46}, {"referenceID": 4, "context": "For example, the maximum entropy principle [7], which is the spirit of the variational method in graphical models [5] and tree augmented naive Bayes [3], is based on the idea of fixing the marginal distributions and fitting a probabilistic model which maximizes the Shannon entropy.", "startOffset": 114, "endOffset": 117}, {"referenceID": 2, "context": "For example, the maximum entropy principle [7], which is the spirit of the variational method in graphical models [5] and tree augmented naive Bayes [3], is based on the idea of fixing the marginal distributions and fitting a probabilistic model which maximizes the Shannon entropy.", "startOffset": 149, "endOffset": 152}, {"referenceID": 7, "context": "Another related information theoretic approach is the minimum mutual information principle [8] which finds the probability distribution with the minimum mutual information between the feature vector and the target variable.", "startOffset": 91, "endOffset": 94}, {"referenceID": 3, "context": "In the continuous setting, the idea of minimizing the worst case misclassification rate leads to the minimax probability machine [4].", "startOffset": 129, "endOffset": 132}, {"referenceID": 1, "context": "The most related algorithm to this work is the recent Discrete Chebyshev Classifier (DCC) algorithm [2].", "startOffset": 100, "endOffset": 103}, {"referenceID": 1, "context": "Furthermore, in contrast to [2] which only considers deterministic decision rules, in this work we", "startOffset": 28, "endOffset": 31}, {"referenceID": 1, "context": "Finally, it is worth noting that the algorithm in [2] requires tree structure to be tight, while our proposed algorithm works on non-tree structures as long as the separability condition is satisfied.", "startOffset": 50, "endOffset": 53}, {"referenceID": 0, "context": "1 with probability 1\u2212 q \u03b4 , for some q \u03b4 \u2208 [0, 1], \u2200x \u2208 X .", "startOffset": 43, "endOffset": 49}, {"referenceID": 0, "context": "Later we show that the proposed surrogate objective is connected to the minimum HGR principle [1].", "startOffset": 94, "endOffset": 97}, {"referenceID": 8, "context": "The simple application of the minimax theorem [9] implies that the saddle point of the above optimization problem exists and moreover, the optimal decision rule is a MAP rule for a certain probability distribution P \u2208 C.", "startOffset": 46, "endOffset": 49}, {"referenceID": 9, "context": "Notice that it is NP-hard to verify the existence of a probability distribution satisfying a given set of low order marginals [10].", "startOffset": 126, "endOffset": 130}, {"referenceID": 10, "context": "Based on this observation and the result in [11], we conjecture that in general, solving (2) is NP-hard in the number variables and the alphabet size even when the set C is nonempty.", "startOffset": 44, "endOffset": 48}, {"referenceID": 8, "context": "Let us continue by utilizing the minimax theorem [9] and obtain the worst case probability distribution in (3) by p \u2208 argmaxp\u2208C min0\u2264q\u03b4\u22641 \u2211 x (q x \u03b4px,1 + (1 \u2212 q x \u03b4 )px,0) , or equivalently, p \u2208 argmax p\u2208C \u2211", "startOffset": 49, "endOffset": 52}, {"referenceID": 1, "context": "However, the following result shows that this decision rule does not achieve the factor two misclassification rate obtained in [2].", "startOffset": 127, "endOffset": 130}, {"referenceID": 0, "context": "In the next section, we resolve the second issue by establishing the connection between problem (5) and the minimum HGR correlation principle [1].", "startOffset": 142, "endOffset": 145}, {"referenceID": 0, "context": "Then, we use the existing result in [1] to develop a computationally efficient approach for calculating the decision rule \u03b4\u0303(\u00b7) for Cpairwise.", "startOffset": 36, "endOffset": 39}, {"referenceID": 6, "context": "A commonplace approach to infer models from data is to employ the maximum entropy principle [7].", "startOffset": 92, "endOffset": 95}, {"referenceID": 7, "context": "To extend this rule to the classification problem, the authors in [8] suggest to pick the distribution maximizing the target entropy conditioned to features, or equivalently minimizing mutual information between target and features.", "startOffset": 66, "endOffset": 69}, {"referenceID": 0, "context": "Here we study an alternative approach of minimum HGR correlation principle [1].", "startOffset": 75, "endOffset": 78}, {"referenceID": 11, "context": "The HGR correlation coefficient between the two random objects X and Y , which was first introduced by Hirschfeld and Gebelein [12, 13] and then studied by R\u00e9nyi [14], is defined as \u03c1(X, Y ) , supf,g E [f(X)g(Y )] , where the maximization is taken over the class of all measurable functions f(\u00b7) and g(\u00b7) with E[f(X)] = E[g(Y )] = 0 and E[f(X)] = E[g(Y )] = 1.", "startOffset": 127, "endOffset": 135}, {"referenceID": 12, "context": "The HGR correlation coefficient between the two random objects X and Y , which was first introduced by Hirschfeld and Gebelein [12, 13] and then studied by R\u00e9nyi [14], is defined as \u03c1(X, Y ) , supf,g E [f(X)g(Y )] , where the maximization is taken over the class of all measurable functions f(\u00b7) and g(\u00b7) with E[f(X)] = E[g(Y )] = 0 and E[f(X)] = E[g(Y )] = 1.", "startOffset": 127, "endOffset": 135}, {"referenceID": 13, "context": "The HGR correlation coefficient between the two random objects X and Y , which was first introduced by Hirschfeld and Gebelein [12, 13] and then studied by R\u00e9nyi [14], is defined as \u03c1(X, Y ) , supf,g E [f(X)g(Y )] , where the maximization is taken over the class of all measurable functions f(\u00b7) and g(\u00b7) with E[f(X)] = E[g(Y )] = 0 and E[f(X)] = E[g(Y )] = 1.", "startOffset": 162, "endOffset": 166}, {"referenceID": 13, "context": "For other properties of the HGR correlation coefficient see [14, 15] and the references therein.", "startOffset": 60, "endOffset": 68}, {"referenceID": 14, "context": "For other properties of the HGR correlation coefficient see [14, 15] and the references therein.", "startOffset": 60, "endOffset": 68}, {"referenceID": 0, "context": "In the next section, we use the result of the recent work [1] to compute the R\u00e9nyi classifier \u03b4\u0303(\u00b7) for a special class of marginals C = Cpairwise.", "startOffset": 58, "endOffset": 61}, {"referenceID": 0, "context": "Next, we state a result from [1] which sheds light on the computation of p\u0303x,0 and p\u0303x,1.", "startOffset": 29, "endOffset": 32}, {"referenceID": 0, "context": "Theorem 3 (Rephrased from [1]) Assume Cpairwise 6= \u2205.", "startOffset": 26, "endOffset": 29}, {"referenceID": 0, "context": "Moreover, this condition is satisfied with a positive measure over the simplex of the all probability distributions, as discussed in [1].", "startOffset": 133, "endOffset": 136}, {"referenceID": 15, "context": "Hence, in practice, the above optimization problem can be estimated using Sample Average Approximation (SAA) method [16, 17] through the optimization problem", "startOffset": 116, "endOffset": 124}, {"referenceID": 16, "context": "Hence, in practice, the above optimization problem can be estimated using Sample Average Approximation (SAA) method [16, 17] through the optimization problem", "startOffset": 116, "endOffset": 124}, {"referenceID": 17, "context": "Notice that in order to bound the SAA error and avoid overfitting, one could restrict the search space for \u1e91 [18].", "startOffset": 109, "endOffset": 113}, {"referenceID": 18, "context": "Shannon mutual information, which is a measure of dependence between two random variables, is used in many recent works as an objective for feature selection [19, 20].", "startOffset": 158, "endOffset": 166}, {"referenceID": 19, "context": "Shannon mutual information, which is a measure of dependence between two random variables, is used in many recent works as an objective for feature selection [19, 20].", "startOffset": 158, "endOffset": 166}, {"referenceID": 20, "context": "Remark 2 Alternating Direction Method of Multipliers (ADMM) algorithm [21] can be used for solving the optimization problem in Algorithm 1; see the supplementary material for more details.", "startOffset": 70, "endOffset": 74}, {"referenceID": 1, "context": "The results are compared with five different benchmarks used in [2]: Discrete Chebyshev Classifier [2], greedy DCC [2], Tree Augmented Naive Bayes [3], Minimax Probabilistic Machine [4], and support vector machines (SVM).", "startOffset": 64, "endOffset": 67}, {"referenceID": 1, "context": "The results are compared with five different benchmarks used in [2]: Discrete Chebyshev Classifier [2], greedy DCC [2], Tree Augmented Naive Bayes [3], Minimax Probabilistic Machine [4], and support vector machines (SVM).", "startOffset": 99, "endOffset": 102}, {"referenceID": 1, "context": "The results are compared with five different benchmarks used in [2]: Discrete Chebyshev Classifier [2], greedy DCC [2], Tree Augmented Naive Bayes [3], Minimax Probabilistic Machine [4], and support vector machines (SVM).", "startOffset": 115, "endOffset": 118}, {"referenceID": 2, "context": "The results are compared with five different benchmarks used in [2]: Discrete Chebyshev Classifier [2], greedy DCC [2], Tree Augmented Naive Bayes [3], Minimax Probabilistic Machine [4], and support vector machines (SVM).", "startOffset": 147, "endOffset": 150}, {"referenceID": 3, "context": "The results are compared with five different benchmarks used in [2]: Discrete Chebyshev Classifier [2], greedy DCC [2], Tree Augmented Naive Bayes [3], Minimax Probabilistic Machine [4], and support vector machines (SVM).", "startOffset": 182, "endOffset": 185}], "year": 2015, "abstractText": "Consider the binary classification problem of predicting a target variable Y from a discrete feature vector X = (X1, . . . , Xd). When the probability distribution P(X, Y ) is known, the optimal classifier, leading to the minimum misclassification rate, is given by the Maximum A-posteriori Probability (MAP) decision rule. However, in practice, estimating the complete joint distribution P(X, Y ) is computationally and statistically impossible for large values of d. Therefore, an alternative approach is to first estimate some low order marginals of the joint probability distribution P(X, Y ) and then design the classifier based on the estimated low order marginals. This approach is also helpful when the complete training data instances are not available due to privacy concerns. In this work, we consider the problem of finding the optimum classifier based on some estimated low order marginals of (X, Y ). We prove that for a given set of marginals, the minimum Hirschfeld-Gebelein-R\u00e9nyi (HGR) correlation principle introduced in [1] leads to a randomized classification rule which is shown to have a misclassification rate no larger than twice the misclassification rate of the optimal classifier. Then, under a separability condition, it is shown that the proposed algorithm is equivalent to a randomized linear regression approach. In addition, this method naturally results in a robust feature selection method selecting a subset of features having the maximum worst case HGR correlation with the target variable. Our theoretical upper-bound is similar to the recent Discrete Chebyshev Classifier (DCC) approach [2], while the proposed algorithm has significant computational advantages since it only requires solving a least square optimization problem. Finally, we numerically compare our proposed algorithm with the DCC classifier and show that the proposed algorithm results in better misclassification rate over various UCI data repository datasets.", "creator": "LaTeX with hyperref package"}}}