{"id": "1511.02124", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Nov-2015", "title": "Barrier Frank-Wolfe for Marginal Inference", "abstract": "We introduce a globally-convergent algorithm for optimizing the tree-reweighted (TRW) variational objective over the marginal polytope. The algorithm is based on the conditional gradient method (Frank-Wolfe) and moves pseudomarginals within the marginal polytope through repeated maximum a posteriori (MAP) calls. This modular structure enables us to leverage black-box MAP solvers (both exact and approximate) for variational inference, and obtains more accurate results than tree-reweighted algorithms that optimize over the local consistency relaxation. Theoretically, we bound the sub-optimality for the proposed algorithm despite the TRW objective having unbounded gradients at the boundary of the marginal polytope. Empirically, we demonstrate the increased quality of results found by tightening the relaxation over the marginal polytope as well as the spanning tree polytope on synthetic and real-world instances.", "histories": [["v1", "Fri, 6 Nov 2015 15:48:53 GMT  (1614kb,D)", "https://arxiv.org/abs/1511.02124v1", "25 pages, 12 figures, To appear in Neural Information Processing Systems (NIPS) 2015"], ["v2", "Wed, 25 Nov 2015 18:57:33 GMT  (1613kb,D)", "http://arxiv.org/abs/1511.02124v2", "25 pages, 12 figures, To appear in Neural Information Processing Systems (NIPS) 2015, Corrected reference and cleaned up bibliography"]], "COMMENTS": "25 pages, 12 figures, To appear in Neural Information Processing Systems (NIPS) 2015", "reviews": [], "SUBJECTS": "stat.ML cs.LG math.OC", "authors": ["rahul g krishnan", "simon lacoste-julien", "david sontag"], "accepted": true, "id": "1511.02124"}, "pdf": {"name": "1511.02124.pdf", "metadata": {"source": "CRF", "title": "Barrier Frank-Wolfe for Marginal Inference", "authors": ["Rahul G. Krishnan", "Simon Lacoste-Julien"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year it is more than ever before in the history of the city."}, {"heading": "2 Background", "text": "We look at the marginal conclusions on pair modes of MRF with the random variables X1, X2,.., XN, where each variable has discrete states, xi VALi. Let us (i) refer to the number of variable states of the variable Xi. We organize the edge-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials-potentials"}, {"heading": "3 Optimizing over Contractions of the Marginal Polytope", "text": "(1) We want to use the fewest possible MAP calls, and (2) we avoid regions near the boundary where the unlimited curvature of the function slows down convergence. (1) A viable response option (1) is possible by using correction steps that, after a Frank Wolfe step, optimize over the previously visited wells of M (referred to as full-corrective Frank Wolfe algorithms). (1) However, this does not require additional MAP calls. (see Sec. 5) Optimizing the TRW objective OverM can unexpectedly hurt performance. This leaves us in a dilemma: correction steps allow us to reduce the target without additional MAP calls, but they can also slow global progress beyond the boundary of polyopes (where the FW statements become fewer barriers)."}, {"heading": "4 Algorithm", "text": "So it is quite possible that our approach applies three levels of Frank-Wolfe: (1) for the (tightening) optimization of T, (2) for the (tightening) optimization of T, (3) for the (tightening) optimization of T, (3) for the (tightening) optimization of T, (2) for the execution of approximate inferences, (4) for the optimization of ~ \u00b5 overM, (3) for the execution of the correction steps (lines 16 and 23). We detail a few heuristics that provide practical help."}, {"heading": "5 Experimental Results", "text": "The L1 marginal error is calculated as follows: http: / / www.libDAI. [1] The L1 marginal error is calculated as follows: http: / / www.libDAI. [2] The L1 marginal error is calculated by adding the duality gap to the prime (as this guarantees an upper limit). [3] We use a non-uniform initialization of the Matrix Tree Theorem (Sontag and Jaakkola, 2007; Koo et al., 2007). We perform 10 updates to a duality gap of 0.5 onM and only perform correction steps at all times. We use the implementation of TRBP and the Junction Tree Algorithm (to compile exact margins)."}, {"heading": "On the Applicability of Approximate MAP Solvers", "text": "Synthetic margins: Fig. 1 (c) shows the accuracy of approximate marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal marginal margmargmarginal margmargmargmargmarginal margmargmargmargmarginal margmargmargmargmarginal margmargmargmarginal margmargmargmarginal margmarginal margmarginal margmargmarginal margmargmargmarginal margmargmargmargmargmarginal margmargmarginal margmargmarginal margmarginal margmargmargmargmargmargmarginal margmargmarginal margmargmarginal margmargmargmarginal margmargmargmargmargmarginal margmargmargmargmargmargmargmarginal margmarginal margmargmarginal margmargmarginal margmargmargmargmargmarg"}, {"heading": "On the Importance of Optimizing over T", "text": "Synthetic cliques: In Fig. 1 (e), 1 (f) we examine the effect of the tightening of T relative to coupling strength \u03b8. We consider the results obtained for the last margins before updating them (step 19) and compare them with the values obtained after the optimization of T (marked with \u03c1opt). The optimization of T has little effect on TRW optimized relative to L\u03b4. In order to optimize M\u03b4, the update realizes better marginalities and is bound to logZ (beyond the values obtained in Sontag and Jaakkola (2007). Chinese characters: Fig. 2 (left) shows marginalities in various iterations of optimization of T. The submodular and supermodular potentials lead to frustrated models for which LB is very loose, resulting in poor results.4 Our method produces reasonable marginal values even before the first update of T, and these improve with streamlining of T."}, {"heading": "Related Work for Marginal Inference with MAP Calls", "text": "Hazan and Jaakkola (2012) estimate logZ by averaging MAP estimates calculated on randomly distorted bloated diagrams. Our implementation of the method went well in approaching logZ, but the margins (estimated by setting the value of each random variable and estimating the logZ for the resulting diagram) were less accurate than our method (Fig. 1 (e), 1 (f)))."}, {"heading": "6 Discussion", "text": "We introduce the first demonstrably convergent algorithm for the TRW target over the marginal polytopic, assuming exact MAP oracles. We quantify the gains made from both marginal inference over M and streamlining over the overall tree polytopic. We provide heuristics that improve the scalability of Frank-Wolfe when used for marginal inference. In particular, our algorithm is suitable for areas where marginal inference is hard, but efficient MAP solvers exist that are able to handle non-submodular potentials. Code is available at https: giubmuthic.com / work-in-general / inferenc.com / inferenceflex-class."}, {"heading": "Acknowledgements", "text": "RK and DS gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA) Probabilistic Programming for Advanced Machine Learning (PPAML) Program under4We perform TRBP for 1000 iterations using damping = 0.9; the algorithm converges with a maximum standard difference between successive iterations of 0.002. Tightening of T has not significantly altered the results of TRBP.Air Force Research Laboratory (AFRL) Prime Contract No. FA8750-14-C-0005. Any opinions, results, conclusions or recommendations expressed in this material are those of the author (s) and do not necessarily reflect the views of DARPA, AFRL or the U.S. Government."}, {"heading": "A Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Summary of Supplementary Material", "text": "The supplementary material is divided into two parts: (1) The first part is devoted to the presentation of the theoretical results presented in the main work. Section B describes the variants of the Frank Wolfe algorithm we use and analyze. Section C provides the proof for theorem 3.4 (fixed \u03b4), Section D provides the proof for theorem 3.5 (adaptive \u03b4). Finally, Section E applies the convergence theorem to the TRW target and examines the relevant constants. (2) The rest of the supplementary material provides more information about the experimental setup and additional experimental results."}, {"heading": "A.2 Descent Lemma", "text": "The following Descent-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lema-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lema-Lema-Lema Lema Lema Lema Lema Lema Lema Lema Lema Lema Lema Lema Lema Lema Lema Lema Lema Lema Lema Lema Lema Lema Lema Lema Lema Lema Lema Lema Lema Lema L"}, {"heading": "B Frank-Wolfe Algorithms", "text": "In this section, we present the various algorithms we use to completely correct Frank-Wolfe (FCFW) with adaptive contractions over domain D, as has been done in our experiments."}, {"heading": "B.1 Overview of the Modified Frank-Wolfe Algorithm (FW with Away Steps)", "text": "This variant of Frank-Wolfe adds the possibility of taking an \"away step\" (see step 5 in Algorithm 3) in order to avoid the zigzagging phenomenon spreading near the boundary of Frank-Wolfe. (see step 5 in Algorithm 3). (see step 5 in Algorithm 3). (see point 5 in Algorithm 3). (see point 5 in Algorithm 3). (see point 5 in Algorithm 3). (see point 5 in Algorithm 3). (see point 5 in Algorithm 3). (see point 5 in Algorithm 3).)................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "B.2 Fully Corrective Frank-Wolfe (FCFW) with Adaptive-\u03b4", "text": "In algorithm 4, we enter the pseudo-code to perform the complete correction of Frank Wolfe optimization via D (by iterative optimization via D\u03b4 with adaptive \u03b4 updates. If this constant (skipped step 10) is maintained, then algorithm 4 implements the fixed Wolfe variant via D\u03b4. We describe the algorithm as maintaining the correction set of atoms V (k + 1) over D (instead of D\u03b4), since it is constantly changing. It is easy to move back and forth between V (k + 1) and its contraction V\u03b4 = (1 \u2212 k) V (k))) V (k + 1) + \u03b4 (k) u0, and so we note that an efficient implementation with either cheaply representation (for example, storing only V (k + 1) and not the perfected version of the correction polytopic) is required. The approximate correction via Ventricity is implemented using an algorithm representation in 3."}, {"heading": "C Bounding the Sub-optimality for Fixed \u03b4 Variant", "text": "The pseudo-code for Dhion optimization is specified in algorithm 4 (by ignoring step 10, which updates Connecticut), and is given with a stop criterion, but can alternatively be used for a fixed number of K iterations. The following theory limits the sub-optimality of iterations relative to the true optimum x-X above D. If you can calculate the constants in the theorem, you can select a target contraction set to guarantee a specific sub-optimality of iterations; otherwise, you can use heuristics to choose that, unlike the adaptive variant of these algorithms, these constants do not converge to the true solution as K-II unless x-II happens that the error can be controlled by selecting small enough. Theorem C.1 (sub-optimality), which we satisfy the properties in problem 3.2 and assume that its gradient."}, {"heading": "D Convergence with Adaptive-\u03b4", "text": "In this section, we will show the convergence of the adaptation gap that we use during this section. First, the definition of our primary gap is the uniform measurement of progress; second, the convergence of the convergence gap. In this section, we will first show some definitions and lemas that are used for the main result of convergence in Theorem D.6. We will begin with the definitions of the duality gap that we use in this section."}, {"heading": "E Properties of the TRW Objective", "text": "In this section, we explicitly calculate limits for the constants that occur in the convergence statements for our Fixed-\u03b4 and Adaptive-\u03b4 algorithms for the optimization problem by: min-\u00b5-M-TRW (~ \u00b5; ~ \u03b8, \u03c1). In particular, we calculate the Lipschitz constant for its gradient above M\u03b4 (property 3.3), we give a form for its module of continuity function \u03c9 (\u00b7) (used in theorem 3.4), and we calculate B, the upper limit for the negative uniform gap (as used in Lemma D.3)."}, {"heading": "E.1 Property 3.3 : Controlled Growth of Lipschitz Constant overM\u03b4", "text": "First of all, we motivate our choice of the standard overM. Let us remember that ~ \u00b5 can be disassembled into two blocks."}, {"heading": "E.2 Modulus of Continuity Function", "text": "We start with the calculation of a module of the continuity function for \u2212 x log x with an additive linear term."}, {"heading": "E.3 Bounded Negative Uniform Gap", "text": "5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5"}, {"heading": "E.4 Summary", "text": "The (strong) convexity of the negative TRW target is represented in (Wainwright et al., 2005; London et al., 2015) M. M is the convex hull of a finite number of vectors that represent mappings to random variables and therefore exhibit a compact convexation rate. Entropy function is continuously differentiable to the relative interior of probability simplex, and thus the TRW target has the same property on the relative interior ofM. Thus \u2212 TRW (~ \u00b5, III) fulfills the properties specified in problem 3.2.Lemma E.5 (suboptimality limited for optimization \u2212 TRW (~ \u00b5) simplex, and thus the TRW target has the same property on the relative interior ofM."}, {"heading": "F Correction and Local Search Steps in Algorithm 2", "text": "Algorithm 5 describes the CORRECTION method used in line 16 of algorithm 2 to implement the correction step of the FCFW algorithm. It uses the modified Frank Wolfe algorithm (FW with path steps) as described in line 3. Algorithm 6 represents the LOCALSEARCH method used in line 17 of algorithm 2. Local search renders FW overM\u03b4 for a fixed solution using the iterated conditional mode algorithm as an approximate FW oracle, which allows the cost-effective search for additional vertices to extend the correction polytopic V. \u2212 Algorithm 5: Re-optimization using the correction polytopic V using MFW, f is the negative TRW object1: CORRECTION (x (0), V, III, III, IV)."}, {"heading": "G Comparison to perturbAndMAP", "text": "We expand the method we used to evaluate PerturbAndMAP in Figure 1 (e) and 1 (f), re-implementing the algorithm to estimate the partition function in Python (as described in Hazan and Jaakkola (2012), Section 4.1), and using toulbar2 (Allouche et al., 2010) to perform MAP conclusions over an inflated graph, in which each variable builds the partition function on five new variables. Log partition function is estimated as the average energy of 10 exact MAP calls on the extended graph, in which the individual node potentials are affected by the rubber distribution. To extract marginals, we ficate the value of a variable for each assignment, estimate the log partition function of the conditioned graph, and calculate beliefs based on averaging."}, {"heading": "I Additional Experiments", "text": "We can also track the average number of ILP calls required for a fixed duality gap to converge, as illustrated in Figure 5 (c). Optimization via T implements three to four times as many MAP calls as the first iteration of the inference.Figure 4 shows further examples from the Chinese character study. Figure 5 (a) also shows the results of a wrapper version of TRBPs in libDAI (Mooij, 2010), which is usually realized via T. Again, we find few gains over optimization via L. Figure 5 (a), 5 (b) shows the comparison of algorithm variants via M andMI (a), 1 (b), 1 (b).J Bounding logZ with Approximate MAP Solversose, which we use for an approximate line 7 of the algorithms."}, {"heading": "Supplementary References", "text": "D. P. Bertsekas. Nonlinear Programming. Athena Scientific, Belmont, MA, 1999. J. Gue \u0301 lat and P. Marcotte. Some Notes on Wolfe's \"away step.\" Mathematical Programming, 35 (1): 110-119, 1986. Y. Nesterov. Introductory Lectures on Convex Optimization. Kluwer Academic Publishers, Norwell, MA, 2004. C. Teo, A. Smola, S. Vishwanathan, and Q. Le. A Scalable Modular Convex Solver for Regulated Risk Mitigation. In KDD, 2007. P. Wolfe. Convergence Theory in Nonlinear Programming. In J. Abadie, editor, Integer and Nonlinear Pro-gramming, pp. 1-23. North Holland, 1970."}], "references": [{"title": "Nonlinear programming", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "Bertsekas.,? \\Q1999\\E", "shortCiteRegEx": "Bertsekas.", "year": 1999}, {"title": "Some comments on Wolfe\u2019s \u2018away step", "author": ["J. Gu\u00e9lat", "P. Marcotte"], "venue": "Mathematical Programming,", "citeRegEx": "Gu\u00e9lat and Marcotte.,? \\Q1986\\E", "shortCiteRegEx": "Gu\u00e9lat and Marcotte.", "year": 1986}, {"title": "Introductory Lectures on Convex Optimization", "author": ["Y. Nesterov"], "venue": null, "citeRegEx": "Nesterov.,? \\Q2004\\E", "shortCiteRegEx": "Nesterov.", "year": 2004}, {"title": "A scalable modular convex solver for regularized risk minimization", "author": ["C. Teo", "A. Smola", "S. Vishwanathan", "Q. Le"], "venue": "In KDD,", "citeRegEx": "Teo et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Teo et al\\.", "year": 2007}, {"title": "Convergence Theory in Nonlinear Programming", "author": ["P. Wolfe"], "venue": "Integer and Nonlinear Programming,", "citeRegEx": "Wolfe.,? \\Q1970\\E", "shortCiteRegEx": "Wolfe.", "year": 1970}], "referenceMentions": [{"referenceID": 4, "context": "Instead, Sontag and Jaakkola (2007) use the conditional gradient method (also called Frank-Wolfe) and offthe-shelf linear programming solvers to optimize TRW over the cycle consistency relaxation. Rather than optimizing over the cycle relaxation, Belanger et al. (2013) optimize the TRW objective over the exact marginal polytope.", "startOffset": 91, "endOffset": 270}, {"referenceID": 4, "context": "Instead, Sontag and Jaakkola (2007) use the conditional gradient method (also called Frank-Wolfe) and offthe-shelf linear programming solvers to optimize TRW over the cycle consistency relaxation. Rather than optimizing over the cycle relaxation, Belanger et al. (2013) optimize the TRW objective over the exact marginal polytope. Then, using Frank-Wolfe, the linear minimization performed in the inner loop can be shown to correspond to MAP inference. The Frank-Wolfe optimization algorithm has seen increasing use in machine learning, thanks in part to its efficient handling of complex constraint sets appearing with structured data (Jaggi, 2013; Lacoste-Julien and Jaggi, 2015). However, applying Frank-Wolfe to variational inference presents challenges that were never resolved in previous work. First, the linear minimization performed in the inner loop is computationally expensive, either requiring repeatedly solving a large linear program, as in Sontag and Jaakkola (2007), or performing MAP inference, as in Belanger et al.", "startOffset": 91, "endOffset": 983}, {"referenceID": 4, "context": "Instead, Sontag and Jaakkola (2007) use the conditional gradient method (also called Frank-Wolfe) and offthe-shelf linear programming solvers to optimize TRW over the cycle consistency relaxation. Rather than optimizing over the cycle relaxation, Belanger et al. (2013) optimize the TRW objective over the exact marginal polytope. Then, using Frank-Wolfe, the linear minimization performed in the inner loop can be shown to correspond to MAP inference. The Frank-Wolfe optimization algorithm has seen increasing use in machine learning, thanks in part to its efficient handling of complex constraint sets appearing with structured data (Jaggi, 2013; Lacoste-Julien and Jaggi, 2015). However, applying Frank-Wolfe to variational inference presents challenges that were never resolved in previous work. First, the linear minimization performed in the inner loop is computationally expensive, either requiring repeatedly solving a large linear program, as in Sontag and Jaakkola (2007), or performing MAP inference, as in Belanger et al. (2013). Second, the TRW objective involves entropy terms whose gradients go to infinity near the boundary of the feasible set, therefore existing convergence guarantees for Frank-Wolfe do not apply.", "startOffset": 91, "endOffset": 1042}, {"referenceID": 4, "context": "The primal convergence of the Frank-Wolfe algorithm is given by Thm. 1 in Jaggi (2013), restated here for convenience: for k \u2265 1, the iterates x satisfy: (4) f(x)\u2212 f(x\u2217) \u2264 2Cf k + 2 , where Cf is called the \u201ccurvature constant\u201d.", "startOffset": 36, "endOffset": 87}, {"referenceID": 4, "context": "A viable option to address (1) is through the use of correction steps, where after a Frank-Wolfe step, one optimizes over the polytope defined by previously visited vertices of M (called the fully-corrective Frank-Wolfe (FCFW) algorithm and proven to be linearly convergence for strongly convex objectives (Lacoste-Julien and Jaggi, 2015)). This does not require additional MAP calls. However, we found (see Sec. 5) that when optimizing the TRW objective overM, performing correction steps can surprisingly hurt performance. This leaves us in a dilemma: correction steps enable decreasing the objective without additional MAP calls, but they can also slow global progress since iterates after correction sometimes lie close to the boundary of the polytope (where the FW directions become less informative). In a manner akin to barrier methods and to Garber and Hazan (2013)\u2019s local linear oracle, our proposed solution maintains the iterates within a contraction of the polytope.", "startOffset": 91, "endOffset": 874}, {"referenceID": 0, "context": "The following descent lemma is proved in Bertsekas (1999) (Prop.", "startOffset": 41, "endOffset": 58}, {"referenceID": 4, "context": "To implement the approximate correction steps in the fully corrective Frank-Wolfe (FCFW) algorithm, we use the Frank-Wolfe algorithm with away steps (Wolfe, 1970), also known as the modified Frank-Wolfe (MFW) algorithm (Gu\u00e9lat and Marcotte, 1986).", "startOffset": 149, "endOffset": 162}, {"referenceID": 1, "context": "To implement the approximate correction steps in the fully corrective Frank-Wolfe (FCFW) algorithm, we use the Frank-Wolfe algorithm with away steps (Wolfe, 1970), also known as the modified Frank-Wolfe (MFW) algorithm (Gu\u00e9lat and Marcotte, 1986).", "startOffset": 219, "endOffset": 246}, {"referenceID": 1, "context": "For a strongly convex objective (with Lipschitz continuous gradient), the MFW was known to have asymptotic linear convergence (Gu\u00e9lat and Marcotte, 1986) and its global linear convergence rate was shown recently (Lacoste-Julien and Jaggi, 2015), accelerating the slow general sublinear rate of Frank-Wolfe.", "startOffset": 126, "endOffset": 153}, {"referenceID": 1, "context": "To implement the approximate correction steps in the fully corrective Frank-Wolfe (FCFW) algorithm, we use the Frank-Wolfe algorithm with away steps (Wolfe, 1970), also known as the modified Frank-Wolfe (MFW) algorithm (Gu\u00e9lat and Marcotte, 1986). We give pseudo-code for MFW in Algorithm 3 (taken from (LacosteJulien and Jaggi, 2015)). This variant of Frank-Wolfe adds the possibility to do an \u201caway step\u201d (see step 5 in Algorithm 3) in order to avoid the zig zagging phenomenon that slows down Frank-Wolfe when the solution is close to the boundary of the polytope. For a strongly convex objective (with Lipschitz continuous gradient), the MFW was known to have asymptotic linear convergence (Gu\u00e9lat and Marcotte, 1986) and its global linear convergence rate was shown recently (Lacoste-Julien and Jaggi, 2015), accelerating the slow general sublinear rate of Frank-Wolfe. When performing a correction over the convex hull over a (somewhat small) set of vertices ofD\u03b4 , this convergence difference was quite significant in our experiments (MFW converging in a small number of iterations to do an approximate correction vs. FW taking hundreds of iterations to reach a similar level of accuracy). We note that the TRW objective is strongly convex when all the edge probabilities are nonzero (Wainwright et al., 2005); and that it has Lipschitz gradient over D\u03b4 (but not D). The gap computed in step 6 of Algorithm 3 is non-standard; it is a sufficient condition to ensure the global linear convergence of the outer FCFW algorithm when using Algorithm 3 as a subroutine to implement the approximate correction step. See Lacoste-Julien and Jaggi (2015) for more details.", "startOffset": 220, "endOffset": 1651}, {"referenceID": 4, "context": "As f has a Lipschitz continuous gradient over D\u03b4 , we can use any standard convergence result of the Frank-Wolfe algorithm to bound the suboptimality of the iterate x over D\u03b4 . Algorithm 4 (with a fixed \u03b4) describes the FCFW algorithm which guarantees at least as much progress as the standard FW algorithm (by step 15 and 20a), and thus we can use the convergence result from Jaggi (2013) as already stated in (4): f(x) \u2212 f(x(\u03b4)) \u2264 2C\u03b4 (k+2) with C\u03b4 \u2264 diam(D\u03b4)L\u03b4 , where L\u03b4 comes from Property 3.", "startOffset": 107, "endOffset": 390}, {"referenceID": 3, "context": "This is a generalization of the technique used in Teo et al. (2007) (also used in the context of Frank-Wolfe in the proof of Theorem C.", "startOffset": 50, "endOffset": 68}, {"referenceID": 3, "context": "This is a generalization of the technique used in Teo et al. (2007) (also used in the context of Frank-Wolfe in the proof of Theorem C.4 in Lacoste-Julien et al. (2013)).", "startOffset": 50, "endOffset": 169}, {"referenceID": 2, "context": "2 in Nesterov (2004)).", "startOffset": 5, "endOffset": 21}], "year": 2015, "abstractText": "We introduce a globally-convergent algorithm for optimizing the tree-reweighted (TRW) variational objective over the marginal polytope. The algorithm is based on the conditional gradient method (Frank-Wolfe) and moves pseudomarginals within the marginal polytope through repeated maximum a posteriori (MAP) calls. This modular structure enables us to leverage black-box MAP solvers (both exact and approximate) for variational inference, and obtains more accurate results than tree-reweighted algorithms that optimize over the local consistency relaxation. Theoretically, we bound the sub-optimality for the proposed algorithm despite the TRW objective having unbounded gradients at the boundary of the marginal polytope. Empirically, we demonstrate the increased quality of results found by tightening the relaxation over the marginal polytope as well as the spanning tree polytope on synthetic and real-world instances.", "creator": "LaTeX with hyperref package"}}}