{"id": "1511.02136", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Nov-2015", "title": "Diffusion-Convolutional Neural Networks", "abstract": "We present a new deterministic relational model derived from convolutional neural networks. Search-Convolutional Neural Networks (SCNNs) extend the notion of convolution to graph search to construct a rich latent representation that extracts local behavior from general graph-structured data. Unlike other neural network models that take graph-structured data as input, SCNNs have a parameterization that is independent of input size, a property that enables transfer learning between datasets. SCNNs can be applied to a wide variety of prediction tasks, including node classification, community detection, and link prediction. Our results indicate that SCNNs can offer considerable lift over off-the-shelf classifiers and simple multilayer perceptrons, and comparable performance to state-of-the-art probabilistic graphical models.", "histories": [["v1", "Fri, 6 Nov 2015 16:09:32 GMT  (7317kb,D)", "https://arxiv.org/abs/1511.02136v1", "Published in NIPS Networks 2015"], ["v2", "Mon, 16 Nov 2015 14:33:30 GMT  (3671kb,D)", "http://arxiv.org/abs/1511.02136v2", "First version published in NIPS Networks 2015; Second version submitted to ICLR 2016"], ["v3", "Fri, 20 Nov 2015 14:38:08 GMT  (3681kb,D)", "http://arxiv.org/abs/1511.02136v3", "First version published in NIPS Networks 2015; Second version submitted to ICLR 2016 as a draft; Third version submitted to ICLR 2016"], ["v4", "Thu, 7 Jan 2016 19:33:18 GMT  (3772kb,D)", "http://arxiv.org/abs/1511.02136v4", "Revised to address reviewer comments"], ["v5", "Tue, 19 Jan 2016 20:36:29 GMT  (3683kb,D)", "http://arxiv.org/abs/1511.02136v5", "Added a note about the time complexity of prediction for sparse graphs to section 2.4.1; added a link to a reference implementation"], ["v6", "Fri, 8 Jul 2016 15:05:17 GMT  (270kb,D)", "http://arxiv.org/abs/1511.02136v6", "Full paper"]], "COMMENTS": "Published in NIPS Networks 2015", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["james atwood", "don towsley"], "accepted": true, "id": "1511.02136"}, "pdf": {"name": "1511.02136.pdf", "metadata": {"source": "CRF", "title": "Diffusion-Convolutional Neural Networks", "authors": ["James Atwood"], "emails": ["jatwood@cs.umass.edu", "towsley@cs.umass.edu"], "sections": [{"heading": "1 Introduction", "text": "On the one hand, finding the right way to express and use the structure of data can lead to improvements in predictive power; on the other hand, finding such a representation can be difficult, and adding a structure to a model can dramatically increase the complexity of predictions and learning processes. The goal of this work is to design a flexible model for a general class of structured data that provides improvements in predictive power while avoiding an increase in complexity. To achieve this, we are expanding Convolutionary Neural Networks (CNN) by performing diffusion control."}, {"heading": "2 Model", "text": "Consider a situation in which we have a series of T-diagrams for predicting Y values, that is, the individual diagrams Gt = (Vt, Et) each consist of indentations Vt and edges Et. The indentations are collectively represented by an Nt \u00b7 F design matrix Xt of the characteristic Gt, where the number of nodes in Gt and the edges Et of an Nt \u00b7 Nt definition matrix on which we can calculate a grade-normalized transition matrix Pt that gives the probability of jumping from a node to a node i in one step. No constraints are placed in the form Gt \u00b7 Nt or unweighted, directed or undirected. Either the nodes, edges, or graphs have labels Y associated with them, with the dimensionality of Y in any case."}, {"heading": "3 Experiments", "text": "In this section, we present various experiments to investigate how well DCNNs perform in node and graph classification tasks. In each case, we compare DCNNs with other known and effective approaches to the task. In each of the following experiments, we use the AdaGrad algorithm [1] for gradient ascent with a learning rate of 0.05. All weights are initialized by scanning from a normal distribution with medium zero and variance 0.01. We select the hyperbolic tangent for the nonlinear differentiable function f and use the multiclass hinge loss between the predictions of the model and soil truth as a training target. The model was implemented in Python with lasagne and theano [2]."}, {"heading": "3.1 Node classification", "text": "This year it is so far that it only takes a few days to reach an agreement."}, {"heading": "3.2 Graph Classification", "text": "This year, we will be able to find a solution that will enable us to find a solution that will enable us to find a solution that will enable us to find a solution that will enable us to find a solution that will enable us to find a solution that will enable us to find a solution that will enable us to find a solution that will enable us to find a solution that will enable us to find a solution that is capable of finding a solution. \""}, {"heading": "4 Limitations", "text": "Scalability DCNNs are realized as a series of operations on dense tensors. Storing the largest tensor (P \u0443, the power series of the transition matrix) requires O (N2t H) memory, which in practice can lead to memory failures on the GPU for very large graphs. DCNNs can therefore be easily applied to graphs of tens to hundreds of thousands of nodes, but not to graphs with millions to billions of nodes. Locality The model is designed to capture local behavior in graph structured data. As a result of the construction of latent representation from diffusion processes starting at each node, we may fail to encode useful spatial dependencies between individual nodes or other non-local graphs."}, {"heading": "5 Related Work", "text": "In this section, we describe existing approaches to the problems of semi-supervised learning, graph classification and edge classification, and discuss their relationship to DCNN.15 Other graph-based neural network models Other researchers have explored how CNNs can be extended from grid structures to more general graphically structured data. [11] However, they propose a spatial method with relationships to hierarchical clusters in which the layers of the network are defined by a hierarchical division of the node set. In the same paper, the authors propose a spectral method that extends the concept of convolution to graph spectra. Later, these techniques are applied to data where a graph is not immediately present, but must be derived. DCNNs falling within the spatial category are distinct from this work because their parameterization makes them transferable; a DCNN learned on a graph can be applied to another."}, {"heading": "6 Conclusion and Future Work", "text": "By learning a representation that summarizes the results of graph diffusion, diffusion-revolutionary neural networks offer performance improvements over probabilistic relational models and core methods in node classification tasks. We intend to explore methods to a) improve the performance of DCNN in graph classification tasks and b) make the model scalable in future work."}, {"heading": "7 Appendix: Representation Invariance for Isomorphic Graphs", "text": "If two graphs G1 and G2 are isomorphic, then their diffusion revolutionary activations are the same. Proof by contradiction; let's assume that G1 and G2 are isomorphic and that their diffusion revolutionary activations are different. Diffusion revolutionary activations can be written asZ1jk = f (W cjk). (W cjk) v \"V1\" V1 \"V1\" P \"1vjv\" X1v \"k / N1) Z2jk = f (W cjk\" V \"V2\" V2 \"V2\" X2v \"k\" N2) Note: V1 \"V2\" VX1vk \"v\" V \"V,\" k \"P\" 1vjv, \"P\" 2vjv \"v\" v \"V\" V, \"j\" Z2 \"Niss\" V. \""}, {"heading": "Acknowledgments", "text": "We would like to thank Bruno Ribeiro, Pinar Yanardag and David Belanger for their feedback on the drafts of this paper."}], "references": [{"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Link-based classification", "author": ["P Sen", "L Getoor"], "venue": "Technical Report", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "An experimental investigation of kernels on graphs for collaborative recommendation and semisupervised classification", "author": ["Fran\u00e7ois Fouss", "Kevin Francoisse", "Luh Yen", "Alain Pirotte", "Marco Saerens"], "venue": "Neural Networks,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Collective Classification in Network Data", "author": ["Prithviraj Sen", "Galileo Mark Namata", "Mustafa Bilgic", "Lise Getoor", "Brian Gallagher", "Tina Eliassi-Rad"], "venue": "AI Magazine,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Deep Graph Kernels", "author": ["Pinar Yanardag", "S V N Vishwanathan"], "venue": "In the 21th ACM SIGKDD International Conference,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Comparison of descriptor spaces for chemical compound retrieval and classification", "author": ["Nikil Wale", "Ian A Watson", "George Karypis"], "venue": "Knowledge and Information Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity", "author": ["Asim Kumar Debnath", "Rosa L Lopez de Compadre", "Gargi Debnath", "Alan J Shusterman", "Corwin Hansch"], "venue": "Journal of medicinal chemistry,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1991}, {"title": "Statistical evaluation of the predictive toxicology challenge 2000\u20132001", "author": ["Hannu Toivonen", "Ashwin Srinivasan", "Ross D King", "Stefan Kramer", "Christoph Helma"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Protein function prediction via graph kernels", "author": ["Karsten M Borgwardt", "Cheng Soon Ong", "Stefan Sch\u00f6nauer", "SVN Vishwanathan", "Alex J Smola", "Hans-Peter Kriegel"], "venue": "Bioinformatics, 21(suppl 1):i47\u2013i56,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Spectral networks and locally connected networks on", "author": ["Joan Bruna", "Wojciech Zaremba", "Arthur Szlam", "Yann LeCun"], "venue": "graphs. arXiv.org,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Deep Convolutional Networks on Graph-Structured Data", "author": ["M Henaff", "J Bruna", "Y LeCun"], "venue": "arXiv.org", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Ah Chung Tsoi", "author": ["F Scarselli", "M Gori"], "venue": "M Hagenbuchner, and G Monfardini. The Graph Neural Network Model. IEEE Transactions on Neural Networks", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Neural Network for Graphs: A Contextual Constructive Approach", "author": ["A Micheli"], "venue": "IEEE Transactions on Neural Networks", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Convolutional Networks on Graphs for Learning Molecular Fingerprints", "author": ["David K Duvenaud", "Dougal Maclaurin", "Jorge Aguilera-Iparraguirre", "Rafael G\u00f3mez-Bombarelli", "Timothy Hirzel", "Al\u00e1n Aspuru-Guzik", "Ryan P Adams"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["Daphne Koller", "Nir Friedman"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Scene segmentation with crfs learned from partially labeled images", "author": ["Jakob Verbeek", "William Triggs"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Efficient Inference in Large Conditional", "author": ["Trevor Cohn"], "venue": "Random Fields. ECML,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "In each of the following experiments, we use the AdaGrad algorithm [1] for gradient ascent with a learning rate of 0.", "startOffset": 67, "endOffset": 70}, {"referenceID": 1, "context": "The model was implemented in Python using Lasagne and Theano [2].", "startOffset": 61, "endOffset": 64}, {"referenceID": 2, "context": "The CRF-LBP result is quoted from [3], which follows the same experimental protocol.", "startOffset": 34, "endOffset": 37}, {"referenceID": 3, "context": "\u2018KED\u2019 and \u2018KLED\u2019 denote the exponential diffusion and Laplacian exponential diffusion kernels-on-graphs, respectively, which have previously been shown to perform well on the Cora dataset [4].", "startOffset": 188, "endOffset": 191}, {"referenceID": 2, "context": "Results for this model are quoted from prior work [3] that uses the same dataset and experimental protocol.", "startOffset": 50, "endOffset": 53}, {"referenceID": 4, "context": "Node Classification Data The Cora corpus [5] consists of 2,708 machine learning papers and the 5,429 citation edges that they share.", "startOffset": 41, "endOffset": 44}, {"referenceID": 4, "context": "The Pubmed corpus [5] consists of 19,717 scientific papers from the Pubmed database on the subject of diabetes.", "startOffset": 18, "endOffset": 21}, {"referenceID": 5, "context": "Deep graph kernels decompose a graph into substructures, treat those substructures as words in a sentence, and fit a word-embedding model to obtain a vectorization [6].", "startOffset": 164, "endOffset": 167}, {"referenceID": 6, "context": "The NCI1 and NCI109 [7] datasets consist of 4100 and 4127 graphs that represent chemical compounds.", "startOffset": 20, "endOffset": 23}, {"referenceID": 7, "context": "MUTAG [8] contains 188 nitro compounds that are labeled as either aromatic or heteroaromatic with seven node features.", "startOffset": 6, "endOffset": 9}, {"referenceID": 8, "context": "PTC [9] contains 344 compounds labeled with whether they are carcinogenic in rats with 19 node features.", "startOffset": 4, "endOffset": 7}, {"referenceID": 9, "context": "Finally, ENZYMES [10] is a balanced dataset containing 600 proteins with three node features.", "startOffset": 17, "endOffset": 21}, {"referenceID": 10, "context": "[11] propose a spatial method with ties to hierarchical clustering, where the layers of the network are defined via a hierarchical partitioning of the node set.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Later, [12] applied these techniques to data where a graph is not immediately present but must be inferred.", "startOffset": 7, "endOffset": 11}, {"referenceID": 12, "context": "A related branch of work that has focused on extending convolutional neural networks to domains where the structure of the graph itself is of direct interest [13, 14, 15].", "startOffset": 158, "endOffset": 170}, {"referenceID": 13, "context": "A related branch of work that has focused on extending convolutional neural networks to domains where the structure of the graph itself is of direct interest [13, 14, 15].", "startOffset": 158, "endOffset": 170}, {"referenceID": 14, "context": "A related branch of work that has focused on extending convolutional neural networks to domains where the structure of the graph itself is of direct interest [13, 14, 15].", "startOffset": 158, "endOffset": 170}, {"referenceID": 14, "context": "For example, [15] construct a deep convolutional model that learns real-valued fingerprint representation of chemical compounds.", "startOffset": 13, "endOffset": 17}, {"referenceID": 15, "context": "Probabilistic Relational Models DCNNs also share strong ties to probabilistic relational models (PRMs), a family of graphical models that are capable of representing distributions over relational data [16].", "startOffset": 201, "endOffset": 205}, {"referenceID": 16, "context": "In practice, the marginal log-likelihood of a partially-observed CRF is computed using a contrastof-partition-functions approach that requires running loopy belief propagation twice; once on the entire graph and once with the observed labels fixed [17].", "startOffset": 248, "endOffset": 252}, {"referenceID": 17, "context": "This algorithm, and thus each step in the numerical optimization, has exponential time complexity O(EtN t ) where Ct is the size of the maximal clique in Gt [18].", "startOffset": 157, "endOffset": 161}, {"referenceID": 3, "context": "Kernel Methods Kernel methods define similarity measures either between nodes (so-called kernels on graphs) [4] or between graphs (graph kernels) and these similarities can serve as a basis for prediction via the kernel trick.", "startOffset": 108, "endOffset": 111}, {"referenceID": 5, "context": "The performance of graph kernels can be improved by decomposing a graph into substructures, treating those substructures as a words in a sentence, and fitting a word-embedding model to obtain a vectorization [6].", "startOffset": 208, "endOffset": 211}], "year": 2016, "abstractText": "We present diffusion-convolutional neural networks (DCNNs), a new model for graph-structured data. Through the introduction of a diffusion-convolution operation, we show how diffusion-based representations can be learned from graphstructured data and used as an effective basis for node classification. DCNNs have several attractive qualities, including a latent representation for graphical data that is invariant under isomorphism, as well as polynomial-time prediction and learning that can be represented as tensor operations and efficiently implemented on the GPU. Through several experiments with real structured datasets, we demonstrate that DCNNs are able to outperform probabilistic relational models and kernel-on-graph methods at relational node classification tasks.", "creator": "LaTeX with hyperref package"}}}