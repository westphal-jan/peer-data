{"id": "1511.02301", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Nov-2015", "title": "The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations", "abstract": "We introduce a new test of how well language models capture meaning in children's books. Unlike standard language modelling benchmarks, it distinguishes the task of predicting syntactic function words from that of predicting lower-frequency words, which carry greater semantic content. We compare a range of state-of-the-art models, each with a different way of encoding what has been previously read. We show that models which store explicit representations of long-term contexts outperform state-of-the-art neural language models at predicting semantic content words, although this advantage is not observed for syntactic function words. Interestingly, we find that the amount of text encoded in a single memory representation is highly influential to the performance: there is a sweet-spot, not too big and not too small, between single words and full sentences that allows the most meaningful information in a text to be effectively retained and recalled. Further, the attention over such window-based memories can be trained effectively through self-supervision. We then assess the generality of this principle by applying it to the CNN QA benchmark, which involves identifying named entities in paraphrased summaries of news articles, and achieve state-of-the-art performance.", "histories": [["v1", "Sat, 7 Nov 2015 04:36:20 GMT  (1852kb,D)", "http://arxiv.org/abs/1511.02301v1", null], ["v2", "Wed, 30 Dec 2015 23:21:58 GMT  (1854kb,D)", "http://arxiv.org/abs/1511.02301v2", null], ["v3", "Tue, 5 Jan 2016 21:10:21 GMT  (1854kb,D)", "http://arxiv.org/abs/1511.02301v3", null], ["v4", "Fri, 1 Apr 2016 05:31:33 GMT  (1854kb,D)", "http://arxiv.org/abs/1511.02301v4", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["felix hill", "antoine bordes", "sumit chopra", "jason weston"], "accepted": true, "id": "1511.02301"}, "pdf": {"name": "1511.02301.pdf", "metadata": {"source": "CRF", "title": "THE GOLDILOCKS PRINCIPLE: READING CHILDREN\u2019S BOOKS WITH EXPLICIT MEMORY REPRESENTATIONS", "authors": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston"], "emails": [], "sections": [{"heading": "1 INTRODUCTION", "text": "The context in which words and sentences are understood, whether a conversation, a book chapter, or a road sign, plays an important role in human understanding (Altmann & Steedman, 1988; Binder & Desai, 2011).In this paper, we examine how well statistical models can use such broader contexts to make predictions about natural language.Our analysis is based on a new benchmark dataset (The Children's Book Test, or CBT) designed to test the role of memory and context in language processing and comprehensibility.The test requires predictions of different types of missing words in children's books, as both nearby words and a broader context from the book are highlighted.People who predict all kinds of words with similar accuracy rely on the broader context to make precise predictions about named entities or nouns, while it is unimportant to predict higher or higher pretensions."}, {"heading": "2 THE CHILDREN\u2019S BOOK TEST", "text": "The experiments in this paper are based on a new resource, the Children's Book Test, which was developed to directly measure how well language models can exploit a broader linguistic context; the CBT is based on books that are freely available thanks to Project Gutenberg.1 The use of children's books guarantees a clear narrative structure that can make the role of context more meaningful; after assigning books to both training, validation or test sentences, we formed sample questions \"(denotes x) from chapters in the book, listing 21 consecutive sentences; in each question, the first 20 sentences form the context (denotes S), and one word (denotes a) is removed from the 21st sentence that becomes the query; the models must identify the answer word from a selection of 10 candidate answers (denotes C) that appear in the context sentences and the query."}, {"heading": "2.1 RELATED RESOURCES", "text": "There are clear parallels between the CBT and the Microsoft Research Sentence Completion Challenge (MSRCC) (MSRCC, 2011), which is also based on the Gutenberg project (but not on children's books in particular).A fundamental difference is that if examples in the MSRCC consist of a single sentence, each query in the CBT is in a broader context, testing the sensitivity of language models to semantic coherence across sentence boundaries. CBT is also larger than the MRSCC (10,000 vs 1,040 test questions), requires models to be selected from more candidates in each question (10 vs 5), covers missing words of different (POS) types, and contains large training and validation sets that match the form of the testset.There are also similarities between the CBT and the CNN / Daily Mail (CNN QA) dataset recently published by Hermann et al. (2015)."}, {"heading": "3 STUDYING MEMORY REPRESENTATION WITH MEMORY NETWORKS", "text": "Memory Networks (Weston et al., 2015b) have shown promising performance in various tasks, such as rationing bAbI tasks (Weston et al., 2015a) or speech modeling (Sukhbaatar et al., 2015).Their application to the CBT allows us to study the effects of different types of context encoding on their semantic processing capability on naturally occurring language."}, {"heading": "3.1 ENCODING MEMORIES AND QUERIES", "text": "Most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to"}, {"heading": "3.2 END-TO-END MEMORY NETWORKS", "text": "In the ree\u00fcBnn nvo emd nree\u00fcdBnn nvo the eSrdteeu, n sdsa the rf\u00fc ide eeirmnmnlrteeaeaeVnn nvo the eSrteeeirmnn nvo the eSrteeeeirmnnmnnn nvo the eeirmnnnnnnnlrf\u00fc the eeirmnmnlrmnn eeirmnr eeeaeeeeeeeeeeeeeeeeeeaeeeeermnn rrrreeeeeeeeeemn rrremnn rreemn reemn the eremn reremn rereeeemn remnr the ereremnn reremnn the eeereemnn nremnn remnn nr the eeeeremnr eremnr eremnn nr the ereeemnr eeemnr emn nr erereemnn nr the eeeeermnr emnr eermnr emnr ernr the ermnr ernr the eeeeermnr eermnr ermnr ermnn"}, {"heading": "3.3 SELF-SUPERVISION FOR WINDOW MEMORIES", "text": "After initial experiments, we observed that the ability to perform multiple hops when accessing memory was only useful in the lexical memory model q1. Therefore, we also tried a simpler single-hop storage network, i.e. a single memory for answering, which exploits a stronger signal for learning memory access. A similar approach was successfully applied by Bordes et al. (2015) to answering questions about knowledge bases. Memory monitoring (knowing which memories to maintain) is not provided at training time, but is automatically derived by the following procedure: Since we know the correct answer during training, we hypothesize the correct supporting memory under the windows, whose correct candidate is the correct answer. In general, where more than one memory contains the correct answer, the model chooses the single memory that is already rated highest in itself, i.e. the highest value is given by the 6We to the model by forcing the input of the A.D."}, {"heading": "4 BASELINE AND COMPARISON MODELS", "text": "In addition to the variants of the storage network, we have also applied many different types of voice modeling and machine-readable architectures to the CBT."}, {"heading": "4.1 NON-LEARNING BASELINES", "text": "We implemented two simple baselines based on word frequencies. First, we selected the most common candidate in the entire training corpus. Second, we selected the most common candidate in his context for a given question. In both cases, we made connections with a random selection. We also tried two more sophisticated methods to classify candidates based on the training data. First, the \"sliding window\" applied to the MCTest by Richardson et al. 6TF-IDF spacing worked almost as well in our experiments, but a random selection of positives did not. (2013) In this method, ten \"windows\" of the query associated with each possible candidate are overlapped word for word by another subsequence at each position. Overlapping the evaluation at a given position is simply word overlapping in the weighted TFIDF style based on frequencies in context (to emphasize less common words)."}, {"heading": "4.2 N-GRAM LANGUAGE MODELS", "text": "We trained an n-gram language model using the KenLM toolkit (Heafield et al., 2013), used Knesser-Ney smoothing and a window size of 5, which performed best in the validation phase, and compared it to a variant of the language model with cache (Kuhn & De Mori, 1990), where we interpolated the n-gram model probabilities linearly with uniigram probabilities calculated from context."}, {"heading": "4.3 SUPERVISED EMBEDDING MODELS", "text": "To test directly how much of the CBT can be resolved by high-quality, dense word representations (word embedding), we implement a supervised embedding model similar to that of (Weston et al., 2010).In these models, we learn both the embedding matrices A, B and Rp \u00b7 d for each word in the vocabulary (p is still the embedding dimension and d is the vocabulary size).For a given input passage q and the possible answer w, the score is calculated as S (q, w) = \u03c6 (q) A > B\u03c6 (w), with the feature function defined in Section 3. These models can be considered as lobotomised memory networks with zero hops, i.e. the attention on the memory component is removed. We encode different parts of the question as an input passage: the entire context + the query, a sub-sequence of the query, defined by a window with a maximum of one word window around each embedding it."}, {"heading": "4.4 RECURRENT LANGUAGE MODELS", "text": "We trained probabilistic RNN language models with LSTM activation units on the training stories (5.5M words of the text) with minibatch SGD to maximize the negative log probability of the next word. Hyperparameters were matched to the validation set. The best model had both hidden levels and word embedding dimension 512. When answering the questions in the CBT, we allow a variant of this model (context + query) to \"burn\" by reading the entire context followed by the query, and another version to read only the query itself (and thus have no access to the context). Unlike the canonical language modeling task, all models have access to the query words by the missing word (i.e. if k is the position of the missing word, we rank the candidate c based on p (q1)."}, {"heading": "4.5 HUMAN PERFORMANCE", "text": "We recruited 15 native English speakers to test randomly selected 10% of each type of CBT question in two modes, either with question only or with question + context (shown to different commentators), and to our knowledge this is the first time human performance has been quantified using a language modeling task based on different word types and context lengths."}, {"heading": "4.6 OTHER RELATED APPROACHES", "text": "Access to document-level functionality can improve both classical language models (Mikolov & Zweig, 2012) and text embedding (Huang et al., 2012). Contrary to the present work, these studies did not examine different representation strategies for the broader context or their effects on the interpretation and prediction of certain word types. Original memory networks (Weston et al., 2015b) used self-monitored memory access and were applied to questions in which language was generated to describe the content of knowledge bases or simulated worlds. Sukhbaatar et al. (2015) and Kumar et al. (2015) consistently trained memory networks with RNN components with soft memory access and applied them to additional language tasks. Attention-based reader models by Hermann et al. (2015) also exhibit many similarities with memory networks that differ in proactive word representation decisions and Hermann et al. (2015) in a proactive way and Hermann et al. (2015)."}, {"heading": "5 RESULTS", "text": "In fact, the LSTM models are better than the people who are able to correctly classify the candidates \"prepositions, but the announcers prefer the less common contexts. In fact, the LSTM models are better than the people in the prepositions, suggesting that there are cases where the prepositions are correct."}, {"heading": "5.1 NEWS ARTICLE QUESTION ANSWERING", "text": "To investigate how well our conclusions generalize to various machine-readable tasks and language styles, we also tested the most powerful memory networks based on the article (Hermann et al., 2015).8 This data set consists of 93k news articles on the CNN website, each associated with a question that came out of the summary of the article, and a single answer that led to a noteworthy entity, and all designated entities that function as potential candidates. We have not warmed the human antennas by including them in the training data, but this could lead to a fair comparison."}, {"heading": "6 CONCLUSION", "text": "We introduced the Children's Book Test, a new benchmark for semantic language modeling. CBT measures how well models can use both local and broader context information to make predictions about different types of words in children's stories. By separating predictions of syntactic function words from more semantically informative terms, CBT provides a robust proxy for how strongly language models can influence applications that require a focus on semantic coherence. We tested a wide range of models on the CBT, each of which represents and retains previously seen content in different ways. This allowed us to gain new insights into the optimal strategies for displaying and accessing semantic information in memory. One consistent finding was that memories that encode subjective chunks (windows) of informative text seem to be most commonly used for neural networks when language is interpreted and modeled vs. the most useful textual task."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors thank H. Pentapelli and M. Paluri for their help in obtaining the human annotations."}, {"heading": "A EXPERIMENTAL DETAILS", "text": "The most important hyperparameters are the embedding dimension p, the learning rate \u03bb, the window size b, the number of hops K, the maximum memory size n (n = all means the use of all potential memory).All models were implemented using the torch library (see Torch.ch).For CBT, all models were trained on all question types: altogether.Optimal hyperparameter values on CBT: \u2022 Embedding model (context + query): p = 300, \u03bb = 0.01. \u2022 Embedding model (query): p = 300, Nexis: Nexis: 0.01. \u2022 Embedding model (query): Nexis: p = 0.005, Embedding model (window). \u2022 Embedding model (query): p = 0.005, Nexis: p = 0.005."}, {"heading": "B RESULTS ON CBT VALIDATION SET", "text": "METHODS NAMED ENTITIES COMMON NOUNS VERBS PREPOSITIONS MAXIMUM FREQUENCY (CORPUS) 0.052 0.192 0.301 0.346 MAXIMUM FREQUENCY (CONTEXT) 0.299 0.273 0.219 0.312 SLIDING WINDOW 0.178 0.199 0.200 0.091 WORD DISTANCE MODEL 0.436 0.371 0.332 0.259 KNESER-NEY LANGUAGE MODEL 0.481 0.577 0.762 0.791 KNESER-NEY LANGUAGE MODEL + CACHE 0.500 0.612 0.693 EMBEDING MODEL (CONTEXT + QUERY 4MOD93) 0.235 0.297 0.356 EMDING MODELLE MODEL WITH MIT WITH MIT WITH MIT WITH MIT WITH MIT WITH MIT WITH MIT WITH MIT WITH MIT WITH MIT WITH MIT WITH MIT WITH MIT WITH MIT WITH MIT WITH MIT WITH MIT WITH MIT WITH MIT WITH MIT WITH MIT MIT WITH MIT WITH MIT MIT WITH MIT MIT WITH MIT MIT MIT WITH MIT WITH MIT MIT MIT MIT WITH MIT MIT MIT WITH MIT WITH MIT MIT MIT MIT WITH MIT MIT MIT WITH MIT MIT MIT WITH MIT WITH MIT WITH MIT WITH MIT MIT WITH MIT MIT WITH MIT WITH MIT MIT MIT WITH MIT MIT WITH MIT WITH MIT MIT WITH MIT MIT WITH MIT MIT MIT WITH MIT MIT MIT MIT WITH MIT MIT"}, {"heading": "C ABLATION STUDY ON CNN QA", "text": "METHODS VALIDATION TEST MEMNNS (WINDOW MEMORY + SELF-SUP. + EXCLUD. COOCURRENCES.) 0.635 0.684 MEMNNS (WINDOW MEMORY + SELF-SUP.) 0.634 0.668 MEMNNS (WINDOW MEM. + SELF-SUP.) -TIME 0.625 0.659 MEMNNS (WINDOW MEM. + SELF-SUP.) -SOFT MEMORY WEIGHTING 0.604 0.620 MEMNNS (WINDOW MEM. + SELF-SUP.) -TIME -SOFT MEMORY WEIGHTING 0.592 0.613 MEMNNS (WINDOW MNNS. + SELF-SUM."}, {"heading": "D EFFECTS OF ANONYMISING ENTITIES IN CBT", "text": "METHODS NAMED ENTITIES COMMON NOUNS VERBS PREPOSITIONS MEMNNS (WORD MEM.) 0.431 0.562 0.798 0.764 MEMNNS (WINDOW MEM.) 0.493 0.554 0.692 0.674 MEMNNS (SENTENCE MEM. + PE) 0.318 0.305 0.502 0.326 MEMNNS (WINDOW MEM. + SUPERV.) 0.666 0.630 0.690 0.703 ANONYMIZED MEMNNS (WINDOW MEM. + SUPERV.) 0.581 0.473 0.474 0.522 In order to see the effects of anonymization of units and words as performed in CNN QA on the self-monitored storage networks on the CBT, we conducted an experiment in which we included the mention of the ten candidates in each question by anonymized placeholders in a series of comparative results where the NBT results were seen relative to the 5 in the table."}], "references": [{"title": "Interaction with context during human sentence processing", "author": ["Altmann", "Gerry", "Steedman", "Mark"], "venue": "Cognition, 30(3):191\u2013238,", "citeRegEx": "Altmann et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Altmann et al\\.", "year": 1988}, {"title": "Word frequency distributions and lexical semantics", "author": ["Baayen", "R Harald", "Lieber", "Rochelle"], "venue": "Computers and the Humanities,", "citeRegEx": "Baayen et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Baayen et al\\.", "year": 1996}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio", "Yoshua", "Simard", "Patrice", "Frasconi", "Paolo"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "The neurobiology of semantic memory", "author": ["Binder", "Jeffrey R", "Desai", "Rutvik H"], "venue": "Trends in cognitive sciences,", "citeRegEx": "Binder et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Binder et al\\.", "year": 2011}, {"title": "Large-scale simple question answering with memory networks", "author": ["Bordes", "Antoine", "Usunier", "Nicolas", "Chopra", "Sumit", "Weston", "Jason"], "venue": "arXiv preprint arXiv:1506.02075,", "citeRegEx": "Bordes et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2015}, {"title": "Transitionbased dependency parsing with stack long short-term memory", "author": ["Dyer", "Chris", "Ballesteros", "Miguel", "Ling", "Wang", "Matthews", "Austin", "Smith", "Noah A"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Unconstrained on-line handwriting recognition with recurrent neural networks", "author": ["Graves", "Alex", "Liwicki", "Marcus", "Bunke", "Horst", "Schmidhuber", "J\u00fcrgen", "Fern\u00e1ndez", "Santiago"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Graves et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2008}, {"title": "Learning to transduce with unbounded memory", "author": ["Grefenstette", "Edward", "Hermann", "Karl Moritz", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": null, "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Goldilocks and the three bears", "author": ["Hassall", "John"], "venue": "Blackie & Son: London,", "citeRegEx": "Hassall and John.,? \\Q1904\\E", "shortCiteRegEx": "Hassall and John.", "year": 1904}, {"title": "Scalable modified Kneser-Ney language model estimation", "author": ["Heafield", "Kenneth", "Pouzyrevsky", "Ivan", "Clark", "Jonathan H", "Koehn", "Philipp"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Heafield et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Heafield et al\\.", "year": 2013}, {"title": "Teaching machines to read and comprehend", "author": ["Hermann", "Karl Moritz", "Ko\u010disk\u00fd", "Tom\u00e1\u0161", "Grefenstette", "Edward", "Espeholt", "Lasse", "Kay", "Will", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Huang", "Eric H", "Socher", "Richard", "Manning", "Christopher D", "Ng", "Andrew Y"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Joulin", "Armand", "Mikolov", "Tomas"], "venue": null, "citeRegEx": "Joulin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2015}, {"title": "A cache-based natural language model for speech recognition", "author": ["Kuhn", "Roland", "De Mori", "Renato"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Kuhn et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Kuhn et al\\.", "year": 1990}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Kumar", "Ankit", "Irsoy", "Ozan", "Su", "Jonathan", "Bradbury", "James", "English", "Robert", "Pierce", "Brian", "Ondruska", "Peter", "Gulrajani", "Ishaan", "Socher", "Richard"], "venue": null, "citeRegEx": "Kumar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2015}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Luong", "Minh-Thang", "Pham", "Hieu", "Manning", "Christopher D"], "venue": "Proceedings of EMNLP,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "The stanford corenlp natural language processing toolkit", "author": ["Manning", "Christopher D", "Surdeanu", "Mihai", "Bauer", "John", "Finkel", "Jenny", "Bethard", "Steven J", "McClosky", "David"], "venue": "In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Context dependent recurrent neural network language model", "author": ["Mikolov", "Tomas", "Zweig", "Geoffrey"], "venue": "In SLT,", "citeRegEx": "Mikolov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "Mctest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Richardson", "Matthew", "Burges", "Christopher JC", "Renshaw", "Erin"], "venue": "In EMNLP,", "citeRegEx": "Richardson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Rush", "Alexander M", "Chopra", "Sumit", "Weston", "Jason"], "venue": "Proceedings of EMNLP,", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "End-to-end memory networks", "author": ["Sukhbaatar", "Sainbayar", "Szlam", "Arthur", "Weston", "Jason", "Fergus", "Rob"], "venue": "Proceedings of NIPS,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Regularization of neural networks using dropconnect", "author": ["Wan", "Li", "Zeiler", "Matthew", "Zhang", "Sixin", "Cun", "Yann L", "Fergus", "Rob"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}, {"title": "Large scale image annotation: learning to rank with joint word-image embeddings", "author": ["Weston", "Jason", "Bengio", "Samy", "Usunier", "Nicolas"], "venue": "Machine learning,", "citeRegEx": "Weston et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2010}, {"title": "Towards ai-complete question answering: a set of prerequisite toy tasks", "author": ["Weston", "Jason", "Bordes", "Antoine", "Chopra", "Sumit", "Mikolov", "Tomas"], "venue": "arXiv preprint arXiv:1502.05698,", "citeRegEx": "Weston et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu", "Kelvin", "Ba", "Jimmy", "Kiros", "Ryan", "Courville", "Aaron", "Salakhutdinov", "Ruslan", "Zemel", "Richard", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "The microsoft research sentence completion challenge", "author": ["Zweig", "Geoffrey", "Burges", "Christopher JC"], "venue": "Technical report, Technical Report MSR-TR-2011-129, Microsoft,", "citeRegEx": "Zweig et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zweig et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 10, "context": "Indeed, we train a Memory Network with these design features to beat the best reported performance on the CNN QA test of entity prediction from news articles (Hermann et al., 2015).", "startOffset": 158, "endOffset": 180}, {"referenceID": 17, "context": "For finer-grained analyses, we evaluated four classes of question by removing distinct types of word: Named Entities, (Common) Nouns, Verbs and Prepositions (based on output from the POS tagger and named-entity-recogniser in the Stanford Core NLP Toolkit (Manning et al., 2014)).", "startOffset": 255, "endOffset": 277}, {"referenceID": 19, "context": "The CBT is also similar to the MCTest of machine comprehension (Richardson et al., 2013), in which children\u2019s stories written by annotators are accompanied by four multiple-choice questions.", "startOffset": 63, "endOffset": 88}, {"referenceID": 10, "context": "There are also similarities between the CBT and the CNN/Daily Mail (CNN QA) dataset recently released by Hermann et al. (2015). This task requires models to identify missing entities from bulletpoint summaries of online news articles.", "startOffset": 105, "endOffset": 127}, {"referenceID": 21, "context": ", 2015a) or language modelling (Sukhbaatar et al., 2015).", "startOffset": 31, "endOffset": 56}, {"referenceID": 21, "context": "To encode word order, time features are added as embeddings indicating the index of each memory, following Sukhbaatar et al. (2015). See Appendix D for a sense of how anonymisation changes the CBT.", "startOffset": 107, "endOffset": 132}, {"referenceID": 21, "context": "We also use Positional Encoding (PE) as introduced by Sukhbaatar et al. (2015) to encode the word positions.", "startOffset": 54, "endOffset": 79}, {"referenceID": 21, "context": "The MemN2N architecture, introduced by Sukhbaatar et al. (2015), allows for a direct training of Memory Networks through backpropagation, and consists of two main steps.", "startOffset": 39, "endOffset": 64}, {"referenceID": 21, "context": "For the lexical memory setting, we also applied ReLU operations to half of the units in each layer following Sukhbaatar et al. (2015).5 In a second stage, an answer distribution \u00e2 is returned given K retrieved memories mo1, .", "startOffset": 109, "endOffset": 134}, {"referenceID": 4, "context": "A related approach was successfully applied by Bordes et al. (2015) to question answering about knowledge bases.", "startOffset": 47, "endOffset": 68}, {"referenceID": 26, "context": "Hard attention yields significant improvements in image captioning (Xu et al., 2015).", "startOffset": 67, "endOffset": 84}, {"referenceID": 26, "context": "Hard attention yields significant improvements in image captioning (Xu et al., 2015). However, where Xu et al. (2015) use the REINFORCE algorithm (Williams, 1992) to train through the max of eq (3), our self-supervision heuristic permits direct backpropagation.", "startOffset": 68, "endOffset": 118}, {"referenceID": 10, "context": "The second method is the word distance benchmark applied by Hermann et al. (2015). For a given instance of a candidate wi in the context, the query q is \u2018superimposed\u2019 on the context so that the missing word lines up with wi, defining a subsequence s of the context.", "startOffset": 60, "endOffset": 82}, {"referenceID": 9, "context": "We trained an n-gram language model using the KenLM toolkit (Heafield et al., 2013).", "startOffset": 60, "endOffset": 83}, {"referenceID": 23, "context": "To directly test how much of the CBT can be resolved by good quality dense representations of words (word embeddings), we implement a supervised embedding model similar to that of (Weston et al., 2010).", "startOffset": 180, "endOffset": 201}, {"referenceID": 20, "context": "We similarly apply a contextbased recurrent model to our language-modelling tasks, but opt for the convolutional presentation of the context applied by Rush et al. (2015) for summarisation.", "startOffset": 152, "endOffset": 171}, {"referenceID": 12, "context": "Access to document-level features can improve both classical language models (Mikolov & Zweig, 2012) and word embeddings (Huang et al., 2012).", "startOffset": 121, "endOffset": 141}, {"referenceID": 11, "context": "Access to document-level features can improve both classical language models (Mikolov & Zweig, 2012) and word embeddings (Huang et al., 2012). Unlike the present work, these studies did not explore different representation strategies for the wider context or their effect on interpreting and predicting specific word types. The original Memory Networks (Weston et al., 2015b) used self-supervised memory access and were applied to question-answering tasks in which language was generated to describe the content of knowledge bases or simulated worlds. Sukhbaatar et al. (2015) and Kumar et al.", "startOffset": 122, "endOffset": 577}, {"referenceID": 11, "context": "Access to document-level features can improve both classical language models (Mikolov & Zweig, 2012) and word embeddings (Huang et al., 2012). Unlike the present work, these studies did not explore different representation strategies for the wider context or their effect on interpreting and predicting specific word types. The original Memory Networks (Weston et al., 2015b) used self-supervised memory access and were applied to question-answering tasks in which language was generated to describe the content of knowledge bases or simulated worlds. Sukhbaatar et al. (2015) and Kumar et al. (2015) trained Memory Networks with RNN components end-to-end with soft memory access, and applied them to additional language tasks.", "startOffset": 122, "endOffset": 601}, {"referenceID": 10, "context": "The attention-based reading models of Hermann et al. (2015) also have many commonalities with Memory Networks, differing in word representation choices and attention procedures.", "startOffset": 38, "endOffset": 60}, {"referenceID": 10, "context": "The attention-based reading models of Hermann et al. (2015) also have many commonalities with Memory Networks, differing in word representation choices and attention procedures. Both Kumar et al. (2015) and Hermann et al.", "startOffset": 38, "endOffset": 203}, {"referenceID": 10, "context": "The attention-based reading models of Hermann et al. (2015) also have many commonalities with Memory Networks, differing in word representation choices and attention procedures. Both Kumar et al. (2015) and Hermann et al. (2015) propose bidirectional RNNs as a way of representing previously read text.", "startOffset": 38, "endOffset": 229}, {"referenceID": 5, "context": "Other recent papers have proposed RNN-like architectures with new ways of reading, storing and updating information to improve their capacity to learn algorithmic or syntactic patterns (Joulin & Mikolov, 2015; Dyer et al., 2015; Grefenstette et al., 2015).", "startOffset": 185, "endOffset": 255}, {"referenceID": 7, "context": "Other recent papers have proposed RNN-like architectures with new ways of reading, storing and updating information to improve their capacity to learn algorithmic or syntactic patterns (Joulin & Mikolov, 2015; Dyer et al., 2015; Grefenstette et al., 2015).", "startOffset": 185, "endOffset": 255}, {"referenceID": 2, "context": "Bengio et al. (1994)).", "startOffset": 0, "endOffset": 21}, {"referenceID": 10, "context": "To examine how well our conclusions generalise to different machine reading tasks and language styles, we also tested the best-performing Memory Networks on the CNN QA task (Hermann et al., 2015).", "startOffset": 173, "endOffset": 195}, {"referenceID": 10, "context": "(\u2217)Results taken from Hermann et al. (2015).", "startOffset": 22, "endOffset": 44}, {"referenceID": 11, "context": "Our use of an ensemble is an alternative way of replicating the application of dropout (Hinton et al., 2012) in the previous best approaches (Hermann et al.", "startOffset": 87, "endOffset": 108}, {"referenceID": 10, "context": ", 2012) in the previous best approaches (Hermann et al., 2015) as ensemble averaging has similar effects to dropout (Wan et al.", "startOffset": 40, "endOffset": 62}, {"referenceID": 22, "context": ", 2015) as ensemble averaging has similar effects to dropout (Wan et al., 2013).", "startOffset": 61, "endOffset": 79}, {"referenceID": 6, "context": "The attentive/impatient reading models encode the articles using bidirectional RNNs (Graves et al., 2008).", "startOffset": 84, "endOffset": 105}, {"referenceID": 9, "context": "As shown in Table 3, our window model without self-supervision achieves similar performance to the best approach proposed for the task by Hermann et al. (2015) when using an ensemble of MemNN models.", "startOffset": 138, "endOffset": 160}, {"referenceID": 6, "context": "The attentive/impatient reading models encode the articles using bidirectional RNNs (Graves et al., 2008). For each word in the article, the combined hidden state of such an RNN naturally focuses on a window-like chunk of surrounding text, much like the window-based memory network or the CLSTM. Together, these results therefore support the principle that the most informative representations of text correspond to sub-sentential chunks. Indeed, the observation that the most informative representations for neural language models correspond to small chunks of text is also consistent with recent work on neural machine translation, in which Luong et al. (2015) demonstrated improved performance by restricting their attention mechanism to small windows of the source sentence.", "startOffset": 85, "endOffset": 663}], "year": 2015, "abstractText": "We introduce a new test of how well language models capture meaning in children\u2019s books. Unlike standard language modelling benchmarks, it distinguishes the task of predicting syntactic function words from that of predicting lowerfrequency words, which carry greater semantic content. We compare a range of state-of-the-art models, each with a different way of encoding what has been previously read. We show that models which store explicit representations of long-term contexts outperform state-of-the-art neural language models at predicting semantic content words, although this advantage is not observed for syntactic function words. Interestingly, we find that the amount of text encoded in a single memory representation is highly influential to the performance: there is a sweet-spot, not too big and not too small, between single words and full sentences that allows the most meaningful information in a text to be effectively retained and recalled. Further, the attention over such window-based memories can be trained effectively through self-supervision. We then assess the generality of this principle by applying it to the CNN QA benchmark, which involves identifying named entities in paraphrased summaries of news articles, and achieve state-of-the-art performance.", "creator": "LaTeX with hyperref package"}}}