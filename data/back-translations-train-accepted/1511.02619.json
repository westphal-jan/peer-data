{"id": "1511.02619", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Nov-2015", "title": "Decomposition Bounds for Marginal MAP", "abstract": "Marginal MAP inference involves making MAP predictions in systems defined with latent variables or missing information. It is significantly more difficult than pure marginalization and MAP tasks, for which a large class of efficient and convergent variational algorithms, such as dual decomposition, exist. In this work, we generalize dual decomposition to a generic power sum inference task, which includes marginal MAP, along with pure marginalization and MAP, as special cases. Our method is based on a block coordinate descent algorithm on a new convex decomposition bound, that is guaranteed to converge monotonically, and can be parallelized efficiently. We demonstrate our approach on marginal MAP queries defined on real-world problems from the UAI approximate inference challenge, showing that our framework is faster and more reliable than previous methods.", "histories": [["v1", "Mon, 9 Nov 2015 10:21:39 GMT  (1610kb,D)", "http://arxiv.org/abs/1511.02619v1", "NIPS 2015 (full-length)"]], "COMMENTS": "NIPS 2015 (full-length)", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.IT math.IT stat.ML", "authors": ["wei ping", "qiang liu", "alexander t ihler"], "accepted": true, "id": "1511.02619"}, "pdf": {"name": "1511.02619.pdf", "metadata": {"source": "CRF", "title": "Decomposition Bounds for Marginal MAP", "authors": ["Wei Ping", "Qiang Liu", "Alexander Ihler"], "emails": ["wping@ics.uci.edu", "ihler@ics.uci.edu", "qliu@cs.dartmouth.edu"], "sections": [{"heading": null, "text": "Marginal MAP conclusions involve the creation of MAP predictions in systems defined by latent variables or missing information. It is much more difficult than pure marginalization and MAP tasks, for which a large class of efficient and convergent variation algorithms such as dual decomposition exists. In this thesis, we generalize dual decomposition to a generic power sum inference that includes marginal MAPs in addition to pure marginalization and MAP as special cases. Our method is based on a block-coordinate-descent algorithm based on a new convex decomposition limit that is guaranteed to be monotonously converged and efficiently paralleled."}, {"heading": "1 Introduction", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "2 Background", "text": "A Markov random field (MRF) on discrete random variables x = [x1,.., xn] \"Xn\" is a probability distribution, p (x; \u03b8) = exp., \"unless it is a series of subsets of variables, each of which is associated with a factor (x\u03b1), and\" p \"is the log partition function. We associate an undirected graph maxi x\" exp \"(V, E) with p\" p \"(x) by mapping each xi to a node i\" V \"and an edge ij\" E \"iff\" there is \"F\" that there is such that \"i.\" We say Node i and j \"are neighbors, if ij\" E. Then F is the subset of clips. \""}, {"heading": "3 Related Work", "text": "This year, it is only a matter of time before agreement is reached."}, {"heading": "4 Fully Decomposed Upper Bound", "text": "In this section, we develop a new general form of upper limit and provide an efficient, monotonously convergent optimization algorithm. Our new limit is based on the complete decomposition of the graph into unconnected cliques, which allows for a very efficient local calculation, but can still be as narrow as AMR or the TRW limit with a large collection of trees once the weights and shift variables are correctly selected or optimized. Our limit is reduced to double decomposition for MAP tasks, but applies to more general mixed inference settings. Our main result is based on the following generalization of classical Holder's inequality [7]: Despite the term \"dual decomposition\" used in this work, we refer to decomposition limits as \"primal\" limits, as they can be considered directly limiting the result of variable elimination. This is in contrast, for example, to the linear programming relaxation of MAP, which only limits the result."}, {"heading": "4.1 Including Cost-shifting Variables", "text": ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"}, {"heading": "4.2 Dual Form and Connection With Existing Bounds", "text": "It is easy to see that our boundary in (5) is reduced to dual decomposition (31) when applied to MAP conclusions with all perci = 0 and thus wi = w\u03b1i = 0. On the other hand, its connection with sum-consequence boundaries such as AMR and TRW is clearer through a dual representation of (5): Theorem 4.2. The narrowest upper boundary that can be reached through (5), that is, min w minutive boundaries such as AMR and TRW become clearer through a dual representation of (5): The narrowest upper boundary is through (5), that is, min w minuzier L (7) = min max b b (G) {< p > p, p > i, b > wiH wiH wiH (xi; bi)."}, {"heading": "5 Monotonically Tightening the Bound", "text": "In this section, we propose a parentage algorithm for block coordinates (algorithm 1) to minimize the upper limit L (\u03b4, w) in (5) w.r.t. of shifting variables \u03b4 and weights etc. Our algorithm has a monotonous convergence property and enables efficient, distributable local calculations based on the complete decomposition of our boundary. Our framework allows generic power sum inference, including max, sum or mixed inference, as special cases by specifying different weights."}, {"heading": "5.1 Moment Matching and Entropy Matching", "text": "We start with the derivation of the gradient of the individual cliques. We show that the zero-gradient equation has a simple form of moment adjustment that forces consistency between the individual beliefs with their related clique beliefs and the concepts of weight."}, {"heading": "5.2 Block Coordinate Descent", "text": "We derive a block coordination method to minimize our limits, in which we can move through all nodes and update each block depending on whether the variables have a zero weight: (1) For nodes with a zero value of 0 (corresponding to the maximum nodes in the marginal MAP) we derive a closed coordinate rule for the associated shifts. (2) For nodes with a zero value of 0 (e.g. sum of nodes in marginal MAP) we lack a closed form of update for a closed form of Ni and wNi, and optimization by local gradation. (2) The nodes with a zero value of 0 (e.g. sum of nodes in marginal MAP) we lack a closed form of update for a closed form of Ni and wNi, and optimization by local gradation. (2) The nodes with a zero value of 0 (e.g. sum of nodes in marginal MAP) we lack a closed form of an update."}, {"heading": "6 Experiments", "text": "This year, it has come to the point that it is only a matter of time before it happens, until it happens, \"he told the German Press Agency in an interview with\" Welt am Sonntag. \""}, {"heading": "7 Conclusion", "text": "In this paper, we propose a new class of decomposition limits for general powered sum conclusions, which can represent a large class of primary variation limits, but are much more efficient in computational terms. In contrast to earlier primary sum limits, our limit disintegrates into computations of small local cliques, increasing efficiency and enabling parallel and monotonous optimization. We derive a lineage algorithm for block coordinates to optimize our limit both through cost-shifting parameters (repair parameters) and weights (fraction numbers), generalizing double decomposition and having a similar monotonic convergence property. Taking advantage of its monotonic convergence, our new algorithm can be widely applied as a building block for improved heuristic construction in search or more efficient learning algorithms."}, {"heading": "Acknowledgments", "text": "This work is partially funded by NSF grants IIS-1065618 and IIS-1254071. Alexander Ihler is also partially funded by the US Air Force under contract number FA8750-14-C-0011 under the DARPA-PPAML program."}, {"heading": "A Experiment on Ising grid", "text": "Our GDD directly optimizes a primary boundary, thus guaranteeing an upper boundary of the partition function even before the algorithm converges, enabling a desirable \"at any time\" property. In contrast, typical implementations of tree-weighted faith propagation optimize the dual free energy function [34], and it is not guaranteed that it is a boundary before convergence. We illustrate this point by experimenting on a toy 5-5 output grid with parameters generated by the normal distribution N (0, 2), and half nodes selected as the maximum nodes for marginal MAP. Figure 4 (a) - (b) shows the TRW target of free energy and GDD, WMB upper boundaries across iterations; we can see that TRW violates the upper boundary property before convergence, while GDD and WMB always give valid boundaries."}, {"heading": "B More Results on Diagnostic Bayesian Networks", "text": "In addition to the marginal MAP results for BN-1 and BN-2 in the main text, we vary the percentage of maximum nodes in generating the marginal MAP problems; the results obtained in Figure 5 (a) - (b) are the best obtained by the various algorithms with the first 20 iterations. In all cases, the results of the GDD are as good or better than WMB. WMB-0.5 (WMB with attenuation ratio 0.5) seems to work well for pure sum and maximum problems (MAP), i.e. when the percentage of maximum nodes is 0% or 100%, but performs very poorly in intermediate settings. The significantly more attenuated WMB-0.04 or WMB-0.02 perform better on average, but exhibit much slower convergence."}, {"heading": "C More Results on Pedigree Linkage Analysis", "text": "We test our algorithm on an additional 6 models of family tree linkage analysis from the UAI08 inference challenge. We construct marginal MAP problems by randomly selecting 50% of the nodes as maximum nodes, and report all the results in Figure 6. We find that our algorithm consistently exceeds WMB with the best possible attenuation ratio."}, {"heading": "D Extensions to Junction Graph", "text": "Our boundary (5) in the main text uses a standard \"factor diagram\" representation in which cost shifts are defined for each pair of variable factors (i, \u03b1) and functions of individual variables are xi. We can expand our boundary to use more general shift parameters by using a set of clusters; this allows us to exploit higher-order clip structures that result in better performance. Let's (C, S) be an intersection diagram of p (x) in which C = [c | c] V} is the set of clusters, and S = ck \u00b2 cl | ck, cl \u00b2 C} is the set of delimiters. Assume p (x;) can be repaired in the form of p (x;)."}, {"heading": "E Proof of Thereom 4.1", "text": "Evidence. Note that the inequality is in the case of woods [\u2211 x-j fj (x) 1 / 0] [\u2211 x-fj (x) 1 / 1] [j], where {fj (x)} are arbitrary positive functions, and in the case of {\u0441j} non-negative numbers corresponding in turn [x1, x2, \u00b7 \u00b7, xn]. Note that we extend the inequality by defining the power sum with \u0394j = 0 to be equal to the maximum operator. Our result follows by applying the inequality in each xi sequentially along the elimination order [x1, x2, \u00b7, xn]."}, {"heading": "F Dual Representations", "text": "The log partition function \u03a6 (\u03b8) has the following variation form (dual) (dual): b) = \"Log\" (\"Exp\" (\"X\") = \"X\" (\"X\") = \"X\" (\"X\") = \"X\" (\"X\") = \"X\" (\"X\") = \"X\" (\"X\") = \"X\" (\"X\") = \"X\" (\"X\") = \"X\" (\"X\") = \"X\" (\"X\") = \"X\" (\"X\") = \"X\" (\"X\") = \"X\" (\"X\"); \"X\" (\"X\") = \"X\" (\"X\") \") =\" X \"(\" X \")\" (\"\") = \"X\" (\") =\" X \"(\")."}, {"heading": "G Proof of Therom 5.1", "text": "For each digit (xi), the relevant digit in L (xi) = digit (xi) = digit (xi) = digit (xi) = digit (i) = digit (i) = digit (i) = digit (i) = digit (i) = digit (i) = digit (i) = digit (i) = digit (i) = digit (i) = digit (i) = digit (i) = digit (i) + digit (i) + digit (i) = digit (i) = digit (i) = digit (i) = digit (i) = digit (i) = digit (i) = digit (i) = (i) (=) (i) (=) (i) (=) (i) = (i)."}, {"heading": "H Derivations of Closed-form Update", "text": "First, we derive the closed-form update rule for the closed-form rule for the closed-form update for the closed-form rule for the closed-form update: closed-form update for the closed-form update for the closed-form update for the closed-form update for the closed-form update for the closed-form update for the closed-form update (i.e.) closed-form update for the closed-form update (i.e.) closed-form update for the closed-form update (i.e.) closed-form update for the closed-form update (i.e.) closed-form update for the closed-form update (i.e.) closed-form update for the closed-form update (i.e.).) closed-form update for the closed-form update (i.e.)."}, {"heading": "I Derivations of Gradient", "text": "Proposition I.1. Given weight vector wxi = [w\u03b11, \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7, w\u03b1c] connected with variables x\u03b1 = {x1, \u00b7, xi \u2212 xi, \u00b7 \u00b7, xc} on clique \u03b1 =, where c = \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 is the sum of output over clique \u03b1, \u03a6w\u03b1 (\u03b1) = log w\u0430 \u2212 xi \u2212 xi \u2212 xi, xi \u2212 xi, xi \u00b2 on clique \u03b1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 is the sum of output over clique \u03b1 (\u03b1), \u03a6i \u03b1 \u00b7 w\u03b1 (\u03b1) \u00b7 w\u03b1 (\u03b1) \u00b7 w\u03b1i (\u03b1) \u00b7 xi \u2212 xi, xi \u2212 xi \u2212 xi, xi \u2212 xi \u2212 xi \u00b2 as the sum of output up to x1: i, Z0 (x\u03b1i) = exp [\u03b1i) = \u03b1 \u00b2, \u03b1 \u00b7 w\u03b1 (\u03b1) \u00b7 w\u03b1) \u00b7 xi, xi \u2212 xi \u2212 xi \u2212 xi, xi \u2212 xi \u2212 xi \u2212 xi, xi \u2212 xi \u2212 xi, xi \u2212 xi \u2212 xi \u2212 xi, xi \u2212 xi \u2212 xi, \u2212 xi \u2212 xi, \u2212 xi \u2212 xi, \u2212 xi \u2212 \u2212 xi, \u2212 xi \u2212 \u2212 \u2212 xi, \u2212 xi \u2212 \u2212 \u2212 xi, \u2212 xi \u2212 \u2212 \u2212 \u2212 \u2212 \u2212, xxi \u2212 xi \u2212 \u2212 \u2212 \u2212 \u2212, w\u03b1, xxi \u2212 xi \u2212 xi \u2212 \u2212 xi, \u2212 xi \u2212 xi \u2212 xi \u2212 \u2212 xi, \u2212 xi \u2212 xi \u2212 \u2212 xi, \u2212 xi \u2212 xi \u2212 xi \u2212 xxi, \u2212 xi \u2212 xi \u2212 xi \u2212 xi, \u2212 xi \u2212 xi \u2212 xi \u2212 xxi \u2212 xi \u2212 xi \u2212 xxi, \u2212 xi \u2212 xi \u2212 xi \u2212 xi \u2212 xxi \u2212 xi \u2212 xxxi, \u2212 xi \u2212 xi \u2212 xi \u2212 xi \u2212 x1, \u2212 xi \u2212 xi \u2212 xi \u2212 xi \u2212 xxi \u2212 xxi \u2212 xxi \u2212 xxxi \u2212 xxi \u2212 xxi, \u2212 xi \u2212 xxi \u2212 xxi \u2212 xxi \u2212 x1, \u2212 xi \u2212 xi \u2212 xxi \u2212 xi \u2212 xi \u2212 xxi \u2212 xi \u2212 xxi \u2212 xi, \u2212 xi \u2212 xi \u2212 xxi \u2212 x1, \u2212 xxi \u2212 xi \u2212 xxi \u2212 xxi \u2212 xxi \u2212 xxi \u2212 xi \u2212 xxi, \u2212 xi \u2212 xi \u2212 xxi, \u2212 xxi \u2212 xxi \u2212 xxxxxxxx"}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "Marginal MAP inference involves making MAP predictions in systems defined<lb>with latent variables or missing information. It is significantly more difficult than<lb>pure marginalization and MAP tasks, for which a large class of efficient and con-<lb>vergent variational algorithms, such as dual decomposition, exist. In this work, we<lb>generalize dual decomposition to a generic power sum inference task, which in-<lb>cludes marginal MAP, along with pure marginalization and MAP, as special cases.<lb>Our method is based on a block coordinate descent algorithm on a new convex<lb>decomposition bound, that is guaranteed to converge monotonically, and can be<lb>parallelized efficiently. We demonstrate our approach on marginal MAP queries<lb>defined on real-world problems from the UAI approximate inference challenge,<lb>showing that our framework is faster and more reliable than previous methods.", "creator": "LaTeX with hyperref package"}}}