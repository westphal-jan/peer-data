{"id": "1511.02793", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Nov-2015", "title": "Generating Images from Captions with Attention", "abstract": "Motivated by the recent progress in generative models, we introduce a model that generates images from natural language descriptions. The proposed model iteratively draws patches on a canvas, while attending to the relevant words in the description. After training on Microsoft COCO, we compare our model with several baseline generative models on image generation and retrieval tasks. We demonstrate that our model produces higher quality samples than other approaches and generates images with novel scene compositions corresponding to previously unseen captions in the dataset.", "histories": [["v1", "Mon, 9 Nov 2015 18:18:53 GMT  (889kb,D)", "http://arxiv.org/abs/1511.02793v1", "Under review as a conference paper at ICLR 2016"], ["v2", "Mon, 29 Feb 2016 17:56:29 GMT  (889kb,D)", "http://arxiv.org/abs/1511.02793v2", "Published as a conference paper at ICLR 2016"]], "COMMENTS": "Under review as a conference paper at ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["elman mansimov", "emilio parisotto", "jimmy lei ba", "ruslan salakhutdinov"], "accepted": true, "id": "1511.02793"}, "pdf": {"name": "1511.02793.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Elman Mansimov", "Emilio Parisotto", "Jimmy Lei"], "emails": ["emansim@cs.toronto.edu,", "eparisotto@cs.toronto.edu,", "rsalakhu@cs.toronto.edu,", "jimmy@psi.utoronto.ca"], "sections": [{"heading": null, "text": "Motivated by recent advances in generative models, we present a model that generates images from descriptions of natural language. The proposed model iteratively draws patches on a canvas, taking into account the relevant words in the description. After a training course on Microsoft COCO, we compare our model with several generative base models for image generation and retrieval. We show that our model produces higher quality samples than other approaches and produces images with novel scene compositions that match previously invisible captions in the dataset."}, {"heading": "1 INTRODUCTION", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "2 RELATED WORK", "text": "While most of the recent successes have been achieved through discriminatory models, generative models have not yet enjoyed the same success. Most of the earlier work in generative models has focused on variants of Boltzmann machines (Smolensky, 1986; Salakhutdinov & Hinton, 2009) and Deep Belief Networks (Hinton et al., 2006). While these models are very powerful, any iteration of education requires a computationally costly step by MCMC to integrate derivatives of an intractable partition constant (normalization constant), making it difficult to scale them to large datasets."}, {"heading": "3 MODEL", "text": "Our proposed model defines a generative process of images based on captions. In particular, captions are represented as a sequence of consecutive words, and images are represented as a sequence of image surfaces that are drawn onto a canvas over time. It can be viewed as part of the sequence-to-sequence framework (Sutskever et al., 2014; Cho et al., 2014; Srivastava et al., 2015)."}, {"heading": "3.1 LANGUAGE MODEL: THE BIDIRECTIONAL ATTENTION RNN", "text": "Let us be the input signature, represented as a sequence of 1-of-K encoded words y = (y1, y2,..., yN), where K is the size of the vocabulary and N is the length of the sequence. We obtain the markup sentence representation by first transforming each word yi into a m-dimensional vector representation hlangi, i = 1,.., N with the bidirectional RNN. In a bidirectional RNN, the two LSTMs (Hochreiter & Schmidhuber, 1997) process the input sequence from both forward and backward with Forge Gates (Gers et al., 2000). The forward LSTM calculates the sequence of hidden states [\u2212 \u2192 h lang1, \u2212 h lang2, \u2212 h lang2,..., \u2212 h lang2], \u2212 \u2192 h langN], while the backward LSTM computes the sequence of hidden states."}, {"heading": "3.2 IMAGE MODEL: THE CONDITIONAL DRAW NETWORK", "text": "To generate an image, we need to expand the DRAW network (Gregor et al., 2015) to obtain a caption at each step, as shown in Figure 2. \u2212 The conditional DRAW network is a stochastic recurrent neural network consisting of a sequence of latent variables. \u2212 The caption is accumulated in all T steps. \u2212 The caption is h-by-w in size and only one color channel. Unlike the original DRAW network, where latent variables are independent, the latent variables in the proposed alignDRAW model depend on the previous hidden states of the LSTM hgent \u2212 1, except for P (Z1) = N (0, I)."}, {"heading": "3.3 LEARNING", "text": "The model is designed in such a way that it can accommodate one x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "3.4 GENERATING IMAGES FROM CAPTIONS", "text": "Due to the blur of the samples generated by the DRAW model, we perform an additional post-processing step in which we use an opposing network trained on remnants of a Lappish pyramid based on the erratic representation of the captions (Kiros et al., 2015) to sharpen the generated images, similar to (Denton et al., 2015).By setting the previous value of the opposing generator to its mean, it is treated as a deterministic neural network that allows us to define the conditional data term in Equation 14 on the sharpened images and estimate the variable lower limit accordingly."}, {"heading": "4 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 MICROSOFT COCO", "text": "Microsoft COCO (Lin et al., 2014) is a large dataset of 82,783 images, each with at least 5 captions. A rich collection of images with a variety of styles, backgrounds and objects makes the task of learning a good generative model very difficult. To ensure consistency with related caption work, we used only the first five captions in the training and evaluation of our model. Images were reduced to 32 x 32 pixels to match other tiny datasets (Krizhevsky, 2009). In the following subsections, we analyzed both the qualitative and quantitative aspects of our model and compared its performance with other related generative models."}, {"heading": "4.1.1 ANALYSIS OF GENERATED IMAGES", "text": "The main objective of this work is to learn a model that can understand the semantic meaning expressed in the text descriptions of the images, such as the properties of objects, the relationships between them, and then use this knowledge to generate relevant images. To understand the 2To see more generated images, visit http: / / www.cs.toronto.edu / \u02dc emansim / cap2im.htmA very large commercial airplane flying in blue sky. A herd of elephants walking across a dry grassy field. A herd of elephants walking across a green grassy field is strange. Figure 4: Bottom: Examples of changing the background while the caption is fixed. Above: The respective closest training images are based on pixel-by-distance L2. The closest images from the training set also indicate that the model simply copies the patterns we observed during the learning phase."}, {"heading": "4.1.2 ANALYSIS OF ATTENTION", "text": "It turned out that during the generational step, the model largely focused on the specific words (or nearby words) that carried the most important semantic meaning expressed in the sentences. Attention scores of the words in the sentences helped us interpret the reasons why the model made the changes when we flipped through certain words. For example, in Fig. 5, top row, we can see that attention to words in the sentence did not change drastically when we changed the word \"desert\" to \"forest,\" suggesting that the model looked at \"desert\" and \"forest\" in their respective sentences with relatively equal probability, making the correct changes. In contrast, if we swap the words \"beach\" and \"sun,\" we can see a drastic change between sentences in the probability distribution over words, indicating that the model completely ignores the word \"sun\" in the second row."}, {"heading": "4.1.3 COMPARISON WITH OTHER MODELS", "text": "In fact, it is as if most of them are able to surpass themselves, and that they see themselves able to surpass themselves, \"he said.\" But it is not as if. \"\" It is as if. \"\" It is as if. \"\" It is as if. \"\" It is as if. \"\" It is as if. \"\" It is as if. \"\" It is as if. \"\" It is as if. \"\" It is as if. \"\" It is as if. \"It is as if.\" It is as if. \"It is as if.\" \"It is as if.\" \"\" It is as if. \".\" \"It is as if.\" \"It is as if.\" \"It is as if.\" \"It is as if.\" \"It is so.\". \"\" It is as if. \"\" It is as if. \""}, {"heading": "5 DISCUSSION", "text": "In this article, we showed that the alignDRAW model, a combination of a recurring varying autoencoder with an alignment model over words, managed to produce images that correspond to a given input signature. By extensively using attention mechanisms, our model gained several advantages: namely, the use of the visual attention mechanism enabled us to break down the problem of image generation into a series of steps, rather than in a single forward step, while attention to words gave us an insight whenever our model could not generate a relevant image. In addition, our model generated images that correspond to captions that were generalized beyond the training set, such as sentences describing novel scenarios that are highly unlikely in real life. Since the alignDRAW model tends to produce slightly blurred patterns, we expanded the model to include a sharpening post-processing step, in which samples are added to the RAW, which is not an ideal solution to the edges of the RAW."}, {"heading": "APPENDIX A: MNIST WITH CAPTIONS", "text": "As an additional experiment, we trained our model on the MNIST artificial caption dataset; either one or two digits from the MNIST training dataset were placed on a 60 x 60 cm blank image; one digit was placed in one of the four corners (top left, top right, bottom left, or bottom right) of the image; two digits were placed either horizontally or vertically in a non-overlapping manner; and the corresponding artificial captions indicated the identity of each digit along with its relative positions, e.g. \"The number three is located at the top of the number one\" or \"The number seven is located at the bottom left of the image.\" The generated images along with the mindfulness alignments are displayed in Figure 6. The model correctly displayed the indicated digits in the described positions and even managed to generalize the configurations that were never present during the training to some extent well. In the case of generating the two steps, the model would dynamically focus the attention on where the specific digits would be in a particular location."}, {"heading": "APPENDIX B: TRAINING DETAILS", "text": "HYPERPARAMETERSEach parameters in alignDRAW which was initialized by stamping from a Gaussian distribution with mean 0 and standard deviation 0.01. The model was trained with RMS prop with an initial learning rate of 0.001. For the Microsoft COCO task, we trained our model for 18 epochs. The learning rate was reduced to 0.0001 after 11 epochs. For the MNIST with Captions task, the model was trained for 150 epochs and the learning rate was reduced to 0.0001 after 110 epochs. During each epoch, 10,000 training samples were randomly used for learning. The norm of progression was truncated at 10 during training to avoid the exploding progressions. We used a vocabulary size of K = 25323 and K = 22 for the Microsoft COCO and MNIST with capture data sets."}, {"heading": "APPENDIX C: EFFECT OF SHARPENING IMAGES.", "text": "Some examples of generated images before (top row) and after (bottom row) that sharpen images using an opposing network trained on remnants of a Lappish pyramid conditioned on the skipped vectors of captions."}], "references": [{"title": "Data generation as sequential decision making", "author": ["Bachman", "Philip", "Precup", "Doina"], "venue": "In NIPS,", "citeRegEx": "Bachman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bachman et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Theano: new features and speed improvements", "author": ["Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian J", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Warde-Farley", "David", "Bengio", "Yoshua"], "venue": "CoRR, abs/1211.5590,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "van Merrienboer", "Bart", "G\u00fcl\u00e7ehre", "\u00c7aglar", "Bahdanau", "Dzmitry", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "In EMNLP,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["Denton", "Emily L", "Chintala", "Soumith", "Szlam", "Arthur", "Fergus", "Robert"], "venue": "In NIPS,", "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["Gers", "Felix", "Schmidhuber", "J\u00fcrgen", "Cummins", "Fred"], "venue": "Neural Computation,", "citeRegEx": "Gers et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian J", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron C", "Bengio", "Yoshua"], "venue": "In NIPS,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Hybrid speech recognition with deep bidirectional LSTM", "author": ["A. Graves", "N. Jaitly", "Mohamed", "A.-r"], "venue": "In IEEE Workshop on Automatic Speech Recognition and Understanding,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Wierstra", "Daan"], "venue": "In ICML,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["Gutmann", "Michael", "Hyv\u00e4rinen", "Aapo"], "venue": "In AISTATS,", "citeRegEx": "Gutmann et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gutmann et al\\.", "year": 2010}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Hinton", "Geoffrey E", "Osindero", "Simon", "Teh", "Yee Whye"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Karpathy", "Andrej", "Li", "Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "In ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R. Zemel"], "venue": "In ICML,", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["Kiros", "Ryan", "Salakhutdinov", "Ruslan", "Zemel", "Richard S"], "venue": "CoRR, abs/1411.2539,", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Skip-thought vectors", "author": ["Kiros", "Ryan", "Zhu", "Yukun", "Salakhutdinov", "Ruslan", "Zemel", "Richard S", "Torralba", "Antonio", "Urtasun", "Raquel", "Fidler", "Sanja"], "venue": "In NIPS,", "citeRegEx": "Kiros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex"], "venue": "Master\u2019s Thesis,", "citeRegEx": "Krizhevsky and Alex.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Alex.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Microsoft COCO: Common objects in context", "author": ["T.Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "In ECCV,", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Deep boltzmann machines", "author": ["Salakhutdinov", "Ruslan", "Hinton", "Geoffrey E"], "venue": "In AISTATS,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2009}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "In ICLR,", "citeRegEx": "Simonyan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2015}, {"title": "Information processing in dynamical systems: foundations of harmony theory", "author": ["Smolensky", "Paul"], "venue": "Parallel Distributed Processing: Explorations in the Microstructure of Cognition,", "citeRegEx": "Smolensky and Paul.,? \\Q1986\\E", "shortCiteRegEx": "Smolensky and Paul.", "year": 1986}, {"title": "Unsupervised learning of video representations using LSTMs", "author": ["Srivastava", "Nitish", "Mansimov", "Elman", "Salakhutdinov", "Ruslan"], "venue": "In ICML,", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["Vinyals", "Oriol", "Toshev", "Alexander", "Bengio", "Samy", "Erhan", "Dumitru"], "venue": "In CVPR,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Image quality assessment: from error visibility to structural similarity", "author": ["Wang", "Zhou", "Bovik", "Alan C", "Sheikh", "Hamid R", "Simoncelli", "Eero P"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Wang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2004}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu", "Kelvin", "Ba", "Jimmy", "Kiros", "Ryan", "Cho", "Kyunghyun", "Courville", "Aaron C", "Salakhutdinov", "Ruslan", "Zemel", "Richard S", "Bengio", "Yoshua"], "venue": "In ICML,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 25, "context": "One approach is to learn a generative model of text conditioned on images, known as caption generation (Kiros et al., 2014a; Karpathy & Li, 2015; Vinyals et al., 2015; Xu et al., 2015).", "startOffset": 103, "endOffset": 184}, {"referenceID": 27, "context": "One approach is to learn a generative model of text conditioned on images, known as caption generation (Kiros et al., 2014a; Karpathy & Li, 2015; Vinyals et al., 2015; Xu et al., 2015).", "startOffset": 103, "endOffset": 184}, {"referenceID": 8, "context": "By extending the Deep Recurrent Attention Writer (DRAW) (Gregor et al., 2015), our model iteratively draws patches on a canvas, while attending to the relevant words in the description.", "startOffset": 56, "endOffset": 77}, {"referenceID": 4, "context": "The images generated by our alignDRAW model are refined in a post-processing step by a deterministic Laplacian pyramid adversarial network (Denton et al., 2015).", "startOffset": 139, "endOffset": 160}, {"referenceID": 19, "context": "We further illustrate how our method, learned on Microsoft COCO (Lin et al., 2014), generalizes to captions describing novel scenes that are not seen in the dataset, such as \u201cA stop sign is flying in blue skies\u201d (see Fig.", "startOffset": 64, "endOffset": 82}, {"referenceID": 18, "context": "Deep Neural Networks have achieved significant success in various tasks such as image recognition (Krizhevsky et al., 2012), speech transcription (Graves et al.", "startOffset": 98, "endOffset": 123}, {"referenceID": 7, "context": ", 2012), speech transcription (Graves et al., 2013), and machine translation (Bahdanau et al.", "startOffset": 30, "endOffset": 51}, {"referenceID": 1, "context": ", 2013), and machine translation (Bahdanau et al., 2015).", "startOffset": 33, "endOffset": 56}, {"referenceID": 10, "context": "Most of the previous work in generative models has focused on variants of Boltzmann Machines (Smolensky, 1986; Salakhutdinov & Hinton, 2009) and Deep Belief Networks (Hinton et al., 2006).", "startOffset": 166, "endOffset": 187}, {"referenceID": 6, "context": "Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) are another type of generative models that use noise-contrastive estimation (Gutmann & Hyv\u00e4rinen, 2010) to avoid calculating an intractable partition function.", "startOffset": 39, "endOffset": 64}, {"referenceID": 1, "context": ", 2013), and machine translation (Bahdanau et al., 2015). While most of the recent success has been achieved by discriminative models, generative models have not yet enjoyed the same level of success. Most of the previous work in generative models has focused on variants of Boltzmann Machines (Smolensky, 1986; Salakhutdinov & Hinton, 2009) and Deep Belief Networks (Hinton et al., 2006). While these models are very powerful, each iteration of training requires a computationally costly step of MCMC to approximate derivatives of an intractable partition function (normalization constant), making it difficult to scale them to large datasets. Kingma & Welling (2014) have introduced the Variational Auto-Encoder (VAE) which can be seen as a neural network with continuous latent variables.", "startOffset": 34, "endOffset": 669}, {"referenceID": 1, "context": ", 2013), and machine translation (Bahdanau et al., 2015). While most of the recent success has been achieved by discriminative models, generative models have not yet enjoyed the same level of success. Most of the previous work in generative models has focused on variants of Boltzmann Machines (Smolensky, 1986; Salakhutdinov & Hinton, 2009) and Deep Belief Networks (Hinton et al., 2006). While these models are very powerful, each iteration of training requires a computationally costly step of MCMC to approximate derivatives of an intractable partition function (normalization constant), making it difficult to scale them to large datasets. Kingma & Welling (2014) have introduced the Variational Auto-Encoder (VAE) which can be seen as a neural network with continuous latent variables. The encoder is used to approximate a posterior distribution and the decoder is used to stochastically reconstruct the data from latent variables. Gregor et al. (2015) further introduced the Deep Recurrent Attention Writer (DRAW), extending the VAE approach by incorporating a novel differentiable attention mechanism.", "startOffset": 34, "endOffset": 959}, {"referenceID": 1, "context": ", 2013), and machine translation (Bahdanau et al., 2015). While most of the recent success has been achieved by discriminative models, generative models have not yet enjoyed the same level of success. Most of the previous work in generative models has focused on variants of Boltzmann Machines (Smolensky, 1986; Salakhutdinov & Hinton, 2009) and Deep Belief Networks (Hinton et al., 2006). While these models are very powerful, each iteration of training requires a computationally costly step of MCMC to approximate derivatives of an intractable partition function (normalization constant), making it difficult to scale them to large datasets. Kingma & Welling (2014) have introduced the Variational Auto-Encoder (VAE) which can be seen as a neural network with continuous latent variables. The encoder is used to approximate a posterior distribution and the decoder is used to stochastically reconstruct the data from latent variables. Gregor et al. (2015) further introduced the Deep Recurrent Attention Writer (DRAW), extending the VAE approach by incorporating a novel differentiable attention mechanism. Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) are another type of generative models that use noise-contrastive estimation (Gutmann & Hyv\u00e4rinen, 2010) to avoid calculating an intractable partition function. The model consists of a generator that generates samples using a uniform distribution and a discriminator that discriminates between real and generated images. Recently, Denton et al. (2015) have scaled those models by training conditional GANs at each level of a Laplacian pyramid of images.", "startOffset": 34, "endOffset": 1526}, {"referenceID": 24, "context": "The model can be viewed as a part of the sequence-to-sequence framework (Sutskever et al., 2014; Cho et al., 2014; Srivastava et al., 2015).", "startOffset": 72, "endOffset": 139}, {"referenceID": 3, "context": "The model can be viewed as a part of the sequence-to-sequence framework (Sutskever et al., 2014; Cho et al., 2014; Srivastava et al., 2015).", "startOffset": 72, "endOffset": 139}, {"referenceID": 23, "context": "The model can be viewed as a part of the sequence-to-sequence framework (Sutskever et al., 2014; Cho et al., 2014; Srivastava et al., 2015).", "startOffset": 72, "endOffset": 139}, {"referenceID": 5, "context": "In a Bidirectional RNN, the two LSTMs (Hochreiter & Schmidhuber, 1997) with forget gates (Gers et al., 2000) process the input sequence from both forward and backward directions.", "startOffset": 89, "endOffset": 108}, {"referenceID": 8, "context": "2 IMAGE MODEL: THE CONDITIONAL DRAW NETWORK To generate an image x conditioned on the caption information y, we extended the DRAW network (Gregor et al., 2015) to include caption representation h at each step, as shown in Fig.", "startOffset": 138, "endOffset": 159}, {"referenceID": 1, "context": "The align function is used to compute the alignment between the input caption and intermediate image generative steps (Bahdanau et al., 2015).", "startOffset": 118, "endOffset": 141}, {"referenceID": 5, "context": "3 is defined by the LSTM network with forget gates (Gers et al., 2000) at a single time-step.", "startOffset": 51, "endOffset": 70}, {"referenceID": 5, "context": "3 is defined by the LSTM network with forget gates (Gers et al., 2000) at a single time-step. To generate the next hidden state h t , the LSTM gen takes the previous hidden state h t\u22121 and combines it with the input from both the latent sample zt and the sentence representation st. The output of the LSTM gen function h t is then passed through the write operator which is added to a cumulative canvas matrix ct \u2208 Rh\u00d7w (Eq. 4). The write operator produces two arrays of 1D Gaussian filter banks Fx(h gen t ) \u2208 Rh\u00d7p and Fy(h gen t ) \u2208 Rw\u00d7p whose filter locations and scales are computed from the generative LSTM hidden state h t (same as defined in Gregor et al. (2015)).", "startOffset": 52, "endOffset": 670}, {"referenceID": 16, "context": "Due to the blurriness of samples generated by the DRAW model, we perform an additional post processing step where we use an adversarial network trained on residuals of a Laplacian pyramid conditioned on the skipthought representation (Kiros et al., 2015) of the captions to sharpen the generated images, similar to (Denton et al.", "startOffset": 234, "endOffset": 254}, {"referenceID": 4, "context": ", 2015) of the captions to sharpen the generated images, similar to (Denton et al., 2015).", "startOffset": 68, "endOffset": 89}, {"referenceID": 19, "context": "1 MICROSOFT COCO Microsoft COCO (Lin et al., 2014) is a large dataset containing 82,783 images, each annotated with at least 5 captions.", "startOffset": 32, "endOffset": 50}, {"referenceID": 16, "context": "We compared the performance of the proposed model to the DRAW model conditioned on captions without the align function (noalignDRAW) as well as the DRAW model conditioned on the skipthought vectors of (Kiros et al., 2015) (skipthoughtDRAW).", "startOffset": 201, "endOffset": 221}, {"referenceID": 4, "context": "The LAPGAN model of (Denton et al., 2015) was trained on a two level Laplacian Pyramid with a GAN as a top layer generator and all stages were conditioned on the same skipthought vector.", "startOffset": 20, "endOffset": 41}, {"referenceID": 26, "context": "Instead of calculating error per pixel, we turn to a smarter metric, the Structural Similarity Index (SSI) (Wang et al., 2004), which incorporates luminance and contrast masking into the error calculation.", "startOffset": 107, "endOffset": 126}], "year": 2015, "abstractText": "Motivated by the recent progress in generative models, we introduce a model that generates images from natural language descriptions. The proposed model iteratively draws patches on a canvas, while attending to the relevant words in the description. After training on Microsoft COCO, we compare our model with several baseline generative models on image generation and retrieval tasks. We demonstrate that our model produces higher quality samples than other approaches and generates images with novel scene compositions corresponding to previously unseen captions in the dataset.", "creator": "LaTeX with hyperref package"}}}