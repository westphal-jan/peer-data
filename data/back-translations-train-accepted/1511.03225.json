{"id": "1511.03225", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2015", "title": "Label Efficient Learning by Exploiting Multi-Class Output Codes", "abstract": "We analyze the popular multi-class algorithmic techniques of one-vs-all and error correcting output-codes, and show the surprising result that under the assumption that they are successful (at learning from labeled data), and under an additional mild distributional assumption, we can learn from unlabeled data (up to a permutation of the labels). The key point is that in cases where they work, these techniques implicitly assume structure on how the classes are related. We show how to exploit this relationship both in the case where the codewords are well separated (which includes the one-vs-all case) and in the case where the code matrix has the property that each bit of the codewords is important for distinguishing at least one class from impossible inputs.", "histories": [["v1", "Tue, 10 Nov 2015 18:50:03 GMT  (179kb,D)", "https://arxiv.org/abs/1511.03225v1", null], ["v2", "Mon, 22 Feb 2016 02:24:20 GMT  (296kb,D)", "http://arxiv.org/abs/1511.03225v2", null], ["v3", "Fri, 29 Jul 2016 03:58:08 GMT  (272kb,D)", "http://arxiv.org/abs/1511.03225v3", null], ["v4", "Fri, 25 Nov 2016 15:52:08 GMT  (251kb,D)", "http://arxiv.org/abs/1511.03225v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["maria-florina balcan", "travis dick", "yishay mansour"], "accepted": true, "id": "1511.03225"}, "pdf": {"name": "1511.03225.pdf", "metadata": {"source": "CRF", "title": "Label Efficient Learning by Exploiting Multi-class Output Codes", "authors": ["Maria Florina Balcan", "Travis Dick", "Yishay Mansour"], "emails": ["ninamf@cs.cmu.edu", "tdick@cs.cmu.edu", "mansour@tau.ac.il"], "sections": [{"heading": "1 Introduction", "text": "The question is whether this is a way in which man puts himself and his environment at the center. (...) The question is whether this is a way in which man puts himself at the center. (...) The question is whether this is a way in which man puts himself at the center. (...) The question is whether this is a way in which man puts himself at the center. (...) The question is whether this is a way in which man puts himself at the center. (...) The question is whether this is a form, this is a form, this is a form. (...) The question is whether this is a form, this is a form, this is a form. (...) The question is whether this is a form, this is a form, this is a form, this is a form, this is a form. (...) The question is whether this is a form, this is a form, this is a form, this is a form, this is a form, this is a form. (...) The question is whether this is a form, this is a form, this is a form, this is a form, this is a form, this is a form. (...)"}, {"heading": "2 Related Work", "text": "Reduction to binary classification is one of the most widely used techniques of applied machine learning for attacking multi-class problems. In fact, the one-on-one, one, and error correction of output codes [Dietterich and Bakiri, 1995] all follow this structure [Mohri et al., 2012, Langford and Beygelzimer, 2005, Beygelzimer et al., 2009, Daniely et al., 2012, Allwein et al., 2000]. There is no previous paper showing error limits for output codes with unmarked data and interaction. There has been a long series of papers on providing detectable limits for semi-supervised learning [Balcan et al., 2004, Balcan and Blum, 1998, Chapelle et al.] and active learning [Balcan et al., 2006, Dasgupta et al., 2011, Balcan and Urner, 2015, Hanneke, 2014]."}, {"heading": "3 Preliminaries", "text": "We consider multi-class learning problems via an instance space in which each dot is labeled with f: X \u2192 {1,.., L} up to one of L classes and the probability that each result is observed x-X is determined by a data distribution P to X. The density function of P is denoted by p: X \u2192 [0, \u221e). In all our results we assume that there is a uniform (but unknown) linear output code classifier defined by a code matrix C (hm (x)). We let dHam (c, c \") label the codeword of class i by Ci and define h (x) = (character (h1 (x)),.., character (hm (x))) as the predicted code word for dot x. We let dHam (c, c\") as the hamming distance between any code words c, c. \""}, {"heading": "4 Error Correcting Output Codes", "text": "We first consider the implicit structure when there is a consistent linear error correction that classifies the output codes: there is a code matrix C (1) and a linear function level (2). hm so that: (1) There is at least one point at which each point x is satisfied by the class y., hm intersect at any point.Part (1) of this condition is a limited number of linear separators that can make an error when the output code predicts the codeword of a new example, part (2) formalizes the requirement to have well-separated codewords, and part (3) requires that the hyperplanes can be in a general position that can be met by adding an arbitrarily small example."}, {"heading": "5 One-Versus-All on the Unit Ball", "text": "In this section we show that even if the codewords are not well separated, we can still exploit the implicit structure of the output codes to reduce the complexity of learning by clustering the data. (1) We look at the implicit structure of a linear one-against-all classifier for all i, (2) for all i, hi (x) = w > i x \u2212 bi with two linear separators h1,. hL like this: (1) point x belongs to the class i iff hi (x) > 0, and (2) for all i, hi (x) = w > i x \u2212 bi with two linear separators h.See Figure 1 for a satisfaction of this condition. Since a one-vs-all classifier is an output code in which the codematrix is the identity, the hamming distance between each pair of codewords is exactly 2. Therefore, in this setting we do not have a result similar to Lemma 1 to ensure that classes are geometrically separated."}, {"heading": "6 The Boundary Features Condition", "text": "In this area we are not yet in a position to decide whether we will be able to differentiate ourselves, whether we are able to differentiate ourselves, whether we are able to differentiate ourselves, and whether we are able to differentiate ourselves, to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be in a position, to be able to be able to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a, to be in a position, to be in a position, to be in a position, to be in a position to be in a, to be in a position, to be in a position, to be in a, to be in a, to be in a, to be in a, to be in a, a position, a, a, a, a, a, a, a, a, a, a, a, a,"}, {"heading": "7 Extensions to the Agnostic Setting", "text": "The majority of our algorithms have two phases: first, we extract a division of the unlabeled data into groups that are likely to be label homogeneous, and second, we question the label of the largest groups. We can extend our results for these algorithms to the agnostic setting by querying several labels from each group and using the label majority. Specifically, we assume that the data is generated according to a label distribution P via X \u00d7 [L], and there is a label function f \u0445, so that Pr (x, y) \u0445 P (x) 6 = y and our assumptions apply when the unlabeled data is taken from the marginal PX label, but the labels are assigned by f-label. That is, the true distribution via class labels contradicts a function f-label that satisfies our assumptions with the probability that we have most labels with 8xX label. In this setting, the first phase of our algorithms that deal with only the data that is not exactly labeled in the way we do."}, {"heading": "8 Conclusion and Discussion", "text": "In this paper, we demonstrated how to exploit the implicit geometric assumptions made by output code techniques among the well-studied cases of one-on-one and well-separated code words, and how to grasp the intuition that each binary learning task should be significant. We provide label-efficient learning algorithms for both consistent and agnostic learning environments with guarantees when the data density has thick layer sets or upper and lower boundaries. In all cases, our algorithms show that the implicit assumptions of output code learning can be used to learn from very limited, labeled data. In this work, we focused on linear output codes used in several practical works. For example, our algorithms use linear output codes for neural decoding of thoughts from fMRI data, Berger [1999] successfully used them for text classification, MIST and multiple Singer data sets, showing that they relate to the 2000 and MIST data sets."}, {"heading": "Acknowledgments", "text": "This work was partially supported by NSF grants CCF-1422910, CCF-1535967, IIS-1618714, a Sloan Research Fellowship, a Microsoft Research Faculty Fellowship, and a Google Research Award."}, {"heading": "9 Appendix for Error Correcting Output Codes", "text": "First, we show that the line segment [x, y] is the decision surface of the linear delimiter hk = 0, and only when h (x) and h (y) differ from each other is the line segment [x, y] with the line hk = 0.Proof. the function f (t) = hk (1 \u2212 t) x + ty) is continuous and satisfied f (0) = hk (x) > 0 and f (1) > the definition of Ki and Kj, we have the line segment [x) > 0 and hk (x) < the function f (t) x + ty is satisfactory f (0) = hk) > 0 and f (1) < 0."}, {"heading": "10 Appendix For One-vs-all on the Unit Ball", "text": "The following result is similar to Theorem 4 and shows that algorithm 3 continues to operate in the agnostic setting of Section 7.Theorem 7. Suppose the data is drawn from the distribution P via X \u00b7 [L] and there is a labeling function f \u0445, so that Pr (x, y) \u0445 P (f) \u0445 (x) 6 = y) \u2264 \u03b7 and assumptions 2 and 3 apply when assigning labels by f \u0445. Suppose that algorithm 3 with the parameter rc (x) = i) \u2265 19\u03b7 applies to all classes i. In case of a possible exceeding error, there is a labeling proof for the probability (clb / (c 2 ubbmin)), so that with the probability at least 1 \u2212 \u03b4, the execution of the parameter rc on an unlabeled sample of size n = O (c4ubd / (2lbb 2 min), there is a probability that the labeling of mass f) d) and the query (NO = n) of each cluster."}, {"heading": "11 Appendix for Boundary Features Condition", "text": "We start by proving the probability limits of a d-dimensional sphere below the uniform distribution volume. (Lemma 5) We assume that r > 0 will be any radius and X will be a random sample, which will be uniformly centered by the sphere of the radius r + 2. (For each width 0 \u2264 0))) We assume that the probability that the first coordinate of the X countries in [0, 0] can be limited as follows: \"The first coordinate of the X countries in [0, 0] like the second coordinate of the X countries in [0, 0] like the second coordinate of the X countries in [0, 0] like the second coordinate of the origin class and S = {x] like the first coordinate of the B class, for which the first coordinate of the X countries exists in the interval. The probability that a universal random sample of the B countries in the subset ol (S / Vol) is the first coordinate of the classes (B)."}], "references": [{"title": "Reducing multiclass to binary: A unifying approach for margin classifiers", "author": ["E. Allwein", "R. Schapire", "Y. Singer"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "Allwein et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Allwein et al\\.", "year": 2000}, {"title": "Unsupervised supervised learning ii: Margin-based classification without labels", "author": ["K. Balasubramanian", "P. Donmez", "G. Lebanon"], "venue": "In AISTATS,", "citeRegEx": "Balasubramanian et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Balasubramanian et al\\.", "year": 2011}, {"title": "Unsupervised supervised learning ii: Margin-based classification without labels", "author": ["K. Balasubramanian", "P. Donmez", "G. Lebanon"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "Balasubramanian et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Balasubramanian et al\\.", "year": 2011}, {"title": "A discriminative model for semi-supervised learning", "author": ["M-F. Balcan", "A. Blum"], "venue": "In Journal of the ACM,", "citeRegEx": "Balcan and Blum.,? \\Q2010\\E", "shortCiteRegEx": "Balcan and Blum.", "year": 2010}, {"title": "Active learning", "author": ["M-F. Balcan", "R. Urner"], "venue": "In Survey in the Encyclopedia of Algorithms,", "citeRegEx": "Balcan and Urner.,? \\Q2015\\E", "shortCiteRegEx": "Balcan and Urner.", "year": 2015}, {"title": "Co-training and expansion: Towards bridging theory and practice", "author": ["M-F. Balcan", "A. Blum", "K. Yang"], "venue": "In NIPS,", "citeRegEx": "Balcan et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2004}, {"title": "Agnostic active learing", "author": ["M-F. Balcan", "A. Beygelzimer", "J. Lanford"], "venue": "In ICML,", "citeRegEx": "Balcan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2006}, {"title": "Exploiting ontology structures and unlabeled data for learning", "author": ["M.-F. Balcan", "A. Blum", "Y. Mansour"], "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML),", "citeRegEx": "Balcan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2013}, {"title": "Error-correcting output coding for text classification", "author": ["A. Berger"], "venue": "In IJCAI Workshop on machine learning for information filtering,", "citeRegEx": "Berger.,? \\Q1999\\E", "shortCiteRegEx": "Berger.", "year": 1999}, {"title": "Solving multiclass learning problems via error-correcting output codes", "author": ["A. Beygelzimer", "J. Langford", "P. Ravikumar"], "venue": "ALT,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2009}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["A. Blum", "T. Mitchell"], "venue": "In COLT,", "citeRegEx": "Blum and Mitchell.,? \\Q1998\\E", "shortCiteRegEx": "Blum and Mitchell.", "year": 1998}, {"title": "On the exponential value of labeled samples", "author": ["V. Castelli", "T. Cover"], "venue": "In Pattern Recognition Letters,", "citeRegEx": "Castelli and Cover.,? \\Q1995\\E", "shortCiteRegEx": "Castelli and Cover.", "year": 1995}, {"title": "Semi-Supervised Learning", "author": ["O. Chapelle", "B. Schlkopf", "A. Zien"], "venue": "The MIT Press, 1st edition,", "citeRegEx": "Chapelle et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2010}, {"title": "Improved output coding for classification using continuous relaxation", "author": ["K. Crammer", "Y. Singer"], "venue": "In NIPS,", "citeRegEx": "Crammer and Singer.,? \\Q2000\\E", "shortCiteRegEx": "Crammer and Singer.", "year": 2000}, {"title": "Multiclass learning approaches: A theoretical comparison with implications", "author": ["A. Daniely", "M. Schapira", "G. Shahaf"], "venue": "In NIPS,", "citeRegEx": "Daniely et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Daniely et al\\.", "year": 2012}, {"title": "Two faces of active learning", "author": ["S. Dasgupta"], "venue": "In Theoretical Computer Science,", "citeRegEx": "Dasgupta.,? \\Q2011\\E", "shortCiteRegEx": "Dasgupta.", "year": 2011}, {"title": "Randomized partition trees for exact nearest neighbor search", "author": ["S. Dasgupta", "K. Sinha"], "venue": "In COLT,", "citeRegEx": "Dasgupta and Sinha.,? \\Q2013\\E", "shortCiteRegEx": "Dasgupta and Sinha.", "year": 2013}, {"title": "Solving multiclass learning problems via error-correcting output codes", "author": ["T.G. Dietterich", "G. Bakiri"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Dietterich and Bakiri.,? \\Q1995\\E", "shortCiteRegEx": "Dietterich and Bakiri.", "year": 1995}, {"title": "Unsupervised supervised learning i: Estimating classification and regression errors without labels", "author": ["P. Donmez", "G. Lebanon", "K. Balasubramanian"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "Donmez et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Donmez et al\\.", "year": 2010}, {"title": "Theory of active learning", "author": ["S. Hanneke"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Hanneke.,? \\Q2014\\E", "shortCiteRegEx": "Hanneke.", "year": 2014}, {"title": "Sensitive error correcting output codes", "author": ["J. Langford", "A. Beygelzimer"], "venue": "COLT,", "citeRegEx": "Langford and Beygelzimer.,? \\Q2005\\E", "shortCiteRegEx": "Langford and Beygelzimer.", "year": 2005}, {"title": "Never-ending learning", "author": ["T. Mitchell", "W. Cohen", "E. Hruschka", "P. Talukdar", "J. Betteridge", "A. Carlson", "B. Dalvi", "M. Gardner", "B. Kisiel", "J. Krishnamurthy", "N. Lao", "K. Mazaitis", "T. Mohamed", "N. Nakashole", "E. Platanios", "A. Ritter", "M. Samadi", "B. Settles", "R. Wang", "D. Wijaya", "A. Gupta", "X. Chen", "A. Saparov", "M. Greaves", "J. Welling"], "venue": "In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15),", "citeRegEx": "Mitchell et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2015}, {"title": "Foundations of Machine Learning", "author": ["M. Mohri", "A. Rostamizadeh", "A. Talwalkar"], "venue": "MIT press,", "citeRegEx": "Mohri et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mohri et al\\.", "year": 2012}, {"title": "Zero-shot learning with semantic output codes", "author": ["M. Palatucci", "D. Pomerleau", "G. Hinton", "T. Mitchell"], "venue": "In NIPS,", "citeRegEx": "Palatucci et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Palatucci et al\\.", "year": 2009}, {"title": "Unlabeled data: Now it helps, now it doesn\u2019t", "author": ["A. Singh", "X. Zhu", "R. Nowak"], "venue": "In NIPS,", "citeRegEx": "Singh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2008}, {"title": "Unsupervised risk estimation with only structural assumptions", "author": ["J. Steinhardt", "P. Liang"], "venue": "In Annals of Statistics,", "citeRegEx": "Steinhardt and Liang.,? \\Q2016\\E", "shortCiteRegEx": "Steinhardt and Liang.", "year": 2016}, {"title": "Explanation-Based Neural Network Learning: A Lifelong Learning Approach", "author": ["S. Thrun"], "venue": null, "citeRegEx": "Thrun.,? \\Q1996\\E", "shortCiteRegEx": "Thrun.", "year": 1996}, {"title": "Learning one more thing", "author": ["S. Thrun", "T. Mitchell"], "venue": "In Proc. 14th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Thrun and Mitchell.,? \\Q1995\\E", "shortCiteRegEx": "Thrun and Mitchell.", "year": 1995}, {"title": "Lifelong robot learning", "author": ["Sebastian Thrun", "Tom M. Mitchell"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "Thrun and Mitchell.,? \\Q1995\\E", "shortCiteRegEx": "Thrun and Mitchell.", "year": 1995}, {"title": "On the uniform convergence of relative frequencies of events to their probabilities", "author": ["V. Vapnik", "A. Chervonenkis"], "venue": "Theory of Probability and its Applications,", "citeRegEx": "Vapnik and Chervonenkis.,? \\Q1971\\E", "shortCiteRegEx": "Vapnik and Chervonenkis.", "year": 1971}], "referenceMentions": [{"referenceID": 17, "context": "In practice, output codes are designed to have this property in order to be robust to prediction errors for the binary classification tasks [Dietterich and Bakiri, 1995].", "startOffset": 140, "endOffset": 169}, {"referenceID": 17, "context": "Indeed, the one-vs-all, one-vs-one, and the error correcting output code approaches [Dietterich and Bakiri, 1995] all follow this structure [Mohri et al.", "startOffset": 84, "endOffset": 113}, {"referenceID": 0, "context": ", 2012, Allwein et al., 2000]. There is no prior work providing error bounds for output codes using unlabeled data and interaction. There has been a long line of work for providing provable bounds for semi-supervised learning [Balcan et al., 2004, Balcan and Blum, 2010, Blum and Mitchell, 1998, Chapelle et al., 2010] and active learning [Balcan et al., 2006, Dasgupta, 2011, Balcan and Urner, 2015, Hanneke, 2014]. These works provide bounds on the benefits of unlabeled data and interaction for significantly different semi-supervised and active learning methods that are based different assumptions, often focusing on binary classification, thus the results are largely incomparable. Another line of recent work considers the multi-class setting and uses unlabeled data to consistently estimate the risk of classifiers when the data is generated from a known family of models [Donmez et al., 2010, Balasubramanian et al., 2011a,b]. Their results do not immediately imply learning algorithms and they consider generative assumptions, while in contrast our work explicitly designs learning algorithms under commonly used discriminative assumptions. Another work related to ours is that of Balcan et al. [2013], where labels are recovered from unlabeled data.", "startOffset": 8, "endOffset": 1212}, {"referenceID": 0, "context": ", 2012, Allwein et al., 2000]. There is no prior work providing error bounds for output codes using unlabeled data and interaction. There has been a long line of work for providing provable bounds for semi-supervised learning [Balcan et al., 2004, Balcan and Blum, 2010, Blum and Mitchell, 1998, Chapelle et al., 2010] and active learning [Balcan et al., 2006, Dasgupta, 2011, Balcan and Urner, 2015, Hanneke, 2014]. These works provide bounds on the benefits of unlabeled data and interaction for significantly different semi-supervised and active learning methods that are based different assumptions, often focusing on binary classification, thus the results are largely incomparable. Another line of recent work considers the multi-class setting and uses unlabeled data to consistently estimate the risk of classifiers when the data is generated from a known family of models [Donmez et al., 2010, Balasubramanian et al., 2011a,b]. Their results do not immediately imply learning algorithms and they consider generative assumptions, while in contrast our work explicitly designs learning algorithms under commonly used discriminative assumptions. Another work related to ours is that of Balcan et al. [2013], where labels are recovered from unlabeled data. The main tool that they use, in order to recover the labels, is the assumption that there are multiple views and an underlying ontology that are known, and restrict the possible labeling. Similarly, Steinhardt and Liang [2016] show how to use the method of moments to estimate the risk of a model from unlabeled data under the assumption that the data has three independent views.", "startOffset": 8, "endOffset": 1488}, {"referenceID": 0, "context": ", 2012, Allwein et al., 2000]. There is no prior work providing error bounds for output codes using unlabeled data and interaction. There has been a long line of work for providing provable bounds for semi-supervised learning [Balcan et al., 2004, Balcan and Blum, 2010, Blum and Mitchell, 1998, Chapelle et al., 2010] and active learning [Balcan et al., 2006, Dasgupta, 2011, Balcan and Urner, 2015, Hanneke, 2014]. These works provide bounds on the benefits of unlabeled data and interaction for significantly different semi-supervised and active learning methods that are based different assumptions, often focusing on binary classification, thus the results are largely incomparable. Another line of recent work considers the multi-class setting and uses unlabeled data to consistently estimate the risk of classifiers when the data is generated from a known family of models [Donmez et al., 2010, Balasubramanian et al., 2011a,b]. Their results do not immediately imply learning algorithms and they consider generative assumptions, while in contrast our work explicitly designs learning algorithms under commonly used discriminative assumptions. Another work related to ours is that of Balcan et al. [2013], where labels are recovered from unlabeled data. The main tool that they use, in order to recover the labels, is the assumption that there are multiple views and an underlying ontology that are known, and restrict the possible labeling. Similarly, Steinhardt and Liang [2016] show how to use the method of moments to estimate the risk of a model from unlabeled data under the assumption that the data has three independent views. Our work is more widely applicable, since it applies when we have only a single view. The output-code formalism is also used by Palatucci et al. [2009] for the purpose of zero shot learning.", "startOffset": 8, "endOffset": 1794}, {"referenceID": 29, "context": "Using a standard VC-bound [Vapnik and Chervonenkis, 1971] together with the fact that balls have VC-dimension d + 1, for n = O((4C)d/( r c )) guarantees that with probability at least 1\u2212 \u03b4/2 the following holds simultaneously for every center x \u2208 R and radius r \u2265 0: \u2223\u2223\u2223\u2223|B(x, r) \u2229 S|/n\u2212 P (B(x, r)) \u2223\u2223\u2223\u2223 \u2264 1 2 \u03bb\u03c3vd, (1)", "startOffset": 26, "endOffset": 57}, {"referenceID": 16, "context": "Recall that a probability measure P is said to have doubling dimension D if for every point x in the support of P and every radius r > 0, we have that P (B(x, 2r)) \u2264 2P (B(x, r)) (see, for example, [Dasgupta and Sinha, 2013]).", "startOffset": 198, "endOffset": 224}, {"referenceID": 15, "context": "Following an analysis similar to that of the cluster tree algorithm of Chaudhuri and Dasgupta [2010] gives the following result.", "startOffset": 85, "endOffset": 101}, {"referenceID": 29, "context": "We use a standard VC bound [Vapnik and Chervonenkis, 1971] to relate the probability constraints in the clusterability definition to the empirical measure P\u0302 .", "startOffset": 27, "endOffset": 58}, {"referenceID": 15, "context": "The proof technique used here follows a similar argument as Chaudhuri and Dasgupta [2010]. We use a standard VC bound [Vapnik and Chervonenkis, 1971] to relate the probability constraints in the clusterability definition to the empirical measure P\u0302 .", "startOffset": 74, "endOffset": 90}, {"referenceID": 29, "context": "Therefore, by a standard VC-bound [Vapnik and Chervonenkis, 1971], if we see an iid sample S of size n = O( 1 \u03b32 (ln 2 d \u03b3 + ln 1 \u03b4 )), then with probability at least 1\u2212 \u03b4 the empirical measure of any ball intersected with up to two half-spaces will be within \u03b3 of its true probability mass.", "startOffset": 34, "endOffset": 65}, {"referenceID": 20, "context": "For example Palatucci et al. [2009] use linear output codes for neural decoding of thoughts from fMRI data, Berger [1999] used them successfully for text classification, and Crammer and Singer [2000] show that they perform well on MNIST and several UCI datasets.", "startOffset": 12, "endOffset": 36}, {"referenceID": 8, "context": "[2009] use linear output codes for neural decoding of thoughts from fMRI data, Berger [1999] used them successfully for text classification, and Crammer and Singer [2000] show that they perform well on MNIST and several UCI datasets.", "startOffset": 79, "endOffset": 93}, {"referenceID": 8, "context": "[2009] use linear output codes for neural decoding of thoughts from fMRI data, Berger [1999] used them successfully for text classification, and Crammer and Singer [2000] show that they perform well on MNIST and several UCI datasets.", "startOffset": 79, "endOffset": 171}], "year": 2016, "abstractText": "We present a new perspective on the popular multi-class algorithmic techniques of one-vs-all and error correcting output codes. Rather than studying the behavior of these techniques for supervised learning, we establish a connection between the success of these methods and the existence of label-efficient learning procedures. We show that in both the realizable and agnostic cases, if output codes are successful at learning from labeled data, they implicitly assume structure on how the classes are related. By making that structure explicit, we design learning algorithms to recover the classes with low label complexity. We provide results for the commonly studied cases of one-vs-all learning and when the codewords of the classes are well separated. We additionally consider the more challenging case where the codewords are not well separated, but satisfy a boundary features condition that captures the natural intuition that every bit of the codewords should be significant.", "creator": "LaTeX with hyperref package"}}}