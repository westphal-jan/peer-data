{"id": "1511.03677", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2015", "title": "Learning to Diagnose with LSTM Recurrent Neural Networks", "abstract": "Clinical medical data, especially in the intensive care unit (ICU), consists of multivariate time series of observations. For each patient visit (or episode), sensor data and lab test results are recorded in the patient's Electronic Health Record (EHR). While potentially containing a wealth of insights, the data is difficult to mine effectively, owing to varying length, irregular sampling and missing data. Recurrent Neural Networks (RNNs), particularly those using Long Short-Term Memory (LSTM) hidden units, are powerful and increasingly popular models for learning from sequence data. They adeptly model varying length sequences and capture long range dependencies. We present the first study to empirically evaluate the ability of LSTMs to recognize patterns in multivariate time series of clinical measurements. Specifically, we consider multilabel classification of diagnoses, training a model to classify 128 diagnoses given 13 frequently but irregularly sampled clinical measurements. First, we establish the effectiveness of a simple LSTM network for modeling clinical data. Then we demonstrate a straightforward and effective deep supervision strategy in which we replicate targets at each sequence step. Trained only on raw time series, our models outperforms several strong baselines on a wide variety of metrics, and nearly matches the performance of a multilayer perceptron trained on carefully hand-engineered features, establishing the usefulness of LSTMs for modeling medical data. The best LSTM model accurately classifies many diagnoses, including diabetic ketoacidosis (F1 score of .714), scoliosis (.677), and status asthmaticus (.632).", "histories": [["v1", "Wed, 11 Nov 2015 21:01:28 GMT  (216kb,D)", "http://arxiv.org/abs/1511.03677v1", null], ["v2", "Fri, 13 Nov 2015 01:31:00 GMT  (216kb,D)", "http://arxiv.org/abs/1511.03677v2", null], ["v3", "Tue, 17 Nov 2015 11:22:34 GMT  (218kb,D)", "http://arxiv.org/abs/1511.03677v3", null], ["v4", "Fri, 20 Nov 2015 19:20:41 GMT  (839kb,D)", "http://arxiv.org/abs/1511.03677v4", null], ["v5", "Thu, 7 Jan 2016 09:29:14 GMT  (775kb,D)", "http://arxiv.org/abs/1511.03677v5", null], ["v6", "Tue, 1 Mar 2016 01:55:57 GMT  (775kb,D)", "http://arxiv.org/abs/1511.03677v6", null], ["v7", "Tue, 21 Mar 2017 21:29:50 GMT  (775kb,D)", "http://arxiv.org/abs/1511.03677v7", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zachary c lipton", "david c kale", "charles elkan", "randall wetzel"], "accepted": true, "id": "1511.03677"}, "pdf": {"name": "1511.03677.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Zachary C. Lipton", "David C. Kale"], "emails": ["zlipton@cs.ucsd.edu", "dkale@usc.edu", "elkan@cs.ucsd.edu", "rwetzel@chla.usc.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "It is about the question of whether and how one sees oneself in a position to survive oneself, and about the question of whether and how one is able to survive oneself. (...) It is about the question of whether and how one is able to survive oneself. (...) It is about the question of how one can survive oneself. (...) It is about the question of how far one can survive oneself. (...) It is about the question of how far one can survive oneself. (...) It is about the question of how far one can survive oneself. (...) It is about the question of how far one has to survive oneself. (...) It is about the question of how far one has to survive oneself. (...) It is about the question of how far one can survive oneself. (...) It is about how far one has to survive oneself. (...) It is about how far one has to survive oneself."}, {"heading": "2 RELATED WORK", "text": "Our research is at the interface of LSTMs, medical informatics and multi-label classification, three well-developed areas, each of which has a long history and a rich body of research results. Although we cannot possibly do justice to all three, we highlight the most relevant work below."}, {"heading": "2.1 LSTM RNNS", "text": "LSTMs were originally introduced in Hochreiter & Schmidhuber (1997), after a long series of research on RNNs for sequence learning. Notable previous work includes Elman (1990), which initially used reproduction over time to train recurrent neural networks to perform supervised machine learning.The design of modern LSTM memory cells has remained close to the original, with the commonly used additions of Forge Gates (Gers et al., 2000b) and peephole connections (Gers et al., 2000a).The connectivity pattern between multiple LSTM layers in our models follows the architecture described by Graves (2013).Pascanu et al. (2013) explores other mechanisms by which a relapsing neural network could be brought to depth."}, {"heading": "2.2 NEURAL NETWORKS FOR MEDICAL DATA", "text": "Neural networks have been applied to medical problems and data for at least 20 years (Caruana et al., 1996; Baxt, 1995), although we are not aware of any work on the application of LSTMs to multivariate clinical time series of the type analyzed here. RNNs have been applied to physiological signals in several studies, including electrocardiograms (Silipo & Marchesi, 1998; Amari & Cichocki, 1998; U \u00bc beyli, 2009) and glucose measurements (Tresp & Briegel, 1998). RNNNs have also been used for predictive problems in genomics (Pollastri et al., 2002; Xu et al., 2007; Vohradsky \u0445, 2001)."}, {"heading": "2.3 NEURAL NETWORKS FOR MULTILABEL CLASSIFICATION", "text": "Few published papers apply LSTMs to tasks of multi-label classification, all of which, to our knowledge, are outside the medical context. Liu et al. (2014) formulate music composition as a task of multi-label classification using sigmoidal output units. Recently, Yeung et al. (2015) used LSTM networks with multi-label outputs to detect actions in videos. While we could not find any published papers that use LSTMs for multi-label classification in the medical field, several papers use feed-forward networks for this task. One of the earliest papers to investigate multi-task neural networks that model the risk in pneumonia patients (Caruana et al., 1996). More recently, Che et al. (2015) formulated the diagnosis as a multi-label classification using a multilayer perception."}, {"heading": "2.4 MACHINE LEARNING FOR CLINICAL TIME SERIES", "text": "In addition to the neural network methodology, a growing number of researchers are applying machine learning to temporal clinical data for tasks such as artifact removal (Aleks et al., 2009; Quinn et al., 2009), early detection and prediction (Stanculescu et al., 2014; Henry et al., 2015), and clustering and subtyping (Marlin et al., 2012; Schulam et al., 2015). Many recent papers use models with latent factors to capture nonlinear dynamics in clinical time series and discover significant representations of health and disease. Gaussian processes and related techniques have proved popular because they directly handle irregular samples and can encode prior knowledge through the choice of covariance functions between time steps and via variables (Marlin et al., 2012; Ghassemi et al., 2015). Saria et al. (2010) combined a hierarchical divergence process with top-rate autoimmune models in early-born disease reactions."}, {"heading": "2.5 KEY DIFFERENCES", "text": "Our experiments show that LSTMs can successfully classify multivariate time series of clinical measurements - a topic that has not been addressed in previous work. While some papers use LSTMs for multi-label classification, our work is the first to address this issue in the medical context. Moreover, this work is the first to classify different length sequences with fixed-length vectors to our knowledge, demonstrating the effectiveness of a target replication strategy that achieves both faster training and better generalization."}, {"heading": "3 DATA DESCRIPTION", "text": "Our experiments use a collection of fully anonymized clinical time series extracted from the EHR system at Children's Hospital LA (Marlin et al., 2012; Che et al., 2015) as part of an IRB-approved study. Data consist of 10, 401 PICU episodes, each with a multivariate time series of 13 variables: diastolic and systolic blood pressure, peripheral capillary refill rate, endtidal CO2, fraction of an inspired scale, blood glucose, heart rate, respiratory rate, blood oxygen saturation, body temperature, and urine output. Episodes range in length from 12 hours to 30 days. Each episode is presented with an inspired CO2 scale, glass globe coma scale, blood glucose rate, blood acidifier rate, blood acidifier rate, blood acidifier rate, blood acidifier rate, blood acidifier rate, blood acidifier rate, blood acidifier rate, blood acidifier rate, blood acidifier rate, blood acidifier rate, blood acid rate, blood oxygen rate, blood oxygen rate."}, {"heading": "4 METHODS", "text": "In this work, we are interested in identifying diagnoses and, more generally, the observable physiological characteristics of patients, a task commonly referred to as phenotyping (Oellrich et al., 2015). We raise the problem of phenotyping clinical time series as multi-label classification. In the face of a series of observations x (1),..., x (T) we learn a classifier to generate hypotheses y of true labels y. Here, we use t-index sequence steps, and for each example, T stands for the length of the sequence. Our proposed LSTM RNN uses memory cells with oblivion gates (Gers et al., 2000b), but without peephole connections (Gers et al., 2003). As output, we use a fully connected layer on the highest LSTM layer followed by an elemental sigmoid activation function (l), as our problem is multi-label."}, {"heading": "4.1 LSTM ARCHITECTURES FOR MULTILABEL CLASSIFICATION", "text": "The first and simplest (Figure 1) runs in chronological order over all inputs and generates outputs only in the last sequence step. In this approach, we have output y only in the last sequence step, where our loss function is the mean of the losses at each output node. Thus, the loss calculated in a single sequence step is the mean of the log loss that is for each label. Loss (y, y) = 1 | L | l = 1 \u2212 (yl \u00b7 log (y, l) + (1 \u2212 yl) \u00b7 log (1 \u2212 y, l)))."}, {"heading": "4.2 SEQUENTIAL TARGET REPLICATION", "text": "One problem with the simple approach is that the network has to learn to pass on information across many sequence steps in order to influence the output. We tackle this problem by replicating our static targets in each sequence step (Figure 2) by providing intermediate targets that provide a local error signal. This approach is partly inspired by the depth monitoring technique that Lee et al. (2014) applies to Convolutionary Networks. In our case, this technique makes particular sense, because we expect the model to accurately predict the results for each sequence step, but not for each hidden layer. For the model with target replication T-T, we generate an output y (t) for each sequence step, because we use the same output weights to calculate y (t) for all t. Furthermore, we use this target replication to generate the results for each sequence step T-T, but not for each hidden layer T-T T T. In the model, we generate a sequence with the target replication with every T-T-loss, our target T-loss is a T-loss T (output)."}, {"heading": "5 JUNKOUT REGULARIZATION WITH AUXILIARY TARGETS", "text": "Given the well-documented success of multi-task learning with shared representations and feedback networks, it seems plausible that we can train a stronger model by using the remaining 301 labels or other information such as diagnostic categories as auxiliary targets (Caruana et al., 1996). These additional targets serve as regulators: the model aims to minimize the loss on the labels of interest, but also needs to learn representations that minimize the loss on the auxiliary targets (Figure 3). Given the lower quality of some of the additional labels, we call this model \"junkout.\""}, {"heading": "6 EXPERIMENTS", "text": "Interestingly, the presence of exploding gradients had no obvious correlation to the loss of the specific example that caused it. To combat exploding gradients, we experimented with '22 weight degradation, gradient clipping, and truncated backpropagation. In addition, we found that in-depth monitoring with target replication not only helped to learn faster, but also served as a regulator. In our final network, we used' 22 regulatory strengths of 10 \u2212 6, depth monitoring with \u03b1 = 0.6, hyperparameters selected from validation data. Our final network used 2 hidden layers and 64 nodes per shift, an architecture that was also based on the validation set. All models are trained on 80% of the data and tested for 10%, with the remaining 10% being used as validation sets."}, {"heading": "6.1 MULTILABEL EVALUATION METHODOLOGY", "text": "We report on micro- and macro-averaged versions in the range of the ROC curve (AUC). By micro-AUC, we mean a single AUC calculated on flat Y and Y matrices, while macro-AUC is an average of the AUC calculated separately for each label. The blind classifier reaches 0.5 macro-AUC, but can exceed 0.5 for micro-AUC by predicting labels in descending order by base rate. In addition, we report on micro- and macro-averaged F1 scores. Likewise, flattened matrices and macro-averages are reported above the labels. F1 metrics require a threshold strategy, and here we select thresholds based on the established validation performance. We refer to Lipton et al. (2014) for an analysis of the strengths and weaknesses of each multi-label F score and a characterization of optimal thresholds. Finally, we report on the probable accuracy of the 10 for the best of the two possible outcomes, AUC."}, {"heading": "6.2 BASELINE CLASSIFIERS", "text": "We provide results for a basic rate model that predicts diagnoses in descending order according to frequency to provide a minimal base of performance. We also report on the performance of logistic regression, which is widely used in clinical research, train a separate classifier for each diagnosis, and choose L2 regularization penalties based on a validation rate. For a much stronger baseline, we train an MLP with rectified linear activations, a dropout of 0.5 and 3 hidden layers and 300 hidden nodes, hyperparameters selected based on validation determination. Each baseline is tested with two sets of inputs: raw time series and hand-made features. For raw time series, we used the first and last six hours. This gave classifiers time information about changes in patient status from admission to discharge within a specified input, as required by all baselines. We found that this worked better than the most advanced of the first 12 hours, or the most advanced of the disease."}, {"heading": "6.3 RESULTS", "text": "The LSTM models, with two levels of 64 hidden units and deep supervision (DS) training, performed best among models that only use raw time series as input. Table 1 shows a summary of results for all models. All three DS models achieve micro and macro AUCs above 0.84 and 0.78, respectively. We believe there is further room for improvement, especially given access to a richer dataset with a greater number of variables and treatment information. Table 2 LSTM's predictive performance for five critical diagnoses with the highest F1 values. Full results per diagnosis can be found in Appendix A. Deep monitoring revealed significant improvements over simple LSTMs across all metrics. It accelerated learning and allowed the model to continue to improve for a greater number of eras. Computer classification errors at each step provide local targets to guide the training, helping the LSTM-DS to model long-distance interactions that could indicate changes in the patient's condition."}, {"heading": "7 DISCUSSION", "text": "Our results suggest that LSTM RNNs, especially when equipped with deep monitoring in the form of target replications, can successfully classify diagnoses of critical patients based on clinical time series data. Our experiments show a clear advantage over all linear baselines and traditional feedback-forward architectures applied to raw measurements, but this is only a first step in this direction. Recognizing current diagnoses shows that LSTMs can capture important signals, but ultimately we want to accurately predict unknown conditions and events, such as mortality and treatment responses."}, {"heading": "8 ACKNOWLEDGEMENTS", "text": "Zachary C. Lipton was supported by the Department of Biomedical Informatics at the University of California, San Diego, through a NIH / NLM Education Scholarship (T15LM011271). David Kale was supported by the Alfred E. Mann Innovation in Engineering Doctoral Fellowship. VPICU was supported by scholarships from the Laura P. and Leland K. Whittier Foundation. We thank NVIDIA Corporation for donating Tesla K40 GPU Hardware and Professors Julian McAuley and Greg Ver Steeg for their support and advice."}, {"heading": "A PER DIAGNOSIS RESULTS", "text": "0.040% of patients with cancer suffer from this disease. 0.040% of patients with cancer suffer from this disease. 0.040% of patients with cancer suffer from this disease. 0.040% of patients with cancer suffer from this disease. 0.040% of patients with cancer suffer from this disease."}], "references": [{"title": "Probabilistic detection of short events, with application to critical care monitoring", "author": ["Aleks", "Norm", "Russell", "Stuart J", "Madden", "Michael G", "Morabito", "Diane", "Staudenmayer", "Kristan", "Cohen", "Mitchell", "Manley", "Geoffrey T"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Aleks et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Aleks et al\\.", "year": 2009}, {"title": "Adaptive blind signal processing-neural network approaches", "author": ["Amari", "Shun-ichi", "Cichocki", "Andrzej"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Amari et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Amari et al\\.", "year": 1998}, {"title": "Joint language and translation modeling with recurrent neural networks", "author": ["Auli", "Michael", "Galley", "Michel", "Quirk", "Chris", "Zweig", "Geoffrey"], "venue": "In EMNLP,", "citeRegEx": "Auli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Auli et al\\.", "year": 2013}, {"title": "Application of artificial neural networks to clinical medicine", "author": ["W.G. Baxt"], "venue": "The Lancet,", "citeRegEx": "Baxt,? \\Q1995\\E", "shortCiteRegEx": "Baxt", "year": 1995}, {"title": "Using the future to \u201csort out\u201d the present: Rankprop and multitask learning for medical risk evaluation", "author": ["Caruana", "Rich", "Baluja", "Shumeet", "Mitchell", "Tom"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Caruana et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Caruana et al\\.", "year": 1996}, {"title": "Deep computational phenotyping", "author": ["Che", "Zhengping", "Kale", "David C", "Li", "Wenzhe", "Bahadori", "Mohammad Taha", "Liu", "Yan"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Che et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Che et al\\.", "year": 2015}, {"title": "A neural network based model for predicting psychological conditions", "author": ["Dabek", "Filip", "Caban", "Jesus J"], "venue": "In Brain Informatics and Health,", "citeRegEx": "Dabek et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dabek et al\\.", "year": 2015}, {"title": "A survey on the application of recurrent neural networks to statistical language modeling", "author": ["De Mulder", "Wim", "Bethard", "Steven", "Moens", "Marie-Francine"], "venue": "Computer Speech & Language,", "citeRegEx": "Mulder et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mulder et al\\.", "year": 2015}, {"title": "Finding structure in time", "author": ["Elman", "Jeffrey L"], "venue": "Cognitive Science,", "citeRegEx": "Elman and L.,? \\Q1990\\E", "shortCiteRegEx": "Elman and L.", "year": 1990}, {"title": "Normal ranges of heart rate and respiratory rate in children from birth to 18 years: A systematic review of observational studies", "author": ["Fleming", "Susannah", "Thompson", "Matthew", "Stevens", "Richard", "Heneghan", "Carl", "Plddemann", "Annette", "Maconochie", "Ian", "Tarassenko", "Lionel", "Mant", "David"], "venue": null, "citeRegEx": "Fleming et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Fleming et al\\.", "year": 2011}, {"title": "Recurrent nets that time and count", "author": ["Gers", "Felix", "Schmidhuber", "J\u00fcrgen"], "venue": "In Neural Networks,", "citeRegEx": "Gers et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["Gers", "Felix A", "Schmidhuber", "J\u00fcrgen", "Cummins", "Fred"], "venue": "Neural Computation,", "citeRegEx": "Gers et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Learning precise timing with lstm recurrent networks", "author": ["Gers", "Felix A", "Schraudolph", "Nicol N", "Schmidhuber", "J\u00fcrgen"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Gers et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2003}, {"title": "A multivariate timeseries modeling approach to severity of illness assessment and forecasting in ICU with sparse, heterogeneous clinical data", "author": ["Ghassemi", "Marzyeh", "Pimentel", "Marco AF", "Naumann", "Tristan", "Brennan", "Thomas", "Clifton", "David A", "Szolovits", "Peter", "Feng", "Mengling"], "venue": "In Proc. Twenty-Ninth AAAI Conf. on Artificial Intelligence,", "citeRegEx": "Ghassemi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ghassemi et al\\.", "year": 2015}, {"title": "Supervised sequence labelling with recurrent neural networks, volume 385", "author": ["Graves", "Alex"], "venue": null, "citeRegEx": "Graves and Alex.,? \\Q2012\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2012}, {"title": "Generating sequences with recurrent neural networks", "author": ["Graves", "Alex"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves and Alex.,? \\Q2013\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2013}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["Graves", "Alex", "Liwicki", "Marcus", "Fern\u00e1ndez", "Santiago", "Bertolami", "Roman", "Bunke", "Horst", "Schmidhuber", "J\u00fcrgen"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Graves et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2009}, {"title": "Pd disease state assessment in naturalistic environments using deep learning", "author": ["Hammerla", "Nils Y", "Fisher", "James M", "Andras", "Peter", "Rochester", "Lynn", "Walker", "Richard", "Pl\u00f6tz", "Thomas"], "venue": "In Proc. Twenty-Ninth AAAI Conf. on Artificial Intelligence,", "citeRegEx": "Hammerla et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hammerla et al\\.", "year": 2015}, {"title": "A targeted real-time early warning score (trewscore) for septic shock", "author": ["Henry", "Katharine E", "Hager", "David N", "Pronovost", "Peter J", "Saria", "Suchi"], "venue": "Science Translational Medicine, 7(299):299ra122\u2013 299ra122,", "citeRegEx": "Henry et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Henry et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Karpathy", "Andrej", "Fei-Fei", "Li"], "venue": "arXiv preprint arXiv:1412.2306,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Computational phenotype discovery using unsupervised feature learning over noisy, sparse, and irregular clinical data", "author": ["Lasko", "Thomas A", "Denny", "Joshua C", "Levy", "Mia A"], "venue": "PLoS ONE,", "citeRegEx": "Lasko et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lasko et al\\.", "year": 2013}, {"title": "Optimal thresholding of classifiers to maximize f1 measure", "author": ["Lipton", "Zachary C", "Elkan", "Charles", "Naryanaswamy", "Balakrishnan"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Lipton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lipton et al\\.", "year": 2014}, {"title": "A critical review of recurrent neural networks for sequence learning", "author": ["Lipton", "Zachary C", "Berkowitz", "John", "Elkan", "Charles"], "venue": "arXiv preprint arXiv:1506.00019,", "citeRegEx": "Lipton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lipton et al\\.", "year": 2015}, {"title": "Bach in 2014: Music composition with recurrent neural network", "author": ["I Liu", "Ramakrishnan", "Bhiksha"], "venue": "arXiv preprint arXiv:1412.3191,", "citeRegEx": "Liu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "A novel approach to online handwriting recognition based on bidirectional long short-term memory networks", "author": ["Liwicki", "Marcus", "Graves", "Alex", "Bunke", "Horst", "Schmidhuber", "J\u00fcrgen"], "venue": "In Proc. 9th Int. Conf. on Document Analysis and Recognition,", "citeRegEx": "Liwicki et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Liwicki et al\\.", "year": 2007}, {"title": "Unsupervised pattern discovery in electronic health care data using probabilistic clustering models", "author": ["Marlin", "Ben M", "Kale", "David C", "Khemani", "Robinder G", "Wetzel", "Randall C"], "venue": "IHI,", "citeRegEx": "Marlin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Marlin et al\\.", "year": 2012}, {"title": "The digital revolution in phenotyping", "author": ["Oellrich", "Anika", "Collier", "Nigel", "Groza", "Tudor", "Rebholz-Schuhmann", "Dietrich", "Shah", "Nigam", "Bodenreider", "Olivier", "Boland", "Mary Regina", "Georgiev", "Ivo", "Liu", "Hongfang", "Livingston", "Kevin", "Luna", "Augustin", "Mallon", "Ann-Marie", "Manda", "Prashanti", "Robinson", "Peter N", "Rustici", "Gabriella", "Simon", "Michelle", "Wang", "Liqin", "Winnenburg", "Rainer", "Dumontier", "Michel"], "venue": "Briefings in Bioinformatics,", "citeRegEx": "Oellrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Oellrich et al\\.", "year": 2015}, {"title": "How to construct deep recurrent neural networks", "author": ["Pascanu", "Razvan", "Gulcehre", "Caglar", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1312.6026,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "PRISM III: an updated Pediatric Risk of Mortality score", "author": ["M.M. Pollack", "K.M. Patel", "U.E. Ruttimann"], "venue": "Critical Care Medicine,", "citeRegEx": "Pollack et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Pollack et al\\.", "year": 1996}, {"title": "Improving the prediction of protein secondary structure in three and eight classes using recurrent neural networks and profiles", "author": ["Pollastri", "Gianluca", "Przybylski", "Darisz", "Rost", "Burkhard", "Baldi", "Pierre"], "venue": "Proteins: Structure, Function, and Bioinformatics,", "citeRegEx": "Pollastri et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Pollastri et al\\.", "year": 2002}, {"title": "Factorial switching linear dynamical systems applied to physiological condition monitoring", "author": ["Quinn", "John", "Williams", "Christopher KI", "McIntosh", "Neil"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Quinn et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Quinn et al\\.", "year": 2009}, {"title": "Use of an artificial neural network to predict head injury", "author": ["Rughani", "Anand I", "Dumont", "Travis M", "Lu", "Zhenyu", "Bongard", "Josh", "Horgan", "Michael A", "Penar", "Paul L", "Tranmer", "Bruce I"], "venue": "outcome: clinical article. Journal of neurosurgery,", "citeRegEx": "Rughani et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rughani et al\\.", "year": 2010}, {"title": "Learning individual and population level traits from clinical temporal data", "author": ["Saria", "Suchi", "Koller", "Daphne", "Penn", "Anna"], "venue": "In Proc. Neural Information Processing Systems (NIPS), Predictive Models in Personalized Medicine workshop. Citeseer,", "citeRegEx": "Saria et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Saria et al\\.", "year": 2010}, {"title": "Clustering longitudinal clinical marker trajectories from electronic health data: Applications to phenotyping and endotype discovery", "author": ["Schulam", "Peter", "Wigley", "Fredrick", "Saria", "Suchi"], "venue": "In Twenty-Ninth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Schulam et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulam et al\\.", "year": 2015}, {"title": "Artificial neural networks for automatic ecg analysis", "author": ["Silipo", "Rosaria", "Marchesi", "Carlo"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "Silipo et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Silipo et al\\.", "year": 1998}, {"title": "Autoregressive hidden markov models for the early detection of neonatal sepsis", "author": ["Stanculescu", "Ioan", "Williams", "Christopher K", "Freer", "Yvonne"], "venue": "Biomedical and Health Informatics, IEEE Journal of,", "citeRegEx": "Stanculescu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Stanculescu et al\\.", "year": 2014}, {"title": "A hierarchical switching linear dynamical system applied to the detection of sepsis in neonatal condition monitoring", "author": ["Stanculescu", "Ioan", "Williams", "Christopher KI", "Freer", "Yvonne"], "venue": null, "citeRegEx": "Stanculescu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Stanculescu et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc VV"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A solution for missing data in recurrent neural networks with an application to blood glucose prediction", "author": ["Tresp", "Volker", "Briegel", "Thomas"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Tresp et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Tresp et al\\.", "year": 1998}, {"title": "Combining recurrent neural networks with eigenvector methods for classification of ecg beats", "author": ["\u00dcbeyli", "Elif Derya"], "venue": "Digital Signal Processing,", "citeRegEx": "\u00dcbeyli and Derya.,? \\Q2009\\E", "shortCiteRegEx": "\u00dcbeyli and Derya.", "year": 2009}, {"title": "Show and tell: A neural image caption generator", "author": ["Vinyals", "Oriol", "Toshev", "Alexander", "Bengio", "Samy", "Erhan", "Dumitru"], "venue": "arXiv preprint arXiv:1411.4555,", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Neural network model of gene expression", "author": ["Vohradsk\u00fd", "Ji\u0159\u0131"], "venue": "The FASEB Journal,", "citeRegEx": "Vohradsk\u00fd and Ji\u0159\u0131\u0301.,? \\Q2001\\E", "shortCiteRegEx": "Vohradsk\u00fd and Ji\u0159\u0131\u0301.", "year": 2001}, {"title": "Inference of genetic regulatory networks with recurrent neural network models using particle swarm optimization", "author": ["Xu", "Rui", "Wunsch II", "Donald", "Frank", "Ronald"], "venue": "IEEE/ACM Transactions on Computational Biology and Bioinformatics (TCBB),", "citeRegEx": "Xu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2007}, {"title": "Every moment counts: Dense detailed labeling of actions in complex videos", "author": ["Yeung", "Serena", "Russakovsky", "Olga", "Jin", "Ning", "Andriluka", "Mykhaylo", "Mori", "Greg", "Fei-Fei", "Li"], "venue": "arXiv preprint arXiv:1507.05738,", "citeRegEx": "Yeung et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yeung et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 26, "context": "Observations, which include sensor data, vital signs, lab test results, and subjective assessments, are sampled irregularly, and are plagued by missing values (Marlin et al., 2012).", "startOffset": 159, "endOffset": 180}, {"referenceID": 2, "context": "Recurrent Neural Networks (RNNs), in particular those based on Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997), powerfully model varying-length sequential data, achieving state-of-the-art results for problems spanning natural language processing, image captioning, handwriting recognition, and genomic analysis (Auli et al., 2013; Sutskever et al., 2014; Vinyals et al., 2014; Karpathy & Fei-Fei, 2014; Liwicki et al., 2007; Graves et al., 2009; Pollastri et al., 2002; Vohradsk\u00fd, 2001; Xu et al., 2007).", "startOffset": 326, "endOffset": 518}, {"referenceID": 38, "context": "Recurrent Neural Networks (RNNs), in particular those based on Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997), powerfully model varying-length sequential data, achieving state-of-the-art results for problems spanning natural language processing, image captioning, handwriting recognition, and genomic analysis (Auli et al., 2013; Sutskever et al., 2014; Vinyals et al., 2014; Karpathy & Fei-Fei, 2014; Liwicki et al., 2007; Graves et al., 2009; Pollastri et al., 2002; Vohradsk\u00fd, 2001; Xu et al., 2007).", "startOffset": 326, "endOffset": 518}, {"referenceID": 41, "context": "Recurrent Neural Networks (RNNs), in particular those based on Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997), powerfully model varying-length sequential data, achieving state-of-the-art results for problems spanning natural language processing, image captioning, handwriting recognition, and genomic analysis (Auli et al., 2013; Sutskever et al., 2014; Vinyals et al., 2014; Karpathy & Fei-Fei, 2014; Liwicki et al., 2007; Graves et al., 2009; Pollastri et al., 2002; Vohradsk\u00fd, 2001; Xu et al., 2007).", "startOffset": 326, "endOffset": 518}, {"referenceID": 25, "context": "Recurrent Neural Networks (RNNs), in particular those based on Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997), powerfully model varying-length sequential data, achieving state-of-the-art results for problems spanning natural language processing, image captioning, handwriting recognition, and genomic analysis (Auli et al., 2013; Sutskever et al., 2014; Vinyals et al., 2014; Karpathy & Fei-Fei, 2014; Liwicki et al., 2007; Graves et al., 2009; Pollastri et al., 2002; Vohradsk\u00fd, 2001; Xu et al., 2007).", "startOffset": 326, "endOffset": 518}, {"referenceID": 16, "context": "Recurrent Neural Networks (RNNs), in particular those based on Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997), powerfully model varying-length sequential data, achieving state-of-the-art results for problems spanning natural language processing, image captioning, handwriting recognition, and genomic analysis (Auli et al., 2013; Sutskever et al., 2014; Vinyals et al., 2014; Karpathy & Fei-Fei, 2014; Liwicki et al., 2007; Graves et al., 2009; Pollastri et al., 2002; Vohradsk\u00fd, 2001; Xu et al., 2007).", "startOffset": 326, "endOffset": 518}, {"referenceID": 30, "context": "Recurrent Neural Networks (RNNs), in particular those based on Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997), powerfully model varying-length sequential data, achieving state-of-the-art results for problems spanning natural language processing, image captioning, handwriting recognition, and genomic analysis (Auli et al., 2013; Sutskever et al., 2014; Vinyals et al., 2014; Karpathy & Fei-Fei, 2014; Liwicki et al., 2007; Graves et al., 2009; Pollastri et al., 2002; Vohradsk\u00fd, 2001; Xu et al., 2007).", "startOffset": 326, "endOffset": 518}, {"referenceID": 43, "context": "Recurrent Neural Networks (RNNs), in particular those based on Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997), powerfully model varying-length sequential data, achieving state-of-the-art results for problems spanning natural language processing, image captioning, handwriting recognition, and genomic analysis (Auli et al., 2013; Sutskever et al., 2014; Vinyals et al., 2014; Karpathy & Fei-Fei, 2014; Liwicki et al., 2007; Graves et al., 2009; Pollastri et al., 2002; Vohradsk\u00fd, 2001; Xu et al., 2007).", "startOffset": 326, "endOffset": 518}, {"referenceID": 4, "context": "Finally, we evaluate the efficacy of using additional information in the patient\u2019s chart as auxiliary outputs, a technique previously used with feed-forward nets (Caruana et al., 1996).", "startOffset": 162, "endOffset": 184}, {"referenceID": 9, "context": "The design of modern LSTM memory cells has remained close to the original, with the commonly used additions of forget gates (Gers et al., 2000b) and peep-hole connections (Gers et al., 2000a). The connectivity pattern among multiple LSTM layers in our models follows the architecture described by Graves (2013). Pascanu et al.", "startOffset": 125, "endOffset": 311}, {"referenceID": 9, "context": "The design of modern LSTM memory cells has remained close to the original, with the commonly used additions of forget gates (Gers et al., 2000b) and peep-hole connections (Gers et al., 2000a). The connectivity pattern among multiple LSTM layers in our models follows the architecture described by Graves (2013). Pascanu et al. (2013) explores other mechanisms by which a recurrent neural network could be made deep.", "startOffset": 125, "endOffset": 334}, {"referenceID": 9, "context": "The design of modern LSTM memory cells has remained close to the original, with the commonly used additions of forget gates (Gers et al., 2000b) and peep-hole connections (Gers et al., 2000a). The connectivity pattern among multiple LSTM layers in our models follows the architecture described by Graves (2013). Pascanu et al. (2013) explores other mechanisms by which a recurrent neural network could be made deep. Surveys of the literature include Graves (2012), a thorough dissertation on sequence labeling with RNNs, De Mulder et al.", "startOffset": 125, "endOffset": 464}, {"referenceID": 7, "context": "Surveys of the literature include Graves (2012), a thorough dissertation on sequence labeling with RNNs, De Mulder et al. (2015), which surveys natural language applications, and Lipton et al.", "startOffset": 108, "endOffset": 129}, {"referenceID": 7, "context": "Surveys of the literature include Graves (2012), a thorough dissertation on sequence labeling with RNNs, De Mulder et al. (2015), which surveys natural language applications, and Lipton et al. (2015), which provides a broad overview of RNNs for sequence learning, focusing on modern applications.", "startOffset": 108, "endOffset": 200}, {"referenceID": 4, "context": "Neural networks have been applied to medical problems and data for at least 20 years (Caruana et al., 1996; Baxt, 1995), although we know of no work on applying LSTMs to multivariate clinical time series of the type we analyze here.", "startOffset": 85, "endOffset": 119}, {"referenceID": 3, "context": "Neural networks have been applied to medical problems and data for at least 20 years (Caruana et al., 1996; Baxt, 1995), although we know of no work on applying LSTMs to multivariate clinical time series of the type we analyze here.", "startOffset": 85, "endOffset": 119}, {"referenceID": 30, "context": "RNNs have also been used for prediction problems in genomics (Pollastri et al., 2002; Xu et al., 2007; Vohradsk\u00fd, 2001).", "startOffset": 61, "endOffset": 119}, {"referenceID": 43, "context": "RNNs have also been used for prediction problems in genomics (Pollastri et al., 2002; Xu et al., 2007; Vohradsk\u00fd, 2001).", "startOffset": 61, "endOffset": 119}, {"referenceID": 32, "context": "Multiple recent papers apply modern deep learning techniques (but not RNNs) to modeling psychological conditions (Dabek & Caban, 2015), head injuries (Rughani et al., 2010), and Parkinson\u2019s disease (Hammerla et al.", "startOffset": 150, "endOffset": 172}, {"referenceID": 17, "context": ", 2010), and Parkinson\u2019s disease (Hammerla et al., 2015).", "startOffset": 33, "endOffset": 56}, {"referenceID": 21, "context": "Recently, feed-forward networks have been applied to medical time series in sliding window fashion to discover meaningful patterns of physiology (Lasko et al., 2013; Che et al., 2015).", "startOffset": 145, "endOffset": 183}, {"referenceID": 5, "context": "Recently, feed-forward networks have been applied to medical time series in sliding window fashion to discover meaningful patterns of physiology (Lasko et al., 2013; Che et al., 2015).", "startOffset": 145, "endOffset": 183}, {"referenceID": 4, "context": "One of the earliest papers to investigate multi-task neural networks modeled risk in pneumonia patients (Caruana et al., 1996).", "startOffset": 104, "endOffset": 126}, {"referenceID": 22, "context": "Liu et al. (2014) formulates music composition as a multilabel classification task, using sigmoidal output units.", "startOffset": 0, "endOffset": 18}, {"referenceID": 22, "context": "Liu et al. (2014) formulates music composition as a multilabel classification task, using sigmoidal output units. Most recently, Yeung et al. (2015) uses LSTM networks with multilabel outputs to recognize actions in videos.", "startOffset": 0, "endOffset": 149}, {"referenceID": 4, "context": "One of the earliest papers to investigate multi-task neural networks modeled risk in pneumonia patients (Caruana et al., 1996). More recently, Che et al. (2015) formulated diagnosis as multilabel classification using a sliding window multilayer perceptron.", "startOffset": 105, "endOffset": 161}, {"referenceID": 0, "context": "Neural network methodology aside, a growing body of research applies machine learning to temporal clinical data for tasks including artifact removal (Aleks et al., 2009; Quinn et al., 2009), early detection and prediction (Stanculescu et al.", "startOffset": 149, "endOffset": 189}, {"referenceID": 31, "context": "Neural network methodology aside, a growing body of research applies machine learning to temporal clinical data for tasks including artifact removal (Aleks et al., 2009; Quinn et al., 2009), early detection and prediction (Stanculescu et al.", "startOffset": 149, "endOffset": 189}, {"referenceID": 36, "context": ", 2009), early detection and prediction (Stanculescu et al., 2014; Henry et al., 2015), and clustering and subtyping (Marlin et al.", "startOffset": 40, "endOffset": 86}, {"referenceID": 18, "context": ", 2009), early detection and prediction (Stanculescu et al., 2014; Henry et al., 2015), and clustering and subtyping (Marlin et al.", "startOffset": 40, "endOffset": 86}, {"referenceID": 26, "context": ", 2015), and clustering and subtyping (Marlin et al., 2012; Schulam et al., 2015).", "startOffset": 38, "endOffset": 81}, {"referenceID": 34, "context": ", 2015), and clustering and subtyping (Marlin et al., 2012; Schulam et al., 2015).", "startOffset": 38, "endOffset": 81}, {"referenceID": 26, "context": "Gaussian processes and related techniques have proved popular because they can directly handle irregular sampling and encode prior knowledge via choice of covariance functions between time steps and across variables (Marlin et al., 2012; Ghassemi et al., 2015).", "startOffset": 216, "endOffset": 260}, {"referenceID": 13, "context": "Gaussian processes and related techniques have proved popular because they can directly handle irregular sampling and encode prior knowledge via choice of covariance functions between time steps and across variables (Marlin et al., 2012; Ghassemi et al., 2015).", "startOffset": 216, "endOffset": 260}, {"referenceID": 0, "context": "Neural network methodology aside, a growing body of research applies machine learning to temporal clinical data for tasks including artifact removal (Aleks et al., 2009; Quinn et al., 2009), early detection and prediction (Stanculescu et al., 2014; Henry et al., 2015), and clustering and subtyping (Marlin et al., 2012; Schulam et al., 2015). Many recent papers use models with latent factors to capture nonlinear dynamics in clinical time series and to discover meaningful representations of health and illness. Gaussian processes and related techniques have proved popular because they can directly handle irregular sampling and encode prior knowledge via choice of covariance functions between time steps and across variables (Marlin et al., 2012; Ghassemi et al., 2015). Saria et al. (2010) combined a hierarchical dirichlet process with autoregressive models to infer latent disease \u201ctopics\u201d in the heart rate signals of premature babies.", "startOffset": 150, "endOffset": 796}, {"referenceID": 0, "context": "Neural network methodology aside, a growing body of research applies machine learning to temporal clinical data for tasks including artifact removal (Aleks et al., 2009; Quinn et al., 2009), early detection and prediction (Stanculescu et al., 2014; Henry et al., 2015), and clustering and subtyping (Marlin et al., 2012; Schulam et al., 2015). Many recent papers use models with latent factors to capture nonlinear dynamics in clinical time series and to discover meaningful representations of health and illness. Gaussian processes and related techniques have proved popular because they can directly handle irregular sampling and encode prior knowledge via choice of covariance functions between time steps and across variables (Marlin et al., 2012; Ghassemi et al., 2015). Saria et al. (2010) combined a hierarchical dirichlet process with autoregressive models to infer latent disease \u201ctopics\u201d in the heart rate signals of premature babies. Quinn et al. (2009) used linear dynamical systems with latent switching variables to model physiologic events like bradycardias.", "startOffset": 150, "endOffset": 965}, {"referenceID": 0, "context": "Neural network methodology aside, a growing body of research applies machine learning to temporal clinical data for tasks including artifact removal (Aleks et al., 2009; Quinn et al., 2009), early detection and prediction (Stanculescu et al., 2014; Henry et al., 2015), and clustering and subtyping (Marlin et al., 2012; Schulam et al., 2015). Many recent papers use models with latent factors to capture nonlinear dynamics in clinical time series and to discover meaningful representations of health and illness. Gaussian processes and related techniques have proved popular because they can directly handle irregular sampling and encode prior knowledge via choice of covariance functions between time steps and across variables (Marlin et al., 2012; Ghassemi et al., 2015). Saria et al. (2010) combined a hierarchical dirichlet process with autoregressive models to infer latent disease \u201ctopics\u201d in the heart rate signals of premature babies. Quinn et al. (2009) used linear dynamical systems with latent switching variables to model physiologic events like bradycardias. Citing inspiration from deep learning, Stanculescu et al. (2015) proposed models with a second \u201clayer\u201d of latent factors to capture correlations between latent states.", "startOffset": 150, "endOffset": 1139}, {"referenceID": 26, "context": "Our experiments use a collection of fully anonymized clinical time series extracted from the EHR system at Children\u2019s Hospital LA (Marlin et al., 2012; Che et al., 2015) as part of an IRB-approved study.", "startOffset": 130, "endOffset": 169}, {"referenceID": 5, "context": "Our experiments use a collection of fully anonymized clinical time series extracted from the EHR system at Children\u2019s Hospital LA (Marlin et al., 2012; Che et al., 2015) as part of an IRB-approved study.", "startOffset": 130, "endOffset": 169}, {"referenceID": 9, "context": ", blood pressure) that are due to age and gender (Fleming et al., 2011)(NHBPEP Working Group 2004).", "startOffset": 49, "endOffset": 71}, {"referenceID": 27, "context": "In this work, we are interested in recognizing diagnoses and, more broadly, the observable physiologic characteristics of patients, a task generally termed phenotyping (Oellrich et al., 2015).", "startOffset": 168, "endOffset": 191}, {"referenceID": 12, "context": ", 2000b) but without peephole connections (Gers et al., 2003).", "startOffset": 42, "endOffset": 61}, {"referenceID": 4, "context": "Given the well-documented successes of multitask learning with shared representations and feedforward networks, it seems plausible that we can train a stronger model by using the remaining 301 labels or other information, such as diagnostic categories, as auxiliary targets (Caruana et al., 1996).", "startOffset": 274, "endOffset": 296}, {"referenceID": 22, "context": "We refer to Lipton et al. (2014) for an analysis of the strengths and weaknesses of each multilabel F-score and a characterization of optimal thresholds.", "startOffset": 12, "endOffset": 33}, {"referenceID": 29, "context": "Our hand-engineered features are inspired by those used in state-of-the-art severity of illness scores (Pollack et al., 1996) and capture extremes (e.", "startOffset": 103, "endOffset": 125}, {"referenceID": 26, "context": "Such features have previously been shown to be very effective for these data (Marlin et al., 2012; Che et al., 2015).", "startOffset": 77, "endOffset": 116}, {"referenceID": 5, "context": "Such features have previously been shown to be very effective for these data (Marlin et al., 2012; Che et al., 2015).", "startOffset": 77, "endOffset": 116}], "year": 2017, "abstractText": "Clinical medical data, especially in the intensive care unit (ICU), consists of multivariate time series of observations. For each patient visit (or episode), sensor data and lab test results are recorded in the patient\u2019s Electronic Health Record (EHR). While potentially containing a wealth of insights, the data is difficult to mine effectively, owing to varying length, irregular sampling and missing data. Recurrent Neural Networks (RNNs), particularly those using Long Short-Term Memory (LSTM) hidden units, are powerful and increasingly popular models for learning from sequence data. They adeptly model varying length sequences and capture long range dependencies. We present the first study to empirically evaluate the ability of LSTMs to recognize patterns in multivariate time series of clinical measurements. Specifically, we consider multilabel classification of diagnoses, training a model to classify 128 diagnoses given 13 frequently but irregularly sampled clinical measurements. First, we establish the effectiveness of a simple LSTM network for modeling clinical data. Then we demonstrate a straightforward and effective deep supervision strategy in which we replicate targets at each sequence step. Trained only on raw time series, our models outperforms several strong baselines on a wide variety of metrics, and nearly matches the performance of a multilayer perceptron trained on carefully hand-engineered features, establishing the usefulness of LSTMs for modeling medical data. The best LSTM model accurately classifies many diagnoses, including diabetic ketoacidosis (F1 score of .714), scoliosis (.677), and status asthmaticus (.632).", "creator": "LaTeX with hyperref package"}}}