{"id": "1511.04056", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2015", "title": "Efficient Non-greedy Optimization of Decision Trees", "abstract": "Decision trees and randomized forests are widely used in computer vision and machine learning. Standard algorithms for decision tree induction optimize the split functions one node at a time according to some splitting criteria. This greedy procedure often leads to suboptimal trees. In this paper, we present an algorithm for optimizing the split functions at all levels of the tree jointly with the leaf parameters, based on a global objective. We show that the problem of finding optimal linear-combination (oblique) splits for decision trees is related to structured prediction with latent variables, and we formulate a convex-concave upper bound on the tree's empirical loss. The run-time of computing the gradient of the proposed surrogate objective with respect to each training exemplar is quadratic in the the tree depth, and thus training deep trees is feasible. The use of stochastic gradient descent for optimization enables effective training with large datasets. Experiments on several classification benchmarks demonstrate that the resulting non-greedy decision trees outperform greedy decision tree baselines.", "histories": [["v1", "Thu, 12 Nov 2015 20:32:28 GMT  (3503kb,D)", "http://arxiv.org/abs/1511.04056v1", "in NIPS 2015"]], "COMMENTS": "in NIPS 2015", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["mohammad norouzi 0002", "maxwell d collins", "matthew a johnson", "david j fleet", "pushmeet kohli"], "accepted": true, "id": "1511.04056"}, "pdf": {"name": "1511.04056.pdf", "metadata": {"source": "CRF", "title": "Efficient Non-greedy Optimization of Decision Trees", "authors": ["Mohammad Norouzi", "Maxwell D. Collins", "Matthew Johnson", "David J. Fleet", "Pushmeet Kohli"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In recent years we have seen an increase in popularity due to their computing power and their applicability to large-scale classification and regression."}, {"heading": "2 Related work", "text": "Finding optimal splitting functions at different levels of a decision tree according to a global goal, such as a regulated empirical risk, is NP-complete [11] due to the discrete and sequential nature of decisions in a tree. Thus, finding an efficient alternative to the greedy approach remains a difficult goal, despite many previous attempts. Bennett [2] proposes a non-greedy multilinear programming approach for global tree optimization and shows that the method produces trees that have a higher classification accuracy than standard greedy trees. However, her method is limited to binary classification with 0-1 loss and has high computational complexity, making it applicable only to trees with few nodes. Working in [15] suggests a means of forming forests in an online setting by gradually expanding the trees as new data points are added. Unlike naive, gradual tree growth, this working model will model decision trees using mondrical processes."}, {"heading": "3 Problem formulation", "text": "For the simplicity of the exposure that proceeds from a weight vector, it is assumed that it is a linear threshold, i.e. a function that relates to a function, i.e. a function that is conducted from the root of the tree to a node. (...) We have performed a binary test by evaluating a specific function of the tree. (...) When we evaluate a distribution via k-classes, x is referred to the left child of the node i. (...) And so is referred to the right child. (...) And so to the tree. (...) Each individual function is parameterized by a weight vector. (...) It is assumed that it is a threshold. (...) It is directed to the right child. (...) And thus to the tree. (...) Every single function is characterized by a weight vector. (...) It is assumed that it is a threshold. (...)"}, {"heading": "4 Decision trees and structured prediction", "text": "To overcome the intractability of optimizing L, we develop a piecemeal smooth upper limit for empirical losses. Our upper limit is inspired by the formulation of a structured prediction using latent variables [26]. A key observation linking decision tree learning to structured predictions is that one can reexpress sgn (Wx) in relation to a latent variable h, that is, sgn (Wx) = argmax h-Hm (hTWx). (5) In this form, the slit functions of decision trees can implicitly map an input x to a binary vector h by maximizing a score function hTWx, the inner product of h and Wx. One can reexpress the score function in a more familiar form of a common feature space on h and x, than wTp (h, x), where two (h, x = Vec (hw) and W = W (W)."}, {"heading": "5 Upper bound on empirical loss", "text": "We develop an upper limit for the loss of the upper limit, i.e., maximization is a problem that we call a problem. (7) We have an upper limit for loss. (7) We have an upper limit for loss. (7) We have an upper limit for loss. (7) We have an upper limit for loss. (7) We have an upper limit for loss. (7) We have an upper limit for loss. (7) We have an upper limit for loss. (7) It is clear that the LHS corresponds to the RHS. (7) For all other values of the RHS, the RHS can only increase if g = h (x). (7) The inequality holds. (7) An algebraic proof of the (7). (7) In the context of the structured prediction, the maximization of the upper limit, i.e."}, {"heading": "6 Optimizing the surrogate objective", "text": "(D) n (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s) s (D) s (D) s) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s) s (D) s (D) s) s (D) s (D) (D) s) (D) (D) s) (D) (D) s) (D) s (D) (D) s) (D) s) (D) (D) s) (D) s (D) (D) s) (D) s (D) (D) s) (D) (D) s (D) s (D) s) (D) (D) s (D) (D) s (D) s (D) (D) s) (D (D) s) (D) s (D (D) s) s (D (D) s) s) s (D (D) s) s (D (D (D) s) s) s (D) s) s (D (D (D) s) s) s (D (D (D) s) s (D) s) s (D (D) s) s (D (D) s) s (D (D) s (D) s (D) s) s (D) s (D) s (D) s (D (D) s (D) s) s (D"}, {"heading": "7 Experiments", "text": "We are also able to analyze the results of our training based on information we are able to compare the different types of trees that we use in different areas. We compare our method for non-greedy learning trees with several greedy base trees, including conventional axis-aligned trees that are based on information gains, OC1 oblique trees that are used to optimize the trees. We compare our method for non-greedy learning trees with several greedy baselines, including conventional axis-aligned trees that are based on information gains, OC1 oblique trees that are used to optimize the trees, and random oblique trees that are selected from a series of randomly generated hyperplanes. We also compare with the results of CO2 [18], which is a specific case of our upper limit applied to trees of depth."}, {"heading": "8 Conclusion", "text": "We present a non-greedy method for learning decision trees with stochastic trajectory to optimize an upper limit on the tree, empirical loss on a training dataset. Our model represents the global training of decision trees in a well-characterized optimization framework. This makes it easier to propose extensions that could be taken into account in future work. Efficiency gains could be achieved through economical splitting functions that are achieved through economy-inducing regularization onW. Furthermore, the kernel optimization problem allows the application of the kernel trick to the linear splitter parameters W, whereby our general model on learning higher value functions or training decision trees using examples in arbitrary reproduction by Hilbert spaces.Acknowledgment. MN was financially supported by a Google community. DF was partially funded by NSERC Canada and the NCAP program of CIFAR.References."}, {"heading": "A Proofs", "text": "For each pair (x, y) loss (sgn (Wx)), y) is limited by the following upper limit: \"(x) (wx), y) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v) (v"}], "references": [{"title": "Global tree optimization: A non-greedy decision tree algorithm", "author": ["K.P. Bennett"], "venue": "Computing Science and Statistics, pages 156\u2013156", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1994}, {"title": "A support vector machine approach to decision trees", "author": ["K.P. Bennett", "J.A. Blue"], "venue": "Department of Mathematical Sciences Math Report No. 97-100, Rensselaer Polytechnic Institute, pages 2396\u20132401", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1997}, {"title": "Enlarging the margins in perceptron decision trees", "author": ["K.P. Bennett", "N. Cristianini", "J. Shawe-Taylor", "D. Wu"], "venue": "Machine Learning, 41(3):295\u2013313", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine Learning, 45(1):5\u201332", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Classification and regression trees", "author": ["L. Breiman", "J. Friedman", "R.A. Olshen", "C.J. Stone"], "venue": "Chapman & Hall/CRC", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1984}, {"title": "Decision Forests for Computer Vision and Medical Image Analysis", "author": ["A. Criminisi", "J. Shotton"], "venue": "Springer", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Greedy function approximation: a gradient boosting machine", "author": ["Jerome H Friedman"], "venue": "Annals of Statistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "Hough forests for object detection", "author": ["J. Gall", "A. Yao", "N. Razavi", "L. Van Gool", "V. Lempitsky"], "venue": "tracking, and action recognition. IEEE Trans. PAMI, 33(11):2188\u20132202", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "The elements of statistical learning (Ed", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": "2). Springer", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Constructing optimal binary decision trees is NP-complete", "author": ["L. Hyafil", "R.L. Rivest"], "venue": "Information Processing Letters, 5(1):15\u201317", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1976}, {"title": "Loss-specific training of non-parametric image restoration models: A new state of the art", "author": ["J. Jancsary", "S. Nowozin", "C. Rother"], "venue": "ECCV", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Hierarchical mixtures of experts and the em algorithm", "author": ["M.I. Jordan", "R.A. Jacobs"], "venue": "Neural Comput., 6(2):181\u2013214", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1994}, {"title": "Neighbourhood approximation forests", "author": ["E. Konukoglu", "B. Glocker", "D. Zikic", "A. Criminisi"], "venue": "Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2012, pages 75\u201382. Springer", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Mondrian forests: Efficient online random forests", "author": ["B. Lakshminarayanan", "D.M. Roy", "Y.H. Teh"], "venue": "Advances in Neural Information Processing Systems, pages 3140\u20133148", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "An empirical comparison of pruning methods for decision tree induction", "author": ["J. Mingers"], "venue": "Machine Learning, 4(2):227\u2013243", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1989}, {"title": "On growing better decision trees from data", "author": ["S.K. Murthy", "S.L. Salzberg"], "venue": "PhD thesis, John Hopkins University", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1995}, {"title": "Co2 forest: Improved random forest by continuous optimization of oblique splits", "author": ["M. Norouzi", "M.D. Collins", "D.J. Fleet", "P. Kohli"], "venue": "arXiv:1506.06155", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Minimal Loss Hashing for Compact Binary Codes", "author": ["M. Norouzi", "D.J. Fleet"], "venue": "ICML", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Hamming Distance Metric Learning", "author": ["M. Norouzi", "D.J. Fleet", "R. Salakhutdinov"], "venue": "NIPS", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Improved information gain estimates for decision tree induction", "author": ["S. Nowozin"], "venue": "ICML", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Induction of decision trees", "author": ["J.R. Quinlan"], "venue": "Machine learning, 1(1):81\u2013106", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1986}, {"title": "et al", "author": ["J. Shotton", "R. Girshick", "A. Fitzgibbon", "T. Sharp", "M. Cook", "M. Finocchio", "R. Moore", "P. Kohli", "A. Criminisi", "A. Kipman"], "venue": "Efficient human pose estimation from single depth images. IEEE Trans. PAMI", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Max-margin Markov networks", "author": ["B. Taskar", "C. Guestrin", "D. Koller"], "venue": "NIPS", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2003}, {"title": "Support vector machine learning for interdependent and structured output spaces", "author": ["I. Tsochantaridis", "T. Hofmann", "T. Joachims", "Y. Altun"], "venue": "ICML", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning structural SVMs with latent variables", "author": ["C.N.J. Yu", "T. Joachims"], "venue": "ICML", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 4, "context": "Decision trees and forests [6, 22, 5] have a long and rich history in machine learning [10, 7].", "startOffset": 27, "endOffset": 37}, {"referenceID": 20, "context": "Decision trees and forests [6, 22, 5] have a long and rich history in machine learning [10, 7].", "startOffset": 27, "endOffset": 37}, {"referenceID": 3, "context": "Decision trees and forests [6, 22, 5] have a long and rich history in machine learning [10, 7].", "startOffset": 27, "endOffset": 37}, {"referenceID": 8, "context": "Decision trees and forests [6, 22, 5] have a long and rich history in machine learning [10, 7].", "startOffset": 87, "endOffset": 94}, {"referenceID": 5, "context": "Decision trees and forests [6, 22, 5] have a long and rich history in machine learning [10, 7].", "startOffset": 87, "endOffset": 94}, {"referenceID": 21, "context": "A case in point is Microsoft Kinect where decision trees are trained on millions of exemplars to enable real-time human pose estimation from depth images [23].", "startOffset": 154, "endOffset": 158}, {"referenceID": 20, "context": "They grow a tree one node at a time following procedures laid out decades ago by frameworks such as ID3 [22] and CART [6].", "startOffset": 104, "endOffset": 108}, {"referenceID": 4, "context": "They grow a tree one node at a time following procedures laid out decades ago by frameworks such as ID3 [22] and CART [6].", "startOffset": 118, "endOffset": 121}, {"referenceID": 19, "context": "While recent work has proposed new objective functions to guide greedy algorithms [21, 12], it continues to be the case that decision tree applications (e.", "startOffset": 82, "endOffset": 90}, {"referenceID": 10, "context": "While recent work has proposed new objective functions to guide greedy algorithms [21, 12], it continues to be the case that decision tree applications (e.", "startOffset": 82, "endOffset": 90}, {"referenceID": 7, "context": ", [9, 14]) utilize the same dated methods of tree induction.", "startOffset": 2, "endOffset": 9}, {"referenceID": 12, "context": ", [9, 14]) utilize the same dated methods of tree induction.", "startOffset": 2, "endOffset": 9}, {"referenceID": 24, "context": "One of the key contributions of this work is establishing a link between the decision tree optimization problem and the problem of structured prediction with latent variables [26].", "startOffset": 175, "endOffset": 179}, {"referenceID": 22, "context": "Inspired by advances in structured prediction [24, 25, 26], we propose a convex-concave upper bound on the empirical loss.", "startOffset": 46, "endOffset": 58}, {"referenceID": 23, "context": "Inspired by advances in structured prediction [24, 25, 26], we propose a convex-concave upper bound on the empirical loss.", "startOffset": 46, "endOffset": 58}, {"referenceID": 24, "context": "Inspired by advances in structured prediction [24, 25, 26], we propose a convex-concave upper bound on the empirical loss.", "startOffset": 46, "endOffset": 58}, {"referenceID": 9, "context": "Finding optimal split functions at different levels of a decision tree according to some global objective, such as a regularized empirical risk, is NP-complete [11] due to the discrete and sequential nature of the decisions in a tree.", "startOffset": 160, "endOffset": 164}, {"referenceID": 0, "context": "Bennett [2] proposes a non-greedy multi-linear programming based approach for global tree optimization and shows that the method produces trees that have higher classification accuracy than standard greedy trees.", "startOffset": 8, "endOffset": 11}, {"referenceID": 13, "context": "The work in [15] proposes a means for training decision forests in an online setting by incrementally extending the trees as new data points are added.", "startOffset": 12, "endOffset": 16}, {"referenceID": 11, "context": "The Hierarchical Mixture of Experts model [13] uses soft splits rather than hard binary decisions to capture situations where the transition from low to high response is gradual.", "startOffset": 42, "endOffset": 46}, {"referenceID": 15, "context": "Murthy and Salzburg [17] argue that non-greedy tree learning methods that work by looking ahead are unnecessary and sometimes harmful.", "startOffset": 20, "endOffset": 24}, {"referenceID": 3, "context": "To avoid this problem, it is a common practice (see Breiman [5] or Criminisi and Shotton [7] for an overview) to limit the tree depth and introduce limits on the number of training instances below which a tree branch is not extended, or to force a diverse ensemble of trees (i.", "startOffset": 60, "endOffset": 63}, {"referenceID": 5, "context": "To avoid this problem, it is a common practice (see Breiman [5] or Criminisi and Shotton [7] for an overview) to limit the tree depth and introduce limits on the number of training instances below which a tree branch is not extended, or to force a diverse ensemble of trees (i.", "startOffset": 89, "endOffset": 92}, {"referenceID": 3, "context": ", a decision forest) through the use of bagging [5] or boosting [8].", "startOffset": 48, "endOffset": 51}, {"referenceID": 6, "context": ", a decision forest) through the use of bagging [5] or boosting [8].", "startOffset": 64, "endOffset": 67}, {"referenceID": 1, "context": "Bennett and Blue [3] describe a different way to overcome overfitting by using max-margin framework and the Support Vector Machines (SVM) at the split nodes of the tree.", "startOffset": 17, "endOffset": 20}, {"referenceID": 2, "context": "[4] show how enlarging the margin of decision tree classifiers results in better generalization performance.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Joint optimization of the split functions and leaf parameters according to a global objective is known to be extremely challenging [11] due to the discrete and sequential nature of the splitting decisions within the tree.", "startOffset": 131, "endOffset": 135}, {"referenceID": 24, "context": "Our upper bound is inspired by the formulation of structured prediction with latent variables [26].", "startOffset": 94, "endOffset": 98}, {"referenceID": 17, "context": "[19, 20] used the same reformulation (5) of linear threshold functions to learn binary similarity preserving hash functions.", "startOffset": 0, "endOffset": 8}, {"referenceID": 18, "context": "[19, 20] used the same reformulation (5) of linear threshold functions to learn binary similarity preserving hash functions.", "startOffset": 0, "endOffset": 8}, {"referenceID": 23, "context": "As a consequence, as with binary SVM and margin-rescaling formulations of structural SVM [25], we introduce a regularizer on the norm of W when optimizing the bound.", "startOffset": 89, "endOffset": 93}, {"referenceID": 15, "context": "We compare our method for non-greedy learning of oblique trees with several greedy baselines, including conventional axis-aligned trees based on information gain, OC1 oblique trees [17] that use coordinate descent for optimization of the splits, and random oblique trees that select the best split function from a set of randomly generated hyperplanes based on information gain.", "startOffset": 181, "endOffset": 185}, {"referenceID": 16, "context": "We also compare with the results of CO2 [18], which is a special case of our upper bound approach applied greedily to trees of depth 1, one node at a time.", "startOffset": 40, "endOffset": 44}, {"referenceID": 14, "context": "Any base algorithm for learning decision trees can be augmented by post-training pruning [16], or building ensembles with bagging [5] or boosting [8].", "startOffset": 89, "endOffset": 93}, {"referenceID": 3, "context": "Any base algorithm for learning decision trees can be augmented by post-training pruning [16], or building ensembles with bagging [5] or boosting [8].", "startOffset": 130, "endOffset": 133}, {"referenceID": 6, "context": "Any base algorithm for learning decision trees can be augmented by post-training pruning [16], or building ensembles with bagging [5] or boosting [8].", "startOffset": 146, "endOffset": 149}, {"referenceID": 16, "context": "Finally, the comparison between non-greedy and CO2 [18] trees concentrates on the non-greediness of the algorithm, as it compares our method with its simpler variant, which is applied greedily one node at a time.", "startOffset": 51, "endOffset": 55}, {"referenceID": 16, "context": "The axisaligned split is used to initialize a greedy variant of the tree training procedure, called CO2 [18].", "startOffset": 104, "endOffset": 108}], "year": 2015, "abstractText": "Decision trees and randomized forests are widely used in computer vision and machine learning. Standard algorithms for decision tree induction optimize the split functions one node at a time according to some splitting criteria. This greedy procedure often leads to suboptimal trees. In this paper, we present an algorithm for optimizing the split functions at all levels of the tree jointly with the leaf parameters, based on a global objective. We show that the problem of finding optimal linear-combination (oblique) splits for decision trees is related to structured prediction with latent variables, and we formulate a convex-concave upper bound on the tree\u2019s empirical loss. The run-time of computing the gradient of the proposed surrogate objective with respect to each training exemplar is quadratic in the the tree depth, and thus training deep trees is feasible. The use of stochastic gradient descent for optimization enables effective training with large datasets. Experiments on several classification benchmarks demonstrate that the resulting non-greedy decision trees outperform greedy decision tree baselines.", "creator": "LaTeX with hyperref package"}}}