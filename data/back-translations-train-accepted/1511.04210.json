{"id": "1511.04210", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Nov-2015", "title": "On the Quality of the Initial Basin in Overspecified Neural Networks", "abstract": "Over the past few years, artificial neural networks have seen a dramatic resurgence in popularity as a tool for solving hard learning problems in AI applications. While it is widely known that neural networks are computationally hard to train in the worst case, in practice, neural networks are trained efficiently using SGD methods and a variety of techniques which accelerate the learning process. One mechanism which has been suggested to explain this is overspecification, which is the training of a network larger than what would be needed with unbounded computational power. Empirically, despite worst-case NP-hardness results, large networks tend to achieve a smaller error over the training set.", "histories": [["v1", "Fri, 13 Nov 2015 09:35:34 GMT  (313kb,D)", "http://arxiv.org/abs/1511.04210v1", null], ["v2", "Tue, 9 Feb 2016 16:22:46 GMT  (164kb,D)", "http://arxiv.org/abs/1511.04210v2", "Significantly different version, with more general results"], ["v3", "Tue, 14 Jun 2016 05:39:27 GMT  (164kb,D)", "http://arxiv.org/abs/1511.04210v3", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["itay safran", "ohad shamir"], "accepted": true, "id": "1511.04210"}, "pdf": {"name": "1511.04210.pdf", "metadata": {"source": "CRF", "title": "On the Quality of the Initial Basin in Overspecified Neural Networks", "authors": ["Itay Safran", "Ohad Shamir"], "emails": ["itay.safran@weizmann.ac.il", "ohad.shamir@weizmann.ac.il"], "sections": [{"heading": null, "text": "In this thesis we strive to understand this phenomenon. In particular, we would like to better understand the behaviour of the error in relation to the sample in relation to the weights of the network, focusing mainly on neural networks consisting of 2 layers, although we also look at individual neural networks and networks of any depth, examine properties such as the number of local minima that the function has, and the probability of initialization from a basin with a given minimum value, with the aim of finding reasonable conditions under which efficient learning of the network is possible."}, {"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Preliminaries and notation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Neural nets and Machine Learning conventions", "text": "We start by giving a formal definition of the type of artificial neural networks that we use in this work."}, {"heading": "2.2 Basin Value Distribution", "text": "Definition 5. (Basin) We define a basin of the target function f in order to optimize it as a contiguous set C, for which the sentence {x-C: f (x) \u2264 \u03b1} is also connected. Basins are of utmost importance in optimization, since a basin cannot contain more than a minimum surface. In the course of our work, we analyze the surface of the objective function by analyzing the minimum value achieved in the basin of initialization. In general, following common practice, we assume that the weights are initialized randomly, explicitly specifying the distribution of weights in each subsection. Definition 6. (Basin Value Distribution) For some > 0, in Section 3, when discussing individual neurons, we let F () indicate the probability that the weights are initialized from a basin, reaching a minimum value at most. In sections 4-6, we use the notation Fn () to denote the same probability, but a specific net function in relation to the result later."}, {"heading": "2.3 Region Partitions", "text": "In sections 4-6 we examine the surface of the objective function (in the context of the ReLU activation functions) via a partition through hyperplanes, which induces the sample to the weight space. Below, the definitions used in this context. In a neural network, each neuron receives its input and then computes a point product with its weights before calculating the output of the ReLU and passing the result to the next layer. Since the ReLU is the zero function on one half space and the identity function on the other, we try to understand the behavior of the objective function on each half space separately. Definition 7. (Region) For each sample instance xt-S, t-1 and b-1-1), we designate the open half space Hbt = {w-Rd: < w, xt > b > 0}."}, {"heading": "2.4 Analysis on the sphere", "text": "In sections 5-6, we refer to some analyses of the sphere to get results on Fn (). In these sections, the following definitions are used: Definition 10. (Unit Sphere) Let Sd \u2212 1 = {x-Rd: conservation x-2 = 1} denote the d \u2212 1 dimensional unity sphere. Definition 11. (Hyper-spherical cap) Let Sd \u2212 1 (a, \u03b8) = {b-Sd \u2212 1: < a, b > [cos] denote the d \u2212 1 dimensional hyperspherical cap of the spherical radius. Definition 12. (Spherical spacing) Let s (a, b): = Arc (< a, b >) denote the spherical distance (the angle between the two vectors) of two points a, b \u2212 Sd \u2212 1. If the spherical radius between a \u2212 b is taken as a smaller angle. < b > the spherical points (the spherical area between the two \u2212 d) of the 1."}, {"heading": "3 Hardness result for single neuron nets", "text": "Considering the complicated nature of the surface of the objective function, the most natural starting point is the simplest possible architecture, namely a single neuron without bias: x 7 \u2192 \u03c6 (< w, x >). Although networks consisting of a single neuron are easy to analyze and program, in the worst case they prove to be very difficult to train because the objective function could contain exponentially many bad local minima as a function of dimension d [2]. However, the authors in [2] also show that in the 0-realizable case, under some mild conditions in terms of loss and activation functions, a single minimal surface of the objective function exists. Beyond that, we first provide a construction that shows that even if the feasibility assumption is only slightly broken, then an exponential number of local minima could still arise. Furthermore, if we assume that the value-formally minimalist of the value-initialized neuron is randomly distributed from a single point of the unisphere of the unity function."}, {"heading": "3.1 Exponentially many local minima for -realizable nets", "text": "??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "3.2 Learning singleton data sets using ReLUs", "text": "We assume that a single neuron in this case is likely that learning is still a difficult task. (0,..), (0,.), (0,.), (0,.), (0,.), (0,.), (0,.), (0,.), (0,.), (0,.), (0,.), (0,.), (0,.), (0,.), (0,.), (0,.), (0,.), (0,.), (0,.), (0), (0,.), (0,.), (0,.), (0,.), (0,.), (0,.), (0,.), (0,.), (0,.), (0,.), (0,.), (0,.), (0, (.), (0,.), (0,.), (0,.), (0,.), (0,.), (0,.), (0,.), (0,.), (0,.), (0,.), (0,.), (0,.), (0,.), (0,.), (0,.), (0, (0,.), (0,.), (0,.), (0,.), (0,.), (0, (0,.), (0,.), (0,.), (0,.,., \"(.,\" (., \"(.,\" (., \"(.,\" (., \"(.,\" (.), \"(.,\" (., \"(.),\" (., \"(.,\" (.), \"(.,\" (., \"(.,\" (.), \"(.,\" (., \"(.,\" (.), \"(.),\" (., \"(.,\" (.), \"(.),\" (., \"(.,\" (., \"(.),\" (.), \"(.,"}, {"heading": "4 Structural properties of the objective function", "text": "In this section, we note and prove some results on the geometry of the lens function, which allow us to gain more insight into its structure, and lay the foundations for the results to be expected in sections 5, 6. Starting from this section, we will always consider ReLU activation functions and square loss."}, {"heading": "4.1 Existence of a path to the global minimum", "text": "& # 8222; & # 8222; & # 8222; & # 8222; & # 8222; & # 8222; & # 8222; & # 8222; & # 8222; & # 8222; & # 8222; & # 8222; & # 8222; & # 8222; & # 8222; & # 8222; & # 8222; & # 8222; & # 8222; & # 8222; & & # 822; & & & # 822; & & & 222; & 222; & 222; & # 8222; & & # 822; & & 222; & 222; & 222; & 222; & 222; & 222; & 222; & 222; & 222; & # 822; & 222; & 222; & 222; & 222; & 222; & 222; & 222; & 222; & 222; & 222; & 222; & 222; & 222; & 222; & 222; & 222; & 222; & # 822; & 222; & 222; & 222; & 222; & 222; & 222; & # 822; & 222; & 222; & 222; & 222; & 222; & 222; & 222; & # 822 & 222; & 222; & # 822 & 222; & 222; & # 822 & 222; & 222 & 222; & # 822 & 222; & # 822; & # 822 & 222 & 222 & 222; & # 822 & # 822 & 222; 822 & 222 & # 822 & 222 & # 822 & # 822; 822 & 222 & 222 & # 822; 822 & # 822 & 222 & # 822; & # 822 & 222 & # 822 & # 822; 822; 822 & 222 & # 822 & # 822 & # 822 & # 822; 822 & 222 & # 822 & 222"}, {"heading": "4.2 Partitioning the weight space of the objective function", "text": "The aim of this subsection is to capture some important structural properties of such 2-layer networks, which are used in deriving the results of the following sections. Intuitively, we capture the following two important observations: \u2022 First, due to the positive homogeneity of the ReLU function, the neural network is invariant up to the multiplication of each wi and the division of the corresponding vi by the same positive factor c."}, {"heading": "5 Successful random initialization paired with overspecification", "text": "In this section, we show that for data that are feasible, two layer networks are used, the likelihood of initialization of a pelvis with a global minimum of increases, as we add more neurons to the first layer, according to the idea of over-specification. We note that these results increase exponentially without significant additional assumptions, but on the other hand, the number of neurons required to ensure a constant probability of success. In the next section, we show results with a much weaker dependence on d, but under additional assumptions. Following the discussion in the previous section, we consider an initialization scheme in which the weights of the initial neurons of {\u2212 1, + 1} n are fixed and then the weights of each neuron are selected in the first layer."}, {"heading": "6.1 Learning Rank m Data", "text": "We assume that our data matrix X = (Y1) is equal, where m = > number of data points = = >. We note that this immediately means m \u2264 d that the number of training examples is not greater than the dimension. < ai = > ai = > number of data points. < ai = > ai = = effectively, this means that we can examine the surface of the objective function effectively, while also serving as the basis for the cluster data scenario that we will examine in Theorem 8. As in the previous section, Fn is defined in terms of an initialization distribution that randomly determines the output of neurons, and where the optimization is performed in relation to the first level. Our main result in this section is the following theorem, which implies that below the order of precedence, a two-layered network of size O (log m) is sufficient to initialize an error with overwhelming probability."}, {"heading": "6.2 Learning Clustered Data", "text": "In the previous section, we have shown that when we engage with the individual regions, local minima do not matter. Typically, when we train the neural networks, we expect the data to be true - the dimension of the data is smaller than the size of the sample. To say something meaningful in this system, we will consider an extension of the previous result, where we have fewer data points than dimensions d, and therefore assume that the data still does not pose a problem. The difficulty in analyzing the surface of the objective function, if m d arises from the somewhat complicated region associated with the data, would, however, be that in the case of clusters, we would expect the regional partition to be from the centers of the clusters, up to a certain noise that develops around each cluster center that arises from the individual regions."}, {"heading": "7 Conclusions and future work", "text": "In this paper, we investigated the geometry of the surface of the objective function of artificial neural networks by examining the distribution of the minimum value of the basin from which we initiate. We demonstrated an example of the accuracy of individual neural networks that collapse under overspecification with two-layer networks. Then, we proceeded to characterize various properties of the objective function, which enabled us to further investigate the effects of over-specification on the geometry of objective function. In particular, we showed that for ReLU networks of any depth, then with constant probability, a strictly monotonous path from the initial weights to a global optimum exists, so \"valley crossing\" is not strictly necessary. In the more specific context of two-layer ReLU networks, where we randomly select the weights of the initial neurons and optimize them over the weights of the first layer, we showed that if the network size increases, we contribute from a good combination of these results to being overwhelmed in practice."}, {"heading": "8 Appendix", "text": "In this appendix we illustrate two specific constructions of theorem 1, one for ReLU and one for a sigmoid, both of which are paired with the square loss. Furthermore, we illustrate two technical lemmas used in the proof of theorem 5."}, {"heading": "8.1 Exponentially many local minima for -realizable nets with ReLU activation.", "text": "Define \u03c6 (x) = max {0, x}, L (y, y \u2032) = (y \u2212 y \u2032) 2Given > 0, consider the following example: S = {(12, \u221a 2), (\u2212 1, 1)} For i = 1, 2Li (w) = (\u03c6 (wxi) \u2212 yi) 2And designate ES (w) = 12 (L1 (w) + L2 (w)))) Note that ES (\u2212 1) = ES (2 \u221a 2) = 12 are both local minima, and therefore S -realizable. It appears straightforward that IT is convex in (\u2212 \u221e, 0) and (0, \u221e), which can also be seen in Figure 3. So, if we initialize uniformly on the standard ball, then we have a 50% chance of initializing from the bad pelvis."}, {"heading": "8.2 Exponentially many local minima for -realizable nets with sigmoid activation.", "text": "The proof of the Theorem 1 requires that the activation function meets the requirements. < < p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p p? p p? p? p? p? p p p? p? p p p p? p? p? p p? p? p p p p p? p? p p p? p? p p p p p p? p p? p p? p p p p? p p p p p p? p? p p p p p? p p p p? p p p p? p p p? p p p p p? p p p p p p? p p p p p? p p p p? p p p p p p p? p p p p p p? p p p p p p p p? p p p p p p p p p p p p p? p p p p p? p p p p? p? p p p p p p p p? p? p p p? p p p p p p p p? p p p p p p p p p p p? p p p p p p p p p p p p p? p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p"}, {"heading": "8.3 Technical results from section 5", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.3.1 Proof of lemma 4.", "text": "\u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2"}, {"heading": "8.3.2 Proof of lemma 5.", "text": "The proof. Fix i [n] and Compute that Nn (W, v) (x) and Nn (W, v) are Nn (< wi, xt >) is provided....,, wjn Nn (W, v) (x). (x1,.., xn), since Nn is differentiated almost everywhere and has a limited subgradient, we have that it is in wi \"x\" -Lipschitz."}], "references": [{"title": "Learning polynomials with neural networks", "author": ["A. Andoni", "R. Panigrahy", "G. Valiant", "L. Zhang"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1908\u20131916", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Exponentially many local minima for single neurons", "author": ["P. Auer", "M. Herbster", "M.K. Warmuth"], "venue": "NIPS", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1996}, {"title": "Breaking the curse of dimensionality with convex neural networks", "author": ["F. Bach"], "venue": "arXiv preprint arXiv:1412.8690", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(8):1798\u20131828", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Convex neural networks", "author": ["Y. Bengio", "N.L. Roux", "P. Vincent", "O. Delalleau", "P. Marcotte"], "venue": "Advances in neural information processing systems, pages 123\u2013130", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Training a 3-node neural network is np-complete", "author": ["A.L. Blum", "R.L. Rivest"], "venue": "Neural Networks, 5(1):117\u2013127", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1992}, {"title": "The loss surface of multilayer networks", "author": ["A. Choromanska", "M. Henaff", "M. Mathieu", "G.B. Arous", "Y. LeCun"], "venue": "arXiv preprint arXiv:1412.0233", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["G. Cybenko"], "venue": "Mathematics of control, signals and systems, 2(4):303\u2013314", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1989}, {"title": "Improving deep neural networks for lvcsr using rectified linear units and dropout", "author": ["G.E. Dahl", "T.N. Sainath", "G.E. Hinton"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 8609\u20138613. IEEE", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Y. Dauphin", "R. Pascanu", "C. Gulcehre", "K. Cho", "S. Ganguli", "Y. Bengio"], "venue": "NIPS", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Qualitatively characterizing neural network optimization problems", "author": ["I.J. Goodfellow", "O. Vinyals"], "venue": "arXiv preprint arXiv:1412.6544", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods", "author": ["M. Janzamin", "H. Sedghi", "A. Anandkumar"], "venue": "CoRR abs/1506.08473", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Q.V. Le"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 8595\u20138598. IEEE", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributing points on the sphere: partitions", "author": ["P. Leopardi"], "venue": "separation, quadrature and energy. PhD thesis, University of New South Wales", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "Concise formulas for the area and volume of a hyperspherical cap", "author": ["S. Li"], "venue": "Asian Journal of Mathematics and Statistics, 4(1):66\u201370", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "On the computational efficiency of training neural networks", "author": ["R. Livni", "S. Shalev-Shwartz", "O. Shamir"], "venue": "NIPS, pages 855\u2013863", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Relations among complexity measures", "author": ["N. Pippenger", "M.J. Fischer"], "venue": "Journal of the ACM (JACM), 26(2):361\u2013381", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1979}, {"title": "l1 regularization in infinite dimensional feature spaces", "author": ["S. Rosset", "G. Swirszcz", "N. Srebro", "J. Zhu"], "venue": "Learning theory, pages 544\u2013558. Springer", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning kernel-based halfspaces with the 0-1 loss", "author": ["S. Shalev-Shwartz", "O. Shamir", "K. Sridharan"], "venue": "SIAM Journal on Computing, 40(6):1623\u20131646", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "Computer Vision\u2013ECCV 2014, pages 818\u2013833. Springer", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 16, "context": "Artificial neural nets are a known to have broad expressive power [17, 18, 20].", "startOffset": 66, "endOffset": 78}, {"referenceID": 17, "context": "Artificial neural nets are a known to have broad expressive power [17, 18, 20].", "startOffset": 66, "endOffset": 78}, {"referenceID": 19, "context": "Artificial neural nets are a known to have broad expressive power [17, 18, 20].", "startOffset": 66, "endOffset": 78}, {"referenceID": 3, "context": "A recent breakthrough in the effectiveness of neural networks has led to very impressive practical performance on a variety of domains (a few recent examples include [4, 9, 13, 14, 21].", "startOffset": 166, "endOffset": 184}, {"referenceID": 8, "context": "A recent breakthrough in the effectiveness of neural networks has led to very impressive practical performance on a variety of domains (a few recent examples include [4, 9, 13, 14, 21].", "startOffset": 166, "endOffset": 184}, {"referenceID": 12, "context": "A recent breakthrough in the effectiveness of neural networks has led to very impressive practical performance on a variety of domains (a few recent examples include [4, 9, 13, 14, 21].", "startOffset": 166, "endOffset": 184}, {"referenceID": 13, "context": "A recent breakthrough in the effectiveness of neural networks has led to very impressive practical performance on a variety of domains (a few recent examples include [4, 9, 13, 14, 21].", "startOffset": 166, "endOffset": 184}, {"referenceID": 20, "context": "A recent breakthrough in the effectiveness of neural networks has led to very impressive practical performance on a variety of domains (a few recent examples include [4, 9, 13, 14, 21].", "startOffset": 166, "endOffset": 184}, {"referenceID": 5, "context": "By reduction to k-coloring, it has been shown that finding the weights that best fit the training set is NP-hard [6].", "startOffset": 113, "endOffset": 116}, {"referenceID": 1, "context": "It is also known that even neural networks comprised of a single neuron might have exponentially many local minima [2].", "startOffset": 115, "endOffset": 118}, {"referenceID": 16, "context": "It was recently shown that for such networks, under mild assumptions, global optima are ubiquitous, and \u201cmost\u201d starting points will lead to the global optima upon optimizing the weights of the last layer [17].", "startOffset": 204, "endOffset": 208}, {"referenceID": 2, "context": "There is some theoretical and empirical evidence that this is indeed the case [3, 7, 17], but our theoretical understanding of this phenomenon is still quite limited.", "startOffset": 78, "endOffset": 88}, {"referenceID": 6, "context": "There is some theoretical and empirical evidence that this is indeed the case [3, 7, 17], but our theoretical understanding of this phenomenon is still quite limited.", "startOffset": 78, "endOffset": 88}, {"referenceID": 16, "context": "There is some theoretical and empirical evidence that this is indeed the case [3, 7, 17], but our theoretical understanding of this phenomenon is still quite limited.", "startOffset": 78, "endOffset": 88}, {"referenceID": 1, "context": "What we do One way to study the hardness of the training problem has been to consider the number of non-global local minima the objective function has [2].", "startOffset": 151, "endOffset": 154}, {"referenceID": 1, "context": "More specifically, we provide the following results: \u2022 We begin by extending the result of [2] on exponentially many local minimum basins for single neurons, by showing that it holds even if there exists a hypothesis which achieves an arbitrarily small positive training error.", "startOffset": 91, "endOffset": 94}, {"referenceID": 10, "context": "Although this does not ensure that such a global minimum will be reached, it does mean that \u201ccrossing valleys\u201d across the non-convex loss surface is not a must to reach a good solution, and accords with recent empirical evidence to this effect [11].", "startOffset": 244, "endOffset": 248}, {"referenceID": 16, "context": "Related work We begin by discussing the work done in [17], where the authors show that for any non-linear activation function and architecture large enough such that the last hidden layer is at least as large as the size of the sample, then global minima are ubiquitous.", "startOffset": 53, "endOffset": 57}, {"referenceID": 0, "context": "Other recent efforts in the field include [1, 3, 12].", "startOffset": 42, "endOffset": 52}, {"referenceID": 2, "context": "Other recent efforts in the field include [1, 3, 12].", "startOffset": 42, "endOffset": 52}, {"referenceID": 11, "context": "Other recent efforts in the field include [1, 3, 12].", "startOffset": 42, "endOffset": 52}, {"referenceID": 2, "context": "In [3], the author studies an approach reducing the finite dimensional problem of training a 2-layer neural net paired with ReLUs with a fixed architecture into a problem of finding a sparse measure on an infinite dimensional space (building on work in [5, 19].", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "In [3], the author studies an approach reducing the finite dimensional problem of training a 2-layer neural net paired with ReLUs with a fixed architecture into a problem of finding a sparse measure on an infinite dimensional space (building on work in [5, 19].", "startOffset": 253, "endOffset": 260}, {"referenceID": 18, "context": "In [3], the author studies an approach reducing the finite dimensional problem of training a 2-layer neural net paired with ReLUs with a fixed architecture into a problem of finding a sparse measure on an infinite dimensional space (building on work in [5, 19].", "startOffset": 253, "endOffset": 260}, {"referenceID": 0, "context": "In [1], the authors show that two-layer neural nets of sufficient size can learn low degree polynomials using gradient descent.", "startOffset": 3, "endOffset": 6}, {"referenceID": 11, "context": "In [12], the authors present an algorithm with provable guarantees for training neural nets via tensor decompositions.", "startOffset": 3, "endOffset": 7}, {"referenceID": 6, "context": "Another relevant work is [7], in which the authors investigate the surface area of the loss function of neural nets using ReLUs as activation functions, and make somewhat strong distributional assumptions on the data, such as assuming that the coordinates of the observations in the data are standard normally distributed, and in particular the coordinates are independent of one another, and modeling the connection between neurons as a Bernoulli random variable with constant success probability.", "startOffset": 25, "endOffset": 28}, {"referenceID": 9, "context": "See also [10] and references therein for related works along similar lines.", "startOffset": 9, "endOffset": 13}, {"referenceID": 1, "context": "Although simple to analyze and program, nets comprised of a single neuron prove to be very hard to train in the worst case, as the objective function might contain exponentially many poor local minima as a function of the dimension d [2].", "startOffset": 234, "endOffset": 237}, {"referenceID": 1, "context": "However, the authors in [2] also demonstrate that for the 0-realizable case, under some mild conditions on the loss and activation functions, there exists a single minimal surface of the objective function.", "startOffset": 24, "endOffset": 27}, {"referenceID": 0, "context": "Furthermore, since the loss on x1 is constant \u2200w \u2208 [\u22121, 0] and the loss on x2 is constant \u2200w \u2208 [0, 1], we have that the objective function contains two basins meeting at zero, so the sign of w determines which of the basins we fall into.", "startOffset": 95, "endOffset": 101}, {"referenceID": 1, "context": "We now extend our sample to be d-dimensional in a similar manner as did the authors in [2] as follows: For i = 1, 2 and j \u2208 [d], we use the mapping xi,j 7\u2192 (0, .", "startOffset": 87, "endOffset": 90}, {"referenceID": 10, "context": "However, this result nevertheless sheds light on the nature of the surface function, demonstrating that it is not completely sporadic in the sense that \u201ccrossing valleys\u201d is not a must to reach a good solution, and accords with recent empirical evidence to this effect [11].", "startOffset": 269, "endOffset": 273}, {"referenceID": 0, "context": "Assume that there exists a continuous path \u03b3 (t), t \u2208 [0, 1] s.", "startOffset": 54, "endOffset": 60}, {"referenceID": 0, "context": "\u03b3 (0) = winit, \u03b3 (1) = wopt and \u2200t \u2208 [0, 1], there exist an instance x in the training set such that N (\u03b3(t)) (x) 6= 0.", "startOffset": 37, "endOffset": 43}, {"referenceID": 0, "context": "We have \u2016N (winit)\u2212 y\u2016 > \u2016y\u2016 \u21d2 \u2016N (winit)\u2016 \u2212 2 \u3008N (winit) ,y\u3009 > 0 (2) Define Nt := N (\u03b3 (t)), we once again use the positively-homogeneous nature of the ReLU to scale the weights of the net by a positive factor at > 0, while stressing that the addition of a bias term does not affect this trick and nevertheless allows us to scale the weights by a constant positive factor, effectively forcing \u2200t \u2208 [0, 1] the equality \u2016atNt \u2212 y\u2016 = \u2016N0 \u2212 y\u2016 (1\u2212 t) + \u2016y\u2016 t", "startOffset": 399, "endOffset": 405}, {"referenceID": 7, "context": "Although still relatively simple, they already possess universal approximation properties [8], and encapsulate the challenge of handling a highly non-convex objective function.", "startOffset": 90, "endOffset": 93}], "year": 2017, "abstractText": "Over the past few years, artificial neural networks have seen a dramatic resurgence in popularity as a tool for solving hard learning problems in AI applications. While it is widely known that neural networks are computationally hard to train in the worst case, in practice, neural networks are trained efficiently using SGD methods and a variety of techniques which accelerate the learning process. One mechanism which has been suggested to explain this is overspecification, which is the training of a network larger than what would be needed with unbounded computational power. Empirically, despite worst-case NP-hardness results, large networks tend to achieve a smaller error over the training set. In this work, we aspire to understand this phenomenon. In particular, we wish to better understand the behavior of the error over the sample as a function of the weights of the network, where we focus mostly on neural nets comprised of 2 layers, although we will also consider single neuron nets and nets of arbitrary depth, investigating properties such as the number of local minima the function has, and the probability of initializing from a basin with a given minimal value, with the goal of finding reasonable conditions under which efficient learning of the network is possible.", "creator": "LaTeX with hyperref package"}}}