{"id": "1511.04561", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2015", "title": "8-Bit Approximations for Parallelism in Deep Learning", "abstract": "The creation of practical deep learning data-products often requires the parallelization across processors and computers to make deep learning feasible on large data sets, but bottlenecks in communication bandwidth make it difficult to attain good speedups through parallelism. Here we develop and test 8-bit approximation algorithms, which provide improved utilization of the available bandwidth by compressing 32-bit gradients and nonlinear activations to 8-bit approximations. We show that these approximations do not decrease predictive performance on MNIST, CIFAR10, and ImageNet for both model and data parallelism and provide a data transfer speedup of 2x relative to 32-bit parallelism. We compare our data types with other methods and show that 8-bit approximations achieve state-of-the-art speedups for model parallelism in general and data parallelism with up to 200k parameters per layer. Thus 8-bit approximation is the single best method for parameter compression in the parallelization of convolutional networks.", "histories": [["v1", "Sat, 14 Nov 2015 14:04:51 GMT  (218kb,D)", "https://arxiv.org/abs/1511.04561v1", null], ["v2", "Sun, 22 Nov 2015 10:25:58 GMT  (224kb,D)", "http://arxiv.org/abs/1511.04561v2", null], ["v3", "Mon, 4 Jan 2016 20:32:52 GMT  (190kb,D)", "http://arxiv.org/abs/1511.04561v3", null], ["v4", "Fri, 19 Feb 2016 16:26:30 GMT  (191kb,D)", "http://arxiv.org/abs/1511.04561v4", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["tim dettmers"], "accepted": true, "id": "1511.04561"}, "pdf": {"name": "1511.04561.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Tim Dettmers"], "emails": ["tim.dettmers@gmail.com"], "sections": [{"heading": null, "text": "Building practical deep learning data products often requires parallelization between processors and computers to make deep learning feasible on large datasets, but communication bandwidth bottlenecks make it difficult to achieve good acceleration through parallelism. Here, we develop and test 8-bit approximation algorithms that make better use of available bandwidth by compressing 32-bit gradients and nonlinear activations to 8-bit approximations. We show that these approximations do not reduce the predictive capability of MNIST, CIFAR10 and ImageNet for both model and data parallelism, and provide data transmission acceleration of 2 x compared to 32-bit parallelism. We build a predictive model for speedups based on our experimental data, verify its validity on known speedup data, and show that we can achieve acceleration of 50-bit parallelism from a system with a very similar B23 to a system with a very similar B23."}, {"heading": "1 INTRODUCTION", "text": "Deep learning is a field inherently driven by advances in computer processing (GPU).Graphic processing units (GPU) can accelerate deep learning by a factor of up to 10-20 compared to a CPU. Following these breakthroughs, many teams found themselves accelerating the traininfple GPU or computer (GPU).Coates et al., 2013; Dean et al., 2015."}, {"heading": "2 BACKGROUND", "text": "To understand the characteristics of a successful parallel deep-learning algorithm, it is necessary to understand how communication between GPUs works and what the bottlenecks are for both model and data-parallel deep-learning architectures. First, we look at the specific characteristics of data and model parallelism, and then we look at general bottlenecks in GPU-to-GPU communication."}, {"heading": "2.1 DATA PARALLELISM", "text": "In data parallelism, the model is kept constant for all GPUs while each GPU is fed a different mini-batch; after each pass, the gradients are exchanged, i.e. synchronized with each GPU: \u2022 How it is done: For fully connected layers, the data is split by sample size (better cross-validation error) or by feature size (reduces memory usage dramatically; slightly worse cross-validation error; complicates the architecture) \u2022 Irregular synchronization: parameters are synchronized (averaged) once after each complete forward and backward run \u2022 Efficiency: Data parallelism is efficient when the model has few parameters, e.g. long-term recursive neural networks of 1997 (or higher);"}, {"heading": "2.2 MODEL PARALLELISM", "text": "In model parallelism, the data is kept constant for all GPUs, while each GPU contains only a portion of the complete model: \u2022 How it is done: Distribute the parameters of a layer over multiple GPUs (split by input or output dimension); run the same mini-batch through the distributed layer \u2022 Frequent synchronization: The parameters are synchronized once for each layer; the outputs of the layer are stacked or added depending on the matrix operation \u2022 Efficiency: Model parallelism is efficient if the layer has many parameters, e.g. in fully connected layers (because the parameter matrix is reduced by a factor corresponding to the number of GPUs) \u2022 Scaling limitations: Poor performance with larger mini-batch sizes; the larger the mini-batch size, the larger the matrix that needs to be synchronized via GPUs \u2022 Required numerical accuracy: the larger the errors in 2001, the larger the larger the errors may be, as the smaller the mini-size must be."}, {"heading": "2.3 GENERAL BOTTLENECKS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.3.1 PCIE SWITCHES", "text": "PCI Express (PCIe) is built like a normal network, where two PCIe slots share a common switch that can handle an outgoing and an incoming connection at the same time. Thus, only a single device within a pair of devices can communicate with another pair of devices at any given time, both GPUs and InfiniBand cards. PCIe switches must therefore be considered to achieve optimum performance in a multi-GPU system."}, {"heading": "2.3.2 BANDWIDTH", "text": "GPUs within a computer communicate through the PCIe interface, which provides practically 14 GB / s of bandwidth when they contain 2 GPUs and about 7 GB / s of bandwidth when a computer contains more than 2 GPUs. GPUs between computers usually communicate via InfiniBand network cards, which have a handy bandwidth of 3-7 GB / s (Quad Data Rate (QDR) or 14 Data Rate Cards (FDR)). Communication is the biggest bottleneck in deep learning, which can be illustrated by a simple example: AlexNet is a revolutionary network with about 60 million parameters (Krizhevsky et al., 2012); a full reverse is completed in less than 100 ms with the current generation GPUs1. When implementing naive data parallelism with 4 GPUs, this means that the PUs must always synchronize more data than 2 GPUs with the other 3 GPUs for the data transfer of 2 GPUs."}, {"heading": "2.3.3 LATENCY", "text": "PCIe latency between messages is usually in the microsecond range and does not increase significantly with message size, so PCIe latency is negligible. However, on larger systems with multiple computers using InfiniBand, it can lead to significant bottlenecks, especially in collective communication (one-to-one, all-to-one, all-to-all) on clusters (Sur et al., 2005; Singh, 2012). Latency of current InfiniBand systems (FDR) is increasing exponentially, and over 512 kilobytes of data it becomes unmanageable for GPU clusters with more than a dozen nodes (latency > 0.5 ms per message)."}, {"heading": "2.4 OPTIMAL PARALLELISM FOR CONVOLUTIONAL NETS", "text": "The currently best known method for parallelizing coil layers on any number of K-GPUs or computers is to use data parallelism in the coil layers and to model parallelism in the fully connected layers (Krizhevsky, 2014). However, we perform only one parallel step of the model, but K does such steps with a Kth of the full batch size. Thus, after the coil layers - i.e. before the fully connected layers - 1 / Kth of each batch (from here sub-batch) is distributed across all GPUs and then a model is performed in parallel forward and backward up to the folding layers. During each partial forward-backward pass, the next partial batches are distributed to all GPUs, 1https: / / github.com / soumith / convennet-benchmarksthus hide the communication time for all incoming sub-layers below the forward pass compression time. The same procedure can be applied to the synchronization of these partial layers to fully apply the connected activities."}, {"heading": "3 8-BIT APPROXIMATION", "text": "We are trying to develop an approximation for gradients that is small but has enough accuracy to be used for both data and model parallelism; the default size of gradients in deep learning is currently 32-bit, which is the smallest practical dimension for floating-point numbers on GPUs, since CUDA only supports 32 and 64-bit floating-point arithmetic; we chose 8-bit for our gradient approximation data type because (1) it is easy to handle as we can store it in 8-bit characters without signs; and (2) we argued that less than 8 bits would have insufficient accuracy for model parallelism, as existing literature suggested that less than 8 bits can lead to a significant decrease in accuracy (Courbariaux et al., 2014)."}, {"heading": "3.1 DESIGNING 8-BIT DATA TYPES", "text": "Of these 8 bits, one bit is reserved for the character of the number, while the rest can mainly be used for the exponent and the mantissa. The problem with the mantissa is that our range of precision is very limited. With a 3-bit exponent, the mantissa will hold values between 0 and 15, and as such decimal values above 15. To reduce this error, we can use the bits of the mantissa to represent a binary tree with an interval (0.1, 1) marked according to the path that passes through the tree."}, {"heading": "3.2 IMPLEMENTATION AND COMPUTATIONAL PERFORMANCE", "text": "The fastest implementation for 8-bit compression and decompression that we could imagine is a binary search in a sorted table of all positive 128 values in common GPU memory and tracking the signature of the respective number. Shared memory is about 100 times faster than global GPU memory and therefore binary searches in shared memory are very fast.In our implementation, we used one thread per number in a binary search. Additional parallelization is easily possible by dividing the table into n intervals, where n is the number of threads per number. However, the necessary thread synchronization is expensive and the performance gains are probably negligible compared to the additional resource costs (threads).For decompression, the 32-bit values are read into common memory and we seek the 32-bit value for the respective 8-bit value. Here, we use one thread per number / lookup. For decompression, on average, these algorithms perform 0.5 and 0.5 respectively in each compression."}, {"heading": "3.3 THEORETICAL SPEEDUP", "text": "We have measured the average total transfer time (compression, transfer and decompression) for our techniques and compared it to 32-bit transfers between GPUs. We have measured this time on a board2https: / / github.com / TimDettmers / clusterNet /; contact me if you need help integrating the functions into your library, which yields 8 PCIe 3.0 lanes for each GPU, and thus a theoretical bandwidth of about 8 GB / s; however, the bandwidth for small messages is usually much lower. The algorithms have been run on two NVIDIA GTX titans, each transmitted 100 times, and the average total transfer time has been measured. We have the Measurement Interface (MPI) implementation provided by OpenMPI 1.8.5, which uses low CUDA routines to enable GPU to GPU communication."}, {"heading": "3.4 APPROXIMATION ERROR", "text": "We tested the approximation error of our data types on multiple distributions and on the gradients (data parallelism) and activations (model parallelism) on MNIST (see Table 2). We calculated the mean absolute and relative error from a sample of 25 million numbers drawn from normal distributions. For dynamic tree and linear quantification, the sample was standardized by dividing the maximum absolute value and then denoralized after compression. As can be seen from Table 2, the 8-bit dynamic tree provides the overall best performance for random distributions and parallelism. For our tests on MNIST, we used rectified linear units, a 784x1024x1024x10x10 architecture with dropout (2.0)."}, {"heading": "4 COMPARISON TO OTHER METHODS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 OTHER SUB-32-BIT DATA TYPES", "text": "Dynamic fixed point data types are data types that use all their bits for the praying mantis and have a dynamic exponent that is stored for the acquisition of numbers (matrix, vector) and adapted during runtime. Courbariaux et al. (2014) used 10-bit dynamic fixed point data types for the calculation and 12-bit width for parameter updates to train a maximum end-to-end Convolutionary Network. However, their results at PI MNIST, MNIST and CIFAR10 are about 20% worse than the state of the art obtained by Goodfellow et al. (2013). In our paper, we show that we can use 8-bit gradations for parameter updates without degrading performance. However, dynamic fixed point data types can also be used for end-to-end linear training, and as such a combination of both data point and vanity data point could not provide optimal performance in 2011."}, {"heading": "4.2 1-BIT QUANTIZATION", "text": "In fact, it is a very successful solution that is able to put people's needs first."}, {"heading": "5 THEORETICAL MODELS FOR PARALLELISM", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 A THEORETICAL SINGLE NODE MODEL FOR 4 GPUS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1.1 BENCHMARKING AN ARCHITECTURE FOR SINGLE NODE 4 GPU PARALLELISM", "text": "We used NervanaGPU4 benchmarks to generate base and parallel time data for the AlexNet architecture, as implemented in Krizhevsky (2014). NervanaGPU requires a Maxwell GPU or newer, but there are other deep learning libraries 5 that can also easily handle benchmarking. We benchmarked the timing of Convolutionary Cores, Pooling Operations, and Matrix Multiplications in fully connected layers for both model parallelisms (output size divided by 4) and no parallelism. See Table 3 and Table 4 for the generated benchmark data. Note that the timings of pooling operations are added to the Convolutionary Timings; the pooling operations are in the order of magnitude of 5-10% of the timings of the respective preceding conversion. The transfer size is the size of the subgrades of the buffer time (because we have a quarter of the GPU for these grades)."}, {"heading": "5.1.2 ANALYSIS OF SINGLE NODE 4 GPU PARALLELISM", "text": "This year we have never had so much money in our hands as this year, \"he told the Deutsche Presse-Agentur in an interview with the Deutsche Presse-Agentur.\" We have never had so much money in our hands as this year, \"he said.\" We have never had so much money in our hands as this year. \""}, {"heading": "5.1.3 PREDICTIONS OF THE SINGLE NODE 4 GPU MODEL", "text": "If we put everything together, we can calculate the theoretical acceleration. We use the full data parallelism in the revolutionary layers and the model parallelism of K = 4 partial batches through the fully connected layers. In parallel layers, we need the same time as for 1 GPU, but without the fully connected part going through K = 4 model parallelism. On top of that, we add up all penalties for the parallel level of the model, thus getting the expression: Acceleration = number of GPUs \u00b7 Total time (total time \u2212 fc time) + conv Penalty + (# partial batches \u00b7 parallel fc time) + fc Penalty And this results in our example: Acceleration 32 = 416.4 \u00b7 416.4 (104.1 \u2212 6.5) + 0.05 + (4 \u00d7 3.34) + 6.81 '3.53Speedup 8 = Kriernsky 8 = 416.4 (104.1 \u2212 6.5) + 0.05 + (4 x acceleration) 3.674.7 Total time = 104.14.7 Kriernsky 4.7"}, {"heading": "5.2 A THEORETICAL GPU CLUSTER MODEL FOR 32 NODES, 96 GPUS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.2.1 BENCHMARKING COMPONENTS IN A 32 NODE, 96 GPU CLUSTER", "text": "The subcomponents of a GPU cluster resemble a single machine, except that we now also have a network of switches between computers. Bottlenecks in network communication are network bandwidth and network latency. InfiniBand network interfaces usually have very good network bandwidth and network latency, and our analysis is based on an FDR InfiniBand system used in conjunction with MPI software to communicate between nodes in the cluster. Benchmarks for InfiniBand systems that use MPI are readily available online. Table 5 shows the latency for messages in our model-parallel scheme. To find the InfiniBand latencies, we looked at charts showing the latency for a given message size. In this case, the message size is the subgradient (parameter / 32) or the revolutionary activities (depending on the subbatch size). From this data, we also found that most messages have a bandwidth of 6GB / FDR connection."}, {"heading": "5.2.2 ANALYSIS AND SPEEDUP PREDICTION OF 32 NODE 96 GPU PARALLELISM", "text": "From Table 5 we can calculate the time needed to transmit the largest revolutionary layer during data parallelism. For 2 PCIe transfers of a sub-gradient and subsequent 31 InfiniBand transfers7see \"convennet2\" here https: / / github.com / soumith / convennet-benchmarkswe have two messages of 36 kilobytes at 5GB / s and 31 messages of 108 kilobytes at 6GB / s with 0.03ms latency per message, respectively. We do this messaging scheme twice: once we distribute the raw gradients, twice we distribute the cumulative gradient across all nodes. The total time for this gradient synchronization scheme is about 1.9 ms. This shows, as in the case of the 4 GPU, that there is no bottleneck in data parallelisms, twice we distribute the cumulative gradient across all nodes."}], "references": [{"title": "Project adam: Building an efficient and scalable deep learning training system", "author": ["Chilimbi", "Trishul", "Suzue", "Yutaka", "Apacible", "Johnson", "Kalyanaraman", "Karthik"], "venue": "In 11th USENIX Symposium on Operating Systems Design and Implementation", "citeRegEx": "Chilimbi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chilimbi et al\\.", "year": 2014}, {"title": "Multi-column deep neural networks for image classification", "author": ["Ciresan", "Dan", "Meier", "Ueli", "Schmidhuber", "J\u00fcrgen"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Ciresan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ciresan et al\\.", "year": 2012}, {"title": "Deep learning with cots hpc systems", "author": ["Coates", "Adam", "Huval", "Brody", "Wang", "Tao", "Wu", "David", "Catanzaro", "Bryan", "Andrew", "Ng"], "venue": "In Proceedings of the 30th international conference on machine learning,", "citeRegEx": "Coates et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2013}, {"title": "Low precision arithmetic for deep learning", "author": ["Courbariaux", "Matthieu", "Bengio", "Yoshua", "David", "Jean-Pierre"], "venue": "arXiv preprint arXiv:1412.7024,", "citeRegEx": "Courbariaux et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2014}, {"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing", "author": ["Dahl", "George E", "Yu", "Dong", "Deng", "Li", "Acero", "Alex"], "venue": "IEEE Transactions on,", "citeRegEx": "Dahl et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2012}, {"title": "Large scale distributed deep networks", "author": ["Dean", "Jeffrey", "Corrado", "Greg", "Monga", "Rajat", "Chen", "Kai", "Devin", "Matthieu", "Mao", "Mark", "Senior", "Andrew", "Tucker", "Paul", "Yang", "Ke", "Le", "Quoc V"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dean et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2012}, {"title": "Deep learning with limited numerical precision", "author": ["Gupta", "Suyog", "Agrawal", "Ankur", "Gopalakrishnan", "Kailash", "Narayanan", "Pritish"], "venue": "arXiv preprint arXiv:1502.02551,", "citeRegEx": "Gupta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Gradient flow in recurrent nets: the difficulty of learning", "author": ["Hochreiter", "Sepp", "Bengio", "Yoshua", "Frasconi", "Paolo", "Schmidhuber", "J\u00fcrgen"], "venue": "long-term dependencies,", "citeRegEx": "Hochreiter et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "One weird trick for parallelizing convolutional neural networks", "author": ["Krizhevsky", "Alex"], "venue": "arXiv preprint arXiv:1404.5997,", "citeRegEx": "Krizhevsky and Alex.,? \\Q2014\\E", "shortCiteRegEx": "Krizhevsky and Alex.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Learning representations by back-propagating errors", "author": ["Rumelhart", "David E", "Hinton", "Geoffrey E", "Williams", "Ronald J"], "venue": "Cognitive modeling,", "citeRegEx": "Rumelhart et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "Deep learning in neural networks: An overview", "author": ["Schmidhuber", "J\u00fcrgen"], "venue": "Neural Networks,", "citeRegEx": "Schmidhuber and J\u00fcrgen.,? \\Q2015\\E", "shortCiteRegEx": "Schmidhuber and J\u00fcrgen.", "year": 2015}, {"title": "1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns", "author": ["Seide", "Frank", "Fu", "Hao", "Droppo", "Jasha", "Li", "Gang", "Yu", "Dong"], "venue": "In Fifteenth Annual Conference of the International Speech Communication Association,", "citeRegEx": "Seide et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Seide et al\\.", "year": 2014}, {"title": "Optimizing All-to-All and Allgather Communications on GPGPU Clusters", "author": ["Singh", "Ashish Kumar"], "venue": "PhD thesis,", "citeRegEx": "Singh and Kumar.,? \\Q2012\\E", "shortCiteRegEx": "Singh and Kumar.", "year": 2012}, {"title": "Scalable distributed dnn training using commodity gpu cloud computing", "author": ["Strom", "Nikko"], "venue": "In Sixteenth Annual Conference of the International Speech Communication Association,", "citeRegEx": "Strom and Nikko.,? \\Q2015\\E", "shortCiteRegEx": "Strom and Nikko.", "year": 2015}, {"title": "High performance rdma based all-to-all broadcast for infiniband clusters", "author": ["Sur", "Sayantan", "Bondhugula", "Uday Kumar Reddy", "Mamidala", "Amith", "Jin", "H-W", "Panda", "Dhabaleswar K"], "venue": "In High Performance Computing\u2013HiPC", "citeRegEx": "Sur et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sur et al\\.", "year": 2005}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tieleman", "Tijmen", "Hinton", "Geoffrey"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}, {"title": "Improving the speed of neural networks on cpus", "author": ["Vanhoucke", "Vincent", "Senior", "Andrew", "Mao", "Mark Z"], "venue": "In Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop,", "citeRegEx": "Vanhoucke et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vanhoucke et al\\.", "year": 2011}, {"title": "Deep image: Scaling up image recognition", "author": ["Wu", "Ren", "Yan", "Shengen", "Shan", "Yi", "Dang", "Qingqing", "Sun", "Gang"], "venue": "arXiv preprint arXiv:1501.02876,", "citeRegEx": "Wu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "Graphics processing units (GPUs) can accelerate deep learning by a factor of up to 1020 compared to a CPU, and these speedups were integral in achieving breakthroughs in speech recognition and computer vision (Ciresan et al., 2012; Dahl et al., 2012; Krizhevsky et al., 2012).", "startOffset": 209, "endOffset": 275}, {"referenceID": 4, "context": "Graphics processing units (GPUs) can accelerate deep learning by a factor of up to 1020 compared to a CPU, and these speedups were integral in achieving breakthroughs in speech recognition and computer vision (Ciresan et al., 2012; Dahl et al., 2012; Krizhevsky et al., 2012).", "startOffset": 209, "endOffset": 275}, {"referenceID": 10, "context": "Graphics processing units (GPUs) can accelerate deep learning by a factor of up to 1020 compared to a CPU, and these speedups were integral in achieving breakthroughs in speech recognition and computer vision (Ciresan et al., 2012; Dahl et al., 2012; Krizhevsky et al., 2012).", "startOffset": 209, "endOffset": 275}, {"referenceID": 0, "context": "After these breakthroughs, GPUs found widespread use and many teams sought to accelerate the traininfple GPUs or computers (Chilimbi et al., 2014; Coates et al., 2013; Dean et al., 2012; Wu et al., 2015).", "startOffset": 123, "endOffset": 203}, {"referenceID": 2, "context": "After these breakthroughs, GPUs found widespread use and many teams sought to accelerate the traininfple GPUs or computers (Chilimbi et al., 2014; Coates et al., 2013; Dean et al., 2012; Wu et al., 2015).", "startOffset": 123, "endOffset": 203}, {"referenceID": 5, "context": "After these breakthroughs, GPUs found widespread use and many teams sought to accelerate the traininfple GPUs or computers (Chilimbi et al., 2014; Coates et al., 2013; Dean et al., 2012; Wu et al., 2015).", "startOffset": 123, "endOffset": 203}, {"referenceID": 19, "context": "After these breakthroughs, GPUs found widespread use and many teams sought to accelerate the traininfple GPUs or computers (Chilimbi et al., 2014; Coates et al., 2013; Dean et al., 2012; Wu et al., 2015).", "startOffset": 123, "endOffset": 203}, {"referenceID": 11, "context": "The main difficulty in the parallelization of deep learning is the sequential nature of backpropagation, where the parameter updates must be fully completed before the next iteration of stochastic gradient descent can ensue (Rumelhart et al., 1988).", "startOffset": 224, "endOffset": 248}, {"referenceID": 13, "context": "convolutional layers in convolutional nets \u2022 Scaling limitations: Current GPU implementations are optimized for larger matrices, hence data parallelism does not scale indefinitely due to slow matrix operations (especially matrix multiplication) for small mini-batch sizes (< 128 per GPU); convolution implementations that rely on matrix multiplication may suffer from this too; the larger the batch size the slower the convergence to a local minimum which is problematic for large systems \u2022 Requires asymptotic accuracy: Good solutions can be found as long as the sequence of updates converges to the minimum asymptotically (Seide et al., 2014)", "startOffset": 624, "endOffset": 644}, {"referenceID": 8, "context": "\u2022 Scaling limitations: Poor performance for larger mini-batch sizes; the larger the mini-batch size, the larger the matrix that needs to be synchronized across GPUs \u2022 Requires numerical accuracy: Outputs must be precise as small deviation may lead to large errors in later layers; this is similar to the exploding gradient problem (Hochreiter et al., 2001)", "startOffset": 331, "endOffset": 356}, {"referenceID": 10, "context": "Communication is the main bottleneck in deep learning which can be illustrated with a simple example: AlexNet is a convolutional network with about 60 million parameters (Krizhevsky et al., 2012); a full forward-backward pass is completed in under 100ms for current generation GPUs1.", "startOffset": 170, "endOffset": 195}, {"referenceID": 16, "context": "However for larger systems with multiple computers that make use of InfiniBand the latency can be a considerable bottleneck, especially for collective communication (one-to-all, all-to-one, all-to-all) on clusters (Sur et al., 2005; Singh, 2012).", "startOffset": 214, "endOffset": 245}, {"referenceID": 3, "context": "We chose to use 8-bit for our gradient approximation data type because (1) it is easy to handle as we can store it in 8-bit unsigned chars, and (2) we reasoned that less than 8 bits would have insufficient accuracy for model parallelism, since existing literature suggested that less than 8 bits can induce considerable reduction in accuracy (Courbariaux et al., 2014).", "startOffset": 342, "endOffset": 368}, {"referenceID": 18, "context": "Using this method with a 7-bit bisection tree with the range [0,1], we receive a data type which is equivalent to linear quantization (Vanhoucke et al., 2011).", "startOffset": 134, "endOffset": 158}, {"referenceID": 3, "context": "Courbariaux et al. (2014) used dynamic fixed point data types with 10-bit width for computation and 12-bit width for parameter updates to train a maxout convolutional network end-to-end.", "startOffset": 0, "endOffset": 26}, {"referenceID": 3, "context": "Courbariaux et al. (2014) used dynamic fixed point data types with 10-bit width for computation and 12-bit width for parameter updates to train a maxout convolutional network end-to-end. Their results on PI MNIST, MNIST, and CIFAR10 are about 20% worse relative to the state of the art obtained by Goodfellow et al. (2013). In our work we show that we can use 8-bit gradients for the parameter updates without degrading performance.", "startOffset": 0, "endOffset": 323}, {"referenceID": 6, "context": "Gupta et al. (2015) used a 16-bit fixed point data type for end-to-end training of convolutional neural networks and showed that stochastic rounding improves the results significantly.", "startOffset": 0, "endOffset": 20}, {"referenceID": 13, "context": "Compared to 8-bit approximation, 1-bit quantization performs well on medium sized systems that run fully connected architectures such as large fully connected networks for speech recognition (Strom, 2015; Seide et al., 2014).", "startOffset": 191, "endOffset": 224}, {"referenceID": 13, "context": "However, one problem with 1-bit quantization for large systems is the batch size which increases rapidly with the number of GPUs and slows down convergence (Seide et al., 2014; Strom, 2015; Krizhevsky, 2014).", "startOffset": 156, "endOffset": 207}, {"referenceID": 13, "context": "Another useful technique for data parallelism is 1-bit quantization which was introduced by Seide et al. (2014): In 1-bit quantization each 32-bit number of the gradient is quantized to a single bit by a quantization function.", "startOffset": 92, "endOffset": 112}, {"referenceID": 13, "context": "Another useful technique for data parallelism is 1-bit quantization which was introduced by Seide et al. (2014): In 1-bit quantization each 32-bit number of the gradient is quantized to a single bit by a quantization function. This quantization function maintains a cumulative quantization error which is used by the quantization function to smoothen out the error over time. The immediate error in quantization is too high to produce stable and accurate forward passes for model parallelism, but in data parallelism 1-bit quantization will converge to a local minimum seamlessly over time. Compared to 8-bit approximation, 1-bit quantization performs well on medium sized systems that run fully connected architectures such as large fully connected networks for speech recognition (Strom, 2015; Seide et al., 2014). For convolutional layers, 1-bit quantization has no advantage over 32 or 8 bits, as communication can be hidden under backward convolution operations even with 32-bit gradients and very large GPU clusters (see Appendix, section 5.2.2 for a worked example). Although no published example is known to us, 1-bit quantization should work flawlessly in the fully connected layers of convolutional networks. However, one problem with 1-bit quantization for large systems is the batch size which increases rapidly with the number of GPUs and slows down convergence (Seide et al., 2014; Strom, 2015; Krizhevsky, 2014). To mitigate this problem Seide et al. (2014) use adaptive batch size selection which determines the best batch size during runtime to improve convergence.", "startOffset": 92, "endOffset": 1473}], "year": 2016, "abstractText": "The creation of practical deep learning data-products often requires parallelization across processors and computers to make deep learning feasible on large data sets, but bottlenecks in communication bandwidth make it difficult to attain good speedups through parallelism. Here we develop and test 8-bit approximation algorithms which make better use of the available bandwidth by compressing 32-bit gradients and nonlinear activations to 8-bit approximations. We show that these approximations do not decrease predictive performance on MNIST, CIFAR10, and ImageNet for both model and data parallelism and provide a data transfer speedup of 2x relative to 32-bit parallelism. We build a predictive model for speedups based on our experimental data, verify its validity on known speedup data, and show that we can obtain a speedup of 50x and more on a system of 96 GPUs compared to a speedup of 23x for 32-bit. We compare our data types with other methods and show that 8-bit approximations achieve state-of-the-art speedups for model parallelism. Thus 8-bit approximation is an efficient method to parallelize convolutional networks on very large systems of GPUs.", "creator": "LaTeX with hyperref package"}}}