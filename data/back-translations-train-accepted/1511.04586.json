{"id": "1511.04586", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2015", "title": "Character-based Neural Machine Translation", "abstract": "We introduce a neural machine translation model that views the input and output sentences as sequences of characters rather than words. Since word-level information provides a crucial source of bias, our input model composes representations of character sequences into representations of words (as determined by whitespace boundaries), and then these are translated using a joint attention/translation model. In the target language, the translation is modeled as a sequence of word vectors, but each word is generated one character at a time, conditional on the previous character generations in each word. As the representation and generation of words is performed at the character level, our model is capable of interpreting and generating unseen word forms. A secondary benefit of this approach is that it alleviates much of the challenges associated with preprocessing/tokenization of the source and target languages. We show that our model can achieve translation results that are on par with conventional word-based models.", "histories": [["v1", "Sat, 14 Nov 2015 17:36:43 GMT  (201kb,D)", "http://arxiv.org/abs/1511.04586v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wang ling", "isabel trancoso", "chris dyer", "alan w black"], "accepted": true, "id": "1511.04586"}, "pdf": {"name": "1511.04586.pdf", "metadata": {"source": "CRF", "title": "CHARACTER-BASED NEURAL MACHINE TRANSLATION", "authors": ["Wang Ling", "Isabel Trancoso"], "emails": ["wlin@inesc-id.pt", "isabel.trancoso@inesc-id.pt", "cdyer@cs.cmu.edu", "awb@cs.cmu.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "In the past, efforts to perform translations at the character level (Vilar et al., 2007) or at the subword level (Neubig et al., 2013) have not yielded competitive results compared to word-based counterparts (Brown et al., 1993; Koehn et al., 2007; Chiang, 2005). The development of sequence models capable of reading and generating translations and character-based words is attractive for several reasons. Firstly, it opens the possibility for models to think about invisible source words, such as morphological variants of observed words. Secondly, it enables the production of invisible target words that effectively include translation as an open vocabulary task. Finally, we benefit from a significant reduction in the source and target word size as the only character formation that needs to be explicitly modelled."}, {"heading": "2 CHARACTER-BASED MACHINE TRANSLATION MODEL", "text": "This section describes our character-based machine translation model. As an automatic translation task, it performs the translation of a source sentence s = s0,.., sn, where si is the source word for index i into the target sentence t = t0,.., tm, where tj is the target word for index j. This can be broken down into a series of predictions where the model predicts the next target word tp, where the source sentences s and the previously generated target words t0,...., tp \u2212 1.Notation We will present vectors with bold lowercase letters (e.g. a), matrices with bold uppercase letters (e.g. A), scalar values as regular lowercase letters (e.g. a), and set as regular perceived letters (e.g. A). If we refer to whole source and target sentences, we will each use the variables s and t as initial letters. Individual source and targets will be represented by us as the OS, tj, and i respectively."}, {"heading": "2.1 JOINT ALIGNMENT AND TRANSLATION MODEL", "text": "rf\u00fc ide rf\u00fc the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf"}, {"heading": "2.2 CHARACTER-BASED MACHINE TRANSLATION", "text": "For the first time in a long time, we have been able to offer a new version of the Egg-Egg-Egg-Egg-Egg-Egg-Egg-Egg-Egg-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green-Green"}, {"heading": "2.3 TRAINING", "text": "During the training, the entire set of target words t0,.., tm is known, and we simply maximize the log probability that the word sequence p (t0 | a, SOS) +. + log p (tm | a, lfm \u2212 1) logs. Formally, we want to maximize the log probability of the training data defined as follows: raub (s, t), raub (0, m) log p (tq | a, lfm \u2212 1), where D is the amount of parallel records used as training data. In the word-based model, optimizing this function can be done objectively by maximizing the word softmax. In the character-based model, each word is factored into a series of character predictions. Specifically, we maximize the following log probability: space (s, t), raub (0, m), raub (tp, tk, tk, tk)."}, {"heading": "2.4 DECODING", "text": "In previous work (Bahdanau et al., 2015), decoding is done using a bar search. In the word-based approach, we define a stack of translation hypotheses per timestamp A, with each position being a series of translation hypotheses. With A0 = SOS, we condition each of the previous contexts t = Aj \u2212 1 for each timestamp j, and add new hypotheses for all the words obtained in the Softmax function. For example, with partial translations A1 = A, B and vocabulary T = A, B, C, then A2 would be composed of AA, AB, AC, BA, BB, BC. We define a bar cycle that defines the number of hypotheses to be expanded, with the prioritization hypothesis having the highest probability of being extended. A hypothesis is definitive once it returns the end of the OS.eWas list of the next word, giving the most likely candidate a word."}, {"heading": "2.5 LAYER-WISE TRAINING", "text": "Our character-based model defines a three-layer hierarchy. First, characters are composed into word vectors using the C2W model, and then the attention model searches for the next source word to be translated. Finally, the generation of the target word is achieved using the V2C model. In many cases, each of these layers contains several nonlinear projections, and the training will inevitably be much more complex than word-based models. In practice, this means that more epochs are required for convergence, and the model can approach a suboptimal local optimum. Furthermore, while the V2C model is generally more efficient than a word softmax, the introduction of the C2W model significantly slows down the training, as simple word table search is replaced by a compositional model. Inspired by previous work on the formation of deeper multi-layer perceptrons (Bengio et al., 2007), we generate the C2W model, which we train the C2W model by directly replacing the V2W model with the V2W output."}, {"heading": "2.6 WEAK SUPERVISION FOR ATTENTION MODEL", "text": "One problem with attention models is the fact that finding the latent attention coefficients ai requires a large number of eras (Hermann et al., 2015). Furthermore, since the attention model defined in (Bahdanau et al., 2015) does not define expertise in word alignments such as distortion, symmetry, and fertility (Brown et al., 1993), this model is likely to overmatch small amounts of data. To solve this problem, we use IBM Model 4 to create the word alignments for the low-training sentences, place a soft constraint to induce the attention model to produce alignments similar to the word alignments produced by the IBM model. More formally, given that the target word is aligned to index k in IBM Model 4, we want the model to maximize the coefficient ak. Since ak is derived from a Softmax model, this essentially means that we do not want to target a single word or focus on a single word."}, {"heading": "3 EXPERIMENTS", "text": "In this section, we present and analyze the results based on our proposed character-based model."}, {"heading": "3.1 SETUP", "text": "We test our model in two sets of data. First, we define 600k set pairs for training Europarl (Koehn, 2005), in the English-Portuguese language pair. Then we define another 500 set pairs for development and 500 set pairs for testing. We also use the English-French 20k set pairs from BTEC as an experiment on a small scale. As development and test sets, we use the CSTAR03 and IWSLT04 sets, respectively. Both languages were tokenised with Penn Tree Bank tokenizer1. As for disguising, word-based models were trained using the lost parallel data. At the test date, we used the model using the script (Koehn et al., 2007). This is common practice for word-based models, as the thrift induced by the same word in different housings (play vs play) is used for word-based models."}, {"heading": "3.2 RESULTS", "text": "For the BTEC Dataset, the word-based neural model achieves a BLEU score of 15.38 in the French to English translation direction, while the character-based model achieves a score of 15.45. At Europarl, the word-based model receives a BLEU score of 19.29, while our proposed character-based model receives a BLEU score of 19.49. While differences are not significant, this is a strong result considering that previous work (Vilar et al., 2007; Neubig et al., 2013) reveals significantly lower translation quality using character-based approaches. Word Representation To analyze the word representation achieved after the model's training, we generate the vector representation of the C2W models trained on the Europarl translation task with 600k sentence pairs for all the words in the training set. Table1 provides examples of words in English and Portuguese, and words whose representations we measure closest to them in cosmic terms."}, {"heading": "4 RELATED WORK", "text": "Our work refers to recent advances in neural machine translation models, in which a single neural network is trained to maximize the conditional probability of the target t taking into account the source s. The various models proposed define different architectures to estimate this probability, including the use of Convolutionary Architectures (Kalchburner & Blunsom, 2013), LSTM encoder decoders (Sutskever et al., 2014) and attention-based models (Bahdanau et al., 2015). However, in all of these approaches, the representation and generation of words are always performed at word level, using a word search table and softmax.In our work, we focus on defining mechanisms for performing word representation and generation at character level. Therefore, our methods are applicable to previously proposed neural MT models."}, {"heading": "5 CONCLUSION", "text": "In this paper, we presented an approach to automatic translation using characters as atomic units. Our approach uses compositional models to compose words from individual characters to learn orthographically sensing word representations. We then define a generational model for generating translated words at the character level. We show that our methods can be improved over corresponding word-based neural translation models, as our models can learn to interpret and generate invisible words. By presenting an end-to-end translation system that allows for the adoption of an open vocabulary, we leave a lot of scope for future work, as our models make very simplistic assumptions about language. Much of the previous information about morphology (Chahuneau et al., 2013b), cognates (Beinborn et al., 2013) and rare word translations (Sennrich et al., 2015) should be taken into account for better translation."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["REFERENCES Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Improved transition-based parsing by modeling characters instead of words with lstms", "author": ["Ballesteros", "Miguel", "Dyer", "Chris", "Smith", "Noah A"], "venue": "In Proc. EMNLP,", "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Cognate production using character-based machine translation", "author": ["Beinborn", "Lisa", "Zesch", "Torsten", "Gurevych", "Iryna"], "venue": "In Proceedings of the Sixth International Joint Conference on Natural Language Processing,", "citeRegEx": "Beinborn et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Beinborn et al\\.", "year": 2013}, {"title": "Greedy layer-wise training of deep networks", "author": ["Bengio", "Yoshua", "Lamblin", "Pascal", "Popovici", "Dan", "Larochelle", "Hugo"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "The mathematics of statistical machine translation: parameter estimation", "author": ["Brown", "Peter F", "Pietra", "Vincent J. Della", "Stephen A. Della", "Mercer", "Robert L"], "venue": "Comput. Linguist.,", "citeRegEx": "Brown et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Knowledge-rich morphological priors for Bayesian language models", "author": ["Chahuneau", "Victor", "Dyer", "Chris", "Smith", "Noah A"], "venue": "In Proc. NAACL,", "citeRegEx": "Chahuneau et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chahuneau et al\\.", "year": 2013}, {"title": "Translating into morphologically rich languages with synthetic phrases", "author": ["Chahuneau", "Victor", "Schlinger", "Eva", "Smith", "Noah A", "Dyer", "Chris"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Chahuneau et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chahuneau et al\\.", "year": 2013}, {"title": "Listen, attend, and spell", "author": ["Chan", "William", "Jaitly", "Navdeep", "Le", "Quoc V", "Vinyals", "Oriol"], "venue": "CoRR, abs/1508.01211,", "citeRegEx": "Chan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "A hierarchical phrase-based model for statistical machine translation", "author": ["Chiang", "David"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Chiang and David.,? \\Q2005\\E", "shortCiteRegEx": "Chiang and David.", "year": 2005}, {"title": "Producing power-law distributions and damping word frequencies with two-stage language models", "author": ["Goldwater", "Sharon", "Griffiths", "Thomas L", "Johnson", "Mark"], "venue": null, "citeRegEx": "Goldwater et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Goldwater et al\\.", "year": 2011}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["Gutmann", "Michael", "Hyvarinen", "Aapo"], "venue": null, "citeRegEx": "Gutmann et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gutmann et al\\.", "year": 2010}, {"title": "Information Retrieval: Computational and Theoretical Aspects", "author": ["Heaps", "Harold S"], "venue": null, "citeRegEx": "Heaps and S.,? \\Q1978\\E", "shortCiteRegEx": "Heaps and S.", "year": 1978}, {"title": "Teaching machines to read and comprehend", "author": ["Hermann", "Karl Moritz", "Ko\u010disk\u00fd", "Tom\u00e1\u0161", "Grefenstette", "Edward", "Espeholt", "Lasse", "Kay", "Will", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Nal", "Blunsom", "Phil"], "venue": "Seattle, October", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Character-aware neural language models", "author": ["Kim", "Yoon", "Jernite", "Yacine", "Sontag", "David", "Rush", "Alexander M"], "venue": "CoRR, abs/1508.06615,", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Europarl: A Parallel Corpus for Statistical Machine Translation", "author": ["Koehn", "Philipp"], "venue": "In Conference Proceedings: the tenth Machine Translation Summit,", "citeRegEx": "Koehn and Philipp.,? \\Q2005\\E", "shortCiteRegEx": "Koehn and Philipp.", "year": 2005}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Ling", "Wang", "Lu\u0131\u0301s", "Tiago", "Marujo", "Astudillo", "Ram\u00f3n Fernandez", "Amir", "Silvio", "Dyer", "Chris", "Black", "Alan W", "Trancoso", "Isabel"], "venue": "In Proc. EMNLP,", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Lexicon-free conversational speech recognition with neural networks", "author": ["Maas", "Andrew L", "Xie", "Ziang", "Jurafsky", "Dan", "Ng", "Andrew Y"], "venue": "In Proc. NAACL,", "citeRegEx": "Maas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2015}, {"title": "Subword language modeling with neural networks. preprint (http://www", "author": ["Mikolov", "Tom\u00e1\u0161", "Sutskever", "Ilya", "Deoras", "Anoop", "Le", "Hai-Son", "Kombrink", "Stefan", "J. Cernocky"], "venue": "fit. vutbr. cz/imikolov/rnnlm/char. pdf),", "citeRegEx": "Mikolov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "Combining word-level and character-level models for machine translation between closely-related languages. In The 50th Annual Meeting of the Association for Computational Linguistics", "author": ["Nakov", "Preslav", "Tiedemann", "J\u00f6rg"], "venue": "Proceedings of the Conference,", "citeRegEx": "Nakov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nakov et al\\.", "year": 2012}, {"title": "Substring-based machine translation", "author": ["Neubig", "Graham", "Watanabe", "Taro", "Mori", "Shinsuke", "Kawahara", "Tatsuya"], "venue": "Machine Translation,", "citeRegEx": "Neubig et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Neubig et al\\.", "year": 2013}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Papineni", "Kishore", "Roukos", "Salim", "Ward", "Todd", "Zhu", "Wei-Jing"], "venue": "In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["Santos", "Cicero D", "Zadrozny", "Bianca"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Neural machine translation of rare words with subword units", "author": ["Sennrich", "Rico", "Haddow", "Barry", "Birch", "Alexandra"], "venue": "CoRR, abs/1508.07909,", "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Generating text with recurrent neural networks", "author": ["Sutskever", "Ilya", "Martens", "James", "Hinton", "Geoffrey"], "venue": "In Proc. ICML,", "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": "CoRR, abs/1409.3215,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Can we translate letters", "author": ["Vilar", "David", "Peter", "Jan-T", "Ney", "Hermann"], "venue": "In Proceedings of the Second Workshop on Statistical Machine Translation,", "citeRegEx": "Vilar et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Vilar et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 27, "context": "In the past, efforts at performing translation at the character-level (Vilar et al., 2007) or subwordlevel (Neubig et al.", "startOffset": 70, "endOffset": 90}, {"referenceID": 21, "context": ", 2007) or subwordlevel (Neubig et al., 2013) have failed to produce competitive results compared to word-based counterparts (Brown et al.", "startOffset": 24, "endOffset": 45}, {"referenceID": 4, "context": ", 2013) have failed to produce competitive results compared to word-based counterparts (Brown et al., 1993; Koehn et al., 2007; Chiang, 2005), with the exception of closely related languages (Nakov & Tiedemann, 2012).", "startOffset": 87, "endOffset": 141}, {"referenceID": 27, "context": "We show that contrarily to previous belief (Vilar et al., 2007; Neubig et al., 2013), models that work at the character level can generate results competitive with word-based models.", "startOffset": 43, "endOffset": 84}, {"referenceID": 21, "context": "We show that contrarily to previous belief (Vilar et al., 2007; Neubig et al., 2013), models that work at the character level can generate results competitive with word-based models.", "startOffset": 43, "endOffset": 84}, {"referenceID": 4, "context": ", 2013) have failed to produce competitive results compared to word-based counterparts (Brown et al., 1993; Koehn et al., 2007; Chiang, 2005), with the exception of closely related languages (Nakov & Tiedemann, 2012). However, developing sequence to sequence models that are capable at reading and generating and generating words at the character basis is attractive for multiple reasons. Firstly, it opens the possibility for models reason about unseen source words, such as morphological variants of observed words. Secondly, it allows the production of unseen target words effectively recasting translation as an open vocabulary task. Finally, we benefit from a significant reduction of the source and target vocabulary size as only characters need to be modelled explicitly. As the number of word types increases rapidly with the size of the dataset Heaps (1978), while the number of letter types in the majority languages is fixed, character MT models can potentially solve many scalability issues in MT, both in terms of computational speed and memory requirements.", "startOffset": 88, "endOffset": 867}, {"referenceID": 0, "context": "While our model can be applied to any neural translation model , we shall adapt the attention-based translation model presented in (Bahdanau et al., 2015), which is described as follows.", "startOffset": 131, "endOffset": 154}, {"referenceID": 27, "context": "However, unlike previous approaches that attempt to discard the notion of words completely (Vilar et al., 2007; Neubig et al., 2013), we propose an hierarhical architecture, which replaces the word lookup tables (steps 1 and 3) and the word softmax (step 6) with character-based alternatives, which compose the notion of words from individual characters.", "startOffset": 91, "endOffset": 132}, {"referenceID": 21, "context": "However, unlike previous approaches that attempt to discard the notion of words completely (Vilar et al., 2007; Neubig et al., 2013), we propose an hierarhical architecture, which replaces the word lookup tables (steps 1 and 3) and the word softmax (step 6) with character-based alternatives, which compose the notion of words from individual characters.", "startOffset": 91, "endOffset": 132}, {"referenceID": 17, "context": "Character-based Word Representation The work in (Ling et al., 2015; Ballesteros et al., 2015) proposes a compositional model for learning word vectors from characters.", "startOffset": 48, "endOffset": 93}, {"referenceID": 1, "context": "Character-based Word Representation The work in (Ling et al., 2015; Ballesteros et al., 2015) proposes a compositional model for learning word vectors from characters.", "startOffset": 48, "endOffset": 93}, {"referenceID": 0, "context": "In previous work (Bahdanau et al., 2015), decoding is performed using beam search.", "startOffset": 17, "endOffset": 40}, {"referenceID": 3, "context": "Inspired by previous work on training deep multi-layered perceptrons (Bengio et al., 2007), we start by training the attention and V2C models, which are directly linked to the output of the model.", "startOffset": 69, "endOffset": 90}, {"referenceID": 12, "context": "A problem with attention models is the fact that finding the latent attention coefficients ai, requires a large number of epochs (Hermann et al., 2015).", "startOffset": 129, "endOffset": 151}, {"referenceID": 0, "context": "Furthermore, as the attention model defined in (Bahdanau et al., 2015) does not define any domain knowledge regarding word alignments, such as distortion, symmetry and fertility (Brown et al.", "startOffset": 47, "endOffset": 70}, {"referenceID": 4, "context": ", 2015) does not define any domain knowledge regarding word alignments, such as distortion, symmetry and fertility (Brown et al., 1993) this model is likely to overfit for small amounts of data.", "startOffset": 115, "endOffset": 135}, {"referenceID": 22, "context": "Finally, evaluation is performed using BLEU (Papineni et al., 2002) and we always use a single sentence as reference.", "startOffset": 44, "endOffset": 67}, {"referenceID": 22, "context": "Training is performed until there is no BLEU (Papineni et al., 2002) improvement for 5 epochs on the development set.", "startOffset": 45, "endOffset": 68}, {"referenceID": 27, "context": "While, differences are not significant, this is a strong results if we consider that previous work (Vilar et al., 2007; Neubig et al., 2013) revealed significantly lower translation quality using character-based approaches.", "startOffset": 99, "endOffset": 140}, {"referenceID": 21, "context": "While, differences are not significant, this is a strong results if we consider that previous work (Vilar et al., 2007; Neubig et al., 2013) revealed significantly lower translation quality using character-based approaches.", "startOffset": 99, "endOffset": 140}, {"referenceID": 26, "context": "These include the usage of convolutional archituctures (Kalchbrenner & Blunsom, 2013), LSTM encoder-decoders (Sutskever et al., 2014) and attention-based models (Bahdanau et al.", "startOffset": 109, "endOffset": 133}, {"referenceID": 0, "context": ", 2014) and attention-based models (Bahdanau et al., 2015).", "startOffset": 35, "endOffset": 58}, {"referenceID": 15, "context": "On the representation level, it has been shown that words can be composed using character-based approaches (Santos & Zadrozny, 2014; Kim et al., 2015; Ling et al., 2015; Ballesteros et al., 2015).", "startOffset": 107, "endOffset": 195}, {"referenceID": 17, "context": "On the representation level, it has been shown that words can be composed using character-based approaches (Santos & Zadrozny, 2014; Kim et al., 2015; Ling et al., 2015; Ballesteros et al., 2015).", "startOffset": 107, "endOffset": 195}, {"referenceID": 1, "context": "On the representation level, it has been shown that words can be composed using character-based approaches (Santos & Zadrozny, 2014; Kim et al., 2015; Ling et al., 2015; Ballesteros et al., 2015).", "startOffset": 107, "endOffset": 195}, {"referenceID": 7, "context": "For example, speech recognition models (Chan et al., 2015; Maas et al., 2015) and language modeling (Sutskever et al.", "startOffset": 39, "endOffset": 77}, {"referenceID": 18, "context": "For example, speech recognition models (Chan et al., 2015; Maas et al., 2015) and language modeling (Sutskever et al.", "startOffset": 39, "endOffset": 77}, {"referenceID": 25, "context": ", 2015) and language modeling (Sutskever et al., 2011; Mikolov et al., 2012).", "startOffset": 30, "endOffset": 76}, {"referenceID": 19, "context": ", 2015) and language modeling (Sutskever et al., 2011; Mikolov et al., 2012).", "startOffset": 30, "endOffset": 76}, {"referenceID": 9, "context": "While this work is the first neural architecture we are aware of that uses hierarchical models, a variety of Bayesian language and translation models have been proposed that use subword models to generate word types which are in turn used to generate text (Chahuneau et al., 2013a; Goldwater et al., 2011).", "startOffset": 256, "endOffset": 305}, {"referenceID": 2, "context": ", 2013b), cognates (Beinborn et al., 2013) and rare word translation (Sennrich et al.", "startOffset": 19, "endOffset": 42}, {"referenceID": 24, "context": ", 2013) and rare word translation (Sennrich et al., 2015) among others, should be incorporated for better translation.", "startOffset": 34, "endOffset": 57}], "year": 2015, "abstractText": "We introduce a neural machine translation model that views the input and output sentences as sequences of characters rather than words. Since word-level information provides a crucial source of bias, our input model composes representations of character sequences into representations of words (as determined by whitespace boundaries), and then these are translated using a joint attention/translation model. In the target language, the translation is modeled as a sequence of word vectors, but each word is generated one character at a time, conditional on the previous character generations in each word. As the representation and generation of words is performed at the character level, our model is capable of interpreting and generating unseen word forms. A secondary benefit of this approach is that it alleviates much of the challenges associated with preprocessing/tokenization of the source and target languages. We show that our model can achieve translation results that are on par with conventional word-based models.", "creator": "LaTeX with hyperref package"}}}