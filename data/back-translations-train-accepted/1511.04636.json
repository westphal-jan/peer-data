{"id": "1511.04636", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2015", "title": "Deep Reinforcement Learning with a Natural Language Action Space", "abstract": "In this paper, we propose the deep reinforcement relevance network (DRRN), a novel deep architecture, for handling an unbounded action space with applications to language understanding for text-based games. For a particular class of games, a user must choose among a variable number of actions described by text, with the goal of maximizing long-term reward. In these games, the best action is typically that which fits the best to the current situation (modeled as a state in the DRRN), also described by text. Because of the exponential complexity of natural language with respect to sentence length, there is typically an unbounded set of unique actions. Therefore, it is very difficult to pre-define the action set as in the deep Q-network (DQN). To address this challenge, the DRRN extracts high-level embedding vectors from the texts that describe states and actions, respectively, and computes the inner products between the state and action embedding vectors to approximate the Q-function. We evaluate the DRRN on two popular text games, showing superior performance over the DQN.", "histories": [["v1", "Sat, 14 Nov 2015 23:30:39 GMT  (1489kb,D)", "http://arxiv.org/abs/1511.04636v1", null], ["v2", "Thu, 19 Nov 2015 20:24:12 GMT  (1515kb,D)", "http://arxiv.org/abs/1511.04636v2", null], ["v3", "Sun, 10 Jan 2016 01:51:20 GMT  (2990kb,D)", "http://arxiv.org/abs/1511.04636v3", null], ["v4", "Sat, 16 Jan 2016 23:43:40 GMT  (2990kb,D)", "http://arxiv.org/abs/1511.04636v4", null], ["v5", "Wed, 8 Jun 2016 05:58:34 GMT  (3008kb,D)", "http://arxiv.org/abs/1511.04636v5", "accepted by ACL 2016"]], "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.LG", "authors": ["ji he", "jianshu chen", "xiaodong he", "jianfeng gao", "lihong li", "li deng", "mari ostendorf"], "accepted": true, "id": "1511.04636"}, "pdf": {"name": "1511.04636.pdf", "metadata": {"source": "CRF", "title": "DEEP REINFORCEMENT LEARNING WITH AN UNBOUNDED ACTION SPACE", "authors": ["Ji He", "Jianshu Chen", "Xiaodong He", "Jianfeng Gao", "Lihong Li", "Li Deng", "Mari Ostendorf"], "emails": ["ostendor}@uw.edu", "deng}@microsoft.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, most of them will be able to move in a direction in which they are able to move, in which they are able to move."}, {"heading": "2 RELATED WORK", "text": "The \"Deep Q Network\" (DQN) was developed and applied to Atari games (Mnih et al., 2013; Mnih et al., 2015) and it was shown that the performance of the human level is achieved by applying Convolutionary Neural Networks to the raw image pixels. A deep neural network is used as a functional approximation in a variant of Q-Learning (Watkins & Dayan, 1992), and a few techniques are introduced to ensure that the algorithm converges stable. Another stream of work focuses on continuous control with deep reinforcement of learning (Lillicrap et al., 2015), where an actor-critic algorithm operates over a continuous action space."}, {"heading": "3 DEEP REINFORCEMENT RELEVANCE NETWORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 SEQUENTIAL DECISION MAKING IN TEXT UNDERSTANDING", "text": "We look at the sequential decision problem for understanding the text in Figure 1, which we model as the Markov decision process (Q = Q = Q = MDP). At each step t, the actor receives a text sequence describing the state of st = s (i.e., \"state text\") and several strands of text describing all possible actions (i.e., \"action text\").The actor is obliged to understand the texts both from the state side and from the action side, and to measure their relevance to the current context in order to maximize the long-term reward, and then pick up the best actions (i.e., \"action text\").Then the environment will move to another state in which the action of st + 1 = s \u00b2 is performed according to the probability p (s), and the actor will receive a reward for that particular transition."}, {"heading": "3.2 DRRN ARCHITECTURE: FORWARD ACTIVATION", "text": "(.). (.). (.). (.). (.). (.). (.).). In fact, it is so that most of them are able to keep to the rules which they have given themselves. (.). (.). \"In fact, it is so that they are able to determine themselves. (.).\" (.). \"It is so that they are able to determine themselves. (.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). (.). (.). (.). (.). (.). \"(.). (.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.).). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.).). (. \"(.).\" (.).). \"(.). (.). (.).\" (. (.). \"(.).\" (.). \"(.).). (. (.).).\" (.). (. (.). (.).). (. (.).).). (. (.). (.).).). (.).). (.). (. (.). (.). (.).). (.). (.).). (.).). (.). (.). (.). (.). (.).). (.). (.). (.). (.).). (.). (.). (.).)."}, {"heading": "3.3 LEARNING THE DRRN: BACK PROPAGATION", "text": "To learn the DRRN with small random weights, we use the \"experience-replay\" strategy (Lin, 1993), which uses a fixed exploration policy to interact with the environment in order to obtain a data path. Then, we will randomly select a transition stamp (sk, ak, sk + 1) and update the model according to the following recursions: Ws = Ws, k \u2212 1 episodes of time difference error dk = rk \u00b7 Q (sk, ak) -1 episodes, k \u2212 1 episodes of action margin (sk, ak) -1 episodes of action margin (sk, ak)."}, {"heading": "4 EXPERIMENTAL RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 OVERVIEW OF TEXT GAMES", "text": "In fact, most of them will be able to play by the rules they have established in recent years, and they will be able to play by the rules they have established in the past."}, {"heading": "4.2 EXPERIMENT SETUP", "text": "The DRRN is evaluated on the basis of the two games described above. We manually comment on the final rewards for all the different endings in both games (as shown in Appendix C. The magnitude of the reward results are given to describe the polarity of good / bad endings. On the other hand, each unending step is given a small negative reward. In the text game \"Machine of Death,\" we limit ourselves to an episode that may not exceed 500 steps. In \"Saving John,\" all actions are option-based for which the assignment of text strings is at least clear. In \"Machine of Death,\" if actions are hypertext, the actions are substrings of the state. In this case, the complete state description is associated with the substrings without any surrounding context. In our examples, we use different vocabularies for the side of the state and actions."}, {"heading": "4.3 PERFORMANCE", "text": "In Figure 6, we show the learning curves of different models, with the dimension of the hidden layer in DQN and DRRN all set to 100. Error bars are achieved by running five independent experiments. As we see in Figure 6, both the proposed methods and the basic methods start at roughly the same performance (-6.9% -7.1 average rewards for Game 1 and -7.9% -8.2 average rewards for Game 2; most of the time falls into bad endings), which is the random guess policy. However, after about 3000 episodes of experience training, all methods converge much faster than the other three baselines and achieve better long-term rewards. We suspect this is because the DRRN architecture better grasps the relevance between state text and action text. Whereas the linear and DQN methods concatenate the input functions much faster than the other three baselines and achieve better long-term rewards."}, {"heading": "5 DISCUSSION AND FUTURE WORK", "text": "The proposed architecture is able to handle a variable number of actions defined by natural language in text games. Furthermore, we show that DRRN is able to achieve a faster and better solution than the basic deep Q networks by using fewer parameters. Future work will include (i) the use of a more general operation (e.g. a tensor product) to correlate state space and action space, (ii) the addition of an attention model to thoroughly analyze which part of the state / action text corresponds to strategic planning, and (iii) the application of the proposed methods to other tasks with unlimited action space."}, {"heading": "A PERCENTAGE OF CHOICE-BASED AND HYPERTEXT-BASED TEXT GAMES", "text": "As shown in Table 4."}, {"heading": "B BACK PROPAGATION FORMULA FOR LEARNING DRRN", "text": "Let us let hl, s and hl, a stand for the l-th hidden layer for the state and action-side neural networks = Q = Q = Q. For the state side, Wl, s and bl, s stand for the linear transformation weight matrix and the bias vector between the (l \u2212 1) -th and l \u2212 th hidden layers. DRRN has L hidden layers on each side. Further: h1, s = f (W1, sst + b1, s) (12) hi1, a = f (W1, aa \u2212 i + b1, a), i = 1, 3,..., | At | (13) hl, s = f (Wl \u2212 1, shl \u2212 1, s \u2212 1, s) hi1, a = W \u2212 l \u2212 h, a (14) hil, a = 2, 3,..."}, {"heading": "C DEFINE FINAL REWARDS IN THE TWO TEXT GAMES", "text": "As shown in Table 5 and Table 6."}, {"heading": "D EXAMPLES OF STATE-ACTION PAIRS IN THE TWO TEXT GAMES", "text": "As shown in Table 7 and Table 8."}], "references": [{"title": "Fundamentals of game design", "author": ["Adams", "Ernest"], "venue": "Pearson Education,", "citeRegEx": "Adams and Ernest.,? \\Q2014\\E", "shortCiteRegEx": "Adams and Ernest.", "year": 2014}, {"title": "Reinforcement learning for mapping instructions to actions", "author": ["S.R.K. Branavan", "Chen", "Harr", "Zettlemoyer", "Luke", "Barzilay", "Regina"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,", "citeRegEx": "Branavan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Branavan et al\\.", "year": 2009}, {"title": "Learning to win by reading manuals in a monte-carlo framework", "author": ["SRK Branavan", "Silver", "David", "Barzilay", "Regina"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume", "citeRegEx": "Branavan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Branavan et al\\.", "year": 2011}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Process. Mag.,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Recurrent Reinforcement Learning: A Hybrid Approach", "author": ["X. Li", "L. Li", "J. Gao", "X. He", "J. Chen", "L. Deng", "J. He"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Continuous control with deep reinforcement learning", "author": ["Lillicrap", "Timothy P", "Hunt", "Jonathan J", "Pritzel", "Alexander", "Heess", "Nicolas", "Erez", "Tom", "Tassa", "Yuval", "Silver", "David", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Reinforcement learning for robots using neural networks", "author": ["Lin", "Long-Ji"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Lin and Long.Ji.,? \\Q1993\\E", "shortCiteRegEx": "Lin and Long.Ji.", "year": 1993}, {"title": "Playing Atari with Deep Reinforcement Learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Humanlevel control through deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Rusu", "Andrei A", "Veness", "Joel", "Bellemare", "Marc G", "Graves", "Alex", "Riedmiller", "Martin", "Fidjeland", "Andreas K", "Ostrovski", "Georg"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Language understanding for text-based games using deep reinforcement learning", "author": ["Narasimhan", "Karthik", "Kulkarni", "Tejas", "Barzilay", "Regina"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Narasimhan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "Dynamic programming", "author": ["Richard", "Bellman"], "venue": null, "citeRegEx": "Richard and Bellman.,? \\Q1957\\E", "shortCiteRegEx": "Richard and Bellman.", "year": 1957}, {"title": "Automatic learning of dialogue strategy using dialogue simulation and reinforcement learning", "author": ["Scheffler", "Konrad", "Young", "Steve"], "venue": "In Proceedings of the second international conference on Human Language Technology Research,", "citeRegEx": "Scheffler et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Scheffler et al\\.", "year": 2002}, {"title": "Reinforcement learning for spoken dialogue systems", "author": ["Singh", "Satinder P", "Kearns", "Michael J", "Litman", "Diane J", "Walker", "Marilyn A"], "venue": "In Nips, pp", "citeRegEx": "Singh et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Singh et al\\.", "year": 1999}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": "MIT press Cambridge,", "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}], "referenceMentions": [{"referenceID": 9, "context": "performance in several benchmarks of Atari games (Mnih et al., 2015).", "startOffset": 49, "endOffset": 68}, {"referenceID": 8, "context": "The \u201cDeep Q Network\u201d (DQN) was developed and applied to Atari games (Mnih et al., 2013; Mnih et al., 2015) and was shown to achieve human level performance by applying convolutional neural networks to the raw image pixels.", "startOffset": 68, "endOffset": 106}, {"referenceID": 9, "context": "The \u201cDeep Q Network\u201d (DQN) was developed and applied to Atari games (Mnih et al., 2013; Mnih et al., 2015) and was shown to achieve human level performance by applying convolutional neural networks to the raw image pixels.", "startOffset": 68, "endOffset": 106}, {"referenceID": 6, "context": "Another stream of work focuses on continuous control with deep reinforcement learning (Lillicrap et al., 2015), where an actor-critic algorithm operates over continuous action space.", "startOffset": 86, "endOffset": 110}, {"referenceID": 13, "context": "In language processing, reinforcement learning has been applied to a dialogue management system that converses with a human user by taking actions that generate natural language (Scheffler & Young, 2002; Singh et al., 1999).", "startOffset": 178, "endOffset": 223}, {"referenceID": 2, "context": "There has also been interest in extracting textual knowledge to improve game control performance (Branavan et al., 2011), and mapping natural language instructions to sequences of executable actions (Branavan et al.", "startOffset": 97, "endOffset": 120}, {"referenceID": 1, "context": ", 2011), and mapping natural language instructions to sequences of executable actions (Branavan et al., 2009).", "startOffset": 86, "endOffset": 109}, {"referenceID": 10, "context": "Due to the potentially infinite input space, modeling parser-based text games requires restrictions on player input (Narasimhan et al., 2015), such as fixed command structures (one action and one argument object), and limited action-side vocabulary size.", "startOffset": 116, "endOffset": 141}, {"referenceID": 3, "context": "Li et al. (2015) developed a joint training approach for recurrent reinforcement learning and demonstrate its effectiveness on a customer relationship management task.", "startOffset": 0, "endOffset": 17}, {"referenceID": 1, "context": "There has also been interest in extracting textual knowledge to improve game control performance (Branavan et al., 2011), and mapping natural language instructions to sequences of executable actions (Branavan et al., 2009). Narasimhan et al. (2015) applied an LSTM (long short term memory) -DQN framework to the task of learning control policies for parser-based text games, which achieves higher average reward than the random and BOW (bag-of-words) -DQN baselines.", "startOffset": 98, "endOffset": 249}, {"referenceID": 3, "context": "The DNN has been shown to have high capacity and scalability in function approximation, which leads to state-of-the-art performance in many machine learning tasks (Hinton et al., 2012; Krizhevsky et al., 2012).", "startOffset": 163, "endOffset": 209}, {"referenceID": 4, "context": "The DNN has been shown to have high capacity and scalability in function approximation, which leads to state-of-the-art performance in many machine learning tasks (Hinton et al., 2012; Krizhevsky et al., 2012).", "startOffset": 163, "endOffset": 209}, {"referenceID": 9, "context": "scrambles the trajectory from experience replay into a \u201cbag-of-transitions\u201d, which has been shown to avoid oscillations or divergence and achieve faster convergence in the parameters in Q-learning (Mnih et al., 2015).", "startOffset": 197, "endOffset": 216}, {"referenceID": 10, "context": "Previously for parser-based text games, Narasimhan et al. (2015) have defined a fixed set of 222 actions, which is the total number of possible phrases the parser accepts.", "startOffset": 40, "endOffset": 65}], "year": 2017, "abstractText": "In this paper, we propose the deep reinforcement relevance network (DRRN), a novel deep architecture, for handling an unbounded action space with applications to language understanding for text-based games. For a particular class of games, a user must choose among a variable number of actions described by text, with the goal of maximizing long-term reward. In these games, the best action is typically that which fits the best to the current situation (modeled as a state in the DRRN), also described by text. Because of the exponential complexity of natural language with respect to sentence length, there is typically an unbounded set of unique actions. Therefore, it is very difficult to pre-define the action set as in the deep Q-network (DQN). To address this challenge, the DRRN extracts high-level embedding vectors from the texts that describe states and actions, respectively, and computes the inner products between the state and action embedding vectors to approximate the Q-function. We evaluate the DRRN on two popular text games, showing superior performance over the DQN.", "creator": "LaTeX with hyperref package"}}}