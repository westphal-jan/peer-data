{"id": "1511.04646", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2015", "title": "Word Embedding Based Correlation Model for Question/Answer Matching", "abstract": "With the development of community based question answering (Q\\&amp;A) services, a large scale of Q\\&amp;A archives have been accumulated and are an important information and knowledge resource on the web. Question and answer matching has been attached much importance to for its ability to reuse knowledge stored in these systems: it can be useful in enhancing user experience with recurrent questions. In this paper, we try to improve the matching accuracy by overcoming the lexical gap between question and answer pairs. A Word Embedding based Correlation (WEC) model is proposed by integrating advantages of both the translation model and word embedding, given a random pair of words, WEC can score their co-occurrence probability in Q\\&amp;A pairs and it can also leverage the continuity and smoothness of continuous space word representation to deal with new pairs of words that are rare in the training parallel text. An experimental study on Yahoo! Answers dataset and Baidu Zhidao dataset shows this new method's promising potential.", "histories": [["v1", "Sun, 15 Nov 2015 02:59:22 GMT  (301kb)", "https://arxiv.org/abs/1511.04646v1", "8 pages, 2 figures"], ["v2", "Sat, 26 Nov 2016 02:40:12 GMT  (299kb,D)", "http://arxiv.org/abs/1511.04646v2", "8 pages, 2 figures"]], "COMMENTS": "8 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["yikang shen", "wenge rong", "nan jiang", "baolin peng", "jie tang", "zhang xiong"], "accepted": true, "id": "1511.04646"}, "pdf": {"name": "1511.04646.pdf", "metadata": {"source": "META", "title": "Word Embedding Based Correlation Model for Question/Answer Matching", "authors": ["Yikang Shen", "Wenge Rong", "Nan Jiang", "Baolin Peng", "Jie Tang", "Zhang Xiong"], "emails": ["xiongz}@buaa.edu.cn,", "blpeng@se.cuhk.edu.hk,", "jietang@tsinghua.edu.cn"], "sections": [{"heading": "Introduction", "text": "Over the past decade, websites such as Yahoo! Answers, Baidu Zhidao, Quora, and Zhihu have amassed a large Q & A archive, usually organized as a question with a list of candidate answers and associated with metadata, including custom subject categories, answer sequences, and selected correct answers (Zhou et al. 2015).This user-generated content is an important information repository on the Web and makes Q & A archives invaluable resources for various tasks such as answering questions (Jeon et al. 2005; Xue et al. 2008; Nakov et al. 2015) and knowledge mining (Adamic et al al al al al al al al al al al. 2008).Better use of information stored in CQA systems is a fundamental task to align potential answers to the question correctly."}, {"heading": "Methodology", "text": "Problem definition Considering a question q = q1... qn, where Qi is the i-th word in the question, and a series of candidate answers A = {a1, a2,..., an}, where aj = aj1... ajm and a j k is the k-th word in the j-th candidate answer, the goal is to identify the most relevant answer in the best possible way.To solve this problem, we calculate the matching probability between q and each answer ai and then classify the candidates based on their matching probabilities, which are calculated in three steps: 1) Words in questions and answers are represented by vectors in a continuous space; 2) word-to-word correlation value is calculated using a word-correlation function; 3) Question-A-matching probability is achieved by using a phrase-level correlation function."}, {"heading": "Word Embedding", "text": "To correctly represent words in a continuous space, the idea of a neural language model (Bengio et al. 2003) is used to jointly learn the embedding of words in an n-dimensional vector space and to use these vectors to predict how likely a word is to get its context. Skip-gram model (Mikolov et al. 2013a) is a widely used approach to calculating such an embedding. When Skip-gram networks are optimized by gradient ascent, the derivatives modify the word embedding statistics of the matrixL-R (n \u00d7 | V |), where | V | is the size of the vocabulary. Once the word vectors within the embedding matrix capture syntactical and semantic information about the word coevent statistics (Bengio et al. 2003; Mikolov et al. 2013a). Once this matrix is on an unlabeled corpus, it can be replicated by using a spelling for each of the following tasks."}, {"heading": "Word Embedding based Correlation (WEC) Model", "text": "To achieve this goal, we use a translation matrix M to turn words in the answer into words in the question. Faced with a word pair (qi, aj), their WEC scoring function is defined as: C (qi, aj) = cos < vqi, Mvaj > = vTqi | | Mvaj | | (1), where vqi and vaj represent Qi and the d-dimensional word embedding vector; | \u00b7 | is a Euclidean norm; correlation matrix M, Rd \u00d7 d. M is called a translation matrix because it maps the word in response to a possible correlated word in the question."}, {"heading": "WEC + Convolution Neural Networks (CNN)", "text": "WEC is based on the sack-of-word matrix matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-"}, {"heading": "Experimental Study", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Dataset", "text": "To evaluate the proposed WEC model, two sets of data are used in this study, Yahoo! Answer and Baidu Zhidao. Yahoo! Answers \"data set is available on Yahoo! Webscope1, including a large number of questions and the corresponding answers. In addition, the corpus contains a small amount of metadata, such as which answer was chosen as the best answer, and the category and subcategory associated with that question (Surdeanu et al. 2008). To validate the proposed WEC model, we generate three different subsets of data. As shown in Table 1, the first subset contains 53,261 questions categorized as via travel, the second subset contains 57,576 questions categorized as via relationships, and the third subset contains 66,129 questions categorized as via finance. Since the CNN model must limit the maximum length of the questions and answers, all the questions selected are no longer than 50 words, and the answers no longer than 100 words."}, {"heading": "Experimental Settings", "text": "Evaluation Metrics We use the same evaluation method used by (Lu and Li 2013; Shen et al. 2015) to evaluate the accuracy of matching questions and answers. A set of candidate answers is created with size 6 (one positive + five negative) for each question in the test data. We compare the performance of our approach in ranking quality of the six candidate answers against that of other baselines. Discounted cumulative gain (DCG) and Inen 2000) is used to evaluate the ranking quality. The premise of DCG is that highly relevant documents appearing lower in a ranking list should be punished as the graded relevance value is reduced logarithmically proportional to the position of the result."}, {"heading": "Experiment Results", "text": "Table 2 shows the Q & A matching performance of WEC-based methods, translation probability-based methods and traditional query methods in the Yahoo! Answer dataset. In terms of accuracy of candidate responses, WEC easily outperforms the translation probability models. WEC outperforms TRLM and TM in candidate evaluation qualities. By adding CNN to the model, WEC + CNN outperforms all other models. It is possible to interpret that the WEC model performs better than TRLM and TM models, but the mere use of lexical information limits its ability to select the best answer. Therefore, WEC + CNN can improve the result by inserting syntactical information into the model.Table 3 shows the Q & A matching performance of different approaches to Baidu Zhidao dataset. We find that WEC and WEC + CNN outperform all other models. In addition, IBM + CNN are able to improve the result by adding Zhidao to the data set, possibly due to overtactical information in the data set."}, {"heading": "Examples", "text": "To understand that, we need a number of examples of answering questions in connection with answering questions in connection with answering questions in connection with answering questions in connection with answering questions in connection with answering questions in connection with answering questions in connection with answering questions in connection with answering questions in connection with answering questions in connection with answering questions in connection with answering questions in connection with answering questions in connection with answering questions in connection with answering questions in connection with answering questions in connection with answering questions in connection with answering questions in connection with answering questions in connection with answering questions in connection with answering questions in connection with answering questions in connection with answering questions in connection with answering questions in connection with answering answers in connection with answering answers in connection with answering questions in connection with answering in connection with answering questions in connection with answering in connection with answering questions in connection with answering questions in connection with answering questions in connection with answering questions in connection with answering questions in connection with answering questions in connection with answering questions in connection with answering questions in connection with answering questions in connection with answering questions in connection with answering questions in connection with answering questions in connection with answering questions in connection with answering questions in connection with answering questions in connection with answering in connection with answering in connection with answering questions in connection with answering questions in connection with answering in connection with answering in connection with answering in connection with answering questions in connection with answering in connection with answering questions in connection with answering in connection with answering in connection with answering in connection with answering in connection with answering in connection with answering in connection with answering in connection with answering questions in connection with answering in connection with answering in connection with answering in connection with answering in connection with answering in connection with answering in connection with answering in connection with answering in connection with answering in connection with answering in connection with answering in connection with answering in connection with answering in connection with answering in connection with answering in connection with answering in connection with answering in connection with answering in connection with answering in connection with answering in connection with answering in connection with answering in connection with answering in connection with answering in connection with answering in connection with answering in connection with answering in connection with ans"}, {"heading": "Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Lexical Gap Problem in CQA", "text": "In order to make full use of Q & A archives in CQA systems, there are two important tasks for a newly submitted question, including a question query that focuses on matching new questions with archived questions in CQA systems (Jeon et al. 2005; Xue et al. 2008; Cao et al. 2010; Cai et al. 2011; Zhou et al. 2015) and finding answers that focus on finding a potentially appropriate answer within a collection of candidate answers (Berger et al. 2000; Surdeanu et al. 2008; Lu and Li 2013; Shen et al. 2015). A major challenge in both tasks is the lexical gap problem (Chen et al. 2016). Potential solutions to overcome this difficulty include 1) query expansion, 2) statistical translation, 3) latent variable models (Berger et al. 2000). Much importance has been attached to statistical translation."}, {"heading": "Translation Matrix", "text": "Its main advantage is that the representations of similar words in vector space are close together, which facilitates the generalization of new patterns and makes the estimation of the model more robust. Successful follow-up work includes the application to statistical language modeling (Bengio et al. 2003; Mikolov et al. 2013a). Inspired by the vector representation of words, it was proposed to map the vector representation x from one language area to another using cosmic similarity as distance metric (Mikolov et al. 2013b). Our word-based WEC model uses the same translation functions to challenge vector representation from one semantic space. Furthermore, we propose a WEC sentence model to calculate the Q & A comparability (Mikolov et al. 2013b)."}, {"heading": "Conclusion and Future Work", "text": "To address the lexical gap between question and answer, a word-embedding-based correlation (WEC) model is proposed, in which the co-occurrence relationship between words in parallel text is represented as a matrix (or set of matrices); given a random word pair, the WEC model can evaluate its co-occurrence probability in Q & A pairs, like the previous translation model-based approach; and it also uses the continuity and suppleness of continuous space-word representation to deal with new word pairs that are rare in the training parallel text. Our experiments show that WEC and WEC + CNN are superior to the state of the art. There are several interesting directions that require further research in the future. It is possible to apply this model in question-question-question matching or multilingual question-retrieval tasks."}, {"heading": "Acknowledgment", "text": "This work was partially supported by the National Natural Science Foundation of China (No. 61332018), the National Department Public Benefit Research Foundation of China (No. 201510209) and the Fundamental Research Funds for the Central Universities."}], "references": [{"title": "Knowledge sharing and yahoo answers: everyone knows something", "author": ["Lada A. Adamic", "Jun Zhang", "Eytan Bakshy", "Mark S. Ackerman"], "venue": "In Proc. WWW,", "citeRegEx": "Adamic et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Adamic et al\\.", "year": 2008}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Bridging the lexical chasm: Statistical approaches to answer-finding", "author": ["Adam Berger", "Rich Caruana", "David Cohn", "Dayne Freitag", "Vibhu Mittal"], "venue": "In Proc. SIGIR,", "citeRegEx": "Berger et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Berger et al\\.", "year": 2000}, {"title": "Learning the latent topics for question retrieval in community QA", "author": ["Li Cai", "Guangyou Zhou", "Kang Liu", "Jun Zhao"], "venue": "In Proc. IJCNLP,", "citeRegEx": "Cai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2011}, {"title": "A generalized framework of exploring category information for question retrieval in community question answer archives", "author": ["Xin Cao", "Gao Cong", "Bin Cui", "Christian S. Jensen"], "venue": "In Proc. WWW,", "citeRegEx": "Cao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2010}, {"title": "A semantic graph based topic model for question retrieval in community question answering", "author": ["Long Chen", "Joemon M. Jose", "Haitao Yu", "Fajie Yuan", "Dell Zhang"], "venue": "In Proc. ICDM,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Machine translation evaluation meets community question answering", "author": ["Francisco Guzm\u00e1n", "Llu\u0131\u0301s M\u00e0rquez", "Preslav Nakov"], "venue": "In Proc. ACL,", "citeRegEx": "Guzm\u00e1n et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Guzm\u00e1n et al\\.", "year": 2016}, {"title": "Multi-perspective sentence similarity modeling with convolutional neural networks", "author": ["Hua He", "Kevin Gimpel", "Jimmy Lin"], "venue": "In Proc. EMNLP,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Finding similar questions in large question and answer archives", "author": ["Jiwoon Jeon", "W. Bruce Croft", "Joon Ho Lee"], "venue": "In Proc. CIKM,", "citeRegEx": "Jeon et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Jeon et al\\.", "year": 2005}, {"title": "Questionanswer topic model for question retrieval in community question answering", "author": ["Zongcheng Ji", "Fei Xu", "Bin Wang", "Ben He"], "venue": "In Proc. CIKM,", "citeRegEx": "Ji et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2012}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom"], "venue": "In Proc. ACL,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "A deep architecture for matching short texts", "author": ["Zhengdong Lu", "Hang Li"], "venue": "In Proc. NIPS,", "citeRegEx": "Lu and Li.,? \\Q2013\\E", "shortCiteRegEx": "Lu and Li.", "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "CoRR, abs/1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Exploiting similarities among languages for machine translation", "author": ["Tomas Mikolov", "Quoc V. Le", "Ilya Sutskever"], "venue": "CoRR, abs/1309.4168,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Convolutional neural networks over tree structures for programming language processing", "author": ["Lili Mou", "Ge Li", "Lu Zhang", "Tao Wang", "Zhi Jin"], "venue": "In Proc. AAAI,", "citeRegEx": "Mou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2016}, {"title": "Semeval-2015 task 3: Answer selection in community question answering", "author": ["Preslav Nakov", "Llu\u0131\u0301s M\u00e0rquez", "Walid Magdy", "Alessandro Moschitti", "James Glass", "Bilal Randeree"], "venue": "In Proc. SemEval,", "citeRegEx": "Nakov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nakov et al\\.", "year": 2015}, {"title": "SemEval-2016 task 3: Community question answering", "author": ["Preslav Nakov", "Llu\u0131\u0301s M\u00e0rquez", "Alessandro Moschitti", "Walid Magdy", "Hamdy Mubarak", "Abed Alhakim Freihat", "James Glass", "Bilal Randeree"], "venue": "In Proc. SemEval,", "citeRegEx": "Nakov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nakov et al\\.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "In Proc. EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Convolutional neural tensor network architecture for community-based question answering", "author": ["Xipeng Qiu", "Xuanjing Huang"], "venue": "In Proc. IJCAI,", "citeRegEx": "Qiu and Huang.,? \\Q2015\\E", "shortCiteRegEx": "Qiu and Huang.", "year": 2015}, {"title": "Inen. IR evaluation methods for retrieving highly relevant documents", "author": ["Kalervo J.A. Rvelin", "Jaana Kek A. L"], "venue": "In Proc. SIGIR,", "citeRegEx": "Rvelin and L.,? \\Q2000\\E", "shortCiteRegEx": "Rvelin and L.", "year": 2000}, {"title": "Question/answer matching for CQA system via combining lexical and sequential information", "author": ["Yikang Shen", "Wenge Rong", "Zhiwei Sun", "Yuanxin Ouyang", "Zhang Xiong"], "venue": "In Proc. AAAI,", "citeRegEx": "Shen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2015}, {"title": "Learning from the past: answering new questions with past answers", "author": ["Anna Shtok", "Gideon Dror", "Yoelle Maarek", "Idan Szpektor"], "venue": "In Proc. WWW,", "citeRegEx": "Shtok et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Shtok et al\\.", "year": 2012}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning"], "venue": "In Proc. EMNLP,", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D. Manning", "Andrew Ng"], "venue": "In Proc. NIPS,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Learning to rank answers on large online QA collections", "author": ["Mihai Surdeanu", "Massimiliano Ciaramita", "Hugo Zaragoza"], "venue": "In Proc. ACL,", "citeRegEx": "Surdeanu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2008}, {"title": "Retrieval models for question and answer archives", "author": ["Xiaobing Xue", "Jiwoon Jeon", "W. Bruce Croft"], "venue": "In Proc. SIGIR,", "citeRegEx": "Xue et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2008}, {"title": "Semantic parsing for single-relation question answering", "author": ["Wen-tau Yih", "Xiaodong He", "Christopher Meek"], "venue": "In Proc. ACL,", "citeRegEx": "Yih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2014}, {"title": "A classification-based approach to question routing in community question answering", "author": ["Tom Chao Zhou", "Michael R. Lyu", "Irwin King"], "venue": "In Proc. WWW,", "citeRegEx": "Zhou et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2012}, {"title": "Learning continuous word embedding with metadata for question retrieval in community question answering", "author": ["Guangyou Zhou", "Tingting He", "Jun Zhao", "Po Hu"], "venue": "In Proc. ACL,", "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}, {"title": "Bilingual word embeddings for phrasebased machine translation", "author": ["Will Y. Zou", "Richard Socher", "Daniel M. Cer", "Christopher D. Manning"], "venue": "In Proc. EMNLP,", "citeRegEx": "Zou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 29, "context": "Over the last decade, websites, such as Yahoo! Answers, Baidu Zhidao, Quora, and Zhihu, have accumulated large scale question and answer (Q&A) archives, which are usually organised as a question with a list of candidate answers and associated with metadata including user tagged subject categories, answer popularity votes, and selected correct answer (Zhou et al. 2015).", "startOffset": 352, "endOffset": 370}, {"referenceID": 8, "context": "This user-generated content is an important information repository on the web and makes Q&A archives invaluable resources for various tasks such as question-answering (Jeon et al. 2005; Xue et al. 2008; Nakov et al. 2015; 2016) and knowledge mining (Adamic et al.", "startOffset": 167, "endOffset": 227}, {"referenceID": 26, "context": "This user-generated content is an important information repository on the web and makes Q&A archives invaluable resources for various tasks such as question-answering (Jeon et al. 2005; Xue et al. 2008; Nakov et al. 2015; 2016) and knowledge mining (Adamic et al.", "startOffset": 167, "endOffset": 227}, {"referenceID": 16, "context": "This user-generated content is an important information repository on the web and makes Q&A archives invaluable resources for various tasks such as question-answering (Jeon et al. 2005; Xue et al. 2008; Nakov et al. 2015; 2016) and knowledge mining (Adamic et al.", "startOffset": 167, "endOffset": 227}, {"referenceID": 0, "context": "2015; 2016) and knowledge mining (Adamic et al. 2008).", "startOffset": 33, "endOffset": 53}, {"referenceID": 22, "context": "To make better use of information stored in CQA systems, a fundamental task is to properly matching potential candidate answers to the question, since many questions recur enough to allow for at least a few new questions to be answered by past materials (Shtok et al. 2012).", "startOffset": 254, "endOffset": 273}, {"referenceID": 2, "context": "There are several challenges for this task among which the lexical gap or lexical chasm between the question and candidate answers is a difficult one (Berger et al. 2000).", "startOffset": 150, "endOffset": 170}, {"referenceID": 27, "context": "\u201d This Q&A pair share no more than 4 words in common, including \u201cthe\u201d and \u201cis\u201d, but they are strongly associated by synonyms, hyponyms, or other weaker semantic associations (Yih et al. 2014).", "startOffset": 174, "endOffset": 191}, {"referenceID": 8, "context": "A possible approach for the lexical gap problem is to employ translation model, which will leverage the Q&A pairs to learn the semantically related words (Jeon et al. 2005; Xue et al. 2008; Zhou et al. 2012; Guzm\u00e1n et al. 2016).", "startOffset": 154, "endOffset": 227}, {"referenceID": 26, "context": "A possible approach for the lexical gap problem is to employ translation model, which will leverage the Q&A pairs to learn the semantically related words (Jeon et al. 2005; Xue et al. 2008; Zhou et al. 2012; Guzm\u00e1n et al. 2016).", "startOffset": 154, "endOffset": 227}, {"referenceID": 28, "context": "A possible approach for the lexical gap problem is to employ translation model, which will leverage the Q&A pairs to learn the semantically related words (Jeon et al. 2005; Xue et al. 2008; Zhou et al. 2012; Guzm\u00e1n et al. 2016).", "startOffset": 154, "endOffset": 227}, {"referenceID": 6, "context": "A possible approach for the lexical gap problem is to employ translation model, which will leverage the Q&A pairs to learn the semantically related words (Jeon et al. 2005; Xue et al. 2008; Zhou et al. 2012; Guzm\u00e1n et al. 2016).", "startOffset": 154, "endOffset": 227}, {"referenceID": 1, "context": "In spite of its wide use in many natural language processing tasks, discrete space representation has two majors disadvantages: 1) the curse of dimensionality (Bengio et al. 2003), for a natural language with a vocabulary V of size N , we need to learn at most N word-to-word translation probabilities; 2) the generalisation structure is not obvious: it is difficult to estimate the probability of exact word if they are rare in the training parallel text (Zou et al.", "startOffset": 159, "endOffset": 179}, {"referenceID": 30, "context": "2003), for a natural language with a vocabulary V of size N , we need to learn at most N word-to-word translation probabilities; 2) the generalisation structure is not obvious: it is difficult to estimate the probability of exact word if they are rare in the training parallel text (Zou et al. 2013).", "startOffset": 282, "endOffset": 299}, {"referenceID": 3, "context": "Some work proposed to learn the latent topics aligned across the question-answer pairs to bridge the lexical gap, with the assumption that a question and its answer should share similar topic distribution (Cai et al. 2011; Ji et al. 2012).", "startOffset": 205, "endOffset": 238}, {"referenceID": 9, "context": "Some work proposed to learn the latent topics aligned across the question-answer pairs to bridge the lexical gap, with the assumption that a question and its answer should share similar topic distribution (Cai et al. 2011; Ji et al. 2012).", "startOffset": 205, "endOffset": 238}, {"referenceID": 21, "context": "Recently, inspired by the success of word embedding, some papers propose to leverage the advantage of the vector representation of words to overcome the lexical gap (Shen et al. 2015; Zhou et al. 2015) by using similarity of word vector to represent the word-to-word relation.", "startOffset": 165, "endOffset": 201}, {"referenceID": 29, "context": "Recently, inspired by the success of word embedding, some papers propose to leverage the advantage of the vector representation of words to overcome the lexical gap (Shen et al. 2015; Zhou et al. 2015) by using similarity of word vector to represent the word-to-word relation.", "startOffset": 165, "endOffset": 201}, {"referenceID": 1, "context": "Because local smoothness properties of continuous space word representations, generalisation can be obtain more easily (Bengio et al. 2003).", "startOffset": 119, "endOffset": 139}, {"referenceID": 29, "context": "However question and answers are heterogeneous in many aspects, semantic similarities can be weak between questions and answers (Zhou et al. 2015).", "startOffset": 128, "endOffset": 146}, {"referenceID": 8, "context": "and semantic model, in this paper we propose a Word Embedding Correlation (WEC) model, which integrates the advantages of both the translation model (Jeon et al. 2005; Xue et al. 2008) and word embedding (Bengio et al.", "startOffset": 149, "endOffset": 184}, {"referenceID": 26, "context": "and semantic model, in this paper we propose a Word Embedding Correlation (WEC) model, which integrates the advantages of both the translation model (Jeon et al. 2005; Xue et al. 2008) and word embedding (Bengio et al.", "startOffset": 149, "endOffset": 184}, {"referenceID": 1, "context": "2008) and word embedding (Bengio et al. 2003; Mikolov et al. 2013a; Pennington et al. 2014).", "startOffset": 25, "endOffset": 91}, {"referenceID": 18, "context": "2008) and word embedding (Bengio et al. 2003; Mikolov et al. 2013a; Pennington et al. 2014).", "startOffset": 25, "endOffset": 91}, {"referenceID": 1, "context": "If co-occurrences of exact words are rare in the training parallel text, C(qi, aj) can also estimate their correlations strength because of the local smoothness properties of continuous space word representations (Bengio et al. 2003).", "startOffset": 213, "endOffset": 233}, {"referenceID": 11, "context": "Furthermore, we combine our model with convolution neural network (CNN) (LeCun et al. 1998; Shen et al. 2015) to integrate both lexical and syntactical information stored in Q&A to estimate the matching probability.", "startOffset": 72, "endOffset": 109}, {"referenceID": 21, "context": "Furthermore, we combine our model with convolution neural network (CNN) (LeCun et al. 1998; Shen et al. 2015) to integrate both lexical and syntactical information stored in Q&A to estimate the matching probability.", "startOffset": 72, "endOffset": 109}, {"referenceID": 1, "context": "In order to properly represent words in a continuous space, the idea of a neural language model (Bengio et al. 2003) is employed to enable jointly learn embedding of words into an n-dimensional vector space and to use these vectors to predict how likely a word is given its context.", "startOffset": 96, "endOffset": 116}, {"referenceID": 1, "context": "The word vectors inside the embedding matrix capture distributional syntactic and semantic information via the word co-occurrence statistics (Bengio et al. 2003; Mikolov et al. 2013a).", "startOffset": 141, "endOffset": 183}, {"referenceID": 23, "context": "same bag-of-words representation, their real meaning could be completely opposite (Socher et al. 2011).", "startOffset": 82, "endOffset": 102}, {"referenceID": 7, "context": "To overcome this limitation, several approaches have been proposed and one possible solution is to use the convolution neural network (CNN) model (He et al. 2015; Mou et al. 2016).", "startOffset": 146, "endOffset": 179}, {"referenceID": 15, "context": "To overcome this limitation, several approaches have been proposed and one possible solution is to use the convolution neural network (CNN) model (He et al. 2015; Mou et al. 2016).", "startOffset": 146, "endOffset": 179}, {"referenceID": 10, "context": "(Kalchbrenner et al. 2014) proposed that the convolutional and dynamic pooling layer in CNN can relate phrases far apart in the input sentence.", "startOffset": 0, "endOffset": 26}, {"referenceID": 21, "context": "In (Shen et al. 2015), S+CNN model is proposed for Q&A matching to integrate both syntactical and lexical information to estimate the matching probability.", "startOffset": 3, "endOffset": 21}, {"referenceID": 11, "context": "Then the similarity matrix is used as an input of a CNN instead of an image in (LeCun et al. 1998), the output of the CNN is the matching score of the Q&A pair.", "startOffset": 79, "endOffset": 98}, {"referenceID": 25, "context": "In addition, the corpus contains a small amount of meta data, such as, which answer was selected as the best answer, and the category and sub-category assigned to this question (Surdeanu et al. 2008).", "startOffset": 177, "endOffset": 199}, {"referenceID": 21, "context": "The Baidu Zhidao dataset is provided in (Shen et al. 2015), and contains 99,909 questions and their best answers.", "startOffset": 40, "endOffset": 58}, {"referenceID": 21, "context": "Evaluation Metrics We use the same evaluation method employed by (Lu and Li 2013; Shen et al. 2015) to evaluate the accuracy of matching questions and answers.", "startOffset": 65, "endOffset": 99}, {"referenceID": 8, "context": "Baseline We compare WEC model against Translation model (TM) (Jeon et al. 2005), Translation based language model (TRLM) (Xue et al.", "startOffset": 61, "endOffset": 79}, {"referenceID": 26, "context": "2005), Translation based language model (TRLM) (Xue et al. 2008), Okapi model (Jeon et al.", "startOffset": 47, "endOffset": 64}, {"referenceID": 8, "context": "2008), Okapi model (Jeon et al. 2005) and Language model (LM) (Jeon et al.", "startOffset": 19, "endOffset": 37}, {"referenceID": 8, "context": "2005) and Language model (LM) (Jeon et al. 2005).", "startOffset": 30, "endOffset": 48}, {"referenceID": 8, "context": "Given a question q and answer a, TM (Jeon et al. 2005) can be define as:", "startOffset": 36, "endOffset": 54}, {"referenceID": 26, "context": "TRLM (Xue et al. 2008) can be define as:", "startOffset": 5, "endOffset": 22}, {"referenceID": 21, "context": "In (Shen et al. 2015), word vector cosine similarity is used as word translation probabilities in TM and TRLM.", "startOffset": 3, "endOffset": 21}, {"referenceID": 8, "context": "To fully use Q&A archives in CQA systems, there are two important tasks for a newly submitted question, including question retrieval, which focuses on matching new questions with archived questions in CQA systems (Jeon et al. 2005; Xue et al. 2008; Cao et al. 2010; Cai et al. 2011; Zhou et al. 2015) and answer locating, which focuses finding a potentially suitable answer within a collection of candidate answers (Berger et al.", "startOffset": 213, "endOffset": 300}, {"referenceID": 26, "context": "To fully use Q&A archives in CQA systems, there are two important tasks for a newly submitted question, including question retrieval, which focuses on matching new questions with archived questions in CQA systems (Jeon et al. 2005; Xue et al. 2008; Cao et al. 2010; Cai et al. 2011; Zhou et al. 2015) and answer locating, which focuses finding a potentially suitable answer within a collection of candidate answers (Berger et al.", "startOffset": 213, "endOffset": 300}, {"referenceID": 4, "context": "To fully use Q&A archives in CQA systems, there are two important tasks for a newly submitted question, including question retrieval, which focuses on matching new questions with archived questions in CQA systems (Jeon et al. 2005; Xue et al. 2008; Cao et al. 2010; Cai et al. 2011; Zhou et al. 2015) and answer locating, which focuses finding a potentially suitable answer within a collection of candidate answers (Berger et al.", "startOffset": 213, "endOffset": 300}, {"referenceID": 3, "context": "To fully use Q&A archives in CQA systems, there are two important tasks for a newly submitted question, including question retrieval, which focuses on matching new questions with archived questions in CQA systems (Jeon et al. 2005; Xue et al. 2008; Cao et al. 2010; Cai et al. 2011; Zhou et al. 2015) and answer locating, which focuses finding a potentially suitable answer within a collection of candidate answers (Berger et al.", "startOffset": 213, "endOffset": 300}, {"referenceID": 29, "context": "To fully use Q&A archives in CQA systems, there are two important tasks for a newly submitted question, including question retrieval, which focuses on matching new questions with archived questions in CQA systems (Jeon et al. 2005; Xue et al. 2008; Cao et al. 2010; Cai et al. 2011; Zhou et al. 2015) and answer locating, which focuses finding a potentially suitable answer within a collection of candidate answers (Berger et al.", "startOffset": 213, "endOffset": 300}, {"referenceID": 2, "context": "2015) and answer locating, which focuses finding a potentially suitable answer within a collection of candidate answers (Berger et al. 2000; Surdeanu et al. 2008; Lu and Li 2013; Shen et al. 2015).", "startOffset": 120, "endOffset": 196}, {"referenceID": 25, "context": "2015) and answer locating, which focuses finding a potentially suitable answer within a collection of candidate answers (Berger et al. 2000; Surdeanu et al. 2008; Lu and Li 2013; Shen et al. 2015).", "startOffset": 120, "endOffset": 196}, {"referenceID": 21, "context": "2015) and answer locating, which focuses finding a potentially suitable answer within a collection of candidate answers (Berger et al. 2000; Surdeanu et al. 2008; Lu and Li 2013; Shen et al. 2015).", "startOffset": 120, "endOffset": 196}, {"referenceID": 5, "context": "One major challenge in both tasks is the lexical gap (chasm) problem (Chen et al. 2016).", "startOffset": 69, "endOffset": 87}, {"referenceID": 2, "context": "Potential solutions to overcome this difficulty include 1) query expansion, 2) statistical translation, 3) latent variable models (Berger et al. 2000).", "startOffset": 130, "endOffset": 150}, {"referenceID": 8, "context": "Classical methods include translation model (TM) (Jeon et al. 2005) and translation language model (TRLM) (Xue et al.", "startOffset": 49, "endOffset": 67}, {"referenceID": 26, "context": "2005) and translation language model (TRLM) (Xue et al. 2008).", "startOffset": 44, "endOffset": 61}, {"referenceID": 3, "context": "Apart from word-level translation, phrase-level translation for question and answer retrieval has also achieved promising results (Cai et al. 2011).", "startOffset": 130, "endOffset": 147}, {"referenceID": 3, "context": "Proposals have been made to learn the latent topics aligned across the question-answer pairs to bridge the lexical gap, on the assumption that question and its answer should share a similar topic distribution (Cai et al. 2011; Ji et al. 2012).", "startOffset": 209, "endOffset": 242}, {"referenceID": 9, "context": "Proposals have been made to learn the latent topics aligned across the question-answer pairs to bridge the lexical gap, on the assumption that question and its answer should share a similar topic distribution (Cai et al. 2011; Ji et al. 2012).", "startOffset": 209, "endOffset": 242}, {"referenceID": 21, "context": "Furthermore, inspired by the recent success of word embedding, several approaches have been proposed to leverage the advantages of the vector representation to overcome the lexical gap (Shen et al. 2015; Zhou et al. 2015).", "startOffset": 185, "endOffset": 221}, {"referenceID": 29, "context": "Furthermore, inspired by the recent success of word embedding, several approaches have been proposed to leverage the advantages of the vector representation to overcome the lexical gap (Shen et al. 2015; Zhou et al. 2015).", "startOffset": 185, "endOffset": 221}, {"referenceID": 1, "context": "Successful follow-up work includes application to statistical language modelling (Bengio et al. 2003; Mikolov et al. 2013a).", "startOffset": 81, "endOffset": 123}, {"referenceID": 24, "context": "Similarly neural tensor network (NTN) is also implemented to model relational information (Socher et al. 2013; Qiu and Huang 2015).", "startOffset": 90, "endOffset": 130}, {"referenceID": 24, "context": "The NTN\u2019s main advantage is that it can relate two inputs multiplicatively instead of only implicitly through non-linearity as with standard neural networks where the entity vectors are simply concatenated (Socher et al. 2013).", "startOffset": 206, "endOffset": 226}], "year": 2016, "abstractText": "The large scale of Q&A archives accumulated in community based question answering (CQA) servivces are important information and knowledge resource on the web. Question and answer matching task has been attached much importance to for its ability to reuse knowledge stored in these systems: it can be useful in enhancing user experience with recurrent questions. In this paper, a Word Embedding based Correlation (WEC) model is proposed by integrating advantages of both the translation model and word embedding. Given a random pair of words, WEC can score their co-occurrence probability in Q&A pairs, while it can also leverage the continuity and smoothness of continuous space word representation to deal with new pairs of words that are rare in the training parallel text. An experimental study on Yahoo! Answers dataset and Baidu Zhidao dataset shows this new method\u2019s promising", "creator": "TeX"}}}