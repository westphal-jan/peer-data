{"id": "1511.04707", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2015", "title": "Deep Linear Discriminant Analysis", "abstract": "We introduce Deep Linear Discriminant Analysis (DeepLDA) which learns linearly separable latent representations in an end-to-end fashion. Classic LDA extracts features which preserve class separability and is used for dimensionality reduction for many classification problems. The central idea of this paper is to put LDA on top of a deep neural network. This can be seen as a non-linear extension of classic LDA. Instead of maximizing the likelihood of target labels for individual samples, we propose an objective function that pushes the network to produce feature distributions which: (a) have low variance within the same class and (b) high variance between different classes. Our objective is derived from the general LDA eigenvalue problem and still allows to train with stochastic gradient descent and back-propagation. For evaluation we test our approach on three different benchmark datasets (MNIST, CIFAR-10 and STL-10). DeepLDA produces competitive results on all three datasets and sets a new state of the art on STL-10 with a test set accuracy of 81.4%.", "histories": [["v1", "Sun, 15 Nov 2015 14:33:26 GMT  (2261kb,D)", "http://arxiv.org/abs/1511.04707v1", "Under review as a conference paper at ICLR 2016"], ["v2", "Tue, 17 Nov 2015 08:05:10 GMT  (2093kb,D)", "http://arxiv.org/abs/1511.04707v2", "Under review as a conference paper at ICLR 2016"], ["v3", "Sat, 21 Nov 2015 17:59:18 GMT  (2084kb,D)", "http://arxiv.org/abs/1511.04707v3", "Under review as a conference paper at ICLR 2016"], ["v4", "Mon, 28 Dec 2015 09:52:47 GMT  (2107kb,D)", "http://arxiv.org/abs/1511.04707v4", "Under review as a conference paper at ICLR 2016"], ["v5", "Wed, 17 Feb 2016 08:32:47 GMT  (2114kb,D)", "http://arxiv.org/abs/1511.04707v5", "Published as a conference paper at ICLR 2016"]], "COMMENTS": "Under review as a conference paper at ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["matthias dorfer", "rainer kelz", "gerhard widmer"], "accepted": true, "id": "1511.04707"}, "pdf": {"name": "1511.04707.pdf", "metadata": {"source": "CRF", "title": "DEEP LINEAR DISCRIMINANT ANALYSIS", "authors": ["Matthias Dorfer", "Rainer Kelz", "Gerhard Widmer"], "emails": ["gerhard.widmer}@jku.at"], "sections": [{"heading": null, "text": "We are introducing Deep Linear Discriminant Analysis (DeepLDA), which learns end-to-end linear separable latent representations, and the central idea of this paper is to place LDA on a deep neural network, which can be considered a nonlinear extension of classical LDA. Rather than maximizing the likelihood of target markers for individual samples, we propose an objective function that causes the network to generate trait distributions that: (a) exhibit small variations within the same class and (b) high variations between different classes. Our goal is derived from the general LDA eigenvalue problem and continues to allow training with stochastic gradient descent and backpropagation. To evaluate our approach, we test our approach on three different benchmark datasets (MNIST, CIFAR-10 and STL-10). DeepLDA delivers competitive results on three new datasets with an accuracy of 8L% and a new 1.4%."}, {"heading": "1 INTRODUCTION", "text": "The aim of this paper is to define the beneficial properties of classical LDA (low intra-class variability, high inter-class variability, optimal decision limits) by formulating its goal of learning linear separable representations based on a deep neural network (DNN). Recently, methods related to LDA have been very successful in combination with deep neural networks. Andrew et al. published a deep version of canonical correlation analysis (DCCA) (Andrew et al., 2013). In their evaluations, DCCA is used to generate correlated representations of multimodal input data from simultaneously recorded acoustic and articulatory speech data."}, {"heading": "1.1 MAIN IDEA OF THIS PAPER", "text": "The approaches mentioned so far all have in common that they are based on well-established methods of multivariate statistics. Inspired by their work, we propose an end-to-end deep DNN version of the LDA - Deep Linear Discriminant Analysis (DeepLDA). DeepLDA is motivated by the fact that when the requirements of the LDA are met, it is able to find linear combinations of the input characteristics that enable optimal linear decision boundaries (Krizhevsky et al., 2012). The intuition of our method is to use LDA as a target on a powerful feature learning algorithm. Instead of maximizing the likelihood of target markers for individual samples, we propose a value-oriented target function that drives the network."}, {"heading": "2 DEEP NEURAL NETWORKS", "text": "Since the proposed model is based on a DNN, we briefly describe the training paradigm of a network used for classification problems such as object detection. A neural network with hidden P layers is presented as a nonlinear function f (\u0443) with model parameters \u0432 = {\u04351,..., \u0435P}. In the monitored environment, we additionally obtain a set of N tension samples x1,... xN together with corresponding classification targets t1,... tN probabilities. We also assume that the network performance pi = (pi, 1,..., pi, C) = f (xi, \u0432) is normalized by the softmax function to achieve class (pseudo) probabilities. The network is then optimized using stochastic gradient descences (SGD) with the aim of optimum model parameterization in relation to a specific loss function li (xi) (xi), xi) etti."}, {"heading": "3 DEEP LINEAR DISCRIMINANT ANALYSIS (DEEPLDA)", "text": "In this section, we first present a general introduction to LDA. Based on this introduction, we propose DeepLDA, which optimizes an LDA-based optimization target in an end-to-end DNN manner. Finally, we describe how DeepLDA is used to predict class probabilities of invisible test samples."}, {"heading": "3.1 LINEAR DISCRIMINANT ANALYSIS", "text": "Let x1,..., xN = X-RN \u00b7 d be a set of N samples belonging to the C classes, c = 1,..., C. The input representation X can be either hand-constructed features or hidden spatial representations H produced by a DNN (Andrew et al., 2013). The LDA goal of finding a linear projection A-Rl \u00b7 d in a lower l-dimensional subspace L, in which l = C-1. The resulting linear combinations of features xiAT are maximally separated in this space (Fisher, 1936). The LDA goal of finding a projection matrix A is formulated as follows: arg max A-ASbAT | ASwAT | (3) where Sb is the one between the scatter matrix St and defined over the entire scatter matrix St and within the scatter matrix Sw the eigenmatrix c-variance Sb = St Sw \u2212 is coded as the mean value of the individual class St."}, {"heading": "3.2 DEEPLDA MODEL CONFIGURATION", "text": "Figure 1b shows a schematic diagram of DeepLDA. Instead of a random optimization of the CCE loss to the predicted class probabilities (see section 2), we set an LDA layer over the DNN. This means in particular that we do not penalize the misclassification of individual samples, but try to generate characteristics that exhibit low class internal and high class variability. We address this maximization problem by a modified version of the general LDA eigenvalue problem proposed in the following section. Unlike CCE, DeepLDA optimization works on the basis of the properties of the distribution parameters on the hidden representation generated by the neural network. Since eigenvalue optimization is bound to the corresponding eigenvalue (a linear projection matrix), DeepLDA can also be interpreted as a special case of a dense layer."}, {"heading": "3.3 MODIFIED DEEPLDA OPTIMIZATION TARGET", "text": "On the basis of Section 3.1, we reformulate the LDA target as suitable for a combination with deep learning. As already stated by Stuhlsatz et al. (2012) and Lu et al. (2005), the estimate of Sw is overemphasized, while small eigenvalues are considered too low. To weaken this effect, Friedman proposes to regulate the within the scattering matrix by adding a multiple of the identity matrix Sw + \u03bbI. Adding the identity matrix has the second advantage of stabilizing small eigenvalues. The resulting eigenvalue problem is then formulated in such a way that Sw + \u03bbimme I) ei (7), where e = e1, eC \u2212 1 are the resulting eigenvectors and v = v1 the corresponding eigenvalue values."}, {"heading": "3.4 CLASSIFICATION BY DEEPLDA", "text": "This section describes how to assign the most likely class name to an invisible test sample as soon as the network is trained and parameterized. In a first step, we calculate the uppermost hidden representation H on the entire test sample X. From this hidden representation, we calculate the LDA as described in Section 3.1 and 3.3 and generate the corresponding eigenvectors e = {ei}. The vector of the class probabilities for a new test sample xt is then calculated as follows: p \u2032 c = 11 + ht (H-ce) e T \u2212 12diag (H-ceT) (10), where ht is the hidden representation of the test sample xt and H-c = (h-T1,..., h-T C) is a matrix containing the rows of hidden representations over the entire train set per class. In a last step, the probabilities are normalized to point to a PC = p \u2032 c / ently p \u2032 i. Finally, we assign the class name with the highest probability as maxi pi."}, {"heading": "4 EXPERIMENTS", "text": "In this section we present an experimental evaluation of DeepLDA using three benchmark datasets - MNIST, CIFAR-10 and STL-10 (see Figure 2 for some sample images).We compare the results of DeepLDA with the CCE-based optimization target and the current state of the art of the respective datasets, as well as providing details on the network architectures, hyperparameters and the respective training / optimization approaches used in our experiments."}, {"heading": "4.1 EXPERIMENTAL SETUP", "text": "The overall structure of the networks is similar for all three datasets and identical for CIFAR10 and STL-10. The architecture follows the VGG model with sequences of 3 x 3 turns (Simonyan & Zisserman, 2014). Instead of a dense classification layer, we use global average pooling on the characteristic maps of the last coil layer (Lin et al., 2013). We continue to apply batch normalization (Ioffe & Szegedy, 2015) after each coil layer, which (1) has helped to increase the convergence speed and (2) improve the performance of all our models. Batch normalization has a positive effect on both CCE and DeepLDA-based optimization. In Table 1, we outline the structure of our models in detail. All networks are set by SGD with Nesterov dynamics of Nesterov to 0.1 and the dynamics is set to 0.9 for all our models."}, {"heading": "4.2 EXPERIMENTAL RESULTS", "text": "We describe the benchmark data sets as well as the pre-processing and data enhancement used for the training. We present our results and correlate them with the state-of-the-art technology for each data set. As DeepLDA is designed to generate a linearly separable attribute space, we also report the results of a linear support vector machine that has been trained in the latent space of DeepLDA (marked with LinSVM). The results of our CCE-trained network architecture are marked as OurNetCCE."}, {"heading": "4.2.1 MNIST", "text": "The MNIST dataset consists of 28 x 28 grayscale images with handwritten digits from 0 to 9. We present the results for two different scenarios. In the MNIST-50k scenario, we train the model for the same number of periods as in MNIST-50k and use the validation set to select the parameterization that delivers the best results on the validation set. In the MNIST-60k scenario, we train the model for the same number of periods as in MNIST-50k, but also use the validation set for training. Finally, we report on the accuracy of the model on the test set after the last training period. This approach has also been used in (Lin et al., 2013), which produces the best state-of-the-art results on the datasets. Table 2 summarizes all the results on the MNIST dataset."}, {"heading": "4.2.2 CIFAR-10", "text": "The CIFAR-10 dataset consists of tiny 32 x 32 natural RGB images with samples from 10 different classes. The dataset is structured into 50,000 tensile samples and 10,000 test samples. We pre-processed the dataset using global contrast normalization and ZCA whitening, as proposed by Goodfellow et al. (2013). During the training, we only apply random left-right flips on the images - no additional data augmentation is used. As a training approach, we follow the MNIST dataset procedure described above to utilize the entire 50,000 tensile images. Table 3 summarizes our results and relates them to the current state of the art. Our models OurNetCCE and DeepLDA both provide current results on the dataset when no data augmentation is used. Although DeepLDA performs slightly worse than CCE, it is able to achieve competitive results on CIFAR-10."}, {"heading": "4.2.3 STL-10", "text": "Like CIFAR-10, the STL-10 dataset contains natural RGB images of 10 different object categories. However, at 96 x 96 pixels, the size of the images is larger, and the training set is considerably smaller, with only 4000 images. 1000 and 8000 images are available for validation and verification. In addition, STL-10 contains 100,000 unlabeled images, but we are not currently using these additional data. As with MNIST-50k, we train our models on 4000 railway images and use the validation set to select the most powerful parameters. Our results on STL-10 are in Table 4. Zhao et al. (2015) have achieved a test kit accuracy of 74.80% using both unlabeled and labeled data, and our CCE-trained model architecture delivers an accuracy of 78.39%, which is already an improvement of 3.6 percentage points. DeepLDA continues to improve the results and sets a new state of the art of 846%."}, {"heading": "5 INVESTIGATONS ON DEEPLDA AND DISCUSSIONS", "text": "In this section we give a deeper insight into the representations learned by DeepLDA. We examine experimentally the eigenvalue structure of the representations learned by DeepLDA and their relationship to the classification potential of the respective networks."}, {"heading": "5.1 DOES IMAGE SIZE AFFECT DEEPLDA?", "text": "The main difference between STL-10 and CIFAR-10, apart from the number of retraction frames, is the size of the images included (see Figure 2 to get an idea of the ratios).To get a deeper insight into the influence of this parameter, we are conducting the following additional experiment: (1) We are creating a reduced version of the STL-10 dataset with the same image dimensions as CIFAR-10 (32 \u00d7 32).( 2) We are repeating the experiment described in Section 4.2.3 on the reduced 32 \u00d7 32 dataset. The results are presented in Figure 3 as curves showing the train development and validation accuracy during the training. As expected, the reduction reduces the performance of both the CCE and the DeepLDA datasets. Furthermore, we note that DeepLDA performs best when trained on larger images and has a disadvantage for the small images."}, {"heading": "5.2 EIGENVALUE STRUCTURE OF DEEPLDA REPRESENTATIONS", "text": "DeepLDA optimization does not focus on maximizing the target class probability of individual samples. As suggested in Section 3, we encourage the network to learn characteristic representations with discriminatory distribution parameters (within and between class dispersion) by using the eigenvalue structure of the general LDA eigenvalue problem and using it as a deep learning objective. Figure 4a shows the evolution of the traction and test set accuracy of STL-10 along with the mean of all eigenvalues in each training period. Note that in epoch 0, almost all eigenvalues (1-7) begin at a value of 0. This emphasizes the importance of the design of our objective function (compare (9), which allows us to draw the discriminability in addition to the intrinsic structure of the lower 410."}, {"heading": "6 CONCLUSION", "text": "Our modified version of the LDA optimization goal drives the network to distribute discriminatory latent representations in all dimensions of the latent attribute space. Experimental results show that representations learned with DeepLDA are discriminatory and have a positive effect on classification accuracy. Or models achieve competitive results on MNIST and CIFAR-10 and set a new state-of-the-art standard on STL-10 with a test kit classification accuracy of 81.4%. Results and further research suggest that DeepLDA performs best when applied to images of reasonable size (in this case 96 x 96 pixels). Finally, we consider DeepLDA as a specific example of a generally fruitful strategy: the use of well-understood machine learning or classification models such as LDA with certain desirable properties to optimally utilize and represent these networks."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Sepp Hochreiter for the helpful discussions and all the developers of Theano and Lasagne for providing such great deep learning frameworks. The project is approved by the Software Competence Center Hagenberg. The Tesla K40, which was used for this research, was donated by the NVIDIA Corporation."}, {"heading": "APPENDIX: DEEPLDA LATENT REPRESENTATION", "text": "The following two figures compare the latent spatial representations on the STL-10 dataset. We show N-to-n scatter plots of the latent features on the first 1000 test samples. Figure 5 shows the test samples after projection into the C-1-dimensional DeepLDA subspace. The figure suggests that DeepLDA utilizes all available feature dimensions. An interesting observation is that many of the internal representations are orthogonal to each other (which is an implication of LDA), which naturally favors linear decision boundaries. For direct comparison, we show in Figure 6 the internal representation of the top layer of a CCE (Global Average Pooling) trained network."}], "references": [{"title": "Deep canonical correlation analysis", "author": ["Andrew", "Galen", "Arora", "Raman", "Bilmes", "Jeff", "Livescu", "Karen"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Andrew et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Andrew et al\\.", "year": 2013}, {"title": "Pcanet: A simple deep learning baseline for image classification", "author": ["Chan", "Tsung-Han", "Jia", "Kui", "Gao", "Shenghua", "Lu", "Jiwen", "Zeng", "Zinan", "Ma", "Yi"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Chan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "Rectified factor networks", "author": ["Clevert", "Djork-Arn\u00e9", "Unterthiner", "Thomas", "Mayr", "Andreas", "Ramsauer", "Hubert", "Hochreiter", "Sepp"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Clevert et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Clevert et al\\.", "year": 2015}, {"title": "Discriminative unsupervised feature learning with convolutional neural networks", "author": ["Dosovitskiy", "Alexey", "Springenberg", "Jost Tobias", "Riedmiller", "Martin", "Brox", "Thomas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dosovitskiy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dosovitskiy et al\\.", "year": 2014}, {"title": "The use of multiple measurements in taxonomic problems", "author": ["Fisher", "Ronald A"], "venue": "Annals of eugenics,", "citeRegEx": "Fisher and A.,? \\Q1936\\E", "shortCiteRegEx": "Fisher and A.", "year": 1936}, {"title": "Regularized discriminant analysis", "author": ["Friedman", "Jerome H"], "venue": "Journal of the American statistical association,", "citeRegEx": "Friedman and H.,? \\Q1989\\E", "shortCiteRegEx": "Friedman and H.", "year": 1989}, {"title": "Spatially-sparse convolutional neural networks", "author": ["Graham", "Benjamin"], "venue": "CoRR, abs/1409.6070,", "citeRegEx": "Graham and Benjamin.,? \\Q2014\\E", "shortCiteRegEx": "Graham and Benjamin.", "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "shift. CoRR,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Regularization studies of linear discriminant analysis in small sample size scenarios with application to face recognition", "author": ["Lu", "Juwei", "Plataniotis", "Konstantinos N", "Venetsanopoulos", "Anastasios N"], "venue": "Pattern Recognition Letters,", "citeRegEx": "Lu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2005}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Feature extraction with deep neural networks by a generalized discriminant analysis", "author": ["Stuhlsatz", "Andre", "Lippel", "Jens", "Zielke", "Thomas"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on,", "citeRegEx": "Stuhlsatz et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Stuhlsatz et al\\.", "year": 2012}, {"title": "Multi-task bayesian optimization", "author": ["Swersky", "Kevin", "Snoek", "Jasper", "Adams", "Ryan P"], "venue": "In Advances in Neural Information Processing Systems, pp. 2004\u20132012,", "citeRegEx": "Swersky et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Swersky et al\\.", "year": 2013}, {"title": "Stacked what-where autoencoders", "author": ["Zhao", "Junbo", "Mathieu", "Michael", "Goroshin", "Ross", "Lecun", "Yann"], "venue": "arXiv preprint arXiv:1506.02351,", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "published a deep version of Canonical Correlation Analysis (DCCA) (Andrew et al., 2013).", "startOffset": 66, "endOffset": 87}, {"referenceID": 2, "context": "propose Rectified Factor Networks (RFNs) which are a neural network interpretation of classic factor analysis (Clevert et al., 2015).", "startOffset": 110, "endOffset": 132}, {"referenceID": 1, "context": "However, one bottleneck of their approach is its limitation to very shallow architectures (two stages) (Chan et al., 2015).", "startOffset": 103, "endOffset": 122}, {"referenceID": 11, "context": "already picked up the idea of combining LDA with a neural networks and proposed a generalized version of LDA (Stuhlsatz et al., 2012).", "startOffset": 109, "endOffset": 133}, {"referenceID": 11, "context": "In (Stuhlsatz et al., 2012) this problem is tackled by a heuristic weighting scheme for computing the within-class scatter matrix required for LDA optimization.", "startOffset": 3, "endOffset": 27}, {"referenceID": 0, "context": "Andrew et al. published a deep version of Canonical Correlation Analysis (DCCA) (Andrew et al., 2013). In their evaluations DCCA is used to produce correlated representation of multi-modal input data of simultaneously recorded acoustic and articulatory speech data. Clevert et al. propose Rectified Factor Networks (RFNs) which are a neural network interpretation of classic factor analysis (Clevert et al., 2015). RFNs are used for unsupervised pre-training and help to improve classification performance on four different benchmark datasets. Chan et al. (2015) propose PCANet as well as an LDA based variation.", "startOffset": 0, "endOffset": 563}, {"referenceID": 8, "context": "Deep learning has become the state of the art in automatic feature learning and replaced existing approaches based on hand engineered feature in many fields such as object recognition (Krizhevsky et al., 2012).", "startOffset": 184, "endOffset": 209}, {"referenceID": 11, "context": "This replaces the weighting scheme of (Stuhlsatz et al., 2012) and allows to operate on the original formulation of LDA.", "startOffset": 38, "endOffset": 62}, {"referenceID": 0, "context": "The input representation X can either be hand engineered features, or hidden space representations H produced by a DNN (Andrew et al., 2013).", "startOffset": 119, "endOffset": 140}, {"referenceID": 10, "context": "As already discussed by Stuhlsatz et al. (2012) and Lu et al.", "startOffset": 24, "endOffset": 48}, {"referenceID": 9, "context": "(2012) and Lu et al. (2005) the estimation of Sw overemphasises high eigenvalues whereas small eigenvalues are estimated as too low.", "startOffset": 11, "endOffset": 28}, {"referenceID": 9, "context": "(2012) and Lu et al. (2005) the estimation of Sw overemphasises high eigenvalues whereas small eigenvalues are estimated as too low. To weaken this effect Friedman (1989) proposed to regularize the within scatter matrix by adding a multiple of the identity matrix Sw + \u03bbI.", "startOffset": 11, "endOffset": 171}, {"referenceID": 11, "context": "This was already discussed by (Stuhlsatz et al., 2012) and tackled by a weighted computation of the between covariance matrix Sb.", "startOffset": 30, "endOffset": 54}, {"referenceID": 0, "context": "Related methods already showed that mini-batch learning on distribution parameters (in this case covariance matrices) is feasible if the batch-size is sufficiently large to be representative for the entire population (Andrew et al., 2013).", "startOffset": 217, "endOffset": 238}, {"referenceID": 13, "context": "Zhao et al. (2015) achieved with their joint training approach a test set accuracy of 74.", "startOffset": 0, "endOffset": 19}, {"referenceID": 11, "context": "Multi-Task Bayesian Optimization (Swersky et al. (2013)) 70.", "startOffset": 34, "endOffset": 56}, {"referenceID": 3, "context": "10% Exemplar Convnets (Dosovitskiy et al. (2014)) 72.", "startOffset": 23, "endOffset": 49}, {"referenceID": 3, "context": "10% Exemplar Convnets (Dosovitskiy et al. (2014)) 72.80% Stacked What-Where Auto-encoders (Zhao et al. (2015)) 74.", "startOffset": 23, "endOffset": 110}], "year": 2017, "abstractText": "We introduce Deep Linear Discriminant Analysis (DeepLDA) which learns linearly separable latent representations in an end-to-end fashion. Classic LDA extracts features which preserve class separability and is used for dimensionality reduction for many classification problems. The central idea of this paper is to put LDA on top of a deep neural network. This can be seen as a non-linear extension of classic LDA. Instead of maximizing the likelihood of target labels for individual samples, we propose an objective function that pushes the network to produce feature distributions which: (a) have low variance within the same class and (b) high variance between different classes. Our objective is derived from the general LDA eigenvalue problem and still allows to train with stochastic gradient descent and back-propagation. For evaluation we test our approach on three different benchmark datasets (MNIST, CIFAR-10 and STL-10). DeepLDA produces competitive results on all three datasets and sets a new state of the art on STL-10 with a test set accuracy of 81.4%.", "creator": "LaTeX with hyperref package"}}}