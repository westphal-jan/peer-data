{"id": "1511.04773", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2015", "title": "Large-Scale Approximate Kernel Canonical Correlation Analysis", "abstract": "Kernel Canonical correlation analysis (KCCA) is a fundamental method with broad applicability in statistics and machine learning. Although there exist closed-form solution to the KCCA objective by solving an $N\\times N$ eigenvalue system where $N$ is the training set size, the computational requirements of this approach in both memory and time prohibit its usage in the large scale. Various approximation techniques have been developed for KCCA. A recently proposed approach is to first transform original inputs to a $M$-dimensional feature space using random kitchen sinks so that inner product in the feature space approximates the kernel function, and then apply linear CCA to the transformed inputs. In challenging applications, however, the dimensionality $M$ of the feature space may need to be very large in order to reveal the nonlinear correlations, and then it becomes non-trivial to solve linear CCA for data matrices of very high dimensionality. We propose to use the recently proposed stochastic optimization algorithm for linear CCA and its neural-network extension to further alleviate the computation requirements of approximate KCCA. This approach allows us to run approximate KCCA on a speech dataset with $1.4$ million training samples and random feature space of dimensionality $M=100000$ on a normal workstation.", "histories": [["v1", "Sun, 15 Nov 2015 22:20:02 GMT  (46kb)", "https://arxiv.org/abs/1511.04773v1", "Submission to ICLR 2016"], ["v2", "Tue, 17 Nov 2015 16:31:14 GMT  (46kb)", "http://arxiv.org/abs/1511.04773v2", "Submission to ICLR 2016"], ["v3", "Thu, 7 Jan 2016 00:27:20 GMT  (49kb)", "http://arxiv.org/abs/1511.04773v3", "Submission to ICLR 2016"], ["v4", "Mon, 29 Feb 2016 16:04:46 GMT  (49kb)", "http://arxiv.org/abs/1511.04773v4", "Published as a conference paper at International Conference on Learning Representations (ICLR) 2016"]], "COMMENTS": "Submission to ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["weiran wang", "karen livescu"], "accepted": true, "id": "1511.04773"}, "pdf": {"name": "1511.04773.pdf", "metadata": {"source": "CRF", "title": "LARGE-SCALE APPROXIMATE KERNEL CANONICAL CORRELATION ANALYSIS", "authors": ["Weiran Wang"], "emails": ["weiranwang@ttic.edu", "klivescu@ttic.edu"], "sections": [{"heading": null, "text": "ar Xiv: 151 1.04 773v 4 [cs.L G] 29 Feb 2016 Published as conference contribution at ICLR 2016"}, {"heading": "1 INTRODUCTION", "text": "(CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA). (CCA)."}, {"heading": "2 APPROXIMATE KCCA", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 KCCA SOLUTION", "text": "In KCCA we can define the input factors (xi) Ni = 1 of view 1 and {yi} N = 1 of view 2 using feature mappings (Lai & Fyfe, 2000; Akaho, 2001; Bach & Jordan, 2002; Hardoon et al., 2004) The key property of such kernels is that kx (x, x))) represents the original input factors (xi, yi) with (xi), (xi), (xi), (xi), (xi) the input factors (xi, yi) with (xi), (xi), (xi), (xi), (xi), (xi), (xi), (xi), (xi), (xi), (xi)."}, {"heading": "2.2 APPROXIMATION VIA RANDOM FEATURES", "text": "We describe a further approximate KCCA formulation that is particularly suitable for large problems. (...) It is known from the harmonious analysis that a shift-invariant kernel of the form k (x, x, x) = 1 fortuitous, fortuitous, fortuitous, fortuitous, fortuitous, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen,"}, {"heading": "3 STOCHASTIC OPTIMIZATION OF APPROXIMATE KCCA", "text": "If the dimensionality M of the random Fourier characteristics is very large, the solution to the resulting linear CCA problem is still very expensive, since one has to save the M-M matrix characteristics (1). (1) It is therefore desirable to develop memory-efficient stochastic optimization algorithms for CCA, where any update of the projection forecasts depends only on a small minibatch of B examples, thereby reducing the storage costs to O (bM). (In contrast to the classification or regression objectives, which usually come with random Fourier characteristics (Rahimi & Recht, 2009; Huang et al)."}, {"heading": "4 RELATED WORK", "text": "Continuous efforts have been made to scale classical methods such as principal component analysis and partial minimum squares with stochastic / online updates (Krasulina, 1969; Oja & Karhunen, 1985; Warmuth & Kuzmin, 2008; Arora et al., 2013; Balsubramani et al., 2013; Shamir, 2015; Xie et al., 2015).The CCA target is more demanding due to the limitations, as also noted by Arora et al., (2012).Avron et al. (2013) suggest an algorithm to select a subset of training samples that accelerate linear CCA."}, {"heading": "5 EXPERIMENTS", "text": "In this section we will demonstrate the KNOI algorithm on two major issues and compare it with several alternatives: \u2022 CCA, accurately solved by SVD. \u2022 FKCCA, slightly approaching KCCA using random Fourier functions, where the CCA step is exactly solved by SVD. \u2022 NKCCA, slightly approaching KCCA using the Nystro \ufffd m method, where the CCA step is exactly solved by SVD. We implement KNOI in MATLAB with GPU support. Since our algorithm mainly consists of simple matrix operations, executing it on a GPU provides significant acceleration."}, {"heading": "5.1 MNIST 8M", "text": "The data set consists of 8.1 million 28 x 28 grayscale images of the digits 0-9. We divide each image into the left and right halves and use them as the two views in KCCA, so the input dimensionality of the KCCA totals KCCA ratios 392 for both views. The data set is randomly divided into training / test sets of size 8M / 0.1M. The task is to learn L = 50 dimensional projections with KCCA, and the evaluation criterion is the total canonical correlation achieved on the test set (above 50). The same task is used by Xie et al. (2015), although we have another training / test split. As in Xie et al. (2015), we fix the core widths using the \"median\" trick6 for all algorithms."}, {"heading": "5.2 X-RAY MICROBEAM SPEECH DATA", "text": "In the second series of experiments, we apply approximately KCCA to the task of learning acoustic features for automatic speech recognition. We use the Wisconsin X-ray microbeam (XRMB) 6Following Xie et al. (2015), the core distances are used from the mean distances between 4000 randomly selected training parameters (in U and V) of the DSGD authors to provide their MATLAB implementation of DSGD. (2015) one uses a version of random Fourier features with both functions, so that the number of learnable parameters (in U and V) of the DSGD types is twice as large that the KNOI types are recognized for the same M.corpus et al. (Westbury, 1994) of the language and articulation measurements of 47 American English teachers. It has already been shown that multiview feature learning via CCA / KCCA greatly improves phonetic recognition of audio input (Arvesu, Wang et)."}, {"heading": "6 CONCLUSION", "text": "We have proposed Kernel Nonlinear Orthogonal Iterations (KNOI), a memory-efficient approximate KCCA algorithm based on random Fourier characteristics and stochastic formation of linear CCA. It scales better to large data and outperforms previous approximate KCCA algorithms in both objective values (total canonical correlation) and runtime (with GPU support). It is easy to study the faster random characteristics of Le et al. (2013) that can be generated in time O (NM log dx) instead of O (NMdx), or the Taylor characteristics of Cotter et al. (2011), which are preferable for sparse inputs, and random characteristics for dot products or polynomial cores (Kar & Karnick, 2012; Hamid et al., 2014; Pennington et al., 2015), which have proven useful for various domains."}, {"heading": "ACKNOWLEDGEMENT", "text": "The opinions expressed in this work are those of the authors and do not necessarily reflect the views of the funding agency. Tesla K40 GPUs used for this research were donated by NVIDIA Corporation. We thank Bo Xie for implementing the double stochastic gradient algorithm for approximate KCCA and Nati Srebro for helpful discussions."}], "references": [{"title": "A kernel method for canonical correlation analysis", "author": ["Akaho", "Shotaro"], "venue": "In Proceedings of the International Meeting of the Psychometric Society (IMPS2001),", "citeRegEx": "Akaho and Shotaro.,? \\Q2001\\E", "shortCiteRegEx": "Akaho and Shotaro.", "year": 2001}, {"title": "An Introduction to Multivariate Statistical Analysis", "author": ["T.W. Anderson"], "venue": null, "citeRegEx": "Anderson,? \\Q2003\\E", "shortCiteRegEx": "Anderson", "year": 2003}, {"title": "Deep canonical correlation analysis", "author": ["Andrew", "Galen", "Arora", "Raman", "Bilmes", "Jeff", "Livescu", "Karen"], "venue": "In Proc. of the 30th Int. Conf. Machine Learning (ICML", "citeRegEx": "Andrew et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Andrew et al\\.", "year": 2013}, {"title": "Kernel CCA for multi-view learning of acoustic features using articulatory measurements", "author": ["Arora", "Raman", "Livescu", "Karen"], "venue": "In Symposium on Machine Learning in Speech and Language Processing (MLSLP),", "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "Multi-view CCA-based acoustic features for phonetic recognition across speakers and domains", "author": ["Arora", "Raman", "Livescu", "Karen"], "venue": "In Proc. of the IEEE Int. Conf. Acoustics, Speech and Sig. Proc. (ICASSP\u201913),", "citeRegEx": "Arora et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2013}, {"title": "Stochastic optimization for PCA and PLS", "author": ["Arora", "Raman", "Cotter", "Andy", "Livescu", "Karen", "Srebro", "Nati"], "venue": "In 50th Annual Allerton Conference on Communication,", "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "Stochastic optimization of PCA with capped MSG", "author": ["Arora", "Raman", "Cotter", "Andy", "Srebro", "Nati"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Arora et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2013}, {"title": "Efficient dimensionality reduction for canonical correlation analysis", "author": ["Avron", "Haim", "Boutsidis", "Christos", "Toledo", "Sivan", "Zouzias", "Anastasios"], "venue": "In Proc. of the 30th Int. Conf. Machine Learning (ICML", "citeRegEx": "Avron et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Avron et al\\.", "year": 2013}, {"title": "Kernel independent component analysis", "author": ["Bach", "Francis R", "Jordan", "Michael I"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bach et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2002}, {"title": "A probabilistic interpretation of canonical correlation analysis", "author": ["Bach", "Francis R", "Jordan", "Michael I"], "venue": "Technical Report 688,", "citeRegEx": "Bach et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2005}, {"title": "The fast convergence of incremental PCA", "author": ["Balsubramani", "Akshay", "Dasgupta", "Sanjoy", "Freund", "Yoav"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Balsubramani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Balsubramani et al\\.", "year": 2013}, {"title": "On the regularization of canonical correlation analysis", "author": ["Bie", "Tijl De", "Moor", "Bart De"], "venue": "www.esat.kuleuven.ac.be/sista-cosic-docarch/,", "citeRegEx": "Bie et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bie et al\\.", "year": 2003}, {"title": "Semi-supervised kernel canonical correlation analysis with application to human fMRI", "author": ["Blaschkoa", "Matthew B", "Sheltonb", "Jacquelyn A", "Bartelsc", "Andreas", "Lamperte", "Christoph H", "Gretton", "Arthur"], "venue": "Pattern Recognition Letters,", "citeRegEx": "Blaschkoa et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Blaschkoa et al\\.", "year": 2011}, {"title": "Canonical correlation: A tutorial", "author": ["Borga", "Magnus"], "venue": null, "citeRegEx": "Borga and Magnus.,? \\Q2001\\E", "shortCiteRegEx": "Borga and Magnus.", "year": 2001}, {"title": "The tradeoffs of large scale learning", "author": ["Bottou", "Leon", "Bousquet", "Olivier"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Bottou et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 2008}, {"title": "Multi-view clustering via canonical correlation analysis", "author": ["Chaudhuri", "Kamalika", "Kakade", "Sham M", "Livescu", "Karen", "Sridharan", "Karthik"], "venue": "In Proc. of the 26th Int. Conf. Machine Learning", "citeRegEx": "Chaudhuri et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2009}, {"title": "Information bottleneck for Gaussian variables", "author": ["Chechik", "Gal", "Globerson", "Amir", "Tishby", "Naftali", "Weiss", "Yair"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Chechik et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chechik et al\\.", "year": 2005}, {"title": "Explicit approximations of the gaussian kernel", "author": ["Cotter", "Andrew", "Keshet", "Joseph", "Srebro", "Nathan"], "venue": "[cs.AI], September", "citeRegEx": "Cotter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cotter et al\\.", "year": 2011}, {"title": "Multi-view learning of word embeddings via CCA", "author": ["Dhillon", "Paramveer", "Foster", "Dean", "Ungar", "Lyle"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Dhillon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dhillon et al\\.", "year": 2011}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["Faruqui", "Manaal", "Dyer", "Chris"], "venue": "In Proceedings of EACL,", "citeRegEx": "Faruqui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2014}, {"title": "Multi-view dimensionality reduction via canonical correlation analysis", "author": ["Foster", "Dean P", "Johnson", "Rie", "Kakade", "Sham M", "Zhang", "Tong"], "venue": null, "citeRegEx": "Foster et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Foster et al\\.", "year": 2009}, {"title": "Statistical consistency of kernel canonical correlation analysis", "author": ["Fukumizu", "Kenji", "Bach", "Francis R", "Gretton", "Arthur"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Fukumizu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Fukumizu et al\\.", "year": 2007}, {"title": "Revisiting the nystrom method for improved large-scale machine learning", "author": ["Gittens", "Alex", "Mahoney", "Michael"], "venue": "In Proc. of the 30th Int. Conf. Machine Learning (ICML", "citeRegEx": "Gittens et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gittens et al\\.", "year": 2013}, {"title": "Linear Algebra for Signal Processing, volume 69 of The IMA Volumes in Mathematics and its Applications, chapter The Canonical Correlations of Matrix Pairs", "author": ["Golub", "Gene H", "Zha", "Hongyuan"], "venue": null, "citeRegEx": "Golub et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Golub et al\\.", "year": 1995}, {"title": "Learning bilingual lexicons from monolingual corpora", "author": ["Haghighi", "Aria", "Liang", "Percy", "Berg-Kirkpatrick", "Taylor", "Klein", "Dan"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL", "citeRegEx": "Haghighi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Haghighi et al\\.", "year": 2008}, {"title": "An algorithm for the principal component analysis of large data sets", "author": ["Halko", "Nathan", "Martinsson", "Per-Gunnar", "Shkolnisky", "Yoel", "Tygert", "Mark"], "venue": "SIAM J. Sci. Comput.,", "citeRegEx": "Halko et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Halko et al\\.", "year": 2011}, {"title": "Compact random feature maps", "author": ["Hamid", "Raffay", "Xiao", "Ying", "Gittens", "Alex", "Decoste", "Dennis"], "venue": "In Proc. of the 31st Int. Conf. Machine Learning (ICML", "citeRegEx": "Hamid et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hamid et al\\.", "year": 2014}, {"title": "Canonical correlation analysis: An overview with application to learning methods", "author": ["Hardoon", "David R", "Szedmak", "Sandor", "Shawe-Taylor", "John"], "venue": "Neural Computation,", "citeRegEx": "Hardoon et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hardoon et al\\.", "year": 2004}, {"title": "Unsupervised analysis of fMRI data using kernel canonical correlation", "author": ["Hardoon", "David R", "Mour\u00e4o-Miranda", "Janaina", "Brammer", "Michael", "Shawe-Taylor", "John"], "venue": null, "citeRegEx": "Hardoon et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hardoon et al\\.", "year": 2007}, {"title": "Tandem connectionist feature extraction for conventional HMM systems", "author": ["Hermansky", "Hynek", "Ellis", "Daniel P. W", "Sharma", "Sangita"], "venue": "In Proc. of the IEEE Int. Conf. Acoustics, Speech and Sig. Proc", "citeRegEx": "Hermansky et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Hermansky et al\\.", "year": 2000}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["Hodosh", "Micah", "Young", "Peter", "Hockenmaier", "Julia"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Hodosh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "Relations between two sets of variates", "author": ["Hotelling", "Harold"], "venue": "Biometrika, 28(3/4):321\u2013377,", "citeRegEx": "Hotelling and Harold.,? \\Q1936\\E", "shortCiteRegEx": "Hotelling and Harold.", "year": 1936}, {"title": "Kernel methods match deep neural networks on TIMIT: Scalable learning in high-dimensional random Fourier spaces", "author": ["Huang", "Po-Sen", "Avron", "Haim", "Sainath", "Tara", "Sindhwani", "Vikas", "Ramabhadran", "Bhuvana"], "venue": "In Proc. of the IEEE Int. Conf. Acoustics, Speech and Sig. Proc. (ICASSP\u201914),", "citeRegEx": "Huang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2014}, {"title": "Multi-view regression via canonical correlation analysis", "author": ["Kakade", "Sham M", "Foster", "Dean P"], "venue": "In Proc. of the 20th Annual Conference on Learning Theory (COLT\u201907),", "citeRegEx": "Kakade et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2007}, {"title": "Random feature maps for dot product kernels", "author": ["Kar", "Purushottam", "Karnick", "Harish"], "venue": "In Proc. of the 15th Int. Workshop on Artificial Intelligence and Statistics (AISTATS", "citeRegEx": "Kar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kar et al\\.", "year": 2012}, {"title": "Discriminative learning and recognition of image set classes using canonical correlations", "author": ["Kim", "Tae-Kyun", "Kittler", "Josef", "Cipolla", "Roberto"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Kim et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2007}, {"title": "A method of stochastic approximation for the determination of the least eigenvalue of a symmetric matrix", "author": ["T.P. Krasulina"], "venue": "USSR Computational Mathematics and Mathematical Physics,", "citeRegEx": "Krasulina,? \\Q1969\\E", "shortCiteRegEx": "Krasulina", "year": 1969}, {"title": "Sampling methods for the nystr\u00f6m method", "author": ["Kumar", "Sanjiv", "Mohri", "Mehryar", "Talwalkar", "Ameet"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Kumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2012}, {"title": "Kernel and nonlinear canonical correlation analysis", "author": ["P.L. Lai", "C. Fyfe"], "venue": "Int. J. Neural Syst.,", "citeRegEx": "Lai and Fyfe,? \\Q2000\\E", "shortCiteRegEx": "Lai and Fyfe", "year": 2000}, {"title": "Fastfood - computing Hilbert space expansions in loglinear time", "author": ["Le", "Quoc", "Sarlos", "Tamas", "Smola", "Alexander"], "venue": "In Proc. of the 30th Int. Conf. Machine Learning (ICML", "citeRegEx": "Le et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Le et al\\.", "year": 2013}, {"title": "Making large-scale Nystr\u00f6m approximation possible", "author": ["M. Li", "J.T. Kwok", "B. Lu"], "venue": "In Proc. of the 27th Int. Conf. Machine Learning (ICML", "citeRegEx": "Li et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "Using KCCA for japanese-english cross-language information retrieval and classification", "author": ["Li", "Yaoyong", "Shawe-Taylor", "John"], "venue": "Journal of Intelligent Information Systems,", "citeRegEx": "Li et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Li et al\\.", "year": 2005}, {"title": "Training invariant support vector machines using selective sampling", "author": ["Loosli", "Ga\u00eblle", "Canu", "St\u00e9phane", "Bottou", "L\u00e9on"], "venue": "In Large Scale Kernel Machines,", "citeRegEx": "Loosli et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Loosli et al\\.", "year": 2007}, {"title": "The randomized dependence coefficient", "author": ["Lopez-Paz", "David", "Hennig", "Philipp", "Sch\u00f6lkopf", "Bernhard"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Lopez.Paz et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lopez.Paz et al\\.", "year": 2013}, {"title": "Randomized nonlinear component analysis", "author": ["Lopez-Paz", "David", "Sra", "Suvrit", "Smola", "Alex", "Ghahramani", "Zoubin", "Schoelkopf", "Bernhard"], "venue": "In Proc. of the 31st Int. Conf. Machine Learning (ICML", "citeRegEx": "Lopez.Paz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lopez.Paz et al\\.", "year": 2014}, {"title": "Deep multilingual correlation for improved word embeddings", "author": ["Lu", "Ang", "Wang", "Weiran", "Bansal", "Mohit", "Gimpel", "Kevin", "Livescu", "Karen"], "venue": "In The 2015 Conference of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL-HLT 2015),", "citeRegEx": "Lu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2015}, {"title": "Large scale canonical correlation analysis with iterative least squares", "author": ["Lu", "Yichao", "Foster", "Dean P"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Lu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2014}, {"title": "How to scale up kernel methods to be as good as deep neural nets", "author": ["Lu", "Zhiyun", "May", "Avner", "Liu", "Kuan", "Garakani", "Alireza Bagheri", "Guo", "Dong", "Bellet", "Aur\u00e9lien", "Fan", "Linxi", "Collins", "Michael", "Kingsbury", "Brian", "Picheny", "Sha", "Fei"], "venue": "[cs.LG],", "citeRegEx": "Lu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2015}, {"title": "Finding linear structure in large datasets with scalable canonical correlation analysis", "author": ["Ma", "Zhuang", "Lu", "Yichao", "Foster", "Dean"], "venue": "In Proc. of the 32st Int. Conf. Machine Learning (ICML", "citeRegEx": "Ma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "An efficient algorithm for information decomposition and extraction", "author": ["Makur", "Anuran", "Kozynski", "Fab\u0131\u0301an", "Huang", "Shao-Lun", "Zheng", "Lizhong"], "venue": "In 53nd Annual Allerton Conference on Communication, Control and Computing,", "citeRegEx": "Makur et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Makur et al\\.", "year": 2015}, {"title": "Nonlinear feature extraction using generalized canonical correlation analysis", "author": ["Melzer", "Thomas", "Reiter", "Michael", "Bischof", "Horst"], "venue": "In Proc. of the 11th Int. Conf. Artificial Neural Networks", "citeRegEx": "Melzer et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Melzer et al\\.", "year": 2001}, {"title": "Memory limited, streaming PCA", "author": ["Mitliagkas", "Ioannis", "Caramanis", "Constantine", "Jain", "Prateek"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Mitliagkas et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mitliagkas et al\\.", "year": 2013}, {"title": "Chemometric classification of some European wines using pyrolysis mass spectrometry", "author": ["Montanarella", "Luca", "Bassani", "Maria Rosa", "Br\u00e9as", "Olivier"], "venue": "Rapid Communications in Mass Spectrometry,", "citeRegEx": "Montanarella et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Montanarella et al\\.", "year": 1995}, {"title": "On stochastic approximation of the eigenvectors and eigenvalues of the expectation of a random matrix", "author": ["Oja", "Erkki", "Karhunen", "Juha"], "venue": "J. Math. Anal. Appl.,", "citeRegEx": "Oja et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Oja et al\\.", "year": 1985}, {"title": "Spherical random features for polynomial kernels", "author": ["Pennington", "Jeffrey", "Yu", "Felix", "Kumar", "Sanjiv"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Pennington et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2015}, {"title": "The Kaldi speech recognition toolkit", "author": ["Povey", "Daniel", "Ghoshal", "Arnab", "Boulianne", "Gilles", "Burget", "Lukas", "Glembek", "Ondrej", "Goel", "Nagendra", "Hannemann", "Mirko", "Motlicek", "Petr", "Qian", "Yanmin", "Schwarz", "Silovsky", "Jan", "Stemmer", "Georg", "Vesely", "Karel"], "venue": "In Proc. of the 2011 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU", "citeRegEx": "Povey et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Povey et al\\.", "year": 2011}, {"title": "Random features for large-scale kernel machines", "author": ["Rahimi", "Ali", "Recht", "Ben"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Rahimi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Rahimi et al\\.", "year": 2008}, {"title": "Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning", "author": ["Rahimi", "Ali", "Recht", "Benjamin"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Rahimi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Rahimi et al\\.", "year": 2009}, {"title": "A stochastic gradient method with an exponential convergence rate for finite training sets", "author": ["Roux", "Nicolas Le", "Schmidt", "Mark", "Bach", "Francis"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Roux et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Roux et al\\.", "year": 2012}, {"title": "Correcting errors in speech recognition with articulatory dynamics", "author": ["Rudzicz", "Frank"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Rudzicz and Frank.,? \\Q2010\\E", "shortCiteRegEx": "Rudzicz and Frank.", "year": 2010}, {"title": "Learning with Kernels. Support Vector Machines, Regularization, Optimization, and Beyond", "author": ["Sch\u00f6lkopf", "Bernhard", "Smola", "Alexander J"], "venue": "Adaptive Computation and Machine Learning Series", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2001}, {"title": "A stochastic PCA and SVD algorithm with an exponential convergence rate", "author": ["Shamir", "Ohad"], "venue": "In Proc. of the 32st Int. Conf. Machine Learning (ICML", "citeRegEx": "Shamir and Ohad.,? \\Q2015\\E", "shortCiteRegEx": "Shamir and Ohad.", "year": 2015}, {"title": "Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora", "author": ["Socher", "Richard", "Li", "Fei-Fei"], "venue": "In Proc. of the 2010 IEEE Computer Society Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Socher et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2010}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Sutskever", "Ilya", "Martens", "James", "Dahl", "George", "Hinton", "Geoffrey"], "venue": "In Proc. of the 30th Int. Conf. Machine Learning (ICML", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Canonical ridge and econometrics of joint production", "author": ["H.D. Vinod"], "venue": "Journal of Econometrics,", "citeRegEx": "Vinod,? \\Q1976\\E", "shortCiteRegEx": "Vinod", "year": 1976}, {"title": "Inferring a semantic representation of text via cross-language correlation analysis", "author": ["Vinokourov", "Alexei", "Cristianini", "Nello", "Shawe-Taylor", "John"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Vinokourov et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Vinokourov et al\\.", "year": 2003}, {"title": "Unsupervised learning of acoustic features via deep canonical correlation analysis", "author": ["Wang", "Weiran", "Arora", "Raman", "Livescu", "Karen", "Bilmes", "Jeff"], "venue": "In Proc. of the IEEE Int. Conf. Acoustics, Speech and Sig. Proc. (ICASSP\u201915),", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "On deep multi-view representation learning", "author": ["Wang", "Weiran", "Arora", "Raman", "Livescu", "Karen", "Bilmes", "Jeff"], "venue": "In Proc. of the 32st Int. Conf. Machine Learning (ICML", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Stochastic optimization for deep cca via nonlinear orthogonal iterations", "author": ["Wang", "Weiran", "Arora", "Raman", "Srebro", "Nati", "Livescu", "Karen"], "venue": "In 53nd Annual Allerton Conference on Communication, Control and Computing,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Randomized online PCA algorithms with regret bounds that are logarithmic in the dimension", "author": ["Warmuth", "Manfred K", "Kuzmin", "Dima"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Warmuth et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Warmuth et al\\.", "year": 2008}, {"title": "X-Ray Microbeam Speech Production", "author": ["Westbury", "John R"], "venue": "Database User\u2019s Handbook Version", "citeRegEx": "Westbury and R.,? \\Q1994\\E", "shortCiteRegEx": "Westbury and R.", "year": 1994}, {"title": "Using the Nystr\u00f6m method to speed up kernel machines", "author": ["Williams", "Christopher K. I", "Seeger", "Matthias"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Williams et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2001}, {"title": "A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis", "author": ["Witten", "Daniela M", "Tibshirani", "Robert", "Hastie", "Trevor"], "venue": null, "citeRegEx": "Witten et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Witten et al\\.", "year": 2009}, {"title": "Scale up nonlinear component analysis with doubly stochastic gradients", "author": ["Xie", "Bo", "Liang", "Yingyu", "Song", "Le"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Xie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2015}, {"title": "Kernel Methods in Computational Biology, chapter Heterogeneous Data Comparison and Gene Selection with Kernel Canonical Correlation Analysis, pp. 209\u2013229", "author": ["Yamanishi", "Yoshihiro", "Vert", "Jean-Philippe", "Kanehisa", "Minoru"], "venue": null, "citeRegEx": "Yamanishi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Yamanishi et al\\.", "year": 2004}, {"title": "Quasi-Monte Carlo feature maps for shift-invariant kernels", "author": ["Yang", "Jiyan", "Sindhwani", "Vikas", "Avron", "Haim", "Mahoney", "Michael"], "venue": "In Proc. of the 31st Int. Conf. Machine Learning (ICML", "citeRegEx": "Yang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2014}, {"title": "Nystr\u00f6m method vs random Fourier features: A theoretical and empirical comparison", "author": ["Yang", "Tianbao", "Li", "Yu-Feng", "Mahdavi", "Mehrdad", "Jin", "Rong", "Zhou", "Zhi-Hua"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Yang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2012}, {"title": "Adaptive canonical correlation analysis based on matrix manifolds", "author": ["Yger", "Florian", "Berar", "Maxime", "Gasso", "Gilles", "Rakotomamonjy", "Alain"], "venue": "In Proc. of the 29th Int. Conf. Machine Learning (ICML", "citeRegEx": "Yger et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yger et al\\.", "year": 2012}, {"title": "The HTK book version 2.2", "author": ["Young", "Steve J", "Kernshaw", "Dan", "Odell", "Julian", "Ollason", "Dave", "Valtchev", "Valtcho", "Woodland", "Phil"], "venue": "Technical report, Entropic,", "citeRegEx": "Young et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Young et al\\.", "year": 1999}, {"title": "Density-weighted Nystr\u00f6m method for computing large kernel eigen-systems", "author": ["Zhang", "Kai", "Kwok", "James T"], "venue": "Neural Computation,", "citeRegEx": "Zhang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2009}, {"title": "Clustered Nystr\u00f6m method for large scale manifold learning and dimension reduction", "author": ["Zhang", "Kai", "Kwok", "James T"], "venue": "IEEE Trans. Neural Networks,", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 1, "context": "Canonical correlation analysis (CCA, Hotelling, 1936) and its extensions are ubiquitous techniques in scientific research areas for revealing the common sources of variability in multiple views of the same phenomenon, including meteorology (Anderson, 2003), chemometrics (Montanarella et al.", "startOffset": 240, "endOffset": 256}, {"referenceID": 52, "context": "Canonical correlation analysis (CCA, Hotelling, 1936) and its extensions are ubiquitous techniques in scientific research areas for revealing the common sources of variability in multiple views of the same phenomenon, including meteorology (Anderson, 2003), chemometrics (Montanarella et al., 1995), genomics (Witten et al.", "startOffset": 271, "endOffset": 298}, {"referenceID": 72, "context": ", 1995), genomics (Witten et al., 2009), computer vision (Kim et al.", "startOffset": 18, "endOffset": 39}, {"referenceID": 35, "context": ", 2009), computer vision (Kim et al., 2007; Socher & Li, 2010), speech recognition (Rudzicz, 2010; Arora & Livescu, 2013; Wang et al.", "startOffset": 25, "endOffset": 62}, {"referenceID": 65, "context": ", 2015a), and natural language processing (Vinokourov et al., 2003; Haghighi et al., 2008; Dhillon et al., 2011; Hodosh et al., 2013; Faruqui & Dyer, 2014; Lu et al., 2015a).", "startOffset": 42, "endOffset": 173}, {"referenceID": 24, "context": ", 2015a), and natural language processing (Vinokourov et al., 2003; Haghighi et al., 2008; Dhillon et al., 2011; Hodosh et al., 2013; Faruqui & Dyer, 2014; Lu et al., 2015a).", "startOffset": 42, "endOffset": 173}, {"referenceID": 18, "context": ", 2015a), and natural language processing (Vinokourov et al., 2003; Haghighi et al., 2008; Dhillon et al., 2011; Hodosh et al., 2013; Faruqui & Dyer, 2014; Lu et al., 2015a).", "startOffset": 42, "endOffset": 173}, {"referenceID": 30, "context": ", 2015a), and natural language processing (Vinokourov et al., 2003; Haghighi et al., 2008; Dhillon et al., 2011; Hodosh et al., 2013; Faruqui & Dyer, 2014; Lu et al., 2015a).", "startOffset": 42, "endOffset": 173}, {"referenceID": 64, "context": "U\u03a3xxU = V\u03a3yyV = I, ui \u03a3xyvj = 0, for i 6= j, where (U,V) are the projection matrices for each view, \u03a3xy = 1 N \u2211N i=1 xiy \u22a4 i , \u03a3xx = 1 N \u2211N i=1 xix \u22a4 i + rxI, \u03a3yy = 1 N \u2211N i=1 yiy \u22a4 i + ryI are the cross- and auto-covariance matrices, and (rx, ry) \u2265 0 are regularization parameters (Vinod, 1976; Bie & Moor, 2003).", "startOffset": 282, "endOffset": 313}, {"referenceID": 15, "context": "2 The theoretical properties of CCA (Kakade & Foster, 2007; Chaudhuri et al., 2009; Foster et al., 2009) and its connection to other methods (Borga, 2001; Bach & Jordan, 2005; Chechik et al.", "startOffset": 36, "endOffset": 104}, {"referenceID": 20, "context": "2 The theoretical properties of CCA (Kakade & Foster, 2007; Chaudhuri et al., 2009; Foster et al., 2009) and its connection to other methods (Borga, 2001; Bach & Jordan, 2005; Chechik et al.", "startOffset": 36, "endOffset": 104}, {"referenceID": 16, "context": ", 2009) and its connection to other methods (Borga, 2001; Bach & Jordan, 2005; Chechik et al., 2005) have also been studied.", "startOffset": 44, "endOffset": 100}, {"referenceID": 50, "context": "To overcome this issue, kernel CCA (KCCA) was proposed indepedently by several researchers (Lai & Fyfe, 2000; Akaho, 2001; Melzer et al., 2001) and has become a common technique in statistics and machine learning (Bach & Jordan, 2002; Hardoon et al.", "startOffset": 91, "endOffset": 143}, {"referenceID": 27, "context": ", 2001) and has become a common technique in statistics and machine learning (Bach & Jordan, 2002; Hardoon et al., 2004).", "startOffset": 77, "endOffset": 120}, {"referenceID": 27, "context": "KCCA has been successfully used for cross-modality retrieval (Hardoon et al., 2004; Li & Shawe-Taylor, 2005; Socher & Li, 2010; Hodosh et al., 2013), acoustic feature learning (Arora & Livescu, 2013), computational biology (Yamanishi et al.", "startOffset": 61, "endOffset": 148}, {"referenceID": 30, "context": "KCCA has been successfully used for cross-modality retrieval (Hardoon et al., 2004; Li & Shawe-Taylor, 2005; Socher & Li, 2010; Hodosh et al., 2013), acoustic feature learning (Arora & Livescu, 2013), computational biology (Yamanishi et al.", "startOffset": 61, "endOffset": 148}, {"referenceID": 74, "context": ", 2013), acoustic feature learning (Arora & Livescu, 2013), computational biology (Yamanishi et al., 2004; Hardoon et al., 2007; Blaschkoa et al., 2011), and statistical independence measurement (Bach & Jordan, 2002; Fukumizu et al.", "startOffset": 82, "endOffset": 152}, {"referenceID": 28, "context": ", 2013), acoustic feature learning (Arora & Livescu, 2013), computational biology (Yamanishi et al., 2004; Hardoon et al., 2007; Blaschkoa et al., 2011), and statistical independence measurement (Bach & Jordan, 2002; Fukumizu et al.", "startOffset": 82, "endOffset": 152}, {"referenceID": 12, "context": ", 2013), acoustic feature learning (Arora & Livescu, 2013), computational biology (Yamanishi et al., 2004; Hardoon et al., 2007; Blaschkoa et al., 2011), and statistical independence measurement (Bach & Jordan, 2002; Fukumizu et al.", "startOffset": 82, "endOffset": 152}, {"referenceID": 21, "context": ", 2011), and statistical independence measurement (Bach & Jordan, 2002; Fukumizu et al., 2007; Lopez-Paz et al., 2013).", "startOffset": 50, "endOffset": 118}, {"referenceID": 43, "context": ", 2011), and statistical independence measurement (Bach & Jordan, 2002; Fukumizu et al., 2007; Lopez-Paz et al., 2013).", "startOffset": 50, "endOffset": 118}, {"referenceID": 44, "context": "With rank-M approximations of the kernel matrices, the cost of solving approximate KCCA reduces to O(MN) (see, e.g., Bach & Jordan, 2002; Lopez-Paz et al., 2014).", "startOffset": 105, "endOffset": 161}, {"referenceID": 76, "context": "Typically, ranks of a few hundred to a few thousand are used for the low-rank kernel approximations (Yang et al., 2012; Le et al., 2013; Lopez-Paz et al., 2014).", "startOffset": 100, "endOffset": 160}, {"referenceID": 39, "context": "Typically, ranks of a few hundred to a few thousand are used for the low-rank kernel approximations (Yang et al., 2012; Le et al., 2013; Lopez-Paz et al., 2014).", "startOffset": 100, "endOffset": 160}, {"referenceID": 44, "context": "Typically, ranks of a few hundred to a few thousand are used for the low-rank kernel approximations (Yang et al., 2012; Le et al., 2013; Lopez-Paz et al., 2014).", "startOffset": 100, "endOffset": 160}, {"referenceID": 44, "context": "The particular variant of approximate KCCA we use, called randomized CCA (Lopez-Paz et al., 2014), transforms the original inputs to an M -dimensional feature space using random features (Rahimi & Recht, 2008; 2009) so that inner products in the new feature space approximate the kernel function.", "startOffset": 73, "endOffset": 97}, {"referenceID": 48, "context": "We then make use of a stochastic optimization algorithm, recently proposed for linear CCA and its deep neural network extension deep CCA (Ma et al., 2015; Wang et al., 2015c), to reduce the memory requirement for solving the resulting linear CCA problem.", "startOffset": 137, "endOffset": 174}, {"referenceID": 50, "context": "In KCCA, we transform the inputs {xi}i=1 of view 1 and {yi} N i=1 of view 2 using feature mappings \u03c6x and \u03c6y associated with some positive semi-definite kernels kx and ky respectively, and then solve the linear CCA problem (1) for the feature-mapped inputs (Lai & Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach & Jordan, 2002; Hardoon et al., 2004).", "startOffset": 257, "endOffset": 352}, {"referenceID": 27, "context": "In KCCA, we transform the inputs {xi}i=1 of view 1 and {yi} N i=1 of view 2 using feature mappings \u03c6x and \u03c6y associated with some positive semi-definite kernels kx and ky respectively, and then solve the linear CCA problem (1) for the feature-mapped inputs (Lai & Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach & Jordan, 2002; Hardoon et al., 2004).", "startOffset": 257, "endOffset": 352}, {"referenceID": 27, "context": "Various kernel approximation techniques have been proposed to scale up KCCA, including Cholesky decomposition (Bach & Jordan, 2002), partial Gram-Schmidt (Hardoon et al., 2004), and incremental SVD (Arora & Livescu, 2012).", "startOffset": 154, "endOffset": 176}, {"referenceID": 76, "context": "We can extract such features for both views, and apply linear CCA to them to approximate the KCCA solution (Yang et al., 2012; Lopez-Paz et al., 2014).", "startOffset": 107, "endOffset": 150}, {"referenceID": 44, "context": "We can extract such features for both views, and apply linear CCA to them to approximate the KCCA solution (Yang et al., 2012; Lopez-Paz et al., 2014).", "startOffset": 107, "endOffset": 150}, {"referenceID": 40, "context": "Although there have been various sampling/approximation strategies for the Nystr\u00f6m method (Li et al., 2010; Zhang & Kwok, 2009; 2010; Kumar et al., 2012; Gittens & Mahoney, 2013), their constructions are more involved.", "startOffset": 90, "endOffset": 178}, {"referenceID": 37, "context": "Although there have been various sampling/approximation strategies for the Nystr\u00f6m method (Li et al., 2010; Zhang & Kwok, 2009; 2010; Kumar et al., 2012; Gittens & Mahoney, 2013), their constructions are more involved.", "startOffset": 90, "endOffset": 178}, {"referenceID": 44, "context": "as (Lopez-Paz et al., 2014)", "startOffset": 3, "endOffset": 27}, {"referenceID": 26, "context": "This approach has been extended to other types of kernels (Kar & Karnick, 2012; Hamid et al., 2014; Pennington et al., 2015).", "startOffset": 58, "endOffset": 124}, {"referenceID": 54, "context": "This approach has been extended to other types of kernels (Kar & Karnick, 2012; Hamid et al., 2014; Pennington et al., 2015).", "startOffset": 58, "endOffset": 124}, {"referenceID": 75, "context": "A careful quasi-Monte Carlo scheme for sampling from p(w) (Yang et al., 2014), and structured feature transformation for accelerating the computation of w i x (Le et al.", "startOffset": 58, "endOffset": 77}, {"referenceID": 39, "context": ", 2014), and structured feature transformation for accelerating the computation of w i x (Le et al., 2013), have also been studied.", "startOffset": 89, "endOffset": 106}, {"referenceID": 26, "context": "This approach has been extended to other types of kernels (Kar & Karnick, 2012; Hamid et al., 2014; Pennington et al., 2015). A careful quasi-Monte Carlo scheme for sampling from p(w) (Yang et al., 2014), and structured feature transformation for accelerating the computation of w i x (Le et al., 2013), have also been studied. Leveraging this result, Rahimi & Recht (2009) propose to first extract M -dimensional random Fourier features for input x as (with slight abuse of notation)", "startOffset": 80, "endOffset": 374}, {"referenceID": 76, "context": "Although the Nystr\u00f6m approximation can be more accurate at the same rank (Yang et al., 2012), the computational efficiency and smaller memory cost of random features make them more appealing for large-scale problems in practice.", "startOffset": 73, "endOffset": 92}, {"referenceID": 43, "context": "g, Lu et al., 2015b used the recently proposed stochastic gradient method by Roux et al. 2012 for the task of multinomial logistic regression with random Fourier features). Rahimi & Recht (2009) showed that it allows us to effectively learn nonlinear models and still obtain good learning guarantees.", "startOffset": 3, "endOffset": 195}, {"referenceID": 43, "context": "Lopez-Paz et al. (2014) have recently applied the random feature idea to KCCA, by extracting M dimensional random Fourier features {(\u03c6x(xi), \u03c6y(yi))}i=1 for both views and solving exactly a linear CCA on the transformed pairs.", "startOffset": 0, "endOffset": 24}, {"referenceID": 32, "context": "Notice, however, in contrast to the classification or regression objectives that are more commonly used with random Fourier features (Rahimi & Recht, 2009; Huang et al., 2014; Lu et al., 2015b), the CCA objective (1) can not be written as an unconstrained sum or expectation of losses incurred at each training sample (in fact all training samples are coupled together through the constraints).", "startOffset": 133, "endOffset": 193}, {"referenceID": 2, "context": "(2015c) have developed stochastic optimization algorithms, referred to as AppGrad (Augmented Approximate Gradient) and NOI (Nonlinear Orthogonal Iterations) respectively, for linear CCA and its deep neural network extension deep CCA (Andrew et al., 2013).", "startOffset": 233, "endOffset": 254}, {"referenceID": 31, "context": "Notice, however, in contrast to the classification or regression objectives that are more commonly used with random Fourier features (Rahimi & Recht, 2009; Huang et al., 2014; Lu et al., 2015b), the CCA objective (1) can not be written as an unconstrained sum or expectation of losses incurred at each training sample (in fact all training samples are coupled together through the constraints). As a result, stochastic gradient descent, which requires unbiased gradient estimates computed from small minibatches, is not directly applicable here. Fortunately, Ma et al. (2015); Wang et al.", "startOffset": 156, "endOffset": 576}, {"referenceID": 31, "context": "Notice, however, in contrast to the classification or regression objectives that are more commonly used with random Fourier features (Rahimi & Recht, 2009; Huang et al., 2014; Lu et al., 2015b), the CCA objective (1) can not be written as an unconstrained sum or expectation of losses incurred at each training sample (in fact all training samples are coupled together through the constraints). As a result, stochastic gradient descent, which requires unbiased gradient estimates computed from small minibatches, is not directly applicable here. Fortunately, Ma et al. (2015); Wang et al. (2015c) have developed stochastic optimization algorithms, referred to as AppGrad (Augmented Approximate Gradient) and NOI (Nonlinear Orthogonal Iterations) respectively, for linear CCA and its deep neural network extension deep CCA (Andrew et al.", "startOffset": 156, "endOffset": 597}, {"referenceID": 25, "context": "With this observation, Lu & Foster (2014) solve these least squares problems using randomized PCA (Halko et al., 2011) and a batch gradient algorithm.", "startOffset": 98, "endOffset": 118}, {"referenceID": 48, "context": "Although unbiased gradient estimates of these subproblems do not lead to unbiased gradient estimates of the original CCA objective, local convergence results (that the optimum of CCA is a fixed point of AppGrad, and the AppGrad iterate converges linearly to the optimal solution when started in its neighborhood) have been established for AppGrad (Ma et al., 2015).", "startOffset": 347, "endOffset": 364}, {"referenceID": 64, "context": ", Wang et al., 2015c, Section III. A for more details). With this observation, Lu & Foster (2014) solve these least squares problems using randomized PCA (Halko et al.", "startOffset": 2, "endOffset": 98}, {"referenceID": 25, "context": "With this observation, Lu & Foster (2014) solve these least squares problems using randomized PCA (Halko et al., 2011) and a batch gradient algorithm. Ma et al. (2015); Wang et al.", "startOffset": 99, "endOffset": 168}, {"referenceID": 25, "context": "With this observation, Lu & Foster (2014) solve these least squares problems using randomized PCA (Halko et al., 2011) and a batch gradient algorithm. Ma et al. (2015); Wang et al. (2015c) take a step further and replace the exact solutions to the least squares problems with efficient stochastic gradient descent updates.", "startOffset": 99, "endOffset": 189}, {"referenceID": 25, "context": "With this observation, Lu & Foster (2014) solve these least squares problems using randomized PCA (Halko et al., 2011) and a batch gradient algorithm. Ma et al. (2015); Wang et al. (2015c) take a step further and replace the exact solutions to the least squares problems with efficient stochastic gradient descent updates. Although unbiased gradient estimates of these subproblems do not lead to unbiased gradient estimates of the original CCA objective, local convergence results (that the optimum of CCA is a fixed point of AppGrad, and the AppGrad iterate converges linearly to the optimal solution when started in its neighborhood) have been established for AppGrad (Ma et al., 2015). It has also been observed that the stochastic algorithms converge fast to approximate solutions that are on par with the exact solution or solutions by batch-based optimizers. We give our stochastic optimization algorithm for approximate KCCA, named KNOI (Kernel Nonlinear Orthogonal Iterations), in Algorithm 1. Our algorithm is adapted from the NOI algorithm of Wang et al. (2015c), which allows the use of smaller minibatches (through the time constant \u03c1) than does the AppGrad algorithm of Ma et al.", "startOffset": 99, "endOffset": 1073}, {"referenceID": 25, "context": "With this observation, Lu & Foster (2014) solve these least squares problems using randomized PCA (Halko et al., 2011) and a batch gradient algorithm. Ma et al. (2015); Wang et al. (2015c) take a step further and replace the exact solutions to the least squares problems with efficient stochastic gradient descent updates. Although unbiased gradient estimates of these subproblems do not lead to unbiased gradient estimates of the original CCA objective, local convergence results (that the optimum of CCA is a fixed point of AppGrad, and the AppGrad iterate converges linearly to the optimal solution when started in its neighborhood) have been established for AppGrad (Ma et al., 2015). It has also been observed that the stochastic algorithms converge fast to approximate solutions that are on par with the exact solution or solutions by batch-based optimizers. We give our stochastic optimization algorithm for approximate KCCA, named KNOI (Kernel Nonlinear Orthogonal Iterations), in Algorithm 1. Our algorithm is adapted from the NOI algorithm of Wang et al. (2015c), which allows the use of smaller minibatches (through the time constant \u03c1) than does the AppGrad algorithm of Ma et al. (2015). In each iteration, KNOI adaptively estimates the covariance of the projections of each view (\u2208 R) using a convex combination (with \u03c1 \u2208 [0, 1)) of the previous estimate and the estimate based on the current minibatch,4 uses them to whiten the targets of the cross-view least squares regression problems, derives gradients from these problems,5 and finally updates the projection matrices (U,V) with momentum.", "startOffset": 99, "endOffset": 1200}, {"referenceID": 63, "context": "Empirically, we find that using momentum \u03bc \u2208 [0, 1) helps the algorithm to make rapid progress in the objective with a few passes over the training set, as observed by the deep learning community (Sutskever et al., 2013).", "startOffset": 196, "endOffset": 220}, {"referenceID": 36, "context": "There have been continuous efforts to scale up classical methods such as principal component analysis and partial least squares with stochastic/online updates (Krasulina, 1969; Oja & Karhunen, 1985; Warmuth & Kuzmin, 2008; Arora et al., 2012; 2013; Mitliagkas et al., 2013; Balsubramani et al., 2013; Shamir, 2015; Xie et al., 2015).", "startOffset": 159, "endOffset": 332}, {"referenceID": 3, "context": "There have been continuous efforts to scale up classical methods such as principal component analysis and partial least squares with stochastic/online updates (Krasulina, 1969; Oja & Karhunen, 1985; Warmuth & Kuzmin, 2008; Arora et al., 2012; 2013; Mitliagkas et al., 2013; Balsubramani et al., 2013; Shamir, 2015; Xie et al., 2015).", "startOffset": 159, "endOffset": 332}, {"referenceID": 51, "context": "There have been continuous efforts to scale up classical methods such as principal component analysis and partial least squares with stochastic/online updates (Krasulina, 1969; Oja & Karhunen, 1985; Warmuth & Kuzmin, 2008; Arora et al., 2012; 2013; Mitliagkas et al., 2013; Balsubramani et al., 2013; Shamir, 2015; Xie et al., 2015).", "startOffset": 159, "endOffset": 332}, {"referenceID": 10, "context": "There have been continuous efforts to scale up classical methods such as principal component analysis and partial least squares with stochastic/online updates (Krasulina, 1969; Oja & Karhunen, 1985; Warmuth & Kuzmin, 2008; Arora et al., 2012; 2013; Mitliagkas et al., 2013; Balsubramani et al., 2013; Shamir, 2015; Xie et al., 2015).", "startOffset": 159, "endOffset": 332}, {"referenceID": 73, "context": "There have been continuous efforts to scale up classical methods such as principal component analysis and partial least squares with stochastic/online updates (Krasulina, 1969; Oja & Karhunen, 1985; Warmuth & Kuzmin, 2008; Arora et al., 2012; 2013; Mitliagkas et al., 2013; Balsubramani et al., 2013; Shamir, 2015; Xie et al., 2015).", "startOffset": 159, "endOffset": 332}, {"referenceID": 49, "context": "(2015c) we use here, an intuitively similar approach is proposed in the context of alternating conditional expectation (Makur et al., 2015).", "startOffset": 119, "endOffset": 139}, {"referenceID": 3, "context": "There have been continuous efforts to scale up classical methods such as principal component analysis and partial least squares with stochastic/online updates (Krasulina, 1969; Oja & Karhunen, 1985; Warmuth & Kuzmin, 2008; Arora et al., 2012; 2013; Mitliagkas et al., 2013; Balsubramani et al., 2013; Shamir, 2015; Xie et al., 2015). The CCA objective is more challenging due to the constraints, as also pointed out by Arora et al. (2012). Avron et al.", "startOffset": 223, "endOffset": 439}, {"referenceID": 3, "context": "There have been continuous efforts to scale up classical methods such as principal component analysis and partial least squares with stochastic/online updates (Krasulina, 1969; Oja & Karhunen, 1985; Warmuth & Kuzmin, 2008; Arora et al., 2012; 2013; Mitliagkas et al., 2013; Balsubramani et al., 2013; Shamir, 2015; Xie et al., 2015). The CCA objective is more challenging due to the constraints, as also pointed out by Arora et al. (2012). Avron et al. (2013) propose an algorithm for selecting a subset of training samples that retain the most information for accelerating linear CCA, when there are many more training samples (large N ) than features (small M in our case).", "startOffset": 223, "endOffset": 460}, {"referenceID": 3, "context": "There have been continuous efforts to scale up classical methods such as principal component analysis and partial least squares with stochastic/online updates (Krasulina, 1969; Oja & Karhunen, 1985; Warmuth & Kuzmin, 2008; Arora et al., 2012; 2013; Mitliagkas et al., 2013; Balsubramani et al., 2013; Shamir, 2015; Xie et al., 2015). The CCA objective is more challenging due to the constraints, as also pointed out by Arora et al. (2012). Avron et al. (2013) propose an algorithm for selecting a subset of training samples that retain the most information for accelerating linear CCA, when there are many more training samples (large N ) than features (small M in our case). While this approach effectively reduces the training set size N , it provides no remedy for the large M scenario we face in approximate KCCA. In terms of online/stochastic CCA, Yger et al. (2012) propose an adaptive CCA algorithm with efficient online updates based on matrix manifolds defined by the constraints (and they use a similar form of adaptive estimates for the covariance matrices).", "startOffset": 223, "endOffset": 872}, {"referenceID": 3, "context": "There have been continuous efforts to scale up classical methods such as principal component analysis and partial least squares with stochastic/online updates (Krasulina, 1969; Oja & Karhunen, 1985; Warmuth & Kuzmin, 2008; Arora et al., 2012; 2013; Mitliagkas et al., 2013; Balsubramani et al., 2013; Shamir, 2015; Xie et al., 2015). The CCA objective is more challenging due to the constraints, as also pointed out by Arora et al. (2012). Avron et al. (2013) propose an algorithm for selecting a subset of training samples that retain the most information for accelerating linear CCA, when there are many more training samples (large N ) than features (small M in our case). While this approach effectively reduces the training set size N , it provides no remedy for the large M scenario we face in approximate KCCA. In terms of online/stochastic CCA, Yger et al. (2012) propose an adaptive CCA algorithm with efficient online updates based on matrix manifolds defined by the constraints (and they use a similar form of adaptive estimates for the covariance matrices). However, the goal of their algorithm is anomaly detection for streaming data with a varying distribution, rather than to perform CCA for a given dataset. Regarding the stochastic CCA algorithms of Ma et al. (2015); Wang et al.", "startOffset": 223, "endOffset": 1284}, {"referenceID": 3, "context": "There have been continuous efforts to scale up classical methods such as principal component analysis and partial least squares with stochastic/online updates (Krasulina, 1969; Oja & Karhunen, 1985; Warmuth & Kuzmin, 2008; Arora et al., 2012; 2013; Mitliagkas et al., 2013; Balsubramani et al., 2013; Shamir, 2015; Xie et al., 2015). The CCA objective is more challenging due to the constraints, as also pointed out by Arora et al. (2012). Avron et al. (2013) propose an algorithm for selecting a subset of training samples that retain the most information for accelerating linear CCA, when there are many more training samples (large N ) than features (small M in our case). While this approach effectively reduces the training set size N , it provides no remedy for the large M scenario we face in approximate KCCA. In terms of online/stochastic CCA, Yger et al. (2012) propose an adaptive CCA algorithm with efficient online updates based on matrix manifolds defined by the constraints (and they use a similar form of adaptive estimates for the covariance matrices). However, the goal of their algorithm is anomaly detection for streaming data with a varying distribution, rather than to perform CCA for a given dataset. Regarding the stochastic CCA algorithms of Ma et al. (2015); Wang et al. (2015c) we use here, an intuitively similar approach is proposed in the context of alternating conditional expectation (Makur et al.", "startOffset": 223, "endOffset": 1305}, {"referenceID": 3, "context": "There have been continuous efforts to scale up classical methods such as principal component analysis and partial least squares with stochastic/online updates (Krasulina, 1969; Oja & Karhunen, 1985; Warmuth & Kuzmin, 2008; Arora et al., 2012; 2013; Mitliagkas et al., 2013; Balsubramani et al., 2013; Shamir, 2015; Xie et al., 2015). The CCA objective is more challenging due to the constraints, as also pointed out by Arora et al. (2012). Avron et al. (2013) propose an algorithm for selecting a subset of training samples that retain the most information for accelerating linear CCA, when there are many more training samples (large N ) than features (small M in our case). While this approach effectively reduces the training set size N , it provides no remedy for the large M scenario we face in approximate KCCA. In terms of online/stochastic CCA, Yger et al. (2012) propose an adaptive CCA algorithm with efficient online updates based on matrix manifolds defined by the constraints (and they use a similar form of adaptive estimates for the covariance matrices). However, the goal of their algorithm is anomaly detection for streaming data with a varying distribution, rather than to perform CCA for a given dataset. Regarding the stochastic CCA algorithms of Ma et al. (2015); Wang et al. (2015c) we use here, an intuitively similar approach is proposed in the context of alternating conditional expectation (Makur et al., 2015). Another related approach is that of Xie et al. (2015), who propose the Doubly Stochastic Gradient Descent (DSGD) algorithm for approximate kernel machines (including KCCA) based on random Fourier features.", "startOffset": 223, "endOffset": 1492}, {"referenceID": 42, "context": "In the first set of experiments, we demonstrate the scability and efficiency of KNOI on the MNIST8M dataset (Loosli et al., 2007).", "startOffset": 108, "endOffset": 129}, {"referenceID": 42, "context": "In the first set of experiments, we demonstrate the scability and efficiency of KNOI on the MNIST8M dataset (Loosli et al., 2007). The dataset consists of 8.1 million 28 \u00d7 28 grayscale images of the digits 0-9. We divide each image into the left and right halves and use them as the two views in KCCA, so the input dimensionality is 392 for both views. The dataset is randomly split into training/test sets of size 8M/0.1M. The task is to learn L = 50 dimensional projections using KCCA, and the evaluation criterion is the total canonical correlation achieved on the test set (upperbounded by 50). The same task is used by Xie et al. (2015), although we use a different training/test split.", "startOffset": 109, "endOffset": 642}, {"referenceID": 42, "context": "In the first set of experiments, we demonstrate the scability and efficiency of KNOI on the MNIST8M dataset (Loosli et al., 2007). The dataset consists of 8.1 million 28 \u00d7 28 grayscale images of the digits 0-9. We divide each image into the left and right halves and use them as the two views in KCCA, so the input dimensionality is 392 for both views. The dataset is randomly split into training/test sets of size 8M/0.1M. The task is to learn L = 50 dimensional projections using KCCA, and the evaluation criterion is the total canonical correlation achieved on the test set (upperbounded by 50). The same task is used by Xie et al. (2015), although we use a different training/test split. As in Xie et al. (2015), we fix the kernel widths using the \u201cmedian\u201d trick6 for all algorithms.", "startOffset": 109, "endOffset": 716}, {"referenceID": 42, "context": "In the first set of experiments, we demonstrate the scability and efficiency of KNOI on the MNIST8M dataset (Loosli et al., 2007). The dataset consists of 8.1 million 28 \u00d7 28 grayscale images of the digits 0-9. We divide each image into the left and right halves and use them as the two views in KCCA, so the input dimensionality is 392 for both views. The dataset is randomly split into training/test sets of size 8M/0.1M. The task is to learn L = 50 dimensional projections using KCCA, and the evaluation criterion is the total canonical correlation achieved on the test set (upperbounded by 50). The same task is used by Xie et al. (2015), although we use a different training/test split. As in Xie et al. (2015), we fix the kernel widths using the \u201cmedian\u201d trick6 for all algorithms. We vary the rank M for FKCCA and NKCCA from 256 to 6000. For comparison, we use the same hyperparamters as those of Xie et al. (2015)7: data minibatch size b = 1024, feature minibatch size 2048, total number of random Fourier features M = 20480,8 and a decaying step size schedule.", "startOffset": 109, "endOffset": 922}, {"referenceID": 73, "context": "We use the Wisconsin X-ray microbeam (XRMB) Following Xie et al. (2015), kernel widths are estimated from the median of pairwise distances between 4000 randomly selected training samples.", "startOffset": 54, "endOffset": 72}, {"referenceID": 73, "context": "We use the Wisconsin X-ray microbeam (XRMB) Following Xie et al. (2015), kernel widths are estimated from the median of pairwise distances between 4000 randomly selected training samples. We thank the authors for providing their MATLAB implementation of DSGD. Xie et al. (2015) used a version of random Fourier features with both cos and sin functions, so the number of learnable parameters (in U and V) of DSGD is twice that of KNOI for the same M .", "startOffset": 54, "endOffset": 278}, {"referenceID": 78, "context": "(2015a;b), who used the HTK toolkit (Young et al., 1999), we use the Kaldi speech recognition toolkit (Povey et al.", "startOffset": 36, "endOffset": 56}, {"referenceID": 55, "context": ", 1999), we use the Kaldi speech recognition toolkit (Povey et al., 2011) for feature extraction and recognition with hidden Markov models.", "startOffset": 53, "endOffset": 73}, {"referenceID": 29, "context": "All of the learned feature types are used in a \u201ctandem\u201d speech recognition approach (Hermansky et al., 2000), i.", "startOffset": 84, "endOffset": 108}, {"referenceID": 29, "context": "All of the learned feature types are used in a \u201ctandem\u201d speech recognition approach (Hermansky et al., 2000), i.e., they are appended to the original 39D features and used in a standard hidden Markov model (HMM)-based recognizer with Gaussian mixture observation distributions. For each fold, we select the hyperparameters based on recognition accuracy on the tuning set. For each algorithm, the feature dimensionality L is tuned over {30, 50, 70}, and the kernel widths for each view are tuned by grid search. We initially set M = 5000 for FKCCA/NKCCA, and also test FKCCA at M = 30000 (the largest M at which we could afford to obtain an exact SVD solution on a workstation with 64G main memory) with kernel widths tuned at M = 5000; we could not obtain results for NKCCA with M = 30000 in 48 hours. For KNOI, we set M = 100000 and tune the optimization parameters on a rough grid. The tuned KNOI uses minibatch size b = 2500, time constant \u03c1 = 0, fixed learning rate \u03b7 = 0.01, and momentum \u03bc = 0.995. For this combination of b and M , we are able to run the algorithm on a Tesla K40 GPU (with 12G memory), and each epoch (one pass over the 1.43M training samples) takes only 7.3 minutes. We run KNOI for 5 epochs and use the resulting acoustic view projection for recognition. We have also tried to run KNOI for 10 epochs and the recognition performance does not change, even though the total canonical correlation keeps improving on both training and tuning sets. For comparison, we report the performance of a baseline recognizer that uses only the original MFCC features, and the performance of deep CCA (DCCA) as described in Wang et al. (2015b), which uses 3 hidden layers of 1500ReLU units followed by a linear output layer in the acoustic view, and only a linear output layer in the articulatory view.", "startOffset": 85, "endOffset": 1653}, {"referenceID": 29, "context": "All of the learned feature types are used in a \u201ctandem\u201d speech recognition approach (Hermansky et al., 2000), i.e., they are appended to the original 39D features and used in a standard hidden Markov model (HMM)-based recognizer with Gaussian mixture observation distributions. For each fold, we select the hyperparameters based on recognition accuracy on the tuning set. For each algorithm, the feature dimensionality L is tuned over {30, 50, 70}, and the kernel widths for each view are tuned by grid search. We initially set M = 5000 for FKCCA/NKCCA, and also test FKCCA at M = 30000 (the largest M at which we could afford to obtain an exact SVD solution on a workstation with 64G main memory) with kernel widths tuned at M = 5000; we could not obtain results for NKCCA with M = 30000 in 48 hours. For KNOI, we set M = 100000 and tune the optimization parameters on a rough grid. The tuned KNOI uses minibatch size b = 2500, time constant \u03c1 = 0, fixed learning rate \u03b7 = 0.01, and momentum \u03bc = 0.995. For this combination of b and M , we are able to run the algorithm on a Tesla K40 GPU (with 12G memory), and each epoch (one pass over the 1.43M training samples) takes only 7.3 minutes. We run KNOI for 5 epochs and use the resulting acoustic view projection for recognition. We have also tried to run KNOI for 10 epochs and the recognition performance does not change, even though the total canonical correlation keeps improving on both training and tuning sets. For comparison, we report the performance of a baseline recognizer that uses only the original MFCC features, and the performance of deep CCA (DCCA) as described in Wang et al. (2015b), which uses 3 hidden layers of 1500ReLU units followed by a linear output layer in the acoustic view, and only a linear output layer in the articulatory view. With this architecture, each epoch of DCCA takes about 8 minutes on a Tesla K40 GPU, on par with KNOI. Note that this DCCA architecture was tuned carefully for low PER rather than high canonical correlation. This architecture produces a total correlation of about 25 (out of a maximum of L = 70) on tuning data, while KNOI achieves 46.7. DCCA using deeper nonlinear networks for the second view can achieve even better total canonical correlation, but its PER performance then becomes significantly worse. Phone error rates (PERs) obtained by different algorithms are given in Table 2, where smaller PER indicates better recognition performance. It is clear that all CCA-based features significantly improve over the baseline. Also, a large M is necessary for KCCA to be competitive with deep neural network methods, which is consistent with the findings of Huang et al. (2014); Lu et al.", "startOffset": 85, "endOffset": 2690}, {"referenceID": 29, "context": "All of the learned feature types are used in a \u201ctandem\u201d speech recognition approach (Hermansky et al., 2000), i.e., they are appended to the original 39D features and used in a standard hidden Markov model (HMM)-based recognizer with Gaussian mixture observation distributions. For each fold, we select the hyperparameters based on recognition accuracy on the tuning set. For each algorithm, the feature dimensionality L is tuned over {30, 50, 70}, and the kernel widths for each view are tuned by grid search. We initially set M = 5000 for FKCCA/NKCCA, and also test FKCCA at M = 30000 (the largest M at which we could afford to obtain an exact SVD solution on a workstation with 64G main memory) with kernel widths tuned at M = 5000; we could not obtain results for NKCCA with M = 30000 in 48 hours. For KNOI, we set M = 100000 and tune the optimization parameters on a rough grid. The tuned KNOI uses minibatch size b = 2500, time constant \u03c1 = 0, fixed learning rate \u03b7 = 0.01, and momentum \u03bc = 0.995. For this combination of b and M , we are able to run the algorithm on a Tesla K40 GPU (with 12G memory), and each epoch (one pass over the 1.43M training samples) takes only 7.3 minutes. We run KNOI for 5 epochs and use the resulting acoustic view projection for recognition. We have also tried to run KNOI for 10 epochs and the recognition performance does not change, even though the total canonical correlation keeps improving on both training and tuning sets. For comparison, we report the performance of a baseline recognizer that uses only the original MFCC features, and the performance of deep CCA (DCCA) as described in Wang et al. (2015b), which uses 3 hidden layers of 1500ReLU units followed by a linear output layer in the acoustic view, and only a linear output layer in the articulatory view. With this architecture, each epoch of DCCA takes about 8 minutes on a Tesla K40 GPU, on par with KNOI. Note that this DCCA architecture was tuned carefully for low PER rather than high canonical correlation. This architecture produces a total correlation of about 25 (out of a maximum of L = 70) on tuning data, while KNOI achieves 46.7. DCCA using deeper nonlinear networks for the second view can achieve even better total canonical correlation, but its PER performance then becomes significantly worse. Phone error rates (PERs) obtained by different algorithms are given in Table 2, where smaller PER indicates better recognition performance. It is clear that all CCA-based features significantly improve over the baseline. Also, a large M is necessary for KCCA to be competitive with deep neural network methods, which is consistent with the findings of Huang et al. (2014); Lu et al. (2015b) when using random Fourier features for speech data (where the task is frame classification).", "startOffset": 85, "endOffset": 2709}, {"referenceID": 26, "context": "(2011) which is preferable for sparse inputs, and random features for dot product or polynomial kernels (Kar & Karnick, 2012; Hamid et al., 2014; Pennington et al., 2015), which have proven to be useful for different domains.", "startOffset": 104, "endOffset": 170}, {"referenceID": 54, "context": "(2011) which is preferable for sparse inputs, and random features for dot product or polynomial kernels (Kar & Karnick, 2012; Hamid et al., 2014; Pennington et al., 2015), which have proven to be useful for different domains.", "startOffset": 104, "endOffset": 170}, {"referenceID": 36, "context": "It is straightforward to incorporate in our algorithm the faster random features of Le et al. (2013) which can be generated (for view 1) in time O(NM log dx) instead of O(NMdx), or the Taylor features of Cotter et al.", "startOffset": 84, "endOffset": 101}, {"referenceID": 16, "context": "(2013) which can be generated (for view 1) in time O(NM log dx) instead of O(NMdx), or the Taylor features of Cotter et al. (2011) which is preferable for sparse inputs, and random features for dot product or polynomial kernels (Kar & Karnick, 2012; Hamid et al.", "startOffset": 110, "endOffset": 131}, {"referenceID": 16, "context": "(2013) which can be generated (for view 1) in time O(NM log dx) instead of O(NMdx), or the Taylor features of Cotter et al. (2011) which is preferable for sparse inputs, and random features for dot product or polynomial kernels (Kar & Karnick, 2012; Hamid et al., 2014; Pennington et al., 2015), which have proven to be useful for different domains. It is also worth exploring parallelization and multiple kernel learning strategies of Lu et al. (2015b) with random Fourier features to further bridge the gap between kernel methods and deep neural network methods.", "startOffset": 110, "endOffset": 454}], "year": 2016, "abstractText": "Kernel canonical correlation analysis (KCCA) is a nonlinear multi-view representation learning technique with broad applicability in statistics and machine learning. Although there is a closed-form solution for the KCCA objective, it involves solving an N \u00d7 N eigenvalue system where N is the training set size, making its computational requirements in both memory and time prohibitive for large-scale problems. Various approximation techniques have been developed for KCCA. A commonly used approach is to first transform the original inputs to an M -dimensional random feature space so that inner products in the feature space approximate kernel evaluations, and then apply linear CCA to the transformed inputs. In many applications, however, the dimensionality M of the random feature space may need to be very large in order to obtain a sufficiently good approximation; it then becomes challenging to perform the linear CCA step on the resulting very high-dimensional data matrices. We show how to use a stochastic optimization algorithm, recently proposed for linear CCA and its neuralnetwork extension, to further alleviate the computation requirements of approximate KCCA. This approach allows us to run approximate KCCA on a speech dataset with 1.4 million training samples and a random feature space of dimensionality M = 100000 on a typical workstation.", "creator": "LaTeX with hyperref package"}}}