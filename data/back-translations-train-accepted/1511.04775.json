{"id": "1511.04775", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2015", "title": "Expressive Recommender Systems through Normalized Nonnegative Models", "abstract": "We introduce normalized nonnegative models (NNM) for explorative data analysis. NNMs are partial convexifications of models from probability theory. We demonstrate their value at the example of item recommendation. We show that NNM-based recommender systems satisfy three criteria that all recommender systems should ideally satisfy: high predictive power, computational tractability, and expressive representations of users and items. Expressive user and item representations are important in practice to succinctly summarize the pool of customers and the pool of items. In NNMs, user representations are expressive because each user's preference can be regarded as normalized mixture of preferences of stereotypical users. The interpretability of item and user representations allow us to arrange properties of items (e.g., genres of movies or topics of documents) or users (e.g., personality traits) hierarchically.", "histories": [["v1", "Sun, 15 Nov 2015 22:39:58 GMT  (227kb,D)", "http://arxiv.org/abs/1511.04775v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["cyril j stark"], "accepted": true, "id": "1511.04775"}, "pdf": {"name": "1511.04775.pdf", "metadata": {"source": "CRF", "title": "Expressive recommender systems through normalized nonnegative models", "authors": ["Cyril J. Stark"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The reason why it could come to this point lies in the fact that both countries are a country in which most people are not in a position to be able to feel themselves. (1) There is a risk that people are in a position to feel themselves. (2) There is a risk that people are in a position to feel themselves. (3) There is a risk that people are in a position to feel themselves. (2) There is a risk that people are in a position to be able to feel themselves. (3) There is a risk that people are in a position to be in a position to feel themselves. (2) There is a risk that people are in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position in a position to be in a position to be in a position to be in a position in a position to be in a position in a position to be in a position in a position to be in a position to be in a position to be in a position to be in a position to be in a position in a position to be in a position to be in a position in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position in a position to be in a position in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be able to be in a position to be in a position to be in a position to be in a position to be"}, {"heading": "2 Notation", "text": "In practice, we know only a subset of the entries of R. To mark the known entries, we use the entries of R. To mark the known entries, we use the entries of R. To mark the known entries, we use the entries of R. A finite probability space is described in terms of an example space, i.e. (u, i) if Rui a priori is known. To mark the simplicity of probability, we use the simplicity of R. A finite probability space is described in terms of an example space, i.e., (u, i) if Rui a priori is known."}, {"heading": "3 Probability theory recap", "text": "According to Kolmogorov, a random experiment with finite sample space is described by the following threefold: \u2022 A sample space is an element of probability magnitude simplex. \u2022 A random variable is an element of probability magnitude simplex. \u2022 A random variable is an element of probability magnitude E, i.e. a function E, i.e. E, i.e. E, 1,..., Z, for any alphabet magnitude Z, N.We use P, E, Z to denote the probability of the event. Therefore, P, E, E, E, Z, P, P, E, 1, E, E, E, 1 (z, z) follows."}, {"heading": "4 Normalized nonnegative models", "text": "The system we are interested in is the part of our mind that determines the result of the question \"Do you like item i?\" (i-i-i). Each question \"Do you like item i?\" is described in alphabet with reference to a random variable E-i (Z-5 for 5-star ratings). We use Pu [E-i = z] to denote the probability for the user u to value item i with z-z [Z]. Thus, Pu [E-i = z] = Pu [E-1i (z) = ~ E-T iz-pu (3) denotes the probability for the user u with z-z [Z-Z]."}, {"heading": "4.1 Non-categorical", "text": "In the previous description of the random variables E-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z"}, {"heading": "5 Interpretability", "text": "One of the reasons for the popularity of models from probability theory is their interpretability. To describe this in more detail, let us imagine how to flip a coin. The natural probable description of a single coin flip is in the form of an example room where the event heads are the event heads and the event heads are the event heads. We designate the deterministic distributions on the spectral heads or spectral heads. (For example: (~ \u03b4heads) \u03c9 = 1 if \u03c9 = spectral heads and (~ \u03b4heads) \u03c9 = 0 otherwise.) Therefore, we can describe the states simply and clearly: ~ \u03b4heads is the state that always returns heads, and ~ \u03b4tails is the state that always returns tails. Through these precise descriptions, we obtain an intuitive understanding of the states ~ \u03b4heads and ~ \u03b4tails. Any state ~ p = (heads, ptails) of the coin is an intuitive mix of intuition."}, {"heading": "5.1 Understanding stereotypes through tags", "text": "We often not only have access to player ratings, but we also have access to information about players in terms of labels. For example, the MovieLens 1M Dataset provides genre tags for movies; each movie is assigned (sometimes multiple) genres such as action, adventure, animation, etc. Note that we have an intuitive understanding of these genres - just as we have an intuitive understanding of coin states. So, using the example of genre tags next, we will explain how page information can be used to intuitively characterize stereotypes. We assume that each movie is assigned to some genres."}, {"heading": "5.2 Understanding stereotypes without tags", "text": "In the previous section, we proposed a method for the characterization of stereotypes. This method is always applicable when articles are provided with interpretable tags. What can we do if such tags are not available? Suppose we only have access to user reviews of articles. In these cases, we propose to characterize each of the stereotypical users on the basis of a list of articles they like. For a stereotype, these articles can be found by first collecting all articles with the property (~ EiZ) \u2248 1 (e.g. vectors associated with 5 stars). Let us name these highly rated films after the factor 1,..., \u0394 M. Then, in a second step, we select from the set {\u03b3\u03c91,..., \u0421\u0430\u043c\u0430\u043c\u0430\u043c\u0430\u043c\u0430\u043c\u0430\u043c1} those articles that are popular (i.e., many people)."}, {"heading": "5.3 Stereotypes in general matrix factorization models", "text": "Let us designate user vectors calculated in a general matrix factorization model. In principle, the calculation of the convex hull of the cone spanned by the set {~ ru} u [U] could determine user vectors ~ ri1,..., ~ riT with the property that for all ~ ru coefficients \u03bb1,..., \u03bbT \u2265 0 exist, so that, as in NNMs, we can express any user vector as a mixture of other user vectors. However, there are at least two major problems with this approach. Firstly, the calculation of the convex hull of the span of {~ ru} u [U] is not computationally tractable; even for a small number of users, we expect that the number of T of extreme rays R ~ ri1,..., R ~ riT of the convex hull of the span of {~ ru} u [U] could be very large; even for a small number of users, they are very difficult to interpret."}, {"heading": "6 Hierarchical structure of tags", "text": "For example, if the elements are movies, these markers could indicate which genre each movie belongs to. We designate the markers according to scale 1,..., itmt} all the elements that have a brand name from the totality of all brand names (6), so the likelihood is that the user does not like a randomly selected brand name. (6) allows us to classify brand names in a hierarchical manner. We note that if the brand names are not defined as brand names, the brand names cannot be defined as brand names. (5) We note that if the brand names are not defined as brand names, the brand names are not defined as brand names. (5) If the brand names are not defined, we cannot define the brand names as brand names."}, {"heading": "7 Computation of normalized nonnegative models", "text": "The simplest approach to calculating an NNM is probably via alternating restricted optimization to solvemin ~ pu, (~ Eiz) z, (Z), (u, i), (~ ETiz ~ pu \u2212 Rui / Z) 2, (9) i.e. the algorithm switches back and forth between optimization (~ pu) and [U] (retention (~ Eiz) i, (I], z [Z] and optimization (~ Eiz) i, (I], z [Z] (retention (~ pu) u [U]). Each of these tasks can be calculated efficiently and in parallel. Furthermore, this approach is guaranteed to converge to a local minimum. In the context of the recommendation, training data is typically subject to a large selection distortion: a large number of evaluations are high evaluations. This significantly affects the model that we fit to the data."}, {"heading": "8 Computational tractability", "text": "All the steps in algorithm 1 can be easily parallelized, the biggest bottleneck being the many sums in the objective functions of user and item updates. With standard methods, this becomes a potential problem during the first two iterations (i.e. pre-processing). There are at least two loopholes. The simplest solution is to scan a fixed number of unknown entries and replace only those with zeros. Here, the number of sampled entries should be comparable to the number of known entries, so that we can compensate for the selection distortions toward positive ratings. Alternatively, we can run algorithm 1 for a subset of all users and entries. We label these users and entries with {un} Nn = 1 or {in} Mm = 1, respectively, if N and M are equal to a few thousand, then we can easily execute algorithm 1; see section \"Experiments.\" How can we calculate representations of the remaining users and entries} To determine this phase {= 1, we can quickly {determine} the process for {1}."}, {"heading": "9 Experiments", "text": "We evaluate the predictability and interpretability of NNMs using the example of the omnipresent MovieLens (ML) 100K and 1M datasets.1 Due to the lack of space, we have moved the details of the configuration of all algorithms and results for the MovieLens 100K dataset into the appendix. To illustrate the interpretation capabilities of NNMs by the rating matrix R, we have called NNMs in Matlab using cvx [2]. We calculate Figure 1 to illustrate the interpretation capabilities of NNMs by user stereotypes. We calculate Figure 2 to evaluate the interpretation capabilities of NMs by user stereotypes."}, {"heading": "10 Related work", "text": "In fact, it is not as if you are able to follow the rules, but as if you follow the rules you follow. (...) It is not as if you follow the rules. (...) It is not as if you follow the rules. (...) It is not as if you follow the rules. (...) It is not as if you follow the rules. (...) It is not as if you follow the rules. (...) It is as if you follow the rules. (...) It is as if you follow the rules. (...) It is not as if you follow the rules. (...) It is not as if you follow the rules. (...) It is as if you follow the rules. (...) It is as if you do not follow the rules. (...) It is as if you do not follow the rules."}, {"heading": "11 Conclusion", "text": "We introduced NNMs using the example of Item Recommendation. We discussed the extent to which these models meet the predictive, traceable, and interpretable criteria that are ideally fulfilled by recommendation systems. NNMs \"main strength is their high degree of interpretability, which can be used to characterize user behavior in an interpretable way, and this quality can be used to derive hierarchical orders of item and user characteristics. Fortunately, as numerical experiments show, these properties of NNMs are not sacrificed at the price, predictability, or computational traceability, so we believe that NNMs will prove valuable in the recommendation and beyond."}, {"heading": "12 Acknowledgments", "text": "I would like to thank Patrick Pletscher and Sharon Wulff for interesting and fruitful discussions. I acknowledge the funding provided by the ARO funding contract W911NF-12-0486, and I acknowledge the funding provided by the Swiss National Science Foundation as part of a postdoctoral fellowship."}, {"heading": "A MAE vs RMSE", "text": "Suppose that the items under consideration are movies, and then it may happen that a user u likes the movie i but rates it poorly due to the particular reason that one of the actors is a member of Scientology. Thus, the reason for the poor rating is independent of the movie taste of the user u; it is rather a consequence of the fact that user u is atheist. It seems very unlikely that information of this kind will be captured by the few numbers we use to describe a person's movie taste. Moreover, in practice, we probably would not have enough data about the movie taste i to conclude that one of the actors is a member of science. Therefore, even extremely good models for measuring movie taste will make some predictions that are completely wrong - nothing else should be expected. In summary, if we measure the quality of a model in terms of a fault size, we may want to consider error metrics that are not too sensitive to a few outliers (i.e., predictions that are completely wrong)."}, {"heading": "B Configurations of algorithms", "text": "Table 2 specifies the configuration of algorithms as called by the Java LibRec library [4]."}], "references": [{"title": "Factorization meets the neighborhood: a multifaceted collaborative filtering model", "author": ["Yehuda Koren"], "venue": "In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "CVX: Matlab software for disciplined convex programming, version 2.1", "author": ["Michael Grant", "Stephen Boyd"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "SDPT3 a MATLAB software package for semidefinite programming, version 1.3", "author": ["Kim-Chuan Toh", "Michael J Todd", "Reha H T\u00fct\u00fcnc\u00fc"], "venue": "Optimization methods and software,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1999}, {"title": "Grouplens: an open architecture for collaborative filtering of netnews", "author": ["Paul Resnick", "Neophytos Iacovou", "Mitesh Suchak", "Peter Bergstrom", "John Riedl"], "venue": "In Proceedings of the 1994 ACM conference on Computer supported cooperative work,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1994}, {"title": "Bpr: Bayesian personalized ranking from implicit feedback", "author": ["Steffen Rendle", "Christoph Freudenthaler", "Zeno Gantner", "Lars Schmidt-Thieme"], "venue": "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Algorithms for non-negative matrix factorization", "author": ["Daniel D Lee", "H Sebastian Seung"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Latent class models for collaborative filtering", "author": ["Thomas Hofmann", "Jan Puzicha"], "venue": "In IJCAI,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1999}, {"title": "Probabilistic latent semantic indexing", "author": ["Thomas Hofmann"], "venue": "Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Recommender systems with social regularization", "author": ["Hao Ma", "Dengyong Zhou", "Chao Liu", "Michael R Lyu", "Irwin King"], "venue": "In Proceedings of the fourth ACM international conference on Web search and data mining,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "The nested chinese restaurant process and bayesian nonparametric inference of topic hierarchies", "author": ["David M Blei", "Thomas L Griffiths", "Michael I Jordan"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Nested hierarchical dirichlet processes", "author": ["John Paisley", "Chingyue Wang", "David M Blei", "Michael Jordan"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Probabilistic matrix factorization", "author": ["Andriy Mnih", "Ruslan Salakhutdinov"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Multi-label learning with millions of labels: Recommending advertiser bid phrases for web pages", "author": ["Rahul Agrawal", "Archit Gupta", "Yashoteja Prabhu", "Manik Varma"], "venue": "In Proceedings of the 22nd international conference on World Wide Web,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Multi-label prediction via compressed sensing", "author": ["Daniel Hsu", "Sham Kakade", "John Langford", "Tong Zhang"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Extracting shared subspace for multi-label classification", "author": ["Shuiwang Ji", "Lei Tang", "Shipeng Yu", "Jieping Ye"], "venue": "In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "We show that in mean-average-error, NNMs outperform methods like SVD++ [1] on MovieLens datasets.", "startOffset": 71, "endOffset": 74}, {"referenceID": 3, "context": ", z \u2208 [5] in case of 5-star ratings).", "startOffset": 6, "endOffset": 9}, {"referenceID": 0, "context": ", SVD++ [1]), and the potential possibilities motivate a thorough analysis of the modeling of categorical variables in terms of general matrix factorization.", "startOffset": 8, "endOffset": 11}, {"referenceID": 1, "context": ", g i ni ) i\u2208[I] and to all item vectors ( ~ Eiz ) i\u2208[I],z\u2208[Z] (z \u2208 [2]; z = 1 means \u2018like\u2019 and z = 2 means \u2018dislike\u2019).", "startOffset": 68, "endOffset": 71}, {"referenceID": 0, "context": "For every choice of \u03b5 \u2208 [0, 1], the graph G induces an approximate hierarchical ordering of tags; see figure 2.", "startOffset": 24, "endOffset": 30}, {"referenceID": 1, "context": "We computed NNMs in Matlab using cvx [2] calling SDPT3 [3].", "startOffset": 37, "endOffset": 40}, {"referenceID": 2, "context": "We computed NNMs in Matlab using cvx [2] calling SDPT3 [3].", "startOffset": 55, "endOffset": 58}, {"referenceID": 0, "context": "We notice that Algorithm 1 outperforms SVD++ [1] in MAE.", "startOffset": 45, "endOffset": 48}, {"referenceID": 3, "context": "MAE (100K) RMSE (100K) MAE (1M) RMSE (1M) UserKNN [5] 0.", "startOffset": 50, "endOffset": 53}, {"referenceID": 4, "context": "905 ItemKNN [6] 0.", "startOffset": 12, "endOffset": 15}, {"referenceID": 5, "context": "876 NMF [7] 0.", "startOffset": 8, "endOffset": 11}, {"referenceID": 0, "context": "920 SVD++ [1] 0.", "startOffset": 10, "endOffset": 13}, {"referenceID": 6, "context": "In aspect models, pLSA [8, 9] (and similarly in their extension LDA [10]) we model (u, i) as random variable with distribution", "startOffset": 23, "endOffset": 29}, {"referenceID": 7, "context": "In aspect models, pLSA [8, 9] (and similarly in their extension LDA [10]) we model (u, i) as random variable with distribution", "startOffset": 23, "endOffset": 29}, {"referenceID": 8, "context": "In aspect models, pLSA [8, 9] (and similarly in their extension LDA [10]) we model (u, i) as random variable with distribution", "startOffset": 68, "endOffset": 72}, {"referenceID": 9, "context": "Similarly, more general NMF-based models [11] agree with NNM-based models in that they make prediction in terms of inner products of nonnegative vectors but they differ from NNM-based models through different interpretations and regularizations of those nonnegative vectors.", "startOffset": 41, "endOffset": 45}, {"referenceID": 3, "context": "For instance, to model multiple outcomes like (z \u2208 [5]) we need to first decide on a particular graphical model (see section 2.", "startOffset": 51, "endOffset": 54}, {"referenceID": 6, "context": "3 in [8]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 10, "context": "Moreover, to extract hierarchical classifications of topics, we need to imagine generative processes like the nested Chinese restaurant process [12], or we need to employ nested hierarchical Dirichlet processes [13].", "startOffset": 144, "endOffset": 148}, {"referenceID": 11, "context": "Moreover, to extract hierarchical classifications of topics, we need to imagine generative processes like the nested Chinese restaurant process [12], or we need to employ nested hierarchical Dirichlet processes [13].", "startOffset": 211, "endOffset": 215}, {"referenceID": 12, "context": "On the other hand, probabilistic matrix factorization (PMF, [14]) leads to another interesting and related class of models where we assume that entries Rui of the rating matrix are independent Gaussian random variables with mean ~ U u ~ Vi and variance \u03c3.", "startOffset": 60, "endOffset": 64}, {"referenceID": 13, "context": "The evaluation of NNMs in the extreme multi-label setting [15, 16, 17, 18] is still outstanding.", "startOffset": 58, "endOffset": 74}, {"referenceID": 14, "context": "The evaluation of NNMs in the extreme multi-label setting [15, 16, 17, 18] is still outstanding.", "startOffset": 58, "endOffset": 74}, {"referenceID": 15, "context": "The evaluation of NNMs in the extreme multi-label setting [15, 16, 17, 18] is still outstanding.", "startOffset": 58, "endOffset": 74}], "year": 2015, "abstractText": "We introduce normalized nonnegative models (NNM) for explorative data analysis. NNMs are partial convexifications of models from probability theory. We demonstrate their value at the example of item recommendation. We show that NNM-based recommender systems satisfy three criteria that all recommender systems should ideally satisfy: high predictive power, computational tractability, and expressive representations of users and items. Expressive user and item representations are important in practice to succinctly summarize the pool of customers and the pool of items. In NNMs, user representations are expressive because each user\u2019s preference can be regarded as normalized mixture of preferences of stereotypical users. The interpretability of item and user representations allow us to arrange properties of items (e.g., genres of movies or topics of documents) or users (e.g., personality traits) hierarchically.", "creator": "LaTeX with hyperref package"}}}