{"id": "1511.04839", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2015", "title": "Nonparametric Canonical Correlation Analysis", "abstract": "Canonical correlation analysis (CCA) is a fundamental technique in multi-view data analysis and representation learning. Several nonlinear extensions of the classical linear CCA method have been proposed, including kernel and deep neural network methods. These approaches restrict attention to certain families of nonlinear projections, which the user must specify (by choosing a kernel or a neural network architecture), and are computationally demanding. Interestingly, the theory of nonlinear CCA without any functional restrictions, has been studied in the population setting by Lancaster already in the 50's. However, these results, have not inspired practical algorithms. In this paper, we revisit Lancaster's theory, and use it to devise a practical algorithm for nonparametric CCA (NCCA). Specifically, we show that the most correlated nonlinear projections of two random vectors can be expressed in terms of the singular value decomposition of a certain operator associated with their joint density. Thus, by estimating the population density from data, NCCA reduces to solving an eigenvalue system, superficially like kernel CCA but, importantly, without having to compute the inverse of any kernel matrix. We also derive a partially linear CCA (PLCCA) variant in which one of the views undergoes a linear projection while the other is nonparametric. PLCCA turns out to have a similar form to the classical linear CCA, but with a nonparametric regression term replacing the linear regression in CCA. Using a kernel density estimate based on a small number of nearest neighbors, our NCCA and PLCCA algorithms are memory-efficient, often run much faster, and achieve better performance than kernel CCA and comparable performance to deep CCA.", "histories": [["v1", "Mon, 16 Nov 2015 06:25:59 GMT  (753kb)", "https://arxiv.org/abs/1511.04839v1", "Submission to ICLR 2016"], ["v2", "Tue, 17 Nov 2015 16:26:17 GMT  (753kb)", "http://arxiv.org/abs/1511.04839v2", "Submission to ICLR 2016"], ["v3", "Sun, 10 Jan 2016 15:16:00 GMT  (754kb)", "http://arxiv.org/abs/1511.04839v3", "Submission to ICLR 2016"], ["v4", "Sun, 7 Feb 2016 16:11:45 GMT  (1436kb)", "http://arxiv.org/abs/1511.04839v4", null]], "COMMENTS": "Submission to ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["tomer michaeli", "weiran wang", "karen livescu"], "accepted": true, "id": "1511.04839"}, "pdf": {"name": "1511.04839.pdf", "metadata": {"source": "CRF", "title": "Nonparametric Canonical Correlation Analysis", "authors": ["Tomer Michaeli", "Weiran Wang", "Karen Livescu"], "emails": ["tomer.m@technion.ac.il", "weiranwang@ttic.edu", "klivescu@ttic.edu"], "sections": [{"heading": null, "text": "ar Xiv: 151 1.04 839v 4 [cs.L G] 7F eb"}, {"heading": "1 Introduction", "text": "A common task in data analysis is to show the frequent variability in multiple views of the same phenomenon while suppressing visual noise factors, and the resulting correlation analysis (CCA) is a classic statistical method aimed at achieving this goal. CCA, however, attempts to relate the resulting low-dimensional vectors to each other to the maximum extent. This tool has found widespread use in various areas, including its recent application to natural language processing [Dhillon et al.], speech recognition [Arora and Livescu, 2013], Genomics [Witten and Tibshirani, 2009], and cross-modal retrieval realized, 2014. One of CCA's shortcomings is its limitation to linear mapping, since many real-world data aspects of datasets."}, {"heading": "2 Background", "text": "We start by examining the original CCA algorithm in multiple senses: it maximizes mutual expectations for the distribution of information (RDx); we start by analyzing the original CCA algorithm in multiple senses. (Let X) RDx and Y (RDY) RDy are both random vectors (views); the goal in CCA is to find a pair of L-dimensional projections that are maximally correlated, but where different dimensions within each view must be uncorrelated. (W) RDY is that X and Y mean zero that the CCA problem as1max W1, W2 E [W '1 X) can be written. (W '2 Y) RDY is (W '2 Y)."}, {"heading": "3 Nonparametric and partially linear CCA", "text": "We deal with the following two variants of the nonlinear CCA problem (3): (i) nonparametric CCA, where both A and B are the set of all (nonparametric) measurable functions; (ii) partial linear CCA (PLCCA), where A is the set of all linear functions f (x) = WTx and B is the set of all (nonparametric) measurable functions g (y). We start by deriving solutions in closed form in the population, and then add an empirical estimate of p (x, y)."}, {"heading": "3.1 Nonparametric CCA (NCCA)", "text": "Let A and B be the sets of all (non-parametric) measurable functions of X and Y (respectively) (> p). Note that the coordinates of f (x) and g (y) are limited, E [f2i (X)] = E [g 2 i (Y)] = 1, so that we can write (3) as an optimization problem about the Hilbert spaceHx = {q: RDx \u2192 R [q2 (X)] < Hy = {u: RDy \u2192 R [u2] E [u2 (Y)] equipped with the internal products < q, r > Hx = E [q) and < u > Hy = E [u (Y) v > Hy = E [u (Y). To do this, we express the correlation between fi (X) and gi (Y) asE [fi]."}, {"heading": "3.2 Partially linear CCA (PLCCA)", "text": "The above NCCA derivatives can easily be adapted to cases in which A and B are different function families (1). Next, we derive PLCCA in which A is the quantity of all linear functions of X, while B is still the quantity of all (non-parametric) measurable functions of Y. (1) Let us allow f (x) = W x, in which W RDx x x L the number of x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x"}, {"heading": "3.3 Practical implementations", "text": "The NCCA and PLCCA solutions require knowledge of the common probability density p (x, y) of the views. In view of a set of training data (xi, yi) Ni = 1, drawn independently of p (x, y), we can estimate p (x, y) and incorporate it into our formulas. There are many ways to estimate this density. (14) Next, we present the algorithms resulting from the use of a particular choice, namely the nuclear density (KDEs) p (x) = 1N, Ni = 1 w (x \u2212 xi), (14) p (y), resulting from the use of a particular choice, namely the kernel density (KDEs) p (x, y) p) p, y = 1N."}, {"heading": "4 Related work", "text": "Several recent multiview learning algorithms use products or sums of affinity matrices, diffusion matrices or Markov transition matrices. The combined cores constructed in these methods are similar to our matrix S = WxWy. Such an approach has been used, for example, for multiview spectral clusters [de Sa, 2005, Zhou and Burges, 2007, Kumar et al., 2011], metric fusion [Wang et al., 2012], common manifold learning [Lederman and Talmon, 2014], and multilinear system identification [Boots and Gordon, 2012]. Note, however, that the matrix S in NCCA only corresponds to the product WxWy if a separable Gaussian core is used to estimate the joint density p (x, y). However, if a non-separable density estimate is used, then the matrix S no longer resembles the previously proposed multiview cores. Moreover, the motivation between these views is not entirely similar to CCA, although a correction results in CCA."}, {"heading": "5 Experiments", "text": "In the following experiments we compare PLCCA / NCCA with linear CCA, two kernel CCA approximations based on random Fourier characteristics (FKCCA, Lopez-Paz et al. [2014]) and Nystro-m approximation (NKCCA, Williams and Seeger [2001]), as in Wang et al. [2015b], and deep CCA (DCCA, Andrew et al. [2013]).Illustrative example We begin with the 2D synthetic data (1000 training samples) in Fig. 2 (a, b), where samples of the two input manifolds are colored according to their common degree of freedom. Clearly, a linear mapping in view 1 cannot unfold the diversity to the two views, and linear CCA actually fails (results are not shown).We extract a one-dimensional projection for each view using other nonlinear CCAs and draw the projection (y) vs. (x) of data (different distribution)."}, {"heading": "6 Conclusion", "text": "We have presented closed-loop solutions to the non-parametric CCA problems (NCCA) and the partially linear CCA problems (PLCCA). Unlike the kernel CCA, which limits non-parametric projections to a predefined RKHS, we have dealt with the unconstrained environment. We have shown that the optimal non-parametric predictions can be derived from the SVD of a core defined by the pointed mutual information between views, resulting in a simple algorithm that outperforms KCCA and matches deep CCA on multiple datasets, while being more computationally efficient than one of the two for medium-sized datasets."}, {"heading": "A Proof of Lemma 3.1", "text": "Let the composition of the second order moment be E [f (X) | Y] as E [f (X) | Y] as E [f (X) | Y] and g [f (Y) = A g (Y) as E [f (Y). Then the target can be written in (3) as E [f (X) g (Y) = E [f (Y)] as E [f (Y)]. Similarly, the constraint I = E [g (Y) g (Y) = Y] = E [A A [f (X) | Y] can be expressed as E (A g (Y) | Y] as E [f)."}, {"heading": "Acknowledgement", "text": "Thanks to Nathan Srebro, Ryota Tomioka and Yochai Blau for a fruitful discussion. This research was supported by the NSF grant IIS-1321015. The opinions expressed in this work are those of the authors and do not necessarily reflect the views of the funding agency."}], "references": [{"title": "A kernel method for canonical correlation analysis", "author": ["S. Akaho"], "venue": "In Proceedings of the International Meeting of the Psychometric Society (IMPS2001). Springer-Verlag,", "citeRegEx": "Akaho.,? \\Q2001\\E", "shortCiteRegEx": "Akaho.", "year": 2001}, {"title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "author": ["A. Andoni", "P. Indyk"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Andoni and Indyk.,? \\Q2006\\E", "shortCiteRegEx": "Andoni and Indyk.", "year": 2006}, {"title": "Deep canonical correlation analysis", "author": ["G. Andrew", "R. Arora", "J. Bilmes", "K. Livescu"], "venue": "In Proc. of the 30th Int. Conf. Machine Learning (ICML", "citeRegEx": "Andrew et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Andrew et al\\.", "year": 2013}, {"title": "Kernel CCA for multi-view learning of acoustic features using articulatory measurements", "author": ["R. Arora", "K. Livescu"], "venue": "In Symposium on Machine Learning in Speech and Language Processing (MLSLP),", "citeRegEx": "Arora and Livescu.,? \\Q2012\\E", "shortCiteRegEx": "Arora and Livescu.", "year": 2012}, {"title": "Multi-view CCA-based acoustic features for phonetic recognition across speakers and domains", "author": ["R. Arora", "K. Livescu"], "venue": "In Proc. of the IEEE Int. Conf. Acoustics, Speech and Sig. Proc. (ICASSP\u201913),", "citeRegEx": "Arora and Livescu.,? \\Q2013\\E", "shortCiteRegEx": "Arora and Livescu.", "year": 2013}, {"title": "An optimal algorithm for approximate nearest neighbor searching fixed dimensions", "author": ["S. Arya", "D.M. Mount", "N.S. Netanyahu", "R. Silverman", "A.Y. Wu"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Arya et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Arya et al\\.", "year": 1998}, {"title": "Kernel independent component analysis", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bach and Jordan.,? \\Q2002\\E", "shortCiteRegEx": "Bach and Jordan.", "year": 2002}, {"title": "A probabilistic interpretation of canonical correlation analysis", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "Technical Report 688,", "citeRegEx": "Bach and Jordan.,? \\Q2005\\E", "shortCiteRegEx": "Bach and Jordan.", "year": 2005}, {"title": "Sparse additive functional and kernel CCA", "author": ["S. Balakrishnan", "K. Puniyani", "J. Lafferty"], "venue": "In Proc. of the 29th Int. Conf. Machine Learning (ICML", "citeRegEx": "Balakrishnan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Balakrishnan et al\\.", "year": 2012}, {"title": "Spectral Clustering and Biclustering: Learning Large Graphs and Contingency Tables", "author": ["M. Bolla"], "venue": null, "citeRegEx": "Bolla.,? \\Q2013\\E", "shortCiteRegEx": "Bolla.", "year": 2013}, {"title": "Two manifold problems with applications to nonlinear system identification", "author": ["B. Boots", "G. Gordon"], "venue": "In Proc. of the 29th Int. Conf. Machine Learning (ICML", "citeRegEx": "Boots and Gordon.,? \\Q2012\\E", "shortCiteRegEx": "Boots and Gordon.", "year": 2012}, {"title": "Canonical correlation: A tutorial", "author": ["M. Borga"], "venue": null, "citeRegEx": "Borga.,? \\Q2001\\E", "shortCiteRegEx": "Borga.", "year": 2001}, {"title": "Estimating optimal transformations for multiple regression and correlation", "author": ["L. Breiman", "J.H. Friedman"], "venue": "Journal of the American statistical Association,", "citeRegEx": "Breiman and Friedman.,? \\Q1985\\E", "shortCiteRegEx": "Breiman and Friedman.", "year": 1985}, {"title": "LIBSVM: A library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Trans. Intelligent Systems and Technology,", "citeRegEx": "Chang and Lin.,? \\Q2011\\E", "shortCiteRegEx": "Chang and Lin.", "year": 2011}, {"title": "Information bottleneck for Gaussian variables", "author": ["G. Chechik", "A. Globerson", "N. Tishby", "Y. Weiss"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Chechik et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chechik et al\\.", "year": 2005}, {"title": "Spectral clustering with two views", "author": ["V. de Sa"], "venue": "In Workshop on Learning with Multiple Views", "citeRegEx": "Sa.,? \\Q2005\\E", "shortCiteRegEx": "Sa.", "year": 2005}, {"title": "Multi-view learning of word embeddings via CCA", "author": ["P. Dhillon", "D. Foster", "L. Ungar"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Dhillon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dhillon et al\\.", "year": 2011}, {"title": "Polynomial expansions of bivariate distributions", "author": ["G. Eagleson"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Eagleson.,? \\Q1964\\E", "shortCiteRegEx": "Eagleson.", "year": 1964}, {"title": "MMSE whitening and subspace whitening", "author": ["Y.C. Eldar", "A.V. Oppenheim"], "venue": "IEEE Trans. Info. Theory,", "citeRegEx": "Eldar and Oppenheim.,? \\Q2003\\E", "shortCiteRegEx": "Eldar and Oppenheim.", "year": 2003}, {"title": "Mean shift based clustering in high dimensions: A texture classification example", "author": ["B. Georgescu", "I. Shimshoni", "P. Meer"], "venue": "In Proc. 9th Int. Conf. Computer Vision (ICCV\u201903),", "citeRegEx": "Georgescu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Georgescu et al\\.", "year": 2003}, {"title": "Improving image-sentence embeddings using large weakly annotated photo collections", "author": ["Y. Gong", "L. Wang", "M. Hodosh", "J. Hockenmaier", "S. Lazebnik"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "An algorithm for the principal component analysis of large data sets", "author": ["N. Halko", "P.-G. Martinsson", "Y. Shkolnisky", "M. Tygert"], "venue": "SIAM J. Sci. Comput.,", "citeRegEx": "Halko et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Halko et al\\.", "year": 2011}, {"title": "The general theory of canonical correlation and its relation to functional analysis", "author": ["E.J. Hannan"], "venue": "Journal of the Australian Mathematical Society,", "citeRegEx": "Hannan.,? \\Q1961\\E", "shortCiteRegEx": "Hannan.", "year": 1961}, {"title": "Canonical correlation analysis: An overview with application to learning methods", "author": ["D.R. Hardoon", "S. Szedmak", "J. Shawe-Taylor"], "venue": "Neural Computation,", "citeRegEx": "Hardoon et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hardoon et al\\.", "year": 2004}, {"title": "Relations between two sets of variates", "author": ["H. Hotelling"], "venue": "Biometrika, 28(3/4):321\u2013377,", "citeRegEx": "Hotelling.,? \\Q1936\\E", "shortCiteRegEx": "Hotelling.", "year": 1936}, {"title": "Co-regularized multi-view spectral clustering", "author": ["A. Kumar", "P. Rai", "H. Daum\u00e9 III"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Kumar et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2011}, {"title": "Kernel and nonlinear canonical correlation analysis", "author": ["P.L. Lai", "C. Fyfe"], "venue": "Int. J. Neural Syst.,", "citeRegEx": "Lai and Fyfe.,? \\Q2000\\E", "shortCiteRegEx": "Lai and Fyfe.", "year": 2000}, {"title": "The structure of bivariate distributions", "author": ["H. Lancaster"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Lancaster.,? \\Q1958\\E", "shortCiteRegEx": "Lancaster.", "year": 1958}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proc. IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Common manifold learning using alternating-diffusion", "author": ["R.R. Lederman", "R. Talmon"], "venue": "Technical Report YALEU/DCS/TR-1497,", "citeRegEx": "Lederman and Talmon.,? \\Q2014\\E", "shortCiteRegEx": "Lederman and Talmon.", "year": 2014}, {"title": "Randomized nonlinear component analysis", "author": ["D. Lopez-Paz", "S. Sra", "A. Smola", "Z. Ghahramani", "B. Schoelkopf"], "venue": "In Proc. of the 31st Int. Conf. Machine Learning (ICML", "citeRegEx": "Lopez.Paz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lopez.Paz et al\\.", "year": 2014}, {"title": "An efficient algorithm for information decomposition and extraction", "author": ["A. Makur", "F. Kozynski", "S.-L. Huang", "L. Zheng"], "venue": "In 53rd Annual Allerton Conference on Communication, Control, and Computing,", "citeRegEx": "Makur et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Makur et al\\.", "year": 2015}, {"title": "Multivariate Analysis", "author": ["K.V. Mardia", "J.T. Kent", "J.M. Bibby"], "venue": null, "citeRegEx": "Mardia et al\\.,? \\Q1979\\E", "shortCiteRegEx": "Mardia et al\\.", "year": 1979}, {"title": "Nonlinear feature extraction using generalized canonical correlation analysis", "author": ["T. Melzer", "M. Reiter", "H. Bischof"], "venue": "In Proc. of the 11th Int. Conf. Artificial Neural Networks", "citeRegEx": "Melzer et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Melzer et al\\.", "year": 2001}, {"title": "On estimating regression", "author": ["E.A. Nadaraya"], "venue": "Theory of Probability & Its Applications,", "citeRegEx": "Nadaraya.,? \\Q1964\\E", "shortCiteRegEx": "Nadaraya.", "year": 1964}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In Proc. of the 27th Int. Conf. Machine Learning (ICML", "citeRegEx": "Nair and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "Submanifold density estimation", "author": ["A. Ozakin", "A. Gray"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Ozakin and Gray.,? \\Q2009\\E", "shortCiteRegEx": "Ozakin and Gray.", "year": 2009}, {"title": "Variable kernel density estimation", "author": ["G.R. Terrell", "D.W. Scott"], "venue": "The Annals of Statistics,", "citeRegEx": "Terrell and Scott.,? \\Q1992\\E", "shortCiteRegEx": "Terrell and Scott.", "year": 1992}, {"title": "Visualizing data using t-SNE", "author": ["L.J.P. van der Maaten", "G.E. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten and Hinton.,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Unsupervised metric fusion by cross diffusion", "author": ["B. Wang", "J. Jiang", "W. Wang", "Z.-H. Zhou", "Z. Tu"], "venue": "In Proc. of the 2012 IEEE Computer Society Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Unsupervised learning of acoustic features via deep canonical correlation analysis", "author": ["W. Wang", "R. Arora", "K. Livescu", "J. Bilmes"], "venue": "In Proc. of the IEEE Int. Conf. Acoustics, Speech and Sig. Proc. (ICASSP\u201915),", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "On deep multi-view representation learning", "author": ["W. Wang", "R. Arora", "K. Livescu", "J. Bilmes"], "venue": "In Proc. of the 32st Int. Conf. Machine Learning (ICML 2015),", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Smooth regression analysis", "author": ["G.S. Watson"], "venue": "Sankhya\u0304: The Indian Journal of Statistics,", "citeRegEx": "Watson.,? \\Q1964\\E", "shortCiteRegEx": "Watson.", "year": 1964}, {"title": "X-Ray Microbeam Speech Production", "author": ["J.R. Westbury"], "venue": "Database User\u2019s Handbook Version", "citeRegEx": "Westbury.,? \\Q1994\\E", "shortCiteRegEx": "Westbury.", "year": 1994}, {"title": "Using the Nystr\u00f6m method to speed up kernel machines", "author": ["C.K.I. Williams", "M. Seeger"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Williams and Seeger.,? \\Q2001\\E", "shortCiteRegEx": "Williams and Seeger.", "year": 2001}, {"title": "Extensions of sparse canonical correlation analysis with applications to genomic data", "author": ["D.M. Witten", "R.J. Tibshirani"], "venue": "Statistical applications in genetics and molecular biology,", "citeRegEx": "Witten and Tibshirani.,? \\Q2009\\E", "shortCiteRegEx": "Witten and Tibshirani.", "year": 2009}, {"title": "Spectral clustering and transductive learning with multiple views", "author": ["D. Zhou", "C.J.C. Burges"], "venue": "In Proc. of the 24th Int. Conf. Machine Learning", "citeRegEx": "Zhou and Burges.,? \\Q2007\\E", "shortCiteRegEx": "Zhou and Burges.", "year": 2007}], "referenceMentions": [{"referenceID": 24, "context": "Canonical correlation analysis (CCA) [Hotelling, 1936] is a classical statistical technique that targets this goal.", "startOffset": 37, "endOffset": 54}, {"referenceID": 16, "context": "This tool has found widespread use in various fields, including recent application to natural language processing [Dhillon et al., 2011], speech recognition [Arora and Livescu, 2013], genomics [Witten and Tibshirani, 2009], and cross-modal retrieval [Gong et al.", "startOffset": 114, "endOffset": 136}, {"referenceID": 4, "context": ", 2011], speech recognition [Arora and Livescu, 2013], genomics [Witten and Tibshirani, 2009], and cross-modal retrieval [Gong et al.", "startOffset": 28, "endOffset": 53}, {"referenceID": 45, "context": ", 2011], speech recognition [Arora and Livescu, 2013], genomics [Witten and Tibshirani, 2009], and cross-modal retrieval [Gong et al.", "startOffset": 64, "endOffset": 93}, {"referenceID": 20, "context": ", 2011], speech recognition [Arora and Livescu, 2013], genomics [Witten and Tibshirani, 2009], and cross-modal retrieval [Gong et al., 2014].", "startOffset": 121, "endOffset": 140}, {"referenceID": 2, "context": "In deep CCA (DCCA) [Andrew et al., 2013], the projections are obtained from two deep neural networks that are trained to output maximally correlated vectors.", "startOffset": 19, "endOffset": 40}, {"referenceID": 0, "context": "In kernel CCA (KCCA) [Akaho, 2001, Melzer et al., 2001, Bach and Jordan, 2002, Hardoon et al., 2004], these nonlinear mappings are chosen from two reproducing kernel Hilbert spaces (RKHS). In deep CCA (DCCA) [Andrew et al., 2013], the projections are obtained from two deep neural networks that are trained to output maximally correlated vectors. Nonparametric CCA-type methods, which are not limited to specific function classes, include the alternating conditional expectations (ACE) algorithm and its extensions [Breiman and Friedman, 1985, Balakrishnan et al., 2012, Makur et al., 2015]. Nonlinear CCA methods are advantageous over linear CCA in a range of applications [Hardoon et al., 2004, Melzer et al., 2001, Wang et al., 2015b]. However, existing nonlinear CCA approaches are very computationally demanding, and are often impractical to apply on large data. Interestingly, the problem of finding the most correlated nonlinear projections of two random variables has been studied by Lancaster [1958] and Hannan [1961], long before the derivation of KCCA, DCCA and ACE.", "startOffset": 22, "endOffset": 1009}, {"referenceID": 0, "context": "In kernel CCA (KCCA) [Akaho, 2001, Melzer et al., 2001, Bach and Jordan, 2002, Hardoon et al., 2004], these nonlinear mappings are chosen from two reproducing kernel Hilbert spaces (RKHS). In deep CCA (DCCA) [Andrew et al., 2013], the projections are obtained from two deep neural networks that are trained to output maximally correlated vectors. Nonparametric CCA-type methods, which are not limited to specific function classes, include the alternating conditional expectations (ACE) algorithm and its extensions [Breiman and Friedman, 1985, Balakrishnan et al., 2012, Makur et al., 2015]. Nonlinear CCA methods are advantageous over linear CCA in a range of applications [Hardoon et al., 2004, Melzer et al., 2001, Wang et al., 2015b]. However, existing nonlinear CCA approaches are very computationally demanding, and are often impractical to apply on large data. Interestingly, the problem of finding the most correlated nonlinear projections of two random variables has been studied by Lancaster [1958] and Hannan [1961], long before the derivation of KCCA, DCCA and ACE.", "startOffset": 22, "endOffset": 1027}, {"referenceID": 24, "context": "2 Background We start by reviewing the original CCA algorithm [Hotelling, 1936].", "startOffset": 62, "endOffset": 79}, {"referenceID": 11, "context": "This objective has been extensively studied and is known to be optimal in several senses: It maximizes the mutual information for certain distributions p(x,y) [Borga, 2001], maximizes the likelihood for certain latent variable models [Bach and Jordan, 2005], and is equivalent to the information bottleneck method when p(x,y) is Gaussian [Chechik et al.", "startOffset": 159, "endOffset": 172}, {"referenceID": 7, "context": "This objective has been extensively studied and is known to be optimal in several senses: It maximizes the mutual information for certain distributions p(x,y) [Borga, 2001], maximizes the likelihood for certain latent variable models [Bach and Jordan, 2005], and is equivalent to the information bottleneck method when p(x,y) is Gaussian [Chechik et al.", "startOffset": 234, "endOffset": 257}, {"referenceID": 14, "context": "This objective has been extensively studied and is known to be optimal in several senses: It maximizes the mutual information for certain distributions p(x,y) [Borga, 2001], maximizes the likelihood for certain latent variable models [Bach and Jordan, 2005], and is equivalent to the information bottleneck method when p(x,y) is Gaussian [Chechik et al., 2005].", "startOffset": 338, "endOffset": 360}, {"referenceID": 32, "context": "The CCA solution can be expressed as (W1,W2) = (\u03a3 \u22121/2 xx U,\u03a3 \u22121/2 yy V), where \u03a3xx = E[XX \u22a4], \u03a3yy = E[Y Y \u22a4], \u03a3xy = E[XY \u22a4], and U \u2208 RDx\u00d7L and V \u2208 RDy\u00d7L are the top L left and right singular vectors of the matrix T = \u03a3\u22121/2 xx \u03a3xy\u03a3 \u22121/2 yy (see [Mardia et al., 1979]).", "startOffset": 245, "endOffset": 266}, {"referenceID": 23, "context": ", Hardoon et al. [2004]).", "startOffset": 2, "endOffset": 24}, {"referenceID": 12, "context": "Alternating conditional expectations (ACE): The ACE method [Breiman and Friedman, 1985] treats the case of a single projection (L = 1), where B is the class of all zero-mean scalar-valued functions g(Y ), and A is the class of additive models f(X) = \u2211Dx l=1 \u03b3l\u03c6l(Xl) with zero-mean scalar-valued functions \u03c6l(Xl).", "startOffset": 59, "endOffset": 87}, {"referenceID": 12, "context": "Alternating conditional expectations (ACE): The ACE method [Breiman and Friedman, 1985] treats the case of a single projection (L = 1), where B is the class of all zero-mean scalar-valued functions g(Y ), and A is the class of additive models f(X) = \u2211Dx l=1 \u03b3l\u03c6l(Xl) with zero-mean scalar-valued functions \u03c6l(Xl). The ACE algorithm minimizes the objective (3) by iteratively computing the conditional expectation of each view given the other. Recently, Makur et al. [2015] extended ACE to multiple dimensions by whitening the vector-valued f(X) and g(Y ) during each iteration.", "startOffset": 60, "endOffset": 473}, {"referenceID": 2, "context": "Deep CCA (DCCA): In the more recently proposed DCCA approach [Andrew et al., 2013], A and B are the families of functions that can be implemented using two deep neural networks of predefined architecture.", "startOffset": 61, "endOffset": 82}, {"referenceID": 25, "context": "Population solutions: Lancaster [1958] studied a variant of problem (3), where A and B are the families of all measurable functions.", "startOffset": 22, "endOffset": 39}, {"referenceID": 17, "context": "Eagleson [1964] extended this analysis to the Gamma, Poisson, binomial, negative binomial, and hypergeometric distributions.", "startOffset": 0, "endOffset": 16}, {"referenceID": 17, "context": "Eagleson [1964] extended this analysis to the Gamma, Poisson, binomial, negative binomial, and hypergeometric distributions. Hannan [1961] gave Lancaster\u2019s characterization a functional analysis interpretation, which confirmed its validity also for multi-dimensional views.", "startOffset": 0, "endOffset": 139}, {"referenceID": 18, "context": "As shown 5A simpler version of this lemma, in which f(x) = y and g is linear, appeared in Eldar and Oppenheim [2003]. The proof of Lemma 3.", "startOffset": 90, "endOffset": 117}, {"referenceID": 36, "context": "in [Ozakin and Gray, 2009], if the data lies on an r-dimensional manifold, then the KDE converges to the true density at a rate of6 O(n\u2212 4 r+4 ).", "startOffset": 3, "endOffset": 26}, {"referenceID": 19, "context": "Indeed, KDEs have been shown to work well in practice in relatively high dimensions [Georgescu et al., 2003], as is also confirmed in our experiments.", "startOffset": 84, "endOffset": 108}, {"referenceID": 44, "context": "To obtain out-of-sample mapping for a new view 1 test sample x, we use the Nystr\u00f6m method [Williams and Seeger, 2001], which avoids recomputing SVD.", "startOffset": 90, "endOffset": 117}, {"referenceID": 39, "context": ", 2011], metric fusion [Wang et al., 2012], common manifold learning [Lederman and Talmon, 2014], and", "startOffset": 23, "endOffset": 42}, {"referenceID": 29, "context": ", 2012], common manifold learning [Lederman and Talmon, 2014], and", "startOffset": 34, "endOffset": 61}, {"referenceID": 10, "context": "multi-view nonlinear system identification [Boots and Gordon, 2012].", "startOffset": 43, "endOffset": 67}, {"referenceID": 29, "context": "5 Experiments In the following experiments, we compare PLCCA/NCCA with linear CCA, two kernel CCA approximations using random Fourier features (FKCCA, Lopez-Paz et al. [2014]) and Nystr\u00f6m approximation (NKCCA, Williams and Seeger [2001]) as described in Wang et al.", "startOffset": 151, "endOffset": 175}, {"referenceID": 29, "context": "5 Experiments In the following experiments, we compare PLCCA/NCCA with linear CCA, two kernel CCA approximations using random Fourier features (FKCCA, Lopez-Paz et al. [2014]) and Nystr\u00f6m approximation (NKCCA, Williams and Seeger [2001]) as described in Wang et al.", "startOffset": 151, "endOffset": 237}, {"referenceID": 29, "context": "5 Experiments In the following experiments, we compare PLCCA/NCCA with linear CCA, two kernel CCA approximations using random Fourier features (FKCCA, Lopez-Paz et al. [2014]) and Nystr\u00f6m approximation (NKCCA, Williams and Seeger [2001]) as described in Wang et al. [2015b], and deep CCA (DCCA, Andrew et al.", "startOffset": 151, "endOffset": 274}, {"referenceID": 2, "context": "[2015b], and deep CCA (DCCA, Andrew et al. [2013]).", "startOffset": 29, "endOffset": 50}, {"referenceID": 43, "context": "X-Ray Microbeam Speech Data The University of Wisconsin X-Ray Micro-Beam (XRMB) corpus [Westbury, 1994] consists of simultaneously recorded speech and articulatory measurements.", "startOffset": 87, "endOffset": 103}, {"referenceID": 30, "context": "As in [Lopez-Paz et al., 2014], we randomly shuffle the frames and generate splits of 30K/10K/11K frames for training/tuning/testing, and we refer to the result as the \u2019JW11-s\u2019 setup (random splits better satisfy the i.", "startOffset": 6, "endOffset": 30}, {"referenceID": 2, "context": "assumption of train/tune/test data than splits by utterances as in [Andrew et al., 2013]).", "startOffset": 67, "endOffset": 88}, {"referenceID": 35, "context": "As in prior work, for both FKCCA and NKCCA we use rank-6000 approximations for the kernel matrices; for DCCA we use two ReLU [Nair and Hinton, 2010] hidden layers of width 1800/1200 for view 1/2 respectively and run stochastic optimization with minibatch size 750", "startOffset": 125, "endOffset": 148}, {"referenceID": 2, "context": "Following Andrew et al. [2013] and Lopez-Paz et al.", "startOffset": 10, "endOffset": 31}, {"referenceID": 2, "context": "Following Andrew et al. [2013] and Lopez-Paz et al. [2014], the acoustic view inputs are 39D Mel-frequencey cepstral coefficients and the articulatory view inputs are horizontal/vertical displacement of 8 pellets attached to different parts of the vocal tract, each then concatenated over a 7-frame context window, for speaker \u2019JW11\u2019.", "startOffset": 10, "endOffset": 59}, {"referenceID": 28, "context": "View 1 inputs are randomly rotated images (28 \u00d7 28, gray scale) from the original MNIST dataset [LeCun et al., 1998], and the corresponding view 2 inputs are randomly chosen images with the same identity plus additive uniform pixel noise.", "startOffset": 96, "endOffset": 116}, {"referenceID": 13, "context": "-one linear SVM [Chang and Lin, 2011] on the projections with highest cluster accuracy for each algorithm (we reveal labels of 10% of the training set for fast SVM training) and report the classification error rates.", "startOffset": 16, "endOffset": 37}, {"referenceID": 21, "context": "For NCCA/PLCCA we first reduce dimensionality to 100 by PCA for density estimation and exact nearest neighbor search, and use a randomized algorithm [Halko et al., 2011] to compute the SVD of the 450K \u00d7 450K matrix S; for RKCCA/NKCCA we use an approximation rank of 5000; for DCCA we use 3 ReLU hidden layers of 1500 units in each view and train with stochastic optimization of minibatch size 4500.", "startOffset": 149, "endOffset": 169}, {"referenceID": 35, "context": "Noisy MNIST handwritten digits dataset We now demonstrate the algorithms on a noisy MNIST dataset, generated identically to that of Wang et al. [2015b] but with a larger training set.", "startOffset": 132, "endOffset": 152}, {"referenceID": 26, "context": "View 1 inputs are randomly rotated images (28 \u00d7 28, gray scale) from the original MNIST dataset [LeCun et al., 1998], and the corresponding view 2 inputs are randomly chosen images with the same identity plus additive uniform pixel noise. We generate 450K/10K/10K pairs of images for training/tuning/testing (Wang et al. [2015b] uses a 50K-pair training set).", "startOffset": 97, "endOffset": 329}, {"referenceID": 26, "context": "View 1 inputs are randomly rotated images (28 \u00d7 28, gray scale) from the original MNIST dataset [LeCun et al., 1998], and the corresponding view 2 inputs are randomly chosen images with the same identity plus additive uniform pixel noise. We generate 450K/10K/10K pairs of images for training/tuning/testing (Wang et al. [2015b] uses a 50K-pair training set). This dataset satisfies the multi-view assumption that given the label, the views are uncorrelated, so that the most correlated subspaces should retain class information and exclude the noise. Following Wang et al. [2015b], we extract a low-dimensional projection of the view 1 images with each algorithm, run spectral clustering to partition the splits into 10 classes (with clustering parameters tuned as in [Wang et al.", "startOffset": 97, "endOffset": 582}, {"referenceID": 37, "context": "Second, we have not explored the space of choices for density estimates; alternative choices, such as adaptive KDE [Terrell and Scott, 1992], could also further improve performance.", "startOffset": 115, "endOffset": 140}], "year": 2016, "abstractText": "Canonical correlation analysis (CCA) is a classical representation learning technique for finding correlated variables in multi-view data. Several nonlinear extensions of the original linear CCA have been proposed, including kernel and deep neural network methods. These approaches seek maximally correlated projections among families of functions, which the user specifies (by choosing a kernel or neural network structure), and are computationally demanding. Interestingly, the theory of nonlinear CCA, without functional restrictions, had been studied in the population setting by Lancaster already in the 1950s, but these results have not inspired practical algorithms. We revisit Lancaster\u2019s theory to devise a practical algorithm for nonparametric CCA (NCCA). Specifically, we show that the solution can be expressed in terms of the singular value decomposition of a certain operator associated with the joint density of the views. Thus, by estimating the population density from data, NCCA reduces to solving an eigenvalue system, superficially like kernel CCA but, importantly, without requiring the inversion of any kernel matrix. We also derive a partially linear CCA (PLCCA) variant in which one of the views undergoes a linear projection while the other is nonparametric. Using a kernel density estimate based on a small number of nearest neighbors, our NCCA and PLCCA algorithms are memory-efficient, often run much faster, and perform better than kernel CCA and comparable to deep CCA.", "creator": "LaTeX with hyperref package"}}}