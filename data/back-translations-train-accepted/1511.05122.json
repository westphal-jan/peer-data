{"id": "1511.05122", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2015", "title": "Adversarial Manipulation of Deep Representations", "abstract": "We show that the internal representations of an image in a deep neural network (DNN) can be manipulated to mimic those of other natural images with only minor, imperceptible perturbations to the original image. Previous methods for generating adversarial images focused on image manipulation designed to produce erroneous class labels, while here we look at internal layers of the representation. Despite the similarities in the generation process, our class of adversarial images differs qualitatively from previous ones. Specifically, while the input image is perceptually similar to an image of one class, its internal representation appear remarkably like that of natural images in a different class. This phenomenon raises questions about DNN representations, as well as the properties of natural images themselves.", "histories": [["v1", "Mon, 16 Nov 2015 20:48:20 GMT  (4769kb,D)", "http://arxiv.org/abs/1511.05122v1", "Under review as a conference paper at ICLR 2016"], ["v2", "Thu, 19 Nov 2015 21:00:44 GMT  (5272kb,D)", "http://arxiv.org/abs/1511.05122v2", "Under review as a conference paper at ICLR 2016"], ["v3", "Mon, 23 Nov 2015 20:56:44 GMT  (5273kb,D)", "http://arxiv.org/abs/1511.05122v3", "Under review as a conference paper at ICLR 2016"], ["v4", "Fri, 11 Dec 2015 21:03:14 GMT  (8849kb,D)", "http://arxiv.org/abs/1511.05122v4", "Under review as a conference paper at ICLR 2016"], ["v5", "Thu, 7 Jan 2016 20:59:55 GMT  (9280kb,D)", "http://arxiv.org/abs/1511.05122v5", "Under review as a conference paper at ICLR 2016"], ["v6", "Tue, 12 Jan 2016 20:51:51 GMT  (9213kb,D)", "http://arxiv.org/abs/1511.05122v6", "Under review as a conference paper at ICLR 2016"], ["v7", "Wed, 13 Jan 2016 20:57:33 GMT  (9463kb,D)", "http://arxiv.org/abs/1511.05122v7", "Under review as a conference paper at ICLR 2016"], ["v8", "Tue, 1 Mar 2016 20:51:06 GMT  (9461kb,D)", "http://arxiv.org/abs/1511.05122v8", "Accepted as a conference paper at ICLR 2016"], ["v9", "Fri, 4 Mar 2016 20:21:24 GMT  (9461kb,D)", "http://arxiv.org/abs/1511.05122v9", "Accepted as a conference paper at ICLR 2016"]], "COMMENTS": "Under review as a conference paper at ICLR 2016", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["sara sabour", "yanshuai cao", "fartash faghri", "david j fleet"], "accepted": true, "id": "1511.05122"}, "pdf": {"name": "1511.05122.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Sara Sabour", "Yanshuai Cao", "Fartash Faghri", "David J. Fleet"], "emails": ["saaraa@cs.toronto.edu", "fleet@cs.toronto.edu", "ycao@architech.ca", "ffaghri@architech.ca"], "sections": [{"heading": null, "text": "We show that the internal representations of an image in a deep neural network (DNN) can be manipulated to mimic those of other natural images with only minor imperceptible disturbances to the original image. Previous methods of creating hostile images have focused on image manipulations aimed at generating erroneous class names, while here we look at the internal layers of representation. Despite the similarities in the creation process, our class of conflicting images differs qualitatively from previous ones. In particular, while the input image is noticeably similar to an image of one class, its internal representation appears remarkably similar to that of natural images of another class. This phenomenon raises questions about DNN representations as well as the properties of natural images themselves."}, {"heading": "1 INTRODUCTION", "text": "Recent publications have shown that deep neural networks (DNNs) for image classification can be circumvented, often in relatively simple ways (DNzi et al., 2015; Goodfellow et al., 2014; Gu & Rigazio et al., 2015; Szegedy et al., 2014b; Tabacof & Valle, 2015) A category of such hostile images disturbs the DNN classification of certain images (sources), even though opponents differ almost imperceptibly from these sources (Goodfellow et al., 2014; Szegedy et al., 2014b). Moreover, they do so with high confidence, both in DNNs and in other models. Such phenomena are important not only because they reveal weaknesses in learned representations and classifiers, but because they provide opportunities to explore fundamental questions about the nature of learned representations, such as whether they are inherent in the problem or the underlying model."}, {"heading": "2 RELATED WORK", "text": "There are two main approaches to constructing opposing images that disrupt the DNN classification, which differ fundamentally in whether they reveal the behavior of DNNs in the vicinity of natural images, or whether the opposing images themselves are unnatural. In the latter case, an evolutionary algorithm can be detected that the DNNs classify as common objects (often 99%), in which case it is completely different from the training data, or the natural images per se. Because the natural images have only a remarkable volume of all possible images, it is perhaps not surprising that the discriminating DNNs are able to solve problems."}, {"heading": "3 ADVERSARIAL IMAGE GENERATION", "text": "The goal is to find a new image that can be the solution to the following limited optimization problem: I\u03b1 = arg min I (I) - \"I\" - \"I\" - \"I\" - \"I\" - \"I\" - \"I\" - \"I\" - \"I\" - \"I\" - \"I\" - \"I\" - \"I\" - \"I\" - \"I\" - \"I\" - \"I\" - \"I\" - \"-\" I \"-\" - \"I\" - \"-\" I \"-\" I \"-\" - \"I\" - \"-\" I \"-\" - \"I\" - \"-\" I \"-\" - \"I\" - \"-\" I \"-\" - \"-\" I \"-\" - \"-\" I \"-\" - \"-\" I \"-\" - \"-\" - \"I\" - \"-\" - \"I -\" - \"-\" - \"we\" - \"-\" we. \"-\" We. \"-\" We. \"-\" We. \"-\" We \"-\" - \"We.\" - \"We.\" - \"-\" We \"-\" - \"We\" - \"-\" - \"we\" - \"-\" - \"we.\" - \"-\" - \"-\" we. \"-\" - \"-\" - \""}, {"heading": "4 EXPERIMENTAL EVALUATION", "text": "We examine the quality of an opposing image by asking two questions: First, does the representation of the opposing image look like its mission statement? Second, does the representation of the opposing image look like a natural image? To answer these questions quantitatively, we use several comparative variables described below to show that the contrary images we receive have tested properties of natural images. They do not appear to be outliers from the training corpus in a significant way, as judged by internal representations. To this end, we focus on the Caffenet benchmark and random sets of initial and guiding images. Both the initial and the guiding images consist of images from the training, testing and validation sets of Imagenet. To facilitate quantitative analysis, we have selected only those images as guiding principles whose labels are correctly predicted by Caffenet from the training and validation on the other networks."}, {"heading": "4.1 SIMILARITY TO THE GUIDE REPRESENTATION", "text": "Using any image as a guide, we show that the generated enemy representation looks similar to the leader. We show that the representation is not only far from the source and close enough to the leader's representation, but also the neighborhood around the opponent is almost exactly the same as the leader's. First, we evaluate the opponent's distance to the source and leader in terms of various prototypical and interpretable distances in the functional space. Second, we investigate that a small disturbance in the image space leads to a relatively large change in the representation of a leader. We report the results of 20 source images. In this sentence, 5 images were randomly selected from each training, testing and validation of Imagenet. Five more images were selected manually from Wikipedia and the Imagenet validation for greater diversity."}, {"heading": "4.2 SIMILARITY TO NATURAL REPRESETNATIONS", "text": "Once we have established that the representation of the counter-image (\u03b1) is close to the representation of the model (g) at the Euclidean distance, the immediate question is whether \u03b1 is like a natural representation near g. In other words, is \u03b1 near g an inlier equipped with the same properties as other points in this neighbourhood? We answer this question by examining two measurements of points in the neighbourhood: firstly, a probable parametric measurement indicating the probability of a point in the manifold tangent in g; secondly, a geometric non-parametric measure describing point relations. In both measurements, our study includes the following measurements: the guideline g; the counterpart \u03b1; a series of reference points Nref consisting of 15 random points in the manifold tangent plane in g; a series of \"narrow\" NNNNs not in the reference group, Nc's not in the reference magnitude Nc; and we do not use (f) Nc 'points in the second row of Nc' = 40 (f); and (f) we use a series of Nf in the space (Nf) of Nf)."}, {"heading": "4.2.1 MANIFOLD TANGENT SPACE", "text": "We build a probabilistic model of the neighbors of g on the manifold tangent plane at g and compare the probability of \u03b1 with other points. To this end, we use a probabilistic PCA (PPCA) constructed on Nref, the main space of which is an intersecting plane that has approximately the same normal direction as the tangent, but generally does not run through g due to the curvature of the manifold. Therefore, we correct the small offset by shifting the plane to go through g. With the PPCA model, this can be achieved by measuring the mean of the high-dimensional Gaussian plane according to g. We then evaluate the probability of a log of a point normalized by the log representation of g under the Gaussian property, which we call L (\u00b7, g) = L () \u2212 L (g) \u2212 L (g) (g). For a large number of guide and source pairs, we repeat this measurement and compare the distribution of the Nc for the fart and the Nf."}, {"heading": "4.2.2 ANGULAR CONSISTENCY MEASURE", "text": "If NN of g is too sparse in the high-dimensional character space, or if the manifold has a high curvature, the resulting linear Gaussian model could be of low accuracy. Therefore, we present another method to find out whether \u03b1 is an inlier near g without relying on the manifold assumption. We take a series of reference points near g and measure the directions from the guide to the reference points and compare the corresponding directions measured for another point, say \u03b1. We refer to this measurement as the angle consistency. In particular, let {xi} ki = 1 be a series of reference points, and let z be another point near g that cannot belong to another NN in the reference group, say. Define vi (z) = xi \u2212 z is the vector from z to xi; and similar for vi (g) is the angle consistency vi (g), and the angle consistency vi (g) from (to), say, and angle (z) to (z)."}, {"heading": "4.3 COMPARISONS TO OTHER CATEGORIES OF ADVERSARIALS", "text": "We are now comparing our examples with others we have created for triggering misclassifications. (2014) We are comparing the way we classify people in the world of work. (2014) We are also applying the fast gradient perturbation idea and comparing it with our limited optimization to show that the phenomenon described in this work is not by linear perturbation. We are referring to our results as a characteristic. (2016) The images we have generated for triggering misclassifications are explained by optimizing scegedy models. (2014) Also the way in which we initiate the classification of people in the world of work. (2014) We are proposing a way in which we make the classification of people in the world of work by taking a small step in line with the world of work. (2014) We are comparing the classification of people in the world of work (2014)."}, {"heading": "5 DISCUSSION", "text": "The approach to generating enemy images with internal representations that mimic those of other images has worked well with a remarkably wide image class, including images from training and test sets and images of all classes in the Imagenet and Places datasets. Nevertheless, there are cases where our optimization has not been successful in generating opposing images. MNIST images and LeNet found it necessary to loosen the order of magnitude associated with the disturbances to the point that the leader became perceptible in the opposing image. With LeNet on CIFAR10, optimization was successful only on some images, but both cases are somewhat atypical as the network is not particularly deep, and the data is not typical of natural images in appearance and resolution. Training with Caffenet in advance on ImageNet and then fine-tuning on the patch-style datasets by Karayev et et al. (2013), we could easily generate a contradictory image with FC8 (the class 7 images that are not normalized)."}], "references": [{"title": "Return of the devil in the details: Delving deep into convolutional nets", "author": ["K REFERENCES Chatfield", "K Simonyan", "A Vedaldi", "A. Zisserman"], "venue": "arXiv preprint arXiv:1405.3531,", "citeRegEx": "Chatfield et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chatfield et al\\.", "year": 2014}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J Deng", "W Dong", "R Socher", "LJ Li", "K Li", "L. Fei-Fei"], "venue": "In IEEE CVPR,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Fundamental limits on adversarial robustness", "author": ["A Fawzi", "O Fawzi", "P. Frossard"], "venue": "In ICML,", "citeRegEx": "Fawzi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fawzi et al\\.", "year": 2015}, {"title": "Deep sparse rectifier neural networks", "author": ["Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua"], "venue": "Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS-11),", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Explaining and harnessing adversarial examples", "author": ["IJ Goodfellow", "J Shlens", "C. Szegedy"], "venue": "In ICLR (arXiv:1412.6572),", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Towards deep neural network architectures robust to adversarial examples", "author": ["S Gu", "L. Rigazio"], "venue": "In Deep Learning and Representation Learning Workshop (arXiv:1412.5068),", "citeRegEx": "Gu and Rigazio,? \\Q2014\\E", "shortCiteRegEx": "Gu and Rigazio", "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y Jia", "E Shelhamer", "J Donahue", "S Karayev", "J Long", "R Girshick", "S Guadarrama", "T. Darrell"], "venue": "In ACM Int. Conf. Multimedia,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A Krizhevsky", "I Sutskever", "Hinton", "GE"], "venue": "In NIPS, pp. 1097\u20131105,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Understanding deep image representations by inverting them", "author": ["A Mahendran", "A. Vedaldi"], "venue": "In IEEE CVPR (arXiv:1412.0035),", "citeRegEx": "Mahendran and Vedaldi,? \\Q2014\\E", "shortCiteRegEx": "Mahendran and Vedaldi", "year": 2014}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A Nguyen", "J Yosinski", "J. Clune"], "venue": "In IEEE CVPR (arXiv:1412.1897),", "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Intriguing properties of neural networks", "author": ["C Szegedy", "W Zaremba", "I Sutskever", "J Bruna", "D Erhan", "I Goodfellow", "R. Fergus"], "venue": "In ICLR (arXiv:1312.6199),", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Exploring the space of adversarial images", "author": ["Tabacof", "Pedro", "Valle", "Eduardo"], "venue": "arXiv preprint arXiv:1510.05328,", "citeRegEx": "Tabacof et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tabacof et al\\.", "year": 2015}, {"title": "Image quality assessment: From error visibility to structural similarity", "author": ["Z Wang", "AC Bovik", "HR Sheikh", "Simoncelli", "EP"], "venue": "IEEE Trans. PAMI,", "citeRegEx": "Wang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2004}, {"title": "Learning deep features for scene recognition using places database", "author": ["B Zhou", "A Lapedriza", "J Xiao", "A Torralba", "A. Oliva"], "venue": "In NIPS, pp. 487\u2013495,", "citeRegEx": "Zhou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 2, "context": "Recent papers have shown that deep neural networks (DNNs) for image classification can be circumvented, often in relatively simple ways (Fawzi et al., 2015; Goodfellow et al., 2014; Gu & Rigazio, 2014; Nguyen et al., 2015; Szegedy et al., 2014b; Tabacof & Valle, 2015).", "startOffset": 136, "endOffset": 268}, {"referenceID": 4, "context": "Recent papers have shown that deep neural networks (DNNs) for image classification can be circumvented, often in relatively simple ways (Fawzi et al., 2015; Goodfellow et al., 2014; Gu & Rigazio, 2014; Nguyen et al., 2015; Szegedy et al., 2014b; Tabacof & Valle, 2015).", "startOffset": 136, "endOffset": 268}, {"referenceID": 9, "context": "Recent papers have shown that deep neural networks (DNNs) for image classification can be circumvented, often in relatively simple ways (Fawzi et al., 2015; Goodfellow et al., 2014; Gu & Rigazio, 2014; Nguyen et al., 2015; Szegedy et al., 2014b; Tabacof & Valle, 2015).", "startOffset": 136, "endOffset": 268}, {"referenceID": 4, "context": "One category of such adversarial images disrupts DNN classification of given images (sources) even though the adversarials differ almost imperceptibly from those sources (Goodfellow et al., 2014; Szegedy et al., 2014b).", "startOffset": 170, "endOffset": 218}, {"referenceID": 4, "context": ", whether they are inherent in the problem or the underlying model, and 2) such adversarial images might be harnessed to guide the development of new learning algorithms that exhibit improved robustness and better generalization (Goodfellow et al., 2014; Gu & Rigazio, 2014).", "startOffset": 229, "endOffset": 274}, {"referenceID": 7, "context": "In the latter case, Nguyen et al. (2015) describe an evolutionary algorithm to generate images comprising 2D patterns that are classified by DNNs as common objects with high confidence (often 99%).", "startOffset": 20, "endOffset": 41}, {"referenceID": 7, "context": "In the latter case, Nguyen et al. (2015) describe an evolutionary algorithm to generate images comprising 2D patterns that are classified by DNNs as common objects with high confidence (often 99%). While interesting, such adversarial images are totally different from the training data, or natural images per se. Because natural images only occupy a remarkably small volume of the space of all possible images, it is perhaps not surprising that discriminative DNNs trained on natural images have trouble coping with such out-of-sample data. In our approach, similar to Szegedy et al. (2014b), we focus on adversarial images that appear natural.", "startOffset": 20, "endOffset": 592}, {"referenceID": 7, "context": "In the latter case, Nguyen et al. (2015) describe an evolutionary algorithm to generate images comprising 2D patterns that are classified by DNNs as common objects with high confidence (often 99%). While interesting, such adversarial images are totally different from the training data, or natural images per se. Because natural images only occupy a remarkably small volume of the space of all possible images, it is perhaps not surprising that discriminative DNNs trained on natural images have trouble coping with such out-of-sample data. In our approach, similar to Szegedy et al. (2014b), we focus on adversarial images that appear natural. To construct an adversarial image, Szegedy et al. (2014b) use gradient-based optimization on the classification loss with respect to the image perturbation, , with a penalty on the magnitude of .", "startOffset": 20, "endOffset": 703}, {"referenceID": 7, "context": "In the latter case, Nguyen et al. (2015) describe an evolutionary algorithm to generate images comprising 2D patterns that are classified by DNNs as common objects with high confidence (often 99%). While interesting, such adversarial images are totally different from the training data, or natural images per se. Because natural images only occupy a remarkably small volume of the space of all possible images, it is perhaps not surprising that discriminative DNNs trained on natural images have trouble coping with such out-of-sample data. In our approach, similar to Szegedy et al. (2014b), we focus on adversarial images that appear natural. To construct an adversarial image, Szegedy et al. (2014b) use gradient-based optimization on the classification loss with respect to the image perturbation, , with a penalty on the magnitude of . Given an image I , a DNN classifier f , and an erroneous label l, they find the perturbation that minimizes loss(f(I + ), l) + c\u2016 \u2016. Here, c is chosen by line-search to find the smallest that achieves f(I + ) = l. The authors argue that the resulting adversarial images occupy low probability \u201cpockets\u201d in the manifold, which act like \u201cblind spots\u201d to the DNN. The adversarial construction in our paper extends the approach of Szegedy et al. (2014b). In Sec.", "startOffset": 20, "endOffset": 1291}, {"referenceID": 3, "context": "Later work by Goodfellow et al. (2014) shows that adversarial images are more common, and can be found by taking steps in the direction of the gradient of loss(f(I + ), l).", "startOffset": 14, "endOffset": 39}, {"referenceID": 3, "context": "Later work by Goodfellow et al. (2014) shows that adversarial images are more common, and can be found by taking steps in the direction of the gradient of loss(f(I + ), l). Goodfellow et al. (2014) also show that adversarial examples exist for many other models, including linear classifiers.", "startOffset": 14, "endOffset": 198}, {"referenceID": 2, "context": "Fawzi et al. (2015) later propose a more general framework to explain adversarial images, formalizing the intuition that the problem occurs when DNNs and other models are not sufficiently \u201cflexible\u201d for the given classification task.", "startOffset": 0, "endOffset": 20}, {"referenceID": 12, "context": ", see Wang et al. (2004)), it is superior to the L2 norm.", "startOffset": 6, "endOffset": 25}, {"referenceID": 6, "context": "Figure 1 shows some adversarial images from this generation process, using the the well-known BVLC Caffe Reference model (Caffenet) (Jia et al., 2014).", "startOffset": 132, "endOffset": 150}, {"referenceID": 1, "context": "We take 100 random pair of images from Imagenet (Deng et al., 2009) used to train Caffenet, apply optimization at FC7 with delta = 10, and we find that the class label assigned to the adversarial image is never equal that of the source image, and in 95% of cases matches the guide class.", "startOffset": 48, "endOffset": 67}, {"referenceID": 7, "context": "In addition to Caffenet, we apply our approach to AlexNet (Krizhevsky et al., 2012), GoogleNet (Szegedy et al.", "startOffset": 58, "endOffset": 83}, {"referenceID": 0, "context": "), and VGG CNN-S (Chatfield et al., 2014), all trained on the the Imagenet ILSVRC dataset (Deng et al.", "startOffset": 17, "endOffset": 41}, {"referenceID": 1, "context": ", 2014), all trained on the the Imagenet ILSVRC dataset (Deng et al., 2009).", "startOffset": 56, "endOffset": 75}, {"referenceID": 13, "context": "We also used AlexNet trained on the Places205 dataset, and on a hybrid dataset comprising 205 scene classes and 977 classes from ImageNet (Zhou et al., 2014).", "startOffset": 138, "endOffset": 157}, {"referenceID": 0, "context": "), and VGG CNN-S (Chatfield et al., 2014), all trained on the the Imagenet ILSVRC dataset (Deng et al., 2009). We also used AlexNet trained on the Places205 dataset, and on a hybrid dataset comprising 205 scene classes and 977 classes from ImageNet (Zhou et al., 2014). In all these cases, using 100 random source-guide pairs we observe similar behaviour. Class labels assigned to adversarial images do not match the source. Rather, in 97% to 100% of all cases the predicted class label is that of the guide. Similar to other approaches for generating adversarial images, we find that adversarials generated on one network are similarly misclassified in other networks. Using the same 100 source-guide pairs on each of the above models we find that, on average, 54% of adversarial images obtained from one network are misclassified by the other networks as well. However, they are not usually classified with the label of the guide when tested on one of the other networks. We next turn to consider internal representations \u2013 do they resemble those of the source, the guide, or some combination of the two? One way to probe the internal representations, following Mahendran & Vedaldi (2014), is to invert the mapping, allowing us to display the image reconstructed from the internal representation.", "startOffset": 18, "endOffset": 1191}, {"referenceID": 6, "context": "Model Layer \u22293NN = 3 \u22293NN \u2265 2 \u2206r3 median, [min, max] (%) CaffeNet (Jia et al., 2014) FC7 71 95 \u22125.", "startOffset": 66, "endOffset": 84}, {"referenceID": 7, "context": "00] AlexNet (Krizhevsky et al., 2012) FC7 72 97 \u22125.", "startOffset": 12, "endOffset": 37}, {"referenceID": 0, "context": "10] VGG CNN S (Chatfield et al., 2014) FC7 84 100 \u22123.", "startOffset": 14, "endOffset": 38}, {"referenceID": 13, "context": "00] Places205 AlexNet (Zhou et al., 2014) FC7 91 100 \u22121.", "startOffset": 22, "endOffset": 41}, {"referenceID": 13, "context": "04] Places205 Hybrid (Zhou et al., 2014) FC7 85 100 \u22121.", "startOffset": 21, "endOffset": 40}, {"referenceID": 8, "context": "We now compare our adversarial examples to other ones created for triggering mis-classification by the optimization of Szegedy et al. (2014b), and by the fast gradient perturbation method of Goodfellow et al.", "startOffset": 119, "endOffset": 142}, {"referenceID": 3, "context": "(2014b), and by the fast gradient perturbation method of Goodfellow et al. (2014). We also apply the fast gradient perturbation idea on internal representations, and compare to our constrained optimization in order to demonstrate that the phenomenon reported in this work cannot be explained by linear perturbation.", "startOffset": 57, "endOffset": 82}, {"referenceID": 3, "context": "(2014b), and by the fast gradient perturbation method of Goodfellow et al. (2014). We also apply the fast gradient perturbation idea on internal representations, and compare to our constrained optimization in order to demonstrate that the phenomenon reported in this work cannot be explained by linear perturbation. We hereby refer to our results as feature adversarials via optimization (feat-opt). The adversarial images generated for triggering mis-classification via the optimization of Szegedy et al. (2014b) is briefly described in Sec.", "startOffset": 57, "endOffset": 514}, {"referenceID": 3, "context": "(2014b), and by the fast gradient perturbation method of Goodfellow et al. (2014). We also apply the fast gradient perturbation idea on internal representations, and compare to our constrained optimization in order to demonstrate that the phenomenon reported in this work cannot be explained by linear perturbation. We hereby refer to our results as feature adversarials via optimization (feat-opt). The adversarial images generated for triggering mis-classification via the optimization of Szegedy et al. (2014b) is briefly described in Sec.2, and referred to as label adversarials via optimization (label-opt) from here on. Goodfellow et al. (2014) also proposed a way to construct adversarial examples that confuse classifiers by taking a small step consistent with the gradient, i.", "startOffset": 57, "endOffset": 651}, {"referenceID": 3, "context": "It is well known that DNNs with ReLU activation units have sparse activations (Glorot et al. (2011)).", "startOffset": 79, "endOffset": 100}, {"referenceID": 4, "context": "One implication of observations on the sparsity patterns is that the linear perturbation explanation of label adversarial examples in Goodfellow et al. (2014) does not seem to apply to our class of represenation adversarial examples.", "startOffset": 134, "endOffset": 159}], "year": 2017, "abstractText": "We show that the internal representations of an image in a deep neural network (DNN) can be manipulated to mimic those of other natural images with only minor, imperceptible perturbations to the original image. Previous methods for generating adversarial images focused on image manipulation designed to produce erroneous class labels, while here we look at internal layers of the representation. Despite the similarities in the generation process, our class of adversarial images differs qualitatively from previous ones. Specifically, while the input image is perceptually similar to an image of one class, its internal representation appear remarkably like that of natural images in a different class. This phenomenon raises questions about DNN representations, as well as the properties of natural images themselves.", "creator": "LaTeX with hyperref package"}}}