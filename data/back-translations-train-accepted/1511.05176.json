{"id": "1511.05176", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2015", "title": "MuProp: Unbiased Backpropagation for Stochastic Neural Networks", "abstract": "Deep neural networks are powerful parametric models that can be trained efficiently using the backpropagation algorithm. Stochastic neural networks combine the power of large parametric functions with that of graphical models, which makes it possible to learn very complex distributions. However, as backpropagation is not directly applicable to stochastic networks that include discrete sampling operations within their computational graph, training such networks remains difficult. We present MuProp, an unbiased gradient estimator for stochastic networks, designed to make this task easier. MuProp improves on the likelihood-ratio estimator by reducing its variance using a control variate based on the first-order Taylor expansion of a mean-field network. Crucially, unlike prior attempts at using backpropagation for training stochastic networks, the resulting estimator is unbiased and well behaved. Our experiments on structured output prediction and discrete latent variable modeling demonstrate that MuProp yields consistently good performance across a range of difficult tasks.", "histories": [["v1", "Mon, 16 Nov 2015 21:08:25 GMT  (2097kb,D)", "https://arxiv.org/abs/1511.05176v1", null], ["v2", "Thu, 7 Jan 2016 21:44:35 GMT  (2206kb,D)", "http://arxiv.org/abs/1511.05176v2", null], ["v3", "Thu, 25 Feb 2016 20:36:21 GMT  (2206kb,D)", "http://arxiv.org/abs/1511.05176v3", "Published as a conference paper at ICLR 2016"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shixiang gu", "sergey levine", "ilya sutskever", "riy mnih"], "accepted": true, "id": "1511.05176"}, "pdf": {"name": "1511.05176.pdf", "metadata": {"source": "CRF", "title": "MUPROP: UNBIASED BACKPROPAGATION FOR STOCHASTIC NEURAL NETWORKS", "authors": ["Shixiang Gu", "Sergey Levine", "Ilya Sutskever", "Andriy Mnih"], "emails": ["shanegu@google.com", "slevine@google.com", "ilyasu@google.com", "amnih@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, it is as if most of us are able to survive ourselves, and that they feel able to survive themselves, and that they are able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they are able to survive themselves. \"(...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...)"}, {"heading": "2 RELATED WORK", "text": "Probabilistic latent variable models described by neural networks are based on early work on sigmoidal faith networks and Helmholtz machines (Neal, 1992; Dayan et al., 1995), but their introduction has been hampered by a lack of both efficient and theoretically sound training methods. Algorithms based on the Markov chain Monte Carlo (Neal, 1992) and the midfield reference (Saul et al., 1996) are well-founded in theory, but not well-suited for large models. The wake-sleep algorithm (Hinton et al., 1995) scales to large models, but does not optimize well-defined dobjective function, and therefore offers no convergence guarantees."}, {"heading": "2.1 LIKELIHOOD-RATIO GRADIENT ESTIMATION", "text": "Consider a simple stochastic system with discrete random variables x, the probability of which is specified by p\u03b8 (x), and a loss function f (x).This formula summarizes the training objectives of popular generative models such as sigmoid faith networks with or without inference networks.The goal of the training is to minimize the expected costs L (\u03b8) = Ep\u03b8 (x) [f (x)].The gradient g = mps is usually difficult to calculate accurately and must therefore be estimated with an estimator g (x), i.e. g \u2248 m = 1 g (xi) / m, with xi \u00b2 pTB (x) and m the number of Monte Carlo samples.The probability quotient (LR) estimator, of which the popular REINFORCE algorithm is a special case (Williams, 1992; Peters & Schaal, 2006), provides a convenient method for estimating this gradient and serves as a basis for all discussed probability quotient (LR) estimators, of which the popular REINFORCE algorithm is a special case (SchaORCE, 1992; Peters & Schaal, 2006)."}, {"heading": "2.2 VARIANCE REDUCTION WITH THE LIKELIHOOD-RATIO ESTIMATOR", "text": "High variance of LR estimators can make convergence very slow or impossible, exacerbated by the fact that LR estimators for minibatch formation of neural networks typically use m = 1 to maximize the variety of examples seen by the network (Kingma & Welling, 2014; Gregor et al., 2014; Rezende et al., 2014; Mnih & Gregor, 2014). The easiest way to reduce variance is to increase m, but this is mathematically expensive. Efficient and effective techniques for reducing variance are therefore crucial for the LR estimator to be workable. Deriving MuProp in Section 3 uses a variance reduction technique known as control variants (Paisley et al, 2012). The main idea is to subtract an analytically comprehensible term from the LR estimation in order to reduce the variance of the Monte Carlo estimate, then catch up on the analytical expectation."}, {"heading": "3 MUPROP", "text": "The MuProp estimator has two components: a deterministic term g'MF, which is calculated by backpropagation through a mid-range network, which we describe in Section 3.1, and an LR term g'R, which takes into account the residuals to produce an unbiased gradient estimate. However, the following can summarize all values that do not depend on x. The basic idea of MuProp is summarized in the following equation: g '(3) = Taylor Log PTB (x) \u00b7 [f'f'f (x) \u2212 f'f'f'f'f'f'f'f'f'f's expansion (x'x'). The basic idea of MuProp-R + f'f'x'x (x's) is an integration method."}, {"heading": "3.1 GENERALIZED BACKPROPAGATION WITH MEAN-FIELD NETWORKS", "text": "If the calculation curve contains a discrete sampling operation such as Figure 1, such an operation cannot directly define a continuous gradient. As a solution, we use a deterministic mid-range network that ignores the sampling operations in the original diagram and propagates the averages instead of the samples. Figure 1 shows how reverse trajectory calculates the gradient in relation to the circuit. MuProp calculates the Taylor expansion using the fully differentiable mid-range network and uses these terms to reduce the deviation. While the choice of the mid-range network appears arbitrary, we show that proper recursive derivation of Equation 3 for deep networks naturally results in recursive Taylor extensions by the mean functions."}, {"heading": "4 COMPARISON WITH OTHER GRADIENT ESTIMATORS", "text": "While MuProp is derived from the LR estimator, which is also unbiased, it is closely related to two biased estimators commonly used to form stochastic binary networks. In this section, we describe these two estimators, the straight-through estimator and the 1 / 2. To simplify the notation, we describe the estimators assuming a stochastic variable x. However, the provided estimator expressions can easily be generalized to any stochastic arithmetic. For the sake of completeness, the above-mentioned estimators are summarized in Table 1. 2Only the LR and MuProp estimators have a principal extension to deep networks. The other methods can be heuristically extended to deep networks. We do not consider the weighted sample-based approaches (Raiko et al., 2015; Tang & Salakhutdinov, 2013; Bornschein & Bengio, 2015; Burda et al., 2015) as an optimization of objective approaches."}, {"heading": "4.1 THE STRAIGHT-THROUGH ESTIMATOR", "text": "The Straight-Through Estimator (ST) (Bengio et al., 2013; Raiko et al., 2015) is a biased but low-variance estimator designed primarily for binary stochastic neurons3. The idea is to multiply backwards through the threshold function as if it were the identity function. The estimator is given below: g (ST) = f \"(x) x\" x \"x\" x \"(\u03b8) (4) The estimator is similar to the g\" MF term in our estimator in Equation 3, because for a stochastic binary network the activation function is the mean function. The difference is that this estimator depends on sampled xi during backpropagation, while our formulation propagates the gradient backwards through midfield activation. Despite the heuristic derivative, this simple distorted estimator works well in practice, most likely due to its general similarity to the reverse-cutting algorithm, but we show that it is more reliable with the pattern algorithm."}, {"heading": "4.2 THE 1/2 ESTIMATOR", "text": "Gregor et al. (2014) proposed a distorted estimator for the Deep AutoRegressive Network (DARN), which is known as the \"1 / 2\" estimator because of its particular choice of baseline. Like the StraightThrough estimator, the 1 / 2 estimator is also specialized in stochastic binary networks. (For models with only one stochastic unit, the 1 / 2 estimator corresponds to the use of the Taylor expansion around the sample x, which is evaluated at a fixed point x, as the baseline, so that h (x) = f (x) + f (x) (x) (x). Since the expectation for h (x) cannot be analytically calculated and x cannot be selected to make the baseline a mean for any function f, this estimator is distorted. Critically, the 1 / 2 estimator is derived for models with only one unit x, but applied to large models, treating each unit as the only unit of the model."}, {"heading": "5 EXPERIMENTS", "text": "We compare the LR, ST, and 1 / 2 estimators with the MuProp estimator for tasks that use a variety of network architectures. For the LR and MuProp estimators, \"-C,\" \"-VN,\" and \"-IDB\" indicate a constant middle baseline, variance normalization, and an input-dependent baseline, respectively. The first task does not use an inference network and involves the direct optimization of an approximation to the expected goal. The second task is to train a sigmoid belief network with an inference network by maximizing the variable lower limit of the intractable log probability. MuProp performs consistently well in both tasks."}, {"heading": "5.1 STRUCTURED OUTPUT PREDICTION", "text": "In this experiment, we follow the setup proposed by Raiko et al. (2015). The two tasks are predicting the bottom half of an MNIST digit against the top half, and predicting multiple facial expressions from an average face with Toronto Face Dataset (TFD); the output distribution in both tasks has complex multimodality. For MNIST, the output pixels are binarized using the same protocol as in (Raiko et al., 2015). In the face of an input x, an output y and stochastic hidden variable h, the goal is to maximize the hi pixels (h | x) [Log-m-i = 1 p.hi], an emphasis-placed estimate of the probability target (Raiko et al., 2015; Burda et al., 2015; Burda et al., we use m = 1 for training, and m = 100 for validation and testing."}, {"heading": "5.2 VARIATIONAL TRAINING OF GENERATIVE MODELS", "text": "This year, more than ever before in the history of a country in which it is a country in which it is a country in which it is a country in which it is a country, a country in which it is a country, a country, a country, a country, a country, a country, a country, a country, a city, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a"}, {"heading": "6 DISCUSSION", "text": "In this paper, we introduced MuProp, an unbiased estimator of derivatives in stochastic computational curves that combines the statistical efficiency of backpropagation with the accuracy of a method of probability ratio.MuProp has a number of natural extensions. First, we could consider using functions other than the Taylor expansion, which could be learned in a manner similar to Q-Learning (Watkins & Dayan, 1992) and target propagation (Lee et al., 2015). In the field of reinforcement learning, customized Q functions obtained by estimating the expected return on a given policy could summarize all future costs, and a good Q function could greatly simplify the time credit allocation problem. Combining MuProp with such customized Q functions could significantly reduce the variance of the estimator and make it more suitable for very deep computational graphs, such as recurring networks and neural amplification."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Jascha Solh-dickstein, Laurent Dinh, Ben Poole and Quoc Le for their helpful discussions and the Google Brain team for their support."}, {"heading": "7 RECURSIVE DERIVATION OF MUPROP", "text": "Given a multi-level model with discrete random variables x1... xn and input x0, the general loss function can be written as L (x0, \u03b8) = Ep (x1,..., xn) (x1,..., xn) [fp (x0: n)]. To make it easier, we show that p (x1,..., xn, x), x (n), x (n), x (n), x (n), x (n), x (n), x (n), x (n), x (n), x (n), x (n), x (n), x (n), x (n), x (n), x (n), x (x), x (n), x (n), x (n), x (n), x (n), x (x), x (n, x, x (n), x (n), x (n), x (n), x, x (n), x (n), x (n), x (n), x (n), x (n), x (n), x (n), x (n), x (n), x, x (n), x, x (n), x (n), x, x (n), x (n, x (n), x (n), x (n, x (n), x (n), x, x (n), x (n), x (n (n), x (n), x (n), x (n, x (n), x (n), x (n), x, x (n), x (n, x (n), x (n), x (n, x (n), x (n), x (n), x (x (n), x, x (n), x (x (n), x (n), x (x1, x1, x1, x1, x1, x1, x1, x (n), x (n), x1, x (n), x1, x (n (n), x (n), x ("}, {"heading": "8 MUPROP WITH AUTOMATIC DIFFERENTIATION", "text": "We present a simple algorithm for calculating the MuProp gradient, taking advantage of the automatic differentiation functionalities. Algorithm 1 assumes that the automatic differentiation library provides two functionalities: gradients (costs, inputs), which calculate cost derivatives in relation to inputs, and StopGradient (x), which returns a node of equal value to x, but stops the gradient when gradients are called. It assumes that the graph consists of n stochastic nodes {xi} i = 1: n and loss function f, which can be the sum of multiple loss functions at different parts of the graph. PARENTSxi denotes the stochastic parent node of xi \u2212 ForwardPass, the symbolic graph whose results can be differentiated with gradients. If stochastic = true, each stochastic node is valid and encloses the value with stopdient Graxf."}], "references": [{"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "author": ["Bengio", "Yoshua", "L\u00e9onard", "Nicholas", "Courville", "Aaron"], "venue": "arXiv preprint arXiv:1308.3432,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Importance weighted autoencoders", "author": ["Burda", "Yuri", "Grosse", "Roger", "Salakhutdinov", "Ruslan"], "venue": "arXiv preprint arXiv:1509.00519,", "citeRegEx": "Burda et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Burda et al\\.", "year": 2015}, {"title": "The helmholtz machine", "author": ["Dayan", "Peter", "Hinton", "Geoffrey E", "Neal", "Radford M", "Zemel", "Richard S"], "venue": "Neural computation,", "citeRegEx": "Dayan et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dayan et al\\.", "year": 1995}, {"title": "Deep autoregressive networks", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Mnih", "Andriy", "Blundell", "Charles", "Wierstra", "Daan"], "venue": "In ICML,", "citeRegEx": "Gregor et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2014}, {"title": "Neural networks for machine learning", "author": ["Hinton", "Geoffrey"], "venue": "Coursera, video lectures,", "citeRegEx": "Hinton and Geoffrey.,? \\Q2012\\E", "shortCiteRegEx": "Hinton and Geoffrey.", "year": 2012}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Hinton", "Geoffrey", "Deng", "Li", "Yu", "Dong", "Dahl", "George E", "Mohamed", "Abdel-rahman", "Jaitly", "Navdeep", "Senior", "Andrew", "Vanhoucke", "Vincent"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "The \u201dwake-sleep\u201d algorithm for unsupervised neural networks", "author": ["Hinton", "Geoffrey E", "Dayan", "Peter", "Frey", "Brendan J", "Neal", "Radford M"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1995}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In NIPS, pp", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Difference target propagation", "author": ["Lee", "Dong-Hyun", "Zhang", "Saizheng", "Fischer", "Asja", "Bengio", "Yoshua"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Lee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Neural variational inference and learning in belief networks", "author": ["Mnih", "Andriy", "Gregor", "Karol"], "venue": "In ICML,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Recurrent models of visual attention", "author": ["Mnih", "Volodymyr", "Heess", "Nicolas", "Graves", "Alex"], "venue": "In NIPS,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Connectionist learning of belief networks", "author": ["Neal", "Radford M"], "venue": "Artificial intelligence,", "citeRegEx": "Neal and M.,? \\Q1992\\E", "shortCiteRegEx": "Neal and M.", "year": 1992}, {"title": "Variational bayesian inference with stochastic search", "author": ["Paisley", "John", "Blei", "David", "Jordan", "Michael"], "venue": null, "citeRegEx": "Paisley et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Paisley et al\\.", "year": 2012}, {"title": "Policy gradient methods for robotics", "author": ["Peters", "Jan", "Schaal", "Stefan"], "venue": "In Intelligent Robots and Systems,", "citeRegEx": "Peters et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2006}, {"title": "Techniques for learning binary stochastic feedforward neural networks", "author": ["Raiko", "Tapani", "Berglund", "Mathias", "Alain", "Guillaume", "Dinh", "Laurent"], "venue": null, "citeRegEx": "Raiko et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Raiko et al\\.", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": null, "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Learning representations by backpropagating", "author": ["Rumelhart", "David E", "Hinton", "Geoffrey E", "Williams", "Ronald J"], "venue": "errors. Nature,", "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Mean field theory for sigmoid belief networks", "author": ["Saul", "Lawrence K", "Jaakkola", "Tommi", "Jordan", "Michael I"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "Saul et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Saul et al\\.", "year": 1996}, {"title": "Gradient estimation using stochastic computation graphs", "author": ["Schulman", "John", "Heess", "Nicolas", "Weber", "Theophane", "Abbeel", "Pieter"], "venue": null, "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Learning stochastic feedforward neural networks", "author": ["Tang", "Yichuan", "Salakhutdinov", "Ruslan"], "venue": "In NIPS,", "citeRegEx": "Tang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2013}, {"title": "The optimal reward baseline for gradient-based reinforcement learning", "author": ["Weaver", "Lex", "Tao", "Nigel"], "venue": "In UAI,", "citeRegEx": "Weaver et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Weaver et al\\.", "year": 2001}, {"title": "Reinforcement learning neural turing machines", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "Machine learning,", "citeRegEx": "Zaremba et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 1992}], "referenceMentions": [{"referenceID": 8, "context": "Deep neural networks (Krizhevsky et al., 2012; Hinton et al., 2012; Sutskever et al., 2014) are responsible for numerous state-of-the-art results in a variety of domains, including computer vision, speech recognition, and natural language processing.", "startOffset": 21, "endOffset": 91}, {"referenceID": 5, "context": "Deep neural networks (Krizhevsky et al., 2012; Hinton et al., 2012; Sutskever et al., 2014) are responsible for numerous state-of-the-art results in a variety of domains, including computer vision, speech recognition, and natural language processing.", "startOffset": 21, "endOffset": 91}, {"referenceID": 20, "context": "Deep neural networks (Krizhevsky et al., 2012; Hinton et al., 2012; Sutskever et al., 2014) are responsible for numerous state-of-the-art results in a variety of domains, including computer vision, speech recognition, and natural language processing.", "startOffset": 21, "endOffset": 91}, {"referenceID": 17, "context": "The cornerstone of their success has been the simple and scalable backpropagation algorithm (Rumelhart et al., 1986).", "startOffset": 92, "endOffset": 116}, {"referenceID": 10, "context": "Such stochastic networks are studied in policy gradient reinforcement learning methods (Williams, 1992; Weaver & Tao, 2001; Peters & Schaal, 2006), probabilistic latent variable models for structured prediction, unsupervised learning of generative models (Tang & Salakhutdinov, 2013; Kingma & Welling, 2014), and most recently, attention and memory networks (Mnih et al., 2014; Zaremba & Sutskever, 2015).", "startOffset": 358, "endOffset": 404}, {"referenceID": 16, "context": "Models with continuous latent variables and simple approximate posteriors can already be trained efficiently using the variational lower bound along with the reparameterization trick, which makes it possible to train both the model and the inference network using backpropagation (Kingma & Welling, 2014; Rezende et al., 2014).", "startOffset": 280, "endOffset": 326}, {"referenceID": 0, "context": "Unbiased estimators based on the likelihood-ratio method tend to be significantly less effective than biased estimators, such as the straight-through method (Bengio et al., 2013; Raiko et al., 2015) and the estimator proposed by Gregor et al.", "startOffset": 157, "endOffset": 198}, {"referenceID": 15, "context": "Unbiased estimators based on the likelihood-ratio method tend to be significantly less effective than biased estimators, such as the straight-through method (Bengio et al., 2013; Raiko et al., 2015) and the estimator proposed by Gregor et al.", "startOffset": 157, "endOffset": 198}, {"referenceID": 0, "context": "Unbiased estimators based on the likelihood-ratio method tend to be significantly less effective than biased estimators, such as the straight-through method (Bengio et al., 2013; Raiko et al., 2015) and the estimator proposed by Gregor et al. (2014). We hypothesize that this is due to the fact that, unlike the biased estimators, the unbiased ones do not take advantage of the gradient information provided by the backpropagation algorithm.", "startOffset": 158, "endOffset": 250}, {"referenceID": 2, "context": "With these models, which are notoriously difficult to train, biased methods often significantly outperform the unbiased ones (Dayan et al., 1995; Gregor et al., 2014; Raiko et al., 2015), except in certain cases (Mnih & Gregor, 2014).", "startOffset": 125, "endOffset": 186}, {"referenceID": 3, "context": "With these models, which are notoriously difficult to train, biased methods often significantly outperform the unbiased ones (Dayan et al., 1995; Gregor et al., 2014; Raiko et al., 2015), except in certain cases (Mnih & Gregor, 2014).", "startOffset": 125, "endOffset": 186}, {"referenceID": 15, "context": "With these models, which are notoriously difficult to train, biased methods often significantly outperform the unbiased ones (Dayan et al., 1995; Gregor et al., 2014; Raiko et al., 2015), except in certain cases (Mnih & Gregor, 2014).", "startOffset": 125, "endOffset": 186}, {"referenceID": 16, "context": ", Schulman et al. (2015)).", "startOffset": 2, "endOffset": 25}, {"referenceID": 2, "context": "Probabilistic latent variable models described by neural networks date back to pioneering early work on sigmoidal belief networks and Helmholtz machines (Neal, 1992; Dayan et al., 1995).", "startOffset": 153, "endOffset": 185}, {"referenceID": 18, "context": "Algorithms based on Markov chain Monte Carlo (Neal, 1992) and mean-field inference (Saul et al., 1996) are theoretically well-grounded, but do not scale well to large models.", "startOffset": 83, "endOffset": 102}, {"referenceID": 6, "context": "The wakesleep algorithm (Hinton et al., 1995) scales to large models, but does not optimize a well definedobjective function, and therefore does not provide convergence guarantees.", "startOffset": 24, "endOffset": 45}, {"referenceID": 3, "context": "A number of these algorithms use likelihood ratio estimators with variance reduction techniques (Ranganath et al., 2014; Mnih & Gregor, 2014; Gregor et al., 2014), inspired by methods from reinforcement learning (Williams, 1992; Weaver & Tao, 2001; Peters & Schaal, 2006).", "startOffset": 96, "endOffset": 162}, {"referenceID": 15, "context": "Instead of performing inference using an inference network, these algorithms either use importance sampling to reweight samples from the prior (Tang & Salakhutdinov, 2013; Raiko et al., 2015), or rely on heuristics for approximate backpropagation through the stochastic units (Bengio et al.", "startOffset": 143, "endOffset": 191}, {"referenceID": 0, "context": ", 2015), or rely on heuristics for approximate backpropagation through the stochastic units (Bengio et al., 2013; Raiko et al., 2015; Gregor et al., 2014).", "startOffset": 92, "endOffset": 154}, {"referenceID": 15, "context": ", 2015), or rely on heuristics for approximate backpropagation through the stochastic units (Bengio et al., 2013; Raiko et al., 2015; Gregor et al., 2014).", "startOffset": 92, "endOffset": 154}, {"referenceID": 3, "context": ", 2015), or rely on heuristics for approximate backpropagation through the stochastic units (Bengio et al., 2013; Raiko et al., 2015; Gregor et al., 2014).", "startOffset": 92, "endOffset": 154}, {"referenceID": 3, "context": "This is further exacerbated by the fact that LR estimators for minibatch training of neural networks typically use m = 1, in order to maximize the variety of examples seen by the network (Kingma & Welling, 2014; Gregor et al., 2014; Rezende et al., 2014; Mnih & Gregor, 2014).", "startOffset": 187, "endOffset": 275}, {"referenceID": 16, "context": "This is further exacerbated by the fact that LR estimators for minibatch training of neural networks typically use m = 1, in order to maximize the variety of examples seen by the network (Kingma & Welling, 2014; Gregor et al., 2014; Rezende et al., 2014; Mnih & Gregor, 2014).", "startOffset": 187, "endOffset": 275}, {"referenceID": 13, "context": "The derivation of MuProp in Section 3 uses a variance reduction technique known as control variates (Paisley et al., 2012).", "startOffset": 100, "endOffset": 122}, {"referenceID": 3, "context": "This is further exacerbated by the fact that LR estimators for minibatch training of neural networks typically use m = 1, in order to maximize the variety of examples seen by the network (Kingma & Welling, 2014; Gregor et al., 2014; Rezende et al., 2014; Mnih & Gregor, 2014). The simplest way to reduce variance is to increase m, but this is computationally expensive. Efficient and effective variance reduction techniques are therefore crucial for the LR estimator to be practical. The derivation of MuProp in Section 3 uses a variance reduction technique known as control variates (Paisley et al., 2012). The main idea is to subtract an analytically tractable term from the LR estimate in order to reduce the variance of the Monte Carlo estimate, and then add back the analytical expectation of this term to recover an unbiased estimator: Ep\u03b8(x)[\u2207\u03b8 log p\u03b8(x) \u00b7 f(x)] = Ep\u03b8(x)[\u2207\u03b8 log p\u03b8(x) \u00b7 (f(x)\u2212 b\u2212 h(x))] + \u03bc (2) In this example, b+ h(x) is a control variate, which is also known as a sample-dependent baseline. The expectation of the baseline needs to be added to the expression to make the resulting estimator unbiased. If the baseline is constant (i.e. h(x) = 0), its contribution to the gradient estimate is always zero in expectation, since \u2211 x p\u03b8(x)\u2207\u03b8 log p\u03b8(x) = \u2211 x\u2207\u03b8p\u03b8(x) = \u2207\u03b8 \u2211 x p\u03b8(x) = \u2207\u03b81 = 0. Otherwise, we must compute the expectation of the baseline (b + h(x))\u2207 log p\u03b8(x) analytically, as \u03bc = Ep\u03b8(x)[\u2207\u03b8 log p\u03b8(x) \u00b7 h(x)]. In our experiments, we utilize three types of variance reduction techniques for the estimators that include the same term as Eq. 2. While these techniques, proposed in a similar context in (Mnih & Gregor, 2014), do not result in the optimal variance reduction, they work well enough to make LR estimators useful in practice.1 For the precise details of these variance reduction techniques, please refer to Mnih & Gregor (2014). For reference, the techniques are:", "startOffset": 212, "endOffset": 1871}, {"referenceID": 13, "context": "The idea of using Taylor expansion as a baseline is also explored by (Paisley et al., 2012) and (Gregor et al.", "startOffset": 69, "endOffset": 91}, {"referenceID": 3, "context": ", 2012) and (Gregor et al., 2014); however, (Paisley et al.", "startOffset": 12, "endOffset": 33}, {"referenceID": 13, "context": ", 2014); however, (Paisley et al., 2012) does not explore the estimator in the context of arbitrary stochastic computational graphs, and (Gregor et al.", "startOffset": 18, "endOffset": 40}, {"referenceID": 3, "context": ", 2012) does not explore the estimator in the context of arbitrary stochastic computational graphs, and (Gregor et al., 2014) chooses a different form of Taylor expansion that makes the estimator biased.", "startOffset": 104, "endOffset": 125}, {"referenceID": 15, "context": "We do not consider the importance-sampling based approaches (Raiko et al., 2015; Tang & Salakhutdinov, 2013; Bornschein & Bengio, 2015; Burda et al., 2015) as these can be interpreted as optimizing a different objective (Burda et al.", "startOffset": 60, "endOffset": 155}, {"referenceID": 1, "context": "We do not consider the importance-sampling based approaches (Raiko et al., 2015; Tang & Salakhutdinov, 2013; Bornschein & Bengio, 2015; Burda et al., 2015) as these can be interpreted as optimizing a different objective (Burda et al.", "startOffset": 60, "endOffset": 155}, {"referenceID": 1, "context": ", 2015) as these can be interpreted as optimizing a different objective (Burda et al., 2015) that requires sampling the latent variables multiple times.", "startOffset": 72, "endOffset": 92}, {"referenceID": 0, "context": "The straight-through estimator (ST) (Bengio et al., 2013; Raiko et al., 2015) is a biased but lowvariance estimator, devised primarily for binary stochastic neurons3.", "startOffset": 36, "endOffset": 77}, {"referenceID": 15, "context": "The straight-through estimator (ST) (Bengio et al., 2013; Raiko et al., 2015) is a biased but lowvariance estimator, devised primarily for binary stochastic neurons3.", "startOffset": 36, "endOffset": 77}, {"referenceID": 3, "context": "\u011d(1/2) = \u2207\u03b8 log p\u03b8(x) \u00b7 (f \u2032(x)T \u00b7 (x\u2212 x\u0304)) (5) If x consists of binary random variables, the expression can be further simplified by using x\u0304 = 1/2 (see (Gregor et al., 2014) for justification); however, for non-quadratic f , the estimator is biased:", "startOffset": 154, "endOffset": 175}, {"referenceID": 15, "context": "For simplicity, here we always assume that the ST estimator includes the derivative of the sigmoid, which is often essential for achieving the best performance (Raiko et al., 2015).", "startOffset": 160, "endOffset": 180}, {"referenceID": 0, "context": "The ST estimator was proposed by Hinton (2012) and named in Bengio et al. (2013). The ST estimator can include the derivative of the sigmoid or not.", "startOffset": 60, "endOffset": 81}, {"referenceID": 0, "context": "The ST estimator was proposed by Hinton (2012) and named in Bengio et al. (2013). The ST estimator can include the derivative of the sigmoid or not. Raiko et al. (2015) derive the version with the derivative of the sigmoid using another interpretation and differentiate it from the original ST estimator in their work.", "startOffset": 60, "endOffset": 169}, {"referenceID": 15, "context": "For MNIST, the output pixels are binarized using the same protocol as in (Raiko et al., 2015).", "startOffset": 73, "endOffset": 93}, {"referenceID": 15, "context": "Given an input x, an output y, and stochastic hidden variables h, the objective is to maximize Ehi\u223cp\u03b8(h|x) [ log ( 1 m \u2211m i=1 p\u03b8(y|hi) )] , an importance-sampled estimate of the likelihood objective (Raiko et al., 2015; Burda et al., 2015).", "startOffset": 199, "endOffset": 239}, {"referenceID": 1, "context": "Given an input x, an output y, and stochastic hidden variables h, the objective is to maximize Ehi\u223cp\u03b8(h|x) [ log ( 1 m \u2211m i=1 p\u03b8(y|hi) )] , an importance-sampled estimate of the likelihood objective (Raiko et al., 2015; Burda et al., 2015).", "startOffset": 199, "endOffset": 239}, {"referenceID": 14, "context": "In this experiment, we follow the setup proposed by Raiko et al. (2015). The two tasks are to predict the lower half of an MNIST digit given the top half, and to predict multiple facial expressions from an average face using Toronto Face dataset (TFD); the output distribution in both tasks exhibits complex multi-modality.", "startOffset": 52, "endOffset": 72}, {"referenceID": 1, "context": ", 2015; Burda et al., 2015). We use m = 1 for training, and m = 100 for validation and testing. For MNIST, a fixed learning rate is chosen from {0.003, 0.001, .., 0.00003}, and the best test result is reported for each method. For the TFD dataset, the learning rate is chosen from the same list, but each learning rate is 10 times smaller. We used a momentum of 0.9 and minibatches of size 100. The input-dependent baseline (IDB) of Mnih & Gregor (2014) uses both the input and output as its input and has a single hidden-layer of 100 Tanh units.", "startOffset": 8, "endOffset": 454}, {"referenceID": 3, "context": "If we opt to use discrete latent variables, we would have to choose between gradient estimators that are biased but have low variance (Gregor et al., 2014) or are unbiased but higher variance (Mnih & Gregor, 2014).", "startOffset": 134, "endOffset": 155}, {"referenceID": 3, "context": "If we opt to use discrete latent variables, we would have to choose between gradient estimators that are biased but have low variance (Gregor et al., 2014) or are unbiased but higher variance (Mnih & Gregor, 2014). As MuProp is unbiased and has relatively low variance due to its use of backpropagation for gradient estimation, we expect it to be particularly well-suited for training such models. We will concentrate on training layered belief networks with either Bernoulli or multinomial latent variables. The model in question is equivalent to a sigmoid belief network if it does not have autoregressive connections, or to fDARN if it has autoregressive connections Gregor et al. (2014). The multinomial model uses 200 latent variables with 10 categories each (k = 10), and thus is referred to as the 200\u00d710 model.", "startOffset": 135, "endOffset": 691}, {"referenceID": 9, "context": "First, we might consider using other functions for the baseline rather than just the Taylor expansion, which could be learned in a manner that resembles Q-learning (Watkins & Dayan, 1992) and target propagation (Lee et al., 2015).", "startOffset": 211, "endOffset": 229}], "year": 2016, "abstractText": "Deep neural networks are powerful parametric models that can be trained efficiently using the backpropagation algorithm. Stochastic neural networks combine the power of large parametric functions with that of graphical models, which makes it possible to learn very complex distributions. However, as backpropagation is not directly applicable to stochastic networks that include discrete sampling operations within their computational graph, training such networks remains difficult. We present MuProp, an unbiased gradient estimator for stochastic networks, designed to make this task easier. MuProp improves on the likelihood-ratio estimator by reducing its variance using a control variate based on the first-order Taylor expansion of a mean-field network. Crucially, unlike prior attempts at using backpropagation for training stochastic networks, the resulting estimator is unbiased and well behaved. Our experiments on structured output prediction and discrete latent variable modeling demonstrate that MuProp yields consistently good performance across a range of difficult tasks.", "creator": "LaTeX with hyperref package"}}}