{"id": "1511.05212", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2015", "title": "Binary embeddings with structured hashed projections", "abstract": "We consider the hashing mechanism for constructing binary data embeddings, that involves pseudo-random projections followed by nonlinear (sign function) mappings. The pseudo-random projection is described by a matrix, where not all entries are independent random variables but instead a fixed ''budget of randomness'' is distributed across the matrix. Such matrices can be efficiently stored in sub-quadratic or even linear space, provide reduction in randomness usage (i.e. number of required random values), and very often lead to computational speed ups. We prove several theoretical results showing that projections via various structured matrices followed by nonlinear mappings accurately preserve the angular distance between input high-dimensional vectors. To the best of our knowledge, these results are the first that give theoretical ground for the use of general structured matrices in the nonlinear setting. Thus, they significantly generalize previous extensions of the Johnson-Lindenstrauss lemma and prove the plausibility of the approach that was so far only heuristically confirmed for some special structured matrices. Consequently, we show that many structured matrices can be used as an efficient information compression mechanism. Our findings also build a better understanding of certain deep architectures, which contain randomly weighted and untrained layers, and yet achieve high performance on different learning tasks. We are interested in how the action of random projection followed by non-linear transformation may influence learning. We empirically verify our theoretical findings and show the dependence of learning via structured hashed projections on the network performance.", "histories": [["v1", "Mon, 16 Nov 2015 23:01:12 GMT  (479kb,D)", "https://arxiv.org/abs/1511.05212v1", "arXiv admin note: text overlap witharXiv:1505.03190"], ["v2", "Tue, 2 Feb 2016 03:56:09 GMT  (555kb,D)", "http://arxiv.org/abs/1511.05212v2", "arXiv admin note: text overlap witharXiv:1505.03190"], ["v3", "Tue, 24 May 2016 01:51:33 GMT  (555kb,D)", "http://arxiv.org/abs/1511.05212v3", "arXiv admin note: text overlap witharXiv:1505.03190"], ["v4", "Tue, 31 May 2016 12:05:06 GMT  (555kb,D)", "http://arxiv.org/abs/1511.05212v4", "arXiv admin note: text overlap witharXiv:1505.03190"], ["v5", "Fri, 1 Jul 2016 16:39:05 GMT  (557kb,D)", "http://arxiv.org/abs/1511.05212v5", "arXiv admin note: text overlap witharXiv:1505.03190"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1505.03190", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["anna choromanska", "krzysztof choromanski", "mariusz bojarski", "tony jebara", "sanjiv kumar", "yann lecun"], "accepted": true, "id": "1511.05212"}, "pdf": {"name": "1511.05212.pdf", "metadata": {"source": "META", "title": "Binary embeddings with structured hashed projections", "authors": ["Anna Choromanska", "Krzysztof Choromanski", "Mariusz Bojarski", "Tony Jebara", "Sanjiv Kumar", "Yann LeCun"], "emails": ["ACHOROMA@CIMS.NYU.EDU", "KCHORO@GOOGLE.COM", "MBOJARSKI@NVIDIA.COM", "JEBARA@CS.COLUMBIA.EDU", "SANJIVK@GOOGLE.COM", "YANN@CS.NYU.EDU"], "sections": [{"heading": null, "text": "1Same contribution."}, {"heading": "1. Introduction", "text": "In fact, most people are able to recognize themselves and understand what they are doing to change and change the world."}, {"heading": "2. Related work", "text": "In fact, most of them will be able to play by the rules they have shown in the past, and they will be able to play by the rules they have shown in the past."}, {"heading": "3. Hashing mechanism", "text": "In this section, we explain our hashing mechanism for reducing dimensionality, which we will analyze next."}, {"heading": "3.1. Structured matrices", "text": "Do we see ourselves in a position to ask ourselves what is the future of humanity, what is the future of humanity? (...) Do we see what is the future of humanity? (...) Do we see what is the future of the world? (...) Do we see what is the future of humanity? (...) Do we see what is the future of the world? (...) Do we see what is the future of the world? (...) Do we see what is the future of the world? (...) Do we see what is the future of the world? (...) Do we see what is the future of humanity? (...) Do we see what is the future of the world? (...) Do we see what is the future of the world? (...) Do we see what is the future of the world? (...) Do we see what is the future of the world? (...) Do we see what is the future of the world? (...) Do we see the future of the world? (...) Do we see the future of the world? (...) Do we see the future of the world?"}, {"heading": "3.2. Hashing methods", "text": "It is a function that is satisfactorily limx (x) and limx (x). (c) We will consider two methods, both of which consist of what we call the pre-processing step, followed by the actual hash step, where the latter consists of pseudo-random projections, followed by nonlinear (drawing function) mapping. (c) The first mechanism we expand is a random diagonal matrix R applied to the data point x, then the L2 normalized matrix H, the next random diagonal matrix D, then the irregular projection matrix P, and finally the applied point applied matrix R to the data point x, then the L2 normalized matrix H, the random diagonal matrix D, then the irregular projection matrix P, and finally the applied function DP applied to the data point x x x, then the L2 normalized matrix H, then the irregular projection matrix P, and finally the applied point applied matrix R to the data point x x x x x x, then the L2 normalized matrix H, then the random diagonal matrix D, then the irregular projection matrix P, and finally the function DP applied point applied to the data point x P, finally, and finally the data point applied matrix R to the data point x x x x x x x x x x x x x x, then the L2 normalized matrix H, the random projection matrix D, and finally the irregular projection matrix P, and finally the function DP applied to the data point of the data point x P, and finally the data point matrix P, and finally the data point matrix R, the data point of the data point x, the data point x, the data point x, the data point x, mx (x)."}, {"heading": "4. Theoretical results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Unbiasedness of the estimator", "text": "We are willing to give theoretical guarantees with respect to the quality of the hash generated. Our guarantees are given for a drawing function, i.e for the definition as follows: \"D's\" s \"s\" s \"s\" s \"s\" s \"s\" s. \"D's\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" D's \"s\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s"}, {"heading": "5. Numerical experiments", "text": "This year it has come to the point that it will be able to retaliate, \"he said.\" We have never waited so long to be able to do so, \"he said."}, {"heading": "6. Conclusions", "text": "This paper shows that structured hash projections maintain the angular spacing between instances of the input data well. Our theoretical results take into account the mapping of the data into the subdimensional space using various structured matrices followed by structured linear projections signed nonlinearity, a nonlinear operation that has not been considered in previous related theoretical work for such a wide range of structured matrices. Of course, the theoretical setting applies to the multilayered network, where the basic components of the architecture perform matrix vector multiplication followed by nonlinear mapping. We test our theoretical results empirically and show how the use of structured hash projections to reduce dimensions affects the performance of neural networks and adjacent classifiers."}, {"heading": "7. Proof of Theorem 4.1", "text": "We start with the following technical problem: Lemma 7.1. Let {Z1,..., Zk = Q = Q = Q = Q = Q = Q Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q ="}, {"heading": "8. Proof of Theorem 4.2", "text": "We will deduce some notations from the last section of the evidence in Theorem 4.1. However, we assume that in this setting no pre-processing is applied with the use of MatricesH andR (1). (1) We assume that the following does not apply (1). (n) We assume that the following does not apply (1). (n) We assume that the following does not apply (1). (n) We assume that the following does not apply (1). (Ui). (UiUi = 1). (Uj = 1). (Uj = 1). (n) We assume that the following does not apply. (n) We assume that the following does not apply. (Ui). (UiUi = 1). (Uj = 1). (Uj = 1)."}, {"heading": "9. Additional figures", "text": "Figure 8a and Figure 8b show how the mean tensile error is affected by the size of the hash, and Figure 8c shows how the mean tensile error changes with the size of the reduction for the experiment with neural networks. In Table 3 we give both the mean and the standard deviation of the tensile error in our neural network experiments. The baseline refers to the network with a hidden layer of 100 hidden units in which all parameters are traversed. Figure 9a shows the original version of Figure 6a (before the zoom). Figure 9b shows the original version of Figure 7a (before the zoom). Finally, Table 4 shows the mean and standard deviation of the test error from the size of the hash (k) / size of the reduction (n / k) for 1-NN."}], "references": [{"title": "Database-friendly random projections: Johnson-lindenstrauss with binary coins", "author": ["D. Achlioptas"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "Achlioptas,? \\Q2003\\E", "shortCiteRegEx": "Achlioptas", "year": 2003}, {"title": "An Introduction to Kernel and NearestNeighbor Nonparametric Regression", "author": ["N.S. Altman"], "venue": "The American Statistician,", "citeRegEx": "Altman,? \\Q1992\\E", "shortCiteRegEx": "Altman", "year": 1992}, {"title": "Random projection in dimensionality reduction: Applications to image and text data", "author": ["E. Bingham", "H. Mannila"], "venue": "In KDD,", "citeRegEx": "Bingham and Mannila,? \\Q2001\\E", "shortCiteRegEx": "Bingham and Mannila", "year": 2001}, {"title": "Random projection, margins, kernels, and feature-selection", "author": ["A. Blum"], "venue": "In SLSFS,", "citeRegEx": "Blum,? \\Q2006\\E", "shortCiteRegEx": "Blum", "year": 2006}, {"title": "Initialization and self-organized optimization of recurrent neural network connectivity", "author": ["J. Boedecker", "O. Obst", "N.M. Mayer", "M. Asada"], "venue": "HFSP journal,", "citeRegEx": "Boedecker et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Boedecker et al\\.", "year": 2009}, {"title": "Online algorithms and stochastic approximations. In Online Learning and Neural Networks", "author": ["L. Bottou"], "venue": null, "citeRegEx": "Bottou,? \\Q1998\\E", "shortCiteRegEx": "Bottou", "year": 1998}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["Charikar", "Moses"], "venue": "In Proceedings on 34th Annual ACM Symposium on Theory of Computing, May 19-21,", "citeRegEx": "Charikar and Moses.,? \\Q2002\\E", "shortCiteRegEx": "Charikar and Moses.", "year": 2002}, {"title": "Compressing neural networks with the hashing", "author": ["W. Chen", "J.T. Wilson", "S. Tyree", "K.Q. Weinberger", "Y. Chen"], "venue": "trick. CoRR,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "An exploration of parameter redundancy in deep networks with circulant projections", "author": ["Cheng", "Yu", "Felix X", "Feris", "Rog\u00e9rio Schmidt", "Kumar", "Sanjiv", "Choudhary", "Alok N", "Chang", "Shih-Fu"], "venue": "IEEE International Conference on Computer Vision,", "citeRegEx": "Cheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2015}, {"title": "Differentially-private learning of low dimensional manifolds", "author": ["A. Choromanska", "K. Choromanski", "G. Jagannathan", "C. Monteleoni"], "venue": "In ALT,", "citeRegEx": "Choromanska et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2013}, {"title": "The loss surfaces of multilayer networks", "author": ["A. Choromanska", "M. Henaff", "M. Mathieu", "Arous", "G. Ben", "Y. LeCun"], "venue": "In AISTATS,", "citeRegEx": "Choromanska et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2015}, {"title": "Recycling randomness with structure for sublinear time kernel", "author": ["Choromanski", "Krzysztof", "Sindhwani", "Vikas"], "venue": "expansions. ICML2016,", "citeRegEx": "Choromanski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Choromanski et al\\.", "year": 2016}, {"title": "Triplespin - a generic compact paradigm for fast machine learning computations", "author": ["Choromanski", "Krzysztof", "Fagan", "Francois", "Gouy-Pailler", "C\u00e9dric", "Morvan", "Anne", "Sarl\u00f3s", "Tam\u00e1s", "Atif", "Jamal"], "venue": "CoRR, abs/1605.09046,", "citeRegEx": "Choromanski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Choromanski et al\\.", "year": 2016}, {"title": "Learning mixtures of gaussians", "author": ["S. Dasgupta"], "venue": "In FOCS,", "citeRegEx": "Dasgupta,? \\Q1999\\E", "shortCiteRegEx": "Dasgupta", "year": 1999}, {"title": "Experiments with random projection", "author": ["S. Dasgupta"], "venue": "In UAI,", "citeRegEx": "Dasgupta,? \\Q2000\\E", "shortCiteRegEx": "Dasgupta", "year": 2000}, {"title": "Random projection trees and low dimensional manifolds", "author": ["S. Dasgupta", "Y. Freund"], "venue": "In STOC,", "citeRegEx": "Dasgupta and Freund,? \\Q2008\\E", "shortCiteRegEx": "Dasgupta and Freund", "year": 2008}, {"title": "Predicting parameters in deep learning", "author": ["M. Denil", "B. Shakibi", "L. Dinh", "M. Ranzato", "N.D. Freitas"], "venue": "In NIPS", "citeRegEx": "Denil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2013}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["E. Denton", "W. Zaremba", "J. Bruna", "Y. LeCun", "R. Fergus"], "venue": "In NIPS", "citeRegEx": "Denton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2014}, {"title": "Random projection for high dimensional data clustering: A cluster ensemble approach", "author": ["X.Z. Fern", "C.E. Brodley"], "venue": "In ICML,", "citeRegEx": "Fern and Brodley,? \\Q2003\\E", "shortCiteRegEx": "Fern and Brodley", "year": 2003}, {"title": "Compressed sensing, sparsity, and dimensionality in neuronal information processing and data analysis", "author": ["S. Ganguli", "H. Sompolinsky"], "venue": "Annual Review of Neuroscience,", "citeRegEx": "Ganguli and Sompolinsky,? \\Q2012\\E", "shortCiteRegEx": "Ganguli and Sompolinsky", "year": 2012}, {"title": "Deep neural networks with random gaussian weights: A universal classification strategy", "author": ["R. Giryes", "G. Sapiro", "A.M. Bronstein"], "venue": "CoRR, abs/1504.08291,", "citeRegEx": "Giryes et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Giryes et al\\.", "year": 2015}, {"title": "Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval", "author": ["Y. Gong", "S. Lazebnik", "A. Gordo", "F. Perronnin"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Gong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2013}, {"title": "Angular quantization-based binary codes for fast similarity search", "author": ["Gong", "Yunchao", "Kumar", "Sanjiv", "Verma", "Vishal", "Lazebnik", "Svetlana"], "venue": "In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems", "citeRegEx": "Gong et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2012}, {"title": "Learning binary codes for high-dimensional data using bilinear projections", "author": ["Gong", "Yunchao", "Kumar", "Sanjiv", "Rowley", "Henry A", "Lazebnik", "Svetlana"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Gong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2013}, {"title": "Toeplitz compressed sensing matrices with applications to sparse channel estimation", "author": ["J. Haupt", "W.U. Bajwa", "G. Raz", "R. Nowak"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Haupt et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Haupt et al\\.", "year": 2010}, {"title": "Johnson-lindenstrauss lemma for circulant matrices", "author": ["A. Hinrichs", "J. Vybral"], "venue": "Random Struct. Algorithms,", "citeRegEx": "Hinrichs and Vybral,? \\Q2011\\E", "shortCiteRegEx": "Hinrichs and Vybral", "year": 2011}, {"title": "Extreme learning machine: Theory and applications", "author": ["Huang", "G.-B", "Zhu", "Q.-Y", "Siew", "C.-K"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2006}, {"title": "Robust 1-bit compressive sensing via binary stable embeddings of sparse vectors", "author": ["L. Jacques", "J.N. Laska", "P. Boufounos", "R.G. Baraniuk"], "venue": "CoRR, abs/1104.3160,", "citeRegEx": "Jacques et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jacques et al\\.", "year": 2011}, {"title": "Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication", "author": ["H. Jaeger", "H. Haas"], "venue": "Science, pp", "citeRegEx": "Jaeger and Haas,? \\Q2004\\E", "shortCiteRegEx": "Jaeger and Haas", "year": 2004}, {"title": "Efficient and Robust Compressed Sensing Using Optimized Expander Graphs", "author": ["S. Jafarpour", "W. Xu", "B. Hassibi", "R. Calderbank"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Jafarpour et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jafarpour et al\\.", "year": 2009}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. LeCun"], "venue": "In ICCV,", "citeRegEx": "Jarrett et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jarrett et al\\.", "year": 2009}, {"title": "Suprema of chaos processes and the restricted isometry property", "author": ["F. Krahmer", "S. Mendelson", "H. Rauhut"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "Krahmer et al\\.,? \\Q1877\\E", "shortCiteRegEx": "Krahmer et al\\.", "year": 1877}, {"title": "Very sparse random projections", "author": ["P. Li", "T.J. Hastie", "K.W. Church"], "venue": "In KDD,", "citeRegEx": "Li et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Li et al\\.", "year": 2006}, {"title": "Random projectionbased multiplicative data perturbation for privacy preserving distributed data mining", "author": ["K. Liu", "H. Kargupta", "J. Ryan"], "venue": "IEEE Trans. on Knowl. and Data Eng.,", "citeRegEx": "Liu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2006}, {"title": "Fast training of convolutional networks through ffts", "author": ["M. Mathieu", "M. Henaff", "Y. LeCun"], "venue": "In ICLR,", "citeRegEx": "Mathieu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mathieu et al\\.", "year": 2014}, {"title": "An Evaluation of the Invariance Properties of a Biologically-Inspired System for Unconstrained Face Recognition", "author": ["N. Pinto", "D.D. Cox"], "venue": "In BIONETICS,", "citeRegEx": "Pinto and Cox,? \\Q2010\\E", "shortCiteRegEx": "Pinto and Cox", "year": 2010}, {"title": "A high-throughput screening approach to discovering good forms of biologically inspired visual representation", "author": ["N. Pinto", "D. Doukhan", "J.J. DiCarlo", "D.D. Cox"], "venue": "PLoS Computational Biology,", "citeRegEx": "Pinto et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Pinto et al\\.", "year": 2009}, {"title": "Dimension reduction by random hyperplane tessellations", "author": ["Y. Plan", "R. Vershynin"], "venue": "Discrete & Computational Geometry,", "citeRegEx": "Plan and Vershynin,? \\Q2014\\E", "shortCiteRegEx": "Plan and Vershynin", "year": 2014}, {"title": "Locality-sensitive binary codes from shift-invariant kernels", "author": ["M. Raginsky", "S. Lazebnik"], "venue": "In NIPS", "citeRegEx": "Raginsky and Lazebnik,? \\Q2009\\E", "shortCiteRegEx": "Raginsky and Lazebnik", "year": 2009}, {"title": "Restricted isometries for partial random circulant matrices", "author": ["H. Rauhut", "J.K. Romberg", "J.A. Tropp"], "venue": "CoRR, abs/1010.1847,", "citeRegEx": "Rauhut et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rauhut et al\\.", "year": 2010}, {"title": "On random weights and unsupervised feature learning", "author": ["A. Saxe", "P.W. Koh", "Z. Chen", "M. Bhand", "B. Suresh", "A. Ng"], "venue": "In ICML,", "citeRegEx": "Saxe et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2011}, {"title": "Structure preserving embedding", "author": ["Shaw", "Blake", "Jebara", "Tony"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Shaw et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Shaw et al\\.", "year": 2009}, {"title": "Structured transforms for small-footprint deep learning", "author": ["V. Sindhwani", "T. Sainath", "S. Kumar"], "venue": "In NIPS,", "citeRegEx": "Sindhwani et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sindhwani et al\\.", "year": 2015}, {"title": "Algorithmic derandomization via complexity theory", "author": ["D. Sivakumar"], "venue": "In STOC,", "citeRegEx": "Sivakumar,? \\Q2002\\E", "shortCiteRegEx": "Sivakumar", "year": 2002}, {"title": "Fast approximations to structured sparse coding and applications to object classification", "author": ["Szlam", "Arthur", "Gregor", "Karol", "LeCun", "Yann"], "venue": "In Computer Vision - ECCV 2012 - 12th European Conference on Computer", "citeRegEx": "Szlam et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Szlam et al\\.", "year": 2012}, {"title": "A variant of the johnsonlindenstrauss lemma for circulant matrices", "author": ["J. Vybral"], "venue": "Journal of Functional Analysis,", "citeRegEx": "Vybral,? \\Q2011\\E", "shortCiteRegEx": "Vybral", "year": 2011}, {"title": "Semisupervised hashing for large-scale search", "author": ["Wang", "Jun", "Kumar", "Sanjiv", "Chang", "Shih-Fu"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Short-term memory in orthogonal neural networks", "author": ["O.L. White", "D.D. Lee", "H. Sompolinsky"], "venue": "Physical review letters,", "citeRegEx": "White et al\\.,? \\Q2004\\E", "shortCiteRegEx": "White et al\\.", "year": 2004}, {"title": "The restricted isometry property for block diagonal matrices", "author": ["H.L. Yap", "A. Eftekhari", "M.B. Wakin", "C.J. Rozell"], "venue": "In CISS,", "citeRegEx": "Yap et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yap et al\\.", "year": 2011}, {"title": "Binary embedding: Fundamental limits and fast algorithm", "author": ["X. Yi", "C. Caramanis", "E. Price"], "venue": "CoRR, abs/1502.05746,", "citeRegEx": "Yi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yi et al\\.", "year": 2015}, {"title": "Circulant binary embedding", "author": ["F.X. Yu", "S. Kumar", "Y. Gong", "Chang", "S.-F"], "venue": "In ICML,", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "On binary embedding using circulant matrices", "author": ["Yu", "Felix X", "Bhaskara", "Aditya", "Kumar", "Sanjiv", "Gong", "Yunchao", "Chang", "Shih-Fu"], "venue": "CoRR, abs/1511.06480,", "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 22, "context": ", 2008), (Gong et al., 2012), (Gong et al.", "startOffset": 9, "endOffset": 28}, {"referenceID": 46, "context": ", 2013a), (Wang et al., 2012), (Gong et al.", "startOffset": 10, "endOffset": 29}, {"referenceID": 50, "context": ", 2013b), (Plan & Vershynin, 2014), (Yu et al., 2014), (Yi et al.", "startOffset": 36, "endOffset": 53}, {"referenceID": 49, "context": ", 2014), (Yi et al., 2015)), and in particular it was observed that by using linear projections and then applying sign function as a nonlinear map one does not loose completely the information about the angular distance between vectors, but instead the information might be approximately reconstructed from the Hamming distance between hashes.", "startOffset": 9, "endOffset": 26}, {"referenceID": 43, "context": "We prove an extension of the Johnson-Lindenstrauss lemma (Sivakumar, 2002) for general pseudo-random structured projections followed by nonlinear mappings.", "startOffset": 57, "endOffset": 74}, {"referenceID": 45, "context": "This result is also new compared to previous extensions (Hinrichs & Vybral, 2011; Vybral, 2011) of the Johnson-Lindenstrauss lemma, that consider special cases of our structured projections (namely: circulant matrices) and do not consider at all the action of the non-linear mapping.", "startOffset": 56, "endOffset": 95}, {"referenceID": 49, "context": "We give theoretical explanation of the approach that was so far only heuristically confirmed for some special structured matrices (see: (Yi et al., 2015), (Yu et al.", "startOffset": 136, "endOffset": 153}, {"referenceID": 50, "context": ", 2015), (Yu et al., 2014)).", "startOffset": 9, "endOffset": 26}, {"referenceID": 40, "context": "(Saxe et al., 2011; Sindhwani et al., 2015; Cheng et al., 2015; Mathieu et al., 2014)).", "startOffset": 0, "endOffset": 85}, {"referenceID": 42, "context": "(Saxe et al., 2011; Sindhwani et al., 2015; Cheng et al., 2015; Mathieu et al., 2014)).", "startOffset": 0, "endOffset": 85}, {"referenceID": 8, "context": "(Saxe et al., 2011; Sindhwani et al., 2015; Cheng et al., 2015; Mathieu et al., 2014)).", "startOffset": 0, "endOffset": 85}, {"referenceID": 34, "context": "(Saxe et al., 2011; Sindhwani et al., 2015; Cheng et al., 2015; Mathieu et al., 2014)).", "startOffset": 0, "endOffset": 85}, {"referenceID": 40, "context": "Little is known from the theoretical point of view about these fast deep architectures, which achieve significant speed ups of computation and space usage reduction with simultaneous little or no loss in performance (Saxe et al., 2011; Cheng et al., 2015; Sindhwani et al., 2015; Jarrett et al., 2009; Pinto et al., 2009; Pinto & Cox, 2010; Huang et al., 2006).", "startOffset": 216, "endOffset": 360}, {"referenceID": 8, "context": "Little is known from the theoretical point of view about these fast deep architectures, which achieve significant speed ups of computation and space usage reduction with simultaneous little or no loss in performance (Saxe et al., 2011; Cheng et al., 2015; Sindhwani et al., 2015; Jarrett et al., 2009; Pinto et al., 2009; Pinto & Cox, 2010; Huang et al., 2006).", "startOffset": 216, "endOffset": 360}, {"referenceID": 42, "context": "Little is known from the theoretical point of view about these fast deep architectures, which achieve significant speed ups of computation and space usage reduction with simultaneous little or no loss in performance (Saxe et al., 2011; Cheng et al., 2015; Sindhwani et al., 2015; Jarrett et al., 2009; Pinto et al., 2009; Pinto & Cox, 2010; Huang et al., 2006).", "startOffset": 216, "endOffset": 360}, {"referenceID": 30, "context": "Little is known from the theoretical point of view about these fast deep architectures, which achieve significant speed ups of computation and space usage reduction with simultaneous little or no loss in performance (Saxe et al., 2011; Cheng et al., 2015; Sindhwani et al., 2015; Jarrett et al., 2009; Pinto et al., 2009; Pinto & Cox, 2010; Huang et al., 2006).", "startOffset": 216, "endOffset": 360}, {"referenceID": 36, "context": "Little is known from the theoretical point of view about these fast deep architectures, which achieve significant speed ups of computation and space usage reduction with simultaneous little or no loss in performance (Saxe et al., 2011; Cheng et al., 2015; Sindhwani et al., 2015; Jarrett et al., 2009; Pinto et al., 2009; Pinto & Cox, 2010; Huang et al., 2006).", "startOffset": 216, "endOffset": 360}, {"referenceID": 26, "context": "Little is known from the theoretical point of view about these fast deep architectures, which achieve significant speed ups of computation and space usage reduction with simultaneous little or no loss in performance (Saxe et al., 2011; Cheng et al., 2015; Sindhwani et al., 2015; Jarrett et al., 2009; Pinto et al., 2009; Pinto & Cox, 2010; Huang et al., 2006).", "startOffset": 216, "endOffset": 360}, {"referenceID": 16, "context": "These findings coincide with the notion of high redundancy in network parametrization (Denil et al., 2013; Denton et al., 2014; Choromanska et al., 2015).", "startOffset": 86, "endOffset": 153}, {"referenceID": 17, "context": "These findings coincide with the notion of high redundancy in network parametrization (Denil et al., 2013; Denton et al., 2014; Choromanska et al., 2015).", "startOffset": 86, "endOffset": 153}, {"referenceID": 10, "context": "These findings coincide with the notion of high redundancy in network parametrization (Denil et al., 2013; Denton et al., 2014; Choromanska et al., 2015).", "startOffset": 86, "endOffset": 153}, {"referenceID": 1, "context": "Finally, we show how our structured nonlinear embeddings can be used in the knn setting (Altman, 1992).", "startOffset": 88, "endOffset": 102}, {"referenceID": 13, "context": "The idea of using random projections to facilitate learning with high-dimensional data stems from the early work on random projections (Dasgupta, 1999) showing in particular that learning of high-dimensional mixtures of Gaussians can be simplified when first projecting the data into a randomly chosen subspace of low dimension (this is a consequence of the curse of dimensionality and the fact that high-dimensional data often has low intrinsic dimensionality).", "startOffset": 135, "endOffset": 151}, {"referenceID": 14, "context": "This idea was subsequently successfully applied to both synthetic and real datasets (Dasgupta, 2000; Bingham & Mannila, 2001), and then adopted to a number of learning approaches such as random projection trees (Dasgupta & Freund, 2008), kernel and featureselection techniques (Blum, 2006), clustering (Fern & Brodley, 2003), privacy-preserving machine learning (Liu et al.", "startOffset": 84, "endOffset": 125}, {"referenceID": 3, "context": "This idea was subsequently successfully applied to both synthetic and real datasets (Dasgupta, 2000; Bingham & Mannila, 2001), and then adopted to a number of learning approaches such as random projection trees (Dasgupta & Freund, 2008), kernel and featureselection techniques (Blum, 2006), clustering (Fern & Brodley, 2003), privacy-preserving machine learning (Liu et al.", "startOffset": 277, "endOffset": 289}, {"referenceID": 33, "context": "This idea was subsequently successfully applied to both synthetic and real datasets (Dasgupta, 2000; Bingham & Mannila, 2001), and then adopted to a number of learning approaches such as random projection trees (Dasgupta & Freund, 2008), kernel and featureselection techniques (Blum, 2006), clustering (Fern & Brodley, 2003), privacy-preserving machine learning (Liu et al., 2006; Choromanska et al., 2013), learning with large databases (Achlioptas, 2003), sparse learning settings (Li et al.", "startOffset": 362, "endOffset": 406}, {"referenceID": 9, "context": "This idea was subsequently successfully applied to both synthetic and real datasets (Dasgupta, 2000; Bingham & Mannila, 2001), and then adopted to a number of learning approaches such as random projection trees (Dasgupta & Freund, 2008), kernel and featureselection techniques (Blum, 2006), clustering (Fern & Brodley, 2003), privacy-preserving machine learning (Liu et al., 2006; Choromanska et al., 2013), learning with large databases (Achlioptas, 2003), sparse learning settings (Li et al.", "startOffset": 362, "endOffset": 406}, {"referenceID": 0, "context": ", 2013), learning with large databases (Achlioptas, 2003), sparse learning settings (Li et al.", "startOffset": 39, "endOffset": 57}, {"referenceID": 32, "context": ", 2013), learning with large databases (Achlioptas, 2003), sparse learning settings (Li et al., 2006), and more recently - deep learning (see (Saxe et al.", "startOffset": 84, "endOffset": 101}, {"referenceID": 40, "context": ", 2006), and more recently - deep learning (see (Saxe et al., 2011) for convenient review of such approaches).", "startOffset": 48, "endOffset": 67}, {"referenceID": 20, "context": "Using linear projections with completely random Gaussian weights, instead of learned ones, was recently studied from both theoretical and practical point of view in (Giryes et al., 2015), but that work did not consider structured matrices which is a central point of our interest since structured matrices can be stored much more efficiently.", "startOffset": 165, "endOffset": 186}, {"referenceID": 13, "context": "Beyond applying methods that use random Gaussian matrix projections (Dasgupta, 1999; 2000; Giryes et al., 2015) and random binary matrix projections (Achlioptas, 2003), it is also possible to construct deterministic projections that preserve angles and distances (Jafarpour et al.", "startOffset": 68, "endOffset": 111}, {"referenceID": 20, "context": "Beyond applying methods that use random Gaussian matrix projections (Dasgupta, 1999; 2000; Giryes et al., 2015) and random binary matrix projections (Achlioptas, 2003), it is also possible to construct deterministic projections that preserve angles and distances (Jafarpour et al.", "startOffset": 68, "endOffset": 111}, {"referenceID": 0, "context": ", 2015) and random binary matrix projections (Achlioptas, 2003), it is also possible to construct deterministic projections that preserve angles and distances (Jafarpour et al.", "startOffset": 45, "endOffset": 63}, {"referenceID": 29, "context": ", 2015) and random binary matrix projections (Achlioptas, 2003), it is also possible to construct deterministic projections that preserve angles and distances (Jafarpour et al., 2009).", "startOffset": 159, "endOffset": 183}, {"referenceID": 24, "context": "The point-wise nonlinearity was not considered in many previous works on structured matrices (Haupt et al., 2010; Rauhut et al., 2010; Krahmer et al., 2014; Yap et al., 2011) (moreover note that these works also consider the set of structured matrices which is a strict subset of the class of matrices considered here).", "startOffset": 93, "endOffset": 174}, {"referenceID": 39, "context": "The point-wise nonlinearity was not considered in many previous works on structured matrices (Haupt et al., 2010; Rauhut et al., 2010; Krahmer et al., 2014; Yap et al., 2011) (moreover note that these works also consider the set of structured matrices which is a strict subset of the class of matrices considered here).", "startOffset": 93, "endOffset": 174}, {"referenceID": 48, "context": "The point-wise nonlinearity was not considered in many previous works on structured matrices (Haupt et al., 2010; Rauhut et al., 2010; Krahmer et al., 2014; Yap et al., 2011) (moreover note that these works also consider the set of structured matrices which is a strict subset of the class of matrices considered here).", "startOffset": 93, "endOffset": 174}, {"referenceID": 46, "context": "Designing binary embeddings for high dimensional data with low distortion is addressed in many recent works (Weiss et al., 2008; Wang et al., 2012; Gong et al., 2013b;a; 2012; Yu et al., 2014; Yi et al., 2015; Raginsky & Lazebnik, 2009; Salakhutdinov & Hinton, 2009).", "startOffset": 108, "endOffset": 266}, {"referenceID": 50, "context": "Designing binary embeddings for high dimensional data with low distortion is addressed in many recent works (Weiss et al., 2008; Wang et al., 2012; Gong et al., 2013b;a; 2012; Yu et al., 2014; Yi et al., 2015; Raginsky & Lazebnik, 2009; Salakhutdinov & Hinton, 2009).", "startOffset": 108, "endOffset": 266}, {"referenceID": 49, "context": "Designing binary embeddings for high dimensional data with low distortion is addressed in many recent works (Weiss et al., 2008; Wang et al., 2012; Gong et al., 2013b;a; 2012; Yu et al., 2014; Yi et al., 2015; Raginsky & Lazebnik, 2009; Salakhutdinov & Hinton, 2009).", "startOffset": 108, "endOffset": 266}, {"referenceID": 49, "context": "In the context of our work, one of the recent articles (Yi et al., 2015) is especially important since the authors introduce the pipeline of constructing hashes with the use of structured matrices in the linear step, instead of completely random ones.", "startOffset": 55, "endOffset": 72}, {"referenceID": 27, "context": "They prove several theoretical results regarding the quality of the produced hash, and extend some previous theoretical results (Jacques et al., 2011; Plan & Vershynin, 2014).", "startOffset": 128, "endOffset": 174}, {"referenceID": 50, "context": "Some general results (unbiasedness of the angular distance estimator) were also known for short hashing pipelines involving circulant matrices ((Yu et al., 2014)).", "startOffset": 144, "endOffset": 161}, {"referenceID": 49, "context": "In contrast to (Yi et al., 2015), we present our theoretical results for simpler hashing models (our hashing mechanism is explained in Section 3 and consists of two very simple steps that we call preprocessing step and the actual hashing step, where", "startOffset": 15, "endOffset": 32}, {"referenceID": 51, "context": "In (Yu et al., 2015) theoretical guarantees regarding bounding the variance of the angle estimator in the circulant setting were presented.", "startOffset": 3, "endOffset": 20}, {"referenceID": 11, "context": "Strong concentration results regarding several structured matrices were given in (Choromanski & Sindhwani, 2016; Choromanski et al., 2016), following our work.", "startOffset": 81, "endOffset": 138}, {"referenceID": 26, "context": "Introducing randomness to networks was explored for various architectures, in example feedforward networks (Huang et al., 2006), convolutional networks (Jarrett et al.", "startOffset": 107, "endOffset": 127}, {"referenceID": 30, "context": ", 2006), convolutional networks (Jarrett et al., 2009; Saxe et al., 2011), and recurrent networks (Jaeger & Haas, 2004; White et al.", "startOffset": 32, "endOffset": 73}, {"referenceID": 40, "context": ", 2006), convolutional networks (Jarrett et al., 2009; Saxe et al., 2011), and recurrent networks (Jaeger & Haas, 2004; White et al.", "startOffset": 32, "endOffset": 73}, {"referenceID": 47, "context": ", 2011), and recurrent networks (Jaeger & Haas, 2004; White et al., 2004; Boedecker et al., 2009).", "startOffset": 32, "endOffset": 97}, {"referenceID": 4, "context": ", 2011), and recurrent networks (Jaeger & Haas, 2004; White et al., 2004; Boedecker et al., 2009).", "startOffset": 32, "endOffset": 97}, {"referenceID": 7, "context": "Very recently (see: (Chen et al., 2015)) it was empirically showed that hashing in neural nets may achieve drastic reduction in model sizes with no significant loss of the quality, by heavily exploiting the phenomenon of redundancies in neural nets.", "startOffset": 20, "endOffset": 39}, {"referenceID": 7, "context": "HashedNets introduced in (Chen et al., 2015) do not give any theoretical guarantees regarding the quality of the proposed hashing.", "startOffset": 25, "endOffset": 44}, {"referenceID": 44, "context": "Structured hashing was applied also in (Szlam et al., 2012), but in a very different context than ours.", "startOffset": 39, "endOffset": 59}, {"referenceID": 5, "context": "All networks were trained for 30 epochs using SGD (Bottou, 1998).", "startOffset": 50, "endOffset": 64}, {"referenceID": 50, "context": "Toeplitz matrix-vector multiplications can be efficiently implemented via Fast Fourier Transform (Yu et al., 2014).", "startOffset": 97, "endOffset": 114}], "year": 2016, "abstractText": "We consider the hashing mechanism for constructing binary embeddings, that involves pseudo-random projections followed by nonlinear (sign function) mappings. The pseudorandom projection is described by a matrix, where not all entries are independent random variables but instead a fixed \u201cbudget of randomness\u201d is distributed across the matrix. Such matrices can be efficiently stored in sub-quadratic or even linear space, provide reduction in randomness usage (i.e. number of required random values), and very often lead to computational speed ups. We prove several theoretical results showing that projections via various structured matrices followed by nonlinear mappings accurately preserve the angular distance between input highdimensional vectors. To the best of our knowledge, these results are the first that give theoretical ground for the use of general structured matrices in the nonlinear setting. In particular, they generalize previous extensions of the JohnsonLindenstrauss lemma and prove the plausibility of the approach that was so far only heuristically confirmed for some special structured matrices. Consequently, we show that many structured matrices can be used as an efficient information compression mechanism. Our findings build a better understanding of certain deep architectures, which contain randomly weighted and untrained layers, and yet achieve high performance on different learning tasks. We empirically verify our theoretical findings and show the dependence of learning via structured hashed projections on the performance of neural network as well as nearest neighbor classifier. Equal contribution.", "creator": "LaTeX with hyperref package"}}}