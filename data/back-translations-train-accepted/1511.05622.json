{"id": "1511.05622", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2015", "title": "Predicting distributions with Linearizing Belief Networks", "abstract": "Conditional belief networks introduce stochastic binary variables in neural networks. Contrary to a classical neural network, a belief network can predict more than the expected value of the output $Y$ given the input $X$. It can predict a distribution of outputs $Y$ which is useful when an input can admit multiple outputs whose average is not necessarily a valid answer. Such networks are particularly relevant to inverse problem such as image prediction for denoising, or text to speech. However, traditional sigmoid belief networks are hard to train and are not suited to continuous problems. This work introduces a new family of networks called linearizing belief nets or LBNs. A LBN decomposes into a deep linear network where each linear unit can be turned on or off by non-deterministic binary latent units. It is a universal approximator of real-valued conditional distributions and can be trained using gradient descent. Moreover, the linear pathways efficiently propagate continuous information and they act as multiplicative skip-connections that help optimization by removing gradient diffusion. This yields a model which trains efficiently and improves the state-of-the-art on image denoising and facial expression generation with the Toronto faces dataset.", "histories": [["v1", "Tue, 17 Nov 2015 23:50:35 GMT  (1154kb,D)", "https://arxiv.org/abs/1511.05622v1", null], ["v2", "Fri, 20 Nov 2015 00:40:38 GMT  (1154kb,D)", "http://arxiv.org/abs/1511.05622v2", null], ["v3", "Tue, 24 Nov 2015 01:45:01 GMT  (1154kb,D)", "http://arxiv.org/abs/1511.05622v3", null], ["v4", "Mon, 2 May 2016 03:22:01 GMT  (1309kb,D)", "http://arxiv.org/abs/1511.05622v4", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["yann n dauphin", "david grangier"], "accepted": true, "id": "1511.05622"}, "pdf": {"name": "1511.05622.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["LINEARIZING BELIEF NETWORKS", "Yann N. Dauphin", "David Grangier"], "emails": ["ynd@fb.com", "grangier@fb.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, it is the case that one is able to follow the rules that one had in the past and then move to another world, in which one is able to create a new world."}, {"heading": "2 LINEARIZING BELIEF NETS", "text": "x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x,"}, {"heading": "3 LEARNING NON-DETERMINISTIC GATING UNITS", "text": "We implement g with a neural network that keeps training simple without compromising the power of the model. An easy choice would be to use g (Wx) = \u03c3 (Wx + b) with the sigmoid function \u03c3: x \u2192 11 + exp (\u2212 x) to decide how likely we are to switch on a unit. This function prefers activations Wix that are greater than their learned threshold bi. Our empirical evaluation has inevitably found better results with deeper functions. Deeper functions allow the gating units to model richer interactions between the factors. Therefore, we suggest a sigmoid multilayer networkg (Wx) = \u03c3 (W (2) \u03c3 (W (1)) + llo ov (1) + b (2)) with the parameters {W (1), W (2), W (2)."}, {"heading": "4 DEEP LINEARIZING BELIEF NETS", "text": "Multiple layers of non-deterministic variables g (1),.., g (L) can be beneficial, which factorises the latent variable distribution as p (g | x) = VP (g (l) | x, g (l) \u2212 1) and increases modeling efficiently, resulting deep LBN is a hierarchical mixture of layers of common factors. This extension yields the following density for two layers p (y | x) = x (1), g (2) p (g (2) | x, g (1) p (x), g (1) | x) p (y (x), g (1), g (2). In this case, the linear expert adds a new linear layer together with the new gating unit (y | x)."}, {"heading": "5 RELATED WORK", "text": "In fact, most of them will be able to move to a different world, in which they will be able to move to a different world than they are able to move to a different world."}, {"heading": "6 EXPERIMENTS", "text": "In this section, the modeling performance of LBNs and other stochastic networks is evaluated using multimodal distributions, experimentally confirming the claim that LBNs learn faster and generalize better than other stochastic networks described in the literature. To this end, we consider the tasks of modeling facial expressions and image denosis using benchmark datasets. We train networks with the gradient-based optimizer Adam (Kingma & Ba, 2014) and the parameter initialization of (Glorot & Bengio, 2010). We found it optimal to initialize the distortions of all units in the gating networks to \u2212 2 in order to promote frugality. Hyperparameters of the network are cross-validated by a grid search where the learning rate is always taken from {10 \u2212 3, 10 \u2212 4, 10 \u2212 5} while the other hyperparameters are found in a task-specific manner."}, {"heading": "6.1 MODELING FACIAL EXPRESSIONS", "text": "This section compares different approaches to stochastic feed-forward neural networks on the task of predicting the distribution of a person's facial expressions with a neutral expression as in Tang & Salakhutdinov (2013) and Raiko et al. (2014).The input x is the average face of the person, and we have a distribution of images ysad,.., yangry of that person with 7 different emotions.The goal is also to be able to produce the complete set of facial expressions for a face that we have not yet seened.The images are taken from the Toronto Face Dataset (TFD) (Susskind et al., 2010), which contains 4,000 images of 900 people who were asked to display 7 different emotions.After the hiring of Tang & Salakhutdinov (2013), we randomly selected 95 people with 1,318 images for training, 5 subjects with 68 images for validation, and 24 individuals in total 343 images for testing."}, {"heading": "6.2 IMAGE DENOISING", "text": "It is a common and challenging task to evaluate unattended models of nature images. Typically, the goal is to remove homogeneous additive debris from the input. Noise destroys information, and so the model must derive the original image from the corrupt signal. To do this, the model must discover local statistics of the distribution of images in order to place a corrupt image in the next valid image. Bengio et al. (2013c) showed that the denoizing models learn the transition process of a Markov chain whose stationary distribution is data distribution. Denoise is an interesting application for LBNs because it is an inverted problem. It is a distribution of clean images that can be corrupted, and this distribution can be multimodal for highly corrupt images."}, {"heading": "6.2.1 IMAGE PATCHES", "text": "In this section we will show that LBNs significantly improve on conditional SBNs for the denocialization of image patches. We will look at small image patches here for the purposes of calculation, since training on larger patches is computationally intensive. We will look at the problem of denociation with standard deviations 25 (over 255), and we will favor the patches by reducing them to the [0, 1] intervals, which are the means of deviation from the norm. We will look at the problem of denociation with standard deviations 25 (over 255), we will prepare the patches by reducing the interval, substracting the interval = 0.5 and dividing by the standard deviation from the norm. We will compare conditional SBNs with the standard deviation from the standard deviation 25 (over 255)."}, {"heading": "6.2.2 FULL IMAGES", "text": "In this section, we evaluate the problem of training LBNs with multiple layers of non-deterministic functions The hidden functions of network functions 4 \u00b7 We would not generate full-screen information; the state-of-the-art method for denoizing natural images is BM3D (Dabov et al., 2009). Neural methods (Burger et al., 2012) were state-of-the-art, however, but they were surpassed by later versions of BM3D. BM3D is a non-parametric denoizing algorithm that uses self-similarity between patches to compensate for noise. While we can expect this method to work well for small noise distributions, it produces blurred images for high noise distributions. To produce large images, we use a convolutionary architecture for the LBNs. In practice, this boils down to replacing the dot products with modifications - both for the activation of linear units and the gates. We found that 128 units use the revolutionary and gates."}, {"heading": "7 CONCLUSION", "text": "This paper introduces linearizing faith networks (LBN), a new class of conditioned faith networks. As a faith network, an LBN relies on stochastic binary units, but is well suited to modelling continuous distributions. In contrast to previous work, stochastic units of the LBN act as gateways to a deep linear network. This multiplicative interaction between stochastic and deterministic units enables better collaboration between the two parts of the network compared to previous additive strategies. Furthermore, LBN linear units disseminate continuous information efficiently and in combination with stochastic binary gating act as skip connections that prevent the diffusion of gradients and contribute to learning. Our experiments on generating facial expression lead to better generalization and faster convergence for LBN compared to alternative faith networks. Our image denocidating experiments also report a better signal-to-overall noise ratio that may suggest different problems for this earlier work."}, {"heading": "ACKNOWLEDGEMENTS", "text": "The authors thank Marc'Aurelio Ranzato for his insightful comments and discussions."}, {"heading": "A ADDITIONAL RESULTS", "text": "A.1 LBNS ON NATURAL IMAGE PATCHES (FROM SECTION 6.2.1) A.2 LBNS ON FULL NATURAL IMAGES (FROM SECTION 6.2.2) A.3 LINS BETWEEN LBNS AND RELU NETWORKSReLU networks can be seen as a particular deterministic subset of the LBN family of networks. The function of a ReLU network is given by f (x) = Vmax (0, Wx) = V ((Wx > 0) \u0445 Wx. This is the form of an LBN with gating units sampled from a Dirac delta distribution."}], "references": [{"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "author": ["Bengio", "Yoshua", "L\u00e9onard", "Nicholas", "Courville", "Aaron"], "venue": "arXiv preprint arXiv:1308.3432,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Deep generative stochastic networks trainable by backprop", "author": ["Bengio", "Yoshua", "Thibodeau-Laufer", "Eric", "Alain", "Guillaume", "Yosinski", "Jason"], "venue": "arXiv preprint arXiv:1306.1091,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Generalized denoising auto-encoders as generative models", "author": ["Bengio", "Yoshua", "Yao", "Li", "Alain", "Guillaume", "Vincent", "Pascal"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "A theoretical analysis of feature pooling in visual recognition", "author": ["Boureau", "Y-Lan", "Ponce", "Jean", "LeCun", "Yann"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Boureau et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Boureau et al\\.", "year": 2010}, {"title": "Image denoising: Can plain neural networks compete with bm3d", "author": ["Burger", "Harold C", "Schuler", "Christian J", "Harmeling", "Stefan"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Burger et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Burger et al\\.", "year": 2012}, {"title": "A spike and slab restricted boltzmann machine", "author": ["Courville", "Aaron C", "Bergstra", "James", "Bengio", "Yoshua"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Courville et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Courville et al\\.", "year": 2011}, {"title": "Bm3d image denoising with shape-adaptive principal component analysis", "author": ["Dabov", "Kostadin", "Foi", "Alessandro", "Katkovnik", "Vladimir", "Egiazarian", "Karen"], "venue": "In SPARS\u201909-Signal Processing with Adaptive Sparse Structured Representations,", "citeRegEx": "Dabov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Dabov et al\\.", "year": 2009}, {"title": "Image denoising via sparse and redundant representations over learned dictionaries", "author": ["Elad", "Michael", "Aharon", "Michal"], "venue": "Image Processing, IEEE Transactions on,", "citeRegEx": "Elad et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Elad et al\\.", "year": 2006}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Learning to linearize under uncertainty", "author": ["Goroshin", "Ross", "Mathieu", "Micha\u00ebl", "LeCun", "Yann"], "venue": "CoRR, abs/1506.03011,", "citeRegEx": "Goroshin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Goroshin et al\\.", "year": 2015}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies", "author": ["Hochreiter", "Sepp", "Bengio", "Yoshua", "Frasconi", "Paolo", "Schmidhuber", "Jrgen"], "venue": "A Field Guide to Dynamical Recurrent Neural Networks,", "citeRegEx": "Hochreiter et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik P", "Ba", "Jimmy"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["Mairal", "Julien", "Bach", "Francis", "Ponce", "Jean", "Sapiro", "Guillermo"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Mairal et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2010}, {"title": "A Wavelet Tour of Signal Processing, Third Edition: The Sparse Way", "author": ["Mallat", "Stephane"], "venue": null, "citeRegEx": "Mallat and Stephane.,? \\Q2008\\E", "shortCiteRegEx": "Mallat and Stephane.", "year": 2008}, {"title": "Connectionist learning of belief networks", "author": ["Neal", "Radford M"], "venue": "Artificial intelligence,", "citeRegEx": "Neal and M.,? \\Q1992\\E", "shortCiteRegEx": "Neal and M.", "year": 1992}, {"title": "Image denoising using scale mixtures of gaussians in the wavelet domain", "author": ["Portilla", "Javier", "Strela", "Vasily", "Wainwright", "Martin J", "Simoncelli", "Eero P"], "venue": "Image Processing, IEEE Transactions on,", "citeRegEx": "Portilla et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Portilla et al\\.", "year": 2003}, {"title": "Techniques for learning binary stochastic feedforward neural networks", "author": ["Raiko", "Tapani", "Berglund", "Mathias", "Alain", "Guillaume", "Dinh", "Laurent"], "venue": "arXiv preprint arXiv:1406.2989,", "citeRegEx": "Raiko et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Raiko et al\\.", "year": 2014}, {"title": "Fields of experts: A framework for learning image priors", "author": ["Roth", "Stefan", "Black", "Michael J"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Roth et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Roth et al\\.", "year": 2005}, {"title": "Recursive bayesian estimation using gaussian sums", "author": ["H.W. Sorenson", "D.L. Alspach"], "venue": null, "citeRegEx": "Sorenson and Alspach,? \\Q1971\\E", "shortCiteRegEx": "Sorenson and Alspach", "year": 1971}, {"title": "Unsupervised learning of video representations using lstms", "author": ["Srivastava", "Nitish", "Mansimov", "Elman", "Salakhutdinov", "Ruslan"], "venue": "arXiv preprint arXiv:1502.04681,", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "The toronto face database", "author": ["Susskind", "Joshua", "Anderson", "Adam", "Hinton", "Geoffrey"], "venue": null, "citeRegEx": "Susskind et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Susskind et al\\.", "year": 2010}, {"title": "Learning stochastic feedforward neural networks", "author": ["Tang", "Yichuan", "Salakhutdinov", "Ruslan R"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Tang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 20, "context": "predicting the next frame of video (Srivastava et al., 2015) or learning unsupervised models with regularized autoencoders (Bengio et al.", "startOffset": 35, "endOffset": 60}, {"referenceID": 0, "context": ", 2015) or learning unsupervised models with regularized autoencoders (Bengio et al., 2013b). Stochastic feed-forward neural networks (Neal, 1992) (SFNN) solve this problem with the introduction of stochastic latent variables to the network. The model can be seen as a mixture of neural networks where each configuration of stochastic variables defines a different neural network. This is efficiently achieved by sharing most of the parameters between configurations. While conventional neural networks fit a single conditional Gaussian to the data, the stochastic latent variables lead to fitting a mixture of conditional Gaussians. This a powerful extension since mixture of Gaussians are universal approximators of distributions (Sorenson & Alspach, 1971). The network can model multi-modal distributions by learning a different network for each mode. Neal (1992) proposes training Sigmoid Belief Networks (SBN) which have only binary stochastic units.", "startOffset": 71, "endOffset": 867}, {"referenceID": 11, "context": "These linear units can be thought of as multiplicative skip connections that allows the gradient to flow without diffusion through deep networks (Hochreiter et al., 2001).", "startOffset": 145, "endOffset": 170}, {"referenceID": 16, "context": "Raiko et al. (2014) suggested to avoid training the latent units, relying only on layers of deterministic units to shape the random distribution.", "startOffset": 0, "endOffset": 20}, {"referenceID": 3, "context": "Adding such pooling interactions to the networks is known to improve the generalization performance (Boureau et al., 2010; Goodfellow et al., 2013).", "startOffset": 100, "endOffset": 147}, {"referenceID": 9, "context": "Different objectives can be employed to train their parameters: data likelihood, variational bounds (Tang & Salakhutdinov, 2013), or a distiguishibility criteria (Goodfellow et al., 2014).", "startOffset": 162, "endOffset": 187}, {"referenceID": 11, "context": "The combination of the linear skipconnections with binary latent variable helps learning as it prevents gradient diffusion (Hochreiter et al., 2001).", "startOffset": 123, "endOffset": 148}, {"referenceID": 0, "context": "Bengio et al. (2013a) proposed a solution based on reinforcement learning, while Tang & Salakhutdinov (2013) explored a variational learning approach.", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "Bengio et al. (2013a) proposed a solution based on reinforcement learning, while Tang & Salakhutdinov (2013) explored a variational learning approach.", "startOffset": 0, "endOffset": 109}, {"referenceID": 0, "context": "Bengio et al. (2013a) proposed a solution based on reinforcement learning, while Tang & Salakhutdinov (2013) explored a variational learning approach. These solutions unfortunately results in high-variance gradient estimates. We use a lower variance estimator introduced recently by Raiko et al. (2014). This approach decomposes the stochastic units into", "startOffset": 0, "endOffset": 303}, {"referenceID": 17, "context": "Noting that the term has zero mean, that is E[g] = g(Wx), Raiko et al. (2014) finds this method incurs only a small bias and works well in practice.", "startOffset": 58, "endOffset": 78}, {"referenceID": 5, "context": "The linearizing net has connections to the spike and slab RBM (Courville et al., 2011) which has both Gaussian and Bernoulli units that interact multiplicatively.", "startOffset": 62, "endOffset": 86}, {"referenceID": 15, "context": "The difficulty of training SFNNs is discussed by Raiko et al. (2014). This work finds that better performance can be achieved by training only the deterministic units and setting the probability of activation of the Bernoulli units to p = 0.", "startOffset": 49, "endOffset": 69}, {"referenceID": 9, "context": "Goroshin et al. (2015) explores an alternative strategy and forgo full probabilistic modeling to focus on MAP inference.", "startOffset": 0, "endOffset": 23}, {"referenceID": 21, "context": "The pictures are taken from the Toronto Face Dataset (TFD) (Susskind et al., 2010) which contains 4,000 images of 900 subjects which were asked to display 7 different emotions.", "startOffset": 59, "endOffset": 82}, {"referenceID": 17, "context": "This section compares different approaches to stochastic feed-forward neural networks on the task of predicting the distribution of facial expressions of a person given a picture with a neutral expression as in Tang & Salakhutdinov (2013) and Raiko et al. (2014). The input x is the average face of the person and we have a distribution of pictures ysad, .", "startOffset": 243, "endOffset": 263}, {"referenceID": 17, "context": "This section compares different approaches to stochastic feed-forward neural networks on the task of predicting the distribution of facial expressions of a person given a picture with a neutral expression as in Tang & Salakhutdinov (2013) and Raiko et al. (2014). The input x is the average face of the person and we have a distribution of pictures ysad, . . . ,yangry of that person with 7 different emotions. The goal is to be able to produce the full set of facial expressions for a face we have not seen before. The pictures are taken from the Toronto Face Dataset (TFD) (Susskind et al., 2010) which contains 4,000 images of 900 subjects which were asked to display 7 different emotions. Following the setting of Tang & Salakhutdinov (2013), we randomly selected 95 subjects with 1,318 images for training, 5 subjects with 68 images for validation and 24 individuals totaling 343 images were used as a test set.", "startOffset": 243, "endOffset": 746}, {"referenceID": 17, "context": "This section compares different approaches to stochastic feed-forward neural networks on the task of predicting the distribution of facial expressions of a person given a picture with a neutral expression as in Tang & Salakhutdinov (2013) and Raiko et al. (2014). The input x is the average face of the person and we have a distribution of pictures ysad, . . . ,yangry of that person with 7 different emotions. The goal is to be able to produce the full set of facial expressions for a face we have not seen before. The pictures are taken from the Toronto Face Dataset (TFD) (Susskind et al., 2010) which contains 4,000 images of 900 subjects which were asked to display 7 different emotions. Following the setting of Tang & Salakhutdinov (2013), we randomly selected 95 subjects with 1,318 images for training, 5 subjects with 68 images for validation and 24 individuals totaling 343 images were used as a test set. This reproduces the setting from Tang & Salakhutdinov (2013) as closely as possible.", "startOffset": 243, "endOffset": 978}, {"referenceID": 17, "context": "This section compares different approaches to stochastic feed-forward neural networks on the task of predicting the distribution of facial expressions of a person given a picture with a neutral expression as in Tang & Salakhutdinov (2013) and Raiko et al. (2014). The input x is the average face of the person and we have a distribution of pictures ysad, . . . ,yangry of that person with 7 different emotions. The goal is to be able to produce the full set of facial expressions for a face we have not seen before. The pictures are taken from the Toronto Face Dataset (TFD) (Susskind et al., 2010) which contains 4,000 images of 900 subjects which were asked to display 7 different emotions. Following the setting of Tang & Salakhutdinov (2013), we randomly selected 95 subjects with 1,318 images for training, 5 subjects with 68 images for validation and 24 individuals totaling 343 images were used as a test set. This reproduces the setting from Tang & Salakhutdinov (2013) as closely as possible. The networks were trained for 200 iterations on the training set with up to k = 200 Monte Carlo samples to estimate the expectation over outcomes. We consider various methods including RSFNNs Raiko et al. (2014), mixtures of factor analysers (MFA), conditional Gaussian RBMs (C-GRBM) and SFNNs.", "startOffset": 243, "endOffset": 1214}, {"referenceID": 17, "context": "This section compares different approaches to stochastic feed-forward neural networks on the task of predicting the distribution of facial expressions of a person given a picture with a neutral expression as in Tang & Salakhutdinov (2013) and Raiko et al. (2014). The input x is the average face of the person and we have a distribution of pictures ysad, . . . ,yangry of that person with 7 different emotions. The goal is to be able to produce the full set of facial expressions for a face we have not seen before. The pictures are taken from the Toronto Face Dataset (TFD) (Susskind et al., 2010) which contains 4,000 images of 900 subjects which were asked to display 7 different emotions. Following the setting of Tang & Salakhutdinov (2013), we randomly selected 95 subjects with 1,318 images for training, 5 subjects with 68 images for validation and 24 individuals totaling 343 images were used as a test set. This reproduces the setting from Tang & Salakhutdinov (2013) as closely as possible. The networks were trained for 200 iterations on the training set with up to k = 200 Monte Carlo samples to estimate the expectation over outcomes. We consider various methods including RSFNNs Raiko et al. (2014), mixtures of factor analysers (MFA), conditional Gaussian RBMs (C-GRBM) and SFNNs. The stochastic networks are trained with 4 layers with either 128 or 256 deterministic hidden units. ReLU activations are used for the deterministic units as they were found to be good for continuous problems. The 2 intermediary layers are augmented with either 32 or 64 random Bernoulli units. The number of hidden units in the LBNs was chosen from {128, 256} with the number of hidden layers fixed to 1. The gating network has 2 hidden layers with {64, 128} hidden units. The hidden units of the gating network are also sampled under a Bernoulli distribution. This allows the gating pattern h to be itself to be multi-modal and results in better results. Table 1 reports our results for RSFNN and LBN as well as the results from Tang & Salakhutdinov (2013) for the other techniques.", "startOffset": 243, "endOffset": 2056}, {"referenceID": 6, "context": "Previous approaches such as the state-of-the-art BM3D (Dabov et al., 2009) method ignore this difficulty and simply predict the conditional average.", "startOffset": 54, "endOffset": 74}, {"referenceID": 0, "context": "Bengio et al. (2013c) showed that under mild conditions the denoising models learn the transition operator of a Markov chain whose stationary distribution is the data distribution.", "startOffset": 0, "endOffset": 22}, {"referenceID": 6, "context": "The state-of-the art method for denoising natural images is BM3D (Dabov et al., 2009).", "startOffset": 65, "endOffset": 85}, {"referenceID": 4, "context": "Neural methods (Burger et al., 2012) were state-of-the-art but they were surpassed by later versions of BM3D.", "startOffset": 15, "endOffset": 36}, {"referenceID": 16, "context": "We compare our algorithm to Gaussian Scale Mixture (GSM) (Portilla et al., 2003), Field of Experts (FoE) (Roth & Black, 2005), K-SVD (Elad & Aharon, 2006), BM3D (Dabov et al.", "startOffset": 57, "endOffset": 80}, {"referenceID": 6, "context": ", 2003), Field of Experts (FoE) (Roth & Black, 2005), K-SVD (Elad & Aharon, 2006), BM3D (Dabov et al., 2009) and Learned Simultaneous Sparse Coding (LSSC) (Mairal et al.", "startOffset": 88, "endOffset": 108}, {"referenceID": 13, "context": ", 2009) and Learned Simultaneous Sparse Coding (LSSC) (Mairal et al., 2010) on the standard 11 test images they used.", "startOffset": 54, "endOffset": 75}, {"referenceID": 9, "context": "training of the model and use an adversarial objective instead (Goodfellow et al., 2014).", "startOffset": 63, "endOffset": 88}], "year": 2016, "abstractText": "Conditional belief networks introduce stochastic binary variables in neural networks. Contrary to a classical neural network, a belief network can predict more than the expected value of the output Y given the inputX . It can predict a distribution of outputs Y which is useful when an input can admit multiple outputs whose average is not necessarily a valid answer. Such networks are particularly relevant to inverse problems such as image prediction for denoising, or text to speech. However, traditional sigmoid belief networks are hard to train and are not suited to continuous problems. This work introduces a new family of networks called linearizing belief nets or LBNs. A LBN decomposes into a deep linear network where each linear unit can be turned on or off by non-deterministic binary latent units. It is a universal approximator of real-valued conditional distributions and can be trained using gradient descent. Moreover, the linear pathways efficiently propagate continuous information and they act as multiplicative skip-connections that help optimization by removing gradient diffusion. This yields a model which trains efficiently and improves the state-of-the-art on image denoising and facial expression generation with the Toronto faces dataset.", "creator": "LaTeX with hyperref package"}}}