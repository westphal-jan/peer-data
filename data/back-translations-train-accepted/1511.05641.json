{"id": "1511.05641", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2015", "title": "Net2Net: Accelerating Learning via Knowledge Transfer", "abstract": "We introduce techniques for rapidly transferring the information stored in one neural net into another neural net. The main purpose is to accelerate the training of a significantly larger neural net. During real-world workflows, one often trains very many different neural networks during the experimentation and design process. This is a wasteful process in which each new model is trained from scratch. Our Net2Net technique accelerates the experimentation process by instantaneously transferring the knowledge from a previous network to each new deeper or wider network. Our techniques are based on the concept of function-preserving transformations between neural network specifications. This differs from previous approaches to to pre-training that altered the function represented by a neural net when adding layers to it. Using our knowledge transfer mechanism to add depth to Inception modules, we demonstrate a new state of the art accuracy rating on the ImageNet dataset.", "histories": [["v1", "Wed, 18 Nov 2015 02:09:20 GMT  (209kb,D)", "http://arxiv.org/abs/1511.05641v1", null], ["v2", "Thu, 19 Nov 2015 19:07:40 GMT  (172kb,D)", "http://arxiv.org/abs/1511.05641v2", "ICLR 2016 submission"], ["v3", "Thu, 7 Jan 2016 22:54:48 GMT  (255kb,D)", "http://arxiv.org/abs/1511.05641v3", "ICLR 2016 submission"], ["v4", "Sat, 23 Apr 2016 23:14:39 GMT  (391kb,D)", "http://arxiv.org/abs/1511.05641v4", "ICLR 2016 submission"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tianqi chen", "ian goodfellow", "jonathon shlens"], "accepted": true, "id": "1511.05641"}, "pdf": {"name": "1511.05641.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["KNOWLEDGE TRANSFER", "Tianqi Chen", "Ian Goodfellow", "Jonathon Shlens"], "emails": ["tqchen@cs.washington.edu,", "goodfellow@google.com", "shlens@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "We propose a new way of operating in order to work on large neural networks: the rapid transfer of the knowledge contained in a neural network to another neural network. We call this the Net2Net family of operations. We use Net2Net as a generic term that describes any process of forming a student network significantly faster than would otherwise be possible by using knowledge from a teacher network that has already been trained on the same task. In this article, we propose two specific Net2Net methods, both based on the idea of functional transformations of neural networks. Specifically, we initialize the student to be a neural network that represents the same function as the teacher, but with a different parameterization. One of these transformations, Net2WiderNet, allows the exchange of a model with an equivalent model that is broader (has more units in each hidden layer). Another of these transformations, Net2DeperNet allows the exchange of a model that fulfills some properties by an equivalent, deeper one."}, {"heading": "2 RELATED WORK", "text": "In fact, it is that most of us are able to follow the rules that they have imposed on themselves, and that they are able to follow the rules that they have imposed on themselves. (...) It is that they are able to change the rules that they have imposed on themselves. (...) It is not that they are able to understand the rules that they have imposed on themselves. (...) It is that they are able to determine for themselves what they want. (...) It is that they are able to break the rules that they have imposed on themselves. (...) It is that they are able to determine for themselves what they want. (...) It is that they are able to break the rules that they have imposed on themselves. (...) It is that they are able to determine for themselves what they want. (...)"}, {"heading": "3 METHODOLOGY", "text": "In this section we describe our new Net2Net operations and how to apply them to real deep neural networks."}, {"heading": "3.1 FEATURE PREDICTION", "text": "We briefly experimented with a method that did not turn out to be particularly beneficial: training a large student network based on random initialization, and introducing a number of additional \"teacher prediction stories\" into the student network. Specifically, several revolutionary hidden layers of the student network were provided as input for new, learned, revolutionary layers; the cost function was modified to include terms that favored the output of these additional layers near a corresponding layer in the teacher network. In other words, the student is trained to use each of his hidden layers to predict the values of the hidden layers in the teacher; the goal was that the teacher would provide a good internal representation of the task that the student could quickly copy and then begin to refine; the approach is similar to the FitNets strategy (Romero et al., 2014) for training very thin networks of moderate depth. Unfortunately, we did not find that this method offers compelling advantages over other basic acceleration methods."}, {"heading": "3.2 FUNCTION-PRESERVING INITIALIZATIONS", "text": "We present two effective networking strategies, both based on the initialization of the student network to represent the same function as the teacher, and then continue the student network by normal means. Specifically, it is assumed that a teacher network is represented by a function y = f, and it is a vector of probabilities that guarantees the behavior of the network, and it is a parameter of the network that identifies with a conventional network, and it is a vector of probabilities that represents the behavior of object categories. Our strategy is to choose a new set of parameters that aim at a student network."}, {"heading": "3.3 NET2WIDERNET", "text": "In fact, most of us are able to abide by the rules they have imposed on ourselves. (...) Most of us have no idea what they are doing. (...) Most of us have no idea what they are doing. (...) Most of us have no idea what they are doing. (...) Most of us have no idea what they are doing. (...) Most of us have no idea what they are doing. (...) Most of us do not know what they are doing. (...) Most of us do not know what they are doing. (...) We do not know what they are doing. (...) We do not know what they are doing. (...) We do not know what they are doing. (...) We do not know what they are doing. (...) We do not know what they are doing. (...) We do not know what they are doing."}, {"heading": "3.4 NET2DEEPERNET", "text": "We are also introducing a second functional transformation, Net2DeeperNet. This allows us to transform an existing network into a deeper one. Specifically, Net2DeeperNet replaces a layer h (i) = \u03c6 (h (i \u2212 1) > W (i)) with two layers h (i) = \u03c6 (U (i) > h (i \u2212 1)))). The new matrixU is initialized to an identity matrix, but remains free to assume any value later on. This operation is only applicable if it is chosen in such a way that it proves to be (v) = \u03c6 (v) for all vectors. This property applies to the reflected linear activation. To obtain Net2DeeperNet for maxout units, one must use a matrix that is similar to the identity but comparable to replicated columns."}, {"heading": "4 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 EXPERIMENTAL SETUP", "text": "We evaluated our Net2Net operators in three different settings. In all cases, we used an InceptionBN network (Ioffe & Szegedy, 2015), which was trained on ImageNet. In the first setting, we show that Net2WiderNet can be used to speed up the formation of a standard inception network by initializing it with a smaller network. In the second setting, we show that Net2DeeperNet allows us to increase the depth of the inception modules. Finally, we use our Net2Net operators in the realistic setting, in which we make more dramatic changes to the model size and explore the model space to achieve better performance. In this setting, we demonstrate an improved result on ImageNet.We will compare our method with some basic methods: \u2022 \"Random Pad\": This is a baseline for Net2WiderNet. We expand the network by adding new units with random weights, rather than performing random initialization in order to replicate random units."}, {"heading": "4.2 NET2WIDERNET", "text": "We start by evaluating the method of Net2WiderNet. We started by building a teacher network that was narrower than the standard inception. We reduced the number of folding channels at each level within all of the inception modules by a factor \u221a 0.3. Since both the input and output number of channels are reduced, the number of parameters in most layers is reduced to 30% of the original amount. To simplify the software for our experiments, we have not changed any component of the network except the input modules. After training this small teacher network, we have used it to speed up the formation of a standard student network. Figure 4 shows the comparison of different approaches. We can find that the proposed approach offers a faster convergence than the baseline approaches. Importantly, Net2WiderNet offers the same final accuracy as the model trained by random initialization. This indicates that the model setting of the net can be used to achieve the same final ability to regulate and achieve the new Net2WiderNet experiments."}, {"heading": "4.3 NET2DEEPERNET", "text": "We show experiments performed with the use of Net2DeeperNet to deepen the network. The revolutionary layers in the inception modules use rectangular cores. The revolutionary layers are arranged in pairs, with one layer using a vertical core followed by a layer with a horizontal core. Each of these layers is a complete layer with a corrective nonlinearity and batch normalization; it is not just a factorization of a linear folding operation into separable parts. For these experiments, we used a standard inception model as a teacher network and increased the depth of each inception module. Everywhere a pair of vertical-horizontal conversion layers appears, we added two more pairs of such layers, which are configured to lead to an identity transformation. The results are shown in Figure 5. We can see that a pair of vertical-horizontal conversion layers appear much faster than the initialization achieved by both accuracy and accuracy in random."}, {"heading": "4.4 EXPLORE MODEL DESIGN SPACE WITH NET2NET", "text": "An important feature of Net2Net is that it enables rapid exploration of the model design space by transforming an existing state-of-the-art architecture. In this experiment, we undertook an ambitious exploration of the model design space in both wider and deeper directions. Specifically, we increased the width of an inception model to 2 times beyond the standard. In addition, we built another deeper network by adding four vertical-horizontal pairs of layers over each inception module in the original inception model. Results are in Fig. 6. This last approach has paid off, producing a model that adds a new state of the art of 78.5% to our ImageNet validation set. We did not train these larger models from scratch due to resource and time constraints from scratch. However, we reported on the convergence curve of training the original inception model as a reference model that should be easier to train than these larger models."}, {"heading": "5 DISCUSSION", "text": "Our Net2Net operators have shown that it is possible, under certain architectural constraints, to quickly transfer knowledge from a small neural network to a much larger neural network. We have shown that this approach can train larger neural networks to improve the performance of ImageNet recognition. Net2Net can now also be used as a technique to study model families more quickly, thereby reducing the time required for typical machine learning processes. We hope that future research will uncover new ways of knowledge transfer between neural networks. In particular, we hope that future research will reveal more general methods of knowledge transfer that can quickly initiate a student network whose architecture is not tied to that of the teacher network."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Jeff Dean and George Dahl for the helpful conversations and also the developers of TensorFlow (Abadi et al., 2015), which we used for all our experiments."}, {"heading": "A CHOICE OF HYPER PARAMETERS FOR FINE TUNING NET2NET TRANSFORMED MODELS", "text": "Most of the hyperparameter settings used to train deep architecture from the ground up can easily be incorporated into the transformed model of Net2Net. The only change is that the learning rate should be adjusted to the low learning rate used towards the end of the training of the teacher model, rather than the high learning rate used for training models from the ground up. As the initial model of Net2Net is already doing quite well and we just need to refine the transformed model, we find that setting 110 of the initial learning rate is a good convergence for Net2Net."}], "references": [{"title": "TensorFlow: Large-scale machine learning", "author": ["Vijay", "Vi\u00e9gas", "Fernanda", "Vinyals", "Oriol", "Warden", "Pete", "Wattenberg", "Martin", "Wicke", "Yu", "Yuan", "Zheng", "Xiaoqiang"], "venue": "on heterogeneous systems,", "citeRegEx": "Vijay et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vijay et al\\.", "year": 2015}, {"title": "The cascade-correlation learning architecture", "author": ["Fahlman", "Scott E", "Lebiere", "Christian"], "venue": null, "citeRegEx": "Fahlman et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Fahlman et al\\.", "year": 1990}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In AISTATS\u20192011,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Knowledge transfer in deep convolutional neural nets", "author": ["Gutstein", "Steven", "Fuentes", "Olac", "Freudenthal", "Eric"], "venue": "International Journal on Artificial Intelligence Tools,", "citeRegEx": "Gutstein et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Gutstein et al\\.", "year": 2008}, {"title": "Distilling the knowledge in a neural network", "author": ["Hinton", "Geoffrey", "Vinyals", "Oriol", "Dean", "Jeff"], "venue": "arXiv preprint arXiv:1503.02531,", "citeRegEx": "Hinton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Hinton", "Geoffrey E", "Osindero", "Simon", "Teh", "Yee Whye"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": null, "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["Jarrett", "Kevin", "Kavukcuoglu", "Koray", "Ranzato", "Marc\u2019Aurelio", "LeCun", "Yann"], "venue": "In Proc. International Conference on Computer Vision (ICCV\u201909),", "citeRegEx": "Jarrett et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jarrett et al\\.", "year": 2009}, {"title": "FitNets and batch normalization", "author": ["Mahayri", "Amjad", "Ballas", "Nicolas", "Courville", "Aaron"], "venue": "Technical report, (unpublished),", "citeRegEx": "Mahayri et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mahayri et al\\.", "year": 2015}, {"title": "FitNets: Hints for thin deep nets", "author": ["Romero", "Adriana", "Ballas", "Nicolas", "Ebrahimi Kahou", "Samira", "Chassang", "Antoine", "Gatta", "Carlo", "Bengio", "Yoshua"], "venue": "Technical Report Arxiv report 1412.6550,", "citeRegEx": "Romero et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Romero et al\\.", "year": 2014}, {"title": "Lifelong machine learning systems: Beyond learning algorithms", "author": ["DL Silver", "Q Yang", "L. Li"], "venue": "In AAAI Spring Symposium-Technical Report,", "citeRegEx": "Silver et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "In ICLR,", "citeRegEx": "Simonyan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2015}, {"title": "Parsing with compositional vector grammars", "author": ["Socher", "Richard", "Bauer", "John", "Manning", "Christopher D", "Ng", "Andrew Y"], "venue": "Proceedings of the ACL conference,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "Technical report,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Lifelong learning: A case study", "author": ["Thrun", "Sebastian"], "venue": "Technical Report CMU-CS-95-208,", "citeRegEx": "Thrun and Sebastian.,? \\Q1995\\E", "shortCiteRegEx": "Thrun and Sebastian.", "year": 1995}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton", "year": 2012}], "referenceMentions": [{"referenceID": 10, "context": "More ambitiously, real machine learning systems will eventually become lifelong learning systems (Thrun, 1995; Silver et al., 2013; Mitchell et al., 2015).", "startOffset": 97, "endOffset": 154}, {"referenceID": 3, "context": "A related approach is to add whole new layers while leaving the lower layers fixed Gutstein et al. (2008). Both these approaches can be considered part of the Net2Net family of operations for rapidly training a new model given a pre-existing one.", "startOffset": 83, "endOffset": 106}, {"referenceID": 9, "context": "Some work on knowledge transfer between neural networks is motivated by a desire to train deeper networks than would otherwise be possible, by incrementally training deeper networks with knowledge provided by smaller ones (Romero et al., 2014; Simonyan & Zisserman, 2015).", "startOffset": 222, "endOffset": 271}, {"referenceID": 14, "context": "This may be due to our use of a very strong baseline: Inception (Szegedy et al., 2014) networks with batch normalization (Ioffe & Szegedy, 2015) trained using RMSProp (Tieleman & Hinton, 2012).", "startOffset": 64, "endOffset": 86}, {"referenceID": 4, "context": "Model compression (Bucilu\u01ce et al., 2006; Hinton et al., 2015) is a technique for transferring knowledge from many models to a single model.", "startOffset": 18, "endOffset": 61}, {"referenceID": 12, "context": "Networks with identity weights at initialization have been used before by, for example, Socher et al. (2013), but to our knowledge that have not previously been used to design function-preserving transformations of pre-existing neural networks.", "startOffset": 88, "endOffset": 109}, {"referenceID": 5, "context": "This is a closely related concept to the justification for greedy layerwise pretraining of deep belief networks (Hinton et al., 2006).", "startOffset": 112, "endOffset": 133}, {"referenceID": 9, "context": "The approach resembles the FitNets (Romero et al., 2014) strategy for training very thin networks of moderate depth.", "startOffset": 35, "endOffset": 56}, {"referenceID": 8, "context": "Mahayri et al. (2015) independently observed that the benefits of the FitNets training strategy were eliminated after changing the model to use batch normalization.", "startOffset": 0, "endOffset": 22}, {"referenceID": 14, "context": "We use this layer formalization for clarity, but our method generalizes to arbitrary composition structure of these layers, such as Inception (Szegedy et al., 2014).", "startOffset": 142, "endOffset": 164}, {"referenceID": 6, "context": "In our description of the method, we assume for simplicity that both the teacher and student networks contain compositions of standard linear neural network layers of the form h = \u03c6(W (i)>h(i\u22121)) where h is the activations of hidden layer i, W (i) is the matrix of incoming weights for this layer, and \u03c6 is an activation function, such as as the rectified linear activation function Jarrett et al. (2009); Glorot et al.", "startOffset": 383, "endOffset": 405}, {"referenceID": 2, "context": "(2009); Glorot et al. (2011) or the maxout activation function Goodfellow et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 2, "context": "(2009); Glorot et al. (2011) or the maxout activation function Goodfellow et al. (2013). We use this layer formalization for clarity, but our method generalizes to arbitrary composition structure of these layers, such as Inception (Szegedy et al.", "startOffset": 8, "endOffset": 88}], "year": 2015, "abstractText": "We introduce techniques for rapidly transferring the information stored in one neural net into another neural net. The main purpose is to accelerate the training of a significantly larger neural net. During real-world workflows, one often trains very many different neural networks during the experimentation and design process. This is a wasteful process in which each new model is trained from scratch. Our Net2Net technique accelerates the experimentation process by instantaneously transferring the knowledge from a previous network to each new deeper or wider network. Our techniques are based on the concept of function-preserving transformations between neural network specifications. This differs from previous approaches to to pre-training that altered the function represented by a neural net when adding layers to it. Using our knowledge transfer mechanism to add depth to Inception modules, we demonstrate a new state of the art accuracy rating on the ImageNet dataset.", "creator": "LaTeX with hyperref package"}}}