{"id": "1511.05650", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2015", "title": "Tree-Guided MCMC Inference for Normalized Random Measure Mixture Models", "abstract": "Normalized random measures (NRMs) provide a broad class of discrete random measures that are often used as priors for Bayesian nonparametric models. Dirichlet process is a well-known example of NRMs. Most of posterior inference methods for NRM mixture models rely on MCMC methods since they are easy to implement and their convergence is well studied. However, MCMC often suffers from slow convergence when the acceptance rate is low. Tree-based inference is an alternative deterministic posterior inference method, where Bayesian hierarchical clustering (BHC) or incremental Bayesian hierarchical clustering (IBHC) have been developed for DP or NRM mixture (NRMM) models, respectively. Although IBHC is a promising method for posterior inference for NRMM models due to its efficiency and applicability to online inference, its convergence is not guaranteed since it uses heuristics that simply selects the best solution after multiple trials are made. In this paper, we present a hybrid inference algorithm for NRMM models, which combines the merits of both MCMC and IBHC. Trees built by IBHC outlines partitions of data, which guides Metropolis-Hastings procedure to employ appropriate proposals. Inheriting the nature of MCMC, our tree-guided MCMC (tgMCMC) is guaranteed to converge, and enjoys the fast convergence thanks to the effective proposals guided by trees. Experiments on both synthetic and real-world datasets demonstrate the benefit of our method.", "histories": [["v1", "Wed, 18 Nov 2015 03:16:27 GMT  (795kb,D)", "http://arxiv.org/abs/1511.05650v1", "12 pages, 10 figures, NIPS-2015"]], "COMMENTS": "12 pages, 10 figures, NIPS-2015", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["juho lee", "seungjin choi"], "accepted": true, "id": "1511.05650"}, "pdf": {"name": "1511.05650.pdf", "metadata": {"source": "CRF", "title": "Tree-Guided MCMC Inference for Normalized Random Measure Mixture Models", "authors": ["Juho Lee"], "emails": ["stonecold@postech.ac.kr", "seungjin@postech.ac.kr"], "sections": [{"heading": "1 Introduction", "text": "The standardized random variables (NRMs) form a broad class of discrete random variables, including Dirichlet Proccess (DP) variables, which are provided. [1] However, the standardized random variables (NRMM) variables (DP), which contain only inverse Gaussian methods [2] and normalized generalized gamma methods [3, 4], are a representative example of NRM models being used as precursors to blending models. Recently, NRMs have been extended to dependent NRMs (DPM) models, where interchangeability fails. Posterior analysis for NRM mixtures (NRMM) models has been developed [8, 9] which provide simple MCMC methods. As in DP blends (DPM) models, there are two paradigms in the MCMC models: (1) marginal and (Sampler) (S2)."}, {"heading": "2 Background", "text": "In the course of this essay, we will use the following notations: Identified by [n] = {1,.., n} a series of indexes, and by X = [xi | i] [n] a row. A partition [n] of [n] is a group of non-empty subsets of [n] whose union is [n]. Cluster c is an entry of [n], i.e. c] and [n]. Data points in cluster c are denoted by Xc = [xi | i] and [n]. For convenience, we often use i to represent a singleton {i} for i [n]. In this section, we will briefly consider NRMM models, existing subordinate inference methods such as MCMC and IBHC."}, {"heading": "2.1 Normalized random measure mixture models", "text": "Let us be a homogeneous, completely random measurement (ECU) on measuring space (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU (ECU) (ECU) (ECU) (ECU) (ECU (ECU) (ECU) (ECU) (ECU) (ECU) (ECU (ECU) (ECU) (ECU) (ECU) (ECU (ECU) (ECU) (ECU) (ECU) (ECU (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (ECU) (EC"}, {"heading": "2.2 MCMC Inference for NRMM models", "text": "The goal of the rear conclusion for NRMM models is to calculate the rear P (\u0432 [n], {d\u03b8c}, du | X) with the marginal probability P (dX). Marginal Gibbs Sampler: marginal Gibbs Sampler is based on the predictive distribution (6). At each iteration, cluster assignments are sampled for each data point, where xi can join an existing cluster c with the probability c (dxi). Slice Sampler: Instead of marginalizing \u00b5 (| c | + 1, u) p (|, u) P (dxi | Xc) or creating a new cluster with the probability proportional to the existing cluster c (1, u) P (dxi). Slice Sampler: Instead of marginalizing \u00b5, slice samplers are explicitly sampled the atoms and weights {wj} of the clusters."}, {"heading": "2.3 IBHC Inference for NRMM models", "text": "Bayesian hierarchical clustering (BHC) is a probabilistic model-based agglomerative clustering, where the marginal probabilities, and builds binary trees embedding the hierarchical cluster structure of datasets. BHC defines the generative probability of binary trees that is maximized during the construction of the tree, and the generative probability provides a lower probability on the marginal likelihood of DPM. For this reason, BHC is considered a posterior inference algorithm for DPM. Incremental BHC (IBHC, 14]) is an extension of the BHC to (dependent) NRMM models."}, {"heading": "3 Main results: A tree-guided MCMC procedure", "text": "Our algorithm, called tree-guided MCMC (tgMCMC), is a combination of deterministic tree-based inference and MCMC, where the trees constructed via IBHC guide MCMC to suggest high-quality specimens. On a current partition of trees and trees, tgMCMC suggests a novel partition by global and local motions. Global motions split or merge clusters to suggest clusters, and local motions alter cluster assignments of individual data points via Gibbs sampling. First, we explain the two key operations used to explain and supplement the structure of the tree."}, {"heading": "3.1 Key operations", "text": "Example Sub (c, p): For a tree tc, a sub-tree tc \"with the probability that all sub-trees of tc contain d (l (c), r (c)) +. will be drawn for leaf nodes whose d (\u00b7, \u00b7) = 0 and is set to the maximum d (\u00b7, \u00b7) among all sub-trees of tc. The drawn sub-tree will probably contain errors that can be corrected by splitting. The probability of drawing tc\" is multiplied by p, with p normally being set to transition probabilities. StocInsert (S, c, p): A stochastic version of IBHC. c can be inserted via SeqInsert (c, \"c) with the probability d \u2212 1 (c,\" c) 1 + \u2211 c \"S d \u2212 1 (c,\" c \"), or simply inserted into S (a new cluster in S) with the probability of 11 + 3\" c \"(c,\" c \") if the probability that the sub-functions will be updated accordingly."}, {"heading": "3.2 Global moves", "text": "The global movements of tgMCMC are tree-based analogy to split-merge-sampling. The global movements of tgMCMC are tree-based analogy to split-merge-sampling-sample. tgMCMC randomly selects a pair of data points in split-merge-sampling, and the division is suggested if they belong to the same cluster, or merged partition is suggested otherwise. Instead, tgMCMC finds the sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-n-sampling-n-sampling-n-sampling-sampling-n-sampling-n"}, {"heading": "3.3 Local moves", "text": "If a leaf node i is moved from c to c, we separate i from tc and perform SeqInsert (c \u2032, i) 3. Instead of performing Gibbs scanning for all data points, we perform Gibbs scanning for a subset of data points S, which is composed as follows. 2We assume that clusters get their own indexes (such as hash values) so that they can be ordered from SampleSub. 3We do not split up even if the deviation of the update exceeds one, as in StocInsert.draw a subtree of tc \u2032 again by SampleSub."}, {"heading": "4 Experiments", "text": "In this section we compare marginal Gibbs samplers (Gibbs), split merge samplers (SM) and tgMCMC on synthetic and real data sets."}, {"heading": "4.1 Toy dataset", "text": "We first compared the samplers with simple toy data sets containing 1300 two-dimensional dots with 13 clusters sampled from the Gaussian mixture with predefined averages and covariances. Since the partition found by IBHC is almost perfect for this simple data, rather than initializing it with IBHC, we initialized the binary tree (and the partition) as follows. As in IBHC, we inserted data points sequentially into existing trees with a random order. However, instead of inserting them via SeqInsert, we only created data points on existing trees so that no split. tgMCMC was initialized with the tree constructed from this method, and Gibbs and SM were initialized with a corresponding partition. We assumed that the Gaussian likelihood and Gaussian wishart base measurement, H (d\u00b5, dB) = N (d\u00b5 | m, (dump), 1 \u2212 dump), was equal to GP, where 1 \u2212 dump = \u2212 1."}, {"heading": "4.2 Large-scale synthetic dataset", "text": "We also compared the three samplers on larger datasets with 10,000 dots, which we will call the 10K dataset, which was generated from a six-dimensional mix of Gaussians with PY (3, 0.8) labels. We used the same baseline and initialization as the toy datasets and previously used the NGGP. We performed the samplers 1,000 seconds long and 10 times. Gibbs and SM were too slow, so the number of samples produced in 1,000 seconds was too low. Therefore, we also compared Gibbs Sub and SM Sub, where we consistently sampled the subset of data points and Gibbs Sweep only for those sample points. We controlled the subset to make their runtime similar to the tgMCMC. The results are summarized in Fig. 4. Again, tgMCMC outperformed other samplers in terms of both protocol probability and ESS Sweep. Interestingly, the SM was even worse than Gibb Sampling, which was even worse than Gibb Slit."}, {"heading": "4.3 NIPS corpus", "text": "We also compared the samplers on NIPS corpus4, which contained 1,500 documents containing 12,419 words. We used the multinomial probability and the symmetrical Dirichlet base measurement (0.1), pre-used NGGP, and initialized the samplers with normal IBHC. For the 10K dataset, we compared Gibbs Sub and SM Sub. We performed the samplers for 10,000 seconds and repeated them ten times. Results are summarized in Fig. 5. tgMCMC outperformed other samplers in terms of log probability; all other samplers were trapped in local optimism and did not reach the states found by tgMCMC. ESS for tgMCMC, however, were the lowest, i.e. the bad mixing rates. We continue to argue that tgMCMC is a better option for this dataset as we think finding better log probability state is more important than mixing rates."}, {"heading": "5 Conclusion", "text": "In this article, we introduced a novel inference algorithm for NRMM models. tgMCMC, our sampler, used the binary trees constructed by IBHC to suggest high-quality samples. tgMCMC explored the space of partitions by means of global and local movements guided by the potential functions of trees. tgMCMC proved to be more powerful than existing samplers in both synthetic and real datasets. Recognition: This work was supported by the IT R & D program of MSIP / IITP (B010115-0307, Machine Learning Center), the National Research Foundation (NRF) of Korea (NRF2013R1A2A2A01067464) and the IITP-MSRA Creative ICT / SW Research Project.4https: / / archive.ics.uci.edu / ml / datasets / Bag of + Material + WordsSupplementary."}, {"heading": "A Key operations", "text": "We first explain two key operations used for tgMCMC.SampleSub (c, p): Given a tree tc, SampleSub (c, p) aims to draw a sub-tree tc \u00b2 of tc. We want tc \u00b2 to have a high inequality between its sub-trees, so we draw tc \u00b2 with a probability proportional to d (l (c) c \u00b2, r (c) c \u00b2) c \u00b2, where the maximum difference between sub-trees is added, the difference between which is defined as zero. See Figure 6.As shown in Figure 6, the probability of drawing the sub-tree tci is calculated as pi. If we draw tci as a result, the corresponding probability pi is multiplied in p; p \u00b7 p \u00b7 pi This method is necessary to calculate the probability of transition."}, {"heading": "B Global moves", "text": "In this sense, it is also necessary that we address the question of how far we are able to surpass ourselves. (...) In this sense, it is also necessary that we consider ourselves able to surpass ourselves. (...) The question is to what extent we are able to surpass ourselves. (...) The question is whether we are able to surpass ourselves. (...) The question is whether we are able to surpass ourselves. (...) The question is to what extent we are able to surpass ourselves. (...) The question is whether we are able to surpass ourselves. (...) The question is whether we are able to surpass ourselves. \"(...)"}, {"heading": "C Local moves", "text": "Local movements process cluster assignments of individual data points (leaf nodes) with Gibbs scanning. However, instead of performing Gibbs scanning for complete data, we select a random subset S and execute Gibbs scanning for data points in S. S. S. Considering a partition \u0430 [n], we try its subtree for each c scanning via SampleSub. Then we try again a subtree of drawn subtrees with SampleSub. This procedure is repeated for D times where D is a user-defined parameter. Figure 10 shows the scanning procedure for c scanning [n] with D = 2. As a result of Figure 10, the leaf nodes {i, j, k} are added to S. After we have collected S for all c scanning [n], we perform Gibbs scanning for these leak nodes in S, and if a pick node c is moved from one to another (we can move the c to a means D), we can move the pick node from one to another with c-speed."}], "references": [{"title": "A Bayesian analysis of some nonparametric problems", "author": ["T.S. Ferguson"], "venue": "The Annals of Statistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1973}, {"title": "Hierarchical mixture modeling with normalized inverse- Gaussian priors", "author": ["A. Lijoi", "R.H. Mena", "I. Pr\u00fcnster"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Generalized Gamma measures and shot-noise Cox processes", "author": ["A. Brix"], "venue": "Advances in Applied Probability,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1999}, {"title": "Controlling the reinforcement in Bayesian nonparametric mixture models", "author": ["A. Lijoi", "R.H. Mena", "I. Pr\u00fcnster"], "venue": "Journal of the Royal Statistical Society B,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Distriubtional results for means of normalized random measures with independent increments", "author": ["E. Regazzini", "A. Lijoi", "I. Pr\u00fcnster"], "venue": "The Annals of Statistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Dependent hierarchical normalized random measures for dynamic topic modeling", "author": ["C. Chen", "N. Ding", "W. Buntine"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Dependent normalized random measures", "author": ["C. Chen", "V. Rao", "W. Buntine", "Y.W. Teh"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Bayesian Poisson process partition calculus with an application to Bayesian L\u00e9vy moving averages", "author": ["L.F. James"], "venue": "The Annals of Statistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Posterior analysis for normalized random measures with independent increments", "author": ["L.F. James", "A. Lijoi", "I. Pr\u00fcnster"], "venue": "Scandinavian Journal of Statistics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "MCMC for normalized random measure mixture models", "author": ["S. Favaro", "Y.W. Teh"], "venue": "Statistical Science,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems", "author": ["C.E. Antoniak"], "venue": "The Annals of Statistics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1974}, {"title": "A split-merge Markov chain Monte Carlo procedure for the Dirichlet process mixture model", "author": ["S. Jain", "R.M. Neal"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Posterior simulation of normalized random measure mixtures", "author": ["J.E. Griffin", "S.G. Walkera"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Incremental tree-based inference with dependent normalized random measures", "author": ["J. Lee", "S. Choi"], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS), Reykjavik,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Bayesian hierarchical clustering", "author": ["K.A. Heller", "Z. Ghahrahmani"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "Normalized random measures (NRMs) form a broad class of discrete random measures, including Dirichlet proccess (DP) [1] normalized inverse Gaussian process [2], and normalized generalized Gamma process [3, 4].", "startOffset": 116, "endOffset": 119}, {"referenceID": 1, "context": "Normalized random measures (NRMs) form a broad class of discrete random measures, including Dirichlet proccess (DP) [1] normalized inverse Gaussian process [2], and normalized generalized Gamma process [3, 4].", "startOffset": 156, "endOffset": 159}, {"referenceID": 2, "context": "Normalized random measures (NRMs) form a broad class of discrete random measures, including Dirichlet proccess (DP) [1] normalized inverse Gaussian process [2], and normalized generalized Gamma process [3, 4].", "startOffset": 202, "endOffset": 208}, {"referenceID": 3, "context": "Normalized random measures (NRMs) form a broad class of discrete random measures, including Dirichlet proccess (DP) [1] normalized inverse Gaussian process [2], and normalized generalized Gamma process [3, 4].", "startOffset": 202, "endOffset": 208}, {"referenceID": 4, "context": "NRM mixture (NRMM) model [5] is a representative example where NRM is used as a prior for mixture models.", "startOffset": 25, "endOffset": 28}, {"referenceID": 5, "context": "Recently NRMs were extended to dependent NRMs (DNRMs) [6, 7] to model data where exchangeability fails.", "startOffset": 54, "endOffset": 60}, {"referenceID": 6, "context": "Recently NRMs were extended to dependent NRMs (DNRMs) [6, 7] to model data where exchangeability fails.", "startOffset": 54, "endOffset": 60}, {"referenceID": 7, "context": "The posterior analysis for NRM mixture (NRMM) models has been developed [8, 9], yielding simple MCMC methods [10].", "startOffset": 72, "endOffset": 78}, {"referenceID": 8, "context": "The posterior analysis for NRM mixture (NRMM) models has been developed [8, 9], yielding simple MCMC methods [10].", "startOffset": 72, "endOffset": 78}, {"referenceID": 9, "context": "The posterior analysis for NRM mixture (NRMM) models has been developed [8, 9], yielding simple MCMC methods [10].", "startOffset": 109, "endOffset": 113}, {"referenceID": 10, "context": "As in DP mixture (DPM) models [11], there are two paradigms in the MCMC algorithms for NRMM models: (1) marginal samplers and (2) slice samplers.", "startOffset": 30, "endOffset": 34}, {"referenceID": 9, "context": "The marginal samplers include the Gibbs sampler [10], and the split-merge sampler [12], although it was not formally extended to NRMM models.", "startOffset": 48, "endOffset": 52}, {"referenceID": 11, "context": "The marginal samplers include the Gibbs sampler [10], and the split-merge sampler [12], although it was not formally extended to NRMM models.", "startOffset": 82, "endOffset": 86}, {"referenceID": 12, "context": "The slice sampler [13] maintains random measures and explicitly samples the weights and atoms of the random measures.", "startOffset": 18, "endOffset": 22}, {"referenceID": 6, "context": "The slice sampler is known to mix faster than the marginal Gibbs sampler when applied to complicated DNRM mixture models where the evaluation of marginal distribution is costly [7].", "startOffset": 177, "endOffset": 180}, {"referenceID": 13, "context": "Recently, a deterministic alternative to MCMC algorithms for NRM (or DNRM) mixture models were proposed [14], extending Bayesian hierarchical clustering (BHC) [15] which was developed as a tree-based inference for DP mixture models.", "startOffset": 104, "endOffset": 108}, {"referenceID": 14, "context": "Recently, a deterministic alternative to MCMC algorithms for NRM (or DNRM) mixture models were proposed [14], extending Bayesian hierarchical clustering (BHC) [15] which was developed as a tree-based inference for DP mixture models.", "startOffset": 159, "endOffset": 163}, {"referenceID": 13, "context": "The algorithm, referred to as incremental BHC (IBHC) [14] builds binary trees that reflects the hierarchical cluster structures of datasets by evaluating the approximate marginal likelihood of NRMM models, and is well suited for the incremental inferences for large-scale or streaming datasets.", "startOffset": 53, "endOffset": 57}, {"referenceID": 8, "context": "([9]) Let (\u03a0[n], {\u03b8c|c \u2208 \u03a0[n]}) be samples drawn from \u03bc/\u03bc(\u0398) where \u03bc \u223c CRM(\u03c1H).", "startOffset": 1, "endOffset": 4}, {"referenceID": 2, "context": "The most general CRM may be used is the generalized Gamma [3], with L\u00e9vy intensity \u03c1(dw) = \u03b1\u03c3 \u0393(1\u2212\u03c3)w \u2212\u03c3\u22121e\u2212wdw.", "startOffset": 58, "endOffset": 61}, {"referenceID": 14, "context": "Bayesian hierarchical clustering (BHC, [15]) is a probabilistic model-based agglomerative clustering, where the marginal likelihood of DPM is evaluated to measure the dissimilarity between nodes.", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "Incremental BHC (IBHC, [14]) is an extension of BHC to (dependent) NRMM models.", "startOffset": 23, "endOffset": 27}, {"referenceID": 13, "context": "The generative probability of trees is described with the potential function [14], which is the unnormalized reformulation of the original definition [15].", "startOffset": 77, "endOffset": 81}, {"referenceID": 14, "context": "The generative probability of trees is described with the potential function [14], which is the unnormalized reformulation of the original definition [15].", "startOffset": 150, "endOffset": 154}, {"referenceID": 14, "context": "If the tree is built and the potential function is computed for the entire dataset X , a lower bound on the joint likelihood (7) is obtained [15, 14]: un\u22121e\u2212\u03c8\u03c1(u)du \u0393(n) \u03c6(X|t[n]) \u2264 P (dX, du).", "startOffset": 141, "endOffset": 149}, {"referenceID": 13, "context": "If the tree is built and the potential function is computed for the entire dataset X , a lower bound on the joint likelihood (7) is obtained [15, 14]: un\u22121e\u2212\u03c8\u03c1(u)du \u0393(n) \u03c6(X|t[n]) \u2264 P (dX, du).", "startOffset": 141, "endOffset": 149}, {"referenceID": 13, "context": "Among these three cases, the one with the highest potential function \u03c6(Xc\u222ai|tc\u222ai) is selected, which can easily be done by comparing d(l(c), r(c)), d(l(c), i) and d(r(c), i) [14].", "startOffset": 174, "endOffset": 178}], "year": 2015, "abstractText": "Normalized random measures (NRMs) provide a broad class of discrete random measures that are often used as priors for Bayesian nonparametric models. Dirichlet process is a well-known example of NRMs. Most of posterior inference methods for NRM mixture models rely on MCMC methods since they are easy to implement and their convergence is well studied. However, MCMC often suffers from slow convergence when the acceptance rate is low. Tree-based inference is an alternative deterministic posterior inference method, where Bayesian hierarchical clustering (BHC) or incremental Bayesian hierarchical clustering (IBHC) have been developed for DP or NRM mixture (NRMM) models, respectively. Although IBHC is a promising method for posterior inference for NRMM models due to its efficiency and applicability to online inference, its convergence is not guaranteed since it uses heuristics that simply selects the best solution after multiple trials are made. In this paper, we present a hybrid inference algorithm for NRMM models, which combines the merits of both MCMC and IBHC. Trees built by IBHC outlines partitions of data, which guides Metropolis-Hastings procedure to employ appropriate proposals. Inheriting the nature of MCMC, our tree-guided MCMC (tgMCMC) is guaranteed to converge, and enjoys the fast convergence thanks to the effective proposals guided by trees. Experiments on both synthetic and realworld datasets demonstrate the benefit of our method.", "creator": "LaTeX with hyperref package"}}}