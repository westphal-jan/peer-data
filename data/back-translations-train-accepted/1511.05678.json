{"id": "1511.05678", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2015", "title": "Expressiveness of Rectifier Networks", "abstract": "Rectified Linear Units (ReLUs) have been shown to ameliorate the vanishing gradient problem, allow for efficient back-propagation, and empirically promote sparsity in the learned parameters. Their use has led to state-of-the-art results in a variety of applications. In this paper, we characterize the expressiveness of ReLU networks. From this perspective, unlike the sign (threshold) and sigmoid activations, ReLU networks are less explored. We show that, while the decision boundary of a two-layer ReLU network can be captured by a sign network, the sign network can require an exponentially larger number of hidden units. Furthermore, we formulate the sufficient conditions for a corresponding logarithmic reduction in the number of hidden units to represent a sign network as a ReLU network. Finally, using synthetic data, we experimentally demonstrate that back propagation can recover the much smaller ReLU networks as predicted by the theory.", "histories": [["v1", "Wed, 18 Nov 2015 07:26:12 GMT  (136kb,D)", "http://arxiv.org/abs/1511.05678v1", null], ["v2", "Thu, 7 Jan 2016 18:53:11 GMT  (87kb,D)", "http://arxiv.org/abs/1511.05678v2", null], ["v3", "Fri, 27 May 2016 05:11:55 GMT  (96kb,D)", "http://arxiv.org/abs/1511.05678v3", "Published in ICML 2016. Supplementary material included"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xingyuan pan", "vivek srikumar"], "accepted": true, "id": "1511.05678"}, "pdf": {"name": "1511.05678.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["RECTIFIER NETWORKS", "Xingyuan Pan"], "emails": ["xpan@cs.utah.edu", "svivek@cs.utah.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "A neural network is characterized by its architecture, the choices of activation functions, and its parameters. We see several activation functions in the literature - the most common are thresholds, logistic, hyperbolic tangents, and rectified linear units (ReLUs). In recent years, deep neural networks with recurrent neurons - defined as R (x) = max (0, x) - have demonstrated performance in multiple tasks such as image and language classification. (Glorot et al., 2010; Krizhevsky et al., 2012; Maas et al., 2013; Zeiler et al.).ReLUs possess several attractive computer properties, while deep networks with sigmoid activation units suffer under the surface."}, {"heading": "1.1 EXPRESSIVENESS OF NETWORKS: RELATED WORK", "text": "From the perspective of learning, the selection of an activation function is driven by two interrelated aspects: the first is the expressivity of the network using the activation function for Xiv: 151 1,05 678v 1 [cs.L G] 18 Nov 201 5given network architecture; the second is the computational complexity of learning. Although this work examines the first aspect, we briefly summarize the previous work along these two lines. It is known that any continuous function with only one hidden layer of sigmoid units (Cybenko, 1989) can be approached to any degree of accuracy, resulting in neural networks being referred to as \"universal approximators.\" With two layers, even discontinuous functions can be represented. Similarly, two layer thresholds networks are able to represent each boolean function. However, these are statements of existence; for a general target function, the number of hidden units is exponentially displayed in the input dimension."}, {"heading": "2 BOOLEAN FUNCTIONS EXPRESSED BY RECTIFIER NETWORKS", "text": "In order to simplify our analysis, this paper focuses primarily on flat networks with a hidden layer of n units and a single binary output. In all cases, the neurons of the hidden layer are the subject of investigation, while the activation function of the output is always the threshold function. In the rest of the essay, we use bold letters to identify vectors. Input function vectors and output binary labels are represented by x or y (\u00b1 1). The number of hidden units is n. The weights and prestresses for the rectifier of the kth are uk and bk; the weights and prestresses for the sign units of the kth are vk and dk. The weights for the output unit are w1 to wn, and the preload for the output unit is w0."}, {"heading": "2.1 THRESHOLD NETWORKS", "text": "Before we get to the main results, we first check the meaningfulness of threshold networks. Suppose there are n hidden units and an output unit, the output of the network can simply be written = sgn (w0 + n \u2211 k = 1 wk sgn (vk \u00b7 x + dk)). (1) Here, both hidden units and output activations are the drawing function. Each hidden unit represents a hyperplane (parameterized by vk and dk) that divides the input space into two half-spaces. By selecting different weights in the hidden layer, we can obtain an arbitrary arrangement of n hyperplanes. The theory of the hyperplane arrangement (Zaslavsky, 1975) shows that for a general arrangement of n hyperplanes in the d dimension, space is divided into certain d s = 0 (n) regions. The output unit consists of a linear combination of the hidden output (using the thresholds) and the values."}, {"heading": "2.2 RECTIFIER NETWORKS", "text": "In this section we will show that the decision limit of any two-layer neural network with rectifier activations is not as large as it can be represented. (1) However, the number of threshold units is exponential compared to the number of ReLUs (1). Consider a network with a hidden layer of n ReLUs designated by R (2). (2) Here we are using weight and bias parameters for the ReLUs in the hidden layer, and the wk's parameters of the output unit. To simplify the output unit, we will use the pre-activation input unit of the kth hidden unit. That is, ak (x) = uk."}, {"heading": "3 TRANSFORMING BETWEEN RELU AND THRESHOLD NETWORKS", "text": "In this section, we will address two related questions: firstly, can we build an equivalent threshold network in the face of an arbitrary ReLU network; secondly, in the face of an arbitrary threshold network, how can we present it with the help of the ReLU network?"}, {"heading": "3.1 FROM RELU TO THRESHOLD", "text": "Theorem 1 essentially gives us a constructive way to represent an arbitrary two-layer ReLU network, which is specified in Eq.3 as a three-layer threshold network. For each selection of the subsets S1 and S2 of the positive and negative units, we can define a Boolean function BS1, S2, as shown in Eq.8. By definition, each of these units is a threshold unit, giving us a total of 2n1 + n2 threshold units. (Remember that n1 and n2 are the quantities of P and N, respectively.) Since the decision function is a CNF or DNF about these functions, it can be represented by a two-layer network over the B's, giving us a total of three layers. We put all 2n1 + n2 threshold units into the first hidden layer, divided into 2n1 groups, with each group consisting of 2n2 units being a different unit (Figure 2 shows the threshold network, which corresponds to our example from the equation)."}, {"heading": "3.2 FROM THRESHOLD TO RELU", "text": "The next question we want to answer is under what condition we can use ReLUs to represent the same decision boundary as a threshold network. In this section we will show a series of results addressing different facets of this question. \u00b7 First, without limitations in the number of ReLUs in the hidden layer, then we can always construct a rectifier network that is equivalent to a threshold network. \u2212 In fact, we have the following problem that a threshold network with n units in the hidden layer is characterized by an arbitrary accuracy of a rectifier network with 2n units in 2n units.The proof of this problem is in Appendix B.Given the exponential increase in the layer that represents a ReLU network, a natural question is whether we can only use logarithmic number of ReLUs to represent a threshold network."}, {"heading": "4 HIDDEN LAYER EQUIVALENCE", "text": "Lemma 3 examines a specific threshold network in which the output layer represents a disjunction about the hidden layer networks. For this network, we can define an additional notion of equivalence by examining the activation of the hidden layer. Activating the hidden layer of this network can be interpreted as a specific type of multi-class classifier that either rejects inputs or labels them. If the output is negative, then clearly none of the hidden layer units is active and the input is rejected. If the output layer is positive, then at least one of the hidden layer units is active and the multiclass label is given by the maximum hidden unit, namely argmaxk vk \u00b7 x + dk. For threshold networks, the number of hidden units we need to learn is equal to the number of classes. The goal is to learn the same concept with retifier units, hopefully with less retifier units than the number of classes."}, {"heading": "5 EXPERIMENTS", "text": "We have seen that each two-layer rectifier network expresses the decision limit of a three-layer threshold network. However, if the initial weights of the former are all positive, then a two-layer threshold network is sufficient. (See the discussion at the end of Section 2.2.) However, the fact that rectifier networks can express the same decision limit more compactly does not guarantee learning ability. Specifically, in this section, using synthetic data, we will examine the following question: For a rectifier network and a threshold network with the same decision limit, can we learn from the data generated by another using back propagation?"}, {"heading": "5.1 DATA GENERATION", "text": "We use randomly constructed two-layer rectifier networks to generate labeled examples. To do this, we specify different values of the input dimensionality and the number of hidden ReLU units in the network. Once we have the network, we randomly create the input points and label them using the network. Based on the data generated, we try to restore both the rectifier network and the threshold network with different numbers of hidden units. We looked at the input dimensions 3, 10 and 50 and used 3 or 10 hidden units each, resulting in a total of six networks. For each network, we have generated 10,000 examples and 1500 of these are used as test examples."}, {"heading": "5.2 RESULTS AND ANALYSIS", "text": "The second setting uses the activation function tanh (cx), which we call compressed tanh activation. At large values of c, this effectively simulates the threshold function. In the second setting, the number of hidden units is still n. The final setting learns with the compressed tanh activation, but with 2n hidden units, as suggested in the discussion in \u00a7 2.2.Figure 3, shows our results on the six data sets. These results confirm several aspects of our theory. First, learning with ReLUs always succeeds with little error, as shown in the left bar of most of all six groups. This is expected because we know that our hypothesis class can successfully express the true concept and training with the help of background information. Second, learning with compressed tanh with the same number of units, which we are not able to achieve the true performance of all six groups, because we know that our hypothesis class can express the true concept and learning with the help of background information."}, {"heading": "6 CONCLUSIONS", "text": "In this paper, we discuss the advantage of neural rectifier networks in terms of their expressiveness. Specifically, for the binary classification, we show that while the decision limit of a two-layer rectifier network can be represented using a threshold network, the number of threshold units required is exponential. Furthermore, while a corresponding general logarithmic reduction of threshold units is not possible, we provide sufficient conditions for certain networks to reduce the threshold network to a much smaller rectifier network. Finally, we also represent a relaxed state in which we can approximately restore a rectifier network that represents a hidden layer corresponding to an exponentially larger threshold network. Our work represents a natural next step: can we use the equivalence of the results given in this paper to investigate the sample complexity of rectifier networks? Another open question is the generalization of these results to deep networks. Finally, we see that our learning power is not enough to guarantee that we are experimenting."}, {"heading": "A PROOF OF THEOREM 1", "text": "Step 1: equivalence of conditions 1 and 2Let us prove that condition 1 first implies condition 2. Suppose y > 0, we can construct a subset S1 of P S1 = {k: k: p and ak (x) \u2265 0}. The decision function yields us1 = sgn [w0 + \u2211 k: S1 ak (x) \u2212 K: N R (ak (x: k: k) \u2012 K: S1 ak (x: k: k) \u2012 K: S2 ak (x). For each subset S2 of N we have k: N R (x: k: k: k: k: k: k: k: k: k: k: k: k: k: k: k: k: k ak (k: k: k k k k: k: k k: k: k k: k: k) ak (k: k k k: k k k: k k k: k k k: k) ak: (k: k k k k: k k k k: k k: k) k: k: k (k: k k: k k: k: k) k: k: k (k: k: k: k) k: k: k: k (k: k: k) k: k: k: k: k: k: k: k: k: k: k: k) k: k: k (k: k: k: k: k: k: k: k: k: k: k) k: k: k: k: k: k: k: k: k: k: k: k: k: k) k: k: k: k: k: k: k: k: k: k: k: k k k: k: k: k k: k: k) k: k: k: k: k: k: k: k k: k: k: k k: k: k: k: k: k k: k k: k k: k k k k k: k k k: k (k k) k: k: k: k: k: k: k: k: k k: k k: k: k k k k: k: k: k k (k k) k: k: k: k: k: k: k: k"}, {"heading": "B PROOF OF LEMMA 1", "text": "If we consider a threshold unit with weight vector v and prestressing d, we have gn (v \u00b7 x + d) '1 [R (v \u00b7 x + d +) \u2212 R (v \u00b7 x + d \u2212)] \u2212 1. where is an arbitrarily small number that determines the approximation accuracy."}, {"heading": "C PROOF OF LEMMA 2", "text": "In this appendix, we provide a simple example of a two-layer threshold network whose decision boundary cannot be represented by a two-layer rectifier network with less hidden units. Consider the case where we have three-dimensional input vectors and three threshold units in the hidden layer. The decision function is of y = sgn [2 + sgn (x1) + sgn (x2) + sgn (x3)], (12) i.e. the weight for each hidden unit is the three base vectors, the preload for each hidden unit is zero, the weights for the output unit are 1 s and the preload is 2. It is easy to see that the decision function of this network is positive if and only if at least one of x1, x2 and x3 is greater than or equal to zero. From a geometric point of view, one of eight octants wwwwwwwwx is marked as negative and all other seven octants are marked as positive."}, {"heading": "D PROOF OF LEMMA 3", "text": "If the weight parameters vk and dk can be written as in Equation (9), then we can construct the two-layer rectifier network y = sgn [w0 + n \u2211 k = 1 R (uk \u00b7 x + bk)]. (15) Then, according to sentence 1, the decision limit of the rectifier network in Equation (15) is the same as the decision limit of the threshold network in Equation (3)."}, {"heading": "E PROOF OF THEOREM 2", "text": "Let us define that after defining the vector standard L \u00b2 (V \u2212 UT) Tx \u00b2 (V \u2212 UT) Tx \u00b2 (V \u2212 UT) Tx) k | for all x and all k. the subscript k denotes the kest component of the vector. After defining the induced standard we have k \u00b2 x for all x and all k. Especially for k \u00b2 (V \u2212 UT) k \u00b2 (UT) k \u00b2 x \u00b2 (16) and for all others k \u00b2 6 = k \u00b2 (UT) k \u00b2 (UT) k \u00b2 x \u00b2 for all x and all k. Special for k \u00b2 (V Tx) k \u00b2 (17) k \u00b2 (UT) k \u00b2 x \u00b2 (16) and for all others k \u00b2 6 = k \u00b2 (UT) k \u00b2 (UT) k \u00b2 (UT) k \u00b2 (V \u00b2) k \u00b2 \u00b2 (17)."}], "references": [{"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio", "Yoshua", "Simard", "Patrice", "Frasconi", "Paolo"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["G. Cybenko"], "venue": "Mathematics of Control, Signals, and Systems,", "citeRegEx": "Cybenko,? \\Q1989\\E", "shortCiteRegEx": "Cybenko", "year": 1989}, {"title": "From average case complexity to improper learning complexity", "author": ["Daniely", "Amit"], "venue": "In SToC,", "citeRegEx": "Daniely and Amit.,? \\Q2014\\E", "shortCiteRegEx": "Daniely and Amit.", "year": 2014}, {"title": "Deep Sparse Rectifier", "author": ["Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua"], "venue": "Neural Networks. AISTATS,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "The vanishing gradient problem during learning recurrent neural nets and problem solutions", "author": ["Hochreiter", "Sepp"], "venue": "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems,", "citeRegEx": "Hochreiter and Sepp.,? \\Q1998\\E", "shortCiteRegEx": "Hochreiter and Sepp.", "year": 1998}, {"title": "Cryptographic hardness for learning intersections of halfspaces", "author": ["Klivans", "Adam R", "Sherstov", "Alexander a"], "venue": "In FOCS,", "citeRegEx": "Klivans et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Klivans et al\\.", "year": 2006}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "NIPS, pp", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "On the computational efficiency of training neural networks", "author": ["Livni", "Roi", "Shalev-Shwartz", "Shai", "Shamir", "Ohad"], "venue": "In NIPS, pp", "citeRegEx": "Livni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Livni et al\\.", "year": 2014}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["Maas", "Andrew L", "Hannun", "Awni Y", "Ng", "Andrew Y"], "venue": "In ICML,", "citeRegEx": "Maas et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2013}, {"title": "On the Number of Linear Regions of Deep Neural Networks", "author": ["Montufar", "Guido", "Pascanu", "Razvan", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In NIPS, pp", "citeRegEx": "Montufar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Montufar et al\\.", "year": 2014}, {"title": "Rectified linear units improve Restricted Boltzmann Machines", "author": ["Nair", "Vinod", "Hinton", "Geoffrey E"], "venue": "In ICML, pp", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "On the number of response regions of deep feedforward networks with piecewise linear activations", "author": ["Pascanu", "Razvan", "Montufar", "Guido", "Bengio", "Yoshua"], "venue": "In ICLR, pp", "citeRegEx": "Pascanu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2014}, {"title": "Facing up to arrangements: face-count formulas for partitions of space by hyperplanes", "author": ["Zaslavsky", "Thomas"], "venue": "American Mathematical Society,", "citeRegEx": "Zaslavsky and Thomas.,? \\Q1975\\E", "shortCiteRegEx": "Zaslavsky and Thomas.", "year": 1975}, {"title": "On rectified linear units for speech processing", "author": ["Zeiler", "Matthew D", "Ranzato", "Marc\u2019Aurelio", "Monga", "Rajat", "Mao", "Min", "Yang", "Kun", "Le", "Quoc Viet", "Nguyen", "Patrick", "Senior", "Alan", "Vanhoucke", "Vincent", "Dean", "Jeffrey"], "venue": "In ICASSP,", "citeRegEx": "Zeiler et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "First, while deep networks with sigmoidal activation units suffer from the vanishing gradient problem (Bengio et al., 1994; Hochreiter, 1998), ReLU networks do not.", "startOffset": 102, "endOffset": 141}, {"referenceID": 3, "context": "Second, rectifying neurons encourage sparsity in the hidden layers (Glorot et al., 2011).", "startOffset": 67, "endOffset": 88}, {"referenceID": 0, "context": "First, while deep networks with sigmoidal activation units suffer from the vanishing gradient problem (Bengio et al., 1994; Hochreiter, 1998), ReLU networks do not. Second, rectifying neurons encourage sparsity in the hidden layers (Glorot et al., 2011). Third, gradient back propagation is efficient because of the piece-wise linear nature of the function. For example, Krizhevsky et al. (2012) report convolutional neural network with ReLUs is six times faster than an equivalent one with hyperbolic tangent neurons.", "startOffset": 103, "endOffset": 396}, {"referenceID": 1, "context": "It is known that any continuous function can be approximated to arbitrary accuracy with only one hidden layer of sigmoid units (Cybenko, 1989), leading to neural networks being called \u201cuniversal approximators\u201d.", "startOffset": 127, "endOffset": 142}, {"referenceID": 1, "context": "It is known that any continuous function can be approximated to arbitrary accuracy with only one hidden layer of sigmoid units (Cybenko, 1989), leading to neural networks being called \u201cuniversal approximators\u201d. With two layers, even discontinuous functions can be represented. Similarly, two layer threshold networks are capable of representing any Boolean function. However, these are existence statements; for a general target function, the number of hidden units may be exponential in the input dimensionality. There has been some recent work that looks at the expressivity of feed-forward ReLU networks. Because rectifier function is piece-wise linear, any network using only ReLUs can only represent piece-wise linear functions. Thus, the number of linear partitions of input space by the network can be viewed as a measure of is complexity. Pascanu et al. (2014) and Montufar et al.", "startOffset": 128, "endOffset": 869}, {"referenceID": 1, "context": "It is known that any continuous function can be approximated to arbitrary accuracy with only one hidden layer of sigmoid units (Cybenko, 1989), leading to neural networks being called \u201cuniversal approximators\u201d. With two layers, even discontinuous functions can be represented. Similarly, two layer threshold networks are capable of representing any Boolean function. However, these are existence statements; for a general target function, the number of hidden units may be exponential in the input dimensionality. There has been some recent work that looks at the expressivity of feed-forward ReLU networks. Because rectifier function is piece-wise linear, any network using only ReLUs can only represent piece-wise linear functions. Thus, the number of linear partitions of input space by the network can be viewed as a measure of is complexity. Pascanu et al. (2014) and Montufar et al. (2014) show that given the same number of ReLUs, a deep architecture can represent functions with exponentially more linear regions than a shallow architecture.", "startOffset": 128, "endOffset": 896}, {"referenceID": 1, "context": "It is known that any continuous function can be approximated to arbitrary accuracy with only one hidden layer of sigmoid units (Cybenko, 1989), leading to neural networks being called \u201cuniversal approximators\u201d. With two layers, even discontinuous functions can be represented. Similarly, two layer threshold networks are capable of representing any Boolean function. However, these are existence statements; for a general target function, the number of hidden units may be exponential in the input dimensionality. There has been some recent work that looks at the expressivity of feed-forward ReLU networks. Because rectifier function is piece-wise linear, any network using only ReLUs can only represent piece-wise linear functions. Thus, the number of linear partitions of input space by the network can be viewed as a measure of is complexity. Pascanu et al. (2014) and Montufar et al. (2014) show that given the same number of ReLUs, a deep architecture can represent functions with exponentially more linear regions than a shallow architecture. More linear regions definitely indicate that more complex functions can be represented. However, this does not directly tell us how expressive a function is. This is because, at prediction time, we cannot directly correlate the number of regions to the way we make the prediction. The learning complexity of neural networks using various activation functions has also been studied. It is known that for inputs from the Boolean hypercube, the two-layer networks with threshold activation functions is not efficiently learnable (Klivans & Sherstov, 2006; Daniely, 2014). Without restricting the weights, two layer networks with sigmoid or ReLU activations are also not efficiently learnable. The recent work of Livni et al. (2014) describe positive and negative learnability results for various activation functions.", "startOffset": 128, "endOffset": 1779}], "year": 2017, "abstractText": "Rectified Linear Units (ReLUs) have been shown to ameliorate the vanishing gradient problem, allow for efficient back-propagation, and empirically promote sparsity in the learned parameters. Their use has led to state-of-the-art results in a variety of applications. In this paper, we characterize the expressiveness of ReLU networks. From this perspective, unlike the sign (threshold) and sigmoid activations, ReLU networks are less explored. We show that, while the decision boundary of a two-layer ReLU network can be captured by a sign network, the sign network can require an exponentially larger number of hidden units. Furthermore, we formulate the sufficient conditions for a corresponding logarithmic reduction in the number of hidden units to represent a sign network as a ReLU network. Finally, using synthetic data, we experimentally demonstrate that back propagation can recover the much smaller ReLU networks as predicted by the theory.", "creator": "LaTeX with hyperref package"}}}