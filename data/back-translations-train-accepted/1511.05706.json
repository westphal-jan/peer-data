{"id": "1511.05706", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2015", "title": "Efficient Output Kernel Learning for Multiple Tasks", "abstract": "The paradigm of multi-task learning is that one can achieve better generalization by learning tasks jointly and thus exploiting the similarity between the tasks rather than learning them independently of each other. While previously the relationship between tasks had to be user-defined in the form of an output kernel, recent approaches jointly learn the tasks and the output kernel. As the output kernel is a positive semidefinite matrix, the resulting optimization problems are not scalable in the number of tasks as an eigendecomposition is required in each step. \\mbox{Using} the theory of positive semidefinite kernels we show in this paper that for a certain class of regularizers on the output kernel, the constraint of being positive semidefinite can be dropped as it is automatically satisfied for the relaxed problem. This leads to an unconstrained dual problem which can be solved efficiently. Experiments on several multi-task and multi-class data sets illustrate the efficacy of our approach in terms of computational efficiency as well as generalization performance.", "histories": [["v1", "Wed, 18 Nov 2015 09:37:54 GMT  (1613kb,D)", "http://arxiv.org/abs/1511.05706v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["pratik jawanpuria", "maksim lapin", "matthias hein 0001", "bernt schiele"], "accepted": true, "id": "1511.05706"}, "pdf": {"name": "1511.05706.pdf", "metadata": {"source": "CRF", "title": "Efficient Output Kernel Learning for Multiple Tasks", "authors": ["Pratik Jawanpuria", "Maksim Lapin", "Bernt Schiele"], "emails": [], "sections": [{"heading": null, "text": "The paradigm of multi-task learning is that you can achieve a better generalization by learning tasks together, taking advantage of the similarity between tasks rather than learning them independently of each other. Whereas in the past, the relationship between tasks had to be custom-defined in the form of an output core, the most recent approaches jointly learn the tasks and the output core. As the output core is a positive semi-defined matrix, the resulting optimization problems are not scalable in the number of tasks, as each step requires a self-decomposition. Using positive semi-defined cores theory, we show in this paper that for a particular class of regulators on the output core the limitation of being positively semi-defined can be removed, as it is automatically fulfilled for the relaxed problem. This leads to an unlimited duplication problem that can be efficiently solved. Experiments on multiple multi-task and multi-class datasets illustrate the effectiveness of our approach to generating."}, {"heading": "1 Introduction", "text": "This year, it is more than ever in the history of the city, where it is so far that it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place."}, {"heading": "2 The Output Kernel Learning Formulation", "text": "However, we assume that all tasks have a common input space X and a common positive definite core function (1).The training data is (xi, yi, ti) ni = 1, where the task is the i-th instance and yi is the corresponding label. Furthermore, we have a positive definite matrix based on the task. (xi, yi, ti) ni = 1, where the ST + set of T is symmetrical and positively semidefined (p.s.d.) matrices.If you arrange the predictions of all tasks in a vector, learning can be seen as a vector-weighted function in an RKHS."}, {"heading": "4 Optimization Algorithm", "text": "The dual problem (29) can be efficiently solved by decomposition problems such as stochastic dual coordinate ascension algorithm (SDCA) [19]. Our algorithm for learning the output kernel matrix and task parameters is summarized in algorithm 1. At each step of the iteration, we optimize the dual target using a randomly selected \u03b1i variable. Let ti = r be the task that corresponds to \u03b1i. We apply the update. The optimization problem of the solution (29) regarding this is as follows: min."}, {"heading": "5 Empirical Results", "text": "In this section, we present our results on benchmark data sets, which compare our algorithm with existing approaches in terms of generalization accuracy and computational efficiency. In Section 5.1, we discuss generalization results in multi-task setting. We evaluate the performance of our algorithm using several current multi-task methods that use cluster and low-dimensional projections of input feature space or output kernel learning. Section 5.2 discusses results from multi-class experiments. Single Task Learning (STL) is a common baseline in these two experiments and uses hinge loss and SVR loss functions for classification and regression problems. Finally, in Section 5.3, we discuss the results on the computational efficiency of our algorithm."}, {"heading": "5.1 Multi-Task Data Sets", "text": "We start by generalizing the results in multi-task setups. The datasets are as follows: Sarcos: A Multi-Task Regression Dataset. The goal is to predict 7 degrees of freedom of a robot arm [28]. Parkinson: A Multi-Task Regression Dataset [29], where one must predict the Parkinson's disease symptom score for 42 patients. Yale: A face recognition data set from the Yale face base with 28 binary classification tasks [30]. Landmine: A dataset with binary classification problems from 19 different landmines [30]. MHC-I: A Bioinformatics dataset with 10 binary classification tasks [12]. Letter: A dataset with handwritten letters from several authors and with 9 binary classification tasks [31]. Table 2 presents the dataset statistics. We compare the following algorithms: MTL [16]: A multi-task learning basis."}, {"heading": "5.2 Multi-Class Data Sets", "text": "In this section, we experimented with two loss functions: a) FMTLp-H - the hinge loss used in SVMs, and b) FMTLp-S - the square loss used in OKL [17]. In these experiments, we also compare our results with MTL-SDCA, a state-of-the-art multi-task learning method [32]. In addition, we report on the results of our KL divergence formulation with square losses (referred to by FMTLkl). Handwritten Digit Recognition: We look at the following two data sets in detail in [10]. USPS: A handwritten digital data set with 10 classes [33] We process the images with PCA and reduce the dimensionality to 87."}, {"heading": "5.3 Scaling Experiment", "text": "We compare the runtime of our solver for FMTL2-S with the OKL solver of [17] and the ConvexOKL solver of [18] on several datasets. All three methods solve the same optimization problem. Figure 3a shows the result of the scaling experiment, in which we vary the number of tasks (classes). The parameters used are those determined by cross-validation. Note that both OKL and ConvexOKL algorithms do not have a well-defined stop criterion, whereas our approach can easily calculate the relative duality gap (set as 10 \u2212 3). We terminate them when they reach the primary objective value achieved by FMTL2-S. Our optimization approach is 7 times and 4.3 times faster than the alternative minimization based on OKL and ConvexOKL, if the number of tasks is maximum. The FTL2 is also significantly faster MTL7 / 3.3 times faster than the SL for the conversion curve value."}, {"heading": "6 Conclusion", "text": "Our most important technical contribution is our analysis of a particular class of regulators on the output kernel matrix, where the positive semi-definitive limitation of the optimization problem can be dropped, but the problem can still be optimally solved. This results in a dual formulation that can be efficiently solved using a stochastic dual coordinate ascent algorithm. Results on benchmark multi-task and multi-class data sets show the effectiveness of the proposed multi-task algorithm in terms of runtime and generalization accuracy. Recognition. P.J. and M.H. confirm the support of the Excellence Cluster of Excellence (MMCI)."}], "references": [{"title": "Learning multiple tasks with kernel methods", "author": ["T. Evgeniou", "C.A. Micchelli", "M. Pontil"], "venue": "JMLR, 6:615\u2013637,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "Convex multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "ML, 73:243\u2013272,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Taking advantage of sparsity in multi-task learning", "author": ["K. Lounici", "M. Pontil", "A.B. Tsybakov", "S. van de Geer"], "venue": "In COLT,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "A dirty model for multi-task learning", "author": ["A. Jalali", "P. Ravikumar", "S. Sanghavi", "C. Ruan"], "venue": "NIPS,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-task multiple kernel learning", "author": ["P. Jawanpuria", "J.S. Nath"], "venue": "SDM,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Sparse coding for multitask and transfer learning", "author": ["A. Maurer", "M. Pontil", "B. Romera-paredes"], "venue": "ICML,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Generalized hierarchical kernel learning", "author": ["P. Jawanpuria", "J.S. Nath", "G. Ramakrishnan"], "venue": "JMLR, 16:617\u2013 652,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "ML, 28:41\u201375,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1997}, {"title": "A convex formulation for learning task relationships in multi-task learning", "author": ["Y. Zhang", "D.Y. Yeung"], "venue": "UAI,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning with whom to share in multi-task feature learning", "author": ["Z. Kang", "K. Grauman", "F. Sha"], "venue": "ICML,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "A convex feature learning formulation for latent task structure discovery", "author": ["P. Jawanpuria", "J.S. Nath"], "venue": "ICML,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Clustered multi-task learning: A convex formulation", "author": ["L. Jacob", "F. Bach", "J.P. Vert"], "venue": "NIPS,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Kernels for multitask learning", "author": ["C.A. Micchelli", "M. Pontil"], "venue": "NIPS,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Universal multi-task kernels", "author": ["A. Caponnetto", "C.A. Micchelli", "M. Pontil", "Y. Ying"], "venue": "JMLR, 9:1615\u2013 1646,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Kernels for vector-valued functions: a review", "author": ["M.A. \u00c1lvarez", "L. Rosasco", "N.D. Lawrence"], "venue": "Foundations and Trends in Machine Learning, 4:195\u2013266,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Regularized multi\u2013task learning", "author": ["T. Evgeniou", "M. Pontil"], "venue": "KDD,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning output kernels with block coordinate descent", "author": ["F. Dinuzzo", "C.S. Ong", "P. Gehler", "G. Pillonetto"], "venue": "ICML,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Convex learning of multiple tasks and their structure", "author": ["C. Ciliberto", "Y. Mroueh", "T. Poggio", "L. Rosasco"], "venue": "ICML,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Stochastic dual coordinate ascent methods for regularized loss", "author": ["S. Shalev-Shwartz", "T. Zhang"], "venue": "JMLR, 14(1):567\u2013599,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning with Kernels", "author": ["B. Sch\u00f6lkopf", "A. Smola"], "venue": "MIT Press,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Kernels, associated structures and generalizations", "author": ["M. Hein", "O. Bousquet"], "venue": "Technical Report TR-127, Max Planck Institute for Biological Cybernetics,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "What is invexity ? J", "author": ["A. Ben-Israel", "B. Mond"], "venue": "Austral. Math. Soc. Ser. B, 28:1\u20139,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1986}, {"title": "Convex Optimization and Euclidean Distance Geometry", "author": ["J. Dattorro"], "venue": "Meboo Publishing,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": "Cambridge University Press,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2004}, {"title": "The theory of infinitely divisible matrices and kernels", "author": ["R.A. Horn"], "venue": "Trans. Amer. Math. Soc., 136:269\u2013286,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1969}, {"title": "Monotonicity for entrywise functions of matrices", "author": ["F. Hiai"], "venue": "Linear Algebra and its Applications, 431(8):1125 \u2013 1146,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Fundamentals of convex analysis", "author": ["J.-B. Hiriart-Urruty", "C. Lemar\u00e9chal"], "venue": "Springer,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2001}, {"title": "Semi-supervised multi-task regression", "author": ["Y. Zhang", "D.Y. Yeung"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Learning multiple tasks with a sparse matrix-normal penalty", "author": ["Y. Zhang", "J. Schneider"], "venue": "NIPS,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "An accelerated gradient method for trace norm minimization", "author": ["S. Ji", "J. Ye"], "venue": "ICML,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Scalable multitask representation learning for scene classification", "author": ["M. Lapin", "B. Schiele", "M. Hein"], "venue": "CVPR,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "A database for handwritten text recognition research", "author": ["J.J. Hull"], "venue": "IEEE PAMI, 16:550\u2013554,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1994}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1998}, {"title": "Learning deep features for scene recognition using places database", "author": ["B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "NIPS,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional network features for scene recognition", "author": ["M. Koskela", "J. Laaksonen"], "venue": "Proceedings of the ACM International Conference on Multimedia,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "SUN database: Large-scale scene recognition from abbey to zoo", "author": ["J. Xiao", "J. Hays", "K.A. Ehinger", "A. Oliva", "A. Torralba"], "venue": "CVPR,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "The advantage of MTL over learning tasks independently has been shown theoretically as well as empirically [1, 2, 3, 4, 5, 6, 7].", "startOffset": 107, "endOffset": 128}, {"referenceID": 1, "context": "The advantage of MTL over learning tasks independently has been shown theoretically as well as empirically [1, 2, 3, 4, 5, 6, 7].", "startOffset": 107, "endOffset": 128}, {"referenceID": 2, "context": "The advantage of MTL over learning tasks independently has been shown theoretically as well as empirically [1, 2, 3, 4, 5, 6, 7].", "startOffset": 107, "endOffset": 128}, {"referenceID": 3, "context": "The advantage of MTL over learning tasks independently has been shown theoretically as well as empirically [1, 2, 3, 4, 5, 6, 7].", "startOffset": 107, "endOffset": 128}, {"referenceID": 4, "context": "The advantage of MTL over learning tasks independently has been shown theoretically as well as empirically [1, 2, 3, 4, 5, 6, 7].", "startOffset": 107, "endOffset": 128}, {"referenceID": 5, "context": "The advantage of MTL over learning tasks independently has been shown theoretically as well as empirically [1, 2, 3, 4, 5, 6, 7].", "startOffset": 107, "endOffset": 128}, {"referenceID": 6, "context": "The advantage of MTL over learning tasks independently has been shown theoretically as well as empirically [1, 2, 3, 4, 5, 6, 7].", "startOffset": 107, "endOffset": 128}, {"referenceID": 7, "context": "It has been noted that naively grouping all the tasks together may be detrimental [8, 9, 10, 11].", "startOffset": 82, "endOffset": 96}, {"referenceID": 8, "context": "It has been noted that naively grouping all the tasks together may be detrimental [8, 9, 10, 11].", "startOffset": 82, "endOffset": 96}, {"referenceID": 9, "context": "It has been noted that naively grouping all the tasks together may be detrimental [8, 9, 10, 11].", "startOffset": 82, "endOffset": 96}, {"referenceID": 10, "context": "It has been noted that naively grouping all the tasks together may be detrimental [8, 9, 10, 11].", "startOffset": 82, "endOffset": 96}, {"referenceID": 9, "context": "Hence, clustered multi-task learning algorithms [10, 12] aim to learn groups of closely related tasks.", "startOffset": 48, "endOffset": 56}, {"referenceID": 11, "context": "Hence, clustered multi-task learning algorithms [10, 12] aim to learn groups of closely related tasks.", "startOffset": 48, "endOffset": 56}, {"referenceID": 0, "context": "The multi-task kernel on input and output is assumed to be decoupled as the product of a scalar kernel and the output kernel, which is a positive semidefinite matrix [1, 13, 14, 15].", "startOffset": 166, "endOffset": 181}, {"referenceID": 12, "context": "The multi-task kernel on input and output is assumed to be decoupled as the product of a scalar kernel and the output kernel, which is a positive semidefinite matrix [1, 13, 14, 15].", "startOffset": 166, "endOffset": 181}, {"referenceID": 13, "context": "The multi-task kernel on input and output is assumed to be decoupled as the product of a scalar kernel and the output kernel, which is a positive semidefinite matrix [1, 13, 14, 15].", "startOffset": 166, "endOffset": 181}, {"referenceID": 14, "context": "The multi-task kernel on input and output is assumed to be decoupled as the product of a scalar kernel and the output kernel, which is a positive semidefinite matrix [1, 13, 14, 15].", "startOffset": 166, "endOffset": 181}, {"referenceID": 0, "context": "In classical multi-task learning algorithms [1, 16], the degree of relatedness between distinct tasks is set to a constant and is optimized as a hyperparameter.", "startOffset": 44, "endOffset": 51}, {"referenceID": 15, "context": "In classical multi-task learning algorithms [1, 16], the degree of relatedness between distinct tasks is set to a constant and is optimized as a hyperparameter.", "startOffset": 44, "endOffset": 51}, {"referenceID": 16, "context": "[17] solves a multi-task formulation in the framework of vector-valued reproducing kernel Hilbert spaces involving squared loss where they penalize the Frobenius norm of the output kernel as a regularizer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "In comparison, [18] recently proposed an efficient barrier method to optimize a generic convex output kernel learning formulation.", "startOffset": 15, "endOffset": 19}, {"referenceID": 8, "context": "On the other hand, [9] proposes a convex formulation to learn low rank output kernel matrix by enforcing a trace constraint.", "startOffset": 19, "endOffset": 22}, {"referenceID": 8, "context": "The above approaches [9, 17, 18] solve the resulting optimization problem via alternate minimization between task parameters and the output kernel.", "startOffset": 21, "endOffset": 32}, {"referenceID": 16, "context": "The above approaches [9, 17, 18] solve the resulting optimization problem via alternate minimization between task parameters and the output kernel.", "startOffset": 21, "endOffset": 32}, {"referenceID": 17, "context": "The above approaches [9, 17, 18] solve the resulting optimization problem via alternate minimization between task parameters and the output kernel.", "startOffset": 21, "endOffset": 32}, {"referenceID": 16, "context": "In this paper we study a similar formulation as [17].", "startOffset": 48, "endOffset": 52}, {"referenceID": 18, "context": "The resulting unconstrained dual problem is amenable to efficient optimization methods such as stochastic dual coordinate ascent [19], which scale well to large data sets.", "startOffset": 129, "endOffset": 133}, {"referenceID": 19, "context": "We denote by \u03c8(\u00b7) the feature map and by Hk the reproducing kernel Hilbert space (RKHS) [20] associated with k.", "startOffset": 88, "endOffset": 92}, {"referenceID": 20, "context": "However, in this paper we use the one-to-one correspondence between real-valued and matrix-valued kernels, see [21], in order to limit the technical overhead.", "startOffset": 111, "endOffset": 115}, {"referenceID": 19, "context": "Proof: The proof is analogous to the standard representer theorem [20].", "startOffset": 66, "endOffset": 70}, {"referenceID": 16, "context": "A similar problem has been analyzed in [17].", "startOffset": 39, "endOffset": 43}, {"referenceID": 21, "context": "For an invex function every stationary point is globally optimal [22].", "startOffset": 65, "endOffset": 69}, {"referenceID": 1, "context": "We follow a different path which leads to a formulation similar to the one of [2] used for learning an input mapping (see also [9]).", "startOffset": 78, "endOffset": 81}, {"referenceID": 8, "context": "We follow a different path which leads to a formulation similar to the one of [2] used for learning an input mapping (see also [9]).", "startOffset": 127, "endOffset": 130}, {"referenceID": 1, "context": "We present a derivation for the general RKHS Hk, analogous to the linear case presented in [2, 9].", "startOffset": 91, "endOffset": 97}, {"referenceID": 8, "context": "We present a derivation for the general RKHS Hk, analogous to the linear case presented in [2, 9].", "startOffset": 91, "endOffset": 97}, {"referenceID": 8, "context": "i,j=1 ( \u0398\u22121 ) sr \u03b2is\u03b2jrkij + \u03bbV (\u0398) (10) Before we analyze the convexity of this problem, we want to illustrate the connection to the formulations in [9, 17].", "startOffset": 150, "endOffset": 157}, {"referenceID": 16, "context": "i,j=1 ( \u0398\u22121 ) sr \u03b2is\u03b2jrkij + \u03bbV (\u0398) (10) Before we analyze the convexity of this problem, we want to illustrate the connection to the formulations in [9, 17].", "startOffset": 150, "endOffset": 157}, {"referenceID": 14, "context": "This identity is known for vector-valued RKHS, see [15] and references therein.", "startOffset": 51, "endOffset": 55}, {"referenceID": 1, "context": "The following result has been shown in [2] (see also [9]).", "startOffset": 39, "endOffset": 42}, {"referenceID": 8, "context": "The following result has been shown in [2] (see also [9]).", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "Proof: It has been shown in [2] and [23][p.", "startOffset": 28, "endOffset": 31}, {"referenceID": 22, "context": "Proof: It has been shown in [2] and [23][p.", "startOffset": 36, "endOffset": 40}, {"referenceID": 23, "context": "and thus we can write the function f as a positive combination of convex functions, where the arguments are composed with linear mappings which preserves convexity [24].", "startOffset": 164, "endOffset": 168}, {"referenceID": 8, "context": "The formulation in (10) is similar to [9, 17, 18].", "startOffset": 38, "endOffset": 49}, {"referenceID": 16, "context": "The formulation in (10) is similar to [9, 17, 18].", "startOffset": 38, "endOffset": 49}, {"referenceID": 17, "context": "The formulation in (10) is similar to [9, 17, 18].", "startOffset": 38, "endOffset": 49}, {"referenceID": 8, "context": "[9] uses the constraint Trace(\u0398) \u2264 1 instead of a regularizer V (\u0398) enforcing low rank of the output kernel.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "On the other hand, [17] employs squared Frobenius norm for V (\u0398) with squared loss function.", "startOffset": 19, "endOffset": 23}, {"referenceID": 17, "context": "[18] proposed an efficient algorithm for convex V (\u0398).", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "V (\u0398) = T \u2211 t,t\u2032=1 |\u0398tt\u2032 | = \u2016\u0398\u2016pp , for p \u2208 [1, 2].", "startOffset": 45, "endOffset": 51}, {"referenceID": 1, "context": "V (\u0398) = T \u2211 t,t\u2032=1 |\u0398tt\u2032 | = \u2016\u0398\u2016pp , for p \u2208 [1, 2].", "startOffset": 45, "endOffset": 51}, {"referenceID": 8, "context": "Several approaches [9, 17, 18] employ alternate minimization scheme, involving costly eigendecompositions of T \u00d7T matrix per iteration (as \u0398 \u2208 S +).", "startOffset": 19, "endOffset": 30}, {"referenceID": 16, "context": "Several approaches [9, 17, 18] employ alternate minimization scheme, involving costly eigendecompositions of T \u00d7T matrix per iteration (as \u0398 \u2208 S +).", "startOffset": 19, "endOffset": 30}, {"referenceID": 17, "context": "Several approaches [9, 17, 18] employ alternate minimization scheme, involving costly eigendecompositions of T \u00d7T matrix per iteration (as \u0398 \u2208 S +).", "startOffset": 19, "endOffset": 30}, {"referenceID": 23, "context": "(14) Using the definition of the conjugate function [24], we get min zi\u2208R C L(yi, zi) + \u03b1izi = C min zi\u2208R L(yi, zi) + \u03b1i C zi = \u2212C max zi\u2208R ( \u2212 \u03b1i C zi \u2212 L(yi, zi) ) (15) = \u2212C Li ( \u2212 \u03b1i C ) , (16) where Li is the conjugate function of Li : z \u2192 L(yi, z).", "startOffset": 52, "endOffset": 56}, {"referenceID": 24, "context": "It has been shown [25], that the elementwise power Ars of a positive semidefinite matrix A is positive definite for all A \u2208 S + and T \u2208 N if and only if l is a positive integer.", "startOffset": 18, "endOffset": 22}, {"referenceID": 16, "context": "The regularizer for p = 2 together with the squared loss has been considered in the primal in [17, 18].", "startOffset": 94, "endOffset": 102}, {"referenceID": 17, "context": "The regularizer for p = 2 together with the squared loss has been considered in the primal in [17, 18].", "startOffset": 94, "endOffset": 102}, {"referenceID": 16, "context": "This is in contrast to the alternating scheme of [17, 18] for the primal problem which involves costly matrix operations.", "startOffset": 49, "endOffset": 57}, {"referenceID": 17, "context": "This is in contrast to the alternating scheme of [17, 18] for the primal problem which involves costly matrix operations.", "startOffset": 49, "endOffset": 57}, {"referenceID": 16, "context": "Our runtime experiments show that our solver for (29) outperforms the solvers of [17, 18].", "startOffset": 81, "endOffset": 89}, {"referenceID": 17, "context": "Our runtime experiments show that our solver for (29) outperforms the solvers of [17, 18].", "startOffset": 81, "endOffset": 89}, {"referenceID": 25, "context": "This set of functions has been characterized by Hiai [26].", "startOffset": 53, "endOffset": 57}, {"referenceID": 25, "context": "Theorem 5 ([26]) Let f : R \u2192 R and A \u2208 S +.", "startOffset": 11, "endOffset": 15}, {"referenceID": 24, "context": "If T is fixed, the set of functions is larger and includes even (large) fractional powers, see [25].", "startOffset": 95, "endOffset": 99}, {"referenceID": 18, "context": "4 Optimization Algorithm The dual problem (29) can be efficiently solved via decomposition based methods like stochastic dual coordinate ascent algorithm (SDCA) [19].", "startOffset": 161, "endOffset": 165}, {"referenceID": 27, "context": "The aim is to predict 7 degrees of freedom of a robotic arm [28].", "startOffset": 60, "endOffset": 64}, {"referenceID": 28, "context": "Yale: A face recognition data set from the Yale face base with 28 binary classification tasks [30].", "startOffset": 94, "endOffset": 98}, {"referenceID": 28, "context": "Landmine: A data set containing binary classification problems from 19 different landmines [30].", "startOffset": 91, "endOffset": 95}, {"referenceID": 11, "context": "MHC-I: A bioinformatics data set having 10 binary classification tasks [12].", "startOffset": 71, "endOffset": 75}, {"referenceID": 29, "context": "Letter: A data set containing handwritten letters from several writers and having 9 binary classification tasks [31].", "startOffset": 112, "endOffset": 116}, {"referenceID": 15, "context": "We compare the following algorithms: MTL [16]: A classical multi-task learning baseline.", "startOffset": 41, "endOffset": 45}, {"referenceID": 11, "context": "CMTL [12]: A clustered multi-task learning algorithm.", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "MTFL [11]: Learns the input kernel and the output kernel matrix as a linear combination of base kernel matrices.", "startOffset": 5, "endOffset": 9}, {"referenceID": 9, "context": "GMTL [10]: A clustered multi-task feature learning approach.", "startOffset": 5, "endOffset": 9}, {"referenceID": 1, "context": "Tasks within a cluster are assumed to share a low dimensional feature subspace [2].", "startOffset": 79, "endOffset": 82}, {"referenceID": 8, "context": "MTRL [9]: A multi-task relationship learning approach.", "startOffset": 5, "endOffset": 8}, {"referenceID": 10, "context": "We follow the experimental protocol1 described in [11].", "startOffset": 50, "endOffset": 54}, {"referenceID": 9, "context": "Also, note that GMTL [10] and MTFL [11] enjoy the advantage of both input and output kernel learning.", "startOffset": 21, "endOffset": 25}, {"referenceID": 10, "context": "Also, note that GMTL [10] and MTFL [11] enjoy the advantage of both input and output kernel learning.", "startOffset": 35, "endOffset": 39}, {"referenceID": 10, "context": "The performance of STL, MTL, CMTL and MTFL are reported from [11].", "startOffset": 61, "endOffset": 65}, {"referenceID": 0, "context": "(p = 2) (p = 4/3) (p = 8/7) Figure 1: Plots of |\u0398| matrices (rescaled to [0,1] and averaged over ten splits) computed by our solver FMTLp for the Landmine data set for different p-norms, with cross-validated hyper-parameter values.", "startOffset": 73, "endOffset": 78}, {"referenceID": 16, "context": "In this section we experimented with two loss functions: a) FMTLp-H \u2013 the hinge loss employed in SVMs, and b) FMTLp-S \u2013 the squared loss employed in OKL [17].", "startOffset": 153, "endOffset": 157}, {"referenceID": 30, "context": "In these experiments, we also compare our results with MTL-SDCA, a state-of-the-art multi-task feature learning method [32].", "startOffset": 119, "endOffset": 123}, {"referenceID": 9, "context": "Handwritten Digit Recognition: We consider the following two data sets and follow the experimental protocol detailed in [10].", "startOffset": 120, "endOffset": 124}, {"referenceID": 31, "context": "USPS: A handwritten digit data sets with 10 classes [33].", "startOffset": 52, "endOffset": 56}, {"referenceID": 32, "context": "MNIST: Another handwritten digit data set with 10 classes [34].", "startOffset": 58, "endOffset": 62}, {"referenceID": 30, "context": "Our approach FMTLp-H obtains better accuracy than GMTL, MTRL and MTL-SDCA [32] on both data sets.", "startOffset": 74, "endOffset": 78}, {"referenceID": 33, "context": "MIT Indoor67 Experiments: We also report results on the MIT Indoor67 benchmark [35] which covers 67 indoor scene categories with over 100 images per class.", "startOffset": 79, "endOffset": 83}, {"referenceID": 34, "context": "Note that these are better than the ones reported in [36] (70.", "startOffset": 53, "endOffset": 57}, {"referenceID": 33, "context": "1%) and [35] (68.", "startOffset": 8, "endOffset": 12}, {"referenceID": 35, "context": "SUN397 Experiments: SUN397 [37] is a challenging scene classification benchmark [35] with 397 scene classes and more than 100 images per class.", "startOffset": 27, "endOffset": 31}, {"referenceID": 33, "context": "SUN397 Experiments: SUN397 [37] is a challenging scene classification benchmark [35] with 397 scene classes and more than 100 images per class.", "startOffset": 80, "endOffset": 84}, {"referenceID": 33, "context": "We employed the CNN features extracted with the convolutional neural network (CNN) provided by [35] using Places 205 database.", "startOffset": 95, "endOffset": 99}, {"referenceID": 0, "context": "(p = 2) (p = 4/3) (p = 8/7) Figure 2: Plots of matrices log(1 + |\u0398|) (rescaled to [0,1] and diagonal entries removed since they reflect high similarity of a task with itself, which is obvious) computed by our solver FMTLp-S for the SUN397 data set for different p-norms, with cross-validated hyper-parameter values.", "startOffset": 82, "endOffset": 87}, {"referenceID": 16, "context": "Our approach FMTL2-S is 7 times faster that OKL [17] and 4.", "startOffset": 48, "endOffset": 52}, {"referenceID": 17, "context": "3 times faster than ConvexOKL [18] when the number of tasks is maximum.", "startOffset": 30, "endOffset": 34}], "year": 2015, "abstractText": "The paradigm of multi-task learning is that one can achieve better generalization by learning tasks jointly and thus exploiting the similarity between the tasks rather than learning them independently of each other. While previously the relationship between tasks had to be user-defined in the form of an output kernel, recent approaches jointly learn the tasks and the output kernel. As the output kernel is a positive semidefinite matrix, the resulting optimization problems are not scalable in the number of tasks as an eigendecomposition is required in each step. Using the theory of positive semidefinite kernels we show in this paper that for a certain class of regularizers on the output kernel, the constraint of being positive semidefinite can be dropped as it is automatically satisfied for the relaxed problem. This leads to an unconstrained dual problem which can be solved efficiently. Experiments on several multi-task and multi-class data sets illustrate the efficacy of our approach in terms of computational efficiency as well as generalization performance.", "creator": "LaTeX with hyperref package"}}}