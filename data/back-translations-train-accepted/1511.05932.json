{"id": "1511.05932", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2015", "title": "On the Global Linear Convergence of Frank-Wolfe Optimization Variants", "abstract": "The Frank-Wolfe (FW) optimization algorithm has lately re-gained popularity thanks in particular to its ability to nicely handle the structured constraints appearing in machine learning applications. However, its convergence rate is known to be slow (sublinear) when the solution lies at the boundary. A simple less-known fix is to add the possibility to take 'away steps' during optimization, an operation that importantly does not require a feasibility oracle. In this paper, we highlight and clarify several variants of the Frank-Wolfe optimization algorithm that have been successfully applied in practice: away-steps FW, pairwise FW, fully-corrective FW and Wolfe's minimum norm point algorithm, and prove for the first time that they all enjoy global linear convergence, under a weaker condition than strong convexity of the objective. The constant in the convergence rate has an elegant interpretation as the product of the (classical) condition number of the function with a novel geometric quantity that plays the role of a 'condition number' of the constraint set. We provide pointers to where these algorithms have made a difference in practice, in particular with the flow polytope, the marginal polytope and the base polytope for submodular optimization.", "histories": [["v1", "Wed, 18 Nov 2015 20:24:43 GMT  (1097kb,D)", "http://arxiv.org/abs/1511.05932v1", "Appears in: Advances in Neural Information Processing Systems 28 (NIPS 2015). 26 pages"]], "COMMENTS": "Appears in: Advances in Neural Information Processing Systems 28 (NIPS 2015). 26 pages", "reviews": [], "SUBJECTS": "math.OC cs.LG stat.ML", "authors": ["simon lacoste-julien", "martin jaggi"], "accepted": true, "id": "1511.05932"}, "pdf": {"name": "1511.05932.pdf", "metadata": {"source": "CRF", "title": "On the Global Linear Convergence of Frank-Wolfe Optimization Variants", "authors": ["Simon Lacoste-Julien", "Martin Jaggi"], "emails": [], "sections": [{"heading": null, "text": "This year, it is only a matter of time before agreement is reached."}, {"heading": "1 Improved Variants of the Frank-Wolfe Algorithm", "text": "(0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (, (1), (1), (1), (1), (1), (1), (1), (1), (1),"}, {"heading": "2 Global Linear Convergence Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Intuition for the Convergence Proofs", "text": "We first give the general intuition for the linear convergence of the different FW variants. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < &lt"}, {"heading": "2.2 Convergence Results", "text": "We now specify the global linear convergence rates for the four variants of the FW algorithm: awaysteps FW (AFW Alg. 1); pairwise FW (PFW Alg. 2); fully corrective FW (FCFW Alg. 3 with approximate correction Alg. 4); and Wolfe's min-norm point algorithm (Alg. 3 with MNP correction as Alg. 5 in Appendix A.1). For the AFW, MNP, and PFW algorithms, we call a drop step when the active sentence shrinks | S (t + 1) | < S (t) | S (t).) For the PFW algorithm, we also have the option of a swap step where there are no swap steps, but | S (t + 1) | S (t)."}, {"heading": "3 Pyramidal Width", "text": "We start with the following definition: A > A = > A = > A (A = A = A = A = A = A = A = A = A) (A = A = A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A (A) (A) (A) (A) (A) (A (A) (A) (A) (A) (A) (A (A) (A) (A (A) (A) (A) (A) (A (A) (A) (A (A) (A) (A) (A) (A) (A (A) (A) (A) (A (A) (A) (A) (A"}, {"heading": "3.1 Properties of Pyramidal Width and Consequences", "text": "The pyramid width of a setA is lower limited by the minimum width across all subsets of atoms, and is therefore strictly greater than zero if the number of atoms is limited. On the other hand, this lower limit is often too loose to be useful, since in particular the pyramid width of the unit cube can be exponentially small in the dimension d width O (d \u2212 d 2). On the other hand, as we show here, the pyramid width of the unit cube is actually 1 / \u221a d, which justifies why we have maintained the tighter but more involved definition (9). See application example B.1 for proof. The pyramid width of the unit cube in Rd is 1 / \u221a."}, {"heading": "4 Non-Strongly Convex Generalization", "text": "In this case, linear convergence is still confronted with the constant number of constants occurring in the ratio of theorem 1. We note that the general matrix A, f, s is, however, not necessarily strongly convex. In this case, linear convergence is still confronted with the constant number of constants occurring in the ratio of theorem 1 in the ratio of theorem 1, which is replaced by the generalized constant in the ratio of theorem 1. Generalized constants in the ratio of lemma 5 Illustrative experimentsWe illustrate the performance of the presented algorithms in two numerical experiments shown in Figure 2. The first example is a limited lasso problem (\"1 regulated constant in the ratio\".5 Illustrative experimentsWe illustrate the performance of the presented algorithms in two numerical experiments shown in Figure 2."}, {"heading": "A More on Frank-Wolfe Algorithm Variants", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Wolfe\u2019s Min-Norm Point (MNP) algorithm", "text": "The generalization of Wolfe's min-norm point (MNP) algorithm [35] for general convex functions is to run algorithm 3 with the correction subroutine in step 7 implemented as below in Algorithm 5. Wolfe's paper [35] refers to the correction step as a small cycle, whereas the FW outer loop is referred to as a large cycle (A), as we mentioned in Section 1 \u2212 the correction as a sequence of affine projections potentially leading to another update. Indeed, standard FCFW optimizes f via conv (t), while MNP does the correction as a sequence of affine projections potentially leading to another update. Algorithm 5 Generalized version of Wolfe's MNP's MNP's MNP's we are generalized."}, {"heading": "A.2 Applications to Submodular Minimization", "text": "An interesting consequence of our global linear convergence result for FW algorithm variants is the potential to reduce the gap between the known theoretical rates and the impressive empirical performance of MNP for submodular function minimization (via the base polytopic). While Bach [3] already showed convergence of FW in this case, Chakrabarty et al. [39] later indicated a weaker convergence rate for Wolfe's MNP variant. For exact submodular function optimization, the total complexity of [39] O (d5F 2) (with some corrections6), where F is the maximum absolute value of the integer-rated submodular function, as opposed to O (d5 log (dF)) for the fastest algorithms [40]."}, {"heading": "A.3 Pairwise Frank-Wolfe", "text": "Our new analysis of the Frank-Wolfe pairing variant, as introduced in Section 1, is motivated by the work of Garber and Hazan [10], who provided the first Frank-Wolfe variant with a global linear convergence rate with explicit constants that do not depend on the location of the optimal x system, for a more complex extension of such a pairing algorithm. An important contribution of the work of Garber and Hazan [10] was to define the concept of the local linear oracle, which (approximately) replaces a linear function at the interface of M and a small sphere by x (t) (hence the name of the local oracle). They showed that if such a local linear oracle was available, one could replace the step toward the most in the standard FW procedure with a constant step size toward the point returned by the local linear oracle in order to obtain a global linear algorithm."}, {"heading": "A.4 Other Related Work", "text": "Recently, after the earlier workshop version of our article [20], Pena et al. [28] presented an alternative geometric quantity for measuring the linear convergence velocity of the AFW algorithm variant. Their approach is motivated by a specific case of the Frank Wolfe method, the von Neumann algorithm. Their complexity constant - the so-called constrained width - is also limited from zero, but their value depends on the location of the optimal solution, which is a disadvantage compared to the earlier existing results of [34, 12, 5] and the working line of [1, 19, 26] which depends on Robinson's condition [30]. More specifically, the limit is the value of the constant given in [28, theorem 4] for the translated atoms A in relation to the optimal point. The constant is not affin-invariant, while the constants \u00b5Af (22) and C A f (26) in our setting are such that the respective value in the discussion C would still be comparable to the polymer."}, {"heading": "B Pyramidal Width", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1 Pyramidal Width of the Cube and Probability Simplex", "text": "This means that we are considering all possible active settings. < p > p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p"}, {"heading": "B.2 Proof of Theorem 3 on the Pyramidal Width", "text": "D (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). ("}, {"heading": "C Affine Invariant Formulation", "text": "The \"old\" and \"new\" optimization problems minx (x) and minx (x) for f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (f) f (x) f (f) f (f) f (x) f (e) f (x) f (x) f (e) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x) f (x (x) f (x) f (x) f (x) f (x) f (x) f (x) f (f (x) f (x) f (x) f (x) f (x) f (e) f (x) f (e) f (x) f (e) f (e) f (e) f (x (x) f (e) f (f (x) f (x (x) f"}, {"heading": "C.1 Lower Bound for the Geometric Strong Convexity Constant \u00b5Af", "text": "The geometric constant \u00b5Af > 0 is slightly weaker than the strong constant function 11 because it depends only on the internal products of the feasible points with the gradient. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <"}, {"heading": "D Linear Convergence Proofs", "text": "Due to the additional possibility of path step in algorithm 1, we must define the following slightly modified additional curvature constant required for linear convergence analysis of the algorithm: CAf: = sup x, s, v, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p,"}, {"heading": "E Empirical Tightness of Linear Rate Constant", "text": "We describe here a simple experiment to test how narrow the constant is in the linear convergence rate of theorem 8. We test both AFW and PFW on the triangular domain with corners at the locations (\u2212 1, 0), (0, 0) and (cos), sin (\u03b8), for ever smaller \u03b8 (see Figure 4). Therefore, the pyramidal width \u03b4 = sin (\u03b82) is as a random quantity vanishingly small as the size of the domain \u2192 0; the diameter is M = 2 cos (\u03b82). We consider the optimization of the function f (x): = 1 2 x \u00b2 the pyramid width 3 with x \u00b2 the size of the domain. (\u2212 0.5, 1) The conditional number of f is L\u00b5 = 1. The bound linear convergence rate of f (according to theorem 8) (using CAf \u2264 LM2 (20) and m3) is."}, {"heading": "F Non-Strongly Convex Generalization", "text": "Here we will examine the generalized setting with target f (x): = g (Ax) + < b, x >, where g (g) is strongly convex (Ax). \u2212 b, where the Euclidean norm over domain AM is constant with strong convectivity. < b, where the constant Hoffman (see [4, Lemma 2.2]) is associated with the matrix. [A] b > B, where the rows of B are the diameter of M and MA the diameter of AM.Let us be the Hoffman constant (see [4, Lemma 2.2]) associated with the matrix. [B >, where the line of B defines the linear inequality that defines the setM. Here we present a generalization of Lemma 2.5 from [4]: Lemma 9. For each x x, M and x we have set in the solution."}, {"heading": "Supplementary References", "text": "[37] H. Allende, E. Frandi, R. Nanculef, and C. Sartori. A novel Frank-Wolfe algorithm. Analysis and applications to large-scale SVM training. arXiv: 1304.1014v2, 2013. [38] D. P. Bertsekas. Nonlinear Programming. Athena Scientific, second edition, 1999. [39] D. Chakrabarty, P. Jain, and P. Kothari. Detectable submodular minimization using the Wolfe algorithm. In NIPS. 2014. [40] S. Iwata. A faster scaling algorithm for minimizing submodular functions. In Integer Program-ming and Combinatorial Optimization, Volume 2337 of Lecture Notes in Computer Science, pp. 1-8. 2002. [41] G. M. Ziegler. Lectures on Polytopes, Volume 152 of Graduate Texts in Mathematics. Springer, 1995.16Here again we have the choice between inequality and inequality from 2.0 to 4 \u2212 16."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": null, "creator": "LaTeX with hyperref package"}}}