{"id": "1511.06018", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2015", "title": "Segmental Recurrent Neural Networks", "abstract": "We introduce segmental recurrent neural networks (SRNNs) which define, given an input sequence, a joint probability distribution over segmentations of the input and labelings of the segments. Representations of the input segments (i.e., contiguous subsequences of the input) are computed by encoding their constituent tokens using bidirectional recurrent neural nets, and these \"segment embeddings\" are used to define compatibility scores with output labels. These local compatibility scores are integrated using a global semi-Markov conditional random field. Both fully supervised training -- in which segment boundaries and labels are observed -- as well as partially supervised training -- in which segment boundaries are latent -- are straightforward. Experiments on handwriting recognition and joint Chinese word segmentation/POS tagging show that, compared to models that do not explicitly represent segments such as BIO tagging schemes and connectionist temporal classification (CTC), SRNNs obtain substantially higher accuracies.", "histories": [["v1", "Wed, 18 Nov 2015 23:02:45 GMT  (398kb,D)", "https://arxiv.org/abs/1511.06018v1", "9 pages"], ["v2", "Tue, 1 Mar 2016 22:46:37 GMT  (405kb,D)", "http://arxiv.org/abs/1511.06018v2", "10 pages, published as a conference paper at ICLR 2016"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["lingpeng kong", "chris dyer", "noah a smith"], "accepted": true, "id": "1511.06018"}, "pdf": {"name": "1511.06018.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["cdyer}@cs.cmu.edu", "nasmith@cs.washington.edu"], "sections": [{"heading": null, "text": "We introduce Segmental Recurrent Neural Networks (SRNNs), which define a common probability distribution based on an input sequence through segmentation of input and segment labels. Presentations of input segments (i.e. contiguous subsequences of input) are calculated by encoding their constituent tokens using bi-directional recursive neural networks, and these \"segment embeddings\" are used to define compatibility values with output labels. These local compatibility values are integrated using a global semi-Markov-induced random field. Both fully monitored training - where segment boundaries and labels are observed - and partially monitored training - where segment boundaries are latent. Experiments on handwriting recognition and common Chinese word segmentation / POS tagging show that when compared to models that do not explicitly represent segments such as segment boundaries, such as BIX classification and tagification are significantly higher."}, {"heading": "1 INTRODUCTION", "text": "For sequential data such as language, handwriting and DNA, segmentation and segment marking are abstractions that capture many common challenges in data analysis. We consider the common task of breaking an input sequence into contiguous segments of arbitrary length as each segment is labeled. Our new approach to this problem is the segmental recursive neural network (SRNN). SRNNs combine two powerful machine learning tools: representation learning and structured prediction. First, bi-directional recursive neural networks (RNNNs) embed each feasible segment of input into a continuous space, and these embeddings are then used to calculate the compatibility of each candidate segment with a label. In contrast to earlier RNN-based approaches (e.g., connectional temporal classification or CTC; Graves et al., 2006), the individual candidate segments are presented explicitly between segments, allowing for application in an explicit manner."}, {"heading": "2 MODEL", "text": "Considering a sequence of input observations x = < x2, x2, x2, max. (1), we are interested in the following prediction problem: \"We are interested in the following prediction problem: We are interested in the following prediction problem: We are interested in the following prediction problem: y = arg max y p (y), y = arg max y p (y), y = arg max y p (y)."}, {"heading": "3 INFERENCE WITH DYNAMIC PROGRAMMING", "text": "We are interested in three inference problems: (i) determining the most likely segmentation / label for a model with a sequence x; (ii) evaluating the partition function Z (x); and (iii) calculating the posterior marginal Z (x, y), which includes all segmentations compatible with a reference sequence y, all of which can be solved by dynamic programming. For simplicity, we assume Markov dependencies between the Yis in zero order. Extensions to the lowest order of the Markov dependencies should be simple. Since each of these algorithms is based on the embedding of forward and backward segments, we will first discuss how they can be calculated before moving to the inference algorithms."}, {"heading": "3.1 COMPUTING SEGMENT EMBEDDINGS", "text": "Allow the \u2212 \u2192 h i, j the \u2212 \u2212 \u2212 \u2212 \u2212 RNN encoding through the input range (i, j), which moves from left to right, and allow \u00b7 \u2212 h i, j the reverse direction encoding using \u2190 \u2212 \u2212 \u2212 RNN. So there are O (| x | 2) vectors to be calculated, each of the lengths O (| x |). Naively, this can be calculated in the time O (| x | 3), but the following dynamic program reduces it to O (| 2): \u2212 h i, i = \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 RNN (\u2212 h 0, ci) \u2212 h, j = \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 RNN, j \u2212 1, cj)."}, {"heading": "4 PARAMETER LEARNING", "text": "We are looking at two different learning objectives."}, {"heading": "4.1 SUPERVISED LEARNING", "text": "In the monitored case, both the segment duration (z) and its label (y) are observed. L = \u2211 (x, y, z) D \u2212 log p (y, z | x) = \u2211 (x, y, z) and DlogZ (x) \u2212 logZ (x, y, z) In this expression, the non-normalized conditional probability of reference segmentation / label is written as Z (x, y, z) when entering x."}, {"heading": "4.2 PARTIALLY SUPERVISED LEARNING", "text": "In the partially monitored case, only the labels are observed and the segments (the z) are not observed and marginalized. L = \u2211 (x, y) \u2022 D \u2212 log p (y | x) = \u2211 (x, y) \u2022 D \u0445 Z (x, y) \u2212 log p (y, z | x) = \u2211 (x, y) \u2022 DlogZ (x, y) \u2212 logZ (x, y) Both for the fully and partially monitored scenarios, the necessary derivatives can be calculated by means of automatic differentiation or (equivalent) with retrograde variants of the aforementioned dynamic programs (Sarawagi & Cohen, 2004)."}, {"heading": "5 EXPERIMENTS", "text": "We present two experimental series to compare segmental relapsing neural networks with models that do not contain explicit segmentation representations: For the task of handwriting recognition, we consider connectionist time classification (CTC) (Graves et al., 2006); for Chinese word segmentation, we consider organic marking. In these experiments, we do not include Markovian dependencies between adjacent labels for our models or baselines."}, {"heading": "5.1 ONLINE HANDWRITING RECOGNITION", "text": "This dataset is an online collection of handwritten words from 150 authors. It records how the coordinates (x, y) are recorded in time, and uses a special way of processing data that is divided into the orbit, development and test categories according to Kassel (1995). Table 1 presents the statistics for the datasets. A well-known variant of this dataset was introduced by Taskar et al. (2004) The dataset al is a \"clean\" subset of about 6,100 words and rasterized and normalized images of each letter. Then the empirical letters (since they are usually the first character in a word) are removed and only the lowercase letters are used."}, {"heading": "5.2 JOINT CHINESE WORD SEGMENTATION AND POS TAGGING", "text": "The fact is that we are able to assert ourselves, that we are able to change the world, and that we are able to change the world in order to change it. \""}, {"heading": "6 RELATED WORK", "text": "In fact, it is such that most of the people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "7 CONCLUSION", "text": "We have proposed a new model for segment labeling problems that learns how segments of an input sequence are represented and then labels them. We are surpassing existing alternatives both when segment information is to be recovered and when it is only latent. We have not trained segment representations to go beyond good labeling (or segmentation) decisions, but an interesting way for future work would be to construct representations that are useful for other tasks."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors thank the anonymous reviewers Yanchuan Sim and Hao Tang for their helpful feedback. This work was partially funded by the Defense Advanced Research Projects Agency (DARPA) Information Innovation Office (I2O) under the Low Resource Languages for Emergent Incidents (LORELEI) program, issued by DARPA / I2O under contract number HR0011-15-C-0114."}], "references": [{"title": "Structural-Functional Analysis of Plant Cyclic Nucleotide Gated Ion Channels", "author": ["Abdel-Hamid", "Huda"], "venue": "PhD thesis, University of Toronto,", "citeRegEx": "Abdel.Hamid and Huda.,? \\Q2013\\E", "shortCiteRegEx": "Abdel.Hamid and Huda.", "year": 2013}, {"title": "End-to-end attention-based large vocabulary speech recognition", "author": ["Bahdanau", "Dzmitry", "Chorowski", "Jan", "Serdyuk", "Dmitriy", "Brakel", "Phil\u00e9mon", "Bengio", "Yoshua"], "venue": "CoRR, abs/1508.04395,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Listen, attend, and spell", "author": ["Chan", "William", "Jaitly", "Navdeep", "Le", "Quoc V", "Vinyals", "Oriol"], "venue": "CoRR, abs/1508.01211,", "citeRegEx": "Chan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "Neural CRF parsing", "author": ["Durrett", "Greg", "Klein", "Dan"], "venue": "In Proc. ACL,", "citeRegEx": "Durrett et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Durrett et al\\.", "year": 2015}, {"title": "Chinese gigaword", "author": ["Graff", "David", "Chen", "Ke"], "venue": "LDC Catalog No.: LDC2003T09,", "citeRegEx": "Graff et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graff et al\\.", "year": 2005}, {"title": "Sequence transduction with recurrent neural networks", "author": ["Graves", "Alex"], "venue": "In Proc. ICML,", "citeRegEx": "Graves and Alex.,? \\Q2012\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2012}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Graves", "Alex", "Jaitly", "Navdeep"], "venue": "In Proc. ICML,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["Graves", "Alex", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Networks,", "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks", "author": ["Graves", "Alex", "Fern\u00e1ndez", "Santiago", "Gomez", "Faustino", "Schmidhuber", "J\u00fcrgen"], "venue": "In Proc. ICML,", "citeRegEx": "Graves et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2006}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["Hannun", "Awni Y", "Case", "Carl", "Casper", "Jared", "Catanzaro", "Bryan C", "Diamos", "Greg", "Elsen", "Erich", "Prenger", "Ryan", "Satheesh", "Sanjeev", "Sengupta", "Shubho", "Coates", "Adam", "Ng", "Andrew Y"], "venue": "CoRR, abs/1412.5567,", "citeRegEx": "Hannun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannun et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Karpathy", "Andrej", "Fei-Fei", "Li"], "venue": "In Proc. CVPR,", "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "A comparison of approaches to on-line handwritten character recognition", "author": ["Kassel", "Robert H"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "Kassel and H.,? \\Q1995\\E", "shortCiteRegEx": "Kassel and H.", "year": 1995}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Two/too simple adaptations of word2vec for syntax problems", "author": ["Ling", "Wang", "Dyer", "Chris", "Black", "Alan W", "Trancoso", "Isabel"], "venue": "In Proc. NAACL,", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Lexicon-free conversational speech recognition with neural networks", "author": ["Maas", "Andrew L", "Xie", "Ziang", "Jurafsky", "Dan", "Ng", "Andrew Y"], "venue": "In Proc. NAACL,", "citeRegEx": "Maas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2015}, {"title": "Conditional neural fields", "author": ["Peng", "Jian", "Bo", "Liefeng", "Xu", "Jinbo"], "venue": "In Proc. NIPS,", "citeRegEx": "Peng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2009}, {"title": "A tutorion on hidden Markov models and selected applications in speech recognition", "author": ["Rabiner", "Lawrence R"], "venue": "Proc. IEEE,", "citeRegEx": "Rabiner and R.,? \\Q1989\\E", "shortCiteRegEx": "Rabiner and R.", "year": 1989}, {"title": "Text chunking using transformation-based learning", "author": ["Ramshaw", "Lance A", "Marcus", "Mitchell P"], "venue": "In Proceedings of the Workshop on Very Large Corpora,", "citeRegEx": "Ramshaw et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Ramshaw et al\\.", "year": 1995}, {"title": "Semi-Markov conditional random fields for information extraction", "author": ["Sarawagi", "Sunita", "Cohen", "William W"], "venue": "In Proc. NIPS,", "citeRegEx": "Sarawagi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Sarawagi et al\\.", "year": 2004}, {"title": "The Penn Chinese TreeBank: Phrase structure annotation of a large corpus", "author": ["Xue", "Naiwen", "Xia", "Fei", "Chiou", "Fu-Dong", "Palmer", "Martha"], "venue": "Natural Language Engineering,", "citeRegEx": "Xue et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 8, "context": "Unlike past RNN-based approaches (e.g., connectionist temporal classification or CTC; Graves et al., 2006) each candidate segment is represented explicitly, allowing application in settings where an alignment between segments and labels is desired as part of the output (e.", "startOffset": 33, "endOffset": 106}, {"referenceID": 8, "context": "This permits tokens to be sensitive to the contexts they occur in, and this is standardly used with neural net sequence labeling models (Graves et al., 2006).", "startOffset": 136, "endOffset": 157}, {"referenceID": 8, "context": "An alternative strategy for avoiding overflow in similar dynamic programs is to rescale the forward summations at each time step (Rabiner, 1989; Graves et al., 2006).", "startOffset": 129, "endOffset": 165}, {"referenceID": 8, "context": "For the handwriting recognition task, we consider connectionist temporal classification (CTC) (Graves et al., 2006); for Chinese word segmentation, we consider BIO tagging.", "startOffset": 94, "endOffset": 115}, {"referenceID": 8, "context": "4 For the decoding of the CTC model, we simply use the best path decoding, where we assume that the most probable path will correspond to the most probable labeling, although it is known that prefix search decoding can slightly improve the results (Graves et al., 2006).", "startOffset": 248, "endOffset": 269}, {"referenceID": 20, "context": "For the joint Chinese word segmentation and POS tagging task, we use the Penn Chinese Treebank 5 (Xue et al., 2005), following the standard train/dev/test splits.", "startOffset": 97, "endOffset": 115}, {"referenceID": 14, "context": "For both tasks, we use Wang2Vec (Ling et al., 2015) to generate the pre-trained character embeddings from the Chinese Gigaword (Graff & Chen, 2005).", "startOffset": 32, "endOffset": 51}, {"referenceID": 2, "context": "Several alternatives to CTC have been approached, such as using various attention mechanisms in place of marginalization (Chan et al., 2015; Bahdanau et al., 2015).", "startOffset": 121, "endOffset": 163}, {"referenceID": 1, "context": "Several alternatives to CTC have been approached, such as using various attention mechanisms in place of marginalization (Chan et al., 2015; Bahdanau et al., 2015).", "startOffset": 121, "endOffset": 163}, {"referenceID": 16, "context": "Finally, using neural networks to provide local features in conditional random field models has also been proposed for sequential models (Peng et al., 2009) and tree-structured models (Durrett & Klein, 2015).", "startOffset": 137, "endOffset": 156}, {"referenceID": 4, "context": "A widely used approach to a segmental labeling problems with neural networks is the connectionist temporal classification (CTC) objective and decoding rule of Graves et al. (2006). CTC reduces the \u201csegmental\u201d sequence label problem to a classical sequence labeling problem in which every position in an input sequence x is explicitly labeled by interpreting repetitions of input labels\u2014or input labels followed by a special \u201cblank\u201d output symbol\u2014as being a single label with a longer duration.", "startOffset": 159, "endOffset": 180}, {"referenceID": 1, "context": ", 2015; Bahdanau et al., 2015). These have been applied to endto-end discriminative speech recognition problem. A more direct alternative to our method\u2014indeed it was proposed to solve several of the same problems we identified\u2014is due to Graves (2012). However, a crucial difference is that our model explicitly constructs representations of segments which are used to label the segment while that model relies on a marginalized frame-level labeling with a null symbol.", "startOffset": 8, "endOffset": 251}, {"referenceID": 1, "context": ", 2015; Bahdanau et al., 2015). These have been applied to endto-end discriminative speech recognition problem. A more direct alternative to our method\u2014indeed it was proposed to solve several of the same problems we identified\u2014is due to Graves (2012). However, a crucial difference is that our model explicitly constructs representations of segments which are used to label the segment while that model relies on a marginalized frame-level labeling with a null symbol. The work of Abdel-Hamid (2013) also seeks to construct embeddings of multi-frame segments.", "startOffset": 8, "endOffset": 500}], "year": 2016, "abstractText": "We introduce segmental recurrent neural networks (SRNNs) which define, given an input sequence, a joint probability distribution over segmentations of the input and labelings of the segments. Representations of the input segments (i.e., contiguous subsequences of the input) are computed by encoding their constituent tokens using bidirectional recurrent neural nets, and these \u201csegment embeddings\u201d are used to define compatibility scores with output labels. These local compatibility scores are integrated using a global semi-Markov conditional random field. Both fully supervised training\u2014in which segment boundaries and labels are observed\u2014as well as partially supervised training\u2014in which segment boundaries are latent\u2014are straightforward. Experiments on handwriting recognition and joint Chinese word segmentation/POS tagging show that, compared to models that do not explicitly represent segments such as BIO tagging schemes and connectionist temporal classification (CTC), SRNNs obtain substantially higher accuracies.", "creator": "LaTeX with hyperref package"}}}