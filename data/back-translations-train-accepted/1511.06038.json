{"id": "1511.06038", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Neural Variational Inference for Text Processing", "abstract": "Recent advances in neural variational inference have spawned a renaissance in deep latent variable models. In this paper we introduce a generic variational inference framework for generative and conditional models of text. While traditional variational methods derive an analytic approximation for the intractable distributions over latent variables, here we construct an inference network conditioned on the discrete text input to provide the variational distribution. We validate this framework on two very different text modelling applications, generative document modelling and supervised question answering. Our neural variational document model combines a continuous stochastic document representation with a bag-of-words generative model and achieves the lowest reported perplexities on two standard test corpora. The neural answer selection model employs a stochastic representation layer within an attention mechanism to extract the semantics between a question and answer pair. On two question answering benchmarks this model exceeds all previous published benchmarks.", "histories": [["v1", "Thu, 19 Nov 2015 01:23:28 GMT  (424kb,D)", "http://arxiv.org/abs/1511.06038v1", "ICLR 2016 submission"], ["v2", "Mon, 30 Nov 2015 14:35:48 GMT  (632kb,D)", "http://arxiv.org/abs/1511.06038v2", "ICLR 2016 submission"], ["v3", "Thu, 7 Jan 2016 19:49:17 GMT  (801kb,D)", "http://arxiv.org/abs/1511.06038v3", "ICLR 2016 submission"], ["v4", "Sat, 4 Jun 2016 06:41:58 GMT  (913kb,D)", "http://arxiv.org/abs/1511.06038v4", "ICML 2016"]], "COMMENTS": "ICLR 2016 submission", "reviews": [], "SUBJECTS": "cs.CL cs.LG stat.ML", "authors": ["yishu miao", "lei yu", "phil blunsom"], "accepted": true, "id": "1511.06038"}, "pdf": {"name": "1511.06038.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["TEXT PROCESSING", "Yishu Miao", "Lei Yu", "Phil Blunsom"], "emails": ["yishu.miao@cs.ox.ac.uk", "lei.yu@cs.ox.ac.uk", "phil.blunsom@cs.ox.ac.uk"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, it is the case that most of them are able to abide by the rules that they have imposed on themselves, and that they are able to abide by the rules that they have imposed on themselves. (...) In fact, it is the case that they are able to abide by the rules. (...) In fact, it is the case that they are able to abide by the rules. (...) In fact, it is the case that they are able to abide by the rules. (...) It is as if they are able to break the rules. \"(...)"}, {"heading": "2 NEURAL VARIATIONAL INFERENCE FRAMEWORK", "text": "In this section we introduce a generic neural variation model, which we apply to both the unattended parent nodes of h and x. We form the variational subordinate L in order to optimize the common distribution: Logbook distribution (z) = Logbook distribution (z) = Logbook distribution. (z) Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution:: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution: Logbook distribution:: Logbook distribution:: Logbook distribution::: Logbook distribution:: Logbook distribution:: Logbook distribution: distribution: Logbook distribution: distribution: Logbook distribution: Logbook distribution:: distribution: Logbook distribution: distribution: Logbook distribution: distribution: Logbook distribution: Logbook distribution:"}, {"heading": "3 NEURAL VARIATIONAL DOCUMENT MODEL", "text": "= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "4 NEURAL ANSWER SELECTION MODEL", "text": "Answer-sentence-selection is an answer-paradigm in which a model must identify the correct sentences. Answer-sentence-selection is an answer-paradigm in which a model must identify the correct sentences. Answer-sentence-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-question-question-question-question-question-question-question-question-question-question-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-answer-question-question-question-answer-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question-question"}, {"heading": "5 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 DATASET & SETUP FOR DOCUMENT MODELLING", "text": "We experiment with NVDM on two standard news corporations: the 20NewsGroups2 and the Reuters RCV1-v23 datasets; the 20NewsGroups dataset is a collection of newsgroup documents consisting of 11,314 training and 7,531 test articles; the RCV1-v2 dataset is a large collection of Reuters Newswire stories with 794,414 training sessions and 10,000 test cases; we apply the standard pre-processing procedure as Hinton & Salakhutdinov (2009); Mnih & Gregor (2014) and set the vocabulary size of the 20NewsGroups and RCV1-v2 datasets to 2,000 and 10,000 respects.To make a direct comparison with the previous work, we follow the same setup as Hinton & Salakhutdinov (2009), Larochelle & Lauly (2012), Srivastava et al. (2013) and Mnih & Gregor (2014)."}, {"heading": "5.2 EXPERIMENTAL RESULTS ON DOCUMENT MODELLING", "text": "Dre rf\u00fc ide r\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die for the R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die for the R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc for the R\u00fc die R\u00fc die R"}, {"heading": "5.3 DATASET & SETUP FOR ANSWER SENTENCE SELECTION", "text": "We are experimenting with two response selection datasets, the QASent and the WikiQA datasets. QASent (Wang et al., 2007) was created from the TREC QA track, and the WikiQA (Yang et al., 2015) comes from Wikipedia. To investigate the effectiveness of our NASM model, we also implemented two strong baseline models - a vanilla LSTM model (LSTM) and an LSTM model with a deterministic attention mechanism (LSTM + Att). To investigate the effectiveness of our NASM model, we also implemented two strong baseline models - a vanilla LSTM model (LSTM) and an LSTM model with a deterministic attention mechanism (LSTM + Att). For the former, the last state outputs from the question and answer LSTM models are directly applied to the QM adaptation function (for the last state output, the Eq16)."}, {"heading": "5.4 EXPERIMENTAL RESULTS ON ANSWER SENTENCE SELECTION", "text": "In fact, most of them will be able to follow the rules they have imposed on themselves. (...) Most of them are able to understand the rules. (...) Most of them are not able to understand the rules. (...) Most of them are not able to understand the rules. (...) Most of them are not able to comply with the rules. (...) Most of them are not able to comply with the rules. (...) Most of them are not able to comply with the rules. (...) Most of them are not able to comply with the rules. (...) Most of them are not able to comply with the rules. (...) Most of them are not able to comply with the rules. (...) Most of them are not able to comply with the rules. (...)"}, {"heading": "6 RELATED WORK", "text": "In recent years, it has become clear that most people who see themselves as capable are not only people, but also people who are able to integrate and integrate, not only people who are able to integrate, but also people who are able to integrate, but people who are able to integrate and integrate."}, {"heading": "7 CONCLUSION", "text": "To demonstrate the effectiveness of this framework, we experimented with two different tasks: document modeling and question-and-answer selection tasks, both of which are state-of-the-art. In addition to the promising results, our model also has the advantages of (1) being simple, expressive and efficient in training with the SGVB algorithm; (2) suitable for both unattended and supervised learning tasks; (3) being able to generalize any type of neural network."}, {"heading": "A T-SNE VISUALISATION OF DOCUMENT REPRESENTATIONS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B DETAILS OF THE DEEP NEURAL NETWORK STRUCTURES", "text": "B.1 NEW VARIATIONAL DOCUMENT MODEL (= q = q = q = q = q (1) Inference Network (h | X): \u03bb = ReLU (W 1X + b1) (21) \u03c0 (ReLU = ReLU (W 2\u03bb + b2)) (22) q (q q q = q q q (23) log\u03c3 (W 4\u03c0 + b4 (24) h \u00b2 N (X), diag (X | h)) (25) (2) Generative Model Pledge (X | h): ei = exp (\u2212 hTRxi + bxi) (26) pxi + bxi (xi | h) = ei \u00b2 N (V | j ej (27) pledge (X | h \u2212 h) = i pledge (xi | h) (28) (3) KL (3) Divergence (DKL [qL) [qL] (h), xi (h): DKL (h) xi (h): DKh (2) (h) (h) (h) (h)"}], "references": [{"title": "An introduction to mcmc for machine learning", "author": ["Andrieu", "Christophe", "De Freitas", "Nando", "Doucet", "Arnaud", "Jordan", "Michael I"], "venue": "Machine learning,", "citeRegEx": "Andrieu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Andrieu et al\\.", "year": 2003}, {"title": "A variational bayesian framework for graphical models", "author": ["Attias", "Hagai"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Attias and Hagai.,? \\Q2000\\E", "shortCiteRegEx": "Attias and Hagai.", "year": 2000}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Variational algorithms for approximate Bayesian inference", "author": ["Beal", "Matthew James"], "venue": "University of London,", "citeRegEx": "Beal and James.,? \\Q2003\\E", "shortCiteRegEx": "Beal and James.", "year": 2003}, {"title": "Question answering with subgraph embeddings", "author": ["Bordes", "Antoine", "Chopra", "Sumit", "Weston", "Jason"], "venue": "Proceedings of EMNLP,", "citeRegEx": "Bordes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Open question answering with weakly supervised embedding models", "author": ["Bordes", "Antoine", "Weston", "Jason", "Usunier", "Nicolas"], "venue": "Proceedings of ECML,", "citeRegEx": "Bordes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Varieties of helmholtz machine", "author": ["Dayan", "Peter", "Hinton", "Geoffrey E"], "venue": "Neural Networks,", "citeRegEx": "Dayan et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Dayan et al\\.", "year": 1996}, {"title": "Made: Masked autoencoder for distribution estimation", "author": ["Germain", "Mathieu", "Gregor", "Karol", "Murray", "Iain", "Larochelle", "Hugo"], "venue": "arXiv preprint arXiv:1502.03509,", "citeRegEx": "Germain et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Germain et al\\.", "year": 2015}, {"title": "Deep autoregressive networks", "author": ["Gregor", "Karol", "Mnih", "Andriy", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1310.8499,", "citeRegEx": "Gregor et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2013}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["Hermann", "Karl Moritz", "Kocisk\u00fd", "Tom\u00e1s", "Grefenstette", "Edward", "Espeholt", "Lasse", "Kay", "Will", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": null, "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Replicated softmax: an undirected topic model", "author": ["Hinton", "Geoffrey E", "Salakhutdinov", "Ruslan"], "venue": "Proceedings of NIPS,", "citeRegEx": "Hinton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2009}, {"title": "Autoencoders, minimum description length, and helmholtz free energy", "author": ["Hinton", "Geoffrey E", "Zemel", "Richard S"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Hinton et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1994}, {"title": "wake-sleep\u201d algorithm for unsupervised neural networks", "author": ["Hinton", "Geoffrey E", "Dayan", "Peter", "Frey", "Brendan J", "Neal", "Radford M. The"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1995}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Probabilistic latent semantic indexing", "author": ["Hofmann", "Thomas"], "venue": "In Proceedings ACM SIGIR,", "citeRegEx": "Hofmann and Thomas.,? \\Q1999\\E", "shortCiteRegEx": "Hofmann and Thomas.", "year": 1999}, {"title": "An introduction to variational methods for graphical models", "author": ["Jordan", "Michael I", "Ghahramani", "Zoubin", "Jaakkola", "Tommi S", "Saul", "Lawrence K"], "venue": "Machine learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik P", "Ba", "Jimmy"], "venue": "In Proceedings of ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Semisupervised learning with deep generative models", "author": ["Kingma", "Diederik P", "Mohamed", "Shakir", "Rezende", "Danilo Jimenez", "Welling", "Max"], "venue": "Proceedings of NIPS,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Kumar", "Ankit", "Irsoy", "Ozan", "Su", "Jonathan", "Bradbury", "James", "English", "Robert", "Pierce", "Brian", "Ondruska", "Peter", "Gulrajani", "Ishaan", "Socher", "Richard"], "venue": null, "citeRegEx": "Kumar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2015}, {"title": "A neural autoregressive topic model", "author": ["Larochelle", "Hugo", "Lauly", "Stanislas"], "venue": "Proceedings of NIPS,", "citeRegEx": "Larochelle et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2012}, {"title": "The neural autoregressive distribution estimator", "author": ["Larochelle", "Hugo", "Murray", "Iain"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Larochelle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2011}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Quoc V", "Mikolov", "Tomas"], "venue": "Proceedings of ICML 2014,", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Learning word vectors for sentiment analysis", "author": ["Maas", "Andrew L", "Daly", "Raymond E", "Pham", "Peter T", "Huang", "Dan", "Ng", "Andrew Y", "Potts", "Christopher"], "venue": "Proceedings of ACL,", "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Gregory S", "Dean", "Jeffrey"], "venue": "Proceedings of NIPS,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Neural variational inference and learning in belief networks", "author": ["Mnih", "Andriy", "Gregor", "Karol"], "venue": "arXiv preprint arXiv:1402.0030,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Probabilistic inference using markov chain monte carlo methods", "author": ["Neal", "Radford M"], "venue": null, "citeRegEx": "Neal and M.,? \\Q1993\\E", "shortCiteRegEx": "Neal and M.", "year": 1993}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo J", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "Proceedings of ICML,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Disi-university of trento modelling input texts: from tree kernels to deep learning", "author": ["Severyn", "Aliaksei"], "venue": null, "citeRegEx": "Severyn and Aliaksei.,? \\Q2015\\E", "shortCiteRegEx": "Severyn and Aliaksei.", "year": 2015}, {"title": "Modeling Documents with Deep Boltzmann Machines", "author": ["Srivastava", "Nitish", "RR Salakhutdinov", "Hinton", "Geoffrey"], "venue": "Proceedings of UAI,", "citeRegEx": "Srivastava et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2013}, {"title": "End-to-end memory networks", "author": ["Sukhbaatar", "Sainbayar", "Szlam", "Arthur", "Weston", "Jason", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1503.08895,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Learning stochastic feedforward neural networks", "author": ["Tang", "Yichuan", "Salakhutdinov", "Ruslan R"], "venue": "Proceedings NIPS,", "citeRegEx": "Tang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2013}, {"title": "Hierarchical dirichlet processes", "author": ["Teh", "Yee Whye", "Jordan", "Michael I", "Beal", "Matthew J", "Blei", "David M"], "venue": "Journal of the American Statistical Asociation,", "citeRegEx": "Teh et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2006}, {"title": "A deep and tractable density estimator", "author": ["Uria", "Benigno", "Murray", "Iain", "Larochelle", "Hugo"], "venue": null, "citeRegEx": "Uria et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Uria et al\\.", "year": 2014}, {"title": "Visualizing data using t-sne", "author": ["Van der Maaten", "Laurens", "Hinton", "Geoffrey"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "What is the jeopardy model? a quasisynchronous grammar for qa", "author": ["Wang", "Mengqiu", "Smith", "Noah A", "Mitamura", "Teruko"], "venue": "Proceedings of EMNLP-CoNLL,", "citeRegEx": "Wang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2007}, {"title": "Faq-based question answering via word alignment", "author": ["Wang", "Zhiguo", "Ittycheriah", "Abraham"], "venue": "arXiv preprint arXiv:1507.02628,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Wikiqa: A challenge dataset for open-domain question answering", "author": ["Yang", "Yi", "Yih", "Wen-tau", "Meek", "Christopher"], "venue": "Proceedings of EMNLP,", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Question answering using enhanced lexical semantic models", "author": ["Yih", "Wen-tau", "Chang", "Ming-Wei", "Meek", "Christopher", "Pastusiak", "Andrzej"], "venue": "Proceedings of ACL,", "citeRegEx": "Yih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2013}, {"title": "Semantic parsing for single-relation question answering", "author": ["Yih", "Wen-tau", "He", "Xiaodong", "Meek", "Christopher"], "venue": "Proceedings of ACL,", "citeRegEx": "Yih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2014}, {"title": "Deep Learning for Answer Sentence Selection", "author": ["Yu", "Lei", "Hermann", "Karl Moritz", "Blunsom", "Phil", "Pulman", "Stephen"], "venue": "In NIPS Deep Learning Workshop,", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Markov chain Monte Carlo (MCMC) (Neal, 1993; Andrieu et al., 2003) and variational inference (Jordan et al.", "startOffset": 32, "endOffset": 66}, {"referenceID": 16, "context": ", 2003) and variational inference (Jordan et al., 1999; Attias, 2000; Beal, 2003) are the standard approaches for approximating these integrals.", "startOffset": 34, "endOffset": 81}, {"referenceID": 28, "context": "This paper introduces a neural variational framework for generative models of text, inspired by the variational auto-encoder (Rezende et al., 2014; Kingma & Welling, 2013).", "startOffset": 125, "endOffset": 171}, {"referenceID": 28, "context": "By using the reparameterisation method (Rezende et al., 2014; Kingma & Welling, 2013), the inference network is trained through back-propagating unbiased and low variance gradients w.", "startOffset": 39, "endOffset": 85}, {"referenceID": 30, "context": "A primary feature of NVDM is that each word is generated directly from a dense continuous document representation instead of the more common binary semantic vector (Hinton & Salakhutdinov, 2009; Larochelle & Lauly, 2012; Srivastava et al., 2013; Mnih & Gregor, 2014).", "startOffset": 164, "endOffset": 266}, {"referenceID": 28, "context": "These models are simple, expressive and can be trained efficiently with the highly scalable stochastic gradient variational Bayes (SGVB) algorithm (Rezende et al., 2014; Kingma & Welling, 2013).", "startOffset": 147, "endOffset": 193}, {"referenceID": 28, "context": "\u03c6 we reparameterise h = \u03bc + \u03c3 \u00b7 and sample (l) \u223c N (0, I) (Rezende et al., 2014; Kingma & Welling, 2013).", "startOffset": 58, "endOffset": 104}, {"referenceID": 30, "context": "In order to make a direct comparison with the prior work we follow the same setup as Hinton & Salakhutdinov (2009), Larochelle & Lauly (2012), Srivastava et al. (2013), and Mnih & Gregor (2014).", "startOffset": 143, "endOffset": 168}, {"referenceID": 30, "context": "In order to make a direct comparison with the prior work we follow the same setup as Hinton & Salakhutdinov (2009), Larochelle & Lauly (2012), Srivastava et al. (2013), and Mnih & Gregor (2014). We train NVDM models with 50 and 200 dimensional document representations respectively.", "startOffset": 143, "endOffset": 194}, {"referenceID": 36, "context": "QASent (Wang et al., 2007) was created from the TREC QA track, and the WikiQA (Yang et al.", "startOffset": 7, "endOffset": 26}, {"referenceID": 38, "context": ", 2007) was created from the TREC QA track, and the WikiQA (Yang et al., 2015) is constructed from Wikipedia.", "startOffset": 59, "endOffset": 78}, {"referenceID": 25, "context": "The word embeddings (K = 50) are obtained by running the word2vec tool (Mikolov et al., 2013) on the English Wikipedia dump and the AQUAINT5 corpus.", "startOffset": 71, "endOffset": 93}, {"referenceID": 35, "context": "QASent (Wang et al., 2007) was created from the TREC QA track, and the WikiQA (Yang et al., 2015) is constructed from Wikipedia. Due to the different approaches for constructing the datasets4, the WikiQA dataset is less noisy and less biased towards lexical overlap. Table 3 summarises the statistics of the two datasets. In order to investigate the effectiveness of our NASM model we also implemented two strong baseline models \u2014 a vanilla LSTM model (LSTM) and an LSTM model with a deterministic attention mechanism (LSTM+Att). For the former, the last state outputs from the question and answer LSTM models are passed directly to the QA matching function (Eq.16). For the latter, we add an attention model that uses the last state output of the question LSTM as the question semantics to extract the context vector (Eq.14). LSTM+Att is the deterministic counterpart of NASM. Following previous work, for each of our models we also add a lexical overlap feature by combining a co-occurrence word count feature with the probability generated from the neural model. We adopt MAP and MRR as the evaluation metrics for this task. To facilitate direct comparison with previous work we follow the same experimental setup as Yu et al. (2014) and Severyn (2015).", "startOffset": 8, "endOffset": 1237}, {"referenceID": 35, "context": "QASent (Wang et al., 2007) was created from the TREC QA track, and the WikiQA (Yang et al., 2015) is constructed from Wikipedia. Due to the different approaches for constructing the datasets4, the WikiQA dataset is less noisy and less biased towards lexical overlap. Table 3 summarises the statistics of the two datasets. In order to investigate the effectiveness of our NASM model we also implemented two strong baseline models \u2014 a vanilla LSTM model (LSTM) and an LSTM model with a deterministic attention mechanism (LSTM+Att). For the former, the last state outputs from the question and answer LSTM models are passed directly to the QA matching function (Eq.16). For the latter, we add an attention model that uses the last state output of the question LSTM as the question semantics to extract the context vector (Eq.14). LSTM+Att is the deterministic counterpart of NASM. Following previous work, for each of our models we also add a lexical overlap feature by combining a co-occurrence word count feature with the probability generated from the neural model. We adopt MAP and MRR as the evaluation metrics for this task. To facilitate direct comparison with previous work we follow the same experimental setup as Yu et al. (2014) and Severyn (2015). The word embeddings (K = 50) are obtained by running the word2vec tool (Mikolov et al.", "startOffset": 8, "endOffset": 1256}, {"referenceID": 39, "context": "edu/LDC2002T31 As stated in (Yih et al., 2013) that the evaluation scripts used by previous work are noisy \u2014 4 out of 72 questions in the test set are treated answered incorrectly.", "startOffset": 28, "endOffset": 46}, {"referenceID": 38, "context": "The LSTM+Att performs Yang et al. (2015) provide detailed explanation of the differences between the two datasets.", "startOffset": 22, "endOffset": 41}, {"referenceID": 38, "context": "The LSTM+Att performs Yang et al. (2015) provide detailed explanation of the differences between the two datasets. https://catalog.ldc.upenn.edu/LDC2002T31 As stated in (Yih et al., 2013) that the evaluation scripts used by previous work are noisy \u2014 4 out of 72 questions in the test set are treated answered incorrectly. This makes the MAP and MRR scores \u223c 4% lower than the true scores. Severyn (2015) and Wang & Ittycheriah (2015), however, use a cleaned-up version of", "startOffset": 22, "endOffset": 404}, {"referenceID": 38, "context": "The LSTM+Att performs Yang et al. (2015) provide detailed explanation of the differences between the two datasets. https://catalog.ldc.upenn.edu/LDC2002T31 As stated in (Yih et al., 2013) that the evaluation scripts used by previous work are noisy \u2014 4 out of 72 questions in the test set are treated answered incorrectly. This makes the MAP and MRR scores \u223c 4% lower than the true scores. Severyn (2015) and Wang & Ittycheriah (2015), however, use a cleaned-up version of", "startOffset": 22, "endOffset": 434}, {"referenceID": 41, "context": "Bigram-CNN is the simple convolutional model reported in (Yu et al., 2014).", "startOffset": 57, "endOffset": 74}, {"referenceID": 13, "context": "Training an inference network to approximate the variational distribution was first proposed in the context of Helmholtz machines (Hinton & Zemel, 1994; Hinton et al., 1995; Dayan & Hinton, 1996), but applications of these directed generative models come up against the problem of establishing low variance gradient estimators.", "startOffset": 130, "endOffset": 195}, {"referenceID": 28, "context": "Recent advances in neural variational inference (Rezende et al., 2014; Kingma & Welling, 2013; Mnih & Gregor, 2014) mitigate this problem by reparameterising the continuous random variables via differentiable transformation or using control variates.", "startOffset": 48, "endOffset": 115}, {"referenceID": 9, "context": "Instantiations of this idea, the Deep Recurrent Attentive Writer (DRAW) (Gregor et al., 2015) and the semi-supervised generative learning framework (Kingma et al.", "startOffset": 72, "endOffset": 93}, {"referenceID": 19, "context": ", 2015) and the semi-supervised generative learning framework (Kingma et al., 2014), have demonstrated strong performance on image generation and classification respectively.", "startOffset": 62, "endOffset": 83}, {"referenceID": 34, "context": "Another class of neural generative models make use of the autoregressive assumption to model highdimensional input distributions (Larochelle & Murray, 2011; Uria et al., 2014; Germain et al., 2015).", "startOffset": 129, "endOffset": 197}, {"referenceID": 7, "context": "Another class of neural generative models make use of the autoregressive assumption to model highdimensional input distributions (Larochelle & Murray, 2011; Uria et al., 2014; Germain et al., 2015).", "startOffset": 129, "endOffset": 197}, {"referenceID": 8, "context": "Deep AutoRegressive Networks (DARN) (Gregor et al., 2013) integrate this idea with variational inference.", "startOffset": 36, "endOffset": 57}, {"referenceID": 33, "context": "Applications of these models on document modelling achieve significant improvements on generating documents, compared to conventional probabilistic topic models (Hofmann, 1999; Blei et al., 2003; Teh et al., 2006) and also the RBMs (Hinton & Salakhutdinov, 2009; Srivastava et al.", "startOffset": 161, "endOffset": 213}, {"referenceID": 30, "context": ", 2006) and also the RBMs (Hinton & Salakhutdinov, 2009; Srivastava et al., 2013).", "startOffset": 26, "endOffset": 81}, {"referenceID": 24, "context": "The semantic word vector model (Maas et al., 2011) also employs a continuous semantic vector to generate words, but the model is trained by MAP inference which does not permit the calculation of the posterior distribution.", "startOffset": 31, "endOffset": 50}, {"referenceID": 40, "context": "Relevant work includes mapping factoid questions with answer triples in the knowledge base by projecting them into a shared vector space using convolutional neural networks (Bordes et al., 2014b;a; Yih et al., 2014).", "startOffset": 173, "endOffset": 215}, {"referenceID": 2, "context": "Recently, the attentionbased learning models (Bahdanau et al., 2014) are applied to QA, where long-term memories act as dynamic knowledge bases (Weston et al.", "startOffset": 45, "endOffset": 68}, {"referenceID": 31, "context": ", 2014) are applied to QA, where long-term memories act as dynamic knowledge bases (Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2015) or the attentive network helps read and comprehend (Hermann et al.", "startOffset": 83, "endOffset": 149}, {"referenceID": 20, "context": ", 2014) are applied to QA, where long-term memories act as dynamic knowledge bases (Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2015) or the attentive network helps read and comprehend (Hermann et al.", "startOffset": 83, "endOffset": 149}, {"referenceID": 10, "context": ", 2015) or the attentive network helps read and comprehend (Hermann et al., 2015).", "startOffset": 59, "endOffset": 81}], "year": 2017, "abstractText": "Recent advances in neural variational inference have spawned a renaissance in deep latent variable models. In this paper we introduce a generic variational inference framework for generative and conditional models of text. While traditional variational methods derive an analytic approximation for the intractable distributions over latent variables, here we construct an inference network conditioned on the discrete text input to provide the variational distribution. We validate this framework on two very different text modelling applications, generative document modelling and supervised question answering. Our neural variational document model combines a continuous stochastic document representation with a bag-of-words generative model and achieves the lowest reported perplexities on two standard test corpora. The neural answer selection model employs a stochastic representation layer within an attention mechanism to extract the semantics between a question and answer pair. On two question answering benchmarks this model exceeds all previous published benchmarks.", "creator": "LaTeX with hyperref package"}}}