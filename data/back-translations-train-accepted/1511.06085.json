{"id": "1511.06085", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Variable Rate Image Compression with Recurrent Neural Networks", "abstract": "Although image compression has been actively studied for decades, there has been relatively little research on learning to compress images with modern neural networks. Standard approaches, such as those employing patch-based autoencoders, have shown a great deal of promise but cannot compete with popular image codecs because they fail to address three questions: 1) how to effectively binarize activations: in the absence of binarization, a bottleneck layer alone tends not to lead to efficient compression; 2) how to achieve variable-rate encoding: a standard autoencoder generates a fixed-length code for each fixed-resolution input patch, resulting in the same cost for low- and high-entropy patches, and requiring the network to be completely retrained to achieve different compression rates; and 3) how to avoid block artifacts: patch-based approaches are prone to block discontinuities. We propose a general framework for variable-rate image compression and a novel architecture based on convolutional and deconvolutional recurrent networks, including LSTMs, that address these issues and report promising results compared to existing baseline codecs. We evaluate the proposed methods on a large-scale benchmark consisting of tiny images (32$\\times$32), which proves to be very challenging for all the methods.", "histories": [["v1", "Thu, 19 Nov 2015 07:50:46 GMT  (121kb,D)", "http://arxiv.org/abs/1511.06085v1", "Under review as a conference paper at ICLR 2016"], ["v2", "Sat, 21 Nov 2015 01:44:51 GMT  (121kb,D)", "http://arxiv.org/abs/1511.06085v2", "Under review as a conference paper at ICLR 2016"], ["v3", "Tue, 12 Jan 2016 02:43:40 GMT  (290kb,D)", "http://arxiv.org/abs/1511.06085v3", "Under review as a conference paper at ICLR 2016"], ["v4", "Wed, 13 Jan 2016 20:57:42 GMT  (233kb,D)", "http://arxiv.org/abs/1511.06085v4", "Under review as a conference paper at ICLR 2016"], ["v5", "Tue, 1 Mar 2016 22:13:44 GMT  (5796kb,D)", "http://arxiv.org/abs/1511.06085v5", "Under review as a conference paper at ICLR 2016"]], "COMMENTS": "Under review as a conference paper at ICLR 2016", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["george toderici", "sean m o'malley", "sung jin hwang", "damien vincent", "david minnen", "shumeet baluja", "michele covell", "rahul sukthankar"], "accepted": true, "id": "1511.06085"}, "pdf": {"name": "1511.06085.pdf", "metadata": {"source": "CRF", "title": "RECURRENT NEURAL NETWORKS", "authors": ["George Toderici", "Sean M. O\u2019Malley", "Sung Jin Hwang", "Damien Vincent", "David Minnen", "Shumeet Baluja", "Michele Covell", "Rahul Sukthankar"], "emails": ["damienv}@google.com", "sukthankar}@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "The task of image compression has been thoroughly explored over the years by researchers and teams such as the Joint Pictures Experts Group, which designed the ubiquitous JPEG and JPEG 2000 (ISO / IEC 15444-1) image formats. Recently, the WebP algorithm was proposed to further improve image compression rates (Google, 2015), all of which approach the compression problem from an empirical standpoint: human experts have designed various heuristics to reduce the amount of information to be obtained, and then the resulting data is transformed in a way that leads to losses."}, {"heading": "2 RELATED WORK", "text": "The basic principles of using feed-forward neural networks for image compression have been known for some time (Jiang, 1999).In this context, networks can support or even fully adopt many of the processes used in a traditional image compression pipeline: more efficient frequency transformations, more effective quantization techniques, improved predictive encoding, etc. More recently, autoencoder architectures (Hinton & Salakhutdinov, 2006) have become practicable as a means of implementing end-to-end compression.A typical autoencoder has three parts: (1) an encoder that consumes an input (e.g. a fixed-dimensional image or a patch) and transforms it into a bottleneck that represents the compressed data, which can then be turned into something resembling the original input by (3) a decoder."}, {"heading": "3 VARIABLE RATE COMPRESSION ARCHITECTURES", "text": "Each subsection describes a different architecture that builds on the previous model and improves compression outcomes. For each architecture, we will discuss a function E, which takes an image patch as input and produces a coded representation, which is then processed by a binarization function B, which is the same across architectures, and is discussed in Section 3.2. Finally, for each architecture, we will also consider a decoder function D, which takes the binary representation produced by B and creates a reconstructed output patch. Together, these three components form an autoencoder, x \u2032 = D (B (E (x)))), which is the basic building block for all compression networks.In all experiments, the input images or patches are scaled to be in the range [\u2212 0.9, 0.9]."}, {"heading": "3.1 IMAGE COMPRESSION FRAMEWORK", "text": "Both architectures share the same design principles and must define an encoder network, a quantizer, and a decoder network. Furthermore, the framework should be attuned to image compression and support variable compression rates without the need to retrain or store multiple encodings of the same image. To make it possible to transfer incremental information, the design should take into account the fact that image decoding will be progressive. With this design goal in mind, we can consider architectures that are built on remnants, with the goal of minimizing residual error in reconstruction as additional information is available to the decoder. Formally, we concatenate multiple copies of a residual autoencoder, Ft, the resistant model defined as: Ft (rt \u2212 1) = Dt (Et \u2212 1))) (1)."}, {"heading": "3.2 BINARY REPRESENTATION", "text": "In our networks, we use a binarization technology first proposed by Williams (1992), which is similar to that used by Krizhevsky & Hinton (2011) and Courbariaux et al. (2015) This binarization has three advantages: (1) bit vectors are trivially serializable / deserializable for wire image transmission, (2) network compression control is achieved simply by placing restrictions on the bitulage, and (3) a binary bottleneck helps the network learn efficient representations compared to standard floating layers, which can have many redundant bit patterns that do not affect output. The binarization process consists of two parts \u2212 The first part consists of generating the required number of outputs (equal to the desired number of output bits) at continuous intervals [\u2212 1, 1] x."}, {"heading": "3.3 FEED-FORWARD FULLY-CONNECTED RESIDUAL ENCODER", "text": "In the simplest instantiation of our variable rate compression architecture, we specify that E and D should consist of stacked, fully connected layers. To make searching for architectures easier, we decided to set the number of outputs in each fully connected layer constant (512), using only tanh nonlinearity. Since E and D can be functions of the encoding stage number, and the statistics of residuals change when we go from level t to level t + 1, we considered two different approaches: in the first, we split the weights across all levels, while in the second, we learn the weights independently at each level. The details of this architecture are shown in Figure 1."}, {"heading": "3.4 LSTM-BASED COMPRESSION", "text": "In this architecture, we examine the use of LSTM models for both the encoder and the decoder. In particular, both E and D consist of stacked LSTM layers. Following the LSTM formulation and notation proposed by Zaremba et al. (2014), we use superscripts to specify the number of layers, and subscripts to specify time steps. Let hlt-Rn detect the hidden state of the L-th LSTM layer at the time of step. We define T ln: Rm \u2192 Rn to be an affine transformationT ln (x) = W lx + bl. Finally, let us detect elemental multiplication and h0t be the input layer to the first LSTM layer at the time. With this notation, the LSTM architecture can be concisely written, as suggested by Graves (2013): ifo g = sigmsigmsigmsighl chl = ctanh (1T \u2212 lt lt lt \u2212 llt)."}, {"heading": "3.5 FEED-FORWARD CONVOLUTIONAL/DECONVOLUTIONAL RESIDUAL ENCODER", "text": "In Section 3.3, a fully connected residual autoencoder has been proposed. We extend this architecture by replacing the fully connected layers with folding operators in encoder E and deconvolutionary operators in decoder D. The last layer of the decoder consists of a 1 \u00d7 1 folding with three filters that convert the decoded representation to RGB values. We represent this architecture in Figure 3 (less the RGB conversion). The deconvolutionary operator is defined as the transposition of the Constitutional Operator. Let us call the Convolutionary Operator a step factor k and let Sk the step operator a step factor k, i.e. Sk (x) (i, j) = x (k \u00d7 i, k \u00d7 j) for 2D multi-channel image x and pixel coordinate (i, j), followed by W k x = Sk (W 1 x)."}, {"heading": "3.6 CONVOLUTIONAL/DECONVOLUTIONAL LSTM COMPRESSION", "text": "The final architecture combines the revolutionary and derevolutionary operators with LSTM. We define the revolutionary LSTM by replacing the transformation T l4n in Equation (8) with convolutions plus bias. Then, the transformation function for revolutionary LSTM with stride length k isT l4n (hl \u2212 1t, h l t \u2212 1) = W l1 k hl \u2212 1t + W l2 1 hlt \u2212 1 + bl. (13) The sub-script belonging to T now refers to the depth (number of channels) in the output function maps. Note that the second convolution term represents the recurring relation of Convolutionary LSTM, so both its input and its output must have the same size. Thus, if a revolutionary LSTM has a stride greater than one, the step size is applied only to the first convolution term, while the second term represents the revolutionary LSTM, so we can always replace the convolutionary LM with this third strict \u2212 STM."}, {"heading": "3.7 DYNAMIC BIT ASSIGNMENT", "text": "In the non-revolutionary approaches presented here, it is natural to assign a different number of bits per patch by allowing a different number of iterations of the encoder, which could be determined by a target quality metric (e.g. PSNR). In the case of the revolutionary approaches, it is not so natural, but a similar method can be used. It is necessary to divide the input image into patches and edit each patch independently, allowing for a different number of bits per region. However, this approach has drawbacks, which will be discussed at the end of this article."}, {"heading": "4 EXPERIMENTS & ANALYSIS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 TRAINING", "text": "To train the various neural network configurations, we used the Adam algorithm proposed by Kingma & Ba (2014). We experimented with learning rates of {0.1, 0.3, 0.5, 0.8, 1}. L2 loss was normalized by the number of pixels in the patch and also by the number of total time steps, i.e. the number of unrolled iterations needed to fully encode the patch. We experimented with the number of steps required to encode each patch, varying them from 8 to 16. For the fully connected networks, we decided to use 8 bits per step for an 8 x 8 patch, which allowed us to refine the compression rate in steps of 8 x 8 bits."}, {"heading": "4.2 EVALUATION PROTOCOL AND METRICS", "text": "Evaluating image compression algorithms is a non-trivial task. The most commonly used metric in this context is the Peak Signal-to-Noise Ratio (PSNR), but PSNR tends to use algorithms designed to minimize L2 loss. This would not be a fair comparison with methods like JPEG that are designed to minimize some form of perception loss. Instead, in our evaluation protocol we use the Structural Similarity Index (SSIM), a popular perception similarity measurement proposed by Wang et al. (2004). Since we evaluate the compression performance of smaller 32 x 32 images, we do not flatten the images (a typical approach for SSIM). Furthermore, since we are interested in quantifying how well local details are preserved, we divide the images into 8 x 8 image fields and compile the SSIM on each color channel independently."}, {"heading": "4.3 32\u00d732 BENCHMARK", "text": "To be included in the dataset, each image must originally have more than 32 pixels on both axes. Qualified images were then sampled down to 32 \u00d7 32, losing their original aspect ratio. This downsampling eliminates pre-existing compression artifacts for most images. The final 32 \u00d7 32 images were then saved lossless (as PNG) before being used for training and testing. 90% of the images were used to train the LSTM models; the remaining 10% were set aside for evaluation. To evaluate the image codecs, we use a subset of this data containing 100 k of random images. Table 1 summarizes the results on the 32 \u00d7 32 benchmark and compares our two LSTM approaches to two JPEG codecs. To avoid unfair prosecution of the JPEG codecs, we are making the header of the file somewhat out of the budget that we cannot use to precisely specify the file size."}, {"heading": "4.4 ANALYSIS", "text": "As can be seen in Figure 4, compressing these images without destroying distinctive visual information or hallucinating false details is particularly challenging. At these low bit rates and spatial resolutions, the JPEG block artifacts become extremely prominent, and the color-smeared artifacts are clearly visible due to the standard (4: 2: 0) chrome subsampling. The non-conventional LSTM model reduces these artifacts by reconstructing each subblock with greater accuracy, but its block boundary artifacts are still recognizable. The Convolutionary model completely eliminates these artifacts, preferring errors on the smoothness side over the addition of incorrect gradients that do not exist in the original image. Note that both LSTM models have perceptible quality levels that are equal to or better than JPEG at 4% - 12% lower average bit rate. We see this improvement despite the fact that JPEG is higher than JPEG (4 - 1)."}, {"heading": "5 CONCLUSION & FUTURE WORK", "text": "We describe various methods for encoding variable-length image fields using neural networks and show that the fully networked LSTM model is comparable to JPEG for the given benchmark, while the Convolutionary / Decvolutionary LSTM model can significantly outperform JPEG in the perceptual metric SSIM. While our current approach on small images yields favorable results compared to modern codecs, codecs that contain an entropy coding element tend to improve (in terms of bits per pixel) at higher resolution, which means that by selecting an arbitrary-sized test image it is always possible to thwart an approach such as the one described in this paper. Therefore, there is an obvious need to expand the current work to arbitrarily large images and exploit spatial redundancy in images in a manner similar to entropy coding."}], "references": [{"title": "BinaryConnect: Training deep neural networks with binary weights during propagations", "author": ["M. Courbariaux", "Y. Bengio", "David", "J.-P"], "venue": "In NIPS,", "citeRegEx": "Courbariaux et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2015}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves,? \\Q2013\\E", "shortCiteRegEx": "Graves", "year": 2013}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. Mohamed", "G. Hinton"], "venue": "In International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "Hinton and Salakhutdinov,? \\Q2006\\E", "shortCiteRegEx": "Hinton and Salakhutdinov", "year": 2006}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Image compression with neural networks\u2013a survey", "author": ["J. Jiang"], "venue": "Signal Processing: Image Communication,", "citeRegEx": "Jiang,? \\Q1999\\E", "shortCiteRegEx": "Jiang", "year": 1999}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma and Ba,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba", "year": 2014}, {"title": "Using very deep autoencoders for content-based image retrieval", "author": ["A. Krizhevsky", "G.E. Hinton"], "venue": "In European Symposium on Artificial Neural Networks,", "citeRegEx": "Krizhevsky and Hinton,? \\Q2011\\E", "shortCiteRegEx": "Krizhevsky and Hinton", "year": 2011}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CoRR, abs/1411.4038,", "citeRegEx": "Long et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Long et al\\.", "year": 2014}, {"title": "On betweencoefficient contrast masking of DCT basis functions", "author": ["N. Ponomarenko", "F. Silvestri", "K. Egiazarian", "M. Carli", "J. Astola", "V. Lukin"], "venue": "In Proc. 3rd Int\u2019l. Workshop on Video Processing and Quality Metrics,", "citeRegEx": "Ponomarenko et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ponomarenko et al\\.", "year": 2007}, {"title": "Techniques for learning binary stochastic feedforward neural networks", "author": ["T. Raiko", "M. Berglund", "G. Alain", "L. Dinh"], "venue": null, "citeRegEx": "Raiko et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Raiko et al\\.", "year": 2015}, {"title": "Convolutional LSTM network: A machine learning approach for precipitation nowcasting", "author": ["X. Shi", "Z. Chen", "H. Wang", "Yeung", "D.-Y", "Wong", "W.-K", "Woo", "W.-C"], "venue": "CoRR, abs/1506.04214,", "citeRegEx": "Shi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "CoRR, abs/1409.3215,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Image quality assessment: from error visibility to structural similarity", "author": ["Z. Wang", "A. Bovik", "A. Conrad", "H.R. Sheikh", "E.P. Simoncelli"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Wang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2004}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": "The basic principles of using feed-forward neural networks for image compression have been known for some time (Jiang, 1999).", "startOffset": 111, "endOffset": 124}, {"referenceID": 8, "context": "When this upsampling process is spatially-aware, resembling a \u201cbackward convolution\u201d, it is commonly referred to as deconvolution (Long et al., 2014).", "startOffset": 130, "endOffset": 149}, {"referenceID": 2, "context": "Long short-term memory (LSTM) networks are a type of recurrent neural network (Hochreiter & Schmidhuber, 1997) that have proven very successful for tasks such as speech recognition (Graves et al., 2013) and machine translation (Sutskever et al.", "startOffset": 181, "endOffset": 202}, {"referenceID": 12, "context": ", 2013) and machine translation (Sutskever et al., 2014).", "startOffset": 32, "endOffset": 56}, {"referenceID": 11, "context": "Many extensions to the standard LSTM model are possible including explicitly incorporating spatial information, which leads to various types of convolutional LSTMs (Shi et al., 2015) that may be better suited for image compression.", "startOffset": 164, "endOffset": 182}, {"referenceID": 0, "context": "In our networks, we employ a binarization technique first proposed by Williams (1992), and similar to Krizhevsky & Hinton (2011) and Courbariaux et al. (2015). This binarization has three benefits: (1) bit vectors are trivially serializable/deserializable for image transmission over the wire, (2) control of the network compression rate is achieved simply by putting constraints on the bit allowance, and (3) a binary bottleneck helps force the network to learn efficient representations compared to standard floating-point layers, which may have many redundant bit patterns that have no effect on the output.", "startOffset": 133, "endOffset": 159}, {"referenceID": 0, "context": "In our networks, we employ a binarization technique first proposed by Williams (1992), and similar to Krizhevsky & Hinton (2011) and Courbariaux et al. (2015). This binarization has three benefits: (1) bit vectors are trivially serializable/deserializable for image transmission over the wire, (2) control of the network compression rate is achieved simply by putting constraints on the bit allowance, and (3) a binary bottleneck helps force the network to learn efficient representations compared to standard floating-point layers, which may have many redundant bit patterns that have no effect on the output. The binarization process consists of two parts. The first part consists of generating the required number of outputs (equal to the desired number of output bits) in the continuous interval [\u22121, 1]. The second part involves taking this real-valued representation as input and producing a discrete output in the set {\u22121, 1} for each value. For the first step in the binarization process, we use a fully-connected layer with tanh activations. For the second part, following Raiko et al. (2015), one possible binarization b(x) of x \u2208 [\u22121, 1] is defined as: b(x) = x+ \u2208 {\u22121, 1}, (4)", "startOffset": 133, "endOffset": 1102}, {"referenceID": 10, "context": "For the backward pass of back-propagation, we take the derivative of the expectation (Raiko et al., 2015).", "startOffset": 85, "endOffset": 105}, {"referenceID": 15, "context": "Following the LSTM formulation and notation proposed by Zaremba et al. (2014), we use superscripts to indicate the layer number, and subscripts to indicate time steps.", "startOffset": 56, "endOffset": 78}, {"referenceID": 1, "context": "Using this notation, the LSTM architecture can be written succinctly as proposed by Graves (2013): \uf8ec\uf8ed i fo g \uf8f7\uf8f8 = \uf8ec\uf8edsigm sigm sigm tanh \uf8f7\uf8f8T l 4n(hl\u22121 t ht\u22121 ) , (8)", "startOffset": 84, "endOffset": 98}, {"referenceID": 13, "context": "In our evaluation protocol we instead employ the Structural Similarity Index (SSIM), a popular perceptual similarity measure proposed by Wang et al. (2004). Since we\u2019re evaluating compression performance on small 32\u00d732 images, we don\u2019t do any smoothing of the images (a typical preprocess", "startOffset": 137, "endOffset": 156}], "year": 2015, "abstractText": "Although image compression has been actively studied for decades, there has been relatively little research on learning to compress images with modern neural networks. Standard approaches, such as those employing patch-based autoencoders, have shown a great deal of promise but cannot compete with popular image codecs because they fail to address three questions: 1) how to effectively binarize activations: in the absence of binarization, a bottleneck layer alone tends not to lead to efficient compression; 2) how to achieve variable-rate encoding: a standard autoencoder generates a fixed-length code for each fixed-resolution input patch, resulting in the same cost for lowand high-entropy patches, and requiring the network to be completely retrained to achieve different compression rates; and 3) how to avoid block artifacts: patch-based approaches are prone to block discontinuities. We propose a general framework for variable-rate image compression and a novel architecture based on convolutional and deconvolutional recurrent networks, including LSTMs, that address these issues and report promising results compared to existing baseline codecs. We evaluate the proposed methods on a large-scale benchmark consisting of tiny images (32\u00d7 32), which proves to be very challenging for all the methods.", "creator": "LaTeX with hyperref package"}}}