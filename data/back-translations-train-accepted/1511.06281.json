{"id": "1511.06281", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Density Modeling of Images using a Generalized Normalization Transformation", "abstract": "We introduce a parametric nonlinear transformation that is well-suited for Gaussianizing data from natural images. After a linear transformation of the data, each component is normalized by a pooled activity measure, computed by exponentiating a weighted sum of rectified and exponentiated components and an additive constant. We optimize the parameters of this transformation (linear transform, exponents, weights, constant) over a database of natural images, directly minimizing the negentropy of the responses. We find that the optimized transformation successfully Gaussianizes the data, achieving a significantly smaller mutual information between transformed components than previous methods including ICA and radial Gaussianization. The transformation is differentiable and can be efficiently inverted, and thus induces a density model on images. We show that samples of this model are visually similar to samples of natural image patches. We also demonstrate the use of the model as a prior density in removing additive noise. Finally, we show that the transformation can be cascaded, with each layer optimized (unsupervised) using the same Gaussianization objective, to capture additional probabilistic structure.", "histories": [["v1", "Thu, 19 Nov 2015 17:52:01 GMT  (1810kb,D)", "http://arxiv.org/abs/1511.06281v1", "under review as a conference paper at ICLR 2016"], ["v2", "Thu, 7 Jan 2016 22:05:15 GMT  (1812kb,D)", "http://arxiv.org/abs/1511.06281v2", "under review as a conference paper at ICLR 2016"], ["v3", "Sun, 17 Jan 2016 03:14:40 GMT  (1830kb,D)", "http://arxiv.org/abs/1511.06281v3", "under review as a conference paper at ICLR 2016"], ["v4", "Mon, 29 Feb 2016 21:07:30 GMT  (1812kb,D)", "http://arxiv.org/abs/1511.06281v4", "published as a conference paper at ICLR 2016"]], "COMMENTS": "under review as a conference paper at ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["johannes ball\\'e", "valero laparra", "eero p simoncelli"], "accepted": true, "id": "1511.06281"}, "pdf": {"name": "1511.06281.pdf", "metadata": {"source": "CRF", "title": "GENERALIZED NORMALIZATION TRANSFORMATION", "authors": ["Johannes Ball\u00e9", "Valero Laparra", "Eero P. Simoncelli"], "emails": ["johannes.balle@nyu.edu", "valero@nyu.edu", "eero.simoncelli@nyu.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2 PARAMETRIC GAUSSIANIZATION", "text": "Given a parametric family of transformations y = g (x; \u03b8), we will select parameters to convert the input vector x to a normal standard random vector (i.e., zero mean, identity covariance matrix).For a differentiable transformation, the input and output densities are linked by: px (x) = equivalent output densities. If we fix py as the standard normal distribution (as N), the shape of px \u2212 \u2212 tens is determined solely by the transformation. Thus, g induces a density model to x, specified by the parameters. We can achieve this by applying the Kullback normal (KL) divergence between the production density and the standard density, known as negentropy."}, {"heading": "3 DIVISIVE NORMALIZATION TRANSFORMATIONS", "text": "Dividing normalization, a form of amplification control in which the reactions are divided by the pooled activity of the neighbors, has become a standard model for describing the nonlinear properties of sensory neurons (Carandini & Heeger, 2012). A commonly used form for this transformation is: yi = \u03b3 x\u03b1i \u03b2\u03b1 + \u2211 j x \u03b1 j, which are parameters; a weighted form of normalization (with exponents fixed at \u03b1 = 2) was introduced in (Schwartz & Simoncelli, 2001) and showed that it produces approximately Gaussian reactions with greatly reduced dependencies; weights were optimized using a collection of photographic images to maximize the likelihood of reactions under a Gaussian model."}, {"heading": "3.1 PROPOSED GENERALIZED DIVISIVE NORMALIZATION (GDN) TRANSFORM", "text": "Specifically, we define a vector-weighted parametric transformation as a composition of a linear transformation followed by a generalized form of dividing normalization: y = g (x; \u03b8) s.t. z = Hx (5) and yi = zi (\u03b2i + \u2211 j \u03b3ij | zj | \u03b1ij) \u03b5i. The complete parameter vector consists of the vectors \u03b2 and \u03b5 as well as the matrices H, \u03b1 and \u03b3 for a total of 2N + 3N2 parameters (where N is the dimensionality of the input space).We call this transformation a generalized dividing normalization (GDN), as it generalizes several previously developed models: \u2022 the choice of radial transformation of line shapes: \u2022 line shapes of line shapes of line shapes of line shapes of line shapes of line shapes of line shapes of line shapes of line shapes of line shapes of line shapes of line shapes of line shapes of line shapes of line shapes 1, and the regulation of the identity matrix of line shapes of line shapes of line shapes of line shapes of line shapes of line shapes of line shapes."}, {"heading": "3.2 WELL-DEFINEDNESS AND INVERTIBILITY", "text": "For the linear part of the transformation, we just need to make sure that the matrixH is non-singular. However, for the normalization part, we need to consider the partial derivatives: partial derivatives: partial derivatives: partial derivatives: partial derivatives: partial derivatives: partial derivatives: to ensure continuity, we need all partial derivatives to be finite for all z-RN. Specifically, we need all exponents in Eq. (6) to be non-negative, as well as the parenthesized expression in the positives. Normalization can be shown to be part of the transformation if these partial transformations are generally non-negative."}, {"heading": "4 EXPERIMENTS", "text": "The model has been optimized to capture the distribution of image data by stochastic origin of the gradient expressed in Equation (3). Subsequently, we conducted a series of experiments to evaluate the validity of the adapted model for natural images."}, {"heading": "4.1 JOINT DENSITY OF PAIRS OF WAVELET COEFFICIENTS", "text": "First, we examined the paired statistics of the model responses, both for our GDN model, as well as for the ICA model and the RG model. First, we calculated the responses of an oriented filter (specifically, we used a subband of the controllable pyramid (Simoncelli & Freeman, 1995)) for images taken from the van Hateren dataset (van Hateren & van der Schaaf, 1998) and extracted coefficient pairs at different spatial offsets up to d = 1024. We then transformed these two-dimensional datasets using ICA, RG and GDN. Figure 1 (modelled according to Figure 4 by Lyu & Simoncelli (2009b) shows the mutual information in the transformed data (Note: In our case, the mutual information is related to the negentropy by an additive constant.) For small distances, a linear ICA transformation reduces some of the dependencies in the raw data."}, {"heading": "4.2 JOINT DENSITY OVER IMAGE PATCHES", "text": "We estimated the parameters of the model on vectorized image patches of 16 \u00d7 16 pixels, each 2,011. We used the stochastic optimization algorithm ADAM to facilitate optimization (Kingma & Ba, 2014) and reduced somewhat the complexity -0,00,5 -0,5 -0,000,5 -0,000,5 -0,000,5 -0,000,5 -0,000,5 -0,000,5 -0,000,5 -0,000,5 -0,000,5 -0,000,5 -0,000,00 -0,000,00 -0,000,00 -0,000,00 -0,000,00 -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -,"}, {"heading": "4.3 TWO-STAGE CASCADED MODEL", "text": "In recent years, it has become clear that most of them are people who are able to flourish, and that they are able to flourish."}, {"heading": "5 CONCLUSION", "text": "We have introduced a new probability model for natural images, implicitly defined in terms of an invertable nonlinear transformation optimized to gaussianize the data. This transformation is formed as the composition of a linear operation and a generalized form of divisive normalization, a local amplification operation commonly used to model reaction properties of sensory neurons. We developed an efficient algorithm to adjust the parameters of this transformation that minimizes the KL divergence of the distribution of transformed data against a Gaussian target. Our method emerges as a natural combination of concepts derived from two different research efforts, the first aimed at explaining the architecture and functional properties of biological sensory systems resulting from principles of coding efficiency (Barlow, 1961; Rieke et al., 1995; Bell & Sejnowski, 1997; Schwartz & Simoncelli, 2001)."}, {"heading": "6 APPENDIX", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 NEGENTROPY", "text": "To see that the neggentropy J of the transformed data y can be written as an expectation of the original data, consider a variable change: J (py) = Ey (log py (y) \u2212 logN (y) = p y (y) (log py (y) \u2212 logN (y)) dy = p x (x)."}, {"heading": "6.2 INVERTIBILITY", "text": "At this point we show that a transformation g: x 7 \u2192 y is invertible if it is continuous and its Jacobin g: x 7 \u2192 \u2202 y \u2202 x positively definitely everywhere. g is invertible if and only if two non-identical inputs xa, xb are mapped to non-identical outputs ya, yb and vice versa. First, we can write the inequality of the two right-sided vectors as u: u > \u0445 y 6 = 0, where \u2206 y is their difference. Second, we can calculate y by integrating the Jacobin lab along a straight line between xa and xb."}, {"heading": "ACKNOWLEDGMENTS", "text": "JB and EPS were supported by the Howard Hughes Medical Institute. VL was supported by APOSTD / 2014 / 095 Generalitat Valenciana (Spain)."}], "references": [{"title": "Learning sparse filterbank transforms with convolutional ICA", "author": ["Ball\u00e9", "Johannes", "Simoncelli", "Eero P"], "venue": "In 2014 IEEE International Conference on Image Processing (ICIP),", "citeRegEx": "Ball\u00e9 et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ball\u00e9 et al\\.", "year": 2014}, {"title": "Possible principles underlying the transformations of sensory messages", "author": ["Barlow", "Horace B"], "venue": "In Sensory Communication,", "citeRegEx": "Barlow and B.,? \\Q1961\\E", "shortCiteRegEx": "Barlow and B.", "year": 1961}, {"title": "The independent components of natural scenes are edge filters", "author": ["Bell", "Anthony J", "Sejnowski", "Terrence J"], "venue": "Vision Research,", "citeRegEx": "Bell et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Bell et al\\.", "year": 1997}, {"title": "Normalization as a canonical neural computation", "author": ["Carandini", "Matteo", "Heeger", "David J"], "venue": "Nature Reviews Neuroscience,", "citeRegEx": "Carandini et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Carandini et al\\.", "year": 2012}, {"title": "Dependence, correlation and Gaussianity in independent component analysis", "author": ["Cardoso", "Jean-Fran\u00e7ois"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Cardoso and Jean.Fran\u00e7ois.,? \\Q2003\\E", "shortCiteRegEx": "Cardoso and Jean.Fran\u00e7ois.", "year": 2003}, {"title": "Wavelet-based image estimation: an empirical bayes approach using Jeffrey\u2019s noninformative prior", "author": ["M.A.T. Figueiredo", "R.D. Nowak"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Figueiredo and Nowak,? \\Q2001\\E", "shortCiteRegEx": "Figueiredo and Nowak", "year": 2001}, {"title": "Projection pursuit density estimation", "author": ["Friedman", "Jerome H", "Stuetzle", "Werner", "Schroeder", "Anne"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Friedman et al\\.,? \\Q1984\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 1984}, {"title": "Normalization of cell responses in cat striate cortex", "author": ["Heeger", "David J"], "venue": "Visual Neuroscience,", "citeRegEx": "Heeger and J.,? \\Q1992\\E", "shortCiteRegEx": "Heeger and J.", "year": 1992}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["Jarrett", "Kevin", "Kavukcuoglu", "Koray", "Ranzato", "Marc\u2019Aurelio", "LeCun", "Yann"], "venue": "IEEE 12th International Conference on Computer Vision,", "citeRegEx": "Jarrett et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jarrett et al\\.", "year": 2009}, {"title": "Adam: A method for stochastic optimization. ArXiv eprints", "author": ["Kingma", "Diederik P", "Ba", "Jimmy Lei"], "venue": "San Diego,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Iterative Gaussianization: From ICA to random rotations", "author": ["Laparra", "Valero", "Camps-Valls", "Gustavo", "Malo", "Jes\u00fas"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Laparra et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Laparra et al\\.", "year": 2011}, {"title": "Handwritten zip code recognition with multilayer networks", "author": ["LeCun", "Yann", "O. Matan", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel", "H.S. Baird"], "venue": "In Proceedings, 10th International Conference on Pattern Recognition,", "citeRegEx": "LeCun et al\\.,? \\Q1990\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1990}, {"title": "Divisive normalization: Justification and effectiveness as efficient coding transform", "author": ["Lyu", "Siwei"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Lyu and Siwei.,? \\Q2010\\E", "shortCiteRegEx": "Lyu and Siwei.", "year": 2010}, {"title": "Nonlinear image representation using divisive normalization", "author": ["Lyu", "Siwei", "Simoncelli", "Eero P"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Lyu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lyu et al\\.", "year": 2008}, {"title": "Modeling multiscale subbands of photographic images with fields of Gaussian scale mixtures", "author": ["Lyu", "Siwei", "Simoncelli", "Eero P"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Lyu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lyu et al\\.", "year": 2008}, {"title": "Nonlinear extraction of independent components of natural images using radial Gaussianization", "author": ["Lyu", "Siwei", "Simoncelli", "Eero P"], "venue": "Neural Computation,", "citeRegEx": "Lyu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lyu et al\\.", "year": 2009}, {"title": "Psychophysically tuned divisive normalization approximately factorizes the PDF of natural images", "author": ["Malo", "Jes\u00fas", "Laparra", "Valero"], "venue": "Neural Computation,", "citeRegEx": "Malo et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Malo et al\\.", "year": 2010}, {"title": "Non-linear image representation for efficient perceptual coding", "author": ["Malo", "Jes\u00fas", "I. Epifanio", "R. Navarro", "Simoncelli", "Eero P"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Malo et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Malo et al\\.", "year": 2006}, {"title": "An empirical bayes estimator of the mean of a normal population", "author": ["K. Miyasawa"], "venue": "Bulletin de l\u2019Institut international de Statistique,", "citeRegEx": "Miyasawa,? \\Q1961\\E", "shortCiteRegEx": "Miyasawa", "year": 1961}, {"title": "Image denoising using scale mixtures of Gaussians in the wavelet domain", "author": ["Portilla", "Javier", "Strela", "Vasily", "Wainwright", "Martin J", "Simoncelli", "Eero P"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Portilla et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Portilla et al\\.", "year": 2003}, {"title": "Least squares estimation without priors or supervision", "author": ["Raphan", "Martin", "Simoncelli", "Eero P"], "venue": "Neural Computation,", "citeRegEx": "Raphan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Raphan et al\\.", "year": 2011}, {"title": "Naturalistic stimuli increase the rate and efficiency of information transmission by primary auditory afferents", "author": ["F. Rieke", "D.A. Bodnar", "W. Bialek"], "venue": "Proceedings of the Royal Society of London B: Biological Sciences,", "citeRegEx": "Rieke et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Rieke et al\\.", "year": 1995}, {"title": "The statistics of natural images. Network: Computation", "author": ["Ruderman", "Daniel L"], "venue": "Neural Systems,", "citeRegEx": "Ruderman and L.,? \\Q1994\\E", "shortCiteRegEx": "Ruderman and L.", "year": 1994}, {"title": "Natural signal statistics and sensory gain control", "author": ["Schwartz", "Odelia", "Simoncelli", "Eero P"], "venue": "Nature Neuroscience,", "citeRegEx": "Schwartz et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Schwartz et al\\.", "year": 2001}, {"title": "The steerable pyramid: A flexible architecture for multi-scale derivative computation", "author": ["Simoncelli", "Eero P", "Freeman", "William T"], "venue": "IEEE International Conference on Image Processing (ICIP),", "citeRegEx": "Simoncelli et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Simoncelli et al\\.", "year": 1995}, {"title": "Lp-nested symmetric distributions", "author": ["Sinz", "Fabian", "Bethge", "Matthias"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Sinz et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sinz et al\\.", "year": 2010}, {"title": "Independent component filters of natural images compared with simple cells in primary visual cortex", "author": ["J.H. van Hateren", "A. van der Schaaf"], "venue": "Proceedings of the Royal Society of London B: Biological Sciences,", "citeRegEx": "Hateren and Schaaf,? \\Q1998\\E", "shortCiteRegEx": "Hateren and Schaaf", "year": 1998}, {"title": "Scale mixtures of Gaussians and the statistics of natural images", "author": ["Wainwright", "Martin J", "Simoncelli", "Eero P"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Wainwright et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Wainwright et al\\.", "year": 2000}, {"title": "Image quality assessment: From error visibility to structural similarity", "author": ["Wang", "Zhou", "Bovik", "Alan Conrad", "H.R. Sheikh", "Simoncelli", "Eero P"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Wang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 21, "context": "The notion of optimizing a transformation so as to achieve desired statistical properties at the output is central to theories of efficient sensory coding in neurobiology (Barlow, 1961; Ruderman, 1994; Rieke et al., 1995; Bell & Sejnowski, 1997; Schwartz & Simoncelli, 2001), and also lends itself naturally to the design of cascaded representations such as deep neural networks.", "startOffset": 171, "endOffset": 274}, {"referenceID": 6, "context": "Specifically, variants of ICA-MG transformations have been applied in iterative cascades to learn densities (Friedman et al., 1984; Chen & Gopinath, 2000; Laparra et al., 2011).", "startOffset": 108, "endOffset": 176}, {"referenceID": 10, "context": "Specifically, variants of ICA-MG transformations have been applied in iterative cascades to learn densities (Friedman et al., 1984; Chen & Gopinath, 2000; Laparra et al., 2011).", "startOffset": 108, "endOffset": 176}, {"referenceID": 8, "context": "Simple forms of divisive normalization have been shown to offer improvements in recognition recognition performance of deep neural networks (Jarrett et al., 2009).", "startOffset": 140, "endOffset": 162}, {"referenceID": 10, "context": "\u2022 Choosing matrix \u03b3 to be diagonal eliminates the cross terms in the normalization pool, and the model is then a particular form of ICA-MG, or the first iteration of the Gaussianization algorithms described in Chen & Gopinath (2000) or Laparra et al. (2011): a linear \u201cunmixing\u201d transform, followed by a pointwise, Gaussianizing nonlinearity.", "startOffset": 236, "endOffset": 258}, {"referenceID": 10, "context": "\u2022 Choosing matrix \u03b3 to be diagonal eliminates the cross terms in the normalization pool, and the model is then a particular form of ICA-MG, or the first iteration of the Gaussianization algorithms described in Chen & Gopinath (2000) or Laparra et al. (2011): a linear \u201cunmixing\u201d transform, followed by a pointwise, Gaussianizing nonlinearity. \u2022 Choosing \u03b1ij \u2261 2 and setting all elements of \u03b2, \u03b5, and \u03b3 equal, the transformation assumes a radial form: y = z ( \u03b2 + \u03b3 \u2211 j z 2 j )\u03b5 = z \u2016z\u20162 g2(\u2016z\u20162) where g2(r) = r/(\u03b2 + \u03b3r) is a scalar-valued transformation on the radial component of z, ensuring that the normalization operation preserves the vector direction of z. If, in addition, H is a whitening transformation such as ZCA (Bell & Sejnowski, 1997), the overall transformation is a form of RG Lyu & Simoncelli (2009b). \u2022 More generally, if we allow exponents \u03b1ij \u2261 p, the induced distribution is anLp-symmetric distribution, a family which has been shown to capture statistical properties of natural images (Sinz & Bethge, 2010).", "startOffset": 236, "endOffset": 819}, {"referenceID": 17, "context": "Other types of iterations based on matrix inversion have been proposed for this purpose in the literature (Malo et al., 2006; Lyu & Simoncelli, 2008).", "startOffset": 106, "endOffset": 149}, {"referenceID": 18, "context": "For GDN, we use the empirical Bayes solution of Miyasawa (1961); Raphan & Simoncelli (2011), which expresses the least-squares optimal solution directly as a function of the distribution of the noisy data: x\u0302 = x\u0303+ \u03c3\u2207 log px\u0303(x\u0303), (7) where x\u0303 is the noisy observation, px\u0303 is the density of the noisy data, \u03c3 is the noise variance, and x\u0302 is the optimal estimate.", "startOffset": 48, "endOffset": 64}, {"referenceID": 18, "context": "For GDN, we use the empirical Bayes solution of Miyasawa (1961); Raphan & Simoncelli (2011), which expresses the least-squares optimal solution directly as a function of the distribution of the noisy data: x\u0302 = x\u0303+ \u03c3\u2207 log px\u0303(x\u0303), (7) where x\u0303 is the noisy observation, px\u0303 is the density of the noisy data, \u03c3 is the noise variance, and x\u0302 is the optimal estimate.", "startOffset": 48, "endOffset": 92}, {"referenceID": 19, "context": "For comparison, we implemented two denoising methods that operate on orthogonal wavelet coefficients, one assuming a marginal model (Figueiredo & Nowak, 2001), and the other a spherically symmetric Gaussian scale mixture (GSM) model (Portilla et al., 2003).", "startOffset": 233, "endOffset": 256}, {"referenceID": 28, "context": "We also report numerical scores: the Peak Signal to Noise ratio (PSNR), and Structural Similarity Index (SSIM; Wang et al., 2004).", "startOffset": 104, "endOffset": 129}, {"referenceID": 10, "context": "One way of achieving this is to cascade Gaussianizing transformations (Chen & Gopinath, 2000; Laparra et al., 2011).", "startOffset": 70, "endOffset": 115}, {"referenceID": 11, "context": "or simply increasing the size of the transformation, which would require a much larger number of parameters as opposed to convolutional \u201cweight sharing\u201d (LeCun et al., 1990).", "startOffset": 153, "endOffset": 173}, {"referenceID": 21, "context": "The first aims to explain the architecture and functional properties of biological sensory systems as arising from principles of coding efficiency (Barlow, 1961; Rieke et al., 1995; Bell & Sejnowski, 1997; Schwartz & Simoncelli, 2001).", "startOffset": 147, "endOffset": 234}, {"referenceID": 6, "context": "The second area of endeavor arises from the statistics literature on projection pursuit, and its use in problems of density estimation (Friedman et al., 1984).", "startOffset": 135, "endOffset": 158}, {"referenceID": 10, "context": "More recent examples include marginal and radial Gaussianization (Chen & Gopinath, 2000; Lyu & Simoncelli, 2009a; Laparra et al., 2011).", "startOffset": 65, "endOffset": 135}], "year": 2017, "abstractText": "We introduce a parametric nonlinear transformation that is well-suited for Gaussianizing data from natural images. After a linear transformation of the data, each component is normalized by a pooled activity measure, computed by exponentiating a weighted sum of rectified and exponentiated components and an additive constant. We optimize the parameters of this transformation (linear transform, exponents, weights, constant) over a database of natural images, directly minimizing the negentropy of the responses. We find that the optimized transformation successfully Gaussianizes the data, achieving a significantly smaller mutual information between transformed components than previous methods including ICA and radial Gaussianization. The transformation is differentiable and can be efficiently inverted, and thus induces a density model on images. We show that samples of this model are visually similar to samples of natural image patches. We also demonstrate the use of the model as a prior density in removing additive noise. Finally, we show that the transformation can be cascaded, with each layer optimized (unsupervised) using the same Gaussianization objective, to capture additional probabilistic structure.", "creator": "LaTeX with hyperref package"}}}