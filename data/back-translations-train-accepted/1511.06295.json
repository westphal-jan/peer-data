{"id": "1511.06295", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Policy Distillation", "abstract": "Policies for complex visual tasks have been successfully learned with deep reinforcement learning, using an approach called deep Q-networks (DQN), but relatively large (task-specific) networks and extensive training are needed to achieve good performance. In this work, we present a novel method called policy distillation that can be used to extract the policy of a reinforcement learning agent and train a new network that performs at the expert level while being dramatically smaller and more efficient. Furthermore, the same method can be used to consolidate multiple task-specific policies into a single policy. We demonstrate these claims using the Atari domain and show that the multi-task distilled agent outperforms the single-task teachers as well as a jointly-trained DQN agent.", "histories": [["v1", "Thu, 19 Nov 2015 18:38:47 GMT  (371kb,D)", "http://arxiv.org/abs/1511.06295v1", "Submitted to ICLR 2016"], ["v2", "Thu, 7 Jan 2016 18:43:03 GMT  (578kb,D)", "http://arxiv.org/abs/1511.06295v2", "Submitted to ICLR 2016"]], "COMMENTS": "Submitted to ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andrei a rusu", "sergio gomez colmenarejo", "caglar gulcehre", "guillaume desjardins", "james kirkpatrick", "razvan pascanu", "volodymyr mnih", "koray kavukcuoglu", "raia hadsell"], "accepted": true, "id": "1511.06295"}, "pdf": {"name": "1511.06295.pdf", "metadata": {"source": "CRF", "title": "POLICY DISTILLATION", "authors": ["Andrei A. Rusu", "Sergio G\u00f3mez Colmenarejo", "\u00c7a\u011flar G\u00fcl\u00e7ehre", "Guillaume Desjardins", "James Kirkpatrick", "Razvan Pascanu", "Volodymyr Mnih", "Koray Kavukcuoglu", "Raia Hadsell"], "emails": ["raia}@google.com,", "gulcehrc@iro.umontreal.ca"], "sections": [{"heading": "1 INTRODUCTION", "text": "Recent advances in in-depth reinforcement learning have shown that strategies can be encoded from reward signals through complex strategies, and that these pixel-to-action strategies can deliver superhuman performance on many challenging tasks (Mnih et al., 2015). The Deep Q Network (DQN) algorithm interacts with an environment and receives observations and rewards during a lengthy training process. At each step, an agent selects the action that maximizes his predicted cumulative reward, and a revolutionary network is trained to approximate the optimal action value function based on these observations, rewards, and actions. The DQN algorithm requires long training times to compress a network with a relatively large capacity on a single task. In this paper, we are introducing a policy distillation to transfer one or more action policies from QNetworks to an untrained network. The method has multiple advantages that can be compressed up to 15 without affecting performance."}, {"heading": "2 PREVIOUS WORK", "text": "This work relates to three different areas of research: model compression with distillation, deep amplification learning, and multi-task learning. The concept of model compression through the formation of a student network with the results of a teacher network was first proposed by Bucila et al. (2006), who used it as a means of compressing a large ensemble model into a single network. In an extension of this work, Ba and Caruana (2014) used compression to transfer knowledge from a deep network to a flat network. Other authors applied the same concept in a slightly different way: Liang et al. (2008) proposed an approach for the formation of a fast logistic regression model with data labeled by a lower structured output CRF model; Menke and Martinez used model transfer as a regulatory technique. Hinton et al. (2014) introduced the term distillation and suggested increasing the temperature of the softmax distribution in order to transfer knowledge about all output classes from teacher to student network."}, {"heading": "3 APPROACH", "text": "Before describing political distillation, we will first give a brief overview of in-depth Q-learning, as DQN serves both as a basis for performance comparisons and as a teacher for political distillation. Note that the proposed method is not bound to DQN and can be applied to models trained with other RL algorithms. After summarizing the DQN, we will describe political distillation for single and multiple tasks."}, {"heading": "3.1 DEEP Q-LEARNING", "text": "DQN is a state-of-the-art model-free approach to amplification learning using deep networks in discrete action environments, achieving superhuman performance on a large number of different Atari 2600 games (Mnih et al., 2015). In deep Q-Learning, a neural network is optimized to predict the average discounted future return on each possible action over a small number of consecutive observations. Thus, the action with the highest predicted return is cho-sensed by the agent. Thus, in an environment E whose interface at the time of learning includes i actions, ai-A = {1,..., K}, observations xi-Rd and rewards ri-R, a sequence st = x1, a1, x2, a2,..., at \u2212 1, xt, and a future return at a time t with discounting."}, {"heading": "3.2 SINGLE-GAME POLICY DISTILLATION", "text": "In the simplest case, as illustrated in Figure 1 (a), distillation is a method of transferring knowledge from a single teacher model to a student model S. The distillation goals from a classification network are typically achieved by transferring the weighted sums of the last network layer through a Softmax function, resulting in production probabilities that often come close to a one-hot distribution. To transfer more of the knowledge of the network, teacher results can be softened by softening the network performance through a relaxed (higher temperature) Softmax function than the one used for training. Therefore, for a selected temperature, the new teacher results are given by softmax (q), where qT is the vector of the Q values of T. These can be learned by regression. However, in the case of transferring a Q function instead of a classifier, we assume that predicting Q values of all actions in the face of the difficult regression task is a ression observation."}, {"heading": "3.3 MULTI-TASK POLICY DISTILLATION", "text": "The approach to multi-task policy distillation, as illustrated in Figure 1 (b), is simple. We use n DQN single-game experts, each trained separately, these agents produce inputs and targets, just as in single-game distillation, and the data is stored in separate memory buffers. The distillation agent then learns sequentially from n data stores, switching to a different output in each episode. As different tasks often have different action groups, a separate output layer (the so-called controller layer) is trained for each task, and the task is used to switch to the correct output during training and evaluation. We also experiment with the KL and NLL distillation loss functions for multi-task learning. An important contribution of this work is the comparison of the performance of multi-task DQN agents with multi-task interactions."}, {"heading": "4 RESULTS AND DISCUSSION", "text": "A brief overview of the training and evaluation structure is given below; full details can be found in Annex A."}, {"heading": "4.1 TRAINING AND EVALUATION", "text": "The individual tasks of the policy are a process of data generation by the teacher network (a trained DQN agent) and supervised training by the student network, as illustrated in Figure 1 (a). For each game, we have trained a separate DQN agent, as in Mnih et al. (2015). Each agent was then fixed (no Q-learning) and used as a teacher for a single student political network. Results of the DQN agent are held in a buffer next to the inputs (images). We have used a similar training method for multi-task distillation, as in Figure 1 (b). The network used to train the DQN agent is in (Mnih et al., 2015). The same network was used for the student, except for the compression experiments, which scaled the number of units in each shift."}, {"heading": "4.2 SINGLE-GAME POLICY DISTILLATION RESULTS", "text": "In this section, we show that the Kullback Leibler (KL) cost function leads to the highest performing student agents, and that these distilled agents outperform their DQN teachers in most games. Table 1 compares the effectiveness of different political distillation cost functions in terms of generalization performance in four Atari games, while maintaining the same network architecture as DQN. Only four games were used in this experiment to set parameters for the loss functions, which are then fixed in other experiments (which use ten games). Note, however, that the evaluation uses human starting points to test generalization robustly (see Section 4.1). Students trained with an MSE loss performed worse than KL or NLL, although we successfully minimize the square error. This is not surprising considering that greedy action decisions are based on very small differences in Q values, which do not receive a low weight in MSE situations."}, {"heading": "4.3 POLICY DISTILLATION WITH MODEL COMPRESSION", "text": "DQN networks, however, are relatively large, partly due to optimization problems such as local minima, which are mitigated by over-complete models. In practice, DQN benefits considerably from increased network capacity, but it is likely that the final policy does not require all or indeed most of that capacity. We evaluate distilled singleplayer and DQN teachers against 10 different Atari games and use student networks that were significantly smaller (25%, 7% and 4% of DQN network parameters).The distilled agents, which are four times smaller than DQN (Dist-KL-net1, 428,000 parameters), are actually more powerful than DQN."}, {"heading": "4.4 MULTI-GAME POLICY DISTILLATION RESULTS", "text": "We train a multi-task DQN agent using the standard DQN algorithm, which is applied to interleaved experiences from three games (Multi-DQN), and compare it to distilled agents (Multi-Dist) trained on objectives from three different DQN teachers (see Figure 3).All three agents use an identical multi-controller architecture of similar size to a single teacher network. About 90% of the parameters are divided, with only 3 small MLP \"controllers\" at the top, which are task specific and allow different groups of action between different games. The multi-task DQN agent learns the three tasks at 83.5% of the single-task DQN performance (see Figure 3).In contrast, both distilled agents perform better than their DQN teachers, with the mean values of 105.1% for Multi-Dist-NLL and 116.9% for Multi-DQN."}, {"heading": "4.5 ONLINE POLICY DISTILLATION RESULTS", "text": "As a final contribution, we examined the distillation of online strategies, where the student must follow the DQN teacher during Q-Learning. It was not obvious whether these efforts would be successful, as it was observed that the DQN guidelines change dramatically during the training, as different parts of the game are explored and mastered. To test this, the DQN was trained normally and the network was regularly stored when it reached a new high value. This network was then used by the distillation student until it was updated by a higher DQN value. The results of this experiment are shown in Figure 5. The learning curves show the DQN value with high variance, the best value so far, and the Qbert by two distillation agents initiated with different seeds. The distillation agent is much more stable than the DQN teacher and achieves similar or equal performance in all games (see Appendix C for more examples of online distillation)."}, {"heading": "5 DISCUSSION", "text": "This is of interest both theoretically and practically, as very small models can be used, reused or combined in different ways. One possible application of political distillation is to build active ingredients that are capable of performing a huge amount of additional tasks. Multi-task learning is difficult for DQN, but distillation provides a means of transferring and bringing together strategies online while the dedicated DQN actors continue to learn. We have demonstrated and tested the building blocks of such a process: distillation into much smaller networks, multi-task distillation and online distillation during Q-Learning. Another area of future research is the development of RL optimization techniques that use distillation to stabilize and accelerate learning."}, {"heading": "A EXPERIMENTAL DETAILS", "text": "In fact, most of them are able to survive on their own, without being able to survive on their own."}, {"heading": "B SUPPORTING TABLES FOR POLICY DISTILLATION FIGURES", "text": "DQN Dist-KL-net1 Dist-KL-net2 Dist-KL-net3score% DQN score% DQN score% DQN score% DQN score% DQN score% DQN Beamrider 8672.4 7552.8 87.1 7393.3 85.3 6521.2 75.2 75.2 Breakout 303.9 321.0 105.6 298.2 98.1 238.8 78.6 Enduro 475.6 677.9 142.5 672.2 141.3 556.7 117.1 Freeway 25.8 26.7 103.5 26.7 103.5 103.5 103.5 103.5 103.5 Ms.Pacman 763.5 782.5 102.5 86.4 734.3 96.2 Pong 16.2 100.6 100.6 100.6 100.6 100.3 104.3 104.3 104.3 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 104.7 10"}, {"heading": "C ADDITIONAL RESULTS USING ONLINE POLICY DISTILLATION", "text": "0.0 0.2 0.4 0.6 0.8 1.0Steps 1e8201001020g a me sco reDQN Best-DQN Online-Dist-seed = 1 Online-Dist-seed = 2Pong0.0 0.2 0.4 0.6 0.8 1.0Steps 1e8050100150200250300350g a me sco reDQN Best-DQN Online-Dist-seed = 1 Online-Dist-seed = 2BreakoutFigure C1: Online Policy Distillation during DQN learning (pale blue) The currently best DQN policy (green) is distilled into a new network during the DQN training."}], "references": [{"title": "A dozen tricks with multitask learning", "author": ["Rich Caruana"], "venue": null, "citeRegEx": "Caruana.,? \\Q1998\\E", "shortCiteRegEx": "Caruana.", "year": 1998}, {"title": "Transferring knowledge from a rnn to a dnn", "author": ["William Chan", "Nan Rosemary Ke", "Ian Lane"], "venue": "arXiv preprint arXiv:1504.01483,", "citeRegEx": "Chan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "How not to lie with statistics: The correct way to summarize benchmark results", "author": ["Philip J. Fleming", "John J. Wallace"], "venue": "Commun. ACM,", "citeRegEx": "Fleming and Wallace.,? \\Q1986\\E", "shortCiteRegEx": "Fleming and Wallace.", "year": 1986}, {"title": "Variance reduction techniques for gradient estimates in reinforcement learning", "author": ["Evan Greensmith", "Peter L. Bartlett", "Jonathan Baxter"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Greensmith et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Greensmith et al\\.", "year": 2004}, {"title": "Deep learning for realtime atari game play using offline monte-carlo tree search planning", "author": ["Xiaoxiao Guo", "Satinder P. Singh", "Honglak Lee", "Richard L. Lewis", "Xiaoshi Wang"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Guo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Distilling the Knowledge in a Neural Network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "Deep Learning and Representation Learning Workshop,", "citeRegEx": "Hinton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2014}, {"title": "Bandit based monte-carlo planning", "author": ["Levente Kocsis", "Csaba Szepesv\u00e1ri"], "venue": "In Machine Learning: ECML", "citeRegEx": "Kocsis and Szepesv\u00e1ri.,? \\Q2006\\E", "shortCiteRegEx": "Kocsis and Szepesv\u00e1ri.", "year": 2006}, {"title": "Learning small-size dnn with output-distribution-based criteria", "author": ["Jinyu Li", "Rui Zhao", "Jui-Ting Huang", "Yifan Gong"], "venue": "In Proc. Interspeech,", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Structure compilation: trading structure for features", "author": ["Percy Liang", "Hal Daum III", "Dan Klein"], "venue": "In Proceedings of International Conference on Machine Learning (ICML),", "citeRegEx": "Liang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2008}, {"title": "Improving supervised learning by adapting the problem to the learner", "author": ["Joshua Menke", "Tony Martinez"], "venue": "International Journal of Neural Systems,", "citeRegEx": "Menke and Martinez.,? \\Q2009\\E", "shortCiteRegEx": "Menke and Martinez.", "year": 2009}, {"title": "Playing atari with deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin A. Riedmiller"], "venue": "Deep Learning Workshop,", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A. Rusu", "Joel Veness", "Marc G. Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K. Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Massively parallel methods for deep reinforcement learning", "author": ["Arun Nair", "Praveen Srinivasan", "Sam Blackwell", "Cagdas Alcicek", "Rory Fearon", "Alessandro De Maria", "Vedavyas Panneershelvam", "Mustafa Suleyman", "Charles Beattie", "Stig Petersen", "Shane Legg", "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver"], "venue": "CoRR, abs/1507.04296,", "citeRegEx": "Nair et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2015}, {"title": "Fitnets: Hints for thin deep nets", "author": ["Adriana Romero", "Nicolas Ballas", "Samira Ebrahimi Kahou", "Antoine Chassang", "Carlo Gatta", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.6550,", "citeRegEx": "Romero et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Romero et al\\.", "year": 2014}, {"title": "SelfieBoost: A Boosting Algorithm for Deep Learning", "author": ["S. Shalev-Shwartz"], "venue": "ArXiv e-prints,", "citeRegEx": "Shalev.Shwartz.,? \\Q2014\\E", "shortCiteRegEx": "Shalev.Shwartz.", "year": 2014}, {"title": "Introduction to Reinforcement Learning", "author": ["Richard S. Sutton", "Andrew G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Knowledge transfer pre-training", "author": ["Zhiyuan Tang", "Dong Wang", "Yiqiao Pan", "Zhiyong Zhang"], "venue": "arXiv preprint arXiv:1506.02256,", "citeRegEx": "Tang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Lecture 6.5\u2014RmsProp: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton.", "year": 2012}, {"title": "Visualizing high-dimensional data using t-sne", "author": ["L.J.P. van der Maaten", "G.E. Hinton"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Maaten and Hinton.,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Recurrent neural network training with dark knowledge transfer", "author": ["Dong Wang", "Chao Liu", "Zhiyuan Tang", "Zhiyong Zhang", "Mengyuan Zhao"], "venue": "arXiv preprint arXiv:1505.04630,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "Recently, advances in deep reinforcement learning have shown that policies can be encoded through end-to-end learning from reward signals, and that these pixel-to-action policies can deliver superhuman performance on many challenging tasks (Mnih et al., 2015).", "startOffset": 240, "endOffset": 259}, {"referenceID": 5, "context": ", 2006), and it has since been extended to the problem of creating a single network from an ensemble model (Hinton et al., 2014).", "startOffset": 107, "endOffset": 128}, {"referenceID": 14, "context": "It also shows merit as an optimization method that acts to stabilize learning over large datasets or in dynamic domains (Shalev-Shwartz, 2014).", "startOffset": 120, "endOffset": 142}, {"referenceID": 7, "context": "Distillation has since been applied in various ways: from larger to smaller networks, from smaller to larger networks, from feedforward to recurrent networks, and from wide to narrow networks (Li et al., 2014; Romero et al., 2014; Chan et al., 2015; Wang et al., 2015; Tang et al., 2015).", "startOffset": 192, "endOffset": 287}, {"referenceID": 13, "context": "Distillation has since been applied in various ways: from larger to smaller networks, from smaller to larger networks, from feedforward to recurrent networks, and from wide to narrow networks (Li et al., 2014; Romero et al., 2014; Chan et al., 2015; Wang et al., 2015; Tang et al., 2015).", "startOffset": 192, "endOffset": 287}, {"referenceID": 1, "context": "Distillation has since been applied in various ways: from larger to smaller networks, from smaller to larger networks, from feedforward to recurrent networks, and from wide to narrow networks (Li et al., 2014; Romero et al., 2014; Chan et al., 2015; Wang et al., 2015; Tang et al., 2015).", "startOffset": 192, "endOffset": 287}, {"referenceID": 19, "context": "Distillation has since been applied in various ways: from larger to smaller networks, from smaller to larger networks, from feedforward to recurrent networks, and from wide to narrow networks (Li et al., 2014; Romero et al., 2014; Chan et al., 2015; Wang et al., 2015; Tang et al., 2015).", "startOffset": 192, "endOffset": 287}, {"referenceID": 16, "context": "Distillation has since been applied in various ways: from larger to smaller networks, from smaller to larger networks, from feedforward to recurrent networks, and from wide to narrow networks (Li et al., 2014; Romero et al., 2014; Chan et al., 2015; Wang et al., 2015; Tang et al., 2015).", "startOffset": 192, "endOffset": 287}, {"referenceID": 6, "context": "(2014) has achieved superhuman Atari scores on some games through a process of UCT training that builds a search tree using the ATARI emulator (Kocsis and Szepesv\u00e1ri, 2006) followed by transfer of the learned policy to a neural network via either regression or classification.", "startOffset": 143, "endOffset": 172}, {"referenceID": 0, "context": "Another review (Caruana, 1998) notes that naive multi-task learning of arbitrary tasks competing for the same resources (in our case network parameters) can very quickly lead to sub-optimal results.", "startOffset": 15, "endOffset": 30}, {"referenceID": 0, "context": "In an extension of this work, Ba and Caruana (2014) used compression to transfer knowledge from a deep network to a shallow network.", "startOffset": 37, "endOffset": 52}, {"referenceID": 0, "context": "In an extension of this work, Ba and Caruana (2014) used compression to transfer knowledge from a deep network to a shallow network. Other authors applied the same concept in somewhat different ways: Liang et al. (2008) proposed an approach for training a fast logistic regression model using data labeled by a slower structured-output CRF model; Menke and Martinez (2009) used model transfer as a regularization technique.", "startOffset": 37, "endOffset": 220}, {"referenceID": 0, "context": "In an extension of this work, Ba and Caruana (2014) used compression to transfer knowledge from a deep network to a shallow network. Other authors applied the same concept in somewhat different ways: Liang et al. (2008) proposed an approach for training a fast logistic regression model using data labeled by a slower structured-output CRF model; Menke and Martinez (2009) used model transfer as a regularization technique.", "startOffset": 37, "endOffset": 373}, {"referenceID": 0, "context": "In an extension of this work, Ba and Caruana (2014) used compression to transfer knowledge from a deep network to a shallow network. Other authors applied the same concept in somewhat different ways: Liang et al. (2008) proposed an approach for training a fast logistic regression model using data labeled by a slower structured-output CRF model; Menke and Martinez (2009) used model transfer as a regularization technique. Hinton et al. (2014) introduced the term distillation and suggested raising the temperature of the softmax distribution in order to transfer knowledge about all the output classes from teacher to student network.", "startOffset": 37, "endOffset": 445}, {"referenceID": 0, "context": "In an extension of this work, Ba and Caruana (2014) used compression to transfer knowledge from a deep network to a shallow network. Other authors applied the same concept in somewhat different ways: Liang et al. (2008) proposed an approach for training a fast logistic regression model using data labeled by a slower structured-output CRF model; Menke and Martinez (2009) used model transfer as a regularization technique. Hinton et al. (2014) introduced the term distillation and suggested raising the temperature of the softmax distribution in order to transfer knowledge about all the output classes from teacher to student network. Distillation has since been applied in various ways: from larger to smaller networks, from smaller to larger networks, from feedforward to recurrent networks, and from wide to narrow networks (Li et al., 2014; Romero et al., 2014; Chan et al., 2015; Wang et al., 2015; Tang et al., 2015). These algorithms are not concerned with reinforcement learning or policies, nor are they addressing multi-task learning through distillation of single-task learners. In reinforcement learning, Guo et al. (2014) has achieved superhuman Atari scores on some games through a process of UCT training that builds a search tree using the ATARI emulator (Kocsis and Szepesv\u00e1ri, 2006) followed by transfer of the learned policy to a neural network via either regression or classification.", "startOffset": 37, "endOffset": 1137}, {"referenceID": 11, "context": "DQN is a state-of-the-art model-free approach to reinforcement learning using deep networks, in environments with discrete action choices, which has achieved super-human performance on a large collection of diverse Atari 2600 games (Mnih et al., 2015).", "startOffset": 232, "endOffset": 251}, {"referenceID": 5, "context": "In the third case, we adopt the distillation setup of Hinton et al. (2014) and use the Kullback-Leibler divergence (KL) with temperature \u03c4 :", "startOffset": 54, "endOffset": 75}, {"referenceID": 3, "context": "Also, policies are inherently lower variance than value functions, which should help performance and stability (Greensmith et al., 2004).", "startOffset": 111, "endOffset": 136}, {"referenceID": 10, "context": "For each game we trained a separate DQN agent, as reported in Mnih et al. (2015). Each agent was subsequently fixed (no Q-learning) and used as a teacher for a single student policy network.", "startOffset": 62, "endOffset": 81}, {"referenceID": 11, "context": "The network used to train the DQN agents is described in (Mnih et al., 2015).", "startOffset": 57, "endOffset": 76}, {"referenceID": 10, "context": "The network used to train the DQN agents is described in (Mnih et al., 2015). The same network was used for the student, except for the compression experiments which scaled down the number of units in each layer. A larger network (four times more parameters, with an additional fully connected layer) was used to train on multi-task distillation with 10 games. The multi-task networks had a separate MLP output (controller) layer for each task. See AppendixA for full details of training procedure and networks. Because many Atari games are highly deterministic, a learner could potentially memorize and reproduce action sequences from a few starting points. To rigorously test the generalization capability of both DQN teachers and distilled agents we followed the evaluation techniques introduced by Nair et al. (2015) and adopted by subsequent research (van Hasselt et al.", "startOffset": 58, "endOffset": 821}, {"referenceID": 10, "context": "However, it has been found that training a smaller DQN agent results in considerably lower performance across games (Mnih et al., 2013).", "startOffset": 116, "endOffset": 135}, {"referenceID": 15, "context": "We speculate that training a larger network accelerates the policy iteration cycle (Sutton and Barto, 1998) of DQN.", "startOffset": 83, "endOffset": 107}], "year": 2015, "abstractText": "Policies for complex visual tasks have been successfully learned with deep reinforcement learning, using an approach called deep Q-networks (DQN), but relatively large (task-specific) networks and extensive training are needed to achieve good performance. In this work, we present a novel method called policy distillation that can be used to extract the policy of a reinforcement learning agent and train a new network that performs at the expert level while being dramatically smaller and more efficient. Furthermore, the same method can be used to consolidate multiple task-specific policies into a single policy. We demonstrate these claims using the Atari domain and show that the multi-task distilled agent outperforms the single-task teachers as well as a jointly-trained DQN agent.", "creator": "LaTeX with hyperref package"}}}