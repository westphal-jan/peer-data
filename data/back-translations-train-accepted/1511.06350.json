{"id": "1511.06350", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Structured Prediction Energy Networks", "abstract": "We introduce structured prediction energy networks (SPENs), a flexible framework for structured prediction. A deep architecture is used to define an energy function of candidate labels, and then predictions are produced by using back-propagation to iteratively optimize the energy with respect to the labels. This deep architecture captures dependencies between labels that would lead to intractable graphical models, and performs structure learning by automatically learning discriminative features of the structured output. One natural application of our technique is multi-label classification, which traditionally has required strict prior assumptions about the interactions between labels to ensure tractable learning and prediction problems. We are able to apply SPENs to multi-label problems with substantially larger label sets than previous applications of structured prediction, while modeling high-order interactions using minimal structural assumptions. Overall, deep learning provides remarkable tools for learning features of the inputs to a prediction problem, and this work extends these techniques to learning features of the outputs. Our experiments provide impressive performance on a variety of benchmark multi-label classification tasks, demonstrate that our technique can be used to provide interpretable structure learning, and illuminate fundamental trade-offs between feed-forward and iterative structured prediction techniques.", "histories": [["v1", "Thu, 19 Nov 2015 20:39:59 GMT  (65kb,D)", "http://arxiv.org/abs/1511.06350v1", "Under review as a conference paper at ICLR 2016"], ["v2", "Thu, 7 Jan 2016 16:28:36 GMT  (87kb,D)", "http://arxiv.org/abs/1511.06350v2", "Updated version of ICLR 2016 submission"], ["v3", "Thu, 23 Jun 2016 20:21:11 GMT  (201kb,D)", "http://arxiv.org/abs/1511.06350v3", "ICML 2016"]], "COMMENTS": "Under review as a conference paper at ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["david belanger", "andrew mccallum"], "accepted": true, "id": "1511.06350"}, "pdf": {"name": "1511.06350.pdf", "metadata": {"source": "CRF", "title": "STRUCTURED PREDICTION ENERGY NETWORKS", "authors": ["David Belanger"], "emails": ["belanger@cs.umass.edu", "mccallum@cs.umass.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "We consider two basic approaches to predicting a function (forecast): low (forecast) low (forecast) low (forecast) low (forecast) low (forecast) low (forecast) = low (forecast) = low (forecast) = low (forecast) = low (forecast) = low (forecast) = low (forecast) = low (forecast) = low (forecast) = low (forecast) = low (forecast) = low (forecast) = low (forecast) = low (forecast) = low (forecast) = low (forecast) = low (forecast) = (forecast) = low (forecast) = low (forecast) = low (forecast) = low (forecast) = low (forecast) = low (forecast) = low (forecast) = low (forecast) = low (forecast) = low (forecast) = (deep) = (deep) = (deep) = (deep) = (deep) (forecast) = (deep) = (deep) = (deep) = (deep) (deep) = (deep) = (deep) = (deep) (deep) = (deep) = (deep) = (deep) = (deep) = (deep) = (deep) = (deep) = (deep) = (deep = (deep) = (deep) = (deep) = (deep = = = (deep) = = (deep) = (deep = = (deep) = = = (deep) = (deep = = = = = = (deep) = = = (deep = = = = = = = =) = (deep = = = (deep =) = (deep =) = (deep = (deep =) = = (deep =) = (deep = =) = (deep = (deep =) = (deep =) = (deep) = (deep =) = (deep = (deep) = (deep) = (deep = =) = (deep = (deep) = (deep) = (deep) = (deep = (deep) = (deep) = = = = (deep) = (deep) (deep) = (deep = (deep) = = = (deep) = = (deep) (deep = = (deep) (deep) = (deep = (deep) = = = (deep) = = = (deep) (deep = = = = = (deep) (deep = = (deep) = ="}, {"heading": "2 STRUCTURED PREDICTION ENERGY NETWORKS", "text": "\"It's as if we're in a position to hide to find a solution,\" he says."}, {"heading": "3 EXAMPLE SPEN ARCHITECTURE", "text": "All our experiments use the general configuration described in this section. We use upper case matrices and lower case vectors. We use g () to denote a coordinate-wise nonlinearity function, and can use different nonlinearities, e.g. sigmoid vs. rectifier, in different places. (3) Our power network is the sum of two terms. First, the local power network y scores as the sum of L independent linear models: Elocalx (y) = L \u00b2 i = 1 y \u00b2 ib > i f (x). (4) Here, each bi is an F dimensional vector of parameters for each label.This score is added to the output of the label network, which has configurations for an independent energy division problem."}, {"heading": "3.1 CONDITIONAL RANDOM FIELDS AS SPENS", "text": "There are important parallels between the example of the SPEN architecture given above and the parameterization of a CRF (Lafferty et al., 2001; Sutton & McCallum, 2011).In this context, we use CRF to refer to any structured linear model that may or may not be formed to maximize the probability of the conditional protocol. Let's consider, for the sake of simplicity of notation, a fully networked paired CRF with local potentials that depend on x but on data-independent paired potentials. Let's () flatten a matrix to a vector. Suppose we apply Ex () directly to y instead of relaxing y. The corresponding label would be: Ecrfx (y) = s > 2 vec (yy >), (7) In applications with large label spaces, it is problematic to apply both the statistical efficiency of the parameter estimation and the computation efficiency of the prediction."}, {"heading": "4 LEARNING SPENS", "text": "In section 2, we describe a technique for making predictions by performing continuous optimizations in the space of results. Now, we discuss a gradient-based technique for learning the parameters of the deep architecture Ex (y).In many structured prediction applications, the practitioner is able to interact with the model in only two ways: (1) we evaluate the energy of the model at a given value of y, and (2) we minimize the energy in relation to the y. This happens, for example, when we predict combinatorial structures such as bipartite matches and graphs. A popular technique in these settings is the structured support of the vector machine (SSVM) (Taskar et al, 2004; Tsochantaridis et al., 2004) If we assume that our prediction method is not subject to optimization, then we apply (1) and (2) to our model."}, {"heading": "5 APPLICATIONS OF SPENS", "text": "Our experiments focus on multi-label classification, an important task in a variety of machine learning applications. The data consists of {x, y} pairs, where y = {y1,.., yL,.,. 0, 1} L is a set of several binary labels that we want to predict, and x is a feature vector. In many cases, we do not get a structure under the L labels a priori, although the labels may be correlated. SPENs are a very natural model for multi-label classification, as learning the measurement matrix C1 in (5) provides an automatic method of detecting this interaction structure. Section 6.3 discusses the relationship between SPENs and previous work on multi-label classification. However, SPENs are very general and can be applied to any prediction problem that can be represented as MAP inference in an undirected graphical model. In many applications of graphical models, the practitioner presupposes a specific identifier."}, {"heading": "6 RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 ITERATIVE PREDICTION USING NEURAL NETWORKS", "text": "Our use of back progation to perform gradient-based predictions differs from most deep learning applications where back propagation is used to update network parameters. However, back propagation-based predictions have been useful in a variety of deep learning applications, including Siamese networks (Bromley et al., 1993), methods for generating conflicting examples (Szegedy et al., 2014; Goodfellow et al., 2014), methods for embedding documents as dense vectors (Le & Mikolov, 2014) and successful techniques for image generation and texture synthesis (Mordvintsev et al., 2015; Gatys et al., 2015a; b). At the same time (Carreira et al., 2015), we propose an iteratively structured prediction method for human pose estimation, with Ex (y) not returning a number, but an increment prediction."}, {"heading": "6.2 CONDITIONAL RANDOM FIELDS", "text": "A natural alternative to SPENs for structured predictions is to encode Ex (y) as CRFs. The main advantage of SPENs is that the CRF inference in the tree width of the diagram is exponential, while the measurements used by SPENs can extract information from any number of labels at once. Although the complexity of CRF prediction per iteration is superior to CRFs of comparable expressivity, it is difficult to analyze their total cost compared to CRFs, e.g. with belief propagation, as both perform nonconvex optimizations. CRFs that use SSVM loss are more conceptually attractive than SPENs, but in sloppy graphical models it is traceable to solve LP relaxation of MAP inferences using graph sections or message transmission techniques (Boykov & Kolmogorov, 2004; Globerakson, 2008)."}, {"heading": "6.3 MULTI-LABEL CLASSIFICATION", "text": "The simplest approach to multi-label classification is to independently predict each label yi using a separate classifier, also known as a \"binary relevance model\" (Tsoumakas & Katakis, 2006), but this approach can work poorly, especially if certain labels are rare or some are highly correlated. Modeling improvements use label margins or ranking losses that relate directly to label structure with multiple labels (Elisseeff & Weston, 2001; Godbole & Sarawagi, 2004; Zhang & Zhou, 2006; Bucak et al., 2009).Correlations between labels can be explicitly modelled using low-label models (Ji & Ye, 2009; Cabral et al., 2011; Xu et al., 2014; Bhatia et al., 2015). This can be achieved, for example, by making predictions with low parameters."}, {"heading": "7 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 MULTI-LABEL CLASSIFICATION BENCHMARKS", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "7.2 PERFORMING STRUCTURE LEARNING USING SPENS", "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "7.3 ANALYZING THE EFFECT OF SEARCH ERRORS ON SSVM TRAINING", "text": "Due to scalability considerations, previous multi-classification applications of CRFs have been limited to much smaller L's than those in Table 1. In Table 3, we look at the 14-label yeast dataset (Elisseeff & Weston, 2001), which is the largest labeling space matching a CRF in Finley & Joachims (2008) and Meshi et al. (2010). In Table 3, we analyze the impact of inaccurate predictions on SSVM training and test time prediction. Table 3, however, looks at greedy predictions, loopy belief propagation, accurate predictions with an ILP solver, the solution of LP relaxation, and SPENs, where the same technique is applied at train and test time.All results, except SPENs, are from Finley & Joachims (2008), which also consider cases where different methods are used in the train versus test."}, {"heading": "8 CONCLUSION AND FUTURE WORK", "text": "Structured prediction energy networks use deep architectures to perform representative learning for structured objects, together over x and across architecture. This provides easy prediction using gradient descent and an expressive, interpretable framework for energy functionality. We believe that more accurate models can be trained using limited data using the energy-based approach, due to superior parsimony and better opportunities for practitioners to inject domain knowledge. Deep networks have changed our ability to learn hierarchies of features for input into signal processing problems, such as computer vision and speech recognition. SPENs offer a step toward applying this feature learning revolution to the results of structured prediction. Such modeling provides the ability to improve accuracy on a variety of problems with rich dependencies between starting materials.We have found that SPEN predictions are often rated 0 or 1, despite optimizing a non-concessional prediction of energy, we expect to match the amount of energy generated from this prediction to [1]."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Nvidia for a generous hardware grant, supported in part by the Center for Intelligent Information Retrieval and in part by the NSF grant # CNS-0958392. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor."}, {"heading": "A APPENDIX", "text": "In fact, it is such that the greater people who are able to move, move, move, move, move, move, move, move, move, move, move, move, move, move, process, process, process, process, process, process, process, process, process, process, process, process, process, process, process, process, process, process, process, process, process, accelerate, move, accelerate, accelerate, accelerate, process, accelerate, process, accelerate, process, accelerate, accelerate, accelerate, process, accelerate, accelerate, accelerate, accelerate, accelerate, accelerate, accelerate, accelerate, accelerate, accelerate, accelerate, accelerate, accelerate, accelerate, accelerate, destruct, accelerate, destruct, accelerate, accelerate, destruct, destruct, accelerate, destruct, accelerate, destruct, accelerate, destruct, accelerate, accelerate, destruct, accelerate, accelerate, destruct, accelerate, accelerate, destruct, accelerate, destruct, accelerate, accelerate, accelerate, destruct, accelerate, accelerate, accelerate, destruct, accelerate, accelerate, accelerate, accelerate, destruct, accelerate, accelerate, accelerate, accelerate, destruct, accelerate, accelerate, accelerate, destruct, accelerate, accelerate, accelerate, destruct, accelerate, accelerate, accelerate, destruct, accelerate, accelerate, accelerate, accelerate, destruct, accelerate, accelerate, accelerate, destruct, accelerate, accelerate, accelerate, accelerate, destruct, accelerate, accelerate, accelerate, destruct, accelerate, accelerate, accelerate, destruct, accelerate, accelerate, accelerate, destruct, accelerate, destruct, accelerate, accelerate, accelerate, destruct, accelerate, accelerate, accelerate, accelerate, destruct, accelerate, accelerate, accelerate, accelerate, accelerate, accelerate, destruct, accelerate, destruct, accelerate, accelerate, accelerate, accelerate, accelerate,"}], "references": [{"title": "Multi-label learning with millions of labels: Recommending advertiser bid phrases for web pages", "author": ["Agrawal", "Rahul", "Gupta", "Archit", "Prabhu", "Yashoteja", "Varma", "Manik"], "venue": "In Proceedings of the 22nd international conference on World Wide Web, pp. 13\u201324. International World Wide Web Conferences Steering Committee,", "citeRegEx": "Agrawal et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2013}, {"title": "Mirror descent and nonlinear projected subgradient methods for convex optimization", "author": ["Beck", "Amir", "Teboulle", "Marc"], "venue": "Operations Research Letters,", "citeRegEx": "Beck et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Beck et al\\.", "year": 2003}, {"title": "Locally nonlinear embeddings for extreme multi-label learning", "author": ["Bhatia", "Kush", "Jain", "Himanshu", "Kar", "Purushottam", "Prateek", "Varma", "Manik"], "venue": "CoRR, abs/1507.02743,", "citeRegEx": "Bhatia et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bhatia et al\\.", "year": 2015}, {"title": "An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision", "author": ["Boykov", "Yuri", "Kolmogorov", "Vladimir"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Boykov et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Boykov et al\\.", "year": 2004}, {"title": "Signature verification using a siamese time delay neural network", "author": ["Bromley", "Jane", "Bentz", "James W", "Bottou", "L\u00e9on", "Guyon", "Isabelle", "LeCun", "Yann", "Moore", "Cliff", "S\u00e4ckinger", "Eduard", "Shah", "Roopak"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence,", "citeRegEx": "Bromley et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Bromley et al\\.", "year": 1993}, {"title": "Efficient multi-label ranking for multi-class learning: application to object recognition", "author": ["Bucak", "Serhat S", "Mallapragada", "Pavan Kumar", "Jin", "Rong", "Jain", "Anil K"], "venue": "In Computer Vision,", "citeRegEx": "Bucak et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bucak et al\\.", "year": 2009}, {"title": "Multi-label learning with incomplete class assignments", "author": ["Bucak", "Serhat Selcuk", "Jin", "Rong", "Jain", "Anil K"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Bucak et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bucak et al\\.", "year": 2011}, {"title": "Matrix completion for multi-label image classification", "author": ["Cabral", "Ricardo S", "Torre", "Fernando", "Costeira", "Jo\u00e3o P", "Bernardino", "Alexandre"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Cabral et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cabral et al\\.", "year": 2011}, {"title": "Human pose estimation with iterative error feedback", "author": ["Carreira", "Joao", "Agrawal", "Pulkit", "Fragkiadaki", "Katerina", "Malik", "Jitendra"], "venue": "arXiv preprint arXiv:1507.06550,", "citeRegEx": "Carreira et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Carreira et al\\.", "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["Collobert", "Ronan", "Weston", "Jason", "Bottou", "L\u00e9on", "Karlen", "Michael", "Kavukcuoglu", "Koray", "Kuksa", "Pavel"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Learning graphical model parameters with approximate marginal inference", "author": ["Domke", "Jens"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Domke and Jens.,? \\Q2013\\E", "shortCiteRegEx": "Domke and Jens.", "year": 2013}, {"title": "A kernel method for multi-labelled classification", "author": ["Elisseeff", "Andr\u00e9", "Weston", "Jason"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Elisseeff et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Elisseeff et al\\.", "year": 2001}, {"title": "Training structural svms when exact inference is intractable", "author": ["Finley", "Thomas", "Joachims", "Thorsten"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Finley et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Finley et al\\.", "year": 2008}, {"title": "A neural algorithm of artistic", "author": ["Gatys", "Leon A", "Ecker", "Alexander S", "Bethge", "Matthias"], "venue": "style. CoRR,", "citeRegEx": "Gatys et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gatys et al\\.", "year": 2015}, {"title": "Texture synthesis using convolutional neural networks", "author": ["Gatys", "Leon A", "Ecker", "Alexander S", "Bethge", "Matthias"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Gatys et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gatys et al\\.", "year": 2015}, {"title": "Collective multi-label classification", "author": ["Ghamrawi", "Nadia", "McCallum", "Andrew"], "venue": "In Proceedings of the 14th ACM international conference on Information and knowledge management,", "citeRegEx": "Ghamrawi et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ghamrawi et al\\.", "year": 2005}, {"title": "Fixing max-product: Convergent message passing algorithms for map lp-relaxations", "author": ["Globerson", "Amir", "Jaakkola", "Tommi S"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Globerson et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Globerson et al\\.", "year": 2008}, {"title": "Discriminative methods for multi-labeled classification", "author": ["Godbole", "Shantanu", "Sarawagi", "Sunita"], "venue": "In Advances in Knowledge Discovery and Data Mining,", "citeRegEx": "Godbole et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Godbole et al\\.", "year": 2004}, {"title": "Explaining and harnessing adversarial examples", "author": ["Goodfellow", "Ian J", "Shlens", "Jonathon", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1412.6572,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Large scale maxmargin multi-label classification with priors", "author": ["Hariharan", "Bharath", "Zelnik-Manor", "Lihi", "Varma", "Manik", "Vishwanathan", "Svn"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Hariharan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hariharan et al\\.", "year": 2010}, {"title": "Deep unfolding: Model-based inspiration of novel deep architectures", "author": ["Hershey", "John R", "Roux", "Jonathan Le", "Weninger", "Felix"], "venue": "arXiv preprint arXiv:1409.2574,", "citeRegEx": "Hershey et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hershey et al\\.", "year": 2014}, {"title": "Multi-label prediction via compressed sensing", "author": ["Hsu", "Daniel", "Kakade", "Sham", "Langford", "John", "Zhang", "Tong"], "venue": "In NIPS,", "citeRegEx": "Hsu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2009}, {"title": "Bidirectional LSTM-CRF models for sequence tagging", "author": ["Huang", "Zhiheng", "Xu", "Wei", "Yu", "Kai"], "venue": "CoRR, abs/1508.01991,", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Consistent label tree classifiers for extreme multi-label classification", "author": ["Jasinska", "Kalina", "Dembczyski", "Krzysztof"], "venue": "In ICML 2015 Workshop on Extreme Classification,", "citeRegEx": "Jasinska et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jasinska et al\\.", "year": 2015}, {"title": "A fast variational approach for learning markov random field language models", "author": ["Jernite", "Yacine", "Rush", "Alexander M", "Sontag", "David"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,", "citeRegEx": "Jernite et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jernite et al\\.", "year": 2015}, {"title": "Linear dimensionality reduction for multi-label classification", "author": ["Ji", "Shuiwang", "Ye", "Jieping"], "venue": "In IJCAI,", "citeRegEx": "Ji et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2009}, {"title": "An introduction to variational methods for graphical models", "author": ["Jordan", "Michael I", "Ghahramani", "Zoubin", "Jaakkola", "Tommi S", "Saul", "Lawrence K"], "venue": "Machine learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Multilabel classification using bayesian compressed sensing", "author": ["Kapoor", "Ashish", "Viswanathan", "Raajay", "Jain", "Prateek"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kapoor et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kapoor et al\\.", "year": 2012}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["Koller", "Daphne", "Friedman", "Nir"], "venue": "MIT press,", "citeRegEx": "Koller et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Koller et al\\.", "year": 2009}, {"title": "Structured learning with approximate inference", "author": ["Kulesza", "Alex", "Pereira", "Fernando"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Kulesza et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kulesza et al\\.", "year": 2007}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Lafferty", "John D", "McCallum", "Andrew", "Pereira", "Fernando CN"], "venue": "In Proceedings of the Eighteenth International Conference on Machine Learning,", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Quoc", "Mikolov", "Tomas"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "A tutorial on energy-based learning", "author": ["LeCun", "Yann", "Chopra", "Sumit", "Hadsell", "Raia", "M Ranzato", "F. Huang"], "venue": "Predicting structured data,", "citeRegEx": "LeCun et al\\.,? \\Q2006\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2006}, {"title": "Multi-label learning with posterior regularization", "author": ["Lin", "Victoria (Xi", "Singh", "Sameer", "He", "Luheng", "Taskar", "Ben", "Zettlemoyer", "Luke"], "venue": "In NIPS Workshop on Modern Machine Learning and Natural Language Processing,", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Long", "Jonathan", "Shelhamer", "Evan", "Darrell", "Trevor"], "venue": "CVPR (to appear),", "citeRegEx": "Long et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Long et al\\.", "year": 2015}, {"title": "Gradient-based hyperparameter optimization through reversible learning", "author": ["Maclaurin", "Dougal", "Duvenaud", "David", "Adams", "Ryan P"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Maclaurin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Maclaurin et al\\.", "year": 2015}, {"title": "Learning efficiently with approximate inference via dual losses", "author": ["Meshi", "Ofer", "Sontag", "David", "Globerson", "Amir", "Jaakkola", "Tommi S"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Meshi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Meshi et al\\.", "year": 2010}, {"title": "Inceptionism: Going deeper into neural networks", "author": ["Mordvintsev", "Alexander", "Olah", "Christopher", "Tyka", "Mike"], "venue": "URL http://googleresearch.blogspot.com/2015/ 06/inceptionism-going-deeper-into-neural.html", "citeRegEx": "Mordvintsev et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mordvintsev et al\\.", "year": 2015}, {"title": "Label filters for large scale multilabel classification", "author": ["Niculescu-Mizil", "Alexandru", "Abbasnejad", "Ehsan"], "venue": "In ICML 2015 Workshop on Extreme Classification,", "citeRegEx": "Niculescu.Mizil et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Niculescu.Mizil et al\\.", "year": 2015}, {"title": "Submodular multi-label learning", "author": ["Petterson", "James", "Caetano", "Tib\u00e9rio S"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Petterson et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Petterson et al\\.", "year": 2011}, {"title": "Classifier chains for multi-label classification", "author": ["Read", "Jesse", "Pfahringer", "Bernhard", "Holmes", "Geoff", "Frank", "Eibe"], "venue": "Machine learning,", "citeRegEx": "Read et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Read et al\\.", "year": 2011}, {"title": "Learning distributed representations for structured output prediction", "author": ["Srikumar", "Vivek", "Manning", "Christopher D"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Srikumar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srikumar et al\\.", "year": 2014}, {"title": "An introduction to conditional random fields", "author": ["Sutton", "Charles", "McCallum", "Andrew"], "venue": "Machine Learning,", "citeRegEx": "Sutton et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2011}, {"title": "Intriguing properties of neural networks", "author": ["Szegedy", "Christian", "Zaremba", "Wojciech", "Sutskever", "Ilya", "Bruna", "Joan", "Erhan", "Dumitru", "Goodfellow", "Ian", "Fergus", "Rob"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Support vector machine learning for interdependent and structured output spaces", "author": ["Tsochantaridis", "Ioannis", "Hofmann", "Thomas", "Joachims", "Thorsten", "Altun", "Yasemin"], "venue": "In Proceedings of the twenty-first international conference on Machine learning,", "citeRegEx": "Tsochantaridis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2004}, {"title": "Multi-label classification: An overview", "author": ["Tsoumakas", "Grigorios", "Katakis", "Ioannis"], "venue": "Dept. of Informatics, Aristotle University of Thessaloniki,", "citeRegEx": "Tsoumakas et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Tsoumakas et al\\.", "year": 2006}, {"title": "Grammar as a foreign language", "author": ["Vinyals", "Oriol", "Kaiser", "Lukasz", "Koo", "Terry", "Petrov", "Slav", "Sutskever", "Ilya", "Hinton", "Geoffrey"], "venue": "In CoRR.,", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Learning low-rank label correlations for multi-label classification with missing labels", "author": ["Xu", "Linli", "Wang", "Zhen", "Shen", "Zefan", "Yubo", "Chen", "Enhong"], "venue": "In Data Mining (ICDM),", "citeRegEx": "Xu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Large-scale multi-label learning with missing labels", "author": ["Yu", "Hsiang-Fu", "Jain", "Prateek", "Kar", "Purushottam", "Dhillon", "Inderjit S"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Multilabel neural networks with applications to functional genomics and text categorization", "author": ["Zhang", "Min-Ling", "Zhou", "Zhi-Hua"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "Zhang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2006}, {"title": "Conditional random fields as recurrent neural networks", "author": ["Zheng", "Shuai", "Jayasumana", "Sadeep", "Romera-Paredes", "Bernardino", "Vineet", "Vibhav", "Su", "Zhizhong", "Du", "Dalong", "Huang", "Chang", "Torr", "Philip"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "Zheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 32, "context": "We consider two principal approaches to structured prediction: (a) as a feed-forward function y = f(x), and (b) using an energy-based viewpoint y = arg miny\u2032 Ex(y) (LeCun et al., 2006).", "startOffset": 164, "endOffset": 184}, {"referenceID": 9, "context": "Feed-forward approaches include, for example, predictors using local convolutions plus a classification layer (Collobert et al., 2011), fully-convolutional networks (Long et al.", "startOffset": 110, "endOffset": 134}, {"referenceID": 34, "context": ", 2011), fully-convolutional networks (Long et al., 2015), or sequenceto-sequence predictors (Vinyals et al.", "startOffset": 38, "endOffset": 57}, {"referenceID": 46, "context": ", 2015), or sequenceto-sequence predictors (Vinyals et al., 2014).", "startOffset": 43, "endOffset": 65}, {"referenceID": 30, "context": "In contrast, the energy-based approach may involve non-trivial optimization to perform predictions, and includes, for example, conditional random fields (CRFs) (Lafferty et al., 2001).", "startOffset": 160, "endOffset": 183}, {"referenceID": 32, "context": "For example, in a CRF, the tables of potentials of an undirected graphical model can be parametrized via a deep network applied to x (LeCun et al., 2006; Collobert et al., 2011; Huang et al., 2015).", "startOffset": 133, "endOffset": 197}, {"referenceID": 9, "context": "For example, in a CRF, the tables of potentials of an undirected graphical model can be parametrized via a deep network applied to x (LeCun et al., 2006; Collobert et al., 2011; Huang et al., 2015).", "startOffset": 133, "endOffset": 197}, {"referenceID": 22, "context": "For example, in a CRF, the tables of potentials of an undirected graphical model can be parametrized via a deep network applied to x (LeCun et al., 2006; Collobert et al., 2011; Huang et al., 2015).", "startOffset": 133, "endOffset": 197}, {"referenceID": 44, "context": "The parameters of the network are trained using an adaptation of a structured SVM (Taskar et al., 2004; Tsochantaridis et al., 2004).", "startOffset": 82, "endOffset": 132}, {"referenceID": 36, "context": "using CRFs, have been limited to notably smaller problems than our experiments consider, since the techniques\u2019 computational complexity either grows super-linearly in the number of labels L (Ghamrawi & McCallum, 2005; Finley & Joachims, 2008; Meshi et al., 2010; Petterson & Caetano, 2011), or requires strict assumptions about the dependencies between labels (Read et al.", "startOffset": 190, "endOffset": 289}, {"referenceID": 40, "context": ", 2010; Petterson & Caetano, 2011), or requires strict assumptions about the dependencies between labels (Read et al., 2011; Jasinska & Dembczyski, 2015; Niculescu-Mizil & Abbasnejad, 2015).", "startOffset": 105, "endOffset": 189}, {"referenceID": 26, "context": "In the posterior inference literature, mean-field approaches also consider a relaxation from y to \u0233, where \u0233i would be interpreted as the marginal probability that yi = 1 (Jordan et al., 1999).", "startOffset": 171, "endOffset": 192}, {"referenceID": 30, "context": "There are important parallels between the example SPEN architecture given above and the parametrization of a CRF (Lafferty et al., 2001; Sutton & McCallum, 2011).", "startOffset": 113, "endOffset": 161}, {"referenceID": 24, "context": "(Srikumar & Manning, 2014; Jernite et al., 2015), or using a deep architecture to map x to a table of CRF potentials (LeCun et al.", "startOffset": 0, "endOffset": 48}, {"referenceID": 32, "context": ", 2015), or using a deep architecture to map x to a table of CRF potentials (LeCun et al., 2006).", "startOffset": 76, "endOffset": 96}, {"referenceID": 44, "context": "A popular technique in these settings is the structured support vector machine (SSVM) (Taskar et al., 2004; Tsochantaridis et al., 2004).", "startOffset": 86, "endOffset": 136}, {"referenceID": 4, "context": "However, backpropagation-based prediction has been useful in a variety of deep learning applications, including siamese networks (Bromley et al., 1993), methods for generating adversarial examples (Szegedy et al.", "startOffset": 129, "endOffset": 151}, {"referenceID": 43, "context": ", 1993), methods for generating adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2014), methods for embedding documents as dense vectors (Le & Mikolov, 2014), and successful techniques for image generation and texture synthesis (Mordvintsev et al.", "startOffset": 53, "endOffset": 100}, {"referenceID": 18, "context": ", 1993), methods for generating adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2014), methods for embedding documents as dense vectors (Le & Mikolov, 2014), and successful techniques for image generation and texture synthesis (Mordvintsev et al.", "startOffset": 53, "endOffset": 100}, {"referenceID": 8, "context": "In concurrent work, (Carreira et al., 2015) propose an iterative structured prediction method for human pose estimation, where Ex(y), doesn\u2019t return a number, but instead an increment \u2206(x, y).", "startOffset": 20, "endOffset": 43}, {"referenceID": 20, "context": "An alternative line of work has constructed feed-forward predictors using energy-based models as motivation (Domke, 2013; Hershey et al., 2014; Zheng et al., 2015).", "startOffset": 108, "endOffset": 163}, {"referenceID": 50, "context": "An alternative line of work has constructed feed-forward predictors using energy-based models as motivation (Domke, 2013; Hershey et al., 2014; Zheng et al., 2015).", "startOffset": 108, "endOffset": 163}, {"referenceID": 5, "context": "Modeling improvements use max-margin or ranking losses that directly address the multi-label structure (Elisseeff & Weston, 2001; Godbole & Sarawagi, 2004; Zhang & Zhou, 2006; Bucak et al., 2009).", "startOffset": 103, "endOffset": 195}, {"referenceID": 7, "context": "Correlations between labels can be modeled explicitly using models with low-dimensional embeddings of labels (Ji & Ye, 2009; Cabral et al., 2011; Yu et al., 2014; Xu et al., 2014; Bhatia et al., 2015).", "startOffset": 109, "endOffset": 200}, {"referenceID": 48, "context": "Correlations between labels can be modeled explicitly using models with low-dimensional embeddings of labels (Ji & Ye, 2009; Cabral et al., 2011; Yu et al., 2014; Xu et al., 2014; Bhatia et al., 2015).", "startOffset": 109, "endOffset": 200}, {"referenceID": 47, "context": "Correlations between labels can be modeled explicitly using models with low-dimensional embeddings of labels (Ji & Ye, 2009; Cabral et al., 2011; Yu et al., 2014; Xu et al., 2014; Bhatia et al., 2015).", "startOffset": 109, "endOffset": 200}, {"referenceID": 2, "context": "Correlations between labels can be modeled explicitly using models with low-dimensional embeddings of labels (Ji & Ye, 2009; Cabral et al., 2011; Yu et al., 2014; Xu et al., 2014; Bhatia et al., 2015).", "startOffset": 109, "endOffset": 200}, {"referenceID": 36, "context": "However, these techniques\u2019 computational complexity grows super-linearly in L (Ghamrawi & McCallum, 2005; Finley & Joachims, 2008; Meshi et al., 2010; Petterson & Caetano, 2011) , or requires practitioners to impose strict assumption about the dependencies between labels (Read et al.", "startOffset": 78, "endOffset": 177}, {"referenceID": 40, "context": ", 2010; Petterson & Caetano, 2011) , or requires practitioners to impose strict assumption about the dependencies between labels (Read et al., 2011; Jasinska & Dembczyski, 2015; Niculescu-Mizil & Abbasnejad, 2015).", "startOffset": 129, "endOffset": 213}, {"referenceID": 21, "context": "Our parametrization of the label energy network (5) in terms of linear measurements of the labels is inspired by prior approaches using compressed sensing and error-correcting codes for multi-label classification (Hsu et al., 2009; Hariharan et al., 2010; Kapoor et al., 2012).", "startOffset": 213, "endOffset": 276}, {"referenceID": 19, "context": "Our parametrization of the label energy network (5) in terms of linear measurements of the labels is inspired by prior approaches using compressed sensing and error-correcting codes for multi-label classification (Hsu et al., 2009; Hariharan et al., 2010; Kapoor et al., 2012).", "startOffset": 213, "endOffset": 276}, {"referenceID": 27, "context": "Our parametrization of the label energy network (5) in terms of linear measurements of the labels is inspired by prior approaches using compressed sensing and error-correcting codes for multi-label classification (Hsu et al., 2009; Hariharan et al., 2010; Kapoor et al., 2012).", "startOffset": 213, "endOffset": 276}, {"referenceID": 33, "context": "When yi = 0, they are treated as missing data, whose values can be imputed using assumptions about the rank (Lin et al., 2014) or sparsity (Bucak et al.", "startOffset": 108, "endOffset": 126}, {"referenceID": 6, "context": ", 2014) or sparsity (Bucak et al., 2011; Agrawal et al., 2013) of the matrix of training labels.", "startOffset": 20, "endOffset": 62}, {"referenceID": 0, "context": ", 2014) or sparsity (Bucak et al., 2011; Agrawal et al., 2013) of the matrix of training labels.", "startOffset": 20, "endOffset": 62}, {"referenceID": 33, "context": "For example, the approach of (Lin et al., 2014) achieves 44.", "startOffset": 29, "endOffset": 47}, {"referenceID": 44, "context": "LR: the low-rank-weights method of Yu et al. (2014). All results besides MLP and SPEN, are taken from Lin et al.", "startOffset": 35, "endOffset": 52}, {"referenceID": 30, "context": "All results besides MLP and SPEN, are taken from Lin et al. (2014). We report the \u2018example averaged\u2019 F1 measure.", "startOffset": 49, "endOffset": 67}, {"referenceID": 30, "context": "All results besides MLP and SPEN, are taken from Lin et al. (2014). We report the \u2018example averaged\u2019 F1 measure. For Bibtex and Delicious, we tune parameters by first jack-knifing a separate train-test split. For Bookmarks, we use the same train-dev-test split as Lin et al. (2014). For SPENs, we obtain predictions by rounding \u0233i above a threshold tuned on held-out data.", "startOffset": 49, "endOffset": 282}, {"referenceID": 36, "context": "the 14-label yeast dataset (Elisseeff & Weston, 2001), which is the largest label space fit using a CRF in Finley & Joachims (2008) and Meshi et al. (2010). Finley & Joachims (2008) analyze the effects of inexact prediction on SSVM training and on test-time prediction.", "startOffset": 136, "endOffset": 156}, {"referenceID": 36, "context": "the 14-label yeast dataset (Elisseeff & Weston, 2001), which is the largest label space fit using a CRF in Finley & Joachims (2008) and Meshi et al. (2010). Finley & Joachims (2008) analyze the effects of inexact prediction on SSVM training and on test-time prediction.", "startOffset": 136, "endOffset": 182}, {"referenceID": 36, "context": "the 14-label yeast dataset (Elisseeff & Weston, 2001), which is the largest label space fit using a CRF in Finley & Joachims (2008) and Meshi et al. (2010). Finley & Joachims (2008) analyze the effects of inexact prediction on SSVM training and on test-time prediction. Table 3 considers greedy prediction, loopy belief propagation, exact prediction using an ILP solver, solving the LP relaxation, and SPENs, where the same technique is used at train and test time. All results, besides SPENs, are from Finley & Joachims (2008), which also considers cases where different methods are used in train vs.", "startOffset": 136, "endOffset": 528}, {"referenceID": 36, "context": "the 14-label yeast dataset (Elisseeff & Weston, 2001), which is the largest label space fit using a CRF in Finley & Joachims (2008) and Meshi et al. (2010). Finley & Joachims (2008) analyze the effects of inexact prediction on SSVM training and on test-time prediction. Table 3 considers greedy prediction, loopy belief propagation, exact prediction using an ILP solver, solving the LP relaxation, and SPENs, where the same technique is used at train and test time. All results, besides SPENs, are from Finley & Joachims (2008), which also considers cases where different methods are used in train vs. test. We report hamming error, using 10-fold cross validation. A key argument of Finley & Joachims (2008) is that SSVM training is more effective when the traintime inference method will not under-generate margin violations.", "startOffset": 136, "endOffset": 708}, {"referenceID": 35, "context": "For example, we can apply the technique of Maclaurin et al. (2015) to update the model parameters by differentiating, with respect to the model parameters \u03a8, the process of performing iterative gradient-based optimization with respect to \u0233.", "startOffset": 43, "endOffset": 67}], "year": 2017, "abstractText": "We introduce structured prediction energy networks (SPENs), a flexible framework for structured prediction. A deep architecture is used to define an energy function of candidate labels, and then predictions are produced by using backpropagation to iteratively optimize the energy with respect to the labels. This deep architecture captures dependencies between labels that would lead to intractable graphical models, and performs structure learning by automatically learning discriminative features of the structured output. One natural application of our technique is multi-label classification, which traditionally has required strict prior assumptions about the interactions between labels to ensure tractable learning and prediction problems. We are able to apply SPENs to multi-label problems with substantially larger label sets than previous applications of structured prediction, while modeling high-order interactions using minimal structural assumptions. Overall, deep learning provides remarkable tools for learning features of the inputs to a prediction problem, and this work extends these techniques to learning features of the outputs. Our experiments provide impressive performance on a variety of benchmark multi-label classification tasks, demonstrate that our technique can be used to provide interpretable structure learning, and illuminate fundamental trade-offs between feed-forward and iterative structured prediction techniques.", "creator": "LaTeX with hyperref package"}}}