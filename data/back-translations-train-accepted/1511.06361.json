{"id": "1511.06361", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Order-Embeddings of Images and Language", "abstract": "Hypernymy, textual entailment, and image captioning can be seen as special cases of a single visual-semantic hierarchy over words, sentences, and images. In this paper we advocate for explicitly modeling the partial order structure of this hierarchy. Towards this goal, we introduce a general method for learning ordered representations, and show how it can be applied to a variety of tasks involving images and language. We show that the resulting representations improve performance over current approaches for hypernym prediction and image-caption retrieval.", "histories": [["v1", "Thu, 19 Nov 2015 20:56:14 GMT  (4288kb,D)", "http://arxiv.org/abs/1511.06361v1", null], ["v2", "Tue, 8 Dec 2015 21:19:30 GMT  (4290kb,D)", "http://arxiv.org/abs/1511.06361v2", null], ["v3", "Thu, 10 Dec 2015 04:32:53 GMT  (4290kb,D)", "http://arxiv.org/abs/1511.06361v3", null], ["v4", "Thu, 7 Jan 2016 04:58:08 GMT  (4290kb,D)", "http://arxiv.org/abs/1511.06361v4", null], ["v5", "Sun, 17 Jan 2016 03:08:20 GMT  (4290kb,D)", "http://arxiv.org/abs/1511.06361v5", null], ["v6", "Tue, 1 Mar 2016 08:23:50 GMT  (4290kb,D)", "http://arxiv.org/abs/1511.06361v6", "ICLR camera-ready version"]], "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.CV", "authors": ["ivan vendrov", "ryan kiros", "sanja fidler", "raquel urtasun"], "accepted": true, "id": "1511.06361"}, "pdf": {"name": "1511.06361.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Ivan Vendrov", "Ryan Kiros", "Sanja Fidler", "Raquel Urtasun"], "emails": ["vendrov@cs.toronto.edu", "rkiros@cs.toronto.edu", "fidler@cs.toronto.edu", "urtasun@cs.toronto.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, most of them are able to survive themselves without being able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...)"}, {"heading": "2 LEARNING ORDER-EMBEDDINGS", "text": "In order to unify our treatment of various tasks, we introduce the problem of partial completion of the order. In partial completion of the order, we are given a set of positive examples P = {(u, v)} of ordered pairs drawn from a partial completion of the order (X, X), and a set of negative examples N, which we know to be disordered. Our goal is to predict whether an invisible pair (u \u2032, v \u2032) is ordered. Note that hypernym prediction, caption image restoration, and textual alienation are all special cases of this task, as they all allow the classification of pairs of terms in the (partially ordered) visual-semantic hierarchy. We address this problem by embedding an image of X in a partially ordered order of hierarchical spaces (Y, Y).The idea is to include the order of an invisible pair of terms in the (partially ordered visual) hierarchical hierarchies of hierarchical spaces (by partially integrating) the hierarchical order of the hierarchical ones."}, {"heading": "2.2 PENALIZING ORDER VIOLATIONS", "text": "Once we have determined the embedding space and the order, we now consider the problem of embedding an order in this space. In practice, the embedding condition of the order (definition 1) is too restrictive to impose as a hard constraint. Instead, we strive to find an approximate embedding of the order: a figure that violates the condition of embedding the order, which is imposed as a soft constraint, as little as possible. More precisely, we define a penalty that measures the degree to which a pair of points violates the product order. In particular, we define the penalty for an ordered pair of points (x, y) in RN + asE (x, y) = | max (0, y \u2212 x) | 2 (2) Decisive is E (x, y) depending on the reversed product order."}, {"heading": "3 HYPERNYM PREDICTION", "text": "To test our model's ability to learn sub-orders from incomplete data, our first task is to predict withheld hypernym pairs in WordNet (Miller, 1995).A hypernym pair is a pair of terms where the first word is a specialization or instance of the second, e.g. (woman, person) or (New York, city).Our setup is very different from previous work in that we only use the WordNet hierarchy as training data.The most similar evaluation was that of Baroni et al. (2012), who use external linguistic data in the form of distribution semantic vectors.Bordes et al. (2011) and Socher et al. (2013) also evaluate the WordNet hierarchy, but use other relationships in WordNet as training data (and external linguistic data, in Socher's case).In addition, the latter two consider only direct hypernymes and not the complete transitive hypernymorrelation."}, {"heading": "3.1 LOSS FUNCTION", "text": "To apply order embedding to hypernymia, we follow the setup of Socher et al. (2013) by learning a N-dimensional vector for each concept in WordNet, but replacing their neural tensor network with our penalty for administrative violations defined in Equation (2). Just as they do, we corrupt each hypernymphomaniac pair by replacing one of the two concepts with a randomly selected concept, and use these corrupt pairs as negative examples for both training and evaluation. We use their maximum margin loss, which sets the penalty for administrative violations to zero for positive examples and to more than one margin \u03b1 for negative examples: \u2022 (u, v \u2032) \u2022 WordNetE (f (v) + max {0, \u03b1 \u2212 E (f (u \u2032), f (v \u2032)}, (4), where E is our penalty for administrative violations and (u \u2032, v \u2032) a corrupt version of (u)."}, {"heading": "3.2 DATASET", "text": "The transitive closure of the WordNet hierarchy gives us 838,073 edges between 82,192 concepts in WordNet. Like Bordes et al. (2011), we randomly select 4,000 edges for the test split and another 4,000 for the development set. Note that the majority of the edges of the test set can be derived simply by applying transitivity, which gives us a strong baseline."}, {"heading": "3.3 DETAILS OF TRAINING", "text": "We learn a 50-dimensional nonnegative vector for each concept in WordNet using the max margin objective (4) with margin \u03b1 = 1, sampling 500 real and 500 false hypernym pairs per batch. We train for 30-50 eras with the Adam Optimizer (Kingma & Ba, 2015) at a learning rate of 0.01 and an early stop of the validation rate. During the evaluation, we find the optimal classification threshold on the validation line and then apply it to the testset.3.4 RESULTSBecause our setup is new, there are no published numbers to compare with. We therefore compare three variants of our model with two baselines, with results that are in Table 1.The transitive closure baseline does not include learning; it simply classifies hypernym pairs as positive when they are in the transitive completion of the fusion of edges in education and validation diagrams."}, {"heading": "4 CAPTION-IMAGE RETRIEVAL", "text": "Caption-image-retrieval task has become a standard assessment of common image and language models (Hodosh et al., 2013; Lin et al., 2014a). The task is to evaluate a large dataset of images by relevance to a query retrieval and captions by relevance to a query retrieval. Given a series of aligned picture-caption pairs as training data, the goal is then to learn a Caption-image compatibility score S (c, i) used at the time of testing. Many modern approaches model the caption-image relationship symmetrically, either by embedding it in a common \"visual-semantic\" space with internal product similarity (Socher et al., 2014; Kiros et al., 2014) or by using canonical correlations between distributed images and captions (Klein et al, 2015)."}, {"heading": "4.1 LOSS FUNCTION", "text": "To facilitate comparison, we use the same pair-by-pair ranking loss that Socher et al. (2014), Kiros et al. (2014) and Karpathy & Li (2015) used in this task - simply replacing their symmetrical measure of similarity with our asymmetrical punishment for order violations. These loss functions encourage S (c, i) to make image and image pairs larger for the truth on the ground than for all other pairs, by one margin: \u2211 (c, i) (\u2211 c \u00b2 max {0, \u03b1 \u2212 S (c \u2032, i)} + \u2211 i \u00b2 max {0, \u03b1 \u2212 S (c, i \u2212 S (c, i \u2212 c, i \u2032)}) (5), where (c, i) there is a picture and image pair for the truth on the ground, c \u00b2 is a caption that i does not describe, and i \u2032 is an image that is not described by c."}, {"heading": "4.2 IMAGE AND CAPTION EMBEDDINGS", "text": "To learn fc and fi, we use the approach of Kiros et al. (2014), except that (since we embed ourselves in RN +) we limit the embedding vectors to obtain non-negative entries by taking their absolute value. To embed images, we use fi (i) = | Wi \u00b7 CNN (i) | (6), where Wi is a learned N \u00d7 4096 matrix, where N is the dimensionality of the embedding space. CNN (i) is the same image characteristic used by Klein et al. (2015): We re-scale images to have the smallest 256 pixels, we take 224 x 224 sections from the corners, the middle and their horizontal reflections, perform the 10 sections through the 19-layer VGG network of Simonyan & Zisserman (2015) and average their fc7 value. To embed the captions, we use fc, a neural (GRO) activator (2014)."}, {"heading": "4.3 DATASET", "text": "We evaluate the Microsoft COCO dataset (Lin et al., 2014b), which contains more than 120,000 images, each with at least five captions per image, which is by far the largest dataset commonly used to retrieve captions. We use Karpathy & Li data splits (2015) for training (113,287 images), validation (5000 images) and testing (5000 images)."}, {"heading": "4.4 DETAILS OF TRAINING", "text": "To train the model, we use the standard pair ranking objective of Equation (5). We sample 128 random image-caption pairs and extract all the contrasting terms from the minibatch, giving us 127 contrasting images for each image for each caption and caption. We train for 15-30 eras with the Adam Optimizer at the learning rate 0.001 and stop early with the validation set. We set the dimension of the embedding space and the GRU hidden state N to 1024, the dimension of the learned word embedding to 300 and the margin \u03b1 to 0.05. All these hyperparameters, as well as the learning rate and batch size were selected with the validation set. To prevent matching, we limit the labeling and image embedding to the unit L2 standard."}, {"heading": "4.5 RESULTS", "text": "Faced with a query caption or an image, we sort all images or captions of the test set in the order of increasing punishment. We use standard ranking metrics for evaluation. We measure Recall @ K, the percentage of queries for which the GT term is one of the first K; and the middle and middle rank, which is statistics on the position of the GT term in the query order. Table 2 shows a comparison between all current and some older methodologies2 along with our own; see Ma et al. (2015) for a more complete listing. The best results overall are bold, and the best results using 1-crop VGG image characteristics are underlined. Note that the comparison is additionally complicated by the following: \u2022 m-CNNENS is an interaction of four different models, while the other entries are all individual models. \u2022 STV and FV use external text corporations to learn their language characteristics, while the comparison methods used by others are different from those used to evaluate the components."}, {"heading": "4.6 EXPLORATION", "text": "Why would order embeddings do well with such a flat partial order? Why are they much more helpful for image restoration than for image caption restoration? Intuitively, symmetrical embeddings should fail if an image has captions with very different levels of detail because the captions are so dissimilar that it is impossible to map both embeddings close to the same image embedding.Order embeddings do not have this problem: the less detailed captions can be embedded very far away from the image while remaining in the partial order above. To evaluate this intuition, we use the image caption length as a substitute for detail depth and choose among the pairs of simultaneous captions in our validation group the 100 pairs with the largest length difference. To restore images with 1000 target images, the mean rank above picture captions in this group is 6.4 for re-embeddings of this total image order, and if we use the larger captions for some of these larger captions than the 4mm."}, {"heading": "5 TEXTUAL ENTAILMENT / NATURAL LANGUAGE INFERENCE", "text": "An inference from natural language can be seen as a generalization of the hypernymia of words into longer sentences. For example, from \"woman walking her dog in the park\" we can conclude both \"woman walking her dog\" and \"dog in the park,\" but not from \"old woman\" or \"black dog.\" Using two sentences, our task is to predict whether we can derive the second sentence (the hypothesis) from the first (the premise)."}, {"heading": "5.1 LOSS FUNCTION", "text": "To apply order embedding to this task, we consider it again as partial order completion - we can deduce a hypothesis from a premise exactly when the hypothesis is above the premise in the visual-semantic hierarchy. In contrast to our other tasks, for which we had to generate contrastive negatives, data sets for natural language inferences contain negative examples. We can therefore simply use a maximum margin loss: \u0445 (p, h) E (f (p), f (h)) + \u2211 (p \u2032, h \u2032) max {0, \u03b1 \u2212 E (f (p \u2032), f (h \u2032)) (7), where (p, h) are positive and (p \u2032, h \u2032) negative pairs of premise and hypothesis. To embed sentences, we use the same GRU encoder as for the caption restoration task."}, {"heading": "5.2 DATASET", "text": "To evaluate the embedding of order in the task of natural language, we use the recently proposed SNLI corpus (Bowman et al., 2015), which contains 570,000 pairs of sentences, each of which is marked with \"entropy\" if the conclusion is valid, \"contradiction\" if the two sentences contradict, or \"neutral\" if the conclusion is invalid but there is no contradiction. Our method only allows us to distinguish between entropy and non-entropy, so we merge the classes \"contradiction\" and \"neutral\" into our negative examples."}, {"heading": "5.3 IMPLEMENTATION DETAILS", "text": "Just as with the capture image ranking, we set the dimensions of the embedding space and the GRU hidden state to 1024, the dimension of the word embedding to 300, and limit the embedding to the unit L2 standard. We train for 10 epochs with batches of 128 set pairs. We use the Adam Optimizer with learning rate 0.001 and an early end of the validation set. During the evaluation, we find the optimal classification threshold for validation, then we use the threshold to classify the test set."}, {"heading": "5.4 RESULTS", "text": "The most modern method for the 3-class classification on SNLI is that of Rockta \ufffd schel et al. (2015). Unfortunately, they do not calculate 2-class accuracy, so we cannot directly compare them. To facilitate a comparison, we use a challenging baseline that can be evaluated for both the 2-class and 3-class problems. The baseline, known as the skip idea, includes a forward-facing neural network above vectors with skipped thoughts (Kiros et al., 2015), a state-of-the-art semantic representation of sentences. Given the pairs of sentence vectors u and v, input into the network is the concatenation of u, v and the absolute difference | u \u2212 v |. We have adjusted the number of layers, the layer dimensionality and the failure rates of the sentences to optimize performance on the development set."}, {"heading": "6 CONCLUSION AND FUTURE WORK", "text": "We have introduced a simple method of encoding order in learned distributed representations, which allows us to explicitly model the partial order structure of the visual-semantic hierarchy. Our method can easily be integrated into existing methods of relational learning, as we have shown in three challenging tasks related to computer vision and natural language processing. In two of these tasks, our methods surpass all previous work. A promising direction for future work is to learn better classifiers in ImageNet (Deng et al., 2009), which has over 21k image classes arranged by the WordNet hierarchy. Previous approaches, including Frome et al. (2013) and Norouzi et al. (2014), have embedded words and images in a common semantic space with symmetrical similarity - which our experiments suggest is ill-suited to the partial order structure of the visual-semantic hierarchy. We expect significant advances in the image classification and associated problems with zero-image similarity."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Kaustav Kundu for many fruitful discussions during the development of this paper. The work was partially supported by an NSERC graduate scholarship."}, {"heading": "7 SUPPLEMENTARY MATERIAL", "text": "Mikolov et al. (2013) showed that word representations in the learned vector space exhibit semantic regularities, such as king-man + wife-queen. Kiros et al. (2014) showed that similar regularities also apply to common image-language models. We find that order embedding exhibits a novel form of regularity, as in Figure 4. The elementary maximum and minimum operations in the embedding space roughly correspond to composition and abstraction. To get an intuition for what our learned embedding space looks like, we use t-SNE (Van der Maaten & Hinton, 2008) to embed the images and phrases from our first illustration of the visual-semantic hierarchy. Results for both the row embedding and the symmetric cosine spacing line are shown in Figure 5."}], "references": [{"title": "Entailment above the word level in distributional semantics", "author": ["Baroni", "Marco", "Bernardi", "Raffaella", "Do", "Ngoc-Quynh", "Shan", "Chung-chieh"], "venue": "In EACL,", "citeRegEx": "Baroni et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2012}, {"title": "Learning structured embeddings of knowledge bases", "author": ["Bordes", "Antoine", "Weston", "Jason", "Collobert", "Ronan", "Bengio", "Yoshua"], "venue": "In AAAI,", "citeRegEx": "Bordes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2011}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Bowman", "Samuel R", "Angeli", "Gabor", "Potts", "Christopher", "Manning", "Christopher D"], "venue": "In EMNLP,", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoderdecoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "Van Merri\u00ebnboer", "Bart", "Gulcehre", "Caglar", "Bahdanau", "Dzmitry", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "In EMNLP,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning a similarity metric discriminatively, with application to face verification", "author": ["Chopra", "Sumit", "Hadsell", "Raia", "LeCun", "Yann"], "venue": "In CVPR,", "citeRegEx": "Chopra et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chopra et al\\.", "year": 2005}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "Li", "Li-Jia", "Kai", "Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["Frome", "Andrea", "Corrado", "Greg S", "Shlens", "Jon", "Bengio", "Samy", "Dean", "Jeff", "Mikolov", "Tomas"], "venue": "In NIPS,", "citeRegEx": "Frome et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Frome et al\\.", "year": 2013}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Framing image description as a ranking task: Data, models and evaluation", "author": ["Hodosh", "Micah", "Young", "Peter", "Hockenmaier", "Julia"], "venue": "metrics. JAIR,", "citeRegEx": "Hodosh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": null, "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Karpathy", "Andrej", "Li", "Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "In ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["Kiros", "Ryan", "Salakhutdinov", "Ruslan", "Zemel", "Richard S"], "venue": "arXiv preprint arXiv:1411.2539,", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Associating neural word embeddings with deep image representations using fisher vectors", "author": ["Klein", "Benjamin", "Lev", "Guy", "Sadeh", "Gil", "Wolf", "Lior"], "venue": "In CVPR,", "citeRegEx": "Klein et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Visual semantic search: Retrieving videos via complex textual queries", "author": ["Lin", "Dahua", "Fidler", "Sanja", "Kong", "Chen", "Urtasun", "Raquel"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Microsoft coco: Common objects in context", "author": ["Lin", "Tsung-Yi", "Maire", "Michael", "Belongie", "Serge", "Hays", "James", "Perona", "Pietro", "Ramanan", "Deva", "Doll\u00e1r", "Piotr", "Zitnick", "C Lawrence"], "venue": "In ECCV,", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Multimodal convolutional neural networks for matching image and sentence", "author": ["Ma", "Lin", "Lu", "Zhengdong", "Shang", "Lifeng", "Li", "Hang"], "venue": null, "citeRegEx": "Ma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["Mao", "Junhua", "Xu", "Wei", "Yang", "Yi", "Wang", "Jiang", "Yuille", "Alan"], "venue": "In ICLR,", "citeRegEx": "Mao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2015}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Mikolov", "Tomas", "Yih", "Wen-tau", "Zweig", "Geoffrey"], "venue": "In HLT-NAACL,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["Miller", "George A"], "venue": "Communications of the ACM,", "citeRegEx": "Miller and A.,? \\Q1995\\E", "shortCiteRegEx": "Miller and A.", "year": 1995}, {"title": "Zero-shot learning by convex combination of semantic embeddings", "author": ["Norouzi", "Mohammad", "Mikolov", "Tomas", "Bengio", "Samy", "Singer", "Yoram", "Shlens", "Jonathon", "Frome", "Andrea", "Corrado", "Greg S", "Dean", "Jeffrey"], "venue": "In ICLR,", "citeRegEx": "Norouzi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Norouzi et al\\.", "year": 2014}, {"title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-tosentence models", "author": ["Plummer", "Bryan", "Wang", "Liwei", "Cervantes", "Chris", "Caicedo", "Juan", "Hockenmaier", "Julia", "Lazebnik", "Svetlana"], "venue": "arXiv preprint arXiv:1505.04870,", "citeRegEx": "Plummer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Plummer et al\\.", "year": 2015}, {"title": "Reasoning about entailment with neural attention", "author": ["Rockt\u00e4schel", "Tim", "Grefenstette", "Edward", "Hermann", "Karl Moritz", "Ko\u010disk\u1ef3", "Tom\u00e1\u0161", "Blunsom", "Phil"], "venue": "arXiv preprint arXiv:1509.06664,", "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "In ICLR,", "citeRegEx": "Simonyan and Zisserman,? \\Q2015\\E", "shortCiteRegEx": "Simonyan and Zisserman", "year": 2015}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Socher", "Richard", "Chen", "Danqi", "Manning", "Christopher D", "Ng", "Andrew"], "venue": "In NIPS,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Socher", "Richard", "Karpathy", "Andrej", "Le", "Quoc V", "Manning", "Christopher D", "Ng", "Andrew Y"], "venue": "TACL,", "citeRegEx": "Socher et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Visualizing data using t-sne", "author": ["Van der Maaten", "Laurens", "Hinton", "Geoffrey"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Word representations via gaussian embedding", "author": ["Vilnis", "Luke", "McCallum", "Andrew"], "venue": "In ICLR,", "citeRegEx": "Vilnis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vilnis et al\\.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Vinyals", "Oriol", "Toshev", "Alexander", "Bengio", "Samy", "Erhan", "Dumitru"], "venue": "In CVPR,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["Young", "Peter", "Lai", "Alice", "Hodosh", "Micah", "Hockenmaier", "Julia"], "venue": "TACL,", "citeRegEx": "Young et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Young et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 29, "context": "Recent work in vision has moved beyond discriminating between a fixed set of object classes, to automatically generating open-ended lingual descriptions of images (Vinyals et al., 2015).", "startOffset": 163, "endOffset": 185}, {"referenceID": 29, "context": "Recent work in vision has moved beyond discriminating between a fixed set of object classes, to automatically generating open-ended lingual descriptions of images (Vinyals et al., 2015). Recent methods for natural language processing such as Young et al. (2014) learn the semantics of language by grounding it in the visual world.", "startOffset": 164, "endOffset": 262}, {"referenceID": 4, "context": "One line of work, exemplified by Chopra et al. (2005) and first applied to the caption-image relationship by Socher et al.", "startOffset": 33, "endOffset": 54}, {"referenceID": 4, "context": "One line of work, exemplified by Chopra et al. (2005) and first applied to the caption-image relationship by Socher et al. (2014), requires the mapping to be distance-preserving: semantically", "startOffset": 33, "endOffset": 130}, {"referenceID": 1, "context": "Bordes et al. (2011); Socher et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "Bordes et al. (2011); Socher et al. (2013); Ma et al.", "startOffset": 0, "endOffset": 43}, {"referenceID": 1, "context": "Bordes et al. (2011); Socher et al. (2013); Ma et al. (2015). Notably, no existing approach directly imposes the transitivity and antisymmetry of the partial order, leaving the model to induce these properties from data.", "startOffset": 0, "endOffset": 61}, {"referenceID": 30, "context": "The visual-semantic hierarchy can then be seen as a special case of the subset relation, a connection also used by Young et al. (2014).", "startOffset": 115, "endOffset": 135}, {"referenceID": 0, "context": "The most similar evaluation has been that of Baroni et al. (2012), who use external linguistic data in the form of distributional semantic vectors.", "startOffset": 45, "endOffset": 66}, {"referenceID": 0, "context": "The most similar evaluation has been that of Baroni et al. (2012), who use external linguistic data in the form of distributional semantic vectors. Bordes et al. (2011) and Socher et al.", "startOffset": 45, "endOffset": 169}, {"referenceID": 0, "context": "The most similar evaluation has been that of Baroni et al. (2012), who use external linguistic data in the form of distributional semantic vectors. Bordes et al. (2011) and Socher et al. (2013) also evaluate on the WordNet hierarchy, but they use other relations in WordNet as training data (and external linguistic data, in Socher\u2019s case).", "startOffset": 45, "endOffset": 194}, {"referenceID": 25, "context": "1 LOSS FUNCTION To apply order-embeddings to hypernymy, we follow the setup of Socher et al. (2013) in learning an N-dimensional vector for each concept in WordNet, but we replace their neural tensor network with our order-violation penalty defined in Eq.", "startOffset": 79, "endOffset": 100}, {"referenceID": 1, "context": "Like Bordes et al. (2011), we randomly select 4000 edges for the test split, and another 4000 for the development set.", "startOffset": 5, "endOffset": 26}, {"referenceID": 25, "context": "order-embeddings (bilinear) replaces our penalty with the bilinear model used by Socher et al. (2013). order-embeddings is our full model.", "startOffset": 81, "endOffset": 102}, {"referenceID": 8, "context": "The caption-image retrieval task has become a standard evaluation of joint models of vision and language (Hodosh et al., 2013; Lin et al., 2014a).", "startOffset": 105, "endOffset": 145}, {"referenceID": 26, "context": "Many modern approaches model the caption-image relationship symmetrically, either by embedding into a common \u201cvisual-semantic\u201d space with inner-product similarity (Socher et al., 2014; Kiros et al., 2014), or by using Canonical Correlations Analysis between distributed representations of images and captions (Klein et al.", "startOffset": 163, "endOffset": 204}, {"referenceID": 12, "context": "Many modern approaches model the caption-image relationship symmetrically, either by embedding into a common \u201cvisual-semantic\u201d space with inner-product similarity (Socher et al., 2014; Kiros et al., 2014), or by using Canonical Correlations Analysis between distributed representations of images and captions (Klein et al.", "startOffset": 163, "endOffset": 204}, {"referenceID": 13, "context": ", 2014), or by using Canonical Correlations Analysis between distributed representations of images and captions (Klein et al., 2015).", "startOffset": 112, "endOffset": 132}, {"referenceID": 29, "context": "An alternative is to learn an unconstrained binary relation, either with a neural language model conditioned on the image (Vinyals et al., 2015; Mao et al., 2015) or using a multimodal CNN (Ma et al.", "startOffset": 122, "endOffset": 162}, {"referenceID": 18, "context": "An alternative is to learn an unconstrained binary relation, either with a neural language model conditioned on the image (Vinyals et al., 2015; Mao et al., 2015) or using a multimodal CNN (Ma et al.", "startOffset": 122, "endOffset": 162}, {"referenceID": 17, "context": ", 2015) or using a multimodal CNN (Ma et al., 2015).", "startOffset": 34, "endOffset": 51}, {"referenceID": 8, "context": "The caption-image retrieval task has become a standard evaluation of joint models of vision and language (Hodosh et al., 2013; Lin et al., 2014a). The task involves ranking a large dataset of images by relevance for a query caption (Image Retrieval), and ranking captions by relevance for a query image (Caption Retrieval). Given a set of aligned image-caption pairs as training data, the goal is then to learn a caption-image compatibility score S(c, i) to be used at test time. Many modern approaches model the caption-image relationship symmetrically, either by embedding into a common \u201cvisual-semantic\u201d space with inner-product similarity (Socher et al., 2014; Kiros et al., 2014), or by using Canonical Correlations Analysis between distributed representations of images and captions (Klein et al., 2015). While Karpathy & Li (2015) and Plummer et al.", "startOffset": 106, "endOffset": 838}, {"referenceID": 8, "context": "The caption-image retrieval task has become a standard evaluation of joint models of vision and language (Hodosh et al., 2013; Lin et al., 2014a). The task involves ranking a large dataset of images by relevance for a query caption (Image Retrieval), and ranking captions by relevance for a query image (Caption Retrieval). Given a set of aligned image-caption pairs as training data, the goal is then to learn a caption-image compatibility score S(c, i) to be used at test time. Many modern approaches model the caption-image relationship symmetrically, either by embedding into a common \u201cvisual-semantic\u201d space with inner-product similarity (Socher et al., 2014; Kiros et al., 2014), or by using Canonical Correlations Analysis between distributed representations of images and captions (Klein et al., 2015). While Karpathy & Li (2015) and Plummer et al. (2015) model a finer-grained alignment between regions in the image and segments of the caption, the similarity they use is still symmetric.", "startOffset": 106, "endOffset": 864}, {"referenceID": 24, "context": "To facilitate comparison, we use the same pairwise ranking loss that Socher et al. (2014), Kiros et al.", "startOffset": 69, "endOffset": 90}, {"referenceID": 12, "context": "(2014), Kiros et al. (2014) and Karpathy & Li (2015) have used on this task\u2014simply replacing their symmetric similarity measure with our asymmetric order-violation penalty.", "startOffset": 8, "endOffset": 28}, {"referenceID": 12, "context": "(2014), Kiros et al. (2014) and Karpathy & Li (2015) have used on this task\u2014simply replacing their symmetric similarity measure with our asymmetric order-violation penalty.", "startOffset": 8, "endOffset": 53}, {"referenceID": 12, "context": "For learning fc and fi, we use the approach of Kiros et al. (2014) except that (since we are embedding into R+ ) we constrain the embedding vectors to have nonnegative entries by taking their absolute value.", "startOffset": 47, "endOffset": 67}, {"referenceID": 12, "context": "For learning fc and fi, we use the approach of Kiros et al. (2014) except that (since we are embedding into R+ ) we constrain the embedding vectors to have nonnegative entries by taking their absolute value. Thus, to embed images, we use fi(i) = |Wi \u00b7 CNN(i)| (6) where Wi is a learned N \u00d7 4096 matrix, N being the dimensionality of the embedding space. CNN(i) is the same image feature used by Klein et al. (2015): we rescale images to have smallest side 256 pixels, we take 224 \u00d7 224 crops from the corners, center, and their horizontal reflections, run the 10 crops through the 19-layer VGG network of Simonyan & Zisserman (2015), and average their fc7 features.", "startOffset": 47, "endOffset": 415}, {"referenceID": 12, "context": "For learning fc and fi, we use the approach of Kiros et al. (2014) except that (since we are embedding into R+ ) we constrain the embedding vectors to have nonnegative entries by taking their absolute value. Thus, to embed images, we use fi(i) = |Wi \u00b7 CNN(i)| (6) where Wi is a learned N \u00d7 4096 matrix, N being the dimensionality of the embedding space. CNN(i) is the same image feature used by Klein et al. (2015): we rescale images to have smallest side 256 pixels, we take 224 \u00d7 224 crops from the corners, center, and their horizontal reflections, run the 10 crops through the 19-layer VGG network of Simonyan & Zisserman (2015), and average their fc7 features.", "startOffset": 47, "endOffset": 633}, {"referenceID": 12, "context": "Caption Retrieval Image Retrieval Model R@1 R@10 Med r Mean r R@1 R@10 Med r Mean r 1k Test Images MNLM (Kiros et al., 2014) 43.", "startOffset": 104, "endOffset": 124}, {"referenceID": 18, "context": "9 3 * m-RNN (Mao et al., 2015) 41.", "startOffset": 12, "endOffset": 30}, {"referenceID": 13, "context": "6 4 * FV (Klein et al., 2015) 39.", "startOffset": 9, "endOffset": 29}, {"referenceID": 17, "context": "1 m-CNN (Ma et al., 2015) 38.", "startOffset": 8, "endOffset": 25}, {"referenceID": 13, "context": "Metrics for our models on 1k test images are averages over five 1000-image splits of the 5000-image test set, as in (Klein et al., 2015).", "startOffset": 116, "endOffset": 136}, {"referenceID": 3, "context": "To embed the captions, we use a recurrent neural net encoder with GRU activations (Cho et al., 2014), so fc(c) = |GRU(c)|, the absolute value of hidden state after processing the last word.", "startOffset": 82, "endOffset": 100}, {"referenceID": 15, "context": "We evaluate on the Microsoft COCO dataset (Lin et al., 2014b), which has over 120,000 images, each with at least five human-annotated captions per image. This is by far the largest dataset commonly used for caption-image retrieval. We use the data splits of Karpathy & Li (2015) for training (113,287 images), validation (5000 images), and test (5000 images).", "startOffset": 43, "endOffset": 279}, {"referenceID": 17, "context": "Table 2 shows a comparison between all state-of-the-art and some older methods2 along with our own; see Ma et al. (2015) for a more complete listing.", "startOffset": 104, "endOffset": 121}, {"referenceID": 14, "context": "Between these four models, the only previous work whose results are incommensurable with ours is DVSA, since it uses the less discriminative CNN of Krizhevsky et al. (2012) but 20 region features instead of a single whole-image feature.", "startOffset": 148, "endOffset": 173}, {"referenceID": 2, "context": "To evaluate order-embeddings on the natural language inference task, we use the recently proposed SNLI corpus (Bowman et al., 2015), which contains 570,000 pairs of sentences, each labeled with \u201centailment\u201d if the inference is valid, \u201ccontradiction\u201d if the two sentences contradict , or \u201cneutral\u201d if the inference is invalid but there is no contradiction.", "startOffset": 110, "endOffset": 131}, {"referenceID": 23, "context": "Neural Attention (Rockt\u00e4schel et al., 2015) * 83.", "startOffset": 17, "endOffset": 43}, {"referenceID": 2, "context": "5 EOP classifier (Bowman et al., 2015) 75.", "startOffset": 17, "endOffset": 38}, {"referenceID": 7, "context": "Batch normalization (Ioffe & Szegedy, 2015) and PReLU units (He et al., 2015) were used.", "startOffset": 60, "endOffset": 77}, {"referenceID": 2, "context": "We also evaluate against EOP classifier, a 2-class baseline introduced by (Bowman et al., 2015), and against a version of our model where our order-violation penalty is replaced with the symmetric cosine distance, order-embeddings (symmetric).", "startOffset": 74, "endOffset": 95}, {"referenceID": 20, "context": "The state-of-the-art method for 3-class classification on SNLI is that of Rockt\u00e4schel et al. (2015). Unfortunately, they do not compute 2-class accuracy, so we cannot compare to them directly.", "startOffset": 74, "endOffset": 100}, {"referenceID": 2, "context": "We also evaluate against EOP classifier, a 2-class baseline introduced by (Bowman et al., 2015), and against a version of our model where our order-violation penalty is replaced with the symmetric cosine distance, order-embeddings (symmetric). The results for all models are shown in Table 3. We see that order-embeddings outperform the skipthought baseline despite not using external text corpora. While our method is almost certainly worse than the state-of-the-art method of Rockt\u00e4schel et al. (2015), which uses a word-by-word attention mechanism, it is also much simpler.", "startOffset": 75, "endOffset": 504}, {"referenceID": 5, "context": "A promising direction of future work is to learn better classifiers on ImageNet (Deng et al., 2009), which has over 21k image classes arranged by the WordNet hierarchy.", "startOffset": 80, "endOffset": 99}, {"referenceID": 5, "context": "A promising direction of future work is to learn better classifiers on ImageNet (Deng et al., 2009), which has over 21k image classes arranged by the WordNet hierarchy. Previous approaches, including Frome et al. (2013) and Norouzi et al.", "startOffset": 81, "endOffset": 220}, {"referenceID": 5, "context": "A promising direction of future work is to learn better classifiers on ImageNet (Deng et al., 2009), which has over 21k image classes arranged by the WordNet hierarchy. Previous approaches, including Frome et al. (2013) and Norouzi et al. (2014) have embedded words and images into a shared semantic space with symmetric similarity\u2014which our experiments suggest to be a poor fit with the partial order structure of the visual-semantic hierarchy.", "startOffset": 81, "endOffset": 246}], "year": 2017, "abstractText": "Hypernymy, textual entailment, and image captioning can be seen as special cases of a single visual-semantic hierarchy over words, sentences, and images. In this paper we advocate for explicitly modeling the partial order structure of this hierarchy. Towards this goal, we introduce a general method for learning ordered representations, and show how it can be applied to a variety of tasks involving images and language. We show that the resulting representations improve performance over current approaches for hypernym prediction and image-caption retrieval.", "creator": "LaTeX with hyperref package"}}}