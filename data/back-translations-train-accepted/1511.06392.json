{"id": "1511.06392", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Neural Random-Access Machines", "abstract": "In this paper, we propose and investigate a new neural network architecture called Neural Random Access Machine. It can manipulate and dereference pointers to an external variable-size random-access memory. The model is trained from pure input-output examples using backpropagation.", "histories": [["v1", "Thu, 19 Nov 2015 21:36:28 GMT  (53kb,D)", "http://arxiv.org/abs/1511.06392v1", "ICLR submission, 13 pages with bibliography and appendix, 3 figures, 2 tables"], ["v2", "Thu, 7 Jan 2016 10:27:06 GMT  (124kb,D)", "http://arxiv.org/abs/1511.06392v2", "ICLR submission, 17 pages, 9 figures, 6 tables (with bibliography and appendix)"], ["v3", "Tue, 9 Feb 2016 21:29:07 GMT  (124kb,D)", "http://arxiv.org/abs/1511.06392v3", "ICLR submission, 17 pages, 9 figures, 6 tables (with bibliography and appendix)"]], "COMMENTS": "ICLR submission, 13 pages with bibliography and appendix, 3 figures, 2 tables", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["karol kurach", "marcin", "rychowicz", "ilya sutskever"], "accepted": true, "id": "1511.06392"}, "pdf": {"name": "1511.06392.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Karol Kurach"], "emails": ["kkurach@google.com", "marcina@google.com", "ilyasu@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Deep learning is responsible for two reasons: firstly, that deep neural networks are the \"right\" type of function; secondly, that deep neural networks can potentially be improved if they go deeper and have fewer parameters; and that they are closer to a practical implementation of the system; and thirdly, that they are able to play by the rules; and thirdly, that they are able to play by the rules; and thirdly, that they are able to play by the rules."}, {"heading": "2 RELATED WORK", "text": "There has been a significant interest in the problem of algorithm learning in recent years, the most relevant recent work being Neural Turing Machines (NTMs) (Graves et al., 2014), which was the first work to explicitly suggest the idea that it is worth training a computerized universal neural network and achieves encouraging results; a successor model that aimed to learn algorithms was the Stack-Augmented Recurrent Neural Network (Joulin & Mikolov, 2015), which showed that stack-augmented RNN can generalize to long-term problems; and a related model is the Reinforcement Learning Neural Turing Machine (Zaremba & Sutskever, 2015), which attempts to use amplification techniques to train a discrete, continuous hybrid model."}, {"heading": "3 MODEL", "text": "In this section, we do not directly describe the NRAM model. We start with a description of the simplified version of our model that does not use external memory, and then explain how it can be expanded with a variable-size memory. \u2212 j The core of the model is a neural controller that acts as a \"processor.\" The controller can be a feedback neural network or an LSTM, and it is the only transferable part of the model. The model contains R registers, each of which has an integer value. To make our model portable with gradient descent, we have made it completely differentiable. Hence, each register represents an integer value with a distribution over the set {0, 1,.,., M \u2212 1}, for some constant M. We do not assume that these distributions have any special shape - they are simply stored as vectors p."}, {"heading": "3.1 CONTROLLER\u2019S INPUTS", "text": "A na\u00efve approach is to use the values of the registers as inputs to the controller. However, the values of the registers are probability distributions and are stored as vectors p-RM. If the entire distributions were given as inputs to the controller, then the number of parameters of the model would depend on M. However, this would not be desirable because, as explained in the next section, the value M is randomly coupled to the size of an external memory tape, thus preventing the model from generalizing to different memory sizes. Therefore, for each 1 \u2264 i \u2264 R input, the controller receives only one scalar from each register, namely P (ri = 0) - the probability that the value in the register is equal to 0. This solution has an additional advantage, namely limiting the problem if the b = ri module forces the available information to resolve the case."}, {"heading": "3.2 MEMORY TAPE", "text": "One could use the previously described model to learn sequence-to-sequence transformations by initializing the registers with the input sequence and training the model to generate the desired output sequence in its registers after a certain number of time pauses. The disadvantage of such a model is that it would not be able to generalize longer sequences, because the length of the sequence the model can process corresponds to the number of registers it has, which is constant. Therefore, we expand the model with a variable-size memory band consisting of M memory cells, each of which stores a distribution over the set {0, 1,., M \u2212 1}. Note that any distribution stored in a memory cell or register can be interpreted as a blurred address in memory and used as a blurred pointer. We will therefore become integers in the set {0, 1,., M \u2212 1} with the memory pointers."}, {"heading": "3.3 INPUTS AND OUTPUTS HANDLING", "text": "The memory tape also serves as an input-output channel - the memory of the model is initialized with the input sequence and the model is expected to generate the output in memory. In addition, we decide in a novel way how many timesteps to run. After each timestep, we let the controller decide whether to continue or stop execution. In this case, the current state of the memory is called output. Specifically, after the timestep t, the controller prints a scalar ft value [0, 1] 3 indicating the readiness to stop execution in the current timestep. Therefore, the probability that execution was not terminated before the timestep t is equal. \u2212 1 \u2212 i = 1 (1 \u2212 fi), and the probability that the output is produced exactly at the timestep t."}, {"heading": "3.4 DISCRETIZATION", "text": "Calculating the output of the modules, represented as probability distributions, is therefore a mathematically expensive operation. For example, calculating the output of the READ module takes time, since it requires multiplying the matrixM-RMM and the vector p-RM. However, it can be assumed (and we confirm this assertion empirically in Sec. 4) that the NRAM model naturally learns solutions where the distributions of the intermediate values have a very low entropy. The argument for this hypothesis is that vagueness in the intermediate values would likely spread to the output and cause a higher value of the cost function. To test this hypothesis, we trained the model and then used its discredited version during interference. In the discredited version, each module gets as input the values of modules (or registers) that are most likely to generate the given input according to the distribution issued by the controller."}, {"heading": "4 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 TRAINING PROCEDURE", "text": "This year it is more than ever before in the history of the city."}, {"heading": "4.2 TASKS", "text": "We now describe the tasks used in our experiments. For each task, the input is given to the network on the memory tape, and the goal of the network is to modify the memory according to the specification of the task. We allow the network to modify the original input. The final error for a test case is calculated as cm, where c is the number of correctly written cells and m is the total number of cells that should be modified.Due to the limited space we describe the tasks here only briefly. The detailed memory layout of the inputs and outputs can be found in Appendix A.1. Access If you specify a value k and an array A, you specify A [k].2. Increment If you specify an array, you increase all of its elements by 1.3. Copy If you return an array and a pointer to the target, you copy all elements from the array to the specified position A.4. If you return a value k and an array to the specified position 5. you copy all of the array back to the array and a pointer to the array."}, {"heading": "4.3 MODULES", "text": "In all our experiments we used the same sequence of 14 modules: READ (described in paragraph 3.2), ZERO (a, b) = 0, ONE (a, b) = 1, TWO (a, b) = 2, INC (a, b) = (a + 1) mod M, ADD (a, b) = (a + b) mod M, SUB (a, b) = (a \u2212 b) mod M, DEC (a, b) = (a \u2212 1) mod M, LESS-THAN (a, b) = [a < b], LESS-OR-EQUAL-THAN (a, b) = [a \u2264 b], EQUALITY-TEST (a, b) = [a = b) mod M, MIN (a, b) = min (a, b), MAX (a, b), WRITE (1. in paragraph."}, {"heading": "4.4 RESULTS", "text": "As described in 4.2, our measurement value is the accuracy of the memory cells that should be changed. To calculate it, we take the continuous memory state that our network generates, discredit it (each cell contains the value with the highest probability), and finally compare it with the expected output. The results of the experiments are in Table 1. Below, we describe our results for all 10 tasks in more detail. We classify them into two categories: \"simple\" and \"hard\" tasks. Simple tasks are a category of tasks that have low error values for many sets of parameters, and we did not need to spend much time setting them up. Initially, 5 problems from our task list belong to this category. Hard tasks, on the other hand, are problems that have often been trained to a low error rate in only a very small number of cases, e.g. 1 in 100."}, {"heading": "4.4.1 EASY TASKS", "text": "This category includes the following issues: Access, Increment, Reverse, Swap."}, {"heading": "4.5 COMPARISON TO EXISTING MODELS", "text": "Comparing it to other models is difficult because we are the first to consider problems with pointers. NTM can solve tasks such as copying or reversing, but suffers from the inability to store a pointer to a fixed location in memory, making it unlikely that it could solve tasks such as ListK, ListSearch or WalkBST, since the pointers used in these tasks point to absolute positions. What sets our model apart from most previous attempts (including NTMs, storage networks, pointer networks) is the lack of content-based addressing. It was a conscious design decision, as this type of addressing naturally slows memory access. By contrast, our model can access memory at a constant point in time when it is discredited. NRAM is also the first model where we are aware that it uses a differentiated mechanism to decide when to complete the calculation."}, {"heading": "4.6 EXEMPLARY EXECUTION", "text": "For example, we are using a very small model with 12 memory cells, 4 registers, and the standard set of 14 modules. The controller for this model is a feedback network, and we are running it for 11 timetables. Table 2 contains the state of memory and the registers at the beginning of the timetable for each timetable. The model can run different circuits at different timetables. In particular, we observed that the first timetable is slightly different from the rest because it has to handle the initialization. Starting with the second step, all the generated circuits are the same. We present this circuit in Fig. 3. The r2 register is constant and keeps the offset between the target array and the source array (in this case 6 \u2212 1 = 5). Register r3 is responsible for incrementing the pointer in the source array. Its value is set to r48, which is copied by the Register 4, which is also used by the RE."}, {"heading": "5 CONCLUSIONS", "text": "In this thesis, we introduced the Neural Random-Access Machine, which can learn to solve problems that require explicit manipulation and dereference of pointers. We showed that this model can learn to solve a number of algorithmic problems and generalize well to inputs that are longer than those seen during the training, especially for some problems it generalizes to inputs of any length. However, we found that the optimization problem resulting from back propagation through the executing track of the program is very difficult for standard optimization techniques. It seems likely that a method that can search in a simpler \"abstract\" space is more effective at solving such problems.8In our case, r3 < r2, so the MIN module always gives the value r3 + 1. It is not satisfied in the last step, but then the array is already copied."}, {"heading": "A DETAILED TASKS DESCRIPTIONS", "text": "In this section we describe in detail the memory layout of the inputs and outputs for the tasks used in our experiments. In all the descriptions below, upper case letters represent arrays and lower case pointers. NULL denotes the value 0 and is used to denote the end of an array or binary. Accessing a value k and an array A, the return of A [k]. Input is given as k, A [0], A [n \u2212 1], NULL and the first memory element with A [k]. Increment given an array and an array, increments all its elements as k, A [n]."}, {"heading": "B DETAILS OF CURRICULUM TRAINING", "text": "As stated in several papers (Bengio et al., 2009; Zaremba & Sutskever, 2014), learning the curriculum is crucial for forming deep networks on very complicated problems. We followed the Zaremba & Sutskever curriculum (2014) without any changes. For each of the tasks, we manually defined a sequence of sub-tasks with increasing difficulty, the difficulty usually measured by the length of the input sequence. During the training, the input-output examples are sampled from a distribution determined by the current difficulty level D. The level is increased (up to a certain maximum value) whenever the error rate of the model falls below a threshold. Furthermore, we ensure that successive increases of D are sampled by a certain number of batches.Specifically, in order to generate an input-output example, we will first scan a difficulty d from a distribution determined by the current level D and then draw the example with the difficulty level d."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio", "Yoshua", "Simard", "Patrice", "Frasconi", "Paolo"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Curriculum learning", "author": ["Bengio", "Yoshua", "Louradour", "J\u00e9r\u00f4me", "Collobert", "Ronan", "Weston", "Jason"], "venue": "In Proceedings of the 26th annual international conference on machine learning,", "citeRegEx": "Bengio et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "Listen, attend and spell", "author": ["Chan", "William", "Jaitly", "Navdeep", "Le", "Quoc V", "Vinyals", "Oriol"], "venue": "arXiv preprint arXiv:1508.01211,", "citeRegEx": "Chan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "Neural turing machines", "author": ["Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Learning to transduce with unbounded memory", "author": ["Grefenstette", "Edward", "Hermann", "Karl Moritz", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "arXiv preprint arXiv:1506.02516,", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Joulin", "Armand", "Mikolov", "Tomas"], "venue": "arXiv preprint arXiv:1503.01007,", "citeRegEx": "Joulin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2015}, {"title": "Grid long short-term memory", "author": ["Kalchbrenner", "Nal", "Danihelka", "Ivo", "Graves", "Alex"], "venue": "arXiv preprint arXiv:1507.01526,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Luong", "Minh-Thang", "Pham", "Hieu", "Manning", "Christopher D"], "venue": "arXiv preprint arXiv:1508.04025,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Vinod", "Hinton", "Geoffrey E"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "A formal theory of inductive inference", "author": ["Solomonoff", "Ray J"], "venue": "i. Information and control,", "citeRegEx": "Solomonoff and J.,? \\Q1964\\E", "shortCiteRegEx": "Solomonoff and J.", "year": 1964}, {"title": "End-to-end memory networks", "author": ["Sukhbaatar", "Sainbayar", "Szlam", "Arthur", "Weston", "Jason", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1503.08895,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Grammar as a foreign language", "author": ["Vinyals", "Oriol", "Kaiser", "Lukasz", "Koo", "Terry", "Petrov", "Slav", "Sutskever", "Ilya", "Hinton", "Geoffrey"], "venue": "arXiv preprint arXiv:1412.7449,", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Learning to execute", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1410.4615,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Reinforcement learning neural turing machines", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1505.00521,", "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 4, "context": "The first model that we know of that attempted to train extremely deep networks with a large memory and few parameters is the Neural Turing Machine (NTM) (Graves et al., 2014) \u2014 a computationally universal deep neural network that is trainable with backpropagation.", "startOffset": 154, "endOffset": 175}, {"referenceID": 5, "context": "Other models with this property include variants of Stack-Augmented recurrent neural networks (Joulin & Mikolov, 2015; Grefenstette et al., 2015), and the Grid-LSTM (Kalchbrenner et al.", "startOffset": 94, "endOffset": 145}, {"referenceID": 8, "context": ", 2015), and the Grid-LSTM (Kalchbrenner et al., 2015)\u2014of which the Grid-LSTM has achieved the greatest success on both synthetic and real tasks.", "startOffset": 27, "endOffset": 54}, {"referenceID": 4, "context": "The most relevant recent paper is Neural Turing Machines (NTMs) (Graves et al., 2014).", "startOffset": 64, "endOffset": 85}, {"referenceID": 8, "context": "The Grid-LSTM (Kalchbrenner et al., 2015) is a highly interesting extension of LSTM, which allows to use LSTM cells for both deep and sequential computation.", "startOffset": 14, "endOffset": 41}, {"referenceID": 10, "context": "Indeed, this model has proven to be highly versatile, and variants of this model have achieved state-of-the-art results on machine translation (Luong et al., 2015), speech recognition (Chan et al.", "startOffset": 143, "endOffset": 163}, {"referenceID": 3, "context": ", 2015), speech recognition (Chan et al., 2015), and syntactic parsing (Vinyals et al.", "startOffset": 28, "endOffset": 47}, {"referenceID": 14, "context": ", 2015), and syntactic parsing (Vinyals et al., 2014), without the use of almost any domain-specific tuning.", "startOffset": 31, "endOffset": 53}, {"referenceID": 2, "context": "The most relevant recent paper is Neural Turing Machines (NTMs) (Graves et al., 2014). It was the first paper to explicitly suggest the notion that it is worth training a computationally universal neural network, and achieved encouraging results. A follow-up model that had the goal of learning algorithms was the Stack-Augmented Recurrent Neural Network (Joulin & Mikolov, 2015) This work demonstrated that the Stack-Augmented RNN can generalize to long problem instances from short problem instances. A related model is the Reinforcement Learning Neural Turing Machine (Zaremba & Sutskever, 2015), which attempted to use reinforcement learning techniques to train a discrete-continuous hybrid model. The memory network (Weston et al., 2014) is an early model that attempted to explicitly separate the memory from computation in a neural network model. The followup work of Sukhbaatar et al. (2015) combined the memory network with the soft attention mechanism, which allowed it to be trained with less supervision.", "startOffset": 65, "endOffset": 900}, {"referenceID": 0, "context": ", 2015) is somewhat different from the above models in that it does not have a writable memory \u2014 it is more similar to the attention model of Bahdanau et al. (2014) in this regard.", "startOffset": 142, "endOffset": 165}, {"referenceID": 0, "context": ", 2015) is somewhat different from the above models in that it does not have a writable memory \u2014 it is more similar to the attention model of Bahdanau et al. (2014) in this regard. Despite not having a memory, this model was able to solve a number of difficult algorithmic problems that include the convex hull and the approximate 2D travelling salesman problem (TSP). Finally, it is important to mention the attention model of Bahdanau et al. (2014). Although this work is not explicitly aimed at learning algorithms, it is by far the most practical model that has an \u201calgorithmic bent\u201d.", "startOffset": 142, "endOffset": 451}, {"referenceID": 2, "context": "Curriculum learning As noticed in several papers (Bengio et al., 2009; Zaremba & Sutskever, 2014), curriculum learning is crucial for training deep networks on very complicated problems.", "startOffset": 49, "endOffset": 97}, {"referenceID": 1, "context": "Curriculum learning As noticed in several papers (Bengio et al., 2009; Zaremba & Sutskever, 2014), curriculum learning is crucial for training deep networks on very complicated problems. We followed the curriculum learning schedule from Zaremba & Sutskever (2014) without any modifications.", "startOffset": 50, "endOffset": 264}, {"referenceID": 1, "context": "In networks of such depth, the gradients can often \u201cexplode\u201d (Bengio et al., 1994), what makes training by backpropagation much harder.", "startOffset": 61, "endOffset": 82}], "year": 2015, "abstractText": "In this paper, we propose and investigate a new neural network architecture called Neural Random Access Machine. It can manipulate and dereference pointers to an external variable-size random-access memory. The model is trained from pure input-output examples using backpropagation. We evaluate the new model on a number of simple algorithmic tasks whose solutions require pointer manipulation and dereferencing. Our results show that the proposed model can learn to solve algorithmic tasks of such type and is capable of operating on simple data structures like linked-lists and binary trees. For easier tasks, the learned solutions generalize to sequences of arbitrary length. Moreover, memory access during inference can be done in a constant time under some assumptions.", "creator": "LaTeX with hyperref package"}}}