{"id": "1511.06393", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Fixed Point Quantization of Deep Convolutional Networks", "abstract": "In recent years increasingly complex architectures for deep convolution networks (DCNs) have been proposed to boost the performance on image recognition tasks. However, the gains in performance have come at a cost of substantial increase in compute resources, the model size and processing speed of the network for training and evaluation. Fixed point implementation of these networks has the potential to alleviate some of the burden of these additional complexities. In this paper, we propose a quantizer design for fixed point implementation for DCNs. We then formulate an optimization problem to identify optimal fixed point bit-width allocation across DCN layers. We perform experiments on a recently proposed DCN architecture for CIFAR-10 benchmark that generates test error of less than 7%. We evaluate the effectiveness of our proposed fixed point bit-width allocation for this DCN. Our experiments show that in comparison to equal bit-width settings, the fixed point DCNs with optimized bit width allocation offer &gt;20% reduction in the model size without any loss in performance. We also demonstrate that fine tuning can further enhance the accuracy of fixed point DCNs beyond that of the original floating point model. In doing so, we report a new state-of-the-art fixed point performance of 6.78% error-rate on CIFAR-10 benchmark.", "histories": [["v1", "Thu, 19 Nov 2015 21:37:06 GMT  (516kb,D)", "https://arxiv.org/abs/1511.06393v1", null], ["v2", "Thu, 7 Jan 2016 22:20:06 GMT  (528kb,D)", "http://arxiv.org/abs/1511.06393v2", null], ["v3", "Thu, 2 Jun 2016 06:21:42 GMT  (524kb,D)", "http://arxiv.org/abs/1511.06393v3", "ICML 2016"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["darryl dexu lin", "sachin s talathi", "v sreekanth annapureddy"], "accepted": true, "id": "1511.06393"}, "pdf": {"name": "1511.06393.pdf", "metadata": {"source": "META", "title": "Fixed Point Quantization of Deep Convolutional Networks", "authors": ["Darryl D. Lin", "Sachin S. Talathi", "Sreekanth Annapureddy"], "emails": ["DARRYL.DLIN@GMAIL.COM", "TALATHI@GMAIL.COM", "SREEKANTHAV@GMAIL.COM"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them will be able to move into a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they live."}, {"heading": "2. Related work", "text": "In recent years, the number of those who are able to reform has doubled. (...) In recent years, the number of those who are able to reform has multiplied. (...) In recent years, the number of those who are able to reform has multiplied. (...) In recent years, the number of those who are able to reform has multiplied. (...) In recent years, the number of those who are able to reform has decreased. (...) In recent years, the number of those who are able to reform has multiplied. (...) The number of those who are able to reform has decreased. (...) In recent years, the number of those who are able to reform has increased. (...) The number of those who are able to reform has increased. \""}, {"heading": "3. Floating point to fixed point conversion", "text": "In this section we propose an algorithm to convert a floating point DCN into a fixed point. The aim of the conversion for a given layer DCN is to represent the input activations, the output activations and the parameters of that layer in a fixed point. This can be seen as a process of quantification."}, {"heading": "3.1. Optimal uniform quantizer", "text": "There are three interdependent parameters for the fixed point representation of a floating point effectiveness DCN: bit width, step size (resolution) and dynamic range. These are related as: range \u2248 step size \u00b7 2 bit width (1) However, given a fixed bit width, the trade-off is between large enough to reduce the likelihood of overflow and small enough resolution to reduce the quantization error. The problem of representing the best trade-off between overflow error and quantization error has been extensively studied in the literature. Table 1 below shows the step sizes of the optimal symmetric uniform quantizer for uniform quantization, Gauss, Laplacian and Gamma distributions. The quantizers are optimal in terms of minimizing the SQNR.For example, the input is Gauss distributed with zero mean and unit variance. If we need a uniform quantizer with a bit width of 1 (i.e. 98 levels), the best approach to quantify is to place 0.2."}, {"heading": "3.2. Empirical distributions in a pre-trained DCN", "text": "Figure 2 (a) and Figure 2 (b) show the empirical distributions of weights and activations for the convective layers of the DCN that we designed for the CIFAR-10 benchmark (see Section 5). Note that the activations shown here precede the application of the activation functions. Given the similarity of these distributions to the Gaussian distribution, we have used the Gaussian distribution in all our analyses for both weights and activations. However, we also note that the distribution of weights and activations for some layers is less Gaussian. It will therefore be interesting to experiment with increments for other distributions (see Table 1), which goes beyond the scope of the present work."}, {"heading": "3.3. Model conversion", "text": "Each floating-point DCN model can be converted into a fixed point by the following steps: \u2022 Perform a floating-point forward motion with a large number of typical inputs and record the activations. \u2022 Collect statistics of weights, distortions and activations for each level. \u2022 Determine the fixed-point formats of weights, distortions and activations for each level. Note that determining the fixed-point format corresponds to determining the resolution, which in turn means determining the number of fractions needed to represent the number. \u2022 The following equations can be used to calculate the number of fractions: \u2022 Determine the effective standard deviation of the quantifiable size: \u2022 Calculate the step size using Table 1: s = fraction size (\u03b2). \u2022 Calculate the number of fractions: n = \u2212 dlog2 se.In these equations, the effective standard deviation of the quantifiable standard size is the quantifiable standard size we want to quantify the quantify the width of the quantifiable quantifiable quantifiable quantity of the quantifiable quantity we want to quantify the quantify the quantified quantity of the quantified quantity of the quantity."}, {"heading": "4. Bit-width optimization across a deep network", "text": "In the absence of model fine-tuning, converting a floating-point depth network into a fixed-point depth network is essentially a process of introducing quantization noise into the neural network. In areas such as audio processing or digital communications, it is generally known that system performance decreases as quantization noise increases, and the effect of quantization noise can be accurately captured in a single quantity, the SQNR. In depth learning, there is no well-formulated relationship between SQNR and classification accuracy. However, it is reasonable to assume that generally a higher quantization noise level leads to poorer classification performance. Since SQNR can be approached theoretically and analyzed layer by layer, we will focus on developing a theoretical framework for optimizing the SQNR. Subsequently, we will conduct empirical studies of how the proposed optimization of the SQNR affects the classification accuracy of the CN."}, {"heading": "4.1. Impact of quantization on SQNR", "text": "In this section, we will derive the relationship between the quantization of weight, bias, and activation values and the resulting output SQNR."}, {"heading": "4.1.1. QUANTIZATION OF INDIVIDUAL VALUES", "text": "The quantification of individual values in a DCN, whether it is an activation or a weight value, willingly follows the quantization discussion in Section 3.1. For example, for the weight value w the quantified version w can be written as follows: w = w + nw, (3) where nw is the quantization noise. As shown in Figure 1, if w follows approximately a uniform Gaussian, laplactic or gamma distribution, the SQNR as a result of the quantization process can be written as follows: 10 log (\u03b3w) = 10 log E [w2] E [n2w] \u0445 \u00b7 \u03b2, (4) where w is the quantization efficiency and \u03b2 the quantization bitwidth."}, {"heading": "4.1.2. QUANTIZATION OF BOTH ACTIVATIONS AND WEIGHTS", "text": "Consider the case in which weight w is multiplied by activation a, in which both w and a are quantified by quantization noise nw or na. The product can be approximated for small nw and na as follows: w-a = (w + nw) \u00b7 (a + na) = w-a + w-na + nw-a + nw-na-na = w-w \u00b7 w-w-na-nw-a. (5) The final equality applies if | na | < < < | a and | nw | < < | w | w |. A very important observation is that the SQNR of the product w-a is satisfactory as a result of quantification. (6) This is characteristic of a linear system. The decisive advantage of this finding is that the introduction of quantization noise is independent of weights and activation of the addition of total noise according to the product operation."}, {"heading": "4.1.3. FORWARD PASS THROUGH ONE LAYER", "text": "In a multi-layered DCN, the calculation of the ith activation in layer l + 1 of the DCN can be expressed as follows: a (l + 1) i = N \u2211 j = 1 w (l + 1) i, j a (l) j + b (l + 1) i, (7) where (l) represents the lth layer, N represents the number of additions, wi, j the weight and bi the distortion. Since a (i + 1) i is simply a sum of terms such as w (l + 1) i, j a (l) j j, which in quantified form all have the same SQNR w (l + 1) \u00d7 a (l) (l), assuming the product concepts w (l + 1) i, j a (l) j j j j j j j are independent, it follows that the value of a (i + 1) i i, prior to further quantification, has the reverse SQNR percw (l l l l + 1) j l (l) j l (l 1 l l l l (l), although the terms w (l (1 l l) l l (1 l l l l l l l l (1 l) l l l l (1 l l l l l l l l l), 1 l l l l (1 l l l l (l)."}, {"heading": "4.1.4. FORWARD PASS THROUGH ENTIRE NETWORK", "text": "Equation 8 can be generalized to all levels of a DCN (although we have empirically found that the approximation applies better to the revolutionary layers than to fully connected layers). Consider layer L in a deep network where all activations and weights are quantified. If you expand Equation 8, you get the SQNR (\u03b3output) at the output of layer L as: 1\u03b3output = 1\u03b3a (0) + 1\u03b3w (1) + 1 \u03b3a (1) + \u00b7 + 1 \u03b3w (L) + 1 \u03b3a (L) (9) In other words, the SQNR at the output of a layer in DCN is the harmonious mean of the SQNs of all preceding quantization steps. This simple relationship reveals some very interesting findings: \u2022 All quantization steps contribute equally to the general SQNR of the output, regardless of whether it is the quantization of weights, activations or inputs in DCN, and regardless of where it is (especially where it is not) the SQNR occurs."}, {"heading": "4.1.5. EFFECTS OF OTHER NETWORK COMPONENTS", "text": "\u2022 Batch normalization: Batch normalization (Ioffe & Szegedy, 2015), however, improves the speed of forming a deep network by normalizing the layer input. Therefore, after the network is trained, the batch normalization layer is a fixed linear transformation and can be absorbed into the adjacent revolutionary layer or completely connected layer. Therefore, the quantization effect does not need to be explicitly modelled due to the batch normalization. \u2022 ReLU: In Equation 7, for convenience, we have omitted the activation function applied to a (l) j. In other words, if the activation function is ReLU and the quantization noise is small, all positive values at the input of the activation function will have the same SQNR at the output, and the negative values will be zero (effectively reducing the number of additions, N). In other words: one (l + 1) LW = 1 W (l + 1), i, j (a)."}, {"heading": "4.2. Cross-layer bit-width optimization", "text": "Equation 9 indicates that trade-offs can be made between quantifiers of different layers to provide the same quantifiable performance, i.e. we can choose to use smaller bit widths for some layers by increasing bit widths for other layers, which may be desirable, for example, for the following reasons: \u2022 Some layers may require a large number of network parameters (weights) (multiple accumulation operations). Reducing bit widths for these layers would reduce the overall network computational load. \u2022 Some layers may contain a large number of network parameters (weights). Reducing the weight bit widths for these layers would reduce the overall model size. Interestingly, such goals can be formulated and accurately resolved as an optimization problem. Suppose that our goal is to reduce the model size while maintaining a minimum SQNR at the DCN output."}, {"heading": "5. Experiments", "text": "In this section, we will examine the impact of a reduced bit width for both weights and activations compared to the traditional 32-bit single-precision floating-point approach (16-bit half-precision floating-point optimization will in most cases produce comparable results), in particular, we will implement the fixed-point quantization algorithm described in Section 3, and further refine the effectiveness of the bit-width optimization algorithm in Section 4. In addition, we will use the quantified fixed-point network as a starting point within the constrained alphabets of weights and activations."}, {"heading": "5.1. Bit-width optimization for CIFAR-10 classification", "text": "We evaluate our proposed bit width optimization algorithm based on the CIFAR-10 benchmark using the algorithm prescribed in Section 4.2. In Table 2, we calculate the number of parameters in each layer of our CIFAR-10 network. Considering the goal of minimizing the overall model size, assuming the quantization efficiency is 3 dB / bit, our derivative in Section 4.2 shows that the optimal bit width of layer 0 and conve1 would differ by 10 log (0.295 / 0.007) or 5 bits, respectively. Assuming the bit width of the layer 0 is \u03b20, the subsequent folding layers should have bit width values, as in Table 3.In our experiment in this section, we ignore the fully connected layer and assume that a fixed bit width of 16 is specified for both layers and activations. This is because fully connected layers have and must be optimized with different SQNR characteristics."}, {"heading": "5.2. Bit-width optimization for ImageNet classification", "text": "In Section 5.1, we performed a cross-layer bit width optimization for our CIFAR-10 network with the goal of minimizing the overall size of the model while maintaining accuracy. Here, we are performing a similar exercise for an AlexNetlike DCN trained on ImageNet-1000. However, the DCN architecture is described in Table 4.For determining the bit width of the layers of this DCN, we will follow the steps in Section 5.1 with the assumption that the bit width for the layer is conven1 \u03b21. The resulting bit width allocation for all the conventional layers is shown in Table 5.For fully connected layers, we first consider the network as a glide point and quantify the weights of fully connected layers. We then reduce the bit width of fully connected layers until the classification accuracy begins to deteriorate."}, {"heading": "5.3. Validation for SQNR prediction", "text": "To verify that our SQNR calculation as described in Section 4.1 is valid, we are conducting a small experiment. Specifically, we are focusing on the optimized networks in Figure 3 and comparing the measured SQNR per layer with the SQNR predictions according to Equation 4 and 8. Table 6 compares the theoretical SQNR and the measured SQNR (in dB) for layers conven1 to conven5 for two of the optimized networks. We observe that the two SQNR values do not match numerically, but follow a similar downward trend as the activations spread deeper into the network. It should be noted that our theoretical SQNR predictions are based solely on the weight and activation bit widths of each layer and quantization efficiency."}, {"heading": "5.4. Model fine-tuning", "text": "Although our focus is on fixed-point implementation without training, our quantization design can also be used as a starting point for further fine-tuning of the model when the training model and training parameters are available.Table 7 provides the classification error rate (in%) for the CIFAR-10 network after fine-tuning the model for 30 epochs. We experiment with different weight and activation bit width combinations, ranging from floating combination over 4-bit, 8-bit and 16-bit fixed point. It turns out that even the (4b, 4b) bit width combination works well (8.30% error rate) when the network is fine-tuned after quantization. In addition, the setting (float, 8b) generates a 6.78% error rate, which is the new state-of-the-art result, although the activations are only 8-bit fixed-point numbers. This can be attributed to the regulating effect of additional quantification noise (Yo, Lusiet 2015)."}, {"heading": "6. Conclusions", "text": "In this paper, we develop a principled approach to converting a pre-trained floating-point DCN model into its fixed-point equivalent. We show that the naive method of quantifying all layers in the DCN with a uniform bit-width value leads to DCN networks with below-average performance in terms of error rates compared to our proposed approach of SQNR-based optimization of bit-widths. In particular, we present results for a floating-point DCIFAR-10 benchmark that leads to a > 20% reduction in model size without loss of accuracy when converted to its fixed-point counterpart."}, {"heading": "Acknowledgements", "text": "We would like to thank our colleagues David Julian, Anthony Sarah, Daniel Fontijne, Somdeb Majumdar, Aniket Vartak, Blythe Towal and Mark Staskauskas for fruitful discussions and valuable feedback."}], "references": [{"title": "Convex Optimization", "author": ["Boyd", "Stephen", "Vandenberghe", "Lieven"], "venue": null, "citeRegEx": "Boyd et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Boyd et al\\.", "year": 2004}, {"title": "Return of the devil in the details: Delving deep into convolutional nets", "author": ["K. Chatfield", "S. Simonyan", "V. Vedaldi", "A. Zisserman"], "venue": null, "citeRegEx": "Chatfield et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chatfield et al\\.", "year": 2014}, {"title": "Compressing neural networks with the hashing trick", "author": ["W. Chen", "J.T. Wilson", "S. Tyree", "K.Q. Weinberger", "Y. Chen"], "venue": "In ICML,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Low precision arithmetic for deep learning", "author": ["M. Courbariaux", "Y. Bengio", "J. David"], "venue": null, "citeRegEx": "Courbariaux et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2014}, {"title": "New types of deep neural network learning for speech recognition and related applications: an overview", "author": ["L. Deng", "G.E", "Hinton", "B. Kingsbury"], "venue": "In IEEE International Conference on Acoustic, Speech and Signal Processing,", "citeRegEx": "Deng et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2013}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "In CVPR,", "citeRegEx": "Girshick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Y. Gong", "L. Liu", "M. Yang", "L. Bourdev"], "venue": null, "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Deep learning with limited numerical precision", "author": ["S. Gupta", "A. Agrawal", "K. Gopalakrishnan", "P. Narayanan"], "venue": null, "citeRegEx": "Gupta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2015}, {"title": "A deep neural network compression pipeline: Pruning, quantization", "author": ["S. Han", "H. Mao", "W.J. Dally"], "venue": "Huffman encoding", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Spatial pyramid pooling in deep convolutional networks for visual recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2014\\E", "shortCiteRegEx": "He et al\\.", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Distilling knowledge in a neural network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "In Proc. Deep Learning and Representation Learning NIPS Workshop,", "citeRegEx": "Hinton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": null, "citeRegEx": "Ioffe and Szegedy,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy", "year": 2015}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Fixed-point feedforward deep neural network design using weights +1, 0 and -1", "author": ["H. Kyuyeon", "W. Sung"], "venue": "In IEEE Workshop on Signal Processing Systems (SiPS),", "citeRegEx": "Kyuyeon and Sung,? \\Q2014\\E", "shortCiteRegEx": "Kyuyeon and Sung", "year": 2014}, {"title": "Overcoming challenges in fixed point training of deep convolutional networks", "author": ["D.D. Lin", "S.S. Talathi"], "venue": "In ICML Workshop,", "citeRegEx": "Lin and Talathi,? \\Q2016\\E", "shortCiteRegEx": "Lin and Talathi", "year": 2016}, {"title": "Neural networks with few multiplications", "author": ["Z. Lin", "M. Courbariaux", "R. Memisevic", "Y. Bengio"], "venue": null, "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "XNOR-Net: ImageNet classification using binary convolutional neural networks", "author": ["M. Rastegari", "V. Ordonez", "J. Redmon", "A. Farhadi"], "venue": null, "citeRegEx": "Rastegari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rastegari et al\\.", "year": 2016}, {"title": "CNN features off-the-shelf: An astounding baseline for recognition", "author": ["A.S. Razavian", "H. Azizpour", "J. Sullivan", "Carlsson"], "venue": "In CVPR,", "citeRegEx": "Razavian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Razavian et al\\.", "year": 2014}, {"title": "Fixed point optimization of deep convolutional neural networks for object recognition", "author": ["A. Sajid", "H. Kyuyeon", "W. Sung"], "venue": "In IEEE International Conference on Acoustic, Speech and Signal Processing,", "citeRegEx": "Sajid et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sajid et al\\.", "year": 2015}, {"title": "OverFeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": null, "citeRegEx": "Sermanet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2013}, {"title": "Image and Video Compression for Multimedia Engineering: Fundamentals, Algorithms, and Standards", "author": ["Shi", "Yun Q", "Sun", "Huifang"], "venue": null, "citeRegEx": "Shi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2008}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": null, "citeRegEx": "Simonyan and Zisserman,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman", "year": 2014}, {"title": "Improving the speed of neural networks on CPUs", "author": ["V. Vanhoucke", "A. Senior", "M. Mao"], "venue": "In Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop,", "citeRegEx": "Vanhoucke et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vanhoucke et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 13, "context": "Recent advances in the development of deep convolution networks (DCNs) have led to significant progress in solving non-trivial machine learning problems involving image recognition (Krizhevsky et al., 2012) and speech recognition (Deng et al.", "startOffset": 181, "endOffset": 206}, {"referenceID": 4, "context": ", 2012) and speech recognition (Deng et al., 2013).", "startOffset": 31, "endOffset": 50}, {"referenceID": 1, "context": "advances in the design of DCNs (Zeiler & Fergus, 2014; Simonyan & Zisserman, 2014; Szegedy et al., 2014; Chatfield et al., 2014; He et al., 2014; Ioffe & Szegedy, 2015) have not only led to a further boost in achieved accuracy on image recognition tasks but also have played a crucial role as a feature generator for other machine learning tasks such as object detection (Krizhevsky et al.", "startOffset": 31, "endOffset": 168}, {"referenceID": 9, "context": "advances in the design of DCNs (Zeiler & Fergus, 2014; Simonyan & Zisserman, 2014; Szegedy et al., 2014; Chatfield et al., 2014; He et al., 2014; Ioffe & Szegedy, 2015) have not only led to a further boost in achieved accuracy on image recognition tasks but also have played a crucial role as a feature generator for other machine learning tasks such as object detection (Krizhevsky et al.", "startOffset": 31, "endOffset": 168}, {"referenceID": 13, "context": ", 2014; Ioffe & Szegedy, 2015) have not only led to a further boost in achieved accuracy on image recognition tasks but also have played a crucial role as a feature generator for other machine learning tasks such as object detection (Krizhevsky et al., 2012) and localization (Sermanet et al.", "startOffset": 233, "endOffset": 258}, {"referenceID": 20, "context": ", 2012) and localization (Sermanet et al., 2013), semantic segmentation (Girshick et al.", "startOffset": 25, "endOffset": 48}, {"referenceID": 5, "context": ", 2013), semantic segmentation (Girshick et al., 2014) and image retrieval (Krizhevsky et al.", "startOffset": 31, "endOffset": 54}, {"referenceID": 13, "context": ", 2014) and image retrieval (Krizhevsky et al., 2012; Razavian et al., 2014).", "startOffset": 28, "endOffset": 76}, {"referenceID": 18, "context": ", 2014) and image retrieval (Krizhevsky et al., 2012; Razavian et al., 2014).", "startOffset": 28, "endOffset": 76}, {"referenceID": 1, "context": "These advances have come with an added cost of computational complexity, resulting from DCN designs involving any combinations of: increasing the number of layers in the DCN (Szegedy et al., 2014; Simonyan & Zisserman, 2014; Chatfield et al., 2014), increasing the number of filters per convolution layer (Zeiler & Fergus, 2014), decreasing stride per convolution layer (Sermanet et al.", "startOffset": 174, "endOffset": 248}, {"referenceID": 20, "context": ", 2014), increasing the number of filters per convolution layer (Zeiler & Fergus, 2014), decreasing stride per convolution layer (Sermanet et al., 2013; Simonyan & Zisserman, 2014) and hybrid architectures that combine various DCN layers (Szegedy et al.", "startOffset": 129, "endOffset": 180}, {"referenceID": 9, "context": ", 2013; Simonyan & Zisserman, 2014) and hybrid architectures that combine various DCN layers (Szegedy et al., 2014; He et al., 2014; Ioffe & Szegedy, 2015).", "startOffset": 93, "endOffset": 155}, {"referenceID": 17, "context": "the second approach may produce networks with superior accuracy numbers (Rastegari et al., 2016; Lin & Talathi, 2016), it requires tight integration between the network design, training and implementation, which is not always feasible.", "startOffset": 72, "endOffset": 117}, {"referenceID": 3, "context": "Fixed point implementation of DCNs has been explored in earlier works (Courbariaux et al., 2014; Gupta et al., 2015).", "startOffset": 70, "endOffset": 116}, {"referenceID": 7, "context": "Fixed point implementation of DCNs has been explored in earlier works (Courbariaux et al., 2014; Gupta et al., 2015).", "startOffset": 70, "endOffset": 116}, {"referenceID": 3, "context": "Fixed point implementation of DCNs has been explored in earlier works (Courbariaux et al., 2014; Gupta et al., 2015). These works primarily focused on training DCNs using low precision fixed-point arithmetic. More recently, Lin et al. (2015) showed that deep neural networks can be effectively trained using only binary weights, which in some cases can even improve classification accuracy relative to the floating point baseline.", "startOffset": 71, "endOffset": 242}, {"referenceID": 19, "context": "In a follow-up paper (Sajid et al., 2015), the authors applied their proposed algorithm to DCN models where they analyzed the quantization sensitivity of the network for each layer and then manually decide the quantization bit-widths.", "startOffset": 21, "endOffset": 41}, {"referenceID": 16, "context": "The works of Kyuyeon & Sung (2014); Sajid et al. (2015) more closely resemble our work.", "startOffset": 36, "endOffset": 56}, {"referenceID": 16, "context": "The works of Kyuyeon & Sung (2014); Sajid et al. (2015) more closely resemble our work. In Kyuyeon & Sung (2014), the authors propose a floating point to fixed point conversion algorithm for fully-connected networks.", "startOffset": 36, "endOffset": 113}, {"referenceID": 16, "context": "The works of Kyuyeon & Sung (2014); Sajid et al. (2015) more closely resemble our work. In Kyuyeon & Sung (2014), the authors propose a floating point to fixed point conversion algorithm for fully-connected networks. The authors used an exhaustive search strategy to identify optimal fixed point bit-width for the entire network. In a follow-up paper (Sajid et al., 2015), the authors applied their proposed algorithm to DCN models where they analyzed the quantization sensitivity of the network for each layer and then manually decide the quantization bit-widths. Other works that are somewhat closely related are Vanhoucke et al. (2011); Gong et al.", "startOffset": 36, "endOffset": 639}, {"referenceID": 6, "context": "(2011); Gong et al. (2014). Vanhoucke et al.", "startOffset": 8, "endOffset": 27}, {"referenceID": 6, "context": "(2011); Gong et al. (2014). Vanhoucke et al. (2011) quantized the weights and activations of pre-trained deep networks using 8-bit fixed-point representation to improve inference speed.", "startOffset": 8, "endOffset": 52}, {"referenceID": 6, "context": "(2011); Gong et al. (2014). Vanhoucke et al. (2011) quantized the weights and activations of pre-trained deep networks using 8-bit fixed-point representation to improve inference speed. Gong et al. (2014) on the other hand applied codebook based on scalar and vector quantization methods in order to reduce the model size.", "startOffset": 8, "endOffset": 205}, {"referenceID": 6, "context": "(2011); Gong et al. (2014). Vanhoucke et al. (2011) quantized the weights and activations of pre-trained deep networks using 8-bit fixed-point representation to improve inference speed. Gong et al. (2014) on the other hand applied codebook based on scalar and vector quantization methods in order to reduce the model size. In the spirit of Sajid et al. (2015), we also focus on optimizing DCN models that are pre-trained with floating point precision.", "startOffset": 8, "endOffset": 360}, {"referenceID": 6, "context": "(2011); Gong et al. (2014). Vanhoucke et al. (2011) quantized the weights and activations of pre-trained deep networks using 8-bit fixed-point representation to improve inference speed. Gong et al. (2014) on the other hand applied codebook based on scalar and vector quantization methods in order to reduce the model size. In the spirit of Sajid et al. (2015), we also focus on optimizing DCN models that are pre-trained with floating point precision. However, as opposed to exhaustive search method adopted by Sajid et al. (2015), our objective is to convert the pre-trained DCN model into a fixed-point model using an optimization strategy based on signal-toquantization-noise-ratio (SQNR).", "startOffset": 8, "endOffset": 531}, {"referenceID": 6, "context": "(2011); Gong et al. (2014). Vanhoucke et al. (2011) quantized the weights and activations of pre-trained deep networks using 8-bit fixed-point representation to improve inference speed. Gong et al. (2014) on the other hand applied codebook based on scalar and vector quantization methods in order to reduce the model size. In the spirit of Sajid et al. (2015), we also focus on optimizing DCN models that are pre-trained with floating point precision. However, as opposed to exhaustive search method adopted by Sajid et al. (2015), our objective is to convert the pre-trained DCN model into a fixed-point model using an optimization strategy based on signal-toquantization-noise-ratio (SQNR). In doing so, we aim to improve upon the inference speed of the network and reduce storage requirements. The benefit of our approach as opposed to the brute force method is that it is grounded in a theoretical framework and offers an analytical solution for bit-width choice per layer to optimize the SQNR for the network. This offers an easier path to generalize to networks with significantly large number of layers such as the one recently proposed by He et al. (2015).", "startOffset": 8, "endOffset": 1164}, {"referenceID": 2, "context": "(2014), (b) compressing neural networks using hashing (Chen et al., 2015), and (c) combining pruning and quantization during training to reduce the model size without affecting the accuracy (Han et al.", "startOffset": 54, "endOffset": 73}, {"referenceID": 8, "context": ", 2015), and (c) combining pruning and quantization during training to reduce the model size without affecting the accuracy (Han et al., 2015).", "startOffset": 124, "endOffset": 142}, {"referenceID": 9, "context": "Other approaches to handle complexity of deep networks include: (a) leveraging high complexity networks to boost performance of low complexity networks, as proposed in Hinton et al. (2014), (b) compressing neural networks using hashing (Chen et al.", "startOffset": 168, "endOffset": 189}, {"referenceID": 16, "context": "This may be attributed to the regularization effect of the added quantization noise (Lin et al., 2015; Luo & Yang, 2014).", "startOffset": 84, "endOffset": 120}], "year": 2016, "abstractText": "In recent years increasingly complex architectures for deep convolution networks (DCNs) have been proposed to boost the performance on image recognition tasks. However, the gains in performance have come at a cost of substantial increase in computation and model storage resources. Fixed point implementation of DCNs has the potential to alleviate some of these complexities and facilitate potential deployment on embedded hardware. In this paper, we propose a quantizer design for fixed point implementation of DCNs. We formulate and solve an optimization problem to identify optimal fixed point bit-width allocation across DCN layers. Our experiments show that in comparison to equal bitwidth settings, the fixed point DCNs with optimized bit width allocation offer> 20% reduction in the model size without any loss in accuracy on CIFAR-10 benchmark. We also demonstrate that fine-tuning can further enhance the accuracy of fixed point DCNs beyond that of the original floating point model. In doing so, we report a new state-of-the-art fixed point performance of 6.78% error-rate on CIFAR-10 benchmark.", "creator": "LaTeX with hyperref package"}}}