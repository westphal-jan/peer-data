{"id": "1511.06396", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Multilingual Relation Extraction using Compositional Universal Schema", "abstract": "When building a knowledge base (KB) of entities and relations from multiple structured KBs and text, universal schema represents the union of all input schema, by jointly embedding all relation types from input KBs as well as textual patterns expressing relations. In previous work, textual patterns are parametrized as a single embedding, preventing generalization to unseen textual patterns. In this paper we employ an LSTM to compositionally capture the semantics of relational text. We dramatically demonstrate the flexibility of our approach by evaluating in a multilingual setting, in which the English training data entities overlap with the seed KB, but the Spanish text does not. Additional improvements are obtained by tying word embeddings across languages. In extensive experiments on the English and Spanish TAC KBP benchmark, our techniques provide substantial accuracy improvements. Furthermore we find that training with the additional non-overlapping Spanish also improves English relation extraction accuracy. Our approach is thus suited to broad-coverage automated knowledge base construction in low-resource domains and languages.", "histories": [["v1", "Thu, 19 Nov 2015 21:42:23 GMT  (36kb)", "https://arxiv.org/abs/1511.06396v1", "Under review as a conference paper at ICLR 2016"], ["v2", "Thu, 3 Mar 2016 20:28:36 GMT  (313kb,D)", "http://arxiv.org/abs/1511.06396v2", "Accepted to NAACL 2016"]], "COMMENTS": "Under review as a conference paper at ICLR 2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["patrick verga", "david belanger", "emma strubell", "benjamin roth", "andrew mccallum"], "accepted": true, "id": "1511.06396"}, "pdf": {"name": "1511.06396.pdf", "metadata": {"source": "CRF", "title": "Multilingual Relation Extraction using Compositional Universal Schema", "authors": ["Patrick Verga", "David Belanger", "Emma Strubell", "Benjamin Roth", "Andrew McCallum"], "emails": ["mccallum}@cs.umass.edu"], "sections": [{"heading": null, "text": "In most previous applications of the universal scheme, each text pattern is presented as a single embedding, preventing the generalization of invisible patterns. Recent work uses a neural network to capture the compositional semantics of patterns and provides a generalization for all possible input texts. In response, this paper introduces significant further improvements in the coverage and flexibility of the universal schema relation extraction: predictions for entities that do not occur in training, and multilingual transfer learning in non-annotated domains. We evaluate our model using extensive experiments on the English and Spanish TAC KBP benchmarks, surpassing the top system of TAC 2013 slot filling without handwritten patterns or additional annotation. We also consider a multilingual environment in which English training data units overlap with the seed domains, but not Spanish text. Although we do not have an English annotation database for additional annotation, we have additional anecdotation capabilities in the construction of the seed domains."}, {"heading": "1 Introduction", "text": "This year we have the opportunity to establish ourselves in another country, in a country where we are able to live in another country, \"he said."}, {"heading": "2 Background", "text": "AKBC extracts simple attributes of form (subject, attribute), typed binary relationships of form (subject, relation, object), or relationships of higher order. We refer to subjects and objects as entities. This work focuses exclusively on the extraction of binary relationships, although many of our techniques naturally generalize to simple predictions. Generally, relationships of higher order, for example, are expressed in Freebase (Bollacker et al., 2008) in terms of collections of binary relations. We will now describe previous work on approaches to AKBC. They all aim to predict (s, r, o) triples, but differ in terms of: (1) input data that is foreign-determined, (2) types of annotations are required, (3) definition of relation label schemes, and (4) whether they are able to predict relationships for entities that are not visible in the training data. Note that all these methods may require preliminary processing to perform an additional KB."}, {"heading": "2.1 Relation Extraction as Link Prediction", "text": "A knowledge base is naturally described as a graph in which units are nodes and relationships are referred to as edges (Suchanek et al., 2007; Bollacker et al., 2008). In the case of completion of knowledge graphs, the task of linking predictions is similar, assuming an initial set of (s, r, o) triples. See Nickel et al. (2015) for a review. No accompanying text data is required, as linkages can be predicted based on properties of the graph, such as transitivity. To generalize well, the prediction is often presented as a matrix or tensor factoring. A variety of model variants have been suggested where the probability of an existing edge depends on a multilinear form (Nickel et al., 2011; Garc\u00ed'a-Dura'n et al., 2015; Yanget al., 2015; Bordes et al., 2014; linear approaches, or not)."}, {"heading": "2.2 Relation Extraction as Sentence Classification", "text": "In this case, the training data consists of (1) a text corpus and (2) a KB of seed facts with provenance, i.e. evidence, in the corpus. In view of a single sentence and pre-defined units, a classifier predicts whether the sentence expresses a relationship to a target scheme. To train such a classifier, KB facts must be reconciled with evidence in the text, but this is often difficult. For example, not all sentences containing Barack and Michelle Obama indicate that they are married. A variety of bullet and iterative methods have addressed the alignment problem (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010; Surdeanu et al., 2012; Min et al., 2013; Zeng et al., 2015). An additional measure of freedom in these approaches is whether they classify individual sentences or group them on a basket level, preferably between individual sentences and practices."}, {"heading": "2.3 Open-Domain Relation Extraction", "text": "In the two preceding approaches, the prediction of a defined scheme R of possible relationships r is performed, overlooking distinctive relationships that are expressed in the text but do not occur in the scheme. In response, the Open Domain Information Extraction (OpenIE) lets the text speak for itself: R contains all possible text patterns that occur between units s and o (Banko et al., 2007; Etzioni et al., 2008; Yates and Etzioni, 2007), achieved by filtering and normalizing the raw text. It provides impressive coverage, avoids remote monitoring problems, and provides a useful exploratory tool. On the other hand, OpenIE predictions are difficult to use in downstream tasks that require information from a fixed schema. Table 1 provides examples of OpenIE patterns. The examples in series two and three illustrate relational contexts for which similarities are difficult to be captured by an OpenIE approach due to their syntactically complex constructions."}, {"heading": "2.4 Universal Schema", "text": "When applying universal scheme (Riedel et al., 2013) (USchema) to relationship > r patterns, we combine the OpenIE and link prediction perspectives. By jointly modelling both OpenIE patterns and the elements of a target scheme, the method captures broader relationship patterns than multi-class classification approaches that model only the target scheme. In addition, the method avoids the remote monitoring alignment difficulties of section 2.2.Riedel et al. (2013) expand a knowledge graph from a seed KB with additional edges that are observed in the corpus. Even if the user does not attempt to predict these new edges, a common model across all edges regularities of the OpenIE edges edges can be used to improve the labels from the target scheme. The data still consists of (s, r, o) triples that are pushed out with the linking of these new edges corners."}, {"heading": "2.5 Multilingual Embeddings", "text": "Most of this work uses aligned sentences from the Europarl dataset (Koehn, 2005) to align word embedding across languages (Gouws et al., 2015; Luong et al., 2015; Hermann and Blunsom, 2014); others (Mikolov et al., 2013; Faruqui et al., 2014) align separate singleanguage embedding models using a word-level dictionary. Mikolov et al. (2013) use translation pairs to learn a linear transformation from one embedding space to another. However, there is very little work on multilingual relationship extraction. Faruqui and Kumar (2015) perform multilingual OpenIE relation extraction by projecting all languages into English using Google. However, as explained in Section 2.3, the OpenIE paradigm is not capable of predicting within a fixed scheme. Furthermore, we can generalize our translation methodology to improve our own translation methodology, where this is not available."}, {"heading": "3 Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Universal Schema as Sentence Classifier", "text": "Similar to many left-handed predictions, this is also about transductive learning patterns, where a model is learned jointly through traction and test data. Predictions are made by using the model to identify edges that were not observed in the test data but are likely to be true. It is prone to the cold-start problem in collab Figure 2: Universal scheme jointly embedded KB and textual relationships from Spanish and English, which avoid dense representations of entity pairs and relationships using matrix factorization. Cells with a 1 point to triples observed during training (left). The bold score represents a test-time prediction by the model (right). Using the transitivity of KB / English overlaps, our model can predict that a text pattern in Spain produces a KB relationship despite no overlap between Spain / KB entity pairs."}, {"heading": "3.2 Using a Compositional Sentence Encoder to Predict Unseen Text Patterns", "text": "The pattern scoring approach is subject to an additional cold-start problem: input data may contain patterns not seen in the training. This section describes a method by which we use USchema to form a relation identifier that inputs arbitrary context markers (Section 2.3). Fortunately, the cold-start problem for context markers is less harmful than that of entities, since we can exploit statistical regularities in the text: Similar sequences of context markers should be similarly embedded. Therefore, following Toutanova et al. (2015), we avoid embedding raw context markers compositively using a deep architecture. Unlike Riedel et al. (2013), this does not require manual rules to map text to OpenIE patterns and embed possible input strings. The modified USchema similarity is: P (s, r, o)) = Spheres (u s > oder."}, {"heading": "3.3 Modeling Frequent Text Patterns", "text": "Despite the coverage advantages of using a deep typewriter, the separate embedding of each OpenIE pattern, as in Riedel et al. (2013), has significant advantages. In practice, we have found that many high-precision patterns occur quite frequently, and there is enough data to model them with independent embedding per pattern, resulting in a minimal inductive bias in the relationship between patterns. In addition, some discriminatory phrases are idiomatic, i.e. their meaning is not constructed compositively from their components, for which a sentence coder may be inappropriate. Therefore, pattern embedding and deep token-based encoders have very different strengths and weaknesses. One defines specificity and models the head of the text distribution well, while the other has high coverage and captures the tail. In experimental results, we show that the interaction of both models performs much better than both isolates."}, {"heading": "3.4 Multilingual Relation Extraction with Zero Annotation", "text": "The models described in the previous two sections provide broad relation extraction that can be generalized to all kinds of input devices and text patterns, while avoiding error-prone alignment of remote supervision to a corpus. Next, we describe techniques for an even more difficult generalization task: relation classification for input sentences in completely different languages. Formation of a sentence-level relation classifier requires either the use of the alignment-based techniques of Section 2.2 or the alignment-free method of Section 3.1 an available KB of seed facts that have evidence in the corpus. Unfortunately, the available KBs overlap only slightly with corpora in many languages, as the KBs have cultural and geographical distortions. In response, we perform a multilingual relation extraction by collectively modelling a high-resource language, such as English, and an alternative language without notation of KB."}, {"heading": "3.5 Tied Sentence Encoders", "text": "The sentence encoder approach of Section 3.2 complements our multilingual modeling technique: We simply use a separate encoder for each language. However, this approach is suboptimal, as each sentence encoder has a separate matrix of word embeddings for its vocabulary, although there may be a significant common structure between languages. In response, we propose a simple method of linking the parameters of sentence encoders across languages. Using the dictionary-based techniques described in Section 2.5, we first obtain a list of word-word translation pairs between languages using a translation dictionary. The first level of our deep text encoder consists of an embedding table. For the associated word types, we use a single cross-language embeddings. Details of our approach are described in Appendix 7.5."}, {"heading": "4 Task and System Description", "text": "We focus on the TAC KBP embedding task. Many related work on the embedding of knowledge bases evaluates the FB15k dataset (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Yang et al., 2015; Toutanova et al., 2015). Here, relation extraction is used as a link predictor for a subset of the freebase. This task does not capture the specific difficulties we are dealing with: (1) evaluation of entities and text that are not visible during the training, and (2) zero notation learning of a predictor for a language with limited resources. Also, note that both Toutanova et al. (2015) and Riedel et al. (2013) examine the advantages and disadvantages of learning embedding for entities as compared to separate embedding for each entity. As this is orthogonal to our contributions, we only consider the entries at the end of the pair-only if there are sufficient entries."}, {"heading": "4.1 TAC Slot-Filling Benchmark", "text": "The objective of the TAC benchmark is to improve both the scope and quality of the evaluation of relation extraction, compared to merely checking the extracted facts against a knowledge base that may be incomplete and in which the provenances are not verified. In the task of filling the slots, each system is given a series of paired query units and relationships or \"slots\" to fill, and the goal is to fill as many slots as possible along with the provenance from the corpus. For example, the responses of all participating teams, along with a human search (with timeout), are manually assessed for accuracy, i.e. whether the provenance specified by the system actually expresses the relationship in the question. In addition to reviewing our models to the English slot filling task of 2013 and 2014, we are classified our Spanish models as flawed based on the Spanish 2012 lot evaluation."}, {"heading": "4.2 Retrieval Pipeline", "text": "Our retrieval pipeline first generates all valid slot filler candidates for each query unit and slot, based on entities extracted from the corpus using FACTORIE (McCallum et al., 2009) to perform tokenization, segmentation, and entity extraction. We perform entity linkages by heuristically linking all entity mentions from our text corpus to a freebase entity using anchor text in Wikipedia. Taking advantage of the fact that most freebase entries contain a link to the corresponding Wikipedia page, we link all1Following Surdeanu et al. (2012) we remove facts about undiscovered entities to correct them for a reminder. Entity mentions from our text corporia are linked to a freebase entity by the following process: First, a set of undiscovered entities is achieved by the following linkage entries."}, {"heading": "4.3 Model Details", "text": "All models are designed to maximize Formula 1 on the 2012 TAC KBP Slot Fill Rating. Additionally, we adjust the thresholds of our model goal scorer based on a ratio to maximize Formula 1 based on the 2012 TAC Slot Fill Rating for English and the 2012 Spanish Slot Fill Rating for Spanish. As in Riedel et al. (2013), we train based on the BPR loss of Rendle et al. (2009). Our CNN is implemented as in Toutanova et al. (2015), with width 3 windings, followed by tanh and max pool layers. LSTM uses a bidirectional architecture that averages the forward and backward representations of each hidden state, followed by maximum pooling over time. See Section 7.2We also report the results, including an alternative name (AN) heuristic, which uses automatically extracted rules to detect the TAC. \""}, {"heading": "5 Experimental Results", "text": "In experiments on the English and Spanish TAC-KBC slot filling tasks, we found that both USchema and LSTM models perform better across all languages than CNN, and that the LSTM tends to perform slightly better than USchema as the only model. The similarity of the LSTM and USchema models further increases the final F1 values in all experiments, suggesting that the two different model types complement each other well. In fact, in Section 5.3, we present quantitative and qualitative analyses of our results, which further confirm this hypothesis: the LSTM and USchema models perform better at different sample lengths and are distinguished by different precision recall compromises."}, {"heading": "5.1 English TAC Slot-filling Results", "text": "Tables 2 and 3 present the performance of our models in the 2013 and 2014 English TAC slot filling tasks. Taking into account the alternative names (AN) described in Section 4.3 heuristically increases Formula 1 by a further 2 points over 2013, resulting in an F1 score that competes with the state of the art. We also show the effect of learning English and Spanish models together on the performance of English slot filling. Taking into account Spanish data improves our Formula 1 scores by 1.5 points over 2013 and 1.1 over 2014 compared to using English alone. These places are higher than the top position in the 2013 TAC slot filling task, although our system does not use handwritten rules. The state of the art of the technical systems in this task is all based on matching handwritten patterns to find additional answers, while our models only use automatically generated indirect supervision; even our AN heuristics (Section 4.2) are automatically generated."}, {"heading": "5.2 Spanish TAC Slot-filling Results", "text": "Linking word embeddings between the two languages results in significant improvements for the LSTM. We see that linking the non-verbal LSTM with USchema alone brings a slight boost over USchema, but linking the dictionary-bound LSTM with USchema brings a significant increase of almost 4 F1 points over the highest-rated single model USchema. Grounding Spanish data using a translation dictionary clearly provides much better Spanish word representation. These improvements complement the USchema basic model and deliver impressive results when combined. In addition to embedding semantically similar phrases from English and Spanish to have a high similarity, our models also learn high-quality multilingual word embeddings. In Table 5, we compare Spanish neighbors of English queer words learned from LSTM with dictionaries from English and Spanish, while using our STM to achieve high-quality word embeddings without multilingual similarity."}, {"heading": "5.3 USchema vs LSTM", "text": "We continue to analyze the differences between the USchema and LSTM to better understand why the matching of the models results in the best functioning system. Figure 3 shows precise recall curves for the two models on the slot fill task of 2013. As observed in previous results, the LSTM achieves a higher recall with loss of some precision, whereas the LSTM can make more accurate predictions at a lower threshold for recalls. In Figure 4, we observe evidence for these different recalls: USchema performs higher in F1 on shorter patterns, while the LSTM scores higher on longer patterns. Predictably, USchema successfully matches shorter patterns than the LSTM, making more precise predictions at the expense of inability to predict patterns that cannot be seen during training. LSTM can be predicted from any text between the units observed at the test date by achieving reset factors in the loss of rests."}, {"heading": "6 Conclusion", "text": "By embedding English and Spanish corpora together with a KB, we can train an accurate Spanish relation extraction model, without direct annotations for relationships in the Spanish data. This approach has the added advantage of providing significant improvements in accuracy for the English model, surpassing the top of the class in the 2013 TAC KBC slot fill task, without using the hand-coded rules or additional annotations of alternative systems. By using deep sentence encoders, we can make predictions for any input text and for units invisible in education. Sentence encoders also offer opportunities to improve lingual transmission learning by sharing word embeddings between different languages. In future work, we will apply this model to many other languages and domains in addition to the newswire text. We also want to avoid the problem of entity detection by using a deep architecture to identify both entity changes and relationships between them."}, {"heading": "Acknowledgments", "text": "Many thanks to Arvind Neelakantan for good ideas and discussion. We are also pleased to receive a generous hardware grant from nVidia. This work was supported in part by the Center for Intelligent Information Retrieval, in part by the Defense Advanced Research Projects Agency (DARPA) under Agreement # FA8750-13-2-0020 and Contract # HR0011-15-2-0036, and in part by the grant numbers DMR1534431, IIS-1514053, and CNS-0958392 of the National Science Foundation (NSF). The U.S. government is authorized to reproduce and distribute copies for government purposes, notwithstanding any copyright notice thereof, in part by DARPA through Agreement # DFA8750-13-2-0020 and the NSF grant # CNS0958392. All opinions, results, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the sponsor's."}, {"heading": "7 Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Additional Qualitative Results", "text": "Qualitative analysis of our multilingual models also suggests that they successfully embed semantically similar relationships between languages, using bound entities pairs and translation dictionaries as grounding. Table 7 lists three closest neighbors in English for multiple Spanish patterns from the text. In any case, the English patterns capture the relationship represented in the Spanish text. Our model embeds KB relationships along with English and Spanish text. We show that plausible text patterns are embedded close to the KB relationships they express. Table 8 shows English and Spanish patterns with the highest scores, which are derived from sample relationships from our TAC KB."}, {"heading": "7.2 Implementation and Hyperparameters", "text": "We conducted a small web search on the learning rates 0.0001, 0.005, 0.001, dropouts 0.0, 0.1, 0.25, 0.5, dimension 50, 100, '2 Gradient Section 1, 10, 50 and epsilon 1e-8, 1e-6, 1e-4. All models are trained for a maximum of 15 epochs. CNN and LSTM both use 100-D embeddings, while USchema 50-D. Both CNN and LSTM learned 100-dimensional word embeddings that were randomly initialized, and the use of pre-trained embeddings had no significant impact on the results. Entity pairs for the USchema model are randomly initialized."}, {"heading": "7.3 Details Concerning Cosine Similarity Computation", "text": "We measure the similarity between rtext and rschema by calculating the cosmic similarity of the vectors. However, such a distance is not precisely defined, since the model was trained using internal products between entity vectors and relation vectors, not between two relation vectors. In the USA, the probability is invariable for invertible transformations of the latent coordinate system, since \u03c3 (u > s, ovr) = \u03c3 (((A > us, o) > A \u2212 1vr) for any invertible A. However, when taking internal products between two v-terms, the implicit A \u2212 1 terms do not cancel."}, {"heading": "7.4 Data Pre-processing, Distant Supervision and Extraction Pipeline", "text": "For each set, we then extract all entity pairs and the text between them as surface patterns, ignoring patterns longer than 20 tokens, resulting in 48 million English \"relationships.\" In Section 7.6, we describe a technique for normalizing surface patterns. We filter out entity pairs that occurred less than 10 times in the data, and extract the largest coherent component in this entity occurrence diagram. This is necessary for the US base model, as otherwise learning will be decoupled into independent problems per associated component. Although the components are interconnected when using set coders, we only use a single component to facilitate a fair comparison between these modeling approaches. We add the remote supervision training factors from the RelationFactory system, i.e. 352,236 entity pair relationship patterns derived from freebase encoders, we use only one set of models to allow comparison between these individual components to form a fair component."}, {"heading": "7.5 Generation of Cross-Lingual Tied Word Types", "text": "First, for each language, we select the 6000 words that appear in the lowercase Europarl dataset and get a Google translation. Then, we filter duplicates and translations that result in multi-word phrases. Also, we remove participles from the English past (ending in -ed), because we find that the Google translation interprets them as adjectives (e.g. \"she read the borrowed book\" instead of \"she borrowed the book\") and much of the relationship structure we model in the language is captured by verbs, resulting in 6201 pairs of translations that occurred in our text corpus. Although higher-quality translation dictionaries would likely improve this technique, our experimental results show that such automatically generated dictionaries work well."}, {"heading": "7.6 Open IE Pattern Normalization", "text": "To improve the generalization of the US, our US relations use log-abbreviated patterns, where the middle symbols are simplified into patterns of more than five symbols. For each long pattern, we take the first two symbols and the last two symbols, and replace all k remaining symbols with the number log k. For example, Barack Obama's pattern would be converted to a person named Michelle Obama: Barack Obama is married [1] person named Michelle Obama. This shortening works slightly better than whole patterns. LSTM and CNN variants use the entire order of symbols."}], "references": [{"title": "Open information extraction from the web", "author": ["Banko et al.2007] Michele Banko", "Michael J Cafarella", "Stephen Soderland", "Matt Broadhead", "Oren Etzioni"], "venue": "In International Joint Conference on Artificial Intelligence", "citeRegEx": "Banko et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Banko et al\\.", "year": 2007}, {"title": "University of texas at austin kbp 2014 slot filling system: Bayesian logic programs for textual inference", "author": ["Bentor et al.2014] Yinon Bentor", "Vidhoon Viswanathan", "Raymond Mooney"], "venue": "In Proceedings of the Seventh Text Analysis Conference: Knowledge Base Population (TAC", "citeRegEx": "Bentor et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bentor et al\\.", "year": 2014}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Nicolas Usunier", "Alberto Garc\u0131\u0301a-Dur\u00e1n", "Jason Weston", "Oksana Yakhnenko"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Bordes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Question answering with subgraph embeddings", "author": ["Sumit Chopra", "Jason Weston"], "venue": "arXiv preprint arXiv:1406.3676", "citeRegEx": "Bordes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Learning to extract relations from the web using minimal supervision", "author": ["Bunescu", "Mooney2007] Razvan Bunescu", "Raymond Mooney"], "venue": "In Annual meeting-association for Computational Linguistics,", "citeRegEx": "Bunescu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bunescu et al\\.", "year": 2007}, {"title": "Toward an architecture for never-ending language learning", "author": ["Justin Betteridge", "Bryan Kisiel", "Burr Settles", "Estevam R. Hruschka"], "venue": null, "citeRegEx": "Carlson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Carlson et al\\.", "year": 2010}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Classifying relations by ranking with convolutional neural networks", "author": ["Bing Xiang", "Bowen Zhou"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference", "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Open information extraction from the web", "author": ["Etzioni et al.2008] Oren Etzioni", "Michele Banko", "Stephen Soderland", "Daniel S Weld"], "venue": "Communications of the ACM,", "citeRegEx": "Etzioni et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Etzioni et al\\.", "year": 2008}, {"title": "Multilingual open relation extraction using crosslingual projection", "author": ["Faruqui", "Kumar2015] Manaal Faruqui", "Shankar Kumar"], "venue": "arXiv preprint arXiv:1503.06450", "citeRegEx": "Faruqui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Jesse Dodge", "Sujay K Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A Smith"], "venue": "arXiv preprint arXiv:1411.4166", "citeRegEx": "Faruqui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2014}, {"title": "Combining two and three-way embeddings models for link prediction in knowledge bases. CoRR, abs/1506.00999", "author": ["Antoine Bordes", "Nicolas Usunier", "Yves Grandvalet"], "venue": null, "citeRegEx": "Garc\u0131\u0301a.Dur\u00e1n et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Garc\u0131\u0301a.Dur\u00e1n et al\\.", "year": 2015}, {"title": "Incorporating vector space similarity in random walk inference over knowledge bases", "author": ["Gardner et al.2014] Matt Gardner", "Partha Talukdar", "Jayant Krishnamurthy", "Tom Mitchell"], "venue": "In Empirical Methods in Natural Language Processing", "citeRegEx": "Gardner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gardner et al\\.", "year": 2014}, {"title": "Traversing knowledge graphs in vector space. arXiv preprint arXiv:1506.01094", "author": ["Gu et al.2015] Kelvin Gu", "John Miller", "Percy Liang"], "venue": null, "citeRegEx": "Gu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2015}, {"title": "Multilingual models for compositional distributed semantics. arXiv preprint arXiv:1404.4641", "author": ["Hermann", "Blunsom2014] Karl Moritz Hermann", "Phil Blunsom"], "venue": null, "citeRegEx": "Hermann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "In Neural Computation", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Knowledge-based weak supervision for information extraction of overlapping relations", "author": ["Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S Weld"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Hoffmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2011}, {"title": "A convolutional neural network for modelling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": "Proceedings of the 52nd Annual Meeting of the Association", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": null, "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Adam: A method for stochastic optimization. In 3rd International Conference for Learning Representations (ICLR)", "author": ["Kingma", "Ba2015] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Philipp Koehn"], "venue": "In MT summit,", "citeRegEx": "Koehn.,? \\Q2005\\E", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Random walk inference and learning in a large scale knowledge base", "author": ["Lao et al.2011] Ni Lao", "Tom Mitchell", "William W. Cohen"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Lao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lao et al\\.", "year": 2011}, {"title": "Reading the web with learned syntactic-semantic inference rules", "author": ["Lao et al.2012] Ni Lao", "Amarnag Subramanya", "Fernando Pereira", "William W. Cohen"], "venue": "In Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning", "citeRegEx": "Lao et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lao et al\\.", "year": 2012}, {"title": "Zero-data learning of new tasks", "author": ["Dumitru Erhan", "Yoshua Bengio"], "venue": "In National Conference on Artificial Intelligence", "citeRegEx": "Larochelle et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2008}, {"title": "When are tree structures necessary for deep learning of representations? arXiv preprint arXiv:1503.00185", "author": ["Li et al.2015] Jiwei Li", "Dan Jurafsky", "Eudard Hovy"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Lin et al.2015] Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu"], "venue": "Proceedings of AAAI", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Bilingual word representations with monolingual quality in mind", "author": ["Luong et al.2015] Thang Luong", "Hieu Pham", "Christopher D Manning"], "venue": "In Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "FACTORIE: Probabilistic programming via imperatively defined factor graphs", "author": ["Karl Schultz", "Sameer Singh"], "venue": "In Neural Information Processing Systems (NIPS)", "citeRegEx": "McCallum et al\\.,? \\Q2009\\E", "shortCiteRegEx": "McCallum et al\\.", "year": 2009}, {"title": "Exploiting Similarities among Languages for Machine Translation", "author": ["Quoc V Le", "Ilya Sutskever"], "venue": "In arXiv preprint arXiv:1309.4168v1,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mintz et al.2009] Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky"], "venue": "In Association for Computational Linguistics and International Joint Conference on Natural Language Processing", "citeRegEx": "Mintz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Compositional vector space models for knowledge base completion", "author": ["Benjamin Roth", "Andrew McCallum"], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "A three-way model for collective learning on multi-relational data", "author": ["Volker Tresp", "HansPeter Kriegel"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Nickel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "A review of relational machine learning for knowledge graphs: From multirelational link prediction to automated knowledge graph construction. arXiv preprint arXiv:1503.00759", "author": ["Kevin Murphy", "Volker Tresp", "Evgeniy Gabrilovich"], "venue": null, "citeRegEx": "Nickel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2015}, {"title": "Bpr: Bayesian personalized ranking from implicit feedback", "author": ["Christoph Freudenthaler", "Zeno Gantner", "Lars Schmidt-Thieme"], "venue": "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Rendle et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Rendle et al\\.", "year": 2009}, {"title": "Modeling relations and their mentions without labeled text", "author": ["Limin Yao", "Andrew McCallum"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Riedel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2010}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["Limin Yao", "Andrew McCallum", "Benjamin M. Marlin"], "venue": "HLTNAACL", "citeRegEx": "Riedel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2013}, {"title": "Injecting logical background knowledge into embeddings for relation extraction", "author": ["Sameer Singh", "Sebastian Riedel"], "venue": "In Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)", "citeRegEx": "Rocktaschel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rocktaschel et al\\.", "year": 2015}, {"title": "Relationfactory: A fast, modular and effective system for knowledge base population", "author": ["Roth et al.2014] Benjamin Roth", "Tassilo Barth", "Grzegorz Chrupa\u0142a", "Martin Gropp", "Dietrich Klakow"], "venue": null, "citeRegEx": "Roth et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Roth et al\\.", "year": 2014}, {"title": "Methods and metrics for cold-start recommendations", "author": ["Alexandrin Popescul", "Lyle H Ungar", "David M Pennock"], "venue": "In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Schein et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Schein et al\\.", "year": 2002}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Danqi Chen", "Christopher D Manning", "Andrew Ng"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Yago: A core of semantic knowledge", "author": ["Gjergji Kasneci", "Gerhard Weikum"], "venue": "In Proceedings of the 16th International Conference on World Wide Web", "citeRegEx": "Suchanek et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Suchanek et al\\.", "year": 2007}, {"title": "Overview of the english slot filling track at the tac2014", "author": ["Surdeanu", "Ji.2014] Mihai Surdeanu", "Heng Ji"], "venue": null, "citeRegEx": "Surdeanu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2014}, {"title": "Multiinstance multi-label learning for relation extraction", "author": ["Julie Tibshirani", "Ramesh Nallapati", "Christopher D Manning"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural", "citeRegEx": "Surdeanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. V Le"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Representing text for joint embedding of text and knowledge bases", "author": ["Danqi Chen", "Patrick Pantel", "Hoifung Poon", "Pallavi Choudhury", "Michael Gamon"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Toutanova et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2015}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Wang et al.2014] Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "venue": "In Proceedings of the TwentyEighth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Classifying relations via long short term memory networks along shortest dependency paths", "author": ["Xu et al.2015] Yan Xu", "Lili Mou", "Ge Li", "Yunchuan Chen", "Hao Peng", "Zhi Jin"], "venue": "In Proceedings of Conference on Empirical Methods in Natural Language Processing (to appear)", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Embedding entities and relations for learning and inference in knowledge bases", "author": ["Yang et al.2015] Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng"], "venue": "International Conference on Learning Representations", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Collective cross-document relation extraction without labelled data", "author": ["Yao et al.2010] Limin Yao", "Sebastian Riedel", "Andrew McCallum"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Yao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2010}, {"title": "Universal schema for entity type prediction", "author": ["Yao et al.2013] Limin Yao", "Sebastian Riedel", "Andrew McCallum"], "venue": "In Proceedings of the 2013 workshop on Automated knowledge base construction,", "citeRegEx": "Yao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2013}, {"title": "Unsupervised resolution of objects and relations on the web", "author": ["Yates", "Etzioni2007] Alexander Yates", "Oren Etzioni"], "venue": "In North American Chapter of the Association for Computational Linguistics", "citeRegEx": "Yates et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yates et al\\.", "year": 2007}, {"title": "Distant supervision for relation extraction via piecewise convolutional neural networks. EMNLP", "author": ["Zeng et al.2015] Daojian Zeng", "Kang Liu", "Yubo Chen", "Jun Zhao"], "venue": null, "citeRegEx": "Zeng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": "The goal of automatic knowledge base construction (AKBC) is to build a structured knowledge base (KB) of facts using a noisy corpus of raw text evidence, and perhaps an initial seed KB to be augmented (Carlson et al., 2010; Suchanek et al., 2007; Bollacker et al., 2008).", "startOffset": 201, "endOffset": 270}, {"referenceID": 40, "context": "The goal of automatic knowledge base construction (AKBC) is to build a structured knowledge base (KB) of facts using a noisy corpus of raw text evidence, and perhaps an initial seed KB to be augmented (Carlson et al., 2010; Suchanek et al., 2007; Bollacker et al., 2008).", "startOffset": 201, "endOffset": 270}, {"referenceID": 35, "context": "Universal schema (Riedel et al., 2013) along with its extensions (Yao et al.", "startOffset": 17, "endOffset": 38}, {"referenceID": 49, "context": ", 2013) along with its extensions (Yao et al., 2013; Gardner et al., 2014; Neelakantan et al., 2015; Rocktaschel et al., 2015), avoids alignment by jointly embedding KB relations, entities, and surface text patterns.", "startOffset": 34, "endOffset": 126}, {"referenceID": 12, "context": ", 2013) along with its extensions (Yao et al., 2013; Gardner et al., 2014; Neelakantan et al., 2015; Rocktaschel et al., 2015), avoids alignment by jointly embedding KB relations, entities, and surface text patterns.", "startOffset": 34, "endOffset": 126}, {"referenceID": 30, "context": ", 2013) along with its extensions (Yao et al., 2013; Gardner et al., 2014; Neelakantan et al., 2015; Rocktaschel et al., 2015), avoids alignment by jointly embedding KB relations, entities, and surface text patterns.", "startOffset": 34, "endOffset": 126}, {"referenceID": 36, "context": ", 2013) along with its extensions (Yao et al., 2013; Gardner et al., 2014; Neelakantan et al., 2015; Rocktaschel et al., 2015), avoids alignment by jointly embedding KB relations, entities, and surface text patterns.", "startOffset": 34, "endOffset": 126}, {"referenceID": 44, "context": "Recently, Toutanova et al. (2015) addressed this issue by embedding text patterns using a deep sentence encoder, which captures the compositional semantics of textual relations and allows for prediction on inputs never seen before.", "startOffset": 10, "endOffset": 34}, {"referenceID": 40, "context": "A knowledge base is naturally described as a graph, in which entities are nodes and relations are labeled edges (Suchanek et al., 2007; Bollacker et al., 2008).", "startOffset": 112, "endOffset": 159}, {"referenceID": 31, "context": "A variety of model variants have been suggested, where the probability of a given edge existing depends on a multi-linear form (Nickel et al., 2011; Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Yang et al., 2015; Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015), or non-linear interactions between s, r, and o (Socher et al.", "startOffset": 127, "endOffset": 253}, {"referenceID": 11, "context": "A variety of model variants have been suggested, where the probability of a given edge existing depends on a multi-linear form (Nickel et al., 2011; Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Yang et al., 2015; Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015), or non-linear interactions between s, r, and o (Socher et al.", "startOffset": 127, "endOffset": 253}, {"referenceID": 47, "context": "A variety of model variants have been suggested, where the probability of a given edge existing depends on a multi-linear form (Nickel et al., 2011; Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Yang et al., 2015; Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015), or non-linear interactions between s, r, and o (Socher et al.", "startOffset": 127, "endOffset": 253}, {"referenceID": 2, "context": "A variety of model variants have been suggested, where the probability of a given edge existing depends on a multi-linear form (Nickel et al., 2011; Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Yang et al., 2015; Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015), or non-linear interactions between s, r, and o (Socher et al.", "startOffset": 127, "endOffset": 253}, {"referenceID": 45, "context": "A variety of model variants have been suggested, where the probability of a given edge existing depends on a multi-linear form (Nickel et al., 2011; Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Yang et al., 2015; Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015), or non-linear interactions between s, r, and o (Socher et al.", "startOffset": 127, "endOffset": 253}, {"referenceID": 25, "context": "A variety of model variants have been suggested, where the probability of a given edge existing depends on a multi-linear form (Nickel et al., 2011; Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Yang et al., 2015; Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015), or non-linear interactions between s, r, and o (Socher et al.", "startOffset": 127, "endOffset": 253}, {"referenceID": 39, "context": ", 2015), or non-linear interactions between s, r, and o (Socher et al., 2013).", "startOffset": 56, "endOffset": 77}, {"referenceID": 3, "context": "Other approaches model the compositionality of multi-hop paths, typically for question answering (Bordes et al., 2014; Gu et al., 2015; Neelakantan et al., 2015).", "startOffset": 97, "endOffset": 161}, {"referenceID": 13, "context": "Other approaches model the compositionality of multi-hop paths, typically for question answering (Bordes et al., 2014; Gu et al., 2015; Neelakantan et al., 2015).", "startOffset": 97, "endOffset": 161}, {"referenceID": 30, "context": "Other approaches model the compositionality of multi-hop paths, typically for question answering (Bordes et al., 2014; Gu et al., 2015; Neelakantan et al., 2015).", "startOffset": 97, "endOffset": 161}, {"referenceID": 25, "context": "See Nickel et al. (2015) for a review.", "startOffset": 4, "endOffset": 25}, {"referenceID": 29, "context": "A variety of one-shot and iterative methods have addressed the alignment problem (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Min et al., 2013; Zeng et al., 2015).", "startOffset": 81, "endOffset": 249}, {"referenceID": 34, "context": "A variety of one-shot and iterative methods have addressed the alignment problem (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Min et al., 2013; Zeng et al., 2015).", "startOffset": 81, "endOffset": 249}, {"referenceID": 48, "context": "A variety of one-shot and iterative methods have addressed the alignment problem (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Min et al., 2013; Zeng et al., 2015).", "startOffset": 81, "endOffset": 249}, {"referenceID": 16, "context": "A variety of one-shot and iterative methods have addressed the alignment problem (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Min et al., 2013; Zeng et al., 2015).", "startOffset": 81, "endOffset": 249}, {"referenceID": 42, "context": "A variety of one-shot and iterative methods have addressed the alignment problem (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Min et al., 2013; Zeng et al., 2015).", "startOffset": 81, "endOffset": 249}, {"referenceID": 51, "context": "A variety of one-shot and iterative methods have addressed the alignment problem (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Min et al., 2013; Zeng et al., 2015).", "startOffset": 81, "endOffset": 249}, {"referenceID": 24, "context": "Prior work has applied deep learning to small-scale relation extraction problems, where functional relationships are detected between common nouns (Li et al., 2015; dos Santos et al., 2015).", "startOffset": 147, "endOffset": 189}, {"referenceID": 7, "context": ", 2015; dos Santos et al., 2015). Xu et al. (2015) apply an LSTM to a parse path, while Zeng et al.", "startOffset": 12, "endOffset": 51}, {"referenceID": 7, "context": ", 2015; dos Santos et al., 2015). Xu et al. (2015) apply an LSTM to a parse path, while Zeng et al. (2015) use a CNN on the raw text, with a special temporal pooling operation to separately embed the text around each entity.", "startOffset": 12, "endOffset": 107}, {"referenceID": 0, "context": "In response, open-domain information extraction (OpenIE) lets the text speak for itself: R contains all possible patterns of text occurring between entities s and o (Banko et al., 2007; Etzioni et al., 2008; Yates and Etzioni, 2007).", "startOffset": 165, "endOffset": 232}, {"referenceID": 8, "context": "In response, open-domain information extraction (OpenIE) lets the text speak for itself: R contains all possible patterns of text occurring between entities s and o (Banko et al., 2007; Etzioni et al., 2008; Yates and Etzioni, 2007).", "startOffset": 165, "endOffset": 232}, {"referenceID": 35, "context": "When applying Universal Schema (Riedel et al., 2013) (USchema) to relation extraction, we combine the OpenIE and link-prediction perspectives.", "startOffset": 31, "endOffset": 52}, {"referenceID": 34, "context": "When applying Universal Schema (Riedel et al., 2013) (USchema) to relation extraction, we combine the OpenIE and link-prediction perspectives. By jointly modeling both OpenIE patterns and the elements of a target schema, the method captures broader relational structure than multi-class classification approaches that just model the target schema. Furthermore, the method avoids the distant supervision alignment difficulties of Section 2.2. Riedel et al. (2013) augment a knowledge graph from a seed KB with additional edges corresponding to OpenIE patterns observed in the corpus.", "startOffset": 32, "endOffset": 463}, {"referenceID": 34, "context": "When applying Universal Schema (Riedel et al., 2013) (USchema) to relation extraction, we combine the OpenIE and link-prediction perspectives. By jointly modeling both OpenIE patterns and the elements of a target schema, the method captures broader relational structure than multi-class classification approaches that just model the target schema. Furthermore, the method avoids the distant supervision alignment difficulties of Section 2.2. Riedel et al. (2013) augment a knowledge graph from a seed KB with additional edges corresponding to OpenIE patterns observed in the corpus. Even if the user does not seek to predict these new edges, a joint model over all edges can exploit regularities of the OpenIE edges to improve modeling of the labels from the target schema. The data still consist of (s, r, o) triples, which can be predicted using link-prediction techniques such as lowrank factorization. Riedel et al. (2013) explore a variety of approximations to the 3-mode (s, r, o) tensor.", "startOffset": 32, "endOffset": 927}, {"referenceID": 34, "context": "All of the exposition and results in this paper use this factorization, though many of the techniques we present later could be applied easily to the other factorizations described in Riedel et al. (2013). Note that learning unique embeddings for OpenIE relations does not guarantee that similar patterns, such as the final two in Table 1, will be embedded similarly.", "startOffset": 184, "endOffset": 205}, {"referenceID": 33, "context": "Riedel et al. (2013) employ the Bayesian Personalized Ranking (BPR) approach of Rendle et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 33, "context": "(2013) employ the Bayesian Personalized Ranking (BPR) approach of Rendle et al. (2009), which does not explicitly model unobserved edges as negative, but instead seeks to rank the probability of observed triples above unobserved triples.", "startOffset": 66, "endOffset": 87}, {"referenceID": 21, "context": "Additional work has modeled the regularities of multi-hop paths through knowledge graph augmented with text patterns (Lao et al., 2011; Lao et al., 2012; Gardner et al., 2014; Neelakantan et al., 2015).", "startOffset": 117, "endOffset": 201}, {"referenceID": 22, "context": "Additional work has modeled the regularities of multi-hop paths through knowledge graph augmented with text patterns (Lao et al., 2011; Lao et al., 2012; Gardner et al., 2014; Neelakantan et al., 2015).", "startOffset": 117, "endOffset": 201}, {"referenceID": 12, "context": "Additional work has modeled the regularities of multi-hop paths through knowledge graph augmented with text patterns (Lao et al., 2011; Lao et al., 2012; Gardner et al., 2014; Neelakantan et al., 2015).", "startOffset": 117, "endOffset": 201}, {"referenceID": 30, "context": "Additional work has modeled the regularities of multi-hop paths through knowledge graph augmented with text patterns (Lao et al., 2011; Lao et al., 2012; Gardner et al., 2014; Neelakantan et al., 2015).", "startOffset": 117, "endOffset": 201}, {"referenceID": 40, "context": "Recently, Toutanova et al. (2015) extended USchema to not learn individual pattern embeddings vr, but instead to embed text patterns using a deep architecture applied to word tokens.", "startOffset": 10, "endOffset": 34}, {"referenceID": 20, "context": "Most of this work uses aligned sentences from the Europarl dataset (Koehn, 2005) to align word embeddings across languages (Gouws et al.", "startOffset": 67, "endOffset": 80}, {"referenceID": 26, "context": "Most of this work uses aligned sentences from the Europarl dataset (Koehn, 2005) to align word embeddings across languages (Gouws et al., 2015; Luong et al., 2015; Hermann and Blunsom, 2014).", "startOffset": 123, "endOffset": 190}, {"referenceID": 28, "context": "Others (Mikolov et al., 2013; Faruqui et al., 2014) align separate singlelanguage embedding models using a word-level dictionary.", "startOffset": 7, "endOffset": 51}, {"referenceID": 10, "context": "Others (Mikolov et al., 2013; Faruqui et al., 2014) align separate singlelanguage embedding models using a word-level dictionary.", "startOffset": 7, "endOffset": 51}, {"referenceID": 9, "context": ", 2013; Faruqui et al., 2014) align separate singlelanguage embedding models using a word-level dictionary. Mikolov et al. (2013) use translation pairs to learn a linear transform from one embedding space to another.", "startOffset": 8, "endOffset": 130}, {"referenceID": 35, "context": "Similar to many link prediction approaches, (Riedel et al., 2013) perform transductive learning, where a model is learned jointly over train and test data.", "startOffset": 44, "endOffset": 65}, {"referenceID": 38, "context": "orative filtering (Schein et al., 2002): it is unclear how to form predictions for unseen entity pairs, without refactorizing the entire matrix or applying heuristics.", "startOffset": 18, "endOffset": 39}, {"referenceID": 42, "context": "Therefore, following Toutanova et al. (2015), we embed raw context tokens compositionally using a deep architecture.", "startOffset": 21, "endOffset": 45}, {"referenceID": 34, "context": "Unlike Riedel et al. (2013), this requires no manual rules to map text to OpenIE patterns and can embed any possible input string.", "startOffset": 7, "endOffset": 28}, {"referenceID": 6, "context": "CNNs have been useful in a variety of NLP applications (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014).", "startOffset": 55, "endOffset": 117}, {"referenceID": 17, "context": "CNNs have been useful in a variety of NLP applications (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014).", "startOffset": 55, "endOffset": 117}, {"referenceID": 18, "context": "CNNs have been useful in a variety of NLP applications (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014).", "startOffset": 55, "endOffset": 117}, {"referenceID": 6, "context": "CNNs have been useful in a variety of NLP applications (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014). Unlike Toutanova et al. (2015), we also consider RNNs, specifically Long-Short Term Memory Networks", "startOffset": 56, "endOffset": 150}, {"referenceID": 43, "context": "LSTMs have proven successful in a variety of tasks requiring encoding sentences as vectors (Sutskever et al., 2014; Vinyals et al., 2014).", "startOffset": 91, "endOffset": 137}, {"referenceID": 44, "context": "There are two key differences between our sentence encoder and that of Toutanova et al. (2015). First, we use the encoder at test time, since we process the context tokens for held-out data.", "startOffset": 71, "endOffset": 95}, {"referenceID": 44, "context": "There are two key differences between our sentence encoder and that of Toutanova et al. (2015). First, we use the encoder at test time, since we process the context tokens for held-out data. On the other hand, Toutanova et al. (2015) adopt the transductive approach where the encoder is only used to help train better representations for the relations in the target schema; it is ignored when forming predictions.", "startOffset": 71, "endOffset": 234}, {"referenceID": 44, "context": "There are two key differences between our sentence encoder and that of Toutanova et al. (2015). First, we use the encoder at test time, since we process the context tokens for held-out data. On the other hand, Toutanova et al. (2015) adopt the transductive approach where the encoder is only used to help train better representations for the relations in the target schema; it is ignored when forming predictions. Second, we apply the encoder to the raw text between entities, while Toutanova et al. (2015) first perform syntactic dependency parsing on the data and then apply an encoder to the path between the two entities in the parse tree.", "startOffset": 71, "endOffset": 507}, {"referenceID": 34, "context": "Despite the coverage advantages of using a deep sentence encoder, separately embedding each OpenIE pattern, as in Riedel et al. (2013), has key advantages.", "startOffset": 114, "endOffset": 135}, {"referenceID": 23, "context": "Note that we are not performing zero-shot learning of a Spanish model (Larochelle et al., 2008).", "startOffset": 70, "endOffset": 95}, {"referenceID": 2, "context": "the FB15k dataset (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Yang et al., 2015; Toutanova et al., 2015).", "startOffset": 18, "endOffset": 119}, {"referenceID": 45, "context": "the FB15k dataset (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Yang et al., 2015; Toutanova et al., 2015).", "startOffset": 18, "endOffset": 119}, {"referenceID": 25, "context": "the FB15k dataset (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Yang et al., 2015; Toutanova et al., 2015).", "startOffset": 18, "endOffset": 119}, {"referenceID": 47, "context": "the FB15k dataset (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Yang et al., 2015; Toutanova et al., 2015).", "startOffset": 18, "endOffset": 119}, {"referenceID": 44, "context": "the FB15k dataset (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Yang et al., 2015; Toutanova et al., 2015).", "startOffset": 18, "endOffset": 119}, {"referenceID": 2, "context": "the FB15k dataset (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Yang et al., 2015; Toutanova et al., 2015). Here, relation extraction is posed as link prediction on a subset of Freebase. This task does not capture the particular difficulties we address: (1) evaluation on entities and text unseen during training, and (2) zero-annotation learning of a predictor for a low-resource language. Also, note both Toutanova et al. (2015) and Riedel et al.", "startOffset": 19, "endOffset": 444}, {"referenceID": 2, "context": "the FB15k dataset (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Yang et al., 2015; Toutanova et al., 2015). Here, relation extraction is posed as link prediction on a subset of Freebase. This task does not capture the particular difficulties we address: (1) evaluation on entities and text unseen during training, and (2) zero-annotation learning of a predictor for a low-resource language. Also, note both Toutanova et al. (2015) and Riedel et al. (2013) explore the pros and cons of learning embeddings for entity pairs vs.", "startOffset": 19, "endOffset": 469}, {"referenceID": 27, "context": "Our retrieval pipeline first generates all valid slot filler candidates for each query entity and slot, based on entities extracted from the corpus using FACTORIE (McCallum et al., 2009) to perform tokenization, segmentation, and entity extraction.", "startOffset": 163, "endOffset": 186}, {"referenceID": 41, "context": "1Following Surdeanu et al. (2012) we remove facts about undiscovered entities to correct for recall.", "startOffset": 11, "endOffset": 34}, {"referenceID": 37, "context": "The document retrieval and entity matching components of our relation extraction pipeline are based on RelationFactory (Roth et al., 2014), the top-ranked system of the 2013 English slot-filling task.", "startOffset": 119, "endOffset": 138}, {"referenceID": 33, "context": "As in Riedel et al. (2013), we train using the BPR loss of Rendle et al.", "startOffset": 6, "endOffset": 27}, {"referenceID": 33, "context": "(2013), we train using the BPR loss of Rendle et al. (2009). Our CNN is implemented as described in Toutanova et al.", "startOffset": 39, "endOffset": 60}, {"referenceID": 33, "context": "(2013), we train using the BPR loss of Rendle et al. (2009). Our CNN is implemented as described in Toutanova et al. (2015), using width-3 convolutions, followed by tanh and max pool layers.", "startOffset": 39, "endOffset": 124}, {"referenceID": 41, "context": "For this reason, Surdeanu et al. (2012) restrict the evaluation to answer candidates returned by their system and effectively rescaling recall.", "startOffset": 17, "endOffset": 40}, {"referenceID": 37, "context": "7 Roth et al. (2014) 35.", "startOffset": 2, "endOffset": 21}, {"referenceID": 37, "context": "LSTM+USchema ensemble outperforms any single model, including the highly-tuned top 2013 system of Roth et al. (2014), despite using no handwritten patterns.", "startOffset": 98, "endOffset": 117}, {"referenceID": 37, "context": "First, our RelationFactory (Roth et al., 2014) retrieval pipeline was a top retrieval pipeline on the 2013 task, but was outperformed on the 2014 task which introduced new challenges such as confusable entities.", "startOffset": 27, "endOffset": 46}, {"referenceID": 37, "context": "(2014), the 4th place team in the 2014 evaluation, used the same retrieval pipeline (Roth et al., 2014) as our model and achieved an F1 score of 32.", "startOffset": 84, "endOffset": 103}, {"referenceID": 1, "context": "Bentor et al. (2014), the 4th place team in the 2014 evaluation, used the same retrieval pipeline (Roth et al.", "startOffset": 0, "endOffset": 21}], "year": 2016, "abstractText": "Universal schema builds a knowledge base (KB) of entities and relations by jointly embedding all relation types from input KBs as well as textual patterns expressing relations from raw text. In most previous applications of universal schema, each textual pattern is represented as a single embedding, preventing generalization to unseen patterns. Recent work employs a neural network to capture patterns\u2019 compositional semantics, providing generalization to all possible input text. In response, this paper introduces significant further improvements to the coverage and flexibility of universal schema relation extraction: predictions for entities unseen in training and multilingual transfer learning to domains with no annotation. We evaluate our model through extensive experiments on the English and Spanish TAC KBP benchmark, outperforming the top system from TAC 2013 slot-filling using no handwritten patterns or additional annotation. We also consider a multilingual setting in which English training data entities overlap with the seed KB, but Spanish text does not. Despite having no annotation for Spanish data, we train an accurate predictor, with additional improvements obtained by tying word embeddings across languages. Furthermore, we find that multilingual training improves English relation extraction accuracy. Our approach is thus suited to broad-coverage automated knowledge base construction in a variety of languages and domains.", "creator": "LaTeX with hyperref package"}}}