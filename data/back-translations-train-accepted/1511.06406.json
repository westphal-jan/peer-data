{"id": "1511.06406", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Denoising Criterion for Variational Auto-Encoding Framework", "abstract": "Denoising autoencoders (DAE) are trained to reconstruct their clean input with noise injected at the input level, while variational autoencoders (VAE) are trained with noise injected in their stochastic hidden layer, with a regularizer that encourages this noise injection. In this paper, we show that injecting noise both in input and in the stochastic hidden layer can be advantageous and we propose a modified variational lower bound as an improved objective function in this setup. If noise is injected in input, then the standard VAE lower bound involves marginalizing the encoder conditional distribution over the input noise, which makes the training criterion intractable. Instead, we propose a modified training criterion which corresponds to a tighter bound, when noise is injected in input. Experimentally, we find that the proposed denoising variational autoencoder (DVAE) yields better average log-likelihood than the VAE and the importance weighted auto-encoder on the MNIST and Frey Face datasets.", "histories": [["v1", "Thu, 19 Nov 2015 21:56:21 GMT  (3409kb,D)", "http://arxiv.org/abs/1511.06406v1", "ICLR conference submission"], ["v2", "Mon, 4 Jan 2016 15:12:46 GMT  (3592kb,D)", "http://arxiv.org/abs/1511.06406v2", "ICLR conference submission"]], "COMMENTS": "ICLR conference submission", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["daniel jiwoong im", "sungjin ahn", "roland memisevic", "yoshua bengio"], "accepted": true, "id": "1511.06406"}, "pdf": {"name": "1511.06406.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Daniel Jiwoong Im", "Sungjin Ahn", "Roland Memisevic", "Yoshua Bengio"], "emails": ["imdaniel@iro.umontreal.ca", "ahnsungj@iro.umontreal.ca", "memisevr@iro.umontreal.ca", "<findme>@iro.umontreal.ca"], "sections": [{"heading": "1 INTRODUCTION", "text": "This year is the highest in the history of the country."}, {"heading": "2 BACKGROUND", "text": "??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "2.1 VARIATIONAL AUTOENCODERS", "text": "The Variational Autoencoder (UAE) (Kingma & Welling, 2014) is a certain type of variational inference framework that is closely related to our focus in this work. In the UAE, the rear distribution is defined as p\u03b8 (z | x). However, we use a parameterized distribution to define the observation model (x | z). A typical choice for the parameterized distribution is the use of a neural network in which the input is z and the output of a parametric distribution over x, like the Gaussian or Bernoulli, depending on the data type. Then, we become the weights of the neural network."}, {"heading": "3 DENOISING CRITERION IN VARIATIONAL FRAMEWORK", "text": "With the encoder criterion around (Seung, 1998; Vincent et al., 2008), the input is corrupted according to a certain noise distribution, and the model must learn to reconstruct the original input or to maximize the log probability of clean input x-x-x-x-x-x-x-x-x-x-corrupt input x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-"}, {"heading": "3.1 THE DENOISING VARIATIONAL LOWER BOUND", "text": "In the past, we have described that the integration of the denociation criterion into the variational auto-encoding framework is equivalent to a stochastic layer at the bottom of the inference network, and then we will examine the variable lower limit when an additional stochastic layer is added to the inference network and can be integrated via the stochastic variables. Lemma 1. Consider an approximate posterior distribution of the following form: the variable lower limit is examined when an additional stochastic layer is added to the inference network."}, {"heading": "3.2 TRAINING PROCEDURE", "text": "One can consider a simple way to form VAE using the denocidating criterion, which is similar to the way the VAE encoder is formed: (i) sampling damaged input x (m), (ii) sampling (z) sampling (z) sampling (m), and (iii) sampling reconstructed images from the generative network pump system (x). This method is very similar to regular VAE except that the input is corrupted by a noise distribution at each update. The above method can be considered a specific case of optimization of the following target, easily sampled by Monte Carlo sampling.Eq (z) Ep (x) sampling (x) sampling (z) sampling (x) sampling (x) sampling (z) sampling (z) sampling (x) sampling (x)"}, {"heading": "4 EXPERIMENTS", "text": "We conducted empirical studies of the DVAE, in which variable subordinate limits were discussed, as in Section 3. To assess whether adding such a criterion to the varied autocoding models increases performance or not, we tested on the basis of the performance of the UAE and IWAE during the experiments. We also conducted experiments with the classical education technology, using the proposed education techniques (DVAE) to understand the performance of the UAE and IWAE, since the choice of corruption distribution is crucial, compared with different distributions of different distributions of different distributions of different noise levels. We considered two datasets that are binarized to represent the face of Frey dataset dataset datas.We included 60,000 images for education and 10,000 images for testing and each image is 28 pixels for handwritten digital. We conducted empirical studies from 0 to Cun (Leal)."}, {"heading": "Q: Does adding the denoising criterion improve the performance of variational autoencoders?", "text": "Yes. All methods with denocialization criterion exceeded the performance of vanilla UAE and vanilla IWAE as shown in Table 1 and Table 2. But it depends on choosing the right level of corruption; for a large amount of noise, as expected, it tends to perform worse than vanilla UAE and IWAE."}, {"heading": "Q: How sensitive is the model for the type and the level of the noise?", "text": "It seems that both models are not very sensitive to the two types of noise: Gaussian and salt and pepper. They are more sensitive to the sound level than to the type. Based on the experiments, the optimal corruption level is in between (0, 5], as all results in this range are better than the 0% noise. It is natural to see this result, considering that if the sound level is excessive, (i) the model loses information required to reconstruct the original input, and (ii) there is a large gap between the distributions of the (corrupted) training data set and the test data set."}, {"heading": "Q: How do the sample sizes M and L affect to the result?", "text": "In Figure 1 we show the results of different configurations of M and L. As shown, increasing the sample size helps to approximate more quickly in terms of the number of eras. Note, however, that increasing the sample size requires more calculation. In practice, therefore, the use of M = L = 1 seems to be a reasonable choice. Convergent values of UAE are 95.5, 95.22 and 95.10 for M = L = 1, 5 and 10, respectively, and 88.62, 88.45 and 88.42 for IWAE."}, {"heading": "Q: How does the DVAE perform compared to the DVAE\u2205?", "text": "For proper sound distribution, both training criteria offer better performance than the vanilla UAE and IWAE itself with a single sample per updated version. Furthermore, DIWAE exceeds DIWAE in most cases. It is also interesting to note that the performance gap between DIWAE and DIWAE \u2205 is significantly larger than that between DVAE and DVAE. This considerable gain of DIWAE compared to DVAE raises a question; whether the gain is due to the IWAE target or to a larger sample size K, where K = 5 for DIWAE and K = 1 for DVAE. So to verify this, we trained VAE with K = 5 and with the tanh activation function for an inference network with two hidden layers."}, {"heading": "Q: Data augmentation v.s. data corruption?", "text": "Here we consider a specific data amplification where our data lies between 0 and 1, x (0, 1) D such as MNIST. We consider a new binary data point x \"{0, 1} D where the previous data is treated as a probability for each pixel value that turns on, i.e. p (x\") = x. Then we expand the data by pounding the data from x in each iteration. Although this setting is not realistic, we were curious to see if the performance of this data augmentation is compared with the denoization criterion. The performance of such data augmentation on MNIST gave 93.88 \u00b1 0.08 and 92.51 \u00b1 0.07 for VAE and IWAE. Comparing these negative log probabilities with the performance of DVAE and DIWAE, which are 95.45 \u00b1 0.11 and 88.76 \u00b1 0.11, data augmentation VAE was better than DVAE, but the data augmentation image was worse than the distribution IQ."}, {"heading": "5 CONCLUSIONS", "text": "In this paper, we investigated the denocialization criterion for a general class of varying inference models, where the approximate posterior distribution depends on the input size x. The main result of our work was the introduction of the denociating variable lower limit, which, if a reasonable corruption function is present, can be narrower than the standard variational lower limit for loud inputs. We asserted that this training criterion enables us to learn more flexible and robust approximate posterior distributions such as the mixture of Gaussian than the standard training method without corruption. In the experiments, we empirically observed that the proposed method can consistently contribute to improving the performance of the varying autoencoder and the importance of the weighted auto encoder. Although we observed considerable improvements in our experiments with simple corruption distributions, the question of how to achieve the reasonable corruption distribution is still an important open question for us to think of using a more parochial method with a better understanding of corruption distribution."}], "references": [{"title": "What regularized auto-encoders learn from the data generating distribution", "author": ["Alain", "Guillaume", "Bengio", "Yoshua"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "Alain et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Alain et al\\.", "year": 2014}, {"title": "Generalized denoising autoencoders as generative models", "author": ["Bengio", "Yoshua", "Yao", "Li", "Alain", "Guillaume", "Vincent", "Pascal"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["Bergstra", "James", "Breuleux", "Olivier", "Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Desjardins", "Guillaume", "Turian", "Joseph", "Warde-Farley", "David", "Bengio", "Yoshua"], "venue": "In Proc. SciPy,", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Reweighted wake-sleep", "author": ["Bornschein", "J\u00f6rg", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1406.2751,", "citeRegEx": "Bornschein et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bornschein et al\\.", "year": 2014}, {"title": "Importance weighted auto-encoder", "author": ["Burda", "Yuri", "Grosse", "Roger", "Salakhutdinov", "Ruslan"], "venue": "In http://arxiv.org/pdf/1509.00519v1.pdf,", "citeRegEx": "Burda et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Burda et al\\.", "year": 2015}, {"title": "The helmholtz machine", "author": ["Dayan", "Peter", "Hinton", "Geoffrey E", "Neal", "Radford M", "Zemel", "Richard S"], "venue": "Neural computation,", "citeRegEx": "Dayan et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dayan et al\\.", "year": 1995}, {"title": "Nice: Non-linear independent component estimation", "author": ["Dinh", "Laurent", "Krueger", "David", "Bengio", "Yoshua"], "venue": "In International Conference on Learning Representation Workshop,", "citeRegEx": "Dinh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dinh et al\\.", "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Rezende", "Danilo Jimenez", "Wierstra", "Daan"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Conservativeness of untied auto-encoders", "author": ["Im", "Daniel Jiwoong", "Belghazi", "Mohamed Ishmael Diwan", "Memisevic", "Roland"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Im et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Im et al\\.", "year": 2016}, {"title": "An introduction to variational methods for graphical models", "author": ["Jordan", "Micheal", "Ghahramani", "Zoubin", "Jaakkola", "Tommi S", "Saul", "Lawarence K"], "venue": "Machine Learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Auto-encoding varational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "In Proceedings of the Neural Information Processing Systems (NIPS),", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "Leon", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Neural variational inference and learning in belief networks", "author": ["Mnih", "Andriy", "Gregor", "Karol"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Probabilistic inference using markov chain monte carlo methods", "author": ["R.M. Neal"], "venue": "Technical Report CRG-TR-93-1,", "citeRegEx": "Neal,? \\Q1993\\E", "shortCiteRegEx": "Neal", "year": 1993}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Variational inference with normalizing flows", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir"], "venue": "In Proceedings of the International Conference of Machine Learning (ICML),", "citeRegEx": "Rezende et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2015}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["Rifai", "Salah"], "venue": "In Proceedings of the 28th International Conference on Machine Learning (ICML),", "citeRegEx": "Rifai and Salah.,? \\Q2011\\E", "shortCiteRegEx": "Rifai and Salah.", "year": 2011}, {"title": "Markov chain monte carlo and variational inference: Bridging the gap", "author": ["Salimans", "Tim", "Kingma", "Diederik P", "Welling", "Max"], "venue": "In Proceedings of the International Conference of Machine Learning (ICML),", "citeRegEx": "Salimans et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2015}, {"title": "Learning continuous attractors in recurrent networks", "author": ["Seung", "H.Sebastian"], "venue": "In Proceedings of the Neural Information Processing Systems (NIPS),", "citeRegEx": "Seung and H.Sebastian.,? \\Q1998\\E", "shortCiteRegEx": "Seung and H.Sebastian.", "year": 1998}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Bengio", "Yoshua", "P.A. Manzagol"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "Adadelta: An adaptive learning rate method", "author": ["Zeiler", "Matthew D"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "Zeiler and D.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler and D.", "year": 2012}], "referenceMentions": [{"referenceID": 10, "context": "Variational inference (Jordan et al., 1999) has been a core component of approximate Bayesian inference along with the Markov chain Monte Carlo (MCMC) method (Neal, 1993).", "startOffset": 22, "endOffset": 43}, {"referenceID": 16, "context": ", 1999) has been a core component of approximate Bayesian inference along with the Markov chain Monte Carlo (MCMC) method (Neal, 1993).", "startOffset": 122, "endOffset": 134}, {"referenceID": 7, "context": "It has been popular to many researchers and practitioners because the problem of learning an intractable posterior distribution is formulated as an optimization problem which has many advantages compared to MCMC; (i) we can easily take advantage of many advanced optimization tools (Kingma & Ba, 2014b; Duchi et al., 2011; Zeiler, 2012), (ii) the training by optimization is usually faster than the MCMC sampling, and (iii) unlike MCMC where it is difficult to decide when to finish the sampling, the stopping criterion in variational inference is clear.", "startOffset": 282, "endOffset": 336}, {"referenceID": 5, "context": "One remarkable recent advance in variational inference is to use the inference network (also known as the recognition network) as the approximate posterior distribution (Kingma & Welling, 2014; Rezende & Mohamed, 2014; Dayan et al., 1995; Bornschein & Bengio, 2014).", "startOffset": 169, "endOffset": 265}, {"referenceID": 22, "context": "On the other hand, the denoising criterion, where the input is corrupted by adding some noise and the model is asked to recover the original input, has been studied extensively for deterministic generative models (Seung, 1998; Vincent et al., 2008; Bengio et al., 2013).", "startOffset": 213, "endOffset": 269}, {"referenceID": 1, "context": "On the other hand, the denoising criterion, where the input is corrupted by adding some noise and the model is asked to recover the original input, has been studied extensively for deterministic generative models (Seung, 1998; Vincent et al., 2008; Bengio et al., 2013).", "startOffset": 213, "endOffset": 269}, {"referenceID": 22, "context": "The study showed that the denoising criterion plays an important role in achieving good generalization performance (Vincent et al., 2008) because it makes the nearby data points in the low dimensional manifold to be robust against the presence of small noise in the high dimensional observation space (Seung, 1998; Vincent et al.", "startOffset": 115, "endOffset": 137}, {"referenceID": 22, "context": ", 2008) because it makes the nearby data points in the low dimensional manifold to be robust against the presence of small noise in the high dimensional observation space (Seung, 1998; Vincent et al., 2008; Rifai, 2011; Alain & Bengio, 2014; Im et al., 2016).", "startOffset": 171, "endOffset": 258}, {"referenceID": 9, "context": ", 2008) because it makes the nearby data points in the low dimensional manifold to be robust against the presence of small noise in the high dimensional observation space (Seung, 1998; Vincent et al., 2008; Rifai, 2011; Alain & Bengio, 2014; Im et al., 2016).", "startOffset": 171, "endOffset": 258}, {"referenceID": 4, "context": "Because applying this approximate distribution to the standard VAE objective makes the training intractable, we propose a new objective, called the denoising variational lower bound, and show that, given a sensible corruption function, this is (i) tighter than the standard variational lower bound on noisy inputs, (ii) efficient to train, and (iii) easily applicable to many existing models such as the variational autoencoder, the importance reweighted autoencoder (IWAE) (Burda et al., 2015), and the neural variational inference and learning (NVIL) (Mnih & Gregor, 2014).", "startOffset": 474, "endOffset": 494}, {"referenceID": 16, "context": "Salimans et al. (2015) integrated MCMC steps into the variational inference such that the variational distribution becomes closer to the target distribution as it takes more MCMC steps inside each iteration of the variational inference.", "startOffset": 0, "endOffset": 23}, {"referenceID": 4, "context": "Similar ideas but applying a sequence of invertible non-linear transformations rather than MCMC are also proposed by Dinh et al. (2015) and Rezende & Mohamed (2015).", "startOffset": 117, "endOffset": 136}, {"referenceID": 4, "context": "Similar ideas but applying a sequence of invertible non-linear transformations rather than MCMC are also proposed by Dinh et al. (2015) and Rezende & Mohamed (2015). On the other hand, the denoising criterion, where the input is corrupted by adding some noise and the model is asked to recover the original input, has been studied extensively for deterministic generative models (Seung, 1998; Vincent et al.", "startOffset": 117, "endOffset": 165}, {"referenceID": 1, "context": ", 2008; Bengio et al., 2013). The study showed that the denoising criterion plays an important role in achieving good generalization performance (Vincent et al., 2008) because it makes the nearby data points in the low dimensional manifold to be robust against the presence of small noise in the high dimensional observation space (Seung, 1998; Vincent et al., 2008; Rifai, 2011; Alain & Bengio, 2014; Im et al., 2016). Therefore, it is natural to ask if the denoising criterion (where we add the noise to the inputs) can also be advantageous for the variational autoencoding framework where the noise is added to the latent variables, not the inputs, and if so, how can we formulate the problem for efficient training. Although it has not been considerably studied how to combine these, there has been some evidences of its usefulness1. For example, Rezende & Mohamed (2014) pointed out that injecting additional noise to the recognition model is crucial to achieve the reported accuracy for unseen data, advocating that in practice denoising can help the regularization of probabilistic generative models as well.", "startOffset": 8, "endOffset": 876}, {"referenceID": 10, "context": "For example, in the mean-field variational inference (Jordan et al., 1999), the distributions in Q treat all dependent variables as independent, i.", "startOffset": 53, "endOffset": 74}, {"referenceID": 8, "context": "This includes other recent models such as the importance weighted autoencoders (IWAE), the neural variational inference and learning (NVIL), and DRAW (Gregor et al., 2015).", "startOffset": 150, "endOffset": 171}, {"referenceID": 22, "context": "With the denoising autoencoder criterion (Seung, 1998; Vincent et al., 2008), the input is corrupted according to some noise distribution, and the model needs to learn to reconstruct the original input or maximize the log-probability of the clean input x, given the corrupted input x\u0303.", "startOffset": 41, "endOffset": 76}, {"referenceID": 4, "context": "Note that a similar idea is explored in IWAE (Burda et al., 2015).", "startOffset": 45, "endOffset": 65}, {"referenceID": 14, "context": "The MNIST dataset contains 60,000 images for training and 10,000 images for test and each of the images is 28 \u00d7 28 pixels for handwritten digits from 0 to 9 (LeCun et al., 1998).", "startOffset": 157, "endOffset": 177}, {"referenceID": 4, "context": "We used softmax activations for VAE and tanh activations for IWAE following the same configuration of the original papers of Kingma & Welling (2014) and Burda et al. (2015). For binarized MNIST dataset, the last layer of the generative network was sigmoid and the usual cross-entropy term was used.", "startOffset": 153, "endOffset": 173}, {"referenceID": 2, "context": "Acknowledgments: We thank the developers of Theano (Bergstra et al., 2010) for their great work.", "startOffset": 51, "endOffset": 74}], "year": 2015, "abstractText": "Denoising autoencoders (DAE) are trained to reconstruct their clean inputs with noise injected at the input level, while variational autoencoders (VAE) are trained with noise injected in their stochastic hidden layer, with a regularizer that encourages this noise injection. In this paper, we show that injecting noise both in input and in the stochastic hidden layer can be advantageous and we propose a modified variational lower bound as an improved objective function in this setup. When input is corrupted, then the standard VAE lower bound involves marginalizing the encoder conditional distribution over the input noise, which makes the training criterion intractable. Instead, we propose a modified training criterion which corresponds to a tighter bound when input is corrupted. Experimentally, we find that the proposed denoising variational autoencoder (DVAE) yields better average loglikelihood than the VAE and the importance weighted autoencoder on the MNIST and Frey Face datasets.", "creator": "LaTeX with hyperref package"}}}