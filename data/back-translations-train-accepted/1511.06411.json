{"id": "1511.06411", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Training Deep Neural Networks via Direct Loss Minimization", "abstract": "Supervised training of deep neural nets typically relies on minimizing cross-entropy. However, in many domains, we are interested in performing well on specific application-specific metrics. In this paper we proposed a direct loss minimization approach to train deep neural networks, taking into account the application-specific loss functions. This can be non-trivial, when these functions are non-smooth and non-decomposable. We demonstrate the effectiveness of our approach in the context of maximizing average precision for ranking problems. Towards this goal, we propose a dynamic programming algorithm that can efficiently compute the weight updates. Our approach proves superior to a variety of baselines in the context of action classification and object detection.", "histories": [["v1", "Thu, 19 Nov 2015 22:02:26 GMT  (504kb,D)", "http://arxiv.org/abs/1511.06411v1", null], ["v2", "Thu, 2 Jun 2016 00:56:59 GMT  (764kb,D)", "http://arxiv.org/abs/1511.06411v2", "ICML2016"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yang song", "alexander g schwing", "richard s zemel", "raquel urtasun"], "accepted": true, "id": "1511.06411"}, "pdf": {"name": "1511.06411.pdf", "metadata": {"source": "CRF", "title": "DIRECT LOSS MINIMIZATION FOR TRAINING DEEP NEURAL NETS", "authors": ["Yang Song", "Alexander G. Schwing", "Raquel Urtasun"], "emails": ["songyang12@mails.tsinghua.edu.cn", "aschwing@cs.toronto.edu", "zemel@cs.toronto.edu", "urtasun@cs.toronto.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, most of the people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move"}, {"heading": "2 DIRECT LOSS MINIMIZATION FOR NEURAL NETWORKS", "text": "In this section, we provide a formulation for learning neural networks by minimizing the task loss. In relation to this goal, our first main result is a theorem that refers to direct losses. (But it is not as if it is a complete scoring system that depends on the input data. (It is not as if it is a complete scoring system, but a complete scoring system). (It is not as if it is a complete scoring system that depends on the input data.) It is then performed by selecting the output scores with maximum score, i.e. yw = arg y y, w).Given a dataset of input-output pairs D = (x, y)}, a standardized approach to learning is to optimize the parameters of the scoring function F by minimizing."}, {"heading": "3 DIRECT LOSS MINIMIZATION FOR AVERAGE PRECISION", "text": "To directly optimize task losses, we need to calculate the gradient defined in Eq. (2) As mentioned above, we need to solve both the standard task and the problem of loss augmentation inference specified in Eq. (3) While the former is typically simple, the latter depends on how complex it is to solve if the loss is not decomposable. In this paper, we look at ranking problems where the desired task loss is average precision (AP). For linear adjustment, efficient algorithms for positive loss augmented inference with AP loss have been proposed by Yue et al. (2007) and Mohapatra et al. (2014) Their results can be extended to the non-linear setting only in the positive case where ydirect = arg maxy Y F (x, y, y) +."}, {"heading": "4 EXPERIMENTAL EVALUATION", "text": "To evaluate the performance of our approach, we conduct experiments with both synthetic and real data sets. We compare the positive and negative versions of our direct loss minimization approach to a number of different baselines."}, {"heading": "4.1 SYNTHETIC DATA", "text": "In particular, we use a normal distribution with zero mean and unit variance. We generate 20,000 input data xi from a 10-dimensional standard Gauss. We calculate the score of each sample xi and sort them in descending order. The top 20% of the samples are assigned to the positive set P. We then randomly divide the generated data into a training set of 10,000 elements and a test set that contains the rest. To ensure that we do not suffer from the mischaracterization of the model, we use the same network structure when the parameters from random initializations are trained with different algorithms. We evaluate the positive and negative versions of our direct loss minimization when we use two different task losses: AP and 0-1 loss."}, {"heading": "4.2 ACTION CLASSIFICATION TASK", "text": "Dataset: In the next experiment, we will use the PASCAL VOC2012 Action Classification dataset provided by Everingham et al. (2014), which contains 4588 images and 6278 \"trainvale\" person boxes. For each of the 10 target classes, we will split the trainval dataset into equally large training, validation and test sets. We have set the learning rate, regularization weight and algorithms based on their performance based on our validation dataset and report the results on the test set. For all algorithms, we have used the entire available training set in one batch and performed 300 iterations. Algorithms: We will train our non-linear direct loss minimization and all baselines individually for each class. As baselines, we will use a deep network trained with cross entropy and hinged loss as baselines. The deep network used in these experiments follows the architecture of Krizsky (2012) heval."}, {"heading": "4.3 OBJECT DETECTION TASK", "text": "The dataset contains 5717 images for training, 5823 images for validation and 10991 images for testing. Algorithms: On this dataset we follow the RCNN paradigm Girshick et al. (2014). For direct loss minimization, we determine the dimensions of the top layer of the network Krizhevsky et al. (2012) up to the fine tuning with the prefabricated weights Girshick et al. (2014). For direct loss minimization, we determine the dimensions of the top layer of the network Krizhevsky et al. (2012) up to the fine tuning with the ILSVRC2012. (Russakovsky et al., 2015) up to the direct loss minimization for all 20 classes separately."}, {"heading": "5 CONCLUSION", "text": "In this paper, we have proposed a direct loss minimization approach to train deep neural networks. We have demonstrated the effectiveness of our approach in maximizing average accuracy in assessing problems, including minimizing non-smooth and non-decomposable loss. To this end, we have proposed a dynamic programming algorithm that can efficiently calculate weight updates. Our experiments have shown that this is advantageous compared to a variety of baselines in the context of action classification and object recognition. In the future, we plan to investigate direct loss minimization in connection with other non-decomposable losses, such as intersections over connections."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank David McAllester for the discussions. This work was partially supported by ONR Grant N00014-14-1-0232 and a Google Research Award."}, {"heading": "5.1 PROOF OF THE GENERAL LOSS GRADIENT THEOREM", "text": "\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "5.2 PROOF FOR LEMMA 1", "text": "Next, we provide the proof for Lemma 1, which we repeat for completeness. We point out again that the cost function value obtained by restricting the loss-increasing conclusion to the subsets is: h (i, j) = max y (i), p (n), p (n), p (n), p (n), p (n), p (n), p (n), p (n), p (n), p (n), p (n), p (n), p (n), p (n), p (n), p (n, p), p), p (n, p), p (n), p (n), p (p), p (n), p (n), p (p, p, p, p, p, p, p, p, p, p (, p, p, p, p, p), p (n), p (p, p, p, p, p, p, p, p (n), p (p, p, p, p), p (n), p (p, p, p, p, p, p (n), p (n), p (p, p, p, p, p, p, p (n), p (n), p (n, p, p, p, p, p, p, p, p (n), p (n, p, p, p, p, p, p), p (n), p (n, p (n), p (n, p, p, p, p (n), p (n, p, p, p, p (n), p, p (n), p (n, p (n, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p (n), p, p (n), p (n, p (n), p (n, p, p (n, p, p, p, p, p, p, p, p (n, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p"}, {"heading": "5.3 EXPERIMENTS ON LINEAR SYNTHETIC DATA", "text": "To test the linear case, we created two different datasets, one of which is linearly separable, while the other is not. Randomly, we generated 20,000 data points by scanning from a 10-dimensional standard Gaussian distribution. The data points with a sum of numbers in all dimensions greater than 0 are assigned to the positive class, while those with a negative sum are classified as negative objects. We then divide the entire dataset into training sets and test sets of 10,000 elements each. To create the nonlinear dataset, we randomly reverse 20% of the binary labels. In this linear setting, we either choose either either \"x\" or \"wx.\" The results are presented in Fig. 4 and Fig. 5. In the noiseless linear case, we observe the negative update to achieve slightly better performance than the positive update. The Perceptron method also performs well."}], "references": [{"title": "Learning Deep Structured Models", "author": ["\u0303bengioy/dlbook. Chen", "L.-C", "A.G. Schwing", "A.L. Yuille", "R. Urtasun"], "venue": "Deep learning. Book in preparation for MIT Press,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Matrix updates for perceptron training of continuous density hidden markov models", "author": ["Cheng", "C.-C", "F. Sha", "L.K. Saul"], "venue": "In Proc. ICML,", "citeRegEx": "Cheng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2009}, {"title": "Direct loss minimization inverse optimal control", "author": ["A. Doerr", "N. Ratliff", "J. Bohg", "M. Toussaint", "S. Schaal"], "venue": "Proc. of robotics: science and systems (R: SS),", "citeRegEx": "Doerr et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Doerr et al\\.", "year": 2015}, {"title": "The pascal visual object classes challenge: A retrospective", "author": ["M. Everingham", "A.S.M. Eslami", "L. van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": null, "citeRegEx": "Everingham et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Everingham et al\\.", "year": 2014}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "In Proc. CVPR,", "citeRegEx": "Girshick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "Direct Error Rate Minimization of Hidden Markov Models", "author": ["J. Keshet", "Cheng", "C.-C", "M. Stoehr", "D. McAllester"], "venue": "In Proc. Interspeech,", "citeRegEx": "Keshet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Keshet et al\\.", "year": 2011}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Proc. NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Loss Functions for Discriminative Training of Energy-Based Models", "author": ["Y. LeCun", "F.J. Huang"], "venue": "In Proc. AISTATS,", "citeRegEx": "LeCun and Huang,? \\Q2005\\E", "shortCiteRegEx": "LeCun and Huang", "year": 2005}, {"title": "Direct loss minimization for structured prediction", "author": ["D.A. McAllester", "J. Keshet", "T. Hazan"], "venue": "In Proc. NIPS,", "citeRegEx": "McAllester et al\\.,? \\Q2010\\E", "shortCiteRegEx": "McAllester et al\\.", "year": 2010}, {"title": "Efficient Optimization for Average Precision SVM", "author": ["P. Mohapatra", "C.V. Jawahar", "M.P. Kumar"], "venue": "In Proc. NIPS,", "citeRegEx": "Mohapatra et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mohapatra et al\\.", "year": 2014}, {"title": "Structured Output Learning with High Order Loss Functions", "author": ["D. Tarlow", "R.S. Zemel"], "venue": "In Proc. AISTATS,", "citeRegEx": "Tarlow and Zemel,? \\Q2012\\E", "shortCiteRegEx": "Tarlow and Zemel", "year": 2012}, {"title": "Large Margin Methods for Structured and Interdependent Output Variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": null, "citeRegEx": "Tsochantaridis et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2005}, {"title": "Selective search for object recognition", "author": ["J. Uijlings", "K. van de Sande", "T. Gevers", "A. Smeulders"], "venue": null, "citeRegEx": "Uijlings et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Uijlings et al\\.", "year": 2013}, {"title": "BoltzRank: Learning to Maximize Expected Ranking Gain", "author": ["M.N. Volkovs", "R.S. Zemel"], "venue": "In Proc. ICML,", "citeRegEx": "Volkovs and Zemel,? \\Q2009\\E", "shortCiteRegEx": "Volkovs and Zemel", "year": 2009}, {"title": "A support vector method for optimizing average precision", "author": ["Y. Yue", "T. Finley", "F. Radlinski", "T. Joachims"], "venue": "In Proc. SIGIR,", "citeRegEx": "Yue et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 14, "context": "Various efficient methods have been proposed for incorporating discrete and complicated loss functions into this approach (Yue et al., 2007; Volkovs & Zemel, 2009; Tarlow & Zemel, 2012; Mohapatra et al., 2014).", "startOffset": 122, "endOffset": 209}, {"referenceID": 9, "context": "Various efficient methods have been proposed for incorporating discrete and complicated loss functions into this approach (Yue et al., 2007; Volkovs & Zemel, 2009; Tarlow & Zemel, 2012; Mohapatra et al., 2014).", "startOffset": 122, "endOffset": 209}, {"referenceID": 8, "context": "In their seminal work, McAllester et al. (2010) showed that a form of structured prediction learning computes the gradients of the task loss.", "startOffset": 23, "endOffset": 48}, {"referenceID": 10, "context": ", Tarlow & Zemel (2012); Yue et al. (2007); Mohapatra et al.", "startOffset": 25, "endOffset": 43}, {"referenceID": 6, "context": "(2007); Mohapatra et al. (2014). While these methods include the task loss in the objective, they are still not minimizing it.", "startOffset": 8, "endOffset": 32}, {"referenceID": 6, "context": "McAllester et al. (2010) showed that it is possible to asymptotically optimize the task-loss when the function F is linear in the parameters i.", "startOffset": 0, "endOffset": 25}, {"referenceID": 6, "context": "McAllester et al. (2010) showed that it is possible to asymptotically optimize the task-loss when the function F is linear in the parameters i.e., F (x, y, w) = w>\u03c6(x, y). This work has produced encouraging results. For example, McAllester et al. (2010) used this technique for phoneme-to-speech alignment on the TIMIT dataset optimizing the \u03c4 -alignment loss and the \u03c4 -insensitive loss, while Keshet et al.", "startOffset": 0, "endOffset": 254}, {"referenceID": 4, "context": "(2010) used this technique for phoneme-to-speech alignment on the TIMIT dataset optimizing the \u03c4 -alignment loss and the \u03c4 -insensitive loss, while Keshet et al. (2011) illustrated applicability of the method to hidden Markov models for speech.", "startOffset": 148, "endOffset": 169}, {"referenceID": 2, "context": "Direct loss minimization was also shown to work well for inverse optimal control by Doerr et al. (2015). The first contribution of our work is to generalize this theorem to arbitrary scoring functions, i.", "startOffset": 84, "endOffset": 104}, {"referenceID": 11, "context": "We borrow terminology from the structured prediction literature, where this perturbed inference problem is commonly referred to as loss-augmented inference (Tsochantaridis et al., 2005; Chen et al., 2015).", "startOffset": 156, "endOffset": 204}, {"referenceID": 0, "context": "We borrow terminology from the structured prediction literature, where this perturbed inference problem is commonly referred to as loss-augmented inference (Tsochantaridis et al., 2005; Chen et al., 2015).", "startOffset": 156, "endOffset": 204}, {"referenceID": 0, "context": ", 2005; Chen et al., 2015). In the following section we derive an efficient dynamic programming algorithm to perform loss-augmented inference when the task loss is average precision. Note that loss-augmented inference, as specified in Eq. (3), can take the task loss into account in a positive or a negative way. Depending on the sign of s, the gradient direction changes. McAllester et al. (2010) provide a nice intuition for the two different directions.", "startOffset": 8, "endOffset": 398}, {"referenceID": 13, "context": "For the linear setting, efficient algorithms for positive loss-augmented inference with AP loss were proposed by Yue et al. (2007) and Mohapatra et al.", "startOffset": 113, "endOffset": 131}, {"referenceID": 9, "context": "(2007) and Mohapatra et al. (2014). Their results can be extended to the non-linear setting only in the positive case, where ydirect = arg max\u0177\u2208Y F (x, \u0177, w) + \u2206(y, \u0177).", "startOffset": 11, "endOffset": 35}, {"referenceID": 9, "context": "(2007) and Mohapatra et al. (2014). Their results can be extended to the non-linear setting only in the positive case, where ydirect = arg max\u0177\u2208Y F (x, \u0177, w) + \u2206(y, \u0177). For the negative setting inequalities required in their proof do not hold and thus their method is not applicable in this case. In this section we propose a more general algorithm that can handle both cases. To compute the AP loss we are given a set of positive (i.e., relevant) and negative samples. The sample xi belongs to the positive class if i \u2208 P = {1, . . . , |P|}, and xi is part of the negative class if i \u2208 N = {|P|+ 1, . . . , |P|+ |N |}. We define the output to be composed of pairwise comparisons, with yi,j = 1 if sample i is ranked higher than j, yi,i = 0, and yi,j = \u22121 otherwise. We subsumed all these pairwise comparisons in y = (\u00b7 \u00b7 \u00b7 , yi,j , \u00b7 \u00b7 \u00b7 ). Similarly x = (x1, \u00b7 \u00b7 \u00b7 , xN ) contains all inputs, with N = |P|+ |N | the total number of data points in the training set. In addition, we assume the ranking across all samples y to be complete, i.e., consistent. During inference we obtain a ranking by predicting scores \u03c6(xi, w) for all data samples xi which are easily sorted afterwards. For learning we generalize the feature function defined by Yue et al. (2007) and Mohapatra et al.", "startOffset": 11, "endOffset": 1261}, {"referenceID": 9, "context": "(2007) and Mohapatra et al. (2014). Their results can be extended to the non-linear setting only in the positive case, where ydirect = arg max\u0177\u2208Y F (x, \u0177, w) + \u2206(y, \u0177). For the negative setting inequalities required in their proof do not hold and thus their method is not applicable in this case. In this section we propose a more general algorithm that can handle both cases. To compute the AP loss we are given a set of positive (i.e., relevant) and negative samples. The sample xi belongs to the positive class if i \u2208 P = {1, . . . , |P|}, and xi is part of the negative class if i \u2208 N = {|P|+ 1, . . . , |P|+ |N |}. We define the output to be composed of pairwise comparisons, with yi,j = 1 if sample i is ranked higher than j, yi,i = 0, and yi,j = \u22121 otherwise. We subsumed all these pairwise comparisons in y = (\u00b7 \u00b7 \u00b7 , yi,j , \u00b7 \u00b7 \u00b7 ). Similarly x = (x1, \u00b7 \u00b7 \u00b7 , xN ) contains all inputs, with N = |P|+ |N | the total number of data points in the training set. In addition, we assume the ranking across all samples y to be complete, i.e., consistent. During inference we obtain a ranking by predicting scores \u03c6(xi, w) for all data samples xi which are easily sorted afterwards. For learning we generalize the feature function defined by Yue et al. (2007) and Mohapatra et al. (2014) to a non-linear scoring function, using F (x, y, w) = 1 |P||N | \u2211 i\u2208P,j\u2208N yi,j (\u03c6(xi, w)\u2212 \u03c6(xj , w)) .", "startOffset": 11, "endOffset": 1289}, {"referenceID": 14, "context": "To solve the loss-augmented inference task we have to solve the following program arg max \u0177 F (x, \u0177, w)\u00b1 \u2206AP(rank(y), rank(\u0177)), (5) In the following we derive a dynamic programming algorithm that can handle both the positive and negative case and has the same complexity as Yue et al. (2007). Towards this goal, we first note that Observation 1 of Yue et al.", "startOffset": 274, "endOffset": 292}, {"referenceID": 14, "context": "To solve the loss-augmented inference task we have to solve the following program arg max \u0177 F (x, \u0177, w)\u00b1 \u2206AP(rank(y), rank(\u0177)), (5) In the following we derive a dynamic programming algorithm that can handle both the positive and negative case and has the same complexity as Yue et al. (2007). Towards this goal, we first note that Observation 1 of Yue et al. (2007) holds for both the positive and the negative case.", "startOffset": 274, "endOffset": 366}, {"referenceID": 14, "context": "To solve the loss-augmented inference task we have to solve the following program arg max \u0177 F (x, \u0177, w)\u00b1 \u2206AP(rank(y), rank(\u0177)), (5) In the following we derive a dynamic programming algorithm that can handle both the positive and negative case and has the same complexity as Yue et al. (2007). Towards this goal, we first note that Observation 1 of Yue et al. (2007) holds for both the positive and the negative case. For completeness, we repeat their observation here and adapt it to our notation. Observation 1 (Yue et al. (2007)).", "startOffset": 274, "endOffset": 531}, {"referenceID": 8, "context": "in (McAllester et al., 2010), the negative augmented inference is not competitive in our setting.", "startOffset": 3, "endOffset": 28}, {"referenceID": 4, "context": "Inspired by RCNN (Girshick et al., 2014), we cropped the regions of each image with a padding of 16 pixels and interpolated them to a size of 227\u00d7 227\u00d7 3 to fit the input data dimension of the network.", "startOffset": 17, "endOffset": 40}, {"referenceID": 3, "context": "2 ACTION CLASSIFICATION TASK Dataset: In the next experiment we use the PASCAL VOC2012 action classification dataset provided by Everingham et al. (2014). The dataset contains 4588 images and 6278 \u201ctrainval\u201d person bounding boxes.", "startOffset": 129, "endOffset": 154}, {"referenceID": 3, "context": "2 ACTION CLASSIFICATION TASK Dataset: In the next experiment we use the PASCAL VOC2012 action classification dataset provided by Everingham et al. (2014). The dataset contains 4588 images and 6278 \u201ctrainval\u201d person bounding boxes. For each of the 10 target classes, we divide the trainval dataset into equally sized training, validation and test sets. We tuned the learning rate, regularization weight, and for all the algorithms based on their performance on our validation dataset, and report the results on the test set. For all algorithms we used the entire available training set in a single batch and performed 300 iterations. Algorithms: We train our nonlinear direct loss minimization as well as all the baselines individually for each class. As baselines we use a deep network trained with cross entropy and hinge loss as baselines. The deep network used in those experiments follows the architecture of Krizhevsky et al. (2012), with the top dimension adjusted to 1 and the parameters initialized using weights trained on ILSVRC2012 (Russakovsky et al.", "startOffset": 129, "endOffset": 938}, {"referenceID": 3, "context": "2 ACTION CLASSIFICATION TASK Dataset: In the next experiment we use the PASCAL VOC2012 action classification dataset provided by Everingham et al. (2014). The dataset contains 4588 images and 6278 \u201ctrainval\u201d person bounding boxes. For each of the 10 target classes, we divide the trainval dataset into equally sized training, validation and test sets. We tuned the learning rate, regularization weight, and for all the algorithms based on their performance on our validation dataset, and report the results on the test set. For all algorithms we used the entire available training set in a single batch and performed 300 iterations. Algorithms: We train our nonlinear direct loss minimization as well as all the baselines individually for each class. As baselines we use a deep network trained with cross entropy and hinge loss as baselines. The deep network used in those experiments follows the architecture of Krizhevsky et al. (2012), with the top dimension adjusted to 1 and the parameters initialized using weights trained on ILSVRC2012 (Russakovsky et al., 2015). Inspired by RCNN (Girshick et al., 2014), we cropped the regions of each image with a padding of 16 pixels and interpolated them to a size of 227\u00d7 227\u00d7 3 to fit the input data dimension of the network. All the algorithms we compare to as well as our approach use raw pixels as input. Results: We provide quantitative results in Tab. 1. Our direct AP loss minimization clearly outperforms the baselines by 1.9% and 3.1% respectively. This is quite significant for this task. 4.3 OBJECT DETECTION TASK Dataset: We use the PASCAL VOC2012 object detection dataset collected by Everingham et al. (2014). The dataset contains 5717 images for training, 5823 images for validation and 10991 images for test.", "startOffset": 129, "endOffset": 1669}, {"referenceID": 3, "context": "2 ACTION CLASSIFICATION TASK Dataset: In the next experiment we use the PASCAL VOC2012 action classification dataset provided by Everingham et al. (2014). The dataset contains 4588 images and 6278 \u201ctrainval\u201d person bounding boxes. For each of the 10 target classes, we divide the trainval dataset into equally sized training, validation and test sets. We tuned the learning rate, regularization weight, and for all the algorithms based on their performance on our validation dataset, and report the results on the test set. For all algorithms we used the entire available training set in a single batch and performed 300 iterations. Algorithms: We train our nonlinear direct loss minimization as well as all the baselines individually for each class. As baselines we use a deep network trained with cross entropy and hinge loss as baselines. The deep network used in those experiments follows the architecture of Krizhevsky et al. (2012), with the top dimension adjusted to 1 and the parameters initialized using weights trained on ILSVRC2012 (Russakovsky et al., 2015). Inspired by RCNN (Girshick et al., 2014), we cropped the regions of each image with a padding of 16 pixels and interpolated them to a size of 227\u00d7 227\u00d7 3 to fit the input data dimension of the network. All the algorithms we compare to as well as our approach use raw pixels as input. Results: We provide quantitative results in Tab. 1. Our direct AP loss minimization clearly outperforms the baselines by 1.9% and 3.1% respectively. This is quite significant for this task. 4.3 OBJECT DETECTION TASK Dataset: We use the PASCAL VOC2012 object detection dataset collected by Everingham et al. (2014). The dataset contains 5717 images for training, 5823 images for validation and 10991 images for test. Moreover, for each image, we use the fast mode of selective search by Uijlings et al. (2013) to produce around 2000 bounding boxes.", "startOffset": 129, "endOffset": 1864}, {"referenceID": 3, "context": "2 ACTION CLASSIFICATION TASK Dataset: In the next experiment we use the PASCAL VOC2012 action classification dataset provided by Everingham et al. (2014). The dataset contains 4588 images and 6278 \u201ctrainval\u201d person bounding boxes. For each of the 10 target classes, we divide the trainval dataset into equally sized training, validation and test sets. We tuned the learning rate, regularization weight, and for all the algorithms based on their performance on our validation dataset, and report the results on the test set. For all algorithms we used the entire available training set in a single batch and performed 300 iterations. Algorithms: We train our nonlinear direct loss minimization as well as all the baselines individually for each class. As baselines we use a deep network trained with cross entropy and hinge loss as baselines. The deep network used in those experiments follows the architecture of Krizhevsky et al. (2012), with the top dimension adjusted to 1 and the parameters initialized using weights trained on ILSVRC2012 (Russakovsky et al., 2015). Inspired by RCNN (Girshick et al., 2014), we cropped the regions of each image with a padding of 16 pixels and interpolated them to a size of 227\u00d7 227\u00d7 3 to fit the input data dimension of the network. All the algorithms we compare to as well as our approach use raw pixels as input. Results: We provide quantitative results in Tab. 1. Our direct AP loss minimization clearly outperforms the baselines by 1.9% and 3.1% respectively. This is quite significant for this task. 4.3 OBJECT DETECTION TASK Dataset: We use the PASCAL VOC2012 object detection dataset collected by Everingham et al. (2014). The dataset contains 5717 images for training, 5823 images for validation and 10991 images for test. Moreover, for each image, we use the fast mode of selective search by Uijlings et al. (2013) to produce around 2000 bounding boxes. We train algorithms on the training set and report results on the validation set. Algorithms: On this dataset we follow the RCNN paradigm Girshick et al. (2014). For direct loss minimization, we adjust the dimension of the top layer of the network Krizhevsky et al.", "startOffset": 129, "endOffset": 2064}, {"referenceID": 3, "context": "2 ACTION CLASSIFICATION TASK Dataset: In the next experiment we use the PASCAL VOC2012 action classification dataset provided by Everingham et al. (2014). The dataset contains 4588 images and 6278 \u201ctrainval\u201d person bounding boxes. For each of the 10 target classes, we divide the trainval dataset into equally sized training, validation and test sets. We tuned the learning rate, regularization weight, and for all the algorithms based on their performance on our validation dataset, and report the results on the test set. For all algorithms we used the entire available training set in a single batch and performed 300 iterations. Algorithms: We train our nonlinear direct loss minimization as well as all the baselines individually for each class. As baselines we use a deep network trained with cross entropy and hinge loss as baselines. The deep network used in those experiments follows the architecture of Krizhevsky et al. (2012), with the top dimension adjusted to 1 and the parameters initialized using weights trained on ILSVRC2012 (Russakovsky et al., 2015). Inspired by RCNN (Girshick et al., 2014), we cropped the regions of each image with a padding of 16 pixels and interpolated them to a size of 227\u00d7 227\u00d7 3 to fit the input data dimension of the network. All the algorithms we compare to as well as our approach use raw pixels as input. Results: We provide quantitative results in Tab. 1. Our direct AP loss minimization clearly outperforms the baselines by 1.9% and 3.1% respectively. This is quite significant for this task. 4.3 OBJECT DETECTION TASK Dataset: We use the PASCAL VOC2012 object detection dataset collected by Everingham et al. (2014). The dataset contains 5717 images for training, 5823 images for validation and 10991 images for test. Moreover, for each image, we use the fast mode of selective search by Uijlings et al. (2013) to produce around 2000 bounding boxes. We train algorithms on the training set and report results on the validation set. Algorithms: On this dataset we follow the RCNN paradigm Girshick et al. (2014). For direct loss minimization, we adjust the dimension of the top layer of the network Krizhevsky et al. (2012) to be 1 and fine-tune using weights pre-trained on ILSVRC2012 (Russakovsky et al.", "startOffset": 129, "endOffset": 2176}, {"referenceID": 4, "context": "As baselines, we use the fine-tuned softmax network of Girshick et al. (2014). Since this network was jointly trained for all classes, we also evaluate a network which uses cross-entropy and is trained separately for each class.", "startOffset": 55, "endOffset": 78}, {"referenceID": 4, "context": "As baselines, we use the fine-tuned softmax network of Girshick et al. (2014). Since this network was jointly trained for all classes, we also evaluate a network which uses cross-entropy and is trained separately for each class. We can thus evaluate whether a separate training of each class results in significant benefits. Again, the network structure was chosen to be identical and we use the parameters provided by Russakovsky et al. (2015) for initialization.", "startOffset": 55, "endOffset": 445}, {"referenceID": 8, "context": "As in (McAllester et al., 2010), we assume in the last equality above that any joint measure \u03c1 on \u2206F j,1 w \u00b7 \u00b7 \u00b7\u2206F i,n w ,\u2206L(y) ,\u2206wT\u2207\u2206F i,j w can be expressed as a measure \u03bc on \u2206L(y) ,\u2206wT\u2207\u2206F i,j w and a bounded continuous conditional density function f .", "startOffset": 6, "endOffset": 31}, {"referenceID": 8, "context": "The conditions for the above results to hold are similar to the conditions for the proof for the binary linear case (McAllester et al., 2010).", "startOffset": 116, "endOffset": 141}, {"referenceID": 7, "context": "We think this is the reason why McAllester et al. (2010) report the negative update to perform better.", "startOffset": 32, "endOffset": 57}, {"referenceID": 1, "context": "Note that Cheng et al. (2009) also reported good performance for the perceptron method on the TIMIT dataset, the same one used in McAllester et al.", "startOffset": 10, "endOffset": 30}, {"referenceID": 1, "context": "Note that Cheng et al. (2009) also reported good performance for the perceptron method on the TIMIT dataset, the same one used in McAllester et al. (2010). Negative and perceptron updates perform similarly on the noisy and not linearly separable dataset.", "startOffset": 10, "endOffset": 155}], "year": 2017, "abstractText": "Supervised training of deep neural nets typically relies on minimizing crossentropy. However, in many domains, we are interested in performing well on specific application-specific metrics. In this paper we proposed a direct loss minimization approach to train deep neural networks, taking into account the application-specific loss functions. This can be non-trivial, when these functions are non-smooth and non-decomposable. We demonstrate the effectiveness of our approach in the context of maximizing average precision for ranking problems. Towards this goal, we propose a dynamic programming algorithm that can efficiently compute the weight updates. Our approach proves superior to a variety of baselines in the context of action classification and object detection.", "creator": "LaTeX with hyperref package"}}}