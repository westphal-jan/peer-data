{"id": "1511.06426", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Reasoning in Vector Space: An Exploratory Study of Question Answering", "abstract": "Question answering tasks have shown remarkable progress with distributed vector representations. In this paper, we look into the recently proposed Facebook 20 tasks (FB20). Finding the answers for questions in FB20 requires complex reasoning. Because the previous work on FB20 consists of end-to-end models, it is unclear whether errors come from imperfect understanding of semantics or in certain steps of the reasoning. To address this issue, we propose two vector space models inspired by tensor product representation (TPR) to perform analysis, knowledge representation, and reasoning based on common-sense inference. We achieve near-perfect accuracy on all categories, including positional reasoning and pathfinding that have proved difficult for all previous approaches due to the special two-dimensional relationships identified from this study. The exploration reported in this paper and our subsequent work on generalizing the current model to the TPR formalism suggest the feasibility of developing further reasoning models in tensor space with learning capabilities.", "histories": [["v1", "Thu, 19 Nov 2015 22:30:10 GMT  (37kb)", "https://arxiv.org/abs/1511.06426v1", null], ["v2", "Thu, 7 Jan 2016 22:30:01 GMT  (42kb)", "http://arxiv.org/abs/1511.06426v2", null], ["v3", "Tue, 19 Jan 2016 11:16:46 GMT  (42kb)", "http://arxiv.org/abs/1511.06426v3", null], ["v4", "Fri, 26 Feb 2016 18:49:34 GMT  (42kb)", "http://arxiv.org/abs/1511.06426v4", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["moontae lee", "xiaodong he", "wen-tau yih", "jianfeng gao", "li deng", "paul smolensky"], "accepted": true, "id": "1511.06426"}, "pdf": {"name": "1511.06426.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["moontae@cs.cornell.edu", "xiaohe@microsoft.com", "scottyih@microsoft.com", "jfgao@microsoft.com", "deng@microsoft.com", "smolensky@jhu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 151 1,06 426v 4 [piece"}, {"heading": "1 INTRODUCTION", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country and in which it is a country."}, {"heading": "2 RELATED WORK", "text": "In computer literature, Skipgram / CBoW (Mikolov et al. (2013) and GloVe (Pennington et al. (2014)) are popular models based on distributional similarities in word and deed; they have often been used as a starting point for a variety of other NLP tasks. In cognitive science literature, however, BEAGLE (Jones & Mewhort) and DVRS (Ustun et al.) are treated differently, with random initials and cycles. They have two vectors for each word to describe physical properties and a lexical vector."}, {"heading": "3 MODELS AND ANALYSIS", "text": "The bAbI dataset consists of twenty different types of questions, each of which is atomic and independent of the others (Weston et al. (2015)). In this section we examine groups of categories with QA problems, analyze which logical properties are common across different types and explain on the basis of our vector space models, why it is difficult to achieve good accuracy in certain categories: position logic and pathfinding."}, {"heading": "3.1 CONTAINEE-CONTAINER RELATIONSHIP", "text": "The first three categories of bAbI ask for the current or previous locations of actors and objects based on the statements made before the question. Category 1-3 questions each require exactly one, two or three supporting facts in order to determine the correct answers. Figure 1 illustrates sample statements and questions extracted from real examples in the training set. \"An object belonging to an actor requires a simple rule of common sense that is dropped in a particular place until someone permanently holds it in that place and moves with it.\" While two independent relationships, Pick / Drop and Move appear to be involved in parallel in the Category 2 tasks, these questions can all be formally answered under the transitivity of an actor."}, {"heading": "3.2 MULTIPLE RELATIONSHIPS", "text": "This year it is so far that it is not as far as it has been in recent years."}, {"heading": "4 EXPERIMENTAL RESULTS", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "5 CONCLUSION", "text": "The most important contributions of this paper are twofold: First, we thoroughly analyze the recently celebrated bAbI questions by grouping the twenty categories based on their relational characteristics. Our analysis shows that most categories are determined by unirelational characteristics other than position logic and pathfinding. As it turns out that these conclusions similarly support transitivity, it could be dangerous to evaluate the capacity of network models based solely on their performance on bAbI. In contrast, two more difficult categories require the ability to perform multirelational thinking, an ability that appears to be lacking in most previous models. Later, a more complex dataset could be developed that requires much harder reasoning by introducing multiple relationships. Second, we propose two vector space models that can perform logistic thinking for QA with distributed representations. While TPR can perform for various problems such as tree / grammar coding and lambda calculation, logistical reasoning is a new area of reasoning that can be used for a shared reasoning."}], "references": [{"title": "A neural probabilistic language model", "author": ["Bengio", "Yoshua", "Ducharme", "Rejean", "Vincent", "Pascal", "Janvin", "Christian"], "venue": "JMLR, 3:1137\u20131155,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Semantic parsing on Freebase from questionanswer pairs", "author": ["Berant", "Jonathan", "Chou", "Andrew", "Frostig", "Roy", "Liang", "Percy"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Berant et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Question answering with subgraph embeddings", "author": ["Bordes", "Antoine", "Chopra", "Sumit", "Weston", "Jason"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Bordes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Deconstructing ai-complete question-answering: going beyond toy tasks", "author": ["Dupoux", "Emmanuel"], "venue": null, "citeRegEx": "Dupoux and Emmanuel.,? \\Q2015\\E", "shortCiteRegEx": "Dupoux and Emmanuel.", "year": 2015}, {"title": "Towards a formal distributional semantics: Simulating logical calculi with tensors", "author": ["Grefenstette", "Edward"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Grefenstette and Edward.,? \\Q2013\\E", "shortCiteRegEx": "Grefenstette and Edward.", "year": 2013}, {"title": "Representing word meaning and order information in a composite holographic lexicon", "author": ["Jones", "Michael N", "Mewhort", "Douglas J. K"], "venue": "Psychological Review,", "citeRegEx": "Jones et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Jones et al\\.", "year": 2007}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Kumar", "Ankit", "Irsoy", "Ozan", "Su", "Jonathan", "Bradbury", "James", "English", "Robert", "Pierce", "Brian", "Ondruska", "Peter", "Gulrajani", "Ishaan", "Socher", "Richard"], "venue": "CoRR, abs/1506.07285,", "citeRegEx": "Kumar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Towards neural network-based reasoning", "author": ["Peng", "Baolin", "Lu", "Zhengdong", "Li", "Hang", "Wong", "Kam-Fai"], "venue": "CoRR, abs/1508.05508,", "citeRegEx": "Peng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Pennington", "Jeffrey", "Socher", "Richard", "Manning", "Christopher D"], "venue": "pp. 1532\u20131543,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Low-dimensional embeddings of logic", "author": ["Rocktaschel", "Tim", "Singh", "Sameer", "Bosnjak", "Matko", "Riedel", "Sebastian"], "venue": null, "citeRegEx": "Rocktaschel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rocktaschel et al\\.", "year": 2014}, {"title": "Tensor product variable binding and the representation of symbolic structures in connectionist systems", "author": ["Smolensky", "Paul"], "venue": "Artificial Intelligence,", "citeRegEx": "Smolensky and Paul.,? \\Q1990\\E", "shortCiteRegEx": "Smolensky and Paul.", "year": 1990}, {"title": "Symbolic functions from neural computation", "author": ["Smolensky", "Paul"], "venue": "Philosophical Transactions of the Royal Society,", "citeRegEx": "Smolensky and Paul.,? \\Q2012\\E", "shortCiteRegEx": "Smolensky and Paul.", "year": 2012}, {"title": "The Harmonic Mind: From Neural Computation to OptimalityTheoretic GrammarVolume I: Cognitive Architecture", "author": ["Smolensky", "Paul", "Legendre", "Geraldine"], "venue": null, "citeRegEx": "Smolensky et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Smolensky et al\\.", "year": 2006}, {"title": "Basic reasoning with tensor product representations", "author": ["Smolensky", "Paul", "Lee", "Moontae", "He", "Xiaodong", "Yih", "Wen-tau", "Gao", "Jianfeng", "Deng", "Li"], "venue": "Technical Report, Microsoft Research,", "citeRegEx": "Smolensky et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Smolensky et al\\.", "year": 2016}, {"title": "End-to-end memory networks", "author": ["Sukhbaatar", "Sainbayar", "Szlam", "Arthur", "Weston", "Jason", "Fergus", "Rob"], "venue": "CoRR, abs/1503.08895,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Distributed vector representations of words in the sigma cognitive architecture", "author": ["Ustun", "Volkan", "Rosenbloom", "Paul S", "Sagae", "Kenji", "Demski", "Abram"], "venue": null, "citeRegEx": "Ustun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ustun et al\\.", "year": 2014}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks. volume abs/1502.05698", "author": ["Weston", "Jason", "Bordes", "Antoine", "Chopra", "Sumit", "Mikolov", "Tomas", "Rush", "Alexander M", "van Merrienboer", "Bart"], "venue": null, "citeRegEx": "Weston et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Semantic parsing via staged query graph generation: Question answering with knowledge", "author": ["Yih", "Wen-Tau", "Chang", "MingWei", "He", "Xiaodong", "Gao", "Jianfeng"], "venue": null, "citeRegEx": "Yih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 15, "context": "Recently the Facebook bAbI tasks were introduced to evaluate complex reading comprehension via QA (Weston et al. (2015)); these have received considerable attention.", "startOffset": 99, "endOffset": 120}, {"referenceID": 1, "context": "Understanding natural questions, for example in WebQuestions tasks (Berant et al. (2013)), requires significant comprehension of the semantics, yet reasoning out the answers is then relatively simple (e.", "startOffset": 68, "endOffset": 89}, {"referenceID": 1, "context": "Understanding natural questions, for example in WebQuestions tasks (Berant et al. (2013)), requires significant comprehension of the semantics, yet reasoning out the answers is then relatively simple (e.g., Bordes et al. (2014);", "startOffset": 68, "endOffset": 228}, {"referenceID": 14, "context": "As the previous work on bAbI consists only of end-to-end models (Weston et al. (2014); Kumar et al.", "startOffset": 65, "endOffset": 86}, {"referenceID": 6, "context": "(2014); Kumar et al. (2015); Sukhbaatar et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 6, "context": "(2014); Kumar et al. (2015); Sukhbaatar et al. (2015); Peng et al.", "startOffset": 8, "endOffset": 54}, {"referenceID": 6, "context": "(2014); Kumar et al. (2015); Sukhbaatar et al. (2015); Peng et al. (2015)), it is unclear whether incorrect answers arise from an imperfect semantic understanding, inadequate knowledge encoding, or insufficient model capacity (Dupoux (2015)).", "startOffset": 8, "endOffset": 74}, {"referenceID": 6, "context": "(2014); Kumar et al. (2015); Sukhbaatar et al. (2015); Peng et al. (2015)), it is unclear whether incorrect answers arise from an imperfect semantic understanding, inadequate knowledge encoding, or insufficient model capacity (Dupoux (2015)).", "startOffset": 8, "endOffset": 241}, {"referenceID": 8, "context": "Such analysis enables us to conjecture why most existing models, in spite of their complexity, have failed to achieve good accuracy on positional reasoning and path finding tasks, whereas Peng et al. (2015) achieved successful results.", "startOffset": 188, "endOffset": 207}, {"referenceID": 13, "context": "Due to the page limit, this theoretical foundation is relegated to the supplementary materials (Smolensky et al. (2016)).", "startOffset": 96, "endOffset": 120}, {"referenceID": 0, "context": "Since the seminal work of Bengio et al. (2003), researchers have paid increasing attention to various distributed representations in continuous vector spaces.", "startOffset": 26, "endOffset": 47}, {"referenceID": 0, "context": "Since the seminal work of Bengio et al. (2003), researchers have paid increasing attention to various distributed representations in continuous vector spaces. In the computer science literature, Skipgram/CBoW (Mikolov et al. (2013)) and GloVe (Pennington et al.", "startOffset": 26, "endOffset": 232}, {"referenceID": 0, "context": "Since the seminal work of Bengio et al. (2003), researchers have paid increasing attention to various distributed representations in continuous vector spaces. In the computer science literature, Skipgram/CBoW (Mikolov et al. (2013)) and GloVe (Pennington et al. (2014)) are popular models that are trained based on the distributional similarities in word co-occurrence patterns; they have been frequently utilized as initial embeddings for a variety of other NLP tasks.", "startOffset": 26, "endOffset": 269}, {"referenceID": 0, "context": "Since the seminal work of Bengio et al. (2003), researchers have paid increasing attention to various distributed representations in continuous vector spaces. In the computer science literature, Skipgram/CBoW (Mikolov et al. (2013)) and GloVe (Pennington et al. (2014)) are popular models that are trained based on the distributional similarities in word co-occurrence patterns; they have been frequently utilized as initial embeddings for a variety of other NLP tasks. In the cognitive science literature, on the other hand, BEAGLE (Jones & Mewhort (2007)) and DVRS (Ustun et al.", "startOffset": 26, "endOffset": 557}, {"referenceID": 0, "context": "Since the seminal work of Bengio et al. (2003), researchers have paid increasing attention to various distributed representations in continuous vector spaces. In the computer science literature, Skipgram/CBoW (Mikolov et al. (2013)) and GloVe (Pennington et al. (2014)) are popular models that are trained based on the distributional similarities in word co-occurrence patterns; they have been frequently utilized as initial embeddings for a variety of other NLP tasks. In the cognitive science literature, on the other hand, BEAGLE (Jones & Mewhort (2007)) and DVRS (Ustun et al. (2014)) are trained differently, with random initializations and circular convolution.", "startOffset": 26, "endOffset": 588}, {"referenceID": 10, "context": "Similarly, Rocktaschel et al. (2014) try to find low-dimensional embeddings which can model first-order logic in a vectorial manner.", "startOffset": 11, "endOffset": 37}, {"referenceID": 14, "context": "Since the proposal of the basic MemNN (Weston et al. (2014)) model, the Adaptive/Nonlinear MemNN (Weston et al.", "startOffset": 39, "endOffset": 60}, {"referenceID": 14, "context": "Since the proposal of the basic MemNN (Weston et al. (2014)) model, the Adaptive/Nonlinear MemNN (Weston et al. (2015)), DMN (Kumar et al.", "startOffset": 39, "endOffset": 119}, {"referenceID": 6, "context": "(2015)), DMN (Kumar et al. (2015)), and MemN2N (Sukhbaatar et al.", "startOffset": 14, "endOffset": 34}, {"referenceID": 6, "context": "(2015)), DMN (Kumar et al. (2015)), and MemN2N (Sukhbaatar et al. (2015)) models have been developed by varying certain parts of these modules.", "startOffset": 14, "endOffset": 73}, {"referenceID": 6, "context": "(2015)), DMN (Kumar et al. (2015)), and MemN2N (Sukhbaatar et al. (2015)) models have been developed by varying certain parts of these modules. Nonetheless, none of these models except Peng et al. (2015) successfully accomplish either positional reasoning or path finding tasks.", "startOffset": 14, "endOffset": 204}, {"referenceID": 6, "context": "(2015)), DMN (Kumar et al. (2015)), and MemN2N (Sukhbaatar et al. (2015)) models have been developed by varying certain parts of these modules. Nonetheless, none of these models except Peng et al. (2015) successfully accomplish either positional reasoning or path finding tasks. Our speculation about the performance by Peng et al. (2015) will be given in a later section based on our bAbI analysis.", "startOffset": 14, "endOffset": 339}, {"referenceID": 17, "context": "The bAbI dataset consists of twenty different types of questions where each question category is claimed to be atomic and independent from the others (Weston et al. (2015)).", "startOffset": 151, "endOffset": 172}, {"referenceID": 8, "context": "Recently, Neural Reasoner (NR) by Peng et al. (2015) improves the accuracy for these two difficult categories by a large margin, achieving 97.", "startOffset": 34, "endOffset": 53}], "year": 2016, "abstractText": "Question answering tasks have shown remarkable progress with distributed vector representation. In this paper, we investigate the recently proposed Facebook bAbI tasks which consist of twenty different categories of questions that require complex reasoning. Because the previous work on bAbI are all end-to-end models, errors could come from either an imperfect understanding of semantics or in certain steps of the reasoning. For clearer analysis, we propose two vector space models inspired by Tensor Product Representation (TPR) to perform knowledge encoding and logical reasoning based on common-sense inference. They together achieve near-perfect accuracy on all categories including positional reasoning and path finding that have proved difficult for most of the previous approaches. We hypothesize that the difficulties in these categories are due to the multi-relations in contrast to uni-relational characteristic of other categories. Our exploration sheds light on designing more sophisticated dataset and moving one step toward integrating transparent and interpretable formalism of TPR into existing learning paradigms.", "creator": "LaTeX with hyperref package"}}}