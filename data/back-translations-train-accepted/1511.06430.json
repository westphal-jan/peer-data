{"id": "1511.06430", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Deconstructing the Ladder Network Architecture", "abstract": "Manual labeling of data is and will remain a costly endeavor. For this reason, semi-supervised learning remains a topic of practical importance. The recently proposed Ladder Network is one such approach that has proven to be very successful. In addition to the supervised objective, the Ladder Network also adds an unsupervised objective corresponding to the reconstruction costs of a stack of denoising autoencoders. Although the results are impressive, the Ladder Network has many components intertwined, whose contributions are not obvious in such a complex architecture. In order to help elucidate and disentangle the different ingredients in the Ladder Network recipe, this paper presents an extensive experimental investigation of variants of the Ladder Network in which we replace or remove individual components to gain more insight into their relative importance. We find that all of the components are necessary for achieving optimal performance, but they do not contribute equally. For semi-supervised tasks, we conclude that the most important contribution is made by the lateral connection, followed by the application of noise, and finally the choice of combinator function in the decoder path. We also find that as the number of labeled training examples increases, the lateral connections and reconstruction criterion become less important, with most of the improvement in generalization due to the injection of noise in each layer. Furthermore, we present a new type of combinator functions that outperforms the original design in both fully- and semi-supervised tasks, reducing record test error rates on Permutation-Invariant MNIST to 0.57% for supervised setting, and to 0.97% and 1.0% for semi-supervised settings with 1000 and 100 labeled examples respectively.", "histories": [["v1", "Thu, 19 Nov 2015 22:45:20 GMT  (972kb,D)", "http://arxiv.org/abs/1511.06430v1", "15 pages"], ["v2", "Fri, 27 Nov 2015 18:17:44 GMT  (1076kb,D)", "http://arxiv.org/abs/1511.06430v2", null], ["v3", "Tue, 5 Jan 2016 09:23:24 GMT  (936kb,D)", "http://arxiv.org/abs/1511.06430v3", null], ["v4", "Tue, 24 May 2016 15:53:23 GMT  (825kb,D)", "http://arxiv.org/abs/1511.06430v4", "Proceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016"]], "COMMENTS": "15 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mohammad pezeshki", "linxi fan", "philemon brakel", "aaron c courville", "yoshua bengio"], "accepted": true, "id": "1511.06430"}, "pdf": {"name": "1511.06430.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Mohammad Pezeshki", "Linxi Fan", "Aaron Courville", "Yoshua Bengio"], "emails": ["mohammad.pezeshki@umontreal.ca", "linxi.fan@columbia.edu", "pbpop3@gmail.com,", "<findme>}@iro.umontreal.ca"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, the fact is that most of them will be able to be in a position to be in what they are in."}, {"heading": "2 THE LADDER NETWORK ARCHITECTURE", "text": "The goal is to learn a function f (x) to model P (y) x, in which the noise is injected into all hidden layers. (See Figure 1) However, the objective function is a sum of the higher-level costs of the higher-level entropy and the higher-level costs. (See Figure 1) The objective function is a sum of the higher-level costs and the higher-level costs of the higher-level task. (See Figure 1) The objective costs of the higher-level entropy and the higher-level costs of the higher-level task. (See Figure 2) The objective costs of the higher-level task and the higher-level costs of the higher-level task. (See Figure 2) The objective function is a sum of the higher-level costs and the higher-level costs of the higher-level task."}, {"heading": "3 COMPONENTS OF THE LADDER NETWORK", "text": "Now that the exact architecture of the conductor network has been described in detail, we can identify a few important additions to the standard, forward-facing neural network architecture that can have a significant impact on performance. A distinction can also be made between those design decisions that naturally result from the motivation of the conductor network as a deep auto encoder, and those that are more ad hoc and task-specific. The most obvious addition is the additional reconstruction costs for each hidden layer and the input layer. While it is obvious that reconstruction provides an unattended goal to harness the unlabeled examples, it is not clear how important the punishment is for each layer and what role it plays for fully supervised tasks. A second important change is the addition of Gaussian noise to the input and the hidden representations. While adding the noise of the first layer is part of denoising the first layer, it is not necessary to fit this part of the encoding, again."}, {"heading": "4 EXPERIMENTAL SETUP", "text": "In this section, we present various variants of the vanilla conductor architecture and describe our experimental method. Each variant differs from either the vanilla model or an earlier variant in only one component. This allows us to isolate each component and observe its effects, while other components remain unchanged. Table 1 shows the different variants and their abbreviations that we examined."}, {"heading": "4.1 NOISE VARIANTS", "text": "Different configurations of noise suppression, reconstruction error punishments and lateral connection removal suggest four different variants: \u2022 Only add noise to the first layer (FirstNoise) \u2022 Punish only the reconstruction in the first layer (FirstN & R), i.e. all lateral connections of FirstN & R are set to 0 (l \u2265 1) \u2022 Apply both of the above changes: Add noise and punish the reconstruction only in the first layer (FirstN & R) \u2022 Remove all lateral connections of FirstN & R."}, {"heading": "4.2 VANILLA COMBINATOR VARIANTS", "text": "We are trying different variants of the vanilla combinator function, which combines the two information streams from the lateral and vertical connection in an unusual way. As defined in Eq.18, the output of the vanilla combinator depends on u, z and u z, 1, which are connected to the output via two paths, one linear and the other by a sigmoid nonlinearity unit. (See Figure 2 (a)) Note that the vanilla combinator is initialized in a very specific way (Eq.19), which sets the initial weights for the lateral connection z to 1 and the vertical connection u to 0. This particular scheme encourages the conductor decoder path to learn more from the lateral information flow z than the vertical u at the beginning of the training. We are examining two variants of the initialization scheme: \u2022 random initialization (random initialization): all per element weights and distortions."}, {"heading": "4.3 GAUSSIAN COMBINATOR VARIANTS", "text": "Another choice for the combination function is the Gaussian combinator proposed in the original papers on the conductor architecture (Rasmus et al., 2015; Valpola, 2014). It is defined as: g (z, u) = (z-\u00b5 (u)) \u03c3 (u) + \u00b5 (u), (21) \u00b5 (u) = w1 sigmoid (w2 u + w3) + w4 u + w5, (22) \u03c3 (u) = w6 sigmoid (w7 u + w8) + w9 u + w10. (23) This combinator assumes that z is a Gaussian specification. Mean \u00b5 (u) and standard deviation \u03c3 (u) are modeled as non-linearity applied to u in equations 22 and 23. Strictly speaking, this is not a proper standard deviation as it is not guaranteed to be positive all the time."}, {"heading": "4.4 MLP COMBINATOR VARIANTS", "text": "We also propose a different type of elementary combinator functions based on completely hidden MLPs. The design is motivated by the fact that neural networks are universal function approximators that should theoretically be able to approximate each combination function. We have examined two classes in this family. The first, simply referred to as MLP, maps two scalars [u, z] to a single output g (z, u) (Figure 2 (b)). We determine empirically the selection of the activation function for the hidden layers. Preliminary experiments show that the Leaky Rectifier Linear Unit (LReLU) (LReLU) (Maas et al., 2013) performs better than either the conventional ReLU or the sigmoid unit. Our LReLU function is called LReLU (x) = {x, if x \u2265 0, 0,1x, otherwise (26) We experiment with one number of continuous layers and one in each layer."}, {"heading": "4.5 METHODOLOGY", "text": "The experimental settings include two semi-supervised classification tasks with 100 and 1000 labeled examples and a fully supervised classification task with 60000 labeled examples for the handwritten classification of the PermutationInvariant MNIST digits. In all of our experiments labeled examples are randomly selected, but the number of examples for different classes is balance.The test set is not used during the entire search and matching of hyperparameters. Each experiment is repeated 10 times with 10 different but fixed random seeds to measure the standard error of the results for different parameter initializations and different selections of labeled examples. All standard errors are directly comparable as the arrangement of 10 seeds is the same. All variants and the vanilla conductor network itself are trained with the ADAM optimization algorithm (Kingma & Ba, 2014) with a learning rate of 0.002 for 100 iterations followed by 50 iterations with a learning rate, each falling to a full-zero for the inscription."}, {"heading": "5 RESULTS & DISCUSSION", "text": "Table 2 collects all the results for the variants and baselines. The results are divided into three main categories in Table 2. Box plots of the results are also shown in Figure 3.The baseline model is a simple, forward-facing neural network without reconstruction penalty and the baseline + noise is the same network, but with additive noise at each level. The best results in terms of the average error rate on the test set are achieved by the proposed AMLP combination function: in the fully monitored environment, the best average error rate is 0.569 \u00b1 0.010, while in the semi-monitored environment with 100 or 1000 marked examples, the averages are 1,002 \u00b1 0.037 or 0.974 \u00b1 0.021, respectively."}, {"heading": "5.1 NOISE VARIANTS", "text": "The results in the first part of the table suggest that adding noise to either the first layer or all layers leads to a lower error rate in relation to the base layers. Our intuition is that the effect of additive noise is very similar in the layers of the Method of Weight Noise Regulation (Graves, 2011) or Exposure (Hinton et al., 2012) method. Furthermore, the error rates in the first part of the table show us that removing the lateral joints is much more damaging than the absence of noise injections or reconstruction penalties in the intermediate layers. It is also worth noting that the hyperparameter setting zero weights for punishing reconstruction errors in all layers except the input layer in the fully monitored task for the vanilla model. Something similar also occurs in NoLateral, where the hyperparameter setting zero reconstruction weights for all layers including the base layer gives the same noise + other words: relative to the noise in the first part of the table."}, {"heading": "5.2 COMBINATOR FUNCTION VARIANTS", "text": "The second and third parts of Table 2 show the relative performance of different combinator functions. Unsurprisingly, performance deteriorates considerably when we remove the sigmoid nonlinearity (NoSig) or the multiplicative term (NoMult) or both (linear) from the vanilla combinator. Measured by the magnitude of the increase in average error rates, the multiplicative term is more important than the sigmoid unit. As described in 4.2 and Equation 19, the element weights of the lateral compounds are initialized to one, while those of the vertical ones are initialized to zeros. Interestingly, the results for the RandInit variant, in which these weights are randomly initialized, are slightly worse than the RevInit variant as a random initialization scheme. We suspect that the reason for this is that the optimization algorithm finds it easier to reconstruct the binative forms of z-z models than those starting from their noisy version."}, {"heading": "5.3 PROBABILISTIC INTERPRETATIONS OF THE LADDER NETWORK", "text": "Since many of the motivations behind the regulated auto-encoder architectures are based on observations of generative models, we briefly discuss how the PCB network can be related to some other models with varying degrees of probable interpretability. Given that the components that most define the PCB network appear to be the most important for semi-monitored learning, comparisons with generative models are at least intuitively appealing to gain more insight into how the model learns from unmarked examples. By training the individual auto-encoders that make up the PCB network with a single objective function, this coupling goes so far as to produce lower-level representations that can easily be reconstructed from the upper levels."}, {"heading": "6 CONCLUSION", "text": "The paper systematically compares different variants of the most recent PCB architecture (Rasmus et al., 2015; Valpola, 2014) with two forward-facing neural networks as baselines and the standard architecture (proposed in the original paper). Comparisons are made in a deconstructive manner based on the standard architecture. Based on comparisons of different variants, we come to the following conclusion: \u2022 Unsurprisingly, the cost of reconstruction is crucial for obtaining the desired regulation from unspecified data. \u2022 Applying additive noise to each layer, and in particular the first layer, has a regulating effect that contributes to generalization. This appears to be one of the most important factors in the performance of the fully monitored task. \u2022 The lateral connection is a critical component in the PCB architecture to the extent that its removal significantly degrades performance for all semi-monitored tasks. \u2022 The precise choice of the combinator function has less dramatic effects, although the vanilla combinator contributes to the performance enhancement of the MP, which can be replaced by the supervised MP."}, {"heading": "ACKNOWLEDGMENTS", "text": "The experiments were carried out with the libraries Theano (Bergstra et al., 2010), (Bastien et al., 2012), Blocks and Fuel (van Merrie \u0178nboer et al., 2015) and the authors are grateful for the financial support of NSERC, Calcul Quebec, Compute Canada, the Canada Research Chairs and CIFAR."}], "references": [{"title": "What regularized auto-encoders learn from the data generating distribution", "author": ["Alain", "Guillaume", "Bengio", "Yoshua"], "venue": "In ICLR\u20192013. also arXiv report 1211.4246,", "citeRegEx": "Alain et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Alain et al\\.", "year": 2013}, {"title": "Theano: new features and speed improvements", "author": ["Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Warde-Farley", "David", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1211.5590,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Generalized denoising autoencoders as generative models", "author": ["Bengio", "Yoshua", "Yao", "Li", "Alain", "Guillaume", "Vincent", "Pascal"], "venue": "In NIPS\u20192013,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Random search for hyper-parameter optimization", "author": ["Bergstra", "James", "Bengio", "Yoshua"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bergstra et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2012}, {"title": "Theano: a cpu and gpu math expression compiler", "author": ["Bergstra", "James", "Breuleux", "Olivier", "Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Desjardins", "Guillaume", "Turian", "Joseph", "Warde-Farley", "David", "Bengio", "Yoshua"], "venue": "In Proceedings of the Python for scientific computing conference (SciPy),", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Practical variational inference for neural networks", "author": ["Graves", "Alex"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Graves and Alex.,? \\Q2011\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2011}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Hinton", "Geoffrey E", "Osindero", "Simon", "Teh", "Yee-Whye"], "venue": "Neural computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Technical report,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Semisupervised learning with deep generative models", "author": ["Kingma", "Diederik P", "Mohamed", "Shakir", "Rezende", "Danilo Jimenez", "Welling", "Max"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Classification using discriminative restricted boltzmann machines", "author": ["Larochelle", "Hugo", "Bengio", "Yoshua"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Larochelle et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2008}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["Maas", "Andrew L", "Hannun", "Awni Y", "Ng", "Andrew Y"], "venue": "In Proc. ICML,", "citeRegEx": "Maas et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2013}, {"title": "Semi-supervised learning of compact document representations with deep networks", "author": ["Ranzato", "Marc\u2019Aurelio", "Szummer", "Martin"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Ranzato et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2008}, {"title": "Semisupervised learning with ladder network", "author": ["Rasmus", "Antti", "Valpola", "Harri", "Honkala", "Mikko", "Berglund", "Mathias", "Raiko", "Tapani"], "venue": "arXiv preprint arXiv:1507.02672,", "citeRegEx": "Rasmus et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rasmus et al\\.", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo J", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "In ICML\u20192014,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "From neural pca to deep unsupervised learning", "author": ["Valpola", "Harri"], "venue": "arXiv preprint arXiv:1411.7783,", "citeRegEx": "Valpola and Harri.,? \\Q2014\\E", "shortCiteRegEx": "Valpola and Harri.", "year": 2014}, {"title": "Blocks and fuel: Frameworks for deep learning", "author": ["van Merri\u00ebnboer", "Bart", "Bahdanau", "Dzmitry", "Dumoulin", "Vincent", "Serdyuk", "Dmitriy", "Warde-Farley", "David", "Chorowski", "Jan", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1506.00619,", "citeRegEx": "Merri\u00ebnboer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2015}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Lajoie", "Isabelle", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": "J. Machine Learning Res.,", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 6, "context": "Since unsupervised methods for pre-training layers or neural networks were an essential part of the first wave of deep learning methods (Hinton et al., 2006; Vincent et al., 2008; Bengio, 2009), \u2217Yoshua Bengio is a CIFAR Senior Fellow", "startOffset": 136, "endOffset": 193}, {"referenceID": 18, "context": "Since unsupervised methods for pre-training layers or neural networks were an essential part of the first wave of deep learning methods (Hinton et al., 2006; Vincent et al., 2008; Bengio, 2009), \u2217Yoshua Bengio is a CIFAR Senior Fellow", "startOffset": 136, "endOffset": 193}, {"referenceID": 8, "context": "More recent examples of approaches for semi-supervised deep learning are the semisupervised Variational Autoencoder (Kingma et al., 2014) and the Ladder Network (Rasmus et al.", "startOffset": 116, "endOffset": 137}, {"referenceID": 13, "context": ", 2014) and the Ladder Network (Rasmus et al., 2015) which obtained very impressive, state of the art results (1.", "startOffset": 31, "endOffset": 52}, {"referenceID": 19, "context": "The Ladder Network adds an unsupervised component to the supervised learning objective of a deep feedforward network by treating this network as part of a deep stack of denoising autoencoders or DAEs (Vincent et al., 2010) that learns to reconstruct each layer (including the input) based on a corrupted version of it, using feedback from upper levels.", "startOffset": 200, "endOffset": 222}, {"referenceID": 13, "context": "For a more detailed explanation of the Ladder architecture, see (Rasmus et al., 2015; Valpola, 2014).", "startOffset": 64, "endOffset": 100}, {"referenceID": 13, "context": "Another choice for the combinator function is the Gaussian combinator proposed in the original papers about the Ladder Architecture (Rasmus et al., 2015; Valpola, 2014).", "startOffset": 132, "endOffset": 168}, {"referenceID": 11, "context": "Preliminary experiments show that the Leaky Rectifier Linear Unit (LReLU) (Maas et al., 2013) performs better than either the conventional ReLU or the sigmoid unit.", "startOffset": 74, "endOffset": 93}, {"referenceID": 7, "context": "Our intuition is that the effect of additive noise to layers is very similar to the weight noise regularization method (Graves, 2011) or dropout (Hinton et al., 2012).", "startOffset": 145, "endOffset": 166}, {"referenceID": 14, "context": "autoencoder (Rezende et al., 2014; Bengio, 2014).", "startOffset": 12, "endOffset": 48}, {"referenceID": 2, "context": "When one simply views the Ladder Network as a peculiar type of denoising autoencoder, one could extend the recent work on the generative interpretation of denoising autoencoders (Alain & Bengio, 2013; Bengio et al., 2013) to interpret the Ladder Network as a generative model as well.", "startOffset": 178, "endOffset": 221}, {"referenceID": 13, "context": "The paper systematically compares different variants of the recent Ladder Network architecture (Rasmus et al., 2015; Valpola, 2014) with two feedforward neural networks as the baselines and the standard architecture (proposed in the original paper).", "startOffset": 95, "endOffset": 131}], "year": 2017, "abstractText": "Manual labeling of data is and will remain a costly endeavor. For this reason, semi-supervised learning remains a topic of practical importance. The recently proposed Ladder Network is one such approach that has proven to be very successful. In addition to the supervised objective, the Ladder Network also adds an unsupervised objective corresponding to the reconstruction costs of a stack of denoising autoencoders. Although the results are impressive, the Ladder Network has many components intertwined, whose contributions are not obvious in such a complex architecture. In order to help elucidate and disentangle the different ingredients in the Ladder Network recipe, this paper presents an extensive experimental investigation of variants of the Ladder Network in which we replace or remove individual components to gain more insight into their relative importance. We find that all of the components are necessary for achieving optimal performance, but they do not contribute equally. For semi-supervised tasks, we conclude that the most important contribution is made by the lateral connection, followed by the application of noise, and finally the choice of combinator function in the decoder path. We also find that as the number of labeled training examples increases, the lateral connections and reconstruction criterion become less important, with most of the improvement in generalization due to the injection of noise in each layer. Furthermore, we present a new type of combinator functions that outperforms the original design in both fullyand semi-supervised tasks, reducing record test error rates on Permutation-Invariant MNIST to 0.57% for supervised setting, and to 0.97% and 1.0% for semi-supervised settings with 1000 and 100 labeled examples respectively.", "creator": "LaTeX with hyperref package"}}}