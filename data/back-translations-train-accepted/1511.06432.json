{"id": "1511.06432", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Delving Deeper into Convolutional Networks for Learning Video Representations", "abstract": "We propose an approach to learn spatio-temporal features in videos from intermediate visual representations we call \"percepts\" using Gated-Recurrent-Unit Recurrent Networks (GRUs).Our method relies on percepts that are extracted from all level of a deep convolutional network trained on the large ImageNet dataset. While high-level percepts contain highly discriminative information, they tend to have a low-spatial resolution. Low-level percepts, on the other hand, preserve a higher spatial resolution from which we can model finer motion patterns. Using low-level percepts can leads to high-dimensionality video representations. To mitigate this effect and control the model number of parameters, we introduce a variant of the GRU model that leverages the convolution operations to enforce sparse connectivity of the model units and share parameters across the input spatial locations.", "histories": [["v1", "Thu, 19 Nov 2015 22:46:13 GMT  (387kb,D)", "http://arxiv.org/abs/1511.06432v1", "ICLR submission"], ["v2", "Mon, 23 Nov 2015 02:46:54 GMT  (386kb,D)", "http://arxiv.org/abs/1511.06432v2", "ICLR submission"], ["v3", "Thu, 7 Jan 2016 19:43:19 GMT  (387kb,D)", "http://arxiv.org/abs/1511.06432v3", "ICLR submission"], ["v4", "Tue, 1 Mar 2016 18:54:11 GMT  (388kb,D)", "http://arxiv.org/abs/1511.06432v4", "ICLR 2016"]], "COMMENTS": "ICLR submission", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["nicolas ballas", "li yao", "chris pal", "aaron courville"], "accepted": true, "id": "1511.06432"}, "pdf": {"name": "1511.06432.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["DELVING DEEPER", "Nicolas Ballas", "Li Yao", "Chris Pal", "Aaron Courville"], "emails": [], "sections": [{"heading": "1 INTRODUCTION", "text": "In recent years, it has become clear that most of them are people who are able to survive themselves, but also people who are able to survive themselves. Most of them are not able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves because they are not able to survive themselves."}, {"heading": "2 GRU: GATED RECURRENT UNITS", "text": "ht = Gated Recurrent Unit (GRU) network which is a particular type of RNN. An RNN model is applied to a sequence of inputs that may have variable lengths. It defines a recursive hidden state, the activation of which at any time depends on the previous time. RNNs are defined as RNNN hidden state at time t (ht \u2212 1, xt), which is a non-linear activation function, due to the exploding or disappearing gradient effect (Bengio et al., 1994). Variants of RNNNs such as Long Short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) or Gated Recurrent Units (Cho et al, 2014) have empirically demonstrated their ability to perform long-term imaging under GRU."}, {"heading": "3 DELVING DEEPER IN THE CONVOLUTIONAL NEURAL NETWORK", "text": "This section delves into the most important contributions in this paper. We aim to use visual perceptions of different wave planes to capture temporal patterns that occur at different spatial resolutions. Let's consider (x1t,..., x L \u2212 1 t, x L t) (t = 1.. T) a series of 2D wave maps extracted from L layers in different time steps in a video. We propose two alternative RCN architectures, GRU-RCN and Stacked-GRU-RCN (Figure 2), which combine information from these wave maps."}, {"heading": "3.1 GRU-RCN:", "text": "In the first RCN architecture, we propose to apply L RNNs independently of each other in order to apply the underlying environment maps directly to us. We define L RNNNs as \u03c61,..., \u03c6L, so that the hidden representation of the last time stage h1T,..., h L T are then fed into the case of action recognition, or into a text decoder RNN for the label generation.To implement, we propose the recursive function to use the recursive unit (Cho et al., 2014). GRUs were originally introduced for machine translation. There are models that use hidden transitions with completely hidden units. However, convolutionary map inputs are 3D tensors (spatial dimensions and input channels)."}, {"heading": "3.2 STACKED GRU-RCN:", "text": "In the second RCN architecture, we examine the importance of the bottom-up connection between the RNNs. While GRU-RCN applies each level independently of each other, stacked GRU-RNN presupposes each GRU RNN to the performance of the previous GRU RNN in the current time step: hlt = \u03c6 l (hlt \u2212 1, h \u2212 1 t, x l t). The previous hidden RNN representation is given as an extra input to the GRU turning units: zll = \u03c3 (W l \u00b2 xlt + Wlzl \u00b2 h \u2212 1 t + U l \u00b2 hlt \u2212 1), (9) rlt = \u03c3 (W l \u00b2 xlt + Wlrl \u00b2 h \u2212 1 t + U l \u00b2 t \u2212 1), (10) h \u00b2 l = tanh (W l \u00b2 xlt + Ul \u00b2 hlt \u2212 1), (11) hlt = 1 l \u00b2 l \u2212 1 l \u2212 1)."}, {"heading": "4 RELATED WORK", "text": "However, unlike image classification (Simonyan & Zisserman, 2014b), CNNs have not made much improvement over these traditional methods (Lan et al., 2014) by highlighting the difficulty of learning video representations even with large training data sets. Simonyan & Zisserman (2014a), however, have introduced a two-stream framework in which they train CNNs independently at the RGB level and optical flow inputs."}, {"heading": "5 EXPERIMENTATION", "text": "This section presents an empirical evaluation of the proposed architectures GRU-RCN and Stacked GRU-RCN. We conduct experiments on two different tasks: detection of human actions and generation of video labels."}, {"heading": "5.1 ACTION RECOGNITION", "text": "We evaluate our approach based on the UCF101 dataset Soomro et al. (2012), which includes 101 action classes spanning over 13320 YouTube video clips. Videos composing the dataset are subject to large camera movements, viewing angle changes, and cluttered backgrounds. We report the results of the first part of the UCF101 dataset, as it is most widely used in the literature. To perform a correct hyperparameter search, we use the videos from the UCF-Thumos validation split Jiang et al. (2014) as a validation set."}, {"heading": "5.1.1 MODEL ARCHITECTURE", "text": "This year it has come to the point where we will be able to put ourselves in the lead, \"he said in an interview with the Deutsche Presse-Agentur.\" We have never lost so much time, \"he said.\" But it is too early to say thank you that it is as good as possible. \""}, {"heading": "5.1.2 MODEL TRAINING AND EVALUATION", "text": "We follow the training method introduced by the Simonyan & Zisserman two-stream frame (2014a), which randomly scans a series of 64 videos at each corner. To compare the cut-out width and height of 256, 224, 192, 168, the temporal cut-out size is set to 10. We then reduce the cut-out volume to 224 x 224 x 10 cm. We value each model by maximizing the model log-in licenses: L (1), L (2), L (2), L (2), L (3), L (3), L (4), L), L (4), L (4), L (4), L (4), L (4), L (4), L (5), L (4), L (4), L, \"L (L)."}, {"heading": "5.2 VIDEO CAPTIONNING", "text": "The data set includes 1,970 video clips with multiple descriptions of natural language for each video clip. The data set is open domain and covers a wide range of topics such as sports, animals, music and film clips. Following Yao et al. (2015b), we divided the data set into a training set of 1,200 video clips, a validation set of 100 clips and a test set consisting of the remaining clips."}, {"heading": "5.2.1 MODEL SPECIFICATIONS", "text": "To perform video labeling, we use the so-called encoder decoder framework Cho et al. (2014). Encoder maps input videos into abstract representations that require the generation of captions. As far as the encoder is concerned, we compare both VGG-16 CNN and bi-directional GRU-RCN. Both models are tuned to the UCF-101 dataset and therefore focus on action detection. To extract an abstract representation from a video, we try K-segments with the same space. When using the encoder VGG16, we provide the fc7 activation of the first frame of each segment as an input to the text decoder. For the GRU-RCN, we apply our model to the first 10 frames of the segment. We concatenate the encoder GRU-RCN hidden-layer representation of the first frame of each segment as a text decoder."}, {"heading": "5.2.2 TRAINING", "text": "For all video subtitling models, we estimated the parameters of the decoder by maximizing the log probability: L (\u03b8) = 1 N \u2211 n = 1 tn \u2211 i = 1 log p (yni | yn < i, xn, \u03b8), where there are N training pairs (xn, yn) and each description yn is two words long. We used Adadelta Liners (2012) to optimize the hyperparameters (e.g. number of LSTM units and word embedding dimensionality, number of segmentK) by means of random search (Bergstra & Bengio, 2012) to maximize the log probability of the validation set."}, {"heading": "5.2.3 RESUTS", "text": "Table 2 reports on the performance of our proposed method using three automatic evaluation metrics: BLEU in Papineni et al. (2002), METEOR in Denkowski & Lavie (2014) and CIDER in Vedantam et al. (2014) We use the evaluation script prepared and implemented in Chen et al. (2015) All models are based on the negative log probability (NLL) of the validation set, and then we select the model that performs best on the metric validation rate when taken into account. The first two lines of Table 2 compare the performance of the VGG-16 and the bi-dirich GRURCN encoder. Results clearly show the superiority of the Bi-Directional GRU-RCN encoder as it maps the VGG-16 encoder on all three metrics. Specifically, GRU-RCN encoder achieves a 10% performance gain."}, {"heading": "6 CONCLUSION", "text": "In this paper, we address the challenging problem of learning discriminatory and abstract representations from videos. We identify and emphasize the importance of modeling temporal variations of \"visual perceptions\" at different spatial resolutions. While high-level perceptions contain highly discriminatory information, they tend to have low spatial resolution. Low perceptions, on the other hand, maintain a higher spatial resolution from which we can model finer motion patterns. We have introduced a novel, recurring Constitutional Network Architecture that uses revolutionary maps from all levels of a deep Constitutional network trained on the ImageNet dataset to use \"perceptions\" from different spatial resolutions. We have empirically validated our approach to recognizing human actions and video capture tasks by using UCF-101 and YouTube2Text datasets to achieve additional results."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors would like to acknowledge the support of the following research funding and computer support agencies: NSERC, Calcul Que'bec, Compute Canada, the Canada Research Chairs and CIFAR. We would also like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012) for developing such a powerful tool for scientific computing."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Theano: new features and speed improvements", "author": ["Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian J", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Bengio", "Yoshua"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Random search for hyper-parameter optimization", "author": ["Bergstra", "James", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Bergstra et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2012}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["Bergstra", "James", "Breuleux", "Olivier", "Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Desjardins", "Guillaume", "Turian", "Joseph", "Warde-Farley", "David", "Bengio", "Yoshua"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Large displacement optical flow: descriptor matching in variational motion estimation", "author": ["T. Brox", "J. Malik"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Brox and Malik,? \\Q2011\\E", "shortCiteRegEx": "Brox and Malik", "year": 2011}, {"title": "Collecting highly parallel data for paraphrase evaluation", "author": ["Chen", "David L", "Dolan", "William B"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume", "citeRegEx": "Chen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "Microsoft coco captions: Data collection and evaluation", "author": ["Chen", "Xinlei", "Fang", "Hao", "Lin", "Tsung-Yi", "Vedantam", "Ramakrishna", "Gupta", "Saurabh", "Dollar", "Piotr", "Zitnick", "C Lawrence"], "venue": "server. arXiv 1504.00325,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung", "Junyoung", "Gulcehre", "Caglar", "Cho", "KyungHyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "author": ["Denkowski", "Michael", "Lavie", "Alon"], "venue": "In EACL Workshop,", "citeRegEx": "Denkowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denkowski et al\\.", "year": 2014}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "arXiv preprint arXiv:1411.4389,", "citeRegEx": "Donahue et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["A. Graves", "N. Jaitly"], "venue": "In ICML,", "citeRegEx": "Graves and Jaitly,? \\Q2014\\E", "shortCiteRegEx": "Graves and Jaitly", "year": 2014}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Thumos challenge: Action recognition with a large number of classes", "author": ["YG Jiang", "J Liu", "A Roshan Zamir", "G Toderici", "I Laptev", "M Shah", "R. Sukthankar"], "venue": "Technical Report,", "citeRegEx": "Jiang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2014}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["Karpathy", "Andrej", "Toderici", "George", "Shetty", "Sachin", "Leung", "Tommy", "Sukthankar", "Rahul", "Fei-Fei", "Li"], "venue": "In CVPR. IEEE,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Beyond gaussian pyramid: Multi-skip feature stacking for action recognition", "author": ["Lan", "Zhenzhong", "Lin", "Ming", "Li", "Xuanchong", "Hauptmann", "Alexander G", "Raj", "Bhiksha"], "venue": "arXiv preprint arXiv:1411.6660,", "citeRegEx": "Lan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lan et al\\.", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Beyond short snippets: Deep networks for video classification", "author": ["Ng", "Joe Yue-Hei", "Hausknecht", "Matthew", "Vijayanarasimhan", "Sudheendra", "Vinyals", "Oriol", "Monga", "Rajat", "Toderici", "George"], "venue": "arXiv preprint arXiv:1503.08909,", "citeRegEx": "Ng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2015}, {"title": "Hierarchical recurrent neural encoder for video representation with application to captioning", "author": ["Pan", "Pingbo", "Xu", "Zhongwen", "Yang", "Yi", "Wu", "Fei", "Zhuang", "Yueting"], "venue": "arXiv preprint arXiv:1511.03476,", "citeRegEx": "Pan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2015}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Papineni", "Kishore", "Roukos", "Salim", "Ward", "Todd", "Zhu", "Wei-Jing"], "venue": "In ACL,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Action bank: A high-level representation of activity in video", "author": ["S. Sadanand", "J. Corso"], "venue": "In CVPR. IEEE,", "citeRegEx": "Sadanand and Corso,? \\Q2012\\E", "shortCiteRegEx": "Sadanand and Corso", "year": 2012}, {"title": "Convolutional lstm network: A machine learning approach for precipitation nowcasting", "author": ["Shi", "Xingjian", "Chen", "Zhourong", "Wang", "Hao", "Yeung", "Dit-Yan", "Wong", "Wai-Kin", "Woo", "Wangchun"], "venue": "arXiv preprint arXiv:1506.04214,", "citeRegEx": "Shi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2015}, {"title": "Two-stream convolutional networks for action recognition in videos", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Ucf101: A dataset of 101 human actions classes from videos in the wild", "author": ["Soomro", "Khurram", "Zamir", "Amir Roshan", "Shah", "Mubarak"], "venue": "arXiv preprint arXiv:1212.0402,", "citeRegEx": "Soomro et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Soomro et al\\.", "year": 2012}, {"title": "Unsupervised learning of video representations using lstms", "author": ["N. Srivastava", "E. Mansimov", "R. Salakhutdinov"], "venue": "In ICML,", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Integrating language and vision to generate natural language descriptions of videos in the wild", "author": ["Thomason", "Jesse", "Venugopalan", "Subhashini", "Guadarrama", "Sergio", "Saenko", "Kate", "Mooney", "Raymond"], "venue": "In COLING,", "citeRegEx": "Thomason et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Thomason et al\\.", "year": 2014}, {"title": "C3d: generic features for video analysis", "author": ["D. Tran", "L. Bourdev", "R. Fergus", "L. Torresani", "M. Paluri"], "venue": "arXiv preprint arXiv:1412.0767,", "citeRegEx": "Tran et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tran et al\\.", "year": 2014}, {"title": "CIDEr: Consensus-based image description evaluation", "author": ["Vedantam", "Ramakrishna", "Zitnick", "C Lawrence", "Parikh", "Devi"], "venue": null, "citeRegEx": "Vedantam et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vedantam et al\\.", "year": 2014}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["Venugopalan", "Subhashini", "Xu", "Huijuan", "Donahue", "Jeff", "Rohrbach", "Marcus", "Mooney", "Raymond", "Saenko", "Kate"], "venue": null, "citeRegEx": "Venugopalan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2015}, {"title": "Action recognition by dense trajectories", "author": ["H. Wang", "A. Kl\u00e4ser", "C. Schmid", "C. Liu"], "venue": "In CVPR. IEEE,", "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Action recognition with trajectory-pooled deepconvolutional descriptors", "author": ["Wang", "Limin", "Qiao", "Yu", "Tang", "Xiaoou"], "venue": "arXiv preprint arXiv:1505.04868,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Towards good practices for very deep two-stream convnets", "author": ["Wang", "Limin", "Xiong", "Yuanjun", "Zhe", "Qiao", "Yu"], "venue": "arXiv preprint arXiv:1507.02159,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Trainable performance upper bounds for image and video captioning", "author": ["Yao", "Li", "Ballas", "Nicolas", "Cho", "Kyunghyun", "Smith", "John R", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1511.0459,", "citeRegEx": "Yao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2015}, {"title": "Describing videos by exploiting temporal structure", "author": ["Yao", "Li", "Torabi", "Atousa", "Cho", "Kyunghyun", "Ballas", "Nicolas", "Pal", "Christopher", "Larochelle", "Hugo", "Courville", "Aaron"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "Yao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2015}, {"title": "Video paragraph captioning using hierarchical recurrent neural networks", "author": ["Yu", "Haonan", "Wang", "Jiang", "Huang", "Zhiheng", "Yang", "Yi", "Xu", "Wei"], "venue": "arXiv 1510.07712,", "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Zeiler", "Matthew D"], "venue": "Technical report,", "citeRegEx": "Zeiler and D.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler and D.", "year": 2012}], "referenceMentions": [{"referenceID": 33, "context": "While previous work has traditionally relied on hand-crafted and task-specific representations (Wang et al., 2011; Sadanand & Corso, 2012), there is a growing interest in designing general video representations that could help solve tasks in video understanding such as human action recognition, video retrieval or video captionning (Tran et al.", "startOffset": 95, "endOffset": 138}, {"referenceID": 30, "context": ", 2011; Sadanand & Corso, 2012), there is a growing interest in designing general video representations that could help solve tasks in video understanding such as human action recognition, video retrieval or video captionning (Tran et al., 2014).", "startOffset": 226, "endOffset": 245}, {"referenceID": 33, "context": "However, such models discard temporal information that has been shown to provide important cues in videos (Wang et al., 2011).", "startOffset": 106, "endOffset": 125}, {"referenceID": 0, "context": "On the other hand, recurrent neural networks (RNN) have demonstrated the ability to understand temporal sequences in various learning tasks such as speech recognition (Graves & Jaitly, 2014) or machine translation (Bahdanau et al., 2014).", "startOffset": 214, "endOffset": 237}, {"referenceID": 27, "context": "Consequently, Recurrent Convolution Networks (RCN) (Srivastava et al., 2015; Donahue et al., 2014; Ng et al., 2015) that leverage both recurrence and convolution have recently been introduced for learning video representation.", "startOffset": 51, "endOffset": 115}, {"referenceID": 11, "context": "Consequently, Recurrent Convolution Networks (RCN) (Srivastava et al., 2015; Donahue et al., 2014; Ng et al., 2015) that leverage both recurrence and convolution have recently been introduced for learning video representation.", "startOffset": 51, "endOffset": 115}, {"referenceID": 19, "context": "Consequently, Recurrent Convolution Networks (RCN) (Srivastava et al., 2015; Donahue et al., 2014; Ng et al., 2015) that leverage both recurrence and convolution have recently been introduced for learning video representation.", "startOffset": 51, "endOffset": 115}, {"referenceID": 18, "context": "CNNs, however, hierachically build-up spatial invariance through pooling layers (LeCun et al., 1998; Simonyan & Zisserman, 2014b) as Figure 2 highlights.", "startOffset": 80, "endOffset": 129}, {"referenceID": 8, "context": "We extend the GRU-RNN model (Cho et al., 2014) and replace the fully-connected RNN linear product operation with a convolution.", "startOffset": 28, "endOffset": 46}, {"referenceID": 8, "context": "We extend the GRU-RNN model (Cho et al., 2014) and replace the fully-connected RNN linear product operation with a convolution. Our GRU-extension therefore encodes the locality and temporal smoothness prior of videos directly in the model structure. We evaluate our solution on UCF101 human action recognition from Soomro et al. (2012) as well as YouTube2text video captioning dataset from Chen & Dolan (2011).", "startOffset": 29, "endOffset": 336}, {"referenceID": 8, "context": "We extend the GRU-RNN model (Cho et al., 2014) and replace the fully-connected RNN linear product operation with a convolution. Our GRU-extension therefore encodes the locality and temporal smoothness prior of videos directly in the model structure. We evaluate our solution on UCF101 human action recognition from Soomro et al. (2012) as well as YouTube2text video captioning dataset from Chen & Dolan (2011). Experiments show that leveraging \u201cpercepts\u201d at mutiple resolutions to model temporal variation improve over baseline model with respective gain of 3.", "startOffset": 29, "endOffset": 410}, {"referenceID": 2, "context": "RNNs are known to be difficult to train due to the exploding or vanishing gradient effect (Bengio et al., 1994).", "startOffset": 90, "endOffset": 111}, {"referenceID": 8, "context": "However, variants of RNNs such as Long Short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) or Gated Recurrent Units (GRU) (Cho et al., 2014) have empirically demonstrated their ability to model long-term temporal dependency in various task such as machine translation or image/video caption generation.", "startOffset": 128, "endOffset": 146}, {"referenceID": 9, "context": "In this paper, we will mainly focus on the GRU as they have shown similar performance than LSTM but with a lower memory requirement (Chung et al., 2014).", "startOffset": 132, "endOffset": 152}, {"referenceID": 8, "context": "To implement the RNN recurrent function \u03c6, we propose to leverage Gated Recurrent Unit (Cho et al., 2014).", "startOffset": 87, "endOffset": 105}, {"referenceID": 15, "context": "Deep learning approaches have recently been used to learn video representation and produced stateof-art results (Karpathy et al., 2014; Simonyan & Zisserman, 2014a; Wang et al., 2015b; Tran et al., 2014).", "startOffset": 112, "endOffset": 203}, {"referenceID": 30, "context": "Deep learning approaches have recently been used to learn video representation and produced stateof-art results (Karpathy et al., 2014; Simonyan & Zisserman, 2014a; Wang et al., 2015b; Tran et al., 2014).", "startOffset": 112, "endOffset": 203}, {"referenceID": 17, "context": "However, unlike image classification (Simonyan & Zisserman, 2014b), CNNs did not yield large improvement over these traditional methods (Lan et al., 2014) highlighting the difficulty of learning video representation even with large training dataset.", "startOffset": 136, "endOffset": 154}, {"referenceID": 15, "context": "Deep learning approaches have recently been used to learn video representation and produced stateof-art results (Karpathy et al., 2014; Simonyan & Zisserman, 2014a; Wang et al., 2015b; Tran et al., 2014). Karpathy et al. (2014); Tran et al.", "startOffset": 113, "endOffset": 228}, {"referenceID": 15, "context": "Deep learning approaches have recently been used to learn video representation and produced stateof-art results (Karpathy et al., 2014; Simonyan & Zisserman, 2014a; Wang et al., 2015b; Tran et al., 2014). Karpathy et al. (2014); Tran et al. (2014) proposed to use 3D CNN learn a video representations, leveraging large training datasets such as Sport 1 Million.", "startOffset": 113, "endOffset": 248}, {"referenceID": 15, "context": "Deep learning approaches have recently been used to learn video representation and produced stateof-art results (Karpathy et al., 2014; Simonyan & Zisserman, 2014a; Wang et al., 2015b; Tran et al., 2014). Karpathy et al. (2014); Tran et al. (2014) proposed to use 3D CNN learn a video representations, leveraging large training datasets such as Sport 1 Million. However, unlike image classification (Simonyan & Zisserman, 2014b), CNNs did not yield large improvement over these traditional methods (Lan et al., 2014) highlighting the difficulty of learning video representation even with large training dataset. Simonyan & Zisserman (2014a) introduced a two-stream framework where they train CNNs independently on RGB and optical flow inputs.", "startOffset": 113, "endOffset": 641}, {"referenceID": 15, "context": "Deep learning approaches have recently been used to learn video representation and produced stateof-art results (Karpathy et al., 2014; Simonyan & Zisserman, 2014a; Wang et al., 2015b; Tran et al., 2014). Karpathy et al. (2014); Tran et al. (2014) proposed to use 3D CNN learn a video representations, leveraging large training datasets such as Sport 1 Million. However, unlike image classification (Simonyan & Zisserman, 2014b), CNNs did not yield large improvement over these traditional methods (Lan et al., 2014) highlighting the difficulty of learning video representation even with large training dataset. Simonyan & Zisserman (2014a) introduced a two-stream framework where they train CNNs independently on RGB and optical flow inputs. While the flow stream focuses only on motion information, the RGB can leverage 2D CNN pre-trained on image datasets. Based on the Two Stream representation, Wang et al. (2015a) extracted deep feature and conducted trajectory constrained pooling to aggregate convolutional feature as video representations.", "startOffset": 113, "endOffset": 920}, {"referenceID": 18, "context": "Ng et al. (2015); Donahue et al.", "startOffset": 0, "endOffset": 17}, {"referenceID": 11, "context": "(2015); Donahue et al. (2014) applied an RNN on top of the the two-stream framework, while Srivastava et al.", "startOffset": 8, "endOffset": 30}, {"referenceID": 11, "context": "(2015); Donahue et al. (2014) applied an RNN on top of the the two-stream framework, while Srivastava et al. (2015) proposed, in addition, to investigate the benefit of learning a video representation in an unsupervised manner.", "startOffset": 8, "endOffset": 116}, {"referenceID": 11, "context": "(2015); Donahue et al. (2014) applied an RNN on top of the the two-stream framework, while Srivastava et al. (2015) proposed, in addition, to investigate the benefit of learning a video representation in an unsupervised manner. Previous works on this topic tend to focus only on high-level CNN \u201cvisual percepts\u201d. By contrast, our approach proposes to leverage visual \u201cpercepts\u201d extracted from different layers in the 2D-CNN. Recently, Shi et al. (2015) also proposed to leverage convolutional units inside an RNN network.", "startOffset": 8, "endOffset": 453}, {"referenceID": 25, "context": "We evaluate our approach on the UCF101 dataset Soomro et al. (2012). This dataset has 101 action classes spanning over 13320 YouTube videos clips.", "startOffset": 47, "endOffset": 68}, {"referenceID": 14, "context": "To perform proper hyperparameter seach, we use the videos from the UCF-Thumos validation split Jiang et al. (2014) as the validation set.", "startOffset": 95, "endOffset": 115}, {"referenceID": 33, "context": "One could further improve the classification performance by considering other inputs such as the video optical flow Simonyan & Zisserman (2014a); Wang et al. (2015b). However, the main purpose of this experiment is to demonstrate the benefit of visual clues from multiple CNN layers when modeling the temporal information in a videos.", "startOffset": 146, "endOffset": 166}, {"referenceID": 33, "context": "One could further improve the classification performance by considering other inputs such as the video optical flow Simonyan & Zisserman (2014a); Wang et al. (2015b). However, the main purpose of this experiment is to demonstrate the benefit of visual clues from multiple CNN layers when modeling the temporal information in a videos. We design and evaluate three RCN architectures for action recognition. We use a VGG-16 CNN pretrained on ImageNet (Simonyan & Zisserman, 2014b) that is fine-tuned on the UCF-101 dataset, following the protocol in Wang et al. (2015b). We then extract the convolution maps from pool2, pool3, pool4, pool5 layers and the fully-connected map from layer fc-7 (which can be view as a feature map with a 1 \u00d7 1 spatial dimension).", "startOffset": 146, "endOffset": 568}, {"referenceID": 11, "context": "8 RGB-Stream + LSTM Donahue et al. (2014) 71.", "startOffset": 20, "endOffset": 42}, {"referenceID": 11, "context": "8 RGB-Stream + LSTM Donahue et al. (2014) 71.1 RGB-Stream + LSTM + Unsupervised Srivastava et al. (2015) 77.", "startOffset": 20, "endOffset": 105}, {"referenceID": 11, "context": "8 RGB-Stream + LSTM Donahue et al. (2014) 71.1 RGB-Stream + LSTM + Unsupervised Srivastava et al. (2015) 77.7 Improved RGB Stream Wang et al. (2015b) 79.", "startOffset": 20, "endOffset": 150}, {"referenceID": 11, "context": "8 RGB-Stream + LSTM Donahue et al. (2014) 71.1 RGB-Stream + LSTM + Unsupervised Srivastava et al. (2015) 77.7 Improved RGB Stream Wang et al. (2015b) 79.8 C3D one network Tran et al. (2014), 1 million videos as training 82.", "startOffset": 20, "endOffset": 190}, {"referenceID": 11, "context": "8 RGB-Stream + LSTM Donahue et al. (2014) 71.1 RGB-Stream + LSTM + Unsupervised Srivastava et al. (2015) 77.7 Improved RGB Stream Wang et al. (2015b) 79.8 C3D one network Tran et al. (2014), 1 million videos as training 82.3 C3D ensemble Tran et al. (2014), 1 million videos as training 85.", "startOffset": 20, "endOffset": 257}, {"referenceID": 11, "context": "8 RGB-Stream + LSTM Donahue et al. (2014) 71.1 RGB-Stream + LSTM + Unsupervised Srivastava et al. (2015) 77.7 Improved RGB Stream Wang et al. (2015b) 79.8 C3D one network Tran et al. (2014), 1 million videos as training 82.3 C3D ensemble Tran et al. (2014), 1 million videos as training 85.2 Deep networks Karpathy et al. (2014), 1 million videos as training 65.", "startOffset": 20, "endOffset": 329}, {"referenceID": 33, "context": "VGG-16 is the 2D spatial stream that is described in Wang et al. (2015b). We take the VGG-16 model, pretrained on Image-Net and fine-tune it on the UCF-101 dataset.", "startOffset": 53, "endOffset": 73}, {"referenceID": 30, "context": "C3D Tran et al. (2014) obtains the best performance on UCF-101 with 85.", "startOffset": 4, "endOffset": 23}, {"referenceID": 36, "context": "Following Yao et al. (2015b), we split the dataset into a training set of 1,200 video clips, a validation set of 100 clips and a test set consisting of the remaining clips.", "startOffset": 10, "endOffset": 29}, {"referenceID": 8, "context": "To perform video captioning, we use the so-called encoder-decoder framework Cho et al. (2014). Encoder maps input videos into abstract representations that precondition caption-generating decoder.", "startOffset": 76, "endOffset": 94}, {"referenceID": 8, "context": "To perform video captioning, we use the so-called encoder-decoder framework Cho et al. (2014). Encoder maps input videos into abstract representations that precondition caption-generating decoder. As for encoder, we compare both VGG-16 CNN and Bi-directional GRU-RCN. Both models have been fine-tuned on the UCF-101 dataset and therefore focus on detecting actions. To extract an abstract representation from a video, we sample K equally-space segments. When using the VGG16 encoder, we provide the fc7 layer activations of the each segment\u2019s first frame as the input to the text-decoder. For the GRU-RCN, we apply our model on the segment\u2019s 10 first frames. We concatenate the GRU-RCN hidden-representation from the last time step. The concatenated vector is given as the input to the text decoder. As it has been shown that characterizing entities in addition of action is important for the caption-generation task Yao et al. (2015a), we also use as encoder a CNN Szegedy et al.", "startOffset": 76, "endOffset": 936}, {"referenceID": 8, "context": "To perform video captioning, we use the so-called encoder-decoder framework Cho et al. (2014). Encoder maps input videos into abstract representations that precondition caption-generating decoder. As for encoder, we compare both VGG-16 CNN and Bi-directional GRU-RCN. Both models have been fine-tuned on the UCF-101 dataset and therefore focus on detecting actions. To extract an abstract representation from a video, we sample K equally-space segments. When using the VGG16 encoder, we provide the fc7 layer activations of the each segment\u2019s first frame as the input to the text-decoder. For the GRU-RCN, we apply our model on the segment\u2019s 10 first frames. We concatenate the GRU-RCN hidden-representation from the last time step. The concatenated vector is given as the input to the text decoder. As it has been shown that characterizing entities in addition of action is important for the caption-generation task Yao et al. (2015a), we also use as encoder a CNN Szegedy et al. (2014), pretrained on ImageNet, that focuses on detecting static visual object categories.", "startOffset": 76, "endOffset": 988}, {"referenceID": 8, "context": "To perform video captioning, we use the so-called encoder-decoder framework Cho et al. (2014). Encoder maps input videos into abstract representations that precondition caption-generating decoder. As for encoder, we compare both VGG-16 CNN and Bi-directional GRU-RCN. Both models have been fine-tuned on the UCF-101 dataset and therefore focus on detecting actions. To extract an abstract representation from a video, we sample K equally-space segments. When using the VGG16 encoder, we provide the fc7 layer activations of the each segment\u2019s first frame as the input to the text-decoder. For the GRU-RCN, we apply our model on the segment\u2019s 10 first frames. We concatenate the GRU-RCN hidden-representation from the last time step. The concatenated vector is given as the input to the text decoder. As it has been shown that characterizing entities in addition of action is important for the caption-generation task Yao et al. (2015a), we also use as encoder a CNN Szegedy et al. (2014), pretrained on ImageNet, that focuses on detecting static visual object categories. As for the decoder, we use LSTM text-generator with soft-attention on the video temporal frames Yao et al. (2015b).", "startOffset": 76, "endOffset": 1187}, {"referenceID": 20, "context": "6801 GoogleNet + HRNE (Pan et al., 2015) 0.", "startOffset": 22, "endOffset": 40}, {"referenceID": 38, "context": "321 VGG + p-RNN (Yu et al., 2015) 0.", "startOffset": 16, "endOffset": 33}, {"referenceID": 38, "context": "311 VGG + C3D + p-RNN (Yu et al., 2015) 0.", "startOffset": 22, "endOffset": 39}, {"referenceID": 20, "context": "6801 GoogleNet + HRNE (Pan et al., 2015) 0.436 0.321 VGG + p-RNN (Yu et al., 2015) 0.443 0.311 VGG + C3D + p-RNN (Yu et al., 2015) 0.499 0.326 Soft-attention Yao et al. (2015b) 0.", "startOffset": 23, "endOffset": 177}, {"referenceID": 20, "context": "6801 GoogleNet + HRNE (Pan et al., 2015) 0.436 0.321 VGG + p-RNN (Yu et al., 2015) 0.443 0.311 VGG + C3D + p-RNN (Yu et al., 2015) 0.499 0.326 Soft-attention Yao et al. (2015b) 0.4192 0.2960 0.5167 Venugopalan et al. Venugopalan et al. (2015) 0.", "startOffset": 23, "endOffset": 243}, {"referenceID": 20, "context": "6801 GoogleNet + HRNE (Pan et al., 2015) 0.436 0.321 VGG + p-RNN (Yu et al., 2015) 0.443 0.311 VGG + C3D + p-RNN (Yu et al., 2015) 0.499 0.326 Soft-attention Yao et al. (2015b) 0.4192 0.2960 0.5167 Venugopalan et al. Venugopalan et al. (2015) 0.3119 0.2687 + Extra Data (Flickr30k, COCO) 0.3329 0.2907 Thomason et al. Thomason et al. (2014) 0.", "startOffset": 23, "endOffset": 341}, {"referenceID": 36, "context": "Representations obtained with the proposed RCN architecture combined with decoders from Yao et al. (2015b) offer a significant performance boost, reaching the performance of the other state-ofthe-art models.", "startOffset": 88, "endOffset": 107}, {"referenceID": 38, "context": "According to the BLEU metric, we also outperform other approaches that uses more complex decoder scheme such as spatial and temporal attention decoder (Yu et al., 2015) or a Hierarchical RNN decoder (Pan et al.", "startOffset": 151, "endOffset": 168}, {"referenceID": 20, "context": ", 2015) or a Hierarchical RNN decoder (Pan et al., 2015) Our approach is on par with Yu et al.", "startOffset": 38, "endOffset": 56}, {"referenceID": 18, "context": "These are BLEU in Papineni et al. (2002), METEOR in Denkowski & Lavie (2014) and CIDEr in Vedantam et al.", "startOffset": 18, "endOffset": 41}, {"referenceID": 18, "context": "These are BLEU in Papineni et al. (2002), METEOR in Denkowski & Lavie (2014) and CIDEr in Vedantam et al.", "startOffset": 18, "endOffset": 77}, {"referenceID": 18, "context": "These are BLEU in Papineni et al. (2002), METEOR in Denkowski & Lavie (2014) and CIDEr in Vedantam et al. (2014). We use the evaluation script prepared and introduced in Chen et al.", "startOffset": 18, "endOffset": 113}, {"referenceID": 6, "context": "We use the evaluation script prepared and introduced in Chen et al. (2015). All models are early-stopped based on the negative-log-likelihood (NLL) of the validation set.", "startOffset": 56, "endOffset": 75}, {"referenceID": 6, "context": "We use the evaluation script prepared and introduced in Chen et al. (2015). All models are early-stopped based on the negative-log-likelihood (NLL) of the validation set. We then select the model that performs best on the validation set according to the metric at consideration. The first two lines of Table 2 compare the performances of the VGG-16 and Bi-direcitonal GRURCN encoder. Results clearly show the superiority of the Bi-Directional GRU-RCN Encoder as it outperforms the VGG-16 Encoder on all three metrics. In particular, GRU-RCN Encoder obtains a performance gain of 10% compared to the VGG-16 Encoder according to the BLEU metric. Combining our GRU-RCN Encoder that focuses on action with a GoogleNet Encoder that captures visual entities further improve the performances. Our GoogleNet + Bi-directional GRU-RCN approach signficantly outperforms Soft-attention Yao et al. (2015b) that relies on a GoogLeNet and cuboids-based 3D-CNN Encoder, in conjunction to a similar soft-attention decoder.", "startOffset": 56, "endOffset": 893}, {"referenceID": 6, "context": "We use the evaluation script prepared and introduced in Chen et al. (2015). All models are early-stopped based on the negative-log-likelihood (NLL) of the validation set. We then select the model that performs best on the validation set according to the metric at consideration. The first two lines of Table 2 compare the performances of the VGG-16 and Bi-direcitonal GRURCN encoder. Results clearly show the superiority of the Bi-Directional GRU-RCN Encoder as it outperforms the VGG-16 Encoder on all three metrics. In particular, GRU-RCN Encoder obtains a performance gain of 10% compared to the VGG-16 Encoder according to the BLEU metric. Combining our GRU-RCN Encoder that focuses on action with a GoogleNet Encoder that captures visual entities further improve the performances. Our GoogleNet + Bi-directional GRU-RCN approach signficantly outperforms Soft-attention Yao et al. (2015b) that relies on a GoogLeNet and cuboids-based 3D-CNN Encoder, in conjunction to a similar soft-attention decoder. This result indicates that our approach is able to offer more effective representions. According to the BLEU metric, we also outperform other approaches that uses more complex decoder scheme such as spatial and temporal attention decoder (Yu et al., 2015) or a Hierarchical RNN decoder (Pan et al., 2015) Our approach is on par with Yu et al. (2015), without the need of using a C3D-encoder that requires to be trained on large-scale video dataset.", "startOffset": 56, "endOffset": 1356}], "year": 2017, "abstractText": "We propose an approach to learn spatio-temporal features in videos from intermediate visual representations we call \u201cpercepts\u201d using Gated-Recurrent-Unit Recurrent Networks (GRUs). Our method relies on percepts that are extracted from all levels of a deep convolutional network trained on the large ImageNet dataset. While high-level percepts contain highly discriminative information, they tend to have a low-spatial resolution. Low-level percepts, on the other hand, preserve a higher spatial resolution from which we can model finer motion patterns. Using low-level percepts, however, can lead to high-dimensionality video representations. To mitigate this effect and control the number of parameters, we introduce a variant of the GRU model that leverages the convolution operations to enforce sparse connectivity of the model units and share parameters across the input spatial locations. We empirically validate our approach on both Human Action Recognition and Video Captioning tasks. In particular, we achieve results equivalent to state-of-art on the YouTube2Text dataset using a simpler caption-decoder model and without extra 3D CNN features.", "creator": "LaTeX with hyperref package"}}}