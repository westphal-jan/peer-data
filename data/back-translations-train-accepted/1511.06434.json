{"id": "1511.06434", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks", "abstract": "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.", "histories": [["v1", "Thu, 19 Nov 2015 22:50:32 GMT  (9027kb,D)", "http://arxiv.org/abs/1511.06434v1", "Under review as a conference paper at ICLR 2016"], ["v2", "Thu, 7 Jan 2016 23:09:39 GMT  (9028kb,D)", "http://arxiv.org/abs/1511.06434v2", "Under review as a conference paper at ICLR 2016"]], "COMMENTS": "Under review as a conference paper at ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["alec radford", "luke metz", "soumith chintala"], "accepted": true, "id": "1511.06434"}, "pdf": {"name": "1511.06434.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["alec@indico.io", "luke@indico.io", "soumith@fb.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In the context of computer vision, the virtually unlimited amount of unlabeled images and videos can be used to learn good intermediate representations that can then be used on a variety of monitored learning tasks such as image classification. We suggest that one way to build good image representations is to train Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) and later reuse parts of the generator and discriminator network as feature extractors for monitored tasks. GANs offer an attractive alternative to maximum probability techniques. It can also be argued that their learning process and lack of a heuristic cost function (such as pixel-by-pixel independent Mean Square errors) are attractive for imaging learning. GANs are known to be unstable to train generators that produce nonsensical results."}, {"heading": "2 RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 REPRESENTATION LEARNING FROM UNLABELED DATA", "text": "A classic approach to unattended imaging learning is to cluster the data (for example, using K-means) and use the clusters for improved classification values. In the context of images, one can perform hierarchical clustering of image fields (Coates & Ng, 2012) to learn powerful imaging; another popular method is the formation of auto-encoders (stacked in a revolutionary manner (Vincent et al., 2010) that separate the what and where of code components (Zhao et al., 2015); conductor structures (Rasmus et al., 2015) that encode an image into a compact code and decrypt the code to reconstruct the image as accurately as possible; these methods have also proven to be well suited to learning good imaging properties of image pixels (Lee et al., 2009)."}, {"heading": "2.2 GENERATING NATURAL IMAGES", "text": "Generative image models are well studied and can be divided into two categories: parametric and non-parametric. Non-parametric models often fit together from a database of existing images, often with cutouts, and have been shown to be used in texture synthesis (Efros et al., 1999), high resolution (Freeman et al., 2002), and painting (Hays & Efros, 2007). Parametric models for creating images have been extensively researched (for example, on MNIST digits or for texture synthesis (Portilla & Simoncelli, 2000). Another approach generates natural images of the real world that have not had much success until recently. A varying sampling approach to image generation (Kingma & Welling, 2013) has also had some success, but the samples often suffer from blur. Another approach generates images with an iterative forward diffusion process (Sohl-Dickstein al, 2015 et al)."}, {"heading": "2.3 VISUALIZING THE INTERNALS OF CNNS", "text": "A constant criticism of the use of neural networks has been that they are black box methods that have little understanding of what networks do in the form of a simple algorithm that can be consumed by humans. In the context of CNNs, Zeiler et al. (Zeiler & Fergus, 2014) showed that by using deconvolutions and filters of maximum activations, one can find the approximate purpose of each folding filter in the network. Similarly, the use of a gradient lowering at the inputs lets us examine the ideal image that activates certain subsets of filters (Mordvintsev et al.)."}, {"heading": "3 APPROACH AND MODEL ARCHITECTURE", "text": "In fact, most of them are able to survive on their own."}, {"heading": "4 DETAILS OF ADVERSARIAL TRAINING", "text": "We trained DCGANs on three datasets, Large Scene Understanding (LSUN) (Yu et al., 2015), Imagenet-1k, and a newly compiled Faces dataset. Details of the use of each of these datasets are given below. In addition to scaling to the area of the Tanh activation function [-1, 1], no pre-processing was applied to training images. All models were trained with stochastic gradient descent in minibatch (SGD) with a minibatch size of 128. All weights were initialized from a zero-centered normal distribution with standard deviation 0.02. In LeakyReLU, the slope of the leak was set to 0.2 in all models. While previous GAN work used dynamics to accelerate training, we used the Adam Optimizer (Kingma & Ba, 2014) with coordinated hyperparameters."}, {"heading": "4.1 LSUN", "text": "To show how our model scales work with more data and higher resolution, we train a model on the LSUN bedroom dataset with just over 3 million training examples. Recent analyses have shown that there is a direct correlation between the rapid learning of models and their generalization performance (Hardt et al., 2015). We show examples from a training period (Fig.2) that mimic online learning, in addition to samples by convergence (Fig.3), to show that our model does not produce high-quality samples simply by refurbishing / memorizing training examples. No data augmentation was applied to the images."}, {"heading": "4.1.1 DEDUPLICATION", "text": "To further reduce the likelihood of the generator memorizing input examples (Fig.2), we perform a simple image deduplication process. We adapt a RELU autoencoder 3072-128-3072 to 32 x 32 sampled training examples, and the resulting activations of the code layer are then binarized by swelling of the ReLU activation, which has proven to be an effective information-conserving technique (Srivastava et al., 2014) and provides a convenient form of semantic hashing, allowing linear time deduplication. Visual inspection of hash collisions showed high accuracy with an estimated false positive rate of less than 1 in 100. In addition, the technique detected and removed approximately 275,000 duplicates nearby, indicating a high recall rate."}, {"heading": "4.2 FACES", "text": "We scratched images of human faces from random web image queries of people. the personal names were taken from dbpedia, with a criterion that they were born in modern times. This dataset contains 3M images of 10K people. We operate an OpenCV face detector on these images, which maintains sufficiently high resolution, giving us approximately 350,000 visual fields. We use these visual fields for training. No data magnification was applied to the images."}, {"heading": "4.3 IMAGENET-1K", "text": "We use Imagenet-1k (Deng et al., 2009) as a source of natural images for unattended training. We train on 32 x 32 minute medium-sized cultures. No data magnification was applied to the images."}, {"heading": "5 EMPIRICAL VALIDATION OF DCGANS CAPABILITIES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 CLASSIFYING CIFAR-10 USING GANS AS A FEATURE EXTRACTOR", "text": "A common way to evaluate the quality of unattended display learning algorithms is to apply them as a feature extractor to monitored data sets and to evaluate the performance of linear models based on these characteristics. On the CIFAR-10 dataset, a very strong base performance from a well-tuned single-layer feature extraction pipeline using K-means as a feature learning algorithm was demonstrated. Using a very large amount of feature maps (4800), this technique achieves an accuracy of 80.6%. An unattended multi-layer extension of the base algorithm achieves an accuracy of 82.0% (Coates & Ng, 2011). To evaluate the quality of representations learned by DCGANs for monitored tasks, we train on Imagenet-1k and then use the discriminator's revolutionary properties from all layers, maxpooling each layer representation to generate a 4 x spatial grid."}, {"heading": "6 INVESTIGATING AND VISUALIZING THE INTERNALS OF THE NETWORKS", "text": "We examine the trained generators and discriminators in many ways. We do not perform any kind of neighborhood search on the training set. Nearby neighbors in pixel or feature pace are trivially deceived by small image transformations (Theis et al., 2015). Nor do we use log probability metrics to quantitatively evaluate the model as it is a poor metric (Theis et al., 2015)."}, {"heading": "6.1 WALKING IN THE LATENT SPACE", "text": "The first experiment we conducted was to understand the landscape of latent space. Walking on the learned multiplicity can usually provide us with information about signs of memory (when there are sharp transitions) and about the way space collapses hierarchically. If walking in this latent space leads to semantic changes in image generations (such as the addition and removal of objects), we can assume that the model has learned relevant and interesting representations, the results of which are shown in Fig.4."}, {"heading": "6.2 VISUALIZING THE DISCRIMINATOR FEATURES", "text": "Previous work has shown that supervised training of CNNs on large image datasets leads to very powerful learned characteristics (Zeiler & Fergus, 2014); in addition, supervised CNNs on site learn object detectors (Oquab et al., 2014); we show that an unsupervised DCGAN trained on a large image dataset can also learn a hierarchy of interesting characteristics; by means of guided reverse propagation, as proposed by Springenberg et al., 2014 (Springenberg et al., 2014), we show in Fig.5 that the characteristics learned by the discriminator are activated in typical parts of a bedroom such as beds and windows; for comparison, we give a baseline for randomly initialized characteristics that are not activated on anything semantically relevant or interesting."}, {"heading": "6.3 MANIPULATING THE GENERATOR REPRESENTATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.3.1 FORGETTING TO DRAW CERTAIN OBJECTS", "text": "The quality of the samples suggests that the generator learns specific object representations for important scene components such as beds, windows, lamps, doors and other furniture. To explore the shape of these representations, we conducted an experiment to try to remove windows completely from the generator. On 150 samples, 52 window boundary boxes were drawn manually. At the second highest folding layer, logistical regression was applied to predict whether or not an activation of a feature was in a window by applying the criterion that the activations within the drawn boundary boxes are positive and random samples from the same images are negative. On the basis of this simple model, all feature cards with weights greater than zero (a total of 200) were removed from all spatial positions. Subsequently, random new samples with and without the characteristic sketch were removed."}, {"heading": "6.3.2 VECTOR ARITHMETIC ON FACE SAMPLES", "text": "A canonical example showed that the vector (\"king\") - vector (\"man\") + vector (\"woman\") led to a vector whose closest neighbor was the vector for Queen. We investigated whether a similar structure appeared in the Z-representation of our generators. We performed similar arithmetic on the Z-vectors of sample visual concepts. Experiments that worked only on individual samples per concept were unstable, but the averaging of the Z-vector for three examples showed consistent and stable generations that obeyed arithmetic semantically. In addition to the object manipulation shown in (Fig. 7), we show that the visual posture is also modelled linearly in the Z-space (Fig. 8). These demonstrations suggest interesting applications that can be developed on the basis of our models."}, {"heading": "7 CONCLUSION AND FUTURE WORK", "text": "We propose a more stable set of architectures for training generative adversarial networks, and suggest that adversarial networks learn good imaging for supervised learning and generative modeling. There are still some forms of model instability - we noticed that when models are trained for longer, sometimes a subset of filters breaks down into a single oscillating mode. To remove this from instability, more work is needed. We think that extending this framework to other areas such as video (for predicting images) and audio (pre-trained speech synthesis functions) should be very interesting."}, {"heading": "ACKNOWLEDGMENTS", "text": "We are happy and grateful for all the advice and guidance we have received during this work, especially from Ian Goodfellow, Tobias Springenberg, Arthur Szlam and Durk Kingma. We would also like to thank all the people of indico for their support, resources and conversations, especially the other two members of the indico research team, Dan Kuster and Nathan Lintz. Finally, we would like to thank Nvidia for donating a Titan-X GPU used in this work."}, {"heading": "8 SUPPLEMENTARY MATERIAL", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.1 EVALUATING DCGANS CAPABILITY TO CAPTURE DATA DISTRIBUTIONS", "text": "We have trained a DCGAN on MNIST (fork of a 10K validation set) and a permutation invariant GAN baseline, and evaluated the models using a neighbor classifier introduced by batch norm, which compares real data with a set of generated conditional samples. We found that removing scale and bias parameters from batch norm yields better results for both models. We speculate that the noise introduced by batch norm helps the generative models better explore and generate the underlying data distribution; the results are shown in Table 2, which compares our models with other techniques; and the DCGAN model yields the same test error as a neighboring neighbor classifier that matches the training dataset - suggesting that the DCGAN model has done an excellent job in modeling the conditional distributions of that dataset."}], "references": [{"title": "Selecting receptive fields in deep networks", "author": ["Coates", "Adam", "Ng", "Andrew"], "venue": null, "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "Learning feature representations with k-means", "author": ["Coates", "Adam", "Ng", "Andrew Y"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "Coates et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2012}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "Li", "Li-Jia", "Kai", "Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["Denton", "Emily", "Chintala", "Soumith", "Szlam", "Arthur", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1506.05751,", "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Learning to generate chairs with convolutional neural networks", "author": ["Dosovitskiy", "Alexey", "Springenberg", "Jost Tobias", "Brox", "Thomas"], "venue": "arXiv preprint arXiv:1411.5928,", "citeRegEx": "Dosovitskiy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dosovitskiy et al\\.", "year": 2014}, {"title": "Discriminative unsupervised feature learning with exemplar convolutional neural networks", "author": ["Dosovitskiy", "Alexey", "Fischer", "Philipp", "Springenberg", "Jost Tobias", "Riedmiller", "Martin", "Brox", "Thomas"], "venue": "In Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Dosovitskiy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dosovitskiy et al\\.", "year": 2015}, {"title": "Texture synthesis by non-parametric sampling", "author": ["Efros", "Alexei", "Leung", "Thomas K"], "venue": "In Computer Vision,", "citeRegEx": "Efros et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Efros et al\\.", "year": 1999}, {"title": "Example-based super-resolution", "author": ["Freeman", "William T", "Jones", "Thouis R", "Pasztor", "Egon C"], "venue": "Computer Graphics and Applications, IEEE,", "citeRegEx": "Freeman et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Freeman et al\\.", "year": 2002}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian J", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron C", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Train faster, generalize better: Stability of stochastic gradient descent", "author": ["Hardt", "Moritz", "Recht", "Benjamin", "Singer", "Yoram"], "venue": "arXiv preprint arXiv:1509.01240,", "citeRegEx": "Hardt et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hardt et al\\.", "year": 2015}, {"title": "Dreaming more data: Class-dependent distributions over diffeomorphisms for learned data augmentation", "author": ["Hauberg", "Sren", "Freifeld", "Oren", "Larsen", "Anders Boesen Lindbo", "Fisher III", "John W", "Hansen", "Lars Kair"], "venue": "arXiv preprint arXiv:1510.02795,", "citeRegEx": "Hauberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hauberg et al\\.", "year": 2015}, {"title": "Scene completion using millions of photographs", "author": ["Hays", "James", "Efros", "Alexei A"], "venue": "ACM Transactions on Graphics (TOG),", "citeRegEx": "Hays et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hays et al\\.", "year": 2007}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik P", "Ba", "Jimmy Lei"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["Lee", "Honglak", "Grosse", "Roger", "Ranganath", "Rajesh", "Ng", "Andrew Y"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Lee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "Training invariant support vector machines using selective sampling", "author": ["Loosli", "Ga\u00eblle", "Canu", "St\u00e9phane", "Bottou", "L\u00e9on"], "venue": "Large Scale Kernel Machines,", "citeRegEx": "Loosli et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Loosli et al\\.", "year": 2007}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["Maas", "Andrew L", "Hannun", "Awni Y", "Ng", "Andrew Y"], "venue": "In Proc. ICML,", "citeRegEx": "Maas et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Inceptionism : Going deeper into neural networks. http://googleresearch.blogspot.com/2015/06/ inceptionism-going-deeper-into-neural.html", "author": ["Mordvintsev", "Alexander", "Olah", "Christopher", "Tyka", "Mike"], "venue": null, "citeRegEx": "Mordvintsev et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mordvintsev et al\\.", "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Vinod", "Hinton", "Geoffrey E"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Learning and transferring mid-level image representations using convolutional neural networks", "author": ["M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic"], "venue": "In CVPR,", "citeRegEx": "Oquab et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Oquab et al\\.", "year": 2014}, {"title": "A parametric texture model based on joint statistics of complex wavelet coefficients", "author": ["Portilla", "Javier", "Simoncelli", "Eero P"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Portilla et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Portilla et al\\.", "year": 2000}, {"title": "Semisupervised learning with ladder network", "author": ["Rasmus", "Antti", "Valpola", "Harri", "Honkala", "Mikko", "Berglund", "Mathias", "Raiko", "Tapani"], "venue": "arXiv preprint arXiv:1507.02672,", "citeRegEx": "Rasmus et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rasmus et al\\.", "year": 2015}, {"title": "Deep unsupervised learning using nonequilibrium thermodynamics", "author": ["Sohl-Dickstein", "Jascha", "Weiss", "Eric A", "Maheswaranathan", "Niru", "Ganguli", "Surya"], "venue": "arXiv preprint arXiv:1503.03585,", "citeRegEx": "Sohl.Dickstein et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sohl.Dickstein et al\\.", "year": 2015}, {"title": "Striving for simplicity: The all convolutional net", "author": ["Springenberg", "Jost Tobias", "Dosovitskiy", "Alexey", "Brox", "Thomas", "Riedmiller", "Martin"], "venue": "arXiv preprint arXiv:1412.6806,", "citeRegEx": "Springenberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2014}, {"title": "Understanding locally competitive networks", "author": ["Srivastava", "Rupesh Kumar", "Masci", "Jonathan", "Gomez", "Faustino", "Schmidhuber", "J\u00fcrgen"], "venue": "arXiv preprint arXiv:1410.1165,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "A note on the evaluation of generative models", "author": ["L. Theis", "A. van den Oord", "M. Bethge"], "venue": null, "citeRegEx": "Theis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2015}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Lajoie", "Isabelle", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Empirical evaluation of rectified activations in convolutional network", "author": ["Xu", "Bing", "Wang", "Naiyan", "Chen", "Tianqi", "Li", "Mu"], "venue": "arXiv preprint arXiv:1505.00853,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Construction of a large-scale image dataset using deep learning with humans in the loop", "author": ["Yu", "Fisher", "Zhang", "Yinda", "Song", "Shuran", "Seff", "Ari", "Xiao", "Jianxiong"], "venue": "arXiv preprint arXiv:1506.03365,", "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Visualizing and understanding convolutional networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "Zeiler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2014}, {"title": "Stacked what-where autoencoders", "author": ["Zhao", "Junbo", "Mathieu", "Michael", "Goroshin", "Ross", "Lecun", "Yann"], "venue": "arXiv preprint arXiv:1506.02351,", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "We propose that one way to build good image representations is by training Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), and later reusing parts of the generator and discriminator networks as feature extractors for supervised tasks.", "startOffset": 114, "endOffset": 139}, {"referenceID": 29, "context": "Another popular method is to train auto-encoders (convolutionally, stacked (Vincent et al., 2010), separating the what and where components of the code (Zhao et al.", "startOffset": 75, "endOffset": 97}, {"referenceID": 33, "context": ", 2010), separating the what and where components of the code (Zhao et al., 2015), ladder structures (Rasmus et al.", "startOffset": 62, "endOffset": 81}, {"referenceID": 24, "context": ", 2015), ladder structures (Rasmus et al., 2015)) that encode an image into a compact code, and decode the code to reconstruct the image as accurately as possible.", "startOffset": 27, "endOffset": 48}, {"referenceID": 16, "context": "Deep belief networks (Lee et al., 2009) have also been shown to work well in learning hierarchical representations.", "startOffset": 21, "endOffset": 39}, {"referenceID": 6, "context": "The non-parametric models often do matching from a database of existing images, often matching patches of images, and have been shown to be used in texture synthesis (Efros et al., 1999), superresolution (Freeman et al.", "startOffset": 166, "endOffset": 186}, {"referenceID": 7, "context": ", 1999), superresolution (Freeman et al., 2002) and in-painting (Hays & Efros, 2007).", "startOffset": 25, "endOffset": 47}, {"referenceID": 25, "context": "Another approach generates images using an iterative forward diffusion process (Sohl-Dickstein et al., 2015).", "startOffset": 79, "endOffset": 108}, {"referenceID": 8, "context": "Generative Adversarial Networks (Goodfellow et al., 2014) generated images suffering from being noisy and incomprehensible.", "startOffset": 32, "endOffset": 57}, {"referenceID": 3, "context": "A laplacian pyramid extension to this approach (Denton et al., 2015) showed higher quality images, but they still suffered from the objects looking wobbly because of noise introduced in chaining multiple models.", "startOffset": 47, "endOffset": 68}, {"referenceID": 9, "context": "A recurrent network approach (Gregor et al., 2015) and a deconvolution network approach (Dosovitskiy et al.", "startOffset": 29, "endOffset": 50}, {"referenceID": 4, "context": ", 2015) and a deconvolution network approach (Dosovitskiy et al., 2014) have also recently had some success with generating natural images.", "startOffset": 45, "endOffset": 71}, {"referenceID": 3, "context": "This motivated the authors of LAPGAN (Denton et al., 2015) to develop an alternative approach to iteratively upscale low resolution generated images which can be modeled more reliably.", "startOffset": 37, "endOffset": 58}, {"referenceID": 26, "context": "The first is the all convolutional net (Springenberg et al., 2014) which replaces deterministic spatial pooling functions (such as maxpooling) with strided convolutions, allowing the network to learn its own spatial downsampling.", "startOffset": 39, "endOffset": 66}, {"referenceID": 18, "context": "Within the discriminator we found the leaky rectified activation (Maas et al., 2013) (Xu et al.", "startOffset": 65, "endOffset": 84}, {"referenceID": 30, "context": ", 2013) (Xu et al., 2015) to work well, especially for higher resolution modeling.", "startOffset": 8, "endOffset": 25}, {"referenceID": 31, "context": "We trained DCGANs on three datasets, Large-scale Scene Understanding (LSUN) (Yu et al., 2015), Imagenet-1k and a newly assembled Faces dataset.", "startOffset": 76, "endOffset": 93}, {"referenceID": 10, "context": "Recent analysis has shown that there is a direct link between how fast models learn and their generalization performance (Hardt et al., 2015).", "startOffset": 121, "endOffset": 141}, {"referenceID": 27, "context": "The resulting code layer activations are then binarized via thresholding the ReLU activation which has been shown to be an effective information preserving technique (Srivastava et al., 2014) and provides a convenient form of semantic-hashing, allowing for linear time de-duplication .", "startOffset": 166, "endOffset": 191}, {"referenceID": 2, "context": "We use Imagenet-1k (Deng et al., 2009) as a source of natural images for unsupervised training.", "startOffset": 19, "endOffset": 38}, {"referenceID": 5, "context": "The performance of DCGANs is still less than that of Exemplar CNNs (Dosovitskiy et al., 2015), a technique which trains normal discriminative CNNs in an unsupervised fashion to differentiate between specifically chosen, aggressively augmented, exemplar samples from the source dataset.", "startOffset": 67, "endOffset": 93}, {"referenceID": 28, "context": "Nearest neighbors in pixel or feature space are trivially fooled (Theis et al., 2015) by small image transforms.", "startOffset": 65, "endOffset": 85}, {"referenceID": 28, "context": "We also do not use log-likelihood metrics to quantitatively assess the model, as it is a poor (Theis et al., 2015) metric.", "startOffset": 94, "endOffset": 114}, {"referenceID": 22, "context": "scene classification learn object detectors (Oquab et al., 2014).", "startOffset": 44, "endOffset": 64}, {"referenceID": 26, "context": "Using guided backpropagation as proposed by (Springenberg et al., 2014), we show in Fig.", "startOffset": 44, "endOffset": 71}, {"referenceID": 19, "context": "In the context of evaluating learned representations of words (Mikolov et al., 2013) demonstrated that simple arithmetic operations revealed rich linear structure in representation space.", "startOffset": 62, "endOffset": 84}, {"referenceID": 4, "context": "It has been previously demonstrated that conditional generative models can learn to convincingly model object attributes like scale, rotation, and position (Dosovitskiy et al., 2014).", "startOffset": 156, "endOffset": 182}], "year": 2015, "abstractText": "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks demonstrating their applicability as general image representations.", "creator": "LaTeX with hyperref package"}}}