{"id": "1511.06438", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Joint Word Representation Learning Using a Corpus and a Semantic Lexicon", "abstract": "Methods for learning word representations using large text corpora have received much attention lately due to their impressive performance in numerous natural language processing (NLP) tasks such as, semantic similarity measurement, and word analogy detection. Despite their success, these data-driven word representation learning methods do not consider the rich semantic relational structure between words in a co-occurring context. On the other hand, already much manual effort has gone into the construction of semantic lexicons such as the WordNet that represent the meanings of words by defining the various relationships that exist among the words in a language. We consider the question, can we improve the word representations learnt using a corpora by integrating the knowledge from semantic lexicons?. For this purpose, we propose a joint word representation learning method that simultaneously predicts the co-occurrences of two words in a sentence subject to the relational constrains given by the semantic lexicon. We use relations that exist between words in the lexicon to regularize the word representations learnt from the corpus. Our proposed method statistically significantly outperforms previously proposed methods for incorporating semantic lexicons into word representations on several benchmark datasets for semantic similarity and word analogy.", "histories": [["v1", "Thu, 19 Nov 2015 22:58:10 GMT  (321kb)", "http://arxiv.org/abs/1511.06438v1", "Accepted to AAAI-2016"]], "COMMENTS": "Accepted to AAAI-2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["danushka bollegala", "mohammed alsuhaibani", "takanori maehara", "ken-ichi kawarabayashi"], "accepted": true, "id": "1511.06438"}, "pdf": {"name": "1511.06438.pdf", "metadata": {"source": "CRF", "title": "Joint Word Representation Learning using a Corpus and a Semantic Lexicon", "authors": ["Danushka Bollegala", "Takanori Maehara", "Ken-ichi Kawarabayashi"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 151 1,06 438v 1 [cs.C L] 19 Nov 2"}, {"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Related Work", "text": "It is a question of whether it concerns a way in which people live in the world, in the world in which they live, in the world in which they live, in the world in which they live, in the world in which they live, in the world in which they live, in the world in which they live, in the world in which they live, in the world in which they live, in the world in which they live, in which they live, in the world in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in the world, in the world, in the world, in which they live, in which they live, in"}, {"heading": "3 Learning Word Representations", "text": "Considering the semantic lexicon S described in this paper, we describe a method for learning word representations wi-Rd for words wi in the corpus. We use the word to name the word (vector) when it is the proposed method that needs to be defined by the user in advance, and the vocabulary that specifies the semantic relationships that exist between words. The dimensionality of the vector representation is a hyperparameter of the proposed method that needs to be defined by the user in advance. Any semantic lexicon that specifies the semantic relationships between words could be used as S., such as the WordNet (Miller 1995), FrameNet et al. 1998), or the Paraphrase Word (Ganitkevitch et al). In 2013, we do not proceed from any structural properties unique in a particular semantic lexicon."}, {"heading": "4 Experiments and Results", "text": "We have the idea that we have made our own in order to understand the world. \"The idea that we have made our own in order to understand the world is not new.\" The idea that we have made our own is not new. \"The idea that we have made our own is not new.\" The idea that we have made our own is not new. \"\" The idea that we have made our own is not new. \"\" The idea that we have made our own is not new. \"The idea that we have made our own is not new.\" The idea that we have made our own is not new. \"The idea that we have made our own.\" The idea that we have made our own, \"that we want, that we want, that we want, that we want.\" The idea that we want, that we want. \"The idea that we want.\" The idea that we have. \""}, {"heading": "5 Conclusion", "text": "To this end, we proposed a method of using the information available in a semantic lexicon to improve word representations learned from a corpus. Experiments with ukWaC as a corpus and WordNet as a semantic lexicon show that we can significantly improve word representations learned with the corpus alone by incorporating information from the semantic lexicon. Furthermore, the proposed method significantly exceeds the methods previously proposed for learning word representations with both a corpus and a semantic lexicon in both a semantic prediction task and a task for recognizing word analogies. In the future, the effectiveness of the semantic lexicon is outstanding when the corpus size is small. In addition, the performance of the proposed method is stable across a wide range of dimensions of word representations. In the future, we plan to apply the word representations learned through the proposed NP method to aggregated applications."}, {"heading": "633, 1965.", "text": "[2012] Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. Semantic composition through recursive matrix vector spaces."}, {"heading": "1201\u20131211, 2012.", "text": "[2010] Joseph Turian, Lev Ratinov, and Yoshua Bengio. Word representations: A simple and general method for semi-supervised learning. In ACL, pp. 384-394, 2010. [2010] Peter D. Turney and Patrick Pantel. From frequency to meaning: Vector space models of semantics. Journal of Aritificial Intelligence Research, 37: 141-188, 2010. [2014] Chang Xu, Yalong Bai, Jiang Bian, Bin Gao, Gang Wang, Xiaoguang Liu, and Tie-Yan Liu. Rc-net: A general framework for incorporating knowledge in word representations. In Proc. of CIKM, pp. 1219-1228, 2014. [2014] Mo Yu and Mark Dredze. Improving lexical embeddings with semantic knowledge. In Proc. of ACL, pp. 545-550, 2014."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "Methods for learning word representations using large text corpora have received much attention lately due to their impressive performance in numerous natural language processing (NLP) tasks such as, semantic similarity measurement, and word analogy detection. Despite their success, these datadriven word representation learning methods do not consider the rich semantic relational structure between words in a cooccurring context. On the other hand, already much manual effort has gone into the construction of semantic lexicons such as the WordNet that represent the meanings of words by defining the various relationships that exist among the words in a language. We consider the question, can we improve the word representations learnt using a corpora by integrating the knowledge from semantic lexicons?. For this purpose, we propose a joint word representation learning method that simultaneously predicts the co-occurrences of two words in a sentence subject to the relational constrains given by the semantic lexicon. We use relations that exist between words in the lexicon to regularize the word representations learnt from the corpus. Our proposed method statistically significantly outperforms previously proposed methods for incorporating semantic lexicons into word representations on several benchmark datasets for semantic similarity and word analogy.", "creator": "LaTeX with hyperref package"}}}