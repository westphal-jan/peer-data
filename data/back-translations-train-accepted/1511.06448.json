{"id": "1511.06448", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks", "abstract": "One of the challenges in modeling cognitive events from electroencephalogram (EEG) data is finding representations that are invariant to inter- and intra-subject differences, as well as to inherent noise associated with such data. Herein, we propose a novel approach for learning such representations from multi-channel EEG time-series, and demonstrate its advantages in the context of mental load classification task. First, we transform EEG activities into a sequence of topology-preserving multi-spectral images, as opposed to standard EEG analysis techniques that ignore such spatial information. Next, we train a deep recurrent-convolutional network inspired by state-of-the-art video classification to learn robust representations from the sequence of images. The proposed approach is designed to preserve the spatial, spectral, and temporal structure of EEG which leads to finding features that are less sensitive to variations and distortions within each dimension. Empirical evaluation on the cognitive load classification task demonstrated significant improvements in classification accuracy over current state-of-the-art approaches in this field.", "histories": [["v1", "Thu, 19 Nov 2015 23:29:55 GMT  (9942kb,D)", "http://arxiv.org/abs/1511.06448v1", null], ["v2", "Thu, 7 Jan 2016 22:04:23 GMT  (17736kb,D)", "http://arxiv.org/abs/1511.06448v2", null], ["v3", "Mon, 29 Feb 2016 21:33:45 GMT  (25079kb,D)", "http://arxiv.org/abs/1511.06448v3", "To be published as a conference paper at ICLR 2016"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["pouya bashivan", "irina rish", "mohammed yeasin", "noel codella"], "accepted": true, "id": "1511.06448"}, "pdf": {"name": "1511.06448.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Pouya Bashivan", "Irina Rish"], "emails": ["pbshivan@memphis.edu", "rish@us.ibm.com", "myeasin@memphis.edu", "nccodell@us.ibm.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, most of them are able to go to another world, to go to another world, to find themselves in another world, to find themselves in another world."}, {"heading": "2 OUR APPROACH", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 MAKING IMAGES FROM EEG TIME-SERIES", "text": "In fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight,"}, {"heading": "2.2 ARCHITECTURE", "text": "ConvNets were used to deal with variations in spatial and frequency ranges because they were able to learn a good two-dimensional representation of the data. Wherever necessary, the extracted representations were fed into a different layer to take into account temporal variations in the data. We evaluated different types of layers used to extract temporal patterns, including Convolutionary and Recursive layers. Essentially, we evaluated the following two primary approaches to the cognitive state problem: 1) Single image approach, where a single image was constructed from spectral measurements over the entire duration of the experiment. The constructed image was then used as input into the ConvNet. 2) Multi-image approach, in which we split each study into 0.5-second windows and constructed the images over each time window. The sequence of the images was then used as data input into the recursive-constant network."}, {"heading": "2.2.1 CONVNET ARCHITECTURE", "text": "This network has a highly scalable architecture using stacked folding layers with small receiver fields. All folding layers use small 3 x 3 receiver layers with 1 pixel increment with ReLU activation functionality. Input of the folding layer is increased by 1 pixel to obtain the spatial resolution after folding. Multiple folding layers are stacked together followed by a Maxipool layer. Max pooling occurs via a 2 x 2 window with step width of 2 pixels. The number of cores within each folding layer increases by a factor of 2 with layer in deeper stacks. Stacking multiple folding layers results in an effective receptive field of higher dimensions, requiring significantly fewer parameters (Simonyan & Zisserman, 2015)."}, {"heading": "2.2.2 SINGLE-FRAME APPROACH", "text": "To do this, we first examined a simplified version of the problem by calculating the average activity over the entire trial period. To this end, we calculated all performance characteristics over the entire trial period. Subsequently, the EEG recording was reduced to a single multi-channel image for each study. We evaluated ConvNet configurations of different depths, as shown in Table 1. The conversion parameters are referred to here as the conv < receptive field size > - < number of channels >. Essentially, configuration A comprises only two ConvNets (Conv3-32) stacked, followed by maxpool layer; configuration B adds two more ConvNets (Conv3-64) on architecture A, followed by configuration C-64vNet (ConvNet-3 > 12v4), followed by configuration-12v12)."}, {"heading": "2.2.3 MULTI-FRAME APPROACH", "text": "To reduce the number of parameters in the network, we evaluated three approaches to extract temporal information from the sequence of activity maps (Figure 2); 1) Max pooling over time \u2212 2) Temporal convolution; 3) LSTM. Finally, the outputs from the last layer are fed to a fully connected layer with 512 hidden units, followed by a four-member Softmax layer. We kept the number of neurons in the fully connected layer relatively low to control the total number of parameters in the network. 50% dropout was applied to the last two fully connected layers."}, {"heading": "2.3 TRAINING", "text": "The weight distribution in ConvNets results in very different gradients in different layers and therefore a lower learning rate is commonly used in the application of SGD. We trained the recursive-revolutionary network using the Adam algorithm (Kingma & Ba, 2015) with a learning factor of 103 and a first and second moment decay rate of 0.9 and 0.999 respectively. Batch size was increased to 20. Adam has shown that it achieves competitively fast convergence rates when training ConvNets and multilayered neural networks. In addition, the VGG architecture requires fewer epochs due to implicit regulation forced by greater depth and smaller folding filter sizes. Figure 3 shows the validation loss with the number of epochs over the training set. We found that the network parameters converge after about 600 iterations (5 epochs)."}, {"heading": "3 BASELINE METHODS", "text": "We compared our approach with various classifiers commonly used in the field, including support vector machines (SVM), random forest, sparse logistic regression, and deep supply networks (DBN). Here we briefly describe some details and parameter settings used in these methods.SVM: SVM hyperparameters consisting of regularization parameters (C) and inverse RBF cores Standard deviation (\u03b3 = 1 / \u03c3) were selected by cross-validation on the training set (C = {0.01, 0.1, 1, 10, 100}, \u03b3 = {0.1, 0.2,..., 1, 2,...,..., 10}). Random Forest: Random Forest is an ensemble method consisting of a group of independent random decision trees. Each tree is bred with a randomly selected subset of characteristics."}, {"heading": "4 EXPERIMENTS ON AN EEG DATASET", "text": "The EEG was recorded when fifteen participants (eight women) performed a standardized working memory experiment. Details of the data recording and purification procedures are reported in our previous publication (Bashivan et al., 2014). In short, the continuous EEG was recorded by 64 electrodes placed above the scalp at standard 10-10 locations with a sampling frequency of 500 Hz. Electrodes were placed at 10% intervals along the medial-lateral contours. Data for two of the subjects were excluded from the data set because of excessive noise and artifacts occurring in their recorded data. During the experiment, an array of English characters was shown for 0.5 seconds (SET) and participants were instructed to memorize the characters. A test sign was shown three seconds later and participants indicated whether the test sign belonged to the first array or not by pressing a button. Each participant was shown the character repeated 240 times (SET), the number of the SET was determined randomly by SET 8, the number of the SET was determined by SET 8."}, {"heading": "5 RESULTS", "text": "We examined the EEG dataset from two approaches: In the first approach (single image), we extracted the performance characteristics by applying FFT over the entire duration of each study, resulting in a single 3-channel image that matched each study; in the second approach, each study was split into multiple time frames and performance characteristics were extracted separately for each window, resulting in the preservation of temporal information rather than intersecting it into individual parts of the activity map."}, {"heading": "5.1 SINGLE-FRAME CLASSIFICATION", "text": "The purpose of this part was to find empirically the most powerful ConvNet architecture that works with images from complete EEG time series. We evaluated different configurations with different numbers of folding and Maxpool layers. We followed the VGG architecture in selecting the number of filters in each layer and grouped folding layers with small receptive fields. Table 1, which we presented earlier, summarized the architectures we looked at. Table 2 shows the number of parameters used by each architecture and the corresponding error that was achieved on the test set. We see that increasing the number of layers to seven slightly improved the achievable error rates on the test set. The best result was achieved with Architecture D, which contained 7 folding layers. Most network parameters are in the last two layers (fully connected and softmax), which contained approximately 1 million parameters.In VGG style, the number of selected layers with 7-13k each containing the same number of folding layers (the same number of 137-13k) was included."}, {"heading": "5.2 MULTI-FRAME CLASSIFICATION", "text": "For the multi-frame classification, we used ConvNet with architecture D from the previous step and applied it to each image. We examined the four different approaches to aggregating temporal characteristics from multiple images (Figure 2). The use of temporal folding and LSTM significantly improved classification accuracy (see Table 3). A closer look at the accuracies derived for each individual shows that while both methods achieve near-perfect classification accuracies for eight of the participants, most of the differences are due to differences in the accuracy of the remaining five individuals (Table 4). This observation motivated us to use a combination of temporal folding and LSTM structures in a single structure, resulting in our best results on the datasets. While our approach does not directly affect raw EEG time series, we have drastically reduced the amount of data required by manually extracting performance characteristics from the EEG. In addition, complex time relationships, such as those that are linked to EG networks, can be transformed into neural EEG by means of different spectral properties."}, {"heading": "6 CONCLUSIONS", "text": "This work is motivated by the overall goal of finding robust representations of EEG data that would be invariant to sub-subject and sub-subject differences and inherent noise associated with the collection of EEG data. We propose a new method to learn representations from multi-channel EEG time series and demonstrate their advantages in the context of the task of classifying mental stress. Our approach is fundamentally different from previous attempts to learn high-level representations from the EEG by using deep neural networks. Instead of presenting the properties of the EEG as a vector at the low level, we transform the data into a sequence of topology-preserving multispectral images (EEG film), as opposed to standard EEG analysis techniques that ignore such spatial information. We then train deep recursive-revolutionary networks inspired by state-of-the-art video classifications to learn robust representations from the sequence of images that show significant improvements in the classification approach proposed."}], "references": [{"title": "A trivariate cloughtocher scheme for tetrahedral data", "author": ["Alfeld", "Peter"], "venue": "Computer Aided Geometric Design,", "citeRegEx": "Alfeld and Peter.,? \\Q1984\\E", "shortCiteRegEx": "Alfeld and Peter.", "year": 1984}, {"title": "Spectrotemporal dynamics of the EEG during working memory encoding and maintenance predicts individual behavioral capacity", "author": ["Bashivan", "Pouya", "Bidelman", "Gavin M", "Yeasin", "Mohammed"], "venue": "European Journal of Neuroscience,", "citeRegEx": "Bashivan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bashivan et al\\.", "year": 2014}, {"title": "Greedy layer-wise training of deep networks", "author": ["Bengio", "Yoshua", "Lamblin", "Pascal", "Popovici", "Dan", "Larochelle", "Hugo"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Convolutional neural networks for P300 detection with application to brain-computer interfaces", "author": ["Cecotti", "Hubert", "Gr\u00e4ser", "Axel"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Cecotti et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cecotti et al\\.", "year": 2011}, {"title": "Unconstrained online handwriting recognition with recurrent neural networks", "author": ["Graves", "Alex", "S Fern\u00e1ndez", "Liwicki", "Marcus"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Graves et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2008}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves", "Alex", "Mohamed", "Abdel-Rahman", "Hinton", "Geoffrey E"], "venue": "Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Recurrent neural networks employing Lyapunov exponents for EEG signals classification", "author": ["N Guler", "E Ubeyli", "I. Guler"], "venue": "Expert Systems with Applications,", "citeRegEx": "Guler et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Guler et al\\.", "year": 2005}, {"title": "Teaching Machines to Read and Comprehend", "author": ["Hermann", "Karm Moritz", "Ko\u010disk\u00fd", "Tom\u00e1\u0161", "Grefenstette", "Edward", "Espeholt", "Lasse", "Kay", "Will", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "arXiv, pp", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long Short-Term Memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Frontal theta activity in humans increases with memory load in a working memory", "author": ["Jensen", "Ole", "Tesche", "Claudia D"], "venue": "task. Neuroscience,", "citeRegEx": "Jensen et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Jensen et al\\.", "year": 2002}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["Karpathy", "Andrej", "G. Toderici"], "venue": "Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Adam: a Method for Stochastic Optimization", "author": ["Kingma", "Diederik P", "Ba", "Jimmy Lei"], "venue": "International Conference on Learning Representations,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks. Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "systems, pp. 1097\u20131105,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "A Review of Classification Algorithms for EEG-based Brain-Computer Interfaces", "author": ["F Lotte", "M. Congedo"], "venue": "Journal of neural engineering,", "citeRegEx": "Lotte and Congedo,? \\Q2007\\E", "shortCiteRegEx": "Lotte and Congedo", "year": 2007}, {"title": "Classification of patterns of EEG synchronization for seizure prediction", "author": ["Mirowski", "Piotr", "Madhavan", "Deepak", "LeCun", "Yann", "Kuzniecky", "Ruben"], "venue": "Clinical Neurophysiology,", "citeRegEx": "Mirowski et al\\.,? \\Q1927\\E", "shortCiteRegEx": "Mirowski et al\\.", "year": 1927}, {"title": "Beyond Short Snippets: Deep Networks for Video Classification", "author": ["Ng", "Jyh", "M. Hausknecht"], "venue": "In CVPR,", "citeRegEx": "Ng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2015}, {"title": "Deep learning for neuroimaging: a validation study", "author": ["Plis", "Sergey M", "Hjelm", "Devon R", "Salakhutdinov", "Ruslan", "Allen", "Elena a", "Bockholt", "Henry J", "Long", "Jeffrey D", "Johnson", "Hans J", "Paulsen", "Jane S", "Turner", "Jessica a", "Calhoun", "Vince D"], "venue": "Frontiers in Neuroscience,", "citeRegEx": "Plis et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Plis et al\\.", "year": 2014}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["K Simonyan", "A. Zisserman"], "venue": "In ICLR, pp", "citeRegEx": "Simonyan and Zisserman,? \\Q2015\\E", "shortCiteRegEx": "Simonyan and Zisserman", "year": 2015}, {"title": "Map projections\u2013A working manual, volume 1395", "author": ["Snyder", "John Parr"], "venue": "US Government Printing Office,", "citeRegEx": "Snyder and Parr.,? \\Q1987\\E", "shortCiteRegEx": "Snyder and Parr.", "year": 1987}, {"title": "EEG signal classification using PCA, ICA, LDA and support vector machines", "author": ["Subasi", "Abdulhamit", "M. Ismail Gursoy"], "venue": "Expert Systems with Applications,", "citeRegEx": "Subasi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Subasi et al\\.", "year": 2010}, {"title": "Text Understanding from Scratch", "author": ["Zhang", "Xiang", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1502.01710,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 12, "context": "Deep neural networks have recently achieved great success in recognition tasks within a wide range of applications including images, videos, speech, and text (Krizhevsky et al., 2012; Graves et al., 2013; Karpathy & Toderici, 2014; Zhang & LeCun, 2015; Hermann et al., 2015).", "startOffset": 158, "endOffset": 274}, {"referenceID": 5, "context": "Deep neural networks have recently achieved great success in recognition tasks within a wide range of applications including images, videos, speech, and text (Krizhevsky et al., 2012; Graves et al., 2013; Karpathy & Toderici, 2014; Zhang & LeCun, 2015; Hermann et al., 2015).", "startOffset": 158, "endOffset": 274}, {"referenceID": 7, "context": "Deep neural networks have recently achieved great success in recognition tasks within a wide range of applications including images, videos, speech, and text (Krizhevsky et al., 2012; Graves et al., 2013; Karpathy & Toderici, 2014; Zhang & LeCun, 2015; Hermann et al., 2015).", "startOffset": 158, "endOffset": 274}, {"referenceID": 13, "context": "Convolutional neural networks (ConvNets) lie at the core of best current architectures working with images and video data, primarily due to their ability to extract representations that are robust to partial translation and deformation of input patterns (LeCun et al., 1998).", "startOffset": 254, "endOffset": 274}, {"referenceID": 5, "context": "On the other hand, recurrent neural networks have delivered state-of-the-art performance in many applications involving dynamics in temporal sequences, such as, for example, handwriting and speech recognition (Graves et al., 2013; 2008).", "startOffset": 209, "endOffset": 236}, {"referenceID": 6, "context": "In other works, convolutional and recurrent neural networks have been used to extract representations from EEG time series (Cecotti & Gr\u00e4ser, 2011; Guler et al., 2005).", "startOffset": 123, "endOffset": 167}, {"referenceID": 14, "context": ", 2014; Mirowski et al., 2009). Plis et al. (2014) showed that adding several Restricted Boltzman Machine layers to a deep belief network and using supervised pretraining results into networks that can learn increasingly complex representation of the data and achieve considerable increase in classification accuracy.", "startOffset": 8, "endOffset": 51}, {"referenceID": 1, "context": "Oscillatory cortical activity related to memory operations primarily exists in three frequency bands of theta (4-7Hz), alpha (813Hz), and beta (13-30Hz) (Bashivan et al., 2014; Jensen & Tesche, 2002).", "startOffset": 153, "endOffset": 199}, {"referenceID": 2, "context": "Parameters of each layer of DBN were greedily pre-trained to improve learning by shifting the initial random parameter values toward a good local minimum (Bengio et al., 2007).", "startOffset": 154, "endOffset": 175}, {"referenceID": 1, "context": "Details of procedures for data recording and cleaning are reported in our previous publication (Bashivan et al., 2014).", "startOffset": 95, "endOffset": 118}], "year": 2017, "abstractText": "One of the challenges in modeling cognitive events from electroencephalogram (EEG) data is finding representations that are invariant to interand intra-subject differences, as well as to inherent noise associated with such data. Herein, we propose a novel approach for learning such representations from multi-channel EEG time-series, and demonstrate its advantages in the context of mental load classification task. First, we transform EEG activities into a sequence of topologypreserving multi-spectral images, as opposed to standard EEG analysis techniques that ignore such spatial information. Next, we train a deep recurrent-convolutional network inspired by state-of-the-art video classification to learn robust representations from the sequence of images. The proposed approach is designed to preserve the spatial, spectral, and temporal structure of EEG which leads to finding features that are less sensitive to variations and distortions within each dimension. Empirical evaluation on the cognitive load classification task demonstrated significant improvements in classification accuracy over current state-of-the-art approaches in this field.", "creator": "LaTeX with hyperref package"}}}