{"id": "1511.06464", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2015", "title": "Unitary Evolution Recurrent Neural Networks", "abstract": "Recurrent neural networks (RNNs) are notoriously difficult to train. When the eigenvalues of the hidden to hidden weight matrix deviate from absolute value 1, optimization becomes difficult due to the well studied issue of vanishing and exploding gradients, especially when trying to learn long-term dependencies. To circumvent this problem, we propose a new architecture that learns a unitary weight matrix, with eigenvalues of absolute value exactly 1. The challenge we address is that of parametrizing unitary matrices in a way that does not require expensive computations (such as eigendecomposition) after each weight update. We construct an expressive unitary weight matrix by composing several structured matrices that act as building blocks with parameters to be learned. Optimization with this parameterization becomes feasible only when considering hidden states in the complex domain. We demonstrate the potential of this architecture by achieving state of the art results in several hard tasks involving very long-term dependencies.", "histories": [["v1", "Fri, 20 Nov 2015 00:37:33 GMT  (802kb,D)", "http://arxiv.org/abs/1511.06464v1", null], ["v2", "Sat, 28 Nov 2015 18:42:08 GMT  (854kb,D)", "http://arxiv.org/abs/1511.06464v2", null], ["v3", "Thu, 18 Feb 2016 00:52:28 GMT  (914kb,D)", "http://arxiv.org/abs/1511.06464v3", null], ["v4", "Wed, 25 May 2016 23:34:38 GMT  (919kb,D)", "http://arxiv.org/abs/1511.06464v4", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE stat.ML", "authors": ["mart\u00edn arjovsky", "amar shah", "yoshua bengio"], "accepted": true, "id": "1511.06464"}, "pdf": {"name": "1511.06464.pdf", "metadata": {"source": "CRF", "title": "UNITARY EVOLUTION RECURRENT NEURAL NETWORKS", "authors": ["Martin Arjovsky", "Amar Shah", "Yoshua Bengio"], "emails": ["marjovsky@dc.uba.ar", "as793@cam.ac.uk"], "sections": [{"heading": null, "text": "If the eigenvalues of the hidden weight matrix deviate from the absolute value 1, optimization becomes difficult due to the well-studied problem of disappearing and exploding gradients, especially when trying to learn long-term dependencies. To get around this problem, we propose a new architecture that learns a uniform weight matrix after each weight update, with eigenvalues of the absolute value exactly 1. The challenge we face is to parameterize uniform matrices in a way that does not require expensive compositions (such as self-decomposition). We construct an expressive uniform weight matrix by assembling several structured matrices that function as building blocks with parameters to be learned. Optimization with this parameterization can only be carried out taking into account hidden states in the complex area."}, {"heading": "1 INTRODUCTION", "text": "This year it is more than ever before."}, {"heading": "2 ORTHOGONAL WEIGHTS AND BOUNDING THE LONG-TERM GRADIENT", "text": "It is not as if the hidden unit vectors for hidden layers T and t of a neural network with hidden layers and T t. If C is the goal that we are trying to minimize, then the disappearing and exploding gradient problems will point to the decay or growth of the number of layers, T, the number of unhidden layers and T will grow. If C is the goal that we are trying to minimize, then the disappearing and exploding gradient problems point to the decay or growth of the number of layers, T, it is a point that is not hidden layers and T. If C is the goal, we are trying to minimize the layer, then the disappearing and exploding gradient problems will point to the decay or growth of the number of C layers, T, it is a point that is not hidden layers."}, {"heading": "3 UNITARY EVOLUTION RNNS", "text": "Actually real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real"}, {"heading": "4 ARCHITECTURE DETAILS", "text": "In this section, we describe the nonlinearity we use, how we link real values with complex hidden units, and how we map from complex hidden states to real output."}, {"heading": "4.1 COMPLEX HIDDEN UNITS", "text": "Our implementation represents all complex numbers using real values in relation to their real and imaginary parts. Within this framework, we circumvent the lack of support for complex numbers by most deep learning frames. Consider the multiplication of the complex weight matrix W = A + iB by the complex hidden vector h = x + iy, where A, B, x, y are real. It is trivially true that we calculate complex matrix vector products with real numbers as follows (Re (Wh) + i (Ay + Bx): (A \u2212 B A) (Re (h) Im (h))): (7) More generally speaking, we let f: Cn \u2192 Cn be any complex function and z = x + iy any complex vector."}, {"heading": "4.2 INPUT TO HIDDEN, NONLINEARITY, HIDDEN TO OUTPUT", "text": "As is the case with most recursive networks, our uRNN follows the same hidden to hidden mapping method as Equation 1 with Vt = V and Wt = W. Call the size of the complex estimated hidden states nh. Input into the hidden matrix is rated complex. As discussed in the introduction, using a ReLU is a natural choice in combination with a norm-preserving weight matrix. We first experimented with placing separate ReLU activations on the real and imaginary parts of the hidden states, but found that such nonlinearity usually works poorly. Our intuition is that applying separate ReLU nonlinearity to the real and imaginary parts of the LU matrix brutally affects the phase of a complex number, making it difficult to learn the prediction.We speculate that maintaining the phase of hidden states may be important for this large complexity that we support."}, {"heading": "4.3 INITIALIZATION", "text": "Due to the stability of the norm-preserving operations of our network, we found that performance was not very sensitive to the initialization of parameters. To ensure full disclosure and reproducibility, we explain our initialization strategy for each parameter below. \u2022 We initialize V and U (the input and output matrices) as in Glorot & Bengio (2010), with weights captured independently of uniforms, U [\u2212 \u221a 6 \u221a nin + nout, \u221a 6 \u221a nin + nout]. \u2022 The distortions b and bo are initialized to 0. This means that the network is linear with uniform weights during initialization, which seems to support early optimization (Saxe et al., 2014). \u2022 The reflection vectors for R1 and R2 are initialized coordinated from a uniform U [\u2212 1, 1]."}, {"heading": "5 EXPERIMENTS", "text": "In this section, we examine the performance of our uRNN in relation to (a) RNN with Tanh activations, (b) IRNN (Le et al., 2015), which is an RNN with ReLU activations and with the recurring weight matrix initialized with identity, and (c) LSTM (Hochreiter & Schmidhuber, 1994) models. We show that the uRNN excels quantitatively when it comes to modeling long-term dependencies and exhibits qualitatively different learning characteristics from the other models. We selected a handful of tasks to evaluate the performance of the various models, which were specifically designed to be pathologically hard, and were used as benchmarks to capture the ability of a model to capture long-term memory (Hochreiter & Schmidhuber, 1994; Le et al., 2015; Graves et al., al., 2014; Martens & Sutskate, 2011) from the handful of optimization algorithms we had tried out."}, {"heading": "5.1 COPYING MEMORY PROBLEM", "text": "It is well known that recurring networks have difficulty remembering information about inputs that have been seen many time steps before (Bengio et al., 1994; Pascanu et al., 2010).Therefore, we want to test the ability of the uRNN to retrieve exact data that was seen a long time ago.After a setup similar to Hochreiter & Schmidhuber (1994), we outline the copy memory task. Consider 10 categories, {ai} 9i = 0. The input takes the form of a T + 20 length vector of categories, where we test over a range of T. The first 10 entries are sampled uniformly, independently and with substitution of {ai} 7i = 0, representing the sequence to remember. The next T \u2212 1 entries are set to a8, which can be thought of as the \"blanks.\""}, {"heading": "5.2 ADDING PROBLEM", "text": "We follow the addition problem defined in Hochreiter & Schmidhuber (1994) to explain the task at hand. Each input consists of two length-T sequences; the first sequence, which we call x, consists of numbers that are randomly sampled uniformly in the first half of the sequence; the second sequence is an indicator sequence consisting of exactly two entries of 1 and remaining entries 0; the first 1 entry is randomly sampled in the first half of the sequence, while the second 1 entry is uniformly random; the sum of the two entries in the first sequence corresponds to where the entries 1 are in the second sequence; a naive strategy of predicting 1 as output independent of the input sequence yields an expected mean quadratic error of 0.167, the variance of the sum of the two independent equal distributions zeros."}, {"heading": "5.3 PIXEL-BY-PIXEL MNIST", "text": "In this task, proposed by Le et al. (2015), algorithms are fed pixels from MNIST (LeCun et al., 1998) one after the other and must end up with a class name. We consider two tasks: one in which pixels are read in sequence (from left to right, from bottom to top) and one in which pixels are all randomly permutated using the same randomly generated permutation matrix. The same model architectures as for the added problem were used for this task, except we now use a softmax for category classification. We have run the optimization algorithms to the convergence of the mean categorical cross entropy on test data and the plot test accuracy in Figure 3.Both the uRNN and the LSTM perform admirably well here. On the correct, non-permutated MNIST pixels, the LSTM performs better than the test accuracy of ST95.1% compared to ST98.1% and ST98.1% for STN."}, {"heading": "5.4 EXPLORATORY EXPERIMENTS", "text": "As discussed in Section 2, the key to the ability to learn long-term dependencies is in the control of \u2202 C \u2202 ht. With this in mind, we have investigated how each model propagates gradients by investigating the problem of hidden time. It is clear that the uRNN gradients are perfect, while each other model exhibits exponentially disappearing gradients. After 100 iterations of training, each model experiences disappearing gradients, but the uRNN is best able to disseminate information, with much less decay. We maintain that typical recurring architectures are saturated, in the sense that after they acquire some information, it becomes much more difficult to obtain further information about long-term dependencies."}, {"heading": "6 DISCUSSION", "text": "There are a host of other ideas that can be explored from our findings, both in terms of learning representation and efficient implementation. For example, one hurdle to modeling long sequences with recurring networks is the requirement to store all hidden state values for gradient retransmission purposes, which can be prohibitive because GPU memory is typically a limiting factor in neural network optimization. However, since our weight matrix is uniform, its reversal could be its conjugated transpose, which is just as easy to use. If we continued to use an inverted nonlinearity function, we would no longer need to store hidden states as they can be recalculated in reverse, which could potentially have huge implications as we would be able to reduce memory usage by a sequence of T, the number of time steps. This would allow us to have huge hidden layers that may allow for huge storage displays."}], "references": [{"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio", "Yoshua", "Simard", "Patrice", "Frasconi", "Paolo"], "venue": "IEE Transactions on Neural Networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["Bergstra", "James", "Breuleux", "Olivier", "Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Desjardins", "Guillaume", "Turian", "Joseph", "Warde-Farley", "David", "Bengio", "Yoshua"], "venue": "Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "On the properties of neural machine translation: Encoder\u2013Decoder approaches", "author": ["Cho", "Kyunghyun", "van Merri\u00ebnboer", "Bart", "Bahdanau", "Dzmitry", "Bengio", "Yoshua"], "venue": "In Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Collobert", "Ronan", "Weston", "Jason", "Bottou", "L\u00e9on", "Karlen", "Michael", "Kavukcuoglu", "Koray", "Kuksa", "Pavel"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Deep sparse rectifier neural networks", "author": ["Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua"], "venue": "International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Neural turing machines", "author": ["Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["Hinton", "Geoffrey", "Deng", "Li", "Yu", "Dong", "Dahl", "George", "Mohamed", "Abdel-rahman", "Jaitly", "Navdeep", "Senior", "Andrew", "Vanhoucke", "Vincent", "Nguyen", "Patrick", "Sainath", "Tara", "Kingsbury", "Brian"], "venue": "Signal Processing Magazine,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Complex-valued neural networks: theories and applications, volume 5", "author": ["Hirose", "Akira"], "venue": "World Scientific Publishing Company Incorporated,", "citeRegEx": "Hirose and Akira.,? \\Q2003\\E", "shortCiteRegEx": "Hirose and Akira.", "year": 2003}, {"title": "Untersuchungen zu dynamischen neuronalen Netzen", "author": ["S. Hochreiter"], "venue": "Diploma thesis, T.U. Mu\u0308nich,", "citeRegEx": "Hochreiter,? \\Q1991\\E", "shortCiteRegEx": "Hochreiter", "year": 1991}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1994}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Fastfood - approximating kernel expansions in loglinear time", "author": ["Le", "Quoc", "Sarl\u00f3s", "Tam\u00e1s", "Smola", "Alex"], "venue": "International Conference on Machine Learning,", "citeRegEx": "Le et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Le et al\\.", "year": 2010}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Le", "Quoc V", "Navdeep", "Jaitly", "Hinton", "Geoffrey E"], "venue": "arXiv preprint arXiv:1504.00941,", "citeRegEx": "Le et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Learning recurrent neural networks with hessian-free optimization", "author": ["Martens", "James", "Sutskever", "Ilya"], "venue": "International Conference on Machine Learning,", "citeRegEx": "Martens et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Martens et al\\.", "year": 2011}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Vinod", "Hinton", "Geoffrey E"], "venue": "International Conference on Machine Learning,", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Pascanu", "Razvan", "Mikolov", "Tomas", "Bengio", "Yoshua"], "venue": "International Conference on Machine Learning,", "citeRegEx": "Pascanu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2010}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Saxe", "Andrew M", "McLelland", "James L", "Ganguli", "Surya"], "venue": "International Conference in Learning Representations,", "citeRegEx": "Saxe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2014}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tieleman", "Tijmen", "Hinton", "Geoffrey"], "venue": "Coursera: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}, {"title": "Deep fried convnets", "author": ["Yang", "Zichao", "Moczulski", "Marcin", "Denil", "Misha", "de Freitas", "Nando", "Smola", "Alex", "Song", "Le", "Wang", "Ziyu"], "venue": "International Conference on Computer Vision (ICCV),", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Comparison of the complex valued and real valued neural networks trained with gradient descent and random search algorithms", "author": ["Zimmermann", "Hans-Georg", "Minin", "Alexey", "Kusherbaeva", "Victoria"], "venue": "ESANN,", "citeRegEx": "Zimmermann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zimmermann et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 7, "context": "Deep Neural Networks have shown remarkably good performance on a wide range of complex data problems including speech recognition (Hinton et al., 2012), image recognition (Krizhevsky et al.", "startOffset": 130, "endOffset": 151}, {"referenceID": 11, "context": ", 2012), image recognition (Krizhevsky et al., 2012) and natural language processing (Collobert et al.", "startOffset": 27, "endOffset": 52}, {"referenceID": 3, "context": ", 2012) and natural language processing (Collobert et al., 2011).", "startOffset": 40, "endOffset": 64}, {"referenceID": 0, "context": "Although the long-term dependencies problem appears intractable in the absolute (Bengio et al., 1994) for parametrized dynamical systems, several heuristics have recently been found to help reduce its effect, such as the use of self-loops and gating units in the LSTM (Hochreiter & Schmidhuber, 1994) and GRU (Cho et al.", "startOffset": 80, "endOffset": 101}, {"referenceID": 2, "context": ", 1994) for parametrized dynamical systems, several heuristics have recently been found to help reduce its effect, such as the use of self-loops and gating units in the LSTM (Hochreiter & Schmidhuber, 1994) and GRU (Cho et al., 2014) recurrent architectures.", "startOffset": 215, "endOffset": 233}, {"referenceID": 18, "context": "Recent work also supports the idea of using orthogonal weight matrices to assist optimization (Saxe et al., 2014; Le et al., 2015).", "startOffset": 94, "endOffset": 130}, {"referenceID": 13, "context": "Recent work also supports the idea of using orthogonal weight matrices to assist optimization (Saxe et al., 2014; Le et al., 2015).", "startOffset": 94, "endOffset": 130}, {"referenceID": 1, "context": ", 2012) and natural language processing (Collobert et al., 2011). However, training very deep models remains a difficult task. The main issue surrounding the training of deep networks is the vanishing and exploding gradients problems introduced by Hochreiter (1991) and shown by Bengio et al.", "startOffset": 41, "endOffset": 266}, {"referenceID": 0, "context": "The main issue surrounding the training of deep networks is the vanishing and exploding gradients problems introduced by Hochreiter (1991) and shown by Bengio et al. (1994) to be necessarily arising when trying to learn to reliably store bits of information in any parametrized dynamical system.", "startOffset": 152, "endOffset": 173}, {"referenceID": 0, "context": "The main issue surrounding the training of deep networks is the vanishing and exploding gradients problems introduced by Hochreiter (1991) and shown by Bengio et al. (1994) to be necessarily arising when trying to learn to reliably store bits of information in any parametrized dynamical system. If gradients propagated back through a network vanish, the credit assignment role of backpropagation is lost, as information about small changes in states in the far past has no influence on future states. If gradients explode, gradient-based optimization algorithms struggle to traverse down a cost surface, because gradient-based optimization assumes small changes in parameters yield small changes in the objective function. As the number of time steps considered in the sequence of states grows, the shrinking or expanding effects associated with the state-to-state transformation at individual time steps can grow exponentially, yielding respectively vanishing or exploding gradients. See Pascanu et al. (2010) for a review.", "startOffset": 152, "endOffset": 1012}, {"referenceID": 21, "context": "Complex valued representations have been considered for neural networks in the past, but with limited success and adoption (Hirose, 2003; Zimmermann et al., 2011).", "startOffset": 123, "endOffset": 162}, {"referenceID": 1, "context": "Whilst our model uses complex valued matrices and parameters, all implementation and optimization is possible with real numbers and has been done in Theano (Bergstra et al., 2010).", "startOffset": 156, "endOffset": 179}, {"referenceID": 5, "context": "This argument makes the rectified linear unit (ReLU) nonlinearity an attractive choice (Glorot et al., 2011; Nair & Hinton, 2010).", "startOffset": 87, "endOffset": 129}, {"referenceID": 17, "context": "Most notably, this result holds for a network of arbitrary depth and renders engineering tricks like gradient clipping unnecessary (Pascanu et al., 2010).", "startOffset": 131, "endOffset": 153}, {"referenceID": 0, "context": "Bengio et al. (1994) suggest that having a large memory may be crucial for solving difficult tasks with long ranging dependencies: the smaller the state dimension, the more information necessarily has to be eliminated when mapping a long sequence to a fixed-dimension state.", "startOffset": 0, "endOffset": 21}, {"referenceID": 12, "context": "This combined with earlier work (Le et al., 2010) suggests that it is possible to create highly expressive matrices by composing simple matrices with few parameters.", "startOffset": 32, "endOffset": 49}, {"referenceID": 18, "context": "Yang et al. (2015) construct a real valued, non-orthogonal matrix using a similar parameterization with the motivation of parameter reduction by an order of magnitude on an industrial sized network.", "startOffset": 0, "endOffset": 19}, {"referenceID": 18, "context": "This implies that at initialization, the network is linear with unitary weights, which seems to help early optimization (Saxe et al., 2014).", "startOffset": 120, "endOffset": 139}, {"referenceID": 13, "context": "In this section we explore the performance of our uRNN in relation to (a) RNN with tanh activations, (b) IRNN (Le et al., 2015), that is an RNN with ReLU activations and with the recurrent weight matrix initialized to the identity, and (c) LSTM (Hochreiter & Schmidhuber, 1994) models.", "startOffset": 110, "endOffset": 127}, {"referenceID": 13, "context": "The tasks were especially created to be be pathologically hard, and have been used as benchmarks for testing the ability of a model to capture long-term memory (Hochreiter & Schmidhuber, 1994; Le et al., 2015; Graves et al., 2014; Martens & Sutskever, 2011) Of the handful of optimization algorithms we tried on the various models, RMSProp (Tieleman & Hinton, 2012) lead to fastest convergence and is what we stuck to for all experiments here on in.", "startOffset": 160, "endOffset": 257}, {"referenceID": 6, "context": "The tasks were especially created to be be pathologically hard, and have been used as benchmarks for testing the ability of a model to capture long-term memory (Hochreiter & Schmidhuber, 1994; Le et al., 2015; Graves et al., 2014; Martens & Sutskever, 2011) Of the handful of optimization algorithms we tried on the various models, RMSProp (Tieleman & Hinton, 2012) lead to fastest convergence and is what we stuck to for all experiments here on in.", "startOffset": 160, "endOffset": 257}, {"referenceID": 0, "context": "Recurrent networks have been known to have trouble remembering information about inputs seen many time steps previously (Bengio et al., 1994; Pascanu et al., 2010).", "startOffset": 120, "endOffset": 163}, {"referenceID": 17, "context": "Recurrent networks have been known to have trouble remembering information about inputs seen many time steps previously (Bengio et al., 1994; Pascanu et al., 2010).", "startOffset": 120, "endOffset": 163}, {"referenceID": 0, "context": "Recurrent networks have been known to have trouble remembering information about inputs seen many time steps previously (Bengio et al., 1994; Pascanu et al., 2010). We therefore want to test the uRNN\u2019s ability to recall exactly data seen a long time ago. Following a similar setup to Hochreiter & Schmidhuber (1994), we outline the copy memory task.", "startOffset": 121, "endOffset": 316}, {"referenceID": 6, "context": "This behaviour is consistent with the results of Graves et al. (2014), in which poor performance is reported for the LSTM for a very similar long term memory problem.", "startOffset": 49, "endOffset": 70}, {"referenceID": 9, "context": "We closely follow the adding problem defined in Hochreiter & Schmidhuber (1994) to explain the task at hand.", "startOffset": 48, "endOffset": 80}, {"referenceID": 9, "context": "We closely follow the adding problem defined in Hochreiter & Schmidhuber (1994) to explain the task at hand. Each input consists of two sequences of length T . The first sequence, which we denote x, consists of numbers sampled uniformly at random U [0, 1]. The second sequence is an indicator sequence consisting of exactly two entries of 1 and remaining entries 0. The first 1 entry is located uniformly at random in the first half of the sequence, whilst the second 1 entry is located uniformly at random in the second half. The output is the sum of the two entries of the first sequence, corresponding to where the 1 entries are located in the second sequence. A naive strategy of predicting 1 as the output regardless of the input sequence gives an expected mean squared error of 0.167, the variance of the sum of two independent uniform distributions. This is our baseline to beat. We chose to use 128 hidden units for the RNN with tanh, IRNN and LSTM and 512 for the uRNN. This equates to roughly 16K parameters for the RNN with tanh and IRNN, 60K for the LSTM and almost 9K for the uRNN. All models were trained using batch sizes of 20 and 50 with the best results being reported. Our results are shown in Figure 2. The LSTM and uRNN models are able to convincingly beat the baseline up to T = 400 time steps. Both models do well when T = 750, but the mean squared error does not reach close to 0. The uRNN achieves lower test error, but with more noise at testing. Despite having vastly more parameters, we monitored the LSTM performance to ensure no overfitting. The RNN with tanh and IRNN were not able to beat the baseline for any number of time steps. Le et al. (2015) report that their RNN solve the problem for T = 150 and the IRNN for T = 300, but they require over a million iterations before they start learning.", "startOffset": 48, "endOffset": 1681}, {"referenceID": 12, "context": "explained by our use of RMSprop with significantly higher learning rates (10\u22123 as opposed to 10\u22128) than Le et al. (2015) use for SGD with momentum.", "startOffset": 104, "endOffset": 121}, {"referenceID": 14, "context": "(2015), algorithms are fed pixels of MNIST (LeCun et al., 1998) sequentially and required to output a class label at the end.", "startOffset": 43, "endOffset": 63}, {"referenceID": 13, "context": "This result is state of the art on this task, beating the IRNN (Le et al., 2015), which reaches close to 82% after 1 million training iterations.", "startOffset": 63, "endOffset": 80}, {"referenceID": 12, "context": "In this task, suggested by Le et al. (2015), algorithms are fed pixels of MNIST (LeCun et al.", "startOffset": 27, "endOffset": 44}, {"referenceID": 1, "context": "Acknowledgments: We thank the developers of Theano Bergstra et al. (2010) for their great work.", "startOffset": 51, "endOffset": 74}], "year": 2015, "abstractText": "Recurrent neural networks (RNNs) are notoriously difficult to train. When the eigenvalues of the hidden to hidden weight matrix deviate from absolute value 1, optimization becomes difficult due to the well studied issue of vanishing and exploding gradients, especially when trying to learn long-term dependencies. To circumvent this problem, we propose a new architecture that learns a unitary weight matrix, with eigenvalues of absolute value exactly 1. The challenge we address is that of parametrizing unitary matrices in a way that does not require expensive computations (such as eigendecomposition) after each weight update. We construct an expressive unitary weight matrix by composing several structured matrices that act as building blocks with parameters to be learned. Optimization with this parameterization becomes feasible only when considering hidden states in the complex domain. We demonstrate the potential of this architecture by achieving state of the art results in several hard tasks involving very long-term dependencies.", "creator": "LaTeX with hyperref package"}}}