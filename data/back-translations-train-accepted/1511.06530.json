{"id": "1511.06530", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2015", "title": "Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications", "abstract": "Although the latest high-end smartphone has powerful CPU and GPU, running deeper convolutional neural networks (CNNs) for complex tasks such as ImageNet classification on mobile devices is challenging. To deploy deep CNNs on mobile devices, we present a simple and effective scheme to compress the entire CNN, which we call one-shot whole network compression. The proposed scheme consists of three steps: (1) rank selection with variational Bayesian matrix factorization, (2) Tucker decomposition on kernel tensor, and (3) fine-tuning to recover accumulated loss of accuracy, and each step can be easily implemented using publicly available tools. We demonstrate the effectiveness of the proposed scheme by testing the performance of various compressed CNNs (AlexNet, VGGS, GoogLeNet, and VGG-16) on the smartphone. Significant reductions in model size, runtime, and energy consumption are obtained, at the cost of small loss in accuracy. In addition, we address the important implementation level issue on 1?1 convolution, which is a key operation of inception module of GoogLeNet as well as CNNs compressed by our proposed scheme.", "histories": [["v1", "Fri, 20 Nov 2015 09:20:08 GMT  (1272kb,D)", "http://arxiv.org/abs/1511.06530v1", null], ["v2", "Wed, 24 Feb 2016 11:52:12 GMT  (1272kb,D)", "http://arxiv.org/abs/1511.06530v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["yong-deok kim", "eunhyeok park", "sungjoo yoo", "taelim choi", "lu yang", "dongjun shin"], "accepted": true, "id": "1511.06530"}, "pdf": {"name": "1511.06530.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Yong-Deok Kim", "Taelim Choi", "Lu Yang", "Dongjun Shin"], "emails": ["d.j.shin}@samsung.com", "sungjoo.yoo}@gmail.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "This year it has come to the point that it is a purely reactionary project, in which it is a reactionary, reactionary and reactionary party."}, {"heading": "2 RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 CNN COMPRESSION", "text": "According to Denil et al. (2013), the possibility of eliminating the redundancy of neural networks was shown, and several CNN compression techniques were proposed. However, more recently, various methods based on vector quantification (Gong et al., 2014) showed that the weight matrix of a fully connected layer can be decompressed by applying truncated individual values. (Chen et al., 2015), circular projection (Cheng et al., 2015), and tensor train decomposition (Novikov et al., 2015), proposals were made that have a better compression capacity than SVD. To accelerate the conventional layers, several methods based on the composition of conventional nuclei were proposed."}, {"heading": "2.2 TENSOR DECOMPOSITION", "text": "A tensor is a multipath data field. For example, a vector is a one-way tensor and a matrix is a two-way tensor. Two of the most popular tensor decomposition models are the CANDECOMP / PARAFAC model (Carroll & Chang, 1970; Harshman & Lundy, 1994; Shashua & Hazan, 2005) and the Tucker model (Tucker, 1966; De Lathauwer et al., 2000; Kim & Choi, 2007). In this paper, we extensively use the Tucker model for overall network compression. Tucker decomposition is an extension of the higher order of singular value substitution (SVD) of the matrix in the perspective of the compression of the oral spaces associated with the various modes of a tensor."}, {"heading": "3 PROPOSED METHOD", "text": "Fig. 2 illustrates our complete network compression scheme, which consists of three steps: (1) rank selection; (2) tucker decomposition; (3) fine tuning. In the first step, we analyze the main subspace of the mode 3 and mode 4 matricization of the kernel tensor of each layer with global analytical variation of the Bayesian matrix factorization. Then we apply the tucker decomposition to the kernel tensor of each layer with a pre-determined rank. Finally, we tune the entire network with standard back propagation."}, {"heading": "3.1 TUCKER DECOMPOSITION ON KERNEL TENSOR", "text": "The convolution of the kernel tensor: In CNNs, the convolution represents an input (source) tensor X of size H \u00b7 W \u00b7 S in the output (target) tensor Y of size H \u2032 \u00b7 W \u2032 \u00b7 T with the following linear mapping: Yh \u2032, w \u2032, t = D \u2211 i = 1 D \u2211 j = 1 S \u2211 s = 1 Ki, j \u00b7 s, t Xhi, wj, s, hi = (h \u2032 \u2212 1) p \u2212 P and wj = (w \u2032 1) p \u2212 P, (1) where K is a 4-way tensor of size D \u00b7 D \u00b7 S \u00b7 T is stride, and P is zero padding size. The composition: The rank (R1, R2 Y2, R4) tucker decomposition of the 4-way tensor K has the form: Ki, j s, s = R1 R2 R2 R2 R2 = 4 R2."}, {"heading": "3.2 RANK SELECTION WITH GLOBAL ANALYTIC VBMF", "text": "Rank (R3, R4) are very important hyperparameters that control the trade-off between performance improvement (memory, speed, energy) and loss of accuracy. Instead of selecting rank (R3, R4) by time-consuming trial-and-error, we looked at data-driven one-shot decisions on empirical bayes (MacKay, 1992) with automatic relevance determination (ARD) (Tipping, 2001). The first time we designed a probabilistic Tucker model similar to that (M\u00f8rup & Hansen, 2009) and applied empirical variational Bayesian learning. However, the ranking results were highly unreliable as they depended heavily on (1) the initial condition, (2) the estimation of noise variance and (3) the threshold setting for pruning."}, {"heading": "3.3 FINE-TUNING", "text": "Since we minimize the reconstruction error of linear kernel tensors instead of non-linear responses, the accuracy decreases significantly after total network compression (e.g. over 50% in the case of AlexNet). However, as shown in Fig. 4, we can easily restore accuracy by fine-tuning with ImageNet training data sets. We observed that accuracy is quickly restored in an epoch. However, more than 10 epochs are required to restore the original accuracy. While (Lebedev et al., 2015; Zhang et al., 2015a) reported difficulties in finding a good SGD learning rate, our rule for scheduling the single learning rate for various compressed CNNs works well. In our experiment, we set the learning base to 10 \u2212 3 and reduced it by a factor of 10 every 5 epochs. Due to the limitation of GPU memory, we fixed the batch size: 128, 128, 1264, and Network Runtime for AlexG-GT, GoogG-VT, and Goognet architecture)."}, {"heading": "4 EXPERIMENTS", "text": "We used four representative CNNs, AlexNet, VGG-S, GoogLeNet and VGG-16, which can be downloaded on Berkeley's Caffe model zoo. In the case of the founding module of GoogLeNet, we compressed only the 3 \u00d7 3 folding core, which is the most important computational part. In the case of VGG-16, we compressed only the folding layers as in (Zhang et al., 2015a). The accuracy of the top 5 view is measured using 50,000 validation images from the ImageNet2012 dataset. We conducted experiments on Nvidia Titan X (for fine-tuning and runtime comparison on Caffe + cuDNN2) and a smartphone, Samsung Galaxy S6 (for comparing runtime and energy consumption), with the application processor of the smartphone (Exynos 7420) equipped with a mobile GPU, ARM Mali T760 experiments. Compared to the GPU, the GPU is used on the GPG13 (the GPU is used on the VGU)."}, {"heading": "4.1 OVERALL RESULTS", "text": "Table 1 shows the overall results for the three CNNs. Our proposed scheme provides reductions in total weights and FLOPs in the ranges \u00d7 5.46 / \u00d7 2.67 (AlexNet), \u00d7 7.40 / \u00d7 4.80 (VGG-S), \u00d7 1.28 / \u00d7 2.06 (GoogLeNet) and \u00d7 1.09 / \u00d7 4.93 (VGGG16). Such reductions offer runtime improvements of 1.42 x 3.68 (\u00d7 1.23 x 2.33) for the smartphone (Titan X). We report on the energy consumption of the mobile GPU and memory. The smartphone delivers greater reduction rates (e.g. \u00d7 3.41 vs. \u00d7 2.72 for AlexNet) for energy consumption than for runtime. We will perform a detailed analysis in the following paragraph. Comparison with the method of Zhang et al. (2015a): The accuracy of our compressed VGG-16 is 89.40% for theoretical acceleration and is comparable to the theoretical 89.6% (88.6) (88.6) x 2015x."}, {"heading": "4.2 LAYERWISE ANALYSIS", "text": "Tables 2, 3, 4 and 5 1 show the detailed comparisons. Each row has two results (the above for the original uncompressed CNN and the other for the compressed CNN) and improvements. For example, in Table 2, the second Convolutionary Layer with the input and output channel dimensions of 48 \u00d7 2 is compressed to give the three matrix multiplications for each of the weights, FLOPs and runtime. After compression, one layer in the compressed network leads to three matrix multiplications. We enter the details of the three matrix multiplications for each of the weights, and runtime. For example, on the S6 in Table 2, the second layer in the compressed network multiplications."}, {"heading": "4.3 ENERGY CONSUMPTION ANALYSIS", "text": "In this context, it should be noted that we have enlarged the timeline of the compressed networks for a better comparison. We have consumed VGG-16 in a similar way since VGG-16. The figure shows that the compression of the power consumption (Y axis) is smaller than that of the uncompressed memory (X axis), which explains why the reduction of the energy consumption is greater than that of the runtime in Table 1. Fig. 5 that the GPU power consumption is smaller than that of the uncompressed CNN axis."}, {"heading": "5 DISCUSSION", "text": "Although we can achieve very promising results with a one-shot ranking selection, it is not yet fully investigated whether the chosen rank is really optimal or not. In the future, we will investigate the optimality of our proposed scheme. 1 x 1 folding is a key operation in our compressed model as well as in the founding module of GoogLeNet. Due to its properties, such as channel compression and computation reduction, we expect 1 x 1 folding to become increasingly popular in the future. However, to solve this problem, we propose a one-shot compression scheme that uses a single general low-rank approach and a global ranking method. Our oneshot compression allows for quick design and easy implementation with publicly available tools. To address this problem, we propose a one-shot compression scheme that uses a single general low-rank approach and a global ranking method."}, {"heading": "A EXPERIMENTAL SETUP", "text": "This section describes the details of the experimental setup including the power consumption measurement system and illustrates the measured data.A.1 MEASUREMENT SYSTEMS Fig. 6 shows the power measurement system. As shown in the figure, it consists of a test board (left) with a Samsung Galaxy S6 smartphone and current probes and a monitor board (right).The test board provides 8 probes connected to the power pins of the application processor (shown below).The power profiling monitor stomps the electric current every 0.1ms for each mains probe and provides power consumption data with a time stamp.Fig. 7 shows the main board of the smartphone (Fig. 7 (a), the application processor chip package (red rectangle in Fig. 2 (a)), consisting of the application processor and the main memory (LPDDR4 DRAM) in the smartphone (Fig.) and a simplified block diagram (block diagram)."}, {"heading": "B LAYERWISE ANALYSIS", "text": "We report on detailed comparison results VGG-S, GoogLeNet and VGG-16."}], "references": [{"title": "Matlab tensor toolbox version 2.6", "author": ["Bader", "Brett W", "Kolda", "Tamara G"], "venue": "Available online,", "citeRegEx": "Bader et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bader et al\\.", "year": 2015}, {"title": "High performance convolutional neural networks for document processing", "author": ["Chellapilla", "Kumar", "Puri", "Sidd", "Simard", "Patrice"], "venue": "In Tenth International Workshop on Frontiers in Handwriting Recognition. Suvisoft,", "citeRegEx": "Chellapilla et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chellapilla et al\\.", "year": 2006}, {"title": "Compressing neural networks with the hashing trick", "author": ["Chen", "Wenlin", "Wilson", "James T", "Tyree", "Stephen", "Weinberger", "Kilian Q", "Yixin"], "venue": "arXiv preprint arXiv:1504.04788,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Fast neural networks with circulant projections", "author": ["Cheng", "Yu", "Felix X", "Feris", "Rogerio S", "Kumar", "Sanjiv", "Choudhary", "Alok", "Chang", "Shih-Fu"], "venue": "arXiv preprint arXiv:1502.03436,", "citeRegEx": "Cheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2015}, {"title": "A multilinear singular value decomposition", "author": ["De Lathauwer", "Lieven", "De Moor", "Bart", "Vandewalle", "Joos"], "venue": "SIAM journal on Matrix Analysis and Applications,", "citeRegEx": "Lathauwer et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Lathauwer et al\\.", "year": 2000}, {"title": "Tensor rank and the ill-posedness of the best low-rank approximation problem", "author": ["De Silva", "Vin", "Lim", "Lek-Heng"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Silva et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Silva et al\\.", "year": 2008}, {"title": "Predicting parameters in deep learning", "author": ["Denil", "Misha", "Shakibi", "Babak", "Dinh", "Laurent", "de Freitas", "Nando"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Denil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2013}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Denton", "Emily L", "Zaremba", "Wojciech", "Bruna", "Joan", "LeCun", "Yann", "Fergus", "Rob"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Denton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2014}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Gong", "Yunchao", "Liu", "Yang", "Ming", "Bourdev", "Lubomir"], "venue": "arXiv preprint arXiv:1412.6115,", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "A deep neural network compression pipeline: Pruning, quantization, huffman encoding", "author": ["Han", "Song", "Mao", "Huizi", "Dally", "William J"], "venue": "arXiv preprint arXiv:1510.00149,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Learning both weights and connections for efficient neural networks", "author": ["Han", "Song", "Pool", "Jeff", "Tran", "John", "Dally", "William J"], "venue": "arXiv preprint arXiv:1506.02626,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Parafac: Parallel factor analysis", "author": ["Harshman", "Richard A", "Lundy", "Margaret E"], "venue": "Computational Statistics & Data Analysis,", "citeRegEx": "Harshman et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Harshman et al\\.", "year": 1994}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In IEEE International Conference on Computer Vision,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["M. Jaderberg", "A. Vedaldi", "A. Zisserman"], "venue": "In British Machine Vision Conference,", "citeRegEx": "Jaderberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2014}, {"title": "Nonnegative Tucker decomposition", "author": ["Kim", "Y.-D", "S. Choi"], "venue": "In Proceedings of the IEEE CVPR2007 Workshop on Component Analysis Methods, Minneapolis, Minnesota,", "citeRegEx": "Kim et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2007}, {"title": "Tensor decompositions and applications", "author": ["Kolda", "Tamara G", "Bader", "Brett W"], "venue": "SIAM review,", "citeRegEx": "Kolda et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kolda et al\\.", "year": 2009}, {"title": "Speeding-up convolutional neural networks using fine-tuned cp-decomposition", "author": ["Lebedev", "Vadim", "Ganin", "Yaroslav", "Rakhuba", "Maksim", "Oseledets", "Ivan", "Lempitsky", "Victor"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Lebedev et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lebedev et al\\.", "year": 2015}, {"title": "Bayesian interpolation", "author": ["MacKay", "David JC"], "venue": "Neural computation,", "citeRegEx": "MacKay and JC.,? \\Q1992\\E", "shortCiteRegEx": "MacKay and JC.", "year": 1992}, {"title": "Fast training of convolutional networks through ffts", "author": ["Mathieu", "Michael", "Henaff", "Mikael", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1312.5851,", "citeRegEx": "Mathieu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mathieu et al\\.", "year": 2013}, {"title": "Automatic relevance determination for multi-way models", "author": ["M\u00f8rup", "Morten", "Hansen", "Lars Kai"], "venue": "Journal of Chemometrics,", "citeRegEx": "M\u00f8rup et al\\.,? \\Q2009\\E", "shortCiteRegEx": "M\u00f8rup et al\\.", "year": 2009}, {"title": "Variational Bayesian matrix factorization version", "author": ["Nakajima", "Shinichi"], "venue": "URL https: //sites.google.com/site/shinnkj23/downloads", "citeRegEx": "Nakajima and Shinichi.,? \\Q2015\\E", "shortCiteRegEx": "Nakajima and Shinichi.", "year": 2015}, {"title": "Perfect dimensionality recovery by variational bayesian pca", "author": ["Nakajima", "Shinichi", "Tomioka", "Ryota", "Sugiyama", "Masashi", "Babacan", "S Derin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Nakajima et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nakajima et al\\.", "year": 2012}, {"title": "Global analytic solution of fully-observed variational bayesian matrix factorization", "author": ["Nakajima", "Shinichi", "Sugiyama", "Masashi", "Babacan", "S Derin", "Tomioka", "Ryota"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Nakajima et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nakajima et al\\.", "year": 2013}, {"title": "Non-negative tensor factorization with applications to statistics and computer vision", "author": ["Shashua", "Amnon", "Hazan", "Tamir"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "Shashua et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Shashua et al\\.", "year": 2005}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Simonyan and Zisserman,? \\Q2015\\E", "shortCiteRegEx": "Simonyan and Zisserman", "year": 2015}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Sparse bayesian learning and the relevance vector machine", "author": ["Tipping", "Michael E"], "venue": "The journal of machine learning research,", "citeRegEx": "Tipping and E.,? \\Q2001\\E", "shortCiteRegEx": "Tipping and E.", "year": 2001}, {"title": "Some mathematical notes on three-mode factor analysis", "author": ["Tucker", "Ledyard R"], "venue": null, "citeRegEx": "Tucker and R.,? \\Q1966\\E", "shortCiteRegEx": "Tucker and R.", "year": 1966}, {"title": "Improving the speed of neural networks on cpus", "author": ["Vanhoucke", "Vincent", "Senior", "Andrew", "Mao", "Mark Z"], "venue": "In Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop,", "citeRegEx": "Vanhoucke et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vanhoucke et al\\.", "year": 2011}, {"title": "Generalized low rank approximations of matrices", "author": ["Ye", "Jieping"], "venue": "Machine Learning,", "citeRegEx": "Ye and Jieping.,? \\Q2005\\E", "shortCiteRegEx": "Ye and Jieping.", "year": 2005}, {"title": "Accelerating very deep convolutional networks for classification and detection", "author": ["Zhang", "Xiangyu", "Zou", "Jianhua", "He", "Kaiming", "Sun", "Jian"], "venue": "arXiv preprint arXiv:1505.06798,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Efficient and accurate approximations of nonlinear convolutional networks. 2015b", "author": ["Zhang", "Xiangyu", "Zou", "Jianhua", "Ming", "Xiang", "He", "Kaiming", "Sun", "Jian"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 14, "context": "Deep neural networks are known to be over-parameterized, which facilitates convergence to good local minima of the loss function during training (Hinton et al., 2012; Denil et al., 2013).", "startOffset": 145, "endOffset": 186}, {"referenceID": 6, "context": "Deep neural networks are known to be over-parameterized, which facilitates convergence to good local minima of the loss function during training (Hinton et al., 2012; Denil et al., 2013).", "startOffset": 145, "endOffset": 186}, {"referenceID": 16, "context": "Recently, there are several studies to apply lowrank approximations to compress CNNs by exploiting redundancy (Jaderberg et al., 2014; Denton et al., 2014; Lebedev et al., 2015).", "startOffset": 110, "endOffset": 177}, {"referenceID": 7, "context": "Recently, there are several studies to apply lowrank approximations to compress CNNs by exploiting redundancy (Jaderberg et al., 2014; Denton et al., 2014; Lebedev et al., 2015).", "startOffset": 110, "endOffset": 177}, {"referenceID": 19, "context": "Recently, there are several studies to apply lowrank approximations to compress CNNs by exploiting redundancy (Jaderberg et al., 2014; Denton et al., 2014; Lebedev et al., 2015).", "startOffset": 110, "endOffset": 177}, {"referenceID": 28, "context": "Such compressions typically focus on convolution layers since they dominate total computation cost especially in deep neural networks (Simonyan & Zisserman, 2015; Szegedy et al., 2015).", "startOffset": 134, "endOffset": 184}, {"referenceID": 24, "context": "\u2022 In the proposed scheme, Tucker decomposition (Tucker, 1966) with the rank determined by a global analytic solution of variational Bayesian matrix factorization (VBMF) (Nakajima et al., 2012) is applied on each kernel tensor.", "startOffset": 169, "endOffset": 192}, {"referenceID": 0, "context": "\u2022 Each step of our scheme can be easily implemented using publicly available tools, (Nakajima, 2015) for VBMF, (Bader et al., 2015) for Tucker decomposition, and Caffe for finetuning.", "startOffset": 111, "endOffset": 131}, {"referenceID": 7, "context": "A recent study (Denton et al., 2014) showed that the weight matrix of a fully-connected layer can be compressed by applying truncated singular value decomposition (SVD) without significant drop in the prediction accuracy.", "startOffset": 15, "endOffset": 36}, {"referenceID": 9, "context": "More recently, various methods based on vector quantization (Gong et al., 2014), hashing techniques (Chen et al.", "startOffset": 60, "endOffset": 79}, {"referenceID": 2, "context": ", 2014), hashing techniques (Chen et al., 2015), circulant projection (Cheng et al.", "startOffset": 28, "endOffset": 47}, {"referenceID": 3, "context": ", 2015), circulant projection (Cheng et al., 2015), and tensor train decomposition (Novikov et al.", "startOffset": 30, "endOffset": 50}, {"referenceID": 7, "context": "To speed up the convolutional layers, several methods based on low-rank decomposition of convolutional kernel tensor were proposed (Denton et al., 2014; Jaderberg et al., 2014; Lebedev et al., 2015), but they compress only single or a few layers.", "startOffset": 131, "endOffset": 198}, {"referenceID": 16, "context": "To speed up the convolutional layers, several methods based on low-rank decomposition of convolutional kernel tensor were proposed (Denton et al., 2014; Jaderberg et al., 2014; Lebedev et al., 2015), but they compress only single or a few layers.", "startOffset": 131, "endOffset": 198}, {"referenceID": 19, "context": "To speed up the convolutional layers, several methods based on low-rank decomposition of convolutional kernel tensor were proposed (Denton et al., 2014; Jaderberg et al., 2014; Lebedev et al., 2015), but they compress only single or a few layers.", "startOffset": 131, "endOffset": 198}, {"referenceID": 4, "context": "After Denil et al. (2013) showed the possibility of removing the redundancy of neural networks, several CNN compression techniques have been proposed.", "startOffset": 6, "endOffset": 26}, {"referenceID": 2, "context": ", 2014), hashing techniques (Chen et al., 2015), circulant projection (Cheng et al., 2015), and tensor train decomposition (Novikov et al., 2015) were proposed and showed better compression capability than SVD. To speed up the convolutional layers, several methods based on low-rank decomposition of convolutional kernel tensor were proposed (Denton et al., 2014; Jaderberg et al., 2014; Lebedev et al., 2015), but they compress only single or a few layers. Concurrent with our work, Zhang et al. (2015b) presented \u201casymmetric (3d) decomposition\u201d to accelerate the entire convolutional layers, where the original D \u00d7 D convolution is decomposed to D \u00d7 1, 1 \u00d7 D, and 1 \u00d7 1 convolution.", "startOffset": 29, "endOffset": 505}, {"referenceID": 24, "context": "convolutional and fully-connected layers, (2) the kernel tensor reconstruction error is minimized instead of non-linear response, (3) a global analytic solution of VBMF (Nakajima et al., 2012) is applied to determine the rank of each layer, and (4) a single run of fine-tuning is performed to account for the accumulation of errors.", "startOffset": 169, "endOffset": 192}, {"referenceID": 21, "context": "The FFT method was used to speed-up convolution (Mathieu et al., 2013).", "startOffset": 48, "endOffset": 70}, {"referenceID": 31, "context": "In (Vanhoucke et al., 2011), CPU code optimizations to speed-up the execution of CNN are extensively explored.", "startOffset": 3, "endOffset": 27}, {"referenceID": 28, "context": ", 2014) and extensively used in inception module of GoogLeNet (Szegedy et al., 2015).", "startOffset": 62, "endOffset": 84}, {"referenceID": 7, "context": "Tucker vs CP: Recently, CP decomposition is applied to approximate the convolution layers of CNNs for ImageNet which consist of 8 layers (Denton et al., 2014; Lebedev et al., 2015).", "startOffset": 137, "endOffset": 180}, {"referenceID": 19, "context": "Tucker vs CP: Recently, CP decomposition is applied to approximate the convolution layers of CNNs for ImageNet which consist of 8 layers (Denton et al., 2014; Lebedev et al., 2015).", "startOffset": 137, "endOffset": 180}, {"referenceID": 19, "context": "However it cannot be applied to the entire layers and the instability issue of low-rank CP decomposition is reported (De Silva & Lim, 2008; Lebedev et al., 2015).", "startOffset": 117, "endOffset": 161}, {"referenceID": 25, "context": "We employed recently developed global analytic solutions for variational Bayesian matrix factorization (VBMF) (Nakajima et al., 2013).", "startOffset": 110, "endOffset": 133}, {"referenceID": 24, "context": "The global analytic VBMF is a very promising tool because it can automatically find noise variance, rank and even provide theoretical condition for perfect rank recovery (Nakajima et al., 2012).", "startOffset": 170, "endOffset": 193}, {"referenceID": 19, "context": "While (Lebedev et al., 2015; Zhang et al., 2015a) reported difficulty on finding a good SGD learning rate, our single learning rate scheduling rule works well for various compressed CNNs.", "startOffset": 6, "endOffset": 49}, {"referenceID": 13, "context": "We leave the use of other initialization methods (Glorot & Bengio, 2010; He et al., 2015) and batch normalization (Ioffe & Szegedy, 2015) as future work.", "startOffset": 49, "endOffset": 89}, {"referenceID": 33, "context": "Comparison with Zhang et al. (2015a)\u2019s method: The accuracy of our compressed VGG-16 is 89.", "startOffset": 16, "endOffset": 37}, {"referenceID": 1, "context": "When executing convolutions, we apply optimization techniques such as Caffeinated convolution(Chellapilla et al., 2006).", "startOffset": 93, "endOffset": 119}], "year": 2015, "abstractText": "Although the latest high-end smartphone has powerful CPU and GPU, running deeper convolutional neural networks (CNNs) for complex tasks such as ImageNet classification on mobile devices is challenging. To deploy deep CNNs on mobile devices, we present a simple and effective scheme to compress the entire CNN, which we call one-shot whole network compression. The proposed scheme consists of three steps: (1) rank selection with variational Bayesian matrix factorization, (2) Tucker decomposition on kernel tensor, and (3) fine-tuning to recover accumulated loss of accuracy, and each step can be easily implemented using publicly available tools. We demonstrate the effectiveness of the proposed scheme by testing the performance of various compressed CNNs (AlexNet, VGGS, GoogLeNet, and VGG-16) on the smartphone. Significant reductions in model size, runtime, and energy consumption are obtained, at the cost of small loss in accuracy. In addition, we address the important implementation level issue on 1\u00d7 1 convolution, which is a key operation of inception module of GoogLeNet as well as CNNs compressed by our proposed scheme.", "creator": "LaTeX with hyperref package"}}}