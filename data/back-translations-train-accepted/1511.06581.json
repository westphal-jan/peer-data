{"id": "1511.06581", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2015", "title": "Dueling Network Architectures for Deep Reinforcement Learning", "abstract": "In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning inspired by advantage learning. Our dueling architecture represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art Double DQN method of van Hasselt et al. (2015) in 46 out of 57 Atari games.", "histories": [["v1", "Fri, 20 Nov 2015 13:07:54 GMT  (507kb,D)", "http://arxiv.org/abs/1511.06581v1", "14 pages, 6 figures, and 5 tables"], ["v2", "Fri, 8 Jan 2016 11:37:42 GMT  (880kb,D)", "http://arxiv.org/abs/1511.06581v2", "14 pages, 6 figures, and 5 tables"], ["v3", "Tue, 5 Apr 2016 09:03:06 GMT  (695kb,D)", "http://arxiv.org/abs/1511.06581v3", "15 pages, 5 figures, and 5 tables"]], "COMMENTS": "14 pages, 6 figures, and 5 tables", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ziyu wang 0001", "tom schaul", "matteo hessel", "hado van hasselt", "marc lanctot", "nando de freitas"], "accepted": true, "id": "1511.06581"}, "pdf": {"name": "1511.06581.pdf", "metadata": {"source": "CRF", "title": "DUELING NETWORK ARCHITECTURES FOR DEEP REINFORCEMENT LEARNING", "authors": ["Ziyu Wang", "Nando de Freitas", "Marc Lanctot"], "emails": ["ziyu@google.com", "nandodefreitas@google.com", "lanctot@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "This year, it has come to the point that it will only be a matter of time before it is ready, until it is ready."}, {"heading": "1.1 RELATED WORK", "text": "The idea of maintaining separate value and benefit functions dates back to Baird (1993).In Baird's original benefit update algorithm, the Bellman common residual update equation is broken down into two updates: one for a state value function and one for the associated benefit function; the benefit update converges faster than Q-Learning in simple continuous time ranges (Harmon et al., 1995).Its successor, the advantage action algorithm, represents only a single benefit function (Harmon & Baird, 1996).The dueling architecture represents both the value V \u03c0 (s) and the advantage A\u03c0 (s, a) functions with a single deep model, the output of which combines the two to produce a state action value Q (s, a).Unlike the benefit update, the representation and the algorithm are decoupled by design. Consequently, the dueling architecture can be used in combination with a variety of model-free RL algorithms."}, {"heading": "2 BACKGROUND", "text": "We consider sequential decision-making, in which an agent interacts with an environment E through discrete time steps (see Sutton & Barto (1998) for an introduction. In the Atari domain, for example, the agent perceives a video camera consisting of M image frames: st = (xt \u2212 M + 1,..) The agent strives to maximize the expected discounted rate of return (defining the discounted rate of return as Rt = t value).In this formulation, [0, 1] is a discount factor that weighs the importance of immediate and future rewards. For an agent behaving according to a stochastic policy, the values of the state-action pair (s, a) and state-time are a discount factor that weighs the importance of immediate and future rewards."}, {"heading": "2.1 DEEP Q-NETWORKS", "text": "To estimate this network, we optimize the following sequence of loss functions in the iteration i: Li (\u03b8i) = Es, a, r, s \"[(yDQNi \u2212 Q (s, a; \u03b8i))) 2], (5) with yDQNi = r + \u03b3a\" Q \"(s,\" a \"; \u03b8 \u2212 i), (6), where it represents the parameters of a fixed and separate target network. We could try to learn the parameters of the network Q (s, a\") online, but in practice this estimator performs only insufficiently. A key innovation in the (Mnih et al., 2015) is the freezing of the parameters of the target network Q \"(s,\" a. \"i\"), that iteration of the iterators represents a significant improvement."}, {"heading": "2.2 DOUBLE DEEP Q-NETWORKS", "text": "In this paper, we use the improved Double DQN (DDQN) learning algorithm by van Hasselt et al. (2015). In Q-learning and DQN, the Max operator uses the same values to both select and evaluate an action, which can therefore lead to overoptimistic estimates (van Hasselt, 2010). To mitigate this problem, DDQN uses the following target: yDDQNi = r + \u03b3Q (s \u2032, argmax a \u2032 Q (s \u2032, a \u2032; \u03b8i), (9) The pseudo-code for DDQN is the same as for DQN (see Mnih et al. (2015)), but with the target yDQNi replaced by y DQN i."}, {"heading": "3 THE DUELING NETWORK ARCHITECTURE", "text": "The key insight behind our new architecture, as illustrated in Figure 2, is that for many states, it is unnecessary to estimate the value of each choice of action (Q = \u03b2). For example, in the Enduro game setting, when we know whether we are moving left or right, only matters when a collision is eminent. In some states, it is crucial to know what action to take, but in many other states, the action does not affect what happens. To bring this insight to fruition, we need to design a network architecture that is able to learn the value of a state regardless of the choice of action. In addition, this architecture must include a component that determines which actions are preferable in a given state. We achieve this by incorporating a decomposition of the Q network into two separate streams: a value stream and an advantage, as illustrated in Figure 1."}, {"heading": "4 EXPERIMENTS", "text": "We will now show the practical performance of the dueling network. We will start with a simple policy evaluation task and then show greater results for learning strategies for the general Atari game."}, {"heading": "4.1 POLICY EVALUATION", "text": "We start by measuring the performance of the Q values learned on a policy assessment task. This task is ideal for evaluating Q network architectures as it is free of disruptive factors, such as the choice of exploration strategy and the interaction between policy improvement and policy assessment. In this experiment, we use temporal difference learning (without authorization tracks, i.e., \u03bb = 0) to LearnQ values. More specifically, in the face of a behavioral policy \u03c0, we try to estimate the government action value Q\u03c0 (\u00b7 \u00b7) by optimizing the sequence of costs of the equation (5), with targetyi = r + \u03b3Ea \u00b2 values of operability (s \u2032, a \u00b2 values). The above updating rule is the same as that of the expected SARSA (van Seijen et al., 2009). However, we do not change the behavioral policy as in expected SARSA values. To evaluate the Q values learned, we choose a simple environment in which exact SARSA values are interconnected."}, {"heading": "4.2 GENERAL ATARI GAME-PLAYING", "text": "We are conducting a comprehensive evaluation of our proposed architectural streams. In detail, there are three complementary layers that we have fully connected with each other (Bellemare et al., 2013), which are composed of 57 Atari games. (The challenge is to deploy a single algorithm and a random number of hyperparameters to learn that all games containing only raw pixel observations and game rewards are very demanding because they both have a large number of very different games and the observations are highly dimensioned. (2015) and compare them with their results. Our network architecture has the same conventional structure of DQN (Mnih et al), 2015; van Hasselt et al.), but it includes the dueling streams described in Section 3 according to the conventional layers. We are duplicating the fully connected layers of the DQN architecture to which the architecture is constructed."}, {"heading": "5 CONCLUSIONS", "text": "The new architecture, combined with some algorithmic improvements, leads to dramatic improvements over existing approaches to deep RL in the challenging Atari domain. It has also been shown that the new architecture leads to performance improvements in policy evaluation. Furthermore, we found that gradient clipping is very effective in this area and recommend its use for the deep RL community. Since recent advances in deep RL are the result of algorithmic improvements (not architectural improvements), we believe that our dual architecture, in combination with these new algorithms, could lead to further improvements. Particularly promising is the combination of dual architecture with prioritized replay."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Hado van Hasselt, Arthur Guez, Vlad Mnih, Nicolas Hess, Marc Bellemare, Georg Ostrovski, Tom Schaul and all the people at Google DeepMind for making this possible."}, {"heading": "A APPENDIX", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Multiple object recognition with visual attention", "author": ["J. Ba", "V. Mnih", "K. Kavukcuoglu"], "venue": "In ICLR,", "citeRegEx": "Ba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2015}, {"title": "Advantage updating", "author": ["L.C. Baird"], "venue": "Technical Report WL-TR-93-1146, Wright-Patterson Air Force Base,", "citeRegEx": "Baird,? \\Q1993\\E", "shortCiteRegEx": "Baird", "year": 1993}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Increasing the action gap: New operators for reinforcement learning", "author": ["M.G. Bellemare", "G. Ostrovski", "A. Guez", "P.S. Thomas", "R. Munos"], "venue": "In AAAI,", "citeRegEx": "Bellemare et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2016}, {"title": "Advances in optimizing recurrent networks", "author": ["Y. Bengio", "N. Boulanger-Lewandowski", "R. Pascanu"], "venue": "In ICASSP,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position", "author": ["K. Fukushima"], "venue": "Biological Cybernetics,", "citeRegEx": "Fukushima,? \\Q1980\\E", "shortCiteRegEx": "Fukushima", "year": 1980}, {"title": "Deep learning for real-time Atari game play using offline Monte-Carlo tree search planning", "author": ["X. Guo", "S. Singh", "H. Lee", "R.L. Lewis", "X. Wang"], "venue": "In NIPS,", "citeRegEx": "Guo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Multi-player residual advantage learning with general function approximation", "author": ["M.E. Harmon", "L.C. Baird"], "venue": "Technical Report WL-TR-1065, Wright-Patterson Air Force Base,", "citeRegEx": "Harmon and Baird,? \\Q1996\\E", "shortCiteRegEx": "Harmon and Baird", "year": 1996}, {"title": "Advantage updating applied to a differential game", "author": ["M.E. Harmon", "L.C. Baird", "A.H. Klopf"], "venue": null, "citeRegEx": "Harmon et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Harmon et al\\.", "year": 1995}, {"title": "End-to-end training of deep visuomotor policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": "arXiv preprint arXiv:1504.00702,", "citeRegEx": "Levine et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2015}, {"title": "Reinforcement learning for robots using neural networks", "author": ["L.J. Lin"], "venue": "PhD thesis,", "citeRegEx": "Lin,? \\Q1993\\E", "shortCiteRegEx": "Lin", "year": 1993}, {"title": "Move Evaluation in Go Using Deep Convolutional Neural Networks", "author": ["C.J. Maddison", "A. Huang", "I. Sutskever", "D. Silver"], "venue": "In ICLR,", "citeRegEx": "Maddison et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Maddison et al\\.", "year": 2015}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Massively parallel methods for deep reinforcement learning", "author": ["A. Nair", "P. Srinivasan", "S. Blackwell", "C. Alcicek", "R. Fearon", "Maria", "A. De", "V. Panneershelvam", "M. Suleyman", "C. Beattie", "S. Petersen", "S. Legg", "V. Mnih", "K. Kavukcuoglu", "D. Silver"], "venue": "In Deep Learning Workshop,", "citeRegEx": "Nair et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2015}, {"title": "Prioritized experience replay", "author": ["T. Schaul", "J. Quan", "I. Antonoglou", "D. Silver"], "venue": "Technical report,", "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "High-dimensional continuous control using generalized advantage estimation", "author": ["J. Schulman", "P. Moritz", "S. Levine", "M.I. Jordan", "P. Abbeel"], "venue": "arXiv preprint arXiv:1506.02438,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "arXiv preprint arXiv:1312.6034,", "citeRegEx": "Simonyan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2013}, {"title": "Incentivizing exploration in reinforcement learning with deep predictive models", "author": ["B.C. Stadie", "S. Levine", "P. Abbeel"], "venue": "arXiv preprint arXiv:1507.00814,", "citeRegEx": "Stadie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Stadie et al\\.", "year": 2015}, {"title": "Introduction to reinforcement learning", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["R.S. Sutton", "D. Mcallester", "S. Singh", "Y. Mansour"], "venue": "In NIPS, pp", "citeRegEx": "Sutton et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2000}, {"title": "Deep reinforcement learning with double Q-learning", "author": ["H. van Hasselt", "A. Guez", "D. Silver"], "venue": "arXiv preprint arXiv:1509.06461,", "citeRegEx": "Hasselt et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2015}, {"title": "A theoretical and empirical analysis of Expected Sarsa", "author": ["H. van Seijen", "H. van Hasselt", "S. Whiteson", "M. Wiering"], "venue": "In IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning,", "citeRegEx": "Seijen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Seijen et al\\.", "year": 2009}, {"title": "Embed to control: A locally linear latent dynamics model for control from raw images", "author": ["M. Watter", "J.T. Springenberg", "J. Boedecker", "M.A. Riedmiller"], "venue": "In NIPS,", "citeRegEx": "Watter et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Watter et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 20, "context": "Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art Double DQN method of van Hasselt et al. (2015) in 46 out of 57 Atari games.", "startOffset": 116, "endOffset": 138}, {"referenceID": 12, "context": "Notable examples include deep Q-learning (Mnih et al., 2015), deep visuomotor policies (Levine et al.", "startOffset": 41, "endOffset": 60}, {"referenceID": 9, "context": ", 2015), deep visuomotor policies (Levine et al., 2015), attention with recurrent networks (Ba et al.", "startOffset": 34, "endOffset": 55}, {"referenceID": 0, "context": ", 2015), attention with recurrent networks (Ba et al., 2015), and model predictive control with embeddings (Watter et al.", "startOffset": 43, "endOffset": 60}, {"referenceID": 22, "context": ", 2015), and model predictive control with embeddings (Watter et al., 2015).", "startOffset": 54, "endOffset": 75}, {"referenceID": 13, "context": "Other recent successes include massively parallel frameworks (Nair et al., 2015) and expert move prediction in the game of Go (Maddison et al.", "startOffset": 61, "endOffset": 80}, {"referenceID": 11, "context": ", 2015) and expert move prediction in the game of Go (Maddison et al., 2015), which have produced policies matching those of Monte Carlo tree search programs.", "startOffset": 53, "endOffset": 76}, {"referenceID": 16, "context": "These maps were generated by computing the Jacobians of the trained value and advantage streams with respect to the input video, following the method proposed by Simonyan et al. (2013). (The experimental section describes this methodology in more detail.", "startOffset": 162, "endOffset": 185}, {"referenceID": 12, "context": "We show that our approach outperforms the baseline Deep Q-Networks (DQN) of Mnih et al. (2015) on 50 out of 57 games and the state-of-the-art Double DQN of van Hasselt et al.", "startOffset": 76, "endOffset": 95}, {"referenceID": 12, "context": "We show that our approach outperforms the baseline Deep Q-Networks (DQN) of Mnih et al. (2015) on 50 out of 57 games and the state-of-the-art Double DQN of van Hasselt et al. (2015)) on 46 out of 57 games.", "startOffset": 76, "endOffset": 182}, {"referenceID": 8, "context": "Advantage updating was shown to converge faster than Q-learning in simple continuous time domains in (Harmon et al., 1995).", "startOffset": 101, "endOffset": 122}, {"referenceID": 1, "context": "The notion of maintaining separate value and advantage functions goes back to Baird (1993). In Baird\u2019s original advantage updating algorithm, the shared Bellman residual update equation is decomposed into two updates: one for a state value function, and one for its associated advantage function.", "startOffset": 78, "endOffset": 91}, {"referenceID": 18, "context": "There is a long history of advantage functions in policy gradients, starting with Sutton et al. (2000). As a recent example of this line of work, Schulman et al.", "startOffset": 82, "endOffset": 103}, {"referenceID": 15, "context": "As a recent example of this line of work, Schulman et al. (2015) estimate advantage values online to reduce the variance of policy gradient algorithms.", "startOffset": 42, "endOffset": 65}, {"referenceID": 11, "context": "There have been several attempts at playing Atari with deep reinforcement learning, including Mnih et al. (2015); Guo et al.", "startOffset": 94, "endOffset": 113}, {"referenceID": 6, "context": "(2015); Guo et al. (2014); Stadie et al.", "startOffset": 8, "endOffset": 26}, {"referenceID": 6, "context": "(2015); Guo et al. (2014); Stadie et al. (2015); Nair et al.", "startOffset": 8, "endOffset": 48}, {"referenceID": 6, "context": "(2015); Guo et al. (2014); Stadie et al. (2015); Nair et al. (2015) and van Hasselt et al.", "startOffset": 8, "endOffset": 68}, {"referenceID": 6, "context": "(2015); Guo et al. (2014); Stadie et al. (2015); Nair et al. (2015) and van Hasselt et al. (2015). The results of van Hasselt et al.", "startOffset": 8, "endOffset": 98}, {"referenceID": 6, "context": "(2015); Guo et al. (2014); Stadie et al. (2015); Nair et al. (2015) and van Hasselt et al. (2015). The results of van Hasselt et al. (2015) are the current published state-of-the-art.", "startOffset": 8, "endOffset": 140}, {"referenceID": 3, "context": "(2015)) and in increasing the action gap (Bellemare et al., 2016) have also outperformed the state-of-the-art (van Hasselt et al.", "startOffset": 41, "endOffset": 65}, {"referenceID": 12, "context": "Simultaneously with our work, innovations in prioritized memory replay (see the co-submission by Schaul et al. (2015)) and in increasing the action gap (Bellemare et al.", "startOffset": 97, "endOffset": 118}, {"referenceID": 12, "context": "A key innovation in (Mnih et al., 2015) was to freeze the parameters of the target network Q(s\u2032, a\u2032; \u03b8\u2212 i ) for a fixed number of iterations while updating the", "startOffset": 20, "endOffset": 39}, {"referenceID": 10, "context": "Another key ingredient behind the success of DQN is experience replay (Lin, 1993; Mnih et al., 2015).", "startOffset": 70, "endOffset": 100}, {"referenceID": 12, "context": "Another key ingredient behind the success of DQN is experience replay (Lin, 1993; Mnih et al., 2015).", "startOffset": 70, "endOffset": 100}, {"referenceID": 12, "context": "The previous section described the main components of DQN as presented in (Mnih et al., 2015).", "startOffset": 74, "endOffset": 93}, {"referenceID": 12, "context": "The previous section described the main components of DQN as presented in (Mnih et al., 2015). In this paper, we use the improved Double DQN (DDQN) learning algorithm of van Hasselt et al. (2015). In Q-learning and DQN, the max operator uses the same values to both select and evaluate an action.", "startOffset": 75, "endOffset": 196}, {"referenceID": 12, "context": "The pseudo-code for DDQN is the same as for DQN (see Mnih et al. (2015)), but with the target y i replaced by y DDQN i .", "startOffset": 53, "endOffset": 72}, {"referenceID": 2, "context": "We perform a comprehensive evaluation of our proposed method on the Arcade Learning Environment (Bellemare et al., 2013), which is composed of 57 Atari games.", "startOffset": 96, "endOffset": 120}, {"referenceID": 12, "context": "Our network architecture has the same low-level convolutional structure of DQN (Mnih et al., 2015; van Hasselt et al., 2015), but it incorporates the dueling streams described in Section 3 after the convolutional layers.", "startOffset": 79, "endOffset": 124}, {"referenceID": 5, "context": "Rectifier non-linearities (Fukushima, 1980) are inserted between all adjacent layers.", "startOffset": 26, "endOffset": 43}, {"referenceID": 18, "context": "We follow closely the setup of van Hasselt et al. (2015) and compare to their results.", "startOffset": 35, "endOffset": 57}, {"referenceID": 4, "context": "This clipping is not standard practice in deep RL, but a common in recurrent network training (Bengio et al., 2013).", "startOffset": 94, "endOffset": 115}, {"referenceID": 19, "context": "We adopt the optimizers and hyper-parameters of van Hasselt et al. (2015), with the exception of the learning rate which we chose to be slightly lower (we do not do this for double DQN as it can deteriorate its performance).", "startOffset": 52, "endOffset": 74}, {"referenceID": 13, "context": "To obtain a more robust measure, we adopt the methodology of Nair et al. (2015). Specifically, for each game, we use 100 starting points sampled from a human expert\u2019s trajectory.", "startOffset": 61, "endOffset": 80}, {"referenceID": 16, "context": "To better understand the roles of the value and the advantage streams, we compute saliency maps (Simonyan et al., 2013).", "startOffset": 96, "endOffset": 119}], "year": 2015, "abstractText": "In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning inspired by advantage learning. Our dueling architecture represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art Double DQN method of van Hasselt et al. (2015) in 46 out of 57 Atari games.", "creator": "LaTeX with hyperref package"}}}