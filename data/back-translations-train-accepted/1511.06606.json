{"id": "1511.06606", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2015", "title": "Data Representation and Compression Using Linear-Programming Approximations", "abstract": "We propose `Dracula', a new framework for unsupervised feature selection from sequential data such as text. Dracula learns a dictionary of $n$-grams that efficiently compresses a given corpus and recursively compresses its own dictionary; in effect, Dracula is a `deep' extension of Compressive Feature Learning. It requires solving a binary linear program that may be relaxed to a linear program. Both problems exhibit considerable structure, their solution paths are well behaved, and we identify parameters which control the depth and diversity of the dictionary. We also discuss how to derive features from the compressed documents and show that while certain unregularized linear models are invariant to the structure of the compressed dictionary, this structure may be used to regularize learning. Experiments are presented that demonstrate the efficacy of Dracula's features.", "histories": [["v1", "Fri, 20 Nov 2015 14:21:44 GMT  (56kb,D)", "https://arxiv.org/abs/1511.06606v1", null], ["v2", "Tue, 19 Jan 2016 22:37:58 GMT  (60kb,D)", "http://arxiv.org/abs/1511.06606v2", null], ["v3", "Tue, 23 Feb 2016 23:13:18 GMT  (60kb,D)", "http://arxiv.org/abs/1511.06606v3", null], ["v4", "Mon, 2 May 2016 11:06:31 GMT  (190kb,D)", "http://arxiv.org/abs/1511.06606v4", null], ["v5", "Tue, 3 May 2016 01:02:04 GMT  (190kb,D)", "http://arxiv.org/abs/1511.06606v5", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hristo s paskov", "john c mitchell", "trevor j hastie"], "accepted": true, "id": "1511.06606"}, "pdf": {"name": "1511.06606.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Hristo S. Paskov", "John C. Mitchell"], "emails": ["hpaskov@stanford.edu", "jcm@stanford.edu", "hastie@stanford.edu"], "sections": [{"heading": null, "text": "We propose \"Dracula,\" a new framework for the unsupervised selection of attributes from sequential data such as text. Dracula learns a dictionary of n-grams that efficiently compresses a particular corpus and compresses its own dictionary recursively; in reality, Dracula is a \"deep\" extension of Compressive Feature Learning. It requires solving a binary linear program that can be loosened to a linear program. Both problems have a considerable structure, their solution paths are well thought out, and we identify parameters that control the depth and diversity of the dictionary. We also discuss how to derive attributes from the compressed documents, and show that while certain uncontrolled linear models are invariant to the structure of the compressed dictionary, this structure can be used to regulate learning. Experiments are presented that demonstrate the effectiveness of the attributes of Dracula."}, {"heading": "1 INTRODUCTION", "text": "In fact, most people are able to determine for themselves what they want and what they want. In fact, most people are able to determine for themselves what they want and what they don't want."}, {"heading": "2 DRACULA", "text": "This section introduces Dracula by showing how to extend CFL to a deep architecture that compresses its own dictionary elements. Also, we show how to interpret any Dracula solution as a directional acyclic graph (DAG) that specifies the term depth and provides useful statistical insights. Finally, we prove that Dracula NP is complete and discuss linear relaxation schemes. Notation In this paper, each sub-line of a Dk and C = {D1,..., DN} is a fixed document corpus containing any Dk-C pointer string Dk = ck1... pointer of characters cki that are used. A n-gram is any sub-line string of a Dk and S is the set of all n-grams in the document corpus, including the original documents. For each Dk-S-pointer-pointer-pointer-pointer-pointer-pointer-P."}, {"heading": "2.1 CFL", "text": "The CFL presents the problem by integrating the optimal pointer pairs into the C definitions. (1) The CFL presents the optimum pointer definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions-definitions"}, {"heading": "2.2 ADDING DEPTH WITH DRACULA", "text": "It is a basic shortcoming that reflects itself in way real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real"}, {"heading": "2.3 COMPUTATIONAL HARDNESS AND RELAXATION", "text": "The document and the dictionary reconstruction modules RDk / R are the basic building blocks of Dracula Q; when the dictionary t is fixed, solving the equation (5) is equivalent to solving the reconstruction modules separately.The discussion in the appendix section A.1 shows that for a fixed binary t, solving RDk or R's is simple due to the structure of the delimiting matrices XDk / Xs, because it is easy to find the corresponding dictionary t by checking which strings are used (in linear time relative to the number of pointers).One would hope that the difficulty of Dracula's subproblems leads to a simple general learning problem. However, learning the dictionaries and pointer sets at the same time makes this problem difficult: Dracula is NPComplete."}, {"heading": "3 LEARNING WITH COMPRESSED FEATURES", "text": "The first part of this section shows how to \"walk\" around the surface of the Dracula polyhedron and highlights some \"landmark\" compressions that are encountered, including those that lead to classic pouch or n-gram characteristics. Our discussion refers to both the binary and the relaxed versions of Dracula, as the former can be considered LP over a polyhedron, and highlights some \"landmark\" compressions that are encountered, including those that lead to classic pouch or n-gram characteristics. Our discussion refers to both the binary and the relaxed versions of Dracula, as the former can be viewed as LP over a polyhedron."}, {"heading": "3.1 DRACULA\u2019S SOLUTION PATH", "text": "s constraint polyhedron Q or the polyhedron of its relaxation. We use F (Q) to denote the number of faces of the polyhedron Q (including Q). We prove the following theorem in the appendix section A.3: theorem 1. Let Q (Rd) have a bounded polyhedron section: [0, 1] let Rd have a continuous function in the appendix section A.3: theorem 1. Let it have a bounded polyhedron with nonempty interior and b: [0, 1] let Rd have a continuous function in the appendix section."}, {"heading": "3.1.1 LANDMARKS ON DRACULA\u2019S POLYHEDRON", "text": "While Dracula's representations are typically deep and space-saving, it is important to note that valid Dracula solutions include all CFL solutions as well as a set of fully redundant representations that use as many pointers as possible. BoN characteristics, calculated from these \"space-maximizing\" compressions, result in the traditional BoN characteristics, which include all n-grams up to a maximum length K. A cost scheme that includes all pointers with all n-grams up to length K is obtained by setting all costs as negative except for all s-S, where | s | > K (to ban these strings) is used. Optimal compression then includes all pointers with negative costs, and each document position is reconstructed K times. Furthermore, it is possible to limit the representation to valid CFL solutions by forbidding all non-unigrammatic pointers for dictionary reconstruction, i.e., by forbidding c, if one string is not a p."}, {"heading": "3.2 DICTIONARY DIFFUSION", "text": "We discuss now how we let the information from a compression D = (S, P, P, P) flow into the BoN features. (S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S,"}, {"heading": "4 EXPERIMENTS", "text": "Our primary goal is to investigate whether deep compression can provide better characteristics for learning than flat compression or the traditional \"fully redundant\" BoN representation (using all n-grams up to a maximum length).Since each of these representations can be obtained from Dracula using a reasonable cost scheme, positive evidence of deep compression is revealed for both the hierarchical structure, which is useful for compression and learning at the same time. We also offer a level of compressed size that counts the number of pointers used by each representation, i.e. the result of evaluating each compression with a \"common sense\" spatial goal that shows all costs. We use Top to refer to BoN characteristics that count only document pointers (X in the previous section), Flat for dictionary diffusion characteristics (i.e. X), CFL characteristics for BoN characteristics, and all traditional Bocula characteristics are considered."}, {"heading": "5 CONCLUSION", "text": "Dracula expands CFL, which finds a flat dictionary of n-grams that can be used to compress a document corpus by applying the compression recursively to the dictionary, thereby learning a deep representation of the n-gram dictionary and document corpus. Experiments with biological, styometric, and natural language data confirm the usefulness of features derived from Dracula, suggesting that deep compression reveals relevant structures in the data. A variety of extensions are possible, the most immediate of which is the design of an algorithm that takes advantage of the problem structure in Dracula. We have identified the basic sub-problems that encompass Dracula, as well as the essential structure in these sub-problems that can be used to scale compression to large datasets. Ultimately, we hope to use Dracula to explore large and basic datasets, such as human genomes and their structures."}, {"heading": "ACKNOWLEDGEMENTS", "text": "Dedicated to Ivan i Kalinka Hand ievi (Ivan and Kalinka Handjievi), funded by the Office of Scientific Research of the Air Force and the National Science Foundation."}, {"heading": "A APPENDIX", "text": "It is so that we use the basic building blocks of Dracula (s) when it comes to the structure of Xsw (s). (t, V Dk) = RDk (t, c) and Ts (t, ts) (t, ts).P (s).P (s).P (s).P (s).P (s).P (s).P (s).P (s).P (s).P (s).P (s).P (s).P (s).P (s).P (s).P (s).P (s).P (s).P (s).P (s).P (s).P (s).P (s).P (s).P (s).S (t).S (t).S (t).S (t).S (t).S (t).S (t).S (t).S (t).S (t).S (t).S (s) (s).T (s).S (s).T (s).S (s)"}], "references": [{"title": "Optimization over integers", "author": ["Bertsimas", "Dimitris", "Weismantel", "Robert"], "venue": "Athena Scientific,", "citeRegEx": "Bertsimas et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bertsimas et al\\.", "year": 2005}, {"title": "Spam filtering using statistical data compression models", "author": ["Bratko", "Andrej", "Filipi\u010d", "Bogdan", "Cormack", "Gordon V", "Lynam", "Thomas R", "Zupan", "Bla\u017e"], "venue": null, "citeRegEx": "Bratko et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bratko et al\\.", "year": 2006}, {"title": "Text categorization with many redundant features: Using aggressive feature selection to make SVMs competitive with C4.5", "author": ["Gabrilovich", "Evgeniy", "Markovitch", "Shaul"], "venue": "In ICML,", "citeRegEx": "Gabrilovich et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Gabrilovich et al\\.", "year": 2004}, {"title": "Algorithms on Strings, Trees, and Sequences - Computer Science and Computational Biology", "author": ["Gusfield", "Dan"], "venue": null, "citeRegEx": "Gusfield and Dan.,? \\Q1997\\E", "shortCiteRegEx": "Gusfield and Dan.", "year": 1997}, {"title": "A tutorial on energy-based learning", "author": ["LeCun", "Yann", "Chopra", "Sumit", "Hadsell", "Raia", "Ranzato", "Marc\u2019Aurelio", "Huang", "Fu-Jie"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q2006\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2006}, {"title": "Normal fans of polyhedral convex sets", "author": ["Lu", "Shu", "Robinson", "Stephen M"], "venue": "Set-Valued Analysis,", "citeRegEx": "Lu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2008}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["Pang", "Bo", "Lee", "Lillian"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Pang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2005}, {"title": "Compressive feature learning", "author": ["Paskov", "Hristo", "West", "Robert", "Mitchell", "John", "Hastie", "Trevor"], "venue": "In NIPS,", "citeRegEx": "Paskov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Paskov et al\\.", "year": 2013}, {"title": "An efficient algorithm for large scale compressive feature learning", "author": ["Paskov", "Hristo", "Mitchell", "John", "Hastie", "Trevor"], "venue": "In AISTATS,", "citeRegEx": "Paskov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Paskov et al\\.", "year": 2014}, {"title": "Learning deep generative models", "author": ["Salakhutdinov", "Ruslan"], "venue": null, "citeRegEx": "Salakhutdinov and Ruslan.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov and Ruslan.", "year": 2009}, {"title": "Combinatorial Optimization - Polyhedra and Efficiency", "author": ["A. Schrijver"], "venue": null, "citeRegEx": "Schrijver,? \\Q2003\\E", "shortCiteRegEx": "Schrijver", "year": 2003}, {"title": "Deep learning for NLP (without magic)", "author": ["Socher", "Richard", "Manning", "Christopher D"], "venue": "In Conference of the North American Chapter of the Association of Computational Linguistics,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["Wang", "Sida", "Manning", "Christopher D"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "A universal algorithm for sequential data", "author": ["Ziv", "Jacob", "Lempel", "Abraham"], "venue": "compression. TIT,", "citeRegEx": "Ziv et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Ziv et al\\.", "year": 1977}], "referenceMentions": [{"referenceID": 1, "context": "Meanwhile, off-the-shelf compressors, such as LZ-77 (Ziv & Lempel (1977)), have been successfully applied to natural language problems as kernels that compute pairwise document similarities (Bratko et al. (2006)).", "startOffset": 191, "endOffset": 212}, {"referenceID": 7, "context": "This recursion makes Dracula a deep extension of Compressive Feature Learning (CFL) (Paskov et al. (2013)) that can find exponentially smaller representations and promotes similar n-grams to enter the dictionary.", "startOffset": 85, "endOffset": 106}, {"referenceID": 7, "context": "This recursion makes Dracula a deep extension of Compressive Feature Learning (CFL) (Paskov et al. (2013)) that can find exponentially smaller representations and promotes similar n-grams to enter the dictionary. As noted in (Paskov et al. (2013)), feature representations derived from off-the-shelf compressors are inferior because the algorithms used are sensitive to document order; both Dracula and CFL are invariant to document order.", "startOffset": 85, "endOffset": 247}, {"referenceID": 4, "context": "This is a notable departure from traditional deep learners (Salakhutdinov (2009); Socher & Manning (2013); LeCun et al. (2006)), which are formulated as non-convex, non-linear optimization problems.", "startOffset": 107, "endOffset": 127}, {"referenceID": 7, "context": "In particular, it requires solving a binary LP (which are NP-Complete in general) and it generalizes CFL which is itself NP-Complete (Paskov et al. (2014)) (see section 3.", "startOffset": 134, "endOffset": 155}, {"referenceID": 10, "context": "In fact, the Chv\u00e1tal-Gomory theorem (Schrijver (2003)) shows that we may \u201cprune\u201d", "startOffset": 37, "endOffset": 54}], "year": 2016, "abstractText": "We propose \u2018Dracula\u2019, a new framework for unsupervised feature selection from sequential data such as text. Dracula learns a dictionary of n-grams that efficiently compresses a given corpus and recursively compresses its own dictionary; in effect, Dracula is a \u2018deep\u2019 extension of Compressive Feature Learning. It requires solving a binary linear program that may be relaxed to a linear program. Both problems exhibit considerable structure, their solution paths are well behaved, and we identify parameters which control the depth and diversity of the dictionary. We also discuss how to derive features from the compressed documents and show that while certain unregularized linear models are invariant to the structure of the compressed dictionary, this structure may be used to regularize learning. Experiments are presented that demonstrate the efficacy of Dracula\u2019s features.", "creator": "LaTeX with hyperref package"}}}