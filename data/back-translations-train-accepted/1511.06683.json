{"id": "1511.06683", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2015", "title": "Top-k Multiclass SVM", "abstract": "Class ambiguity is typical in image classification problems with a large number of classes. When classes are difficult to discriminate, it makes sense to allow k guesses and evaluate classifiers based on the top-k error instead of the standard zero-one loss. We propose top-k multiclass SVM as a direct method to optimize for top-k performance. Our generalization of the well-known multiclass SVM is based on a tight convex upper bound of the top-k error. We propose a fast optimization scheme based on an efficient projection onto the top-k simplex, which is of its own interest. Experiments on five datasets show consistent improvements in top-k accuracy compared to various baselines.", "histories": [["v1", "Fri, 20 Nov 2015 16:49:33 GMT  (675kb,D)", "http://arxiv.org/abs/1511.06683v1", "NIPS 2015"]], "COMMENTS": "NIPS 2015", "reviews": [], "SUBJECTS": "stat.ML cs.CV cs.LG", "authors": ["maksim lapin", "matthias hein 0001", "bernt schiele"], "accepted": true, "id": "1511.06683"}, "pdf": {"name": "1511.06683.pdf", "metadata": {"source": "CRF", "title": "Top-k Multiclass SVM", "authors": ["Maksim Lapin", "Matthias Hein", "Bernt Schiele"], "emails": [], "sections": [{"heading": null, "text": "This year is the highest in the history of the country."}, {"heading": "2.1 Multiclass Support Vector Machine", "text": "In this section, we review the multi-class SVM of Crammer and Singer (1), which is extended to the top-k multi-class SVM (1). We mainly follow the notation of [27]. (1) Since our optimization scheme is based on fennel duality, we also need a convex conjugation of the primary loss function (1). Let's c, 1 \u2212 eyi, where 1 is the all-one vector and ej-th is the default base vector in Rm, let's define a component as aj, < wj, xi > wyi, and let. (2), and let's leave the all-one vector and ej-th standard base vector in Rm."}, {"heading": "3 Optimization Framework", "text": "We start with a general \"2-regulated\" classification, which is then applied in practice. (<) We start with a general \"2-regulated\" classification, in which we do not specify the loss function. (<) We do not stop the loss function. (<) We start with a general \"2-regulated\" classification, in which we do not specify the loss function. (<) We do not specify the loss function as \"2-regulated.\" (<) We assume that we cannot apply the matrix of the dual variables. (Before we can prove our main results from these sections, we must first impose a technical limitation on a loss function compatible with the choice of the basic truth coordinate."}, {"heading": "3.2.1 Dual Variables Update", "text": "For the proposed top-k hinge loss in section 2, optimizing the dual target D (A) < < K (A) via the given other variables is an example of a regulated (tendentious) projection problem on the top-k-simplex-simplex-simplex-simplex-simplex-simplex-simplex-simplex. Get a\\ j by removing the j-th coordinate from vector a.Proposition 4. The following two problems are equivalent to a\\ yii = \u2212 x and ayi, i = < 1, x > max ai {D (A) | < 1, ai > = 0}."}, {"heading": "4.1 Continuous Quadratic Knapsack Problem", "text": "The finding of the Euclidean projection on the simplex problem is an example of the general optimization problem minx = > problem. < < b = 0 and r = 1) This is a well-studied problem and several highly efficient algorithms are available (see the surveys [20, 21]).The first main difference to our approach is the upper limit to the xi's. All existing algorithms expect u = 0 and r = 1 to be fixed, which allows them to solve the decompositions minxi (ai \u2212 xi) 2, which can be solved in a closed form. In our case, the upper limit is 1k < 1, x > the coupling runs across all variables, making the existing algorithms inapplicable."}, {"heading": "6 Experimental Results", "text": "We have two main objectives in the experiments: First, we show that the (distorted) projection on the top k simplex is scalable and comparable to an efficient algorithm [14] for the simplex projection. Second, we show that the top k multi-class SVM, using both versions of the top k hinge loss (3) and (5), which are referred to as the top k SVM\u03b1 and top k SVM\u03b2 respectively, leads to improvements in the top k accuracy across all data sets and choices of k. In particular, we note improvements compared to the top 1 SVM\u03b1 / top k SVM\u03b2 multiclass SVM by Crammer and Singer [6], which corresponds to the top 1 SVM\u03b1 / top-1 SVM\u03b2. We publish our implementation of the projection procedures and both SDCA solvers as C + + library2 with a Matlab interface. 2https: / / github.com / mlapin / libsca6.Scalizing the projection of the basic C + 1 is a top x problem."}, {"heading": "6.2 Image Classification Experiments", "text": "We evaluate our method using five image classification datasets of varying magnitude and complexity: Caltech 101 Silhouettes [29] (m = 101, n = 4100), MIT Indoor 67 [23] (m = 67, n = 5354), SUN 397 [32] (m = 397, n = 19850), Orte 205 [33] (m = 205, n = 2448873) and ImageNet 2012 [25] (m = 1000, n = 1281167) for Caltech, d = 784, and for the others d = 4096. Results of the two large datasets are in completion. We validate hyperparameters in the range 10 \u2212 5 to 103 and extend them when the optimal value is at the limit. We use LibLinear [8] for SVMOVA, SVMPerf [12] with the corresponding loss function for Recall @ k, and the code we do not discuss for the Topk method as we do."}, {"heading": "7 Conclusion", "text": "We demonstrated the scalability and effectiveness of the proposed Top-k multiclass SVM using five image recognition data sets, which resulted in consistent improvements in Top-k performance. In the future, it could be investigated whether the Top-k hinge loss (3) can be generalized to the family of rank losses [30]. Similar to the Top-k loss, this could lead to tighter convex ceilings of the corresponding discrete losses."}], "references": [{"title": "Solving multiclass support vector machines with LaRank", "author": ["A. Bordes", "L. Bottou", "P. Gallinari", "J. Weston"], "venue": "ICML, pages 89\u201396", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Convex Analysis and Nonlinear Optimization: Theory and Examples", "author": ["J.M. Borwein", "A.S. Lewis"], "venue": "Cms Books in Mathematics Series. Springer Verlag", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "The tradeoffs of large scale learning", "author": ["O. Bousquet", "L. Bottou"], "venue": "NIPS, pages 161\u2013168", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": "Cambridge University Press", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "Superpixel segmentation based structural scene recognition", "author": ["S. Bu", "Z. Liu", "J. Han", "J. Wu"], "venue": "MM, pages 681\u2013684. ACM", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "On the algorithmic implementation of multiclass kernel-based vector machines", "author": ["K. Crammer", "Y. Singer"], "venue": "The Journal of Machine Learning Research, 2:265\u2013292", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "Mid-level visual element discovery as discriminative mode seeking", "author": ["C. Doersch", "A. Gupta", "A.A. Efros"], "venue": "NIPS, pages 494\u2013502", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin"], "venue": "Journal of Machine Learning Research, 9:1871\u20131874", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-scale orderless pooling of deep convolutional activation features", "author": ["Y. Gong", "L. Wang", "R. Guo", "S. Lazebnik"], "venue": "ECCV", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Training highly multiclass classifiers", "author": ["M.R. Gupta", "S. Bengio", "J. Weston"], "venue": "JMLR, 15:1461\u20131492", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "A support vector method for multivariate performance measures", "author": ["T. Joachims"], "venue": "ICML, pages 377\u2013384", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "Blocks that shout: distinctive parts for scene classification", "author": ["M. Juneja", "A. Vedaldi", "C. Jawahar", "A. Zisserman"], "venue": "CVPR", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Variable fixing algorithms for the continuous quadratic knapsack problem", "author": ["K. Kiwiel"], "venue": "Journal of Optimization Theory and Applications, 136(3):445\u2013458", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Convolutional network features for scene recognition", "author": ["M. Koskela", "J. Laaksonen"], "venue": "Proceedings of the ACM International Conference on Multimedia, pages 1169\u20131172. ACM", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Scalable multitask representation learning for scene classification", "author": ["M. Lapin", "B. Schiele", "M. Hein"], "venue": "CVPR", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Top rank optimization in linear time", "author": ["N. Li", "R. Jin", "Z.-H. Zhou"], "venue": "NIPS, pages 1502\u20131510", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient euclidean projections in linear time", "author": ["J. Liu", "J. Ye"], "venue": "ICML, pages 657\u2013664", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Minimizing the sum of the k largest functions in linear time", "author": ["W. Ogryczak", "A. Tamir"], "venue": "Information Processing Letters, 85(3):117\u2013122", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "A survey on the continuous nonlinear resource allocation problem", "author": ["M. Patriksson"], "venue": "European Journal of Operational Research, 185(1):1\u201346", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Algorithms for the continuous nonlinear resource allocation problem \u2013 new implementations and numerical studies", "author": ["M. Patriksson", "C. Str\u00f6mberg"], "venue": "European Journal of Operational Research, 243(3):703\u2013 722", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["K.B. Petersen", "M.S. Pedersen"], "venue": "The matrix cookbook. Technical University of Denmark, 450:7\u201315", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "Recognizing indoor scenes", "author": ["A. Quattoni", "A. Torralba"], "venue": "CVPR", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Cnn features off-the-shelf: an astounding baseline for recognition", "author": ["A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "arXiv preprint arXiv:1403.6382", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "and L", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg"], "venue": "Fei-Fei. ImageNet Large Scale Visual Recognition Challenge", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Image classification with the Fisher vector: theory and practice", "author": ["J. S\u00e1nchez", "F. Perronnin", "T. Mensink", "J. Verbeek"], "venue": "IJCV, pages 1\u201324", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization", "author": ["S. Shalev-Shwartz", "T. Zhang"], "venue": "Mathematical Programming, pages 1\u201341", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning discriminative part detectors for image classification and cosegmentation", "author": ["J. Sun", "J. Ponce"], "venue": "ICCV, pages 3400\u20133407", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Probabilistic n-choose-k models for classification and ranking", "author": ["K. Swersky", "B.J. Frey", "D. Tarlow", "R.S. Zemel", "R.P. Adams"], "venue": "NIPS, pages 3050\u20133058", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Ranking with ordered weighted pairwise classification", "author": ["N. Usunier", "D. Buffoni", "P. Gallinari"], "venue": "ICML, pages 1057\u20131064", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Wsabie: scaling up to large vocabulary image annotation", "author": ["J. Weston", "S. Bengio", "N. Usunier"], "venue": "IJCAI, pages 2764\u20132770", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "SUN database: Large-scale scene recognition from abbey to zoo", "author": ["J. Xiao", "J. Hays", "K.A. Ehinger", "A. Oliva", "A. Torralba"], "venue": "CVPR", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning deep features for scene recognition using places database", "author": ["B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "NIPS", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 31, "context": "Figure 1: Images from SUN 397 [32] illustrating class ambiguity.", "startOffset": 30, "endOffset": 34}, {"referenceID": 9, "context": "As the number of classes increases, two important issues emerge: class overlap and multilabel nature of examples [10].", "startOffset": 113, "endOffset": 117}, {"referenceID": 5, "context": "We propose top-k multiclass SVM as a generalization of the well-known multiclass SVM [6].", "startOffset": 85, "endOffset": 88}, {"referenceID": 29, "context": "While it turns out to be similar to a top-k version of the ranking based loss proposed by [30], we show that the top-k hinge loss is a lower bound on their version and is thus a tighter bound on the top-k zero-one loss.", "startOffset": 90, "endOffset": 94}, {"referenceID": 26, "context": "We propose an efficient implementation based on stochastic dual coordinate ascent (SDCA) [27].", "startOffset": 89, "endOffset": 93}, {"referenceID": 32, "context": "5 million examples and 205 classes [33].", "startOffset": 35, "endOffset": 39}, {"referenceID": 11, "context": "Finally, extensive experiments on several challenging computer vision problems show that top-k multiclass SVM consistently improves in top-k error over the multiclass SVM (equivalent to our top-1 multiclass SVM), one-vs-all SVM and other methods based on different ranking losses [12, 17].", "startOffset": 280, "endOffset": 288}, {"referenceID": 16, "context": "Finally, extensive experiments on several challenging computer vision problems show that top-k multiclass SVM consistently improves in top-k error over the multiclass SVM (equivalent to our top-1 multiclass SVM), one-vs-all SVM and other methods based on different ranking losses [12, 17].", "startOffset": 280, "endOffset": 288}, {"referenceID": 0, "context": "\u3008 w[1], x \u3009 \u2265 \u3008 w[2], x \u3009 \u2265 .", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "\u3008 w[1], x \u3009 \u2265 \u3008 w[2], x \u3009 \u2265 .", "startOffset": 17, "endOffset": 20}, {"referenceID": 5, "context": "In this section we review the multiclass SVM of Crammer and Singer [6] which will be extended to the top-k multiclass SVM in the following.", "startOffset": 67, "endOffset": 70}, {"referenceID": 26, "context": "We mainly follow the notation of [27].", "startOffset": 33, "endOffset": 37}, {"referenceID": 26, "context": "Proposition 1 ([27], \u00a7 5.", "startOffset": 15, "endOffset": 19}, {"referenceID": 0, "context": "\u03c6(a) = max{0, (a+ c)[1]}, \u03c6\u2217(b) = { \u2212\u3008c, b\u3009 if b \u2208 \u2206, +\u221e otherwise.", "startOffset": 20, "endOffset": 23}, {"referenceID": 0, "context": "Note that thresholding with 0 in \u03c6(a) is actually redundant as (a+ c)[1] \u2265 (a+ c)yi = 0 and is only given to enhance similarity to the top-k version defined later.", "startOffset": 69, "endOffset": 72}, {"referenceID": 0, "context": "Since the ground truth score (a+ c)[yi] = 0, we conclude that \u03c8k(a) > 0 \u21d0\u21d2 \u3008 w[1], xi \u3009 \u2265 .", "startOffset": 78, "endOffset": 81}, {"referenceID": 3, "context": "where the sum of the k largest components is known to be convex [4].", "startOffset": 64, "endOffset": 67}, {"referenceID": 18, "context": "Lemma 1 ([19], Lemma 1).", "startOffset": 9, "endOffset": 13}, {"referenceID": 5, "context": "Therefore, we see that the proposed formulation (3) naturally extends the multiclass SVM of Crammer and Singer [6], which is recovered when k = 1.", "startOffset": 111, "endOffset": 114}, {"referenceID": 29, "context": "[30] have recently formulated a very general family of convex losses for ranking and multiclass classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "The bound \u03c6k(a) \u2264 \u03c6\u0303k(a) holds with equality if (a+ c)[1] \u2264 0 or (a+ c)[k] \u2265 0.", "startOffset": 54, "endOffset": 57}, {"referenceID": 29, "context": "While [30] employed LaRank [1] and [10], [31] optimized an approximation of L\u03b2(a), we show in \u00a7 5 how the loss function (5) can be optimized exactly and efficiently within the Prox-SDCA framework.", "startOffset": 6, "endOffset": 10}, {"referenceID": 0, "context": "While [30] employed LaRank [1] and [10], [31] optimized an approximation of L\u03b2(a), we show in \u00a7 5 how the loss function (5) can be optimized exactly and efficiently within the Prox-SDCA framework.", "startOffset": 27, "endOffset": 30}, {"referenceID": 9, "context": "While [30] employed LaRank [1] and [10], [31] optimized an approximation of L\u03b2(a), we show in \u00a7 5 how the loss function (5) can be optimized exactly and efficiently within the Prox-SDCA framework.", "startOffset": 35, "endOffset": 39}, {"referenceID": 30, "context": "While [30] employed LaRank [1] and [10], [31] optimized an approximation of L\u03b2(a), we show in \u00a7 5 how the loss function (5) can be optimized exactly and efficiently within the Prox-SDCA framework.", "startOffset": 41, "endOffset": 45}, {"referenceID": 11, "context": "We employ it in our experiments to evaluate the ranking based methods SVM [12] and TopPush [17].", "startOffset": 74, "endOffset": 78}, {"referenceID": 16, "context": "We employ it in our experiments to evaluate the ranking based methods SVM [12] and TopPush [17].", "startOffset": 91, "endOffset": 95}, {"referenceID": 11, "context": "Another approach to general performance measures is given in [12].", "startOffset": 61, "endOffset": 65}, {"referenceID": 11, "context": "A convex upper bound on recall@k is then optimized in [12] via structured SVM.", "startOffset": 54, "endOffset": 58}, {"referenceID": 11, "context": "While being theoretically very elegant, the approach of [12] does not scale to very large datasets.", "startOffset": 56, "endOffset": 60}, {"referenceID": 21, "context": "Using rank-1 update of the Moore-Penrose pseudoinverse ([22], \u00a7 3.", "startOffset": 56, "endOffset": 60}, {"referenceID": 1, "context": "[2], Theorem 3.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "To complete the proof, we redefine A\u2190 1 \u03bbA for convenience, and use the first order optimality condition ([2], Ex.", "startOffset": 106, "endOffset": 109}, {"referenceID": 26, "context": "7 in [27] as there is a typo in the optimization problem (20) leading to the conclusion that ayi,i must be 0 at the optimum.", "startOffset": 5, "endOffset": 9}, {"referenceID": 26, "context": "1 for details} 10: W \u2190W + xi(ai \u2212 a i )> {rank-1 update} 11: end for 12: until relative duality gap is below As an optimization scheme, we employ the proximal stochastic dual coordinate ascent (Prox-SDCA) framework of Shalev-Shwartz and Zhang [27], which has strong convergence guarantees and is easy to adapt to our problem.", "startOffset": 243, "endOffset": 247}, {"referenceID": 2, "context": "Finally, we have a well-defined stopping criterion as we can compute the duality gap (see discussion in [3]).", "startOffset": 104, "endOffset": 107}, {"referenceID": 19, "context": "This is a well examined problem and several highly efficient algorithms are available (see the surveys [20, 21]).", "startOffset": 103, "endOffset": 111}, {"referenceID": 20, "context": "This is a well examined problem and several highly efficient algorithms are available (see the surveys [20, 21]).", "startOffset": 103, "endOffset": 111}, {"referenceID": 13, "context": "We choose [14] since it is easy to implement, does not require sorting, and scales linearly in practice.", "startOffset": 10, "endOffset": 14}, {"referenceID": 0, "context": "If U 6= \u2205 and M = \u2205, then U = {[1], .", "startOffset": 31, "endOffset": 34}, {"referenceID": 13, "context": "First, we solve the knapsack problem using the algorithm of [14], which also computes the dual variable t.", "startOffset": 60, "endOffset": 64}, {"referenceID": 28, "context": "Top-1 [29] 62.", "startOffset": 6, "endOffset": 10}, {"referenceID": 4, "context": "1 - BLH [5] 48.", "startOffset": 8, "endOffset": 11}, {"referenceID": 6, "context": "3 DGE [7] 66.", "startOffset": 6, "endOffset": 9}, {"referenceID": 23, "context": "87 RAS [24] 69.", "startOffset": 7, "endOffset": 11}, {"referenceID": 28, "context": "0 Top-2 [29] 61.", "startOffset": 8, "endOffset": 12}, {"referenceID": 27, "context": "4 - SP [28] 51.", "startOffset": 7, "endOffset": 11}, {"referenceID": 32, "context": "4 ZLX [33] 68.", "startOffset": 6, "endOffset": 10}, {"referenceID": 14, "context": "24 KL [15] 70.", "startOffset": 6, "endOffset": 10}, {"referenceID": 28, "context": "1 Top-5 [29] 60.", "startOffset": 8, "endOffset": 12}, {"referenceID": 12, "context": "4 - JVJ [13] 63.", "startOffset": 8, "endOffset": 12}, {"referenceID": 8, "context": "10 GWG [9] 68.", "startOffset": 7, "endOffset": 10}, {"referenceID": 11, "context": "Prec@k and Recall@k are SVM [12]; W++,Q/m is Wsabie [10] with an embedding dimension m and the queue size Q; in the first part, m = 101 for Caltech and m = 67 for Indoor.", "startOffset": 28, "endOffset": 32}, {"referenceID": 9, "context": "Prec@k and Recall@k are SVM [12]; W++,Q/m is Wsabie [10] with an embedding dimension m and the queue size Q; in the first part, m = 101 for Caltech and m = 67 for Indoor.", "startOffset": 52, "endOffset": 56}, {"referenceID": 13, "context": "First, we show that the (biased) projection onto the top-k simplex is scalable and comparable to an efficient algorithm [14] for the simplex projection.", "startOffset": 120, "endOffset": 124}, {"referenceID": 5, "context": "In particular, we note improvements compared to the multiclass SVM of Crammer and Singer [6], which corresponds to top-1 SVM\u03b1/top-1 SVM\u03b2 .", "startOffset": 89, "endOffset": 92}, {"referenceID": 31, "context": "SUN 397 (10 splits) Top-1 accuracy XHE [32] 38.", "startOffset": 39, "endOffset": 43}, {"referenceID": 15, "context": "0 LSH [16] 49.", "startOffset": 6, "endOffset": 10}, {"referenceID": 32, "context": "3 ZLX [33] 54.", "startOffset": 6, "endOffset": 10}, {"referenceID": 25, "context": "1 SPM [26] 47.", "startOffset": 6, "endOffset": 10}, {"referenceID": 8, "context": "2 GWG [9] 51.", "startOffset": 6, "endOffset": 9}, {"referenceID": 14, "context": "98 KL [15] 54.", "startOffset": 6, "endOffset": 10}, {"referenceID": 32, "context": "ZLX [33] / BVLC [11] 50.", "startOffset": 4, "endOffset": 8}, {"referenceID": 10, "context": "ZLX [33] / BVLC [11] 50.", "startOffset": 16, "endOffset": 20}, {"referenceID": 17, "context": "We follow the experimental setup of [18].", "startOffset": 36, "endOffset": 40}, {"referenceID": 13, "context": "We sample 1000 points from the normal distribution N (0, 1) and solve the projection problems using the algorithm of [14] (denoted as Knapsack) and using our proposed method of projecting onto the set \u2206k for different values of k = 1, 5, 10.", "startOffset": 117, "endOffset": 121}, {"referenceID": 28, "context": "We evaluate our method on five image classification datasets of different scale and complexity: Caltech 101 Silhouettes [29] (m = 101, n = 4100), MIT Indoor 67 [23] (m = 67, n = 5354), SUN 397 [32] (m = 397, n = 19850), Places 205 [33] (m = 205, n = 2448873), and ImageNet 2012 [25] (m = 1000, n = 1281167).", "startOffset": 120, "endOffset": 124}, {"referenceID": 22, "context": "We evaluate our method on five image classification datasets of different scale and complexity: Caltech 101 Silhouettes [29] (m = 101, n = 4100), MIT Indoor 67 [23] (m = 67, n = 5354), SUN 397 [32] (m = 397, n = 19850), Places 205 [33] (m = 205, n = 2448873), and ImageNet 2012 [25] (m = 1000, n = 1281167).", "startOffset": 160, "endOffset": 164}, {"referenceID": 31, "context": "We evaluate our method on five image classification datasets of different scale and complexity: Caltech 101 Silhouettes [29] (m = 101, n = 4100), MIT Indoor 67 [23] (m = 67, n = 5354), SUN 397 [32] (m = 397, n = 19850), Places 205 [33] (m = 205, n = 2448873), and ImageNet 2012 [25] (m = 1000, n = 1281167).", "startOffset": 193, "endOffset": 197}, {"referenceID": 32, "context": "We evaluate our method on five image classification datasets of different scale and complexity: Caltech 101 Silhouettes [29] (m = 101, n = 4100), MIT Indoor 67 [23] (m = 67, n = 5354), SUN 397 [32] (m = 397, n = 19850), Places 205 [33] (m = 205, n = 2448873), and ImageNet 2012 [25] (m = 1000, n = 1281167).", "startOffset": 231, "endOffset": 235}, {"referenceID": 24, "context": "We evaluate our method on five image classification datasets of different scale and complexity: Caltech 101 Silhouettes [29] (m = 101, n = 4100), MIT Indoor 67 [23] (m = 67, n = 5354), SUN 397 [32] (m = 397, n = 19850), Places 205 [33] (m = 205, n = 2448873), and ImageNet 2012 [25] (m = 1000, n = 1281167).", "startOffset": 278, "endOffset": 282}, {"referenceID": 7, "context": "We use LibLinear [8] for SVM, SVM [12] with the corresponding loss function for Recall@k, and the code provided by [17] for TopPush.", "startOffset": 17, "endOffset": 20}, {"referenceID": 11, "context": "We use LibLinear [8] for SVM, SVM [12] with the corresponding loss function for Recall@k, and the code provided by [17] for TopPush.", "startOffset": 34, "endOffset": 38}, {"referenceID": 16, "context": "We use LibLinear [8] for SVM, SVM [12] with the corresponding loss function for Recall@k, and the code provided by [17] for TopPush.", "startOffset": 115, "endOffset": 119}, {"referenceID": 9, "context": "We implemented Wsabie (denoted W++,Q/m) based on the pseudo-code from Table 3 in [10].", "startOffset": 81, "endOffset": 85}, {"referenceID": 28, "context": "On Caltech 101, we use features provided by [29].", "startOffset": 44, "endOffset": 48}, {"referenceID": 32, "context": "For the scene recognition datasets, we use the Places 205 CNN [33] and for ILSVRC 2012 we use the Caffe reference model [11].", "startOffset": 62, "endOffset": 66}, {"referenceID": 10, "context": "For the scene recognition datasets, we use the Places 205 CNN [33] and for ILSVRC 2012 we use the Caffe reference model [11].", "startOffset": 120, "endOffset": 124}, {"referenceID": 29, "context": "In the future, one could study if the top-k hinge loss (3) can be generalized to the family of ranking losses [30].", "startOffset": 110, "endOffset": 114}], "year": 2015, "abstractText": "Class ambiguity is typical in image classification problems with a large number of classes. When classes are difficult to discriminate, it makes sense to allow k guesses and evaluate classifiers based on the top-k error instead of the standard zero-one loss. We propose top-k multiclass SVM as a direct method to optimize for top-k performance. Our generalization of the well-known multiclass SVM is based on a tight convex upper bound of the top-k error. We propose a fast optimization scheme based on an efficient projection onto the top-k simplex, which is of its own interest. Experiments on five datasets show consistent improvements in top-k accuracy compared to various baselines.", "creator": "LaTeX with hyperref package"}}}