{"id": "1511.06709", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2015", "title": "Improving Neural Machine Translation Models with Monolingual Data", "abstract": "Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data for training. Monolingual data plays an important role in boosting fluency for phrase-based statistical machine translation, and we investigate the use of monolingual data for neural machine translation (NMT). In contrast to previous work, which integrates a separately trained RNN language model into an NMT architecture, we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model, and we explore strategies to include monolingual training data in the training process. Through our use of monolingual data, we obtain substantial improvements on the WMT 15 (+2.8--3.4 BLEU) task for English-&gt;German, and for the low-resourced IWSLT 14 task Turkish-&gt;English (+2.1--3.4 BLEU), obtaining new state-of-the-art results. We also show that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task for English-&gt;German.", "histories": [["v1", "Fri, 20 Nov 2015 17:58:37 GMT  (27kb)", "http://arxiv.org/abs/1511.06709v1", null], ["v2", "Thu, 17 Mar 2016 14:52:55 GMT  (32kb)", "http://arxiv.org/abs/1511.06709v2", null], ["v3", "Thu, 31 Mar 2016 19:54:58 GMT  (32kb)", "http://arxiv.org/abs/1511.06709v3", "fixed tokenization inconsistency in de-&gt;en evaluation"], ["v4", "Fri, 3 Jun 2016 15:09:54 GMT  (32kb)", "http://arxiv.org/abs/1511.06709v4", "accepted to ACL 2016; new section on effect of back-translation quality"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["rico sennrich", "barry haddow", "alexandra birch"], "accepted": true, "id": "1511.06709"}, "pdf": {"name": "1511.06709.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["rico.sennrich@ed.ac.uk,", "a.birch@ed.ac.uk,", "bhaddow@inf.ed.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 151 1,06 709v 1 [cs.C L] 20 Nov 201 5"}, {"heading": "1 Introduction", "text": "Neural Machine Translation (NMT) has maintained state-of-the-art performance for multiple language pairs while using only parallel data for training. Monolingual data plays an important role in increasing the fluctuation for phrase-based statistical machinesThe research presented in this publication was conducted in collaboration with Samsung Electronics Polska sp. z o.o. - Samsung R & D Institute Poland.translation, and we are investigating the use of monolingual data for neural machine translation (NMT). However, language models trained on monolingual data have played a central role in statistical machine translation since the first IBM models (Brown et al., 1990). There are two important reasons for their importance. Firstly, word-based and phrase-based translation models make strong assumptions of independence, with the likelihood of translation units being valued independently of context, and Naser language models assuming different independence."}, {"heading": "2 Neural Machine Translation", "text": "We follow the neural machine translation architecture of Bahdanau et al. (2014), which we will briefly summarize here. However, we note that our approach is not specific to this architecture.The neural machine translation system is implemented as an encoder decoder network with recursive neural networks. The encoder is a bidirectional neural network with gated recurrent units (Cho et al., 2014) that reads an input sequence x = (x1,..., xm) and calculates a forward sequence of hidden states (\u2212 \u2192 h1, \u2212 hm) and a reverse sequence (\u2190 \u2212 h1,... \u2212 hm).The hidden states \u2212 hj and \u2190 \u2212 hj are linked to obtain the annotation vector hj. The decoder is a recursive neural network that predicts a target sequence."}, {"heading": "3 NMT Training with Monolingual Training Data", "text": "Unlike (G\u00fcl\u00e7ehre et al., 2015), which form separate language models into monolingual training data and integrate them into the neural network by shallow or deep fusion, our goal is to use monolingual data for training without increasing the complexity of the neural network, based on the fact that encoder decoders of neural networks (Sutskever et al., 2014; Bahdanau et al., 2014) are already capable of conditioning the probability distribution of the next target to the previous target words. We describe two strategies: providing monolingual training examples with an empty (or dumb) source set, or providing monolingual training data with a synthetic source set resulting from the automatic translation of the target into the language to which we refer as backtranslation."}, {"heading": "3.1 Dummy Source Sentences", "text": "The first technique we use is to treat monolingual training examples as parallel examples with blank input. During training, we use both parallel and monolingual training examples at a ratio of 1: 1 and mix them randomly. We define an epoch as an iteration through the parallel data set and resort to the monolingual data set for each epoch. We pair monolingual sentences with a single word dummy input < zero > to allow processing of both parallel and monolingual training examples using the same network diagram.1 For monolingual minibatches2, we freeze the encoder network parameters and the attention model.1 You could force the context vector ci to be 0 for monolingual training examples, but we found that this does not solve the main problem with this approach discussed below."}, {"heading": "3.2 Synthetic Source Sentences", "text": "In order to ensure that the output layer remains sensitive to the source context and that good parameters are not left unlearned from monolingual data, we suggest linking monolingual training instances with a synthetic source set from which a context vector can be approximated, which we obtain through back translation, i.e. an automatic translation of the monolingual target text into the source language. In principle, we can use any MT system for this task. During the training, we mix synthetic parallel text into the original (humanly translated) parallel text and do not distinguish between the two. It is important that only the source side of these additional training examples is synthetic and that the target side comes from the monolingual corpus."}, {"heading": "3.3 Unknown Words", "text": "G\u00fcl\u00e7ehre et al. (2015) determine the network vocabulary based on parallel training data and replace words from the vocabulary with a special UNK symbol. They remove monolingual sentences with more than 10% UNK symbols. In contrast, we present invisible words as sequences of subword units (Sennrich et al., 2015). As a result, our model can potentially learn valuable information about previously unknown words from monolingual corpora, for example whether a connection that can be formed via subword units is supported by the (monolingual) training data. In all experiments, we determine the network vocabulary (which includes subword units) based on the parallel training corpus and hold the vocabulary when we add monolingual or synthetic training instances."}, {"heading": "4 Evaluation", "text": "We evaluate NMT training courses on parallel text and with additional monolingual data in English \u2192 German and Turkish \u2192 English, using training and test data from WMT 15 and IWSLT 15 for English \u2192 German and IWSLT 14 for Turkish \u2192 English."}, {"heading": "4.1 Data and Methods", "text": "We use Groundhog3 as the implementation of the NMT system for all experiments (Bahdanau et al., 2014; Jean et al., 2015a). We generally follow the settings and training sequences described by Sennrich et al. (2015). Unless we specify that we use early stopping, we train models for a week. Ensembles are scanned from the last 4 stored models of the training run (stored at intervals of 12 hours). Each of these 4 models is refined with fixed embedding for 12 hours. For English \u2192 German, we report case-sensitive BLEU on detokenised text with mteval-v13a.pl for comparison with official WMT and IWSLT results. For Turkish \u2192 English, we report case-sensitive BLEU on tokenised text with multi-bleu.perl for comparison with results from G\u00fcl\u00e7ehre et al. (2015)."}, {"heading": "4.1.1 English\u2192German", "text": "As parallel training data, we use data from the Workshop on Statistical Machine Translation (WMT 15) (Bojar et al., 2015) 4. As additional training data for the experiments with monolingual data, we use the German News Crawl corpora. The amount of training data is used in Table 1. For the experiments with synthetic parallel data, we translate a random sample of 3 600,000 sentences from the German monolingual data set back into English. The German \u2192 English system used for this purpose was trained using the parallel data, using a single model that uses the same Architecture3https: / / github.com / sebastien-j / LV _ groundhog 4http: / / www.statmt.org / wmt15 / as our baseline English \u2192 German system. The translation took about a week on an NVIDIA Titan Black GPU. As a reference, the German \u2192 English system achieves a BLEU score of 25.0 on Latest 2015, thus falling short of the SOTA of 29.3 for the translation we carry out."}, {"heading": "4.1.2 Turkish\u2192English", "text": "We use data for the IWSLT 14 machine translation track (G\u00fcl\u00e7ehre et al., 2014), namely the parallel corpus WIT3 (Cettolo et al., 2012), which consists of TED conversations, and the SETimes corpus (Tyers and Alperen, 2010).6 After removing pairs of sentences containing empty lines or lines with a length ratio above 9, we retain 320 000 pairs of training data. For experiments with monolingual training data, we use the English LDC Gigaword Corpus (Fifth Edition).The amount of training data is shown in Table 2. With only 320 000 sets of parallel data available for training, this is a much more resource-rich translation setting than English. Validity honor et al. (2015), we segment the Turkish text with the morphology tool Zembershopek, followed by a disambiguation of the morphological analysis (Sak al, al, 5We use the English, 5We use the English, 5We use the English).We use the surface analysis."}, {"heading": "4.2 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2.1 English\u2192German WMT 15", "text": "Table 3 shows English \u2192 German results with WMT training and test data. We find that mixing parallel training data with monolingual data with a dummy source page in a ratio of 1-1 improves the quality of individual systems by 0.4-0.5 BLEU and of ensemble systems by 1 BLEU. We train the system for twice as long as the base in order to provide the training algorithm with a similar number of parallel training instances. To ensure that the quality improvement is due to the monolingual training instances, and not only to the extended training time, we have also continued training our base system for another week, but no improvements in BLEU.The inclusion of synthetic data during training is very effective and leads to an improvement over our baseline by 2.8-3.4 BLEU. Our best ensemble system also exceeds a syntax-based baseline (Sennrich and Haddow, 2015) by 1.2-2.1 BLEU. We also perform significantly better than NMT bleb ensembles (presumably from Jean / Haddow, 2017large) and the 2017 / 2017 ensembles (probably)."}, {"heading": "4.2.2 English\u2192German IWSLT 15", "text": "In fact, it is the case that most of them are able to assert themselves that they do not see themselves as being able to achieve their goals. (...) Most of them are not able to achieve their goals. (...) Most of them are not able to achieve their goals. (...) Most of them are not able to achieve their goals. (...) Most of them are not able to achieve their goals. (...) Most of them are not able to achieve their goals. (...) Most of them are not able to achieve their goals. (...) Most of them are not able to achieve their goals. (...) Most of them are not able to achieve their goals. (...) Most of them are not able to achieve their goals. (...)"}, {"heading": "4.2.3 Turkish\u2192English IWSLT 14", "text": "Table 5 shows results for Turkish \u2192 English. On average, we saw an improvement of 0.6 BLEU in test sets by adding monolingual data with a dummy source page at a ratio of 1: 1, although we found a large variation between different test sets. Due to the small amount of available training data, we found optimal BLEU after a relatively small number of epochs. We also experimented with higher ratios of monolingual data, but this led to a decrease in BLEU values. With synthetic training data, we exceed the target page of the parallel training text by an average of 2.7 BLEU and also exceed the results obtained by deep fusion of G\u00fcl\u00e7ehre et al. (2015) by an average of 0.5 BLEU. To compare the extent to which synthetic data has a regulating effect, even without novel training data, we retranslate the target page of the parallel training text to obtain parallel training corpus synth."}, {"heading": "4.2.4 Analysis", "text": "This year, it has come to the point where it only takes one year to get to the next round."}, {"heading": "4.2.5 Summary", "text": "We find that adding monolingual training instances with a dummy source set is a successful and inexpensive strategy to improve translation quality. However, even more effective is combining monolingual data with a synthetic source page obtained through backtranslation. If there is a domain discrepancy between training and test set, even small amounts of in-domain training data (monolingual with backtranslation or parallel) can greatly increase NMT performance by fine-tuning the indomain data."}, {"heading": "5 Related Work", "text": "To our knowledge, the integration of monolingual data for purely neural machine translation architectures was first investigated by (G\u00fcl\u00e7ehre et al., 2015), who independently train monolingual language models and then integrate them during decoding by reranking (flat fusion) or by adding the recurring hidden state of the language model to the decoder state of the encoder decoder network, with an additional control mechanism controlling the size of the LM signal (deep fusion). In deep fusion, the control parameters and output parameters are matched to other parallel training data, but the language model parameters are set during fine-tuning. Jean et al. (2015b) also report on experiments with rescheduling NMT output with a 5 gram language model, but improvements are small, with improvements ranging from 0.1-0.5 BLEU domains based on the retranslation of monolingual targets into the language source to produce synthetic text modes."}, {"heading": "6 Conclusion", "text": "In this paper, we propose two simple methods for using monolingual training data during the training of NMT systems, without any changes to the network architecture. Providing training examples with dummy source context has been successful to some extent, but we achieve significant gains in all tasks and new SOTA results by translating monolingual target data back into the source language and treating this synthetic data as additional training data. We also show that small amounts of domain-internal monolingual data translated back into the source language can be effectively used for domain customization. While our experiments used monolingual training data, we used only a small sample of the available data, especially for experiments with synthetic parallel data. It is conceivable that larger synthetic data sets or data sets obtained through data selection will provide greater performance benefits. As we do not change the neural network architecture to allow monolingual training data to be easily applied to our monolingual approach to the overluminance of the system, we can easily anticipate differences in the number of NMT to be applied to other monolingual training systems."}, {"heading": "Acknowledgments", "text": "The research results presented in this publication were carried out in cooperation with Samsung Electronics Polska sp. z o.o. - Samsung R & D Institute Poland and funded under the European Union's Horizon 2020 research and innovation programme under Funding Agreement 645452 (QT21)."}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Domain adaptation for statistical machine translation with monolingual resources", "author": ["Bertoldi", "Federico2009] Nicola Bertoldi", "Marcello Federico"], "venue": "In Proceedings of the Fourth Workshop on Statistical Machine Translation StatMT 09", "citeRegEx": "Bertoldi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bertoldi et al\\.", "year": 2009}, {"title": "Findings of the 2015 Workshop on Statistical Machine Translation", "author": ["Turchi."], "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 1\u201346, Lisbon, Portugal. Association for Computational Linguistics.", "citeRegEx": "Turchi.,? 2015", "shortCiteRegEx": "Turchi.", "year": 2015}, {"title": "WIT: Web Inventory of Transcribed and Translated Talks", "author": ["Christian Girardi", "Marcello Federico"], "venue": "In Proceedings of the 16 Conference of the European Association for Machine Translation (EAMT),", "citeRegEx": "Cettolo et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cettolo et al\\.", "year": 2012}, {"title": "Report on the 11th IWSLT Evaluation Campaign, IWSLT", "author": ["Jan Niehues", "Sebastian St\u00fcker", "Luisa Bentivogli", "Marcello Federico"], "venue": "In Proceedings of the 11th Workshop on Spoken Language Translation,", "citeRegEx": "Cettolo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cettolo et al\\.", "year": 2014}, {"title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Transla", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Combined Spoken Language Translation", "author": ["Cettolo", "Marcello Federico."], "venue": "International Workshop on Spoken Language Translation, pages 57\u201364, Lake Tahoe, CA, USA.", "citeRegEx": "Cettolo and Federico.,? 2014", "shortCiteRegEx": "Cettolo and Federico.", "year": 2014}, {"title": "Practical Variational Inference for Neural Networks", "author": ["Alex Graves"], "venue": "In J. ShaweTaylor,", "citeRegEx": "Graves.,? \\Q2011\\E", "shortCiteRegEx": "Graves.", "year": 2011}, {"title": "The Edinburgh/JHU Phrase-based Machine Translation Systems", "author": ["Haddow et al.2015] Barry Haddow", "Matthias Huck", "Alexandra Birch", "Nikolay Bogoychev", "Philipp Koehn"], "venue": "WMT", "citeRegEx": "Haddow et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Haddow et al\\.", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors. CoRR, abs/1207.0580", "author": ["lan Salakhutdinov"], "venue": null, "citeRegEx": "Salakhutdinov.,? \\Q2012\\E", "shortCiteRegEx": "Salakhutdinov.", "year": 2012}, {"title": "On Using Very Large Target Vocabulary for Neural Machine Translation", "author": ["Jean et al.2015a] S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Jean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "2015b. Montreal Neural Machine Translation Systems for WMT\u201915", "author": ["Jean et al.2015b] S\u00e9bastien Jean", "Orhan Firat", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation", "citeRegEx": "Jean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Investigations on Translation Model Adaptation Using Monolingual Data", "author": ["Holger Schwenk", "Christophe Servan", "Sadaf Abdul-Rauf"], "venue": "In Proceedings of the Sixth Workshop on Statistical Machine Translation,", "citeRegEx": "Lambert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lambert et al\\.", "year": 2011}, {"title": "Effective Approaches to Attention-based Neural Machine Translation", "author": ["Luong et al.2015] Thang Luong", "Hieu Pham", "Christopher D. Manning"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Morphological Disambiguation of Turkish Text with Perceptron Algorithm", "author": ["Sak et al.2007] Ha\u015fim Sak", "Tunga G\u00fcng\u00f6r", "Murat Sara\u00e7lar"], "venue": "CICLing", "citeRegEx": "Sak et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Sak et al\\.", "year": 2007}, {"title": "A Joint Dependency Model of Morphological and Syntactic Structure for Statistical Machine Translation", "author": ["Sennrich", "Haddow2015] Rico Sennrich", "Barry Haddow"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Neural Machine Translation of Rare Words with Subword Units. CoRR, abs/1508.07909", "author": ["Barry Haddow", "Alexandra Birch"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Sys-", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "SETimes: A parallel corpus of Balkan languages", "author": ["Tyers", "Alperen2010] Francis M. Tyers", "Murat S. Alperen"], "venue": "In Workshop on Exploitation", "citeRegEx": "Tyers et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tyers et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 17, "context": "In (possibly attention-based) encoder-decoder architectures for neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2014), the decoder is essentially an RNN language model", "startOffset": 91, "endOffset": 138}, {"referenceID": 0, "context": "In (possibly attention-based) encoder-decoder architectures for neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2014), the decoder is essentially an RNN language model", "startOffset": 91, "endOffset": 138}, {"referenceID": 0, "context": "We follow the neural machine translation architecture by Bahdanau et al. (2014), which we will briefly summarize here.", "startOffset": 57, "endOffset": 80}, {"referenceID": 5, "context": "The encoder is a bidirectional neural network with gated recurrent units (Cho et al., 2014) that reads an input sequence x = (x1, .", "startOffset": 73, "endOffset": 91}, {"referenceID": 0, "context": "A detailed description can be found in (Bahdanau et al., 2014).", "startOffset": 39, "endOffset": 62}, {"referenceID": 17, "context": ", 2015), who train separate language models on monolingual training data and incorporate them into the neural network through shallow or deep fusion, our goal is to use monolingual data for training without increasing the complexity of the neural network, based on the fact that encoder-decoder neural networks (Sutskever et al., 2014; Bahdanau et al., 2014) already have the capability to condition the probability distribution of the next target word on the previous target words.", "startOffset": 311, "endOffset": 358}, {"referenceID": 0, "context": ", 2015), who train separate language models on monolingual training data and incorporate them into the neural network through shallow or deep fusion, our goal is to use monolingual data for training without increasing the complexity of the neural network, based on the fact that encoder-decoder neural networks (Sutskever et al., 2014; Bahdanau et al., 2014) already have the capability to condition the probability distribution of the next target word on the previous target words.", "startOffset": 311, "endOffset": 358}, {"referenceID": 0, "context": "Bahdanau et al. (2014) sorts sets of 20 minibatches according to length to improve speed, since training time is proportional to the longest sentence in a minibatch.", "startOffset": 0, "endOffset": 23}, {"referenceID": 15, "context": "subword units (Sennrich et al., 2015).", "startOffset": 14, "endOffset": 37}, {"referenceID": 0, "context": "We use Groundhog3 as the implementation of the NMT system for all experiments (Bahdanau et al., 2014; Jean et al., 2015a).", "startOffset": 78, "endOffset": 121}, {"referenceID": 0, "context": "We use Groundhog3 as the implementation of the NMT system for all experiments (Bahdanau et al., 2014; Jean et al., 2015a). We generally follow the settings and training procedure described by Sennrich et al. (2015). Unless we specify that we use early stopping, we train models for a week.", "startOffset": 79, "endOffset": 215}, {"referenceID": 8, "context": "3 for that translation direction (Haddow et al., 2015).", "startOffset": 33, "endOffset": 54}, {"referenceID": 15, "context": "We tokenize and truecase the training data, and represent rare words via BPE (Sennrich et al., 2015).", "startOffset": 77, "endOffset": 100}, {"referenceID": 3, "context": "As indomain training data, IWSLT provides the WIT parallel corpus (Cettolo et al., 2012), which also consists of TED talks.", "startOffset": 66, "endOffset": 88}, {"referenceID": 6, "context": "3 for that translation direction (Haddow et al., 2015). We leave it to future work to explore how sensitive NMT training with synthetic data is to the quality of the back-translation. We tokenize and truecase the training data, and represent rare words via BPE (Sennrich et al., 2015). Specifically, we follow Sennrich et al. (2015) in performing BPE on the joint vocabulary with 89 500 merge operations.", "startOffset": 34, "endOffset": 333}, {"referenceID": 4, "context": "We use data provided for the IWSLT 14 machine translation track (Cettolo et al., 2014), namely the WIT parallel corpus (Cettolo et al.", "startOffset": 64, "endOffset": 86}, {"referenceID": 3, "context": ", 2014), namely the WIT parallel corpus (Cettolo et al., 2012), which consists of TED talks, and the SETimes corpus (Tyers and Alperen, 2010).", "startOffset": 40, "endOffset": 62}, {"referenceID": 14, "context": "(Sak et al., 2007), and removal of non-surface tokens produced by the analysis.", "startOffset": 0, "endOffset": 18}, {"referenceID": 15, "context": "For both Turkish and English, we represent rare words (or morphemes in the case of Turkish) as character bigram sequences (Sennrich et al., 2015).", "startOffset": 122, "endOffset": 145}, {"referenceID": 7, "context": "001) (Graves, 2011), and dropout on the output layer (p=0.", "startOffset": 5, "endOffset": 19}, {"referenceID": 7, "context": "001) (Graves, 2011), and dropout on the output layer (p=0.5) (Hinton et al., 2012). We also use early stopping, based on BLEU measured every three hours on tst2010, which we treat as development set. For Turkish\u2192English, we use gradient clipping with threshold 5, following G\u00fcl\u00e7ehre et al. (2015), in contrast to the threshold 1 that we use for English\u2192German, following (Jean et al.", "startOffset": 6, "endOffset": 297}, {"referenceID": 10, "context": "form NMT results reported by Jean et al. (2015a)", "startOffset": 29, "endOffset": 49}, {"referenceID": 13, "context": "and Luong et al. (2015), who previously reported SOTA result.", "startOffset": 4, "endOffset": 24}, {"referenceID": 13, "context": "and Luong et al. (2015), who previously reported SOTA result.8 We note that the difference is particularly large for single systems, and that our ensemble is presumably weaker because Luong et al. (2015) used 8 ensemble components, and trained them independently, whereas we sampled 4 ensemble components from the same training run.", "startOffset": 4, "endOffset": 204}, {"referenceID": 10, "context": "Jean et al. (2015b) also report on experiments with reranking of NMT output with a 5-gram language model, but improvements are small, with improvements between 0.", "startOffset": 0, "endOffset": 20}, {"referenceID": 12, "context": "The back-translation of monolingual target data into the source language to produce synthetic parallel text has been previously explored for phrase-based SMT (Bertoldi and Federico, 2009; Lambert et al., 2011).", "startOffset": 158, "endOffset": 209}], "year": 2017, "abstractText": "Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data for training. Monolingual data plays an important role in boosting fluency for phrase-based statistical machine translation, and we investigate the use of monolingual data for neural machine translation (NMT). In contrast to previous work, which integrates a separately trained RNN language model into an NMT architecture (G\u00fcl\u00e7ehre et al., 2015), we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model, and we explore strategies to include monolingual training data in the training process. Through our use of monolingual data, we obtain substantial improvements on the WMT 15 (+2.8\u20133.4 BLEU) task for English\u2192German, and for the low-resourced IWSLT 14 task Turkish\u2192English (+2.1\u20133.4 BLEU), obtaining new state-of-the-art results. We also show that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task for English\u2192German.", "creator": "LaTeX with hyperref package"}}}