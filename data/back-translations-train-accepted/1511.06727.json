{"id": "1511.06727", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2015", "title": "Scalable Gradient-Based Tuning of Continuous Regularization Hyperparameters", "abstract": "Hyperparameter selection generally relies on running multiple full training trials, with hyperparameter selection based on validation set performance. We propose a gradient-based approach for locally adjusting hyperparameters on the fly in which we adjust the hyperparameters so as to make the model parameter gradients, and hence updates, more advantageous for the validation cost. We explore the approach for tuning regularization hyperparameters and find that in experiments on MNIST the resulting regularization levels are within the optimal regions. The method is less computationally demanding compared to similar gradient-based approaches to hyperparameter selection, only requires a few trials, and consistently finds solid hyperparameter values which makes it a useful tool for training neural network models.", "histories": [["v1", "Fri, 20 Nov 2015 19:10:16 GMT  (157kb,D)", "https://arxiv.org/abs/1511.06727v1", "8 pages, 5 figures"], ["v2", "Wed, 16 Dec 2015 06:28:07 GMT  (153kb,D)", "http://arxiv.org/abs/1511.06727v2", "8 pages, 5 figures. added references, fixed typos"], ["v3", "Fri, 17 Jun 2016 19:25:32 GMT  (872kb,D)", "http://arxiv.org/abs/1511.06727v3", "9 pages, 7 figures. Accepted at ICML 2016"]], "COMMENTS": "8 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jelena luketina", "tapani raiko", "mathias berglund", "klaus greff"], "accepted": true, "id": "1511.06727"}, "pdf": {"name": "1511.06727.pdf", "metadata": {"source": "META", "title": "Scalable Gradient-Based Tuning of  Continuous Regularization Hyperparameters", "authors": ["Jelena Luketina", "Mathias Berglund", "Klaus Greff", "Tapani Raiko"], "emails": ["JELENA.LUKETINA@AALTO.FI", "MATHIAS.BERGLUND@AALTO.FI", "KLAUS@IDSIA.CH", "TAPANI.RAIKO@AALTO.FI"], "sections": [{"heading": "1. Introduction", "text": "This year it is more than ever before."}, {"heading": "2. Proposed Method", "text": "We propose a method, T1 \u2212 T2, for tuning continuous hyperparameters of a model using the gradient of the performance of the model on a separate validation set T2. Essentially, we train a neural network model on a training set T1 as usual. However, for each update of network weights and distortions, i.e. the elementary parameters of the network, we try to optimize the hyperparameters so that the direction of updating the weight is as advantageous as possible for validation costs on a separate data set T2. Formally, when forming a neural network model, an objective function that depends on the training set is considered to minimize model weights and hyperparameters that determine the strength of possible regulatory concepts. When using gradient descent, we refer to the optimization target function C \u04321 (\u00b7) and the corresponding weight actualization based on T1 (\u04211), where T1 is hyperparameter."}, {"heading": "2.1. Motivation and analysis", "text": "The most similar model previously proposed is the incremental gradient version of the hyperparameter actualization of (Chen and Hagan, 1999). However, their derivation of the hypergradient is based on a Gauss-Newton actualization of the elementary parameters, making the calculation of the gradient and the hypergradient considerably more expensive. A well-justified closed form of the term \"hypergradient\" is available once the elementary gradient has converged (Foo et al., 2008), with the actualization of the form (4). Comparing this expression with the actualization of T1 \u2212 T2, (3) can be considered an approximation (4) if the gradient is close to convergence and the Hessian convergence is good through the identity of 2\u03b8C 1 = I: t + 1 = elementary components of the optimization of C2)."}, {"heading": "2.2. Computational cost", "text": "The most cost-effective concept of the proposed method is (i) (i) (i) (i) (i) (i) (i) (ii) (ii) (ii) (ii) (ii) (ii) (ii) (ii) (ii) (ii) (ii) (ii) (ii) (ii) (ii) (ii) (ii) (ii) (ii) (ii) (ii) (ii) (ii) (ii) (ii) (ii) (ii) (ii) (ii)) (ii) (ii) (ii)) (ii) (ii) (ii) (ii) (ii) (ii) (ii) (ii) (ii) (ii) (ii)) (ii) (ii) (ii) (ii) (ii) (ii) (ii)) (ii) (ii) (ii)) (ii) (ii))) (ii) (ii))) (ii) (ii) (ii)) (ii) (ii)) (ii) (ii) (ii)) (ii))) (ii) (ii)) (ii) (ii)) (ii) (ii)) (ii) (ii))) (ii) (ii)) (ii)) (ii)) (ii) (ii) (ii)) (ii) (ii) (ii) (ii) (ii)) (ii) (ii)) (ii) (ii) (ii)) (ii) (ii) (ii) (ii) (ii) (ii) (ii))) (ii) (ii) (ii) (ii) (ii) (ii)) (ii) (ii)) (ii) (ii) (ii) (ii) (ii) (ii)"}, {"heading": "3. Experiments", "text": "In fact, most of us are able to move to another world, to move to another world, to move to another world, to move to another world."}, {"heading": "3.1. Results", "text": "Figure 1 shows how the hyperparameters change during the training. To see how the T1-T2 method behaves, we have a set of hyperparameter values during the training in hyperparameter space. In other words, the background of the numbers corresponds to the search for the two-dimensional hyperparameter interval. The initial regulation is done with an asterisk, while the final value is marked with a square."}, {"heading": "4. Discussion and Conclusion", "text": "We have proposed a method called T1 \u2212 T2 for the gradual automatic adjustment of continuous hyperparameters during training, based on the performance of the model on a separate validation group. We experimented with adjusting regulation hyperparameters when the initial hyper-parameter assumption was far from the optimal value. Although T1 \u2212 T2 is the best amount of hyperparameters compared to an exhaustive search in which the model is repeatedly trained with a large number of hyperparameter suggestions, the property is that it finds values close to the optimal value, it is unlikely that the hyperparameters will be compared."}, {"heading": "5. Acknowledgements", "text": "We would like to thank many colleagues for their helpful discussions and feedback, especially Dario Amodei and Harri Valpola. Special thanks go to Antti Rasmus for the technical support. Also to Olof Mogren and Mikael Kageback for their detailed comments on the design. Jelena Luketina and Tapani Raiko were financed by the Finnish Academy."}], "references": [{"title": "Gradient-based optimization of hyperparameters", "author": ["Y. Bengio"], "venue": "Neural computation, 12(8), 1889\u20131900.", "citeRegEx": "Bengio,? 2000", "shortCiteRegEx": "Bengio", "year": 2000}, {"title": "Random search for hyper-parameter optimization", "author": ["J. Bergstra", "Y. Bengio"], "venue": "J. Mach. Learn. Res., 13, 281\u2013305.", "citeRegEx": "Bergstra and Bengio,? 2012", "shortCiteRegEx": "Bergstra and Bengio", "year": 2012}, {"title": "Algorithms for hyper-parameter optimization", "author": ["J.S. Bergstra", "R. Bardenet", "Y. Bengio", "B. K\u00e9gl"], "venue": "Ad-", "citeRegEx": "Bergstra et al\\.,? 2011", "shortCiteRegEx": "Bergstra et al\\.", "year": 2011}, {"title": "Optimal use of regularization and cross-validation in neural network modeling", "author": ["D. Chen", "M.T. Hagan"], "venue": "International Joint Conference on Neural Networks, pages 1275\u20131289.", "citeRegEx": "Chen and Hagan,? 1999", "shortCiteRegEx": "Chen and Hagan", "year": 1999}, {"title": "Improving deep neural networks for LVCSR using rectified linear units and dropout", "author": ["G.E. Dahl", "T.N. Sainath", "G.E. Hinton"], "venue": "ICASSP, pages 8609\u20138613.", "citeRegEx": "Dahl et al\\.,? 2013", "shortCiteRegEx": "Dahl et al\\.", "year": 2013}, {"title": "Natural neural networks", "author": ["G. Desjardins", "K. Simonyan", "R. Pascanu", "K. Kavukcuoglu"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Desjardins et al\\.,? 2015", "shortCiteRegEx": "Desjardins et al\\.", "year": 2015}, {"title": "Efficient multiple hyperparameter learning for log-linear models", "author": ["Foo", "C.-s.", "C.B. Do", "A. Ng"], "venue": "Advances in neural information processing systems (NIPS), pages 377\u2013384.", "citeRegEx": "Foo et al\\.,? 2008", "shortCiteRegEx": "Foo et al\\.", "year": 2008}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167.", "citeRegEx": "Ioffe and Szegedy,? 2015", "shortCiteRegEx": "Ioffe and Szegedy", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "the International Conference on Learning Representations (ICLR), San Diego. arXiv:1412.6980.", "citeRegEx": "Kingma and Ba,? 2015", "shortCiteRegEx": "Kingma and Ba", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": null, "citeRegEx": "Krizhevsky,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky", "year": 2009}, {"title": "Adaptive regularization in neural network modeling", "author": ["J. Larsen", "C. Svarer", "L.N. Andersen", "L.K. Hansen"], "venue": "Neural Networks: Tricks of the Trade, pages 113\u2013132. Springer.", "citeRegEx": "Larsen et al\\.,? 1998", "shortCiteRegEx": "Larsen et al\\.", "year": 1998}, {"title": "The MNIST database of handwritten digits", "author": ["Y. LeCun", "C. Cortes", "C.J. Burges"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Gradient-based hyperparameter optimization through reversible learning", "author": ["D. Maclaurin", "D. Duvenaud", "R.P. Adams"], "venue": "International Conference on Machine Learning.", "citeRegEx": "Maclaurin et al\\.,? 2015", "shortCiteRegEx": "Maclaurin et al\\.", "year": 2015}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "NIPS workshop on deep learning and unsupervised feature learning, volume 2011, page 4. Granada, Spain.", "citeRegEx": "Netzer et al\\.,? 2011", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Fast Exact Multiplication by the Hessian", "author": ["B.A. Pearlmutter"], "venue": "Neural Computation, pages 147\u2013160.", "citeRegEx": "Pearlmutter,? 1994", "shortCiteRegEx": "Pearlmutter", "year": 1994}, {"title": "Deep learning made easier by linear transformations in perceptrons", "author": ["T. Raiko", "H. Valpola", "Y. LeCun"], "venue": "International Conference on Artificial Intelligence and Statistics, pages 924\u2013932.", "citeRegEx": "Raiko et al\\.,? 2012", "shortCiteRegEx": "Raiko et al\\.", "year": 2012}, {"title": "Semi-supervised learning with ladder network", "author": ["A. Rasmus", "H. Valpola", "M. Honkala", "M. Berglund", "T. Raiko"], "venue": "Neural Information Processing Systems.", "citeRegEx": "Rasmus et al\\.,? 2015", "shortCiteRegEx": "Rasmus et al\\.", "year": 2015}, {"title": "Fast curvature matrix-vector products for second-order gradient descent", "author": ["N.N. Schraudolph"], "venue": "Neural Computation, 14(7), 1723\u20131738.", "citeRegEx": "Schraudolph,? 2002", "shortCiteRegEx": "Schraudolph", "year": 2002}, {"title": "Practical Bayesian Optimization of Machine Learning Algorithms", "author": ["J. Snoek", "H. Larochelle", "R.P. Adams"], "venue": "ArXiv e-prints.", "citeRegEx": "Snoek et al\\.,? 2012", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Striving for Simplicity: The All Convolutional Net", "author": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M. Riedmiller"], "venue": null, "citeRegEx": "Springenberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research, 15(1), 1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["T.T.D. Team"], "venue": null, "citeRegEx": "Team,? \\Q2016\\E", "shortCiteRegEx": "Team", "year": 2016}, {"title": "Pushing stochastic gradient towards second-order methods\u2013backpropagation learning with transformations in nonlinearities", "author": ["T. Vatanen", "T. Raiko", "H. Valpola", "Y. LeCun"], "venue": "Neural Information Processing, pages 442\u2013449. Springer.", "citeRegEx": "Vatanen et al\\.,? 2013", "shortCiteRegEx": "Vatanen et al\\.", "year": 2013}, {"title": "Stacked denoising autoencoders: learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P. antoine Manzagol"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Fast dropout training", "author": ["S.I. Wang", "C.D. Manning"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML).", "citeRegEx": "Wang and Manning,? 2013", "shortCiteRegEx": "Wang and Manning", "year": 2013}, {"title": "Empirical evaluation of rectified activations in convolutional network", "author": ["B. Xu", "N. Wang", "M. Li"], "venue": "CoRR.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "The process of hyperparameter selection is in practice often based on trial-and-error and grid or random search (Bergstra and Bengio, 2012).", "startOffset": 112, "endOffset": 139}, {"referenceID": 2, "context": "number of automated methods (Bergstra et al., 2011; Snoek et al., 2012), all of which rely on multiple complete training runs with varied fixed hyperparameters, with the hyperparameter selection based on the validation set performance.", "startOffset": 28, "endOffset": 71}, {"referenceID": 18, "context": "number of automated methods (Bergstra et al., 2011; Snoek et al., 2012), all of which rely on multiple complete training runs with varied fixed hyperparameters, with the hyperparameter selection based on the validation set performance.", "startOffset": 28, "endOffset": 71}, {"referenceID": 10, "context": "Similar approaches have been proposed since the late 1990s; however, these methods either require computation of the inverse Hessian (Larsen et al., 1998; Bengio, 2000; Chen and Hagan, 1999; Foo et al., 2008), or propagating gradients", "startOffset": 133, "endOffset": 208}, {"referenceID": 0, "context": "Similar approaches have been proposed since the late 1990s; however, these methods either require computation of the inverse Hessian (Larsen et al., 1998; Bengio, 2000; Chen and Hagan, 1999; Foo et al., 2008), or propagating gradients", "startOffset": 133, "endOffset": 208}, {"referenceID": 3, "context": "Similar approaches have been proposed since the late 1990s; however, these methods either require computation of the inverse Hessian (Larsen et al., 1998; Bengio, 2000; Chen and Hagan, 1999; Foo et al., 2008), or propagating gradients", "startOffset": 133, "endOffset": 208}, {"referenceID": 6, "context": "Similar approaches have been proposed since the late 1990s; however, these methods either require computation of the inverse Hessian (Larsen et al., 1998; Bengio, 2000; Chen and Hagan, 1999; Foo et al., 2008), or propagating gradients", "startOffset": 133, "endOffset": 208}, {"referenceID": 12, "context": "Borrowing the expression from Maclaurin et al. (2015), we refer to the model parameters customarily trained with backpropagation as elementary parameters, and to all other parameters as hyperparameters.", "startOffset": 30, "endOffset": 54}, {"referenceID": 12, "context": "through the entire history of parameter updates Maclaurin et al. (2015). Moreover, these methods make changes to the hyperparameter only once the elementary parameter training has ended.", "startOffset": 48, "endOffset": 72}, {"referenceID": 7, "context": "We add batch normalization (Ioffe and Szegedy, 2015) and adaptive learning rates (Kingma and Ba, 2015) to the process of hyperparameter training, which diminishes some of the problems of gradient-based hyperparameter optimization.", "startOffset": 27, "endOffset": 52}, {"referenceID": 8, "context": "We add batch normalization (Ioffe and Szegedy, 2015) and adaptive learning rates (Kingma and Ba, 2015) to the process of hyperparameter training, which diminishes some of the problems of gradient-based hyperparameter optimization.", "startOffset": 81, "endOffset": 102}, {"referenceID": 23, "context": "Injecting noise is particularly relevant for denoising autoencoders and related models (Vincent et al., 2010; Rasmus et al., 2015), where performance strongly depends on the level of noise.", "startOffset": 87, "endOffset": 130}, {"referenceID": 16, "context": "Injecting noise is particularly relevant for denoising autoencoders and related models (Vincent et al., 2010; Rasmus et al., 2015), where performance strongly depends on the level of noise.", "startOffset": 87, "endOffset": 130}, {"referenceID": 20, "context": "A third, often used, regularization method that involves a hyperparameter choice is dropout (Srivastava et al., 2014).", "startOffset": 92, "endOffset": 117}, {"referenceID": 24, "context": "Moreover, dropout can be seen as a form of multiplicative Gaussian noise (Wang and Manning, 2013).", "startOffset": 73, "endOffset": 97}, {"referenceID": 3, "context": "The most similar previously proposed model is the incremental gradient version of the hyperparameter update from (Chen and Hagan, 1999).", "startOffset": 113, "endOffset": 135}, {"referenceID": 6, "context": "A well justified closed form for the term \u2207\u03bb\u03b8 is available once the elementary gradient has converged (Foo et al., 2008), with the update of the form (4).", "startOffset": 102, "endOffset": 120}, {"referenceID": 12, "context": "Another approach to hypergradient computation is given in Maclaurin et al. (2015). There, the term \u2207\u03bb\u03b8T (T denoting the final iteration number) considers effect of the hyperparameter on the entire history of updates:", "startOffset": 58, "endOffset": 82}, {"referenceID": 7, "context": "However, arguably, batch normalization (Ioffe and Szegedy, 2015) is eliminating part of the problem, by making the Hessian closer to identity (Vatanen et al.", "startOffset": 39, "endOffset": 64}, {"referenceID": 22, "context": "However, arguably, batch normalization (Ioffe and Szegedy, 2015) is eliminating part of the problem, by making the Hessian closer to identity (Vatanen et al., 2013; Raiko et al., 2012), making the approximation more justified.", "startOffset": 142, "endOffset": 184}, {"referenceID": 15, "context": "However, arguably, batch normalization (Ioffe and Szegedy, 2015) is eliminating part of the problem, by making the Hessian closer to identity (Vatanen et al., 2013; Raiko et al., 2012), making the approximation more justified.", "startOffset": 142, "endOffset": 184}, {"referenceID": 5, "context": "Another step towards making even closer approximation are transformations that further whiten the hidden representations (Desjardins et al., 2015).", "startOffset": 121, "endOffset": 146}, {"referenceID": 14, "context": "It can be shown that the cost of computing this term scales comparably to backpropagation, due to the properties of R and L-operators (Pearlmutter, 1994; Schraudolph, 2002).", "startOffset": 134, "endOffset": 172}, {"referenceID": 17, "context": "It can be shown that the cost of computing this term scales comparably to backpropagation, due to the properties of R and L-operators (Pearlmutter, 1994; Schraudolph, 2002).", "startOffset": 134, "endOffset": 172}, {"referenceID": 4, "context": "We test the method on various configurations of multilayer perceptrons (MLPs) with ReLU activation functions (Dahl et al., 2013) trained on the MNIST (LeCun et al.", "startOffset": 109, "endOffset": 128}, {"referenceID": 11, "context": ", 2013) trained on the MNIST (LeCun et al., 1998) and SVHN (Netzer et al.", "startOffset": 29, "endOffset": 49}, {"referenceID": 13, "context": ", 1998) and SVHN (Netzer et al., 2011) data set.", "startOffset": 17, "endOffset": 38}, {"referenceID": 9, "context": "We also test the method on two convolutional architectures (CNNs) using CIFAR10 (Krizhevsky, 2009).", "startOffset": 80, "endOffset": 98}, {"referenceID": 19, "context": "The CNN architectures used were modified versions of model All-CNN-C from (Springenberg et al., 2014) and a baseline model from (Rasmus et al.", "startOffset": 74, "endOffset": 101}, {"referenceID": 16, "context": ", 2014) and a baseline model from (Rasmus et al., 2015), using ReLU and leakyReLU activations (Xu et al.", "startOffset": 34, "endOffset": 55}, {"referenceID": 25, "context": ", 2015), using ReLU and leakyReLU activations (Xu et al., 2015).", "startOffset": 46, "endOffset": 63}, {"referenceID": 21, "context": "The models were implemented with the Theano package (Team, 2016).", "startOffset": 52, "endOffset": 64}, {"referenceID": 8, "context": "To speed up elementary parameter training, we use an annealed ADAM learning rate schedule (Kingma and Ba, 2015) with a step size of 10\u22123 (MLPs) or 2 \u00b7 10\u22123 (CNNs).", "startOffset": 90, "endOffset": 111}], "year": 2016, "abstractText": "Hyperparameter selection generally relies on running multiple full training trials, with selection based on validation set performance. We propose a gradient-based approach for locally adjusting hyperparameters during training of the model. Hyperparameters are adjusted so as to make the model parameter gradients, and hence updates, more advantageous for the validation cost. We explore the approach for tuning regularization hyperparameters and find that in experiments on MNIST, SVHN and CIFAR-10, the resulting regularization levels are within the optimal regions. The additional computational cost depends on how frequently the hyperparameters are trained, but the tested scheme adds only 30% computational overhead regardless of the model size. Since the method is significantly less computationally demanding compared to similar gradientbased approaches to hyperparameter optimization, and consistently finds good hyperparameter values, it can be a useful tool for training neural network models.", "creator": "LaTeX with hyperref package"}}}