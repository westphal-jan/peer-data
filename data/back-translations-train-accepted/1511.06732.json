{"id": "1511.06732", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2015", "title": "Sequence Level Training with Recurrent Neural Networks", "abstract": "Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the BLEU score: a popular metric to compare a sequence to a reference. On three different tasks, our approach outperforms several strong baselines for greedy generation, and it matches their performance with beam search, while being several times faster.", "histories": [["v1", "Fri, 20 Nov 2015 19:25:54 GMT  (1935kb,D)", "http://arxiv.org/abs/1511.06732v1", null], ["v2", "Mon, 14 Dec 2015 16:11:27 GMT  (1960kb,D)", "http://arxiv.org/abs/1511.06732v2", null], ["v3", "Tue, 15 Dec 2015 16:51:31 GMT  (1960kb,D)", "http://arxiv.org/abs/1511.06732v3", null], ["v4", "Wed, 6 Jan 2016 06:24:58 GMT  (1963kb,D)", "http://arxiv.org/abs/1511.06732v4", null], ["v5", "Fri, 12 Feb 2016 16:05:32 GMT  (1963kb,D)", "http://arxiv.org/abs/1511.06732v5", null], ["v6", "Wed, 4 May 2016 13:43:39 GMT  (1963kb,D)", "http://arxiv.org/abs/1511.06732v6", null], ["v7", "Fri, 6 May 2016 21:18:46 GMT  (1995kb,D)", "http://arxiv.org/abs/1511.06732v7", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["marc'aurelio ranzato", "sumit chopra", "michael auli", "wojciech zaremba"], "accepted": true, "id": "1511.06732"}, "pdf": {"name": "1511.06732.pdf", "metadata": {"source": "CRF", "title": "RECURRENT NEURAL NETWORKS", "authors": ["Marc\u2019Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "emails": ["wojciech}@fb.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "This year is the highest in the history of the country."}, {"heading": "2 RELATED WORK", "text": "The sequence models are typically trained to predict the next word by means of cross-entropy loss (also known as negative log-likelihood loss).At test time, it is common practice to explore several alternative pathways by beam search (Sutskever et al., 2014; Bahdanau et al., 2015; Rush et al., 2015).While this typically improves generation by one or two BLEU points (Papineni et al., 2002), it also comes at a cost: it slows generation at least k times where the number of active pathways are in the beam (see Sec. 3.1.1 for more details).The key idea of this work is to improve generation by using the model's own predictions in the training period, as first admonished by Daume III et al. (2009).In their basic work, the authors first noted that structured prediction problems as a specific instance of the reinforced learning algorithm can be structured, and then proposed to SEN."}, {"heading": "3 MODELS", "text": "The learning algorithms we describe in the following sections are agnostic to the choice of the underlying model as long as it is parametric. In this work, we focus on recursive neural networks (RNNs), as they are a popular choice for text generation (Mikolov et al., 2010).RNN is a parametric model that takes a word wt-W as input at each step t-1, T], along with an internal representation ht. This internal representation is a real vector that encodes the history of the words it has seen so far. Optionally, the RNN can also take an additional context vector ct as input. It learns a recursive function to calculate the distribution over the next word: ht + 1 = a special vector that encodes the history of the words it has seen so far."}, {"heading": "3.1 WORD-LEVEL TRAINING", "text": "We start with the simplest and most popular method that optimizes the loss of cross-entropy at each step. We then discuss a recently proposed modification of the standard training of cross-entropy, in which the model predictions are used explicitly during training. We conclude with a simple but novel approach that uses its model prediction during training and also has the ability to spread the gradients throughout the sequence. Although these enhancements tend to make the generation more robust, they still lack explicit monitoring at the sequence level."}, {"heading": "3.1.1 CROSS ENTROPY TRAINING (XENT)", "text": "This is especially true if the target sequence [w1, w2,..], if the target sequence [wt] 1 (wt, w1,.., wt], then the following loss function must be minimized: L = \u2212 log p (w1,.., wT) = - log T = 1 p (wt, w1),., wt \u2212 1) is used as a parametric function in Eq. 5. The above loss function trains the model to predict the next word well in each time step without taking the entire sequence into account. Training runs through time (Rumelhart et al, 1986; Mikolot)."}, {"heading": "3.1.2 DATA AS DEMONSTRATOR (DAD)", "text": "Conventional training with XENT suffers from exposure bias, as basic truth words are used in training as opposed to model predictions. DAD, proposed in (Venkatraman et al., 2015) and also used in (Bengio et al., 2015) for sequence generation, addresses this problem by mixing the basic truth training data with model predictions. At each step and with a certain probability, DAD takes either the prediction from the model in the previous time step or the basic truth data as input. Bengio et al. (2015) proposed different glow schedules for the probability of choosing the basic truth word. The glow schedules are such that the algorithm always chooses the basic truth words at the beginning. However, as the training progresses, the model predictions are selected more frequently, which has the effect of making the model somewhat more aware of how it is used at the test date. Figure 2 illustrates the algorithm."}, {"heading": "3.1.3 END-TO-END BACKPROP (E2E)", "text": "In our effort to bridge the gap between the way text generation models are trained and the way they are used, we also experimented with a novel modification of the standard training of RNs. This is perhaps the most natural and naive approach to approach sequence-level training, which can also be interpreted as a computationally efficient approach to searching for the beam. The key idea is that, in due course, we propagate step t + 1 as input of the uppermost k-words predicted at the previous time step, instead of the basic truth. Specifically, we take the output distribution over words from the next time step t and run them through a k-max layer. This layer dissects all but the largest values of the sequence and re-normalizes them to a sum. The re-normalized distribution is used as input in the current time step: {it + 1, j + 1, j} j-k = 1 x, max-j-x = 1 p)."}, {"heading": "3.2 SEQUENCE LEVEL TRAINING", "text": "We now present a novel algorithm for training at the sequence level, which we call Mixed Incremental Cross-Entropy Reinforce (MIXER). The proposed method not only avoids the problem of exposure bias, but is also optimized directly for the final evaluation metric, namely BLEU. Since MIXER can be regarded as an extension of the REINFORCE algorithm, we first describe the REINFORCE algorithm from the perspective of sequence generation."}, {"heading": "3.2.1 REINFORCE", "text": "To apply the REINFORCE algorithms, the EU is essentially a geometric mean of the reward. We can select any reward function we use. To apply the REINFORCE algorithms, we are essentially able to select the results of a reward that interacts with the external environment (the words and context vector are used as input at each step).The parameters of this agent define a policy, the execution of which leads to an action. In the sequence generation constellation, an action would refer to predicting the next word in the sequence. After taking an action, the agent updates its internal state (the hidden units of the RNN).Once the agent reaches the end of a reward function, we can select any reward function."}, {"heading": "3.2.2 MIXED INCREMENTAL CROSS-ENTROPY REINFORCE (MIXER)", "text": "The MIXER algorithm borrows ideas from both DAGGER (Ross et al., 2011) and DAD (Venkatraman et al., 2015; Bengio et al., 2015) and appropriately modifies the REINFORCE. The first key idea is to change the initial policy of REINFORCE to ensure that the model can effectively deal with the large scope of text generation. Instead of assuming a bad random policy and training of the model to approximate the optimal policy, we do the exact opposite. We start with a series of sequences w1,., wT, with their corresponding context c. Result: RNN optimized for generation initialize RNN randomly and place the model on the optimal policy, we do the exact opposite."}, {"heading": "4 EXPERIMENTS", "text": "In all our experiments, we train conditional RNNs by unfolding them up to a certain maximum length. We chose this length to cover about 95% of the target sets in the data sets we are looking at. The remaining sets are cut to the selected maximum length. For training, we use stochastic gradient lineage with size 32 mini-batches and reset the hidden states at the beginning of each sequence. Before updating the parameters, we re-scale the gradients if their standard is above 10 (Mikolov et al., 2010).For all tasks, we use the same architecture, namely a standard RNN with 128 hidden units. We search over the values of other hyperparameters, such as the initial learning rate, the different plan parameters, the number of epochs, etc., and then use a separate validation set. Then we take the model that performed best during validation, and calculate the probability of the EU test step (the one most likely to the following precision)."}, {"heading": "4.1 TEXT SUMMARIZATION", "text": "For the summary task, we consider only the problem of abstract summary, where, given a piece of \"source code,\" we aim to generate its summary (the \"target\" text).The data set we use to train and evaluate our models consists of a subset of the Gigaword Corpus (Graff et al., 2003), as described in Rush et al. (2015).This is a collection of news articles from different sources over the past two decades. Our version is organized as a set of sample pairs in which each pair is composed of the first sentence of a news article (the source set) and the heading of the corresponding news article (the target sentence).The summary task is then reduced to generating the target sentence. We apply the same preprocessing described in (Rush et al., 2015)."}, {"heading": "4.2 MACHINE TRANSLATION", "text": "We used data from the German-English machine translation track of the IWSLT 2014 evaluation campaign (Cettolo et al., 2014).The corpus consists of sentence-oriented subtitles of TED and TEDx presentations. We process the training data using the tokenizer of the Moses toolkit (Koehn et al., 2007) and remove each shell. The training data consists of approximately 160,000 sentences where the average English sentence is 17.5 words long and the average German sentence 18.5 words long. To maintain at least 95% of this data, we have unrolled our RNN in 25 steps. We linked dev2010 and dev2012 to a validation set of 2052 sentences. The test set consists of a combination of tst2010, tst2011 and tst2012 and comprises 4698 sentences. The English dictionary comprises 23328 words, while the German dictionary comprises 2964 words."}, {"heading": "4.3 IMAGE CAPTIONING", "text": "For captioning, we use the MSCOCO dataset (Lin et al., 2014). We use the entire training set of authors, which consists of approximately 80k images. We then take the original validation set (consisting of approximately 40k images) and sample 5000 images (without substitution) for validation and another 5000 for testing. There are 5 different captions for each image. At training time, we try one of these captions, while at test time, we report the maximum BLEU score over the five captions. For this task, the context is represented by 1024 features extracted from a Convolutional Neural Network (CNN) trained on the Imagenet dataset (Deng et al., 2009); we do not propagate these features backwards. We use a standard RNN as a generative model. Similar to (Bengio et al., 2015), the image attributes are provided to the first generative model in the sequence."}, {"heading": "4.4 RESULTS", "text": "In fact, it is that we are able to go in search of a solution that is both capable and capable of bringing about a solution. (...) We have to be able to bring about a solution. (...) We have to put ourselves in a position to find a solution. (...) We have to put ourselves in a position to find a solution. (...) We have to put ourselves in a position to find a solution. (...) We have to put ourselves in a position to find a solution. (...) We have to put ourselves in a position to find a solution. (...) We have to put ourselves in a position to find a solution. (...) We have to be able to find a solution."}, {"heading": "5 CONCLUSIONS", "text": "Our work is motivated by two major deficiencies in the formation of the current generative models for text generation: exposure bias and a loss that is not carried out at the sequence level. Reinforcement learning is a framework that can solve these problems. First, the model is used during training to generate and optimize a whole sequence of actions. Second, the reward does not have to be considered by single words, nor does it have to be differentiable. Therefore, we can simply and directly operate at the sequence level while training our model towards BLEU, our test time measurement. A challenge in amplification learning is that it struggles with very large spaces of action such as text generation. MIXER, the algorithm we propose, addresses this problem and allows successful training of amplification learning models for text generation. We achieve this by replacing the initial random policy with the optimal policy of a cross-entropy model and replacing the learning framework by saying more and more incrementally in one's own model."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors thank David Grangier, Tomas Mikolov, Leon Bottou, Ronan Collobert and Laurens van der Maaten for their insightful comments. We also thank Alexander M. Rush for his help in compiling the data set for the summary."}, {"heading": "6 SUPPLEMENTARY MATERIAL", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 MODELS", "text": "Input: Model p\u03b8, beam size k Result: word sequence [wg1, w g 2,.., w g n] empty piles {Ht} t = 1,... T; an empty hidden state vector h1; H1.push (1, [\u2205], h1]); for t \u2190 1 to T \u2212 1 dofor i \u2190 1 to min (k, # Ht) do (p, [w1, w2,.., wt], h]) \u2190 Ht.pop (); h \u2032 = os (w, h); for w \u2032 \u2190 k-most likely words w \u2032 from p\u0430 (w \u2032 | wt, h) dop \u2032 = p \u0445 p\u03b8 (w \u2032 w \u2032, h); Ht + 1.push (p \u2032, [w1, w2,., wam); endings (p \u2032, [w1, w2, wT], wh,."}, {"heading": "6.2 EXPERIMENTS", "text": "It's time for a fresh start, but it's far from over."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["S. Bengio", "O. Vinyals", "N. Jaitly", "N. Shazeer"], "venue": "In NIPS,", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Report on the 11th iwslt evaluation campaign", "author": ["M. Cettolo", "J. Niehues", "S. St\u00fcker", "L. Bentivogli", "M. Federico"], "venue": "In Proc. of IWSLT,", "citeRegEx": "Cettolo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cettolo et al\\.", "year": 2014}, {"title": "Search-based structured prediction as classification", "author": ["H. Daume III", "J. Langford", "D. Marcu"], "venue": "Machine Learning Journal,", "citeRegEx": "III et al\\.,? \\Q2009\\E", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "Imagenet: a large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.J. Li", "K. Li", "L. Fei-Fei"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Finding structure in time", "author": ["Elman", "Jeffrey L"], "venue": "Cognitive Science,", "citeRegEx": "Elman and L.,? \\Q1990\\E", "shortCiteRegEx": "Elman and L.", "year": 1990}, {"title": "Improved backing-off for M-gram language modeling", "author": ["Kneser", "Reinhard", "Ney", "Hermann"], "venue": "In Proc. of the International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "Kneser et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Kneser et al\\.", "year": 1995}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Koehn", "Philipp", "Hoang", "Hieu", "Birch", "Alexandra", "Callison-Burch", "Chris", "Federico", "Marcello", "Bertoldi", "Nicola", "Cowan", "Brooke", "Shen", "Wade", "Moran", "Christine", "Zens", "Richard", "Dyer", "Bojar", "Ondrej", "Constantin", "Herbst", "Evan"], "venue": "In Proc. of ACL Demo and Poster Sessions,", "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "An end-to-end discriminative approach to machine translation", "author": ["Liang", "Percy", "Bouchard-C\u00f4t\u00e9", "Alexandre", "Taskar", "Ben", "Klein", "Dan"], "venue": null, "citeRegEx": "Liang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2006}, {"title": "Microsoft coco: Common objects in context", "author": ["T.Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Dollar", "C.L. Zitnick"], "venue": "Technical report,", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Direct loss minimization for structured prediction", "author": ["D. McAllester", "T. Hazan", "J. Keshet"], "venue": "In NIPS,", "citeRegEx": "McAllester et al\\.,? \\Q2010\\E", "shortCiteRegEx": "McAllester et al\\.", "year": 2010}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafit", "L. Burget", "J. Cernock", "S. Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Hierarchical probabilistic neural network language model", "author": ["F. Morin", "Y. Bengio"], "venue": "In AISTATS,", "citeRegEx": "Morin and Bengio,? \\Q2005\\E", "shortCiteRegEx": "Morin and Bengio", "year": 2005}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.J. Zhu"], "venue": "In ACL,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["S. Ross", "G.J. Gordon", "J.A. Bagnell"], "venue": "In AISTATS,", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Expected bleu training for graphs: Bbn system description for wmt11 system combination task", "author": ["Rosti", "Antti-Veikko I", "Zhang", "Bing", "Matsoukas", "Spyros", "Schwartz", "Richard"], "venue": "In Proc. of WMT,", "citeRegEx": "Rosti et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rosti et al\\.", "year": 2011}, {"title": "Learning internal representations by backpropagating", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "errors. Nature,", "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["A.M. Rush", "S. Chopra", "J. Weston"], "venue": "In EMNLP,", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc"], "venue": "In Proc. of NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1988\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1988}, {"title": "Improving multi-step prediction of learned time series models", "author": ["A. Venkatraman", "M. Hebert", "J.A. Bagnell"], "venue": "In AAAI,", "citeRegEx": "Venkatraman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Venkatraman et al\\.", "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine Learning,", "citeRegEx": "Williams,? \\Q1992\\E", "shortCiteRegEx": "Williams", "year": 1992}, {"title": "Reinforcement learning neural turing machines", "author": ["W. Zaremba", "I. Sutskever"], "venue": "Technical report,", "citeRegEx": "Zaremba and Sutskever,? \\Q2015\\E", "shortCiteRegEx": "Zaremba and Sutskever", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "Popular choices for text generation models are language models based on n-grams (Kneser & Ney, 1995), feed-forward neural networks (Morin & Bengio, 2005), and recurrent neural networks (RNNs; Mikolov et al., 2010).", "startOffset": 185, "endOffset": 213}, {"referenceID": 15, "context": "Training these models to directly optimize for BLEU is hard because a) it is not differentiable (Rosti et al., 2011), and b) combinatorial optimization is required to determine which sub-string maximizes BLEU given some context.", "startOffset": 96, "endOffset": 116}, {"referenceID": 10, "context": "Prior attempts (McAllester et al., 2010) at optimizing BLEU have been restricted to linear models.", "startOffset": 15, "endOffset": 40}, {"referenceID": 10, "context": "Prior attempts (McAllester et al., 2010) at optimizing BLEU have been restricted to linear models. This paper proposes a novel training algorithm which results in improved text generation compared to standard models. The algorithm tries to address the two issues discussed above. First, while training the generative model we avoid the exposure bias by using model predictions at training time. Second, we directly optimize for our final evaluation metric, namely BLEU. Our proposed methodology borrows ideas from the reinforcement learning literature (Sutton & Barto, 1988). In particular, we build on the REINFORCE algorithm proposed by Williams (1992), to achieve the", "startOffset": 16, "endOffset": 655}, {"referenceID": 1, "context": "We compare against several strong baselines, including, RNNs trained with cross-entropy and Data as Demonstrator (DAD) (Bengio et al., 2015; Venkatraman et al., 2015).", "startOffset": 119, "endOffset": 166}, {"referenceID": 20, "context": "We compare against several strong baselines, including, RNNs trained with cross-entropy and Data as Demonstrator (DAD) (Bengio et al., 2015; Venkatraman et al., 2015).", "startOffset": 119, "endOffset": 166}, {"referenceID": 18, "context": "At test time, it is common to use beam search to explore multiple alternative paths (Sutskever et al., 2014; Bahdanau et al., 2015; Rush et al., 2015).", "startOffset": 84, "endOffset": 150}, {"referenceID": 0, "context": "At test time, it is common to use beam search to explore multiple alternative paths (Sutskever et al., 2014; Bahdanau et al., 2015; Rush et al., 2015).", "startOffset": 84, "endOffset": 150}, {"referenceID": 17, "context": "At test time, it is common to use beam search to explore multiple alternative paths (Sutskever et al., 2014; Bahdanau et al., 2015; Rush et al., 2015).", "startOffset": 84, "endOffset": 150}, {"referenceID": 13, "context": "While this improves generation by typically one or two BLEU points (Papineni et al., 2002), it also comes at a cost: it makes generation at least k times slower, where k is the number of active paths in the beam (see Sec.", "startOffset": 67, "endOffset": 90}, {"referenceID": 20, "context": "The oracle issue was later addressed by an algorithm called Data As Demonstrator (DAD) (Venkatraman et al., 2015), whereby the target action at step k is the k-th action taken by the optimal policy (ground truth sequence) regardless of which input is fed to the system, whether it is ground truth, or the model\u2019s prediction.", "startOffset": 87, "endOffset": 113}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015; Rush et al., 2015). While this improves generation by typically one or two BLEU points (Papineni et al., 2002), it also comes at a cost: it makes generation at least k times slower, where k is the number of active paths in the beam (see Sec. 3.1.1 for more details). The key idea of this work is to improve generation by letting the model use its own predictions at training time, as first advocated by Daume III et al. (2009). In their seminal work, the authors first noticed that structured prediction problems can be cast as a particular instance of reinforcement learning, and they then proposed SEARN, an algorithm to learn such structured prediction tasks.", "startOffset": 8, "endOffset": 458}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015; Rush et al., 2015). While this improves generation by typically one or two BLEU points (Papineni et al., 2002), it also comes at a cost: it makes generation at least k times slower, where k is the number of active paths in the beam (see Sec. 3.1.1 for more details). The key idea of this work is to improve generation by letting the model use its own predictions at training time, as first advocated by Daume III et al. (2009). In their seminal work, the authors first noticed that structured prediction problems can be cast as a particular instance of reinforcement learning, and they then proposed SEARN, an algorithm to learn such structured prediction tasks. The basic idea is to let the model use its own predictions at training time to produce a sequence of actions (e.g., the choice of the next word). Then, a search algorithm is run to determine the optimal action at each time step, and a classifier (a.k.a. policy) is trained to predict that action. A similar idea was later proposed by Ross et al. (2011) in an imitation learning framework.", "startOffset": 8, "endOffset": 1047}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015; Rush et al., 2015). While this improves generation by typically one or two BLEU points (Papineni et al., 2002), it also comes at a cost: it makes generation at least k times slower, where k is the number of active paths in the beam (see Sec. 3.1.1 for more details). The key idea of this work is to improve generation by letting the model use its own predictions at training time, as first advocated by Daume III et al. (2009). In their seminal work, the authors first noticed that structured prediction problems can be cast as a particular instance of reinforcement learning, and they then proposed SEARN, an algorithm to learn such structured prediction tasks. The basic idea is to let the model use its own predictions at training time to produce a sequence of actions (e.g., the choice of the next word). Then, a search algorithm is run to determine the optimal action at each time step, and a classifier (a.k.a. policy) is trained to predict that action. A similar idea was later proposed by Ross et al. (2011) in an imitation learning framework. Unfortunately, for text generation it is generally intractable to compute an oracle of the optimal target word given the words predicted so far. The oracle issue was later addressed by an algorithm called Data As Demonstrator (DAD) (Venkatraman et al., 2015), whereby the target action at step k is the k-th action taken by the optimal policy (ground truth sequence) regardless of which input is fed to the system, whether it is ground truth, or the model\u2019s prediction. This idea was also recently tested for text generation applications by Bengio et al. (2015), who had the same motivation as our work (see Sec.", "startOffset": 8, "endOffset": 1645}, {"referenceID": 11, "context": "In this work, we focus on Recurrent Neural Networks (RNNs) as they are a popular choice for text generation (Mikolov et al., 2010).", "startOffset": 108, "endOffset": 130}, {"referenceID": 11, "context": "In this paper, we use standard RNNs (Elman, 1990; Mikolov et al., 2010) unless otherwise specified.", "startOffset": 36, "endOffset": 71}, {"referenceID": 16, "context": "Training proceeds by truncated backpropagation through time (Rumelhart et al., 1986; Mikolov et al., 2010).", "startOffset": 60, "endOffset": 106}, {"referenceID": 11, "context": "Training proceeds by truncated backpropagation through time (Rumelhart et al., 1986; Mikolov et al., 2010).", "startOffset": 60, "endOffset": 106}, {"referenceID": 1, "context": "Figure 2: Illustration of DAD (Bengio et al., 2015; Venkatraman et al., 2015).", "startOffset": 30, "endOffset": 77}, {"referenceID": 20, "context": "Figure 2: Illustration of DAD (Bengio et al., 2015; Venkatraman et al., 2015).", "startOffset": 30, "endOffset": 77}, {"referenceID": 20, "context": "DAD, proposed in (Venkatraman et al., 2015) and also used in (Bengio et al.", "startOffset": 17, "endOffset": 43}, {"referenceID": 1, "context": ", 2015) and also used in (Bengio et al., 2015) for sequence generation, addresses this issue by mixing the ground truth training data with model predictions.", "startOffset": 25, "endOffset": 46}, {"referenceID": 1, "context": ", 2015) and also used in (Bengio et al., 2015) for sequence generation, addresses this issue by mixing the ground truth training data with model predictions. At each time step and with a certain probability, DAD takes as input either the prediction from the model at the previous time step or the ground truth data. Bengio et al. (2015) proposed different annealing schedules for the probability of choosing the ground truth word.", "startOffset": 26, "endOffset": 337}, {"referenceID": 21, "context": "In order to apply the REINFORCE algorithm (Williams, 1992; Zaremba & Sutskever, 2015) to the problem of sequence generation we cast our problem in the reinforcement learning (RL) framework (Sutton & Barto, 1988).", "startOffset": 42, "endOffset": 85}, {"referenceID": 13, "context": "Here, we use BLEU (Papineni et al., 2002) since this is the metric we use at test time.", "startOffset": 18, "endOffset": 41}, {"referenceID": 8, "context": "In order to avoid our reward function to evaluate to zero for sequences where there are no higher order n-gram matches, we resort to smoothing the BLEU score, similarly to Liang et al. (2006).2 Like in imitation learning, we have a training set of optimal sequences of The smoothing constant is set to 0.", "startOffset": 172, "endOffset": 192}, {"referenceID": 21, "context": "We refer the reader to prior work (Zaremba & Sutskever, 2015; Williams, 1992) for the full derivation of the gradients.", "startOffset": 34, "endOffset": 77}, {"referenceID": 14, "context": "The MIXER algorithm borrows ideas both from DAGGER (Ross et al., 2011) and DAD (Venkatraman et al.", "startOffset": 51, "endOffset": 70}, {"referenceID": 20, "context": ", 2011) and DAD (Venkatraman et al., 2015; Bengio et al., 2015) and modifies the REINFORCE appropriately.", "startOffset": 16, "endOffset": 63}, {"referenceID": 1, "context": ", 2011) and DAD (Venkatraman et al., 2015; Bengio et al., 2015) and modifies the REINFORCE appropriately.", "startOffset": 16, "endOffset": 63}, {"referenceID": 11, "context": "Before updating the parameters we re-scale the gradients if their norm is above 10 (Mikolov et al., 2010).", "startOffset": 83, "endOffset": 105}, {"referenceID": 17, "context": "We apply the same pre-processing described in (Rush et al., 2015), which consists of lower-casing and removal of very infrequent words.", "startOffset": 46, "endOffset": 65}, {"referenceID": 17, "context": "Our generative model is a conditional RNN, where the conditioning is provided by a convolutional attention module similar to the one described in (Rush et al., 2015).", "startOffset": 146, "endOffset": 165}, {"referenceID": 0, "context": "Then, the actual context vector ct is computed as a weighted sum of these st, where the weights are computed via a softmax on the dot products between the current hidden state ht and the vectors st themselves, a mechanism known as attention (Bahdanau et al., 2015).", "startOffset": 241, "endOffset": 264}, {"referenceID": 16, "context": ", 2003) as described in Rush et al. (2015). This is a collection of news articles taken from different sources over the past two decades.", "startOffset": 24, "endOffset": 43}, {"referenceID": 2, "context": "We use data from the German-English machine translation track of the IWSLT 2014 evaluation campaign (Cettolo et al., 2014).", "startOffset": 100, "endOffset": 122}, {"referenceID": 7, "context": "We pre-process the training data using the tokenizer of the Moses toolkit (Koehn et al., 2007) and remove any casing.", "startOffset": 74, "endOffset": 94}, {"referenceID": 9, "context": "For the image captioning task, we use the MSCOCO dataset (Lin et al., 2014).", "startOffset": 57, "endOffset": 75}, {"referenceID": 4, "context": "For this task, the context is represented by 1024 features extracted by a Convolutional Neural Network (CNN) trained on the Imagenet dataset (Deng et al., 2009); we do not back-propagate through these features.", "startOffset": 141, "endOffset": 160}, {"referenceID": 1, "context": "Similar to (Bengio et al., 2015), the image features are provided to the generative model as the first word in the sequence.", "startOffset": 11, "endOffset": 32}], "year": 2017, "abstractText": "Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the BLEU score: a popular metric to compare a sequence to a reference. On three different tasks, our approach outperforms several strong baselines for greedy generation, and it matches their performance with beam search, while being several times faster.", "creator": "LaTeX with hyperref package"}}}