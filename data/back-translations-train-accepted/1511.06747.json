{"id": "1511.06747", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2015", "title": "Data-Dependent Path Normalization in Neural Networks", "abstract": "We propose a unified framework for neural net normalization, regularization and optimization, which includes Path-SGD and Batch-Normalization and interpolates between them across two different dimensions. Through this framework we investigate issue of invariance of the optimization, data dependence and the connection with natural gradients.", "histories": [["v1", "Fri, 20 Nov 2015 20:27:45 GMT  (107kb,D)", "http://arxiv.org/abs/1511.06747v1", "12 pages, 3 figures"], ["v2", "Mon, 14 Dec 2015 20:52:51 GMT  (109kb,D)", "http://arxiv.org/abs/1511.06747v2", "14 pages, 3 figures"], ["v3", "Fri, 8 Jan 2016 20:13:03 GMT  (134kb,D)", "http://arxiv.org/abs/1511.06747v3", "17 pages, 3 figures"], ["v4", "Tue, 19 Jan 2016 20:57:47 GMT  (294kb,D)", "http://arxiv.org/abs/1511.06747v4", "17 pages, 3 figures"]], "COMMENTS": "12 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["behnam neyshabur", "ryota tomioka", "ruslan salakhutdinov", "nathan srebro"], "accepted": true, "id": "1511.06747"}, "pdf": {"name": "1511.06747.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Behnam Neyshabur", "Ryota Tomioka"], "emails": ["bneyshabur@ttic.edu", "ryoto@microsoft.com", "rsalakhu@cs.toronto.edu", "nati@ttic.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "The choice of the optimization method for non-convex, over-parameterized models such as the feed-forward network is crucial for the success of learning - not only does it influence the runtime to convergence, but there are also impacts that play an important role in regulating and generalizing the resulting model. Optimization methods are inherently tied to a choice of geometry over the parameter space, which in turn induces a geometry over the model space that plays an important role in regulation and generalization (Neyshabur et al). Last year, two efficient alternative optimization approaches for the feed-forward networks based on intuitions about geometry and the geometry of the parameter space were proposed: Path-SGD (Neyshabur et al)."}, {"heading": "2 A UNIFIED FRAMEWORK", "text": "We define a measure for each node as follows: \u03b32v (w) = w > \u2192 vRvw \u2192 v (2), where Rv is a positive semidefinitive matrix. We focus on a specific class of matrices Rv; that is, for each arbitrary node v \u2264 1 we can consider the following measure: Rv = \u03b1E [yN in (v) y > N in (v) + (1 \u2212 \u03b1) diag (\u03b32N in (v) (w)))). (3) Therefore, for each node v the measurement variable \u03b3 2v (w) = \u03b1E [z2v] + (1 \u2212 \u03b1) w > \u2192 v diag (\u03b32N in (v)) w \u2192 v (v / \u30fb Vin), (4) where for each v \u00b2 Vin a normalization can be specified, we define a normalization unit v = 1. Intuitively, \u03b32v is a convex combination of the variance of the input nov and a regulation."}, {"heading": "3 BATCH-NORMALIZED NETWORKS", "text": "Here we formulate the batch normalization based on the measurement quantity. In the batch normalization formula, for each node v the output of the linear layer prior to the activations corresponding to the repairment is normalized. Suppose we use mini-batches of size n during the training to estimate the second moments. Then, the gradients can be calculated as follows: \"W \u2192 v = n x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "4 APPROXIMATE STEEPEST DESCENT OPTIMIZATION", "text": "We now consider the steepest downward direction with respect to the DDP regulator \u03b32. Note that \u03b32net is not convex. However, we can approximate \u03b32net with a convex square function \u03b3-2 net. For each parameter w, if the diagonal of the Hessian H\u03b32net has positive entries, we can approximate the value 2 net with \u03b3-2net, where the Hessian H\u03b3-2net is the diagonal of H\u03b3-2net. Now, the Bregman divergence for \u03b3-2 net can be written as follows: DGP-2net (w, w \") = w-w-w-w-w-2net = E-E-E-W-W-W-2e (we-w\" e) 2 (9) Therefore, the steepest downward actualization with respect to the distance DGP-2net (w, w \") can be described as follows: w (t + 1) min-w-W-W-W-W-W-W-W (W-W-W), W-W-W-W-W-W-W (W-W-W), W-W-W-W-W-W-W (W-W-W)."}, {"heading": "4.1 IMPLEMENTATION", "text": "To calculate the second derivatives, we must first calculate the first derivatives. Instead, the second derivatives can occur through the two derivatives Z (i) and Z (i), but this makes it difficult to find the second derivatives. Instead, we propagate the loss through Z (i), Z (i), Z (i), Z (i) and Z (i), with the net derivatives 2u = (1 \u2212), V (u), N (u), 2Net derivatives 2v (u), V (11), V (i), Z (i) and Z (z), Z (i), Z (z), Z (z), Z (z), Z (z), Z (z), Z (z), Z (z), Z (z), Z (z, Z, Z, Z, Z (z), Z (z), Z (z), Z (z), Z (z, Z, Z, Z, Z (z), Z (z), Z, Z (z), Z (z), Z (z, Z, Z, Z, Z (z), Z (z, Z), Z (z, Z, Z, Z), Z (z, Z, Z, Z (z, Z), Z (z, Z, Z), Z (z, Z, Z), Z (z, Z, Z), Z (z, Z, Z, Z), Z (z, Z, Z), Z (z, Z, Z, Z), Z (z, Z), Z (z, Z, Z, Z), Z (z, Z, Z), Z (z, Z, Z, Z, Z, Z, Z, Z (z, Z), Z (z, Z), Z (z, Z, Z), Z (z, Z, Z, Z, Z (z, Z), Z (z, Z, Z, Z, Z, Z, Z, Z, Z (z, Z), Z (z, Z, Z), Z, Z, Z (z, Z, Z, Z), Z (z, Z, Z (z, Z), Z, Z, Z"}, {"heading": "4.2 RELATION TO NATURAL GRADIENT", "text": "In this section we show that DDP-SGD is a diagonal approximation to the natural gradient. In this case, the update step normalizes each dimension of the gradient with the corresponding element on the diagonal of the Fisher information matrix: w (t + 1) e = we \u2212 F (w) [e, e] x L (w (t)). (17) For a multivariate Gauss with unit variance and centralized on the output of the network, namely log q (c | x) = 12 x \u00b2 v \u00b2 Vout (cv \u2212 zv \u00b2) 2 + const., the diagonal can be calculated as: F (w) [u \u2192 v] [u \u2192 v] = Ex \u00b2 p (x) [y2u \u00b2 v \u00b2 v \u00b2 s \u00b2 sov. \""}, {"heading": "5 NODE-WISE INVARIANCE", "text": "We also say that an update rule A: R | E | R | E | invariant to transformation T, if and only if fA (w) = fA (T (w)). It is desirable that an update rule has the same invariances as its inputs (Neyshabur et al., 2015a).An example of invariance in feedforward networks is node recalculation (or realignment).For each positive scale rule and for each internal node v (v / B Vin and v / E Vout) the following transformation w = T (w) fw = fw: w = w = w = w \u2192 j (j \u00b2 N out (v), (20) w \u00b2 k \u2192 v = \u03b1 \u2212 1wk \u2192 v (k \u00b2 N in (v)."}, {"heading": "5.1 DDP-SGD ON FEEDFORWARD NETWORKS", "text": "To see if DDP-SGD rescales also node-wise invariantly, we calculate the update rule in Equation (20) for w-v-j: w-v-v-j = \u03b1wv-j-\u03b1wv-j (w) \u2202 L\u03b1-wv-j (wv-j) = \u03b1 (w-\u03b3v-j-j) \u0445 L \u2202 wv-j (wv-j-j) = \u03b1w + v-jTherefore, DDP-SGD is node-wise invariant."}, {"heading": "5.2 SGD ON BATCH NORMALIZED NETWORKS", "text": "The SGD update rule for batch normalized networks is non-invariant compared to the invariances of the network. Batch normalized networks are not nodes invariant. Instead, they are invariant to recalculate the incoming weights of the nodes. To verify the invariance of the SGD, consider the following transformation for a scalar \u03b1 > 0: w \u00b2 k \u2192 v = \u03b1wk \u2192 v (k \u00b2 N in (v)) The batch normalized network is invariant compared to the above transformation because the output of each node is normalized. However, the SGD update rule is not invariant for this transformation: w \u00b2 + k \u2192 v = \u03b1wk \u2192 v \u2212 \u03b7 L\u03b1 = wk \u2192 v (wk \u2192 v) 6 = \u03b1 (wk \u2192 v \u2212."}, {"heading": "6 UNDERSTANDING INVARIANCES", "text": "The neural network model fw (x) can alternatively be expressed as the sum of all directed paths from each input node to each output node as follows: fw (x) [v] = \u2211 p-gp (v) gp (x) \u00b7 \u03c0p (w) \u00b7 x [head (p)], (21) where \u044b (v) is the set of all directed paths from each input node to v, head (p) is the first node of path p, gp (x) 1 takes if all rectilinear units along path p are active and otherwise zero, and\u03c0p (w) = \u03c6e E (p) w (e) (22) is the product of the weights along path p; E (p) denotes the set of edges appearing along path p."}, {"heading": "6.1 CONTINUOUSLY DEFORMABLE TRANSFORMATIONS", "text": "We say that network G is invariant to transformation T: R | E | R | E |, if fw = fT (w). Another example of transformation T is that fw = fT (w) switches two units. Suppose two units u, v \u00b2 V share the same set of children and parents. This is typically the case for internal nodes of fully connected layers. Then we can define w = T (w) so that this transformation is smooth (it is indeed linear), but not continuously deformable in the sense that the two parameter vectors w and T (w) can be immensely distant in parameter space. Invariances that are not continuously deformable."}, {"heading": "6.2 THE FISHER INFORMATION AND NATURAL GRADIENT", "text": "One approach to tackling the problem of invariance for training neural networks is the natural gradient algorithm q (Amari, 1998); using the path-based definition of the network model (21), we have a log q (c | x); so we have a log q (c \u00b7 x); we have a log q (c \u00b7 x); we have a log q (c \u00b7 x); we have a log q (c \u00b7 x); we can log (c \u00b7 x); we (c \u00b7 x); we (c \u00b7 x); we (c \u00b7 x); we (c \u00b7 x); we (c \u00b7 \u00b7 x); we (c \u00b7 \u00b7 \u00b7 x); we (c \u00b7 \u00b7 \u00b7 x); we (c \u00b7 x); we (c \u00b7 x); we (c \u00b7 x); we (c); c (c); we (c); c (c); we (c) x; we (c); c (c); we (c); c (c); we (c) x; c); c (we (c); c) x; c); c) we (c); c) we (we (c); c) we (c) x; c) we (c); c) we (c) we (c); c) we (we (c) x; c) we (c) we (c); c) we (we (c) x; c) we (we (c); c) we (c) x; c) we (we (c); c) we (we (c) x; c) we (we (c) x; c) we (we (c); c) we (c) we (we (c) x; c); c) we (we (we (we (c); c) x; c) we (we (we (c); c) we (we (we (c) x) x; c) we (we (we (c) x); c) we (we (we (c) x; c) we (we (we (c) x; c) we (c) we (we (we (c) x) x); c) we (we (we (we (c) x"}, {"heading": "6.3 THE JACOBIAN", "text": "In order to reduce the distribution dependence of the parameters, we must take a closer look at the structure of the rectified functions. (21) We must ask ourselves whether the distribution relations in the world really are as they are. (21) We must ask ourselves whether the distribution relations in the world really are as they are. (21) We must ask ourselves whether the distribution relations in the world really are as they are. (21) We must understand the distribution mechanisms in the world, how the distribution mechanisms function in the world. (21) We must understand the distribution mechanisms in the world, how the distribution mechanisms function in the world. (21) We must understand the distribution mechanisms in the world, how the distribution mechanisms function in the world. (21) We must understand the distribution mechanisms in the world, how the distribution mechanisms function in the world, how the distribution mechanisms function in the world, how the world functions in the world, world in the world, world in the world, world in the world, world in the world, world in the world, in the world, in the world in the world, world in the world, in the world in the world, world in the world, in the world, world in the world in the world, in the world, world in the world in the world, world in the world in the world, world in the world, in the world in the world, world in the world, in the world in the world, world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world, in the world in the world in the world, in the world, in the world in the world, in the, in the world, in the world in the world in the, in the world, in the, in the world in the world in the, in the, in the world in the world in the, in the, in the, in the world in the, in the world in the, in the, in the world in the, in the, in the world in the, in the, in the, in the, in the world in the, in the, in the, in the world in the, in the, in the world in the, in the, in the, in the world in the, in the, in the, in the, in the, in the"}, {"heading": "A NATURAL GRADIENT", "text": "The natural gradient algorithm achieves invariance on the local repair parameter by applying the q layer (q layer) q layer (q layer) to the current parameter w (t) to the negative gradient direction as follows: w (t + 1) = w (t) + p (natural), where \u2206 (natural) = argmin \u00b2 R | E | p layer. (31) Here F (w) is the Fisher information matrix at point W and is defined in relation to the probable vision of the neural network model (1), which we describe in more detail."}, {"heading": "B PROOF OF THEOREM 2", "text": "In fact, it is the case that most of us are able to hide, and that they will be able to abide by the rules, \"he said in an interview with the\" Welt am Sonntag. \""}], "references": [{"title": "Natural gradient works efficiently in learning", "author": ["Amari", "Shun-Ichi"], "venue": "Neural computation,", "citeRegEx": "Amari and Shun.Ichi.,? \\Q1998\\E", "shortCiteRegEx": "Amari and Shun.Ichi.", "year": 1998}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In AISTATS,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Scaling up natural gradient by sparsely factorizing the inverse Fisher matrix", "author": ["Grosse", "Roger", "Salakhudinov", "Ruslan"], "venue": "In ICML,", "citeRegEx": "Grosse et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grosse et al\\.", "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "In ICML,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Exploring strategies for training deep neural networks", "author": ["Larochelle", "Hugo", "Bengio", "Yoshua", "Louradour", "J\u00e9r\u00f4me", "Lamblin", "Pascal"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Larochelle et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2009}, {"title": "Springer Verlag, 1998", "author": ["Le Cun", "Yann", "Bottou", "L\u00e9on", "Orr", "Genevieve B.", "M\u00fcller", "Klaus-Robert. Efficient backprop. In Neural Networks", "Tricks of the Trade", "Lecture Notes in Computer Science LNCS"], "venue": "URL http://leon.bottou.org/papers/lecun-98x.", "citeRegEx": "Cun et al\\.,? 1524", "shortCiteRegEx": "Cun et al\\.", "year": 1524}, {"title": "Deep learning via hessian-free optimization", "author": ["Martens", "James"], "venue": "In ICML,", "citeRegEx": "Martens and James.,? \\Q2010\\E", "shortCiteRegEx": "Martens and James.", "year": 2010}, {"title": "Optimizing neural networks with Kronecker-factored approximate curvature", "author": ["Martens", "James", "Grosse", "Roger"], "venue": "In ICML,", "citeRegEx": "Martens et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Martens et al\\.", "year": 2015}, {"title": "Path-SGD: Path-normalized optimization in deep neural networks", "author": ["Neyshabur", "Behnam", "Salakhutdinov", "Ruslan", "Srebro", "Nathan"], "venue": "In NIPS,", "citeRegEx": "Neyshabur et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neyshabur et al\\.", "year": 2015}, {"title": "Norm-based capacity control in neural networks", "author": ["Neyshabur", "Behnam", "Tomioka", "Ryota", "Srebro", "Nathan"], "venue": "In COLT,", "citeRegEx": "Neyshabur et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neyshabur et al\\.", "year": 2015}, {"title": "In search of the real inductive bias: On the role of implicit regularization in deep learning", "author": ["Neyshabur", "Behnam", "Tomioka", "Ryota", "Srebro", "Nathan"], "venue": "International Conference on Learning Representations (ICLR) workshop track,", "citeRegEx": "Neyshabur et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neyshabur et al\\.", "year": 2015}, {"title": "Revisiting natural gradient for deep networks", "author": ["Pascanu", "Razvan", "Bengio", "Yoshua"], "venue": "In ICLR,", "citeRegEx": "Pascanu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2014}, {"title": "Topmoumoute online natural gradient algorithm", "author": ["Roux", "Nicolas L", "Manzagol", "Pierre-Antoine", "Bengio", "Yoshua"], "venue": "In NIPS,", "citeRegEx": "Roux et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Roux et al\\.", "year": 2008}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Sutskever", "Ilya", "Martens", "James", "Dahl", "George", "Hinton", "Geoffrey"], "venue": "In ICML,", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Krylov subspace descent for deep learning", "author": ["Vinyals", "Oriol", "Povey", "Daniel"], "venue": "In ICML,", "citeRegEx": "Vinyals et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 4, "context": "RELATED WORKS There has been an ongoing effort for better understanding of the optimization in deep networks and several heuristics have been suggested to improve the training (Le Cun et al., 1998; Larochelle et al., 2009; Glorot & Bengio, 2010; Sutskever et al., 2013).", "startOffset": 176, "endOffset": 269}, {"referenceID": 13, "context": "RELATED WORKS There has been an ongoing effort for better understanding of the optimization in deep networks and several heuristics have been suggested to improve the training (Le Cun et al., 1998; Larochelle et al., 2009; Glorot & Bengio, 2010; Sutskever et al., 2013).", "startOffset": 176, "endOffset": 269}, {"referenceID": 12, "context": "Pascanu & Bengio (2014) also discusses the connections between Natural Gradients and some of the other proposed methods for training neural networks: Hessian-Free Optimization (Martens, 2010), Krylov Subspace Descent (Vinyals & Povey, 2011) and TONGA (Roux et al., 2008).", "startOffset": 251, "endOffset": 270}, {"referenceID": 4, "context": ", 1998; Larochelle et al., 2009; Glorot & Bengio, 2010; Sutskever et al., 2013). In order to incorporate the curvature information, several methods have proposed different ways to approximate Fisher information matrix in deep learning models (Desjardins et al., 2015; Martens & Grosse, 2015; Grosse & Salakhudinov, 2015). Pascanu & Bengio (2014) also discusses the connections between Natural Gradients and some of the other proposed methods for training neural networks: Hessian-Free Optimization (Martens, 2010), Krylov Subspace Descent (Vinyals & Povey, 2011) and TONGA (Roux et al.", "startOffset": 8, "endOffset": 346}], "year": 2017, "abstractText": "We propose a unified framework for neural net normalization, regularization and optimization, which includes Path-SGD and Batch-Normalization and interpolates between them across two different dimensions. Through this framework we investigate issue of invariance of the optimization, data dependence and the connection with natural gradients.", "creator": "LaTeX with hyperref package"}}}