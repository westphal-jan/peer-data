{"id": "1511.06856", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2015", "title": "Data-dependent Initializations of Convolutional Neural Networks", "abstract": "Convolutional Neural Networks spread through computer vision like a wildfire, impacting almost all visual tasks imaginable. Despite this, few researchers dare to train their models from scratch. Most work builds on one of a handful of ImageNet pre-trained models, and fine-tunes or adapts these for specific tasks. This is in large part due to the difficulty of properly initializing these networks from scratch. A small miscalibration of the initial weights leads to vanishing or exploding gradients, as well as poor convergence properties. In this work we present a fast and simple data-dependent initialization procedure, that sets the weights of a network such that all units in the network train at roughly the same rate, avoiding vanishing or exploding gradients. Our initialization matches the current state-of-the-art unsupervised or self-supervised pre-training methods on standard computer vision tasks, such as image classification and object detection, while being roughly three orders of magnitude faster. When combined with pre-training methods, our initialization significantly outperforms prior work, narrowing the gap between supervised and unsupervised pre-training.", "histories": [["v1", "Sat, 21 Nov 2015 09:07:08 GMT  (1809kb,D)", "http://arxiv.org/abs/1511.06856v1", "12 pages, Under review at ICLR 2016"], ["v2", "Fri, 29 Apr 2016 03:36:16 GMT  (1960kb,D)", "http://arxiv.org/abs/1511.06856v2", "ICLR 2016"], ["v3", "Thu, 22 Sep 2016 22:14:17 GMT  (1951kb,D)", "http://arxiv.org/abs/1511.06856v3", "ICLR 2016"]], "COMMENTS": "12 pages, Under review at ICLR 2016", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["philipp kr\\\"ahenb\\\"uhl", "carl doersch", "jeff donahue", "trevor darrell"], "accepted": true, "id": "1511.06856"}, "pdf": {"name": "1511.06856.pdf", "metadata": {"source": "CRF", "title": "CONVOLUTIONAL NEURAL NETWORKS", "authors": ["Philipp Kr\u00e4henb\u00fchl", "Carl Doersch", "Jeff Donahue", "Trevor Darrell"], "emails": ["philkr@eecs.berkeley.edu;", "jdonahue@eecs.berkeley.edu;", "trevor@eecs.berkeley.edu;", "cdoersch@cs.cmu.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "In recent years, Convolutionary Neural Networking (CNNs) has expanded to improve performance in many areas of computer vision (Szegedy et al., 2015; Simonyan & Zisserman, 2015; Girshick, 2015), much of which stems from CNNs \"ability to make better use of large datasets than previous methods. Indeed, good performance seems to require large datasets: the most coordinated methods usually start with\" pre-training \"CNNs to solve the challenge of the million-fold ImageNet classification (Russakovsky et al., 2015). This\" pre-trained \"representation then requires large datasets\" over smaller datasets, where target labeling can be more expensive. Generally, these datasets do not completely restrict CNN learning: different initializations can be trained until they achieve equally high training performance, but they will often perform very differently at test times."}, {"heading": "2 PRELIMINARIES", "text": "We are interested in the parameterization (and re-parameterization) of CNNs, where output is a highly nonconvex function of both the input and the parameters. Therefore, we start with a notation that describes how the behavior of a CNN will change when we change the parameters. We focus on feed-oriented networks of formzk = fk (zk \u2212 1; \u03b8k), where zk is a vector of hidden activations of the network, and fk is a transformation with parameters \u03b8k. fk can be a linear transformation fk (z \u2032 k; \u03b8k) = Wkzk \u2212 1 + bk, or it can be a vector of hidden activations of the network, and fk is a transformation with parameters successk + 1 (z \u2032 k), like a linear transformation fk (z \u2032 k; empirical transformation fk), like a linear transformation (z \u2032 k; empirical transformation)."}, {"heading": "3 DATA-DEPENDENT INITIALIZATION", "text": "Considering an N-layer neural network with loss function \"(zN), we first define C2i, j, k to the expected norm of gradients in terms of weights Wk (i, j) in layer k (i, j) in layer k (i, j) in layer k (i, j) in layer k (i, j) in layer k (i) in layer k (i) in layer k (i) in layer k (i). (1), where D is a set of input images (160 in all our experiments) and yk is the retrograde error. Similar considerations can be applied to the bias bk, but where the activations are replaced by the constant 1. In order not to rely on any labels during initialization, we use a random linear loss function\" (zN) =. \""}, {"heading": "3.1 WITHIN-LAYER WEIGHT NORMALIZATION", "text": "Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Scripting-Scripting-Script-Script-Script-Script-Scripting-Script-Script-Script-Script-Script-Script-Script-Script-Script-Scripting-Script-Script-Script-Script-Script-Script-Scripting-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Scripting-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Scripting-Script-Script-Script-Script-Script-Script-Script-Scripting-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Script-Scri"}, {"heading": "3.2 BETWEEN-LAYER SCALE ADJUSTMENT", "text": "Since the initialization specified in Section 3.1 leads to activations zk (i) with unit variance, the expected rate of change C2k, i of a column i of the weight matrix Wk is constant in all columns i, taking into account the approximation rate specified in Equation (4). However, this does not guarantee the scaling of the change rates between layers. We use an iterative method to obtain approximately constant parameter change rates C2k, i across all layers k (as well as all columns i within a layer), since previously initialized weight differences in this behavior are prevented. At each iteration, we estimate the average change ratio (C-k, i, j) per layer. We also estimate a global change algorithm 2 between layer normalization k (as all columns i within a layer), since these initialized weight differences are closer to the layer. At each iteration, we estimate the average change ratio per layer (C-j), as all columns within a layer."}, {"heading": "3.3 WEIGHT INITIALIZATIONS", "text": "So far, we have used a random Gaussian initialization of weights, but our approach does not require it. Therefore, we investigated two data-driven initializations: a PCA-based initialization and a k-mean-based initialization. In PCA-based initialization, we set the weights so that the layer outputs are white and decorative. For each layer k, we record the activations of the characteristics zk \u2212 1 of each channel c across all spatial locations and over 160 images. Then, we use the first M major components of these activations as our weight matrix W (k). In k-mean-based initialization, we follow Coates & Ng (2012) and apply spherical k-means to whitened functional activations. We use the cluster centers of k-means as initial weights for our layers, so that each output unit corresponds to a centrid of k-means. k-means generally does a better job as a job capturing the modes A rather than simply referring the input to the PCs."}, {"heading": "4 EVALUATION", "text": "We implement our initialization and all the experiments in the open source deep learning framework Caffe (Jia et al., 2014). To assess how easily a network with limited data can be fine-tuned, we use the classification and detection challenges in PASCAL VOC 2007 (Everingham et al., 2014), which includes 5011 images for training and 4952 for testing. Architectures Most of our experiments are performed on the 8-layer CaffeNet architecture a small modification of AlexNet (Krizhevsky et al., 2012). We use the default architecture for all comparisons, except Doersch et al. (2015), which removes groups in the folding layers. We also show results on the much deeper GoogLeNet et al al al., 2015) and VGG (Simonyan & Zisserman, 2015), which we use different architectures.Image classification The VOC image classification task consists of being a pre-essence or pre-prediction class."}, {"heading": "4.1 SCALING AND LEARNING ALGORITHMS", "text": "We start our evaluation by measuring and comparing the relative change rate C-K, i, j of all weights in the network (see Equation (2) for different initializations. We estimate C-K, i, j with 100 images of the 2007 VOC dataset. However, we compare our models with an ImageNet upstream model by Krizhevsky et al. (2012), initialized with random Gaussian weights (with standard deviation = 0.01) for all layers, an unscaled k mean initialization, and the Gaussian initialization in Caffe (Jia et al.), for which bias and standard deviations were hand selected. Figure 1a visualizes the average change rate per layer. Our initialization, as well as the ImageNet upstream model, have similar change rates for all layers (i.e. all layers learning at the same rate)."}, {"heading": "4.2 WEIGHT INITIALIZATION", "text": "We compare our Gaussian, PCA, and k mean-based weights with the initializations of Glorot & Bengio (2010) (commonly known as \"xavier\"), He et al. (2015), and a carefully selected Gaussian initialization of Jia et al. (2014). We follow the suggestions of He et al. and use their initialization only for the revolutionary layers, while selecting a random Gaussian initialization for the fully connected layers. We compare all methods both in terms of classification and recognition performance in Table 2. First, it can be noted that both Glorot & Bengio and he et al. perform worse than a carefully selected random Gaussian initialization. One possibility of power decline results from the additional layers, such as pooling or LRN, used in the CaffeNet. Neither Glorot & Bengio nor he et al. consider these layers, but rather focus on linear layers, followed by additional layers or al."}, {"heading": "4.3 COMPARISON TO UNSUPERVISED PRE-TRAINING", "text": "We compare our attitudes with those of our fellow human beings."}, {"heading": "4.4 DIFFERENT ARCHITECTURES", "text": "Finally, we compare our initialization across different architectures, also using the classification and recognition of PASCAL 2007. Unlike previous work, we are able to train these models without intermediate losses or incremental supervised pre-training. We simply add a sigmoid cross-entropy loss to the top of both networks. Unfortunately, neither network has surpassed CaffeNet in the classification tasks. GoogLeNet scores 50.0% and 55.0% mAP for the two initializations, respectively, while 16 layers of VGG have 53.8% and 56.5%, respectively, which may have to do with the limited amount of supervised training data available to the model during the training, which was 4 and 12 times slower than CaffeNet, respectively, making them adverse for detection."}, {"heading": "4.5 IMAGENET TRAINING", "text": "This year, it is only a matter of time before that happens, until that happens, until an agreement is reached."}, {"heading": "5 DISCUSSION", "text": "Our method is a conceptually simple data-dependent initialization strategy for CNNs that forces empirically identically distributed activations locally (within a level) and a roughly uniform global scaling of weight gradients across all levels of arbitrarily deep networks. Our experiments (Section 4) show that this recalculation of weights results in vastly improved CNN representations for tasks with limited marked data (as in the PASCAL VOC classification and recognition training sets), improves representations learned through existing self-monitored and unattended methods, and significantly accelerates the early stages of CNN training on large datasets (e.g. ImageNet). We hope that our initializations will enable further progress in unattended and self-monitored learning as well as a more efficient exploration of deeper and larger CNN architectures."}, {"heading": "ACKNOWLEDGEMENTS", "text": "Thanks to Alyoscha Efros for his input and encouragement, without his \"gelato bet\" most of this work would not have been explored. We thank NVIDIA for their generous GPU donations."}], "references": [{"title": "Learning to see by moving", "author": ["Agrawal", "Pulkit", "Carreira", "Joao", "Malik", "Jitendra"], "venue": null, "citeRegEx": "Agrawal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2015}, {"title": "Learning in modular systems", "author": ["Bradley", "David M"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Bradley and M.,? \\Q2010\\E", "shortCiteRegEx": "Bradley and M.", "year": 2010}, {"title": "Learning feature representations with k-means", "author": ["Coates", "Adam", "Ng", "Andrew Y"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "Coates et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2012}, {"title": "Unsupervised visual representation learning by context prediction", "author": ["Doersch", "Carl", "Gupta", "Abhinav", "Efros", "Alexei A"], "venue": null, "citeRegEx": "Doersch et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Doersch et al\\.", "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In AISTATS, pp", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In ICCV,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "In ICML,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross B", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": "In ACM Multimedia,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Efficient backprop", "author": ["Y. LeCun", "L. Bottou", "G. Orr", "K. Muller"], "venue": "In Neural Networks: Tricks of the trade. Springer,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "ImageNet large scale visual recognition challenge", "author": ["Russakovsky", "Olga", "Deng", "Jia", "Su", "Hao", "Krause", "Jonathan", "Satheesh", "Sanjeev", "Ma", "Sean", "Huang", "Zhiheng", "Karpathy", "Andrej", "Khosla", "Aditya", "Bernstein", "Michael", "Berg", "Alexander C", "Fei-Fei", "Li"], "venue": null, "citeRegEx": "Russakovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Saxe", "Andrew M", "McClelland", "James L", "Ganguli", "Surya"], "venue": "arXiv preprint arXiv:1312.6120,", "citeRegEx": "Saxe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "ICLR,", "citeRegEx": "Simonyan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2015}, {"title": "Random walk initialization for training very deep feedforward networks", "author": ["Sussillo", "David", "Abbot", "Larry"], "venue": "ICLR,", "citeRegEx": "Sussillo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sussillo et al\\.", "year": 2015}, {"title": "Unsupervised learning of visual representations using videos", "author": ["Wang", "Xiaolong", "Gupta", "Abhinav"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "How transferable are features in deep neural networks", "author": ["Yosinski", "Jason", "Clune", "Jeff", "Bengio", "Yoshua", "Lipson", "Hod"], "venue": "In NIPS,", "citeRegEx": "Yosinski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 11, "context": "In fact, good performance seems to require large datasets: the best-performing methods usually begin by \u201cpre-training\u201d CNNs to solve the million-image ImageNet classification challenge (Russakovsky et al., 2015).", "startOffset": 185, "endOffset": 211}, {"referenceID": 16, "context": "Empirical evaluations have found that when transferring deep features across tasks, freezing weights of some layers during fine-tuning generally harms performance (Yosinski et al., 2014).", "startOffset": 163, "endOffset": 186}, {"referenceID": 9, "context": "However, none of the above papers consider more general network including pooling, dropout, LRN layers (Krizhevsky et al., 2012), or DAG-structured networks (Szegedy et al.", "startOffset": 103, "endOffset": 128}, {"referenceID": 10, "context": "Early approaches to data-driven initializations showed that whitening the activations at all layers can mitigate the vanishing gradient problem (LeCun et al., 1998), but it does not ensure all layers train at an equal rate.", "startOffset": 144, "endOffset": 164}, {"referenceID": 8, "context": "Krizhevsky et al. (2012) carefully designed their architecture to ensure gradients neither vanish nor explode.", "startOffset": 0, "endOffset": 25}, {"referenceID": 8, "context": "Krizhevsky et al. (2012) carefully designed their architecture to ensure gradients neither vanish nor explode. However, this is no longer possible for deeper architectures such as VGG (Simonyan & Zisserman, 2015) or GoogLeNet (Szegedy et al., 2015). Glorot & Bengio (2010); Saxe et al.", "startOffset": 0, "endOffset": 273}, {"referenceID": 8, "context": "Krizhevsky et al. (2012) carefully designed their architecture to ensure gradients neither vanish nor explode. However, this is no longer possible for deeper architectures such as VGG (Simonyan & Zisserman, 2015) or GoogLeNet (Szegedy et al., 2015). Glorot & Bengio (2010); Saxe et al. (2013); Sussillo & Abbot (2015); He et al.", "startOffset": 0, "endOffset": 293}, {"referenceID": 8, "context": "Krizhevsky et al. (2012) carefully designed their architecture to ensure gradients neither vanish nor explode. However, this is no longer possible for deeper architectures such as VGG (Simonyan & Zisserman, 2015) or GoogLeNet (Szegedy et al., 2015). Glorot & Bengio (2010); Saxe et al. (2013); Sussillo & Abbot (2015); He et al.", "startOffset": 0, "endOffset": 318}, {"referenceID": 5, "context": "(2013); Sussillo & Abbot (2015); He et al. (2015); Bradley (2010) show that properly scaled random initialization can deal with the vanishing gradient problem, if the architectures are limited to linear transformations, followed by a very specific non-linearities.", "startOffset": 33, "endOffset": 50}, {"referenceID": 5, "context": "(2013); Sussillo & Abbot (2015); He et al. (2015); Bradley (2010) show that properly scaled random initialization can deal with the vanishing gradient problem, if the architectures are limited to linear transformations, followed by a very specific non-linearities.", "startOffset": 33, "endOffset": 66}, {"referenceID": 5, "context": "(2013); Sussillo & Abbot (2015); He et al. (2015); Bradley (2010) show that properly scaled random initialization can deal with the vanishing gradient problem, if the architectures are limited to linear transformations, followed by a very specific non-linearities. Saxe et al. (2013) focus on linear networks, Glorot & Bengio (2010) derive an initialization for networks with tanh non-linearities, while He et al.", "startOffset": 33, "endOffset": 284}, {"referenceID": 5, "context": "(2013); Sussillo & Abbot (2015); He et al. (2015); Bradley (2010) show that properly scaled random initialization can deal with the vanishing gradient problem, if the architectures are limited to linear transformations, followed by a very specific non-linearities. Saxe et al. (2013) focus on linear networks, Glorot & Bengio (2010) derive an initialization for networks with tanh non-linearities, while He et al.", "startOffset": 33, "endOffset": 333}, {"referenceID": 5, "context": "(2013); Sussillo & Abbot (2015); He et al. (2015); Bradley (2010) show that properly scaled random initialization can deal with the vanishing gradient problem, if the architectures are limited to linear transformations, followed by a very specific non-linearities. Saxe et al. (2013) focus on linear networks, Glorot & Bengio (2010) derive an initialization for networks with tanh non-linearities, while He et al. (2015) focus on the more commonly used ReLUs.", "startOffset": 33, "endOffset": 421}, {"referenceID": 9, "context": "Other common non-linearities include local response normalization or pooling (Krizhevsky et al., 2012; Szegedy et al., 2015; Simonyan & Zisserman, 2015).", "startOffset": 77, "endOffset": 152}, {"referenceID": 10, "context": "Despite the non-convexity of this learning problem, backpropagation and Stochastic Gradient Descent often finds good local minima if initialized properly (LeCun et al., 1998).", "startOffset": 154, "endOffset": 174}, {"referenceID": 7, "context": "We implement our initialization and all experiments in the open-source deep learning framework Caffe (Jia et al., 2014).", "startOffset": 101, "endOffset": 119}, {"referenceID": 9, "context": "Architectures Most of our experiments are performed on the 8 layer CaffeNet architecture a small modification of AlexNet (Krizhevsky et al., 2012).", "startOffset": 121, "endOffset": 146}, {"referenceID": 3, "context": "We use the default architecture for all comparisons, except for Doersch et al. (2015) which removed groups in the convolutional layers.", "startOffset": 64, "endOffset": 86}, {"referenceID": 7, "context": "01) for all layers, an unscaled k-means initialization, as well as the Gaussian initialization in Caffe (Jia et al., 2014), for which biases and standard deviations were picked per layer by hand.", "startOffset": 104, "endOffset": 122}, {"referenceID": 8, "context": "We compare our models to an ImageNet pretrained model of Krizhevsky et al. (2012), initialized with random Gaussian weights (with standard deviation \u03c3 = 0.", "startOffset": 57, "endOffset": 82}, {"referenceID": 5, "context": "4% He et al. (2015) 43.", "startOffset": 3, "endOffset": 20}, {"referenceID": 5, "context": "We compare our Gaussian, PCA and k-means based weights, with initializations proposed by Glorot & Bengio (2010) (commonly known as \u201cxavier\u201d), He et al. (2015), and a carefully chosen Gaussian initialization of Jia et al.", "startOffset": 142, "endOffset": 159}, {"referenceID": 5, "context": "We compare our Gaussian, PCA and k-means based weights, with initializations proposed by Glorot & Bengio (2010) (commonly known as \u201cxavier\u201d), He et al. (2015), and a carefully chosen Gaussian initialization of Jia et al. (2014). We followed the suggestions of He et al.", "startOffset": 142, "endOffset": 228}, {"referenceID": 0, "context": "Agrawal et al. (2015) uses egomotion, as measured by a moving car in a city to pre-train a model.", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "Agrawal et al. (2015) uses egomotion, as measured by a moving car in a city to pre-train a model. While this information is not always readily available, it can be read from sensors and is thus \u201cfree.\u201d We believe egomotion information does not often correlate with the kind of semantic information that is required for classification or detection, and hence the egomotion pretrained model performs worse than our random baseline. Wang & Gupta (2015) supervise their pre-training using relative motion of objects in pre-selected youtube videos, as obtained by a tracker.", "startOffset": 0, "endOffset": 450}, {"referenceID": 0, "context": "Agrawal et al. (2015) uses egomotion, as measured by a moving car in a city to pre-train a model. While this information is not always readily available, it can be read from sensors and is thus \u201cfree.\u201d We believe egomotion information does not often correlate with the kind of semantic information that is required for classification or detection, and hence the egomotion pretrained model performs worse than our random baseline. Wang & Gupta (2015) supervise their pre-training using relative motion of objects in pre-selected youtube videos, as obtained by a tracker. Their model is generally quite well scaled and trains well for both classification and detection. Doersch et al. (2015) predict the relative arrangement of image patches to pre-train a model.", "startOffset": 0, "endOffset": 690}, {"referenceID": 3, "context": "For example, ours appears to have some texture and material information, whereas the method of Doersch et al. (2015) seems to preserve more specific shape information.", "startOffset": 95, "endOffset": 117}, {"referenceID": 0, "context": "We find that for two of the three models (Agrawal et al., 2015; Doersch et al., 2015) this rescaling improves results significantly; our rescaling of Wang & Gupta (2015) on the other hand does not improve its performance, indicating it was likely relatively well-scaled globally to begin with.", "startOffset": 41, "endOffset": 85}, {"referenceID": 3, "context": "We find that for two of the three models (Agrawal et al., 2015; Doersch et al., 2015) this rescaling improves results significantly; our rescaling of Wang & Gupta (2015) on the other hand does not improve its performance, indicating it was likely relatively well-scaled globally to begin with.", "startOffset": 41, "endOffset": 85}, {"referenceID": 0, "context": "We find that for two of the three models (Agrawal et al., 2015; Doersch et al., 2015) this rescaling improves results significantly; our rescaling of Wang & Gupta (2015) on the other hand does not improve its performance, indicating it was likely relatively well-scaled globally to begin with.", "startOffset": 42, "endOffset": 170}, {"referenceID": 0, "context": "We find that for two of the three models (Agrawal et al., 2015; Doersch et al., 2015) this rescaling improves results significantly; our rescaling of Wang & Gupta (2015) on the other hand does not improve its performance, indicating it was likely relatively well-scaled globally to begin with. The best-performing method with auxiliary self-supervision using our rescaled features is that of Doersch et al. (2015) \u2013 in this case our rescaling improves its results on the classification task by a relative margin of 18%.", "startOffset": 42, "endOffset": 414}, {"referenceID": 7, "context": "We also test our data-dependent initializations on two well-known CNN architectures which have been successfully applied to the ImageNet LSVRC 1000-way classification task: CaffeNet (Jia et al., 2014) and GoogLeNet (Szegedy et al.", "startOffset": 182, "endOffset": 200}, {"referenceID": 0, "context": "Method Supervision Pretraining time Classification Detection Agrawal et al. (2015) egomotion 10 hours 52.", "startOffset": 61, "endOffset": 83}, {"referenceID": 0, "context": "Method Supervision Pretraining time Classification Detection Agrawal et al. (2015) egomotion 10 hours 52.9% 41.8% Wang & Gupta (2015) motion 1 week 58.", "startOffset": 61, "endOffset": 134}, {"referenceID": 0, "context": "Method Supervision Pretraining time Classification Detection Agrawal et al. (2015) egomotion 10 hours 52.9% 41.8% Wang & Gupta (2015) motion 1 week 58.4% 44.0% Doersch et al. (2015) unsupervised 4 weeks 55.", "startOffset": 61, "endOffset": 182}, {"referenceID": 0, "context": "Method Supervision Pretraining time Classification Detection Agrawal et al. (2015) egomotion 10 hours 52.9% 41.8% Wang & Gupta (2015) motion 1 week 58.4% 44.0% Doersch et al. (2015) unsupervised 4 weeks 55.3% 46.6% Krizhevsky et al. (2012) 1000 class labels 3 days 78.", "startOffset": 61, "endOffset": 240}, {"referenceID": 0, "context": "Method Supervision Pretraining time Classification Detection Agrawal et al. (2015) egomotion 10 hours 52.9% 41.8% Wang & Gupta (2015) motion 1 week 58.4% 44.0% Doersch et al. (2015) unsupervised 4 weeks 55.3% 46.6% Krizhevsky et al. (2012) 1000 class labels 3 days 78.2% 56.8% Ours (k-means) initialization 54 seconds 56.6% 45.6% Ours + Agrawal et al. (2015) egomotion 10 hours 54.", "startOffset": 61, "endOffset": 359}, {"referenceID": 0, "context": "Method Supervision Pretraining time Classification Detection Agrawal et al. (2015) egomotion 10 hours 52.9% 41.8% Wang & Gupta (2015) motion 1 week 58.4% 44.0% Doersch et al. (2015) unsupervised 4 weeks 55.3% 46.6% Krizhevsky et al. (2012) 1000 class labels 3 days 78.2% 56.8% Ours (k-means) initialization 54 seconds 56.6% 45.6% Ours + Agrawal et al. (2015) egomotion 10 hours 54.2% 43.9% Ours + Wang & Gupta (2015) motion 1 week 58.", "startOffset": 61, "endOffset": 417}, {"referenceID": 0, "context": "Method Supervision Pretraining time Classification Detection Agrawal et al. (2015) egomotion 10 hours 52.9% 41.8% Wang & Gupta (2015) motion 1 week 58.4% 44.0% Doersch et al. (2015) unsupervised 4 weeks 55.3% 46.6% Krizhevsky et al. (2012) 1000 class labels 3 days 78.2% 56.8% Ours (k-means) initialization 54 seconds 56.6% 45.6% Ours + Agrawal et al. (2015) egomotion 10 hours 54.2% 43.9% Ours + Wang & Gupta (2015) motion 1 week 58.1% 44.0% Ours + Doersch et al. (2015) unsupervised 4 weeks 65.", "startOffset": 61, "endOffset": 472}, {"referenceID": 7, "context": "We use the SGD hyperparameters from the Caffe (Jia et al., 2014) GoogleNet implementation (specifically, the \u201cquick\u201d version which is trained for 2.", "startOffset": 46, "endOffset": 64}, {"referenceID": 7, "context": "We use the SGD hyperparameters from the Caffe (Jia et al., 2014) GoogleNet implementation (specifically, the \u201cquick\u201d version which is trained for 2.4 million iterations), and also retrain our own instance of the model with the initialization used in the reference model (based on Glorot & Bengio (2010)).", "startOffset": 47, "endOffset": 303}, {"referenceID": 3, "context": "(For Doersch et al. (2015) we display neighbors in fc6 feature space; the rest use the fc7 features.", "startOffset": 5, "endOffset": 27}], "year": 2015, "abstractText": "Convolutional Neural Networks spread through computer vision like a wildfire, impacting almost all visual tasks imaginable. Despite this, few researchers dare to train their models from scratch. Most work builds on one of a handful of ImageNet pre-trained models, and fine-tunes or adapts these for specific tasks. This is in large part due to the difficulty of properly initializing these networks from scratch. A small miscalibration of the initial weights leads to vanishing or exploding gradients, as well as poor convergence properties. In this work we present a fast and simple data-dependent initialization procedure, that sets the weights of a network such that all units in the network train at roughly the same rate, avoiding vanishing or exploding gradients. Our initialization matches the current state-of-the-art unsupervised or self-supervised pre-training methods on standard computer vision tasks, such as image classification and object detection, while being roughly three orders of magnitude faster. When combined with pre-training methods, our initialization significantly outperforms prior work, narrowing the gap between supervised and unsupervised pre-training.", "creator": "LaTeX with hyperref package"}}}