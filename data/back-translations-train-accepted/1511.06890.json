{"id": "1511.06890", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2015", "title": "Gaussian Process Planning with Lipschitz Continuous Reward Functions: Towards Unifying Bayesian Optimization, Active Learning, and Beyond", "abstract": "This paper presents a novel nonmyopic adaptive Gaussian process planning (GPP) framework endowed with a general class of Lipschitz continuous reward functions that can unify some active learning/sensing and Bayesian optimization criteria and offer practitioners some flexibility to specify their desired choices for defining new tasks/problems. In particular, it utilizes a principled Bayesian sequential decision problem framework for jointly and naturally optimizing the exploration-exploitation trade-off. In general, the resulting induced GPP policy cannot be derived exactly due to an uncountable set of candidate observations. A key contribution of our work here thus lies in exploiting the Lipschitz continuity of the reward functions to solve for a nonmyopic adaptive epsilon-optimal GPP (epsilon-GPP) policy. To plan in real time, we further propose an asymptotically optimal, branch-and-bound anytime variant of epsilon-GPP with performance guarantee. We empirically demonstrate the effectiveness of our epsilon-GPP policy and its anytime variant in Bayesian optimization and an energy harvesting task.", "histories": [["v1", "Sat, 21 Nov 2015 14:57:48 GMT  (574kb,D)", "http://arxiv.org/abs/1511.06890v1", "30th AAAI Conference on Artificial Intelligence (AAAI 2016), Extended version with proofs, 17 pages"]], "COMMENTS": "30th AAAI Conference on Artificial Intelligence (AAAI 2016), Extended version with proofs, 17 pages", "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG cs.RO", "authors": ["chun kai ling", "kian hsiang low", "patrick jaillet"], "accepted": true, "id": "1511.06890"}, "pdf": {"name": "1511.06890.pdf", "metadata": {"source": "META", "title": "Gaussian Process Planning with Lipschitz Continuous Reward Functions: Towards Unifying Bayesian Optimization, Active Learning, and Beyond", "authors": ["Chun Kai Ling", "Kian Hsiang Low", "Patrick Jaillet"], "emails": ["lowkh}@comp.nus.edu.sg,", "jaillet@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "The fundamental challenge of integrated planning and learning is to design an autonomous agent that can plan its actions to maximize its expected total consumption (BO) while interacting with an unknown task environment. Recent research efforts addressing this challenge have progressed from the use of simple Markov models that characterize discretely evaluated, independent observations (e.g., in Bayesian reinforcement learning (BRL) (Poupart et al. 2006) to that of a rich class of Bayesian non-parametric Gaussian processes (GP) that characterize continuously evaluated, correlated observations to represent the latent structure of more complex, possibly noisy task environments with higher fidelity. Such a challenge is posed by the following important problems in machine learning, among others: Active Learning / Sensing (AL). In the context of environmental sensory (e.g., adaptive sampling in low chanography, traffic sing in 2007, traffic sensing in 2007, traffic sensing in 2013)."}, {"heading": "2 Gaussian Process Planning (GPP)", "text": "In the time step t > 0, a robot from its previous location can accordingly from its previous location include a finite set of sampling locations that are reachable from its previous location \u2212 1 in a single time step. The state of the robot at its initial starting point s0 is determined by previous observations / data d0, < s0, s0 > available prior to scheduling where s0 and z0 denote, or vectors include locations specified by the robot prior to scheduling and s0 is the last component of s0."}, {"heading": "3 -Optimal GPP ( -GPP)", "text": "The key idea of the construction of our proposed non-myopic fit GPP policy is to approximate the expectation conditions in (1) each stage using a form of deterministic sampling as shown in the figure below. \u2212 \u2212 n The measurement ranges of p (zt + 1 | dt, st + 1) are initially divided into n 2 intervals. \u2212 n The measurements of p (dt, st + 1) are evenly distributed within the limited grey area. \u2212 dt, + 1 | 1 | st, \u00b5st + 1 | st, p + 1 | st, like the measurements of 0,. \u2212 st, like the measurements of 0 and the measurements of 0. \u2212 n The two infinitely long red tails. Note that the measurements of n > 2 must be valid for the partition. The n sample measurements of z0. \u2212 1 are then selected by setting the upper limit of 0."}, {"heading": "4 Experiments and Discussion", "text": "This section empirically evaluates the online planning performance and time efficiency of our -GPP policy \u03c0 = 103,1 = 103,1 = 103,0 = 103,0 = 102,0 = 102,0 = 102,0 = 102,0 = 102,0 = 102,0 = 102,0 = 102,0 = 102,0 = 102,0 = 102,0 = 102,0 = 102,0 = 102,0 = 102,0 = 102,0 = 102,0 = 102,0 = 102,0 = 102,0 = 102,0 = 102,0 = 102,0 = 102,0 = 102,0 = 102,0 = 102,0 = 102,0 = 102,0 = 102,0 = 100,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,102,002,002,0 = 102,002,002,002,0 = 102,002,0 = 102,0 = 102,002,0 = 102,0 = 102,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,0 = 100,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,00"}, {"heading": "BO on Real-World Log-Potassium Concentration Field.", "text": "An agricultural robot has the task of finding the maximum lg-K measurement (i.e., possibly in an overfertilized area) while examining the Broom barn (Webster and Oliver 2007). It is driven by the UCB-based reward function, shown under \"BO\" in Section 2. Figure 2 shows the results of the performance of our -GPP policy and its always available variant, non-myopic UCB (i.e. = 25) and greedy PI, EI, UCB (i.e. H \u2032 = 1), which are above 25 randomly selected robot exit sortable starting locations. Figure 2 shows that the gradients of the achieved total normalization 7 generally increase over time, especially of Fig. 2a, non-myopic UCB, which assume the maximum probability of observations during planning, are much lower than the others - GPP strategies and which always vary after 20 time steps and find a maximum measurement after 3.0."}, {"heading": "5 Conclusion", "text": "This paper describes a novel, non-myopic adaptive GPP framework endowed with a general class of continuous reward functions from Lipschitz that standardize some AL and BO criteria and can be used to define new tasks / problems. In particular, it can jointly and naturally optimize the trade-off in exploration exploitation. Theoretically, we guarantee the performance of our -GPP policy and its always available variant and em-7To simplify the interpretation of results, any reward is normalized by subtracting the previous mean from it. Piritically, we demonstrate its effectiveness in BO and an energy harvesting task. For our future work, we plan to extend -GPP and its always available variant for big data by parallelization (Chen et al. 2013; Low et al. 2015), online learning (Xu et al. 2014) and stochastic variation conclusions (Hoang, Hoang and Low 2015) and extend it to unknown parameters (Hoet al)."}, {"heading": "27: \u3008V \u2217t+1(\u3008st+1, zt \u2295 zi", "text": "* b * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *"}], "references": [{"title": "A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning", "author": ["E. Brochu", "V.M. Cora", "N. de Freitas"], "venue": null, "citeRegEx": "Brochu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Brochu et al\\.", "year": 2010}, {"title": "Multi-robot informative path planning for active sensing of environmental phenomena: A tale of two algorithms", "author": ["N. Cao", "K.H. Low", "J.M. Dolan"], "venue": "Proc. AAMAS.", "citeRegEx": "Cao et al\\.,? 2013", "shortCiteRegEx": "Cao et al\\.", "year": 2013}, {"title": "Decentralized data fusion and active sensing with mobile sensors for modeling and predicting spatiotemporal traffic phenomena", "author": ["J. Chen", "K.H. Low", "C.K.-Y. Tan", "A. Oran", "P. Jaillet", "J.M. Dolan", "G.S. Sukhatme"], "venue": "Proc. UAI, 163\u2013173.", "citeRegEx": "Chen et al\\.,? 2012", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Parallel Gaussian process regression with low-rank covariance matrix approximations", "author": ["J. Chen", "N. Cao", "K.H. Low", "R. Ouyang", "C.K.-Y. Tan", "P. Jaillet"], "venue": "Proc. UAI, 152\u2013161.", "citeRegEx": "Chen et al\\.,? 2013", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Design and power management of a wind-solar-powered polar rover", "author": ["J. Chen", "J. Liang", "T. Wang", "T. Zhang", "Y. Wu"], "venue": "Journal of Ocean and Wind Energy 1(2):65\u201373.", "citeRegEx": "Chen et al\\.,? 2014", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Gaussian process decentralized data fusion and active sensing for spatiotemporal traffic modeling and prediction in mobility-ondemand systems", "author": ["J. Chen", "K.H. Low", "P. Jaillet", "Y. Yao"], "venue": "IEEE Trans. Autom. Sci. Eng. 12:901\u2013921.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Gaussian process-based decentralized data fusion and active sensing for mobility-on-demand system", "author": ["J. Chen", "K.H. Low", "C.K.-Y. Tan"], "venue": "Proc. RSS.", "citeRegEx": "Chen et al\\.,? 2013", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Bayesian reinforcement learning in continuous POMDPs with Gaussian processes", "author": ["P. Dallaire", "C. Besse", "S. Ross", "B. Chaib-draa"], "venue": "Proc. IEEE/RSJ IROS, 2604\u2013 2609.", "citeRegEx": "Dallaire et al\\.,? 2009", "shortCiteRegEx": "Dallaire et al\\.", "year": 2009}, {"title": "Gaussian processes for data-efficient learning in robotics and control", "author": ["M.P. Deisenroth", "D. Fox", "C.E. Rasmussen"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 37(2):408\u2013423.", "citeRegEx": "Deisenroth et al\\.,? 2015", "shortCiteRegEx": "Deisenroth et al\\.", "year": 2015}, {"title": "Cooperative aquatic sensing using the telesupervised adaptive ocean sensor fleet", "author": ["J.M. Dolan", "G. Podnar", "S. Stancliff", "K.H. Low", "A. Elfes", "J. Higinbotham", "J.C. Hosler", "T.A. Moisan", "J. Moisan"], "venue": "Proc. SPIE Conference on Remote Sensing of the Ocean, Sea Ice, and Large Water", "citeRegEx": "Dolan et al\\.,? 2009", "shortCiteRegEx": "Dolan et al\\.", "year": 2009}, {"title": "Entropy search for information-efficient global optimization", "author": ["P. Hennig", "C.J. Schuler"], "venue": "JMLR 13:1809\u2013 1837.", "citeRegEx": "Hennig and Schuler,? 2012", "shortCiteRegEx": "Hennig and Schuler", "year": 2012}, {"title": "Predictive entropy search for efficient global optimization of black-box functions", "author": ["J.M. Hern\u00e1ndez-Lobato", "M.W. Hoffman", "Z. Ghahramani"], "venue": "Proc. NIPS.", "citeRegEx": "Hern\u00e1ndez.Lobato et al\\.,? 2014", "shortCiteRegEx": "Hern\u00e1ndez.Lobato et al\\.", "year": 2014}, {"title": "Nonmyopic -Bayes-optimal active learning of Gaussian processes", "author": ["T.N. Hoang", "K.H. Low", "P. Jaillet", "M. Kankanhalli"], "venue": "Proc. ICML, 739\u2013747.", "citeRegEx": "Hoang et al\\.,? 2014", "shortCiteRegEx": "Hoang et al\\.", "year": 2014}, {"title": "A unifying framework of anytime sparse Gaussian process regression models with stochastic variational inference for big data", "author": ["T.N. Hoang", "Q.M. Hoang", "K.H. Low"], "venue": "Proc. ICML, 569\u2013578.", "citeRegEx": "Hoang et al\\.,? 2015", "shortCiteRegEx": "Hoang et al\\.", "year": 2015}, {"title": "Near-optimal sensor placements in Gaussian processes: Theory, efficient algorithms and empirical studies", "author": ["A. Krause", "A. Singh", "C. Guestrin"], "venue": "JMLR 9:235\u2013284.", "citeRegEx": "Krause et al\\.,? 2008", "shortCiteRegEx": "Krause et al\\.", "year": 2008}, {"title": "Collective motion, sensor networks, and ocean sampling", "author": ["N.E. Leonard", "D.A. Palley", "F. Lekien", "R. Sepulchre", "D.M. Fratantoni", "R.E. Davis"], "venue": "Proc. IEEE 95:48\u201374.", "citeRegEx": "Leonard et al\\.,? 2007", "shortCiteRegEx": "Leonard et al\\.", "year": 2007}, {"title": "Decentralized active robotic exploration and mapping for probabilistic field classification in environmental sensing", "author": ["K.H. Low", "J. Chen", "J.M. Dolan", "S. Chien", "D.R. Thompson"], "venue": "Proc. AAMAS, 105\u2013112.", "citeRegEx": "Low et al\\.,? 2012", "shortCiteRegEx": "Low et al\\.", "year": 2012}, {"title": "Parallel Gaussian process regression for big data: Low-rank representation meets Markov approximation", "author": ["K.H. Low", "J. Yu", "J. Chen", "P. Jaillet"], "venue": "Proc. AAAI.", "citeRegEx": "Low et al\\.,? 2015", "shortCiteRegEx": "Low et al\\.", "year": 2015}, {"title": "Adaptive multi-robot wide-area exploration and mapping", "author": ["K.H. Low", "J.M. Dolan", "P. Khosla"], "venue": "Proc. AAMAS, 23\u201330.", "citeRegEx": "Low et al\\.,? 2008", "shortCiteRegEx": "Low et al\\.", "year": 2008}, {"title": "Informationtheoretic approach to efficient adaptive path planning for mobile robotic environmental sensing", "author": ["K.H. Low", "J.M. Dolan", "P. Khosla"], "venue": "Proc. ICAPS.", "citeRegEx": "Low et al\\.,? 2009", "shortCiteRegEx": "Low et al\\.", "year": 2009}, {"title": "Active Markov information-theoretic path planning for robotic environmental sensing", "author": ["K.H. Low", "J.M. Dolan", "P. Khosla"], "venue": "Proc. AAMAS, 753\u2013760.", "citeRegEx": "Low et al\\.,? 2011", "shortCiteRegEx": "Low et al\\.", "year": 2011}, {"title": "Sequential Bayesian optimisation for spatial-temporal monitoring", "author": ["R. Marchant", "F. Ramos", "S. Sanner"], "venue": "Proc. UAI.", "citeRegEx": "Marchant et al\\.,? 2014", "shortCiteRegEx": "Marchant et al\\.", "year": 2014}, {"title": "Gaussian processes for global optimization", "author": ["M.A. Osborne", "R. Garnett", "S.J. Roberts"], "venue": "Proc. 3rd International Conference on Learning and Intelligent Optimization.", "citeRegEx": "Osborne et al\\.,? 2009", "shortCiteRegEx": "Osborne et al\\.", "year": 2009}, {"title": "Multirobot active sensing of non-stationary Gaussian processbased environmental phenomena", "author": ["R. Ouyang", "K.H. Low", "J. Chen", "P. Jaillet"], "venue": "Proc. AAMAS.", "citeRegEx": "Ouyang et al\\.,? 2014", "shortCiteRegEx": "Ouyang et al\\.", "year": 2014}, {"title": "An analytic solution to discrete Bayesian reinforcement learning", "author": ["P. Poupart", "N. Vlassis", "J. Hoey", "K. Regan"], "venue": "Proc. ICML, 697\u2013704.", "citeRegEx": "Poupart et al\\.,? 2006", "shortCiteRegEx": "Poupart et al\\.", "year": 2006}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": "MIT Press.", "citeRegEx": "Rasmussen and Williams,? 2006", "shortCiteRegEx": "Rasmussen and Williams", "year": 2006}, {"title": "Bayesian reinforcement learning in continuous POMDPs with application to robot navigation", "author": ["S. Ross", "B. Chaib-draa", "J. Pineau"], "venue": "Proc. IEEE ICRA, 2845\u20132851.", "citeRegEx": "Ross et al\\.,? 2008", "shortCiteRegEx": "Ross et al\\.", "year": 2008}, {"title": "Maximum entropy sampling", "author": ["M.C. Shewry", "H.P. Wynn"], "venue": "J. Applied Statistics 14(2):165\u2013170.", "citeRegEx": "Shewry and Wynn,? 1987", "shortCiteRegEx": "Shewry and Wynn", "year": 1987}, {"title": "Focused real-time dynamic programming for MDPs: Squeezing more out of a heuristic", "author": ["T. Smith", "R. Simmons"], "venue": "Proc. AAAI, 1227\u20131232.", "citeRegEx": "Smith and Simmons,? 2006", "shortCiteRegEx": "Smith and Simmons", "year": 2006}, {"title": "Gaussian process optimization in the bandit setting: No regret and experimental design", "author": ["N. Srinivas", "A. Krause", "S. Kakade", "M. Seeger"], "venue": "Proc. ICML, 1015\u20131022.", "citeRegEx": "Srinivas et al\\.,? 2010", "shortCiteRegEx": "Srinivas et al\\.", "year": 2010}, {"title": "Geostatistics for Environmental Scientists", "author": ["R. Webster", "M. Oliver"], "venue": "NY: John Wiley & Sons, Inc., 2nd edition.", "citeRegEx": "Webster and Oliver,? 2007", "shortCiteRegEx": "Webster and Oliver", "year": 2007}, {"title": "GP-Localize: Persistent mobile robot localization using online sparse Gaussian process observation model", "author": ["N. Xu", "K.H. Low", "J. Chen", "K.K. Lim", "E.B. Ozgul"], "venue": "Proc. AAAI, 2585\u20132592.", "citeRegEx": "Xu et al\\.,? 2014", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Near-optimal active learning of multi-output Gaussian processes", "author": ["Y. Zhang", "T.N. Hoang", "K.H. Low", "M. Kankanhalli"], "venue": "Proc. AAAI.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 24, "context": ", in Bayesian reinforcement learning (BRL) (Poupart et al. 2006)) to that of a rich class of Bayesian nonparametric Gaussian process (GP) models characterizing continuous-valued, correlated observations in order to represent the latent structure of more complex, possibly noisy task environments with higher fidelity.", "startOffset": 43, "endOffset": 64}, {"referenceID": 15, "context": ", adaptive sampling in oceanography (Leonard et al. 2007), traffic sensing (Chen et al.", "startOffset": 36, "endOffset": 57}, {"referenceID": 2, "context": "2007), traffic sensing (Chen et al. 2012; Chen, Low, and Tan 2013; Chen et al. 2015)), its objective is to select the most informative (possibly noisy) observations for predicting a spatially varying environmental field (i.", "startOffset": 23, "endOffset": 84}, {"referenceID": 5, "context": "2007), traffic sensing (Chen et al. 2012; Chen, Low, and Tan 2013; Chen et al. 2015)), its objective is to select the most informative (possibly noisy) observations for predicting a spatially varying environmental field (i.", "startOffset": 23, "endOffset": 84}, {"referenceID": 16, "context": ", local maxima) faced by greedy algorithms (Krause, Singh, and Guestrin 2008; Low et al. 2012; Ouyang et al. 2014; Zhang et al. 2016), recent developments have made nonmyopic AL computationally tractable with provable performance guarantees (Cao, Low, and Dolan 2013; Hoang et al.", "startOffset": 43, "endOffset": 133}, {"referenceID": 23, "context": ", local maxima) faced by greedy algorithms (Krause, Singh, and Guestrin 2008; Low et al. 2012; Ouyang et al. 2014; Zhang et al. 2016), recent developments have made nonmyopic AL computationally tractable with provable performance guarantees (Cao, Low, and Dolan 2013; Hoang et al.", "startOffset": 43, "endOffset": 133}, {"referenceID": 32, "context": ", local maxima) faced by greedy algorithms (Krause, Singh, and Guestrin 2008; Low et al. 2012; Ouyang et al. 2014; Zhang et al. 2016), recent developments have made nonmyopic AL computationally tractable with provable performance guarantees (Cao, Low, and Dolan 2013; Hoang et al.", "startOffset": 43, "endOffset": 133}, {"referenceID": 12, "context": "2016), recent developments have made nonmyopic AL computationally tractable with provable performance guarantees (Cao, Low, and Dolan 2013; Hoang et al. 2014; Low, Dolan, and Khosla 2009; 2008; 2011), some of which have further investigated the performance advantage of adaptivity by proposing nonmyopic adaptive observation selection policies that depend on past observations.", "startOffset": 113, "endOffset": 199}, {"referenceID": 10, "context": ", probability of improvement (PI) or expected improvement (EI) over currently found maximum), entropybased (Hennig and Schuler 2012; Hern\u00e1ndez-Lobato, Hoffman, and Ghahramani 2014), or upper confidence bound (UCB) acquisition function (Srinivas et al.", "startOffset": 107, "endOffset": 180}, {"referenceID": 29, "context": ", probability of improvement (PI) or expected improvement (EI) over currently found maximum), entropybased (Hennig and Schuler 2012; Hern\u00e1ndez-Lobato, Hoffman, and Ghahramani 2014), or upper confidence bound (UCB) acquisition function (Srinivas et al. 2010).", "startOffset": 235, "endOffset": 257}, {"referenceID": 25, "context": "Let YS , {Ys}s\u2208S denote a GP, that is, every finite subset of YS has a multivariate Gaussian distribution (Rasmussen and Williams 2006).", "startOffset": 106, "endOffset": 135}, {"referenceID": 24, "context": "Bayes-optimality has been studied in discrete BRL (Poupart et al. 2006) whose assumptions (Section 1) do not hold in GPP.", "startOffset": 50, "endOffset": 71}, {"referenceID": 7, "context": "Continuous BRLs (Dallaire et al. 2009; Ross, Chaib-draa, and Pineau 2008) assume a known parametric form of observation function,", "startOffset": 16, "endOffset": 73}, {"referenceID": 27, "context": "5 log(2\u03c0e\u03c3 2 st+1|st) yields the well-known nonmyopic AL algorithm called maximum entropy sampling (MES) (Shewry and Wynn 1987) which plans/decides locations with maximum entropy to be observed that minimize the posterior entropy remaining in the unobserved areas of the field.", "startOffset": 105, "endOffset": 127}, {"referenceID": 27, "context": "5 log(2\u03c0e\u03c3 2 st+1|st) yields the well-known nonmyopic AL algorithm called maximum entropy sampling (MES) (Shewry and Wynn 1987) which plans/decides locations with maximum entropy to be observed that minimize the posterior entropy remaining in the unobserved areas of the field. Since R(zt+1, st+1) is independent of zt+1, the expectations in (1) go away, thus making MES non-adaptive and hence a straightforward search algorithm not plagued by the issue of uncountable set of candidate measurements. As such, we will not focus on such a degenerate case. This degeneracy vanishes when the environment field is instead a realization of log-Gaussian process. Then, MES becomes adaptive (Low, Dolan, and Khosla 2009) and its reward function can be represented by our Lipschitz continuous reward functions: By setting R1(zt+1) = 0, R2 and g\u03c3st+1|st as identity functions with `2(\u03c3st+1|st) = 1, and R3(st+1) = 0.5 log(2\u03c0e\u03c3 st+1|st), E[R(Zt+1, st+1)|dt, st+1] = \u03bcst+1|dt + 0.5 log(2\u03c0e\u03c3 2 st+1|st). Bayesian optimization (BO). The greedy BO algorithm of Srinivas et al. (2010) utilizes the UCB selection criterion \u03bcst+1|dt + \u03b2\u03c3st+1|st (\u03b2 \u2265 0) to approximately optimize the global BO objective of total field measurements \u2211H t=1 zt taken by the robot or, equivalently, minimize its total regret.", "startOffset": 106, "endOffset": 1069}, {"referenceID": 27, "context": "5 log(2\u03c0e\u03c3 2 st+1|st) yields the well-known nonmyopic AL algorithm called maximum entropy sampling (MES) (Shewry and Wynn 1987) which plans/decides locations with maximum entropy to be observed that minimize the posterior entropy remaining in the unobserved areas of the field. Since R(zt+1, st+1) is independent of zt+1, the expectations in (1) go away, thus making MES non-adaptive and hence a straightforward search algorithm not plagued by the issue of uncountable set of candidate measurements. As such, we will not focus on such a degenerate case. This degeneracy vanishes when the environment field is instead a realization of log-Gaussian process. Then, MES becomes adaptive (Low, Dolan, and Khosla 2009) and its reward function can be represented by our Lipschitz continuous reward functions: By setting R1(zt+1) = 0, R2 and g\u03c3st+1|st as identity functions with `2(\u03c3st+1|st) = 1, and R3(st+1) = 0.5 log(2\u03c0e\u03c3 st+1|st), E[R(Zt+1, st+1)|dt, st+1] = \u03bcst+1|dt + 0.5 log(2\u03c0e\u03c3 2 st+1|st). Bayesian optimization (BO). The greedy BO algorithm of Srinivas et al. (2010) utilizes the UCB selection criterion \u03bcst+1|dt + \u03b2\u03c3st+1|st (\u03b2 \u2265 0) to approximately optimize the global BO objective of total field measurements \u2211H t=1 zt taken by the robot or, equivalently, minimize its total regret. UCB can be represented by our Lipschitz continuous reward functions: By setting R1(zt+1) = 0, R2 and g\u03c3st+1|st as identity functions with `2(\u03c3st+1|st) = 1, and R3(st+1) = \u03b2\u03c3st+1|st , E[R(Zt+1, st+1)|dt, st+1] = \u03bcst+1|dt + \u03b2\u03c3st+1|st . In particular, when \u03b2 = 0, it can be derived that our GPP policy \u03c0\u2217 maximizes the expected total field measurements taken by the robot, hence optimizing the exact global BO objective of Srinivas et al. (2010) in the expected sense.", "startOffset": 106, "endOffset": 1730}, {"referenceID": 27, "context": "5 log(2\u03c0e\u03c3 2 st+1|st) yields the well-known nonmyopic AL algorithm called maximum entropy sampling (MES) (Shewry and Wynn 1987) which plans/decides locations with maximum entropy to be observed that minimize the posterior entropy remaining in the unobserved areas of the field. Since R(zt+1, st+1) is independent of zt+1, the expectations in (1) go away, thus making MES non-adaptive and hence a straightforward search algorithm not plagued by the issue of uncountable set of candidate measurements. As such, we will not focus on such a degenerate case. This degeneracy vanishes when the environment field is instead a realization of log-Gaussian process. Then, MES becomes adaptive (Low, Dolan, and Khosla 2009) and its reward function can be represented by our Lipschitz continuous reward functions: By setting R1(zt+1) = 0, R2 and g\u03c3st+1|st as identity functions with `2(\u03c3st+1|st) = 1, and R3(st+1) = 0.5 log(2\u03c0e\u03c3 st+1|st), E[R(Zt+1, st+1)|dt, st+1] = \u03bcst+1|dt + 0.5 log(2\u03c0e\u03c3 2 st+1|st). Bayesian optimization (BO). The greedy BO algorithm of Srinivas et al. (2010) utilizes the UCB selection criterion \u03bcst+1|dt + \u03b2\u03c3st+1|st (\u03b2 \u2265 0) to approximately optimize the global BO objective of total field measurements \u2211H t=1 zt taken by the robot or, equivalently, minimize its total regret. UCB can be represented by our Lipschitz continuous reward functions: By setting R1(zt+1) = 0, R2 and g\u03c3st+1|st as identity functions with `2(\u03c3st+1|st) = 1, and R3(st+1) = \u03b2\u03c3st+1|st , E[R(Zt+1, st+1)|dt, st+1] = \u03bcst+1|dt + \u03b2\u03c3st+1|st . In particular, when \u03b2 = 0, it can be derived that our GPP policy \u03c0\u2217 maximizes the expected total field measurements taken by the robot, hence optimizing the exact global BO objective of Srinivas et al. (2010) in the expected sense. So, unlike greedy UCB, our nonmyopic GPP framework does not have to explicitly consider an additional weighted exploration term (i.e., \u03b2\u03c3st+1|st ) in its reward function because it can jointly and naturally optimize the exploration-exploitation trade-off, as explained earlier. Nevertheless, if a stronger exploration behavior is desired (e.g., in online planning), then \u03b2 has to be fine-tuned. Different from nonmyopic BO algorithm of Marchant, Ramos, and Sanner (2014) using UCB-based rewards, our proposed nonmyopic -optimal GPP policy (Section 3) does not need to impose an extreme assumption of maximum likelihood observations during planning and, more importantly, provides a performance guarantee, including for the extreme", "startOffset": 106, "endOffset": 2224}, {"referenceID": 28, "context": "To represent such uncertainty at each encountered node, upper & lower heuristic bounds (respectively, V \u2217 t (dt) and V t (dt)) are maintained, like in (Smith and Simmons 2006).", "startOffset": 151, "endOffset": 175}, {"referenceID": 30, "context": "This section empirically evaluates the online planning performance and time efficiency of our -GPP policy \u03c0 and its anytime variant under limited sampling budget in an energy harvesting task on a simulated wind speed field and in BO on simulated plankton density (chl-a) field and real-world logpotassium (lg-K) concentration (mg l\u22121) field (Appendix I) of Broom\u2019s Barn farm (Webster and Oliver 2007).", "startOffset": 375, "endOffset": 400}, {"referenceID": 25, "context": "057 of lg-K field are learned using maximum likelihood estimation (Rasmussen and Williams 2006).", "startOffset": 66, "endOffset": 95}, {"referenceID": 29, "context": "In BO, the performances of our -GPP policy \u03c0 and its anytime variant are compared with that of state-of-the-art nonmyopic UCB (Marchant, Ramos, and Sanner 2014) and greedy PI, EI, UCB (Brochu, Cora, and de Freitas 2010; Srinivas et al. 2010).", "startOffset": 184, "endOffset": 241}, {"referenceID": 4, "context": "A robotic rover equipped with a wind turbine is tasked to harvest energy/power from the wind while exploring a polar region (Chen et al. 2014).", "startOffset": 124, "endOffset": 142}, {"referenceID": 30, "context": ", possibly in an over-fertilized area) while exploring the Broom\u2019s Barn farm (Webster and Oliver 2007).", "startOffset": 77, "endOffset": 102}, {"referenceID": 3, "context": "For our future work, we plan to scale up -GPP and its anytime variant for big data using parallelization (Chen et al. 2013; Low et al. 2015), online learning (Xu et al.", "startOffset": 105, "endOffset": 140}, {"referenceID": 17, "context": "For our future work, we plan to scale up -GPP and its anytime variant for big data using parallelization (Chen et al. 2013; Low et al. 2015), online learning (Xu et al.", "startOffset": 105, "endOffset": 140}, {"referenceID": 31, "context": "2015), online learning (Xu et al. 2014), and stochastic variational inference (Hoang, Hoang, and Low 2015) and extend them to handle unknown hyperparameters (Hoang et al.", "startOffset": 23, "endOffset": 39}, {"referenceID": 12, "context": "2014), and stochastic variational inference (Hoang, Hoang, and Low 2015) and extend them to handle unknown hyperparameters (Hoang et al. 2014).", "startOffset": 123, "endOffset": 142}, {"referenceID": 9, "context": ", possibly in an algal bloom) while exploring a lake (Dolan et al. 2009).", "startOffset": 53, "endOffset": 72}], "year": 2015, "abstractText": "This paper presents a novel nonmyopic adaptive Gaussian process planning (GPP) framework endowed with a general class of Lipschitz continuous reward functions that can unify some active learning/sensing and Bayesian optimization criteria and offer practitioners some flexibility to specify their desired choices for defining new tasks/problems. In particular, it utilizes a principled Bayesian sequential decision problem framework for jointly and naturally optimizing the exploration-exploitation trade-off. In general, the resulting induced GPP policy cannot be derived exactly due to an uncountable set of candidate observations. A key contribution of our work here thus lies in exploiting the Lipschitz continuity of the reward functions to solve for a nonmyopic adaptive -optimal GPP ( -GPP) policy. To plan in real time, we further propose an asymptotically optimal, branch-and-bound anytime variant of -GPP with performance guarantee. We empirically demonstrate the effectiveness of our -GPP policy and its anytime variant in Bayesian optimization and an energy harvesting task.", "creator": "TeX"}}}