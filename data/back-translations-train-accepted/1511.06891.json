{"id": "1511.06891", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2015", "title": "Near-Optimal Active Learning of Multi-Output Gaussian Processes", "abstract": "This paper addresses the problem of active learning of a multi-output Gaussian process (MOGP) model representing multiple types of coexisting correlated environmental phenomena. In contrast to existing works, our active learning problem involves selecting not just the most informative sampling locations to be observed but also the types of measurements at each selected location for minimizing the predictive uncertainty (i.e., posterior joint entropy) of a target phenomenon of interest given a sampling budget. Unfortunately, such an entropy criterion scales poorly in the numbers of candidate sampling locations and selected observations when optimized. To resolve this issue, we first exploit a structure common to sparse MOGP models for deriving a novel active learning criterion. Then, we exploit a relaxed form of submodularity property of our new criterion for devising a polynomial-time approximation algorithm that guarantees a constant-factor approximation of that achieved by the optimal set of selected observations. Empirical evaluation on real-world datasets shows that our proposed approach outperforms existing algorithms for active learning of MOGP and single-output GP models.", "histories": [["v1", "Sat, 21 Nov 2015 15:08:53 GMT  (176kb,D)", "https://arxiv.org/abs/1511.06891v1", "30th AAAI Conference on Artificial Intelligence (AAAI 2016), Extended version with proofs, 13 pages"], ["v2", "Tue, 24 Nov 2015 08:45:36 GMT  (176kb,D)", "http://arxiv.org/abs/1511.06891v2", "30th AAAI Conference on Artificial Intelligence (AAAI 2016), Extended version with proofs, 13 pages"]], "COMMENTS": "30th AAAI Conference on Artificial Intelligence (AAAI 2016), Extended version with proofs, 13 pages", "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG", "authors": ["yehong zhang", "trong nghia hoang", "kian hsiang low", "mohan s kankanhalli"], "accepted": true, "id": "1511.06891"}, "pdf": {"name": "1511.06891.pdf", "metadata": {"source": "CRF", "title": "Near-Optimal Active Learning of Multi-Output Gaussian Processes", "authors": ["Yehong Zhang", "Trong Nghia Hoang", "Kian Hsiang Low", "Mohan Kankanhalli"], "emails": ["mohan}@comp.nus.edu.sg,", "idmhtn@nus.edu.sg"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that it is a matter of the way in which people move in the most diverse parts of the world. (...) In practice, it is often the case that people in the most diverse parts of the world have the most diverse views. (...) It is as if people in the most diverse regions of the world have the same views. (...) It is as if people in the most diverse regions of the world have the same views. (...) It is as if people in the most diverse regions of the world have the same views. (...) It is as if people in the most diverse regions of the world have the same views. (...)"}, {"heading": "2 Modeling Coexisting Phenomena with Multi-Output Gaussian Process (MOGP)", "text": "Convolved MOGP (CMOGP) regression (random) measurement; < < a series of MOGP models such as co-Kriging (Webster and Oliver 2007), Parameter Sharing (Skolidis 2012) and Linear Model of Coregionalization (LMC) (Teh and Seeger 2005; < Bonilla, Chai and Williams 2008) have been proposed to handle several types of correlated outputs. A generalization of the LMC called \"Convolved MOGP\" (CMOGP) model has been shown empirically to outperform the others and will be the MOGP model of our choice due to its approximation, the structure of which can be used to derive our active learning criterion and in which an efficient approximation algorithm can be defined as detailed later.Let M types of coexisting phenomena be defined as realization of a CMOGP phenomenon."}, {"heading": "3 Active Learning of CMOGP", "text": "XE (YZ | YX), 12 Log (2E), 12 Log (2E), 12 Log (XE), 12 Log (XE), 12 Log (XE), 12 Log (XE), 12 Log (XE), 12 Log (XE), 12 Log (XE), 12 Log (XE), 12 Log (XE), 12 Log (XE), 12 Log (XE), 12 Log (XE), 12 Log (XE), 12 Log (XE), 12 Log (XE), 12 Log (XE), 12 Log (XE), 12 Log (XE), 12 Log (XE), 12 Log (XE), 12 Log (XE), 12 Log (XE), 12 Log (XE), 12 Log (XE), 12 Log (XE), 12 Log (XE), 12 Log (XE), 12 Log (XE), 12 Log (XE), 12 Log (XE), 12 Log (XE), 12 Log (XE), 12 Log (XE), 12 Log (XE, 12 Log (XE), 12 Log (XE), 12 Log (XE), 12 Log (XE (XE), 12 Log (XE), 12 Log (XE (XE), 12 Log (XE), 12 Log (XE (XE), 12 Log (XE), 12 Log (XE), 12 Log (XE (XE), 12 (XE), 12 (XE (XE), 12 (XE), 12 (XE), 12 (XE (XE), 12 (XE), 12, 12 (XE (XE), 12 (XE), 12 (XE (XE), 12 (XE), 12 (XE), 12 (XE (XE), 12 (XE), 12 (XE (XE), 12 (XE (XE),"}, {"heading": "4 Approximation Algorithm", "text": "Our new active learning criterion in (6), when optimized, still suffers from poor scalability (< < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < &lt"}, {"heading": "5 Experiments and Discussion", "text": "This section evaluates the forward-looking performance of our approximation algorithms (m-Greedy), which empirically capture the location using three real-world datasets: (a) Jura Dataset (Goovaerts 1997) contains concentrations of 7 heavy metals collected at 359 locations in a Swiss Jura region; (b) Gilgai Dataset et et al. (Webster 1977) contains electrical conductivity and chloride content generated from a line transport sector survey of 365 sites in the Gilgai Territory in New South Wales, Australia. (c) Dataset al. (Bodik et al. 2004) contains temperature and light types measured by 43 temperature sensors and 41 light sensors used in the Intel Berkeley Research Laboratory. Sampling sites for the Jura and IEQ datasets are included in Appendix I. The performance of m-Greedy is compared with (a) maximum variance / entropy (VAR)."}, {"heading": "6 Related Work", "text": "Existing work on active learning with multiple output measurement types is not driven by the MOGP model and has not formally characterized the cross-correlation structure between different types of phenomena: Some spatial scanning algorithms (Bueso, Angulo and Alonso 1998; Angulo and Bueso 2001) have modeled the auxiliary phenomenon merely as a noisy disturbance of the latent target phenomenon, which differs from our work here. Multi-task active learning (MTAL) and active transfer learning (ATL) algorithms have considered predicting any type of phenomenon as a task and used the auxiliary tasks to help in learning the target task, but the Zhang MTAL algorithm (2010) requires that relationships between different classification tasks be specified manually, which is highly non-trivial in practice and not applicable to MOGP regression. Some ATL and active learning algorithms of Zhang (2006) cannot be used on the basis of Zhao and Zhao criteria, for example."}, {"heading": "7 Conclusion", "text": "This paper describes a novel efficient algorithm for active learning of a MOGP model. To solve the problem of poor scalability in optimizing the conventional entropy criterion, we use a common structure for a uniform framework of frugal MOGP models to derive a novel active learning criterion (6). Subsequently, we use the -submodularity property of our new criterion (Lemma 1) to develop a polynomial-time approximation algorithm (9) that guarantees a constant approximation of what is achieved through the optimal compilation of selected observations (Theorem 2). Empirical evaluation of three real-world datasets shows that our m-edy Greedy approximation algorithm exceeds existing algorithms for active learning of MOGP and single-output GP models, especially when the measurements of the target phenomenon are louder than those of the auxiliary types."}, {"heading": "A Derivation of Novel Active Learning", "text": "Criterion (6) arg min X: | X | = N H (YVt\\ Xt | YX) = arg max X: | X | = N H (YVt\\ Xt | LU) \u2212 H (YVt\\ Xt | YX) = arg max X: | X | = N H (YVt\\ Xt | LU) \u2212 H (YVt\\ Xt | LU) + H (YVt\\ Xt | LU) \u2212 H (YVt\\ Xt\\ Xt, YVt\\ Xt\\ Xt) + I (YVt\\ Xt\\ Xt\\ Xt, Yt\\ Xt\\ Xt)."}, {"heading": "B Time Complexity of Evaluating Active", "text": "In the US, this is the case when all tuples in X of measurement type t (i.e., X = XT).I (LU\\ XT\\ XT).I (LU\\ XT\\ XT\\ XT) = H (LU\\ XT) \u2212 H (LU\\ XT) is the time for each X-V; this worst-case complexity occurs when all tuples in X of measurement type t (i.e., X = XT).I (LU\\ XT\\ XT) is the time for each X-V; this worst-case complexity occurs when all tuples in X of measurement type t (i.e., X = XT).I (XT)."}, {"heading": "D Proof of Proposition 1", "text": "Before Proposition 1 is proven, the following terms are required: < < < < 3 For all X > > > V and < x, i > PITC < x, i > < x, i > < X, i > < x, i > < x, i > PITC (Cao, Low, and Dolan 2013).Lemma 4, assuming there are no suppression variables, i > < x, i > < x, i >, < x, i >, i >, < x, i >, < x, i >."}, {"heading": "E Proof of Theorem 1", "text": "If i = t, thenH < x, t > < YX) = > PITC < x = > PITC < x < x < x > Time for each < x > Time for each < x, t > < x > Time for each < x > Time for each < x, t > Time for each < x, t > Time for each < x, t > Vt\\ Xt andO (3 + N3) Time for each (3 + N3) Time for each (3 + N3) This worst-case complexity occurs when all tuples in X have a measurement type."}, {"heading": "F Proof of Lemma 1", "text": "To prove that F (X) is subordinate, we must show that F (X) is subordinate: < < X (X) + < X (X) + < X (X) + < X (X) + < X (V) + < X (V) and < X (V) < X (X) < X (X), X (X), X (X), X (X), X (X), X (X), X (X), X (X), X (X), X (X)."}, {"heading": "G Proof of Theorem 2", "text": "Our proof here is similar to the theorem 1.5 in (Krause and Golovin 2014), which is a generalization of the known result of Nemhauser, Wolsey, and Fisher (1978). \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212:::::\\\\\\\\\\\\\\::::::::::::::::::::. X. X. X. X. X. X. X. X. X. X. X. X. X. \u2212 X. \u2212 X. \u2212. \u2212 X. \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 X. X. \u2212 \u2212 \u2212 \u2212 \u2212 X. \u2212 \u2212 \u2212 \u2212 \u2212 X. \u2212 x \u2212 x \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 X. \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 X. X. X. X. \u2212 X. \u2212 X. \u2212 X. \u2212 X. \u2212 \u2212 \u2212 \u2212 \u2212 X. \u2212 X. \u2212 X. \u2212 X. \u2212 \u2212 X. \u2212 \u2212 \u2212 \u2212 X. \u2212 X. \u2212 X. \u2212 X. \u2212 X. \u2212 X. \u2212 X. \u2212 X. \u2212 X. \u2212 X. \u2212 X. \u2212 \u2212 \u2212 \u2212 \u2212 \u2212"}, {"heading": "H Proof of Lemma 2", "text": "From GP posterior's incremental update formula (see Annex C in (Xu et al. 2014), & \u2212 & \u2212 & \u2212 < x, i > < x, i > < x, i > < x, i > < x, i > < x, i > < x, i > < x, i > < x, i > < x, i > PITC < x, i > < x, i > < x, i > PITC < x, i > < x, i > PITCA < x < x, i > PITC < x, i > < x, i > PITC < < < < &ltC < < < x, i < < < < < < < < < < < < < < < < < < < < x, i < < < < < < < < < < x, < < < < < < < < < PITC < x, < < < PITC < x < < < PITC < x < PITC < x, < PITC < x < x < x < PITC < PITC < x < x < PITC < x < PITC, < x < x < PITC < PITC < x < x < PITC < x < x < PITC < PITC < x < PITC, < PITC < PITC &lt"}, {"heading": "I Jura and IEQ Datasets", "text": "J Signal-to-noise ratio for Jura dataset"}], "references": [{"title": "Computationally efficient convolved multiple output Gaussian processes", "author": ["\u00c1lvarez", "M.A. Lawrence 2011] \u00c1lvarez", "N.D. Lawrence"], "venue": null, "citeRegEx": "\u00c1lvarez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "\u00c1lvarez et al\\.", "year": 2011}, {"title": "Random perturbation methods applied to multivariate spatial sampling design", "author": ["Angulo", "J.M. Bueso 2001] Angulo", "M.C. Bueso"], "venue": "Environmetrics", "citeRegEx": "Angulo et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Angulo et al\\.", "year": 2001}, {"title": "Multi-task Gaussian process prediction", "author": ["Chai Bonilla", "E.V. Williams 2008] Bonilla", "K.M.A. Chai", "C.K. Williams"], "venue": "In Proc. NIPS", "citeRegEx": "Bonilla et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bonilla et al\\.", "year": 2008}, {"title": "A state-space model approach to optimum spatial sampling design based on entropy. Environmental and Ecological Statistics 5(1):29\u201344", "author": ["Angulo Bueso", "M.C. Alonso 1998] Bueso", "J.M. Angulo", "F.J. Alonso"], "venue": null, "citeRegEx": "Bueso et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Bueso et al\\.", "year": 1998}, {"title": "CruzSanjuli\u00e1n, J.; and Garc\u0131\u0301a-Ar\u00f3stegui", "author": ["Bueso"], "venue": "J. L", "citeRegEx": "Bueso,? \\Q1999\\E", "shortCiteRegEx": "Bueso", "year": 1999}, {"title": "Multi-robot informative path planning for active sensing of environmental phenomena: A tale of two algorithms", "author": ["Low Cao", "N. Dolan 2013] Cao", "K.H. Low", "J.M. Dolan"], "venue": "In Proc. AAMAS", "citeRegEx": "Cao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2013}, {"title": "Decentralized data fusion and active sensing with mobile sensors for modeling and predicting spatiotemporal traffic phenomena", "author": ["Chen"], "venue": null, "citeRegEx": "Chen,? \\Q2012\\E", "shortCiteRegEx": "Chen", "year": 2012}, {"title": "Parallel Gaussian process regression with low-rank covariance matrix approximations", "author": ["Chen"], "venue": "In Proc. UAI,", "citeRegEx": "Chen,? \\Q2013\\E", "shortCiteRegEx": "Chen", "year": 2013}, {"title": "Gaussian process decentralized data fusion and active sensing for spatiotemporal traffic modeling and prediction in mobility-on-demand systems", "author": ["Chen"], "venue": "IEEE Trans. Autom", "citeRegEx": "Chen,? \\Q2015\\E", "shortCiteRegEx": "Chen", "year": 2015}, {"title": "Gaussian process-based decentralized data fusion and active sensing for mobility-on-demand system", "author": ["Low Chen", "J. Tan 2013] Chen", "K.H. Low", "C.K.-Y. Tan"], "venue": "In Proc. RSS", "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Elements of Information Theory", "author": ["Cover", "T. Thomas 1991] Cover", "J. Thomas"], "venue": null, "citeRegEx": "Cover et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Cover et al\\.", "year": 1991}, {"title": "Algorithms for subset selection in linear regression", "author": ["Das", "A. Kempe 2008] Das", "D. Kempe"], "venue": "In Proc. STOC,", "citeRegEx": "Das et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Das et al\\.", "year": 2008}, {"title": "Matrix Computations", "author": ["Golub", "G.H. Van Loan 1996] Golub", "C.F. Van Loan"], "venue": null, "citeRegEx": "Golub et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Golub et al\\.", "year": 1996}, {"title": "Nonmyopic -Bayes-optimal active learning of Gaussian processes", "author": ["Hoang"], "venue": "In Proc. ICML,", "citeRegEx": "Hoang,? \\Q2014\\E", "shortCiteRegEx": "Hoang", "year": 2014}, {"title": "A unifying framework of anytime sparse Gaussian process regression models with stochastic variational inference for big data", "author": ["Hoang Hoang", "T.N. Low 2015] Hoang", "Q.M. Hoang", "K.H. Low"], "venue": null, "citeRegEx": "Hoang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hoang et al\\.", "year": 2015}, {"title": "Submodular function maximization", "author": ["Krause", "A. Golovin 2014] Krause", "D. Golovin"], "venue": null, "citeRegEx": "Krause et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2014}, {"title": "Near-optimal nonmyopic value of information in graphical models", "author": ["Krause", "A. Guestrin 2005] Krause", "C. Guestrin"], "venue": "In Proc. UAI", "citeRegEx": "Krause et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2005}, {"title": "Near-optimal sensor placements in Gaussian processes: Theory, efficient algorithms and empirical studies", "author": ["Singh Krause", "A. Guestrin 2008] Krause", "A. Singh", "C. Guestrin"], "venue": null, "citeRegEx": "Krause et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2008}, {"title": "Designing networks for monitoring multivariate environmental fields using data with monotone pattern", "author": ["Sun Le", "N.D. Zidek 2003] Le", "L. Sun", "J.V. Zidek"], "venue": "Technical Report #2003-5,", "citeRegEx": "Le et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Le et al\\.", "year": 2003}, {"title": "Gaussian process planning with Lipschitz continuous reward functions: Towards unifying Bayesian optimization, active learning, and beyond", "author": ["Low Ling", "C.K. Jaillet 2016] Ling", "K.H. Low", "P. Jaillet"], "venue": "In Proc. AAAI", "citeRegEx": "Ling et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2016}, {"title": "Parallel Gaussian process regression for big data: Low-rank representation meets Markov approximation", "author": ["Low"], "venue": "In Proc. AAAI", "citeRegEx": "Low,? \\Q2015\\E", "shortCiteRegEx": "Low", "year": 2015}, {"title": "Adaptive multi-robot wide-area exploration and mapping", "author": ["Dolan Low", "K.H. Khosla 2008] Low", "J.M. Dolan", "P. Khosla"], "venue": "In Proc. AAMAS,", "citeRegEx": "Low et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Low et al\\.", "year": 2008}, {"title": "Information-theoretic approach to efficient adaptive path planning for mobile robotic environmental sensing", "author": ["Dolan Low", "K.H. Khosla 2009] Low", "J.M. Dolan", "P. Khosla"], "venue": "In Proc. ICAPS", "citeRegEx": "Low et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Low et al\\.", "year": 2009}, {"title": "Active Markov information-theoretic path planning for robotic environmental sensing", "author": ["Dolan Low", "K.H. Khosla 2011] Low", "J.M. Dolan", "P. Khosla"], "venue": null, "citeRegEx": "Low et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Low et al\\.", "year": 2011}, {"title": "An analysis of approximations for maximizing submodular set functions - I", "author": ["Wolsey Nemhauser", "G.L. Fisher 1978] Nemhauser", "L.A. Wolsey", "M.L. Fisher"], "venue": "Mathematical Programming", "citeRegEx": "Nemhauser et al\\.,? \\Q1978\\E", "shortCiteRegEx": "Nemhauser et al\\.", "year": 1978}, {"title": "The Matrix Cookbook", "author": ["Petersen", "K.B. Pedersen 2012] Petersen", "M.S. Pedersen"], "venue": null, "citeRegEx": "Petersen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Petersen et al\\.", "year": 2012}, {"title": "About the mutual (conditional) information", "author": ["Renner", "R. Maurer 2002] Renner", "U. Maurer"], "venue": "In Proc. IEEE ISIT", "citeRegEx": "Renner et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Renner et al\\.", "year": 2002}, {"title": "Margin-based active learning for structured output spaces", "author": ["Roth", "D. Small 2006] Roth", "K. Small"], "venue": "In Proc. ECML,", "citeRegEx": "Roth et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Roth et al\\.", "year": 2006}, {"title": "Maximum entropy sampling", "author": ["Shewry", "M.C. Wynn 1987] Shewry", "H.P. Wynn"], "venue": "J. Applied Stat", "citeRegEx": "Shewry et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Shewry et al\\.", "year": 1987}, {"title": "Matrix Perturbation Theory", "author": ["Stewart", "G.W. Sun 1990] Stewart", "Sun", "J.-G"], "venue": null, "citeRegEx": "Stewart et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Stewart et al\\.", "year": 1990}, {"title": "Semiparametric latent factor models", "author": ["Teh", "Y.W. Seeger 2005] Teh", "M. Seeger"], "venue": "In Proc. AISTATS,", "citeRegEx": "Teh et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2005}, {"title": "Geostatistics for Environmental Scientists", "author": ["Webster", "R. Oliver 2007] Webster", "M. Oliver"], "venue": null, "citeRegEx": "Webster et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Webster et al\\.", "year": 2007}, {"title": "GP-Localize: Persistent mobile robot localization using online sparse Gaussian process observation model", "author": ["Xu"], "venue": "In Proc. AAAI,", "citeRegEx": "Xu,? \\Q2014\\E", "shortCiteRegEx": "Xu", "year": 2014}, {"title": "Near-optimal active learning of multi-output Gaussian processes", "author": ["Zhang"], "venue": "In Proc. AAAI", "citeRegEx": "Zhang,? \\Q2016\\E", "shortCiteRegEx": "Zhang", "year": 2016}, {"title": "Active transfer learning for cross-system recommendation", "author": ["Zhao"], "venue": "In Proc. AAAI,", "citeRegEx": "Zhao,? \\Q2013\\E", "shortCiteRegEx": "Zhao", "year": 2013}, {"title": "\u00c1lvarez and Lawrence 2011)) incurs O(|U |) time for every \u3008x, i\u3009 \u2208 V-t \\ X-t and O(|U | + N) time in each iteration, and a one-off cost", "author": ["\u03a3AU A"], "venue": null, "citeRegEx": "A,? \\Q2011\\E", "shortCiteRegEx": "A", "year": 2011}], "referenceMentions": [{"referenceID": 35, "context": "A submodular set function exhibits a natural diminishing returns property: When adding an element to its input set, the increment in its function value decreases with a larger input set. To maximize a nondecreasing and submodular set function, the work of Nemhauser, Wolsey, and Fisher (1978) has proposed a greedy algorithm guaranteeing a (1\u22121/e)-factor approximation of that achieved by the optimal input set.", "startOffset": 0, "endOffset": 293}, {"referenceID": 35, "context": "A submodular set function exhibits a natural diminishing returns property: When adding an element to its input set, the increment in its function value decreases with a larger input set. To maximize a nondecreasing and submodular set function, the work of Nemhauser, Wolsey, and Fisher (1978) has proposed a greedy algorithm guaranteeing a (1\u22121/e)-factor approximation of that achieved by the optimal input set. The main difficulty in proving the submodularity of F (X) (8) lies in its mutual information term being conditioned on X . Some works (Krause and Guestrin 2005; Renner and Maurer 2002) have shown the submodularity of such conditional mutual information by imposing conditional independence assumptions (e.g., Markov chain). In practice, these strong assumptions (e.g., YA \u22a5 Y\u3008x,i\u3009|YVt\\Xt for any A \u2286 X and \u3008x, i\u3009 \u2208 V-t \\ X-t) severely violate the correlation structure of multiple types of coexisting phenomena and are an overkill: The correlation structure can in fact be preserved to a fair extent by relaxing these assumptions, which consequently entails a relaxed form of submodularity of F (X) (8); a performance guarantee similar to that of Nemhauser, Wolsey, and Fisher (1978) can then be derived for our approximation algorithm.", "startOffset": 0, "endOffset": 1196}, {"referenceID": 35, "context": "Its proof (Appendix G) is similar to that of the well-known result of Nemhauser, Wolsey, and Fisher (1978) except for exploiting -submodularity of F (X) in Lemma 1 instead of submodularity.", "startOffset": 11, "endOffset": 107}, {"referenceID": 4, "context": "Existing works on active learning with multiple output measurement types are not driven by the MOGP model and have not formally characterized the cross-correlation structure between different types of phenomena: Some spatial sampling algorithms (Bueso, Angulo, and Alonso 1998; Angulo and Bueso 2001) have simply modeled the auxiliary phenomenon as a noisy perturbation of the target phenomenon that is assumed to be latent, which differs from our work here. Multi-task active learning (MTAL) and active transfer learning (ATL) algorithms have considered the prediction of each type of phenomenon as one task and used the auxiliary tasks to help learn the target task. But, the MTAL algorithm of Zhang (2010) requires relations between different classification tasks to be manually specified, which is highly non-trivial to achieve in practice and not applicable to MOGP regression.", "startOffset": 246, "endOffset": 709}, {"referenceID": 9, "context": "2014; Ling, Low, and Jaillet 2016; Low, Dolan, and Khosla 2009; Low, Dolan, and Khosla 2008; Low, Dolan, and Khosla 2011) of single-output GPs to that of MOGPs and improving its scalability to big data through parallelization (Chen et al. 2013; Low et al. 2015), online learning (Xu et al.", "startOffset": 226, "endOffset": 261}, {"referenceID": 9, "context": "[Chen et al. 2013] Chen, J.", "startOffset": 0, "endOffset": 18}], "year": 2015, "abstractText": "This paperaddresses the problem of active learning of a multi-output Gaussian process (MOGP) model representing multiple types of coexisting correlated environmental phenomena. In contrast to existing works, our active learning problem involves selecting not just the most informative sampling locations to be observed but also the types of measurements at each selected location for minimizing the predictive uncertainty (i.e., posterior joint entropy) of a target phenomenon of interest given a sampling budget. Unfortunately, such an entropy criterion scales poorly in the numbers of candidate sampling locations and selected observations when optimized. To resolve this issue, we first exploit a structure common to sparse MOGP models for deriving a novel active learning criterion. Then, we exploit a relaxed form of submodularity property of our new criterion for devising a polynomial-time approximation algorithm that guarantees a constant-factor approximation of that achieved by the optimal set of selected observations. Empirical evaluation on real-world datasets shows that our proposed approach outperforms existing algorithms for active learning of MOGP and single-output GP models.", "creator": "LaTeX with hyperref package"}}}