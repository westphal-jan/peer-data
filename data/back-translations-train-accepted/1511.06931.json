{"id": "1511.06931", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2015", "title": "Evaluating Prerequisite Qualities for Learning End-to-End Dialog Systems", "abstract": "A long-term goal of machine learning is to build intelligent conversational agents. One recent popular approach is to train end-to-end models on a large amount of real dialog transcripts between humans (Sordoni et al., 2015; Vinyals &amp; Le, 2015; Shang et al., 2015). However, this approach leaves many questions unanswered as an understanding of the precise successes and shortcomings of each model is hard to assess. A contrasting recent proposal are the bAbI tasks (Weston et al., 2015b) which are synthetic data that measure the ability of learning machines at various reasoning tasks over toy language. Unfortunately, those tests are very small and hence may encourage methods that do not scale. In this work, we propose a suite of new tasks of a much larger scale that attempt to bridge the gap between the two regimes. Choosing the domain of movies, we provide tasks that test the ability of models to answer factual questions (utilizing OMDB), provide personalization (utilizing MovieLens), carry short conversations about the two, and finally to perform on natural dialogs from Reddit. We provide a dataset covering 75k movie entities and with 3.5M training examples. We present results of various models on these tasks, and evaluate their performance.", "histories": [["v1", "Sat, 21 Nov 2015 22:26:49 GMT  (50kb)", "https://arxiv.org/abs/1511.06931v1", null], ["v2", "Tue, 15 Dec 2015 09:31:59 GMT  (50kb)", "http://arxiv.org/abs/1511.06931v2", null], ["v3", "Wed, 6 Jan 2016 04:51:54 GMT  (51kb)", "http://arxiv.org/abs/1511.06931v3", null], ["v4", "Fri, 1 Apr 2016 06:22:44 GMT  (52kb)", "http://arxiv.org/abs/1511.06931v4", null], ["v5", "Fri, 15 Apr 2016 20:22:13 GMT  (52kb)", "http://arxiv.org/abs/1511.06931v5", null], ["v6", "Tue, 19 Apr 2016 15:30:29 GMT  (52kb)", "http://arxiv.org/abs/1511.06931v6", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["jesse dodge", "reea gane", "xiang zhang", "antoine bordes", "sumit chopra", "alexander miller", "arthur szlam", "jason weston"], "accepted": true, "id": "1511.06931"}, "pdf": {"name": "1511.06931.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["jessedodge@fb.com", "agane@fb.com", "xiangz@fb.com", "abordes@fb.com", "spchopra@fb.com", "ahm@fb.com", "aszlam@fb.com", "jase@fb.com"], "sections": [{"heading": null, "text": "ar Xiv: 151 1.06 931v 6 [cs.C L] 19 Apr 2A The long-term goal of machine learning is to build intelligent conversation agents. A popular approach of late has been to train end-to-end models on a large number of real human dialogue transcripts (Sordoni et al., 2015; Vinyals & Le, 2015; Shang et al., 2015). However, this approach leaves many questions unanswered, as understanding the precise successes and shortcomings of each model is difficult to assess. A contrasting current proposal is the bAbI tasks (Weston et al., 2015b), which are synthetic data measuring the ability of learning machines to perform various thought tasks on toy language. Unfortunately, these tests are very small and can therefore promote methods that do not scale. In this work, we propose a number of new tasks on a much larger scale that attempt to bridge the gap between the two regimes."}, {"heading": "1 INTRODUCTION", "text": "In recent years, the number of people who are able to survive in the USA, Europe and the world has significantly decreased, not only in the USA, but also in other parts of the world. In the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA,"}, {"heading": "2 THE MOVIE DIALOG DATASET", "text": "We present a series of four tasks to test the ability of end-to-end dialog systems, focusing on movies and movie-related units. They aim to test five skills that we postulate as the key to a fully functioning general dialog system (i.e., not movie-specific per se): \u2022 QA Dataset: Test the ability to answer factoid questions that can be answered without reference to the previous dialog. \u2022 Recommendation Dataset: Test the ability to provide the user with personalized answers using recommendations (in this case, movies) rather than universal facts as above. \u2022 QA + Recommendation Dataset: Test the ability to conduct short dialogs that include both factoid and personalized content where the conversation state must be maintained. \u2022 Reddit Dataset: Test the ability to identify the most likely answers in discussions on Reddit. \u2022 Common Dataset: All of our tasks can be combined into a single dataset."}, {"heading": "2.1 QUESTION ANSWERING (QA)", "text": "The first task we are building is to test whether a dialogue agent is able to answer simple factual questions. The data set was created from the Open Movie Database (OMDb) 4, which contains metadata about movies. The subset we are looking at contains between 15k movies, between 10k actors and between 6k directors. We also assigned these movies to the MovieLens Dataset5 to assign tags to each movie. We build a knowledge base (KB) directly from the combined data, stored as triples like (THE DARK HORSE, 1http: / / en.omdb.org 2http: / / movielens.org 3http: / / movielens.org 3http: / / reddit.com / r / movie 4Downloaded from http: / beforethecode.com / projects / omdb / download.aspx. 5http: / grouplens.org / datasets / movielens.org _ en _ stars _ generated."}, {"heading": "2.2 RECOMMENDATION DATASET", "text": "Not all questions about movies in dialogues have an objective answer, regardless of the person asking them; in fact, much of the human dialogue is based on expressions of opinion and personalized answers. One of the easiest dialogs of this type to evaluate is the recommendation, where we can use existing data resources. We re-use the MovieLens data set, which has a custom article matrix of movie ratings, rated from 1 to 5. We first filter the set of movies to have the same sentence as in the QA task, and additionally keep only movies that had at least 2 ratings, which is about 11k movies.To use this data to evaluate the dialogue, we then use it to generate dialogue gauge exchange. We first select a user at random who participates in the dialogue, and then try 1-8 movies that the user has rated 5. We then make a statement that aims to express the user's feelings about these movies, selected randomly from a set of natural language templates."}, {"heading": "2.3 QA+RECOMMENDATION DIALOG", "text": "The tasks presented so far are only questions, followed by answers, with no context from previous dialogues. This task aims to evaluate answers in the context of several previous exchange processes, without remaining manageable, that evaluation and analysis are still comprehensible. Therefore, we combine the answer and recommendation tasks from before in a multi-answer dialogue, in which dialogues consist of 3 exchange processes (3 rounds by each participant).The first exchange requires a recommendation, which resembles the answer of the model (film proposal), except that they also indicate what genre or theme they are interested in, e.g. \"I am looking for a music film,\" the answer could be \"School of Rock,\" as in the example of Table 3. In the second exchange, given the answer of the model (film proposal), the user asks a factual question about this proposal, e.g. \"What else is it about?,\" \"\" Who plays a role in it? \"and so on. This question refers to the previous dialogue and what makes the additional context of the 1Tim's 1th request for additional information?"}, {"heading": "2.4 REDDIT DISCUSSION", "text": "Our fourth task is to predict responses in film discussions by using real conversation data that comes directly from Reddit, a site where registered community members can submit content in various areas of interest, called \"subreddits.\" We selected the movie Subreddit6 to match our other tasks, and the original discussion data is potentially between multiple participants. To simplify the setup, we flatten them out as two participants (parents and comments), just like in our other tasks. In this way, we collected \"1M dialogs,\" 10k of which are reserved for a development group, and a further 10k for the 6https: / / www.reddit.com / r / movies, by selecting from the data set at https: / / www.reddit.com / r / Datasets / Comments / 3bxlg7.test. Of the dialogs, 76% include a single exchange, and 17% have at least two exchanges, and 7% have at least three exchanges (the longest interchange we have is a 10k to 10k issue)."}, {"heading": "2.5 JOINT TASK", "text": "Finally, we look at a task that consists of combining all four preceding ones. In both the training and the test periods, examples consist of exchanging randomly collected data sets, with the conversation being \"reset\" for each sample, so that the context history always includes only the exchange from the current conversation. We consider this to be the most important task, as it checks whether a model can not only generate chat (task 4), but also provide meaningful answers during the dialog (task 1-3). On the other hand, the point in defining the individual tasks is to assess exactly what type of dialogue a model is successful or not."}, {"heading": "2.6 RELATION TO EXISTING EVALUATION FRAMEWORKS", "text": "Traditional dialogue systems consist of two main modules: (1) a dialogue state tracking component that tracks what has happened in a dialogue, the inclusion in a predefined explicit state system of outputs, user expressions, context from earlier phrases and other external information, and (2) a response generator. The evaluation of the state of dialogue tracking has been available since the PARADISE framework (Walker et al., 1997) and subsequent initiatives (Paek, 2001; Griol et al., 2008), including recent competitions (Williams et al., 2013; Henderson et al., 2014) and situated variants (Rojas-Barahona et al., 2012) and subsequent initiatives (Griol et al.), the fine-grained data annotations related to the described internal states of dialogue and precisely defined user intentions (Goals et al., Henderson et al., 2014) and situated variants (Rojas-Barahona et al.)."}, {"heading": "3 MODELS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 MEMORY NETWORKS", "text": "rf\u00fc ide rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf"}, {"heading": "3.2 SUPERVISED EMBEDDING MODELS", "text": "While one of the most important uses of Word embedding models is to learn unattended embedding over large, unlabeled data sets such as in Word2Vec (Mikolov et al., 2013), there are also very effective Word embedding models for training monitored models when labeled data is available. The simplest approach, which works amazingly well, is to independently combine the word embedding of input and target and then compare it with a similarity metric such as internal product or cosmic similarity. A ranking loss is used to ensure that the right targets are ranked higher than all other targets. Several variants of this approach exist. Comparing two documents under supervision of semantic indexing (SSI) has been shown to be superior to unattended latent semantic indexing (LSI) (Bai et al., 2009). Similar methods have been demonstrated to exceed SVD recommendations for 2013 (Weston) al."}, {"heading": "3.3 RECURRENT LANGUAGE MODELS", "text": "Recurring Neural Networks (RNNs) have proven successful in several tasks relating to natural language, language modeling (Mikolov et al., 2011) and have recently been applied to dialog (Sordoni et al., 2015; Vinyals & Le, 2015; Shang et al., 2015). However, LSTMs are not known for tasks such as QA or item recommendations, and therefore we expect them to find our data sets challenging. There are a large number of variants of RNNNs, including Long-Short Term Memory Activation Units (LSTMs) (Hochreiter & Schmidhuber, 1997), bi-directional LSTMs (Graves et al., 2012), seq2seq models (Sutskever et al., 2014), RNNNNNs that take into account the context of the document (Mikolov & Zweig, 2012), and RNNNNNNs that all use their attention beyond their input in different ways (Hermann et al., 2015)."}, {"heading": "3.4 QUESTION ANSWERING SYSTEMS", "text": "In the specific case of Task 1, we can use existing question-answering systems: Recently, interest has increased in systems that attempt to answer a question asked in natural language by transforming it into a database search using a knowledge base (Berant & Liang, 2014; Kwiatkowski et al., 2013; Fader et al., 2014), which is also a natural setup for our QA task. However, such systems cannot easily solve any of our other tasks, for example, our Task 2 recommendation does not include a factoid answer in a database. Nevertheless, this allows us to compare the performance of end-to-end systems in all of our tasks with a standard QA benchmark. As a starting point, we chose the method of Bordes et al. (2014) 10. This system learns embeddings that match questions with database entries, and then classifies the number of entries and shows that they perform well on the WEBQUESTIONS benchmark (Berant et al, 2013)."}, {"heading": "3.5 SINGULAR VALUE DECOMPOSITION", "text": "Singular Value Decomposition (SVD) is a standard benchmark for recommendations and forms the core of the best end results in the Netflix challenge, see Koren & Bell (2011) for a review, but it has been shown to be outperformed by other variants of matrix factorization, notably by using a ranking loss instead of a square loss (Weston et al., 2013) with which we will compare (see paragraph 3.2), as well as improvements such as SVD + + (Koren, 2008). Collaborative filtering methods are applicable to task 2, but cannot easily be used for any of the other tasks. Even for task 2, while our dialog models use text input, as shown in Table 2, SVD requires a user item matrix, so for this baseline we have pre-processed the text to assign an ID to each unit and discard all other texts. In contrast, the text-to-end dialog models must work as the end-to-learn task."}, {"heading": "3.6 INFORMATION RETRIEVAL MODELS", "text": "To select the candidates \"responses, a standard basis is the retrieval of neighboring information (IR) (Isbell et al., 2000; Jafarpour et al., 2010; Ritter et al., 2011; Sordoni et al., 2015). Often, two simple variants are tried: an input message will either (i) find the most similar message in the (training) data set and output the answer from that exchange; or (ii) find the most similar response to the input directly. In both cases, the default measure of similarity is tf-idf weighted cosmic similarity between the words. Note that the Supervised Embedding Models of Sec. 3.2 effectively implement the same type of model (ii), but with a learned similarity measurement. It has previously been shown that the method (ii) achieves better results (Ritter et al., 2011), and our first IR experiments showed the same result."}, {"heading": "4 RESULTS", "text": "eDi eeisrteeGsrsrrteeeteeteeteeteeteerrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "5 UBUNTU DIALOGUE CORPUS RESULTS", "text": "Since no other authors have published results on our new benchmark so far, to validate the quality of our results, we also apply our most powerful model under different conditions, comparing it on the Ubuntu Dialog Corpus (Lowe et al., 2015), which in particular allows us to compare more complex LSTM models trained discriminatively on the basis of metric learning, as well as additional basic methods, all of which have been trained by the authors. Ubuntu Dialog Corpus contains almost 1 million dialogs with more than 7 phrases (900k dialogs for training, 20k for validation and 20k for testing) and 100 million words. The corpus has been scratched from the Ubuntu IRC channel protocols, in which users ask questions about problems they have with Ubuntu, and receive answers from other users. Most chats can include more than two users, but a number of heuristics to unravel them into dynamical dialogues."}, {"heading": "6 CONCLUSION", "text": "The film dialogue dataset measures how well such models can achieve both objective and subjective goals, both in goal-oriented dialogue, thanks to evaluation metrics for answering questions and recommendation tasks and for less goal-oriented chitchat. A real end-to-end model should perform well in all of these tasks, as it is a necessary but not sufficient prerequisite for a fully functioning dialog agent.We have shown that some end-to-end neural network models can perform reasonably well in all tasks compared to standard baselines per task. In particular, storage networks that incorporate short- and long-term memory can use local contexts and factual knowledge to enhance performance. We believe that this is promising because we have shown that the same architectures can perform well in a separate dialog task, while the Ubuntu Dialog Corpus does not directly transfer these tasks to other areas, and it has been shown that they work well in advance synthetic tasks, but on West-demanding ones."}, {"heading": "A FURTHER EXPERIMENTAL DETAILS", "text": "This year, it is more than ever in the history of the city, where it is so far that it is a place, where it is a place, where it is a place, where it is a place."}, {"heading": "B OPTIMAL HYPER-PARAMETER VALUES", "text": "The most important hyperparameters are the embedding dimension d, the learning rate \u03bb, the number of dictionaries w, the number of hops K for MemNNs and the unfolding depth blen for LSTMs. All models are implemented in the torch library (see torch.ch). Problem 1 (QA) \u2022 QA-System of Bordes et al. (2014): \u03bb = 0.001, d = 50. \u2022 Supervised embedding model: \u03bb = 0.05, d = 1, K = 1. \u2022 LSTM: \u03bb = 0.001, d = 100, blen = 10.Task 2 (recommendation) \u2022 SVD: d = 50, d = 50. \u2022 Supervised embedding model: l = 0.005, d = 100."}, {"heading": "C FURTHER DETAILED RESULTS", "text": "C.1 RESULTS OF TASK 1 (QA) RESULTS OF QUESTION TYPEC.2 RESULTS OF TASK 3 (QA + RECOMMENDATION) RESULTS OF TASK 4 (REDDIT) RESULTS OF RESPONSE TYPEC.3 RESULTS OF TASK 4 (REDDIT) RESULTS OF RESPONSE TYPES"}], "references": [{"title": "Supervised semantic indexing", "author": ["Bai", "Bing", "Weston", "Jason", "Grangier", "David", "Collobert", "Ronan", "Sadamasa", "Kunihiko", "Qi", "Yanjun", "Chapelle", "Olivier", "Weinberger", "Kilian"], "venue": "In Proceedings of the 18th ACM conference on Information and knowledge management,", "citeRegEx": "Bai et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bai et al\\.", "year": 2009}, {"title": "Semantic parsing via paraphrasing", "author": ["Berant", "Jonathan", "Liang", "Percy"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL\u201914),", "citeRegEx": "Berant et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2014}, {"title": "Semantic parsing on freebase from question-answer pairs", "author": ["Berant", "Jonathan", "Chou", "Andrew", "Frostig", "Roy", "Liang", "Percy"], "venue": "In EMNLP, pp", "citeRegEx": "Berant et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Question answering with subgraph embeddings", "author": ["Bordes", "Antoine", "Chopra", "Sumit", "Weston", "Jason"], "venue": "In Proc. EMNLP,", "citeRegEx": "Bordes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Large-scale simple question answering with memory networks", "author": ["Bordes", "Antoine", "Usunier", "Nicolas", "Chopra", "Sumit", "Weston", "Jason"], "venue": "arXiv preprint arXiv:1506.02075,", "citeRegEx": "Bordes et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2015}, {"title": "Performance of recommender algorithms on top-n recommendation tasks", "author": ["Cremonesi", "Paolo", "Koren", "Yehuda", "Turrin", "Roberto"], "venue": "In Proceedings of the fourth ACM conference on Recommender systems,", "citeRegEx": "Cremonesi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cremonesi et al\\.", "year": 2010}, {"title": "Open question answering over curated and extracted knowledge bases", "author": ["Fader", "Anthony", "Zettlemoyer", "Luke", "Etzioni", "Oren"], "venue": "In Proceedings of 20th SIGKDD Conference on Knowledge Discovery and Data Mining (KDD\u201914),", "citeRegEx": "Fader et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2014}, {"title": "Supervised sequence labelling with recurrent neural networks, volume 385", "author": ["Graves", "Alex"], "venue": null, "citeRegEx": "Graves and Alex,? \\Q2012\\E", "shortCiteRegEx": "Graves and Alex", "year": 2012}, {"title": "A statistical approach to spoken dialog systems design and evaluation", "author": ["Griol", "David", "Hurtado", "Llu\u0131\u0301s F", "Segarra", "Encarna", "Sanchis", "Emilio"], "venue": "Speech Communication,", "citeRegEx": "Griol et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Griol et al\\.", "year": 2008}, {"title": "Machine learning for dialog state tracking: A review", "author": ["Henderson", "Matthew"], "venue": "In Proceedings of The First International Workshop on Machine Learning in Spoken Language Processing,", "citeRegEx": "Henderson and Matthew.,? \\Q2015\\E", "shortCiteRegEx": "Henderson and Matthew.", "year": 2015}, {"title": "The second dialog state tracking challenge", "author": ["Henderson", "Matthew", "Thomson", "Blaise", "Williams", "Jason"], "venue": "In 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue,", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Teaching machines to read and comprehend", "author": ["Hermann", "Karl Moritz", "Ko\u010disk\u00fd", "Tom\u00e1\u0161", "Grefenstette", "Edward", "Espeholt", "Lasse", "Kay", "Will", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Hill", "Felix", "Bordes", "Antoine", "Chopra", "Sumit", "Weston", "Jason"], "venue": "arXiv preprint arXiv:1511.02301,", "citeRegEx": "Hill et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Beyond class a: A proposal for automatic evaluation of discourse", "author": ["Hirschman", "Lynette", "Dahl", "Deborah A", "McKay", "Donald P", "Norton", "Lewis M", "Linebarger", "Marcia C"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Hirschman et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Hirschman et al\\.", "year": 1990}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Cobot in lambdamoo: A social statistics agent", "author": ["Isbell", "Charles Lee", "Kearns", "Michael", "Kormann", "Dave", "Singh", "Satinder", "Stone", "Peter"], "venue": "In AAAI/IAAI,", "citeRegEx": "Isbell et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Isbell et al\\.", "year": 2000}, {"title": "Filter, rank, and transfer the knowledge: Learning to chat", "author": ["Jafarpour", "Sina", "Burges", "Christopher JC", "Ritter", "Alan"], "venue": "Advances in Ranking,", "citeRegEx": "Jafarpour et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jafarpour et al\\.", "year": 2010}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Joulin", "Armand", "Mikolov", "Tomas"], "venue": "arXiv preprint:", "citeRegEx": "Joulin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2015}, {"title": "A survey on question answering technology from an information retrieval perspective", "author": ["Kolomiyets", "Oleksandr", "Moens", "Marie-Francine"], "venue": "Information Sciences,", "citeRegEx": "Kolomiyets et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kolomiyets et al\\.", "year": 2011}, {"title": "Factorization meets the neighborhood: a multifaceted collaborative filtering model", "author": ["Koren", "Yehuda"], "venue": "In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Koren and Yehuda.,? \\Q2008\\E", "shortCiteRegEx": "Koren and Yehuda.", "year": 2008}, {"title": "Advances in collaborative filtering", "author": ["Koren", "Yehuda", "Bell", "Robert"], "venue": "In Recommender systems handbook,", "citeRegEx": "Koren et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Koren et al\\.", "year": 2011}, {"title": "Scaling semantic parsers with on-the-fly ontology matching", "author": ["Kwiatkowski", "Tom", "Choi", "Eunsol", "Artzi", "Yoav", "Zettlemoyer", "Luke"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP\u201913),", "citeRegEx": "Kwiatkowski et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2013}, {"title": "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems", "author": ["Lowe", "Ryan", "Pow", "Nissan", "Serban", "Iulian", "Pineau", "Joelle"], "venue": "arXiv preprint arXiv:1506.08909,", "citeRegEx": "Lowe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lowe et al\\.", "year": 2015}, {"title": "Context dependent recurrent neural network language model", "author": ["Mikolov", "Tomas", "Zweig", "Geoffrey"], "venue": "In SLT,", "citeRegEx": "Mikolov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "Extensions of recurrent neural network language model", "author": ["Mikolov", "Tom\u00e1\u0161", "Kombrink", "Stefan", "Burget", "Luk\u00e1\u0161", "\u010cernock\u1ef3", "Jan Honza", "Khudanpur", "Sanjeev"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Language understanding for text-based games using deep reinforcement learning", "author": ["Narasimhan", "Karthik", "Kulkarni", "Tejas", "Barzilay", "Regina"], "venue": "arXiv preprint arXiv:1506.08941,", "citeRegEx": "Narasimhan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "An end-to-end dialog system for tv program discovery", "author": ["Ramachandran", "Deepak", "Yeh", "Peter Z", "Jarrold", "William", "Douglas", "Benjamin", "Ratnaparkhi", "Adwait", "Provine", "Ronald", "Mendel", "Jeremy", "Emfield", "Adam"], "venue": "In Spoken Language Technology Workshop (SLT), 2014 IEEE,", "citeRegEx": "Ramachandran et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ramachandran et al\\.", "year": 2014}, {"title": "Data-driven response generation in social media", "author": ["Ritter", "Alan", "Cherry", "Colin", "Dolan", "William B"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Ritter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "An end-to-end evaluation of two situated dialog systems", "author": ["Rojas-Barahona", "Lina M", "Lorenzo", "Alejandra", "Gardent", "Claire"], "venue": "In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue,", "citeRegEx": "Rojas.Barahona et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rojas.Barahona et al\\.", "year": 2012}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Rush", "Alexander M", "Chopra", "Sumit", "Weston", "Jason"], "venue": "Proceedings of EMNLP,", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Neural responding machine for short-text conversation", "author": ["Shang", "Lifeng", "Lu", "Zhengdong", "Li", "Hang"], "venue": "arXiv preprint arXiv:1503.02364,", "citeRegEx": "Shang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "A neural network approach to contextsensitive generation of conversational responses", "author": ["Sordoni", "Alessandro", "Galley", "Michel", "Auli", "Michael", "Brockett", "Chris", "Ji", "Yangfeng", "Mitchell", "Margaret", "Nie", "Jian-Yun", "Gao", "Jianfeng", "Dolan", "Bill"], "venue": "Proceedings of NAACL,", "citeRegEx": "Sordoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "End-to-end memory networks", "author": ["Sukhbaatar", "Sainbayar", "Szlam", "Arthur", "Weston", "Jason", "Fergus", "Rob"], "venue": "Proceedings of NIPS,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc VV"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A neural conversational model", "author": ["Vinyals", "Oriol", "Le", "Quoc"], "venue": "arXiv preprint arXiv:1506.05869,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Paradise: A framework for evaluating spoken dialogue agents", "author": ["Walker", "Marilyn A", "Litman", "Diane J", "Kamm", "Candace A", "Abella", "Alicia"], "venue": "In Proceedings of the eighth conference on European chapter of the Association for Computational Linguistics,", "citeRegEx": "Walker et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Walker et al\\.", "year": 1997}, {"title": "A trainable generator for recommendations in multimodal dialog", "author": ["Walker", "Marilyn A", "Prasad", "Rashmi", "Stent", "Amanda"], "venue": "In INTERSPEECH,", "citeRegEx": "Walker et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Walker et al\\.", "year": 2003}, {"title": "Towards AI-complete question answering: A set of prerequisite toy tasks", "author": ["J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov"], "venue": "arXiv preprint:", "citeRegEx": "Weston et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Learning to rank recommendations with the k-order statistic loss", "author": ["Weston", "Jason", "Yee", "Hector", "Weiss", "Ron J"], "venue": "In Proceedings of the 7th ACM conference on Recommender systems,", "citeRegEx": "Weston et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2013}, {"title": "Towards ai-complete question answering: a set of prerequisite toy tasks", "author": ["Weston", "Jason", "Bordes", "Antoine", "Chopra", "Sumit", "Mikolov", "Tomas"], "venue": "arXiv preprint arXiv:1502.05698,", "citeRegEx": "Weston et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Fish or fowl: A wizard of oz evaluation of dialogue strategies in the restaurant domain", "author": ["Whittaker", "Steve", "Walker", "Marilyn A", "Moore", "Johanna D"], "venue": "In LREC,", "citeRegEx": "Whittaker et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Whittaker et al\\.", "year": 2002}, {"title": "The dialog state tracking challenge", "author": ["Williams", "Jason", "Raux", "Antoine", "Ramachandran", "Deepak", "Black", "Alan"], "venue": "In Proceedings of the SIGDIAL 2013 Conference,", "citeRegEx": "Williams et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2013}, {"title": "embedded once and cached for the whole test set. This trick is not applicable to the method described above rendering it much slower. To deal with the speed issue one can use our supervised embedding model as a first step, and then only reranking the top 100 results with the LSTM to make it tractable, however performance is still poor as mentioned. We obtained improved results by instead adopting the approach", "author": ["Narasimhan"], "venue": null, "citeRegEx": "Narasimhan,? \\Q2015\\E", "shortCiteRegEx": "Narasimhan", "year": 2015}], "referenceMentions": [{"referenceID": 32, "context": "One recent popular approach is to train end-to-end models on a large amount of real dialog transcripts between humans (Sordoni et al., 2015; Vinyals & Le, 2015; Shang et al., 2015).", "startOffset": 118, "endOffset": 180}, {"referenceID": 31, "context": "One recent popular approach is to train end-to-end models on a large amount of real dialog transcripts between humans (Sordoni et al., 2015; Vinyals & Le, 2015; Shang et al., 2015).", "startOffset": 118, "endOffset": 180}, {"referenceID": 28, "context": "Such end-to-end dialog systems (Ritter et al., 2011; Shang et al., 2015; Vinyals & Le, 2015; Sordoni et al., 2015) directly generate a response given the last user utterance and (potentially) the context from previous dialog turns without relying on the intermediate use of a dialog state tracking component like in traditional dialog systems (e.", "startOffset": 31, "endOffset": 114}, {"referenceID": 31, "context": "Such end-to-end dialog systems (Ritter et al., 2011; Shang et al., 2015; Vinyals & Le, 2015; Sordoni et al., 2015) directly generate a response given the last user utterance and (potentially) the context from previous dialog turns without relying on the intermediate use of a dialog state tracking component like in traditional dialog systems (e.", "startOffset": 31, "endOffset": 114}, {"referenceID": 32, "context": "Such end-to-end dialog systems (Ritter et al., 2011; Shang et al., 2015; Vinyals & Le, 2015; Sordoni et al., 2015) directly generate a response given the last user utterance and (potentially) the context from previous dialog turns without relying on the intermediate use of a dialog state tracking component like in traditional dialog systems (e.", "startOffset": 31, "endOffset": 114}, {"referenceID": 36, "context": "Indeed, traditional systems have a wide range of well defined evaluation paradigms and benchmarks that measure their ability to track user states and/or to reach user-defined goals (Walker et al., 1997; Paek, 2001; Griol et al., 2008; Williams et al., 2013).", "startOffset": 181, "endOffset": 257}, {"referenceID": 8, "context": "Indeed, traditional systems have a wide range of well defined evaluation paradigms and benchmarks that measure their ability to track user states and/or to reach user-defined goals (Walker et al., 1997; Paek, 2001; Griol et al., 2008; Williams et al., 2013).", "startOffset": 181, "endOffset": 257}, {"referenceID": 42, "context": "Indeed, traditional systems have a wide range of well defined evaluation paradigms and benchmarks that measure their ability to track user states and/or to reach user-defined goals (Walker et al., 1997; Paek, 2001; Griol et al., 2008; Williams et al., 2013).", "startOffset": 181, "endOffset": 257}, {"referenceID": 28, "context": "Recent end-to-end models, on the other hand, rely either on very few human scores (Vinyals & Le, 2015), crowdsourcing (Ritter et al., 2011; Shang et al., 2015) or machine translation metrics like BLEU (Sordoni et al.", "startOffset": 118, "endOffset": 159}, {"referenceID": 31, "context": "Recent end-to-end models, on the other hand, rely either on very few human scores (Vinyals & Le, 2015), crowdsourcing (Ritter et al., 2011; Shang et al., 2015) or machine translation metrics like BLEU (Sordoni et al.", "startOffset": 118, "endOffset": 159}, {"referenceID": 32, "context": ", 2015) or machine translation metrics like BLEU (Sordoni et al., 2015) to judge the quality of the generated language only.", "startOffset": 49, "endOffset": 71}, {"referenceID": 27, "context": "Such end-to-end dialog systems (Ritter et al., 2011; Shang et al., 2015; Vinyals & Le, 2015; Sordoni et al., 2015) directly generate a response given the last user utterance and (potentially) the context from previous dialog turns without relying on the intermediate use of a dialog state tracking component like in traditional dialog systems (e.g. in Henderson (2015)).", "startOffset": 32, "endOffset": 369}, {"referenceID": 41, "context": "via Wizard-of-Oz strategies (Whittaker et al., 2002)).", "startOffset": 28, "endOffset": 52}, {"referenceID": 0, "context": "We evaluate on these tasks the performance of various neural network models that can potentially create end-to-end dialogs, ranging from simple supervised embedding models (Bai et al., 2009), RNNs with Long Short-Term Memory (LSTMs) (Hochreiter & Schmidhuber, 1997), and attention-based models, in particular Memory Networks (Sukhbaatar et al.", "startOffset": 172, "endOffset": 190}, {"referenceID": 33, "context": ", 2009), RNNs with Long Short-Term Memory (LSTMs) (Hochreiter & Schmidhuber, 1997), and attention-based models, in particular Memory Networks (Sukhbaatar et al., 2015).", "startOffset": 142, "endOffset": 167}, {"referenceID": 22, "context": "To validate the quality of our results, we also apply our best performing model, Memory Networks, in other conditions by comparing it on the Ubuntu Dialog Corpus (Lowe et al., 2015) against baselines trained by the authors of the corpus.", "startOffset": 162, "endOffset": 181}, {"referenceID": 4, "context": "Using SimpleQuestions, an existing open-domain question answering dataset based on Freebase (Bordes et al., 2015) we identified the subset of questions posed by those human annotators that covered our question types.", "startOffset": 92, "endOffset": 113}, {"referenceID": 5, "context": "Cremonesi et al. (2010).", "startOffset": 0, "endOffset": 24}, {"referenceID": 32, "context": "To evaluate the performance of models, we again separate the problem of evaluating the quality of a response from that of language generation by considering a ranking setup, in line with other recent works (Sordoni et al., 2015).", "startOffset": 206, "endOffset": 228}, {"referenceID": 36, "context": "Evaluation of the dialog state tracking stage is well defined since the PARADISE framework (Walker et al., 1997) and subsequent initiatives (Paek, 2001; Griol et al.", "startOffset": 91, "endOffset": 112}, {"referenceID": 8, "context": ", 1997) and subsequent initiatives (Paek, 2001; Griol et al., 2008), including recent competitons (Williams et al.", "startOffset": 35, "endOffset": 67}, {"referenceID": 42, "context": ", 2008), including recent competitons (Williams et al., 2013; Henderson et al., 2014) as well as situated variants (Rojas-Barahona et al.", "startOffset": 38, "endOffset": 85}, {"referenceID": 10, "context": ", 2008), including recent competitons (Williams et al., 2013; Henderson et al., 2014) as well as situated variants (Rojas-Barahona et al.", "startOffset": 38, "endOffset": 85}, {"referenceID": 29, "context": ", 2014) as well as situated variants (Rojas-Barahona et al., 2012).", "startOffset": 37, "endOffset": 66}, {"referenceID": 37, "context": "Because of language ambiguity and variation, evaluation of the response generation step is complicated and usually relies on human judgement (Walker et al., 2003).", "startOffset": 141, "endOffset": 162}, {"referenceID": 28, "context": "Most existing work (Ritter et al., 2011; Shang et al., 2015; Vinyals & Le, 2015; Sordoni et al., 2015) chose to use human ratings, which does not easily scale.", "startOffset": 19, "endOffset": 102}, {"referenceID": 31, "context": "Most existing work (Ritter et al., 2011; Shang et al., 2015; Vinyals & Le, 2015; Sordoni et al., 2015) chose to use human ratings, which does not easily scale.", "startOffset": 19, "endOffset": 102}, {"referenceID": 32, "context": "Most existing work (Ritter et al., 2011; Shang et al., 2015; Vinyals & Le, 2015; Sordoni et al., 2015) chose to use human ratings, which does not easily scale.", "startOffset": 19, "endOffset": 102}, {"referenceID": 8, "context": ", 1997) and subsequent initiatives (Paek, 2001; Griol et al., 2008), including recent competitons (Williams et al., 2013; Henderson et al., 2014) as well as situated variants (Rojas-Barahona et al., 2012). However, they require fine grained data annotations in terms of labeling internal dialog state and precisely defined user intent (goals). As a result, they do not really scale to large domains and dialogs with high variability in terms of language. Because of language ambiguity and variation, evaluation of the response generation step is complicated and usually relies on human judgement (Walker et al., 2003). End-to-end dialog systems do not rely on explicit internal state and hence do not have state tracking modules, they directly generate responses given user utterances and dialog context and hence can not be evaluated using state tracking test-beds. Unfortunately, as for response generator modules, their evaluation is ill-defined as it is difficult to objectively rate at scale the fit of returned responses. Most existing work (Ritter et al., 2011; Shang et al., 2015; Vinyals & Le, 2015; Sordoni et al., 2015) chose to use human ratings, which does not easily scale. Sordoni et al. (2015) also use the BLEU score to compare to actual user utterances but this is not a completely satisfying measure of success,", "startOffset": 48, "endOffset": 1210}, {"referenceID": 13, "context": "In that sense, it follows the the notion of dialog evaluation by a reference answer introduced in (Hirschman et al., 1990).", "startOffset": 98, "endOffset": 122}, {"referenceID": 21, "context": "Lowe et al. (2015) use a similar ranking evaluation to ours, but only in a chit-chat setting.", "startOffset": 0, "endOffset": 19}, {"referenceID": 21, "context": "Lowe et al. (2015) use a similar ranking evaluation to ours, but only in a chit-chat setting. Our approach of providing a collection of tasks to be jointly solved is related to the evaluation framework of the bAbI tasks (Weston et al., 2015a) and of the collection of sequence prediction tasks of Joulin & Mikolov (2015). However, unlike them, our Tasks 1-3 are much closer to real dialog, being built from human-written text, and with Task 4 actually involving real dialog from Reddit.", "startOffset": 0, "endOffset": 321}, {"referenceID": 13, "context": "In that sense, it follows the the notion of dialog evaluation by a reference answer introduced in (Hirschman et al., 1990). The application of movie recommender systems is connected to that of TV program suggestion proposed by Ramachandran et al. (2014), except that we frame it so that we can generate systematic evaluation from it, where they only rely on human judgement at small scale.", "startOffset": 99, "endOffset": 254}, {"referenceID": 33, "context": "Memory Networks (Weston et al., 2015c; Sukhbaatar et al., 2015) are a recent class of models that perform language understanding by incorporaring a memory component that potentially includes both long-term memory (e.", "startOffset": 16, "endOffset": 63}, {"referenceID": 4, "context": "They have only been evaluated in a few setups: question answering (Bordes et al., 2015), language modeling (Sukhbaatar et al.", "startOffset": 66, "endOffset": 87}, {"referenceID": 33, "context": ", 2015), language modeling (Sukhbaatar et al., 2015; Hill et al., 2015), and language understanding on the bAbI tasks (Weston et al.", "startOffset": 27, "endOffset": 71}, {"referenceID": 12, "context": ", 2015), language modeling (Sukhbaatar et al., 2015; Hill et al., 2015), and language understanding on the bAbI tasks (Weston et al.", "startOffset": 27, "endOffset": 71}, {"referenceID": 3, "context": "They have only been evaluated in a few setups: question answering (Bordes et al., 2015), language modeling (Sukhbaatar et al., 2015; Hill et al., 2015), and language understanding on the bAbI tasks (Weston et al., 2015a), but not so far on dialog tasks such as ours. We employ the MemN2N architecture of Sukhbaatar et al. (2015) in our experiments, with some additional modifications to construct both long-term and short-term context memories.", "startOffset": 67, "endOffset": 329}, {"referenceID": 33, "context": "We also add time features to each memory to denote their position following (Sukhbaatar et al., 2015).", "startOffset": 76, "endOffset": 101}, {"referenceID": 25, "context": "While one of the major uses of word embedding models is to learn unsupervised embeddings over large unlabeled datasets such as in Word2Vec (Mikolov et al., 2013) there are also very effective word embedding models for training supervised models when labeled data is available.", "startOffset": 139, "endOffset": 161}, {"referenceID": 0, "context": "For matching two documents supervised semantic indexing (SSI) was shown to be superior to unsupervised latent semantic indexing (LSI) (Bai et al., 2009).", "startOffset": 134, "endOffset": 152}, {"referenceID": 39, "context": "Similar methods were shown to outperform SVD for recommendation (Weston et al., 2013).", "startOffset": 64, "endOffset": 85}, {"referenceID": 3, "context": "However, we do not expect this method to work as well on question answering tasks, as all the memorization must occur in the individual word embeddings, which was shown to perform poorly in (Bordes et al., 2014).", "startOffset": 190, "endOffset": 211}, {"referenceID": 24, "context": "Recurrent Neural Networks (RNNs) have proven successful at several tasks involving natural language, language modeling (Mikolov et al., 2011), and have been applied recently to dialog (Sordoni et al.", "startOffset": 119, "endOffset": 141}, {"referenceID": 32, "context": ", 2011), and have been applied recently to dialog (Sordoni et al., 2015; Vinyals & Le, 2015; Shang et al., 2015).", "startOffset": 50, "endOffset": 112}, {"referenceID": 31, "context": ", 2011), and have been applied recently to dialog (Sordoni et al., 2015; Vinyals & Le, 2015; Shang et al., 2015).", "startOffset": 50, "endOffset": 112}, {"referenceID": 34, "context": ", 2012), seq2seq models (Sutskever et al., 2014), RNNs that take into account the document context (Mikolov & Zweig, 2012) and RNNs that perform attention over their input in various different ways (Bahdanau et al.", "startOffset": 24, "endOffset": 48}, {"referenceID": 11, "context": ", 2014), RNNs that take into account the document context (Mikolov & Zweig, 2012) and RNNs that perform attention over their input in various different ways (Bahdanau et al., 2015; Hermann et al., 2015; Rush et al., 2015).", "startOffset": 157, "endOffset": 221}, {"referenceID": 30, "context": ", 2014), RNNs that take into account the document context (Mikolov & Zweig, 2012) and RNNs that perform attention over their input in various different ways (Bahdanau et al., 2015; Hermann et al., 2015; Rush et al., 2015).", "startOffset": 157, "endOffset": 221}, {"referenceID": 21, "context": "There has been a recent surge in interest in such systems that try to answer a question posed in natural language by converting it into a database search over a knowledge base (Berant & Liang, 2014; Kwiatkowski et al., 2013; Fader et al., 2014), which is a setup natural for our QA task also.", "startOffset": 176, "endOffset": 244}, {"referenceID": 6, "context": "There has been a recent surge in interest in such systems that try to answer a question posed in natural language by converting it into a database search over a knowledge base (Berant & Liang, 2014; Kwiatkowski et al., 2013; Fader et al., 2014), which is a setup natural for our QA task also.", "startOffset": 176, "endOffset": 244}, {"referenceID": 2, "context": "This system learns embeddings that match questions to database entries, and then ranks the set of entries, and has been shown to achieve good performance on the WEBQUESTIONS benchmark (Berant et al., 2013).", "startOffset": 184, "endOffset": 205}, {"referenceID": 1, "context": "We chose the method of Bordes et al. (2014)10 as our baseline.", "startOffset": 23, "endOffset": 44}, {"referenceID": 39, "context": "However, it has been shown to be outperformed by other flavors of matrix factorization, in particular by using a ranking loss rather than squared loss (Weston et al., 2013) which we will compare to (cf.", "startOffset": 151, "endOffset": 172}, {"referenceID": 15, "context": "To select candidate responses a standard baseline is nearest neighbour information retrieval (IR) (Isbell et al., 2000; Jafarpour et al., 2010; Ritter et al., 2011; Sordoni et al., 2015).", "startOffset": 98, "endOffset": 186}, {"referenceID": 16, "context": "To select candidate responses a standard baseline is nearest neighbour information retrieval (IR) (Isbell et al., 2000; Jafarpour et al., 2010; Ritter et al., 2011; Sordoni et al., 2015).", "startOffset": 98, "endOffset": 186}, {"referenceID": 28, "context": "To select candidate responses a standard baseline is nearest neighbour information retrieval (IR) (Isbell et al., 2000; Jafarpour et al., 2010; Ritter et al., 2011; Sordoni et al., 2015).", "startOffset": 98, "endOffset": 186}, {"referenceID": 32, "context": "To select candidate responses a standard baseline is nearest neighbour information retrieval (IR) (Isbell et al., 2000; Jafarpour et al., 2010; Ritter et al., 2011; Sordoni et al., 2015).", "startOffset": 98, "endOffset": 186}, {"referenceID": 28, "context": "It has been shown previously that method (ii) performs better (Ritter et al., 2011), and our initial IR experiments showed the same result.", "startOffset": 62, "endOffset": 83}, {"referenceID": 36, "context": "Weston et al. (2015b). The We used the code available at: https://github.", "startOffset": 0, "endOffset": 22}, {"referenceID": 3, "context": "1 of Bordes et al. (2014).", "startOffset": 5, "endOffset": 26}, {"referenceID": 32, "context": "LSTMs perform poorly: the posts in Reddit are quite long and the memory of the LSTM is relatively short, as pointed out by Sordoni et al. (2015). In that work they employed a linear reranker that used LSTM prediction as features to better effect.", "startOffset": 123, "endOffset": 145}, {"referenceID": 22, "context": "As no other authors have yet published results on our new benchmark, to validate the quality of our results we also apply our best performing model in other conditions by comparing it on the Ubuntu Dialog Corpus (Lowe et al., 2015).", "startOffset": 212, "endOffset": 231}, {"referenceID": 22, "context": "Methods with \u2020 have been ran by Lowe et al. (2015).", "startOffset": 32, "endOffset": 51}], "year": 2016, "abstractText": "A long-term goal of machine learning is to build intelligent conversational agents. One recent popular approach is to train end-to-end models on a large amount of real dialog transcripts between humans (Sordoni et al., 2015; Vinyals & Le, 2015; Shang et al., 2015). However, this approach leaves many questions unanswered as an understanding of the precise successes and shortcomings of each model is hard to assess. A contrasting recent proposal are the bAbI tasks (Weston et al., 2015b) which are synthetic data that measure the ability of learning machines at various reasoning tasks over toy language. Unfortunately, those tests are very small and hence may encourage methods that do not scale. In this work, we propose a suite of new tasks of a much larger scale that attempt to bridge the gap between the two regimes. Choosing the domain of movies, we provide tasks that test the ability of models to answer factual questions (utilizing OMDB), provide personalization (utilizing MovieLens), carry short conversations about the two, and finally to perform on natural dialogs from Reddit. We provide a dataset covering \u223c75k movie entities and with \u223c3.5M training examples. We present results of various models on these tasks, and evaluate their performance.", "creator": "LaTeX with hyperref package"}}}