{"id": "1511.06939", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2015", "title": "Session-based Recommendations with Recurrent Neural Networks", "abstract": "We apply recurrent neural networks (RNN) on a new domain, namely recommendation system. Real-life recommender systems often face the problem of having to base recommendations only on short session-based data (e.g. a small sportsware website) instead of long user histories (as in the case of Netflix). In this situation the frequently praised matrix factorization approaches are not accurate. This problem is usually overcome in practice by resorting to item-to-item recommendations, i.e. recommending similar items. We argue that by modeling the whole session, more accurate recommendations can be provided. We therefore propose an RNN-based approach for session-based recommendations. Our approach also considers practical aspects of the task and introduces several modifications to classic RNNs such as a ranking loss function that make it more viable for this specific problem. Experimental results on two data-sets show marked improvements over widely used approaches.", "histories": [["v1", "Sat, 21 Nov 2015 23:42:59 GMT  (97kb,D)", "http://arxiv.org/abs/1511.06939v1", "Submitted for ICLR2016 Updated missing values from table 2 on November. 22. 0:40 (CET)"], ["v2", "Thu, 7 Jan 2016 21:13:50 GMT  (98kb,D)", "http://arxiv.org/abs/1511.06939v2", "Submitted for ICLR2016 Updated missing values from table 2 on November. 22. 0:40 (CET)"], ["v3", "Wed, 17 Feb 2016 16:41:37 GMT  (98kb,D)", "http://arxiv.org/abs/1511.06939v3", "Camera ready version (17th February, 2016)"], ["v4", "Tue, 29 Mar 2016 14:52:58 GMT  (98kb,D)", "http://arxiv.org/abs/1511.06939v4", "Camera ready version (17th February, 2016) Affiliation update (29th March, 2016)"]], "COMMENTS": "Submitted for ICLR2016 Updated missing values from table 2 on November. 22. 0:40 (CET)", "reviews": [], "SUBJECTS": "cs.LG cs.IR cs.NE", "authors": ["bal\\'azs hidasi", "alexandros karatzoglou", "linas baltrunas", "domonkos tikk"], "accepted": true, "id": "1511.06939"}, "pdf": {"name": "1511.06939.pdf", "metadata": {"source": "CRF", "title": "RECURRENT NEURAL NETWORKS", "authors": ["Bal\u00e1zs Hidasi", "Alexandros Karatzoglou", "Domonkos Tikk"], "emails": ["balazs.hidasi@gravityrd.com", "alexandros.karatzoglou@telefonica.com", "linas.baltrunas@telefonica.com", "domonkos.tikk@gravityrd.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Although it is a relatively unknown problem in machine learning and recipient systems, most e-commerce recipient systems (especially small retailers) and most news and media sites are generally unable to track the recommendations of users who visit their sites over a long period of time. While cookies and browser fingerprinting can provide some degree of visibility, these technologies are often not reliable enough and also raise privacy issues. Even if tracking is possible, many users have only one or two sessions on a smaller e-commerce site, and in certain domains (such as classified sites) the behavior of users shows session-based traits. Subsequent sessions of the same user should be handled independently. As a result, most session-based recommendation systems used on e-commerce are based on relatively simple methods that do not make use of a user profile."}, {"heading": "2 RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 SESSION-BASED RECOMMENDATION", "text": "Much of the work in the field of recommendation systems focuses on models that work when a user ID is available and a clear user profile can be created. In this setting, matrix factorization methods and neighborhood models dominate the literature and are also used online. One of the most important approaches used in session-based recommendations and a natural solution to the problem of a missing user profile is the item-to-item recommendation approach (Sarwar et al., 2001), (Linden et al., 2003) in this setting, an item-to-item similarity matrix is pre-calculated from the available session data, i.e. items that are frequently clicked together in sessions are considered to be similar. This similarity matrix is then simply used during the session to recommend the most similar items to the one that the user has currently clicked on. While this method has proven to be effective and is widely used, while these methods are effective, these methods are only used under the last click."}, {"heading": "2.2 DEEP LEARNING IN RECOMMENDERS", "text": "One of the first related methods in the literature on neural networks to use the Restricted Boltzmann Machines for Collaborative Filtering Salakhutdinov et al. (2007), in which an RBM is used to model user-item interactions and make recommendations, has proven to be one of the most powerful collaborative filter models. Deep models have been used to extract features from unstructured content such as music or images, which are then used in conjunction with more conventional collaborative filter models. Van den Oord et al. (2013) uses a conventional deep network to extract features from music files, which are then used in a factor model. More recently, Wang et al. (2015) have introduced a more general approach using a deep network to extract generic content features from arbitrary types of items, and these features are then integrated into a standardized filter model to improve the performance."}, {"heading": "3 RECOMMENDATIONS WITH RNNS", "text": "The main difference between RNNs and traditional feedback deep models is the presence of an internal hidden state in the units that make up the network. Standard RNNs update their hidden state h with the following update function: ht = g (Wxt + Uht \u2212 1) (1) Where g is a smooth, limited function, such as a logistic sigmoid function xt, is the input of the unit at a given time. An RNN gives a probability distribution over the next element of the sequence, considering its current state. A gated recurrent unit (GRU) Cho et al. (2014) is a more sophisticated model of an RNN unit aimed at dealing with the disappearing slope problems. GRU gates essentially learn when and by how much to update the hidden state of the unit. The activation of the GRU \u2212 is a linear model of an RNN unit that aims to deal with the disappearing slope problems. GRU gates learn when and by how much to update the hidden state of the unit. The activation of the WHU is given in the Whh \u2212 1 is a linear interpolation between the previous one (Whh = 1 activated during the Whh = 1)."}, {"heading": "3.1 CUSTOMIZING THE GRU MODEL", "text": "We used the GRU-based RNN in our models for session-based recommendations. The input of the network is the actual state of the session, while the output is the element of the next event in the session. The state of the session can be either the element of the actual event or the events in the session so far. In the first case, 1-of-N encoding is used, i.e. the length of the input vector corresponds to the number of elements and only the coordinate corresponding to the active element is one, the others are zeros. The latter setting uses a weighted sum of these representations, where events are discounted if they occurred earlier. To ensure stability, the input vector is then normalized. We also experimented with adding an additional embedding layer, but the 1-of-N encoding is getting better executed.The core of the network is the GRU layer (s) and additional feed layers can be added between the last layer and the output."}, {"heading": "3.1.1 SESSION-PARALLEL MINI-BATCHES", "text": "RNNs for natural language processing tasks usually use successive mini-stacks. For example, it is common to use a sliding window over the words of sentences and place these window fragments next to each other to form mini-stacks. This does not fit our task because (1) the length of sessions can be very different, even more so than sentences: some sessions consist of only 2 events, while others can stretch over a few hundred; (2) our goal is to capture how a session develops over time, so that splitting it into fragments would not make sense. Therefore, we use session-parallel mini-stacks. First, we create a sequence for the sessions. Then we use the first event of the first X sessions to form the input of the first mini-stack (the desired output is the second event of our active sessions). The second mini-stack is formed from the second event, and so on."}, {"heading": "3.1.2 SAMPLING ON THE OUTPUT", "text": "Recommender systems are particularly useful when the number of items is large. Even for a medium-sized webshop, this value is in the range of tens of thousands, but on larger websites it is not uncommon to have hundreds of thousands of items or even a few million. Calculating a score for each item at each step would result in the algorithm scale with the product based on the number of items and the number of events. In practice, this would be useless. Therefore, we have to scan the output and calculate only the score for a small subset of items. This also means that only a portion of the weights will be updated. In addition to the desired output, we have to calculate scores for some negative examples and modify the weights so that the desired output is ranked high. The natural interpretation of an arbitrary missing event is that the user did not know about the existence of the item and therefore there was no interaction. However, there is a low probability that the user will know the apparent item the more likely to interact with it than the other event, the more likely that the user knew it was the item and the more likely to be missing."}, {"heading": "3.1.3 RANKING LOSS", "text": "The core of recommendation systems is the relevance-based ranking of items. Although the task can also be interpreted as a classification task, learning-to-rank approaches Rendle et al. (2009); Shi et al. (2012); Steck (2015) are generally better than other approaches. Ranking can be meaningless, paired or list-wise. Pointwise Ranking estimates the score or rank of item points independently and the loss is defined in such a way that the rank of the relevant items should be low. Paired ranking compares the score or rank of the pairs of a positive and a negative item and the loss forces the rank of the positive item points to be lower than the negative. Listwise Ranking uses the scores and ranks of all items. NS and compares them with the perfect NS order. Since it includes sorting, it is usually more expensive computationally and therefore not often used. Even if there is only one relevant point - like - in our list."}, {"heading": "4 EXPERIMENTS", "text": "In fact, it is the case that most of us are able to outdo ourselves, \"he told the German Press Agency in an interview with\" Welt am Sonntag \":\" I do not believe that we will be able to survive the crisis. \"And further:\" I do not believe that we will be able to get a grip on the crisis. \"Also in the second half, in the second half, in the second half, in the second half, in the second half, in the third half, in the second half, in the second half, in the second half, in the third half, in the third half, in the third half, in the second half, in the third half, in the third half, in the third half, in the third half, in the second half, in the second half, in the second half, in the second half, in the second half, in the second half, in the second half, in the second half, in the second half, in the third half, in the fourth, in the fourth half, in the fourth half, in the third, in the third half, in the third half, in the third, in the third half of the year, in the third, in the third half of the third, in the third, in the third half of the year, in the third, in the third half of the third, in the third, in the third half of the year, in the third, in the third of the third, in the third of the third of the year, in the third, in the third of the third of the year, in the third of the third of the year, in the third of the third, in the third of the third of the year, in the third of the third of the year, in the third of the third of the third of the"}, {"heading": "4.1 BASELINES", "text": "We compare the proposed network with a number of commonly used baselines. \u2022 POP: Popularity predictor that always recommends the most popular items of the training set. Despite its simplicity, it is often a strong baseline in certain areas. \u2022 S-POP: This baseline recommends the most popular items of the current session. The recommendation list changes during the session as more items gain in events. Relationships are broken down based on global popularity values. This baseline is strong in areas with high repeatability. \u2022 Item-KNN: Items that are similar to the actual item are recommended by this baseline and similarity is defined as cosmic similarity between the vector of their sessions, i.e. it is the number of common occurrences of two items in sessions divided by the square root of the product of the number of sessions in which the individual items have occurred. Regularization is also defined as the cosmic similarity between the vector of each of the two seats divided by the number of their seats."}, {"heading": "4.2 PARAMETER & STRUCTURE OPTIMIZATION", "text": "Due to the relatively long evaluation cycles and the number of possible architectures, the full parameter optimization was not carried out. Also, the parameterization we found best for RSC15 was transferred directly to experiments with VIDEO. Thus, there may be parameterizations that lead to better results. After optimization, the minibatch size was set to 200, we set the probability of 0.3 and the learning rate to 0.05. Weight matrices were initialized by random numbers uniformly from [\u2212 x, x] where x depends on the number of rows and columns of the matrix. We experimented with both rmsprop (Dauphin et al., 2015) and adagrad (Duchi et al., 2011)."}, {"heading": "4.3 RESULTS", "text": "Table 2 shows the results of the most powerful networks. Results are compared with the best baseline (item-KNN). We show results with 100 and 1000 hidden units. The former is of practical importance as the complete training on the RSC15 data is completed within 4 hours (using 8 CPU cores), so it can be used in live systems as it can be retrained frequently. Training 1000 hidden units takes 10 hours and, although it can still be trained daily, can be slow in certain areas of application.On the RSC15 data, the GRU-based approach has significant advantages over item-KNN in both ratings, even if the number of units is 100. Increasing the number of units further improves results. It is interesting to see that the TOP1 loss with rmsprop performs better than with adagrad, while BPR with rmsprop was unstable. This is due to the built-in regulation of TOP1."}, {"heading": "5 CONCLUSION & FUTURE WORK", "text": "In this paper, we applied a kind of modern recurrent neural network (GRU) to new areas of application: recommendation systems. We chose the task of session-based recommendations because it is a practically important but not well-researched area. We modified the basic GRU to better meet the task by introducing session-parallel mini-batches, minibatch-based output samples and ranking loss function. We demonstrated that our method can significantly outperform popular baselines used for this task. We think that our work can be the basis of deep learning applications in recommendation systems as well as session-based recommendations in general. Our immediate future work will focus on a more thorough examination of the proposed network."}, {"heading": "ACKNOWLEDGMENTS", "text": "The work that led to these results was funded by the Seventh Framework Programme of the European Union (FP7 / 2007-2013) under CrowdRec Grant Agreement No 610594."}], "references": [{"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Cho", "Kyunghyun", "van Merri\u00ebnboer", "Bart", "Bahdanau", "Dzmitry", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.1259,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Rmsprop and equilibrated adaptive learning rates for non-convex optimization", "author": ["Dauphin", "Yann N", "de Vries", "Harm", "Chung", "Junyoung", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1502.04390,", "citeRegEx": "Dauphin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2015}, {"title": "The YouTube video recommendation system", "author": ["Davidson", "James", "Liebald", "Benjamin", "Liu", "Junning"], "venue": "In Recsys\u201910: ACM Conf. on Recommender Systems,", "citeRegEx": "Davidson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Davidson et al\\.", "year": 2010}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Fast ALS-based tensor factorization for context-aware recommendation from implicit feedback", "author": ["B. Hidasi", "D. Tikk"], "venue": "In ECML-PKDD\u201912, Part II,", "citeRegEx": "Hidasi and Tikk,? \\Q2012\\E", "shortCiteRegEx": "Hidasi and Tikk", "year": 2012}, {"title": "General factorization framework for context-aware recommendations", "author": ["Hidasi", "Bal\u00e1zs", "Tikk", "Domonkos"], "venue": "Data Mining and Knowledge Discovery, pp", "citeRegEx": "Hidasi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hidasi et al\\.", "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Hinton", "Geoffrey", "Deng", "Li", "Yu", "Dong", "Dahl", "George E", "Mohamed", "Abdel-rahman", "Jaitly", "Navdeep", "Senior", "Andrew", "Vanhoucke", "Vincent", "Nguyen", "Patrick", "Sainath", "Tara N"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Factorization meets the neighborhood: a multifaceted collaborative filtering model", "author": ["Y. Koren"], "venue": "ACM Int. Conf. on Knowledge Discovery and Data Mining, pp", "citeRegEx": "Koren,? \\Q2008\\E", "shortCiteRegEx": "Koren", "year": 2008}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Koren", "Yehuda", "Bell", "Robert", "Volinsky", "Chris"], "venue": null, "citeRegEx": "Koren et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Koren et al\\.", "year": 2009}, {"title": "Enlister: Baidu\u2019s recommender system for the biggest Chinese Q&A website", "author": ["Liu", "Qiwen", "Chen", "Tianjian", "Cai", "Jing", "Yu", "Dianhai"], "venue": "In RecSys-12: Proc. of the 6th ACM Conf. on Recommender Systems,", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "BPR: Bayesian personalized ranking from implicit feedback", "author": ["S. Rendle", "C. Freudenthaler", "Z. Gantner", "L. Schmidt-Thieme"], "venue": "In UAI\u201909: 25 Conf. on Uncertainty in Artificial Intelligence,", "citeRegEx": "Rendle et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Rendle et al\\.", "year": 2009}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Russakovsky", "Olga", "Deng", "Jia", "Su", "Hao", "Krause", "Jonathan", "Satheesh", "Sanjeev", "Ma", "Sean", "Huang", "Zhiheng", "Karpathy", "Andrej", "Khosla", "Aditya", "Bernstein", "Michael S", "Berg", "Alexander C", "Li", "Fei-Fei"], "venue": "CoRR, abs/1409.0575,", "citeRegEx": "Russakovsky et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2014}, {"title": "Restricted boltzmann machines for collaborative filtering", "author": ["Salakhutdinov", "Ruslan", "Mnih", "Andriy", "Hinton", "Geoffrey"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2007}, {"title": "Item-based collaborative filtering recommendation algorithms", "author": ["Sarwar", "Badrul", "Karypis", "George", "Konstan", "Joseph", "Riedl", "John"], "venue": "In Proceedings of the 10th international conference on World Wide Web,", "citeRegEx": "Sarwar et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Sarwar et al\\.", "year": 2001}, {"title": "An mdp-based recommender system", "author": ["Shani", "Guy", "Brafman", "Ronen I", "Heckerman", "David"], "venue": "In Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "Shani et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Shani et al\\.", "year": 2002}, {"title": "Climf: Learning to maximize reciprocal rank with collaborative less-is-more filtering", "author": ["Shi", "Yue", "Karatzoglou", "Alexandros", "Baltrunas", "Linas", "Larson", "Martha", "Oliver", "Nuria", "Hanjalic", "Alan"], "venue": "In Proceedings of the Sixth ACM Conference on Recommender Systems,", "citeRegEx": "Shi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2012}, {"title": "Gaussian ranking by matrix factorization", "author": ["Steck", "Harald"], "venue": "Proceedings of the 9th ACM Conference on Recommender Systems,", "citeRegEx": "Steck and Harald.,? \\Q2015\\E", "shortCiteRegEx": "Steck and Harald.", "year": 2015}, {"title": "Deep content-based music recommendation", "author": ["Van den Oord", "Aaron", "Dieleman", "Sander", "Schrauwen", "Benjamin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Oord et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2013}, {"title": "Collaborative deep learning for recommender systems", "author": ["Wang", "Hao", "Naiyan", "Yeung", "Dit-Yan"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Maximum margin matrix factorization for collaborative ranking", "author": ["Weimer", "Markus", "Karatzoglou", "Alexandros", "Le", "Quoc Viet", "Smola", "Alex"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Weimer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Weimer et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 7, "context": "The most common methods used in recommender systems are factor models Koren et al. (2009), Weimer et al.", "startOffset": 70, "endOffset": 90}, {"referenceID": 7, "context": "The most common methods used in recommender systems are factor models Koren et al. (2009), Weimer et al. (2007), Hidasi & Tikk (2012) and neighborhood methods Sarwar et al.", "startOffset": 70, "endOffset": 112}, {"referenceID": 7, "context": "The most common methods used in recommender systems are factor models Koren et al. (2009), Weimer et al. (2007), Hidasi & Tikk (2012) and neighborhood methods Sarwar et al.", "startOffset": 70, "endOffset": 134}, {"referenceID": 7, "context": "The most common methods used in recommender systems are factor models Koren et al. (2009), Weimer et al. (2007), Hidasi & Tikk (2012) and neighborhood methods Sarwar et al. (2001), Koren (2008).", "startOffset": 70, "endOffset": 180}, {"referenceID": 7, "context": "The most common methods used in recommender systems are factor models Koren et al. (2009), Weimer et al. (2007), Hidasi & Tikk (2012) and neighborhood methods Sarwar et al. (2001), Koren (2008). Factor models work by decomposing the sparse user-item interactions matrix to a set of d dimensional vectors one for each item and user in the dataset.", "startOffset": 70, "endOffset": 194}, {"referenceID": 11, "context": "The past few years have seen the tremendous success of deep neural networks in a number of tasks such as image and speech recognition (Russakovsky et al., 2014), (Hinton et al.", "startOffset": 134, "endOffset": 160}, {"referenceID": 6, "context": ", 2014), (Hinton et al., 2012) where unstructured data is processed through several convolutional and standard layers of (usually rectified linear) units.", "startOffset": 9, "endOffset": 30}, {"referenceID": 13, "context": "One of the main approaches that is employed in session-based recommendation and a natural solution to the problem of a missing user profile is the item-to-item recommendation approach (Sarwar et al., 2001),(Linden et al.", "startOffset": 184, "endOffset": 205}, {"referenceID": 14, "context": "A somewhat different approach to session-based recommendation are Markov Decision Processes (MDPs) (Shani et al., 2002).", "startOffset": 99, "endOffset": 119}, {"referenceID": 12, "context": "One of the first related methods in the neural networks literature where the use of Restricted Boltzmann Machines for Collaborative Filtering Salakhutdinov et al. (2007). In this work an RBM is used to model user-item interaction and perform recommendations.", "startOffset": 142, "endOffset": 170}, {"referenceID": 12, "context": "One of the first related methods in the neural networks literature where the use of Restricted Boltzmann Machines for Collaborative Filtering Salakhutdinov et al. (2007). In this work an RBM is used to model user-item interaction and perform recommendations. This model has been shown to be one of the best performing Collaborative Filtering models. Deep Models have been used to extract features from unstructured content such as music or images that are then used together with more conventional collaborative filtering models. In Van den Oord et al. (2013) a convolutional deep network is used to extract feature from music files that are then used in a factor model.", "startOffset": 142, "endOffset": 560}, {"referenceID": 12, "context": "One of the first related methods in the neural networks literature where the use of Restricted Boltzmann Machines for Collaborative Filtering Salakhutdinov et al. (2007). In this work an RBM is used to model user-item interaction and perform recommendations. This model has been shown to be one of the best performing Collaborative Filtering models. Deep Models have been used to extract features from unstructured content such as music or images that are then used together with more conventional collaborative filtering models. In Van den Oord et al. (2013) a convolutional deep network is used to extract feature from music files that are then used in a factor model. More recently Wang et al. (2015) introduce a more generic approach whereby a deep network is used to extract generic content-features from any types of items, these features are then incorporated in a standard collaborative filtering model to enhance the recommendation performance.", "startOffset": 142, "endOffset": 704}, {"referenceID": 0, "context": "A Gated Recurrent Unit (GRU) Cho et al. (2014) is a more elaborate model of an RNN unit that aims at dealing with the vanishing gradient problems.", "startOffset": 29, "endOffset": 47}, {"referenceID": 10, "context": "Although the task can also be interpreted as a classification task, learning-to-rank approaches Rendle et al. (2009); Shi et al.", "startOffset": 96, "endOffset": 117}, {"referenceID": 10, "context": "Although the task can also be interpreted as a classification task, learning-to-rank approaches Rendle et al. (2009); Shi et al. (2012); Steck (2015) generally outperform other approaches.", "startOffset": 96, "endOffset": 136}, {"referenceID": 10, "context": "Although the task can also be interpreted as a classification task, learning-to-rank approaches Rendle et al. (2009); Shi et al. (2012); Steck (2015) generally outperform other approaches.", "startOffset": 96, "endOffset": 150}, {"referenceID": 10, "context": "\u2022 BPR: Bayesian Personalized Ranking (Rendle et al., 2009) is a matrix factorization method that uses pairwise ranking loss.", "startOffset": 37, "endOffset": 58}, {"referenceID": 9, "context": "Recall also usually correlates well with important online KPIs, such as click-through rate (CTR)(Liu et al., 2012; Hidasi & Tikk, 2012).", "startOffset": 96, "endOffset": 135}, {"referenceID": 2, "context": "Despite of its simplicity it is usually a strong baseline (Linden et al., 2003; Davidson et al., 2010).", "startOffset": 58, "endOffset": 102}, {"referenceID": 10, "context": "\u2022 BPR-MF: BPR-MF (Rendle et al., 2009) is one of the commonly used matrix factorization methods.", "startOffset": 17, "endOffset": 38}, {"referenceID": 1, "context": "We experimented with both rmsprop(Dauphin et al., 2015) and adagrad(Duchi et al.", "startOffset": 33, "endOffset": 55}, {"referenceID": 3, "context": ", 2015) and adagrad(Duchi et al., 2011).", "startOffset": 19, "endOffset": 39}], "year": 2015, "abstractText": "We apply recurrent neural networks (RNN) on a new domain, namely recommendation system. Real-life recommender systems often face the problem of having to base recommendations only on short session-based data (e.g. a small sportsware website) instead of long user histories (as in the case of Netflix). In this situation the frequently praised matrix factorization approaches are not accurate. This problem is usually overcome in practice by resorting to item-to-item recommendations, i.e. recommending similar items. We argue that by modeling the whole session, more accurate recommendations can be provided. We therefore propose an RNN-based approach for session-based recommendations. Our approach also considers practical aspects of the task and introduces several modifications to classic RNNs such as a ranking loss function that make it more viable for this specific problem. Experimental results on two data-sets show marked improvements over widely used approaches.", "creator": "LaTeX with hyperref package"}}}