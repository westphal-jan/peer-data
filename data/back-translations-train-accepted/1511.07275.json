{"id": "1511.07275", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2015", "title": "Learning Simple Algorithms from Examples", "abstract": "We present an approach for learning simple algorithms such as copying, multi-digit addition and single digit multiplication directly from examples. Our framework consists of a set of interfaces, accessed by a controller. Typical interfaces are 1-D tapes or 2-D grids that hold the input and output data. For the controller, we explore a range of neural network-based models which vary in their ability to abstract the underlying algorithm from training instances and generalize to test examples with many thousands of digits. The controller is trained using $Q$-learning with several enhancements and we show that the bottleneck is in the capabilities of the controller rather than in the search incurred by $Q$-learning.", "histories": [["v1", "Mon, 23 Nov 2015 15:31:54 GMT  (1414kb,D)", "http://arxiv.org/abs/1511.07275v1", null], ["v2", "Tue, 24 Nov 2015 03:28:35 GMT  (1414kb,D)", "http://arxiv.org/abs/1511.07275v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["wojciech zaremba", "tomas mikolov", "armand joulin", "rob fergus"], "accepted": true, "id": "1511.07275"}, "pdf": {"name": "1511.07275.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Wojciech Zaremba", "Tomas Mikolov", "Armand Joulin", "Rob Fergus"], "emails": ["woj.zaremba@gmail.com", "tmikolov@fb.com", "ajoulin@fb.com", "robfergus@fb.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, it is so that most of them are able to move without being able to move, most of them are able to move, most of them are able to move, most of them are not able to move, most of them are able to move, most of them are not able to move, most of them are able to move, most of them are able to move, most of them are able to move, most of them are not able to move, most of them are not able to move, most of them are not able to move, most of them are not able to move."}, {"heading": "2 MODEL", "text": "This year it is more than ever before."}, {"heading": "3 TASKS", "text": "We look at six different tasks: copy, reverse, walk, multi-digit addition, 3-digit addition, and single-digit multiplication. The copy and reverse input interface is an input band, but an input grid for the others. All tasks use an output tape interface. Unless otherwise specified, all arithmetic operations include the base 10. Examples of the six tasks are shown in Fig. 2. Copy: This involves copying the symbols from the input tape to the output tape. Although simple, the model still needs to learn how to match the input and output symbols, as well as performing the correct action on the input tape. Vice versa: Here, the goal is to reverse a sequence of symbols on the input tape. We put a special character \"r\" to indicate the end of the sequence. The model must learn to move to the right several times until it meets the \"r\" symbol, and then move to the left side of the output tape."}, {"heading": "4 RELATED WORK", "text": "Many of them are different embodiments of the controller interface abstraction formalized in our model. Neural Turing Machine (NTM) [Graves et al. (2014)] uses a modified LSTM [Hochreiter & Schmidhuber (1997)] as controller and has three conclusions: sequential input, delayed output, and differentiable memory. The model is capable of learning simple algorithms, including copying and sorting. RNN [Joulin & Mikolov (2015) has an RNN controller and three interfaces: sequential input, stack memory, and sequential output. Learning simple binary patterns and regular expressions is demonstrated. A closely related work on this [Das et al. (1992), recently expanded in Neural DeQue et al."}, {"heading": "5 SUPERVISED EXPERIMENTS", "text": "To understand the behavior of our model and provide an upper limit of performance, we train our model in a controlled environment, i.e., where the soil truth measures are provided. Note that the controller has yet to learn what symbol to produce, but this can now be done purely with back propagation, since the actions are known. To facilitate comparisons of the difficulty between the tasks, we use a common measure of complexity that corresponds to the number of time steps required to solve each task (using the soil truth measures). For example, a reserve task that requires a sequence of length 10 requires 20 time steps (10 steps to move into the \"r\" and 10 steps back to start).The conversion factors between the sequence lengths and complexity are as follows: copy = 1; walk = 1; additional = 2; 3 line addition = 3 and single digit multiplication of the task, we train a separate model, we train a sequence starting with a separate sequence and 6%."}, {"heading": "6 Q-LEARNING", "text": "In the previous section, we assumed that the optimal controller actions were given during the training, which meant that only the output symbols Q = > E had to be predicted and these could be learned through back propagation. We now look at the setting in which the actions are also learned to test the true capabilities of the models to learn simple algorithms from pairs of input and output sequences. We use Q-Learning, a standard amplification learning algorithm, to learn a sequence of discrete actions that solve a problem. However, a Q function, the estimated sum of future rewards, will be updated during \u2020 interface improvement to enable both reading and writing on the same interface, would provide a mechanism for long-term learning, even with a feed-forward controller. But then, the same lack of generalization problems (encountered with more powerful controllers) becomes a problem. Let's leave the controller state to the autoji, then the autojord relationship."}, {"heading": "6.1 DYNAMIC DISCOUNT", "text": "The purpose of enhanced learning is to learn a policy that yields the highest sum of future rewards. Q-learning does it indirectly by learning a Q function. The optimal policy can be derived from this by recording argmax via Q (s), which makes it easier to predict. We define Q as our repair ametrization. Q (s) should be roughly within the range [0, 1], and it should correspond to the length of the episode, how close we are to V (s). Q could be defined multiplicatively as Q (s, a). Q (s, a) should be roughly within the range [0, 1], and it should correspond to how close we are to V (s)."}, {"heading": "6.4 REINFORCEMENT LEARNING EXPERIMENTS", "text": "Unless otherwise specified, the controller is a single-layer GRU model with 200 units, selected based on its average performance over the six tasks in the supervised environment to successfully detect errors (see Section 5). However, since the performance of reinforcement learning methods tends to be highly stochastical, we repeat each experiment 10 times with a different random seed size. Each model is trained using 3 x 107 characters, which take 4 hours. A model is considered to be successfully solved if it is able to give a perfect answer to 50 test cases, every 100 digits in length. The GRU model is used with a batch size of 20, a learning rate of \u03b1 = 0.1, using the same initialization as [Glorot & Bengio (2010)], but multiplied by 2. All tasks are used with the same curriculum in the supervised experiments."}, {"heading": "7 DISCUSSION", "text": "We have explored the ability of neural network models to learn algorithms for simple arithmetic operations, and through experiments with supervision and reinforcement learning, we have shown that they are capable of doing so, albeit with reservations. Q-Learning has proven to be just as good as the supervised case. However, disappointingly, we have not been able to find a single controller that could solve all the tasks. We found that for some tasks, generalization ability was sensitive to the controller's memory capacity: too little and it would not be able to solve more complex tasks that rely on the temporal transmission of the state; too much of the resulting model would exceed the length of the training sequences. Finding automatic methods to control the model capacity seems to be important for developing robust models for this type of learning problem."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Jason Weston, Marc'Aurelio Ranzato and Przemys\u0142aw Mazur for useful discussions and comments, and Christopher Olah for the LSTM characters used in the essay and accompanying video."}, {"heading": "APPENDIX A: DIFFERENT SOLUTIONS TO ADDITION TASK", "text": "When examining the models we learned in addition, we find that three different solutions have been found. Although they all give the correct answer, they differ in their actions via the input grid, as shown in Fig. 6."}, {"heading": "APPENDIX B:REWARD FREQUENCY VS REWARD RELIABILITY", "text": "For small words, the reward is more common, but less reliable, because the probability that the wrong sequence of actions will produce the right result is relatively high (and vice versa for larger words).When copying and reversing tasks, changing the size of the vocabulary merely changes the variety of symbols on the tape. However, in arithmetic operations, this results in a change in the base that affects the task in a more complex way. For example, adding to base 4 requires memorizing a two-digit addition of size 16 instead of 100 for base 10. Table 3 shows the mean training time as a function of the size of the vocabulary. Results suggest that a rare but reliable reward is preferable to frequent but loud rewards."}, {"heading": "APPENDIX C: REWARD STRUCTURE", "text": "Reward in reinforcement learning systems drives the learning process. In our setting, we control the rewards and decide when and how much reward to give. We now examine different types of rewards and their influence on the learning time of our system. Our vanilla setting gives a reward of 1 for each correct prediction and a reward of 0 for each false prediction. We refer to this setting as \"0 / 1 reward.\" In addition, we look at two other settings, both based on the probabilities of the correct prediction. Let us be the target symbol and pi = p (y = i), i = [0, 9] the probability of predicting labeling i. When setting \"discrete reward,\" we sort pi. This gives us a sequence on indices a1, a2,... a10, i.e. Pa1 \u2265 Pa2 \u00b7 \u00b7 \u00b7 Pa10] is the probability of predicting labeling ii."}], "references": [{"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "van Merrienboer", "Bart", "Gulcehre", "Caglar", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning context-free grammars: Capabilities and limitations of a recurrent neural network with an external stack memory", "author": ["Das", "Sreerupa", "Giles", "C Lee", "Sun", "Guo-Zheng"], "venue": "Proceedings of The Fourteenth Annual Conference of Cognitive Science Society,", "citeRegEx": "Das et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Das et al\\.", "year": 1992}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Genetic Algorithms in Search, Optimization and Machine Learning", "author": ["Goldberg", "David E"], "venue": "AddisonWesley Longman Publishing Co., Inc.,", "citeRegEx": "Goldberg and E.,? \\Q1989\\E", "shortCiteRegEx": "Goldberg and E.", "year": 1989}, {"title": "Neural turing machines", "author": ["Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Learning to transduce with unbounded memory", "author": ["Grefenstette", "Edward", "Hermann", "Karl Moritz", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "arXiv preprint arXiv:1506.02516,", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control and Artificial Intelligence", "author": ["Holland", "John H"], "venue": null, "citeRegEx": "Holland and H.,? \\Q1992\\E", "shortCiteRegEx": "Holland and H.", "year": 1992}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Joulin", "Armand", "Mikolov", "Tomas"], "venue": "arXiv preprint arXiv:1503.01007,", "citeRegEx": "Joulin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2015}, {"title": "Learning dependency-based compositional semantics", "author": ["Liang", "Percy", "Jordan", "Michael I", "Klein", "Dan"], "venue": "Computational Linguistics,", "citeRegEx": "Liang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2013}, {"title": "Playing atari with deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Graves", "Alex", "Antonoglou", "Ioannis", "Wierstra", "Daan", "Riedmiller", "Martin"], "venue": "arXiv preprint arXiv:1312.5602,", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Evolutionary program induction of binary machine code and its applications", "author": ["Nordin", "Peter"], "venue": "Krehl Munster,", "citeRegEx": "Nordin and Peter.,? \\Q1997\\E", "shortCiteRegEx": "Nordin and Peter.", "year": 1997}, {"title": "Optimal ordered problem solver", "author": ["Schmidhuber", "J\u00fcrgen"], "venue": "Machine Learning,", "citeRegEx": "Schmidhuber and J\u00fcrgen.,? \\Q2004\\E", "shortCiteRegEx": "Schmidhuber and J\u00fcrgen.", "year": 2004}, {"title": "A formal theory of inductive inference", "author": ["Solomonoff", "Ray J"], "venue": "Part I. Information and control,", "citeRegEx": "Solomonoff and J.,? \\Q1964\\E", "shortCiteRegEx": "Solomonoff and J.", "year": 1964}, {"title": "Weakly supervised memory networks", "author": ["Sukhbaatar", "Sainbayar", "Szlam", "Arthur", "Weston", "Jason", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1503.08895,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": "MIT press Cambridge,", "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Learning from Delayed Rewards", "author": ["Watkins", "Chris"], "venue": "PhD thesis, Cambrdige University,", "citeRegEx": "Watkins and Chris.,? \\Q1989\\E", "shortCiteRegEx": "Watkins and Chris.", "year": 1989}, {"title": "A representation scheme to perform program induction in a canonical genetic algorithm. In Parallel Problem Solving from NaturePPSN", "author": ["Wineberg", "Mark", "Oppacher", "Franz"], "venue": null, "citeRegEx": "Wineberg et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Wineberg et al\\.", "year": 1994}, {"title": "Reinforcement learning neural turing machines", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1505.00521,", "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "For the controller, we explore several recurrent neural network architectures: two different sizes of 1-layer LSTM [Hochreiter & Schmidhuber (1997)], a gated-recurrent unit (GRU)[Cho et al. (2014)] and a vanilla feed-forward network.", "startOffset": 179, "endOffset": 197}, {"referenceID": 3, "context": "The Neural Turing Machine (NTM) [Graves et al. (2014)] uses a modified LSTM [Hochreiter & Schmidhuber (1997)] as the controller, and has three inferences: sequential input, delayed output and a differentiable memory.", "startOffset": 33, "endOffset": 54}, {"referenceID": 3, "context": "The Neural Turing Machine (NTM) [Graves et al. (2014)] uses a modified LSTM [Hochreiter & Schmidhuber (1997)] as the controller, and has three inferences: sequential input, delayed output and a differentiable memory.", "startOffset": 33, "endOffset": 109}, {"referenceID": 3, "context": "The Neural Turing Machine (NTM) [Graves et al. (2014)] uses a modified LSTM [Hochreiter & Schmidhuber (1997)] as the controller, and has three inferences: sequential input, delayed output and a differentiable memory. The model is able to learn simple algorithms including copying and sorting. The Stack RNN [Joulin & Mikolov (2015)] has an RNN controller and three interfaces: sequential input, a stack memory and sequential output.", "startOffset": 33, "endOffset": 332}, {"referenceID": 1, "context": "A closely related work to this is [Das et al. (1992)], which was recently extended in the Neural DeQue [Grefenstette et al.", "startOffset": 35, "endOffset": 53}, {"referenceID": 1, "context": "A closely related work to this is [Das et al. (1992)], which was recently extended in the Neural DeQue [Grefenstette et al. (2015)] to use a list instead.", "startOffset": 35, "endOffset": 131}, {"referenceID": 1, "context": "A closely related work to this is [Das et al. (1992)], which was recently extended in the Neural DeQue [Grefenstette et al. (2015)] to use a list instead. End-to-End Memory Networks [Sukhbaatar et al. (2015)] use a feed-forward network as the controller and interfaces consisting of a soft-attention input, plus a delayed output (by a fixed number of \u201chops\u201d).", "startOffset": 35, "endOffset": 208}, {"referenceID": 1, "context": "A closely related work to this is [Das et al. (1992)], which was recently extended in the Neural DeQue [Grefenstette et al. (2015)] to use a list instead. End-to-End Memory Networks [Sukhbaatar et al. (2015)] use a feed-forward network as the controller and interfaces consisting of a soft-attention input, plus a delayed output (by a fixed number of \u201chops\u201d). The model is applied to simple Q&A tasks, some of which involve logical reasoning. In contrast, our model automatically determines when to produce output and uses more general interfaces. However, most of these approaches use continuous interfaces that permit training via backpropagation of gradients. Our approach differs in that it uses discrete interfaces thus is more challenging to train since as we must rely on reinforcement learning instead. A notable exception is the Reinforcement Learning Neural Turing Machine (RLNTM) [Zaremba & Sutskever (2015)] which is a version of the NTM with discrete interfaces.", "startOffset": 35, "endOffset": 919}, {"referenceID": 1, "context": "A closely related work to this is [Das et al. (1992)], which was recently extended in the Neural DeQue [Grefenstette et al. (2015)] to use a list instead. End-to-End Memory Networks [Sukhbaatar et al. (2015)] use a feed-forward network as the controller and interfaces consisting of a soft-attention input, plus a delayed output (by a fixed number of \u201chops\u201d). The model is applied to simple Q&A tasks, some of which involve logical reasoning. In contrast, our model automatically determines when to produce output and uses more general interfaces. However, most of these approaches use continuous interfaces that permit training via backpropagation of gradients. Our approach differs in that it uses discrete interfaces thus is more challenging to train since as we must rely on reinforcement learning instead. A notable exception is the Reinforcement Learning Neural Turing Machine (RLNTM) [Zaremba & Sutskever (2015)] which is a version of the NTM with discrete interfaces. The Stack-RNN [Joulin & Mikolov (2015)] also uses a discrete search procedure for its interfaces but it is unclear how this would scale to larger problems.", "startOffset": 35, "endOffset": 1015}, {"referenceID": 1, "context": "A closely related work to this is [Das et al. (1992)], which was recently extended in the Neural DeQue [Grefenstette et al. (2015)] to use a list instead. End-to-End Memory Networks [Sukhbaatar et al. (2015)] use a feed-forward network as the controller and interfaces consisting of a soft-attention input, plus a delayed output (by a fixed number of \u201chops\u201d). The model is applied to simple Q&A tasks, some of which involve logical reasoning. In contrast, our model automatically determines when to produce output and uses more general interfaces. However, most of these approaches use continuous interfaces that permit training via backpropagation of gradients. Our approach differs in that it uses discrete interfaces thus is more challenging to train since as we must rely on reinforcement learning instead. A notable exception is the Reinforcement Learning Neural Turing Machine (RLNTM) [Zaremba & Sutskever (2015)] which is a version of the NTM with discrete interfaces. The Stack-RNN [Joulin & Mikolov (2015)] also uses a discrete search procedure for its interfaces but it is unclear how this would scale to larger problems. The problem of learning algorithms has its origins in the field of program induction [Nordin (1997); Liang et al.", "startOffset": 35, "endOffset": 1232}, {"referenceID": 1, "context": "A closely related work to this is [Das et al. (1992)], which was recently extended in the Neural DeQue [Grefenstette et al. (2015)] to use a list instead. End-to-End Memory Networks [Sukhbaatar et al. (2015)] use a feed-forward network as the controller and interfaces consisting of a soft-attention input, plus a delayed output (by a fixed number of \u201chops\u201d). The model is applied to simple Q&A tasks, some of which involve logical reasoning. In contrast, our model automatically determines when to produce output and uses more general interfaces. However, most of these approaches use continuous interfaces that permit training via backpropagation of gradients. Our approach differs in that it uses discrete interfaces thus is more challenging to train since as we must rely on reinforcement learning instead. A notable exception is the Reinforcement Learning Neural Turing Machine (RLNTM) [Zaremba & Sutskever (2015)] which is a version of the NTM with discrete interfaces. The Stack-RNN [Joulin & Mikolov (2015)] also uses a discrete search procedure for its interfaces but it is unclear how this would scale to larger problems. The problem of learning algorithms has its origins in the field of program induction [Nordin (1997); Liang et al. (2013); Wineberg & Oppacher (1994); Solomonoff (1964)].", "startOffset": 35, "endOffset": 1253}, {"referenceID": 1, "context": "A closely related work to this is [Das et al. (1992)], which was recently extended in the Neural DeQue [Grefenstette et al. (2015)] to use a list instead. End-to-End Memory Networks [Sukhbaatar et al. (2015)] use a feed-forward network as the controller and interfaces consisting of a soft-attention input, plus a delayed output (by a fixed number of \u201chops\u201d). The model is applied to simple Q&A tasks, some of which involve logical reasoning. In contrast, our model automatically determines when to produce output and uses more general interfaces. However, most of these approaches use continuous interfaces that permit training via backpropagation of gradients. Our approach differs in that it uses discrete interfaces thus is more challenging to train since as we must rely on reinforcement learning instead. A notable exception is the Reinforcement Learning Neural Turing Machine (RLNTM) [Zaremba & Sutskever (2015)] which is a version of the NTM with discrete interfaces. The Stack-RNN [Joulin & Mikolov (2015)] also uses a discrete search procedure for its interfaces but it is unclear how this would scale to larger problems. The problem of learning algorithms has its origins in the field of program induction [Nordin (1997); Liang et al. (2013); Wineberg & Oppacher (1994); Solomonoff (1964)].", "startOffset": 35, "endOffset": 1281}, {"referenceID": 1, "context": "A closely related work to this is [Das et al. (1992)], which was recently extended in the Neural DeQue [Grefenstette et al. (2015)] to use a list instead. End-to-End Memory Networks [Sukhbaatar et al. (2015)] use a feed-forward network as the controller and interfaces consisting of a soft-attention input, plus a delayed output (by a fixed number of \u201chops\u201d). The model is applied to simple Q&A tasks, some of which involve logical reasoning. In contrast, our model automatically determines when to produce output and uses more general interfaces. However, most of these approaches use continuous interfaces that permit training via backpropagation of gradients. Our approach differs in that it uses discrete interfaces thus is more challenging to train since as we must rely on reinforcement learning instead. A notable exception is the Reinforcement Learning Neural Turing Machine (RLNTM) [Zaremba & Sutskever (2015)] which is a version of the NTM with discrete interfaces. The Stack-RNN [Joulin & Mikolov (2015)] also uses a discrete search procedure for its interfaces but it is unclear how this would scale to larger problems. The problem of learning algorithms has its origins in the field of program induction [Nordin (1997); Liang et al. (2013); Wineberg & Oppacher (1994); Solomonoff (1964)].", "startOffset": 35, "endOffset": 1300}, {"referenceID": 1, "context": "A closely related work to this is [Das et al. (1992)], which was recently extended in the Neural DeQue [Grefenstette et al. (2015)] to use a list instead. End-to-End Memory Networks [Sukhbaatar et al. (2015)] use a feed-forward network as the controller and interfaces consisting of a soft-attention input, plus a delayed output (by a fixed number of \u201chops\u201d). The model is applied to simple Q&A tasks, some of which involve logical reasoning. In contrast, our model automatically determines when to produce output and uses more general interfaces. However, most of these approaches use continuous interfaces that permit training via backpropagation of gradients. Our approach differs in that it uses discrete interfaces thus is more challenging to train since as we must rely on reinforcement learning instead. A notable exception is the Reinforcement Learning Neural Turing Machine (RLNTM) [Zaremba & Sutskever (2015)] which is a version of the NTM with discrete interfaces. The Stack-RNN [Joulin & Mikolov (2015)] also uses a discrete search procedure for its interfaces but it is unclear how this would scale to larger problems. The problem of learning algorithms has its origins in the field of program induction [Nordin (1997); Liang et al. (2013); Wineberg & Oppacher (1994); Solomonoff (1964)]. In this domain, the model has to infer the source code of a program that solves a given problem. This is a similar goal to ours, but in quite a different setting. I.e. we do produce a computer program, but rather a neural net that can operate with interfaces such as tapes, so implements the program without being humanreadable. A more relevant work is [Schmidhuber (2004)] which learns an algorithms for the Hanoi tower problem, using a simple form of program induction and incremental learning components.", "startOffset": 35, "endOffset": 1675}, {"referenceID": 1, "context": "A closely related work to this is [Das et al. (1992)], which was recently extended in the Neural DeQue [Grefenstette et al. (2015)] to use a list instead. End-to-End Memory Networks [Sukhbaatar et al. (2015)] use a feed-forward network as the controller and interfaces consisting of a soft-attention input, plus a delayed output (by a fixed number of \u201chops\u201d). The model is applied to simple Q&A tasks, some of which involve logical reasoning. In contrast, our model automatically determines when to produce output and uses more general interfaces. However, most of these approaches use continuous interfaces that permit training via backpropagation of gradients. Our approach differs in that it uses discrete interfaces thus is more challenging to train since as we must rely on reinforcement learning instead. A notable exception is the Reinforcement Learning Neural Turing Machine (RLNTM) [Zaremba & Sutskever (2015)] which is a version of the NTM with discrete interfaces. The Stack-RNN [Joulin & Mikolov (2015)] also uses a discrete search procedure for its interfaces but it is unclear how this would scale to larger problems. The problem of learning algorithms has its origins in the field of program induction [Nordin (1997); Liang et al. (2013); Wineberg & Oppacher (1994); Solomonoff (1964)]. In this domain, the model has to infer the source code of a program that solves a given problem. This is a similar goal to ours, but in quite a different setting. I.e. we do produce a computer program, but rather a neural net that can operate with interfaces such as tapes, so implements the program without being humanreadable. A more relevant work is [Schmidhuber (2004)] which learns an algorithms for the Hanoi tower problem, using a simple form of program induction and incremental learning components. Genetic algorithms [Holland (1992); Goldberg (1989)] also can be considered a form of program induction, but are based on a random search strategy rather than a learned one.", "startOffset": 35, "endOffset": 1845}, {"referenceID": 1, "context": "A closely related work to this is [Das et al. (1992)], which was recently extended in the Neural DeQue [Grefenstette et al. (2015)] to use a list instead. End-to-End Memory Networks [Sukhbaatar et al. (2015)] use a feed-forward network as the controller and interfaces consisting of a soft-attention input, plus a delayed output (by a fixed number of \u201chops\u201d). The model is applied to simple Q&A tasks, some of which involve logical reasoning. In contrast, our model automatically determines when to produce output and uses more general interfaces. However, most of these approaches use continuous interfaces that permit training via backpropagation of gradients. Our approach differs in that it uses discrete interfaces thus is more challenging to train since as we must rely on reinforcement learning instead. A notable exception is the Reinforcement Learning Neural Turing Machine (RLNTM) [Zaremba & Sutskever (2015)] which is a version of the NTM with discrete interfaces. The Stack-RNN [Joulin & Mikolov (2015)] also uses a discrete search procedure for its interfaces but it is unclear how this would scale to larger problems. The problem of learning algorithms has its origins in the field of program induction [Nordin (1997); Liang et al. (2013); Wineberg & Oppacher (1994); Solomonoff (1964)]. In this domain, the model has to infer the source code of a program that solves a given problem. This is a similar goal to ours, but in quite a different setting. I.e. we do produce a computer program, but rather a neural net that can operate with interfaces such as tapes, so implements the program without being humanreadable. A more relevant work is [Schmidhuber (2004)] which learns an algorithms for the Hanoi tower problem, using a simple form of program induction and incremental learning components. Genetic algorithms [Holland (1992); Goldberg (1989)] also can be considered a form of program induction, but are based on a random search strategy rather than a learned one.", "startOffset": 35, "endOffset": 1862}, {"referenceID": 1, "context": "A closely related work to this is [Das et al. (1992)], which was recently extended in the Neural DeQue [Grefenstette et al. (2015)] to use a list instead. End-to-End Memory Networks [Sukhbaatar et al. (2015)] use a feed-forward network as the controller and interfaces consisting of a soft-attention input, plus a delayed output (by a fixed number of \u201chops\u201d). The model is applied to simple Q&A tasks, some of which involve logical reasoning. In contrast, our model automatically determines when to produce output and uses more general interfaces. However, most of these approaches use continuous interfaces that permit training via backpropagation of gradients. Our approach differs in that it uses discrete interfaces thus is more challenging to train since as we must rely on reinforcement learning instead. A notable exception is the Reinforcement Learning Neural Turing Machine (RLNTM) [Zaremba & Sutskever (2015)] which is a version of the NTM with discrete interfaces. The Stack-RNN [Joulin & Mikolov (2015)] also uses a discrete search procedure for its interfaces but it is unclear how this would scale to larger problems. The problem of learning algorithms has its origins in the field of program induction [Nordin (1997); Liang et al. (2013); Wineberg & Oppacher (1994); Solomonoff (1964)]. In this domain, the model has to infer the source code of a program that solves a given problem. This is a similar goal to ours, but in quite a different setting. I.e. we do produce a computer program, but rather a neural net that can operate with interfaces such as tapes, so implements the program without being humanreadable. A more relevant work is [Schmidhuber (2004)] which learns an algorithms for the Hanoi tower problem, using a simple form of program induction and incremental learning components. Genetic algorithms [Holland (1992); Goldberg (1989)] also can be considered a form of program induction, but are based on a random search strategy rather than a learned one. Similar to [Mnih et al. (2013)], we train the controller to approximate the Q-function.", "startOffset": 35, "endOffset": 2015}, {"referenceID": 1, "context": "A closely related work to this is [Das et al. (1992)], which was recently extended in the Neural DeQue [Grefenstette et al. (2015)] to use a list instead. End-to-End Memory Networks [Sukhbaatar et al. (2015)] use a feed-forward network as the controller and interfaces consisting of a soft-attention input, plus a delayed output (by a fixed number of \u201chops\u201d). The model is applied to simple Q&A tasks, some of which involve logical reasoning. In contrast, our model automatically determines when to produce output and uses more general interfaces. However, most of these approaches use continuous interfaces that permit training via backpropagation of gradients. Our approach differs in that it uses discrete interfaces thus is more challenging to train since as we must rely on reinforcement learning instead. A notable exception is the Reinforcement Learning Neural Turing Machine (RLNTM) [Zaremba & Sutskever (2015)] which is a version of the NTM with discrete interfaces. The Stack-RNN [Joulin & Mikolov (2015)] also uses a discrete search procedure for its interfaces but it is unclear how this would scale to larger problems. The problem of learning algorithms has its origins in the field of program induction [Nordin (1997); Liang et al. (2013); Wineberg & Oppacher (1994); Solomonoff (1964)]. In this domain, the model has to infer the source code of a program that solves a given problem. This is a similar goal to ours, but in quite a different setting. I.e. we do produce a computer program, but rather a neural net that can operate with interfaces such as tapes, so implements the program without being humanreadable. A more relevant work is [Schmidhuber (2004)] which learns an algorithms for the Hanoi tower problem, using a simple form of program induction and incremental learning components. Genetic algorithms [Holland (1992); Goldberg (1989)] also can be considered a form of program induction, but are based on a random search strategy rather than a learned one. Similar to [Mnih et al. (2013)], we train the controller to approximate the Q-function. However, we introduce several modifications on top of the classical Q-learning. First, we use Watkins Q(\u03bb) [Watkins (1989); Sutton & Barto (1998)].", "startOffset": 35, "endOffset": 2195}, {"referenceID": 1, "context": "A closely related work to this is [Das et al. (1992)], which was recently extended in the Neural DeQue [Grefenstette et al. (2015)] to use a list instead. End-to-End Memory Networks [Sukhbaatar et al. (2015)] use a feed-forward network as the controller and interfaces consisting of a soft-attention input, plus a delayed output (by a fixed number of \u201chops\u201d). The model is applied to simple Q&A tasks, some of which involve logical reasoning. In contrast, our model automatically determines when to produce output and uses more general interfaces. However, most of these approaches use continuous interfaces that permit training via backpropagation of gradients. Our approach differs in that it uses discrete interfaces thus is more challenging to train since as we must rely on reinforcement learning instead. A notable exception is the Reinforcement Learning Neural Turing Machine (RLNTM) [Zaremba & Sutskever (2015)] which is a version of the NTM with discrete interfaces. The Stack-RNN [Joulin & Mikolov (2015)] also uses a discrete search procedure for its interfaces but it is unclear how this would scale to larger problems. The problem of learning algorithms has its origins in the field of program induction [Nordin (1997); Liang et al. (2013); Wineberg & Oppacher (1994); Solomonoff (1964)]. In this domain, the model has to infer the source code of a program that solves a given problem. This is a similar goal to ours, but in quite a different setting. I.e. we do produce a computer program, but rather a neural net that can operate with interfaces such as tapes, so implements the program without being humanreadable. A more relevant work is [Schmidhuber (2004)] which learns an algorithms for the Hanoi tower problem, using a simple form of program induction and incremental learning components. Genetic algorithms [Holland (1992); Goldberg (1989)] also can be considered a form of program induction, but are based on a random search strategy rather than a learned one. Similar to [Mnih et al. (2013)], we train the controller to approximate the Q-function. However, we introduce several modifications on top of the classical Q-learning. First, we use Watkins Q(\u03bb) [Watkins (1989); Sutton & Barto (1998)].", "startOffset": 35, "endOffset": 2218}, {"referenceID": 1, "context": "A closely related work to this is [Das et al. (1992)], which was recently extended in the Neural DeQue [Grefenstette et al. (2015)] to use a list instead. End-to-End Memory Networks [Sukhbaatar et al. (2015)] use a feed-forward network as the controller and interfaces consisting of a soft-attention input, plus a delayed output (by a fixed number of \u201chops\u201d). The model is applied to simple Q&A tasks, some of which involve logical reasoning. In contrast, our model automatically determines when to produce output and uses more general interfaces. However, most of these approaches use continuous interfaces that permit training via backpropagation of gradients. Our approach differs in that it uses discrete interfaces thus is more challenging to train since as we must rely on reinforcement learning instead. A notable exception is the Reinforcement Learning Neural Turing Machine (RLNTM) [Zaremba & Sutskever (2015)] which is a version of the NTM with discrete interfaces. The Stack-RNN [Joulin & Mikolov (2015)] also uses a discrete search procedure for its interfaces but it is unclear how this would scale to larger problems. The problem of learning algorithms has its origins in the field of program induction [Nordin (1997); Liang et al. (2013); Wineberg & Oppacher (1994); Solomonoff (1964)]. In this domain, the model has to infer the source code of a program that solves a given problem. This is a similar goal to ours, but in quite a different setting. I.e. we do produce a computer program, but rather a neural net that can operate with interfaces such as tapes, so implements the program without being humanreadable. A more relevant work is [Schmidhuber (2004)] which learns an algorithms for the Hanoi tower problem, using a simple form of program induction and incremental learning components. Genetic algorithms [Holland (1992); Goldberg (1989)] also can be considered a form of program induction, but are based on a random search strategy rather than a learned one. Similar to [Mnih et al. (2013)], we train the controller to approximate the Q-function. However, we introduce several modifications on top of the classical Q-learning. First, we use Watkins Q(\u03bb) [Watkins (1989); Sutton & Barto (1998)]. This helps to overcome a non-stationary environment. We are unaware of any prior work that uses Watkins Q(\u03bb) for this purpose. Second, we reparametrized Q function, to become invariant to the sequence length. Finally, we penalize ||Q(s, \u2022)||, which might help to remove positive bias [Hasselt (2010)].", "startOffset": 35, "endOffset": 2520}, {"referenceID": 4, "context": "We note that this argument is empirically supported by our results in Table 2, as well as related work such as [Graves et al. (2014)] and [Joulin & Mikolov (2015)] which found limited capacity controllers to be most effective.", "startOffset": 112, "endOffset": 133}, {"referenceID": 4, "context": "We note that this argument is empirically supported by our results in Table 2, as well as related work such as [Graves et al. (2014)] and [Joulin & Mikolov (2015)] which found limited capacity controllers to be most effective.", "startOffset": 112, "endOffset": 163}], "year": 2017, "abstractText": "We present an approach for learning simple algorithms such as copying, multidigit addition and single digit multiplication directly from examples. Our framework consists of a set of interfaces, accessed by a controller. Typical interfaces are 1-D tapes or 2-D grids that hold the input and output data. For the controller, we explore a range of neural network-based models which vary in their ability to abstract the underlying algorithm from training instances and generalize to test examples with many thousands of digits. The controller is trained using Q-learning with several enhancements and we show that the bottleneck is in the capabilities of the controller rather than in the search incurred by Q-learning.", "creator": "LaTeX with hyperref package"}}}