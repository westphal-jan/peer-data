{"id": "1511.07289", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2015", "title": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)", "abstract": "We introduce the \"exponential linear unit\" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parameterized ReLUs (PReLUs), ELUs also avoid a vanishing gradient via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero. Zero means speed up learning because they bring the gradient closer to the unit natural gradient. We show that the unit natural gradient differs from the normal gradient by a bias shift term, which is proportional to the mean activation of incoming units. Like batch normalization, ELUs push the mean towards zero, but with a significantly smaller computational footprint. While other activation functions like LReLUs and PReLUs also have negative values, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. Consequently, dependencies between ELUs are much easier to model and distinct concepts are less likely to interfere. We found that ELUs lead not only to faster learning, but also to better generalization performance once networks have many layers (&gt;= 5). Using ELUs, we obtained the best published single-crop result on CIFAR-100 and CIFAR-10. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with similar classification performance, obtaining less than 10% classification error for a single crop, single model network.", "histories": [["v1", "Mon, 23 Nov 2015 15:58:05 GMT  (458kb,D)", "http://arxiv.org/abs/1511.07289v1", "14 pages, incl. supplement"], ["v2", "Thu, 3 Dec 2015 16:19:05 GMT  (612kb,D)", "http://arxiv.org/abs/1511.07289v2", "14 pages, incl. supplement"], ["v3", "Mon, 11 Jan 2016 17:55:53 GMT  (697kb,D)", "http://arxiv.org/abs/1511.07289v3", "14 pages, incl. supplement"], ["v4", "Mon, 15 Feb 2016 17:29:21 GMT  (697kb,D)", "http://arxiv.org/abs/1511.07289v4", "Published as a conference paper at ICLR 2016"], ["v5", "Mon, 22 Feb 2016 07:02:58 GMT  (697kb,D)", "http://arxiv.org/abs/1511.07289v5", "Published as a conference paper at ICLR 2016"]], "COMMENTS": "14 pages, incl. supplement", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["djork-arn\\'e clevert", "thomas unterthiner", "sepp hochreiter"], "accepted": true, "id": "1511.07289"}, "pdf": {"name": "1511.07289.pdf", "metadata": {"source": "CRF", "title": "EXPONENTIAL LINEAR UNITS (ELUS)", "authors": ["Djork-Arn\u00e9 Clevert", "Thomas Unterthiner"], "emails": ["okko@bioinf.jku.at", "unterthiner@bioinf.jku.at", "hochreit@bioinf.jku.at"], "sections": [{"heading": null, "text": "We introduce the \"exponential linear unit\" (ELU), which accelerates learning in deep neural networks and leads to higher classification accuracies. Like the corrected linear units (ReLUs), the leaky ReLUs (LReLUs) and the parameterized ReLUs (PReLUs), ELUs also avoid a vanishing gradient over identity for positive values. However, ELUs have improved learning characteristics compared to units with other activation functions. In contrast to the ReLUs, ELUs have negative values, which allow them to push average unit activations closer to zero. Zero means accelerating learning, because they bring the gradient closer to the natural gradient of the unit. We show that the natural gradient of the unit differs from the normal gradient by a bias shift term, which is proportional to the mean activation of the incoming units. Like the batch normalization, ELUs push the mean value of the unit down to zero, but we have lower pressure in the direction of the networks with a lower ELUs activation function, while the ELUs ensure that the ELUs have a distinct negative function."}, {"heading": "1 INTRODUCTION", "text": "In fact, it is so that most people are able to survive themselves, and that they are able to survive themselves. (...) In fact, it is so that they are able to survive themselves. (...) In fact, it is so that they are able to survive themselves. (...) It is so that they are able to survive themselves. (...) In fact, it is so that they are able to survive themselves. (...) In fact, it is so that they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves."}, {"heading": "2 ZERO MEAN ACTIVATIONS SPEED UP LEARNING", "text": "The natural gradient unit is an update direction that corrects the gradient direction with the information matrix. (1996); The natural gradient method is based on information geometry Amari (1985); Amari & Murata (1993) and became very popular for independent component analysis Amari et al. (1997); Choi et al. (1998); Amari et al. (1999); The recently introduced Hessian-Free Optimization techniques Martens & Sutskever (2011); Chapelle & Erhan (2011); Kiros (2013) and the Krylov Subspace Descent Methods Mi."}, {"heading": "3 EXPONENTIAL LINEAR UNITS (ELUS)", "text": "We introduce the exponential linear unit (ELU) with 0 < \u03b1: f (x) = {x if x \u2265 0 \u03b1 (x) \u2212 1) if x < 0, (16) f \u2032 (x) = {1 if x \u2265 0 f (x) + \u03b1 if x < 0. (17) The ELU hyperparameter \u03b1 controls the value up to which an ELU is saturated for negative net inputs (see Fig. 1). ELUs avoid the problem of the disappearing gradient as rectified linear units (ReLUs) and leaky ReLUs (LReLUs). The gradient does not disappear because the positive part of these functions is the identity, hence their derivation is one. Therefore, these activation functions are well suited for deep neural networks with many layers where a disappearing negative gradient impedes learning. (LLLLLLU et al al al al al al al al al al al al al al al al al).In contrast to LUs, the ELUs have negative values derived from the ELUs."}, {"heading": "4 EXPERIMENTS USING ELUS", "text": "In this section, we evaluate the performance of exponential linear units (ELUs) when used for unattended and supervised learning of deep autoencoders and deep revolutionary networks by comparing them with (i) Rectified Linear Units (ReLUs) and (ii) Leaky Linear Units (LReLUs). For comparisons, we use the following benchmark data sets to compare ELUs with other units: (i) MNIST (gray images in 10 classes, 60k training and 10k test images), (ii) CIFAR-10 (color images in 10 categories, 50k training and 10k test images), (iii) CIFAR-100 (color images in 100 categories, 50k training and 10k test images) and (iv) ImageNet (color images in 1000 categories, 1.3 M training and 100k test images)."}, {"heading": "4.1 MNIST", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1.1 LEARNING BEHAVIOR", "text": "First, we found that ELUs kept the mean activation closer to zero by classifying fully connected deep neural networks on the MNIST dataset of digits and tracking the activation of each hidden unit during training, for ELUs, ReLUs, and leaky ReLU units with a negative slope of 0.1. Each network had five hidden layers of 256 units each, and was trained by stochastic gradient descent, with a learning rate of 0.01 using size 64 mini-batches for 50 epochs. ELU hyperparameter was set to 1.0 for all experiments, weights were initialized after him and al. (2015), which was specifically designed to accelerate learning with ReLU units. After each epoch, we calculated the average activations of units on a fixed subset of training data."}, {"heading": "4.2 COMPARISON OF ACTIVATION FUNCTIONS", "text": "In this subsection, we conducted a benchmark experiment using a relatively simple deep Convolutionary Network Architecture to compare ELUs with ReLUs and LReLUs based on their learning behaviors.The model used in our baseline experiment CIFAR-100 consists of 11 Convolutionary Layers arranged in stacks of {1 \u00d7 192 \u00d7 5, 2 \u00d7 240 \u00d7 3, 2 \u00d7 260 \u00d7 2, 2 \u00d7 280 \u00d7 300 \u00d7 2, 1 \u00d7 300 \u00d7 1, 1 \u00d7 100 \u00d7 1} layers. 2 \u00d7 2 max-pooling with a stride of 2 was applied after each stack. For network regulation, we used the following drop-out rate for the last layer of each stack {0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.0}. The L2-weight decay regularization term is set to 0.0005."}, {"heading": "4.3 CLASSIFICATION PERFORMANCE ON CIFAR-100 AND CIFAR-10", "text": "Once it has been shown that ELUs actually exhibit the superior learning behavior postulated in Section 3, the following experiments should emphasize their generalization abilities. Therefore, the networks in this section have a more complex architecture than the networks in the previous subsection. The architecture of the Deep Convolution Network consists of 18 Convolutionary Layers \u00d7 Units \u00d7 Receptive Fields stacked in stacks of {1 x 384 x 3, 3 x 640 x 3, 4 x 768 x 3, 3 x 896 x 3, 3 x 1024 x 3, 3 x 1152 x 3, 1 x 100 x 1} layers. 2 x 2 max pooling with a step of 2 was applied after each stack. For network regulation, we used the following drop-out rate for all layers in a stack {0.0, 0.15, 0.3, 0.45, 0.6, 0.8, 0.0}. The L2-weight Decay Regularization Term was set to 0.0005 and the initial learning rate was set to 0.10."}, {"heading": "4.3.1 IMAGENET CHALLENGE DATASET", "text": "In the final experiment, we examined ELU networks on the 1000-class ImageNet dataset, which contained approximately 1.3 million training color images as well as additional 50k images and 100k images for validation and testing. For this task, we designed a 15-layer deep convective network, arranged in stacks of {1 \u00d7 96 \u00d7 6, 3 \u00d7 512 \u00d7 3, 5 \u00d7 768 \u00d7 3, 3 \u00d7 1024 \u00d7 3, 2 \u00d7 4096 \u00d7 FC, 1 \u00d7 1000 \u00d7 FC} layers \u00d7 units of receptive fields or fully bonded (FC). 2 \u00d7 2 max pooling with a strip of 2 was applied after each stack and spatial pyramid pooling (SPP) with 3 layers before the first FC layer."}, {"heading": "5 CONCLUSION", "text": "In this paper, we introduced the novel exponential linear units, units with nonlinear activation for deep learning. These units have negative values that allow the network to bring the mean activation of its units closer to zero. This helps bridge the gap between the normal gradient and the natural gradient of the unit, thereby accelerating learning. We believe that this property is also the reason for the success of activation functions such as LReLUs and PReLUs. In contrast to these methods, ELUs have a clear saturation plateau in their negative regime, which allows them to learn a more robust and stable representation. Experimental results show that ELUs outperform other activation functions in many vision datasets, achieving one of the ten best reported results on CIFAR-10 and setting a new state of the art in CIFAR-100 without the need for multiple evaluation or modelling. In addition, ELU networks have achieved competitive image results in CIFAR-100."}, {"heading": "B QUADRATIC FORM OF MEAN AND INVERSE SECOND MOMENT", "text": "Lemma 2. For a random variable, a HoldsE (a) TE \u2212 1 (a aT) E (a) \u2264 1 (38) and (1 \u2212 E (a) TE \u2212 1 (a aT) E (a) \u2212 1 = 1 + E (a) TVar \u2212 1 (a) E (a). (39) Proof. The Sherman-Morrison theorem states (A + b cT) \u2212 1 = A \u2212 1 \u2212 1 b cT A \u2212 11 + cTA \u2212 1b (41) = cTA \u2212 1b (1 + cTA \u2212 1b) \u2212 (cTA \u2212 1 b) (2 + b cT) 1 b = cTA \u2212 1b \u2212 1 b (41) = cTA \u2212 1b (1 + cTA \u2212 1b) \u2212 1b (41) = cTA \u2212 (cTA \u2212 1 b) \u2212 (1 b) \u2212 (cTA \u2212 1b) \u2212 (cTA \u2212 1 b) \u2212 (cTA \u2212 1 b) \u2212 (cTA \u2212 1 b) \u2212 (cTA \u2212 1 b) (a) (a) ar (a) (a) (a) 2 1 (T) (TV1 (38 a T a) T (T (a) T (T (a a) T (T (a a) T (a) and (T (T (1) T (a (T (a) T (a (1) T (a (T (a) T (a (1) T (a (T (a) T (a (a) T (1 (T (a) T (a (a) T (a (a) T (T (a (a) T (a (a) T (a (a) T (a (a (T) T (a (1) T (a (T)."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "We introduce the \u201cexponential linear unit\u201d (ELU) which speeds up learning in<lb>deep neural networks and leads to higher classification accuracies. Like rectified<lb>linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PRe-<lb>LUs), ELUs also avoid a vanishing gradient via the identity for positive values.<lb>However ELUs have improved learning characteristics compared to the units with<lb>other activation functions. In contrast to ReLUs, ELUs have negative values which<lb>allows them to push mean unit activations closer to zero. Zero means speed up<lb>learning because they bring the gradient closer to the unit natural gradient. We<lb>show that the unit natural gradient differs from the normal gradient by a bias shift<lb>term, which is proportional to the mean activation of incoming units. Like batch<lb>normalization, ELUs push the mean towards zero, but with a significantly smaller<lb>computational footprint. While other activation functions like LReLUs and PRe-<lb>LUs also have negative values, they do not ensure a noise-robust deactivation state.<lb>ELUs saturate to a negative value with smaller inputs and thereby decrease the<lb>propagated variation and information. Therefore ELUs code the degree of pres-<lb>ence of particular phenomena in the input, while they do not quantitatively model<lb>the degree of their absence. Consequently, dependencies between ELUs are much<lb>easier to model and distinct concepts are less likely to interfere.<lb>We found that ELUs lead not only to faster learning, but also to better general-<lb>ization performance once networks have many layers (\u2265 5). ELU networks were<lb>among top 10 reported CIFAR-10 results and yielded the best published result on<lb>CIFAR-100, without resorting to multi-view evaluation or model averaging. On<lb>ImageNet, ELU networks considerably speed up learning compared to a ReLU<lb>network with the same architecture, obtaining less than 10% classification error<lb>for a single crop, single model network.", "creator": "LaTeX with hyperref package"}}}