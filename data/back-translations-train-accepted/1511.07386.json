{"id": "1511.07386", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2015", "title": "Pushing the Boundaries of Boundary Detection using Deep Learning", "abstract": "In this work we show that Deep Convolutional Neural Networks can outperform humans on the task of boundary detection, as measured on the standard Berkeley Segmentation Dataset. Our detector is fully integrated in the popular Caffe framework and processes a 320x420 image in less than a second.", "histories": [["v1", "Mon, 23 Nov 2015 19:54:09 GMT  (6946kb,D)", "http://arxiv.org/abs/1511.07386v1", null], ["v2", "Fri, 22 Jan 2016 15:31:32 GMT  (6945kb,D)", "http://arxiv.org/abs/1511.07386v2", "The previous version reported large improvements w.r.t. the LPO region proposal baseline, which turned out to be due to a wrong computation for the baseline. The improvements are currently less important, and are omitted. We are sorry if the reported results caused any confusion. We have also integrated reviewer feedback regarding human performance on the BSD benchmark"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["iasonas kokkinos"], "accepted": true, "id": "1511.07386"}, "pdf": {"name": "1511.07386.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["SURPASSING HUMANS", "DEEP LEARNING", "Iasonas Kokkinos"], "emails": ["iasonas.kokkinos@ecp.fr"], "sections": [{"heading": null, "text": "In this paper, we demonstrate that Deep Convolutional Neural Networks can outperform humans in the task of boundary detection as measured by the standard Berkeley segmentation dataset. Our detector is fully integrated into the popular Caffe framework and processes a 320x420 image in less than a second. First, our contributions consist of combining a careful design of loss for boundary detection training, a multi-resolution architecture, and training with external data to improve the detection accuracy of the current state of the art - from an optimal dataset scale F-measure of 0.780 to 0.808 - while human performance is 0.803. We continue to improve performance to 0.813 by combining deep learning with grouping and integrating normalized cuts technology into a deep network."}, {"heading": "1 INTRODUCTION", "text": "Over the past three years, we have failed to address the problem. (1998) We have achieved convincing results in high-level tasks such as image classification (Krizhevsky et al., 2013; Sermanet et al., 2013; Simonyan & Zisserman, 2014; Papandreou et al., 2014; Chen et al., 2015) or object recognition (Girshick et al., 2014). Recent work has also shown that DCNNs can equally be applied to pixel-level labeling tasks, including semantic segmentation (Long et al., 2014; Chen et al., 2014)."}, {"heading": "2 HED AND DSN TRAINING", "text": "We start with a brief description of the Holistic Edge Detection (HED) (HED) work of Xie & Tu (2015) as a starting point for our work. HED uses \"Deep Supervised Network\" (DSN) (Lee et al., 2015) training to fine-tune the VGG network for the boundary mapping task presented in Figure 2. The principle behind DSN can be loosely understood as classifying stacking that is adapted to deep learning and proves to be very successful in practice: when a multi-layered architecture is optimized for a given task, one can expect better results by informing each layer of the final objective, rather than relying on the last layer to systematically improve the precursors, both in generic recognition tasks (Lee et al., 2015) and in the context of boundary mapping (Xie Tu, 2015)."}, {"heading": "3 IMPROVED DEEP BOUNDARY DETECTION TRAINING", "text": "After outlining the HED framework, we now turn to our contributions, which consist of (i) Multiple Instance Learning for boundary detection (ii) Graduated Deep Supervision (iii) Multi-Scale Training and the introduction of external data.The improvements resulting from these contributions are summarized in Table 1, where we present our ODS and OIS-based F measures on the BSD test kit alongside average precision (AP).We compare our own HED-like baseline, which delivers a performance slightly below that of the original HED system from Xie & Tu (2015); the latest system from Xie & Tu (2015) has an improved Fmeasure of F = 0.79 due to additional dataset augmentation that we have not yet implemented."}, {"heading": "3.1 DEALING WITH ANNOTATION INCONSISTENCIES", "text": "The first of our contributions is aimed at dealing with the inconsistency of human annotations in BSD, as illustrated in Fig. 4. As can be seen, even if the two notes agree on semantics, they cannot show the boundaries in a common place, making it difficult to explicitly manipulate \"positive\" and \"negative\" learning patterns in the vincinity of boundaries.This problem has already been acknowledged in literature, for example Sironi et al. (2015) turning boundary tracking into a regression problem by explicitly manipulating soil truth to become smoother - but at the cost of localization accuracy. In Xie & Tu (2015), a heuristic one that was only used to view a pixel as positive when it is consistently annotated. However, it is unclear why other pixels should be labeled as negatively. Our approach builds on coconut (2010a), where there are multiple assumptions."}, {"heading": "3.2 GRADUATED DSN TRAINING", "text": "The two terms in the objective function of HED, Equation 5: L (W, w, h) = Lside (W, w) + Lfuse (W, w, h) (9) play a complementary role: The first lateral terms force the interlayers to discriminate and also extract some preliminary classification information; the second, merger-layered terms calibrate the meaning of the intermediate classifications supplied by the sublayers. As discussed in Lee et al. (2015), DSN can be understood as simplifying the associated learning problem in terms of optimization, but once the network parameters are in the right regime, we can discard all the simplifications needed to get there. This is a strategy used in the classic method of graduated convexity without convexity (Blake & Zisserman, 1987), and here we show that it also helps improve the DSN when applied to limit detection."}, {"heading": "3.3 MULTI-RESOLUTION ARCHITECTURE", "text": "The authors of the HED used \"Deep Supervised Network\" (DSN) (Lee et al., 2015) training to refine the VGG network for the task of boundary detection, illustrated in Fig. 5. Image boundaries, however, are in multiple image resolutions (Witkin et al., 1983) and it has repeatedly been shown that merging information from multiple resolutions improves boundary detection, e.g. in Dolla \ufffd r & Zitnick (2015); Arbelaez et al. (2011). Although the authors of the HED use information from multiple scales by merging the results of many layers, detection of multi-resolution boundaries can still be helpful. We first observed that simply averaging the results of the network applied to differently scaled versions of the image significantly improves performance. We then turned to a more precise method to achieve multi-resolution detection: We looked at a DSN-like multi-resolution architecture with weights attached to it, which means dividing the weights with each other."}, {"heading": "3.4 TRAINING WITH EXTERNAL DATA", "text": "The authors of Xie & Tu (2015) originally used 32 geometric transformations (16 twists and reflections) of the 300 images used in the BSD trainvalid set, resulting in a total of approximately 10,000 training images - in a recent version, the authors assume that two additional transformations are taken into account, resulting in approximately 30,000 training images and an increase in performance from F = 0.78 to F = 0.79. We did not use these additional scales in our experiments due to time constraints, but considered the use of boundaries from the VOC context dataset (Mottaghi et al., 2014), where all objects and \"stuff\" in the scene are manually segmented. Our only modification to these boundaries was to label the interiors of houses as \"indifferent,\" as any windows, doors or balconies that were approximately overlooked by these judges are considered \"absolutely legitimate.\""}, {"heading": "4 USING GROUPING IN A DEEP ARCHITECTURE", "text": "The techniques mentioned turn out to be insufficient because they are able to fix the mentioned bugs and because they are not able to fix the mentioned bugs."}, {"heading": "5 SYNERGIES WITH HIGHER-LEVEL TASKS", "text": "Having raised the performance of boundary detection to a good level, we are now turning to use it in conjunction with other higher-level tasks. We are looking at semantic segmentation and object suggestion creation, and seeing significant improvements over the current leading approaches."}, {"heading": "5.1 SEMANTIC SEGMENTATION", "text": "Since our model is completely revolutionary, we can easily combine it with the recent series of papers on FCNN-based semantic segmentation (Long et al., 2014; Chen et al., 2015; Papandreou et al., 2015a; Zheng et al., 2015), which have yielded excellent results, in particular the use of the Dense Conditional Random Field (DenseCRF) by Kra \ufffd henbu \ufffd hl & Koltun (2011) by Chen et al. (2015); Papandreou et al. (2015a); Zheng et al. (2015), has increased the discriminatory power of FCNNs with local evidence gathered by image intensity. (2015) we define the CRF distribution as: P (x) = 1Z exp (\u2212 E), E (x), E (x) = 2)."}, {"heading": "5.2 COMBINATION WITH OBJECT PROPOSAL GENERATION", "text": "Generating regions of interest has been established as a standardized pre-processing step for object detection; a major step toward the widespread adoption of such methods has been the successful inclusion of Selective Search (Uijlings et al., 2013) in the work of Girshick et al. (2014), making it the dominant paradigm for current object detection. Therefore, generating well-placed suggestions is critical for the later success of object detection. To evaluate the merits of our boundary detector, we integrated it into the Learning to Propose Objects (LPO) system from Kra \ufffd henbu \ufffd hl & Koltun (2015), which is currently leading in the creation of object detection suggestions in the region. The original LPO system uses the multi-scale Structured Forest Border Detector from Dolla \ufffd r & Zitnick (2015), which has an F measure of 0.75; it is therefore interesting to compare the overall performance of our detector to the PO."}, {"heading": "6 CONCLUSION", "text": "Our system is fully integrated into the Caffe framework and operates in less than a second per frame. We expect further improvements to be achieved by working together on other low cues such as symmetry (Tsogkas & Kokkinos, 2012) or surface orientation and depth (Eigen & Fergus, 2014). We also intend to further explore the benefits of our detector in relation to high-level tasks such as object detection and detection."}, {"heading": "7 ACKNOWLEDGEMENTS", "text": "I thank the authors of Xie & Tu (2015) for the inspiration, Alp Guler for the illustrations and tables, Kostas Papazafeiropoulos for helping port Damascus to Caffe, George Papandreou for the tutorial on Caffe and Pierre-Andre 'Savalle for providing prototype files like a professional seduser."}, {"heading": "8 SUPPLEMENTAL MATERIAL", "text": "Below you will find qualitative results of images from the Pascal VOC test."}], "references": [{"title": "Fast high-dimensional filtering using the permutohedral lattice", "author": ["Adams", "Andrew", "Baek", "Jongmin", "Davis", "Myers Abraham"], "venue": "In Computer Graphics Forum,", "citeRegEx": "Adams et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Adams et al\\.", "year": 2010}, {"title": "Contour detection and hierarchical image segmentation", "author": ["Arbelaez", "Pablo", "Maire", "Michael", "Fowlkes", "Charless", "Malik", "Jitendra"], "venue": null, "citeRegEx": "Arbelaez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Arbelaez et al\\.", "year": 2011}, {"title": "Laplacian eigenmaps and spectral techniques for embedding and clustering", "author": ["Belkin", "Mikhail", "Niyogi", "Partha"], "venue": "In NIPS,", "citeRegEx": "Belkin et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Belkin et al\\.", "year": 2001}, {"title": "Deepedge: A multi-scale bifurcated deep network for top-down contour detection", "author": ["Bertasius", "Gedas", "Shi", "Jianbo", "Torresani", "Lorenzo"], "venue": "In Proc. CVPR,", "citeRegEx": "Bertasius et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bertasius et al\\.", "year": 2015}, {"title": "Visual Reconstruction", "author": ["Blake", "Andrew", "Zisserman"], "venue": null, "citeRegEx": "Blake et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Blake et al\\.", "year": 1987}, {"title": "Efficient, high-quality image contour detection", "author": ["Catanzaro", "Bryan C", "Su", "Bor-Yiing", "Sundaram", "Narayanan", "Lee", "Yunsup", "Murphy", "Mark", "Keutzer", "Kurt"], "venue": "In Proc. ICCV,", "citeRegEx": "Catanzaro et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Catanzaro et al\\.", "year": 2009}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "author": ["Chen", "Liang-Chieh", "Papandreou", "George", "Kokkinos", "Iasonas", "Murphy", "Kevin", "Yuille", "Alan L"], "venue": "In ICLR,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Spectral segmentation with multiscale graph decomposition", "author": ["Cour", "Timoth\u00e9e", "B\u00e9n\u00e9zit", "Florence", "Shi", "Jianbo"], "venue": "In Proc. CVPR,", "citeRegEx": "Cour et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Cour et al\\.", "year": 2005}, {"title": "Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation", "author": ["Dai", "Jifeng", "He", "Kaiming", "Sun", "Jian"], "venue": "arXiv preprint arXiv:1503.01640,", "citeRegEx": "Dai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2015}, {"title": "Solving the multiple-instance problem with axis-parallel rectangles", "author": ["Dietterich", "Thomas G", "Lathrop", "Richard H", "Lozano-perez", "Tomas"], "venue": "Artificial Intelligence,", "citeRegEx": "Dietterich et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Dietterich et al\\.", "year": 1997}, {"title": "Supervised Learning of Edges and Object Boundaries", "author": ["P. Dollar", "Z. Tu", "S. Belongie"], "venue": "In Proc. CVPR,", "citeRegEx": "Dollar et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dollar et al\\.", "year": 2006}, {"title": "Fast edge detection using structured forests", "author": ["Doll\u00e1r", "Piotr", "Zitnick", "C. Lawrence"], "venue": "PAMI, 37(8):1558\u20131570,", "citeRegEx": "Doll\u00e1r et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Doll\u00e1r et al\\.", "year": 2015}, {"title": "Predicting depth, surface normals and semantic labels with a common multiscale convolutional architecture", "author": ["Eigen", "David", "Fergus", "Rob"], "venue": null, "citeRegEx": "Eigen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Eigen et al\\.", "year": 2014}, {"title": "Depth map prediction from a single image using a multiscale deep network", "author": ["Eigen", "David", "Puhrsch", "Christian", "Fergus", "Rob"], "venue": "In NIPS,", "citeRegEx": "Eigen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Eigen et al\\.", "year": 2014}, {"title": "N\u02c6 4-fields: Neural network nearest neighbor fields for image transforms", "author": ["Ganin", "Yaroslav", "Lempitsky", "Victor"], "venue": "In Computer Vision\u2013ACCV", "citeRegEx": "Ganin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ganin et al\\.", "year": 2014}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Girshick", "Ross", "Donahue", "Jeff", "Darrell", "Trevor", "Malik", "Jitendra"], "venue": "In CVPR,", "citeRegEx": "Girshick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "Pixel-wise deep learning for contour detection", "author": ["Hwang", "J.-J", "Liu", "T.-L"], "venue": "In ICLR,", "citeRegEx": "Hwang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hwang et al\\.", "year": 2015}, {"title": "Visual boundary prediction: A deep neural prediction network and quality dissection", "author": ["Kivinen", "Jyri J", "Williams", "Christopher K. I", "Heess", "Nicolas"], "venue": "In AISTATS,", "citeRegEx": "Kivinen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kivinen et al\\.", "year": 2014}, {"title": "Highly accurate boundary detection and grouping", "author": ["Kokkinos", "Iasonas"], "venue": "In Proc. CVPR,", "citeRegEx": "Kokkinos and Iasonas.,? \\Q2010\\E", "shortCiteRegEx": "Kokkinos and Iasonas.", "year": 2010}, {"title": "Statistical edge detection: Learning and evaluating edge cues", "author": ["S. Konishi", "A. Yuille", "J. Coughlan", "Zhu", "S.-C"], "venue": "IEEE Trans. PAMI,", "citeRegEx": "Konishi et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Konishi et al\\.", "year": 2003}, {"title": "Efficient inference in fully connected crfs with gaussian edge potentials", "author": ["Kr\u00e4henb\u00fchl", "Philipp", "Koltun", "Vladlen"], "venue": "In NIPS,", "citeRegEx": "Kr\u00e4henb\u00fchl et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kr\u00e4henb\u00fchl et al\\.", "year": 2011}, {"title": "Learning to propose objects", "author": ["Kr\u00e4henb\u00fchl", "Philipp", "Koltun", "Vladlen"], "venue": "In Proc. CVPR,", "citeRegEx": "Kr\u00e4henb\u00fchl et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kr\u00e4henb\u00fchl et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2013}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "In Proc. IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Efficient piecewise training of deep structured models for semantic segmentation", "author": ["Lin", "Guosheng", "Shen", "Chunhua", "Reid", "Ian"], "venue": "arXiv preprint arXiv:1504.01013,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Semantic image segmentation via deep parsing network", "author": ["Liu", "Ziwei", "Li", "Xiaoxiao", "Luo", "Ping", "Loy", "Chen Change", "Tang", "Xiaoou"], "venue": "arXiv preprint arXiv:1509.02634,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Long", "Jonathan", "Shelhamer", "Evan", "Darrell", "Trevor"], "venue": "CoRR, abs/1411.4038,", "citeRegEx": "Long et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Long et al\\.", "year": 2014}, {"title": "Learning to detect natural image boundaries using local brightness, color, and texture cues", "author": ["D. Martin", "C. Fowlkes", "J. Malik"], "venue": "IEEE Trans. PAMI,", "citeRegEx": "Martin et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Martin et al\\.", "year": 2004}, {"title": "The role of context for object detection and semantic segmentation in the wild", "author": ["Mottaghi", "Roozbeh", "Chen", "Xianjie", "Liu", "Xiaobai", "Cho", "Nam-Gyu", "Lee", "Seong-Whan", "Fidler", "Sanja", "Urtasun", "Raquel", "Yuille", "Alan"], "venue": "In Proc. CVPR,", "citeRegEx": "Mottaghi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mottaghi et al\\.", "year": 2014}, {"title": "Is object localization for free? - weaklysupervised learning with convolutional neural networks", "author": ["Oquab", "Maxime", "Bottou", "L\u00e9on", "Laptev", "Ivan", "Sivic", "Josef"], "venue": "In Proc. CVPR,", "citeRegEx": "Oquab et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Oquab et al\\.", "year": 2015}, {"title": "Weakly- and semi-supervised learning of a DCNN for semantic image segmentation", "author": ["Papandreou", "George", "Chen", "Liang-Chieh", "Murphy", "Kevin", "Yuille", "Alan L"], "venue": "In Proc. ICCV,", "citeRegEx": "Papandreou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Papandreou et al\\.", "year": 2015}, {"title": "Modeling local and global deformations in deep learning: Epitomic convolution, multiple instance learning, and sliding window detection", "author": ["Papandreou", "George", "Kokkinos", "Iasonas", "Savalle", "Pierre-Andr\u00e9"], "venue": "In Proc. CVPR,", "citeRegEx": "Papandreou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Papandreou et al\\.", "year": 2015}, {"title": "Multiscale helps boundary detection", "author": ["Ren", "Xiaofeng"], "venue": "In ECCV,", "citeRegEx": "Ren and Xiaofeng.,? \\Q2008\\E", "shortCiteRegEx": "Ren and Xiaofeng.", "year": 2008}, {"title": "Discriminatively trained sparse code gradients for contour detection", "author": ["Ren", "Xiaofeng", "Bo", "Liefeng"], "venue": "In NIPS,", "citeRegEx": "Ren et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2012}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "LeCun", "Yann"], "venue": "In ICLR,", "citeRegEx": "Sermanet et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2014}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Sermanet", "Pierre", "Eigen", "David", "Zhang", "Xiang", "Mathieu", "Micha\u00ebl", "Fergus", "Rob", "LeCun", "Yann"], "venue": null, "citeRegEx": "Sermanet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2013}, {"title": "Deepcontour: A deep convolutional feature learned by positive-sharing loss for contour detection", "author": ["Shen", "Wei", "Wang", "Xinggang", "Yan", "Bai", "Xiang", "Zhang", "Zhijiang"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Shen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2015}, {"title": "Normalized cuts and image segmentation", "author": ["Shi", "Jianbo", "Malik", "Jitendra"], "venue": "In Proc. CVPR,", "citeRegEx": "Shi et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Shi et al\\.", "year": 1997}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": null, "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Learning-based symmetry detection in natural images", "author": ["Tsogkas", "Stavros", "Kokkinos", "Iasonas"], "venue": "In Proc. ECCV,", "citeRegEx": "Tsogkas et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tsogkas et al\\.", "year": 2012}, {"title": "Selective search for object recognition", "author": ["Uijlings", "Jasper R. R", "van de Sande", "Koen E. A", "Gevers", "Theo", "Smeulders", "Arnold W. M"], "venue": null, "citeRegEx": "Uijlings et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Uijlings et al\\.", "year": 2013}, {"title": "Scale-space filtering", "author": ["A.P. Witkin"], "venue": "In Proc. Int. Joint Conf. on Artificial Intel., pp. 1019\u20131022,", "citeRegEx": "Witkin,? \\Q1983\\E", "shortCiteRegEx": "Witkin", "year": 1983}, {"title": "Holistically-nested edge detection", "author": ["Xie", "Saining", "Tu", "Zhuowen"], "venue": "In Proc. ICCV,", "citeRegEx": "Xie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2015}, {"title": "Conditional random fields as recurrent neural networks", "author": ["Zheng", "Shuai", "Jayasumana", "Sadeep", "Romera-Paredes", "Bernardino", "Vineet", "Vibhav", "Su", "Zhizhong", "Du", "Dalong", "Huang", "Chang", "Torr", "Philip H. S"], "venue": "In Proc. ICCV,", "citeRegEx": "Zheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2015}, {"title": "Untangling cycles for contour grouping", "author": ["Zhu", "Qihui", "Song", "Gang", "Shi", "Jianbo"], "venue": "In Proc. CVPR,", "citeRegEx": "Zhu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 22, "context": "(1998) have delivered compelling results in high-level vision tasks, such as image classification (Krizhevsky et al., 2013; Sermanet et al., 2013; Simonyan & Zisserman, 2014; Szegedy et al., 2014; Papandreou et al., 2015b) or object detection (Girshick et al.", "startOffset": 98, "endOffset": 222}, {"referenceID": 35, "context": "(1998) have delivered compelling results in high-level vision tasks, such as image classification (Krizhevsky et al., 2013; Sermanet et al., 2013; Simonyan & Zisserman, 2014; Szegedy et al., 2014; Papandreou et al., 2015b) or object detection (Girshick et al.", "startOffset": 98, "endOffset": 222}, {"referenceID": 15, "context": ", 2015b) or object detection (Girshick et al., 2014).", "startOffset": 29, "endOffset": 52}, {"referenceID": 26, "context": "Recent works have also shown that DCNNs can equally well apply to pixel-level labelling tasks, including semantic segmentation (Long et al., 2014; Chen et al., 2015) or normal estimation (Sermanet et al.", "startOffset": 127, "endOffset": 165}, {"referenceID": 6, "context": "Recent works have also shown that DCNNs can equally well apply to pixel-level labelling tasks, including semantic segmentation (Long et al., 2014; Chen et al., 2015) or normal estimation (Sermanet et al.", "startOffset": 127, "endOffset": 165}, {"referenceID": 34, "context": ", 2015) or normal estimation (Sermanet et al., 2014; Eigen et al., 2014).", "startOffset": 29, "endOffset": 72}, {"referenceID": 12, "context": ", 2015) or normal estimation (Sermanet et al., 2014; Eigen et al., 2014).", "startOffset": 29, "endOffset": 72}, {"referenceID": 34, "context": "A convenient component of such works is that the inherently convolutional nature of DCNNS allows for simple and efficient \u2018fully convolutional\u2019 implementations (Sermanet et al., 2014; Eigen et al., 2014; Oquab et al., 2015; Long et al., 2014; Chen et al., 2015).", "startOffset": 160, "endOffset": 261}, {"referenceID": 12, "context": "A convenient component of such works is that the inherently convolutional nature of DCNNS allows for simple and efficient \u2018fully convolutional\u2019 implementations (Sermanet et al., 2014; Eigen et al., 2014; Oquab et al., 2015; Long et al., 2014; Chen et al., 2015).", "startOffset": 160, "endOffset": 261}, {"referenceID": 29, "context": "A convenient component of such works is that the inherently convolutional nature of DCNNS allows for simple and efficient \u2018fully convolutional\u2019 implementations (Sermanet et al., 2014; Eigen et al., 2014; Oquab et al., 2015; Long et al., 2014; Chen et al., 2015).", "startOffset": 160, "endOffset": 261}, {"referenceID": 26, "context": "A convenient component of such works is that the inherently convolutional nature of DCNNS allows for simple and efficient \u2018fully convolutional\u2019 implementations (Sermanet et al., 2014; Eigen et al., 2014; Oquab et al., 2015; Long et al., 2014; Chen et al., 2015).", "startOffset": 160, "endOffset": 261}, {"referenceID": 6, "context": "A convenient component of such works is that the inherently convolutional nature of DCNNS allows for simple and efficient \u2018fully convolutional\u2019 implementations (Sermanet et al., 2014; Eigen et al., 2014; Oquab et al., 2015; Long et al., 2014; Chen et al., 2015).", "startOffset": 160, "endOffset": 261}, {"referenceID": 17, "context": "Over the past three years Deep Convolutional Neural Networks (DCNNs) LeCun et al. (1998) have delivered compelling results in high-level vision tasks, such as image classification (Krizhevsky et al.", "startOffset": 69, "endOffset": 89}, {"referenceID": 1, "context": "As detailed in Arbelaez et al. (2011) we can \u2018benchmark\u2019 humans against each other, by comparing every annotator to the \u2018committee\u2019 formed by the rest: if a user provides details that no committee member has provided these count as false positives, while if a user misses details provided by a committee member, these count as misses.", "startOffset": 15, "endOffset": 38}, {"referenceID": 19, "context": "As in all works following the introduction of human-annotated datasets (Konishi et al., 2003; Martin et al., 2004), e.", "startOffset": 71, "endOffset": 114}, {"referenceID": 27, "context": "As in all works following the introduction of human-annotated datasets (Konishi et al., 2003; Martin et al., 2004), e.", "startOffset": 71, "endOffset": 114}, {"referenceID": 10, "context": "(Dollar et al., 2006; Arbelaez et al., 2011; Ren, 2008; Kokkinos, 2010a; Ren & Bo, 2012; Doll\u00e1r & Zitnick, 2015), we use machine learning to optimize the performance of our boundary detector.", "startOffset": 0, "endOffset": 112}, {"referenceID": 1, "context": "(Dollar et al., 2006; Arbelaez et al., 2011; Ren, 2008; Kokkinos, 2010a; Ren & Bo, 2012; Doll\u00e1r & Zitnick, 2015), we use machine learning to optimize the performance of our boundary detector.", "startOffset": 0, "endOffset": 112}, {"referenceID": 3, "context": "Recent works (Bertasius et al., 2015; Kivinen et al., 2014; Hwang & Liu, 2015) have shown hat DCNNs yield substantial improvements over flat classifiers; the Holistic Edge Detection approach of Xie & Tu (2015) recently achieved dramatic improvements over the previous state-of-the-art, from an F-measure of 0.", "startOffset": 13, "endOffset": 78}, {"referenceID": 17, "context": "Recent works (Bertasius et al., 2015; Kivinen et al., 2014; Hwang & Liu, 2015) have shown hat DCNNs yield substantial improvements over flat classifiers; the Holistic Edge Detection approach of Xie & Tu (2015) recently achieved dramatic improvements over the previous state-of-the-art, from an F-measure of 0.", "startOffset": 13, "endOffset": 78}, {"referenceID": 1, "context": ", 2006; Arbelaez et al., 2011; Ren, 2008; Kokkinos, 2010a; Ren & Bo, 2012; Doll\u00e1r & Zitnick, 2015), we use machine learning to optimize the performance of our boundary detector. Recent works (Bertasius et al., 2015; Kivinen et al., 2014; Hwang & Liu, 2015) have shown hat DCNNs yield substantial improvements over flat classifiers; the Holistic Edge Detection approach of Xie & Tu (2015) recently achieved dramatic improvements over the previous state-of-the-art, from an F-measure of 0.", "startOffset": 8, "endOffset": 388}, {"referenceID": 9, "context": "Our approach builds on Kokkinos (2010a), where Multiple Instance Learning (MIL) (Dietterich et al., 1997) is used to accommodate orientation inconsistencies during the learning of an orientationsensitive boundary detector.", "startOffset": 80, "endOffset": 105}, {"referenceID": 9, "context": "Our approach builds on Kokkinos (2010a), where Multiple Instance Learning (MIL) (Dietterich et al., 1997) is used to accommodate orientation inconsistencies during the learning of an orientationsensitive boundary detector. That work was aimed at learning orientation-sensitive classifiers in the presence of orientation ambiguity in the annotations - we take a similar approach in order to deal with positional ambiguity in the annotations while learning a position-sensitive detector. Standard, \u2018single instance\u2019 learning assumes training samples come in feature-label pairs -or, as in HED above, every pixel is either a boundary or not. Instead, MIL takes as a training sample a set of features (\u2018bag\u2019) and its label. A bag should be labelled positive if at least one of its features is classified as positive, and negative otherwise. In particular, since human annotations come with some positional uncertainty, the standard evaluation protocol of Martin et al. (2004) allows for some slack in the predicted position of a pixel (a", "startOffset": 81, "endOffset": 972}, {"referenceID": 41, "context": "However, image boundaries reside in multiple image resolutions (Witkin, 1983) and it has repeatedly been shown that fusing information from multiple resolutions improves boundary detection, e.", "startOffset": 63, "endOffset": 77}, {"referenceID": 40, "context": "However, image boundaries reside in multiple image resolutions (Witkin, 1983) and it has repeatedly been shown that fusing information from multiple resolutions improves boundary detection, e.g. in Doll\u00e1r & Zitnick (2015); Arbelaez et al.", "startOffset": 64, "endOffset": 222}, {"referenceID": 1, "context": "in Doll\u00e1r & Zitnick (2015); Arbelaez et al. (2011). Even though the authors of HED use information from multiple scales by fusing the outputs of many layers, multi-resolution boundary detection can still help.", "startOffset": 28, "endOffset": 51}, {"referenceID": 28, "context": "We have not used these additional scalings in our experiments due to time constraints, but have considered the use of boundaries from the VOC Context dataset (Mottaghi et al., 2014), where all objects and \u2018stuff\u2019 present in the scene are manually segmented.", "startOffset": 158, "endOffset": 181}, {"referenceID": 44, "context": "The boundary detector only implicitly exploits grouping cues such as closedness or continuity that can often yield improvements in the high-precision regime (Zhu et al., 2007; Kokkinos, 2010b).", "startOffset": 157, "endOffset": 192}, {"referenceID": 43, "context": "The boundary detector only implicitly exploits grouping cues such as closedness or continuity that can often yield improvements in the high-precision regime (Zhu et al., 2007; Kokkinos, 2010b). To capture such information we use the Normalized Cuts (NCuts) technique of Shi & Malik (1997); Arbelaez et al.", "startOffset": 158, "endOffset": 289}, {"referenceID": 1, "context": "To capture such information we use the Normalized Cuts (NCuts) technique of Shi & Malik (1997); Arbelaez et al. (2011). We treat the image as a weighted graph, where nodes corresponding to pixels and weights correspond to low-level affinity between pixels measured in terms of the Intervening Contour cue (Shi & Malik, 1997), where the contours are now estimated by our boundary detector.", "startOffset": 96, "endOffset": 119}, {"referenceID": 5, "context": "(2005), we found it simpler to harness the computational power of GPUs and integrate the Damascene system of (Catanzaro et al., 2009) with the Caffe deep learning framework; when integrated with our boundary detector Damascene yields 8 eigenvectors for a 577\u00d7 865 image in less that 0.", "startOffset": 109, "endOffset": 133}, {"referenceID": 1, "context": "These embeddings can be used for boundary detection in terms of their directional derivatives, in order to provide some \u2018global\u2019 evidence for the presence of a boundary, known as the \u2018spectral probability of boundary\u2019 cue (Arbelaez et al., 2011).", "startOffset": 222, "endOffset": 245}, {"referenceID": 5, "context": "Cour et al. (2005), we found it simpler to harness the computational power of GPUs and integrate the Damascene system of (Catanzaro et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 1, "context": "These embeddings can be used for boundary detection in terms of their directional derivatives, in order to provide some \u2018global\u2019 evidence for the presence of a boundary, known as the \u2018spectral probability of boundary\u2019 cue (Arbelaez et al., 2011). This further improves the performance of our detector, yielding an F-measure of 0.813, which is substantially better than our earlier performance of 0.807, and humans, who operate at 0.803. Due to time constraints we have used a very simple fusion scheme (addition of the posterior with SpectralPB) - we anticipate that adding a few processing layers can further improve performance. We summarize the impact of the different steps described above in Fig. 6 - starting from a baseline (that performs slightly worse than the HED system of Xie & Tu (2015) we have introduced a series of changes that resulted in super-human boundary detection performance.", "startOffset": 223, "endOffset": 800}, {"referenceID": 1, "context": "These embeddings can be used for boundary detection in terms of their directional derivatives, in order to provide some \u2018global\u2019 evidence for the presence of a boundary, known as the \u2018spectral probability of boundary\u2019 cue (Arbelaez et al., 2011). This further improves the performance of our detector, yielding an F-measure of 0.813, which is substantially better than our earlier performance of 0.807, and humans, who operate at 0.803. Due to time constraints we have used a very simple fusion scheme (addition of the posterior with SpectralPB) - we anticipate that adding a few processing layers can further improve performance. We summarize the impact of the different steps described above in Fig. 6 - starting from a baseline (that performs slightly worse than the HED system of Xie & Tu (2015) we have introduced a series of changes that resulted in super-human boundary detection performance. When compared to the current state-of-the-art method of Xie & Tu (2015) our method clearly dominates in terms of all typical performance measures, as shown in Table 2.", "startOffset": 223, "endOffset": 972}, {"referenceID": 26, "context": "Since our model is fully-convolutional we can easily combine it with the recent line of works around FCNN-based semantic segmentation(Long et al., 2014; Chen et al., 2015; Papandreou et al., 2015a; Zheng et al., 2015).", "startOffset": 133, "endOffset": 217}, {"referenceID": 6, "context": "Since our model is fully-convolutional we can easily combine it with the recent line of works around FCNN-based semantic segmentation(Long et al., 2014; Chen et al., 2015; Papandreou et al., 2015a; Zheng et al., 2015).", "startOffset": 133, "endOffset": 217}, {"referenceID": 43, "context": "Since our model is fully-convolutional we can easily combine it with the recent line of works around FCNN-based semantic segmentation(Long et al., 2014; Chen et al., 2015; Papandreou et al., 2015a; Zheng et al., 2015).", "startOffset": 133, "endOffset": 217}, {"referenceID": 6, "context": ", 2014; Chen et al., 2015; Papandreou et al., 2015a; Zheng et al., 2015). These have delivered excellent results, and in particular the use of the Dense Conditional Random Field (DenseCRF) of Kr\u00e4henb\u00fchl & Koltun (2011) by Chen et al.", "startOffset": 8, "endOffset": 219}, {"referenceID": 6, "context": ", 2014; Chen et al., 2015; Papandreou et al., 2015a; Zheng et al., 2015). These have delivered excellent results, and in particular the use of the Dense Conditional Random Field (DenseCRF) of Kr\u00e4henb\u00fchl & Koltun (2011) by Chen et al. (2015); Papandreou et al.", "startOffset": 8, "endOffset": 241}, {"referenceID": 6, "context": ", 2014; Chen et al., 2015; Papandreou et al., 2015a; Zheng et al., 2015). These have delivered excellent results, and in particular the use of the Dense Conditional Random Field (DenseCRF) of Kr\u00e4henb\u00fchl & Koltun (2011) by Chen et al. (2015); Papandreou et al. (2015a); Zheng et al.", "startOffset": 8, "endOffset": 268}, {"referenceID": 6, "context": ", 2014; Chen et al., 2015; Papandreou et al., 2015a; Zheng et al., 2015). These have delivered excellent results, and in particular the use of the Dense Conditional Random Field (DenseCRF) of Kr\u00e4henb\u00fchl & Koltun (2011) by Chen et al. (2015); Papandreou et al. (2015a); Zheng et al. (2015), has enhanced the discriminative power of FCNNs with local evidence gathered by the image intensity.", "startOffset": 8, "endOffset": 289}, {"referenceID": 1, "context": "Method ODS OIS AP gPb-owt-ucm (Arbelaez et al., 2011) 0.", "startOffset": 30, "endOffset": 53}, {"referenceID": 17, "context": "803 DeepNets (Kivinen et al., 2014) 0.", "startOffset": 13, "endOffset": 35}, {"referenceID": 3, "context": "784 DeepEdge (Bertasius et al., 2015) 0.", "startOffset": 13, "endOffset": 37}, {"referenceID": 36, "context": "798 DeepContour (Shen et al., 2015) 0.", "startOffset": 16, "endOffset": 35}, {"referenceID": 6, "context": "Following Chen et al. (2015) we define the CRF distribution as:", "startOffset": 10, "endOffset": 29}, {"referenceID": 0, "context": "Mean-field Inference for this form of pairwise terms can be efficiently implemented with high-dimensional filtering (Adams et al., 2010).", "startOffset": 116, "endOffset": 136}, {"referenceID": 0, "context": "Mean-field Inference for this form of pairwise terms can be efficiently implemented with high-dimensional filtering (Adams et al., 2010). Our modifications are very simple: firstly, we adapt the multi-resolution architecture outlined in the previous section to semantic segmentation. Using multi-resolution processing with tied-weights and performing late score fusion yielded substantially better results than using a single-resolution network: as shown in Table. 3 when combining the multi-scale network\u2019s output with DenseCRF inference, performance increases from 72.7 (single-scale counterpart of Chen et al. (2015)) or 73.", "startOffset": 117, "endOffset": 620}, {"referenceID": 0, "context": "Mean-field Inference for this form of pairwise terms can be efficiently implemented with high-dimensional filtering (Adams et al., 2010). Our modifications are very simple: firstly, we adapt the multi-resolution architecture outlined in the previous section to semantic segmentation. Using multi-resolution processing with tied-weights and performing late score fusion yielded substantially better results than using a single-resolution network: as shown in Table. 3 when combining the multi-scale network\u2019s output with DenseCRF inference, performance increases from 72.7 (single-scale counterpart of Chen et al. (2015)) or 73.9 (skip-layer multi-scale counterpart of Chen et al. (2015)) to 74.", "startOffset": 117, "endOffset": 687}, {"referenceID": 0, "context": "Mean-field Inference for this form of pairwise terms can be efficiently implemented with high-dimensional filtering (Adams et al., 2010). Our modifications are very simple: firstly, we adapt the multi-resolution architecture outlined in the previous section to semantic segmentation. Using multi-resolution processing with tied-weights and performing late score fusion yielded substantially better results than using a single-resolution network: as shown in Table. 3 when combining the multi-scale network\u2019s output with DenseCRF inference, performance increases from 72.7 (single-scale counterpart of Chen et al. (2015)) or 73.9 (skip-layer multi-scale counterpart of Chen et al. (2015)) to 74.8 (our multi-scale) in mean accuracy. Secondly, we integrate the boundary information extracted by our detector into the DenseCRF by using the eigenvectors computed by normalized Cuts to augment the RGB color features of Eq. 12, thereby conveying boundary-based proximity into DenseCRF inference. In particular we augment the dimensionality of Ii in Eq. 12 from 3 to 6, by concatenating the 3 eigenvectors delivered by NCuts with the RGB values. We observe that introducing the Normalized Cut eigenvectors into DenseCRF inference yields a clear improvement over an already high-performing system (from 74.8 to 75.4), while a small additional improvement was obtained we performing graph-cut inference with pairwise terms that depend on the boundary strength (from 75.4 to 75.7). Further improvements can be anticipated though an end-to-end training using the recursive CNN framework of Zheng et al. (2015) as in the currently leading works - we will explore this in future work.", "startOffset": 117, "endOffset": 1600}, {"referenceID": 40, "context": "The generation of regions of interest has been established as a standard pre-processing step for object detection; one big push to the broad adoption of such methods has been the successful incorporation of Selective Search (Uijlings et al., 2013) into the RCNN work of Girshick et al.", "startOffset": 224, "endOffset": 247}, {"referenceID": 15, "context": ", 2013) into the RCNN work of Girshick et al. (2014), making them the dominant paradigm for current object detection.", "startOffset": 30, "endOffset": 53}, {"referenceID": 24, "context": "Method mAP % Adelaide-Context-CNN-CRF-COCO (Lin et al., 2015) 77.", "startOffset": 43, "endOffset": 61}, {"referenceID": 25, "context": "8 CUHK-DPN-COCO (Liu et al., 2015) 77.", "startOffset": 16, "endOffset": 34}, {"referenceID": 24, "context": "5 Adelaide-Context-CNN-CRF-COCO (Lin et al., 2015) 77.", "startOffset": 32, "endOffset": 50}, {"referenceID": 8, "context": "2 MSRA-BoxSup (Dai et al., 2015) 75.", "startOffset": 14, "endOffset": 32}, {"referenceID": 43, "context": "2 Oxford-TVG-CRF-RNN-COCO (Zheng et al., 2015) 74.", "startOffset": 26, "endOffset": 46}, {"referenceID": 6, "context": "7 DeepLab-MSc-CRF-LF-COCO-CJ (Chen et al., 2015) 73.", "startOffset": 29, "endOffset": 48}, {"referenceID": 6, "context": "9 DeepLab-CRF-COCO-LF(Chen et al., 2015) 72.", "startOffset": 21, "endOffset": 40}], "year": 2017, "abstractText": "In this work we show that Deep Convolutional Neural Networks can outperform humans on the task of boundary detection, as measured on the standard Berkeley Segmentation Dataset. Our detector is fully integrated in the popular Caffe framework and processes a 320x420 image in less than a second. Our contributions consist firstly in combining a careful design of the loss for boundary detection training, a multi-resolution architecture and training with external data to improve the detection accuracy of the current state of the art, from an optimal dataset scale F-measure of 0.780 to 0.808 while human performance is at 0.803. We further improve performance to 0.813 by combining deep learning with grouping, integrating the Normalized Cuts technique within a deep network. We also examine the potential of our boundary detector in conjunction with the higher level tasks of object proposal generation and semantic segmentation for both tasks our detector yields clear improvements over state-of-the-art systems.", "creator": "LaTeX with hyperref package"}}}