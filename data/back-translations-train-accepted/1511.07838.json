{"id": "1511.07838", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Nov-2015", "title": "Dynamic Capacity Networks", "abstract": "We introduce the Dynamic Capacity Network (DCN), a neural network that can adaptively assign its capacity across different portions of the input data. This is achieved by combining modules of two types: low-capacity sub-networks and high-capacity sub-networks. The low-capacity sub-networks are applied across most of the input, but also provide a guide to select a few portions of the input on which to apply the high-capacity sub-networks. The selection is made using a novel gradient-based attention mechanism, that efficiently identifies the modules and input features that are most likely to impact the DCN's output and to which we'd like to devote more capacity. We focus our empirical evaluation on the cluttered MNIST and SVHN image datasets. Our findings indicate that DCNs are able to drastically reduce the number of computations, compared to traditional convolutional neural networks, while maintaining similar performance.", "histories": [["v1", "Tue, 24 Nov 2015 19:30:19 GMT  (689kb,D)", "https://arxiv.org/abs/1511.07838v1", "ICLR 2016 submission"], ["v2", "Fri, 27 Nov 2015 19:17:53 GMT  (1636kb,D)", "http://arxiv.org/abs/1511.07838v2", "ICLR 2016 submission"], ["v3", "Thu, 3 Dec 2015 16:13:21 GMT  (1637kb,D)", "http://arxiv.org/abs/1511.07838v3", "ICLR 2016 submission"], ["v4", "Thu, 7 Jan 2016 22:44:43 GMT  (1847kb,D)", "http://arxiv.org/abs/1511.07838v4", "ICLR 2016 submission"], ["v5", "Tue, 9 Feb 2016 16:49:55 GMT  (1854kb,D)", "http://arxiv.org/abs/1511.07838v5", "ICML 2016 submission"], ["v6", "Wed, 6 Apr 2016 19:48:32 GMT  (1865kb,D)", "http://arxiv.org/abs/1511.07838v6", "ICML 2016 submission"], ["v7", "Sun, 22 May 2016 20:58:11 GMT  (1866kb,D)", "http://arxiv.org/abs/1511.07838v7", "ICML 2016"]], "COMMENTS": "ICLR 2016 submission", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["amjad almahairi", "nicolas ballas", "tim cooijmans", "yin zheng", "hugo larochelle", "aaron c courville"], "accepted": true, "id": "1511.07838"}, "pdf": {"name": "1511.07838.pdf", "metadata": {"source": "META", "title": "Dynamic Capacity Networks", "authors": ["Amjad Almahairi", "Nicolas Ballas", "Tim Cooijmans", "Yin Zheng", "Hugo Larochelle", "Aaron Courville"], "emails": ["AMJAD.ALMAHAIRI@UMONTREAL.CA", "NICOLAS.BALLAS@UMONTREAL.CA", "TIM.COOIJMANS@UMONTREAL.CA", "YIN.ZHENG@HULU.COM", "HLAROCHELLE@TWITTER.COM", "AARON.COURVILLE@UMONTREAL.CA"], "sections": [{"heading": "1. Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2. Dynamic Capacity Networks", "text": "In this section, we describe the Dynamic Capacity Network (DCN), which dynamically determines its network capacity via an input.We consider a deep neural network h, which we divide into two parts: h (x) = g (f (x)), where f and g represent the lower and upper layers of the network h respectively, while x is some input data. Lower layers f operate directly on the input and output a representation consisting of a collection of vectors, each representing a region in the input.For example, f can output a characteristic map, i.e. vectors of characteristics that each have a specific spatial location, or a probability map that outputs probability distributions at each different spatial location. Upper layers consider g as input of the lower layers representations f (x) and output of a distribution via labels."}, {"heading": "2.1. Attention-based Inference", "text": "In DCN, we want to get better predictions than those made by the rough models, while keeping the calculation requirements reasonable. This can be done by selecting a few important input regions on which we use the fine representations instead of the rough representations. DCN conclusions must therefore identify the important regions in relation to the task at hand. To do this, we use a novel approach to attention that applies the propagation of the rough representations to identify some vectors in the rough representation to which the distribution via the class label is most sensitive. These vectors correspond to the input regions that we identify as important or task-relevant regions. Given an input image of x, we first apply the rough layers to all input regions to calculate the rough representation of the vectors: fc (x) = (i, j)."}, {"heading": "2.2. End-to-End Training", "text": "In this section, we describe an end-to-end process for training the DCN model that uses our attention mechanisms to learn together. However, we emphasize that DCN modules can be trained independently of each other by training a rough and a fine aspect independently of each other and combining them only during the test period, showing an example of how this modular training can be used for transfer. In the context of image classification, we assume that we have set a training D = {(i), y (i), y (i)))); i = 1.. m}, where each x (i), Rh \u00d7 w is an image, and y (i) is its corresponding designation. We designate the parameters of the coarse, fine and an upper layers of two."}, {"heading": "3. Related Work", "text": "The goal of conditional calculation, as proposed by Bengio (2013), is to train very large models for the same calculation costs as smaller ones, avoiding certain calculation paths depending on the input. However, there have been several contributions in this direction. Bengio et al. (2013) use stochastic neurons as gating units that activate specific parts of a neural network. Our approach, on the other hand, uses a hard attention mechanism that helps the model focus its computationally expensive paths, helping to generate larger effective models and larger input sites.Several recent contributions use attention mechanisms to capture visual structures using focus-like methods, e.g. Larochelle & Hinton, Denil et al al al al al., 2014; Mnih et al al al al."}, {"heading": "4. Experiments", "text": "In this section, we present an experimental assessment of the proposed DCN model. To test the effectiveness of our approach, we first examine the cluttered MNIST dataset (Mnih et al., 2014), then apply our model to a real object recognition task using the Street View house number dataset (SVHN) in a transfer learning environment (Netzer et al., 2011)."}, {"heading": "4.1. Cluttered MNIST", "text": "Each image in this dataset is a handwritten MNIST digit randomly placed on a 100 x 100 black canvas and littered with digital fragments. Therefore, the dataset is the same size as MNIST: 60000 images for training and 10000 for the test."}, {"heading": "4.1.1. MODEL SPECIFICATION", "text": "In this experiment, we train a DCN model end-to-end, where we learn coarse and fine layers together. We use 2 convolutionary layers as coarse layers, 5 convolutionary layers as fine layers, and a convolutionary layer followed by global max pooling and a softmax as top layers. Details of their architecture can be found in Appendix 6.1. The coarse and fine layers generate feature maps, i.e. feature vectors, each with a specific spatial location. The set of selected patches Xs consists of eight patches of size 14 x 14 pixels. Here, we use a sophisticated representation of the complete input fr (x), in which fine feature vectors are replaced by coarse ones: fr (x) = {ri, j | (i, j) \u04411, s1] \u00d7 [1, s2]} (7) ri, j = {ff (xi, j), if xi, j, Xfc (otherwise)."}, {"heading": "4.1.2. BASELINES", "text": "As a basis for our evaluation we use the rough model (upper layers are applied only to rough representations), the fine model (upper layers are applied only to fine representations) and compare it with previous attention-based models RAM (Mnih et al., 2014) and DRAW (Gregor et al., 2015)."}, {"heading": "4.1.3. EMPIRICAL EVALUATION", "text": "The results of our experiments are shown in Table 1. We get our best DCN result by adding the clue text in Equation (6) to the training target we observe to have a regulating effect on DCN. We can see that the DCN model performs significantly better than the previous result achieved by RAM and DRAW models. It also exceeds the fine model, which is a result of the ability to focus only on the digit and ignore clutter. In Figure 2, we study more the effect of the clue text during training and confirm that it can actually minimize the square distance between coarse and fine illustrations. To show how the attention mechanism of the DCN model can help it, we draw in Figure 3 (a) the patches it finds in some images from the validation that it minimizes the square distance between coarse and fine illustrations."}, {"heading": "4.2. SVHN", "text": "In this section, we take on a more challenging task of transcribing multi-digit sequences of natural images using the Street View House Numbers (SVHN) dataset (Netzer et al., 2011).SVHN is composed of real images containing house numbers and originating from house facades; the task is to identify the complete numerical sequence corresponding to a house number that can have a length of 1 to 5 digits. The dataset has three subsets: train (33k), additional (202k) and test (13k).In the following, we trained our models on 230k images from the train and additional subsets in which we took a 5k random sample as validation for selecting hyperparameters. Typical experimental setting in the previous literature, e.g. (Goodfellow et al., 2013; Ba et al., 2014; Jaderberg et al., 2015) uses the location of digital boxes as additional information."}, {"heading": "4.2.1. MULTI-DIGIT RECOGNITION MODEL", "text": "We follow the model proposed in (Goodfellow et al., 2013) to learn a probabilistic model of the number sequence2The only pre-processing we perform with the data is the conversion of images to grayscales. The output sequence S is defined by a collection of N random variables S1,..., SN representing the elements of the sequence and an additional random variable S0 representing their length. The probability of a given sequence s = {s1,.., sn} is derived from: p (S = s | x) = p (S0 = n | x) n-th digit i = 1p (Si = si | x), (9) where p (S0 = n | x) is the conditional distribution of the sequence length and p (Si = si | x) is the conditional distribution of the i-th digit in the sequence. Specifically, our model has on HN a maximum: 1 for the length of the sequence (Si = 1 and Si = 1) for the respective length (x)."}, {"heading": "4.2.2. MODEL SPECIFICATION", "text": "The coarse and fine lower layers, fc and ff, are fully convolutional, each consisting of 7 and 11 layers. The representation generated by either the fine or coarse layers is a probability map that is a collection of independent full sequence prediction vectors, each vector corresponding to a specific region of the input. We refer to the prediction for the i-th output at position (j, k) by p (j, k) (Si | x). The top layer g consists of a global average layer that combines predictions from different spatial locations to generate the final prediction p (S | x). Since we have several outputs in this task, we modify the label size used by the DCN's attention mechanism to be the sum of the entropy of the 5-digit soft maxes."}, {"heading": "4.2.3. BASELINES", "text": "As mentioned in the previous section, each of the rough representation vectors in this experiment corresponds to multi-digit detection probabilities calculated in a particular region, which the top layer g simply indicates as the mean to obtain the rough base model: p (Si | x) = 1d1 \u00b7 d2 \u2211 j, k p (j, k) (Si | x). (11) The fine base model is similarly defined. As an additional baseline, we consider a rough model of \"soft attention\" that takes the rough representation vectors across all input regions, but uses a top layer that performs a weighted average of the resulting site-specific predictions. We use entropy to define a weighting scheme that emphasizes important places: p (Si | x) = j, j wi, j, kp (j, k), kp (Si | x)."}, {"heading": "4.2.4. EMPIRICAL EVALUATION", "text": "Table 2 shows the results of our experiment on SVHN. The rough model has an error rate of 40.6%, while our proposed soft care mechanism lowers the error rate to 31.4%. This confirms that entropy is a good measure for identifying important regions where the task information is not evenly distributed across the input data, whereas the fine model achieves a better error rate of 25.2%, but it is more expensive in arithmetical terms. Our DCN model, which selects only 6 regions where the high-resolution fine layers are applied, achieves an error rate of 20.0%, so the DCN model can outperform the other fundamentals in terms of classification accuracy."}, {"heading": "5. Conclusions", "text": "We presented the DCN model, a novel conditional computing approach. We demonstrated that our network can adapt its capacity to different parts of the input data using our visual attention mechanism, focusing on key regions of the input. Our model achieved state-of-the-art performance in the task of cluttered MNIST number classification and offered computational advantages over traditional Convolutionary Network architectures. We also validated our model in a learning environment for transferring using the SVHN dataset, where we tackled the problem of multi-digit recognition without first using any information about the position of the digits. We demonstrated that our model outperforms other baselines, but remains traceable for inputs with large spatial dimensions."}, {"heading": "6. Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1. Cluttered MNIST Experiment Details", "text": "\u2022 Rough layers: 2 Convolutionary layers of 7 x 7 and 3 x 3 filter sizes, 12 and 24 filters, respectively, and a step of 2 x 2. Each feature in the Rough Characteristics Card covers a range of 11 x 11 pixels, which we expand by 3 pixels on each side to give the fine layers more context. The size of the Rough Characteristics Card is 23 x 23. \u2022 Fine layers: 5 Convolutionary layers, each 3 x 3 3We evaluate all models on an NVIDIA Titan Black GPU card.Filter sizes, 1 x 1 step and 24 filters. We apply 2 x 2 pooling with 2 x 2 steps after the second and fourth layers. We also use 1 x 1 zero padding in all layers except the first and last layers. This architecture has been chosen to map a 14 x 14 patch in a spatial location. \u2022 Top layers: a continuous layer with 4 x 4 filter sizes, 2 x 96 layers, 2 and 2 x layers."}, {"heading": "6.2. SVHN Experiment Details", "text": "\u2022 Coarse layers: The model is fully convolutional with 7 convolutional layers. The first three layers have 24, 48, 128 filters with size 5 x 5 and step 2 x 2. Layer 4 has 192 filters with 4 x 5 and step 1 x 2. Layer 5 has 192 filters with size 1 x 4. Finally, the last two layers are 1 x 1 coils with 1024 filters. We use step 1 x 1 in the last 3 layers and do not use zero padding in one of the coarse layers. The corresponding patch size is 54 x 110. \u2022 Fine layers: 11 convolutional layers. The first 5 convolutional layers have 48, 64, 128, 160 and 192 filters with size 5 x 5 and zero padding. After layers 1, 3 and 5 we use 2 x 2 max pooling with step 2 x 2. The following layers have 3 x convolution with 192 filters."}, {"heading": "Acknowledgements", "text": "The authors would like to thank the following research funding and computer support organizations: Nuance Foundation, Compute Canada and Calcul Que \u0301 bec. We would like to thank the developers of Theano (Bergstra et al., 2011; Bastien et al., 2012) and Blocks / Fuel (Van Merrie \ufffd nboer et al., 2015) for developing such powerful tools for the scientific calculator and our critics for their useful comments."}], "references": [{"title": "Multiple object recognition with visual attention", "author": ["Ba", "Jimmy", "Mnih", "Volodymyr", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1412.7755,", "citeRegEx": "Ba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2014}, {"title": "Theano: new features and speed improvements", "author": ["Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian J", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Bengio", "Yoshua"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Deep learning of representations: Looking forward", "author": ["Bengio", "Yoshua"], "venue": "In Statistical Language and Speech Processing,", "citeRegEx": "Bengio and Yoshua.,? \\Q2013\\E", "shortCiteRegEx": "Bengio and Yoshua.", "year": 2013}, {"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "author": ["Bengio", "Yoshua", "L\u00e9onard", "Nicholas", "Courville", "Aaron"], "venue": "arXiv preprint arXiv:1308.3432,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Approximate nearest neighbor search by residual vector quantization", "author": ["Chen", "Yongjian", "Guan", "Tao", "Wang", "Cheng"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2010}, {"title": "Learning where to attend with deep architectures for image tracking", "author": ["Denil", "Misha", "Bazzani", "Loris", "Larochelle", "Hugo", "de Freitas", "Nando"], "venue": "Neural computation,", "citeRegEx": "Denil et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2012}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Denton", "Emily L", "Zaremba", "Wojciech", "Bruna", "Joan", "LeCun", "Yann", "Fergus", "Rob"], "venue": "In NIPS,", "citeRegEx": "Denton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2014}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Gong", "Yunchao", "Liu", "Yang", "Min", "Bourdev", "Lubomir"], "venue": "CoRR, abs/1412.6115,", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Multi-digit number recognition from street view imagery using deep convolutional neural networks", "author": ["Goodfellow", "Ian J", "Bulatov", "Yaroslav", "Ibarz", "Julian", "Arnoud", "Sacha", "Shet", "Vinay"], "venue": "arXiv preprint arXiv:1312.6082,", "citeRegEx": "Goodfellow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Graves", "Alex", "Jaitly", "Navdeep"], "venue": "In ICML),", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Distilling the knowledge in a neural network", "author": ["Hinton", "Geoffrey", "Vinyals", "Oriol", "Dean", "Jeff"], "venue": "arXiv preprint arXiv:1503.02531,", "citeRegEx": "Hinton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["M. Jaderberg", "A. Vedaldi", "A. Zisserman"], "venue": "In BMVC,", "citeRegEx": "Jaderberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2014}, {"title": "Spatial transformer networks", "author": ["Jaderberg", "Max", "Simonyan", "Karen", "Zisserman", "Andrew", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1506.02025,", "citeRegEx": "Jaderberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2015}, {"title": "Product quantization for nearest neighbor search", "author": ["J\u00e9gou", "Herv\u00e9", "Douze", "Matthijs", "Schmid", "Cordelia"], "venue": "IEEE TPAMI,", "citeRegEx": "J\u00e9gou et al\\.,? \\Q2011\\E", "shortCiteRegEx": "J\u00e9gou et al\\.", "year": 2011}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Learning to combine foveal glimpses with a third-order boltzmann machine", "author": ["Larochelle", "Hugo", "Hinton", "Geoffrey E"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Larochelle et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2010}, {"title": "Recurrent models of visual attention", "author": ["Mnih", "Volodymyr", "Heess", "Nicolas", "Graves", "Alex"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Netzer", "Yuval", "Wang", "Tao", "Coates", "Adam", "Bissacco", "Alessandro", "Wu", "Bo", "Ng", "Andrew Y"], "venue": "In NIPS workshop on deep learning and unsupervised feature learning,", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "On learning where to look", "author": ["Ranzato", "Marc\u2019Aurelio"], "venue": "arXiv preprint arXiv:1405.5488,", "citeRegEx": "Ranzato and Marc.Aurelio.,? \\Q2014\\E", "shortCiteRegEx": "Ranzato and Marc.Aurelio.", "year": 2014}, {"title": "Fitnets: Hints for thin deep nets", "author": ["Romero", "Adriana", "Ballas", "Nicolas", "Kahou", "Samira Ebrahimi", "Chassang", "Antoine", "Gatta", "Carlo", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1412.6550,", "citeRegEx": "Romero et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Romero et al\\.", "year": 2014}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["Simonyan", "Karen", "Vedaldi", "Andrea", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1312.6034,", "citeRegEx": "Simonyan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2013}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Blocks and fuel: Frameworks for deep learning", "author": ["Van Merri\u00ebnboer", "Bart", "Bahdanau", "Dzmitry", "Dumoulin", "Vincent", "Serdyuk", "Dmitriy", "Warde-Farley", "David", "Chorowski", "Jan", "Bengio", "Yoshua"], "venue": "ArXiv e-prints,", "citeRegEx": "Merri\u00ebnboer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 23, "context": "Deep neural networks have recently exhibited state-of-theart performance across a wide range of tasks, including object recognition (Szegedy et al., 2014) and speech recognition (Graves & Jaitly, 2014).", "startOffset": 132, "endOffset": 154}, {"referenceID": 23, "context": "Indeed, convolutional neural networks apply the same set of filters uniformly across the spatial input (Szegedy et al., 2014), while recurrent neural networks apply the same transformation at every time step (Graves & Jaitly, 2014).", "startOffset": 103, "endOffset": 125}, {"referenceID": 18, "context": "This observation has been exploited in attention-based systems (Mnih et al., 2014), which can reduce computations significantly by learning to selectively focus or \u201cattend\u201d to few, task-relevant, input regions.", "startOffset": 63, "endOffset": 82}, {"referenceID": 19, "context": "The Street View House Numbers (SVHN) dataset (Netzer et al., 2011) is an example of such a dataset.", "startOffset": 45, "endOffset": 66}, {"referenceID": 8, "context": "(Goodfellow et al., 2013; Ba et al., 2014; Jaderberg et al., 2015) Figure 1.", "startOffset": 0, "endOffset": 66}, {"referenceID": 0, "context": "(Goodfellow et al., 2013; Ba et al., 2014; Jaderberg et al., 2015) Figure 1.", "startOffset": 0, "endOffset": 66}, {"referenceID": 14, "context": "(Goodfellow et al., 2013; Ba et al., 2014; Jaderberg et al., 2015) Figure 1.", "startOffset": 0, "endOffset": 66}, {"referenceID": 21, "context": "To encourage similarity between the coarse and fine representations while training, we use a hint-based training approach inspired by Romero et al. (2014). Specifically, we add an additional term to the training objective that minimizes the squared distance between coarse and fine representations: \u2211", "startOffset": 134, "endOffset": 155}, {"referenceID": 3, "context": "Bengio et al. (2013) use stochastic neurons as gating units that activate specific parts of a neural network.", "startOffset": 0, "endOffset": 21}, {"referenceID": 5, "context": "(Larochelle & Hinton, 2010; Denil et al., 2012; Ranzato, 2014; Mnih et al., 2014; Ba et al., 2014; Gregor et al., 2015).", "startOffset": 0, "endOffset": 119}, {"referenceID": 18, "context": "(Larochelle & Hinton, 2010; Denil et al., 2012; Ranzato, 2014; Mnih et al., 2014; Ba et al., 2014; Gregor et al., 2015).", "startOffset": 0, "endOffset": 119}, {"referenceID": 0, "context": "(Larochelle & Hinton, 2010; Denil et al., 2012; Ranzato, 2014; Mnih et al., 2014; Ba et al., 2014; Gregor et al., 2015).", "startOffset": 0, "endOffset": 119}, {"referenceID": 10, "context": "(Larochelle & Hinton, 2010; Denil et al., 2012; Ranzato, 2014; Mnih et al., 2014; Ba et al., 2014; Gregor et al., 2015).", "startOffset": 0, "endOffset": 119}, {"referenceID": 10, "context": "On the other hand, the DRAW model (Gregor et al., 2015) uses a \u201csoftattention\u201d mechanism that is fully differentiable, but requires processing the whole input at each time step.", "startOffset": 34, "endOffset": 55}, {"referenceID": 0, "context": ", 2014; Ba et al., 2014; Gregor et al., 2015). In Mnih et al. (2014); Ba et al.", "startOffset": 8, "endOffset": 69}, {"referenceID": 0, "context": ", 2014; Ba et al., 2014; Gregor et al., 2015). In Mnih et al. (2014); Ba et al. (2014), a learned sequential attention model is used to make a hard decision as to where to look in the image, i.", "startOffset": 8, "endOffset": 87}, {"referenceID": 22, "context": "The saliency measure employed by DCN\u2019s attention mechanism is related to pixel-wise saliency measures used in visualizing neural networks (Simonyan et al., 2013).", "startOffset": 138, "endOffset": 161}, {"referenceID": 13, "context": "Other works such as matrix factorization (Jaderberg et al., 2014; Denton et al., 2014) and quantization schemes (Chen et al.", "startOffset": 41, "endOffset": 86}, {"referenceID": 6, "context": "Other works such as matrix factorization (Jaderberg et al., 2014; Denton et al., 2014) and quantization schemes (Chen et al.", "startOffset": 41, "endOffset": 86}, {"referenceID": 4, "context": ", 2014) and quantization schemes (Chen et al., 2010; J\u00e9gou et al., 2011; Gong et al., 2014) take the same computational shortcuts for all instances of the data.", "startOffset": 33, "endOffset": 91}, {"referenceID": 15, "context": ", 2014) and quantization schemes (Chen et al., 2010; J\u00e9gou et al., 2011; Gong et al., 2014) take the same computational shortcuts for all instances of the data.", "startOffset": 33, "endOffset": 91}, {"referenceID": 7, "context": ", 2014) and quantization schemes (Chen et al., 2010; J\u00e9gou et al., 2011; Gong et al., 2014) take the same computational shortcuts for all instances of the data.", "startOffset": 33, "endOffset": 91}, {"referenceID": 11, "context": "Our use of a regression cost for enforcing representations to be similar is related to previous work on model compression (Bucilu et al., 2006; Hinton et al., 2015; Romero et al., 2014).", "startOffset": 122, "endOffset": 185}, {"referenceID": 21, "context": "Our use of a regression cost for enforcing representations to be similar is related to previous work on model compression (Bucilu et al., 2006; Hinton et al., 2015; Romero et al., 2014).", "startOffset": 122, "endOffset": 185}, {"referenceID": 11, "context": ", 2006; Hinton et al., 2015; Romero et al., 2014). The goal of model compression is to train a small model (which is faster in deployment) to imitate a much larger model or an ensemble of models. Furthermore, Romero et al. (2014) have shown that middle layer hints can improve learning in deep and thin neural networks.", "startOffset": 8, "endOffset": 230}, {"referenceID": 18, "context": "To validate the effectiveness of our approach, we first investigate the Cluttered MNIST dataset (Mnih et al., 2014).", "startOffset": 96, "endOffset": 115}, {"referenceID": 19, "context": "We then apply our model in a transfer learning setting to a real-world object recognition task using the Street View House Numbers (SVHN) dataset (Netzer et al., 2011).", "startOffset": 146, "endOffset": 167}, {"referenceID": 18, "context": "We use the 100\u00d7 100 Cluttered MNIST digit classification dataset (Mnih et al., 2014).", "startOffset": 65, "endOffset": 84}, {"referenceID": 18, "context": "We use as baselines for our evaluation the coarse model (top layers applied only on coarse representations), the fine model (top layers applied only on fine representations), and we compare with previous attention-based models RAM (Mnih et al., 2014) and DRAW (Gregor et al.", "startOffset": 231, "endOffset": 250}, {"referenceID": 10, "context": ", 2014) and DRAW (Gregor et al., 2015).", "startOffset": 17, "endOffset": 38}, {"referenceID": 19, "context": "We tackle in this section a more challenging task of transcribing multi-digit sequences from natural images using the Street View House Numbers (SVHN) dataset (Netzer et al., 2011).", "startOffset": 159, "endOffset": 180}, {"referenceID": 8, "context": "(Goodfellow et al., 2013; Ba et al., 2014; Jaderberg et al., 2015), uses the location of digit bounding boxes as extra information.", "startOffset": 0, "endOffset": 66}, {"referenceID": 0, "context": "(Goodfellow et al., 2013; Ba et al., 2014; Jaderberg et al., 2015), uses the location of digit bounding boxes as extra information.", "startOffset": 0, "endOffset": 66}, {"referenceID": 14, "context": "(Goodfellow et al., 2013; Ba et al., 2014; Jaderberg et al., 2015), uses the location of digit bounding boxes as extra information.", "startOffset": 0, "endOffset": 66}, {"referenceID": 8, "context": "We follow the model proposed in (Goodfellow et al., 2013) for learning a probabilistic model of the digit sequence", "startOffset": 32, "endOffset": 57}, {"referenceID": 1, "context": "We would like to thank the developers of Theano (Bergstra et al., 2011; Bastien et al., 2012) and Blocks/Fuel (Van Merri\u00ebnboer et al.", "startOffset": 48, "endOffset": 93}], "year": 2016, "abstractText": "We introduce the Dynamic Capacity Network (DCN), a neural network that can adaptively assign its capacity across different portions of the input data. This is achieved by combining modules of two types: low-capacity subnetworks and high-capacity sub-networks. The low-capacity sub-networks are applied across most of the input, but also provide a guide to select a few portions of the input on which to apply the high-capacity sub-networks. The selection is made using a novel gradient-based attention mechanism, that efficiently identifies input regions for which the DCN\u2019s output is most sensitive and to which we should devote more capacity. We focus our empirical evaluation on the Cluttered MNIST and SVHN image datasets. Our findings indicate that DCNs are able to drastically reduce the number of computations, compared to traditional convolutional neural networks, while maintaining similar or even better performance.", "creator": "LaTeX with hyperref package"}}}