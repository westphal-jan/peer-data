{"id": "1511.08062", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Nov-2015", "title": "Relaxed Majorization-Minimization for Non-Smooth and Non-Convex Optimization", "abstract": "We propose a new majorization-minimization (MM) method for non-smooth and non-convex programs, which is general enough to include the existing MM methods. Besides the local majorization condition, we only require that the difference between the directional derivatives of the objective function and its surrogate function vanishes when the number of iterations approaches infinity, which is a very weak condition. So our method can use a surrogate function that directly approximates the non-smooth objective function. In comparison, all the existing MM methods construct the surrogate function by approximating the smooth component of the objective function. We apply our relaxed MM methods to the robust matrix factorization (RMF) problem with different regularizations, where our locally majorant algorithm shows advantages over the state-of-the-art approaches for RMF. This is the first algorithm for RMF ensuring, without extra assumptions, that any limit point of the iterates is a stationary point.", "histories": [["v1", "Wed, 25 Nov 2015 13:55:57 GMT  (939kb,D)", "http://arxiv.org/abs/1511.08062v1", "AAAI16"]], "COMMENTS": "AAAI16", "reviews": [], "SUBJECTS": "math.OC cs.LG cs.NA", "authors": ["chen xu", "zhouchen lin", "zhenyu zhao", "hongbin zha"], "accepted": true, "id": "1511.08062"}, "pdf": {"name": "1511.08062.pdf", "metadata": {"source": "CRF", "title": "Relaxed Majorization-Minimization for Non-smooth and Non-convex Optimization", "authors": ["Chen Xu", "Zhouchen Lin", "Zhenyu Zhao", "Hongbin Zha"], "emails": ["xuen@pku.edu.cn,", "zlin@pku.edu.cn,", "dwightzzy@gmail.com,", "zha@cis.pku.edu.cn"], "sections": [{"heading": "Introduction", "text": "Consider the following optimization problem: min x-C f (x), (1) where C is a closed convex subset in Rn and f (x): Rn \u2192 R is a continuous function that is limited below and could be uneven and non-convex. Often, f (x) can be divided as: f (x) = f (x) + f (x), (2) where f-x is differentiable and f-x (x) is not smooth. Such an optimization problem is ubiquitous, e.g. in statistics (Chen, 2012), computer vision and image processing (Bruckstein, Donoho and Elad, 2009; Ke and Kanade, 2005), data mining and machine learning (Pan et al., 2014; Kong, Ding and Huang, 2011)."}, {"heading": "Input: x0 \u2208 C.", "text": "1: not converged, but 2: construct a replacement function gk (x) of f (x) in the current iteration xk. 3: Minimize the surrogate to get the next iteration: xk + 1 = argminx \u0441C gk (x). 4: k \u2190 k + 1. 5: end whileOutput: The solution xk.2005), smoothing methods (Chen, 2012) and majority minimization (MM) (Hunter and Lange, 2004). In this paper we focus on the MM methods."}, {"heading": "Existing MM for Non-smooth and Non-convex Optimization", "text": "The MM methods consist conceptually of two steps (see algorithm 1). Firstly, construct a surrogate function gk (x) of f (x) at the current iteration xk. Secondly, minimize the surrogate method gk (x) to update x. The most popular choice of the surrogate method is crit-ar Xiv: 151 1.08 062v 1 [mat h.O C] 25 Nov 201 5ical for the efficiency of the solution (1) and also the quality of the solution. The most popular choice of the surrogate method is the class of \"first-order surrogates,\" the difference of which is distinguished from the objective function with a Lipschitz continuity gradient (Mairal C] 25 Nov 201 5ical for the efficiency of the solution (1) and the quality of the solution (surrogate).The most popular choice of the surrogate method is the class of \"first-order surrogates\" with a Lipschitz continuity gradient (Mairal C] 25 Nov 201 5ical for the efficiency of the solution with an objective gradient)."}, {"heading": "Contributions", "text": "The contributions of this paper are as follows: (a) We further loosen the condition of the difference between the lens and the surrogate. We only require that the directional derivative of the difference disappear as the number of iterations approaches infinity (see (8). Our even weaker condition ensures that the non-smooth and non-convex target can be directly approached. Our relaxed MM is generic enough to include the existing MM methods (Mairal, 2013; Razaviyayn, Hong and Luo, 2013). (b) We also propose the conditions that ensure that the iterates produced by our relaxed MM converge to stationary points2, even for general non-smooth and non-convex targets. (c) As a concrete example, we apply our relaxed MM to the robust matrix factoring (RMF) that limits our problem with different regulations."}, {"heading": "Our Relaxed MM", "text": "Before the introduction of our relaxed MM, we remember some definitions to be used later. (Definition) Definition (Sufficient Descent) {f (xk)} [f (xk)} [f (xk)] s (xk) s (xk) [f (xk)] s [f (xk) [f) s (xk) [f) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s [f) s (xk) s (xk) s (xk) s (xk) [f) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s) [xk) s (xk) s (xk) s (xk) s (xk) s (xk) [xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) s (xk) [xk) s (xk) s (xk) s (xk) s (xk) [xk) s (x"}, {"heading": "Solving Robust Matrix Factorization by Relaxed MM", "text": "It is widely used for the structure of movement (Tomasi and Kanade, 1992), clustering (Kong, Ding, and Huang, 2011), dictatorial learning (Mairal et al., 2010), etc. Normally, the aim is to minimize the error between the given matrix and the product of two matrices at the observed inputs. Therefore, such models are called robust matrix factorization (RMF), and their formulation is as follows: min U-Cu, V-Cv-W (M \u2212 UV T)."}, {"heading": "Minimizing the Surrogate Function", "text": "Now we show how to find the minimizer of the convex Gk (\u2206 U, \u2206 V) in (17), which can easily be done by using the linearized method of alternating direction with parallel division and adaptive punishment (LADMPSAP) (Lin, Liu, and Li, 2013). LADMPSAP fits the solution of the following linear delimited convex programs: min x1, \u00b7 \u00b7 \u00b7, xn \u00b2 j = 1 fj (xj), s.t. n \u00b2 j = 1 Aj (xj) = b, (20) where xj and b could be either vectors or matrices, fj is a proper convex function, and Aj is a linear mapping. To apply LADMPSAP, we first introduce an auxiliary matrix E so that E = M \u2212 UkY Tk \u2212 \u0445V \u2212 V \u00b2 V \u00b2 V \u00b2, and then a minimization of the Gk \u00b2 V \u00b2 t \u00b2 task (17 \u00b0 U \u00b2 K \u00b2 K) E."}, {"heading": "Experiments", "text": "In this section we compare our relaxed MM algorithms with state-of-the-art RMF algorithms: UNuBi (Cabral et al., 2013) for low matrix recovery and '1-NMF (Pan et al., 2014) for robust NMF. UNuBi code (Cabral et al., 2013) was kindly provided by the authors and we implemented the code of' 1-NMF (Pan et al., 2014) ourselves."}, {"heading": "Synthetic Data", "text": "We first conduct experiments with synthetic data. Here, we specify the regularization parameters \u03bbu = \u03bbv = 20 / (m + n) and stop our relaxed MM algorithms if the relative change in objective function is less than 10 \u2212 4.Low Rank Matrix Recovery: We generate a data matrix M = U0V T 0, where U0, R500 \u00d7 10 and V0, R500 \u00d7 10 i.e. are sampled from a Gaussian distribution N (0, 1). In addition, we corrupt 40% entries of M with outliers evenly distributed in [\u2212 10, 10] and select W with missing 80% data. The positions of both outliers and missing data are uniformly randomly selected. We initialize all comparative algorithms with the rank r truncation of the singular value decomposition of W M with outliers evenly distributed in [\u2212 10, 10]."}, {"heading": "Real Data", "text": "In this subsection, we conduct experiments on real data. As there is no basic truth, we measure the relative error of GMV (Mest \u2212 M), where # W is the number of entries observed. For NMF, W then becomes an all-one matrix consisting of 36 images with a resolution of 720 \u00d7 576 pixels. We select a portion of the raw feature points observed from at least 6 views (Fig. 3 (a)). The observed matrix is of size 72 \u00d7 557 with a missing data ratio of 79.5% and shows a diagonal pattern. We register the image origin to the image center (360, 288). We adopt the same initialization and parameter settings as synthetic data."}, {"heading": "Conclusions", "text": "Our relaxed MM is generic enough to include the existing MM methods. In particular, the non-smooth and non-convex objective function can be approached directly, which has never been done before. By looking at the RMF problems as examples, it can be shown that our on-site prevailing relaxed MM by far outperforms the most modern methods in both solution quality and convergence speed. We prove that the iterates converge to stationary points. To the best of our knowledge, this is the first convergence guarantee for variants of the RMF without additional assumptions."}, {"heading": "Acknowledgements", "text": "Zhouchen Lin is supported by the Chinese Basic Research Program (973 Program) (grant number 2015CB352502), the Chinese National Natural Science Foundation (NSFC) (grant numbers 61272341 and 61231002) and the Microsoft Research Asia Collaborative Research Program. Zhenyu Zhao is supported by the NSFC (grant number 61473302) and Hongbin Zha is supported by the 973 Program (grant number 2011CB302202)."}, {"heading": "Supplementary Material", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Proof of Proposition 1", "text": "Consider the minimization of gk (x) \u2212 f (x) in the neighborhood. It reaches the local minimum of 0 at x = xk. According to definition 3, we have: gk (xk; d), gk (xk; d), gk (xk; d), gk (xk; d), lk (x), gk (x), gk (x) \u2212 g, k (x) \u2212 x, 22, (24) Similarly, we have f (xk; d), lk (xk; d), gk (xk; d), gk (x) <. (25) If we compare gk (x) with lk (x), they are combined with two parts, the common part f-k (x) and the continuously differentiable part (xk; d), we get the function gk (x) \u2212 lk (x) & ltxk (x)."}, {"heading": "Proof of Theorem 1", "text": "If we look at the strongly convex replacement function (Mairal, 2013, Lemma B.5), we have the havegk (xk) \u2212 gk (xk + 1) \u2264 2 \u0445 xk \u2212 xk (xk), (29) we have havef (xk) \u2212 f (xk) \u2212 f (xk) \u2212 f (xk) \u2212 f (xk) \u2212 kk \u2212 kk \u2212 kk (30). If we look at the surrogate gk (x) = g \u00b2 kk (x) + kk (x) we have."}, {"heading": "Proof of Proposition 2", "text": "If we designate the I-th rows of \"U\" and \"V\" as \"U\" and \"V,\" we can consider \"W\" (\"U\" V \"T) as the i-th rows of\" U \"and\" V \"resp.\" V \"relax\"....... uTm \"(\" U \"V\" T) as the i-th rows of \"U\" and \"V.\" (37), where the inequality derives from the \"Cauchy-Schwartz inequality\" with \"W\" (i) +, \"i\" (i) = 1,., m and \"V\" = W (.) +, \"W\" (., j) +, \"V\" (i) +, \"n. The equality applies if and only if (U\" V \") = (0.0). Then we have\" G \"k\" (U \"V) +\" V, \"V,\" V \"V,\" V. \"(U\" K \"V.\" (U \"K\") as the i-th rows of \"U.\""}, {"heading": "Proof of Theorem 2", "text": "By combining Fk ((0.0) = Gk (0.0) and RMF-LMMM (0.0), both RMF-GMMM and RMF-LMMM can represent the minimizer of Gk (0.0; Du, Dv) \u2265 Fk (0.0; Du, Dv). (40) By combining Proposition 1 and 2, we can ensure the smoothness of the first order in Infinitylim k. (40) Furthermore, Gk (0.0; Du, Dv) \u2212 Gk (0.0; Du, Dv) = 0, Uk + Du Cu, Vk + Dv Cv. (41) The sequence of Gk (0.0; Du, Dv) \u2212 and the sequence (Uk, Dv) \u2212 and the sequence (Uk, Vv), the function of Vk (Vk) and the limitation of Vk (Vk), the point of Vk and the limitation of Vk (Vk), the limitation of Varik (Vk), the limitation of Vence, the limitation of Vk and the variance (Vk), the limitation of Vk and the variance (Vk)."}, {"heading": "Minimizing the surrogate by LADMPSAP", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Sketch of LADMPSAP", "text": "The LADMPSAP algorithm consists of the following steps (Lin, Liu and Li, 2013): (a) Update xj's (j = 1, \u00b7 \u00b7 \u00b7 \u00b7 \u00b7, n) parallel: xi + 1j = argmin xbogej (xj) + jack (jj) + jack (jack) + jack (jack) + jack (jack) + jack (jack) + jack (jack) + jack (jack), jack (jack), jack (jack), jack (jack) + jack (jack), jack (jack), jack (jack), jack (jack), jack (jack), jack (jack) + jack (jack), jack (jack) + jack (jack), jack (jack), jack (jack), jack (jack), jack (jack), jack (+ jack), jack (jack), jack (jack), jack (+ jack), jack (jack, jack (jack), jack (jack), jack (jack), jack (+ jack), jack (jack), jack (+ jack), jack (jack (jack), jack (jack), jack (jack), jack (+ jack), jack (jack), jack (jack), jack (jack, jack (+ jack), jack, jack (+ jack), jack (jack, jack, jack, jack, jack (+, jack), jack (+, jack), jack (jack), jack (, jack, jack (, jack), jack (, jack, jack, jack (+, jack, jack), jack (, jack, jack, jack, jack (+, jack), jack (, jack), jack (, jack, jack, jack, jack, jack, jack), jack ("}, {"heading": "The Optimization Using LADMPSAP", "text": "We aim to minimise the problem. \u2212 u \u2212 u \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V V \u00b2 V V \u00b2 V V \u00b2 V \u00b2 V V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V V \u00b2 V V \u00b2 V \u00b2 V V \u00b2 V V \u00b2 V \u00b2 V V \u00b2 V \u00b2 V \u00b2 V \u00b2 V V \u00b2 V \u00b2 V \u00b2 V V \u00b2 V \u00b2 V \u00b2 V \u00b2 V V \u00b2 V V V \u00b2 V \u00b2 V \u00b2 V V \u00b2 V V \u00b2 V \u00b2 V \u00b2 V V \u00b2 V V \u00b2 V \u00b2 V \u00b2 V V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V"}, {"heading": "Parameter Setting for LADMPSAP", "text": "Low Rank Matrix Recovery: We set \u03b51 = 10 \u2212 5, \u03b52 = 10 \u2212 4, \u03c10 = 1.5 and \u03b2max = 1010 as default values. Non-negative matrix factorization: we set \u03b51 = 10 \u2212 4, \u03b52 = 10 \u2212 4, \u03c10 = 3 and \u03b2max = 1010 as default values."}], "references": [{"title": "Convergence of descent methods for semi-algebraic and tame problems: proximal algorithms, forward\u2013backward splitting, and regularized Gauss\u2013 Seidel methods", "author": ["H. Attouch", "J. Bolte", "B.F. Svaiter"], "venue": "Mathematical Programming 137(1-2):91\u2013129.", "citeRegEx": "Attouch et al\\.,? 2013", "shortCiteRegEx": "Attouch et al\\.", "year": 2013}, {"title": "A fast iterative shrinkagethresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences 2(1):183\u2013202.", "citeRegEx": "Beck and Teboulle,? 2009", "shortCiteRegEx": "Beck and Teboulle", "year": 2009}, {"title": "Nonlinear Programming", "author": ["D. Bertsekas"], "venue": "Athena Scientific, 2nd edition.", "citeRegEx": "Bertsekas,? 1999", "shortCiteRegEx": "Bertsekas", "year": 1999}, {"title": "Convex analysis and nonlinear optimization: theory and examples", "author": ["J.M. Borwein", "A.S. Lewis"], "venue": "Springer Science & Business Media.", "citeRegEx": "Borwein and Lewis,? 2010", "shortCiteRegEx": "Borwein and Lewis", "year": 2010}, {"title": "From sparse solutions of systems of equations to sparse modeling of signals and images", "author": ["A.M. Bruckstein", "D.L. Donoho", "M. Elad"], "venue": "SIAM review 51(1):34\u201381.", "citeRegEx": "Bruckstein et al\\.,? 2009", "shortCiteRegEx": "Bruckstein et al\\.", "year": 2009}, {"title": "Damped Newton algorithms for matrix factorization with missing data", "author": ["A.M. Buchanan", "A.W. Fitzgibbon"], "venue": "CVPR, volume 2, 316\u2013322. IEEE.", "citeRegEx": "Buchanan and Fitzgibbon,? 2005", "shortCiteRegEx": "Buchanan and Fitzgibbon", "year": 2005}, {"title": "A robust gradient sampling algorithm for nonsmooth, nonconvex optimization", "author": ["J.V. Burke", "A.S. Lewis", "M.L. Overton"], "venue": "SIAM Journal on Optimization 15(3):751\u2013779.", "citeRegEx": "Burke et al\\.,? 2005", "shortCiteRegEx": "Burke et al\\.", "year": 2005}, {"title": "Unifying nuclear norm and bilinear factorization approaches for low-rank matrix decomposition", "author": ["R. Cabral", "F.D.L. Torre", "J.P. Costeira", "A. Bernardino"], "venue": "ICCV, 2488\u20132495. IEEE.", "citeRegEx": "Cabral et al\\.,? 2013", "shortCiteRegEx": "Cabral et al\\.", "year": 2013}, {"title": "Enhancing sparsity by reweighted `1 minimization", "author": ["E.J. Candes", "M.B. Wakin", "S.P. Boyd"], "venue": "Journal of Fourier Analysis and Applications 14(5-6):877\u2013905.", "citeRegEx": "Candes et al\\.,? 2008", "shortCiteRegEx": "Candes et al\\.", "year": 2008}, {"title": "Smoothing methods for nonsmooth, nonconvex minimization", "author": ["X. Chen"], "venue": "Mathematical Programming 134(1):71\u201399.", "citeRegEx": "Chen,? 2012", "shortCiteRegEx": "Chen", "year": 2012}, {"title": "Optimization and nonsmooth analysis, volume 5", "author": ["F.H. Clarke"], "venue": "SIAM.", "citeRegEx": "Clarke,? 1990", "shortCiteRegEx": "Clarke", "year": 1990}, {"title": "Efficient computation of robust weighted low-rank matrix approximations using the L1 norm. Pattern Analysis and Machine Intelligence, IEEE Transactions on 34(9):1681\u20131690", "author": ["A. Eriksson", "A. van den Hengel"], "venue": null, "citeRegEx": "Eriksson and Hengel,? \\Q2012\\E", "shortCiteRegEx": "Eriksson and Hengel", "year": 2012}, {"title": "Accelerating the Lee-Seung algorithm for non-negative matrix factorization", "author": ["E.F. Gonzalez", "Y. Zhang"], "venue": "Dept. Comput. & Appl. Math., Rice Univ., Houston, TX, Tech. Rep. TR-05-02.", "citeRegEx": "Gonzalez and Zhang,? 2005", "shortCiteRegEx": "Gonzalez and Zhang", "year": 2005}, {"title": "A tutorial on MM algorithms", "author": ["D.R. Hunter", "K. Lange"], "venue": "The American Statistician 58(1):30\u201337.", "citeRegEx": "Hunter and Lange,? 2004", "shortCiteRegEx": "Hunter and Lange", "year": 2004}, {"title": "Robust L1-norm factorization in the presence of outliers and missing data by alternative convex programming", "author": ["Q. Ke", "T. Kanade"], "venue": "CVPR, volume 1, 739\u2013746. IEEE.", "citeRegEx": "Ke and Kanade,? 2005", "shortCiteRegEx": "Ke and Kanade", "year": 2005}, {"title": "Robust nonnegative matrix factorization using L21-norm", "author": ["D. Kong", "C. Ding", "H. Huang"], "venue": "CIKM, 673\u2013682. ACM. Lee, D. D., and Seung, H. S. 2001. Algorithms for non-negative matrix factorization. In NIPS, 556\u2013562.", "citeRegEx": "Kong et al\\.,? 2011", "shortCiteRegEx": "Kong et al\\.", "year": 2011}, {"title": "The augmented Lagrange multiplier method for exact recovery of corrupted low-rank matrices", "author": ["Z. Lin", "M. Chen", "Y. Ma"], "venue": "arXiv preprint arXiv:1009.5055.", "citeRegEx": "Lin et al\\.,? 2010", "shortCiteRegEx": "Lin et al\\.", "year": 2010}, {"title": "Linearized alternating direction method with parallel splitting and adaptive penalty for separable convex programs in machine learning", "author": ["Z. Lin", "R. Liu", "H. Li"], "venue": "Machine Learning 1\u201339.", "citeRegEx": "Lin et al\\.,? 2013", "shortCiteRegEx": "Lin et al\\.", "year": 2013}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "The Journal of Machine Learning Research 11:19\u201360.", "citeRegEx": "Mairal et al\\.,? 2010", "shortCiteRegEx": "Mairal et al\\.", "year": 2010}, {"title": "Optimization with first-order surrogate functions", "author": ["J. Mairal"], "venue": "arXiv preprint arXiv:1305.3120.", "citeRegEx": "Mairal,? 2013", "shortCiteRegEx": "Mairal", "year": 2013}, {"title": "Survey of bundle methods for nonsmooth optimization", "author": ["M. M\u00e4kel\u00e4"], "venue": "Optimization Methods and Software 17(1):1\u201329.", "citeRegEx": "M\u00e4kel\u00e4,? 2002", "shortCiteRegEx": "M\u00e4kel\u00e4", "year": 2002}, {"title": "Large-scale matrix factorization with missing data under additional constraints", "author": ["K. Mitra", "S. Sheorey", "R. Chellappa"], "venue": "NIPS, 1651\u20131659.", "citeRegEx": "Mitra et al\\.,? 2010", "shortCiteRegEx": "Mitra et al\\.", "year": 2010}, {"title": "Robust nonnegative dictionary learning", "author": ["Q. Pan", "D. Kong", "C. Ding", "B. Luo"], "venue": "AAAI.", "citeRegEx": "Pan et al\\.,? 2014", "shortCiteRegEx": "Pan et al\\.", "year": 2014}, {"title": "A unified convergence analysis of block successive minimization methods for nonsmooth optimization", "author": ["M. Razaviyayn", "M. Hong", "Z.-Q. Luo"], "venue": "SIAM Journal on Optimization 23(2):1126\u2013 1153.", "citeRegEx": "Razaviyayn et al\\.,? 2013", "shortCiteRegEx": "Razaviyayn et al\\.", "year": 2013}, {"title": "Augmented Lagrangian alternating direction method for matrix separation based on lowrank factorization", "author": ["Y. Shen", "Z. Wen", "Y. Zhang"], "venue": "Optimization Methods and Software 29(2):239\u2013 263.", "citeRegEx": "Shen et al\\.,? 2014", "shortCiteRegEx": "Shen et al\\.", "year": 2014}, {"title": "Shape and motion from image streams under orthography: a factorization method", "author": ["C. Tomasi", "T. Kanade"], "venue": "International Journal of Computer Vision 9(2):137\u2013154.", "citeRegEx": "Tomasi and Kanade,? 1992", "shortCiteRegEx": "Tomasi and Kanade", "year": 1992}, {"title": "Practical low-rank matrix approximation under robust L1-norm", "author": ["Y. Zheng", "G. Liu", "S. Sugimoto", "S. Yan", "M. Okutomi"], "venue": "CVPR, 1410\u20131417. IEEE.", "citeRegEx": "Zheng et al\\.,? 2012", "shortCiteRegEx": "Zheng et al\\.", "year": 2012}, {"title": "Proof of Theorem 1 Consider the \u03c1-strongly convex surrogate gk(x). As xk+1 = arg minx\u2208C gk(x), by the definition of strongly convex function (Mairal", "author": ["\u2207f(xk", "\u2200 xk+d \u2208 C"], "venue": "Lemma B.5),", "citeRegEx": "\u2207f.xk and C,? \\Q2013\\E", "shortCiteRegEx": "\u2207f.xk and C", "year": 2013}], "referenceMentions": [{"referenceID": 9, "context": ", in statistics (Chen, 2012), computer vision and image processing (Bruckstein, Donoho, and Elad, 2009; Ke and Kanade, 2005), data mining and machine learning (Pan et al.", "startOffset": 16, "endOffset": 28}, {"referenceID": 14, "context": ", in statistics (Chen, 2012), computer vision and image processing (Bruckstein, Donoho, and Elad, 2009; Ke and Kanade, 2005), data mining and machine learning (Pan et al.", "startOffset": 67, "endOffset": 124}, {"referenceID": 22, "context": ", in statistics (Chen, 2012), computer vision and image processing (Bruckstein, Donoho, and Elad, 2009; Ke and Kanade, 2005), data mining and machine learning (Pan et al., 2014; Kong, Ding, and Huang, 2011).", "startOffset": 159, "endOffset": 206}, {"referenceID": 10, "context": "Typical methods include subdifferential (Clarke, 1990), bundle methods (M\u00e4kel\u00e4, 2002), gradient sampling (Burke, Lewis, and Overton,", "startOffset": 40, "endOffset": 54}, {"referenceID": 20, "context": "Typical methods include subdifferential (Clarke, 1990), bundle methods (M\u00e4kel\u00e4, 2002), gradient sampling (Burke, Lewis, and Overton,", "startOffset": 71, "endOffset": 85}, {"referenceID": 19, "context": "#1 represents globally majorant MM in (Mairal, 2013), #2 represents strongly convex MM in (Mairal, 2013), #3 represents successive MM in (Razaviyayn, Hong, and Luo, 2013) and #4 represents our relaxed MM.", "startOffset": 38, "endOffset": 52}, {"referenceID": 19, "context": "#1 represents globally majorant MM in (Mairal, 2013), #2 represents strongly convex MM in (Mairal, 2013), #3 represents successive MM in (Razaviyayn, Hong, and Luo, 2013) and #4 represents our relaxed MM.", "startOffset": 90, "endOffset": 104}, {"referenceID": 9, "context": "2005), smoothing methods (Chen, 2012), and majorizationminimization (MM) (Hunter and Lange, 2004).", "startOffset": 25, "endOffset": 37}, {"referenceID": 13, "context": "2005), smoothing methods (Chen, 2012), and majorizationminimization (MM) (Hunter and Lange, 2004).", "startOffset": 73, "endOffset": 97}, {"referenceID": 19, "context": "Mairal (2013) has given a comprehensive review on MM.", "startOffset": 0, "endOffset": 14}, {"referenceID": 19, "context": "The most popular choice of surrogate is the class of \u201cfirst order surrogates\u201d, whose difference from the objective function is differentiable with a Lipschitz continuous gradient (Mairal, 2013).", "startOffset": 179, "endOffset": 193}, {"referenceID": 19, "context": "In parallel to (Mairal, 2013), Razaviyayn, Hong, and Luo (2013) also showed that many popular methods for minimizing non-smooth functions could be regarded as MM methods.", "startOffset": 15, "endOffset": 29}, {"referenceID": 19, "context": "Different from (Mairal, 2013), they suggested using the directional derivative to ensure the first order smoothness between the objective and the surrogate, which is weaker than the condition in (Mairal, 2013) that the difference between the objective and the surrogate should be smooth.", "startOffset": 15, "endOffset": 29}, {"referenceID": 19, "context": "Different from (Mairal, 2013), they suggested using the directional derivative to ensure the first order smoothness between the objective and the surrogate, which is weaker than the condition in (Mairal, 2013) that the difference between the objective and the surrogate should be smooth.", "startOffset": 195, "endOffset": 209}, {"referenceID": 19, "context": "In parallel to (Mairal, 2013), Razaviyayn, Hong, and Luo (2013) also showed that many popular methods for minimizing non-smooth functions could be regarded as MM methods.", "startOffset": 16, "endOffset": 64}, {"referenceID": 19, "context": "In parallel to (Mairal, 2013), Razaviyayn, Hong, and Luo (2013) also showed that many popular methods for minimizing non-smooth functions could be regarded as MM methods. They proposed the block coordinate descent method, where the traditional MM could be regarded as a special case by gathering all variables in one block. Different from (Mairal, 2013), they suggested using the directional derivative to ensure the first order smoothness between the objective and the surrogate, which is weaker than the condition in (Mairal, 2013) that the difference between the objective and the surrogate should be smooth. However, Razaviyayn, Hong, and Luo (2013) only discussed the choice of surrogates by approximating f(x) as (3).", "startOffset": 16, "endOffset": 654}, {"referenceID": 19, "context": "Our relaxed MM is general enough to include the existing MM methods (Mairal, 2013; Razaviyayn, Hong, and Luo, 2013).", "startOffset": 68, "endOffset": 115}, {"referenceID": 19, "context": "We can see that ours is general enough to include existing works (Mairal, 2013; Razaviyayn, Hong, and Luo, 2013).", "startOffset": 65, "endOffset": 112}, {"referenceID": 19, "context": "In the traditional MM, the global majorization condition (Mairal, 2013; Razaviyayn, Hong, and Luo, 2013) is assumed:", "startOffset": 57, "endOffset": 104}, {"referenceID": 19, "context": "So a globally majorant surrogate is likely to produce an inferior solution and converges slower than a locally majorant one ((Mairal, 2013) and our experiments).", "startOffset": 125, "endOffset": 139}, {"referenceID": 19, "context": "These two cases have been discussed in the literature (Mairal, 2013; Razaviyayn, Hong, and Luo, 2013).", "startOffset": 54, "endOffset": 101}, {"referenceID": 19, "context": "Mairal (Mairal, 2013)", "startOffset": 7, "endOffset": 21}, {"referenceID": 25, "context": "It is widely used for structure from motion (Tomasi and Kanade, 1992), clustering (Kong, Ding, and Huang, 2011), dictionary learning (Mairal et al.", "startOffset": 44, "endOffset": 69}, {"referenceID": 18, "context": "It is widely used for structure from motion (Tomasi and Kanade, 1992), clustering (Kong, Ding, and Huang, 2011), dictionary learning (Mairal et al., 2010), etc.", "startOffset": 133, "endOffset": 154}, {"referenceID": 14, "context": ", low-rank matrix recovery (Ke and Kanade, 2005), non-negative matrix factorization (NMF) (Lee and Seung, 2001), and dictionary learning (Mairal et al.", "startOffset": 27, "endOffset": 48}, {"referenceID": 18, "context": ", low-rank matrix recovery (Ke and Kanade, 2005), non-negative matrix factorization (NMF) (Lee and Seung, 2001), and dictionary learning (Mairal et al., 2010).", "startOffset": 137, "endOffset": 158}, {"referenceID": 1, "context": "In RMF-LMMM, \u03c1u and \u03c1v are instead initialized with relatively small values and then increase gradually, using the line search technique in (Beck and Teboulle, 2009) to ensure the locally majorant condition (7).", "startOffset": 140, "endOffset": 165}, {"referenceID": 5, "context": "When the error is measured by the squared Frobenius norm, many algorithms have been proposed (Buchanan and Fitzgibbon, 2005; Mitra, Sheorey, and Chellappa, 2010).", "startOffset": 93, "endOffset": 161}, {"referenceID": 2, "context": "They minimized U and V alternatively, which could easily get stuck at non-stationary points (Bertsekas, 1999).", "startOffset": 92, "endOffset": 109}, {"referenceID": 26, "context": "Some researches (Zheng et al., 2012; Cabral et al., 2013) further extended that by adding different regularizations on (U, V ) and achieved state-of-the-art performance.", "startOffset": 16, "endOffset": 57}, {"referenceID": 7, "context": "Some researches (Zheng et al., 2012; Cabral et al., 2013) further extended that by adding different regularizations on (U, V ) and achieved state-of-the-art performance.", "startOffset": 16, "endOffset": 57}, {"referenceID": 7, "context": "In this paper, we adopt the same formulation as (Cabral et al., 2013):", "startOffset": 48, "endOffset": 69}, {"referenceID": 4, "context": "When the error is measured by the squared Frobenius norm, many algorithms have been proposed (Buchanan and Fitzgibbon, 2005; Mitra, Sheorey, and Chellappa, 2010). For robustness, Ke and Kanade (2005) proposed to adopt the `1-norm.", "startOffset": 94, "endOffset": 200}, {"referenceID": 2, "context": "They minimized U and V alternatively, which could easily get stuck at non-stationary points (Bertsekas, 1999). So Eriksson and van den Hengel (2012) represented V implicitly with U and extended the Wiberg Algorithm to `1-norm.", "startOffset": 93, "endOffset": 149}, {"referenceID": 2, "context": "They minimized U and V alternatively, which could easily get stuck at non-stationary points (Bertsekas, 1999). So Eriksson and van den Hengel (2012) represented V implicitly with U and extended the Wiberg Algorithm to `1-norm. They only proved the convergence of the objective function value, not the sequence {(Uk, Vk)} itself. Moreover, they had to assume that the dependence of V on U is differentiable, which is unlikely to hold everywhere. Additionally, as it unfolds matrix U into a vector and adopt an () its memory requirement is very high, which prevents it from large scale computation. Recently, ADMM was used for matrix recovery. By assuming that the variables are bounded and convergent, Shen, Wen, and Zhang (2014) proved that any accumulation point of their algorithm is the Karush-Kuhn-Tucker (KKT) point.", "startOffset": 93, "endOffset": 729}, {"referenceID": 5, "context": "where the regularizers \u2016U\u2016F and \u2016V \u2016F are for reducing the solution space (Buchanan and Fitzgibbon, 2005).", "startOffset": 74, "endOffset": 105}, {"referenceID": 22, "context": "Recently, Pan et al. (2014) further introduced the `1-norm to handle outliers in non-negative dictionary learning, resulting in the following model:", "startOffset": 10, "endOffset": 28}, {"referenceID": 12, "context": "However, Gonzalez and Zhang (2005) pointed out that with such a multiplicative updating scheme is hard to reach the convergence condition even on toy data.", "startOffset": 9, "endOffset": 35}, {"referenceID": 7, "context": "Experiments In this section, we compare our relaxed MM algorithms with state-of-the-art RMF algorithms: UNuBi (Cabral et al., 2013) for low rank matrix recovery and `1-NMF (Pan et al.", "startOffset": 110, "endOffset": 131}, {"referenceID": 22, "context": ", 2013) for low rank matrix recovery and `1-NMF (Pan et al., 2014) for robust NMF.", "startOffset": 48, "endOffset": 66}, {"referenceID": 7, "context": "The code of UNuBi (Cabral et al., 2013) was kindly provided by the authors.", "startOffset": 18, "endOffset": 39}, {"referenceID": 22, "context": "We implemented the code of `1-NMF (Pan et al., 2014) ourselves.", "startOffset": 34, "endOffset": 52}, {"referenceID": 7, "context": "(a) The locally majorant MM, RMFLMMM, gets better solution in much less iterations than the globally majorant, RMF-GMMM, and the state-of-the-art algorithm, UNuBi (Cabral et al., 2013).", "startOffset": 163, "endOffset": 184}, {"referenceID": 22, "context": "(b) RMF-LMMM outperforms the globally majorant MM, RMF-GMMM, and the state-of-the-art algorithm (also globally majorant), `1NMF (Pan et al., 2014).", "startOffset": 128, "endOffset": 146}, {"referenceID": 25, "context": "Low Rank Matrix Recovery: Tomasi and Kanade (1992) first modelled the affine rigid structure from motion as a rank-4 matrix recovery problem.", "startOffset": 26, "endOffset": 51}, {"referenceID": 7, "context": "(b-d) Full tracks reconstructed by UNuBi (Cabral et al., 2013), RMF-GMMM, and RMF-LMMM, respectively.", "startOffset": 41, "endOffset": 62}, {"referenceID": 22, "context": "Non-negative Matrix Factorization: We test the performance of robust NMF by clustering (Kong, Ding, and Huang, 2011; Pan et al., 2014).", "startOffset": 87, "endOffset": 134}, {"referenceID": 22, "context": "The evaluation metrics we use here are accuracy (ACC), normalized mutual information (NMI) and purity (PUR) (Kong, Ding, and Huang, 2011; Pan et al., 2014).", "startOffset": 108, "endOffset": 155}], "year": 2015, "abstractText": "We propose a new majorization-minimization (MM) method for non-smooth and non-convex programs, which is general enough to include the existing MM methods. Besides the local majorization condition, we only require that the difference between the directional derivatives of the objective function and its surrogate function vanishes when the number of iterations approaches infinity, which is a very weak condition. So our method can use a surrogate function that directly approximates the non-smooth objective function. In comparison, all the existing MM methods construct the surrogate function by approximating the smooth component of the objective function. We apply our relaxed MM methods to the robust matrix factorization (RMF) problem with different regularizations, where our locally majorant algorithm shows great advantages over the state-of-the-art approaches for RMF. This is the first algorithm for RMF ensuring, without extra assumptions, that any limit point of the iterates is a stationary point. Introduction Consider the following optimization problem:", "creator": "TeX"}}}