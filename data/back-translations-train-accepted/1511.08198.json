{"id": "1511.08198", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Nov-2015", "title": "Towards Universal Paraphrastic Sentence Embeddings", "abstract": "In this paper, we show how to create paraphrastic sentence embeddings using the Paraphrase Database (Ganitkevitch et al., 2013), an extensive semantic resource with millions of phrase pairs. We consider several compositional architectures and evaluate them on 24 textual similarity datasets encompassing domains such as news, tweets, web forums, news headlines, machine translation output, glosses, and image and video captions. We present the interesting result that simple compositional architectures based on updated vector averaging vastly outperform long short-term memory (LSTM) recurrent neural networks and that these simpler architectures allow us to learn models with superior generalization. Our models are efficient, very easy to use, and competitive with task-tuned systems. We make them available to the research community with the hope that they can serve as the new baseline for further work on universal paraphrastic sentence embeddings.", "histories": [["v1", "Wed, 25 Nov 2015 20:52:15 GMT  (28kb)", "http://arxiv.org/abs/1511.08198v1", "Under review as a conference paper at ICLR 2016"], ["v2", "Tue, 12 Jan 2016 20:59:39 GMT  (32kb)", "http://arxiv.org/abs/1511.08198v2", "Under review as a conference paper at ICLR 2016"], ["v3", "Fri, 4 Mar 2016 20:54:30 GMT  (40kb)", "http://arxiv.org/abs/1511.08198v3", "Published as a conference paper at ICLR 2016"]], "COMMENTS": "Under review as a conference paper at ICLR 2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["john wieting", "mohit bansal", "kevin gimpel", "karen livescu"], "accepted": true, "id": "1511.08198"}, "pdf": {"name": "1511.08198.pdf", "metadata": {"source": "CRF", "title": "TOWARDS UNIVERSAL PARAPHRASTIC SENTENCE EMBEDDINGS", "authors": ["John Wieting", "Mohit Bansal Kevin Gimpel", "Karen Livescu"], "emails": ["jwieting@ttic.edu", "mbansal@ttic.edu", "kgimpel@ttic.edu", "klivescu@ttic.edu"], "sections": [{"heading": null, "text": "ar Xiv: 151 1,08 198v 1 [cs.C L] 25 Nov 201 5"}, {"heading": "1 INTRODUCTION", "text": "We have developed several researchers and used shared word embeddings trained on large datasets (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014), and these have been effectively used for many downstream tasks (Turian et al., 2010; Socher et al., 2011; Kim, 2014; Bansal et al., 2014; Tai et al., 2015). There has also been recent work on creating representations for word sequences such as phrases or sentences. Many functional architectures have been proposed for model compositions in language based on those based on simple operations such as complement (Mitchell & Lapata, 2010; Iyyer et al., 2015) to those based on richly structured functions such as recursive neural networks (Socher et al., 2011), evolutionary neural networks (Kalchenner et al, 2014), and networks."}, {"heading": "2 RELATED WORK", "text": "Mitchell & Lapata (2008; 2010) looks at Bigram compositionality and compares many functions to compose two word vectors into a single vector to represent their Bigram. Further work by Blacoe & Lapata (2012) has again found that simple operations such as vector addition are strongly performed; many other compositional architectures have been proposed, often using neural networks, including neural bag-of-words (NBOW) models (Kalchbrenner et al., 2014), deep NBOW models (Iyyer et al., 2015), property-weighted avoidance (Yu & Dredze, 2015), recursive neural networks based on parse structure (Socher et al., 2011; I-rsoy & Carhieral, 2014; Wieting, 2015)."}, {"heading": "3 MODELS AND TRAINING", "text": "Our goal is to embed sequences in a low-dimensional space so that cosine similarity in space corresponds to the strength of the paraphrase relationship between the sequences. We experimented with three models with varying degrees of complexity. The simplest model embeds a word sequence x = < x1, x2,..., xn > by averaging the vectors of its tokens. The only parameters learned by this model are the word embedding matrix Ww: gPARAGRAM-PHRASE (x) = 1nn, iW xiwwhere W xiw is the word embedding for word x. We call the learned embedding PARAGRAMPHRASE last embedding Ww: In our second model we learn a projection in addition to the word embedding: gproj (x) = Wp (1nn, iW xiw)."}, {"heading": "3.1 TRAINING", "text": "The training data consists of (possibly noisy) pairs taken directly from the original Paraphrase Database (PPDB) and optimizes marginalized loss.Our training data consists of a set of X phrase pairs < x1, x2 >, where x1 and x2 are assumed to be paraphrases.The objective function results from: min Wc, Ww1 | (\u2211 < x1, x2 > x max (0, \u03b4 \u2212 cos (g (x1), g (x2)) + cos (g (x1), g (t1), g (g (x1)) + cos (g (x2)) + cos (g (x2), g (t2), g (t2), g (t2)), g (negative), g (t2) and tw (t2)."}, {"heading": "3.1.1 SELECTING NEGATIVE EXAMPLES", "text": "To select t1 and t2 in Equation 1, we choose between two approaches: the first, MAX, simply selects the most similar phrase in a set of phrases (except for those in the given phrase pair); to maintain simplicity and reduce the number of tunable parameters, we use the mini-batch, but this could be a separate sentence; for example, we choose t1 for a given < x1, x2 >: t1 = argmax t: < t, \u00b7 < x1, x2 >} cos (g (x1), where Xb X is the current mini-batch; that is, we choose a negative example ti that is similar to xi according to current model parameters; the flip side of this approach is that we occasionally select a set ti that is actually a true paraphrase of xi.The second strategy selects negative examples with probability 0.5 and randomly selects them from the mini-batch strategy that we otherwise agree on in our batch-I strategy."}, {"heading": "4 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 DATA", "text": "We are experimenting on 24 textual similarity datasets covering many areas, including all datasets from each SemEval semantic textual similarity (STS) task (2012-2015); we are also evaluating on the SemEval 2015 Twitter task (Xu et al., 2015b) and the SemEval 2014 Semantic Relatedness task (Marelli et al., 2014), as well as two tasks using PPPDB data (Wieting et al., 2015; Pavlick et al., 2015).The first STS task was held in 2012 and they have been held every year since. Given two sets, the goal of the task is to predict how similar they are on a scale of 0-5, where 0 indicate the sentences on different topics and 5 indicate that they are fully equivalent. Each STS task consists of 4-6 different datasets and the tasks cover a wide variety of domains that we have categorized. Most submissions for these tasks use models that are monitored and prepared for similar data."}, {"heading": "4.2 EXPERIMENTAL SETTINGS", "text": "We used the XL section of the PPDB as the training data. (7) However, due to the long training time of the LSTM model, we used 500k examples sampled from the PPDB XXL for tuning purposes to train for 5 epochs. Then, after finding the hyperparameters that initiate Spearman's behavior in the PPDB task by Pavlick et al. (2015), we trained models across the XL section of the PPDB for 10 epochs. We use PARAGRAM-SL999 embeddings to initialize the word embedding matrix (Ww) for all models. The reason why we chose the task of Pavlick et al. for tuning is that we wanted to apply our entire procedure only to PPDB and not use other resources. So we looked at commented data sets composed exclusively of phrases in the PPDB. (WPDB) We chose this dataset over Annotated Pet al."}, {"heading": "4.3 RESULTS", "text": "The results of all STS tasks as well as the SICK and Twitter tasks are presented in Table 2. We also include the results of the PPDB tasks in Table 3. We report on the performance of our four models: 7PPDB comes in different sizes (S, M, L, XL, XXL and XXXL), where each larger size includes all the smaller ones. Phrases are sorted by a measure of confidence, so the smaller sentences contain higher precision paraphrases. 8For them we searched about {10 \u2212 4, 10 \u2212 5, 10 \u2212 6}, for b we searched about {25, 50, 100}, for which we searched about {10 \u2212 6 \u2212 7, 10 \u2212 8} as well as the setting in which we do not update Ww, and for which we searched about {0,4, 0,6}.PARAGRAM bases (PP), projection (LSTM), LSTM with output."}, {"heading": "4.4 USING REPRESENTATIONS FOR DOWNSTREAM TASKS", "text": "We explore two natural questions regarding our representations that we learned from PPDB: (1) can these embedding enhance the performance of other models through initialization and regularization? (2) can they effectively be used as attributes for downstream textual similarity tasks? To address these questions, we evaluate training data using the Stanford CoreNLP tokenizer (Manning et al., 2014) rather than the included NLTK (Bird et al., 2009) tokenizer. We found that this is the performance of training data using the Stanford CoreNLP tokenizer (Manning et al., 2014) rather than the included NLTK (Bird et al., 2009) tokenizer. We found that this is the performance of skip thought vectors.10 We used the publicly available 300-dimensional vectors trained on the 840 billion tokens common crawl pus corpus."}, {"heading": "4.4.1 REGULARIZATION AND INITIALIZATION TO IMPROVE TEXTUAL SIMILARITY MODELS", "text": "We initialize each model on the basis of the parameters we learned from the PPDB (calling them universal parameters) and extend Equation 4.4 by three separate regularization terms with the following weights: 11It has been shown that this objective function works very strongly for text similarity tasks, significantly better than square or absolute errors. 12Word embeddings are regulated according to their initial state. 13All models tuned to batch size above {25, 50, 50, 100, 100}, output dimensions above {50, 150, 300}, \u03bbc above {10 \u2212 3, 10 \u2212 4, 10 \u2212 5, 10 \u2212 6} have regularizations to {10 \u2212 3, 10 \u2212 4, 10 \u2212 5, 10 \u2212 6, 10 \u2212 7, 10 \u2212 8} as well as the option not to update the embeddings for the projection and LSTM models (LSTM models) specified on the same scale as the STM and LM models."}, {"heading": "4.4.2 REPRESENTATIONS AS FEATURES", "text": "We use a structure similar to that of Kiros et al. (2015), where we encode the sentences with our models and then simply learn the classification parameters without updating the embedding or compositional model parameters. 15 Our results (last row of Table 4) are not as strong in this task as the skipped vectors, whose best model (which has no additional features) has a Pearsons r of 0.8584. However, at 300 dimensions, our representations are significantly smaller compared to the 4800 dimensional skipped vectors, and we use a much simpler architecture to get to them."}, {"heading": "5 DISCUSSION", "text": "The first hypothesis we are testing is based on length. Since PPDB contains short snippets of text of a few words, the LSTM may not know how to handle the longer sentences that occur in our assessment tasks. If true, the LSTM \u2212 \u2212 \u2212 would be much better based on short snippets of text and their performance as their length increases. To test this hypothesis, we took all 12,108 pairs from the 20 SemEval STS tasks and examined them by length. 16 We then calculated Pearson's r for each bin. The results are shown in Table 5 and show that while the LSTM models work better on the shortest text pairs, they are still surpassed, at all lengths, by the PARAGRAM-PHRS-PHRASS3 models."}, {"heading": "6 QUALITATIVE ANALYSIS", "text": "If we take a closer look at our PARAGRAM-PHRASE-references, we can see that most terms, such as poverty, poverty, humanity, 18, and that most people who enter the world, get involved in the world in a different way to the world they live in, in which they live, in which they live. It is not so that they get involved in the world, in which they live, but rather in the world in which they live. It is not so that they get involved in the world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live which they live, in which they live, in which they live, in which they live which they live, in which they live, in which they live, in which they live which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which"}, {"heading": "7 CONCLUSION", "text": "We have introduced an approach to create universal paraphrase embeddings that exceed various strong baselines in many text similarity tasks and many areas. We are not aware of any other paraphrase model that works nearly as well as our PARAGRAM-PHRASE embeddings. In addition, our representations are very easy to use. They do not require the use of a neural network architecture, instead, the embeddings can be downloaded and then grouped for a specific sentence within an application to create the phrase / sentence embeddings. In addition, we find that our representations can improve general text similarity models as we show significant improvements in text delivery, projection and LSTM models by initializing and regulating our representations."}, {"heading": "APPENDIX A UNDER-TRAINED EMBEDDINGS", "text": "One limitation of our new PARAGRAM-PHRASE vectors is that many of our embeddings are insufficiently formed, and the number of unique tokens that occur in our training data, PPDB XL, is 37,366. However, the number of tokens that occur more than 100 times is only 7,113. Therefore, a clear source of improvement to our model would be to look at insufficiently trained embeddings for tokens that occur in our test data. To assess the effect these embeddings have on our model, we calculate the word share in each of our 22 SemEval data sets that does not occur at least 100 times in PPDB XL, together with our performance deviation from the 75th percentile of each data set. The results are in Table 6. Interestingly, although the quality of the submissions has improved over the years (this can be seen from the performance statistics for similar data sets in Table 2), there is a clear trend."}, {"heading": "APPENDIX B ADDING MORE PPDB", "text": "In fact, most of them are able to survive on their own, without being held accountable."}], "references": [{"title": "Semeval-2012 task 6: A pilot on semantic textual similarity", "author": ["Agirre", "Eneko", "Diab", "Mona", "Cer", "Daniel", "Gonzalez-Agirre", "Aitor"], "venue": "In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation,", "citeRegEx": "Agirre et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2012}, {"title": "Semeval-2014 task 10: Multilingual semantic textual similarity", "author": ["Agirre", "Eneko", "Banea", "Carmen", "Cardie", "Claire", "Cer", "Daniel", "Diab", "Mona", "Gonzalez-Agirre", "Aitor", "Guo", "Weiwei", "Mihalcea", "Rada", "Rigau", "German", "Wiebe", "Janyce"], "venue": "In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "Agirre et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2014}, {"title": "Semeval-2015 task 2: Semantic textual similarity, english, spanish and pilot on interpretability", "author": ["Agirre", "Eneko", "Banea", "Carmen"], "venue": "In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "Agirre et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2015}, {"title": "Tailoring continuous word representations for dependency parsing", "author": ["Bansal", "Mohit", "Gimpel", "Kevin", "Livescu", "Karen"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Bansal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "Arabic diacritization with recurrent neural networks", "author": ["Belinkov", "Yonatan", "Glass", "James"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Belinkov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Belinkov et al\\.", "year": 2015}, {"title": "Natural language processing with Python. ", "author": ["Bird", "Steven", "Klein", "Ewan", "Loper", "Edward"], "venue": "O\u2019Reilly Media,", "citeRegEx": "Bird et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bird et al\\.", "year": 2009}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["Blacoe", "William", "Lapata", "Mirella"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Blacoe et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Blacoe et al\\.", "year": 2012}, {"title": "Long short-term memory neural networks for chinese word segmentation", "author": ["Chen", "Xinchi", "Qiu", "Xipeng", "Zhu", "Chenxi", "Liu", "Pengfei", "Huang", "Xuanjing"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Sentence modeling with gated recursive neural network", "author": ["Chen", "Xinchi", "Qiu", "Xipeng", "Zhu", "Chenxi", "Wu", "Shiyu", "Huang", "Xuanjing"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["Collobert", "Ronan", "Weston", "Jason", "Bottou", "L\u00e9on", "Karlen", "Michael", "Kavukcuoglu", "Koray", "Kuksa", "Pavel"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Beetle ii: a system for tutoring and computational linguistics experimentation", "author": ["Dzikovska", "Myroslava O", "Moore", "Johanna D", "Steinhauser", "Natalie", "Campbell", "Gwendolyn", "Farrow", "Elaine", "Callaway", "Charles B"], "venue": "In Proceedings of the ACL 2010 System Demonstrations,", "citeRegEx": "Dzikovska et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dzikovska et al\\.", "year": 2010}, {"title": "Sentence compression by deletion with lstms", "author": ["Filippova", "Katja", "Alfonseca", "Enrique", "Colmenares", "Carlos A", "Kaiser", "Lukasz", "Vinyals", "Oriol"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Filippova et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Filippova et al\\.", "year": 2015}, {"title": "Learning precise timing with lstm recurrent networks", "author": ["Gers", "Felix A", "Schraudolph", "Nicol N", "Schmidhuber", "J\u00fcrgen"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Gers et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2003}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves", "Alan", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Unconstrained on-line handwriting recognition with recurrent neural networks", "author": ["Graves", "Alex", "Liwicki", "Marcus", "Bunke", "Horst", "Schmidhuber", "J\u00fcrgen", "Fern\u00e1ndez", "Santiago"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Graves et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2008}, {"title": "Lstm: A search space odyssey", "author": ["Greff", "Klaus", "Srivastava", "Rupesh Kumar", "Koutn\u0131\u0301k", "Jan", "Steunebrink", "Bas R", "Schmidhuber", "J\u00fcrgen"], "venue": "arXiv preprint arXiv:1503.04069,", "citeRegEx": "Greff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "Multi-perspective sentence similarity modeling with convolutional neural networks", "author": ["He", "Hua", "Gimpel", "Kevin", "Lin", "Jimmy"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["Hermann", "Karl Moritz", "Ko\u010disk\u00fd", "Tom\u00e1\u0161", "Grefenstette", "Edward", "Espeholt", "Lasse", "Kay", "Will", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Hu", "Baotian", "Lu", "Zhengdong", "Li", "Hang", "Chen", "Qingcai"], "venue": "In Advances in Neural Information Processing Systems, pp. 2042\u20132050,", "citeRegEx": "Hu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["\u0130rsoy", "Ozan", "Cardie", "Claire"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "\u0130rsoy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "\u0130rsoy et al\\.", "year": 2014}, {"title": "A convolutional neural network for modelling sentences. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["Kalchbrenner", "Nal", "Grefenstette", "Edward", "Blunsom", "Phil"], "venue": null, "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Kim", "Yoon"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Kim and Yoon.,? \\Q2014\\E", "shortCiteRegEx": "Kim and Yoon.", "year": 2014}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Quoc V", "Mikolov", "Tomas"], "venue": "arXiv preprint arXiv:1405.4053,", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "A hierarchical neural autoencoder for paragraphs and documents", "author": ["Li", "Jiwei", "Luong", "Minh-Thang", "Jurafsky", "Dan"], "venue": "arXiv preprint arXiv:1506.01057,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "A hierarchical neural autoencoder for paragraphs and documents. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)", "author": ["Li", "Jiwei", "Luong", "Thang", "Jurafsky", "Dan"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Ling", "Wang", "Dyer", "Chris", "Black", "Alan W", "Trancoso", "Isabel", "Fermandez", "Ramon", "Amir", "Silvio", "Marujo", "Luis", "Tiago"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Multi-timescale long short-term memory neural network for modelling sentences and documents", "author": ["Liu", "Pengfei", "Qiu", "Xipeng", "Chen", "Xinchi", "Wu", "Shiyu", "Huang", "Xuanjing"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Manning", "Christopher D", "Surdeanu", "Mihai", "Bauer", "John", "Finkel", "Jenny", "Bethard", "Steven J", "McClosky", "David"], "venue": "In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment", "author": ["Marelli", "Marco", "Bentivogli", "Luisa", "Baroni", "Bernardi", "Raffaella", "Menini", "Stefano", "Zamparelli", "Roberto"], "venue": null, "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Vector-based models of semantic composition", "author": ["Mitchell", "Jeff", "Lapata", "Mirella"], "venue": "In ACL,", "citeRegEx": "Mitchell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2008}, {"title": "Composition in distributional models of semantics", "author": ["Mitchell", "Jeff", "Lapata", "Mirella"], "venue": "Cognitive Science,", "citeRegEx": "Mitchell et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2010}, {"title": "Ppdb 2.0: Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification", "author": ["Pavlick", "Ellie", "Rastogi", "Pushpendre", "Ganitkevich", "Juri", "Durme", "Benjamin Van", "Callison-Burch", "Chris"], "venue": "In Association for Computational Linguistics,", "citeRegEx": "Pavlick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pavlick et al\\.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Pennington", "Jeffrey", "Socher", "Richard", "Manning", "Christopher D"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Socher", "Richard", "Huang", "Eric H", "Pennin", "Jeffrey", "Manning", "Christopher D", "Ng", "Andrew Y"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Socher", "Richard", "Huval", "Brody", "Manning", "Christopher D", "Ng", "Andrew Y"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Socher", "Richard", "Perelygin", "Alex", "Wu", "Jean", "Chuang", "Jason", "Manning", "Christopher D", "Ng", "Andrew", "Potts", "Christopher"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc VV"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Tai", "Kai Sheng", "Socher", "Richard", "Manning", "Christopher D"], "venue": "arXiv preprint arXiv:1503.00075,", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Word representations: A simple and general method for semisupervised learning", "author": ["Turian", "Joseph", "Et", "Dpartement Dinformatique", "(diro", "Recherche Oprationnelle", "Montral", "Universit De", "Ratinov", "Lev", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Grammar as a foreign language", "author": ["Vinyals", "Oriol", "Kaiser", "Lukasz", "Koo", "Terry", "Petrov", "Slav", "Sutskever", "Ilya", "Hinton", "Geoffrey"], "venue": "arXiv preprint arXiv:1412.7449,", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "A long short-term memory model for answer sentence selection in question answering", "author": ["Wang", "Di", "Nyberg", "Eric"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems", "author": ["Wen", "Tsung-Hsien", "Gasic", "Milica", "Mrk\u0161i\u0107", "Nikola", "Su", "Pei-Hao", "Vandyke", "David", "Young", "Steve"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Wen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "From paraphrase database to compositional paraphrase model and back", "author": ["Wieting", "John", "Bansal", "Mohit", "Gimpel", "Kevin", "Livescu", "Karen", "Roth", "Dan"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Wieting et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wieting et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu", "Kelvin", "Ba", "Jimmy", "Kiros", "Ryan", "Cho", "Kyunghyun", "Courville", "Aaron C", "Salakhutdinov", "Ruslan", "Zemel", "Richard S", "Bengio", "Yoshua"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Semeval-2015 task 1: Paraphrase and semantic similarity in twitter (pit)", "author": ["Xu", "Wei", "Callison-Burch", "Chris", "Dolan", "William B"], "venue": "In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval),", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Classifying relations via long short term memory networks along shortest dependency paths", "author": ["Xu", "Yan", "Mou", "Lili", "Li", "Ge", "Chen", "Yunchuan", "Peng", "Hao", "Jin", "Zhi"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Convolutional neural network for paraphrase identification", "author": ["Yin", "Wenpeng", "Sch\u00fctze", "Hinrich"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Yin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2015}, {"title": "Learning composition models for phrase embeddings", "author": ["Yu", "Mo", "Dredze", "Mark"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Self-adaptive hierarchical sentence model", "author": ["Zhao", "Han", "Lu", "Zhengdong", "Poupart", "Pascal"], "venue": "In Proceedings of IJCAI,", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "Several researchers have developed and shared word embeddings trained on large datasets (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014), and these have been used effectively for many downstream tasks (Turian et al.", "startOffset": 88, "endOffset": 159}, {"referenceID": 31, "context": "Several researchers have developed and shared word embeddings trained on large datasets (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014), and these have been used effectively for many downstream tasks (Turian et al.", "startOffset": 88, "endOffset": 159}, {"referenceID": 35, "context": "Several researchers have developed and shared word embeddings trained on large datasets (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014), and these have been used effectively for many downstream tasks (Turian et al.", "startOffset": 88, "endOffset": 159}, {"referenceID": 41, "context": ", 2014), and these have been used effectively for many downstream tasks (Turian et al., 2010; Socher et al., 2011; Kim, 2014; Bansal et al., 2014; Tai et al., 2015).", "startOffset": 72, "endOffset": 164}, {"referenceID": 36, "context": ", 2014), and these have been used effectively for many downstream tasks (Turian et al., 2010; Socher et al., 2011; Kim, 2014; Bansal et al., 2014; Tai et al., 2015).", "startOffset": 72, "endOffset": 164}, {"referenceID": 3, "context": ", 2014), and these have been used effectively for many downstream tasks (Turian et al., 2010; Socher et al., 2011; Kim, 2014; Bansal et al., 2014; Tai et al., 2015).", "startOffset": 72, "endOffset": 164}, {"referenceID": 40, "context": ", 2014), and these have been used effectively for many downstream tasks (Turian et al., 2010; Socher et al., 2011; Kim, 2014; Bansal et al., 2014; Tai et al., 2015).", "startOffset": 72, "endOffset": 164}, {"referenceID": 36, "context": ", 2015) to those based on richly-structured functions like recursive neural networks (Socher et al., 2011), convolutional neural networks (Kalchbrenner et al.", "startOffset": 85, "endOffset": 106}, {"referenceID": 22, "context": ", 2011), convolutional neural networks (Kalchbrenner et al., 2014), and recurrent neural networks using long short-term memory (LSTM) (Tai et al.", "startOffset": 39, "endOffset": 66}, {"referenceID": 40, "context": ", 2014), and recurrent neural networks using long short-term memory (LSTM) (Tai et al., 2015).", "startOffset": 75, "endOffset": 93}, {"referenceID": 15, "context": "Lastly, we consider LSTMs because they have been found to be effective for many types of sequential data (Graves et al., 2008; 2013; Greff et al., 2015), including text (Sutskever et al.", "startOffset": 105, "endOffset": 152}, {"referenceID": 16, "context": "Lastly, we consider LSTMs because they have been found to be effective for many types of sequential data (Graves et al., 2008; 2013; Greff et al., 2015), including text (Sutskever et al.", "startOffset": 105, "endOffset": 152}, {"referenceID": 39, "context": ", 2015), including text (Sutskever et al., 2014; Vinyals et al., 2014; Xu et al., 2015a; Hermann et al., 2015; Ling et al., 2015; Wen et al., 2015).", "startOffset": 24, "endOffset": 147}, {"referenceID": 42, "context": ", 2015), including text (Sutskever et al., 2014; Vinyals et al., 2014; Xu et al., 2015a; Hermann et al., 2015; Ling et al., 2015; Wen et al., 2015).", "startOffset": 24, "endOffset": 147}, {"referenceID": 18, "context": ", 2015), including text (Sutskever et al., 2014; Vinyals et al., 2014; Xu et al., 2015a; Hermann et al., 2015; Ling et al., 2015; Wen et al., 2015).", "startOffset": 24, "endOffset": 147}, {"referenceID": 27, "context": ", 2015), including text (Sutskever et al., 2014; Vinyals et al., 2014; Xu et al., 2015a; Hermann et al., 2015; Ling et al., 2015; Wen et al., 2015).", "startOffset": 24, "endOffset": 147}, {"referenceID": 44, "context": ", 2015), including text (Sutskever et al., 2014; Vinyals et al., 2014; Xu et al., 2015a; Hermann et al., 2015; Ling et al., 2015; Wen et al., 2015).", "startOffset": 24, "endOffset": 147}, {"referenceID": 22, "context": "These include neural bag-of-words (NBOW) models (Kalchbrenner et al., 2014), deep NBOW models (Iyyer et al.", "startOffset": 48, "endOffset": 75}, {"referenceID": 36, "context": ", 2015), feature-weighted averaging (Yu & Dredze, 2015), recursive neural networks based on parse structure (Socher et al., 2011; 2012; 2013; \u0130rsoy & Cardie, 2014; Wieting et al., 2015), recursive networks based on hierarchical structure but not parses (Zhao et al.", "startOffset": 108, "endOffset": 185}, {"referenceID": 45, "context": ", 2015), feature-weighted averaging (Yu & Dredze, 2015), recursive neural networks based on parse structure (Socher et al., 2011; 2012; 2013; \u0130rsoy & Cardie, 2014; Wieting et al., 2015), recursive networks based on hierarchical structure but not parses (Zhao et al.", "startOffset": 108, "endOffset": 185}, {"referenceID": 51, "context": ", 2015), recursive networks based on hierarchical structure but not parses (Zhao et al., 2015; Chen et al., 2015b), convolutional neural networks (Kalchbrenner et al.", "startOffset": 75, "endOffset": 114}, {"referenceID": 22, "context": ", 2015b), convolutional neural networks (Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014; Yin & Sch\u00fctze, 2015; He et al., 2015), and recurrent neural networks using long short-term memory (Tai et al.", "startOffset": 40, "endOffset": 133}, {"referenceID": 20, "context": ", 2015b), convolutional neural networks (Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014; Yin & Sch\u00fctze, 2015; He et al., 2015), and recurrent neural networks using long short-term memory (Tai et al.", "startOffset": 40, "endOffset": 133}, {"referenceID": 17, "context": ", 2015b), convolutional neural networks (Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014; Yin & Sch\u00fctze, 2015; He et al., 2015), and recurrent neural networks using long short-term memory (Tai et al.", "startOffset": 40, "endOffset": 133}, {"referenceID": 40, "context": ", 2015), and recurrent neural networks using long short-term memory (Tai et al., 2015; Ling et al., 2015; Liu et al., 2015).", "startOffset": 68, "endOffset": 123}, {"referenceID": 27, "context": ", 2015), and recurrent neural networks using long short-term memory (Tai et al., 2015; Ling et al., 2015; Liu et al., 2015).", "startOffset": 68, "endOffset": 123}, {"referenceID": 28, "context": ", 2015), and recurrent neural networks using long short-term memory (Tai et al., 2015; Ling et al., 2015; Liu et al., 2015).", "startOffset": 68, "endOffset": 123}, {"referenceID": 45, "context": "edu/ \u0303wieting In prior work, we experimented with recursive neural networks on binarized parses of the PPDB (Wieting et al., 2015), but we found that many of the phrases in PPDB are not sentences or even constituents, causing the parser to have unexpected behavior.", "startOffset": 108, "endOffset": 130}, {"referenceID": 7, "context": ", 2015; Chen et al., 2015b), convolutional neural networks (Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014; Yin & Sch\u00fctze, 2015; He et al., 2015), and recurrent neural networks using long short-term memory (Tai et al., 2015; Ling et al., 2015; Liu et al., 2015). In this paper, we focus our comparison between methods based on updated vector addition and those based on LSTMs.6 Most of the work mentioned above learns compositional models in the context of supervised learning. That is, a training set is provided with annotations and the composition function is learned for the purposes of optimizing an objective function based on those annotations. The models are then evaluated on a test set drawn from the same distribution as the training set. In this paper, in contrast, we are primarily interested in creating general purpose, domain independent embeddings for word sequences. There have been research efforts also targeting this goal. Denoted as PARAGRAM-PHRASE-XXL and discussed in Section B.2. As measured by the average Pearson\u2019s r over all datasets in each task. We used the publicly available 300-dimensional vectors that were trained on the 840 billion token Common Crawl corpus, available at http://nlp.stanford.edu/projects/glove/. These are 300-dimensional vectors from Wieting et al. (2015) and are available at http://ttic.", "startOffset": 8, "endOffset": 1317}, {"referenceID": 36, "context": "One approach is to train an autoencoder in an attempt to learn the latent structure of the sequence, whether it be a sentence with a parse tree (Socher et al., 2011), or a longer sequence such as a paragraph or document (Li et al.", "startOffset": 144, "endOffset": 165}, {"referenceID": 12, "context": "LSTMs have recently been shown to produce state-of-the-art results in a variety of sequence processing tasks (Chen et al., 2015a; Filippova et al., 2015; Xu et al., 2015c; Belinkov & Glass, 2015; Wang & Nyberg, 2015).", "startOffset": 109, "endOffset": 216}, {"referenceID": 7, "context": "LSTMs have recently been shown to produce state-of-the-art results in a variety of sequence processing tasks (Chen et al., 2015a; Filippova et al., 2015; Xu et al., 2015c; Belinkov & Glass, 2015; Wang & Nyberg, 2015). We use the version from Gers et al. (2003) which has the following equations:", "startOffset": 110, "endOffset": 261}, {"referenceID": 45, "context": "We mostly follow the approach of Wieting et al. (2015). The training data consists of (possibly noisy) pairs taken directly from the original Paraphrase Database (PPDB) and we optimize a marginbased loss.", "startOffset": 33, "endOffset": 55}, {"referenceID": 30, "context": ", 2015b) and the SemEval 2014 Semantic Relatedness task (Marelli et al., 2014), as well as two tasks that use PPDB data (Wieting et al.", "startOffset": 56, "endOffset": 78}, {"referenceID": 45, "context": ", 2014), as well as two tasks that use PPDB data (Wieting et al., 2015; Pavlick et al., 2015).", "startOffset": 49, "endOffset": 93}, {"referenceID": 34, "context": ", 2014), as well as two tasks that use PPDB data (Wieting et al., 2015; Pavlick et al., 2015).", "startOffset": 49, "endOffset": 93}, {"referenceID": 0, "context": "For more details on these tasks please refer to the relevant publications for the 2012 (Agirre et al., 2012), 2013 (Diab, 2013), 2014 (Agirre et al.", "startOffset": 87, "endOffset": 108}, {"referenceID": 1, "context": ", 2012), 2013 (Diab, 2013), 2014 (Agirre et al., 2014), and 2015 (Agirre et al.", "startOffset": 33, "endOffset": 54}, {"referenceID": 2, "context": ", 2014), and 2015 (Agirre et al., 2015) tasks.", "startOffset": 18, "endOffset": 39}, {"referenceID": 11, "context": "Questions and Answers: Paired answers to the same question from StackExchange (answersforums) and the BEETLE corpus (Dzikovska et al., 2010) (answers-students) were used in 2015.", "startOffset": 116, "endOffset": 140}, {"referenceID": 33, "context": "Then after finding the hyper-parameters which maximize Spearman\u2019s \u03c1 on the PPDB task from Pavlick et al. (2015), we trained models on the entire XL section of PPDB for 10 epochs.", "startOffset": 90, "endOffset": 112}, {"referenceID": 33, "context": "Then after finding the hyper-parameters which maximize Spearman\u2019s \u03c1 on the PPDB task from Pavlick et al. (2015), we trained models on the entire XL section of PPDB for 10 epochs. We use PARAGRAM-SL999 embeddings to initialize the word embedding matrix (Ww) for all models. The reason we chose the task from Pavlick et al. for tuning is that we wanted our entire procedure to only make use of PPDB and use no other resources. So we considered annotated datasets that are exclusively composed of phrases in PPDB. We chose this dataset over Annotated-PPDB from Wieting et al. (2015) due to the larger amount of labeled data in the former (26,456 examples compared to 1,000 examples).", "startOffset": 90, "endOffset": 580}, {"referenceID": 33, "context": "Then after finding the hyper-parameters which maximize Spearman\u2019s \u03c1 on the PPDB task from Pavlick et al. (2015), we trained models on the entire XL section of PPDB for 10 epochs. We use PARAGRAM-SL999 embeddings to initialize the word embedding matrix (Ww) for all models. The reason we chose the task from Pavlick et al. for tuning is that we wanted our entire procedure to only make use of PPDB and use no other resources. So we considered annotated datasets that are exclusively composed of phrases in PPDB. We chose this dataset over Annotated-PPDB from Wieting et al. (2015) due to the larger amount of labeled data in the former (26,456 examples compared to 1,000 examples). However, in practice the datasets are very similar and selecting either to tune on produces similar results. Evidence for this can be seen in Table 3 where the oracle results on the task in Pavlick et al. (2015) and the results tuned on the test set of Annotated-PPDB are very similar.", "startOffset": 90, "endOffset": 893}, {"referenceID": 10, "context": "1 using AdaGrad Duchi et al. (2011) with mini-batches.", "startOffset": 16, "endOffset": 36}, {"referenceID": 35, "context": ", 2015), denoted \u201cST\u201d, averaged GloVe10 vectors (Pennington et al., 2014), and averaged PARAGRAM-SL999 vectors (Wieting et al.", "startOffset": 48, "endOffset": 73}, {"referenceID": 45, "context": ", 2014), and averaged PARAGRAM-SL999 vectors (Wieting et al., 2015), denoted \u201cPSL\u201d.", "startOffset": 45, "endOffset": 67}, {"referenceID": 29, "context": "Note that we pre-processed the training data with the tokenizer from Stanford CoreNLP (Manning et al., 2014) rather than the included NLTK (Bird et al.", "startOffset": 86, "endOffset": 108}, {"referenceID": 5, "context": ", 2014) rather than the included NLTK (Bird et al., 2009) tokenizer.", "startOffset": 38, "endOffset": 57}, {"referenceID": 34, "context": "Model Pavlick et al. (2015) (oracle) Pavlick et al.", "startOffset": 6, "endOffset": 28}, {"referenceID": 34, "context": "Model Pavlick et al. (2015) (oracle) Pavlick et al. (2015) (test) Wieting et al.", "startOffset": 6, "endOffset": 59}, {"referenceID": 34, "context": "Model Pavlick et al. (2015) (oracle) Pavlick et al. (2015) (test) Wieting et al. (2015) PARAGRAM-PHRASE 0.", "startOffset": 6, "endOffset": 88}, {"referenceID": 34, "context": "For the task in Pavlick et al. (2015), we include the oracle result, since this dataset was used for model selection for all tasks, as well as test results where models were tuned on Annotated-PPDB.", "startOffset": 16, "endOffset": 38}, {"referenceID": 40, "context": "section, we minimize the objective function11 from Tai et al. (2015). Given a score for a sentence pair in the range [1,K], where K is an integer, with sentence representations hL and hR, and model parameters \u03b8, they first compute: h\u00d7 = hL \u2299 hR, h+ = |hL \u2212 hR|, hs = \u03c3 (", "startOffset": 51, "endOffset": 69}, {"referenceID": 40, "context": "8676) (Tai et al., 2015) or a convolutional neural network (0.", "startOffset": 6, "endOffset": 24}, {"referenceID": 17, "context": "8686) (He et al., 2015).", "startOffset": 6, "endOffset": 23}, {"referenceID": 40, "context": "We also mention Tai et al. (2015) which had a similar LSTM result on the SICK dataset (Pearson\u2019s r of 85.", "startOffset": 16, "endOffset": 34}, {"referenceID": 34, "context": "The resulting average Pearson\u2019s r, after tuning on the PPDB task in Pavlick et al. (2015), was 0.", "startOffset": 68, "endOffset": 90}, {"referenceID": 25, "context": "(2015) and Li et al. (2015a) require significant training time on GPUs, on the order of multiple weeks.", "startOffset": 11, "endOffset": 29}, {"referenceID": 25, "context": "(2015) and Li et al. (2015a) require significant training time on GPUs, on the order of multiple weeks. Moreover, from these papers, the dependency between the amount of training data or training time versus performance of the model is unclear. In order to illustrate this dependency for our PARAGRAM-PHRASE model, we trained on different amounts of data and plotted the performance. The results are shown in Figure 2. We start with PPDB XL which has 3,033,753 unique phrase pairs. and then divide by powers of two until there are less than 10 phrase pairs.21 For all data points (each division by two), we trained a model with that amount of phrase pairs for 10 epochs. We use the average Pearson correlation for all 22 datasets in Table 2 in our plot. We experimented with two different sets of training data. The first is keeping the order of the phrase pairs in PPDB, which ensures the smaller datasets contain higher quality phrase pairs. In the second, we permute PPDB XL before constructing our smaller datasets. In both ways, each larger dataset contains the previous one plus as many new phase pairs. We fixed the parameters for all models to those found in Section 4.2 and tuned on the PPDB task from Pavlick et al. (2015) to select the best parameters during training.", "startOffset": 11, "endOffset": 1233}], "year": 2015, "abstractText": "In this paper, we show how to create paraphrastic sentence embeddings using the Paraphrase Database (Ganitkevitch et al., 2013), an extensive semantic resource with millions of phrase pairs. We consider several compositional architectures and evaluate them on 24 textual similarity datasets encompassing domains such as news, tweets, web forums, news headlines, machine translation output, glosses, and image and video captions. We present the interesting result that simple compositional architectures based on updated vector averaging vastly outperform long short-term memory (LSTM) recurrent neural networks and that these simpler architectures allow us to learn models with superior generalization. Our models are efficient, very easy to use, and competitive with task-tuned systems. We make them available to the research community1 with the hope that they can serve as the new baseline for further work on universal paraphrastic sentence embeddings.", "creator": "LaTeX with hyperref package"}}}