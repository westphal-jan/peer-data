{"id": "1511.08228", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Nov-2015", "title": "Neural GPUs Learn Algorithms", "abstract": "Learning an algorithm from examples is a fundamental problem that has been widely studied. Recently it has been addressed using neural networks, in particular by Neural Turing Machines (NTMs). These are fully differentiable computers that use backpropagation to learn their own programming. Despite their appeal NTMs have a weakness that is caused by their sequential nature: they cannot be parallelized and are are hard to train due to their large depth when unfolded.", "histories": [["v1", "Wed, 25 Nov 2015 21:17:43 GMT  (18kb)", "http://arxiv.org/abs/1511.08228v1", null], ["v2", "Wed, 6 Jan 2016 14:44:27 GMT  (24kb)", "http://arxiv.org/abs/1511.08228v2", null], ["v3", "Tue, 15 Mar 2016 00:20:54 GMT  (19kb)", "http://arxiv.org/abs/1511.08228v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["{\\l}ukasz kaiser", "ilya sutskever"], "accepted": true, "id": "1511.08228"}, "pdf": {"name": "1511.08228.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["lukaszkaiser@google.com", "ilyasu@google.com"], "sections": [{"heading": null, "text": "ar Xiv: 151 1,08 228v 1 [cs.L G] 25 Nov 201 5"}, {"heading": "1 INTRODUCTION", "text": "In fact, most of them will be able to abide by the rules that they have applied in practice."}, {"heading": "1.1 RELATED WORK", "text": "In fact, it is the case that most of us are in a position to put ourselves in another world, in which they get lost in another world, in which they get lost in another world, in which they get lost in another world, in which they get lost in another world, in which they find themselves in another world, in which they do not find themselves again, in which they do not find themselves again, in which they find themselves in another world, in which they find themselves in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live."}, {"heading": "2 THE NEURAL GPU", "text": "Before we introduce the Neural GPU, let us recall the architecture of a gated recurrent unit (GRU) (Cho et al., 2014). A GRU is similar to an LSTM, but its input and state are equal in size, making it easier for us to generalize it later; a highway network could also be used (Srivastava et al., 2015), but it lacks the reset gate (x, s) in performance similar to LSTMs on a number of tasks (Chung et al., 2014; Greff et al., 2015). A GRU takes an input vector x and a current state vector s, and outputs: GRU (x, s) = performance similar to LSTMs on a number of tasks (Chung et al., 2014; Greff et al., 2015). A GRU takes an input vector x and a current state vector s."}, {"heading": "3 EXPERIMENTS", "text": "In this section, we present experiments that show that a Neural GPU can successfully learn a series of algorithmic tasks and generalize far beyond the length at which it has been trained. This section is organized as follows: We start with the two tasks we have focused on, long binary addition and long binary multiplication; we present the results for these tasks and compare them with other models; and then we show that Neural GPUs work well for some other tasks as well."}, {"heading": "3.1 ADDITION AND MULTIPLICATION", "text": "In fact, it's that we're able to hide, and that we're going to be able to hide, \"he said.\" We've got to be able to hide, \"he said.\" We've got to be able to hide, \"he said.\" We've got to be able to hide, \"he said."}, {"heading": "3.2 OTHER ALGORITHMIC TASKS", "text": "In addition to the two main tasks described above, we tested neuronal GPUs on the following simpler algorithmic tasks. The same architecture as above was capable of solving all tasks described below, i.e., after we were trained on sequences to 41, we were unable to find errors in sequences on any length we had tested (up to 4001). The copying of sequences is the simple task of generating the same sequence on the output as on the input. It is very easy for a neuronal GPU to find errors in sequences on any length that we had tested (up to 4001). Reversing sequences is the simple task of generating the same sequence on the output as on the input. It is for a neuronal GPU very simple, since all models quickly converge and perfectly generalize. Reversing sequences is the task of reversing a sequence of bits. The length n here is exactly the length of the sequence, as in the following example. The input 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0"}, {"heading": "3.3 TRAINING TECHNIQUES", "text": "We focus on the most important elements of our training regime, all the less relevant details can be found in the code released as open source.1Grid Search. Every result we report is obtained by performing a grid search over 36 = 729 instances. We consider 3 settings of the learning rate, initial parameter scale and 4 other hyperparameters discussed below: the relaxation factor, the threshold for learning progress, the gradient scale and dropout. An important effect of this grid search is also that we train 729 models each time with different random seeds. Usually, only some of these models generalize to 2000-bit numbers, but a significant fraction works well on 200-bit numbers as discussed below."}, {"heading": "3.3.1 DROPOUT ON RECURRENT CONNECTIONS", "text": "Dropout is a commonly used technique for regulating neural networks, but applying it to recursive networks was counterproductive - it only worked when applied to non-recursive connections, as Pham et al. (2014) reported. 1The code will be available at https: / / github.com / lukaszkaiser / Neural GPU /. Since a neural GPU does not have recursive connections, it may seem that a dropout will not be useful for this architecture. Surprisingly, we found the opposite - it is useful and improves generalization. The key to effectively using dropout in this setting is to set a small dropout rate. When we do a network search for dropout rates, we vary it between 6%, 9% and 13.5%, meaning that over 85% of the values are always retained."}, {"heading": "3.3.2 PARAMETER SHARING RELAXATION.", "text": "To improve the optimization of our deep network, we use a relaxation technique for common parameters that works as follows: instead of training with parameters divided over time steps, we use r identical sets of undivided parameters (we often use r = 6, larger numbers work better, but use more memory); in the time step t of the neural GPU, we use the i-th set when t mod r = i. The procedure described above relaxes the network since it can now perform various operations in different time steps; training becomes easier, but we now have r parameters instead of the common set we want. To unify them, we add a term to the cost function that represents the distance of each parameter from the average of this parameter in all r-sets. This term in the final cost function is multiplied by a scalar that we call a relaxation train."}, {"heading": "4 DISCUSSION", "text": "What does this mean for you? Although the results presented above are encouraging, it is natural to ask what are the cases where the neuralgia does not work well? For someone who performs decimal surgery, performance is impaired. All tasks can be completed with decimal surgery. One might hope that the neuralgia works well in this case as well."}, {"heading": "5 CONCLUSIONS AND FUTURE WORK", "text": "The results presented in Table 1 clearly show that there is a qualitative difference between what can be achieved with a neural GPU and what was possible with earlier archietctures. Specifically, we are showing for the first time a neural network that learns a non-trivial superlinear time algorithm in a way that generalizes to much higher lengths without errors, opening the way to the use of neural networks in domains that were previously addressed only by discrete methods such as program synthesis. With the surprising data efficiency of neural GPUs, it may even be possible to replicate earlier results of program synthesis, e.g. Kaiser (2012), but in a more scalable way. It is also interesting that a neural GPU can learn symbolic algorithms without using any discrete state at all, and the addition of suspenders and noise only improves its performance. Another promising future work is to use neural GPU's general language architecture, as we have already achieved in calculation architecture in 2013, without using a good word processing architecture."}], "references": [{"title": "Learning regaular sets from queries and counterexamples", "author": ["Angluin", "Dana"], "venue": "Information and Computation,", "citeRegEx": "Angluin and Dana.,? \\Q1987\\E", "shortCiteRegEx": "Angluin and Dana.", "year": 1987}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Automatic Structures", "author": ["Blumensath", "Achim", "Gr\u00e4del", "Erich"], "venue": "In Proceedings of LICS", "citeRegEx": "Blumensath et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Blumensath et al\\.", "year": 2000}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "van Merrienboer", "Bart", "Gulcehre", "Caglar", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "CoRR, abs/1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung", "Junyoung", "G\u00fcl\u00e7ehre", "\u00c7aglar", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "CoRR, abs/1412.3555,", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition", "author": ["Dahl", "George E", "Yu", "Dong", "Deng", "Li", "Acero", "Alex"], "venue": "IEEE Transactions on Audio, Speech & Language Processing,", "citeRegEx": "Dahl et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2012}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Graves", "Alex", "Jaitly", "Navdeep"], "venue": "In Proceedings ICML", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Learning to transduce with unbounded memory", "author": ["Grefenstette", "Edward", "Hermann", "Karl Moritz", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "CoRR, abs/1506.02516,", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "LSTM: A search space odyssey", "author": ["Greff", "Klaus", "Srivastava", "Rupesh Kumar", "Koutn\u0131\u0301k", "Jan", "Steunebrink", "Bas R", "Schmidhuber", "J\u00fcrgen"], "venue": "CoRR, abs/1503.04069,", "citeRegEx": "Greff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "Dimensions in program synthesis", "author": ["Gulwani", "Sumit"], "venue": "In Proceedings of PPDP 2010,", "citeRegEx": "Gulwani and Sumit.,? \\Q2010\\E", "shortCiteRegEx": "Gulwani and Sumit.", "year": 2010}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Joulin", "Armand", "Mikolov", "Tomas"], "venue": "CoRR, abs/1503.01007,", "citeRegEx": "Joulin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2015}, {"title": "Learning games from videos guided by descriptive complexity", "author": ["Kaiser", "\u0141ukasz"], "venue": "In Proceedings of the AAAI-12,", "citeRegEx": "Kaiser and \u0141ukasz.,? \\Q2012\\E", "shortCiteRegEx": "Kaiser and \u0141ukasz.", "year": 2012}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Nal", "Blunsom", "Phil"], "venue": "In Proceedings EMNLP", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Grid long short-term memory", "author": ["Kalchbrenner", "Nal", "Danihelka", "Ivo", "Graves", "Alex"], "venue": "CoRR, abs/1507.01526,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik P", "Ba", "Jimmy"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Inductive programming: A survey of program synthesis techniques", "author": ["Kitzelmann", "Emanuel"], "venue": "In Approaches and Applications of Inductive Programming, AAIP 2009,", "citeRegEx": "Kitzelmann and Emanuel.,? \\Q2010\\E", "shortCiteRegEx": "Kitzelmann and Emanuel.", "year": 2010}, {"title": "Imagenet classification with deep convolutional neural network", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Fast algorithms for convolutional neural networks", "author": ["Lavin", "Andrew", "Gray", "Scott"], "venue": "CoRR, abs/1509.09308,", "citeRegEx": "Lavin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lavin et al\\.", "year": 2015}, {"title": "Dropout improves recurrent neural networks for handwriting recognition", "author": ["Pham", "Vu", "Bluche", "Th\u00e9odore", "Kermorvant", "Christopher", "Louradour", "J\u00e9r\u00f4me"], "venue": "In Frontiers in Handwriting Recognition (ICFHR),", "citeRegEx": "Pham et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc VV"], "venue": "In Advances in Neural Information Processing Systems, pp. 3104\u20133112,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Grammar as a foreign language", "author": ["Vinyals", "Kaiser", "Koo", "Petrov", "Sutskever", "Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Vinyals", "Oriol", "Toshev", "Alexander", "Bengio", "Samy", "Erhan", "Dumitru"], "venue": "CoRR, abs/1411.4555,", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Bayesian learning via stochastic gradient Langevin dynamics", "author": ["Max", "Teh", "Yee Whye"], "venue": "In Proceedings of ICML", "citeRegEx": "Welling et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Welling et al\\.", "year": 2011}, {"title": "Learning to execute", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "CoRR, abs/1410.4615,", "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}, {"title": "Reinforcement learning neural turing machines", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "CoRR, abs/1505.00521,", "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 17, "context": "Deep neural networks have recently proven successful at various tasks, such as computer vision (Krizhevsky et al., 2012), speech recognition (Dahl et al.", "startOffset": 95, "endOffset": 120}, {"referenceID": 5, "context": ", 2012), speech recognition (Dahl et al., 2012), and in other domains.", "startOffset": 28, "endOffset": 47}, {"referenceID": 20, "context": "Sequence-tosequence recurrent neural networks with such cells can learn very complex tasks in an end-to-end manner, such as translation (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014), parsing (Vinyals & Kaiser et al.", "startOffset": 136, "endOffset": 201}, {"referenceID": 1, "context": "Sequence-tosequence recurrent neural networks with such cells can learn very complex tasks in an end-to-end manner, such as translation (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014), parsing (Vinyals & Kaiser et al.", "startOffset": 136, "endOffset": 201}, {"referenceID": 3, "context": "Sequence-tosequence recurrent neural networks with such cells can learn very complex tasks in an end-to-end manner, such as translation (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014), parsing (Vinyals & Kaiser et al.", "startOffset": 136, "endOffset": 201}, {"referenceID": 22, "context": ", 2015), speech recognition (Graves & Jaitly, 2014) or image caption generation (Vinyals et al., 2014).", "startOffset": 80, "endOffset": 102}, {"referenceID": 1, "context": "One way to resolve this problem is by using an attention mechanism (Bahdanau et al., 2014).", "startOffset": 67, "endOffset": 90}, {"referenceID": 6, "context": "Neural Turing Machines (Graves et al., 2014) have this theoretical property.", "startOffset": 23, "endOffset": 44}, {"referenceID": 1, "context": ", 2014; Bahdanau et al., 2014; Cho et al., 2014), parsing (Vinyals & Kaiser et al., 2015), speech recognition (Graves & Jaitly, 2014) or image caption generation (Vinyals et al., 2014). Since so many tasks can be solved with essentially one model, a natural question arises: is this model the best we can hope for in supervised learning? It turns out that despite its recent success the sequence-to-sequence model has limitations. In its basic architecture, the entire input is encoded into a single fixed-size vector, so the model cannot generalize to inputs much longer than this fixed capacity. One way to resolve this problem is by using an attention mechanism (Bahdanau et al., 2014). This allows the network to inspect arbitrary parts of the input in every decoding step, so the basic limitation is removed. But other problems remain, and Joulin & Mikolov (2015) show a number of basic algorithmic tasks on which sequenceto-sequence LSTM networks fail to generalize.", "startOffset": 8, "endOffset": 869}, {"referenceID": 6, "context": "These issues are not limited to Neural Turing Machines, they apply to other architectures too, such as stack-RNNs (Joulin & Mikolov, 2015) or (De)Queue-RNNs (Graves et al., 2014).", "startOffset": 157, "endOffset": 178}, {"referenceID": 20, "context": "The learning of algorithms with neural networks has seen a lot of interest after the success of sequence-to-sequence neural networks on language processing tasks (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014).", "startOffset": 162, "endOffset": 227}, {"referenceID": 1, "context": "The learning of algorithms with neural networks has seen a lot of interest after the success of sequence-to-sequence neural networks on language processing tasks (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014).", "startOffset": 162, "endOffset": 227}, {"referenceID": 3, "context": "The learning of algorithms with neural networks has seen a lot of interest after the success of sequence-to-sequence neural networks on language processing tasks (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014).", "startOffset": 162, "endOffset": 227}, {"referenceID": 6, "context": "Neural Turing Machines (Graves et al., 2014) were shown to learn a number of basic sequence transformations and memory access patters, and their reinforcement learning variant (Zaremba & Sutskever, 2015b) has reasonable perfomance on a number of tasks as well.", "startOffset": 23, "endOffset": 44}, {"referenceID": 7, "context": "Stack, Queue and DeQueue networks (Grefenstette et al., 2015) were also shown to learn basic sequence transformations such as bigram flipping or sequence reversal.", "startOffset": 34, "endOffset": 61}, {"referenceID": 14, "context": "The Grid LSTM (Kalchbrenner et al., 2015) is another powerful architecture that can learn to multiply 15-digit decimal numbers.", "startOffset": 14, "endOffset": 41}, {"referenceID": 1, "context": ", 2014; Bahdanau et al., 2014; Cho et al., 2014). An attempt was even made to learn to evaluate simple python programs with a pure sequence-to-sequence model (Zaremba & Sutskever, 2015a), but more success was seen with more complex models. Neural Turing Machines (Graves et al., 2014) were shown to learn a number of basic sequence transformations and memory access patters, and their reinforcement learning variant (Zaremba & Sutskever, 2015b) has reasonable perfomance on a number of tasks as well. Stack, Queue and DeQueue networks (Grefenstette et al., 2015) were also shown to learn basic sequence transformations such as bigram flipping or sequence reversal. The Grid LSTM (Kalchbrenner et al., 2015) is another powerful architecture that can learn to multiply 15-digit decimal numbers. As we will see in the next section, the Grid-LSTM is quite similar to the Neural GPU \u2013 the main difference is that the Neural GPU is less recurrent and is explicitly constructed from the highly parallel convolution operator. Most comparable to this work are the prior experiments with the stack-augmented RNNs (Joulin & Mikolov, 2015). These networks manage to learn and generalize to unseen lengths on a number of algorithmic tasks. But, as we show in Section 3.1, stack-augmented RNNs trained to add numbers upto 20-bit long generalize only to \u223c100-bit numbers, never to 200-bit ones, and never without error. Still, their generalization is the best we were able to obtain without using the Neural GPU and far surpasses a baseline LSTM sequence-to-sequence model with attention. The quest for learning algorithms has been pursued much more widely with tools other than neural networks. It is known under names such as program synthesis, program induction, automatic programming, or inductive synthesis, and has a long history with many works that we do not cover here; see, e.g., Gulwani (2010) and Kitzelmann (2010) for a more general perspective.", "startOffset": 8, "endOffset": 1890}, {"referenceID": 1, "context": ", 2014; Bahdanau et al., 2014; Cho et al., 2014). An attempt was even made to learn to evaluate simple python programs with a pure sequence-to-sequence model (Zaremba & Sutskever, 2015a), but more success was seen with more complex models. Neural Turing Machines (Graves et al., 2014) were shown to learn a number of basic sequence transformations and memory access patters, and their reinforcement learning variant (Zaremba & Sutskever, 2015b) has reasonable perfomance on a number of tasks as well. Stack, Queue and DeQueue networks (Grefenstette et al., 2015) were also shown to learn basic sequence transformations such as bigram flipping or sequence reversal. The Grid LSTM (Kalchbrenner et al., 2015) is another powerful architecture that can learn to multiply 15-digit decimal numbers. As we will see in the next section, the Grid-LSTM is quite similar to the Neural GPU \u2013 the main difference is that the Neural GPU is less recurrent and is explicitly constructed from the highly parallel convolution operator. Most comparable to this work are the prior experiments with the stack-augmented RNNs (Joulin & Mikolov, 2015). These networks manage to learn and generalize to unseen lengths on a number of algorithmic tasks. But, as we show in Section 3.1, stack-augmented RNNs trained to add numbers upto 20-bit long generalize only to \u223c100-bit numbers, never to 200-bit ones, and never without error. Still, their generalization is the best we were able to obtain without using the Neural GPU and far surpasses a baseline LSTM sequence-to-sequence model with attention. The quest for learning algorithms has been pursued much more widely with tools other than neural networks. It is known under names such as program synthesis, program induction, automatic programming, or inductive synthesis, and has a long history with many works that we do not cover here; see, e.g., Gulwani (2010) and Kitzelmann (2010) for a more general perspective.", "startOffset": 8, "endOffset": 1912}, {"referenceID": 3, "context": "Before we introduce the Neural GPU, let us recall the architecture of a Gated Recurrent Unit (GRU) (Cho et al., 2014).", "startOffset": 99, "endOffset": 117}, {"referenceID": 4, "context": "GRUs have shown performance similar to LSTMs on a number of tasks (Chung et al., 2014; Greff et al., 2015).", "startOffset": 66, "endOffset": 106}, {"referenceID": 8, "context": "GRUs have shown performance similar to LSTMs on a number of tasks (Chung et al., 2014; Greff et al., 2015).", "startOffset": 66, "endOffset": 106}, {"referenceID": 19, "context": "But when applying it to recurrent networks, it has been counter-productive to apply it on recurrent connections \u2013 it only worked when applied to the non-recurrent ones, as reported by Pham et al. (2014). The code will be available at https://github.", "startOffset": 184, "endOffset": 203}], "year": 2015, "abstractText": "Learning an algorithm from examples is a fundamental problem that has been widely studied. Recently it has been addressed using neural networks, in particular by Neural Turing Machines (NTMs). These are fully differentiable computers that use backpropagation to learn their own programming. Despite their appeal NTMs have a weakness that is caused by their sequential nature: they cannot be parallelized and are are hard to train due to their large depth when unfolded. We present a neural network architecture to address this problem: the Neural GPU. It is based on a type of convolutional gated recurrent unit and, like the NTM, is computationally universal. Unlike the NTM, the Neural GPU is highly parallel which makes it easier to train and efficient to run. An essential property of algorithms is their ability to handle inputs of arbitrary size. We show that the Neural GPU can be trained on short instances of an algorithmic task and successfully generalize to long instances. We verified it on a number of tasks including long addition and long multiplication of numbers represented in binary. We train the Neural GPU on numbers with upto 20 bits and observe no errors whatsoever while testing it, even on much longer numbers. To achieve these results we introduce a technique for training deep recurrent networks: parameter sharing relaxation. We also found a small amount of dropout and gradient noise to have a large positive effect on learning and generalization.", "creator": "LaTeX with hyperref package"}}}