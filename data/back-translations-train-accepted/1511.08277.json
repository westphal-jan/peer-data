{"id": "1511.08277", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Nov-2015", "title": "A Deep Architecture for Semantic Matching with Multiple Positional Sentence Representations", "abstract": "Matching natural language sentences is central for many applications such as information retrieval and question answering. Existing deep models rely on a single sentence representation or multiple granularity representations for matching. However, such methods cannot well capture the contextualized local information in the matching process. To tackle this problem, we present a new deep architecture to match two sentences with multiple positional sentence representations. Specifically, each positional sentence representation is a sentence representation at this position, generated by a bidirectional long short term memory (Bi-LSTM). The matching score is finally produced by aggregating interactions between these different positional sentence representations, through $k$-Max pooling and a multi-layer perceptron. Our model has several advantages: (1) By using Bi-LSTM, rich context of the whole sentence is leveraged to capture the contextualized local information in each positional sentence representation; (2) By matching with multiple positional sentence representations, it is flexible to aggregate different important contextualized local information in a sentence to support the matching; (3) Experiments on different tasks such as question answering and sentence completion demonstrate the superiority of our model.", "histories": [["v1", "Thu, 26 Nov 2015 02:57:54 GMT  (161kb,D)", "http://arxiv.org/abs/1511.08277v1", "Accepted by AAAI-2016"]], "COMMENTS": "Accepted by AAAI-2016", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.NE", "authors": ["shengxian wan", "yanyan lan", "jiafeng guo", "jun xu", "liang pang", "xueqi cheng"], "accepted": true, "id": "1511.08277"}, "pdf": {"name": "1511.08277.pdf", "metadata": {"source": "CRF", "title": "A Deep Architecture for Semantic Matching with Multiple Positional Sentence Representations", "authors": ["Shengxian Wan", "Yanyan Lan", "Jiafeng Guo", "Jun Xu", "Liang Pang", "Xueqi Cheng"], "emails": ["pangliang}@software.ict.ac.cn,", "cxq}@ict.ac.cn"], "sections": [{"heading": "Introduction", "text": "In fact, most of them are able to survive on their own, without feeling able to survive on their own."}, {"heading": "Our Approach", "text": "In this section, we present our new deep architecture for the consistency of two sentences with multiple sentence representations, namely MV-LSTM = Long-LSTM. As illustrated in Figure1, MV-LSTM consists of three parts: First, each sentence representation is a sentence representation at one position, generated by a bi-directional long-term memory (Bi-LSTM); second, the interactions between different sentence representations form a similarity matrix / tensor by different similarity functions; finally, the final matching score is generated by aggregating such interactions by k-max pooling and multilayered perception. Step 1: Positional sentence representation Each position set must reflect the representation of the entire sentence when we participate in this position. Therefore, it is natural to use such a representation, because LSTM can capture both long-term and short-term dependencies in the sentences."}, {"heading": "Model Training", "text": "For example, if the task is formalized as a ranking problem, we can use pairs of ranking losses such as hinge losses for training. In a triple (SX, S + Y, S \u2212 Y) where S + Y ranks higher than S \u2212 Y when matching with SX, the loss function is defined as follows: L (SX, S + Y, S \u2212 Y) = max (0, 1 \u2212 s (SX, S + Y) + s (SX, S \u2212 Y)), where s (SX, S + Y) and s (SX, S \u2212 Y) are the appropriate values. All parameters of the model, including the parameters Word Embedding, Bi-LSTM, Interaction Function and MLP, are jointly trained by BackPropagation and stochastic Gradient Descent. Specifically, we use Adagrad (Duchi, Hazan and Singer 2011) for all parameters in training."}, {"heading": "Discussions", "text": "MV-LSTM can cover LSTM-RNN (Palangi et al. 2015) as a special case. If we consider only the last sentence representation of each sentence generated by a single directional LSTM, MV-LSTM is reduced directly to LSTM-RNN. Therefore, MV-LSTM is of a more general nature and has the ability to use more position set representations for match, compared to LSTM-RNN.MV-LSTM has implicitly considered multiple granularity. By using Bi-LSTM, which is able to include both long-term and short-term dependencies in the representation of a set, MV-LSTM has the potential to capture important n-gram matching patterns. Moreover, MV-LSTM is flexible to include important granularity adaptively, compared to CNN-based models with fixed window sizes."}, {"heading": "Experiments", "text": "In this section we show our experiments on two different matching tasks, the answering of questions (QA) and the completion of sentences (SC)."}, {"heading": "Experimental Settings", "text": "First, we introduce our experimental settings, including baselines, parameter settings, and evaluation metrics.Baselines The experiments for the two tasks use the same baselines as follows. \u2022 Random Guess: prints a random ranking for tests. \u2022 BM25: is a popular and strong baseline for gathering information (Robertson et al. 1995). \u2022 ARC-I: uses CNNs to construct sentence representations and relies on an MLP to produce the final matching score (Hu et al. 2014). \u2022 ARC-II: First, generates local matching patterns, and then composes them through multiple evolution layers to produce the matching score (Hu et al. 2014). \u2022 CNTN: based on the structure of ARC-I, but others use a tensor layer to calculate the matching score, instead of an MLP (Qiu and 2015 Huang-LN)."}, {"heading": "Question Answering", "text": "In fact, most of them are able to survive on their own if they do not put themselves in the position they are in."}, {"heading": "Sentence Completion", "text": "In this section, we show our experimental completion results, which attempt to match the first and second sentences in the same sentence. We use exactly the same data set constructed in (Hu et al. 2014) by Reuters (Lewis et al. 2004). Specifically, the sentences that share two \"balanced\" clauses (with 8-28 words) are extracted by a comma from the original Reuters data set, and the two clauses form a positive matching pair. In the case of negative examples, the first sentence is retained and the second sentence is sampled from other clauses similar to it in cosmic similarity. For each positive example, 4 negative examples are constructed, so we also get 20% for P @ 1. The experimental results are listed in Table 5. As we use the same data, some baseline results are quoted directly from (Hu et al. 2014), such as ACR-I, ARCII, RAE and DeepMatch."}, {"heading": "Conclusions", "text": "One advantage of our model is that it can capture both local information and rich context information to determine the meaning of local keywords from the whole sentence view. Our experimental results and case studies show some valuable insights: (1) Assuming that the final matching is determined exclusively by an interaction (i.e. the pooling parameter is set to 1), MV-LSTM can achieve better results than all single sentence representation methods including LSTM-RNN. This means that the best matching position is not always in the last one, so the consideration of multiple position set representations is necessary. (2) If we allow the aggregation of multiple interactions (i.e. the pooling parameter is fixed to greater than 1), MV-LSTM can achieve even better results, which means that the matching degree is usually determined by the combination of matches at different sentence positions (i.e. the compilation of multiple sentences is more effective than multiple representation)."}, {"heading": "Acknowledgments", "text": "This work was funded by the 973 Program of China under grant numbers 2014CB340401 and 2012CB316303, 863 Program of China under grant numbers 2014AA015204, the National Natural Science Foundation of China (NSFC) under grant numbers 61472401, 61433014, 61425016, 61203298 and 61425016, Key Research Program of the Chinese Academy of Sciences under grant numbers KGZD-EW-T03-2 and Youth Innovation Promotion Association CAS under grant numbers 20144310."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Cho Bahdanau", "D. Bengio 2014] Bahdanau", "K. Cho", "Y. Bengio"], "venue": "CoRR abs/1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Bridging the Lexical Chasm: Statistical Approaches to Answer-finding", "author": ["Berger"], "venue": "In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "Berger,? \\Q2000\\E", "shortCiteRegEx": "Berger", "year": 2000}, {"title": "Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources", "author": ["Quirk Dolan", "B. Brockett 2004] Dolan", "C. Quirk", "C. Brockett"], "venue": "In Proceedings of the 20th International Conference on Computational Linguistics (Coling),", "citeRegEx": "Dolan et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Dolan et al\\.", "year": 2004}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Hazan Duchi", "J. Singer 2011] Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research 12:2121\u20132159", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Deep sparse rectifier networks", "author": ["Bordes Glorot", "X. Bengio 2011] Glorot", "A. Bordes", "Y. Bengio"], "venue": "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Mohamed Graves", "A. Hinton 2013] Graves", "A.-r. Mohamed", "G. Hinton"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "R", "author": ["Greff, K.", "Srivastava"], "venue": "K.; Koutn\u0131\u0301k, J.; Steunebrink, B. R.; and Schmidhuber, J.", "citeRegEx": "Greff et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Schmidhuber", "author": ["S. Hochreiter"], "venue": "J.", "citeRegEx": "Hochreiter and Schmidhuber 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Hu"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Hu,? \\Q2014\\E", "shortCiteRegEx": "Hu", "year": 2014}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["Huang"], "venue": "In Proceedings of the 22nd ACM International Conference on Information & Knowledge Management (CIKM),", "citeRegEx": "Huang,? \\Q2013\\E", "shortCiteRegEx": "Huang", "year": 2013}, {"title": "A convolutional neural network for modelling sentences", "author": ["Grefenstette Kalchbrenner", "N. Blunsom 2014] Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "T", "author": ["D.D. Lewis", "Y. Yang", "Rose"], "venue": "G.; and Li, F.", "citeRegEx": "Lewis et al. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "and Xu", "author": ["H. Li"], "venue": "J.", "citeRegEx": "Li and Xu 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Li", "author": ["Z. Lu"], "venue": "H.", "citeRegEx": "Lu and Li 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "G", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "Corrado"], "venue": "S.; and Dean, J.", "citeRegEx": "Mikolov et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "R", "author": ["H. Palangi", "L. Deng", "Y. Shen", "J. Gao", "X. He", "J. Chen", "X. Song", "Ward"], "venue": "K.", "citeRegEx": "Palangi et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Huang", "author": ["X. Qiu"], "venue": "X.", "citeRegEx": "Qiu and Huang 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "M", "author": ["S.E. Robertson", "S. Walker", "S. Jones", "Hancock-Beaulieu"], "venue": "M.; Gatford, M.; and Others.", "citeRegEx": "Robertson et al. 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "K", "author": ["M. Schuster", "Paliwal"], "venue": "K.", "citeRegEx": "Schuster and Paliwal 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval", "author": ["Shen"], "venue": "In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management", "citeRegEx": "Shen,? \\Q2014\\E", "shortCiteRegEx": "Shen", "year": 2014}, {"title": "C", "author": ["R. Socher", "E.H. Huang", "J. Pennington", "A.Y. Ng", "Manning"], "venue": "D.", "citeRegEx": "Socher et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Socher"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Socher,? \\Q2013\\E", "shortCiteRegEx": "Socher", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Socher"], "venue": "Proceedings of the Conference", "citeRegEx": "Socher,? \\Q2013\\E", "shortCiteRegEx": "Socher", "year": 2013}, {"title": "Convolutional Neural Network for Paraphrase Identification", "author": ["Yin", "W. Sch\u00fctze 2015a] Yin", "H. Sch\u00fctze"], "venue": "In The 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL),", "citeRegEx": "Yin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2015}, {"title": "MultiGranCNN: An Architecture for General Matching of Text Chunks on Multiple Levels of Granularity", "author": ["Yin", "W. Sch\u00fctze 2015b] Yin", "H. Sch\u00fctze"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Yin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2015}], "referenceMentions": [], "year": 2015, "abstractText": "Matching natural language sentences is central for many applications such as information retrieval and question answering. Existing deep models rely on a single sentence representation or multiple granularity representations for matching. However, such methods cannot well capture the contextualized local information in the matching process. To tackle this problem, we present a new deep architecture to match two sentences with multiple positional sentence representations. Specifically, each positional sentence representation is a sentence representation at this position, generated by a bidirectional long short term memory (Bi-LSTM). The matching score is finally produced by aggregating interactions between these different positional sentence representations, through k-Max pooling and a multi-layer perceptron. Our model has several advantages: (1) By using Bi-LSTM, rich context of the whole sentence is leveraged to capture the contextualized local information in each positional sentence representation; (2) By matching with multiple positional sentence representations, it is flexible to aggregate different important contextualized local information in a sentence to support the matching; (3) Experiments on different tasks such as question answering and sentence completion demonstrate the superiority of our model.", "creator": "LaTeX with hyperref package"}}}