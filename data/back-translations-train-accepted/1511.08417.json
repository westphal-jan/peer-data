{"id": "1511.08417", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Nov-2015", "title": "TGSum: Build Tweet Guided Multi-Document Summarization Dataset", "abstract": "The development of summarization research has been significantly hampered by the costly acquisition of reference summaries. This paper proposes an effective way to automatically collect large scales of news-related multi-document summaries with reference to social media's reactions. We utilize two types of social labels in tweets, i.e., hashtags and hyper-links. Hashtags are used to cluster documents into different topic sets. Also, a tweet with a hyper-link often highlights certain key points of the corresponding document. We synthesize a linked document cluster to form a reference summary which can cover most key points. To this aim, we adopt the ROUGE metrics to measure the coverage ratio, and develop an Integer Linear Programming solution to discover the sentence set reaching the upper bound of ROUGE. Since we allow summary sentences to be selected from both documents and high-quality tweets, the generated reference summaries could be abstractive. Both informativeness and readability of the collected summaries are verified by manual judgment. In addition, we train a Support Vector Regression summarizer on DUC generic multi-document summarization benchmarks. With the collected data as extra training resource, the performance of the summarizer improves a lot on all the test sets. We release this dataset for further research.", "histories": [["v1", "Thu, 26 Nov 2015 15:22:54 GMT  (64kb,D)", "http://arxiv.org/abs/1511.08417v1", "7 pages, 1 figure in AAAI 2016"]], "COMMENTS": "7 pages, 1 figure in AAAI 2016", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["ziqiang cao", "chengyao chen", "wenjie li", "sujian li", "furu wei", "ming zhou 0001"], "accepted": true, "id": "1511.08417"}, "pdf": {"name": "1511.08417.pdf", "metadata": {"source": "CRF", "title": "TGSum: Build Tweet Guided Multi-Document Summarization Dataset", "authors": ["Ziqiang Cao", "Chengyao Chen", "Wenjie Li", "Sujian Li", "Furu Wei", "Ming Zhou"], "emails": ["cswjli}@comp.polyu.edu.hk", "lisujian@pku.edu.cn", "mingzhou}@microsoft.com"], "sections": [{"heading": "Introduction", "text": "The rapid growth of digital content requires efficient automated summaries with 376 data The rapid growth of digital content requires efficient automated summaries of systems. To date, learning-based models have become the dominant summarization approach. Despite decades of research, the quality of machine-generated summaries is still far from satisfactory. A major bottleneck of tweets, the most common of which are from Document Understanding Conferences2 (DUC) in 01, 02 and Copyright c (2016), Association for the Advancement of Artificial Intelligence (www.aaai.org). In this task, the most commonly used datasets are published by Document Understanding Conferences2 (DUC)."}, {"heading": "TGSum Construction", "text": "This section explains how we build TGSum, which is a data set that summarizes multiple documents guided by tweets. There are four main steps: The step of collecting URLs to capture URLs for messages; these URLs are used in the step of collecting data to extract the linked tweets and message documents; then, message documents are bundled based on the hashtags embedded in tweets; and finally, an Integer Linear Programming (ILP) solution is developed for the summary message documents to create reference summaries that cover as many key points as possible. Below is a detailed description of these steps."}, {"heading": "URL Acquisition", "text": "Initially, we tried to extract linked tweets directly by looking for trends on Twitter. However, most trends are entertainment related and direct people to the image or video pages, so they are not suitable for summarizing documents. Therefore, we are devising an alternative strategy that first detects news URLs. Specifically, through the Twitter search function, 74 active news accounts such as the New York Times, Reuters and CNN that have posted tweets within a month are selected as seed users. All DUC news providers are included to ensure that the generated data set is consistent with DUC. Next, we will use the Twitter user streaming API to track all tweets from the Seed users from August 13 to September 13. Despite many imitations of news titles, these tweets provide the URLs of hot news published by the relevant news accounts. We do not focus on a specific domain or type of news. From the observation, the Open News summaries collected come from a broad range of topics, including finance, and so on."}, {"heading": "Data Collection", "text": "The news content is retrieved with the open Python Message3 package. We reserve only the main body of a document. We then use the Twitter Term Search API to extract linked tweets, and perform careful pre-processing as follows: \u2022 Discard retweets; 3https: / / pypi.python.org / pypi / newspaper \u2022 Delete non-English tokens in a tweet; \u2022 Remove tweets containing less than 5 tokens; \u2022 Merge identical tweets. Finally, we collect 13207 valid tweets from 4483 news documents."}, {"heading": "Cluster Formation", "text": "After removing generic hashtags such as # ThisWeek and # ICYMI (i.e. In Case You Missed It), the news-related hashtags are usually the key terms of the event. # GreeceCrisis, for example, refers to the Greek financial crisis. If we continue to confine ourselves to bundling documents on the same day, it is very likely that the documents to which the same hashtag refers describe the same messages. In a document that is not attached to a hashtag, we insert it into an existing cluster if its cosmic resemblance to the cluster exceeds a threshold, such as 0.5 as we specified. After removing clusters with fewer than 3 documents or fewer than 8 linked tweets, we retain 1114 documents in 204 clusters."}, {"heading": "Reference Generation", "text": "As mentioned above, most tweets are incomplete and contain sounds that are not adequately contained in a summary. As the sentences in the original news documents are usually well-written, we opt for \"synthetic\" summaries by selecting sentences from both news documents and high-quality tweets, as long as they are good representatives of the news information. We expect the generated reference sums to cover most of the key points of the tweets. Here, we take the number of overlapping units such as the n-grams, word sequences or word pairs between the two parts of text. Let's take the widelyused ROUGE-2 as an example."}, {"heading": "Experiment", "text": "First, we check the content of the collected linked tweets. We check whether they really deliver specific message focal points. Then, we analyze the quality of the generated summaries. Automatic as well as manual evaluations are performed. Finally, we design a monitored summary model to verify the effect of the TGSum dataset."}, {"heading": "Linked Tweet Analysis", "text": "During the data collection, we found that most linked tweets are just message title extractions, but each document can still receive an average of three unique linked tweets. We categorize these unique tweets according to the summary task. Extraction: The tweet is extracted directly from the original text. Compression: Over 80% of the words in a tweet can be found in a document set (except extraction). Others: The other tweets. In our observation, most of them are comments. An example of each tweet type in Table 3. All four tweets come from a message cluster that describes # BangkokBlast. We use bold font to indicate the words that appear in the news. It is noteworthy that the original document uses the word \"believe,\" while the abstract tweets are a direct behavior. The four tweets come from a news cluster that describes # BangkokBlast."}, {"heading": "Quality of TGSum", "text": "ROUGE Evaluation Now we verify the accuracy of our ILP-based ROUGE upper bound generation algorithm. We select \u03bb? 0, 1, 0.0001, which means selected sentences according to ROUGE-2, ROUGE-1 and their combination. We refer to the corresponding model sentences as UB-1, UB-2 and UB-combination respectively. In contrast, we design a baseline using the greedy tweet. It iteratively adds the sentence that brings the maximum ROUGE gain into the summary. Likewise, the summaries generated according to ROUGE-1 and ROUGE-2 are referred to formal summaries as GA-1 and GA-2. ROUGE scores measured by linked tweets are shown in Table 4. Obviously, the ILP solution achieves much higher ROUGE scores than the greedy algorithm. Then we compare three types of summaries derived from different equals. It is pointed out that both UB-2 and UB-2-combination."}, {"heading": "Effect of TGSum", "text": "To test the effect of TGSum, we examine whether it can be used to improve the performance of summary systems on DUC benchmarks. To be specific, each sentence in the training set is evaluated by ROUGE-2. Then, we extract characteristics such as TF (the averaged TF values of the set), Length (sentence length), and STOP-RATIO (the ratio of stop words), and train SVR to measure the emphasis of sentences. To test, we follow the greedy algorithm (Li and Li 2014) to select important sentences in a summary. According to (Cao et al. 2015b), the SUC SVR summary test achieves competitive performance against the best data sets in DUC. It is a standard learning-based summary model that is sufficient to emphasize the impact of these training results."}, {"heading": "Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Summarization with Twitter", "text": "Due to the lack of reference summaries, most researchers have to use unattended methods. (Sharifi, Hutton and Kalita 2010a) identified important phrases in tweets using a graph-based algorithm. However, soon the authors (Sharifi, Hutton and Kalita 2010b) developed a simpler \"hybrid TF-IDF\" method that graded tweets using the TF-IDF scheme and achieved even better results. More complicated work was reported by (Liu, Liu and Weng 2011), which relied on Integer Linear Programming to extract sentences with the most prominent n-grams. It is worth noting that this paper highlighted the use of documents associated with the tweet set. Importance was measured using TF in the documents, and their experiments showed that summary sentences could be selected from both tweets and documents."}, {"heading": "ILP for Summarization", "text": "Integer Linear Programming was widely used in the summary because it can adequately model the state of item selection. (McDonald 2007) ILP originally introduced in this area. He constructed summaries by maximizing the importance of the selected sentences and minimizing their pair similarities, which was the extension of a greedy approach called Maximum Marginal Relevance (MMR) (Carbonell and Goldstein 1998). Given the N sentences, his Model O (N2) contains binary variables. Therefore, it is quite inefficient in finding the optimal solution. Later (Gillick and Favre 2009) suggested treating the summary as concept coverage maximization, with redundancy implicitly measured by including each concept only once. They used two grams as concept representation. The same idea was pursued by many researchers (Woodsend and Lapata 2012; Li, Qian and Liu 2013)."}, {"heading": "Conclusion", "text": "We use hashtags to cluster documents on different topics and then \"synthesize\" reference summaries that are able to cover the key points embedded in the linked tweets within the cluster. To measure the coverage ratio, we use ROUGE metrics and develop an ILP solution to determine its upper limit. Manual evaluation confirms the informativeness and legibility of the reference summaries collected. In addition, we train an SVR summarizer to provide general summary benchmarks for multiple documents. With the collected data as an additional training resource, the performance of this summarizer significantly improves in all test sets.The current work focuses on generic summaries of multiple documents. However, we believe that our dataset can be used in many other scenarios. On the one hand, the compression of linked tweets is an ideal source for learning this summary of multiple documents."}], "references": [{"title": "Jointly learning to extract and compress", "author": ["Gillick Berg-Kirkpatrick", "T. Klein 2011] BergKirkpatrick", "D. Gillick", "D. Klein"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume", "citeRegEx": "Berg.Kirkpatrick et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Berg.Kirkpatrick et al\\.", "year": 2011}, {"title": "Abstractive multi-document summarization via phrase selection and merging", "author": ["Bing"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint", "citeRegEx": "Bing,? \\Q2015\\E", "shortCiteRegEx": "Bing", "year": 2015}, {"title": "Ranking with recursive neural networks and its application to multi-document summarization", "author": ["Cao"], "venue": "In Twenty-Ninth AAAI Conference on Artificial Intelligence", "citeRegEx": "Cao,? \\Q2015\\E", "shortCiteRegEx": "Cao", "year": 2015}, {"title": "Learning summary prior representation for extractive summarization", "author": ["Cao"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference", "citeRegEx": "Cao,? \\Q2015\\E", "shortCiteRegEx": "Cao", "year": 2015}, {"title": "The use of mmr, diversity-based reranking for reordering documents and producing summaries", "author": ["Carbonell", "J. Goldstein 1998] Carbonell", "J. Goldstein"], "venue": "In Proceedings of SIGIR,", "citeRegEx": "Carbonell et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Carbonell et al\\.", "year": 1998}, {"title": "Inferring topic-dependent influence roles of twitter users", "author": ["Chen"], "venue": "In Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval,", "citeRegEx": "Chen,? \\Q2014\\E", "shortCiteRegEx": "Chen", "year": 2014}, {"title": "Overcoming the lack of parallel data in sentence compression", "author": ["Filippova", "K. Altun 2013] Filippova", "Y. Altun"], "venue": null, "citeRegEx": "Filippova et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Filippova et al\\.", "year": 2013}, {"title": "A scalable global model for summarization", "author": ["Gillick", "D. Favre 2009] Gillick", "B. Favre"], "venue": "In Proceedings of the Workshop on ILP for NLP,", "citeRegEx": "Gillick et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gillick et al\\.", "year": 2009}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Hu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hu,? \\Q2014\\E", "shortCiteRegEx": "Hu", "year": 2014}, {"title": "Query-focused multi-document summarization: Combining a topic model with graph-based semi-supervised learning", "author": ["Li", "Y. Li 2014] Li", "S. Li"], "venue": "In Proceedings of COLING,", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Multi-document summarization using support vector regression", "author": ["Li"], "venue": "In Proceedings of DUC", "citeRegEx": "Li,? \\Q2007\\E", "shortCiteRegEx": "Li", "year": 2007}, {"title": "Using supervised bigram-based ilp for extractive summarization", "author": ["Qian Li", "C. Liu 2013] Li", "X. Qian", "Y. Liu"], "venue": "In Proceedings of ACL,", "citeRegEx": "Li et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Li et al\\.", "year": 2013}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Lin", "C.-Y"], "venue": "In Proceedings of the ACL Workshop,", "citeRegEx": "Lin and C..Y.,? \\Q2004\\E", "shortCiteRegEx": "Lin and C..Y.", "year": 2004}, {"title": "Why is sxsw trending?: exploring multiple text sources for twitter topic summarization", "author": ["Liu Liu", "F. Weng 2011] Liu", "Y. Liu", "F. Weng"], "venue": "In Proceedings of the Workshop on Languages in Social Media,", "citeRegEx": "Liu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2011}, {"title": "Towards automatic tweet generation: A comparative study from the text summarization perspective in the journalism genre. Expert Systems with Applications 40(16):6624\u20136630", "author": ["Lloret", "E. Palomar 2013] Lloret", "M. Palomar"], "venue": null, "citeRegEx": "Lloret et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lloret et al\\.", "year": 2013}, {"title": "An assessment of the accuracy of automatic evaluation in summarization", "author": ["Owczarzak"], "venue": "In Proceedings of Workshop on Evaluation Metrics and System Comparison for Automatic Summarization,", "citeRegEx": "Owczarzak,? \\Q2012\\E", "shortCiteRegEx": "Owczarzak", "year": 2012}, {"title": "Fear the reaper: A system for automatic multi-document summarization with reinforcement learning", "author": ["Hasan Rioux", "C. Chali 2014] Rioux", "S.A. Hasan", "Y. Chali"], "venue": "In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP),", "citeRegEx": "Rioux et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rioux et al\\.", "year": 2014}, {"title": "Unsupervised extractive summarization via coverage maximization with syntactic and semantic concepts", "author": ["Schluter", "N. S\u00f8gaard 2015] Schluter", "A. S\u00f8gaard"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International", "citeRegEx": "Schluter et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schluter et al\\.", "year": 2015}, {"title": "Summarizing microblogs automatically", "author": ["Hutton Sharifi", "B. Kalita 2010a] Sharifi", "M.A. Hutton", "J. Kalita"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Sharifi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sharifi et al\\.", "year": 2010}, {"title": "Experiments in microblog summarization", "author": ["Hutton Sharifi", "B. Kalita 2010b] Sharifi", "M.A. Hutton", "J.K. Kalita"], "venue": "In Social Computing (SocialCom),", "citeRegEx": "Sharifi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sharifi et al\\.", "year": 2010}, {"title": "Utilizing microblogs for automatic news highlights extraction. COLING", "author": ["Wei", "Z. Gao 2014] Wei", "W. Gao"], "venue": null, "citeRegEx": "Wei et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2014}, {"title": "Multiple aspect summarization using integer linear programming", "author": ["Woodsend", "K. Lapata 2012] Woodsend", "M. Lapata"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Woodsend et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Woodsend et al\\.", "year": 2012}, {"title": "Social context summarization", "author": ["Yang"], "venue": "In Proceedings of the 34th international ACM SIGIR confer-", "citeRegEx": "Yang,? \\Q2011\\E", "shortCiteRegEx": "Yang", "year": 2011}, {"title": "Moodlens: an emoticon-based sentiment analysis system for chinese tweets", "author": ["Zhao"], "venue": "In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Zhao,? \\Q2012\\E", "shortCiteRegEx": "Zhao", "year": 2012}], "referenceMentions": [], "year": 2015, "abstractText": "The development of summarization research has been significantly hampered by the costly acquisition of reference summaries. This paper proposes an effective way to automatically collect large scales of news-related multi-document summaries with reference to social media\u2019s reactions. We utilize two types of social labels in tweets, i.e., hashtags and hyper-links. Hashtags are used to cluster documents into different topic sets. Also, a tweet with a hyper-link often highlights certain key points of the corresponding document. We synthesize a linked document cluster to form a reference summary which can cover most key points. To this aim, we adopt the ROUGE metrics to measure the coverage ratio, and develop an Integer Linear Programming solution to discover the sentence set reaching the upper bound of ROUGE. Since we allow summary sentences to be selected from both documents and highquality tweets, the generated reference summaries could be abstractive. Both informativeness and readability of the collected summaries are verified by manual judgment. In addition, we train a Support Vector Regression summarizer on DUC generic multi-document summarization benchmarks. With the collected data as extra training resource, the performance of the summarizer improves a lot on all the test sets. We release this dataset for further research.", "creator": "LaTeX with hyperref package"}}}