{"id": "1511.08456", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Nov-2015", "title": "A Symbolic SAT-Based Algorithm for Almost-Sure Reachability with Small Strategies in POMDPs", "abstract": "POMDPs are standard models for probabilistic planning problems, where an agent interacts with an uncertain environment. We study the problem of almost-sure reachability, where given a set of target states, the question is to decide whether there is a policy to ensure that the target set is reached with probability 1 (almost-surely). While in general the problem is EXPTIME-complete, in many practical cases policies with a small amount of memory suffice. Moreover, the existing solution to the problem is explicit, which first requires to construct explicitly an exponential reduction to a belief-support MDP. In this work, we first study the existence of observation-stationary strategies, which is NP-complete, and then small-memory strategies. We present a symbolic algorithm by an efficient encoding to SAT and using a SAT solver for the problem. We report experimental results demonstrating the scalability of our symbolic (SAT-based) approach.", "histories": [["v1", "Thu, 26 Nov 2015 17:33:05 GMT  (20kb)", "http://arxiv.org/abs/1511.08456v1", "Full version of \"A Symbolic SAT-based Algorithm for Almost-sure Reachability with Small Strategies in POMDPs\" AAAI 2016"]], "COMMENTS": "Full version of \"A Symbolic SAT-based Algorithm for Almost-sure Reachability with Small Strategies in POMDPs\" AAAI 2016", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["krishnendu chatterjee", "martin chmelik", "jessica davies"], "accepted": true, "id": "1511.08456"}, "pdf": {"name": "1511.08456.pdf", "metadata": {"source": "CRF", "title": "A Symbolic SAT-based Algorithm for Almost-sure Reachability with Small Strategies in POMDPs", "authors": ["Krishnendu Chatterjee"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 151 1,08 456v 1 [cs.A I] 2 6"}, {"heading": "1 Introduction", "text": "The de facto models for dynamic systems with probabilistic and non-deterministic behavior are Markov Decision Processes (MDPs) [29]. MDPs provide the appropriate model for solving control and probabilistic planning problems [26, 39], where nondeterminism is the choice of control measures for the controller (or planner), while the stochastic response of the system to control measures is represented by the likely behavior. In Perfect Observation (or Perfect Information) MDPs, in order to resolve non-deterministic decisions between control measures, the controller closely observes the current state of the system, whereas in partially observable MDPs (POMDPs) the state space is partitioned according to observations that the controller can observe, i.e. the controller can only observe the current state (the partition of the state is part of it), but not the exact state [36]. POMDPs are often used in various applications, such as computer-assisted biology [35]."}, {"heading": "2 Preliminaries", "text": "Definition 1 POMDPs. A Partially Observable Markov Decision Process (POMDP) is defined as tuples P = (S, A, \u03b4, Z, O, I), where: (i) S is a finite series of states; (ii) A is a finite alphabet of actions; (iii) D (S) is a probable transitional function which, in the face of a state s and an action, gives a finite distribution of states, i.e., A is a finite alphabet of actions (s) (s) (s) denotes the transitional probability from s to s (s) given action a; (iv) Z is a finite series of observations; (v) I is the unique initial state; (vi) O: S \u2192 Z is an observation function which maps each state to an ob-conservation."}, {"heading": "3 Almost-Sure Reachability with Memoryless Strategies", "text": "In this section, we present our results with respect to the complexity of almost certain accessibility with memoryless strategies. First, we show that memoryless strategies for almost certain accessibility take a simple form. The following proposal states that it does not matter what positive probability an action is played with. Proposal 1 A POMDP P with an accessibility target Reach (T) has a memoryless accessibility strategy if and only if there is a memoryless accessibility strategy, such as that for all a, a \"A and s,\" if \u03c3 (a) (a) > 0 and \u03c3 (s) (a) > 0 then has a memoryless accessibility strategy if and only if there is no memoryless accessibility strategy. Intuitively, only a distinction is made between actions that must not be played and therefore have probability 0, and those that can be played (probabilities > 0). This statement implies that we do not set exact values for the positive probabilities when we need to design a strategy."}, {"heading": "3.1 SAT Encoding for Memoryless Strategies", "text": "Next, we will show how to define the near-secure accessibility problem for memory-less strategies as a flagged SAT problem. (...) We will define a proposed formula that has a holistic parameter for a memory-less, almost-secure accessibility strategy. (...) We will define a pre-defined strategy for memory-less, almost-secure accessibility strategy. (...) We will seek a function of states to sub-areas of actions. (...) (...) (...) If we have a memory-less, almost-secure accessibility strategy), so that for each state s (...) there is a pre-defined strategy that is compatible with some states to sub-areas of actions. (...) The value of k will be a parameter of SAT coding."}, {"heading": "4 Almost-Sure Reachability with Small-Memory Strategies", "text": "For some POMDPs, a memory-less strategy, which almost certainly wins, may not exist. However, in some cases, it may be helpful to give the agent a small amount of memory. We extend our SAT approach to the case of small memory strategies in this section. Definition 4 A small memory strategy is a strategy with memory. Proposals 1 and 2 and Theorem 1 apply to the case of small memory strategies. Proposal 3 A POMDP P with accessibility goal Reach (T) has a strategy for obtaining smaller memory states, \u00b5, as the size of the small memory strategy. Proposals 1 and Theorem 1 are transferable to the case of small memory strategies. Proposal 3 A POMDP P with accessibility goal Reach (T) has a strategy for obtaining smaller memory states, \u00b5, as the size of the small memory strategy of size. Proposals 1 and Theorem 1 are transferable to the case of small memory strategies. Proposal 3 A POMDP P with accessibility goal Reach (T) has a strategy for obtaining smaller memory states, \u00b5, as the size of the small memory strategy of size. Suggestions 1 and Theorem 1 are transferable to the case of small memory strategies."}, {"heading": "4.1 SAT Encoding for Small-Memory Strategies", "text": "The SAT encoding from Section 3.1 is adapted for the purpose of finding small memory winning strategies. Enter a POMDP P, Reach (G), a finite set of memory states M of size, a initial memory state m0 \"M,\" and a path length k. \"We set a Boolean variable Ama for each memory state m\" M \"and action a\" A, \"a\" L, \"a\" L, \"a\" L, \"a\" L, \"a\" L, \"a\" L, \"a\" L, \"a\" L, \"a\" L, \"a\" L, \"a\" c, \"c, c, c, c, c, c, c, c, c\" c, c, c, c, c, c, c, c, c, c, c, c, c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, c, c, c, c, c, c, c, c, c, \"c, c, c, c, c, c, c,\" c, c, c, c, c, c, c, c, \"c, c, c, c,\" c, c, c, \"c, c, c, c,\" c, c, \"c, c,\" c, c, c, \"c, c,\" c, c, \"c,\" c, c, \"c, c, c,\" c, c, \"c, c, c, c,\" c, \"c,\" c, c, c, \"c,\" c, \"c, c, c,\" c, \"c,\" c, c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, c, \"c, c,\", \"c,\" c, c, c, \"c,\", \",\" c, \"c,\", \"c,\" c, \"c,\" c, \",\" c, \",\" c, \",\", \","}, {"heading": "5 Experimental Results", "text": "In this section, we present our experimental results, which show that small memory retrieval strategies exist for several realistic POMDPs that arise in practice. Our experimental results clearly demonstrate the scalability of our SAT-based approach, which delivers good performance even for large-scale robots where the previous explicit approach does not work well. We have implemented the coding for small storage strategies described in Section 4.1, as a small python program. We compare the explicit graph-based algorithms presented in [14]. This is the state of the art explicitly POMDP solver for almosture accessibility based on path-finding algorithms of [18] with a number of heuristics. We used the SAT solution Minisat, version 2.2.0. The experiments were conducted on an Intel (R) Xeon (R) with a time span of 30 minutes."}, {"heading": "6 Conclusion and Future Work", "text": "In this paper, we present the first symbolic SAT-based algorithm for near-secure accessibility in POMDPs. We have shown that the symbolic algorithm significantly outperforms the explicit algorithm, using a number of examples that resemble problems from literature. In future work, we plan to investigate the possibilities of the incremental SAT solution. Incremental SAT solutions can be beneficial in two ways: First, they can test the efficiency of algorithms to find the smallest near-secure winning strategy, and such an approach can build on our coding. Second, incremental SAT solutions could help in the event that the original POMDP is slightly modified to efficiently resolve the updated SAT instance. Studying the practical effects of incremental SAT solvers on POMDPs is the subject of future work."}, {"heading": "A Appendix: Detailed comparison of qualitative analysis and contingent planning.", "text": "We present examples that are almost certain that G will be achieved in steps. We first explain the conceptual difference and then illustrate the difference with examples. (In contingent planning, it is required that all paths reach the target status.) In other words, contingent planning treats the probable choice as a counterproductive choice. However, it is almost certain to win, although it is true that the exact probabilities do not matter, it is still different from treating the probable choice as a counterproductive answer. We first illustrate the difference with examples of Markov chains.Example 1 (Markov chains.) In Figure 1, we present a Markov Chain M1 (which is a perfect information MDP with a single action) with two states: the initial state s0 and the target state G. The probable transition function in state s0 selects the next state s0 with the probability 12, and the target state G with the remaining probability 1."}, {"heading": "B Appendix: Detailed comparison of qualitative analysis and strong cyclic planning.", "text": "The problem of strong cyclical reduction of our PSD was studied in the perfect information environment in [21] and later extended to the partial information environment in [4]. However, there are two crucial differences in our work wrt [4]: We look at the problem of finding small strategies compared to general strategies. We show that our problem NP is complete, while we determine that our problem NP is complete. Thus, there is a significant difference in the complexity of the problem finding small strategies compared to general strategies [15, 2]. Strong cyclical planning with general strategies taken into account in [4] is also EXPTIME-complete, while we find that our problem NP-complete."}, {"heading": "C Appendix: Deterministic Strategies", "text": "In this part we present a simple extension of our encryption strategy, which handles the case of deterministic strategies. Deterministic strategies. A strategy with memory function is deterministic if both functions \u03c3n and \u03c3u only assign Dirac probability distributions and can be written as follows: \u2022 The action selection function is of type \u03c3n: M \u2192 A. \u2022 The memory update function is of type \u03c3u: M \u00b7 Z \u00b7 A \u2192 M. We present the modification only for the more complicated case of smaller storage strategies, the modifications for the memory-less case are analogous. The selection function for the next action. The part of the encoding defining the selection function for the next action is:"}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "<lb>POMDPs are standard models for probabilistic planning problems, where an<lb>agent interacts with an uncertain environment. We study the problem of almost-<lb>sure reachability, where given a set of target states, the question is to decide<lb>whether there is a policy to ensure that the target set is reached with probability<lb>1 (almost-surely). While in general the problem is EXPTIME-complete, in many<lb>practical cases policies with a small amount of memory suffice. Moreover, the<lb>existing solution to the problem is explicit, which first requires to construct explic-<lb>itly an exponential reduction to a belief-support MDP. In this work, we first study<lb>the existence of observation-stationary strategies, which is NP-complete, and then<lb>small-memory strategies. We present a symbolic algorithm by an efficient encod-<lb>ing to SAT and using a SAT solver for the problem. We report experimental results<lb>demonstrating the scalability of our symbolic (SAT-based) approach.", "creator": "LaTeX with hyperref package"}}}