{"id": "1511.08551", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Nov-2015", "title": "Regularized EM Algorithms: A Unified Framework and Statistical Guarantees", "abstract": "Latent variable models are a fundamental modeling tool in machine learning applications, but they present significant computational and analytical challenges. The popular EM algorithm and its variants, is a much used algorithmic tool; yet our rigorous understanding of its performance is highly incomplete. Recently, work in Balakrishnan et al. (2014) has demonstrated that for an important class of problems, EM exhibits linear local convergence. In the high-dimensional setting, however, the M-step may not be well defined. We address precisely this setting through a unified treatment using regularization. While regularization for high-dimensional problems is by now well understood, the iterative EM algorithm requires a careful balancing of making progress towards the solution while identifying the right structure (e.g., sparsity or low-rank). In particular, regularizing the M-step using the state-of-the-art high-dimensional prescriptions (e.g., Wainwright (2014)) is not guaranteed to provide this balance. Our algorithm and analysis are linked in a way that reveals the balance between optimization and statistical errors. We specialize our general framework to sparse gaussian mixture models, high-dimensional mixed regression, and regression with missing variables, obtaining statistical guarantees for each of these examples.", "histories": [["v1", "Fri, 27 Nov 2015 03:46:36 GMT  (793kb)", "https://arxiv.org/abs/1511.08551v1", "53 pages, 3 figures. A shorter version appears in NIPS 2015"], ["v2", "Sat, 5 Dec 2015 09:54:59 GMT  (793kb)", "http://arxiv.org/abs/1511.08551v2", "53 pages, 3 figures. A shorter version appears in NIPS 2015"]], "COMMENTS": "53 pages, 3 figures. A shorter version appears in NIPS 2015", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["xinyang yi", "constantine caramanis"], "accepted": true, "id": "1511.08551"}, "pdf": {"name": "1511.08551.pdf", "metadata": {"source": "CRF", "title": "Regularized EM Algorithms: A Unified Framework and Statistical Guarantees", "authors": ["Xinyang Yi"], "emails": ["yixy@utexas.edu", "constantine@utexas.edu"], "sections": [{"heading": null, "text": "ar Xiv: 151 1.08 551v 2 [cs.L G] 5D eclatent variable models are a basic modeling tool in machine learning applications, but they pose significant computational and analytical challenges. The popular EM algorithm and its variants are a commonly used algorithmic tool; however, our rigorous understanding of its performance is highly incomplete. Recently, work in Balakrishnan et al. (2014) has shown that EM exhibits linear local convergence for an important class of problems. However, in a high-dimensional environment, the M step may not be well defined. It is precisely this attitude that we encounter through a uniform treatment by regulation. While the regulation of high-dimensional problems is now well understood, the iterative EM algorithm requires careful weighing of progress towards resolution, while determining the right structure (e.g. sparseness or low ranking). Specifically, the regulation of the M step by using the modal falsification algorithm does not require a high-time balance between the two."}, {"heading": "1 Introduction", "text": "In this paper, we provide general conditions and an analytical framework for the convergence of the EM method to high-dimensional parameter estimation in latent variable models. We specialize in several interesting problems, including high-dimensional sparse and low-level mixed regression, sparse Gaussian mixture models, and regression with missing covariants. As we explain below, the key problem is the high-dimensional adjustment of the M step. It is a natural idea to modify this step by appropriate regression, but the choice of the appropriate sequence of the regulators is a critical problem. As we know from the theory of regulated M estimators (e.g. Wainwright (2014), the regulator should be chosen in proportion to the error in the estimate. For EM, however, the error in the estimation changes at each step. The most important contribution of our work is technical. We show how this iterative regularization can be carried out."}, {"heading": "Background and Related Work", "text": "\"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"\" We. \"\" \"\" \".\" \"\" \"\" We. \"\" \"\" \"\" \"We.\" \"\" \"\" We. \"\" \"\" \"\" \"We.\" \"\" \"\" \"\" \"We.\" \"\" \"\" \"\" We. \"\" \"\" \"\" \"We.\" \"\" \"\" \"\" We. \"\" \"\" \"\" \"We.\" \"\" \"\" \"\" We. \"\". \"\" \"\" \"\" \"We.\" \"\" \"\" \".\" \"\" \"\" \"\" We. \"\" \"\" \"\" \"\" \"\" \"\" We. \"\" \"\" \"\" \"\" \"\" \"\" \"\" We. \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" We. \"\" \".\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"We.\" \"\" \"\" \"\" \""}, {"heading": "2 Regularized EM Algorithm", "text": "In this section, we first present a generally regulated EM algorithm, which uses a convex regulator to enforce certain types of structure, then turn to three well-known latent variable models and show how the proposed algorithm can be applied to high-dimensional parameter estimates in these models."}, {"heading": "2.1 Algorithm", "text": "Before introducing our approach, let us first examine the classical EM algorithm. Let us leave Y, Z and \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 in Y, Z. Let us assume that they adhere to the distribution variable (y, z), depending on the model parameters \u03b2 and S, where there is a certain parameter space in Rp. In latent variable models, it is common to assume that we can only obtain samples from Y, while the latent variable cannot be observed. Let us consider the boundary distribution of Y asy\u03b2 (y, z) currently given n i.d. observations y1, y2, y2,., yn of Y, our goal is to estimate the model parameter \u03b2. Let us consider the maximum probability of the estimation that the log function is maximized, namely, \u03b2 and argmax \u03b2 \u03b2 \u03b2 \u00df \u00df \u00df \u00df \u00df \u00df \u00df \u00df \u00df in the basic function."}, {"heading": "2.2 Example Models", "text": "Now we present three well-known latent variable models: For each model we check the specific formulations of the standard EM algorithm, discuss the extensions in a high-dimensional setting and provide the implementations of high-dimensionally regulated EM iterations."}, {"heading": "2.2.1 Gaussian Mixture Model", "text": "We consider the balanced isotropic Gaussian mixing model (GMM) with two components in which the distribution of random variables (Y, Z) = Pr (Z = \u2212 1, 1) = 1 / 2. In this example, the probability density function of N (\u00b5, \u03a3) is used. Z is a latent variable that indicates the cluster ID of each sample. In this example, n i.d. samples {yi} ni = 1, function Qn (\u00b7 \u00b7) defined in (2,3) corresponds to QGMMn (\u03b2 \u2032 | \u03b2) = \u2212 12nn \u00b2 i = 1 [w (yi; \u03b2) = 1 \u2212 p \u2212 (1 \u2212 w (yi; \u03b2) = sparsely in (2,3) corresponds to the structure of QGMMn (\u03b2) = \u2212 12nn \u00b2 (MIS) = 1p (2,2)."}, {"heading": "2.2.2 Mixed Linear Regression", "text": "We assume that X (0, Ip), W (0, 2). In this setting we are with n i.e. samples {yi, xi} ni = 1 pair (Y, X). We assume that we have a pair (Y, X). < We assume that we have a pair (Y, X). < We assume that we have a pair (Y, X). < We assume that we have a pair (Y, X). < We assume that we have a pair (Y, X). < We assume that we have a pair (Y, X)."}, {"heading": "2.2.3 Missing Covariate Regression", "text": "As a last example, we consider the problem of missing covariate regression (MCR) = \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2. To understand this, we assume that each entry of xi is missing regardless of the probability (0, 1). To specify the positions of the missing entries, the observed covariate x-i takes the formx number i, j = {xi, j with the probability 1 \u2212. To simplify the notation, we introduce the vector zi (0, 1) p to specify the positions of the missing entries, i.e. zi, j = 1 if xi, j is missing. In this example, the E step includes the calculation of the distribution of the missing entries taking into account the current parameters rate \u03b2 \u03b2. Under Gaussian construction, we formally specify the vector zi (0, Ip), W \u00b2 n."}, {"heading": "3 General Computational and Statistical Guarantees", "text": "We now turn to the theoretical analysis of the high-dimensional regulated EM algorithm. In Section 3.1, we have created a general analytical framework for regulated EM, in which the key components are separable regulators and several technical conditions are population-based Q (\u00b7 | \u00b7) and sample-based Qn (\u00b7 | \u00b7). In Section 3.2, we first present a resampling version of Algorithm 1 and deliver our main result (Theorem 3.3), which characterizes both the computational and statistical performance of the proposed variant of the regulated EM algorithm."}, {"heading": "3.1 Framework", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1.1 Decomposable Regularizers", "text": "Decomposable regularizers as considered in a work (e.g., Candes and Tao (2007); Negahban et al. (2009); Wainwright (2014); Chen et al. (2014a), has proven useful, both empirically and theoretically, for high-dimensional structural estimation. It also plays an important role in our analytical framework. We begin with the assumption that R: Rp \u2192 R + is a norm, which gives us R (u + v) \u2264 R (u) + R (v)."}, {"heading": "3.1.2 Conditions on Q(\u00b7|\u00b7)", "text": "Next, we will consider three technical conditions originally proposed by Balakrishnan and others. (2014) \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 Q (2015) \u03b2 \u03b2 \u03b2 Q (2015) function. Remember that our theory was developed with the focus on an r neighbouring region around \u03b2, which is defined as B (r; \u03b2): = [u], [u \u2212 \u03b2], [u. \"We first assume that Q (\u00b7 \u03b2) is autonomous, as stated below. (Condition 1) Function Q (\u00b7 \u03b2; \u03b2) is self-consistent, namely\u03b2 = argmax \u03b2\" Q (\u03b2). It is generally assumed that the condition Q (\u00b7 \u03b2) is self-consistent, as stated below. (Condition 1). (Self-consistency) Condition 1 is always fulfilled by following the classical theory of the EM algorithm."}, {"heading": "3.1.3 Conditions on Qn(\u00b7|\u00b7)", "text": "Remember that Qn (\u00b7 \u00b7 S) is calculated by the finite number of samples according to (\u03b2 \u03b2 \u03b2 \u03b2) and (\u03b2 \u03b2 \u03b2 \u03b2 \u03b2). Let us now turn to the two conditions according to Qn (\u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 (\u03b2 \u03b2 \u03b2 \u03b2)). Our first condition, parallel to condition 2 via function Q (\u00b7 \u00b7 \u00b7 S), represents a curvature restriction to Qn (\u00b7 \u00b7 \u00b7 S) among a finite number of samples. To guarantee the estimation error, there could be instructions along which Qn (\u00b7 \u03b2) exists, which are flat, as we have observed in mixed linear regressions, and lack of covariate regression. In contrast to condition 2, we assume that Qn (\u00b7 \u00b7 S) is strongly defined by a certain amount of C (S, S; R), which is defined in terms of subjective regression and lack of covariate regression."}, {"heading": "3.2 Main Results", "text": "In this section we provide the theoretical guarantees for the regularized EM algorithm. Instead, we assume that algorithm 1 is analyzed directly, we introduce a resampling version of algorithm 1, which is good for conditions 4-5. The key idea is to split the entire data set into T-pieces and use a fresh piece of data in each iteration of the regularized EM. We present the details in Algorithm 2.Algorithm 2 High Dimensional Regularized EM algorithm with resampling input samples {yi} ni = 1, Number of iterations T, m = n / T, Initial regulation parameters (0) m, Regularizer R, Initial parameters \u03b2 (0), Estimated statistical error number < 1. 1: Evenly split {yi} ni = 1 in T disjoint subsets D1, D2,.., DT."}, {"heading": "4 Applications to Example Models", "text": "In this section, we apply our high-dimensional regulated algorithm and the analytical framework introduced in Section 3 to the above three sample models: Gaussian mixing model, mixed linear regression, and missing covariate regression. For each model, we provide the appropriate initialization condition, regularization update, computational convergence guarantee, and statistical rate based on the high-dimensional regulated EM iterations introduced in Section 2.2."}, {"heading": "4.1 Gaussian Mixture Model", "text": "We use our analytical framework to analyze the Gaussian Mixture Model (GMM) with its detailed model parameters. We remember that we look at the isotropic, balanced Gaussian Mixture Model with two components in which the Gaussian Mixture is generated from both sides. (4.1) We focus on the high SNR regime in which we assume SNR for some constant results. Note: The work in Ma and Xu (2005) provides empirical and theoretical evidence that the overlap density of two Gaussian clusters is small, standard EM algorithms suffer from sublinear convergence asymptotically."}, {"heading": "4.2 Mixed Linear Regression", "text": "We now turn to Mixed Linear Regression (MLR) (4,5)."}, {"heading": "4.3 Missing Covariate Regression", "text": "We consider the sparse recovery guarantee of algorithm 2 for missing covariant regression to be relative."}, {"heading": "5 Simulations", "text": "In this section we provide the simulation results to support our theory. Note that even our theory is based on resampling technique, it is statistically efficient to use partial data sets in practice. Consequently, we test the performance of the regulated EM algorithm without sample splitting (algorithm 1). We apply algorithm 1 to the four latent variable models introduced in Section 2.2: Gaussian mixing model (GMM), mixed linear regression with sparse vector (MLR sparse), mixed linear low-ranking matrix regression (MLR-LowRank) and missing covariate regression (MCR)."}, {"heading": "5.1 Convergence Rate", "text": "We first evaluate the convergence of algorithm 1 with good initialization \u03b2 (0) (in particular we use \u03b2) to specify a matrix initial parameter for the model MLR-LowRank, i.e. we choose the problem size n = 500, p = 800, s = 5. For MLR-LowRank we choose n = 600, p1 = p2 = p = 30, rang \u03b8 = 3. In addition, we set SNR = 5, g = 0.5 for GMM, MLR-Sparse and MLR-LowRank; we set SNR = 0.5, p = 0.5 and lack of probability for the final convergence T = 20% for the initial optimization it. The initialization error we have set is greater than the one provided by our theory."}, {"heading": "5.2 Statistical Rate", "text": "In the second group of experiments, we evaluate the relationship between the final estimation error of \u03b2 (T) -\u03b2 * 2 and the problem dimensions (n, p, s) or (n, p, \u03b8) for the above-mentioned latent variable models. The choice of algorithmic parameters, i.e. quantities B, B and O (0) n, and the initial parameter follow the first set of experiments in Section 5.1. In addition, we set T = 7 and allow the output \u03b2 = \u03b2 (T). In Figure 3, we determine s = 5 and v = 3 for related models. We find that the same normalized sample complexity results in practically identical estimation errors, which supports the corresponding statistical rate in Section 4."}, {"heading": "6 Proof of Main Result", "text": "In this section we provide the proof of theorem 3.3, which characterizes the computing power and statistical performance of the regularized EM algorithm using the resampling method. First, we present a result showing that the population EM operator M is contractive when the development < \u03b3.Lemma 6.1. Suppose Q (\u00b7 \u00b7) meets all the corresponding conditions set forth in theorem 3.3. Mapping M is contractive over B (r; \u03b2) - namely over M (\u03b2) - and beyond T (\u03b2). Suppose that Q (r; \u03b2) and B (r; \u03b2). A similar result is demonstrated in Balakrishnan et al. (2014). The slight difference is that Balakrishnan et al. (2014) shows Lemma 6.1 with its 2 standard. We assume that we consider the relationship between Q2 standard and arbitrary standard as trivial, so that we are now ready to prove the detail.3.3."}, {"heading": "A Proofs about Gaussian Mixture Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Proof of Lemma 4.1", "text": "In this example, we have M (\u03b2 *) = 2E [w (Y; \u03b2 *) Y] = 2E [11 + exp (\u2212 2 \u03c32 < Z \u00b7 \u03b2 * + W, \u03b2 * >) (Z \u00b7 \u03b2 * + W)], where W \u0445 N (0, \u03c32) and Z have a Rademacher distribution above {\u2212 1, 1}. Due to the rotational invariance of Gaussian equality, we assume without loss of generality \u03b2 \u0445 = Ae1. It is easy to check supp (M (\u03b2 *) = {1}. Besides, the first coordinate of M (\u03b2 *) takes shape (M (\u03b2 *)) 1 = 2E [11 + exp (\u2212 2 \u03c32 (AZ + W1)))) = A, where the last equality is replaced by X = W1, Z = Z, GA = 0, a = A in Lemma D.7. Hence M (\u03b2 *) = \u03b2 follows."}, {"heading": "A.2 Proof of Lemma 4.3", "text": "Condition 4 is a stochastic condition, but for the Gaussian mixing model it is particularly deterministically fulfilled. Note that QGMMn (\u03b2) = \u2212 12nn \u2211 i = 1 [w (yi; \u00df) Vestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestest"}, {"heading": "A.3 Proof of Lemma 4.4", "text": "Note that in this example, the two terms (a) and (b) are different. Following the specific formulations of QGMMn (b) and QGMM (b), it follows that QGMMn (b), QGMM (b) and QGMM (b), (b) and (4,2) are (b) and (b) random (b), random (b) and random (b), random (b), random (b), random (b) and random (b), random (b), random (b), random (b) and random (b) and (b), random (b), random (b), random (b), random (b) and (b)."}, {"heading": "B Proofs about Mixed Linear Regression", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1 Proof of Lemma 4.6", "text": "In this example, we have M (\u03b2 *) = 2E [w (Y, X; \u03b2 *) Y X] = 2E [11 + exp (\u2212 2 (< X, Z \u00b7 \u03b2 * > + W) < X, \u03b2 * > \u03c32) (Z \u00b7 \u03b2 * + W) X], where X-N (0, Ip), W-N (0, \u03c32), Z has the wheel maker distribution. Due to the rotational invariance of the gaussianity, without loss of universality, we can assume \u03b2-N = Ae1. It is easy to check Supp (M (\u03b2 *)) = {1}. Also, (M (\u03b2 *)) 1 = 2E [11 + exp (\u2212 2 \u03c32 (AZX1 + W) AX1) (AZX21 + X1W)] = E (AX21) = A, where the second inequality is followed by the substitution X = W, Z = Z (\u2212 2 \u03c31 (AZ1 + W) + AX1 (X1) = X1."}, {"heading": "B.2 Proof of Lemma 4.7", "text": "Remember that we hope to find such a solution for any given situation. \u2212 Reason (n) \u2212 Reason (n) \u2212 Reason (n) \u2212 Reason (n) \u2212 Reason (n) \u2212 Reason (n) \u2212 Reason (n) \u2212 Reason (n) \u2212 Reason (n) \u2212 Reason (n) \u2212 Reason (n) \u2212 Reason (n) \u2212 Reason (n) \u2212 Reason (n) \u2212 Reason (n) \u2212 Reason (n) \u2212 Reason (n) \u2212 Reason (n) \u2212 Reason (n) \u2212 Reason (n) \u2212 Reason (n) \u2212 Reason (n) \u2212 Reason (n) \u2212 Reason (n) \u2212 Reason (n) \u2212 Reason (n) \u2212 Reason (n) \u2212 Reason (n) \u2212 Reason (n) \u2212 Reason (n) \u2212 Reason (n) \u2212 Reason (n) \u2212 Reason (n) \u2212 Reason (n) \u2212 Reason (n) \u2212 Reason (n) \u2212 Reason (n)"}, {"heading": "B.3 Proof of Lemma 4.8", "text": "Remember that QMLRn (\u03b2, xi, \u03b2) 2 + (1 \u2212 w (yi, xi; \u03b2)) (yi + < xi, \u03b2 >) (yi + < xi, \u03b2) 2 + (1 \u2212 w (yi, xi; \u03b2))) (yi + < xi, \u03b2 >) 2]. For all \u03b2, \u03b2, \u03b2, \"\u03b2,\" \u03b2 \"Rp\" (\u03b2 \"\u03b2) \u2212 QMLRn (\u03b2\" \u03b2 \") \u2212 < QMLRn (\u03b2\" \u03b2 \"), \u03b2\" (\u03b2 \"\u03b2\") (1nn \"i = 1xix\" i) (\u03b2 \"\u03b2\" \u03b2 \"\u03b2\") (B.10) \u2212 Note that we want to find a solution that the right side of (B.10), \u03b2 \"\u2212 \u03b2\" (\u03b2 \"\u03b2\") is less than \u2212 2 (\u03b2 \"xi\")."}, {"heading": "B.4 Proof of Lemma 4.9", "text": "According to the formulations of QMLRn (b) and QMLR (b), we have in (2,12) and (4,5) each a \"QMLRn\" (b). \u2212 < \"QMLR\" (b). \u2212 < \"Y\" (b). \u2212 \"Y.\" (B). (B). (B). (B). (B). (B). (B). (B). (B). (B). (B). (B). (B). (B). (B). (B). (B). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G (G). (G). (G). (G). (G). (G). (G). (G). (G). (G. (G). (G). (G). (G). (G. (G). (G). (G). (G. (G). (G). (G). (G). (G. (G). (G). (G). (G). (G). (G). (G). (G. (G).). (G).). (G.). (G. (G). (G.).). (G. (G"}, {"heading": "B.5 Proof of Lemma 4.11", "text": "Similar to (B.10) we have this for every arbitrary result. (B.16) Note that this result applies to every result. (B.16) Note that this result applies to every result. (B.16) Let's be like this. (Xi) ni = 1 independent random sample of the random matrix X = 1 x Rp1 x p2, where the entries i.d.R. + 2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2 x p2"}, {"heading": "B.6 Proof of Lemma 4.12", "text": "In parallel with (B.12) we have \"QMLRn\" (\"QMLR\") (\"QMLR\") (\"QMLR\") (\"QMLR\") (\"QMLR\") (\"QMLR\") (\"QMLR\") (\"QMLR\") (\"QMLR\") (\"QMLR\") (\"QMLR\") (\"QMLR\") (\"QMLR\") (\"QMLR\") (\"QMX\") (\"QMLX) (\" QMLX) (\") MLX (\") MLX (\") MQLR (\") MLX (\") (\") MLR (\") MLX (\") (\") MLX (\") (\") MLX (\") (\") MLX (\") (\") MLX (\") (\"(\") MLX (\") (\") MLX (\") (\" (\") MLX (\") (\") MLX (\" (\")"}, {"heading": "C Proofs about Missing Covariate Regression", "text": "We start with a result that indicates several properties of the conditional correlation matrix that play an important role in assessing curvature conditions. Remember that the given samples (yi, zi, xi) and the correlation matrix of the population plane are defined (yi, zi, xi) in (2,16)."}, {"heading": "C.1 Proof of Lemma 4.14", "text": "In this example, M (\u03b2) = (E [E] (Y, Z, X)) \u2212 1 E [Y \u00b5\u03b2 (Y, Z, X)]. According to Lemma C.1, we now have E [Y \u00b5\u03b2 (Y, Z, X) = Ip. Meanwhile, we have E [Y \u00b5\u03b2 (Y, Z, X)] = E [(< \u03b2, X > + W) ((((1 \u2212 Z), X + < Z, \u03b2, X > + W, X > + W \u03c32 + HQ, 22Z, \u03b2)] = E [(1 \u2212 Z)."}, {"heading": "C.2 Proof of Lemma 4.15", "text": "If we follow Lemma C.1, Q MCR (\u00b7 | \u03b2) results in a 1-strongly concave concave. For each \u03b2-B (w-\u03b2) that follows (C.3), we have that QMCR (\u00b7 | \u03b2) is \u00b5-smooth, where \u00b5 = 1 + 2\u0445 2 - 1 + 2 - 2 - 2 - 2."}, {"heading": "C.3 Proof of Lemma 4.17", "text": "To show that QMCRn (1 \u2212 Zip.), QMCRn (1 \u2212 Zip.), QMCRn (2 \u2212 Zip.), QMCRn (2 \u2212 Zip.), QMCRn (2 \u2212 Zip.), QMCRn (2 \u2212 Zip.), QMCRn (2 \u2212 Zip.), QMCRn (2 \u2212 Zip.), QMCRn (2 \u2212 Zip.), QMCRn (2 \u2212 Zip.), QMCRn (2 \u2212 Zip.), QMCRn (2 \u2212 Zip.), QMCRn (2 \u2212 Zip.), QMCRn (2 \u2212 Zip.), QMCRn (4 \u2212 Zip.) (QMCRn) (QMCRn), QMCRn (2 \u2212 Zip.)"}, {"heading": "C.4 Proof of Lemma 4.18", "text": "In this example, the number of people residing in a country is greater than the number of people residing in a country. In this example, the number of people residing in a country is equal to the number of people residing in a country. In this example, the number of people residing in a country is the same. In this example, the number of people residing in a country is the same. In this example, the number of people residing in a country is the same. In this example, the number of people residing in a country is the same. In this example, the number of people residing in a country is the same. In this example, the number of people residing in a country is the same. In this example, the number of people residing in a country is the same."}, {"heading": "D Supporting Lemmas", "text": "Lemma D.1. Adopted X1, X2,.., Xn are n in general (2010). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (2011). (). (2011). (2011). (2011). (2011). (2011). (). (2011). (). (2011). (). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012).). (2012). (2012). (2012).). (2012). (2012).). (2012).). (2012). (2012).). (2012).). (2012). (2012).). (2012).). (2012).). (2012). (2012).). (2012). (2012).). (2012).). (2012).). (2012).). (2012).). (2012). (2012). (2012).). (2012). (2012).).). (2012). (2012).).). (2012.). (2012. (2012.). (2012.).). (2012.).)."}], "references": [{"title": "Statistical guarantees for the EM algorithm: From population to sample-based analysis", "author": ["Sivaraman Balakrishnan", "Martin J Wainwright", "Bin Yu"], "venue": "arXiv preprint arXiv:1408.2156,", "citeRegEx": "Balakrishnan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Balakrishnan et al\\.", "year": 2014}, {"title": "Rop: Matrix recovery via rank-one projections", "author": ["T Tony Cai", "Anru Zhang"], "venue": "The Annals of Statistics,", "citeRegEx": "Cai and Zhang.,? \\Q2015\\E", "shortCiteRegEx": "Cai and Zhang.", "year": 2015}, {"title": "A constrained 1 minimization approach to sparse precision matrix estimation", "author": ["Tony Cai", "Weidong Liu", "Xi Luo"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Cai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2011}, {"title": "The Dantzig selector: statistical estimation when p is much larger than n", "author": ["Emmanuel Candes", "Terence Tao"], "venue": "The Annals of Statistics,", "citeRegEx": "Candes and Tao.,? \\Q2007\\E", "shortCiteRegEx": "Candes and Tao.", "year": 2007}, {"title": "Tight oracle inequalities for low-rank matrix recovery from a minimal number of noisy random measurements", "author": ["Emmanuel J Cand\u00e8s", "Yaniv Plan"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Cand\u00e8s and Plan.,? \\Q2011\\E", "shortCiteRegEx": "Cand\u00e8s and Plan.", "year": 2011}, {"title": "Exact matrix completion via convex optimization", "author": ["Emmanuel J Cand\u00e8s", "Benjamin Recht"], "venue": "Foundations of Computational mathematics,", "citeRegEx": "Cand\u00e8s and Recht.,? \\Q2009\\E", "shortCiteRegEx": "Cand\u00e8s and Recht.", "year": 2009}, {"title": "Spectral experts for estimating mixtures of linear regressions", "author": ["Arun Tejasvi Chaganty", "Percy Liang"], "venue": "arXiv preprint arXiv:1306.3729,", "citeRegEx": "Chaganty and Liang.,? \\Q2013\\E", "shortCiteRegEx": "Chaganty and Liang.", "year": 2013}, {"title": "Improved graph clustering", "author": ["Yudong Chen", "Sujay Sanghavi", "Huan Xu"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "A convex formulation for mixed regression with two components: Minimax optimal rates", "author": ["Yudong Chen", "Xinyang Yi", "Constantine Caramanis"], "venue": "In Conf. on Learning Theory,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Exact and stable covariance estimation from quadratic sampling via convex programming", "author": ["Yuxin Chen", "Yuejie Chi", "Andrea Goldsmith"], "venue": "arXiv preprint arXiv:1310.0807,", "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Maximum likelihood from incomplete data via the em algorithm. Journal of the royal statistical society", "author": ["Arthur P Dempster", "Nan M Laird", "Donald B Rubin"], "venue": "Series B (methodological),", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Low-rank matrix completion using alternating minimization", "author": ["Prateek Jain", "Praneeth Netrapalli", "Sujay Sanghavi"], "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,", "citeRegEx": "Jain et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2013}, {"title": "High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity", "author": ["Po-Ling Loh", "Martin J Wainwright"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Loh and Wainwright.,? \\Q2011\\E", "shortCiteRegEx": "Loh and Wainwright.", "year": 2011}, {"title": "Corrupted and missing predictors: Minimax bounds for high-dimensional linear regression", "author": ["Po-Ling Loh", "Martin J Wainwright"], "venue": "In Information Theory Proceedings (ISIT),", "citeRegEx": "Loh and Wainwright.,? \\Q2012\\E", "shortCiteRegEx": "Loh and Wainwright.", "year": 2012}, {"title": "Asymptotic convergence properties of the em algorithm with respect to the overlap in the mixture", "author": ["Jinwen Ma", "Lei Xu"], "venue": null, "citeRegEx": "Ma and Xu.,? \\Q2005\\E", "shortCiteRegEx": "Ma and Xu.", "year": 2005}, {"title": "The EM algorithm and extensions, volume 382", "author": ["Geoffrey McLachlan", "Thriyambakam Krishnan"], "venue": null, "citeRegEx": "McLachlan and Krishnan.,? \\Q2007\\E", "shortCiteRegEx": "McLachlan and Krishnan.", "year": 2007}, {"title": "A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers", "author": ["Sahand Negahban", "Bin Yu", "Martin J Wainwright", "Pradeep K Ravikumar"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Negahban et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Negahban et al\\.", "year": 2009}, {"title": "Estimation of (near) low-rank matrices with noise and high-dimensional scaling", "author": ["Sahand Negahban", "Martin J Wainwright"], "venue": "The Annals of Statistics,", "citeRegEx": "Negahban and Wainwright,? \\Q2011\\E", "shortCiteRegEx": "Negahban and Wainwright", "year": 2011}, {"title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization", "author": ["Benjamin Recht", "Maryam Fazel", "Pablo A Parrilo"], "venue": "SIAM review,", "citeRegEx": "Recht et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Recht et al\\.", "year": 2010}, {"title": "L1-penalization for mixture regression models", "author": ["Nicolas St\u00e4dler", "Peter B\u00fchlmann", "Sara Van De Geer"], "venue": null, "citeRegEx": "St\u00e4dler et al\\.,? \\Q2010\\E", "shortCiteRegEx": "St\u00e4dler et al\\.", "year": 2010}, {"title": "An analysis of the em algorithm and entropy-like proximal point methods", "author": ["Paul Tseng"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Tseng.,? \\Q2004\\E", "shortCiteRegEx": "Tseng.", "year": 2004}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["Roman Vershynin"], "venue": "arXiv preprint arXiv:1011.3027,", "citeRegEx": "Vershynin.,? \\Q2010\\E", "shortCiteRegEx": "Vershynin.", "year": 2010}, {"title": "Structured regularizers for high-dimensional problems: Statistical and computational issues", "author": ["Martin J Wainwright"], "venue": "Annual Review of Statistics and Its Application,", "citeRegEx": "Wainwright.,? \\Q2014\\E", "shortCiteRegEx": "Wainwright.", "year": 2014}, {"title": "High dimensional expectationmaximization algorithm: Statistical optimization and asymptotic normality", "author": ["Zhaoran Wang", "Quanquan Gu", "Yang Ning", "Han Liu"], "venue": "arXiv preprint arXiv:1412.8729,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "On the convergence properties of the em algorithm", "author": ["C.F.Jeff Wu"], "venue": "The Annals of statistics,", "citeRegEx": "Wu.,? \\Q1983\\E", "shortCiteRegEx": "Wu.", "year": 1983}, {"title": "Alternating minimization for mixed linear regression", "author": ["Xinyang Yi", "Constantine Caramanis", "Sujay Sanghavi"], "venue": "arXiv preprint arXiv:1310.3745,", "citeRegEx": "Yi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yi et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Recently, work in Balakrishnan et al. (2014) has demonstrated that for an important class of problems, EM exhibits linear local convergence.", "startOffset": 18, "endOffset": 45}, {"referenceID": 0, "context": "Recently, work in Balakrishnan et al. (2014) has demonstrated that for an important class of problems, EM exhibits linear local convergence. In the high-dimensional setting, however, the M -step may not be well defined. We address precisely this setting through a unified treatment using regularization. While regularization for high-dimensional problems is by now well understood, the iterative EM algorithm requires a careful balancing of making progress towards the solution while identifying the right structure (e.g., sparsity or low-rank). In particular, regularizing the M -step using the state-of-the-art high-dimensional prescriptions (e.g., \u00e0 la Wainwright (2014)) is not guaranteed to provide this balance.", "startOffset": 18, "endOffset": 674}, {"referenceID": 22, "context": ", Wainwright (2014)) the regularizer should be chosen proportional to the target estimation error.", "startOffset": 2, "endOffset": 20}, {"referenceID": 9, "context": ", Dempster et al. (1977); McLachlan and Krishnan (2007)) is a general algorithmic approach for handling latent variable models (including mixtures), popular largely because it is typically computationally highly scalable, and easy to implement.", "startOffset": 2, "endOffset": 25}, {"referenceID": 9, "context": ", Dempster et al. (1977); McLachlan and Krishnan (2007)) is a general algorithmic approach for handling latent variable models (including mixtures), popular largely because it is typically computationally highly scalable, and easy to implement.", "startOffset": 2, "endOffset": 56}, {"referenceID": 9, "context": ", Dempster et al. (1977); McLachlan and Krishnan (2007)) is a general algorithmic approach for handling latent variable models (including mixtures), popular largely because it is typically computationally highly scalable, and easy to implement. On the flip side, despite a fairly long history of studying EM in theory (e.g., Wu (1983); Tseng (2004); McLachlan and Krishnan (2007)), very little has been understood about general statistical guarantees until recently.", "startOffset": 2, "endOffset": 335}, {"referenceID": 9, "context": ", Dempster et al. (1977); McLachlan and Krishnan (2007)) is a general algorithmic approach for handling latent variable models (including mixtures), popular largely because it is typically computationally highly scalable, and easy to implement. On the flip side, despite a fairly long history of studying EM in theory (e.g., Wu (1983); Tseng (2004); McLachlan and Krishnan (2007)), very little has been understood about general statistical guarantees until recently.", "startOffset": 2, "endOffset": 349}, {"referenceID": 9, "context": ", Dempster et al. (1977); McLachlan and Krishnan (2007)) is a general algorithmic approach for handling latent variable models (including mixtures), popular largely because it is typically computationally highly scalable, and easy to implement. On the flip side, despite a fairly long history of studying EM in theory (e.g., Wu (1983); Tseng (2004); McLachlan and Krishnan (2007)), very little has been understood about general statistical guarantees until recently.", "startOffset": 2, "endOffset": 380}, {"referenceID": 0, "context": "Very recent work in Balakrishnan et al. (2014) establishes a general local convergence theorem (i.", "startOffset": 20, "endOffset": 47}, {"referenceID": 0, "context": "Very recent work in Balakrishnan et al. (2014) establishes a general local convergence theorem (i.e., assuming initialization lies in a local region around true parameter) and statistical guarantees for EM, which is then specialized to obtain near-optimal rates for several specific low-dimensional problems \u2013 low-dimensional in the sense of the classical statistical setting where the samples outnumber the dimension. A central challenge in extending EM (and as a corollary, the analysis in Balakrishnan et al. (2014)) to the high-dimensional regime is the M -step.", "startOffset": 20, "endOffset": 519}, {"referenceID": 0, "context": "Very recent work in Balakrishnan et al. (2014) establishes a general local convergence theorem (i.e., assuming initialization lies in a local region around true parameter) and statistical guarantees for EM, which is then specialized to obtain near-optimal rates for several specific low-dimensional problems \u2013 low-dimensional in the sense of the classical statistical setting where the samples outnumber the dimension. A central challenge in extending EM (and as a corollary, the analysis in Balakrishnan et al. (2014)) to the high-dimensional regime is the M -step. On the algorithm side, the M -step will not be stable (or even well-defined in some cases) in the high-dimensional setting. To make matters worse, any analysis that relies on showing that the finite-sample M -step is somehow \u201cclose\u201d to theM -step performed with infinite data (the population-level M -step) simply cannot apply in the high-dimensional regime. Recent work in Wang et al. (2014) treats high-dimensional EM using a truncated M -step.", "startOffset": 20, "endOffset": 960}, {"referenceID": 0, "context": "Very recent work in Balakrishnan et al. (2014) establishes a general local convergence theorem (i.e., assuming initialization lies in a local region around true parameter) and statistical guarantees for EM, which is then specialized to obtain near-optimal rates for several specific low-dimensional problems \u2013 low-dimensional in the sense of the classical statistical setting where the samples outnumber the dimension. A central challenge in extending EM (and as a corollary, the analysis in Balakrishnan et al. (2014)) to the high-dimensional regime is the M -step. On the algorithm side, the M -step will not be stable (or even well-defined in some cases) in the high-dimensional setting. To make matters worse, any analysis that relies on showing that the finite-sample M -step is somehow \u201cclose\u201d to theM -step performed with infinite data (the population-level M -step) simply cannot apply in the high-dimensional regime. Recent work in Wang et al. (2014) treats high-dimensional EM using a truncated M -step. This works in some settings, but also requires specialized treatment for every different setting, precisely because of the difficulty with the M -step. In contrast to work in Wang et al. (2014), we pursue a high-dimensional extension via regularization.", "startOffset": 20, "endOffset": 1208}, {"referenceID": 0, "context": "Very recent work in Balakrishnan et al. (2014) establishes a general local convergence theorem (i.e., assuming initialization lies in a local region around true parameter) and statistical guarantees for EM, which is then specialized to obtain near-optimal rates for several specific low-dimensional problems \u2013 low-dimensional in the sense of the classical statistical setting where the samples outnumber the dimension. A central challenge in extending EM (and as a corollary, the analysis in Balakrishnan et al. (2014)) to the high-dimensional regime is the M -step. On the algorithm side, the M -step will not be stable (or even well-defined in some cases) in the high-dimensional setting. To make matters worse, any analysis that relies on showing that the finite-sample M -step is somehow \u201cclose\u201d to theM -step performed with infinite data (the population-level M -step) simply cannot apply in the high-dimensional regime. Recent work in Wang et al. (2014) treats high-dimensional EM using a truncated M -step. This works in some settings, but also requires specialized treatment for every different setting, precisely because of the difficulty with the M -step. In contrast to work in Wang et al. (2014), we pursue a high-dimensional extension via regularization. The central challenge, as mentioned above, is in picking the sequence of regularization coefficients, as this must control the optimization error (related to the special structure of \u03b2\u2217), as well as the statistical error. Finally, we note that for finite mixture regression, St\u00e4dler et al.St\u00e4dler et al. (2010) consider an l1 regularized EM algorithm for which they develop some asymptotic analysis and oracle inequality.", "startOffset": 20, "endOffset": 1579}, {"referenceID": 0, "context": "Very recent work in Balakrishnan et al. (2014) establishes a general local convergence theorem (i.e., assuming initialization lies in a local region around true parameter) and statistical guarantees for EM, which is then specialized to obtain near-optimal rates for several specific low-dimensional problems \u2013 low-dimensional in the sense of the classical statistical setting where the samples outnumber the dimension. A central challenge in extending EM (and as a corollary, the analysis in Balakrishnan et al. (2014)) to the high-dimensional regime is the M -step. On the algorithm side, the M -step will not be stable (or even well-defined in some cases) in the high-dimensional setting. To make matters worse, any analysis that relies on showing that the finite-sample M -step is somehow \u201cclose\u201d to theM -step performed with infinite data (the population-level M -step) simply cannot apply in the high-dimensional regime. Recent work in Wang et al. (2014) treats high-dimensional EM using a truncated M -step. This works in some settings, but also requires specialized treatment for every different setting, precisely because of the difficulty with the M -step. In contrast to work in Wang et al. (2014), we pursue a high-dimensional extension via regularization. The central challenge, as mentioned above, is in picking the sequence of regularization coefficients, as this must control the optimization error (related to the special structure of \u03b2\u2217), as well as the statistical error. Finally, we note that for finite mixture regression, St\u00e4dler et al.St\u00e4dler et al. (2010) consider an l1 regularized EM algorithm for which they develop some asymptotic analysis and oracle inequality. However, this work doesn\u2019t establish the theoretical properties of local optima arising from regularized EM. Our work addresses this issue from a local convergence perspective by using a novel choice of regularization. Notation: Let u = (u1, u2, . . . , up) \u22a4 \u2208 Rp be a vector and M = [Mi,j ] \u2208 Rp1\u00d7p2 be a matrix. The lq norm of u is defined as \u2016u\u2016p = ( \u2211p i=1 |ui|). We use \u2016M\u2016\u2217 to denote the nuclear norm of M and \u2016M\u20162 to denote its spectral norm. We use \u2299 to denote the Hadamard product between two vectors, i.e., u \u2299 v = (u1v1, u2v2, . . . , upvp)\u22a4. A p-by-p identity matrix is denoted as Ip. We use capital letter (e.g., X) to denote random variable, vector and matrix. For a sub-Gaussian (subexponential) random variable X, we use \u2016X\u2016\u03c82 (\u2016X\u2016\u03c81) to denote its Orlicz norm (see Vershynin (2010) for detailed definitions).", "startOffset": 20, "endOffset": 2490}, {"referenceID": 22, "context": "To provide some intuitions of such choice, we first note that from theory of high dimensional regularized M-estimator Wainwright (2014), suitable \u03bbn should be proportional to the target estimation error.", "startOffset": 118, "endOffset": 136}, {"referenceID": 0, "context": "Inspired by the low-dimensional analysis of EM in Balakrishnan et al. (2014), we expect the optimization error to decay geometrically, so we choose \u03ba \u2208 (0, 1).", "startOffset": 50, "endOffset": 77}, {"referenceID": 6, "context": "Mixed linear regression (MLR), as considered in some recent work (Chaganty and Liang, 2013; Yi et al., 2013; Chen et al., 2014b), is the problem of recovering two or more linear vectors from mixed linear measurements.", "startOffset": 65, "endOffset": 128}, {"referenceID": 25, "context": "Mixed linear regression (MLR), as considered in some recent work (Chaganty and Liang, 2013; Yi et al., 2013; Chen et al., 2014b), is the problem of recovering two or more linear vectors from mixed linear measurements.", "startOffset": 65, "endOffset": 128}, {"referenceID": 3, "context": ",Cand\u00e8s and Recht (2009); Recht et al.", "startOffset": 1, "endOffset": 25}, {"referenceID": 3, "context": ",Cand\u00e8s and Recht (2009); Recht et al. (2010); Cand\u00e8s and Plan (2011); Negahban et al.", "startOffset": 1, "endOffset": 46}, {"referenceID": 3, "context": "(2010); Cand\u00e8s and Plan (2011); Negahban et al.", "startOffset": 8, "endOffset": 31}, {"referenceID": 3, "context": "(2010); Cand\u00e8s and Plan (2011); Negahban et al. (2011); Jain et al.", "startOffset": 8, "endOffset": 55}, {"referenceID": 3, "context": "(2010); Cand\u00e8s and Plan (2011); Negahban et al. (2011); Jain et al. (2013); Chen et al.", "startOffset": 8, "endOffset": 75}, {"referenceID": 3, "context": "(2010); Cand\u00e8s and Plan (2011); Negahban et al. (2011); Jain et al. (2013); Chen et al. (2013); Cai and Zhang (2015)).", "startOffset": 8, "endOffset": 95}, {"referenceID": 1, "context": "(2013); Cai and Zhang (2015)).", "startOffset": 8, "endOffset": 29}, {"referenceID": 3, "context": ", Candes and Tao (2007); Negahban et al.", "startOffset": 2, "endOffset": 24}, {"referenceID": 3, "context": ", Candes and Tao (2007); Negahban et al. (2009); Wainwright (2014); Chen et al.", "startOffset": 2, "endOffset": 48}, {"referenceID": 3, "context": ", Candes and Tao (2007); Negahban et al. (2009); Wainwright (2014); Chen et al.", "startOffset": 2, "endOffset": 67}, {"referenceID": 3, "context": ", Candes and Tao (2007); Negahban et al. (2009); Wainwright (2014); Chen et al. (2014a)), has been shown to be useful, both empirically and theoretically, for high dimensional structural estimation.", "startOffset": 2, "endOffset": 88}, {"referenceID": 0, "context": "2 Conditions on Q(\u00b7|\u00b7) Next, we review three technical conditions, originally proposed by Balakrishnan et al. (2014), about population level Q(\u00b7|\u00b7) function.", "startOffset": 90, "endOffset": 117}, {"referenceID": 0, "context": "2 Conditions on Q(\u00b7|\u00b7) Next, we review three technical conditions, originally proposed by Balakrishnan et al. (2014), about population level Q(\u00b7|\u00b7) function. Recall that \u03a9 \u2286 Rp is the basin of attraction. It is well known that performance of EM algorithm is sensitive to initialization. Analyzing Algorithm 1 with any initial point is not desirable in this paper. Our theory is developed with focus on a r-neighbor region round \u03b2\u2217 that is defined as B(r;\u03b2\u2217) := { u \u2208 \u03a9, \u2016u\u2212 \u03b2\u2217\u2016 \u2264 r } . We first assume that Q(\u00b7|\u03b2\u2217) is self consistent as stated below. Condition 1. (Self Consistency) Function Q(\u00b7|\u03b2\u2217) is self consistent, namely \u03b2\u2217 = argmax \u03b2\u2208\u03a9 Q(\u03b2|\u03b2\u2217). It is usually assumed that \u03b2\u2217 maximizes the population log likelihood function. Under this condition, Condition 1 is always satisfied by following the classical theory of EM algorithmMcLachlan and Krishnan (2007). Basically, we require Q(\u00b7|\u03b2) is differentiable over \u03a9 for any \u03b2 \u2208 \u03a9.", "startOffset": 90, "endOffset": 865}, {"referenceID": 16, "context": "It\u2019s instructive to compare Condition 4 with a related condition proposed by Negahban et al. (2009) for analyzing high dimensional M-estimator.", "startOffset": 77, "endOffset": 100}, {"referenceID": 0, "context": "We note that, in Balakrishnan et al. (2014) and Wang et al.", "startOffset": 17, "endOffset": 44}, {"referenceID": 0, "context": "We note that, in Balakrishnan et al. (2014) and Wang et al. (2014), the statistical error is charactrized in terms of \u2016Mn(\u03b2)\u2212M(\u03b2)\u20162 and \u2016Mn(\u03b2)\u2212M(\u03b2)\u2016\u221e respectively.", "startOffset": 17, "endOffset": 67}, {"referenceID": 0, "context": "We note that, in Balakrishnan et al. (2014) and Wang et al. (2014), the statistical error is charactrized in terms of \u2016Mn(\u03b2)\u2212M(\u03b2)\u20162 and \u2016Mn(\u03b2)\u2212M(\u03b2)\u2016\u221e respectively. As mentioned earlier, in high dimensional setting, Mn(\u03b2) is not well defined in some models such as mixed linear regression. For mixed linear regression, Wang et al. (2014) resolves this issue by invoking a high dimensional inverse covariance matrix estimation algorithm proposed by Cai et al.", "startOffset": 17, "endOffset": 337}, {"referenceID": 0, "context": "We note that, in Balakrishnan et al. (2014) and Wang et al. (2014), the statistical error is charactrized in terms of \u2016Mn(\u03b2)\u2212M(\u03b2)\u20162 and \u2016Mn(\u03b2)\u2212M(\u03b2)\u2016\u221e respectively. As mentioned earlier, in high dimensional setting, Mn(\u03b2) is not well defined in some models such as mixed linear regression. For mixed linear regression, Wang et al. (2014) resolves this issue by invoking a high dimensional inverse covariance matrix estimation algorithm proposed by Cai et al. (2011). Our formulation (3.", "startOffset": 17, "endOffset": 465}, {"referenceID": 14, "context": "Note that the work in Ma and Xu (2005) provides empirical and theoretical evidences that in low SNR regime, where the overlap density of two Gaussian cluster is small, standard EM algorithm suffers from sublinear convergence asymptotically.", "startOffset": 22, "endOffset": 39}, {"referenceID": 0, "context": "See the proof of Lemma 3 in Balakrishnan et al. (2014). Now we turn to the conditions about QGMM n (\u00b7|\u00b7).", "startOffset": 28, "endOffset": 55}, {"referenceID": 7, "context": "The work in Chen et al. (2014b) shows that there exists an unavoidable phase transition of statistical rate from high SNR to low SNR.", "startOffset": 12, "endOffset": 32}, {"referenceID": 0, "context": "In Balakrishnan et al. (2014), it is proved that when r = 1 32\u2016\u03b2\u20162, there exists \u03c4 \u2208 [0, 1/2] such that QMLR(\u00b7|\u00b7) satisfies Condition 3 with parameter \u03c4 when \u03c1 is sufficiently large.", "startOffset": 3, "endOffset": 30}, {"referenceID": 0, "context": "in low dimensional analysis Balakrishnan et al. (2014), arises from the fundamental limits of EM algorithm.", "startOffset": 28, "endOffset": 55}, {"referenceID": 0, "context": "in low dimensional analysis Balakrishnan et al. (2014), arises from the fundamental limits of EM algorithm. It\u2019s worth to note that Chen et al. (2014b) establish near-optimal low dimensional estimation error that does not depend on \u2016\u03b2\u2217\u20162 based on a convex optimization approach.", "startOffset": 28, "endOffset": 152}, {"referenceID": 4, "context": "The established statistical rate matches (up to the logarithmic factor) the (single) low rank matrix estimation rate proved in Cand\u00e8s and Plan (2011); Negahban et al.", "startOffset": 127, "endOffset": 150}, {"referenceID": 4, "context": "The established statistical rate matches (up to the logarithmic factor) the (single) low rank matrix estimation rate proved in Cand\u00e8s and Plan (2011); Negahban et al. (2011), which is known to be minimax optimal.", "startOffset": 127, "endOffset": 174}, {"referenceID": 0, "context": "We revisit the following result about the gradient stability from Balakrishnan et al. (2014). Lemma 4.", "startOffset": 66, "endOffset": 93}, {"referenceID": 0, "context": "See the proof of Corollary 6 in Balakrishnan et al. (2014).", "startOffset": 32, "endOffset": 59}, {"referenceID": 12, "context": "This unusual constraint is in fact unavoidable, as pointed out in Loh and Wainwright (2012). We now turn to validate the conditions about finite sample function QMCR n (\u00b7|\u00b7).", "startOffset": 66, "endOffset": 92}, {"referenceID": 0, "context": "A similar result is proved in Balakrishnan et al. (2014). The slight difference is that Balakrishnan et al.", "startOffset": 30, "endOffset": 57}, {"referenceID": 0, "context": "A similar result is proved in Balakrishnan et al. (2014). The slight difference is that Balakrishnan et al. (2014) shows Lemma 6.", "startOffset": 30, "endOffset": 115}], "year": 2015, "abstractText": "Latent variable models are a fundamental modeling tool in machine learning applications, but they present significant computational and analytical challenges. The popular EM algorithm and its variants, is a much used algorithmic tool; yet our rigorous understanding of its performance is highly incomplete. Recently, work in Balakrishnan et al. (2014) has demonstrated that for an important class of problems, EM exhibits linear local convergence. In the high-dimensional setting, however, the M -step may not be well defined. We address precisely this setting through a unified treatment using regularization. While regularization for high-dimensional problems is by now well understood, the iterative EM algorithm requires a careful balancing of making progress towards the solution while identifying the right structure (e.g., sparsity or low-rank). In particular, regularizing the M -step using the state-of-the-art high-dimensional prescriptions (e.g., \u00e0 la Wainwright (2014)) is not guaranteed to provide this balance. Our algorithm and analysis are linked in a way that reveals the balance between optimization and statistical errors. We specialize our general framework to sparse gaussian mixture models, high-dimensional mixed regression, and regression with missing variables, obtaining statistical guarantees for each of these examples.", "creator": "LaTeX with hyperref package"}}}