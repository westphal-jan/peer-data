{"id": "1511.09460", "review": {"conference": "aaai", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Nov-2015", "title": "Ask, and shall you receive?: Understanding Desire Fulfillment in Natural Language Text", "abstract": "The ability to comprehend wishes or desires and their fulfillment is important to Natural Language Understanding. This paper introduces the task of identifying if a desire expressed by a subject in a given short piece of text was fulfilled. We propose various unstructured and structured models that capture fulfillment cues such as the subject's emotional state and actions. Our experiments with two different datasets demonstrate the importance of understanding the narrative and discourse structure to address this task.", "histories": [["v1", "Mon, 30 Nov 2015 20:37:03 GMT  (817kb,D)", "http://arxiv.org/abs/1511.09460v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL", "authors": ["snigdha chaturvedi", "dan goldwasser", "hal daume iii"], "accepted": true, "id": "1511.09460"}, "pdf": {"name": "1511.09460.pdf", "metadata": {"source": "CRF", "title": "Ask, and shall you receive?: Understanding Desire Fulfillment in Natural Language Text", "authors": ["Snigdha Chaturvedi", "Dan Goldwasser", "Hal Daum\u00e9 III"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In this context, it should be noted that this is not just a showcase project, but a project that relates to the needs of the individual."}, {"heading": "1.1 Problem Setting", "text": "Our problem consists of short texts (called \"Desire Expressions\") collected in such a way that each consists of a desire (marked by a Desire Verb) by a Desire Subject. The Desire Verb is identified by the following verb phrases: \"want to,\" wish to, \"wish to\" or \"hope to\" 1. The three Desire Verbs were identified by lexical matches, while the 1We chose these three phrases for data collection. However, you can also include other expressions of desire if required. We plan to mark the Desire Subject (s) manually. Each Desire Expression is followed by five or fewer evidence fragments (or simply evidence). Desire Expression and Evidence (in sequence) consist of individual sentences that appear together in a paragraph."}, {"heading": "2 Inference Models for Understanding Desire Fulfillment in Narrative Text", "text": "In this section, we present three textual reasoning approaches, each of which follows different assumptions as we approach the task of wish fulfillment, allowing for a principled discussion of which aspects of the narrative text should be modeled. Our first approach assumes that the indication of wish fulfillment will be contained in a single evidence fragment. Our second approach assumes that the decision depends on the evidence text as a whole and not on a single evidence fragment. We test this assumption by representing relevant information extracted from the entire evidence text. This representation (shown in Fig. 3) connects the central figure in the narrative, the desiresubject, with its actions and emotional states shown in the evidence text. This representation is then used for the extraction of characteristics when a binary classifier for wish fulfillment is present. The plan expressed in the objective sets out a stronger structure for each step to be achieved."}, {"heading": "2.1 Textual Entailment (TE) Model", "text": "Recognition of Textual Entailment (RTE) is the task of detecting the presence of a relationship between two text fragments. [8] From this perspective, a textual entailment-based method could be a natural way to approach the task of wishing fulfillment. RTE systems, however, often rely on the alignment of the entities occurring in the text fragments. Therefore, we reduce the task of wishing fulfillment to multiple RTE instances consisting of text-hypotheses pairs, for example by pairing the wish expression (hypotheses) with each of the evidence fragments (text) in this example. However, we \"normalize\" the wish expression so that it would be directly applicable to the RTE task."}, {"heading": "2.2 Unstructured Model", "text": "The Textual Entailment Model described above assumes that the expression of desire is conditioned by one of the individual evidences, and this assumption may not be true in all cases. First, the reference to the fulfillment of desire (or its negation) can be subtle and expressed through indirect references. More often, fragments of multiple evidences can collectively provide the necessary clues to identify the fulfillment of desire, which suggests the need to treat the entire text as a whole when identifying references to the fulfillment of desire. We begin by identifying the desired subject and the desired expressed (using the \"focus word\" described in Section 3) in the expression of desire. Afterwards, we design several semantic features to model the most important mentions of the desired subject, the actions taken (and the respective semantic roles of the desired subject), and the emotional state of the desired subject in the evidences."}, {"heading": "2.3 Latent Structure Narrative Model (LSNM)", "text": "The unstructured model described above captures nuanced signs of wish fulfillment by associating the desired subject with actions, events, and mental states. However, it ignores the narrative structure because it does not model the \"flow of events\" that is represented in the transition between the proofs. Our main hypothesis is that the input text represents a story.Events in history describe the evolving attempts of the main character of the story (the desired subject) to fulfill his wish. Therefore, it is indispensable to understand the flow of history in order to make better judgments about its output.We propose to model the evolution of the narrative using latent variables. We associate a latent state (denoted hj) with each evidence fragment (denoted ej).The latent states refer to discrete values (from H possible values, with H being a parameter for the model) that represent abstractly different degrees of optimism or pessimism in relation to fulfillment."}, {"heading": "2.3.1 Learning and Inference", "text": "During the training, we maximize the cumulative values of all data instances using an iterative process (Alg. 1). Each iteration of this algorithm consists of two steps. In the first step, it uses the Viterbi algorithm for each instance (and weights from the previous iteration, wt \u2212 1) to find the latent state sequence with the highest score, h, which corresponds to the provided label (the fulfillment state), f. In the next step, it uses the state sequence determined above, algorithm 1 for LSNM 1: Input: Described quantity {(d, e, f) i-i-i-ig {1.. D}; and T: Number of iterations to obtain refined weights for the tenth iteration, wt, using the structured perceptor [7]. The algorithm is similar to an EM algorithm with \"hard\" preliminary tasks, although we use the other one during the destructured state."}, {"heading": "3 Features", "text": "This year it is more than ever before."}, {"heading": "3.1 Unstructured Models", "text": "For the unstructured models, we directly used the tail and discourse characteristics (F1 to F3 in table 2), and for the characteristics F4 to F15, we summed up their values over all the proofs of an instance, thus ensuring a constant size of the feature set despite the variable number of evidence fragments per instance."}, {"heading": "3.2 Latent Structure Narrative Model", "text": "Our Structured Model requires three types of characteristics: (a) Content characteristics that help the model to assign latent states to evidence fragments based on their content; (b) Evolutionary characteristics that help model the evidence fragments; (c) Structure Independent characteristics used in the final prediction. Content characteristics: These characteristics depend on the latent state of the model, hj, and the content of the corresponding evidence characteristics, ej (expressed by the characteristics F4 to F15 in Table 2).1. \u03c6 (hj, ej) = \u03b1 if the current state is hj; 0 otherwise, if \u03b1 F4 to F15 are evolutionary characteristics: These characteristics depend on the current and previous latent states, hj and hj \u2212 1 and / or the current evidence fragment, ej: 1. \u03c6 (hj \u2212 1, hj) = 1 if the previous state is exactly as follows the previous state; j \u2212 1 if the current state is \u2212 1."}, {"heading": "4 Datasets", "text": "We used two sets of data from the real world for our experiments: MCTest and SimpleWiki, consisting of 174 and 1004 manually commented instances, respectively. Both sets of data (available on the first author's website) were similarly collected and commented on. Collection and Comment: The MCTest data comes from the Machine Comprehension Test [28] dataset, which contained a set of 660 stories and related questions. Vocabulary and concepts are limited to the extent that the stories are comprehensible to 7-year-olds. We discard the questions and consider only the free text of the stories. The SimpleWiki dataset was created from the text content of a simple English Wikipedia in October 2014. We have discarded all lists, tables and titles in the wiki pages. We have opted for simple English Wikipedia to limit the complexity of the vocabulary and world knowledge."}, {"heading": "5 Empiricial Evaluation", "text": "For the evaluation, we compared the performance of the test sets with the F1 score of the positive (fulfilled) class. We also included a simple Logistic Regression Baseline based on Bag-of-Words (BoW) features. Table 4 reports on the performance of these models. To form the unstructured model, we experimented with various algorithms4http: / / dumps.wikimedia.org / simplewiki / 5http: / / www.crowdflower.com / and show the results for the two best models: LR (Logistic Regression) and DT (Decision Trees). We report average performance values over 100 random restarts of our model as its performance depends on the initialization of the weights. Also, our model requires the number of latent states, H, as input, which was set to be 2 and 15 for the MCTest and SimpleWiki datasets."}, {"heading": "6 Related Work", "text": "Answering desires and desires has attracted psycholinguists [29] and linguists [1] alike. [17] Recognizing desires from texts. Analyzing desires adds a new dimension to more general tasks such as forming opinions [26], where manufacturers and advertisers want to discover the desires or needs of users from online reviews, etc. Another use case would be solving problems for users of community forums. Unlike most systems designed to understand large collections of texts (macro-reading) [12, 3, 13], this work focuses on micro-reading, understanding short pieces of text. [2] Micro-reading, too, but with another goal - answering domain-specific questions about entities in one paragraph. Our task is closely related to recognizing texts [11]."}, {"heading": "7 Conclusion", "text": "To solve this problem, we use three approaches based on different assumptions: First, we use a text tracing model to analyze small fragments of text independently of each other; second, an unstructured model assumes that it is not enough to analyze different pieces of text independently of each other; instead, the entire text should be analyzed as a whole to identify the fulfillment of wishes; third, a structured model, is based on the hypothesis that identifying the fulfillment of wishes requires an understanding of the narrative structure and models it using latent variables; and we compare the performance of these models on two different data sets that we commented on and shared; and our experiments demonstrate the need to incorporate the narrative structure of the text in order to better understand the fulfillment of wishes."}], "references": [{"title": "Acquisition of desires before beliefs: A computational investigation", "author": ["L. Barak", "A. Fazly", "S. Stevenson"], "venue": "Proceedings of CoNLL-2013,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Modeling biological processes for reading comprehension", "author": ["J. Berant", "V. Srikumar", "P.-C. Chen", "A. Vander Linden", "B. Harding", "B. Huang", "P. Clark", "C.D. Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Toward an architecture for neverending language learning", "author": ["A. Carlson", "J. Betteridge", "B. Kisiel", "B. Settles", "E.R.H. Jr.", "T.M. Mitchell"], "venue": "In Proceedings of the Twenty- Fourth AAAI Conference on Artificial Intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Unsupervised learning of narrative event chains", "author": ["N. Chambers", "D. Jurafsky"], "venue": "In Proceedings of the 46th annual meeting of the Association for Computational Linguistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Unsupervised learning of narrative schemas and their participants", "author": ["N. Chambers", "D. Jurafsky"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Predicting instructor\u2019s intervention in mooc forums. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1501\u20131511", "author": ["S. Chaturvedi", "D. Goldwasser", "H. Daum\u00e9 III"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron  algorithms", "author": ["M. Collins"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "Recognizing textual entailment: Rational, evaluation and approaches", "author": ["I. Dagan", "B. Dolan", "B. Magnini", "D. Roth"], "venue": "Natural Language Engineering,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "The PASCAL recognising textual entailment challenge", "author": ["I. Dagan", "O. Glickman", "B. Magnini"], "venue": "In Machine Learning Challenges. Lecture Notes in Computer Science,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Detecting story analogies from annotations of time, action and agency", "author": ["D.K. Elson"], "venue": "In Proceedings of the LREC 2012 Workshop on Computational Models of Narrative,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Dramabank: Annotating agency in narrative discourse", "author": ["D.K. Elson"], "venue": "In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Machine reading", "author": ["O. Etzioni", "M. Banko", "M.J. Cafarella"], "venue": "In Proceedings, The Twenty-First National Conference on Artificial Intelligence and the Eighteenth Innovative Applications of Artificial Intelligence Conference,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Identifying relations for open information extraction", "author": ["A. Fader", "S. Soderland", "O. Etzioni"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "A discriminatively trained, multiscale, deformable part model", "author": ["P.F. Felzenszwalb", "D.A. McAllester", "D. Ramanan"], "venue": "In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Connotation lexicon: A dash of sentiment beneath the surface meaning", "author": ["S. Feng", "J.S. Kang", "P. Kuznetsova", "Y. Choi"], "venue": "In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "The belief-desire-intention model of agency", "author": ["M. Georgeff", "B. Pell", "M. Pollack", "M. Tambe", "M. Wooldridge"], "venue": "In Intelligent Agents V: Agents Theories, Architectures, and Languages,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "May all your wishes come true: A study of wishes and how to recognize them", "author": ["A.B. Goldberg", "N. Fillmore", "D. Andrzejewski", "Z. Xu", "B. Gibson", "X. Zhu"], "venue": "In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Automatically producing plot unit representations for narrative text", "author": ["A. Goyal", "E. Riloff", "H. Daum\u00e9 III"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Attention, intentions, and the structure of discourse", "author": ["B.J. Grosz", "C.L. Sidner"], "venue": "Computational linguistics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1986}, {"title": "Plot units and narrative summarization", "author": ["W.G. Lehnert"], "venue": "Cognitive Science,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1981}, {"title": "The excitement open platform for textual inferences", "author": ["B. Magnini", "R. Zanoli", "I. Dagan", "K. Eichler", "G. Neumann", "T. Noh", "S. Pad\u00f3", "A. Stern", "O. Levy"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Computational Modeling of Narrative", "author": ["I. Mani"], "venue": "Synthesis Lectures on Human Language Technologies. Morgan & Claypool Publishers,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J. Finkel", "S.J. Bethard", "D. McClosky"], "venue": "In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Wordnet: A lexical database for english", "author": ["G.A. Miller"], "venue": "Commun. ACM,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1995}, {"title": "Understanding goal-based stories through model finding and planning", "author": ["E.T. Mueller"], "venue": "Intelligent Narrative Technologies: Papers from the AAAI Fall Symposium,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "Opinion mining and sentiment analysis", "author": ["B. Pang", "L. Lee"], "venue": "Foundations and Trends in Information Retrieval,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "The penn discourse tree-bank 2.0 annotation manual", "author": ["R. Prasad", "E. Miltsakaki", "N. Dinesh", "A. Lee", "A. Joshi", "L. Robaldo", "B. Webber"], "venue": "Technical report,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2007}, {"title": "Mctest: A challenge dataset for the open-domain machine comprehension of text", "author": ["M. Richardson", "C.J.C. Burges", "E. Renshaw"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "The acquisition of mental verbs: A systematic investigation of the first reference to mental", "author": ["M. Shatz", "H.M. Wellman", "S. Silber"], "venue": "state. Cognition,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1983}, {"title": "BIUTEE: A modular open-source system for recognizing textual entailment. In The 50th Annual Meeting of the Association for Computational Linguistics", "author": ["A. Stern", "I. Dagan"], "venue": "Proceedings of the System Demonstrations,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Discovering finegrained sentiment with latent variable structured prediction models", "author": ["O. T\u00e4ckstr\u00f6m", "R. McDonald"], "venue": "In Proceedings of the 33rd European Conference on Advances in Information Retrieval,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Understanding Goal-based Stories", "author": ["R. Wilensky"], "venue": "PhD thesis,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1978}, {"title": "Multi-level structured models for document-level sentiment classification", "author": ["A. Yessenalina", "Y. Yue", "C. Cardie"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}], "referenceMentions": [{"referenceID": 15, "context": "in contexts of rational agent behavior [16], and modeling human dialog interactions [19].", "startOffset": 39, "endOffset": 43}, {"referenceID": 18, "context": "in contexts of rational agent behavior [16], and modeling human dialog interactions [19].", "startOffset": 84, "endOffset": 88}, {"referenceID": 17, "context": "Such expressions can be used to provide rationale for character behaviors when analyzing narrative text [18, 10], extract information about human", "startOffset": 104, "endOffset": 112}, {"referenceID": 9, "context": "Such expressions can be used to provide rationale for character behaviors when analyzing narrative text [18, 10], extract information about human", "startOffset": 104, "endOffset": 112}, {"referenceID": 16, "context": "wishes [17], explain positive and negative sentiment in reviews, and support automatic curation of community forums by identifying unresolved issues raised by users.", "startOffset": 7, "endOffset": 11}, {"referenceID": 7, "context": "Similar to many other natural language understanding tasks [8, 28, 2], performance is evaluated using predic-", "startOffset": 59, "endOffset": 69}, {"referenceID": 27, "context": "Similar to many other natural language understanding tasks [8, 28, 2], performance is evaluated using predic-", "startOffset": 59, "endOffset": 69}, {"referenceID": 1, "context": "Similar to many other natural language understanding tasks [8, 28, 2], performance is evaluated using predic-", "startOffset": 59, "endOffset": 69}, {"referenceID": 3, "context": "Following previous work on narrative representation [4], we track the events and states associated with the narrative\u2019s central character (the Desire-subject).", "startOffset": 52, "endOffset": 55}, {"referenceID": 10, "context": "Recent attempts to support supervised learning of such detailed narrative structures by annotating data [11], result in highly complex structures even for restricted domains.", "startOffset": 104, "endOffset": 108}, {"referenceID": 7, "context": "Recognizing Textual Entailment (RTE) is the task of recognizing the existence of an entailment relationship between two text fragments [8].", "startOffset": 135, "endOffset": 138}, {"referenceID": 22, "context": "\u2022 If the Desire-subject is pronominal, replace it with the appropriate named entity when possible (we used the Stanford CoreNLP coreference resolution system) [23].", "startOffset": 159, "endOffset": 163}, {"referenceID": 29, "context": "Table 1 shows the performance of BIUTEE [30, 21], an RTE system, on the two datasets (see Sec.", "startOffset": 40, "endOffset": 48}, {"referenceID": 20, "context": "Table 1 shows the performance of BIUTEE [30, 21], an RTE system, on the two datasets (see Sec.", "startOffset": 40, "endOffset": 48}, {"referenceID": 14, "context": "We enhance this representation using several knowledge resources identifying word connotations [15] and relations.", "startOffset": 95, "endOffset": 99}, {"referenceID": 6, "context": "to get refined weights for the t iteration, wt, using structured perceptron [7].", "startOffset": 76, "endOffset": 79}, {"referenceID": 14, "context": "tions, and their emotive states using lexical resources like Connotation Lexicon [15], WordNet and our lexicon of conforming and dissenting phrases.", "startOffset": 81, "endOffset": 85}, {"referenceID": 22, "context": "3We obtained pos tags, dependency parses, and resolved co-references using Stanford CoreNLP [23].", "startOffset": 92, "endOffset": 96}, {"referenceID": 29, "context": "Entailment F1 TEPrediction: Binary prediction of the Textual Entailment model [30].", "startOffset": 78, "endOffset": 82}, {"referenceID": 23, "context": "Focal Word F4, F5, F6 focal count, focal syn and focal ant count: Count of occurrences of the focal word(s), their WordNet [24] synonyms and antonyms (respectively) in the Evidence.", "startOffset": 123, "endOffset": 127}, {"referenceID": 14, "context": "in blue), \u2018offered\u2019, is in connotative agreement with the intended action, \u2018help\u2019 (both have positive connotations according to [15]).", "startOffset": 128, "endOffset": 132}, {"referenceID": 26, "context": "These phrases were chosen using various discourse senses mentioned in [27].", "startOffset": 70, "endOffset": 74}, {"referenceID": 27, "context": "Collection and annotation: The MCTest data originated from the Machine Comprehension Test dataset [28] which contained of a set of 660 stories and", "startOffset": 98, "endOffset": 102}, {"referenceID": 28, "context": "Expressions of desires and wishes have attracted psycholinguists [29] and linguists [1] alike.", "startOffset": 65, "endOffset": 69}, {"referenceID": 0, "context": "Expressions of desires and wishes have attracted psycholinguists [29] and linguists [1] alike.", "startOffset": 84, "endOffset": 87}, {"referenceID": 16, "context": "[17] detect wishes from text.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "Analyzing desires adds a new dimension to more general tasks like opinion mining [26] where the manufacturers and advertisers want to discover users\u2019 desires or needs from online reviews etc.", "startOffset": 81, "endOffset": 85}, {"referenceID": 5, "context": "For instance, the number of posts in Massive Open Online Courses forums often overwhelm the instructional staff [6].", "startOffset": 112, "endOffset": 115}, {"referenceID": 27, "context": "Our problem is related to Machine Comprehension [28].", "startOffset": 48, "endOffset": 52}, {"referenceID": 11, "context": "However, unlike most systems, designed for understanding large textual collections (macroreading) [12, 3, 13], this work focuses on Micro-reading, understanding short pieces of text.", "startOffset": 98, "endOffset": 109}, {"referenceID": 2, "context": "However, unlike most systems, designed for understanding large textual collections (macroreading) [12, 3, 13], this work focuses on Micro-reading, understanding short pieces of text.", "startOffset": 98, "endOffset": 109}, {"referenceID": 12, "context": "However, unlike most systems, designed for understanding large textual collections (macroreading) [12, 3, 13], this work focuses on Micro-reading, understanding short pieces of text.", "startOffset": 98, "endOffset": 109}, {"referenceID": 1, "context": "[2] also address micro-reading but with a different goal \u2013 answering domain-specific questions about entities in a paragraph.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Our task is also related to Recognizing Textual Entailment (RTE) [8, 9].", "startOffset": 65, "endOffset": 71}, {"referenceID": 8, "context": "Our task is also related to Recognizing Textual Entailment (RTE) [8, 9].", "startOffset": 65, "endOffset": 71}, {"referenceID": 4, "context": "There have been several attempts at modeling narrative structures which include narrative schemas [5, 4], plot units [20] and Story Intention Graphs [11].", "startOffset": 98, "endOffset": 104}, {"referenceID": 3, "context": "There have been several attempts at modeling narrative structures which include narrative schemas [5, 4], plot units [20] and Story Intention Graphs [11].", "startOffset": 98, "endOffset": 104}, {"referenceID": 19, "context": "There have been several attempts at modeling narrative structures which include narrative schemas [5, 4], plot units [20] and Story Intention Graphs [11].", "startOffset": 117, "endOffset": 121}, {"referenceID": 10, "context": "There have been several attempts at modeling narrative structures which include narrative schemas [5, 4], plot units [20] and Story Intention Graphs [11].", "startOffset": 149, "endOffset": 153}, {"referenceID": 14, "context": "Previous work has also studied connotations and word effects on narrative modeling [15, 18].", "startOffset": 83, "endOffset": 91}, {"referenceID": 17, "context": "Previous work has also studied connotations and word effects on narrative modeling [15, 18].", "startOffset": 83, "endOffset": 91}, {"referenceID": 24, "context": "The AI task of recognizing plans of characters in a narrative viewing them as intentional agents [25, 32, 22] is also relevant.", "startOffset": 97, "endOffset": 109}, {"referenceID": 31, "context": "The AI task of recognizing plans of characters in a narrative viewing them as intentional agents [25, 32, 22] is also relevant.", "startOffset": 97, "endOffset": 109}, {"referenceID": 21, "context": "The AI task of recognizing plans of characters in a narrative viewing them as intentional agents [25, 32, 22] is also relevant.", "startOffset": 97, "endOffset": 109}, {"referenceID": 30, "context": "Latent structured models have been used previously for solving various problems in computer vision and NLP [31, 33, 14] though their problem settings and goals are different.", "startOffset": 107, "endOffset": 119}, {"referenceID": 32, "context": "Latent structured models have been used previously for solving various problems in computer vision and NLP [31, 33, 14] though their problem settings and goals are different.", "startOffset": 107, "endOffset": 119}, {"referenceID": 13, "context": "Latent structured models have been used previously for solving various problems in computer vision and NLP [31, 33, 14] though their problem settings and goals are different.", "startOffset": 107, "endOffset": 119}], "year": 2015, "abstractText": "The ability to comprehend desires and their fulfillment is important to Natural Language Understanding. This paper introduces the task of identifying if a desire expressed by a subject in a given short piece of text was fulfilled. We propose various unstructured and structured models that capture fulfillment cues such as the subject\u2019s emotional state and actions. Our experiments with two different datasets demonstrate the importance of understanding the narrative and discourse structure to address this task.", "creator": "LaTeX with hyperref package"}}}