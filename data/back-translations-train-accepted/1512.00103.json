{"id": "1512.00103", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2015", "title": "Multilingual Language Processing From Bytes", "abstract": "We describe an LSTM-based model which we call Byte-to-Span (BTS) that reads text as bytes and outputs span annotations of the form [start, length, label] where start positions, lengths, and labels are separate entries in our vocabulary. Because we operate on unicode bytes rather than language-specific words or characters, we can analyze text in many languages with a single model. Due to the small vocabulary size, these multilingual models are very compact, but produce results similar to or better than the state-of-the-art in Part-of-Speech tagging and Named Entity Recognition that use only the provided training datasets (no external data sources). Our models are learning \"from scratch\" in that they do not rely on any elements of the standard pipeline in Natural Language Processing.", "histories": [["v1", "Tue, 1 Dec 2015 00:23:44 GMT  (49kb,D)", "http://arxiv.org/abs/1512.00103v1", null], ["v2", "Sat, 2 Apr 2016 16:26:23 GMT  (92kb,D)", "http://arxiv.org/abs/1512.00103v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["dan gillick", "cliff brunk", "oriol vinyals", "amarnag subramanya"], "accepted": true, "id": "1512.00103"}, "pdf": {"name": "1512.00103.pdf", "metadata": {"source": "CRF", "title": "Multilingual Language Processing From Bytes", "authors": ["Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya"], "emails": ["asubram}@google.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them will be able to play by the rules they have set themselves in order to play by the rules."}, {"heading": "2 Related Work", "text": "An essential feature of our work is the use of byte inputs. Character inputs have been used with some success for tasks such as alignment capture (Church, 1993) and text processing (Peng et al., 2003), while some aspects of word choice and morphology are not yet covered. Such approaches often combine character and word traits and are particularly useful for dealing with languages with large character images (Nakagawa, 2004). However, there is almost no work that explicitly uses bytes - with the exception of the use of bytes to identify source code authenticity (Frantzeskou et al, 2006) - but there is nothing that uses our knowledge that we use as multilingual representatives."}, {"heading": "3 Model", "text": "Our model is based on the sequence-to-sequence model used for machine translation (Sutskever et al., 2014), an adaptation of an LSTM that encodes variable-length input as a fixed-length vector and then decodes it into a variable number of outputs. Generally, the sequence-to-sequence LSTM is trained to estimate the conditional probability P (y1,..., yT \u2032 | x1,..., xT), where (x1,..., xT) is an input sequence and (y1,..., yT \u2032) is the corresponding output sequence whose length T \u2032 may differ from T. The coding step calculates a fixed-dimensional representation v of the input (x1,..., xT) given by the hidden state of the LSTM after reading the last input xT. The decoding step calculates the probability of output (yP, M, STyT) with the STyel (STY)."}, {"heading": "3.1 Vocabulary", "text": "The primary difference between our model and the translation model is our novel choice of vocabulary. The set of inputs includes all 256 possible bytes, a special generate output symbol (GO) and a special DROP symbol for regulation, which we will discuss below. The set of inputs includes all possible start span positions (byte 0.. k), all possible span lengths (0.. k), all span designations (PER, LOC, ORG, MISC for the NER task), and a special STOP symbol. A complete span label includes a start, length, and label, but as shown in Figure 1, the model is trained to produce this triple as three separate outputs. This keeps the vocabulary size small and in practice gives better performance (and faster convergence) than when we use the cross-sectional space of the triplexes. Specifically, the prediction at the time is not dependent on all previous input and all predictions."}, {"heading": "3.2 Independent segments", "text": "Ideally, we want our input segments to cover complete documents, so that our predictions are based on as much relevant information as possible, but this is impractical for a few reasons. From a training perspective, a recursive neural network is unrolled to resemble a deep feedback network, with each layer corresponding to a time step. It is known that it is difficult to perform backpropagation over a very deep network, because it is becoming increasingly difficult to estimate the contribution of each layer to the gradient, and in addition, RNNs have difficulty generalizing to different length inputs (Erhan et al., 2009). Instead of creating document-sized input segments, we therefore assume segment independence: we select a fixed length and train the model to segments of length. This has the added advantage of limiting the bandwidth of the start and length label components. It can also allow more efficient, bundled inferences, since we can separate each segment completely from the number of school segments."}, {"heading": "3.3 Sequence ordering", "text": "Our model differs from the translation model in a more important way. Sutskever et al. found that specifying the input words in reverse order and generating the output words in forward order yielded significantly better translations, especially for long sentences. Theoretically, predictions are tied to the entire input, but in practical terms, the learning problem is easier when relevant information is sorted accordingly, since long dependencies are more difficult to learn than short ones. However, since the byte order in the forward direction is more significant (for example, the first byte of a multibyte character indicates the length), we found slightly better results in forward order than in reverse order (less than 1% absolute), but unlike translation, where the output has a complex sequence determined by the syntax of the language, our span annotations are more like a disordered set. We tried to sort them by end position in both forward and backward order, rather than in absolute order (1), and an improvement (1)."}, {"heading": "3.4 Model shape", "text": "We experimented with a few different architectures and found no significant improvements in the use of more than 320 units for the embedding dimension and LSTM memory, as well as 4 stacked LSTMs (see Table 4). This observation applies both to models trained in a single language and to models trained in many languages. Because the vocabulary is so small, the total number of parameters is dominated by the size of the recurring matrices."}, {"heading": "4 Training", "text": "We trained our models with Stochastic Gradient Descent (SGD) on mini-batches of size 128, using an initial learning rate of 0.3. For all other hyperparameter selections, including random initialization, learning decay, and gradient section, we follow Sutskever et al. (2014). Each model is trained on a single CPU over a period of a few days where the development group's results have stabilized. Distributed training on GPUs would likely accelerate training to just a few hours."}, {"heading": "4.1 Dropout and byte-dropout", "text": "Neural network models are often trained using dropouts (Hinton et al., 2012), which tend to improve generalization by limiting the correlations between hidden units. During training, dropouts randomly set some fractions of the elements in the embedding layer and in the model state just before the softmax layer to zero (Zaremba et al., 2014). We were able to further improve generalization by a technique we call byte dropouts: We randomly replace a fraction of the input byte in each segment with a special DROP symbol (without changing the corresponding chip annotations). Intuitively, this leads to a more robust model, perhaps by forcing it to use far-reaching dependencies, rather than memorizing certain local sequences. It is worth noting that noise during training is often added images in the image classification and language in the speech recognition, where we are adding the basic input rather than removing the byte representation in a language, which would mean something like adding 20 to the input."}, {"heading": "5 Inference", "text": "Bar search experiments show no significant improvement (less than 0.2% absolute), and since we assume that each segment is independent, we have to decide how to split the input into segments and how to stitch the results together. The simplest approach is to split the input into segments with no overlap bytes. As the model is trained to ignore incomplete spans, this approach disregards all spans that exceed segment boundaries, which can be a significant number depending on k's choice. We avoid the problem of missing span by selecting segments that overlap so that each span is fully enclosed by at least one segment. In our experiments, we create fixed overlap segments (k / 2 = 30), which means that the model reads 60 bytes of input except for the first segment in a document, but only contains predictions for the last 30 bytes."}, {"heading": "6 Results", "text": "The multilingual datasets allow us to highlight the benefits of using byte-level input: First, we can train a single compact model that can handle many languages at once; second, we demonstrate some lingual abstractions that improve the performance of a single multilingual model over each language model; and in the experiments, we refer to the LSTM structure described above as byte-to-span or BTS. Since our LSTM models read bytes, it is not obvious how to insert information like a word cluster identity to improve a monitored baseline; word clusters or word embeddings estimated from a large corpus are commonly used to deal with the scarcity; and since our LSTM models read bytes, it is not obvious how to insert information like a word cluster identity."}, {"heading": "6.1 Part-of-Speech Tagging", "text": "For our experiments, we use dif-4http: / / universaldependencies.github.io / docs / ferent datasets with different fine-grained tag sets in each language, so our results are not directly comparable. However, we provide basic results (for each language separately) using a conditional random field (Lafferty et al., 2001) with an extensive collection of features comparable to those in the Stanford POS tagger (Manning, 2011). For our experiments, we chose the 13 languages that had at least 50k tokens of training data. We did not examine the training data, although the amount of data differences varied widely across languages."}, {"heading": "6.2 Named Entity Recognition", "text": "In fact, most of us will be able to play by the rules we have set ourselves."}, {"heading": "6.3 Dropout and Stacked LSTMs", "text": "There are many modeling options and hyperparameters that significantly affect the performance of neural networks. Here, we show the results of some experiments that were particularly relevant to the performance obtained above. First, Table 3 shows how dropouts and byte dropouts improve performance for both tasks. Without any type of dropouts, the training process begins to override relatively quickly (the perplexity of development data increases). In POS tagging, we continue to set dropouts and byte dropouts to 0.2, while in NER we set both to 0.3, significantly reducing the transfit problem. Strangely, even after increasing perplexity in development data, the metrics that interest us continue to improve, the accuracy of POS tagging, and the accuracy of NER F1. This may be related to the fact that during training, we feed the gold label from the previous time step rather than the predicted label; the average perplexity may not reflect how robust the model is against the false prediction."}, {"heading": "1 76.15 77.59", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 79.40 79.73", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 81.44 81.93", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 82.13 82.18", "text": "Second, Table 4 shows how performance improves when we increase the size of the model in two ways: the number of units in the state of the model (width) and the number of stacked LSTMs (depth). Extending the model improves performance less than increasing the depth, and once we use four stacked LSTMs, the added value of a much broader model disappears. This result suggests that the model does not learn to distribute the input space according to the source language, but learns a langue-independent representation at the deeper levels."}, {"heading": "7 Conclusions", "text": "We have described a model that uses a sequence sequence sequence LSTM framework that reads one text segment after another and then generates span comments on the inputs. This work delivers a number of novel contributions: 1. We use the bytes as inputs in Unicode encodings of varying lengths, which makes the model vocabulary very small and also allows us to train a multilingual model that improves over monolingual models without using additional parameters. We introduce byte dropout, an analog to added speech noise or blur, which greatly improves generalization.2. The model produces span comments, each being a sequence of three outputs: a starting position, a length, and a label. This decomposition keeps the output vocabulary small and marks a clear departure from the typical BeginInside-Outside (BIO) scheme that is used to label sequences, a length, and a label."}, {"heading": "Acknowledgments", "text": "Many thanks to Fernando Pereira and Dan Ramage for their insights into this project from the beginning. Thanks also to Cree Howard for creating Figure 1."}], "references": [{"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["Ando", "Zhang2005] Rie Kubota Ando", "Tong Zhang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Ando et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ando et al\\.", "year": 2005}, {"title": "End-to-end attentionbased large vocabulary speech recognition", "author": ["Jan Chorowski", "Dmitriy Serdyuk", "Philemon Brakel", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1508.04395", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Improved transition-based parsing by modeling characters instead of words with lstms", "author": ["Chris Dyer", "Noah A Smith"], "venue": "arXiv preprint arXiv:1508.00657", "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks. arXiv preprint arXiv:1506.03099", "author": ["Bengio et al.2015] Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Named entity extraction using adaboost", "author": ["Llu\u0131\u0301s M\u00e0rques", "Llu\u0131\u0301s Padr\u00f3"], "venue": "In Proceedings of CoNLL-2002,", "citeRegEx": "Carreras et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Carreras et al\\.", "year": 2002}, {"title": "Listen, attend and spell", "author": ["Chan et al.2015] William Chan", "Navdeep Jaitly", "Quoc V Le", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1508.01211", "citeRegEx": "Chan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "Variable-length word encodings for neural translation models", "author": ["Chitnis", "DeNero2015] Rohan Chitnis", "John DeNero"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Chitnis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chitnis et al\\.", "year": 2015}, {"title": "Char align: a program for aligning parallel texts at the character level", "author": ["Kenneth Ward Church"], "venue": "In Proceedings of the 31st annual meeting on Association for Computational Linguistics,", "citeRegEx": "Church.,? \\Q1993\\E", "shortCiteRegEx": "Church.", "year": 1993}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Semi-supervised sequence learning. arXiv preprint arXiv:1511.01432", "author": ["Dai", "Le2015] Andrew M Dai", "Quoc V Le"], "venue": null, "citeRegEx": "Dai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2015}, {"title": "Low resource dependency parsing: Cross-lingual parameter sharing in a neural network parser", "author": ["Duong et al.2015] Long Duong", "Trevor Cohn", "Steven Bird", "Paul Cook"], "venue": "In 53rd Annual Meeting of the Association", "citeRegEx": "Duong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Duong et al\\.", "year": 2015}, {"title": "The difficulty of training deep architectures and the effect of unsupervised pre-training", "author": ["Erhan et al.2009] Dumitru Erhan", "Pierre-Antoine Manzagol", "Yoshua Bengio", "Samy Bengio", "Pascal Vincent"], "venue": "In International Conference on artificial intelligence and", "citeRegEx": "Erhan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2009}, {"title": "From speech to letters-using a novel neural network architecture for grapheme based asr", "author": ["Eyben et al.2009] Florian Eyben", "Martin W\u00f6llmer", "Bj\u00f6rn Schuller", "Alex Graves"], "venue": "In Automatic Speech Recognition & Understanding,", "citeRegEx": "Eyben et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Eyben et al\\.", "year": 2009}, {"title": "Named entity recognition through classifier combination", "author": ["Florian et al.2003] Radu Florian", "Abe Ittycheriah", "Hongyan Jing", "Tong Zhang"], "venue": "In Proceedings of the seventh conference on Natural language learning at HLT-NAACL", "citeRegEx": "Florian et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Florian et al\\.", "year": 2003}, {"title": "Effective identification of source code authors using byte-level information", "author": ["Efstathios Stamatatos", "Stefanos Gritzalis", "Sokratis Katsikas"], "venue": "In Proceedings of the 28th international conference on Software engineer-", "citeRegEx": "Frantzeskou et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Frantzeskou et al\\.", "year": 2006}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Graves", "Jaitly2014] Alex Graves", "Navdeep Jaitly"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580", "author": ["Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Bidirectional lstm-crf models for sequence tagging", "author": ["Huang et al.2015] Zhiheng Huang", "Wei Xu", "Kai Yu"], "venue": "arXiv preprint arXiv:1508.01991", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Characteraware neural language models", "author": ["Kim et al.2015] Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": "arXiv preprint arXiv:1508.06615", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Named entity recognition with character-level models", "author": ["Klein et al.2003] Dan Klein", "Joseph Smarr", "Huy Nguyen", "Christopher D Manning"], "venue": "In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume", "citeRegEx": "Klein et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2003}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Andrew McCallum", "Fernando CN Pereira"], "venue": null, "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Ling et al.2015] Wang Ling", "Tiago Lu\u0131\u0301s", "Lu\u0131\u0301s Marujo", "Ram\u00f3n Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Part-ofspeech tagging from 97% to 100%: is it time for some linguistics", "author": ["Christopher D Manning"], "venue": "In Computational Linguistics and Intelligent Text Processing,", "citeRegEx": "Manning.,? \\Q2011\\E", "shortCiteRegEx": "Manning.", "year": 2011}, {"title": "Chinese and japanese word segmentation using word-level and character-level information", "author": ["Tetsuji Nakagawa"], "venue": "In Proceedings of the 20th international conference on Computational Linguistics,", "citeRegEx": "Nakagawa.,? \\Q2004\\E", "shortCiteRegEx": "Nakagawa.", "year": 2004}, {"title": "Learning multilingual named entity recognition from wikipedia", "author": ["Nothman et al.2013] Joel Nothman", "Nicky Ringland", "Will Radford", "Tara Murphy", "James R Curran"], "venue": "Artificial Intelligence,", "citeRegEx": "Nothman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nothman et al\\.", "year": 2013}, {"title": "Lexicon infused phrase embeddings for named entity resolution", "author": ["Vineet Kumar", "Andrew McCallum"], "venue": "arXiv preprint arXiv:1404.5367", "citeRegEx": "Passos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Passos et al\\.", "year": 2014}, {"title": "Language independent authorship attribution using character level language models", "author": ["Peng et al.2003] Fuchun Peng", "Dale Schuurmans", "Shaojun Wang", "Vlado Keselj"], "venue": "In Proceedings of the tenth conference on European chapter of the Association", "citeRegEx": "Peng et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2003}, {"title": "A universal part-of-speech tagset", "author": ["Petrov et al.2011] Slav Petrov", "Dipanjan Das", "Ryan McDonald"], "venue": "arXiv preprint arXiv:1104.2086", "citeRegEx": "Petrov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104\u20133112", "author": ["Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["Ilya Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1409.2329", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Text understanding from scratch. arXiv preprint arXiv:1502.01710", "author": ["Zhang", "LeCun2015] Xiang Zhang", "Yann LeCun"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "This is truly language annotation from scratch (see Collobert et al. (2011) and Zhang and LeCun (2015)).", "startOffset": 52, "endOffset": 76}, {"referenceID": 8, "context": "This is truly language annotation from scratch (see Collobert et al. (2011) and Zhang and LeCun (2015)).", "startOffset": 52, "endOffset": 103}, {"referenceID": 29, "context": "Second, sequence-to-sequence models (Sutskever et al., 2014), allow for flexible input/output dynamics.", "startOffset": 36, "endOffset": 60}, {"referenceID": 16, "context": "Section 2 discusses related work; Section 3 describes our model; Section 4 gives training details including a new variety of dropout (Hinton et al., 2012); Section 5 gives inference details; Section 6 presents", "startOffset": 133, "endOffset": 154}, {"referenceID": 20, "context": "Character-level inputs have been used with some success for tasks like NER (Klein et al., 2003), parallel text alignment (Church, 1993), and authorship attribution (Peng et al.", "startOffset": 75, "endOffset": 95}, {"referenceID": 7, "context": ", 2003), parallel text alignment (Church, 1993), and authorship attribution (Peng et al.", "startOffset": 33, "endOffset": 47}, {"referenceID": 27, "context": ", 2003), parallel text alignment (Church, 1993), and authorship attribution (Peng et al., 2003) as an effective way to deal with n-gram sparsity while still capturing some aspects of word choice and morphology.", "startOffset": 76, "endOffset": 95}, {"referenceID": 24, "context": "Such approaches often combine character and word features and have been especially useful for handling languages with large character sets (Nakagawa, 2004).", "startOffset": 139, "endOffset": 155}, {"referenceID": 14, "context": "uses byte n-grams to identify source code authorship (Frantzeskou et al., 2006) \u2013 but there is nothing, to the best of our knowledge, that exploits bytes as a cross-lingual representation of language.", "startOffset": 53, "endOffset": 79}, {"referenceID": 10, "context": "share some subset of the parameters across languages (Duong et al., 2015) seems to benefit the low-resource languages; however, we are sharing all the parameters among all languages.", "startOffset": 53, "endOffset": 73}, {"referenceID": 22, "context": "Recent work has shown that modeling the sequence of characters in each token with an LSTM can more effectively handle rare and unknown words than independent word embeddings (Ling et al., 2015; Ballesteros et al., 2015).", "startOffset": 174, "endOffset": 219}, {"referenceID": 2, "context": "Recent work has shown that modeling the sequence of characters in each token with an LSTM can more effectively handle rare and unknown words than independent word embeddings (Ling et al., 2015; Ballesteros et al., 2015).", "startOffset": 174, "endOffset": 219}, {"referenceID": 19, "context": "Similarly, language modeling, especially for morphologically complex languages, benefits from a Convolutional Neural Network (CNN) over characters to generate word embeddings (Kim et al., 2015).", "startOffset": 175, "endOffset": 193}, {"referenceID": 2, "context": ", 2015; Ballesteros et al., 2015). Similarly, language modeling, especially for morphologically complex languages, benefits from a Convolutional Neural Network (CNN) over characters to generate word embeddings (Kim et al., 2015). Rather than decompose words into characters, Rohan and Denero (2015) encode rare words with Huffman codes, allowing a neural translation model to learn something about word subcomponents.", "startOffset": 8, "endOffset": 299}, {"referenceID": 8, "context": "Our work is philosophically similar to Collobert et al.\u2019s (2011) experiments with \u201calmost from scratch\u201d language processing.", "startOffset": 39, "endOffset": 65}, {"referenceID": 5, "context": "rectly from acoustic frame sequences (Chan et al., 2015; Bahdanau et al., 2015).", "startOffset": 37, "endOffset": 79}, {"referenceID": 1, "context": "rectly from acoustic frame sequences (Chan et al., 2015; Bahdanau et al., 2015).", "startOffset": 37, "endOffset": 79}, {"referenceID": 12, "context": "This line of work \u2013 discarding intermediate representations in speech \u2013 was pioneered by Graves and Jaitly (2014) and earlier, by Eyben et al. (2009).", "startOffset": 130, "endOffset": 150}, {"referenceID": 29, "context": "Our model is based on the sequence-to-sequence model used for machine translation (Sutskever et al., 2014), an adaptation of an LSTM that encodes a variable length input as a fixed-length vector, then decodes it into a variable number of outputs.", "startOffset": 82, "endOffset": 106}, {"referenceID": 11, "context": "very deep network is hard because it becomes increasingly difficult to estimate the contribution of each layer to the gradient, and further, RNNs have trouble generalizing to different length inputs (Erhan et al., 2009).", "startOffset": 199, "endOffset": 219}, {"referenceID": 29, "context": "parameter choices, including random initialization, learning rate decay, and gradient clipping, we follow Sutskever et al. (2014). Each model is trained on a single CPU over a period of a few days, at which point, development set results have stabilized.", "startOffset": 106, "endOffset": 130}, {"referenceID": 16, "context": "Neural Network models are often trained using dropout (Hinton et al., 2012), which tends to improve generalization by limiting correlations among hidden units.", "startOffset": 54, "endOffset": 75}, {"referenceID": 30, "context": "During training, dropout randomly zeroes some fraction of the elements in the embedding layer and the model state just before the softmax layer (Zaremba et al., 2014).", "startOffset": 144, "endOffset": 166}, {"referenceID": 28, "context": "1 of the Universal Dependency data4, a collection of treebanks across many languages annotated with a universal tagset (Petrov et al., 2011).", "startOffset": 119, "endOffset": 140}, {"referenceID": 22, "context": "The most relevant recent results (Ling et al., 2015) use dif-", "startOffset": 33, "endOffset": 52}, {"referenceID": 21, "context": "However, we provide baseline results (for each language separately) using a Conditional Random Field (Lafferty et al., 2001) with an extensive collection of features comparable to those used in the Stanford POS tagger (Manning, 2011).", "startOffset": 101, "endOffset": 124}, {"referenceID": 23, "context": ", 2001) with an extensive collection of features comparable to those used in the Stanford POS tagger (Manning, 2011).", "startOffset": 101, "endOffset": 116}, {"referenceID": 13, "context": "The best competition results for English and German (Florian et al., 2003) used a large gazetteer and the output of two additional NER classifiers trained on richer datasets.", "startOffset": 52, "endOffset": 74}, {"referenceID": 4, "context": "The 1st place submission in 2002 (Carreras et al., 2002) comment that without extra resources for Spanish, their results drop by about 2% (absolute).", "startOffset": 33, "endOffset": 56}, {"referenceID": 25, "context": "techniques (Ando and Zhang, 2005) and more recently, Passos et al. (2014) claimed the best English results (90.", "startOffset": 53, "endOffset": 74}, {"referenceID": 20, "context": "all 2nd place submission in 2003 (Klein et al., 2003).", "startOffset": 33, "endOffset": 53}, {"referenceID": 25, "context": "Nothman et al. (2013) use", "startOffset": 0, "endOffset": 22}, {"referenceID": 3, "context": "Recent results with scheduled sampling (Bengio et al., 2015), feeding either the gold or predicted label during training according to some schedule, would likely improve our results.", "startOffset": 39, "endOffset": 60}], "year": 2017, "abstractText": "We describe an LSTM-based model which we call Byte-to-Span (BTS) that reads text as bytes and outputs span annotations of the form [start, length, label] where start positions, lengths, and labels are separate entries in our vocabulary. Because we operate on unicode bytes rather than language-specific words or characters, we can analyze text in many languages with a single model. Due to the small vocabulary size, these multilingual models are very compact, but produce results similar to or better than the state-of-the-art in Part-of-Speech tagging and Named Entity Recognition that use only the provided training datasets (no external data sources). Our models are learning \u201cfrom scratch\u201d in that they do not rely on any elements of the standard pipeline in Natural Language Processing.", "creator": "LaTeX with hyperref package"}}}