{"id": "1512.00177", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2015", "title": "LSTM Neural Reordering Feature for Statistical Machine Translation", "abstract": "Artificial neural networks are powerful models, which have been widely applied into many aspects of machine translation, such as language modeling and translation modeling. Though notable improvements have been made in these areas, the reordering problem still remains a challenge in statistical machine translations. In this paper, we present a novel neural reordering model that directly models word pairs and alignment. By utilizing LSTM recurrent neural networks, much longer context could be learned for reordering prediction. Experimental results on NIST OpenMT12 Arabic-English and Chinese-English 1000-best rescoring task show that our LSTM neural reordering feature is robust and achieves significant improvements over various baseline systems.", "histories": [["v1", "Tue, 1 Dec 2015 08:43:19 GMT  (322kb)", "http://arxiv.org/abs/1512.00177v1", "6 pages"], ["v2", "Tue, 8 Mar 2016 01:17:22 GMT  (0kb,I)", "http://arxiv.org/abs/1512.00177v2", "6 pages, withdrawn by the author due to a error in formula"], ["v3", "Thu, 16 Jun 2016 10:01:49 GMT  (240kb,D)", "http://arxiv.org/abs/1512.00177v3", "6 pages, accepted by NAACL2016 short paper"]], "COMMENTS": "6 pages", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.NE", "authors": ["yiming cui", "shijin wang", "jianfeng li"], "accepted": true, "id": "1512.00177"}, "pdf": {"name": "1512.00177.pdf", "metadata": {"source": "CRF", "title": "LSTM Neural Reordering Feature for Statistical Machine Translation", "authors": ["Yiming Cui", "Shijin Wang", "Jianfeng Li", "Yuguang Wang"], "emails": ["ymcui@iflytek.com", "sjwang3@iflytek.com", "jfli3@iflytek.com", "ygwang2@iflytek.com"], "sections": [{"heading": "1 Introduction", "text": "In statistical machine translation, the language model, the translation model and the reordering model are the three most important components. Among these models, the reordering model plays an important role in phrase-based machine translation (Koehn et al., 2003), and remains a major challenge in the current study. In recent years, various methods for reordering phrases for phrase-based SMT systems have been proposed, which can be divided into two broad categories: (1) distance-based RM: repression of phrases with respect to the degree of non-monotonicity (Koehn et al., 2003). (2) Dictionary RM: conditions that reorder probabilities based on current phrase pairs. According to the orientation determinants, lexicalized reordering model can be further classified into word-based RM (Tillman, 2004), phrase-based RM: conditions that reorder probabilities on current phrase pairs."}, {"heading": "2 Related Work", "text": "Recently, various neural network models have been applied to machine translation. Forward-looking neural language models were first proposed by Bengio et al. (2003), which represented a breakthrough in speech modeling. Mikolov et al. (2011) suggested using recursive neural networks in speech modeling, which can include a much longer context history for predicting the next word. Experimental results show that the RNN-based language model is significantly better than the standard forward-looking language model. Devlin et al. (2014) suggested a neural network joint model (NNJM) by conditioning both the source and target context for predicting the next word. Although the network architecture is a simple forward-looking neural network, the results have shown significant improvements over the state of the art. Sundermeyer et al. (2014) we also proposed a neural translational model using N-based RM and STRM."}, {"heading": "3 Lexicalized Reordering Model", "text": "In traditional statistical machine translations, lexicalized reordering models (Koehn et al., 2007) are commonly used, taking into account the alignment of current and previous sentence pairs. Formally, the lexicalized reordering model can be illustrated if it is linked to the source language sentence = #,..., &, target language sentence = #,..., &, and phrase alignment = #,..., &, using Equation 1, which is only linked to) * # and), i.e. previous and current alignment., 12,) * #,) 3 # (1) In Equation 1, the) represents the set of phrase alignments. In the most commonly used MSD-based alignment mode,) assumes three values: M stands for monoton, S for swap, and D for discontinuity. \u2212 The definition of MSD orientations is also based on previous context."}, {"heading": "4 LSTM Neural Reordering Model", "text": "To include more context information to determine the reordering, we propose to use a recurrent neural network that has been shown to perform significantly better in sequence prediction than traditional feed-forward architectures (Mikolov et al., 2011). However, RNN with conventional back-propagation training suffers from gradient disappearance problems (Bengio et al., 1994). Later, long-term memory was proposed to solve the gradient disappearance problem, and it could capture a longer context than conventional RNNNNNNs with sigmoid activation functionality. In this paper, we use LSTM architecture to train the neural reordering model. http: / / www.statmt.org / moses /? n = FactoredTraining.BuildReorderingModel"}, {"heading": "4.1 Training Data Processing", "text": "In order to reduce the complexity of the model and ease of implementation, our neural reordering model is purely lexicalized and trained at the word level. We take the LR orientation as an explanation, while other orientation types (MSD, MSLR) can be similarly induced. If we have a sentence pair and its orientation information, we can induce the word-based reordering information by the following steps: Note that we always evaluate the model in the order of the target sentence. Figure 1 shows an example of data processing. (1) If the current target is aligned one-to-one, then we can directly induce its orientations (left or right). (2) If the current source / target word is aligned one too much, then we evaluate its orientation by the first aligned target / source word, and the other aligned target word is aligned as \"< follow >\" reordering type, which means that these word pairs < < if a pair of the previous word pairs are inserted; < if a pair of the previous word pairs are inserted; < < 3)."}, {"heading": "4.2 LSTM Network Architecture", "text": "After processing the training data, we can directly use the word pairs and their orientation to learn a neural reordering model. In view of a word pair and its orientation, a neural reordering model can be illustrated by Equation 3., 12),,,,"}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Setups", "text": "We tested our approach with Arabic-English and Chinese-English translation. The training corpus selected 7M words with high-quality parallel education for Arabic and 4M words for Chinese selected from NIST OpenMT12. We used the SAMA Tokenizer2 for Arabic word tokenization and the internal segmentator for Chinese words. Development and test set statistics are in Table 1. All development and test sets have 4 references for each segment. The base systems are built using the open source phrase-based SMT toolkit Moses (Koehn et al., 2007). Word alignment and phrase extraction are performed by GIZA + + (Och and Ney, 2003) with L0 normalization (Vaswani et al., 2012) and grow-diag-final refinement rule (Koehn et al., 2003)."}, {"heading": "5.2 Results on different orientation types", "text": "The overall results of BLEU improvements range from 0.42 to 0.79 for Arabic English and from 0.31 to 0.72 for Chinese English. All results are significantly better than baselines (< 0.001). 2 https: / / catalog.ldc.upenn.edu / LDC2010L01In the meantime, we also note that \"left-right\" orientation methods such as LR and MSLR consistently perform better than MSD-based orientations, which may be due to a problem of non-separability, meaning that MSD-based methods are vulnerable to context changes and weak at resolving ambiguities. Similar conclusions can be found in Li et al (2014)."}, {"heading": "5.3 Results on different reordering baselines", "text": "We are also testing our approach on various baselines that contain either a word-based, phrase-based or hierarchical phrase-based rearrangement model. We are only showing the results of the MSLR orientation, which is relatively superior according to the results in Section 5.2. In Table 3 we can see that while we are adding a strongly hierarchical, phrase-based rearrangement model to the baseline, our model can still yield a maximum gain of 0.59 BLEU score, which indicates that our model is applicable and robust in various circumstances. However, we have noticed that the gains in the Ar-En system are relatively greater than in the Zh-En system. This is probably because hierarchical reordering functions tend to work better for Chinese words and our model will therefore provide little remedy to its baseline."}, {"heading": "6 Conclusion", "text": "We present a novel work using a reordering model using LSTM-RNN, which is much sensitive to the change of context and introduce rich context information for reordering prediction. In addition, the proposed model is purely lexicalized and easy to implement. Experimental results on 1000-best rescoring pages show that our neural reordering feature is robust and could give consistent improvements across different baseline systems. In the future, we plan to use our word-based LSTM reordering model to phrase-based one to resolve many more ambiguities and improve reordering accuracy.Reference Yoshua Bengio, Patrice Simard, and Paolo Frainguard, and Paolo Frainguards. 1994. Learning long-term dependencies with gradient descentis difficult. In IEEE Transactions on Neural Networks, 5 (2): 157-166.Yoshua Bengio, R\u00e9jean Ducharme, PascalVincent and Christian Jauvin."}], "references": [{"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi."], "venue": "IEEE Transactions on Neural Networks, 5(2): 157-166.", "citeRegEx": "Bengio et al\\.,? 1994", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "PascalVincent", "Christian Jauvin."], "venue": "Journal of Machine Learning Research, 3:1137-1155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Batch tuning strategies for statistical machine translation", "author": ["Colin Cherry", "George Foster."], "venue": "Proceedings of 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human language technologies, pages 427-436.", "citeRegEx": "Cherry and Foster.,? 2012", "shortCiteRegEx": "Cherry and Foster.", "year": 2012}, {"title": "Better hypothesis testing for statistical machine translation: controlling for optimizer instability", "author": ["Jonathon H. Clark", "Chris Dyer", "Alon Lavie", "Noah A.Smith."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers, pages 176-181.", "citeRegEx": "Clark et al\\.,? 2011", "shortCiteRegEx": "Clark et al\\.", "year": 2011}, {"title": "Fast and Robust Neural Network Joint Models for Statistical Machine Translation", "author": ["Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard Schwartz", "John Makhoul."], "venue": "Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics, pages 1370-1380.", "citeRegEx": "Devlin et al\\.,? 2014", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "A simple and effective hierarchical phrase reordering model", "author": ["Michel Galley", "Christopher D. Manning."], "venue": "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 848-856.", "citeRegEx": "Galley and Manning.,? 2008", "shortCiteRegEx": "Galley and Manning.", "year": 2008}, {"title": "Framewise phoneme classification with bidirectional LSTM networks", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber."], "venue": "Proceedings of ICJNN, pages 2047-2052.", "citeRegEx": "Graves and Schmidhuber.,? 2005", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8): 17351780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Moses: open source toolkit for statistical machine translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens", "Chris dyer", "Ond\u0159ej Bojar", "Alexandra Constantin", "Evan Herbst."], "venue": "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL) on Interactive Poster and Demonstration Sessions, pages 177-180.", "citeRegEx": "Koehn et al\\.,? 2007", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Statistical significance tests for machine translation evaluation", "author": ["Philipp Koehn."], "venue": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 388-395.", "citeRegEx": "Koehn.,? 2004", "shortCiteRegEx": "Koehn.", "year": 2004}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational linguistics on Human Language Technology, vol. 1, pages 48-54.", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Recursive autoencoders for ITG-based translation", "author": ["Peng Li", "Yang Liu", "Maosong Sun."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 567-577.", "citeRegEx": "Li et al\\.,? 2013", "shortCiteRegEx": "Li et al\\.", "year": 2013}, {"title": "A Neural Reordering Model for Phrasebased Translation", "author": ["Peng Li", "Yang Liu", "Maosong Sun", "Tatsuya Izuha", "Dakun Zhang."], "venue": "Proceedings of the 25th International Conference on Computational Linguistics: Technical Papers, pages 1897-1907, Dublin, Ireland. Tomas Mikolov, Stefan Kombrink, Lukas Burget, JH Cernocky, and Sanjeev Khudanpur. 2011. Extensions of recurrent neural network language model. In 2011 IEEE International Conference on Acoustics, Speech and", "citeRegEx": "Li et al\\.,? 2014", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "A comparison of alignment models for statistical machine translation", "author": ["Franz Josef Och", "Hermann Ney."], "venue": "Proceedings of the 18th International Conference on Computational Linguistics, pages 1086-1090.", "citeRegEx": "Och and Ney.,? 2000", "shortCiteRegEx": "Och and Ney.", "year": 2000}, {"title": "Bleu: a Method for Automatic Evaluation of Machine Translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu."], "venue": "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Translation Modeling with Bidirectional Recurrent Neural Networks", "author": ["Martin Sundermeyer", "Tamer Alkhouli", "Joern Wuebker", "Hermann Ney."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 14-25.", "citeRegEx": "Sundermeyer et al\\.,? 2014", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2014}, {"title": "SRILM \u2013 An Extensible Language Modeling Toolkit", "author": ["Andreas Stolcke."], "venue": "Proceedings of the International Conference on Speech and Language Processing (ICSLP), vol. 2, pages 901-904, Denver, CO.", "citeRegEx": "Stolcke.,? 2002", "shortCiteRegEx": "Stolcke.", "year": 2002}, {"title": "A Unigram Orientation Model for Statistical Machine Translation", "author": ["Christoph Tillman."], "venue": "Proceedings of 2004 Conference of the North American Chapter of the Association for Computational Linguistics: Human language technologies, pages 101-104.", "citeRegEx": "Tillman.,? 2004", "shortCiteRegEx": "Tillman.", "year": 2004}, {"title": "Smaller Alignment Models for Better Translations: Unsupervised word Alignment with the l0-norm", "author": ["Ashish Vaswani", "Liang Huang", "David Chiang."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 311-319.", "citeRegEx": "Vaswani et al\\.,? 2012", "shortCiteRegEx": "Vaswani et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 10, "context": "Among these models, the reordering model plays an important role in phrase-based machine translation (Koehn et al., 2003), and it still remains a major challenge in current study.", "startOffset": 101, "endOffset": 121}, {"referenceID": 10, "context": "In recent years, various phrase reordering methods have been proposed for phrase-based SMT systems, which can be classified into two broad categories: (1) Distance-based RM: Penalize phrase displacements with respect to the degree of non-monotonicity (Koehn et al., 2003).", "startOffset": 251, "endOffset": 271}, {"referenceID": 17, "context": "According to the orientation determinants, lexicalized reordering model can further be classified into word-based RM (Tillman, 2004), phrase-based RM (Koehn et al.", "startOffset": 117, "endOffset": 132}, {"referenceID": 8, "context": "According to the orientation determinants, lexicalized reordering model can further be classified into word-based RM (Tillman, 2004), phrase-based RM (Koehn et al., 2007), and hierarchical phrase-based RM (Galley and Manning, 2008).", "startOffset": 150, "endOffset": 170}, {"referenceID": 5, "context": ", 2007), and hierarchical phrase-based RM (Galley and Manning, 2008).", "startOffset": 42, "endOffset": 68}, {"referenceID": 12, "context": "Furthermore, some researchers proposed a reordering model that conditions both current and previous phrase pairs by utilizing recursive auto-encoders (Li et al., 2014), which is applied into decoding step.", "startOffset": 150, "endOffset": 167}, {"referenceID": 7, "context": "We utilize a long short-term memory recurrent neural network (LSTM-RNN) (Hochreiter and Schmidhuber, 1997), and directly models word pairs to predict its most probable orientation.", "startOffset": 72, "endOffset": 106}, {"referenceID": 0, "context": "Feed-forward neural language model was first proposed by Bengio et al. (2003), which was a breakthrough in language modeling.", "startOffset": 57, "endOffset": 78}, {"referenceID": 0, "context": "Feed-forward neural language model was first proposed by Bengio et al. (2003), which was a breakthrough in language modeling. Mikolov et al. (2011) proposed to use recurrent neural network in language modeling, which can include much longer context history for predicting next word.", "startOffset": 57, "endOffset": 148}, {"referenceID": 12, "context": "Also, they suggested that by both including current and previous phrase pairs to determine the phrase orientations could achieve further improvements in accuracy (Li et al., 2014).", "startOffset": 162, "endOffset": 179}, {"referenceID": 8, "context": "In traditional statistical machine translations, lexicalized reordering models (Koehn et al., 2007) have been widely used.", "startOffset": 79, "endOffset": 99}, {"referenceID": 0, "context": "However, RNN with conventional back-propagation training suffers from gradient vanishing issues (Bengio et al., 1994).", "startOffset": 96, "endOffset": 117}, {"referenceID": 6, "context": "We omit rather extensive LSTM equations here, which can be found in (Graves and Schmidhuber, 2005).", "startOffset": 68, "endOffset": 98}, {"referenceID": 8, "context": "The baseline systems are built with the open-source phrase-based SMT toolkit Moses (Koehn et al., 2007).", "startOffset": 83, "endOffset": 103}, {"referenceID": 18, "context": "Word alignment and phrase extraction are done by GIZA++ (Och and Ney, 2003) with L0-normalization (Vaswani et al., 2012), and grow-diag-final refinement rule (Koehn et al.", "startOffset": 98, "endOffset": 120}, {"referenceID": 10, "context": ", 2012), and grow-diag-final refinement rule (Koehn et al., 2003).", "startOffset": 45, "endOffset": 65}, {"referenceID": 16, "context": "Monolingual part of training data is used to train a 5-gram language model using SRILM (Stolcke, 2002).", "startOffset": 87, "endOffset": 102}, {"referenceID": 2, "context": "Parameter tuning is done by K-best MIRA (Cherry and Foster, 2012).", "startOffset": 40, "endOffset": 65}, {"referenceID": 3, "context": "For guarantee of result stability, we tune every system 5 times independently, and take the average BLEU score (Clark et al., 2011).", "startOffset": 111, "endOffset": 131}, {"referenceID": 14, "context": "The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002).", "startOffset": 71, "endOffset": 94}, {"referenceID": 9, "context": "The statistical significance test is also carried out with paired bootstrap resampling method (Koehn, 2004) with p < 0.", "startOffset": 94, "endOffset": 107}, {"referenceID": 11, "context": "Similar conclusion can be found in Li et al. (2014).", "startOffset": 35, "endOffset": 52}], "year": 2015, "abstractText": "Artificial neural networks are powerful models, which have been widely applied into many aspects of machine translation, such as language modeling and translation modeling. Though notable improvements have been made in these areas, the reordering problem still remains a challenge in statistical machine translations. In this paper, we present a novel neural reordering model that directly models word pairs and alignment. By utilizing LSTM recurrent neural networks, much longer context could be learned for reordering prediction. Experimental results on NIST OpenMT12 Arabic-English and Chinese-English 1000-best rescoring task show that our LSTM neural reordering feature is robust and achieves significant improvements over various baseline systems.", "creator": "Word"}}}