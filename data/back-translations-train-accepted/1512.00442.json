{"id": "1512.00442", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2015", "title": "Fast k-Nearest Neighbour Search via Dynamic Continuous Indexing", "abstract": "Existing methods for retrieving k-nearest neighbours suffer from the curse of dimensionality. We argue this is caused in part by inherent deficiencies of space partitioning, which is the underlying strategy used by almost all existing methods. We devise a new strategy that avoids partitioning the vector space and present a novel randomized algorithm that runs in time linear in dimensionality and sub-linear in the size of the dataset and takes space constant in dimensionality and linear in the size of the dataset. The proposed algorithm allows fine-grained control over accuracy and speed on a per-query basis, automatically adapts to variations in dataset density, supports dynamic updates to the dataset and is easy-to-implement. We show appealing theoretical properties and demonstrate empirically that the proposed algorithm outperforms locality-sensitivity hashing (LSH) in terms of approximation quality and speed.", "histories": [["v1", "Tue, 1 Dec 2015 20:53:16 GMT  (862kb,D)", "http://arxiv.org/abs/1512.00442v1", "11 pages, 5 figures"], ["v2", "Fri, 10 Jun 2016 18:47:10 GMT  (681kb,D)", "http://arxiv.org/abs/1512.00442v2", "13 pages, 6 figures; International Conference on Machine Learning (ICML), 2016"], ["v3", "Thu, 6 Apr 2017 06:51:49 GMT  (681kb,D)", "http://arxiv.org/abs/1512.00442v3", "13 pages, 6 figures; International Conference on Machine Learning (ICML), 2016. This version corrects a typo in the pseudocode"]], "COMMENTS": "11 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.DS cs.AI cs.IR cs.LG stat.ML", "authors": ["ke li", "jitendra malik"], "accepted": true, "id": "1512.00442"}, "pdf": {"name": "1512.00442.pdf", "metadata": {"source": "META", "title": "Fast k-Nearest Neighbour Search via Dynamic Continuous Indexing", "authors": ["Ke Li", "Jitendra Malik"], "emails": ["KE.LI@EECS.BERKELEY.EDU", "MALIK@EECS.BERKELEY.EDU"], "sections": [{"heading": "1. Introduction", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "2. Related Work", "text": "In recent decades, the extensive work has yielded a rich collection of algorithms for the rapid query of k-nearest trees (2008). Spatial division forms the basis for almost all of these algorithms. Early approaches store points in deterministic tree-based data structures, such as k-d trees (Bentley, 1975), R-trees (Guttman, 1984), and X-trees (Berchtold et al., 1996; 1998), which effectively divide vector space into a hierarchy of half-spaces, hyperperpendicular angles, or voronoi polygons. These methods achieve quantum times that are very good logarithmically in the number of data points and work. Unfortunately, their quantum times grow exponentially in the dimension of data, because the number of leaves in the tree must be searched exponentially in dimensionality."}, {"heading": "3. Algorithm", "text": "The proposed algorithms rely on the construction of continuous datasets that support both fast search and online updates. (D) To achieve this, we need to engage in one-dimensional processing of the data. (D) Denominator Denominator denominator denominator denominator denominator denominator denominator denominator denominator denominator denominator denominator denominator denominator denominator denominator denominator denominator denominator denominator denominator denominator denominator denominator denominator denominator denominator denominator denominator"}, {"heading": "3.1. Properties of Random 1D Projection", "text": "s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s,"}, {"heading": "3.2. Dataset Density", "text": "We now formally characterize the density of the datasets by defining the following notion of local relative density: Definition 4. Given a dataset D Rd, let Bp (r) be the set of points in D that are within a ball radius r by one point p. We say that D at one point p Rd has a local relative density of p Rd, if there is r, so that the number of points within the ball doubles. If it is close to 1, the dataset is close to the neighborhood of p, as there could be many points in D that are almost as far away from p. Query of the closest neighbors of such a p is intuitively considered \"hard,\" as it would be difficult to say which of these points are the truest neighbors without calculating the distances to these points."}, {"heading": "3.3. Data-Independent Version", "text": "In the file-integrated version of the algorithm that is in the query function, the probability that this will happen below and outside the parameter settings with a high probability is very high. (...) We have the ability to analyze the data in the query function. (...) We have the problem that the algorithm is not able to return the correct number of k-nearest neighbors - this can only happen if a real k-nearest neighbor is not included in any of the S-L's, which means that for each l'E a set of k-nearest points is required, which are not the true k-nearest neighbors but are closer to the query than the true k-nearest neighbors v1l.., vml. We analyze the probability that this will happen below and derive the parameter settings that the algorithms most likely ensure."}, {"heading": "3.4. Data-Dependent Version", "text": "It is a method that is able to potentially adapt to local conditions because it relies on two factors: how likely the index is to index the actual closest neighbors before other points and when the algorithm stops retrieving points from the index so that the preceding sections are focused primarily on the former; in this section we will take a closer look at the latter in this section to determine the number of iterations, it requires knowledge of the global relative thrift of datasets, which is rarely known. Calculation of this data is either very expensive in the case of large datasets or impracticable in the case of streaming data, as global relative thrift may change as new data."}, {"heading": "4. Experiments", "text": "This year, it is more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country."}, {"heading": "5. Conclusion", "text": "In this paper, we have outlined the inherent deficiencies of the spatial division and presented a new strategy for quickly locating k-nearest neighbors, which we call dynamic continuous indexing (DCI). Instead of discrediting space, the proposed algorithm constructs continuous indexes, each of which imposes a sequence of data points in which near positions roughly reflect proximity in vector space. Unlike existing methods, the proposed algorithm allows granular control over accuracy and velocity per query, adjusts to variations in dataset density on-the-fly, and supports online updates of the dataset. We have analyzed the proposed algorithm and demonstrated that it runs linear in dimensionality and sublinear in dataset size, and is constant in dimensionality and linear in dataset size."}, {"heading": "6. Supplementary Material", "text": "Here are the results shown in the main work: Lemma 6. For a global relative spareness dataset (k, p), there are some k-next points (k, p) (k, p), p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p"}], "references": [{"title": "An optimal algorithm for approximate nearest neighbor searching fixed dimensions", "author": ["Arya", "Sunil", "Mount", "David M", "Netanyahu", "Nathan S", "Silverman", "Ruth", "Wu", "Angela Y"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Arya et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Arya et al\\.", "year": 1998}, {"title": "Multidimensional binary search trees used for associative searching", "author": ["Bentley", "Jon Louis"], "venue": "Communications of the ACM,", "citeRegEx": "Bentley and Louis.,? \\Q1975\\E", "shortCiteRegEx": "Bentley and Louis.", "year": 1975}, {"title": "The X-tree : An Index Structure for HighDimensional Data", "author": ["Berchtold", "Stefan", "Keim", "Daniel A", "peter Kriegel", "Hans"], "venue": "In Very Large Data Bases, pp", "citeRegEx": "Berchtold et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Berchtold et al\\.", "year": 1996}, {"title": "Fast nearest neighbor search in high-dimensional space", "author": ["Berchtold", "Stefan", "Ertl", "Bernhard", "Keim", "Daniel A", "Kriegel", "H-P", "Seidl", "Thomas"], "venue": "In Data Engineering,", "citeRegEx": "Berchtold et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Berchtold et al\\.", "year": 1998}, {"title": "Near neighbor search in large metric spaces", "author": ["Brin", "Sergey"], "venue": "VLDB, pp", "citeRegEx": "Brin and Sergey.,? \\Q1995\\E", "shortCiteRegEx": "Brin and Sergey.", "year": 1995}, {"title": "Random projection trees and low dimensional manifolds", "author": ["Dasgupta", "Sanjoy", "Freund", "Yoav"], "venue": "In Proceedings of the fortieth annual ACM symposium on Theory of computing,", "citeRegEx": "Dasgupta et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dasgupta et al\\.", "year": 2008}, {"title": "A branch and bound algorithm for computing k-nearest neighbors", "author": ["Fukunaga", "Keinosuke", "Narendra", "Patrenahalli M"], "venue": "Computers, IEEE Transactions on,", "citeRegEx": "Fukunaga et al\\.,? \\Q1975\\E", "shortCiteRegEx": "Fukunaga et al\\.", "year": 1975}, {"title": "R-trees: a dynamic index structure for spatial searching, volume", "author": ["Guttman", "Antonin"], "venue": null, "citeRegEx": "Guttman and Antonin.,? \\Q1984\\E", "shortCiteRegEx": "Guttman and Antonin.", "year": 1984}, {"title": "Approximate nearest neighbors: towards removing the curse of dimensionality", "author": ["Indyk", "Piotr", "Motwani", "Rajeev"], "venue": "In Proceedings of the thirtieth annual ACM symposium on Theory of computing,", "citeRegEx": "Indyk et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Indyk et al\\.", "year": 1998}, {"title": "Extensions of lipschitz mappings into a hilbert space", "author": ["Johnson", "William B", "Lindenstrauss", "Joram"], "venue": "Contemporary mathematics,", "citeRegEx": "Johnson et al\\.,? \\Q1984\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 1984}, {"title": "Finding nearest neighbors in growth-restricted metrics", "author": ["Karger", "David R", "Ruhl", "Matthias"], "venue": "In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing,", "citeRegEx": "Karger et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Karger et al\\.", "year": 2002}, {"title": "An investigation of practical approximate nearest neighbor algorithms. In Advances in neural information processing", "author": ["Liu", "Ting", "Moore", "Andrew W", "Yang", "Ke", "Gray", "Alexander G"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2004}, {"title": "Scalable recognition with a vocabulary tree", "author": ["Nister", "David", "Stewenius", "Henrik"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Nister et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Nister et al\\.", "year": 2006}, {"title": "Locality sensitive hashing: A comparison of hash function types and querying mechanisms", "author": ["Paulev\u00e9", "Lo\u0131\u0308c", "J\u00e9gou", "Herv\u00e9", "Amsaleg", "Laurent"], "venue": "Pattern Recognition Letters,", "citeRegEx": "Paulev\u00e9 et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Paulev\u00e9 et al\\.", "year": 2010}, {"title": "A fast exact k-nearest neighbors algorithm for high dimensional search using k-means clustering and triangle inequality", "author": ["Wang", "Xueyi"], "venue": "In Neural Networks (IJCNN), The 2011 International Joint Conference on,", "citeRegEx": "Wang and Xueyi.,? \\Q2011\\E", "shortCiteRegEx": "Wang and Xueyi.", "year": 2011}, {"title": "Spectral hashing", "author": ["Weiss", "Yair", "Torralba", "Antonio", "Fergus", "Rob"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Weiss et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 11, "context": "One popular approach used by LSH and spill trees (Liu et al., 2004) to mitigate this effect is to partition the space using overlapping cells and search over all points that lie in any of the cells that contain the query point.", "startOffset": 49, "endOffset": 67}, {"referenceID": 2, "context": "Early approaches store points in deterministic tree-based data structures, such as k-d trees (Bentley, 1975), R-trees (Guttman, 1984) and X-trees (Berchtold et al., 1996; 1998), which effectively partition the vector space into a hierarchy of half-spaces, hyper-rectangles or Voronoi polygons.", "startOffset": 146, "endOffset": 176}, {"referenceID": 11, "context": "More recent methods like spill trees (Liu et al., 2004) and RP trees (Dasgupta & Freund, 2008) extend these approaches by randomizing the dividing hyperplane at each node.", "startOffset": 37, "endOffset": 55}, {"referenceID": 0, "context": "Treebased methods (Arya et al., 1998) have been proposed for this setting; unfortunately, the running time still exhibits exponential dependence on dimensionality.", "startOffset": 18, "endOffset": 37}, {"referenceID": 13, "context": "This motivated the development of data-dependent hashing schemes based on k-means (Paulev\u00e9 et al., 2010) and spectral partitioning (Weiss et al.", "startOffset": 82, "endOffset": 104}, {"referenceID": 15, "context": ", 2010) and spectral partitioning (Weiss et al., 2009).", "startOffset": 34, "endOffset": 54}], "year": 2017, "abstractText": "Existing methods for retrieving k-nearest neighbours suffer from the curse of dimensionality. We argue this is caused in part by inherent deficiencies of space partitioning, which is the underlying strategy used by almost all existing methods. We devise a new strategy that avoids partitioning the vector space and present a novel randomized algorithm that runs in time linear in dimensionality and sub-linear in the size of the dataset and takes space constant in dimensionality and linear in the size of the dataset. The proposed algorithm allows fine-grained control over accuracy and speed on a per-query basis, automatically adapts to variations in dataset density, supports dynamic updates to the dataset and is easy-to-implement. We show appealing theoretical properties and demonstrate empirically that the proposed algorithm outperforms localitysensitivity hashing (LSH) in terms of approximation quality and speed.", "creator": "LaTeX with hyperref package"}}}