{"id": "1512.00818", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2015", "title": "Zero-Shot Event Detection by Multimodal Distributional Semantic Embedding of Videos", "abstract": "We propose a new zero-shot Event Detection method by Multi-modal Distributional Semantic embedding of videos. Our model embeds object and action concepts as well as other available modalities from videos into a distributional semantic space. To our knowledge, this is the first Zero-Shot event detection model that is built on top of distributional semantics and extends it in the following directions: (a) semantic embedding of multimodal information in videos (with focus on the visual modalities), (b) automatically determining relevance of concepts/attributes to a free text query, which could be useful for other applications, and (c) retrieving videos by free text event query (e.g., \"changing a vehicle tire\") based on their content. We embed videos into a distributional semantic space and then measure the similarity between videos and the event query in a free text form. We validated our method on the large TRECVID MED (Multimedia Event Detection) challenge. Using only the event title as a query, our method outperformed the state-of-the-art that uses big descriptions from 12.6% to 13.5% with MAP metric and 0.73 to 0.83 with ROC-AUC metric. It is also an order of magnitude faster.", "histories": [["v1", "Wed, 2 Dec 2015 19:34:00 GMT  (1528kb,D)", "http://arxiv.org/abs/1512.00818v1", "To appear in AAAI 2016"], ["v2", "Wed, 16 Dec 2015 00:58:49 GMT  (1479kb,D)", "http://arxiv.org/abs/1512.00818v2", "To appear in AAAI 2016"]], "COMMENTS": "To appear in AAAI 2016", "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.LG", "authors": ["mohamed elhoseiny", "jingen liu", "hui cheng", "harpreet s sawhney", "ahmed m elgammal"], "accepted": true, "id": "1512.00818"}, "pdf": {"name": "1512.00818.pdf", "metadata": {"source": "META", "title": "Zero-Shot Event Detection by Multimodal Distributional Semantic Embedding of Videos", "authors": ["Mohamed Elhoseiny", "Jingen Liu", "Hui Cheng", "Harpreet Sawhney", "Ahmed Elgammal"], "emails": ["elgammal@cs.rutgers.edu"], "sections": [{"heading": "Introduction", "text": "This year, it is more than ever before before that a decision has to be taken to find a solution that adapts to the needs of the people."}, {"heading": "Related Work", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they live."}, {"heading": "Method", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Problem Definition", "text": "We start with the definition of the representation of event query e, the concept of video e, the video v, the video v, the video v, the video v that is used in our environment. While we show the callability of various modalities, the concept of callability is our main focus in this work. The little keyword event query for the concept of callability is denoted by ec, while the query keywords for OCR and ASR are denoted by other modalities. Hence, under our setting e = {ec, eo}.Concept Set c: We denote the whole concept set in our setting as c, which include visual concepts cv and audio concepts cd, i.e."}, {"heading": "Distributional Semantic Model & \u03b8(\u00b7) Embedding", "text": "We start with the semantic model of (Mikolov et al.) 2013b; 2013a) to learn our semantic diversity. We call the dimensionality of the real vector circumscribed by vec (\u00b7 L + 1, \u00b7 L + 1, wi + L) maximized over the training corpus; 2 \u00b7 L is the context of the window size between vec (wi \u2212 L) and vec (wj) is high if they share a similar context in the context of size 2 \u00d7 L (i.e., semantically similar words share a similar context. Hence similarity between vec (wi \u2212 L) is high if they share a set in the context of size 2 \u00d7 L in the context of the text corpus (i.e., semantically similar words share a similar context."}, {"heading": "EDiSE Computational Performance Benefits", "text": "Here we discuss the computational complexity of concept-based EDISE and ASR / OCR-based EDiSE. The fusion part is negligible because it is a constant time."}, {"heading": "Concept based EDiSE", "text": "The computational complexity for p (ec | v) is essentially linear in relation to the number of videos designated with | V |. We will explain here why the computational complexity of p (ec | v) is almost constant for the e-video, and therefore the computational complexity of s (\u00b7, \u00b7) and N is the number of concepts. Next, we will consider the computational complexity of sp (\u00b7, \u00b7) and st (\u00b7) for the entire set of videos, where Q's computational complexity of p (ec | v) for sp (\u00b7, \u00b7) andN is the number of concepts. We will assume the computational complexity of sp (\u00b7) and st (\u00b7 \u00b7) computational complexity of p. The complexity of p (ec | v) for sp (\u00b7, \u00b7) andN is the number of concepts."}, {"heading": "ASR/OCR based EDiSE", "text": "The computerized complexity of sd (\u03b8 (eo), \u03b8 (vo) 1.56% 1.69% 1.69% 0.66% 0.72% 1.72% | 1.43% (va) are O (| eo | \u00b7 vo | \u00b7 M) and O (| ea | \u00b7 M), Table 1: MED2013 MAP performance on four concept sets (event title query) Our Gnews Wiki (Dalton etal, 2013) TRECVID MED 2013 \u03c1m (\u00b7) \u03c1m (\u00b7) \u03c1m (\u00b7) sp (\u00b7, \u00b7) st (\u00b7) st (\u00b7, \u00b7) st (\u00b7) st (\u00b7) st (\u00b7, \u00b7) sp (\u00b7) st (\u00b7, \u00b7) sp (\u00b7) sp (\u00b7) sp (\u00b7) sp (\u00b7) sp (\u00b7)) concepts G1 (\u00b7), (\u00b7) \u03c1m (\u00b7) sp (\u00b7) \u03c1a (\u00b7) sp (\u00b7) \u03c1a (\u00b7) sp (\u00b7) sp (\u00b7) st (\u00b7) st (\u00b7) st (\u00b7) st (\u00b7) st) st (\u00b7) st (\u00b7) st (\u00b7) st (\u00b7) sp (\u00b7) sp (\u00b7) sp (\u00b7) sp (\u00b7) sp (\u00b7) sp (\u00b7)), (\u00b7) sp (\u00b7) concepts G1 (\u00b7) \u03c1m (\u00b7), (\u00b7) \u03c1m (\u00b7), (\u00b7) \u03c1m (\u00b7) \u03c1m (\u00b7) sp (\u00b7) Concepts G1 (\u00b7), (\u00b7) \u03c1a (\u00b7) \u03c1a (\u00b7) sp (\u00b7) 2.39% 2.38% 2.38% 2.38% 2.14% 2.13% 2.85% 1.70% Concepts G2 (101 concepts) 0.29% 0.96% 20% auto 0.66% 0.66% 0.66% 0.20% 0.66% 0.20% 0.66% 0.20% 0.66% 0.20% 0.20% 0.20%"}, {"heading": "Experiments", "text": "We evaluated our method using the large TRECVID MED (Felzenszwalb, McAllester and Ramanan 2013) and show the MAP (Mean Average Precision) and ROC AUC performance of the named MEDTest set (Felzenszwalb, McAllester and Ramanan 2013) with more than 25,000 videos. Unless otherwise mentioned, our results are on TRECVID MED2013. In our experiments there are two semantic distribution models trained on Wikipedia and GoogleNews using (Mikolov et al. 2013b) The Wikipedia model was trained on 1 billion words, resulting in a vocabulary of about 120,000 words and word vectors of 250 dimensions. The GoogleNews model was trained on 100 billion words, resulting in a vocabulary of 3 million words and word vectors of 300 dimensions. The goal of having two models is to compare how well our EDiSE method works in dependence on the rest of the CASR model's size."}, {"heading": "Concept based Retrieval", "text": "We start by comparing different settings of our method against (Dalton, Allan, and Mirajkar 2013), which indicates that p (e | ci) in Eq. 4 is calculated using the language model in (Dalton, Allan, and Mirajkar 2013) as it is based in (Dalton, Allan, and Mirajkar 2013) for a concept that we compare with exactly the same setting. For our model, we compared the two pooling operations in (\u00b7) and the two different similarity measures onMs space sp (\u00b7) and st (\u00b7). Furthermore, we evaluated the methods on both Wikipedia and GNews language models."}, {"heading": "ASR and OCR based Retrieval", "text": "First, we compared our OCR and ASR retrieval using Wikipedia and GoogleNews language models. Table 3 shows that the GoogleNews MED13 MAP model is better in both ASR and OCR than the Wikipedia MAP model, which is consistent with our conceptual results. Figure 5 shows GoogleNews MED13 AP per event for both OCR and ASR. Furthermore, we show our AP performance for MED14 events 31 to 40 in Fig. 5. To represent the value of our semantic modeling, we calculated the performance of the string matching method as a baseline, which increases the score for each exact match in the calculated text with words in the query. While both our model and the matching model use the same search words and ASR / OCR recognition, semantic properties captured by Ms increase performance compared to the string matching method (see table 3. AUsemantic for AUsemantic similarity)."}, {"heading": "Fusion Experiments and Related Systems", "text": "In Table 5, we start by summarizing our previous ASR / OCR results on MED13. Comparing OCR and ASR performances to concepts performance, it is not hard to see that OCR / ASR produces much lower average performance compared to concepts that are visually in our work. This suggests that OCR / ASR produces much higher false negatives compared to visual concepts. If we have our overall OCR and ASR confidence, we achieve 10.7% MAP performance, but the average performance of AUC is as low as 0.67. We achieved lower MAP for our concepts 8.36% MAP, but the average performance of AUC is as high as 0.834. This suggests that measuring retrieval average average performance on MAP performance is just not informative, so an approach could produce a lower average average average average average average average average average average MAP and vice versa."}, {"heading": "Conclusion", "text": "We proposed a method for zero-shot event detection by embedding distributional semantics of video modalities and only querying the event title. By merging all modalities, our method exceeded the state of the art in the sophisticated TRECVID MED benchmark. Based on this idea, we also demonstrated how the relevance of concepts to an event can be determined automatically on the basis of distributional semantics. Recognition. This work was supported by the Intelligence Advanced Research Projects Activity (IARPA) through contract number D11-PC20066 of the US Department of the Interior. The US government is authorized to reproduce and distribute reprints for government purposes, regardless of the copyright notice contained therein. The views and conclusions contained therein are those of the authors and should not necessarily be interpreted to represent the official guidelines or endorsements of IARPA, DOI / NBC or the US government."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We propose a new zero-shot Event Detection method by Multi-modal Distributional Semantic embedding of videos. Our model embeds object and action concepts as well as other available modalities from videos into a distributional semantic space. To our knowledge, this is the first Zero-Shot event detection model that is built on top of distributional semantics and extends it in the following directions: (a) semantic embedding of multimodal information in videos (with focus on the visual modalities), (b) automatically determining relevance of concepts/attributes to a free text query, which could be useful for other applications, and (c) retrieving videos by free text event query (e.g., \u201dchanging a vehicle tire\u201d) based on their content. We embed videos into a distributional semantic space and then measure the similarity between videos and the event query in a free text form. We validated our method on the large TRECVID MED (Multimedia Event Detection) challenge. Using only the event title as a query, our method outperformed the state-of-the-art that uses big descriptions from 12.6% to 13.5% with MAP metric and 0.73 to 0.83 with ROC-AUC metric. It is also an order of magnitude faster.", "creator": "TeX"}}}