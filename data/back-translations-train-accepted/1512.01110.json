{"id": "1512.01110", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Dec-2015", "title": "Bayesian Matrix Completion via Adaptive Relaxed Spectral Regularization", "abstract": "Bayesian matrix completion has been studied based on a low-rank matrix factorization formulation with promising results. However, little work has been done on Bayesian matrix completion based on the more direct spectral regularization formulation. We fill this gap by presenting a novel Bayesian matrix completion method based on spectral regularization. In order to circumvent the difficulties of dealing with the orthonormality constraints of singular vectors, we derive a new equivalent form with relaxed constraints, which then leads us to design an adaptive version of spectral regularization feasible for Bayesian inference. Our Bayesian method requires no parameter tuning and can infer the number of latent factors automatically. Experiments on synthetic and real datasets demonstrate encouraging results on rank recovery and collaborative filtering, with notably good results for very sparse matrices.", "histories": [["v1", "Thu, 3 Dec 2015 15:16:19 GMT  (133kb,D)", "https://arxiv.org/abs/1512.01110v1", "Accepted to AAAI 2016"], ["v2", "Fri, 25 Dec 2015 02:51:22 GMT  (134kb,D)", "http://arxiv.org/abs/1512.01110v2", "Accepted to AAAI 2016"]], "COMMENTS": "Accepted to AAAI 2016", "reviews": [], "SUBJECTS": "cs.NA cs.AI cs.LG", "authors": ["yang song", "jun zhu"], "accepted": true, "id": "1512.01110"}, "pdf": {"name": "1512.01110.pdf", "metadata": {"source": "CRF", "title": "Bayesian Matrix Completion via Adaptive Relaxed Spectral Regularization", "authors": ["Yang Song", "Jun Zhu"], "emails": ["yang.song@zoho.com", "dcszj@mail.tsinghua.edu.cn"], "sections": [{"heading": "Introduction", "text": "France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, France, France, France, Great Britain, Great Britain, France, France, France, Great Britain, Great Britain, France, France, France, Great Britain, Great Britain, Great Britain, France, France, France, Great Britain, Great Britain, Great Britain, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, Great Britain, Great Britain, Great Britain, Great Britain, France, Great Britain, Great Britain, France, France, France, France, France, France, Great Britain, Great Britain, Great Britain, France, France, France, France, France, Great Britain, Great Britain, Great Britain, France, France, France, France, Great Britain, France, France, Great Britain, France, Great Britain, France, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, France, France, Great Britain, Great Britain, Great Britain, Great Britain, France, Great Britain, France, France, Great Britain, France, Great Britain, Great Britain, Great Britain, France, France, Great Britain, Great Britain, France, Great Britain, France, Great Britain, Great Britain, France, France, France, Great Britain, Great Britain, France, Great Britain, Great Britain, France, Great Britain, France, Great Britain, France, France, Great Britain, France, France, Great Britain, France, France, Great Britain, Great Britain, France, Great Britain, France, France, France, Great Britain, Great Britain, France, Great Britain, France, France, Great Britain, France, France, Great Britain, France, Great Britain, Great Britain, France, Great Britain, France, France, France, Great Britain, France, France, France, France, Great Britain, Great Britain, France, Great Britain, France, France, Great Britain, France, France, Great Britain, France, France, France, Great Britain,"}, {"heading": "Relaxed Spectral Regularization", "text": "It is now easy to do posterior inferencing, as the previous and probable procedures are very difficult when we are trying to develop Bayesian matrix."}, {"heading": "Bayesian Matrix Completion via Adaptive Relaxed Spectral Regularization", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Probabilistic model", "text": "We turn P3 into an equivalent MAP estimation task (pd.f.). Of course, the square error loss in P3 corresponds to the negative logarithm of the Gaussian probability, where uki denotes the ith term in uk; likewise for vkj. For priors we take uniform hair priors on U and V within unit spheres and exponential priors on ~ d, as summarized below: p (uk) = {1, UK-UK-1 0, UK-1 [r] p-priors on Pk spheres, and exponential priors on ~ d, as summarized below: p-spheres (uk) = {1, UK-UK-1-0 [r] p-primary distributions on b spheres."}, {"heading": "Inference", "text": "We now introduce the GASR (Gibbs sampler for each is a): \"We can use the GASR (Gibbs sampler for each is a).\" \"We can use the GASR (Gibbs sampler for each is a).\" \"We can use the GASR (Gibbs sampler).\" \"We.\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" \".\" \"\" We. \"\" \"\" We. \"\" \"\" \".\" \"\" \".\" \"\". \"\". \".\" \"\". \".\" \".\" \".\" \"\". \"\" \".\" \"\" \".\" \"\". \"\". \"\". \"\". \".\". \".\". \"\". \".\" \".\". \".\". \".\". \"\". \".\". \".\". \".\". \"\". \".\". \".\". \".\" \".\". \".\" \"\". \".\". \".\" \"\" \".\". \".\" \".\". \".\" \"\". \"\". \".\". \".\". \".\". \"\" \"\". \".\". \".\" \"\". \".\". \".\". \".\". \".\". \"\". \".\". \"\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\".. \".....\" \"\".... \"\" \".\". \".\".... \"..\".. \"......\"................... \"\".............. \"\" \"\" \"\" \"\" \"\" \".\" \".\" \"\". \"\". \"\".... \"\" \"....\" \"........................................."}, {"heading": "Experiments", "text": "We are now presenting experimental results on both synthetic and real data sets to demonstrate the effectiveness of rank recovery and matrix completion."}, {"heading": "Experiments on synthetic data", "text": "This year, it has reached the stage where it will be able to take the lead in opening up a wide range of future perspectives."}, {"heading": "Conclusions and Discussions", "text": "We present a novel Bayesian matrix completion method with adaptive relaxed spectral regulation, demonstrating the advantages of hierarchical Bayesian methods in inferring the parameters associated with adaptive relaxed spectral regulation, thereby avoiding parameter tuning. We estimate hyperparameters using Monte Carlo EM. Our Gibbs sampler performs well both in ranking synthetic data and in cooperative filtering on real datasets. Our method is based on a new formulation in Theorem 1. These results can be further generalized to other noise sources with little effort. For the Gibbs sampler, we can also extend to non-Gaussian potentials, as long as this potential has a regular p.d.f., which allows efficient sampling. Although we adhere to Gibbs sampling in this paper, it would be interesting to examine other Bayesian inference methods based on a lack of theorem 1, which many problems will arise."}, {"heading": "Acknowledgments", "text": "The work was supported by the National Basic Research Program (Nos. 2013CB329403, 2012CB316301), the National NSF of China (Nos. 61322308, 61332007), the Tsinghua National Laboratory for Information Science and Technology Big Data Initiative, and the Tsinghua Initiative Scientific Research Program (Nos. 20141080934). We thank the Department of Physics of Tsinghua University for reimbursing part of the travel expenses."}, {"heading": "Supplementary Materials", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Proof for Theorem 1", "text": "= 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1"}, {"heading": "Detailed Experimental Results for Different Missing Rates", "text": "In this experiment, we perform both BPMF and GASR over 100 iterations and on average every 100 samples to obtain the final result. Table 3 shows the average RMSE and the corresponding deviations on three randomly generated datasets."}], "references": [{"title": "Optimization with sparsity-inducing penalties. Foundations and Trends in Machine Learning 4(1):1\u2013106", "author": ["Bach"], "venue": null, "citeRegEx": "Bach,? \\Q2012\\E", "shortCiteRegEx": "Bach", "year": 2012}, {"title": "and Girolami", "author": ["S. Byrne"], "venue": "M.", "citeRegEx": "Byrne and Girolami 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Tourneret", "author": ["N. Dobigeon"], "venue": "J.-Y.", "citeRegEx": "Dobigeon and Tourneret 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Eigentaste: A constant time collaborative filtering algorithm. Information Retrieval 4(2):133\u2013151", "author": ["Goldberg"], "venue": null, "citeRegEx": "Goldberg,? \\Q2001\\E", "shortCiteRegEx": "Goldberg", "year": 2001}, {"title": "and Ghahramani", "author": ["T.L. Griffiths"], "venue": "Z.", "citeRegEx": "Griffiths and Ghahramani 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Matrix completion and low-rank SVD via fast alternating least squares. arXiv preprint arXiv:1410.2596", "author": ["Hastie"], "venue": null, "citeRegEx": "Hastie,? \\Q2014\\E", "shortCiteRegEx": "Hastie", "year": 2014}, {"title": "P", "author": ["Hoff"], "venue": "D.", "citeRegEx": "Hoff 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "I", "author": ["James"], "venue": "M.", "citeRegEx": "James 1976", "shortCiteRegEx": null, "year": 1976}, {"title": "Y", "author": ["Y.J. Lim", "Teh"], "venue": "W.", "citeRegEx": "Lim and Teh 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Spectral regularization algorithms for learning large incomplete matrices", "author": ["Hastie Mazumder", "R. Tibshirani 2010] Mazumder", "T. Hastie", "R. Tibshirani"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Mazumder et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mazumder et al\\.", "year": 2010}, {"title": "and Srebro", "author": ["J.D. Rennie"], "venue": "N.", "citeRegEx": "Rennie and Srebro 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "and Mnih", "author": ["R. Salakhutdinov"], "venue": "A.", "citeRegEx": "Salakhutdinov and Mnih 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "T", "author": ["N. Srebro", "J. Rennie", "Jaakkola"], "venue": "S.", "citeRegEx": "Srebro. Rennie. and Jaakkola 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Nonparametric max-margin matrix factorization for collaborative prediction", "author": ["Zhu Xu", "M. Zhang 2012] Xu", "J. Zhu", "B. Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Xu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2012}, {"title": "Fast max-margin matrix factorization with data augmentation", "author": ["Zhu Xu", "M. Zhang 2013] Xu", "J. Zhu", "B. Zhang"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Xu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2013}], "referenceMentions": [], "year": 2015, "abstractText": "Bayesian matrix completion has been studied based on a low-rank matrix factorization formulation with promising results. However, little work has been done on Bayesian matrix completion based on the more direct spectral regularization formulation. We fill this gap by presenting a novel Bayesian matrix completion method based on spectral regularization. In order to circumvent the difficulties of dealing with the orthonormality constraints of singular vectors, we derive a new equivalent form with relaxed constraints, which then leads us to design an adaptive version of spectral regularization feasible for Bayesian inference. Our Bayesian method requires no parameter tuning and can infer the number of latent factors automatically. Experiments on synthetic and real datasets demonstrate encouraging results on rank recovery and collaborative filtering, with notably good results for very sparse matrices. Introduction Matrix completion has found applications in many situations, such as collaborative filtering. Let Zm\u00d7n denote the data matrix with m rows and n columns, of which only a small number of entries are observed, indexed by \u03a9 \u2282 [m] \u00d7 [n]. We denote the possibly noise corrupted observations of Z on \u03a9 as P\u03a9(X), where P\u03a9 is a projection operator that retains entries with indices from \u03a9 and replaces others with 0. The matrix completion task aims at completing missing entries of Z based on P\u03a9(X), under the low-rank assumption rank(Z) min(m,n). When a squared-error loss is adopted, it can be written as solving: min Z 1 2\u03c32 \u2016P\u03a9(X \u2212 Z)\u20162F + \u03bb rank(Z), (P0) where \u2016P\u03a9(A)\u20162F = \u2211 (i,j)\u2208\u03a9 a 2 ij ; \u03bb is a positive regularization parameter; and \u03c3 is the noise variance. Unfortunately, the term rank(Z) makes P0 NP-hard. Therefore, the nuclear norm \u2016Z\u2016\u2217 has been widely adopted as a convex surrogate (Fazel 2002) to the rank function to turn P0 to a convex problem: min Z 1 2\u03c32 \u2016P\u03a9(X \u2212 Z)\u20162F + \u03bb \u2016Z\u2016\u2217 . (P1) Copyright \u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Though P1 is convex, the definition of nuclear norm makes the problem still not easy to solve. Based on a variational formulation of the nuclear norm, it has been popular to solve an equivalent and easier low-rank matrix factorization (MF) form of P1: min A,B 1 2\u03c32 \u2016P\u03a9(X \u2212AB)\u20162F + \u03bb 2 (\u2016A\u20162F + \u2016B\u2016 2 F ). (1) Though not joint convex, this MF formulation can be solved by alternately optimizing over A and B for local optima. As the regularization terms of MF are friendlier than the nuclear norm, many matrix factorization methods have been proposed to complete matrices, including maximummargin matrix factorization (M3F) (Srebro, Rennie, and Jaakkola 2004; Rennie and Srebro 2005) and Bayesian probabilistic matrix factorization (BPMF) (Lim and Teh 2007; Salakhutdinov and Mnih 2008). Furthermore, the simplicity of the MF formulation helps people adapt it and generalize it; e.g., (Xu, Zhu, and Zhang 2012; Xu, Zhu, and Zhang 2013) incorporate maximum entropy discrimination (MED) and nonparametric Bayesian methods to solve a modified MF problem. In contrast, there are relatively fewer algorithms to directly solve P1 without the aid of matrix factorization. Such methods need to handle the spectrum of singular values. These spectral regularization algorithms require optimization on a Stiefel manifold (Stiefel 1935; James 1976), which is defined as the set of k-tuples (u1,u2, \u00b7 \u00b7 \u00b7 ,uk) of orthonormal vectors in R. This is the main difficulty that has prevented the attempts, if any, to develop Bayesian methods based on the spectral regularization formulation. Though matrix completion via spectral regularization is not easy, there are potential advantages over the matrix factorization approach. One of the benefits is the direct control over singular values. By imposing various priors on singular values, we can incorporate abundant information to help matrix completion. For example, Todeschini et al. (Todeschini, Caron, and Chavent 2013) put sparsity-inducing priors on singular values, naturally leading to hierarchical adaptive nuclear norm (HANN) regularization, and they reported promising results. In this paper, we aim to develop a new formulation of the nuclear norm, hopefully having the same simplicity as MF and retaining some good properties of spectral regularar X iv :1 51 2. 01 11 0v 2 [ cs .N A ] 2 5 D ec 2 01 5 ization. The idea is to prove the orthonormality insignificance property of P1. Based on the new formulation, we develop a novel Bayesian model via a sparsity-inducing prior on singular values, allowing various dimensions to have different regularization parameters and automatically infer them. This involves some natural modifications to our new formulation to make it more flexible and adaptive, as people typically do in Bayesian matrix factorization. Empirical Bayesian methods are then employed to avoid parameter tuning. Experiments about rank recovery on synthetic matrices and collaborative filtering on some popular benchmark datasets demonstrate competitive results of our method in comparison with various state-of-the-art competitors. Notably, experiments on synthetic data show that our method performs considerably better when the matrices are very sparse, suggesting the robustness offered by using sparsityinducing priors. Relaxed Spectral Regularization Bayesian matrix completion based on matrix factorization is relatively easy, with many examples (Lim and Teh 2007; Salakhutdinov and Mnih 2008). In fact, we can view (1) as a maximum a posterior (MAP) estimate of a simple Bayesian model, whose likelihood is Gaussian, i.e., for (i, j) \u2208 \u03a9, Xij \u223c N ((AB)ij , \u03c3), and the priors on A and B are also Gaussian, i.e., p(A) \u221d exp(\u2212\u03bb \u2016A\u20162F /2) and p(B) \u221d exp(\u2212\u03bb \u2016B\u20162F /2). It is now easy to do the posterior inference since the prior and likelihood are conjugate. However, the same procedure faces great difficulty when we attempt to develop Bayesian matrix completion based on the more direct spectral regularization formulation P1. This is because the prior p(Z) \u221d exp(\u2212\u03bb \u2016Z\u2016\u2217) is not conjugate to the Gaussian likelihood (or any other common likelihood). To analyze p(Z) more closely, we can conduct singular value decomposition (SVD) on Z to get Z = \u2211r k=1 dkukv T k , where ~ d := {dk : k \u2208 [r]} are singular values; U := {uk : k \u2208 [r]} and V := {vk : k \u2208 [r]} are orthonormal singular vectors on Stiefel manifolds. Though we can define a factorized prior p(Z) = p(~ d)p(U)p(V ), any prior on U or V (e.g., the uniform Haar prior (Todeschini, Caron, and Chavent 2013)) needs to deal with a Stiefel manifold, which is highly nontrivial. In fact, handling distributions embedded on Stiefel manifolds still remains a largely open problem, though some results (Byrne and Girolami 2013; Hoff 2009; Dobigeon and Tourneret 2010) exist in the literature of directional statistics. Fortunately, as we will prove in Theorem 1 that the orthonormality constraints on U and V are not necessary for spectral regularization. Rather, the unit sphere constraints \u2016uk\u2016 \u2264 1 and \u2016vk\u2016 \u2264 1, for all k \u2208 [r], are sufficient to get the same optimal solutions to P1. We call this phenomenon orthonormality insignificance. We will call spectral regularization with orthonormality constraints relaxed by unit sphere constraints relaxed spectral regularization. Orthonormality insignificance for spectral regularization We now present an equivalent formulation of the spectral regularization in P1 by proving its orthornormality insignificance property. With the SVD of Z, we first rewrite P1 equivalently as P1\u2032 to show all constraints explicitly: min ~ d,U,V 1 2\u03c32 \u2225\u2225\u2225\u2225P\u03a9 ( X \u2212 r \u2211 k=1 dkukv T k \u2225\u2225\u2225\u2225 2", "creator": "LaTeX with hyperref package"}}}