{"id": "1512.01370", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2015", "title": "Locally Adaptive Translation for Knowledge Graph Embedding", "abstract": "Knowledge graph embedding aims to represent entities and relations in a large-scale knowledge graph as elements in a continuous vector space. Existing methods, e.g., TransE and TransH, learn embedding representation by defining a global margin-based loss function over the data. However, the optimal loss function is determined during experiments whose parameters are examined among a closed set of candidates. Moreover, embeddings over two knowledge graphs with different entities and relations share the same set of candidate loss functions, ignoring the locality of both graphs. This leads to the limited performance of embedding related applications. In this paper, we propose a locally adaptive translation method for knowledge graph embedding, called TransA, to find the optimal loss function by adaptively determining its margin over different knowledge graphs. Experiments on two benchmark data sets demonstrate the superiority of the proposed method, as compared to the-state-of-the-art ones.", "histories": [["v1", "Fri, 4 Dec 2015 11:09:55 GMT  (73kb,D)", "http://arxiv.org/abs/1512.01370v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL", "authors": ["yantao jia", "yuanzhuo wang", "hailun lin", "xiaolong jin", "xueqi cheng"], "accepted": true, "id": "1512.01370"}, "pdf": {"name": "1512.01370.pdf", "metadata": {"source": "CRF", "title": "Locally Adaptive Translation for Knowledge Graph Embedding", "authors": ["Yantao Jia", "Yuanzhuo Wang", "Hailun Lin", "Xiaolong Jin", "Xueqi Cheng"], "emails": [], "sections": [{"heading": null, "text": "Keywords: local adaptive translation, embedding of knowledge graphs, optimal margin"}, {"heading": "Introduction", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "Related Work", "text": "Existing methods for embedding knowledge aim to represent units and relationships of knowledge graphs as vectors in a continuous vector space, in which they normally define a loss function in order to evaluate representations. Different methods differ in the definition of loss functions in relation to the triple (h, r, t) in knowledge graphs, where h and t denote the head and tail units and r represents their relationship. The loss function implies a kind of transformation of h and t. Among translation-based methods, TransE (Bordes et al. 2013) assumes h + r = t when (h, r, t) is a golden triple, suggesting that t should be the closest neighbor of h + r. Unstructured method (Bordes et al. 2014) is a naive version of Transtail et al = 0. In order to deal with relationships with other mapping properties, TransH Wang et al. 2014 became a model for incorporating neural functions."}, {"heading": "Loss Function Analysis", "text": "In this section, we first examine the loss functions across different knowledge diagrams and find their difference in determining the margins. Then, to find out how margin affects the performance of embedding, we derive a relationship based on the concept of stability for learning algorithms (Bousquet and Elisseeff 2002).Margin setting across different knowledge diagrams As mentioned above, a knowledge diagram is composed of heterogeneous units and relationships. Knowledge diagrams with different types of units and relationships differ andexhibit from different localities in terms of types of their elements. In this sense, existing knowledge using a common margin-based loss function cannot satisfactorily represent the locality of knowledge diagrams. To verify this, we construct knowledge diagrams with different localities. We simply partition a knowledge diagram into different subgraphs that we name in the same way. Each subgraph contains different types of entries and their corresponding relationships."}, {"heading": "How margin affects the performance", "text": "This raises a theoretical question as to how the setting of the margin affects performance as the margin increases. To answer this question, we represent a relationship between the error of performance and marginality. Before we get the relation, let us first give some notations. Denote the embedding method of A. Since the embedding method consists in learning the appropriate representations of entities and relationships in the knowledge diagram, it can be considered a learning algorithm. Suppose that for the embedded method and the knowledge diagram, the training data collection S = {h1, r1),. (hi, ri, ti),. (hn, tn)} the size n is a set of triplets in the knowledge diagram and the range of (hi, ri, ti). Denote Si is obtained from S."}, {"heading": "Locally Adaptive Translation Method", "text": "In this section, we propose a locally adaptive translation method called TransA in order to adjust the optimal margin. As the classical knowledge chart consists entirely of two disjunctive groups, i.e. the totality and the quantity of relations, it makes sense that the optimal margin designated by Mopt consists of two parts, namely the totality-specific marginality and the relationship-specific marginality Mrel. Furthermore, it is natural to combine the two specific marginals linearly using a parameter \u00b5 that controls trade between them. Therefore, it is sufficient to find the optimal margin of embedding Mopt = \u00b5Ment + (1 \u2212 \u00b5) Mrel, (3), where 0 \u2264 \u00b5 \u2264 \u2264 \u2264 1. To show that the margin Mopt is optimal, find the optimal totality-specific margin and the optimal relation that we will elaborate in this section."}, {"heading": "Entity-specific margin", "text": "In this sense, the optimal margin of Ment is actually equal to the distance between two concentric spheres in vector space, as illustrated in Figure 1. Positive entities (as illustrated as \") are limited within the internal sphere, while negative entities (as illustrated as\") are outside the external sphere. Interpretation of Ment is motivated by the work of metric learning, such as (Weinberger and Saul 2009)."}, {"heading": "Relation-specific margin", "text": "The optimal relation-specific margin Mrel of the relation-specific margin is found by looking at the proximity of the relations between the individual entities. (It) It is not the way in which the relationship between the two entities (between the two entities) is defined. (It) It is the way in which the relationship between the two entities (between the two entities) is defined. (It) is the way in which the relationship between the entity-specific entities (between the two entities-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-entity-activity-entity-entity"}, {"heading": "Experiments", "text": "We will conduct experiments on two tasks: link prediction (Bordes et al. 2013) and triple classification (Wang et al. 2014). The data sets we use are available from two widely used knowledge graphics, WordNet (Miller 1995) and Freebase (Bollacker et al. 2008). For WordNet data sets, we use WN18, which is used in (Bordes et al. 2014) and WN11, which is used in (Socher et al. 2013). For Freebase data sets, we use FB15K, which is also used in (Bordes et al. 2014) and FB13 (Socher et al. 2013). Statistics of these data sets are listed in Table 2."}, {"heading": "Link prediction", "text": "Link Prediction aims to predict the missing units h or t for a triple (h, r, t). Namely, it predicts t given (h, r) or h given (r, t). Similar to the recruitment in (Bordes et al. 2011; Bordes et al. 2013; Zhao, Jia, and Wang 2014), the task provides a list of candidates from the knowledge diagram instead of a best answer, and we perform the link prediction task on the two data sets WN18 and FB15K. Following the procedure used in (Bordes et al. 2013), we also adopt the assessment unit, namely the mean order of the correct units (i.e., middle order of the correct units). It is clear that a good predictor has a lower mean. In the test phase, for each test triple (h, r, t), we replace the head or tail unit with all units in the knowledge diagram and rank the units in the order of the decreasing values in the order of the rankings."}, {"heading": "Triple classification", "text": "The triple classification, examined in (Socher et al. 2013; Wang et al. 2014), is to confirm whether a triple (h, r, t) is correct or not, namely a problem of binary classification on the triple. The data sets we use in this task are WN11 and FB13, which are used in (Socher et al. 2013) and FB15K (Wang etal. 2014). Following the evaluation in NTN (Socher et al. 2013), the evaluation requires negative designations. The data sets WN11 and FB13 already have negative triples, which are achieved by corrupting golden triples. For FB15K, we follow the same way to construct negative triples as (Socher et al. 2013). The classification is evaluated as follows. For a triple (h, r, t) are negative triples, which are achieved by transsimilation."}, {"heading": "Conclusion", "text": "In this paper, we addressed the problem of knowledge embedding and proposed a locally adaptive translation method called TransA to adaptively learn the representation of units and relationships in a knowledge diagram. First, we presented the need for an appropriate margin in the marginal loss function. Then, we defined the optimal margin from the aspects of unity and relation and integrated the margin into the frequently used loss function for embedding knowledge. Experimental results confirm the effectiveness of the proposed method."}, {"heading": "Acknowledgement", "text": "This work is supported by the National Grand Fundamental Research 973 Program of China (# 2012CB316303,2014CB340401), the National Natural Science Foundation of China (# 61402442, 61572469, 61572473, 61303244), the Beijing Nova Program (# Z121101002512063) and the Beijing Natural Science Foundation (# 4154086)."}], "references": [{"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Bollacker"], "venue": "In Proceedings of the 2008 ACM SIGMOD international conference on Management of data,", "citeRegEx": "Bollacker,? \\Q2008\\E", "shortCiteRegEx": "Bollacker", "year": 2008}, {"title": "Learning structured embeddings of knowledge bases", "author": ["Bordes"], "venue": "In Conference on Artificial Intelligence,", "citeRegEx": "Bordes,? \\Q2011\\E", "shortCiteRegEx": "Bordes", "year": 2011}, {"title": "Joint learning of words and meaning representations for open-text semantic parsing", "author": ["Bordes"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Bordes,? \\Q2012\\E", "shortCiteRegEx": "Bordes", "year": 2012}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Bordes"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bordes,? \\Q2013\\E", "shortCiteRegEx": "Bordes", "year": 2013}, {"title": "A semantic matching energy function for learning with multi-relational data. Machine Learning 94(2):233\u2013259", "author": ["Bordes"], "venue": null, "citeRegEx": "Bordes,? \\Q2014\\E", "shortCiteRegEx": "Bordes", "year": 2014}, {"title": "V", "author": ["B.E. Boser", "I.M. Guyon", "Vapnik"], "venue": "N.", "citeRegEx": "Boser. Guyon. and Vapnik 1992", "shortCiteRegEx": null, "year": 1992}, {"title": "and Elisseeff", "author": ["O. Bousquet"], "venue": "A.", "citeRegEx": "Bousquet and Elisseeff 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "A metric learning perspective of svm: on the relation of lmnn and svm", "author": ["Do"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Do,? \\Q2012\\E", "shortCiteRegEx": "Do", "year": 2012}, {"title": "T", "author": ["M. Fan", "Q. Zhou", "Zheng"], "venue": "F.; and Grishman, R.", "citeRegEx": "Fan et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "G", "author": ["R. Jenatton", "N.L. Roux", "A. Bordes", "Obozinski"], "venue": "R.", "citeRegEx": "Jenatton et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Openkn: An open knowledge computational engine for network big data", "author": ["Jia"], "venue": "In Advances in Social Networks Analysis and Mining (ASONAM),", "citeRegEx": "Jia,? \\Q2014\\E", "shortCiteRegEx": "Jia", "year": 2014}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Lin"], "venue": "Proceedings of AAAI", "citeRegEx": "Lin,? \\Q2015\\E", "shortCiteRegEx": "Lin", "year": 2015}, {"title": "Lsdh: a hashing approach for large-scale link prediction in microblogs", "author": ["Liu"], "venue": "In Twenty-Eighth AAAI Conference on Artificial Intelligence", "citeRegEx": "Liu,? \\Q2014\\E", "shortCiteRegEx": "Liu", "year": 2014}, {"title": "G", "author": ["Miller"], "venue": "A.", "citeRegEx": "Miller 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "Factorizing yago: scalable machine learning for linked data", "author": ["Tresp Nickel", "M. Kriegel 2012] Nickel", "V. Tresp", "H.-P. Kriegel"], "venue": "In Proceedings of the 21st international conference on World Wide Web,", "citeRegEx": "Nickel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2012}, {"title": "C", "author": ["R. Socher", "D. Chen", "Manning"], "venue": "D.; and Ng, A.", "citeRegEx": "Socher et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "R", "author": ["I. Sutskever", "J.B. Tenenbaum", "Salakhutdinov"], "venue": "R.", "citeRegEx": "Sutskever. Tenenbaum. and Salakhutdinov 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Wang"], "venue": "In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Wang,? \\Q2014\\E", "shortCiteRegEx": "Wang", "year": 2014}, {"title": "L", "author": ["K.Q. Weinberger", "Saul"], "venue": "K.", "citeRegEx": "Weinberger and Saul 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "L", "author": ["K.Q. Weinberger", "Saul"], "venue": "K.", "citeRegEx": "Weinberger and Saul 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "K", "author": ["W. Wu", "H. Li", "H. Wang", "Zhu"], "venue": "Q.", "citeRegEx": "Wu et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Content-structural relation inference in knowledge base", "author": ["Jia Zhao", "Z. Wang 2014] Zhao", "Y. Jia", "Y. Wang"], "venue": "In Twenty-Eighth AAAI Conference on Artificial Intelligence", "citeRegEx": "Zhao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2014}], "referenceMentions": [], "year": 2015, "abstractText": "Knowledge graph embedding aims to represent entities and relations in a large-scale knowledge graph as elements in a continuous vector space. Existing methods, e.g., TransE and TransH, learn embedding representation by defining a global margin-based loss function over the data. However, the optimal loss function is determined during experiments whose parameters are examined among a closed set of candidates. Moreover, embeddings over two knowledge graphs with different entities and relations share the same set of candidate loss functions, ignoring the locality of both graphs. This leads to the limited performance of embedding related applications. In this paper, we propose a locally adaptive translation method for knowledge graph embedding, called TransA, to find the optimal loss function by adaptively determining its margin over different knowledge graphs. Experiments on two benchmark data sets demonstrate the superiority of the proposed method, as compared to the-state-of-the-art ones.", "creator": "LaTeX with hyperref package"}}}