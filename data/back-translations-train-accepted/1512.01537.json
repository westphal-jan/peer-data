{"id": "1512.01537", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2015", "title": "Reuse of Neural Modules for General Video Game Playing", "abstract": "A general approach to knowledge transfer is introduced in which an agent controlled by a neural network adapts how it reuses existing networks as it learns in a new domain. Networks trained for a new domain can improve their performance by routing activation selectively through previously learned neural structure, regardless of how or for what it was learned. A neuroevolution implementation of this approach is presented with application to high-dimensional sequential decision-making domains. This approach is more general than previous approaches to neural transfer for reinforcement learning. It is domain-agnostic and requires no prior assumptions about the nature of task relatedness or mappings. The method is analyzed in a stochastic version of the Arcade Learning Environment, demonstrating that it improves performance in some of the more complex Atari 2600 games, and that the success of transfer can be predicted based on a high-level characterization of game dynamics.", "histories": [["v1", "Fri, 4 Dec 2015 20:43:30 GMT  (577kb,D)", "http://arxiv.org/abs/1512.01537v1", "Accepted at AAAI 16"]], "COMMENTS": "Accepted at AAAI 16", "reviews": [], "SUBJECTS": "cs.NE cs.AI", "authors": ["alexander braylan", "mark hollenbeck", "elliot meyerson", "risto miikkulainen"], "accepted": true, "id": "1512.01537"}, "pdf": {"name": "1512.01537.pdf", "metadata": {"source": "META", "title": "Reuse of Neural Modules for General Video Game Playing", "authors": ["Alexander Braylan", "Mark Hollenbeck", "Elliot Meyerson", "Risto Miikkulainen"], "emails": ["braylan@cs.utexas.edu", "mhollen@cs.utexas.edu", "ekm@cs.utexas.edu", "risto@cs.utexas.edu"], "sections": [{"heading": "Introduction", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "Background", "text": "Transfer learning involves machine learning techniques in which existing source knowledge is reused in another target task or domain. A domain is an environment in which learning takes place, characterized by the input and output space; a task is a specific function from input to output that needs to be learned (Pan and Yang 2010). In sequential decision areas, a task is characterized by the values of sensory actions that correspond to the pursuit of a particular goal. A taxonomy of knowledge types that can be transferred was also enumerated by Pan and Yang. Since the GRUSM approach reuses the structure of existing neural networks, it falls under the function transfer category."}, {"heading": "Transfer Learning for RL", "text": "Transfer Learning for sequential decision-making domains has been extensively studied within the reinforcement learning (RL) paradigm (Taylor and Stone 2009). Reinforcement learning domains are often called Markov decision processes (MDPs), in which the state space includes all possible observations, and the probability of an observation only on the previous observation and action taken by a learning agent. However, many real RL domains are non-markovian, including many Atari 2600 games, for example, the velocity of a moving object cannot be determined by looking on a single frame. The Atari 2600 platform also supported a wide variety of games. Existing RL approaches to transfer differ on the types of differences allowed between source and target task. Some approaches that are generally limited in terms of the type of knowledge that can be transmitted are limited in that they require a consistent agent-space (Konidaris, Scheidwasser, and Barto 2012)."}, {"heading": "General Neural Structure Transfer", "text": "There are existing algorithms that resemble GRUSM in that they enable the reuse of existing neural structures, and they can be applied to a wide range of domains and tasks by automatically selecting source knowledge and avoiding intertask mappings. Thus, Shultz and Rivest (2001) developed a technique to build increasingly complex networks by inserting source networks that are selected according to the degree to which they reduce errors. This technique is applicable only to supervised learning, since source selection depends heavily on an immediate error calculation. In addition, connectivity between source and target networks is limited to the input and output layer of the source. As another example, Swarup and Ray (2006) introduced an approach that creates sparse networks of primitive or commonly used subnetworks that are derived from a library of source networks. This subgraph-mining approach depends on a computationally expensive graphing-mining algorithm that GRUSM is prone of these sources compared to larger GRUSM sources, and that GRUSM is prone of these sources for larger networks."}, {"heading": "Approach", "text": "This section presents the basic idea behind GRUSM, gives an overview of the ESP Neuroevolution Framework and describes the special implementation: GRUSM-ESP."}, {"heading": "General ReUse of Static Modules (GRUSM)", "text": "The underlying idea is that an agent who learns a neural network for a target task selectively implements knowledge from existing neural modules (source networks) while simultaneously developing a new structure for a target task. This approach attempts to balance reuse and innovation in an integrated architecture. Both source networks and new hidden nodes are incorporated into the target network as recruits in order to remain consistent. This mechanism forces the target network to transfer learned knowledge rather than simply overwrite it. Connections to and from source networks to the targets of recruitment are frozen to enable learning of connection parameters that are being recruited."}, {"heading": "Enforced Subpopulations (ESP)", "text": "Forced sub-populations (ESP; Gomez and Miikkulainen 1997; 1999) are a neuroevolutionary technology in which different components of a neural network are developed in separate subpopulations, rather than developing the entire network in a single population. ESP has proven itself in a variety of learning areas for amplification and has shown promising results in extending to POMDP environments, where the use of recurring connections is critical for memory (Gomez and Miikkulainen 1999; Gomez and Schmidhuber 2005; Schmidhuber et al. 2007). Traditionally, there is a single hidden layer in the ESP from which each neuron is developed in its own subpopulation. Recombination occurs only between members of the same subpopulation, and mutants in a subpopulation originate only from members of that subpopulation. The genome of each individual in a subpopulation is a vector of weights corresponding to the weights of the connections to and from that neuron, including the fitness of each one of the networks being randomly assessed by a subnode bias."}, {"heading": "GRUSM-ESP", "text": "The idea of forced subpopulations is extended to the transfer of learning via GRUSM networks. For each reused source network Si, the transmission links in T between Si and M develop in a particular subpopulation. At the same time, M can be added to new hidden nodes; they develop within their own subpopulations in the manner of standard ESP. Thus, the integrated evolutionary process simultaneously searches the space for how each potential source network can be reused and how to be innovative with each new node. GRUSMESP architecture (Figure 1) consists of the following elements: (1) A pool of potential source networks. In the experiments in this paper, each target network uses at most one source at a time; (2) transmission genomes encoding the weights of cross-network connections between source and target network. Each potential source network in the pool has its own subpopulations for the development of transfer genomes between it and the target network."}, {"heading": "Experiments", "text": "In fact, it is as if most people are able to understand themselves and what they are doing. (...) It is not as if they understand the world. (...) It is not as if they understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world."}, {"heading": "Discussion and Future Work", "text": "The results show that problem solving is a problem that is primarily due to the development of strategies and strategies."}, {"heading": "Conclusion", "text": "In a stochastic version of the general video game platform Atari 2600, a specific implementation developed in this paper as the GRUSM-ESP can stimulate learning through the reuse of neural structures in different domains.The success of the transfer has been shown to correlate with intuitive notions of domain complexity.These results point to the potential of general neural reuse to predictably support agents in increasingly complex environments.We would like to thank Ruohan Zhang for useful feedback, which has been supported in part by NSF funding DBI-0939454, NIH funding R01-GM105042, and an NSA-sponsored NPSC fellowship."}], "references": [{"title": "Autonomous cross-domain knowledge transfer in lifelong policy gradient reinforcement learning", "author": ["H.B. Ammar", "E. Eaton", "J.M. Luna", "P. Ruvolo"], "venue": "Proc. of IJCAI.", "citeRegEx": "Ammar et al\\.,? 2015a", "shortCiteRegEx": "Ammar et al\\.", "year": 2015}, {"title": "Unsupervised cross-domain transfer in policy gradient reinforcement learning via manifold alignment", "author": ["H.B. Ammar", "E. Eaton", "P. Ruvolo", "M.E. Taylor"], "venue": "Proc. of AAAI.", "citeRegEx": "Ammar et al\\.,? 2015b", "shortCiteRegEx": "Ammar et al\\.", "year": 2015}, {"title": "Neural reuse: A fundamental organizational principle of the brain", "author": ["M.L. Anderson"], "venue": "Behavioral and Brain Sciences 33:245\u2013266.", "citeRegEx": "Anderson,? 2010", "shortCiteRegEx": "Anderson", "year": 2010}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "JAIR 47:253\u2013279.", "citeRegEx": "Bellemare et al\\.,? 2013", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Frame skip is a powerful parameter for learning to play atari", "author": ["A. Braylan", "M. Hollenbeck", "E. Meyerson", "R. Miikkulainen"], "venue": "Workshops at AAAI-15.", "citeRegEx": "Braylan et al\\.,? 2015a", "shortCiteRegEx": "Braylan et al\\.", "year": 2015}, {"title": "On the cross-domain reusability of neural modules for general video game playing", "author": ["A. Braylan", "M. Hollenbeck", "E. Meyerson", "R. Miikkulainen"], "venue": "IJCAI-15 GIGA Workshop.", "citeRegEx": "Braylan et al\\.,? 2015b", "shortCiteRegEx": "Braylan et al\\.", "year": 2015}, {"title": "Policy transfer using reward shaping", "author": ["T. Brys", "A. Harutyunyan", "M.E. Taylor", "A. Now\u00e9"], "venue": "Proc. of AAMAS 181\u2013188.", "citeRegEx": "Brys et al\\.,? 2015", "shortCiteRegEx": "Brys et al\\.", "year": 2015}, {"title": "Incremental evolution of complex general behavior", "author": ["F.J. Gomez", "R. Miikkulainen"], "venue": "Adaptive Behavior 5(3-4):317\u2013342.", "citeRegEx": "Gomez and Miikkulainen,? 1997", "shortCiteRegEx": "Gomez and Miikkulainen", "year": 1997}, {"title": "Solving non-markovian control tasks with neuroevolution", "author": ["F.J. Gomez", "R. Miikkulainen"], "venue": "Proc. of IJCAI 1356\u20131361.", "citeRegEx": "Gomez and Miikkulainen,? 1999", "shortCiteRegEx": "Gomez and Miikkulainen", "year": 1999}, {"title": "Co-evolving recurrent neurons learn deep memory pomdps", "author": ["F.J. Gomez", "J. Schmidhuber"], "venue": "Proc. of GECCO 491\u2013498.", "citeRegEx": "Gomez and Schmidhuber,? 2005", "shortCiteRegEx": "Gomez and Schmidhuber", "year": 2005}, {"title": "Robust non-linear control through neuroevolution", "author": ["F.J. Gomez"], "venue": "Technical report, UT Austin.", "citeRegEx": "Gomez,? 2003", "shortCiteRegEx": "Gomez", "year": 2003}, {"title": "The impact of determinism on learning atari 2600 games", "author": ["M. Hausknecht", "P. Stone"], "venue": "Workshops at AAAI-15.", "citeRegEx": "Hausknecht and Stone,? 2015", "shortCiteRegEx": "Hausknecht and Stone", "year": 2015}, {"title": "A neuroevolution approach to general atari game playing", "author": ["M. Hausknecht", "J. Lehman", "R. Miikkulainen", "P. Stone"], "venue": "IEEE Trans. on Comp. Intelligence in AI in Games 6(4):355\u2013366.", "citeRegEx": "Hausknecht et al\\.,? 2013", "shortCiteRegEx": "Hausknecht et al\\.", "year": 2013}, {"title": "Co-evolutionary modular neural networks for automatic problem decomposition", "author": ["V.R. Khare", "X. Yao", "B. Sendhoff", "Y. Jin", "H. Wersing"], "venue": "Proc. of CEC 2691\u20132698.", "citeRegEx": "Khare et al\\.,? 2005", "shortCiteRegEx": "Khare et al\\.", "year": 2005}, {"title": "Transfer in reinforcement learning via shared features", "author": ["G. Konidaris", "I. Scheidwasser", "A.G. Barto"], "venue": "JMLR 13(1):1333\u2013 1371.", "citeRegEx": "Konidaris et al\\.,? 2012", "shortCiteRegEx": "Konidaris et al\\.", "year": 2012}, {"title": "Evolving deep unsupervised convolutional networks for vision-based reinforcement learning", "author": ["J. Koutn\u00edk", "J. Schmidhuber", "F.J. Gomez"], "venue": "Proc. of GECCO 541\u2013548.", "citeRegEx": "Koutn\u00edk et al\\.,? 2014", "shortCiteRegEx": "Koutn\u00edk et al\\.", "year": 2014}, {"title": "Reservoir computing approaches to recurrent neural network training", "author": ["M. Luko\u0161evi\u010dius", "H. Jaeger"], "venue": "Computer Science Review 3(3):127\u2013149.", "citeRegEx": "Luko\u0161evi\u010dius and Jaeger,? 2009", "shortCiteRegEx": "Luko\u0161evi\u010dius and Jaeger", "year": 2009}, {"title": "Network motifs: Simple building blocks of complex networks", "author": ["R. Milo", "S. Shen-Orr", "S. Itzkovitz", "N. Kashtan", "D. Chklovskii", "U. Alon"], "venue": "Science 298(5594):824\u2013827.", "citeRegEx": "Milo et al\\.,? 2002", "shortCiteRegEx": "Milo et al\\.", "year": 2002}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M. G Bellemare"], "venue": "Nature", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Trans. on Knowledge and Data Engineering 22(10):1345\u20131359.", "citeRegEx": "Pan and Yang,? 2010", "shortCiteRegEx": "Pan and Yang", "year": 2010}, {"title": "The 2014 general video game playing competition", "author": ["D. Perez", "S. Samothrakis", "J. Togelius", "T Schaul"], "venue": "IEEE Trans. on Comp. Intel. and AI in Games (99)", "citeRegEx": "Perez et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Perez et al\\.", "year": 2015}, {"title": "Evolving reusable neural modules", "author": ["J. Reisinger", "K.O. Stanley", "R. Miikkulainen"], "venue": "Proc. of GECCO 69\u201381.", "citeRegEx": "Reisinger et al\\.,? 2004", "shortCiteRegEx": "Reisinger et al\\.", "year": 2004}, {"title": "A video game description language for modelbased or interactive learning", "author": ["T. Schaul"], "venue": "Proc. of CIG 1\u20138.", "citeRegEx": "Schaul,? 2013", "shortCiteRegEx": "Schaul", "year": 2013}, {"title": "Training recurrent networks by evolino", "author": ["J. Schmidhuber", "D. Wierstra", "M. Gagliolo", "F.J. Gomez"], "venue": "Neural Computation 19(3):757\u2013779.", "citeRegEx": "Schmidhuber et al\\.,? 2007", "shortCiteRegEx": "Schmidhuber et al\\.", "year": 2007}, {"title": "Knowledge-based cascadecorrelation: Using knowledge to speed learning", "author": ["T.R. Shultz", "F. Rivest"], "venue": "Connection Science 13(1):43\u201372.", "citeRegEx": "Shultz and Rivest,? 2001", "shortCiteRegEx": "Shultz and Rivest", "year": 2001}, {"title": "Cross-domain knowledge transfer using structured representations", "author": ["S. Swarup", "S.R. Ray"], "venue": "Proc. of AAAI 506\u2013511.", "citeRegEx": "Swarup and Ray,? 2006", "shortCiteRegEx": "Swarup and Ray", "year": 2006}, {"title": "An experts algorithm for transfer learning", "author": ["E. Talvitie", "S. Singh"], "venue": "Proc. of IJCAI 1065\u20131070.", "citeRegEx": "Talvitie and Singh,? 2007", "shortCiteRegEx": "Talvitie and Singh", "year": 2007}, {"title": "Transfer learning for reinforcement learning domains: A survey", "author": ["M.E. Taylor", "P. Stone"], "venue": "JMLR 1633\u20131685.", "citeRegEx": "Taylor and Stone,? 2009", "shortCiteRegEx": "Taylor and Stone", "year": 2009}, {"title": "Autonomous transfer for reinforcement learning", "author": ["M.E. Taylor", "G. Kuhlmann", "P. Stone"], "venue": "Proc. of AAMAS 283\u2013290.", "citeRegEx": "Taylor et al\\.,? 2008", "shortCiteRegEx": "Taylor et al\\.", "year": 2008}, {"title": "Transfer via intertask mappings in policy search reinforcement learning", "author": ["M.E. Taylor", "S. Whiteson", "P. Stone"], "venue": "Proc. of AAMAS 156\u2013163.", "citeRegEx": "Taylor et al\\.,? 2007", "shortCiteRegEx": "Taylor et al\\.", "year": 2007}, {"title": "Evolving static representations for task transfer", "author": ["P. Verbancsics", "K.O. Stanley"], "venue": "JMLR 11:1737\u20131769.", "citeRegEx": "Verbancsics and Stanley,? 2010", "shortCiteRegEx": "Verbancsics and Stanley", "year": 2010}], "referenceMentions": [{"referenceID": 27, "context": "In long-range sequential control domains, such as robotics and video game-playing, transfer is particularly important, because previous experience can help agents explore new environments efficiently (Taylor and Stone 2009; Konidaris, Scheidwasser, and Barto 2012).", "startOffset": 200, "endOffset": 264}, {"referenceID": 26, "context": "has been provided with this additional relational knowledge, or, if it can be learned (Talvitie and Singh 2007; Taylor, Kuhlmann, and Stone 2008; Ammar et al. 2015b), cases where task mappings are useful.", "startOffset": 86, "endOffset": 165}, {"referenceID": 1, "context": "has been provided with this additional relational knowledge, or, if it can be learned (Talvitie and Singh 2007; Taylor, Kuhlmann, and Stone 2008; Ammar et al. 2015b), cases where task mappings are useful.", "startOffset": 86, "endOffset": 165}, {"referenceID": 17, "context": "This approach is motivated by studies that have shown in both naturally occurring complex networks (Milo et al. 2002) and in artificial neural networks (Swarup and Ray 2006) that certain network structures repeat and can be useful across domains, without any context for how exactly this structure should be used.", "startOffset": 99, "endOffset": 117}, {"referenceID": 25, "context": "2002) and in artificial neural networks (Swarup and Ray 2006) that certain network structures repeat and can be useful across domains, without any context for how exactly this structure should be used.", "startOffset": 40, "endOffset": 61}, {"referenceID": 2, "context": "This work is further motivated by the idea that neural resources in the human brain are reused for countless purposes in varying complex ways (Anderson 2010).", "startOffset": 142, "endOffset": 157}, {"referenceID": 7, "context": "In this paper, an implementation of GRUSM based on the Enforced Subpopulations (ESP) neuroevolution framework (Gomez and Miikkulainen 1997; 1999) is presented.", "startOffset": 110, "endOffset": 145}, {"referenceID": 19, "context": "A domain is an environment in which learning takes place, characterized by the input and output space; a task is a particular function from input to output to be learned (Pan and Yang 2010).", "startOffset": 170, "endOffset": 189}, {"referenceID": 27, "context": "Transfer learning for sequential decision-making domains has been studied extensively within the reinforcement learning (RL) paradigm (Taylor and Stone 2009).", "startOffset": 134, "endOffset": 157}, {"referenceID": 6, "context": "Some approaches that are general with respect to the kind of knowledge that can be transferred are restricted in that they require a consistent agent-space (Konidaris, Scheidwasser, and Barto 2012), or an a priori specification of inter-task mappings defining relationships between source and target state and action variables (Brys et al. 2015).", "startOffset": 327, "endOffset": 345}, {"referenceID": 30, "context": "Existing approaches to transfer learning that encode policies as neural networks require such a specification (Taylor, Whiteson, and Stone 2007; Verbancsics and Stanley 2010).", "startOffset": 110, "endOffset": 174}, {"referenceID": 13, "context": "On the other hand, existing modular neuroevolution approaches that are more general with respect to connectivity (Reisinger, Stanley, and Miikkulainen 2004; Khare et al. 2005) have not been applied to cross-domain transfer.", "startOffset": 113, "endOffset": 175}, {"referenceID": 26, "context": "These approaches are general enough to apply to any reinforcement learning domains, but initial approaches (Taylor, Kuhlmann, and Stone 2008; Talvitie and Singh 2007) were intractable for high dimensional state and action spaces due to combinatorial blowup in the number of possible mappings.", "startOffset": 107, "endOffset": 166}, {"referenceID": 1, "context": "However, recent approaches in policy gradient RL (Ammar et al. 2015b; 2015a) can both tractably learn mappings and be applied across diverse domains.", "startOffset": 49, "endOffset": 76}, {"referenceID": 24, "context": "For example, Shultz and Rivest (2001) developed a technique to build increasingly complex networks by inserting source networks chosen by how much they reduce error.", "startOffset": 13, "endOffset": 38}, {"referenceID": 24, "context": "For example, Shultz and Rivest (2001) developed a technique to build increasingly complex networks by inserting source networks chosen by how much they reduce error. This technique is only applicable to supervised learning, because the source selection depends heavily on an immediate error calculation. Also, connectivity between source and target networks is limited to the input and output layer of the source. As another example, Swarup and Ray (2006) introduced an approach that creates sparse networks out of primitives, or commonly used sub-networks, mined from a library of source networks.", "startOffset": 13, "endOffset": 456}, {"referenceID": 17, "context": "This has been previously observed in naturally occurring complex networks (Milo et al. 2002), as well as cross-domain artificial neural networks (Swarup and Ray 2006).", "startOffset": 74, "endOffset": 92}, {"referenceID": 25, "context": "2002), as well as cross-domain artificial neural networks (Swarup and Ray 2006).", "startOffset": 58, "endOffset": 79}, {"referenceID": 25, "context": "Unlike the subgraph-mining approach to neural structure transfer (Swarup and Ray 2006), this general formalism makes no assumptions as to what subnetworks actually will be useful.", "startOffset": 65, "endOffset": 86}, {"referenceID": 16, "context": "Furthermore, advances in reservoir computing (Luko\u0161evi\u010dius and Jaeger 2009) have demonstrated the power of using large amounts of frozen neural structure to facilitate learning of complex and chaotic tasks.", "startOffset": 45, "endOffset": 75}, {"referenceID": 7, "context": "Enforced Sub-Populations (ESP; Gomez and Miikkulainen 1997; 1999) is a neuroevolution technique in which different components of a neural network are evolved in separate subpopulations rather than evolving the whole network in a single population.", "startOffset": 25, "endOffset": 65}, {"referenceID": 8, "context": "ESP has been shown to perform well in a variety of reinforcement learning domains, and has shown promise in extending to POMDP environments, in which use of recurrent connections for memory is critical (Gomez and Miikkulainen 1999; Gomez and Schmidhuber 2005; Schmidhuber et al. 2007).", "startOffset": 202, "endOffset": 284}, {"referenceID": 9, "context": "ESP has been shown to perform well in a variety of reinforcement learning domains, and has shown promise in extending to POMDP environments, in which use of recurrent connections for memory is critical (Gomez and Miikkulainen 1999; Gomez and Schmidhuber 2005; Schmidhuber et al. 2007).", "startOffset": 202, "endOffset": 284}, {"referenceID": 23, "context": "ESP has been shown to perform well in a variety of reinforcement learning domains, and has shown promise in extending to POMDP environments, in which use of recurrent connections for memory is critical (Gomez and Miikkulainen 1999; Gomez and Schmidhuber 2005; Schmidhuber et al. 2007).", "startOffset": 202, "endOffset": 284}, {"referenceID": 10, "context": ", no improvements were made since the previous burst phase, a new neuron with a new subpopulation may be added (Gomez 2003).", "startOffset": 111, "endOffset": 123}, {"referenceID": 8, "context": "The details of the genetic algorithm in our implementation used to evolve each subpopulation mirror those described by Gomez (2003). This algorithm has been shown to work well within the ESP framework, though any suitable evolutionary algorithm could potentially be substituted in its place.", "startOffset": 119, "endOffset": 132}, {"referenceID": 4, "context": "(Preliminary experiments using this approach were discussed in Braylan et al. (2015b).)", "startOffset": 63, "endOffset": 86}, {"referenceID": 3, "context": "GRUSM-ESP was evaluated in a stochastic version of the Atari 2600 general video game-playing platform using the Arcade Learning Environment simulator (ALE; Bellemare et al. 2013).", "startOffset": 150, "endOffset": 178}, {"referenceID": 3, "context": "GRUSM-ESP was evaluated in a stochastic version of the Atari 2600 general video game-playing platform using the Arcade Learning Environment simulator (ALE; Bellemare et al. 2013). Atari 2600 is currently a very popular platform, because it challenges modern approaches, contains non-markovian games, and entertained a generation of human video game players, who would regularly reuse knowledge gained from previous games when playing new games. To make the simulator more closely resemble the human game-playing experience, the -repeat action approach as suggested by Hausknecht and Stone (2015) is used in this paper to make the environment stochastic; in this manner, like human players, the algorithm cannot as easily find loopholes in the deterministic nature of the simulator.", "startOffset": 156, "endOffset": 596}, {"referenceID": 12, "context": "Neuroevolution techniques are competitive in the Atari 2600 platform (Hausknecht et al. 2013), and ESP in particular has yielded state-of-the-art performance for several games (Braylan et al.", "startOffset": 69, "endOffset": 93}, {"referenceID": 4, "context": "2013), and ESP in particular has yielded state-of-the-art performance for several games (Braylan et al. 2015a).", "startOffset": 88, "endOffset": 110}, {"referenceID": 11, "context": "The input layer consisted of a series of object representations manually generated as previously described by Hausknecht et al. (2013). The location of each object on the screen was represented in an 8 \u00d7 10 input substrate corresponding to the object\u2019s class.", "startOffset": 110, "endOffset": 135}, {"referenceID": 10, "context": ", via convolutional networks as was done by Koutn\u00edk, Schmidhuber, and Gomez (2014).", "startOffset": 70, "endOffset": 83}, {"referenceID": 27, "context": "Negative transfer is a serious concern for many practitioners (Taylor and Stone 2009; Pan and Yang 2010).", "startOffset": 62, "endOffset": 104}, {"referenceID": 19, "context": "Negative transfer is a serious concern for many practitioners (Taylor and Stone 2009; Pan and Yang 2010).", "startOffset": 62, "endOffset": 104}, {"referenceID": 3, "context": "The assignment of features (1), (2) and (3) is completely defined based on game interface (Bellemare et al. 2013).", "startOffset": 90, "endOffset": 113}, {"referenceID": 18, "context": "Only Space Invaders and Seaquest were deemed to require long-term planning (Mnih et al. 2015), since the long-range dynamics of these games penalize reflexive strategies, and as such, agents in these games can perform well with a low frequency decision-making (Braylan et al.", "startOffset": 75, "endOffset": 93}, {"referenceID": 4, "context": "2015), since the long-range dynamics of these games penalize reflexive strategies, and as such, agents in these games can perform well with a low frequency decision-making (Braylan et al. 2015a).", "startOffset": 172, "endOffset": 194}, {"referenceID": 27, "context": "Thus, the analysis is focused on a broad notion of transfer effectiveness (TE), which aggregates metrics such as jumpstart and max overall score, with a weighted approximation of area under the curve (Taylor and Stone 2009).", "startOffset": 200, "endOffset": 223}, {"referenceID": 18, "context": "We also show human and DQN scores (Mnih et al. 2015).", "startOffset": 34, "endOffset": 52}, {"referenceID": 18, "context": "Also, although it is difficult to compare to the deterministic Atari 2600 domain, Table 1 provides a comparison of GRUSM-ESP to recent results in that domain for context (Mnih et al. 2015).", "startOffset": 170, "endOffset": 188}, {"referenceID": 2, "context": "The effectiveness of transfer in complex games aligns with the common-sense notion of hierarchical knowledge representation, as argued previously in transfer learning (Konidaris, Scheidwasser, and Barto 2012) as well as in biology (Anderson 2010; Milo et al. 2002).", "startOffset": 231, "endOffset": 264}, {"referenceID": 17, "context": "The effectiveness of transfer in complex games aligns with the common-sense notion of hierarchical knowledge representation, as argued previously in transfer learning (Konidaris, Scheidwasser, and Barto 2012) as well as in biology (Anderson 2010; Milo et al. 2002).", "startOffset": 231, "endOffset": 264}, {"referenceID": 20, "context": "It will be interesting to investigate whether the same principles extend to other general video game playing platforms, such as VGDL (Perez et al. 2015; Schaul 2013).", "startOffset": 133, "endOffset": 165}, {"referenceID": 22, "context": "It will be interesting to investigate whether the same principles extend to other general video game playing platforms, such as VGDL (Perez et al. 2015; Schaul 2013).", "startOffset": 133, "endOffset": 165}, {"referenceID": 23, "context": ", as by Schmidhuber et al. (2007), when deep memory is a primary concern.", "startOffset": 8, "endOffset": 34}], "year": 2015, "abstractText": "A general approach to knowledge transfer is introduced in which an agent controlled by a neural network adapts how it reuses existing networks as it learns in a new domain. Networks trained for a new domain can improve their performance by routing activation selectively through previously learned neural structure, regardless of how or for what it was learned. A neuroevolution implementation of this approach is presented with application to high-dimensional sequential decision-making domains. This approach is more general than previous approaches to neural transfer for reinforcement learning. It is domain-agnostic and requires no prior assumptions about the nature of task relatedness or mappings. The method is analyzed in a stochastic version of the Arcade Learning Environment, demonstrating that it improves performance in some of the more complex Atari 2600 games, and that the success of transfer can be predicted based on a high-level characterization of game dynamics.", "creator": "TeX"}}}