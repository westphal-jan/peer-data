{"id": "1512.02009", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Dec-2015", "title": "Jointly Modeling Topics and Intents with Global Order Structure", "abstract": "Modeling document structure is of great importance for discourse analysis and related applications. The goal of this research is to capture the document intent structure by modeling documents as a mixture of topic words and rhetorical words. While the topics are relatively unchanged through one document, the rhetorical functions of sentences usually change following certain orders in discourse. We propose GMM-LDA, a topic modeling based Bayesian unsupervised model, to analyze the document intent structure cooperated with order information. Our model is flexible that has the ability to combine the annotations and do supervised learning. Additionally, entropic regularization can be introduced to model the significant divergence between topics and intents. We perform experiments in both unsupervised and supervised settings, results show the superiority of our model over several state-of-the-art baselines.", "histories": [["v1", "Mon, 7 Dec 2015 12:16:58 GMT  (264kb,D)", "http://arxiv.org/abs/1512.02009v1", "Accepted by AAAI 2016"]], "COMMENTS": "Accepted by AAAI 2016", "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG", "authors": ["bei chen", "jun zhu", "nan yang", "tian tian 0001", "ming zhou", "bo zhang"], "accepted": true, "id": "1512.02009"}, "pdf": {"name": "1512.02009.pdf", "metadata": {"source": "CRF", "title": "Jointly Modeling Topics and Intents with Global Order Structure", "authors": ["Bei Chen", "Jun Zhu", "Nan Yang", "Tian Tian", "Ming Zhou", "Bo Zhang"], "emails": ["{chenbei12@mails.,", "dcszj@,", "tiant13@mails.,", "dcszb@}tsinghua.edu.cn;", "mingzhou}@microsoft.com"], "sections": [{"heading": "Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "Unsupervised GMM-LDA Model", "text": "We look at the following document structure learning problem. We are given a corpus D = {sd} Dd = 1 with D documents, where a document sd is a sequence of Nd sentences denoted by sd = (wd1, wd2,..., wdNd) and a sentence wds is a bag of Nds words denoted by wds = {wdsm} Ndsm = 1. The size of the vocabulary is V. We build our models on the following assumptions, 1) Type: Each word in the corpus is either an intentional word or a deliberate word; 2) Order: The intentions of sentences within a document vary according to certain arrangements and the arrangements are similar within a domain; and 3) Coherence: The same word in the corpus is either a deliberate word or a deliberate word; 2) Order: The intentions of sentences within a document vary, and the arrangements are similar within a domain."}, {"heading": "GMM-Multi Prior for Intent Ordering", "text": "To fulfill this assumption, we present a GMMMulti approach on possible procedural permutations. GMM-Multi, motivated by (Chen et al. 2009), is an extension of the generalized Mallows model (Fligner and Verducci 1986). It focuses the probability mass on a small number of similar approaches around a canonical order permutation-0, which confirms to intuition that the attachment orders within a range are similar. The inversion of the approach is applicable: If we specify the canonical sequence mutation-0 as identity permutation-0 (1, 2, K), then each permutation element-1 can be designated as (K \u2212 1) -dimensional vector-1."}, {"heading": "Generative Process of GMM-LDA", "text": "Now we present GMM-LDA, an unattended Bayesian generative model as shown in Fig. 2. GMM-LDA simultaneously models the topics (the blue part in Fig. 2) and the intentions (the red part in Fig. 2). The binary variable bdsm denotes the type of the word wdsm: if bdsm = 1, then wdsm is a topic; if bdsm = 0, then wdsm is an intentional topic. Each sentence has a specific intentional designation zds, while each document sd has a topic that confuses the distribution intentions. For topic words, there is a document-specific theme model. It is a hierarchical Bayesian model that positions each document as an admixture of T-themes, with each topic being a multinomial intentional designation."}, {"heading": "Collapsed Gibbs Sampling", "text": "Leave M = {u, D \u2212 sd, b} all the variables to be learned during the training (= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "Supervised GMM-LDA", "text": "We are not able to solve the problem without human comment. However, as suggested by existing parent topic models (McAuliffe and Lead 2008; Wang, Lead and Li 2009; Zhu, Ahmed and Xing 2012), the predictive power can be greatly improved with a small amount of labeled documents. Here, we present a parent GMM-LDA that combines the known intentions of the sentences during learning. We consider the setting in which a portion of the documents in the corpus are labeled, i.e., each sentence is labeled with a statement of intent. Our goal is to develop a parent model to learn the intentions for the remaining unlabeled documents. The easiest way to use the labeling information is right during learning, rather than trying it out. However, the GMM describes the intent structure in a global way that complicates the process."}, {"heading": "GMM-LDA with Entropic Regularization", "text": "GMM-LDA collectively models the two incompatible structures of documents using a binary variable to specify the type (intention or subject) of each word. It may happen that the same word located in two different positions is assigned with different types, which is somewhat unreasonable. To model the significant divergence between topics and content, we introduce the entropy of words to make our model more descriptive. As we know, the entropy of a discrete random variable X = {x1, x2} Cultural knowledge between topics and content, the entropy of words to make our model more descriptive. As we know, the entropy of a discrete random variable X = xn} Cultural intropy of the word has been explicitly written as H (X)."}, {"heading": "Experimental Results", "text": "We use two real datasets: 1) Chemical (Guo et al. 2010): It contains 965 abstracts of scientific work on 5 types of chemicals, and each abstract focuses on one of the 5 topics. Each sentence contains one of the 7 intention labels: background, target, related work, method, result, conclusion, and future work; and 2) Elements (Chen et al. 2009): It consists of 118 labels from the Labels Wikipedia, and each article talks about one of the 118 chemical elements in the periodic system. Each paragraph is provided with an intention label. We take the 8 most common intention labels: high-level segment, History, Labels, Labels, Labels, Applications, Occurrence, Notable Characteristics, Compounds, Sentral Labels, Sentral Labels, Sentral Labels-Labels, Sentral-Labels-Sentral-Labels, Sentral-Labels-Labels, Sentral-Labels-Labels-Labels-Labels-Labels-Labels-Labels-Labels-Applications, Occurrence, Labels-Segment, Labels-Characteristics-Labels-Compounds, Sentral-Sentral-Labels-Sentral-Sentral-Labels-Sentral-Sentral-Sentral-Sentral-Sentral-Labels-Sentral-Sentral-Sentral-Labels-Sentral-Sentral-Labels-Sentral-Sentral-Sentral-Sentral-Sentral-Labels-Sentral-Sentral-Sentral-Labels-Sentral-Sentral-Sentral-Sentral-Sentral-Sentral-Sentral-Labels-Sentral-Labels-Sentral-Labels-Labels-Sentral-Sentral-Sentral-Sentral-Labels-Labels-Labels-Labels"}, {"heading": "Unsupervised Clustering", "text": "Our goal of uncontrolled clustering is to learn a memorandum of understanding for each set in the corpus. Uncontrolled clustering is a method we use for each set in the corpus. < K = 125 The uncontrolled clustering method is used for each set without real label information. < K = 125 The appropriateness of the margins index (ARI) (Vinh, Epps and Bailey 2010), callback, precision and F-score are used as our intentions. However, F-score is the harmonious means of recall and precision. Higher values are better for all four measures. We look at two variants of our model: 1 = 0.1, 0 = 0.1, \u03b20 = 0.1, because we find that the results are insensitive to them."}, {"heading": "Supervised Classification", "text": "We evaluate our monitored models for classifying sentences. For each set of data, we randomly select 20% documents; comment on their sentences with intent labels and use them for training. Our goal is to learn the intent labels for the sentences in the remaining 80% documents. We report on the accuracy (ACC) and ARI values to show the intentions in comparison to unattended learning. We look again at two variants of our model: 1) sGMM-LDA: Our monitored model; and 2) sEGMM-LDA: the intentions with entropic regulation. The basic methods are: 1) SVM: We use the bag labels, the linear kernel and SVMLight tools (1998). 2) sBoilerplate-LDA: The monitored version of Boilerplate-LDA: \"Elements,\" in which we repair the known labels during learning rather than updating them."}, {"heading": "Related Work", "text": "From an algorithmic point of view, our work is based on thematic models such as the latent dirichlet allocation (LDA) (lead, Ng and Jordan 2003), which has been developed for many NLP tasks. Instead of presenting documents as word bags, many advanced models take into account specific structural constraints (Purver et al. 2006; Gruber, Wei\u00df and Rosen-Zvi 2007). Among different models, our work has a closer relationship to the models with the order structure, but in order modelling, the Markov chain can only capture dependence locally (O'Se \u0301 aghdha and Teufel 2014; Barzilay and Lee 2004; Elsner, Austerweil and Charniak 2007), while the generalized Mallows model (GMM) (Fligner and Verducci 1986) has a global view (Chen et al. 2009; Du, Pate and Johnson 2015; Cheng, Hu \u00bc hn and Hu \u00bc llermeier 2009)."}, {"heading": "Conclusion and Future Work", "text": "We present GMM-LDA (both unsupervised and supervised) for learning document structures that simultaneously model themes and intentions; the generalized Mallows model is used to model the MOU globally; we also look at entropic regularization to make the model more illustrative; our results demonstrate the reasonableness of our intuitions and the effectiveness of our models; for future work, we are interested in enriching our models by combining local coherence limitations.Recognition This work was primarily done when the first author was an intern at Microsoft Research Asia; the work was supported by the National Basic Research Program (973 Program) of China (Nos. 2013CB329403, 2012CB316301), National NSF of China (Nos. 61322308, 61332007), TNList Big Data Initiative and Tsinghua Initiative Scientific Research Program (Nos. 201210810471, 2018042007)."}], "references": [{"title": "Catching the drift: Probabilistic content models, with applications to generation and summarization", "author": ["Barzilay", "R. Lee 2004] Barzilay", "L. Lee"], "venue": null, "citeRegEx": "Barzilay et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Barzilay et al\\.", "year": 2004}, {"title": "Content modeling using latent permutations", "author": ["Chen"], "venue": null, "citeRegEx": "Chen,? \\Q2009\\E", "shortCiteRegEx": "Chen", "year": 2009}, {"title": "Decision tree and instance-based learning for label ranking", "author": ["H\u00fchn Cheng", "W. H\u00fcllermeier 2009] Cheng", "J. H\u00fchn", "E. H\u00fcllermeier"], "venue": null, "citeRegEx": "Cheng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2009}, {"title": "Topic segmentation with an ordering-based topic model", "author": ["Pate Du", "L. Johnson 2015] Du", "J. Pate", "M. Johnson"], "venue": null, "citeRegEx": "Du et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Du et al\\.", "year": 2015}, {"title": "A unified local and global model for discourse coherence", "author": ["Austerweil Elsner", "M. Charniak 2007] Elsner", "J. Austerweil", "E. Charniak"], "venue": null, "citeRegEx": "Elsner et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Elsner et al\\.", "year": 2007}, {"title": "Distance based ranking models. JRSS. Series B (Methodological) 359\u2013369", "author": ["Fligner", "M. Verducci 1986] Fligner", "J. Verducci"], "venue": null, "citeRegEx": "Fligner et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Fligner et al\\.", "year": 1986}, {"title": "A hierarchical bayesian model for unsupervised induction of script knowledge", "author": ["Titov Frermann", "L. Pinkal 2014] Frermann", "I. Titov", "M. Pinkal"], "venue": null, "citeRegEx": "Frermann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Frermann et al\\.", "year": 2014}, {"title": "Hidden topic markov models", "author": ["Weiss Gruber", "A. Rosen-Zvi 2007] Gruber", "Y. Weiss", "M. Rosen-Zvi"], "venue": "In AISTATS", "citeRegEx": "Gruber et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gruber et al\\.", "year": 2007}, {"title": "Identifying the information structure of scientific abstracts: an investigation of three different schemes", "author": ["Guo"], "venue": "In Workshop on Biomedical Natural Language Processing", "citeRegEx": "Guo,? \\Q2010\\E", "shortCiteRegEx": "Guo", "year": 2010}, {"title": "Cohesion in english", "author": ["Halliday", "M. Hasan 1976] Halliday", "R. Hasan"], "venue": null, "citeRegEx": "Halliday et al\\.,? \\Q1976\\E", "shortCiteRegEx": "Halliday et al\\.", "year": 1976}, {"title": "Discourse indicators for content selection in summarization", "author": ["Joshi Louis", "A. Nenkova 2010] Louis", "A. Joshi", "A. Nenkova"], "venue": "In Special Interest Group on Discourse and Dialogue,", "citeRegEx": "Louis et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Louis et al\\.", "year": 2010}, {"title": "Rhetorical structure theory: Toward a functional theory of text organization. Text-Interdisciplinary Journal for the Study of Discourse 8(3):243\u2013281", "author": ["Mann", "W. Thompson 1988] Mann", "S. Thompson"], "venue": null, "citeRegEx": "Mann et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Mann et al\\.", "year": 1988}, {"title": "Supervised topic models", "author": ["McAuliffe", "J. Blei 2008] McAuliffe", "D. Blei"], "venue": "In NIPS", "citeRegEx": "McAuliffe et al\\.,? \\Q2008\\E", "shortCiteRegEx": "McAuliffe et al\\.", "year": 2008}, {"title": "Topic modeling based sentiment analysis on social media for stock market prediction", "author": ["Nguyen", "T. Shirai 2015] Nguyen", "K. Shirai"], "venue": null, "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Unsupervised learning of rhetorical structure with un-topic models", "author": ["\u00d3 S\u00e9aghdha", "D. Teufel 2014] \u00d3 S\u00e9aghdha", "S. Teufel"], "venue": "In COLING", "citeRegEx": "S\u00e9aghdha et al\\.,? \\Q2014\\E", "shortCiteRegEx": "S\u00e9aghdha et al\\.", "year": 2014}, {"title": "The penn discourse treebank as a resource for natural language generation", "author": ["Prasad"], "venue": "In CL Workshop on Using Corpora for NLG", "citeRegEx": "Prasad,? \\Q2005\\E", "shortCiteRegEx": "Prasad", "year": 2005}, {"title": "Unsupervised topic modelling for multi-party spoken discourse", "author": ["Purver"], "venue": null, "citeRegEx": "Purver,? \\Q2006\\E", "shortCiteRegEx": "Purver", "year": 2006}, {"title": "Unsupervised document zone identification using probabilistic graphical models", "author": ["Preotiuc-Pietro Varga", "A. Ciravegna 2012] Varga", "D. PreotiucPietro", "F. Ciravegna"], "venue": null, "citeRegEx": "Varga et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Varga et al\\.", "year": 2012}, {"title": "Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance", "author": ["Epps Vinh", "N. Bailey 2010] Vinh", "J. Epps", "J. Bailey"], "venue": null, "citeRegEx": "Vinh et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vinh et al\\.", "year": 2010}, {"title": "Simultaneous image classification and annotation", "author": ["Blei Wang", "C. Li 2009] Wang", "D. Blei", "F. Li"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2009}, {"title": "Medlda: Maximum margin supervised topic models", "author": ["Ahmed Zhu", "J. Xing 2012] Zhu", "A. Ahmed", "E. Xing"], "venue": null, "citeRegEx": "Zhu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2012}, {"title": "Bayesian inference with posterior regularization and applications to infinite latent svms", "author": ["Chen Zhu", "J. Xing 2014] Zhu", "N. Chen", "E. Xing"], "venue": null, "citeRegEx": "Zhu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2014}], "referenceMentions": [], "year": 2015, "abstractText": "Modeling document structure is of great importance for discourse analysis and related applications. The goal of this research is to capture the document intent structure by modeling documents as a mixture of topic words and rhetorical words. While the topics are relatively unchanged through one document, the rhetorical functions of sentences usually change following certain orders in discourse. We propose GMM-LDA, a topic modeling based Bayesian unsupervised model, to analyze the document intent structure cooperated with order information. Our model is flexible that has the ability to combine the annotations and do supervised learning. Additionally, entropic regularization can be introduced to model the significant divergence between topics and intents. We perform experiments in both unsupervised and supervised settings, results show the superiority of our model over several state-of-the-art baselines.", "creator": "LaTeX with hyperref package"}}}