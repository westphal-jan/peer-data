{"id": "1512.02181", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Dec-2015", "title": "The Teaching Dimension of Linear Learners", "abstract": "Teaching dimension is a learning theoretic quantity that specifies the minimum training set size to teach a target model to a learner. Previous studies on teaching dimension focused on version-space learners which maintain all hypotheses consistent with the training data, and cannot be applied to modern machine learners which select a specific hypothesis via optimization. This paper presents the first known teaching dimension for ridge regression, support vector machines, and logistic regression. We also exhibit optimal training sets that match these teaching dimensions. Our approach generalizes to other linear learners.", "histories": [["v1", "Mon, 7 Dec 2015 19:24:55 GMT  (21kb)", "http://arxiv.org/abs/1512.02181v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ji liu", "xiaojin zhu 0001", "hrag ohannessian"], "accepted": true, "id": "1512.02181"}, "pdf": {"name": "1512.02181.pdf", "metadata": {"source": "CRF", "title": "The Teaching Dimension of Linear Learners", "authors": ["Ji Liu"], "emails": ["jliu@cs.rochester.edu,", "jerryzhu@cs.wisc.edu"], "sections": [{"heading": null, "text": "ar Xiv: 151 2.02 181v 1 [cs.L G] 7D ec"}, {"heading": "1 Introduction", "text": "We consider a teacher who knows both a target model and the learning algorithm used by a machine learner when we want to teach the target model to the learner. (D = 1) The teacher wants to teach the target model to the learner by building a training unit. (D = 1) The teacher can construct any element in the input space. (D = 1) How many training elements are needed? This is the question addressed by the teaching dimension. (D) We give the exact definition in section 2, but first we illustrate the intuition with an example. (D) Let us consider integrators x [1] and the threshold on which we rely so that the learning effect (x) returns when x < and 1 when we keep the hypotheses space H = eleven classifiers H = {h.1} We let the learner maintain a version of the learning space, namely maintain it."}, {"heading": "2 Classic Teaching Dimension and its Limitations", "text": "A hypothesis is a function h: X \u2192 Y. In this thesis, we identify a hypothesis h\u03b8 with its model parameter \u03b8.. (xn, yn)} Hypothesis space H is a series of hypotheses. More importantly, for the purpose of teaching, we do not assume that D is drawn from a distribution i.i.d. Let D specify the set of all learning objectives of all magnitudes. A learning algorithm A: D \u2192 2H takes D in a teaching set and returns a subset of the hypotheses space H. That is, A does not necessarily return a unique hypothesis. Classical teaching dimension analysis is limited to the version-space learner Avs: Avs (D) = {h-H teaching space H. That is, A does not necessarily return a unique hypothesis. Classical teaching dimension analysis is limited to the version-space-learner Avs: Avs (D) = h-teaching space with the teaching space H."}, {"heading": "3 Main Results", "text": "In order to make the dependence of the teaching dimension on the learning algorithm explicit, we write back the teaching dimension with two arguments asTD (h \u043a, A) (4), where h * H is the target model, and A: D \u2192 2H the learning algorithm, which gives a learning set D * D, a series of hypotheses A (D). We define the teaching dimension as the size of the smallest learning set D, so that A (D) = {h *}, the singleton that contains the target model. With this notation, the classic teaching dimension is TD (h *, Avs), where Avs is the size of the version space learning algorithm (2). In this work, instead, we focus on Aopt in (1), namely linear learners in Rd. Linear learners include many popular members, such as both homogeneous members and the learning dimension TD (without abia's term) and inhomogeneous (with a bias) versions of SVression and regression."}, {"heading": "3.1 Lower Bounds on Teaching Dimension TD(\u03b8\u2217,Aopt)", "text": "In this section, we offer three general lower limits based on the teaching dimension. These lower limits cover various aspects of a theorem and should be used in conjunction (i.e., we take the maximum) if applicable. We will instantiate these lower limits for certain learners in Section 3.2. However, below, we will allow X and Y to be the viable region of all xi's and yi's respectively. We will use the notation 1 (\u00b7, \u00b7) in the following way: If it is smooth, then it denotes a singleton group containing only the gradients. w.r.t. the first argument; if (\u00b7, \u00b7) convex, then it denses the subdifferential w.r.t the first argumentation.LB1 comes from a perspective of the degree of freedom. It is necessary to have this set of educational elements for a unique solution in order to exist in (1).Theorem 1."}, {"heading": "3.2 The Teaching Dimension TD(\u03b8\u2217,Aopt) of Three Homogeneous Learners", "text": "To prove that we do indeed have a curriculum for a goal, we must show that this is indeed the case for our constructed curricula. To determine the size of such a curriculum, an upper limit is set for the teaching dimension that we will apply in this section; its derivation will be shown whether such a limit corresponds to the corresponding lower limit. We select three learners to study their teaching dimension based on these learning outcomes: ridge regression, SVM, and logistic regression. It turns out that homogeneous and inhomogeneous versions of these learners require different analyses."}, {"heading": "3.3 The Teaching Dimension TD(\u03b8\u2217,Aopt) of Three Inhomogeneous Learners", "text": "Inhomogeneous learners are defined by \u03b8 = [w; b], with the weight vector w \u0435Rd and the scalar offset b-R. The offset b is not regulated. Similar to the previous section, we must instantiate the lower limits of the teaching dimension and design the theories. We show that the size of our teaching staff corresponds exactly to the lower limit of the inhomogeneous crest regression and differs from the lower limit of the inhomogeneous SVM and logistic regression by rounding at most one. Therefore, until rounding, we also define the teaching dimension for these inhomogeneous learners. Inhomogeneous crest regression solves the problem: min w, Rd, b, b, Rn, i = 11 2 (x i w + b \u2212 yi) 2 + \u03bb 2, w, 2 (31) Proposition 4. For any target model [w, c, b], if w, Rd, b, can be an arbitrary value, the following value is given for year 31:"}, {"heading": "If w\u2217 6= 0, any n = 2 items satisfying the following are a teaching set for a 6= 0:", "text": "We can check if the KT condition is fulfilled by using x1 and y1 as in (32): (x 1 w + b): (x 1 w + b): (x 1 w + b): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): x (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x): (x: (x): (x): (x): (x: (x): (x): (x: (x): (x): (x: (x): (x): (x): (x: (x): (x): (x): (x: (x): (x): (x): (x): (x): (x: (x): (x): (x): (x: (x): (x: (x): (x): (x): (x): (x): (x): (x): (x): (x): (x: (x): (x) (x): (x): (x): (x) ("}, {"heading": "4 Teaching a Decision Boundary Instead of a Parameter", "text": "In Section 3, we considered the teaching objective, where the learner must learn the exact target parameter. > In this section, we will consider this teaching objective. However, if the learner is a classifier, a weaker teaching objective is often sufficient, namely teaching the learner at a target decision boundary. In the case of inhomogeneous linear learners, the linear decision boundary {x | x \u00b2 w + b = 0} is defined by the parameters that produce the target decision boundary. The curriculum succeeds if the learner reaches any parameter within this boundary. In the case of inhomogeneous linear learners, the linear decision boundary {x | x \u00b2 w + b = 0} is set with the parameter {t [w; b \u00b2]: t > 0}. Here, we assume that w \u00b2 is null. The parameter ollinhomogeneous linear learner is only a representative member of the group. Homogeneous learners are similar without b \u00b2."}, {"heading": "5 Related Work", "text": "In fact, it is that we are able to assert ourselves, that we are able to change the world, and that we are able to change the world in order to change the world. \""}, {"heading": "6 Conclusion", "text": "To our knowledge, our teaching dimension for comb regression, SVM and logistic regression is new; the same applies to the lower limits and our analytical technique in general. There are many possible extensions to the present work. For example, we can extend our analysis to non-linear learners, which can potentially be achieved by applying the kernel trick to linear learners. Another example is \"approximate teaching\" by loosening the teaching target so that lessons are considered successful when the learner reaches a model close enough to the target model. Overall, the present work and its extensions are expected to enrich our understanding of optimal teaching and enable new applications."}], "references": [{"title": "Data poisoning attacks against autoregressive models", "author": ["S. Alfeld", "X. Zhu", "P. Barford"], "venue": "AAAI,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Queries revisited", "author": ["D. Angluin"], "venue": "Theoretical Computer Science, 313(2):175\u2013194,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Teachers, learners and black boxes", "author": ["D. Angluin", "M. Krikis"], "venue": "COLT,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning from different teachers", "author": ["D. Angluin", "M. Krikis"], "venue": "Machine Learning, 51(2):137\u2013163,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Measuring teachability using variants of the teaching dimension", "author": ["F.J. Balbach"], "venue": "Theor. Comput. Sci., 397(1-3):94\u2013113, May", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Teaching randomized learners", "author": ["F.J. Balbach", "T. Zeugmann"], "venue": "COLT, pages 229\u2013243,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Recent developments in algorithmic teaching", "author": ["F.J. Balbach", "T. Zeugmann"], "venue": "Proceedings of the 3rd International Conference on Language and Automata Theory and Applications, pages 1\u201318,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "The security of machine learning", "author": ["M. Barreno", "B. Nelson", "A.D. Joseph", "J.D. Tygar"], "venue": "Machine Learning Journal, 81(2):121\u2013148,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Self-directed learning and its relation to the VC-dimension and to teacher-directed learning", "author": ["S. Ben-David", "N. Eiron"], "venue": "Machine Learning, 33(1):87\u2013104,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1998}, {"title": "Mixed-initiative active learning", "author": ["M. Cakmak", "A. Thomaz"], "venue": "ICML Workshop on Combining Learning Strategies to Reduce Label Cost,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "On the LambertW function", "author": ["R. Corless", "G. Gonnet", "D. Hare", "D. Jeffrey", "D. Knuth"], "venue": "Advances in Computational Mathematics, 5(1):329\u2013359,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1996}, {"title": "Recursive teaching dimension, VC-dimension and sample compression", "author": ["T. Doliwa", "G. Fan", "H.U. Simon", "S. Zilles"], "venue": "Journal of Machine Learning Research, 15:3107\u20133131,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "On the complexity of teaching", "author": ["S. Goldman", "M. Kearns"], "venue": "Journal of Computer and Systems Sciences, 50(1):20\u201331,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1995}, {"title": "Teaching a smarter learner", "author": ["S. Goldman", "H. Mathias"], "venue": "Journal of Computer and Systems Sciences, 52(2):255267,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1996}, {"title": "A theory of formal synthesis via inductive learning", "author": ["S. Jha", "S.A. Seshia"], "venue": "CoRR, abs/1505.03953,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "How do humans teach: On curriculum learning and teaching dimension", "author": ["F. Khan", "X. Zhu", "B. Mutlu"], "venue": "NIPS,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Complexity of teaching by a restricted number of examples", "author": ["H. Kobayashi", "A. Shinohara"], "venue": "COLT, pages 293\u2013302,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "A model of interactive teaching", "author": ["H.D. Mathias"], "venue": "J. Comput. Syst. Sci., 54(3):487\u2013501,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1997}, {"title": "The security of latent Dirichlet allocation", "author": ["S. Mei", "X. Zhu"], "venue": "AISTATS,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Using machine teaching to identify optimal training-set attacks on machine learners", "author": ["S. Mei", "X. Zhu"], "venue": "AAAI,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimal teaching for limited-capacity human learners", "author": ["K. Patil", "X. Zhu", "L. Kopec", "B. Love"], "venue": "NIPS,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Being taught can be faster than asking questions", "author": ["R.L. Rivest", "Y.L. Yin"], "venue": "COLT,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1995}, {"title": "Teachability in computational learning", "author": ["A. Shinohara", "S. Miyano"], "venue": "New Generation Computing, 8(4):337\u2013348,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1991}, {"title": "Machine teaching for Bayesian learners in the exponential family", "author": ["X. Zhu"], "venue": "NIPS,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Machine teaching: an inverse problem to machine learning and an approach toward optimal education", "author": ["X. Zhu"], "venue": "AAAI,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Models of cooperative teaching and learning", "author": ["S. Zilles", "S. Lange", "R. Holte", "M. Zinkevich"], "venue": "Journal of Machine Learning Research, 12:349\u2013384,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 12, "context": "How many training items are needed? This is the question addressed by the teaching dimension [13, 23].", "startOffset": 93, "endOffset": 101}, {"referenceID": 22, "context": "How many training items are needed? This is the question addressed by the teaching dimension [13, 23].", "startOffset": 93, "endOffset": 101}, {"referenceID": 11, "context": "[12]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "For any such \u03b4 , simple algebra verifies that \u03b8\u2217+ t\u03b4 satisfies the KKT condition (7) for any t \u2208 [0, 1].", "startOffset": 97, "endOffset": 103}, {"referenceID": 0, "context": "\u2022 If the loss function l(\u00b7, \u00b7) is smooth (not necessary convex) in the first argument, we have f(\u03b8\u2217) = f(\u03b8\u2217 + \u03b4) by using the Taylor expansion (recall f is defined in equation 1): f(\u03b8\u2217 + \u03b4) =f(\u03b8\u2217) + \u3008\u2207f(\u03b8\u2217 + t\u03b4), \u03b4\u3009 (for some t \u2208 [0, 1]) =f(\u03b8\u2217) + \u3008 n \u2211", "startOffset": 230, "endOffset": 236}, {"referenceID": 0, "context": "I(a) = \uf8f1 \uf8f4\uf8f2 \uf8f4\uf8f3 1, if a < 1 [0, 1], if a = 1 0, otherwise .", "startOffset": 27, "endOffset": 33}, {"referenceID": 0, "context": "where the last line is due to I ( \u03bb\u2016\u03b8\u2217\u20162 \u2308\u03bb\u2016\u03b8\u2217\u20162\u2309 ) giving either the set [0, 1] or the value 1.", "startOffset": 74, "endOffset": 80}, {"referenceID": 10, "context": "However, due to the negative log likelihood term we have a coefficient defined by the Lambert W function [11], which we denote by Wlam.", "startOffset": 105, "endOffset": 109}, {"referenceID": 12, "context": "It was proposed independently in [13, 23].", "startOffset": 33, "endOffset": 41}, {"referenceID": 22, "context": "It was proposed independently in [13, 23].", "startOffset": 33, "endOffset": 41}, {"referenceID": 25, "context": "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].", "startOffset": 0, "endOffset": 45}, {"referenceID": 6, "context": "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].", "startOffset": 0, "endOffset": 45}, {"referenceID": 1, "context": "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].", "startOffset": 0, "endOffset": 45}, {"referenceID": 2, "context": "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].", "startOffset": 0, "endOffset": 45}, {"referenceID": 13, "context": "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].", "startOffset": 0, "endOffset": 45}, {"referenceID": 17, "context": "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].", "startOffset": 0, "endOffset": 45}, {"referenceID": 5, "context": "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].", "startOffset": 0, "endOffset": 45}, {"referenceID": 4, "context": "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].", "startOffset": 0, "endOffset": 45}, {"referenceID": 16, "context": "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].", "startOffset": 0, "endOffset": 45}, {"referenceID": 3, "context": "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].", "startOffset": 0, "endOffset": 45}, {"referenceID": 21, "context": "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].", "startOffset": 0, "endOffset": 45}, {"referenceID": 8, "context": "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].", "startOffset": 0, "endOffset": 45}, {"referenceID": 11, "context": "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].", "startOffset": 0, "endOffset": 45}, {"referenceID": 12, "context": "For a finite hypothesis space H, Goldman and Kearns [13] proved the relation V C(H)/ log(|H|) \u2264 TD(H) \u2264 V C(H) + |H| \u2212 2 C(H).", "startOffset": 52, "endOffset": 56}, {"referenceID": 23, "context": "For example, Zhu [24] demonstrated that to learn a 1D threshold classifier within \u01eb error, the teaching dimension is a constant TD=2 regardless of \u01eb, while active learning would require O(log 1 \u01eb ) queries which can be arbitrarily larger than TD.", "startOffset": 17, "endOffset": 21}, {"referenceID": 24, "context": "The optimal teaching set is then well-defined, and represents the best personalized lesson for the student [25, 24, 16].", "startOffset": 107, "endOffset": 119}, {"referenceID": 23, "context": "The optimal teaching set is then well-defined, and represents the best personalized lesson for the student [25, 24, 16].", "startOffset": 107, "endOffset": 119}, {"referenceID": 15, "context": "The optimal teaching set is then well-defined, and represents the best personalized lesson for the student [25, 24, 16].", "startOffset": 107, "endOffset": 119}, {"referenceID": 20, "context": "training set [21].", "startOffset": 13, "endOffset": 17}, {"referenceID": 7, "context": "In particular, optimal teaching is the mathematical formalism to study the so-called data poisoning attacks [8, 20, 19, 1].", "startOffset": 108, "endOffset": 122}, {"referenceID": 19, "context": "In particular, optimal teaching is the mathematical formalism to study the so-called data poisoning attacks [8, 20, 19, 1].", "startOffset": 108, "endOffset": 122}, {"referenceID": 18, "context": "In particular, optimal teaching is the mathematical formalism to study the so-called data poisoning attacks [8, 20, 19, 1].", "startOffset": 108, "endOffset": 122}, {"referenceID": 0, "context": "In particular, optimal teaching is the mathematical formalism to study the so-called data poisoning attacks [8, 20, 19, 1].", "startOffset": 108, "endOffset": 122}, {"referenceID": 9, "context": "Teaching dimension also has applications in interactive machine learning to quantify the minimum human interaction necessary [10], and in formal synthesis to generate computer programs satisfying a specification [15].", "startOffset": 125, "endOffset": 129}, {"referenceID": 14, "context": "Teaching dimension also has applications in interactive machine learning to quantify the minimum human interaction necessary [10], and in formal synthesis to generate computer programs satisfying a specification [15].", "startOffset": 212, "endOffset": 216}], "year": 2015, "abstractText": "Teaching dimension is a learning theoretic quantity that specifies the minimum training set size to teach a target model to a learner. Previous studies on teaching dimension focused on version-space learners which maintain all hypotheses consistent with the training data, and cannot be applied to modern machine learners which select a specific hypothesis via optimization. This paper presents the first known teaching dimension for ridge regression, support vector machines, and logistic regression. We also exhibit optimal training sets that match these teaching dimensions. Our approach generalizes to other linear learners.", "creator": "LaTeX with hyperref package"}}}