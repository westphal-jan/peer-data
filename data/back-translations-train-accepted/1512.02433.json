{"id": "1512.02433", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Dec-2015", "title": "Minimum Risk Training for Neural Machine Translation", "abstract": "We propose minimum risk training for end-to-end neural machine translation. Unlike conventional maximum likelihood estimation, minimum risk training is capable of optimizing model parameters directly with respect to evaluation metrics. Experiments on Chinese-English and English-French translation show that our approach achieves significant improvements over maximum likelihood estimation on a state-of-the-art neural machine translation system.", "histories": [["v1", "Tue, 8 Dec 2015 12:42:00 GMT  (8kb,D)", "http://arxiv.org/abs/1512.02433v1", "9 pages"], ["v2", "Wed, 9 Dec 2015 14:20:36 GMT  (8kb,D)", "http://arxiv.org/abs/1512.02433v2", "Some typos corrected; the case-sensitiveness of BLEU scores are given"], ["v3", "Wed, 15 Jun 2016 00:07:05 GMT  (103kb,D)", "http://arxiv.org/abs/1512.02433v3", "Accepted for publication in Proceedings of ACL 2016"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shiqi shen", "yong cheng", "zhongjun he", "wei he", "hua wu", "maosong sun", "yang liu 0005"], "accepted": true, "id": "1512.02433"}, "pdf": {"name": "1512.02433.pdf", "metadata": {"source": "CRF", "title": "Minimum Risk Training for Neural Machine Translation", "authors": ["Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "emails": ["liuyang2011@tsinghua.edu.cn."], "sections": [{"heading": "1 Introduction", "text": "Recently, end-to-end neural machine translation (NMT) [Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015] has attracted the increasing attention of the community. Providing a new paradigm for machine translation, NMT aims to form a single, large neural network that transforms a source-language sentence directly into a target-language sentence without explicitly modelling latent structures (e.g. word alignment, phrase segmentation, phrase rearrangement, and SCFG derivation) present in conventional statistical machine translation (MRI) [Brown et al., 1993; Koehn et al., 2003; Chiang, 2005].Current NMT models rely on the encoder decoder framework [Cho et al., 2014; Sutskever et al., 2014, to read a source code and encode]."}, {"heading": "2 Background", "text": "Given a source-language sentence x = x1,.. xm,.. xm,.. xM and a target-language sentence y = y1,.., yN, end-to-end neuronal MT, the translation probability is directly modelled: P (y | x; \u03b8) = N \u00b2 n = 1 P (yn | x, y < n; \u03b8) (1), which is a set of model parameters and y < n = y1,. (2), yn \u2212 1. Predicting the n-th target word can be done by using a recursive neural network: P (yn | x, y < n;.) when it is a set of model parameters and y < n = y1,. (2), where zn the n-th target word is a hidden state on the target page, cn is the context for generating the n-th target word, and q is a non-linear function."}, {"heading": "3 Minimum Risk Training for Neural Machine", "text": "Translation Minimal risk training (MRI) aimed at minimizing the expected loss of training data (J = J = J = Y = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z (Z = Z = Z = Z = Z = Z = Z = Z = Z (Z = Z = Z = Z).In this thesis, we introduce MRI to holistic neuronal machine translation; Z = Z = Z = Z = Z = Z (8) = Z = Z = Z = Z = Z (7) = Z = Z = Z (Z = Z = 1 Z = Y (Z) (Z) and Y (Z) (Z); Z = Z = Z = Z = Z (Z); Z = Z = Z = Z = Z (Z); Z = Z = Z = Z = Z)."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Setup", "text": "We evaluated our approach on two translation tasks: Chinese-English and English-French. The evaluation metric is case-insensitive BLEU [Papineni et al., 2002] as used by the Multi-bleu.perl script.For Chinese-English, the training data consists of 67.5M Chinese words and 74.8M English words, respectively. We used the NIST 2006 dataset as the validation set for optimizing hyper-parameters and model selection and the NIST 2002, 2004, 2005, and 2008 datasets as as as as test sets. For English-French, to compare with the results on neural machine translation et al. [Sutskever et al., 2015], we used the subset of the WMT 2014, and 2008 datasets as as as the test sets as a \"s.\""}, {"heading": "4.2 Results on Chinese-English Translation", "text": "Table 1 shows BLEU values on Sino-English datasets. We find that the introduction of minimal risk training in neural MT leads to surprisingly significant improvements over Moses and GroundHog with MLE as the training criterion (up to + 7.74 and + 6.86 BLEU points, respectively) across all test sets. All improvements are statistically significant. Table 2 shows some sample translations. We find that Moses translates a Chinese string \"yi wei fuze yu pingrang dangju da jiaodao de qian guowuyuan guanyuan\" which requires a misalignment over long distances, which is a notorious challenge for statistical machine translation. In this example, GroundHogMLE seems to lack consistency in this case thanks to the ability of gated RNNNs to detect dependencies over long distances. However, since MLE uses a loss function defined only at word level, its translation seems to be lacking consistency at the EU level \"during two sets of Chinese: before MRT occurs.\""}, {"heading": "4.3 Results on English-French Translation", "text": "They differ in network architectures and vocabulary sizes. Our GroundHog-MLE system achieves a BLEU score comparable to that of Bahdanau et al. [2015] and Jean et al. [2015]. GroundHog-MRI achieves the highest BLEU score in this environment, even with a smaller vocabulary than Luong et al. [2015] and Sutskever et al. [2014]. 1 Note that our approach does not require specific architectures and in principle can be applied to all NMT systems."}, {"heading": "5 Related Work", "text": "Our work is based on minimal risk training algorithms in conventional statistical machine translation [Och, 2003; Smith and Eisner, 2006; He and Deng, 2012]. Och [2003] describes a smoothed error count to enable the calculation of gradients, which directly inspires us to use a parameter \u03b1 to adjust the smoothness of the objective function. Smith and Eisner [2006] introduce a minimum risk glow for the formation of loglinear models, which is able to gradually focus on the 1-best hypothesis. He and Deng [2012] apply a minimum risk training to learning phrase translation probabilities. 1We have not adopted the recently introduced techniques such as the interplay of multiple model predictions [Sutskever et al, 2014], increase the vocabulary by sampling importance [Jean et al, 2015], and apply the rare word problem of translation to Luong [further improve translation performance in 2015]."}, {"heading": "6 Conclusion", "text": "In this paper, we have presented a framework for minimal risk training in end-to-end neural machine translation. Experiments show that minimal risk training leads to significant improvements over the maximum probability assessment for neural machine translation, especially for widely related languages such as Chinese and English.In the future, we plan to test our approach on more language pairs and more end-to-end neural MT systems.We believe that minimal risk training will also benefit from end-to-end neural architectures for other NLP tasks."}, {"heading": "Acknowledgements", "text": "This work was done when Shiqi Shen and Yong Cheng visited Baidu as interns. Maosong Sun and Hua Wu are supported by the 973 Program (2014CB340501 & 2014CB34505), Yang Liu is supported by the National Natural Science Foundation of China (No. 61522204 and No. 61432013), which is also supported by the Singapore National Research Foundation through its International Research Centre @ Singapore Funding Initiative and managed by the IDM Programme."}], "references": [{"title": "A hierarchical phrase-based model for statistical machine translation", "author": ["David Chiang"], "venue": "In Proceedings of ACL,", "citeRegEx": "Chiang.,? \\Q2005\\E", "shortCiteRegEx": "Chiang.", "year": 2005}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning continuous phrase representations for translation modeling", "author": ["Jianfeng Gao", "Xiaodong He", "Wen tao Yih", "Li Deng"], "venue": "In Proceedings of ACL,", "citeRegEx": "Gao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2014}, {"title": "Maximum expected bleu training of phrase and lexicon translation models", "author": ["Xiaodong He", "Li Deng"], "venue": "In Proceedings of ACL,", "citeRegEx": "He and Deng.,? \\Q2012\\E", "shortCiteRegEx": "He and Deng.", "year": 2012}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["Sebastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "In Proceedings of ACL,", "citeRegEx": "Jean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Kalchbrenner and Blunsom.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Factored translation models", "author": ["Philipp Koehn", "Hieu Hoang"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Koehn and Hoang.,? \\Q2007\\E", "shortCiteRegEx": "Koehn and Hoang.", "year": 2007}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz J. Och", "Daniel Marcu"], "venue": "In Proceedings of HLT-NAACL,", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Minh-Thang Luong", "Ilya Sutskever", "Quoc V. Le", "Oriol Vinyals", "Wojciech Zaremba"], "venue": "In Proceedings of ACL,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz J. Och"], "venue": "In Proceedings of ACL,", "citeRegEx": "Och.,? \\Q2003\\E", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of ACL,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Sequence level training with recurrent neural networks", "author": ["MarcAurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": null, "citeRegEx": "Ranzato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2015}, {"title": "Minimum risk annealing for training loglinear models", "author": ["David A. Smith", "Jason Eisner"], "venue": "In Proceedings of ACL,", "citeRegEx": "Smith and Eisner.,? \\Q2006\\E", "shortCiteRegEx": "Smith and Eisner.", "year": 2006}, {"title": "Srilm - am extensible language modeling toolkit", "author": ["Andreas Stolcke"], "venue": "In Proceedings of ICSLP,", "citeRegEx": "Stolcke.,? \\Q2002\\E", "shortCiteRegEx": "Stolcke.", "year": 2002}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J. Willams"], "venue": "Machine Learning,", "citeRegEx": "Willams.,? \\Q1992\\E", "shortCiteRegEx": "Willams.", "year": 1992}], "referenceMentions": [{"referenceID": 5, "context": "Recently, end-to-end neural machine translation (NMT) [Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015] has attracted increasing attention from the community.", "startOffset": 54, "endOffset": 133}, {"referenceID": 14, "context": "Recently, end-to-end neural machine translation (NMT) [Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015] has attracted increasing attention from the community.", "startOffset": 54, "endOffset": 133}, {"referenceID": 7, "context": ", word alignment, phrase segmentation, phrase reordering, and SCFG derivation) present in conventional statistical machine translation (SMT) [Brown et al., 1993; Koehn et al., 2003; Chiang, 2005].", "startOffset": 141, "endOffset": 195}, {"referenceID": 0, "context": ", word alignment, phrase segmentation, phrase reordering, and SCFG derivation) present in conventional statistical machine translation (SMT) [Brown et al., 1993; Koehn et al., 2003; Chiang, 2005].", "startOffset": 141, "endOffset": 195}, {"referenceID": 1, "context": "Current NMT models build on the encoder-decoder framework [Cho et al., 2014; Sutskever et al., 2014], with an encoder to read and encode a source language sentence into a vector and a decoder to generate a target-language sentence from the vector.", "startOffset": 58, "endOffset": 100}, {"referenceID": 14, "context": "Current NMT models build on the encoder-decoder framework [Cho et al., 2014; Sutskever et al., 2014], with an encoder to read and encode a source language sentence into a vector and a decoder to generate a target-language sentence from the vector.", "startOffset": 58, "endOffset": 100}, {"referenceID": 0, "context": ", 2003; Chiang, 2005]. Current NMT models build on the encoder-decoder framework [Cho et al., 2014; Sutskever et al., 2014], with an encoder to read and encode a source language sentence into a vector and a decoder to generate a target-language sentence from the vector. While early efforts encode the input into a fixedlength vector, Bahdanau et al. [2015] introduce the mechanism of attention to dynamically generate a context vector for a target word being generated.", "startOffset": 8, "endOffset": 358}, {"referenceID": 9, "context": "While MRT has been widely used in conventional SMT [Och, 2003; Smith and Eisner, 2006; He and Deng, 2012] and deep learning based MT [Gao et al.", "startOffset": 51, "endOffset": 105}, {"referenceID": 12, "context": "While MRT has been widely used in conventional SMT [Och, 2003; Smith and Eisner, 2006; He and Deng, 2012] and deep learning based MT [Gao et al.", "startOffset": 51, "endOffset": 105}, {"referenceID": 3, "context": "While MRT has been widely used in conventional SMT [Och, 2003; Smith and Eisner, 2006; He and Deng, 2012] and deep learning based MT [Gao et al.", "startOffset": 51, "endOffset": 105}, {"referenceID": 2, "context": "While MRT has been widely used in conventional SMT [Och, 2003; Smith and Eisner, 2006; He and Deng, 2012] and deep learning based MT [Gao et al., 2014], to the best of our knowledge, this work is the first effort to introduce MRT into end-to-end neural machine translation.", "startOffset": 133, "endOffset": 151}, {"referenceID": 8, "context": "Ranzato et al. [2015] indicate two drawbacks of maximum likelihood estimation (MLE) for neural machine translation: (1) the models are only exposed to the training data distribution instead of model predictions and (2) the loss function is defined at the word level instead of sentence level.", "startOffset": 0, "endOffset": 22}, {"referenceID": 14, "context": "Please refer to [Sutskever et al., 2014; Bahdanau et al., 2015] for more details.", "startOffset": 16, "endOffset": 63}, {"referenceID": 11, "context": "This is referred to as exposure bias [Ranzato et al., 2015].", "startOffset": 37, "endOffset": 59}, {"referenceID": 10, "context": "Second, MLE usually uses the cross-entropy loss to maximize the probability of the next correct word, which might hardly correlate well with evaluation metrics such as BLEU [Papineni et al., 2002].", "startOffset": 173, "endOffset": 196}, {"referenceID": 9, "context": "Minimum risk training (MRT), which aims to minimize the expected loss on the training data, has been widely used in conventional statistical machine translation [Och, 2003; Smith and Eisner, 2006; He and Deng, 2012] and deep learning based MT [Gao et al.", "startOffset": 161, "endOffset": 215}, {"referenceID": 12, "context": "Minimum risk training (MRT), which aims to minimize the expected loss on the training data, has been widely used in conventional statistical machine translation [Och, 2003; Smith and Eisner, 2006; He and Deng, 2012] and deep learning based MT [Gao et al.", "startOffset": 161, "endOffset": 215}, {"referenceID": 3, "context": "Minimum risk training (MRT), which aims to minimize the expected loss on the training data, has been widely used in conventional statistical machine translation [Och, 2003; Smith and Eisner, 2006; He and Deng, 2012] and deep learning based MT [Gao et al.", "startOffset": 161, "endOffset": 215}, {"referenceID": 2, "context": "Minimum risk training (MRT), which aims to minimize the expected loss on the training data, has been widely used in conventional statistical machine translation [Och, 2003; Smith and Eisner, 2006; He and Deng, 2012] and deep learning based MT [Gao et al., 2014].", "startOffset": 243, "endOffset": 261}, {"referenceID": 9, "context": "where S(x) \u2282 Y(x) is a subset of the full search space, Q(y|x;\u03b8, \u03b1) is a distribution defined on the subspace S(x), and \u03b1 is a hyper-parameter that controls the sharpness of the Q distribution [Och, 2003].", "startOffset": 193, "endOffset": 204}, {"referenceID": 10, "context": "The evaluation metric is case-insensitive BLEU [Papineni et al., 2002] as calculated by the multi-bleu.", "startOffset": 47, "endOffset": 70}, {"referenceID": 14, "context": "For English-French, to compare with the results reported by previous work on neural machine translation [Sutskever et al., 2014; Bahdanau et al., 2015], we used the same subset of the WMT 2014 training corpus that contains 12M sentence pairs with 348M French words and 304M English words.", "startOffset": 104, "endOffset": 151}, {"referenceID": 6, "context": "Moses [Koehn and Hoang, 2007]: a phrase-based SMT system;", "startOffset": 6, "endOffset": 29}, {"referenceID": 13, "context": "Moses uses the parallel corpus to train the phrase-based translation model and the target part to train a 4-gram language model using the SRILM toolkit [Stolcke, 2002].", "startOffset": 152, "endOffset": 167}, {"referenceID": 9, "context": "The log-linear model is trained using the minimum error rate training (MERT) algorithm [Och, 2003].", "startOffset": 87, "endOffset": 98}, {"referenceID": 6, "context": "Moses is a state-ofthe-art phrase-based statistical machine translation system [Koehn and Hoang, 2007] that uses the minimum error rate training (MERT) algorithm for training the log-linear model.", "startOffset": 79, "endOffset": 102}, {"referenceID": 4, "context": "45 Jean et al. [2015] single gated RNN with search 30K 29.", "startOffset": 3, "endOffset": 22}, {"referenceID": 4, "context": "45 Jean et al. [2015] single gated RNN with search 30K 29.97 Luong et al. [2015] single LSTM with 4 layers 40K 29.", "startOffset": 3, "endOffset": 81}, {"referenceID": 4, "context": "45 Jean et al. [2015] single gated RNN with search 30K 29.97 Luong et al. [2015] single LSTM with 4 layers 40K 29.50 Luong et al. [2015] single LSTM with 6 layers 40K 30.", "startOffset": 3, "endOffset": 137}, {"referenceID": 4, "context": "45 Jean et al. [2015] single gated RNN with search 30K 29.97 Luong et al. [2015] single LSTM with 4 layers 40K 29.50 Luong et al. [2015] single LSTM with 6 layers 40K 30.40 Sutskever et al. [2014] single LSTM with 4 layers 80K 30.", "startOffset": 3, "endOffset": 197}, {"referenceID": 4, "context": "[2015] and Jean et al. [2015]. GroundHog-MRT achieves the highest BLEU score in this setting even with a smaller vocabulary size than Luong et al.", "startOffset": 11, "endOffset": 30}, {"referenceID": 4, "context": "[2015] and Jean et al. [2015]. GroundHog-MRT achieves the highest BLEU score in this setting even with a smaller vocabulary size than Luong et al. [2015] and Sutskever et al.", "startOffset": 11, "endOffset": 154}, {"referenceID": 4, "context": "[2015] and Jean et al. [2015]. GroundHog-MRT achieves the highest BLEU score in this setting even with a smaller vocabulary size than Luong et al. [2015] and Sutskever et al. [2014]. 1 Note that our approach does not assume specific architectures and can in principle be applied to any NMT systems.", "startOffset": 11, "endOffset": 182}, {"referenceID": 9, "context": "Our work originates from the minimum risk training algorithms in conventional statistical machine translation [Och, 2003; Smith and Eisner, 2006; He and Deng, 2012].", "startOffset": 110, "endOffset": 164}, {"referenceID": 12, "context": "Our work originates from the minimum risk training algorithms in conventional statistical machine translation [Och, 2003; Smith and Eisner, 2006; He and Deng, 2012].", "startOffset": 110, "endOffset": 164}, {"referenceID": 3, "context": "Our work originates from the minimum risk training algorithms in conventional statistical machine translation [Och, 2003; Smith and Eisner, 2006; He and Deng, 2012].", "startOffset": 110, "endOffset": 164}, {"referenceID": 14, "context": "1We have not adopted the recently introduced techniques such as ensemble of multiple model predictions [Sutskever et al., 2014], increasing vocabulary by importance sampling [Jean et al.", "startOffset": 103, "endOffset": 127}, {"referenceID": 4, "context": ", 2014], increasing vocabulary by importance sampling [Jean et al., 2015], and addressing the rare word problem [Luong et al.", "startOffset": 54, "endOffset": 73}, {"referenceID": 8, "context": ", 2015], and addressing the rare word problem [Luong et al., 2015] to further improve translation performance.", "startOffset": 46, "endOffset": 66}, {"referenceID": 3, "context": "Our work originates from the minimum risk training algorithms in conventional statistical machine translation [Och, 2003; Smith and Eisner, 2006; He and Deng, 2012]. Och [2003] describes a smoothed error count to allow for calculating gradients, which directly inspires us to use a parameter \u03b1 to adjust the smoothness of the objective function.", "startOffset": 146, "endOffset": 177}, {"referenceID": 3, "context": "Our work originates from the minimum risk training algorithms in conventional statistical machine translation [Och, 2003; Smith and Eisner, 2006; He and Deng, 2012]. Och [2003] describes a smoothed error count to allow for calculating gradients, which directly inspires us to use a parameter \u03b1 to adjust the smoothness of the objective function. Smith and Eisner [2006] introduce minimum risk annealing for training log-linear models that is capable of gradually annealing to focus on the 1-best hypothesis.", "startOffset": 146, "endOffset": 370}, {"referenceID": 3, "context": "Our work originates from the minimum risk training algorithms in conventional statistical machine translation [Och, 2003; Smith and Eisner, 2006; He and Deng, 2012]. Och [2003] describes a smoothed error count to allow for calculating gradients, which directly inspires us to use a parameter \u03b1 to adjust the smoothness of the objective function. Smith and Eisner [2006] introduce minimum risk annealing for training log-linear models that is capable of gradually annealing to focus on the 1-best hypothesis. He and Deng [2012] apply minimum risk training to learning phrase translation probabilities.", "startOffset": 146, "endOffset": 527}, {"referenceID": 11, "context": "The Mixed Incremental Cross-Entropy Reinforce (MIXER) algorithm [Ranzato et al., 2015] is in spirit most close to our work.", "startOffset": 64, "endOffset": 86}, {"referenceID": 2, "context": "More recently, Gao et al. [2014] use MRT for learning continuous phrase representations for statistical machine translation.", "startOffset": 15, "endOffset": 33}, {"referenceID": 2, "context": "More recently, Gao et al. [2014] use MRT for learning continuous phrase representations for statistical machine translation. The difference is that they use MRT to optimize a sub-model of statistical machine translation while we are interested in directly optimizing end-to-end neural translation models. The Mixed Incremental Cross-Entropy Reinforce (MIXER) algorithm [Ranzato et al., 2015] is in spirit most close to our work. Building on the REINFORCE algorithm proposed by Willams [1992], MIXER allows for incremental learning and the use of hybrid loss function that combines both REINFORCE and cross-entropy.", "startOffset": 15, "endOffset": 492}, {"referenceID": 2, "context": "More recently, Gao et al. [2014] use MRT for learning continuous phrase representations for statistical machine translation. The difference is that they use MRT to optimize a sub-model of statistical machine translation while we are interested in directly optimizing end-to-end neural translation models. The Mixed Incremental Cross-Entropy Reinforce (MIXER) algorithm [Ranzato et al., 2015] is in spirit most close to our work. Building on the REINFORCE algorithm proposed by Willams [1992], MIXER allows for incremental learning and the use of hybrid loss function that combines both REINFORCE and cross-entropy. The major difference is that Ranzato et al. [2015] leverage reinforcement learning while our work resorts to minimum risk training.", "startOffset": 15, "endOffset": 666}], "year": 2017, "abstractText": "We propose minimum risk training for end-to-end neural machine translation. Unlike conventional maximum likelihood estimation, minimum risk training is capable of optimizing model parameters directly with respect to evaluation metrics. Experiments on Chinese-English and EnglishFrench translation show that our approach achieves significant improvements over maximum likelihood estimation on a state-of-the-art neural machine translation system.", "creator": "LaTeX with hyperref package"}}}