{"id": "1512.02595", "review": {"conference": "icml", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Dec-2015", "title": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin", "abstract": "We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.", "histories": [["v1", "Tue, 8 Dec 2015 19:13:50 GMT  (871kb,D)", "http://arxiv.org/abs/1512.02595v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["dario amodei", "rishita anubhai", "eric battenberg", "carl case", "jared casper", "bryan catanzaro", "jingdong chen", "mike chrzanowski", "adam coates", "greg diamos", "erich elsen", "jesse engel", "linxi fan", "christopher fougner", "tony han", "awni hannun", "billy jun", "patrick legresley", "libby lin", "sharan narang", "rew ng", "sherjil ozair", "ryan prenger", "jonathan raiman", "sanjeev satheesh", "david seetapun", "shubho sengupta", "yi wang", "zhiqian wang", "chong wang", "bo xiao", "dani yogatama", "jun zhan", "zhenyao zhu"], "accepted": true, "id": "1512.02595"}, "pdf": {"name": "1512.02595.pdf", "metadata": {"source": "CRF", "title": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin", "authors": ["Dario Amodei", "Rishita Anubhai", "Eric Battenberg", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Jingdong Chen", "Mike Chrzanowski", "Adam Coates", "Greg Diamos", "Erich Elsen", "Jesse Engel", "Linxi Fan", "Christopher Fougner", "Tony Han", "Awni Hannun", "Billy Jun", "Patrick LeGresley", "Libby Lin", "Sharan Narang", "Andrew Ng", "Sherjil Ozair", "Ryan Prenger", "Jonathan Raiman", "Sanjeev Satheesh", "David Seetapun", "Shubho Sengupta", "Yi Wang", "Zhiqian Wang", "Chong Wang", "Bo Xiao", "Dani Yogatama", "Jun Zhan", "Zhenyao Zhu"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "2 Related Work", "text": "This year, the time has come for it to be a purely reactionary project, capable of retaliating."}, {"heading": "3 Model Architecture", "text": "A simple multi-layer model with a single recursive layer cannot yield thousands of hours of written language. To learn from such large data sets, we increase model capacity by depth. We study architectures with up to 11 layers, including many bi-directional recurring layers and convective layers. These models have almost eight times as much computation per data sample as the models in Deep Speech 1, which makes rapid optimization and computation critical. To successfully optimize these models, we use batch normalization for RNNs and a new optimization curriculum called Sort Degree. In addition, we use long steps between the RNN inputs to reduce the computation per example by a factor of 3. This is helpful for both training and evaluation, although some modifications are needed to work well with CTC. Finally, although many of our research results use bi-directional recurrent layers, we find that excellent models only add 40 times to the performance of unidirectional models - making them much more recursive."}, {"heading": "3.1 Preliminaries", "text": "The architecture of the DS2 system, which at its core is similar to the previous DS1 system (26): a recurrent neural network (RNN) that specializes in speech spectra and can generate text transcriptions. - Each utterance, x (i), is a time series of length T (i) in which each time loop is a vector of audio characteristics, x (i) t, t = 0, T (i) \u2212 1. We use a spectrogram of the power of standardized audio clips as features of the system, so that x (i) t denotes the power of the p'th frequency in the audio frame."}, {"heading": "3.2 Batch Normalization for Deep RNNs", "text": "In fact, it is not the case that one sees oneself in a position to embark on the search for a solution, as they have in recent years in the USA, in Europe, in Europe, in the USA, in Europe, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in Europe, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA"}, {"heading": "3.3 SortaGrad", "text": "In fact, the majority of people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight"}, {"heading": "3.4 Comparison of simple RNNs and GRUs", "text": "The models we have shown so far are simple RNNs that have modeled bidirectional recurrent layers with the same results as the recurrent layers in time and backwards in time. Recent research in speech and language processing has shown that a more complex repetition of the network can lead to more time steps, while being able to provide more computing power when many other variations exist. The latest comprehensive study of thousands of variations of LSTM and GRU architectures shows that a gated recurrent unit (Gated Recurrent Units) is capable of restituting itself."}, {"heading": "3.5 Frequency Convolutions", "text": "This type of convolution was first proposed for neural networks in speech more than 25 years ago [67]. Many neural network speech models have a first layer that processes input-rate audio [16, 66]. The DS1 system achieves this by using an input and temporary convolution in the first layer with a step parameter that calculates the number of recursive neural networks."}, {"heading": "3.6 Striding", "text": "In the Convolutionary Layers, we use a longer step and broader context to speed up the training, as fewer time steps are required to model a given utterance. In our Mandarin models, we use incremental (through FFT and Convolutionary Step Speed), which reduces the number of time steps and arithmetic steps required in the following layers, and the number of characters in the English language per time step is high enough to cause problems with Striding2. To overcome this, we can enrich the English alphabet with symbols that represent alternative names such as whole words, syllables, or non-overlapping nouns. In practice, we do not use overlapping bi-graphs or bigrams, as they are easy to construct, and there are few alternatives that overlap an entire sentence for space."}, {"heading": "3.7 Row Convolution and Unidirectional Models", "text": "x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "3.8 Language Model", "text": "We train our RNN models on millions of unique expressions, enabling the network to learn a powerful implicit language model Q. Our best models are pretty skillful at spelling, with no external linguistic limitations. In addition, we find many instances in our development datasets where our models can implicitly disprove homophones - for example, \"he expects the Japanese agent to sell it for two hundred and seventy-five thousand dollars.\" However, the labeled training data is small compared to the size of the unlabeled text corpora that is available. Thus, we find that WHO improves when we add a language model trained from external text to our system. We use an n-gram language model, since it scales well to large amounts of unlabeled text [26]. For English, our language model is a Kneser-Ney smoothed 5-gram model trained with the KenLM toolkit. [28] On Common Recycle 3 positional text."}, {"heading": "3.9 Adaptation to Mandarin", "text": "The techniques we have described so far can be used to build an end-to-end Mandarin speech recognition system that outputs Chinese characters directly, eliminating the need to construct a pronunciation model that is often a fairly complicated component for porting language systems to other languages [59]. The only architectural changes we make to our networks are the characteristics of the Chinese character set. First, the output layer of the network prints about 6,000 characters, including the Roman alphabet, as hybrid Chinese-English transcripts are common. We suffer a vocabulary error in the evaluation when a character is not included in this sentence. This is not a big problem, as our test set contains only 0.74% of vocabulary."}, {"heading": "4 System Optimizations", "text": "Because our ability to evaluate hypotheses about our data and models depends on the ability to train models quickly, we have developed a highly optimized training system consisting of two main components - a deep learning library written in C + + and a high-performance linear algebra library written in both CUDA and C + +. Our optimized software, which runs on dense computing nodes with 8 titanium X GPUs per node, allows us to maintain 24 precise teraFLOP / second, equivalent to 45% of each node's theoretical top computing throughput, when building a single model on a node. We can also scale to multiple nodes, as described in the next subsection."}, {"heading": "4.1 Scalability and Data-Parallelism", "text": "In fact, most of them are able to decide for themselves what they want."}, {"heading": "4.2 GPU implementation of CTC loss function", "text": "Calculating the CTC loss function is more complicated than performing the forward and backward propagation on our RNN architectures. Originally, we transferred activations from the GPUs to the CPU, where we calculated the loss function using an OpenMP parallelized implementation of CTC. However, this implementation severely limited our scalability for two reasons: First, it became more important in terms of computing, as we improved the efficiency and scalability of the RNN itself. Second, transferring large activation matrices between the CPU and GPU required us to spend bandwidth on networking the CTC instead of transferring gradient matrices in order to scale the use of data parallelism to more processors. To overcome this, we wrote a GPU implementation of the CTC loss function. Our parallel implementation relies on a slight adjustment of the CTC bandwidth, as well as on unifying the use of the CTC bandwidth in the PTC implementation."}, {"heading": "4.3 Memory allocation", "text": "Our system makes frequent use of dynamic memory allocations to GPU and CPU memory, mainly to store activation data for variable length allocation and for intermediate results. Individual allocations can be very large; over 1GB for the longest allocations. This is because both cudaMalloc and std: malloc have introduced very large memory allocations to the operating system or the GPU driver in order to update the system side tables - over a twofold slowdown in the use of std: malloc in some cases. This is because both cudaMalloc and std: malloc forward very large memory allocations to the operating system to update the system side tables. This is a good optimization for systems that run multiple memory resources, but editing page tables is a pure overhead for our system where nodes are completely dedicated to the operation of a single model."}, {"heading": "5 Training Data", "text": "Extensive deep learning systems require a wealth of marked training data. We have collected a comprehensive training data set for both English and Mandarin models, and supplement our training with publicly available data sets. In English, we use 11,940 hours of marked language data with 8 million utterances summarized in Table 9. In Mandarin, we use 9,400 hours of marked audio with 11 million utterances. Mandarin language data consists of internal Baidu corporations that represent a mixture of read language and spontaneous language, both in standard Mandarin and accented Mandarin."}, {"heading": "5.1 Dataset Construction", "text": "The length of these clips ranged from a few minutes to more than an hour, making it impossible to unroll them in time in the RNN during training. To solve this problem, we developed pipeline alignment, segmentation and filtering that can create a training scenario with shorter pronouncements and few incorrect transcriptions. The first step in the pipeline is to use an existing bi-directional RNN model that is trained with CTC to make the transcription to the frames of the audio transcripts. The first step in the pipeline is to align the transcriptions to the transcriptions."}, {"heading": "5.2 Data Augmentation", "text": "We augment our training data by adding noise to increase the effective size of our training data and improve our robustness to loud speech. [26] Although the training data contains some intrinsic noise, we can increase the quantity and variety of noise by magnification. Excessive magnification of noise tends to make optimization difficult and can lead to worse results, and insufficient magnification of noise makes the system less robust to low signal-to-noise speech. We find that a good balance is to add noise to 40% of randomly selected expressions. The noise source consists of several thousand hours of randomly selected audio clips that are combined to produce hundreds of hours of noise."}, {"heading": "5.3 Scaling Data", "text": "Our English and Mandarin corpora are significantly larger than those commonly described in the language recognition literature. In Table 10, we show the effects of increasing the amount of labeled training data on WHO by randomly sampling the entire data set prior to training. For each data set, the model was trained for up to 20 epochs, although it was usually stopped early due to the error on a pre-set development set. We note that the WHO decreases with a performance law for both the regular and the noisy development sets. WHO decreases by approximately 40% relative to each factor in the tenfold increase in training data. We also observe a consistent gap in WHO (approximately 60% relative) between the regular and loud data sets, which implies that both cases benefit equally from more data. This implies that a language system with more labeled training data will continue to improve. We expect that increasing the raw number of hours is as important as increasing the number of language contexts recorded in the data sets."}, {"heading": "6 Results", "text": "In order to better assess the applicability of our language system in the real world, we evaluate using a wide range of test sets. We use several publicly available benchmarks and several internally collected test sets. Together, these test sets represent a wide range of demanding language environments, including low signal-to-noise ratios (loud and wide-angle), accented, read, spontaneous, and linguistic speaking. All models are used for 20 epochs using either the complete English data set described in Table 9, or the full Mandarin data set described in Section 5. We use stochastic gradient pedigree with Nesterov dynamics [61] along with a minibatch of 512 expressions. If the gradient standard exceeds a threshold of 400, it is reduced to 400 [47]. The model that performs best on a reserved development group during the training is selected for evaluation. The learning rate is selected from [1 \u00d7 4, 4 \u00d7 1.2, 4 \u00d7 1.2, and to achieve a faster consistency."}, {"heading": "6.1 English", "text": "The best DS2 model has 11 layers with 3 layers of 2D folding, 7 bidirectional recurring layers, a fully connected output layer along with batch normalization. The first layer prints bigrams with a time step of 3. In comparison, the DS1 model has 5 layers with a single bidirectional recurring layer, and there are unigrams with a time step of 2 in the first layer. To put the performance of our system into context, we name most of our results against human workers, as speech recognition is an audio perception and speech understanding problem where humans excel. We get a measure of human level performance by paying workers from Amazon Mechanical Turk to hand transcribe all of our test sets."}, {"heading": "6.1.1 Model Size", "text": "In order to obtain the best generalization error, we expect the model size to increase in order to fully exploit the patterns in the data. In Section 3.2, we will examine the effects of the model depth while specifying the number of parameters. In contrast, we will show here the effects of different model sizes on the performance of the language system. We will only vary the size of each layer while maintaining a constant depth and other architectural parameters. We will evaluate the models using the same regular and loud development sets that we use in Section 3.5. The models in Table 11 differ from those in Table 3 by increasing the step to 3 and the output to Bigrams. As we increase the model size to up to 100 million parameters, we will find that an increase in the increment in this model is necessary to achieve fast calculation and memory limitations."}, {"heading": "6.1.2 Read Speech", "text": "Reading high-signal-to-noise speech is probably the simplest large vocabulary for a continuous speech recognition task. We measure our system against two Wall Street Journal (WSJ) test sets from read news articles available in the LDC catalog as LDC94S13B and LDC93S6B. We also use the recently developed LibriSpeech corpus constructed from audio books from the LibriVox project [46]. Table 13 shows that the DS2 system outperforms humans in 3 out of 4 test sets and is competitive in the fourth. Given this result, we suspect that there is little room for a generic language system to further improve the clean language read without further adjusting the domain."}, {"heading": "6.1.3 Accented Speech", "text": "Our source for accented speaking is the publicly available VoxForge dataset (http: / / www.voxforge.org), in which speakers with many different accents read clean language. We group these accents into four categories; the American-Canadian and Indian groups are self-explanatory; the Commonwealth accent refers to speakers with British, Irish, South African, Australian and New Zealand accents; the European group includes speakers with accents from countries in Europe that do not have English as their native language; and we construct a test set of 1024 examples from each accent group for a total of 4096 examples from the VoxForge data. Performance in these test sets is, to some extent, a measure of the breadth and quality of our training data. Table 14 shows that our performance with all accents has improved if we incorporate more accented training data and use an architecture that can effectively train on this data."}, {"heading": "6.1.4 Noisy Speech", "text": "We test our performance using publicly available test kits from the recently completed third CHiME challenge [4]. This data set includes 1320 statements from the WSJ test set that are read in various noisy environments, including a bus, cafe, street, and pedestrian area. The CHiME data set also includes 1320 statements with simulated noise from the same environment, as well as the control set that contains the same statements made by the same speakers in a noise-free environment. Differences between the results on the control set and the noisy sets provide a measure of the network's ability to handle a variety of real and synthetic noise conditions. CHiME audio has 6 channels, and the use of all of them can result in significant performance improvements [69]. We use a single channel for all of our results, as multi-channel audio is not comprehensive on most devices. Table 15 shows that DS2 significantly improves DS1 performance, but DS2 performance on noisy data."}, {"heading": "6.2 Mandarin", "text": "In Table 16, we compare several architectures trained in the Mandarin Chinese language with a development set of 2000 expressions and a test set of 1882 examples of loud language. This development set was also used to optimize the decoding parameters. We see that the deepest model with 2D invariant folding and BatchNorm outperforms the shallow RNN by 48% relatively, continuing the trend we have seen in the English system - several layers of bidirectional reciprocity significantly improve performance. We find that our best Mandarin Chinese language system transcribes short language queries better than a typical Mandarin Chinese speaker. To make a comparison with humans, we conducted a test with 100 randomly selected expressions and had a group of 5 people mark all together. We found that the group of people had an error rate of 4.0% compared to the language system performance of 3.7%."}, {"heading": "7 Deployment", "text": "The system used in Section 6.1 is not well designed for this task for several reasons. First, because the RNN has multiple bi-directional layers, transcribing the first part of an utterance requires that the entire utterance be submitted to the RNN. Second, because we use a broad beam when decoding with a language model, beam searching can be expensive, especially in Mandarin, where the number of possible next characters is very large (about 6000). Third, as described in Section 3, we normalize performance across an entire utterance, which in turn requires that the entire utterance be available in advance. We solve the power normalization problem by using some statistics from our training set to perform an adaptive normalization of speech inputs during online transcription."}, {"heading": "7.1 Batch Dispatch", "text": "Most Internet applications process requests individually, while the processor must load all the weights of the network for each individual request, lowering the arithmetic intensity of work performance and increasing bandwidth, as it is difficult to use the caches effectively when the requests are presented individually."}, {"heading": "7.2 Deployment Optimized Matrix Multiply Kernels", "text": "We found that providing our models with half-precision (16-bit) floating-point arithmetic does not measurably alter detection accuracy. As deployment does not require network weight updates, it is much less sensitive to numerical precision than training. Using half-precision arithmetic saves disk space and bandwidth, which is particularly useful for deployment since the RNN assessment is dominated by the cost of caching and streaming the weight matrices. As seen in Section 7.1, the stack size during deployment is much smaller than in training. We found that the standard BLAS libraries are inefficient at this stack size. To overcome this, we wrote our own semi-precise matrix matrix matrix kernel multiply. For 10 concurrent streams over 90 percent of the stacks, these stacks are \u2264 4, a matrix that is multiplied in the matrix regime."}, {"heading": "7.3 Beam Search", "text": "In Mandarin, this results in over 1 million searches per 40 ms increment of the language data, which is too slow to provide. To solve this problem, we use heuristics to further trim the bar search. Instead of considering all characters as useful additions to the bar, we consider only the smallest number of characters whose cumulative probability is at least p. In practice, we have found that p = 0.99 works well. In addition, we limit ourselves to no more than 40 characters. This speeds up the search time of the Mandarin language model by a factor of 150 and has a negligible effect on the CER (0.1-0.3% relative)."}, {"heading": "7.4 Results", "text": "Our research system achieves an error rate of 5.81 characters, while the system used achieves an error rate of 6.10 characters, which is only a 5% relative degradation of the system used. To achieve this, we use a low-latency neural network architecture, reduce the precision of our network to 16 bits, build a batch scheduler for more efficient evaluation of RNNNs, and find simple heuristics to reduce the cost of beam search. The model consists of five recursive layers with 2560 hidden units, a sequence folding layer (Section 3.7) with progression = 19, and a fully connected layer with 2560 hidden units. These techniques allow us to use Deep Speech cost-effectively for interactive applications."}, {"heading": "8 Conclusion", "text": "In fact, our results show that, compared to the previous incarnation, Deep Speech has significantly closed the gap in transcription performance in human workers by using more data and larger models. As the approach is very general, we have shown that it can be quickly applied to new languages. Creating powerful recognition systems for two very different languages, English and Mandarin, essentially did not require language expertise. Finally, we have also shown that this approach can be used efficiently by bundling user requests together on a GPU server, paving the way to provide users with end-to-end deep learning technologies. To achieve these results, we have researched various network architectures and found several effective techniques: improvements in numerical optimization through sorting degree and batch normalization, the evaluation of RNs with greater effort for English, the search for bidirectional and large universal models that we already use in the overall system and for optimization."}, {"heading": "Acknowledgments", "text": "We would like to thank the Baidu Language Technology Group for their help with data preparation and useful conversations. We would like to thank Scott Gray, Amir Khosrowshahi and everyone at Nervana Systems for their excellent matrix multiplication routines and useful discussions. We would also like to thank Natalia Gimelshein at NVIDIA for useful discussions and thoughts on implementing our fast deployment matrix multiplication."}, {"heading": "A Scalability improvements", "text": "In this section, we discuss some of our scalability improvements in more detail.A.1 Node and Cluster ArchitectureUs, we have the complex amount of database data based on a dense node made up of 2 Intel CPUs and 8 NVIDIA Titan X GPUs. We use the CPU memory to store our input data so that we are not directly exposed to the low bandwidth and high latency of the spinning disks. We replicate our English and Mandarin datasets on each local disk. This allows us to use our network only for weight updates and avoid relying on a schematic diagram of our nodes."}], "references": [{"title": "Applying convolutional neural networks concepts to hybrid nn-hmm model for speech recognition", "author": ["O. Abdel-Hamid", "A.-r. Mohamed", "H. Jang", "G. Penn"], "venue": "In ICASSP,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "ICLR,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "End-to-end attention-based large vocabulary speech recognition", "author": ["D. Bahdanau", "J. Chorowski", "D. Serdyuk", "P. Brakel", "Y. Bengio"], "venue": "abs/1508.04395,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "The third \u2019CHiME\u2019 speech separation and recognition challenge: Dataset, task and baselines", "author": ["J. Barker", "E. Marxer", "Ricard Vincent", "S. Watanabe"], "venue": "Submitted to IEEE", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Curriculum learning", "author": ["Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston"], "venue": "International Conference on Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Connectionist Speech Recognition: A Hybrid Approach", "author": ["H. Bourlard", "N. Morgan"], "venue": "Kluwer Academic Publishers, Norwell, MA,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1993}, {"title": "Listen, attend, and spell", "author": ["W. Chan", "N. Jaitly", "Q. Le", "O. Vinyals"], "venue": "abs/1508.01211,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Project adam: Building an efficient and scalable deep learning training system", "author": ["T. Chilimbi", "Y. Suzue", "J. Apacible", "K. Kalyanaraman"], "venue": "USENIX Symposium on Operating Systems Design and Implementation,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "EMNLP,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "End-to-end continuous speech recognition using attention-based recurrent nn: First results", "author": ["J. Chorowski", "D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "abs/1412.1602,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "The Fisher corpus: a resource for the next generations of speech-totext", "author": ["C. Cieri", "D. Miller", "K. Walker"], "venue": "LREC, volume 4, pages 69\u201371,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2004}, {"title": "Text detection and character recognition in scene images with unsupervised feature learning", "author": ["A. Coates", "B. Carpenter", "C. Case", "S. Satheesh", "B. Suresh", "T. Wang", "D.J. Wu", "A.Y. Ng"], "venue": "International Conference on Document Analysis and Recognition,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep learning with COTS HPC", "author": ["A. Coates", "B. Huval", "T. Wang", "D.J. Wu", "A.Y. Ng", "B. Catanzaro"], "venue": "International Conference on Machine Learning,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Large vocabulary continuous speech recognition with context-dependent DBN-HMMs", "author": ["G. Dahl", "D. Yu", "L. Deng"], "venue": "Proc. ICASSP,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Context-dependent pre-trained deep neural networks for large vocabulary speech recognition", "author": ["G. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G.S. Corrado", "R. Monga", "K. Chen", "M. Devin", "Q. Le", "M. Mao", "M. Ranzato", "A. Senior", "P. Tucker", "K. Yang", "A. Ng"], "venue": "Advances in Neural Information Processing Systems 25,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Size matters: An empirical study of neural network training for large vocabulary continuous speech recognition", "author": ["D. Ellis", "N. Morgan"], "venue": "ICASSP, volume 2, pages 1013\u20131016. IEEE,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1999}, {"title": "Optimizing RNN performance. http://svail.github.io/rnn_perf", "author": ["E. Elsen"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Support vector machines for noise robust ASR", "author": ["M.J.F. Gales", "A. Ragni", "H. Aldamarki", "C. Gautier"], "venue": "ASRU, pages 205\u20132010,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "F. Gomez", "J. Schmidhuber"], "venue": "ICML, pages 369\u2013376. ACM,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["A. Graves", "N. Jaitly"], "venue": "ICML,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-r. Mohamed", "G. Hinton"], "venue": "In ICASSP,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["H.H. Sak", "A. Senior", "F. Beaufays"], "venue": "Interspeech,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["A. Hannun", "C. Case", "J. Casper", "B. Catanzaro", "G. Diamos", "E. Elsen", "R. Prenger", "S. Satheesh", "S. Sengupta", "A. Coates", "A.Y. Ng"], "venue": "1412.5567,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "First-pass large vocabulary continuous speech recognition using bi-directional recurrent DNNs", "author": ["A.Y. Hannun", "A.L. Maas", "D. Jurafsky", "A.Y. Ng"], "venue": "abs/1408.2873,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Scalable modified Kneser-Ney language model estimation", "author": ["K. Heafield", "I. Pouzyrevsky", "J.H. Clark", "P. Koehn"], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, Sofia, Bulgaria, 8", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine, 29(November):82\u201397,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, 9(8):1735\u20141780,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1997}, {"title": "Vocal tract length perturbation (VTLP) improves speech recognition", "author": ["N. Jaitly", "G. Hinton"], "venue": "ICML Workshop on Deep Learning for Audio, Speech, and Language Processing,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "An empirical exploration of recurrent network architectures", "author": ["R. Jozefowicz", "W. Zaremba", "I. Sutskever"], "venue": "ICML,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "A big data approach to acoustic model training corpus selection", "author": ["O. Kapralova", "J. Alex", "E. Weinstein", "P. Moreno", "O. Siohan"], "venue": "Interspeech,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "A fast storage allocator", "author": ["K.C. Knowlton"], "venue": "Commun. ACM, 8(10):623\u2013624, Oct.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1965}, {"title": "Audio augmentation for speech recognition", "author": ["T. Ko", "V. Peddinti", "D. Povey", "S. Khudanpur"], "venue": "Interspeech,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems 25, pages 1106\u20131114,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "Batch normalized recurrent neural networks", "author": ["C. Laurent", "G. Pereyra", "P. Brakel", "Y. Zhang", "Y. Bengio"], "venue": "abs/1510.01378,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Q. Le", "M. Ranzato", "R. Monga", "M. Devin", "K. Chen", "G. Corrado", "J. Dean", "A. Ng"], "venue": "International Conference on Machine Learning,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning methods for generic object recognition with invariance to pose and lighting", "author": ["Y. LeCun", "F.J. Huang", "L. Bottou"], "venue": "Computer Vision and Pattern Recognition, volume 2, pages 97\u2013104,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2004}, {"title": "Lexicon-free conversational speech recognition with neural networks", "author": ["A. Maas", "Z. Xie", "D. Jurafsky", "A. Ng"], "venue": "NAACL,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "EESEN: End-to-end speech recognition using deep rnn models and wfst-based decoding", "author": ["Y. Miao", "M. Gowayyed", "F. Metz"], "venue": "ASRU,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Acoustic modeling using deep belief networks", "author": ["A. Mohamed", "G. Dahl", "G. Hinton"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, (99),", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2011}, {"title": "Application of pretrained deep neural networks to large vocabulary speech recognition", "author": ["A.S.N. Jaitly", "P. Nguyen", "V. Vanhoucke"], "venue": "Interspeech,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2012}, {"title": "Context-dependent deep neural networks for commercial mandarin speech recognition applications", "author": ["J. Niu", "L. Xie", "L. Jia", "N. Hu"], "venue": "APSIPA,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2013}, {"title": "Librispeech: an asr corpus based on public domain audio books", "author": ["V. Panayotov", "G. Chen", "D. Povey", "S. Khudanpur"], "venue": "ICASSP,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "abs/1211.5063,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2012}, {"title": "Bandwidth optimal all-reduce algorithms for clusters of workstations", "author": ["P. Patarasuk", "X. Yuan"], "venue": "J. Parallel Distrib. Comput., 69(2):117\u2013124, Feb.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2009}, {"title": "Large-scale deep unsupervised learning using graphics processors", "author": ["R. Raina", "A. Madhavan", "A. Ng"], "venue": "26th International Conference on Machine Learning,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2009}, {"title": "Connectionist probability estimators in HMM speech recognition", "author": ["S. Renals", "N. Morgan", "H. Bourlard", "M. Cohen", "H. Franco"], "venue": "IEEE Transactions on Speech and Audio Processing, 2(1):161\u2013174,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1994}, {"title": "The use of recurrent neural networks in continuous speech recognition", "author": ["T. Robinson", "M. Hochberg", "S. Renals"], "venue": "pages 253\u2013258,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1996}, {"title": "Convolutional, long short-term memory, fully connected deep neural networks", "author": ["T. Sainath", "O. Vinyals", "A. Senior", "H. Sak"], "venue": "ICASSP,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep convolutional neural networks for LVCSR", "author": ["T.N. Sainath", "A. rahman Mohamed", "B. Kingsbury", "B. Ramabhadran"], "venue": "In ICASSP,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2013}, {"title": "Fast and accurate recurrent neural network acoustic models for speech recognition", "author": ["H. Sak", "A. Senior", "K. Rao", "F. Beaufays"], "venue": "abs/1507.06947,", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence discriminative distributed training of long shortterm memory recurrent neural networks", "author": ["H. Sak", "O. Vinyals", "G. Heigold", "A. Senior", "E. McDermott", "R. Monga", "M. Mao"], "venue": "Interspeech,", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2014}, {"title": "A fast data collection and augmentation procedure for object recognition", "author": ["B. Sapp", "A. Saxena", "A. Ng"], "venue": "AAAI Twenty-Third Conference on Artificial Intelligence,", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2008}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "K.K. Paliwal"], "venue": "IEEE Transactions on Signal Processing, 45(11):2673\u20132681,", "citeRegEx": "57", "shortCiteRegEx": null, "year": 1997}, {"title": "Conversational speech transcription using context-dependent deep neural networks", "author": ["F. Seide", "G. Li", "D. Yu"], "venue": "Interspeech, pages 437\u2013440,", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2011}, {"title": "Search by voice in mandarin chinese", "author": ["J. Shan", "G. Wu", "Z. Hu", "X. Tang", "M. Jansche", "P. Moreno"], "venue": "Interspeech,", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2010}, {"title": "Joint training of convolutional and non-convolutional neural networks", "author": ["H. Soltau", "G. Saon", "T. Sainath"], "venue": "ICASSP,", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2014}, {"title": "On the importance of momentum and initialization in deep learning", "author": ["I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton"], "venue": "30th International Conference on Machine Learning,", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": null, "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["C. Szegedy", "S. Ioffe"], "venue": "abs/1502.03167,", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimization of collective communication operations in mpich", "author": ["R. Thakur", "R. Rabenseifner"], "venue": "International Journal of High Performance Computing Applications, 19:49\u201366,", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2005}, {"title": "Sequence-discriminative training of deep neural networks", "author": ["K. Vesely", "A. Ghoshal", "L. Burget", "D. Povey"], "venue": "Interspeech,", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2013}, {"title": "Phoneme recognition using time-delay neural networks,\u00e2\u0102\u0130 acoustics speech and signal processing", "author": ["A. Waibel", "T. Hanazawa", "G. Hinton", "K. Shikano", "K. Lang"], "venue": "IEEE Transactions on Acoustics, Speech and Signal Processing, 37(3):328\u2013339,", "citeRegEx": "67", "shortCiteRegEx": null, "year": 1989}, {"title": "An efficient gradient-based algorithm for online training of recurrent network trajectories", "author": ["R. Williams", "J. Peng"], "venue": "Neural computation, 2:490\u2013501,", "citeRegEx": "68", "shortCiteRegEx": null, "year": 1990}, {"title": "The ntt chime-3 system: Advances in speech enhancement and recognition for mobile multi-microphone devices", "author": ["T. Yoshioka", "N. Ito", "M. Delcroix", "A. Ogawa", "K. Kinoshita", "M.F.C. Yu", "W.J. Fabian", "M. Espi", "T. Higuchi", "S. Araki", "T. Nakatani"], "venue": "IEEE ASRU,", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to execute", "author": ["W. Zaremba", "I. Sutskever"], "venue": "abs/1410.4615,", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 23, "context": "Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system [26].", "startOffset": 109, "endOffset": 113}, {"referenceID": 23, "context": "A simple but powerful alternative solution is to train such ASR models end-to-end, using deep learning to replace most modules with a single model [26].", "startOffset": 147, "endOffset": 151}, {"referenceID": 23, "context": "We show that through these techniques we are able to reduce error rates of our previous end-to-end system [26] in English by up to 43%, and can also recognize Mandarin speech with high accuracy.", "startOffset": 106, "endOffset": 110}, {"referenceID": 19, "context": "In particular, we describe numerous experiments with neural networks trained with the Connectionist Temporal Classification (CTC) loss function [22] to predict speech transcriptions from audio.", "startOffset": 144, "endOffset": 148}, {"referenceID": 59, "context": "We consider networks composed of many layers of recurrent connections, convolutional filters, and nonlinearities, as well as the impact of a specific instance of Batch Normalization [63] (BatchNorm) applied to RNNs.", "startOffset": 182, "endOffset": 186}, {"referenceID": 23, "context": "We not only find networks that produce much better predictions than those in previous work [26], but also find instances of recurrent models that can be deployed in a production setting with no significant loss in accuracy.", "startOffset": 91, "endOffset": 95}, {"referenceID": 15, "context": "In contrast to previous large-scale training approaches that use parameter servers and asynchronous updates [18, 10], we use synchronous SGD, which is easier to debug while testing new ideas, and also converges faster for the same degree of data parallelism.", "startOffset": 108, "endOffset": 116}, {"referenceID": 7, "context": "In contrast to previous large-scale training approaches that use parameter servers and asynchronous updates [18, 10], we use synchronous SGD, which is easier to debug while testing new ideas, and also converges faster for the same degree of data parallelism.", "startOffset": 108, "endOffset": 116}, {"referenceID": 23, "context": "We benchmark our system on several publicly available test sets and compare the results to our previous end-to-end system [26].", "startOffset": 122, "endOffset": 126}, {"referenceID": 5, "context": "Feed-forward neural network acoustic models were explored more than 20 years ago [7, 50, 19].", "startOffset": 81, "endOffset": 92}, {"referenceID": 46, "context": "Feed-forward neural network acoustic models were explored more than 20 years ago [7, 50, 19].", "startOffset": 81, "endOffset": 92}, {"referenceID": 16, "context": "Feed-forward neural network acoustic models were explored more than 20 years ago [7, 50, 19].", "startOffset": 81, "endOffset": 92}, {"referenceID": 47, "context": "Recurrent neural networks and networks with convolution were also used in speech recognition around the same time [51, 67].", "startOffset": 114, "endOffset": 122}, {"referenceID": 62, "context": "Recurrent neural networks and networks with convolution were also used in speech recognition around the same time [51, 67].", "startOffset": 114, "endOffset": 122}, {"referenceID": 39, "context": "More recently DNNs have become a fixture in the ASR pipeline with almost all state of the art speech work containing some form of deep neural network [42, 29, 17, 16, 43, 58].", "startOffset": 150, "endOffset": 174}, {"referenceID": 26, "context": "More recently DNNs have become a fixture in the ASR pipeline with almost all state of the art speech work containing some form of deep neural network [42, 29, 17, 16, 43, 58].", "startOffset": 150, "endOffset": 174}, {"referenceID": 14, "context": "More recently DNNs have become a fixture in the ASR pipeline with almost all state of the art speech work containing some form of deep neural network [42, 29, 17, 16, 43, 58].", "startOffset": 150, "endOffset": 174}, {"referenceID": 13, "context": "More recently DNNs have become a fixture in the ASR pipeline with almost all state of the art speech work containing some form of deep neural network [42, 29, 17, 16, 43, 58].", "startOffset": 150, "endOffset": 174}, {"referenceID": 40, "context": "More recently DNNs have become a fixture in the ASR pipeline with almost all state of the art speech work containing some form of deep neural network [42, 29, 17, 16, 43, 58].", "startOffset": 150, "endOffset": 174}, {"referenceID": 54, "context": "More recently DNNs have become a fixture in the ASR pipeline with almost all state of the art speech work containing some form of deep neural network [42, 29, 17, 16, 43, 58].", "startOffset": 150, "endOffset": 174}, {"referenceID": 0, "context": "Convolutional networks have also been found beneficial for acoustic models [1, 53].", "startOffset": 75, "endOffset": 82}, {"referenceID": 49, "context": "Convolutional networks have also been found beneficial for acoustic models [1, 53].", "startOffset": 75, "endOffset": 82}, {"referenceID": 21, "context": "Recurrent neural networks, typically LSTMs, are just beginning to be deployed in state-of-the art recognizers [24, 25, 55] and work well together with convolutional layers for the feature extraction [52].", "startOffset": 110, "endOffset": 122}, {"referenceID": 22, "context": "Recurrent neural networks, typically LSTMs, are just beginning to be deployed in state-of-the art recognizers [24, 25, 55] and work well together with convolutional layers for the feature extraction [52].", "startOffset": 110, "endOffset": 122}, {"referenceID": 51, "context": "Recurrent neural networks, typically LSTMs, are just beginning to be deployed in state-of-the art recognizers [24, 25, 55] and work well together with convolutional layers for the feature extraction [52].", "startOffset": 110, "endOffset": 122}, {"referenceID": 48, "context": "Recurrent neural networks, typically LSTMs, are just beginning to be deployed in state-of-the art recognizers [24, 25, 55] and work well together with convolutional layers for the feature extraction [52].", "startOffset": 199, "endOffset": 203}, {"referenceID": 21, "context": "Models with both bidirectional [24] and unidirectional recurrence have been explored as well.", "startOffset": 31, "endOffset": 35}, {"referenceID": 20, "context": "End-to-end speech recognition is an active area of research, showing compelling results when used to re-score the outputs of a DNN-HMM [23] and standalone [26].", "startOffset": 135, "endOffset": 139}, {"referenceID": 23, "context": "End-to-end speech recognition is an active area of research, showing compelling results when used to re-score the outputs of a DNN-HMM [23] and standalone [26].", "startOffset": 155, "endOffset": 159}, {"referenceID": 8, "context": "The RNN encoderdecoder paradigm uses an encoder RNN to map the input to a fixed length vector and a decoder network to expand the fixed length vector into a sequence of output predictions [11, 62].", "startOffset": 188, "endOffset": 196}, {"referenceID": 58, "context": "The RNN encoderdecoder paradigm uses an encoder RNN to map the input to a fixed length vector and a decoder network to expand the fixed length vector into a sequence of output predictions [11, 62].", "startOffset": 188, "endOffset": 196}, {"referenceID": 1, "context": "Adding an attentional mechanism to the decoder greatly improves performance of the system, particularly with long inputs or outputs [2].", "startOffset": 132, "endOffset": 135}, {"referenceID": 9, "context": "In speech, the RNN encoder-decoder with attention performs well both in predicting phonemes [12] or graphemes [3, 8].", "startOffset": 92, "endOffset": 96}, {"referenceID": 2, "context": "In speech, the RNN encoder-decoder with attention performs well both in predicting phonemes [12] or graphemes [3, 8].", "startOffset": 110, "endOffset": 116}, {"referenceID": 6, "context": "In speech, the RNN encoder-decoder with attention performs well both in predicting phonemes [12] or graphemes [3, 8].", "startOffset": 110, "endOffset": 116}, {"referenceID": 19, "context": "The other commonly used technique for mapping variable length audio input to variable length output is the CTC loss function [22] coupled with an RNN to model temporal information.", "startOffset": 125, "endOffset": 129}, {"referenceID": 20, "context": "The CTCRNN model performs well in end-to-end speech recognition with grapheme outputs [23, 27, 26, 40].", "startOffset": 86, "endOffset": 102}, {"referenceID": 24, "context": "The CTCRNN model performs well in end-to-end speech recognition with grapheme outputs [23, 27, 26, 40].", "startOffset": 86, "endOffset": 102}, {"referenceID": 23, "context": "The CTCRNN model performs well in end-to-end speech recognition with grapheme outputs [23, 27, 26, 40].", "startOffset": 86, "endOffset": 102}, {"referenceID": 37, "context": "The CTCRNN model performs well in end-to-end speech recognition with grapheme outputs [23, 27, 26, 40].", "startOffset": 86, "endOffset": 102}, {"referenceID": 38, "context": "The CTC-RNN model has also been shown to work well in predicting phonemes [41, 54], though a lexicon is still needed in this case.", "startOffset": 74, "endOffset": 82}, {"referenceID": 50, "context": "The CTC-RNN model has also been shown to work well in predicting phonemes [41, 54], though a lexicon is still needed in this case.", "startOffset": 74, "endOffset": 82}, {"referenceID": 50, "context": "Furthermore it has been necessary to pre-train the CTC-RNN network with a DNN cross-entropy network that is fed frame-wise alignments from a GMM-HMM system [54].", "startOffset": 156, "endOffset": 160}, {"referenceID": 33, "context": "Exploiting scale in deep learning has been central to the success of the field thus far [36, 38].", "startOffset": 88, "endOffset": 96}, {"referenceID": 35, "context": "Exploiting scale in deep learning has been central to the success of the field thus far [36, 38].", "startOffset": 88, "endOffset": 96}, {"referenceID": 45, "context": "Training on a single GPU resulted in substantial performance gains [49], which were subsequently scaled linearly to two [36] or more GPUs [15].", "startOffset": 67, "endOffset": 71}, {"referenceID": 33, "context": "Training on a single GPU resulted in substantial performance gains [49], which were subsequently scaled linearly to two [36] or more GPUs [15].", "startOffset": 120, "endOffset": 124}, {"referenceID": 12, "context": "Training on a single GPU resulted in substantial performance gains [49], which were subsequently scaled linearly to two [36] or more GPUs [15].", "startOffset": 138, "endOffset": 142}, {"referenceID": 12, "context": "We build on the past work in using modelparallelism [15], data-parallelism [18] or a combination of the two [64, 26] to create a fast and highly scalable system for training deep RNNs in speech recognition.", "startOffset": 52, "endOffset": 56}, {"referenceID": 15, "context": "We build on the past work in using modelparallelism [15], data-parallelism [18] or a combination of the two [64, 26] to create a fast and highly scalable system for training deep RNNs in speech recognition.", "startOffset": 75, "endOffset": 79}, {"referenceID": 23, "context": "We build on the past work in using modelparallelism [15], data-parallelism [18] or a combination of the two [64, 26] to create a fast and highly scalable system for training deep RNNs in speech recognition.", "startOffset": 108, "endOffset": 116}, {"referenceID": 23, "context": "Data has also been central to the success of end-to-end speech recognition, with over 7000 hours of labeled speech used in Deep Speech 1 (DS1) [26].", "startOffset": 143, "endOffset": 147}, {"referenceID": 36, "context": "Data augmentation has been highly effective in improving the performance of deep learning in computer vision [39, 56, 14].", "startOffset": 109, "endOffset": 121}, {"referenceID": 52, "context": "Data augmentation has been highly effective in improving the performance of deep learning in computer vision [39, 56, 14].", "startOffset": 109, "endOffset": 121}, {"referenceID": 11, "context": "Data augmentation has been highly effective in improving the performance of deep learning in computer vision [39, 56, 14].", "startOffset": 109, "endOffset": 121}, {"referenceID": 18, "context": "This has also been shown to improve speech systems [21, 26].", "startOffset": 51, "endOffset": 59}, {"referenceID": 23, "context": "This has also been shown to improve speech systems [21, 26].", "startOffset": 51, "endOffset": 59}, {"referenceID": 23, "context": "Techniques used for data augmentation in speech range from simple noise addition [26] to complex perturbations such as simulating changes to the vocal tract length and rate of speech of the speaker [31, 35].", "startOffset": 81, "endOffset": 85}, {"referenceID": 28, "context": "Techniques used for data augmentation in speech range from simple noise addition [26] to complex perturbations such as simulating changes to the vocal tract length and rate of speech of the speaker [31, 35].", "startOffset": 198, "endOffset": 206}, {"referenceID": 32, "context": "Techniques used for data augmentation in speech range from simple noise addition [26] to complex perturbations such as simulating changes to the vocal tract length and rate of speech of the speaker [31, 35].", "startOffset": 198, "endOffset": 206}, {"referenceID": 42, "context": "In one approach, the authors use one speech engine to align and filter a thousand hours of read speech [46].", "startOffset": 103, "endOffset": 107}, {"referenceID": 30, "context": "In another approach, a heavy-weight offline speech recognizer is used to generate transcriptions for tens of thousands of hours of speech [33].", "startOffset": 138, "endOffset": 142}, {"referenceID": 23, "context": "Figure 1 shows the architecture of the DS2 system which at its core is similar to the previous DS1 system [26]: a recurrent neural network (RNN) trained to ingest speech spectrograms and generate text transcriptions.", "startOffset": 106, "endOffset": 110}, {"referenceID": 53, "context": "Following the convolutional layers are one or more bidirectional recurrent layers [57].", "startOffset": 82, "endOffset": 86}, {"referenceID": 27, "context": "The function g(\u00b7) can also represent more complex recurrence operations such as the Long Short-Term Memory (LSTM) units [30] and the gated recurrent units (GRU) [11].", "startOffset": 120, "endOffset": 124}, {"referenceID": 8, "context": "The function g(\u00b7) can also represent more complex recurrence operations such as the Long Short-Term Memory (LSTM) units [30] and the gated recurrent units (GRU) [11].", "startOffset": 161, "endOffset": 165}, {"referenceID": 19, "context": "The model is trained using the CTC loss function [22].", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "In the following subsections we describe the architectural and algorithmic improvements made relative to DS1 [26].", "startOffset": 109, "endOffset": 113}, {"referenceID": 21, "context": "Previous work has examined doing so by increasing the number of consecutive bidirectional recurrent layers [24].", "startOffset": 107, "endOffset": 111}, {"referenceID": 59, "context": "We explore Batch Normalization (BatchNorm) as a technique to accelerate training for such networks [63] since they often suffer from optimization issues.", "startOffset": 99, "endOffset": 103}, {"referenceID": 34, "context": "Recent research has shown that BatchNorm improves the speed of convergence of recurrent nets, without showing any improvement in generalization performance [37].", "startOffset": 156, "endOffset": 160}, {"referenceID": 34, "context": "We consider two methods of extending BatchNorm to bidirectional RNNs [37].", "startOffset": 69, "endOffset": 73}, {"referenceID": 34, "context": "We find that sequence-wise normalization [37] overcomes these issues.", "startOffset": 41, "endOffset": 45}, {"referenceID": 59, "context": "Instead, we store a running average of the mean and variance for the neuron collected during training, and use these for evaluation in deployment [63].", "startOffset": 146, "endOffset": 150}, {"referenceID": 63, "context": "One possible solution is truncating backpropagation through time [68], so that all examples have the same sequence length during training [52].", "startOffset": 65, "endOffset": 69}, {"referenceID": 48, "context": "One possible solution is truncating backpropagation through time [68], so that all examples have the same sequence length during training [52].", "startOffset": 138, "endOffset": 142}, {"referenceID": 4, "context": "Other works have found that presenting examples in order of difficulty can accelerate online learning [6, 70].", "startOffset": 102, "endOffset": 109}, {"referenceID": 65, "context": "Other works have found that presenting examples in order of difficulty can accelerate online learning [6, 70].", "startOffset": 102, "endOffset": 109}, {"referenceID": 8, "context": "A common theme in many sequence learning problems including machine translation and speech recognition is that longer examples tend to be more challenging [11].", "startOffset": 155, "endOffset": 159}, {"referenceID": 48, "context": "Current research in speech and language processing has shown that having a more complex recurrence can allow the network to remember state over more time-steps while making them more computationally expensive to train [52, 8, 62, 2].", "startOffset": 218, "endOffset": 232}, {"referenceID": 6, "context": "Current research in speech and language processing has shown that having a more complex recurrence can allow the network to remember state over more time-steps while making them more computationally expensive to train [52, 8, 62, 2].", "startOffset": 218, "endOffset": 232}, {"referenceID": 58, "context": "Current research in speech and language processing has shown that having a more complex recurrence can allow the network to remember state over more time-steps while making them more computationally expensive to train [52, 8, 62, 2].", "startOffset": 218, "endOffset": 232}, {"referenceID": 1, "context": "Current research in speech and language processing has shown that having a more complex recurrence can allow the network to remember state over more time-steps while making them more computationally expensive to train [52, 8, 62, 2].", "startOffset": 218, "endOffset": 232}, {"referenceID": 27, "context": "Two commonly used recurrent architectures are the Long Short-Term Memory (LSTM) units [30] and the Gated Recurrent Units (GRU) [11], though many other variations exist.", "startOffset": 86, "endOffset": 90}, {"referenceID": 8, "context": "Two commonly used recurrent architectures are the Long Short-Term Memory (LSTM) units [30] and the Gated Recurrent Units (GRU) [11], though many other variations exist.", "startOffset": 127, "endOffset": 131}, {"referenceID": 29, "context": "A recent comprehensive study of thousands of variations of LSTM and GRU architectures showed that a GRU is comparable to an LSTM with a properly initialized forget gate bias, and their best variants are competitive with each other [32].", "startOffset": 231, "endOffset": 235}, {"referenceID": 62, "context": "This type of convolution was first proposed for neural networks in speech more than 25 years ago [67].", "startOffset": 97, "endOffset": 101}, {"referenceID": 13, "context": "Many neural network speech models have a first layer that processes input frames with some context window [16, 66].", "startOffset": 106, "endOffset": 114}, {"referenceID": 61, "context": "Many neural network speech models have a first layer that processes input frames with some context window [16, 66].", "startOffset": 106, "endOffset": 114}, {"referenceID": 23, "context": "The DS1 system accomplished this through the use of a spectrogram as input and temporal convolution in the first layer with a stride parameter to reduce the number of time-steps [26].", "startOffset": 178, "endOffset": 182}, {"referenceID": 0, "context": "Convolutions in frequency and time domains, when applied to the spectral input features prior to any other processing, can slightly improve ASR performance [1, 53, 60].", "startOffset": 156, "endOffset": 167}, {"referenceID": 49, "context": "Convolutions in frequency and time domains, when applied to the spectral input features prior to any other processing, can slightly improve ASR performance [1, 53, 60].", "startOffset": 156, "endOffset": 167}, {"referenceID": 56, "context": "Convolutions in frequency and time domains, when applied to the spectral input features prior to any other processing, can slightly improve ASR performance [1, 53, 60].", "startOffset": 156, "endOffset": 167}, {"referenceID": 3, "context": "We report results on two datasets\u2014a development set of 2048 utterances (\u201cRegular Dev\u201d) and a much noisier dataset of 2048 utterances (\u201cNoisy Dev\u201d) randomly sampled from the CHiME 2015 development datasets [4].", "startOffset": 205, "endOffset": 208}, {"referenceID": 23, "context": "We use an n-gram language model since they scale well to large amounts of unlabeled text [26].", "startOffset": 89, "endOffset": 93}, {"referenceID": 25, "context": "For English, our language model is a Kneser-Ney smoothed 5-gram model with pruning that is trained using the KenLM toolkit [28] on cleaned text from the Common Crawl Repository3.", "startOffset": 123, "endOffset": 127}, {"referenceID": 23, "context": "This is a linear combination of log probabilities from the CTC trained network and language model, along with a word insertion term [26].", "startOffset": 132, "endOffset": 136}, {"referenceID": 24, "context": "We use a beam search to find the optimal transcription [27].", "startOffset": 55, "endOffset": 59}, {"referenceID": 55, "context": "This precludes the need to construct a pronunciation model, which is often a fairly involved component for porting speech systems to other languages [59].", "startOffset": 149, "endOffset": 153}, {"referenceID": 55, "context": "For example we do not need to model Mandarin tones explicitly, as some speech systems must do [59, 45].", "startOffset": 94, "endOffset": 102}, {"referenceID": 41, "context": "For example we do not need to model Mandarin tones explicitly, as some speech systems must do [59, 45].", "startOffset": 94, "endOffset": 102}, {"referenceID": 15, "context": "[18] typically do not provide reproducibility and are therefore more difficult to debug.", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "Since all-reduce is critical to the scalability of our training, we wrote our own implementation of the ring algorithm [48, 65] for higher performance and better stability.", "startOffset": 119, "endOffset": 127}, {"referenceID": 60, "context": "Since all-reduce is critical to the scalability of our training, we wrote our own implementation of the ring algorithm [48, 65] for higher performance and better stability.", "startOffset": 119, "endOffset": 127}, {"referenceID": 31, "context": "Our implementation follows the approach of the last level shared allocator in jemalloc: all allocations are carved out of contiguous memory blocks using the buddy algorithm [34].", "startOffset": 173, "endOffset": 177}, {"referenceID": 10, "context": "The Wall Street Journal (WSJ), Switchboard and Fisher [13] corpora are all published by the Linguistic Data Consortium.", "startOffset": 54, "endOffset": 58}, {"referenceID": 42, "context": "The LibriSpeech dataset [46] is available free on-line.", "startOffset": 24, "endOffset": 28}, {"referenceID": 50, "context": "fixed delay and this can happen with unidirectional RNNs [54].", "startOffset": 57, "endOffset": 61}, {"referenceID": 23, "context": "We augment our training data by adding noise to increase the effective size of our training data and to improve our robustness to noisy speech [26].", "startOffset": 143, "endOffset": 147}, {"referenceID": 57, "context": "We use stochastic gradient descent with Nesterov momentum [61] along with a minibatch of 512 utterances.", "startOffset": 58, "endOffset": 62}, {"referenceID": 43, "context": "If the norm of the gradient exceeds a threshold of 400, it is rescaled to 400 [47].", "startOffset": 78, "endOffset": 82}, {"referenceID": 42, "context": "We also take advantage of the recently developed LibriSpeech corpus constructed using audio books from the LibriVox project [46].", "startOffset": 124, "endOffset": 128}, {"referenceID": 3, "context": "We test our performance on noisy speech using the publicly available test sets from the recently completed third CHiME challenge [4].", "startOffset": 129, "endOffset": 132}, {"referenceID": 64, "context": "The CHiME audio has 6 channels and using all of them can provide substantial performance improvements [69].", "startOffset": 102, "endOffset": 106}, {"referenceID": 0, "context": "Figure 7: Comparison of kernels that compute Ax = b where A is a matrix with dimension 2560\u00d7 2560, and x is a matrix with dimension 2560\u00d7 Batch size, where Batch size \u2208 [1, 10].", "startOffset": 169, "endOffset": 176}, {"referenceID": 7, "context": "Figure 7: Comparison of kernels that compute Ax = b where A is a matrix with dimension 2560\u00d7 2560, and x is a matrix with dimension 2560\u00d7 Batch size, where Batch size \u2208 [1, 10].", "startOffset": 169, "endOffset": 176}, {"referenceID": 17, "context": "0, more details are found here [20].", "startOffset": 31, "endOffset": 35}], "year": 2015, "abstractText": "We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech\u2014two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system [26]. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.", "creator": "LaTeX with hyperref package"}}}