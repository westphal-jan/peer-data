{"id": "1512.04152", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Dec-2015", "title": "Fighting Bandits with a New Kind of Smoothness", "abstract": "We define a novel family of algorithms for the adversarial multi-armed bandit problem, and provide a simple analysis technique based on convex smoothing. We prove two main results. First, we show that regularization via the \\emph{Tsallis entropy}, which includes EXP3 as a special case, achieves the $\\Theta(\\sqrt{TN})$ minimax regret. Second, we show that a wide class of perturbation methods achieve a near-optimal regret as low as $O(\\sqrt{TN \\log N})$ if the perturbation distribution has a bounded hazard rate. For example, the Gumbel, Weibull, Frechet, Pareto, and Gamma distributions all satisfy this key property.", "histories": [["v1", "Mon, 14 Dec 2015 01:57:02 GMT  (28kb,D)", "http://arxiv.org/abs/1512.04152v1", "In Proceedings of NIPS, 2015"]], "COMMENTS": "In Proceedings of NIPS, 2015", "reviews": [], "SUBJECTS": "cs.LG cs.GT stat.ML", "authors": ["jacob d abernethy", "chansoo lee", "ambuj tewari"], "accepted": true, "id": "1512.04152"}, "pdf": {"name": "1512.04152.pdf", "metadata": {"source": "CRF", "title": "Fighting Bandits with a New Kind of Smoothness", "authors": ["Jacob Abernethy", "Chansoo Lee", "Ambuj Tewari"], "emails": ["jabernet@umich.edu", "chansool@umich.edu", "tewaria@umich.edu"], "sections": [{"heading": null, "text": "\u221a TN) minimaxregret. Secondly, we show that a broad class of disturbance methods achieves an almost optimal regret to O (\u221a TN logN) when the disturbance distribution has a limited hazard rate. For example, the Gumbel, Weibull, Frechet, Pareto and Gamma distributions all fulfill this key property."}, {"heading": "1 Introduction", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves, \"he said in an interview with the\" New York Times, \"in which he played the role of the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" New York Times, \"the\" the \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" the \"New York Times,\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\""}, {"heading": "2 Gradient-Based Prediction Algorithms for the Multi-Armed Bandit", "text": "Let us now introduce the opposing, multi-armed bandit problem. In each turn t = 1,.., T, a learner must select a distribution pt = N over the number of available N actions. The opponent (Nature) selects a loss vector gt [\u2212 1, 0] n. The learner plays an action he has scanned according to pt and suffers the loss gt, it. The learner observes only a single coordinate gt, he does not receive information about the values gt, j for j 6 = it. This limited information feedback makes the bandit problem much more difficult than the full information environment in which the whole gt is observed. (3) The goal of the learner is to minimize regret. Regret is defined as the difference between the realized loss and the loss of the best fixed action afterwards: RegretT: = max i [N] T = 1 (gt, i \u2212 gt, it). (3) To be precise, let us consider the expected regret, where the best fixed action is T = 1."}, {"heading": "2.1 The Gradient-Based Algorithmic Template", "text": "We are examining a particular algorithm template described in Framework 1 that is a slight deviation from the Gradient Based Prediction Algorithm (GBPA) by Abernethy et al. (2014). Note: The algorithm (i) maintains an unbiased estimate of cumulative losses G, (ii) Updates G, (ii) Updates G, and (ii) uses the gradient of a convex function that forms the basis of much of the published work on adversarial MAB algorithms (Auer et al., 2003; Kujala and Elomaa, 2005; New and Barto. k, 2013) mainly for two reasons."}, {"heading": "2.2 A New Kind of Smoothness", "text": "What has emerged as a guiding principle through machine learning is that the stability of an algorithm leads to performance guarantees (i.e., small modifications of the input data should not dramatically change the output. (In the context of GBPA, it has been shown that a uniform approach (prediction in each time step) by definition directly guarantees complete information adjustment. (In the bandit environment, however, a uniform tie to the Lipschitz continuity of the gradient is insufficient; the regret (Lemma 2.1) includes conditions of the form G-1 + g-t, G-1), where the incremental quantity g-t can be as large as the reversal of the smallest probability of p (G-t-1)."}, {"heading": "3 A Minimax Bandit Algorithm via Tsallis Smoothing", "text": "Auer et al. (2003) has proven that its EXP3 algorithm performs a very simple and intuitive analysis based on two natural consistencies. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <"}, {"heading": "4 Near-Optimal Bandit Algorithms via Stochastic Smoothing", "text": "Let D be a continuous distribution over an unlimited support with probability density function f and cumulative density function F. Consider the GBPA with potential function of the form: \u03a6 (G; D) = E Z1,..., ZN iid \u0445 D max i {Gi + Zi} (8), which is a stochastic smoothing of the (maxiGi) function. Since the maximum function is convex, it is also convex. However, from Bertsekas (1973) we can change the order of differentiation and expectation. (G; D) = E Z1,..., ZN iid \u00b2 D ei \u2212 pervex, where i \u0445 = arg max i = 1,..., N {Gi + Zi}. (9) Even if the function is not universally distinguishable, the swap is still possible with any subgradient under certain mild conditions."}, {"heading": "4.1 Connection to Follow the Perturbed Leader", "text": "The sampling step of the bandit GBPA (Framework 1) with stochastically smoothed function (Eq.8) can be performed efficiently. Instead of evaluating the expectation (Eq.9), we simply take a random sample. In fact, this corresponds to the Follow the Perturbed Leader Algorithm (FTPL) applied to the bandit environment (Kalai and Vempala, 2005). On the other hand, the estimation step is difficult because there is usually no closed form expression for the iterations. To solve this problem, Neu and Barto \"k (2013) proposed a Geometric Resampling (GR). GR uses an iterative resampling method for estimation. They showed that if we stop after M iterations, the additional regret due to the abandonment of the estimate is at most NTeM (additive term). That is, all of our GBPA remorse limits in this section apply to the corresponding PL algorithm with an additional NT. However, this does not apply to the NT."}, {"heading": "4.2 Hazard Rate analysis", "text": "In this section we show that the performance of the GBPA (GBPA) (GBPA) (GBPA) (GBPA) (GBPA) (GBPA) (GBPA) (GBPA) (GBPA) (GBPA) (GBPA) (GBPA)) (GBPA) (GBPA) (GBPA) (GBZ) (GBZ) (GBZ) (GBZ) (GBZ) (GBZ) (GBZ) (GBZ) (GBZ) (GBZ) (GBZ) (GBZ) (GBZ) (GBZ) (GBZ) (GBZ) (GBZ) (GBZ) (GBZ) (GBZ) (GBZ) (GBZ) (GBZ) (GBZ) (GBZ) (GBZ) (GBZ) (GBPA) (GBPA) (GBPA) (GBPA) (GBPA) (GBPA) (GBPA) (GBPA) (GBPA) (GBPA) (GBPA) (GBPA) (GBPA) (GBPA) (GBPA) (GBZ) (GBZ) (GBZ) (GBZ) (GBZ) (GBZ) (GBZ (GBZ) (GBZ) (GBZ) (GBZ) (GBZ) (GBZ) (GBZ) (GBZ) (GBZ (GBZ) (GBZ) (GBZ) (GBZ) (GBZ) (GBZ) (GBZ) (GBZ) (GBPA) (GBPA) (GBPA) (GBPA) (GBPA) (GBPA) (GBPA) (GBPA) (GBPA) (GBPA) (GBPA) (GBPA) (GBPA) (GBPA (GBPA) (GBPA) (GBPA) (GBPA) (GBPA"}, {"heading": "A Proof of the GBPA Regret Bound (Lemma 2.1)", "text": "Lemma A.1. The expected regret of the algorithm 2 can be written as follows: Regret = Regret (0) \u2212 Regret (0) \u2212 Overestimation Penalty + (GT) \u2212 Regret (GT) \u2212 Underestimation Penalty + T = 1D (Gt, Gt \u2212 1) Divergence Penalty. Note that since 0 (0) = 0, (GT) = (0) \u2212 (Gt) \u2212 (Gt \u2212 1) \u2212 (Gt, Gt \u2212 1) (Gt \u2212 1) (ERegret def = E \u2212 T = 1 < (Gt \u2212 1) (Gt \u2212 1) (Gt \u2212 1) (Gt \u2212 1) (Gt) (Gt \u2212 1) (Gt) (Gt \u2212 1) (Gt \u2212 1) (Gt \u2212 1), Penalty (Gt \u2212 1) (Gt \u2212 1) (Gt \u2212 1) (Gt \u2212 1)."}, {"heading": "B Relaxing Assumptions on the Distribution", "text": "B.1 Mirror trick to extend the support Let X have support on x > > 0 with density f and CDF F. Let us define Y by mirroring the density of X by zero, i.e. Y has density g (y) = 12f (| y |) and CDF G (y) = 1 2 (1 + character (y) F (| y |). Note that | Y | hazard is distributed as X and therefore E [maxi Yi] \u2264 E [max i | Yi |] = E [max i Xi].The danger hY (y) for y \u2265 0 is f (y) / (1 \u2212 F (y) and for y < 0 is f (\u2212 y) / (\u2212 F (\u2212 y) \u2264 F (\u2212 y) \u2264 F (\u2212 y) \u2264 F (\u2212 y) / (\u2212 F (\u2212 y))."}, {"heading": "C Detailed derivation of extreme value behavior", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C.1 Maximum of iid Gumbel", "text": "The CDF of the Gumbel distribution is exp (\u2212 exp (\u2212 x)) and the expected value is \u03b30, the Euler (Euler-Mascheroni) constant. Thus, the CDF of the maximum of n iid Gumbel random variables (exp (\u2212 exp (\u2212 x))) is N = exp (\u2212 exp (\u2212 (x \u2212 logN))), which is also Gumbel, but with a mean increased by logN."}, {"heading": "C.2 Maximum of iid Frechet", "text": "The Frechet CDF is exp (\u2212 x \u2212 \u03b1) and it has the mean \u0441 (\u2212 x \u2212 1\u03b1) as long as \u03b1 > 1 (otherwise it is infinite), so the CDF of the maximum of the N iid Frechet random variables (exp (\u2212 x \u2212 \u03b1)) is N = exp (\u2212 Nx \u2212 \u03b1) = exp (\u2212 (xN 1 \u03b1) \u2212 \u03b1), which is also Frechet, but with an average scaled by N1 / \u03b1."}, {"heading": "C.3 Maximum of iid Weibull", "text": "Let Xi have a modified Weibull distribution with CDF 1 \u2212 exp (\u2212 (x + 1) k + 1). Thus, for non-negative random variable X and any arbitrary u > 0 E [X] = [X > t) \u2264 NP (X1 > t) = N exp (\u2212 (t + 1) k + 1) dx. Let us assume k = 1 / m, where m \u2265 1 is a positive integer. Therefore, we have E [max i Xi] \u2264 u + 0 P (X > x) dx \u2264 u + 1) dx. Let us assume k = 1 / m, where m \u2265 1 is a positive integer. Therefore, E [max i Xi] \u2264 u + \u00b2 u N exp (\u2212 x + 1) n exp (\u2212 x + 1) k!"}, {"heading": "C.4 Maximum of iid Gamma", "text": "Then Y \u2212 dNcN follows the Gumbel distribution, where cN = \u03b2 \u2212 1 and dN = \u03b2 \u2212 1 (logN + (\u03b1 \u2212 1) log logN \u2212 logN (\u03b1)). In the language of extreme value theory, the gamma distribution belongs to the maximum attraction sphere of the Gumbel distribution with parameters (Embrechts et al., 1997). As mentioned in Section C.1, the Gumbel distribution has an average value \u03b30."}, {"heading": "C.5 Maximum of iid Pareto", "text": "Let Xi modify the Pareto distribution with CDF 1 \u2212 1 / (1 + x) \u03b1. Consequently, for the non-negative random variable X and any u > 0 E [X] = 1 \u00b0 0 P (X > x) dx \u2264 u + 2 \u00b0 u P (X > x) dx. Therefore, for \u03b1 > 1, E [max i Xi] \u2264 u + 2 \u00b0 1 \u00b0 0 P (X > x) \u03b1 dx = u + N (\u03b1 \u2212 1) (1 + u) \u03b1 \u2212 1. If we set u = N1 / \u03b1 \u2212 1, the limit E [max i Xi] \u2264 \u03b1\u03b1 \u2212 1 N1 / \u03b1 results."}, {"heading": "D Hazard Functions of Modified Distributions and the Frechet Case", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "D.1 Pareto distribution", "text": "With the conditioning trick we consider for \u03b1 > 1 (otherwise the mean is infinite) the modified Pareto distribution with pdf (x) = \u03b1 (x + 1) \u03b1 + 1, which is based on (0, \u221e). Its CDF is 1 \u2212 1 / (x + 1) \u03b1. Its hazard function is h (x) = \u03b1x + 1, which decreases in x and is limited by \u03b1. The expected maximum of N iid Pareto random variables is limited by \u03b1N1 / \u03b1 / (\u03b1 \u2212 1) (see Annex C.5)."}, {"heading": "D.2 Frechet distribution", "text": "The Frechet CDF is exp (\u2212 x \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 exp (\u2212 x \u2212 \u03b1) 1 \u2212 exp (\u2212 x \u2212 \u03b1) 1 \u2212 exp (\u2212 x \u2212 \u03b1) 1 \u2212 exp (\u2212 x \u2212 \u03b1), which is difficult to optimize analytically, but can be above the limit (for \u03b1 > 1, above elementary calculations given below by 2\u03b1. The CDF of the maximum iid Frechet random variables is exp (\u2212 x / N1 / \u03b1) \u2212 \u03b1) -\u03b1 (\u03b1) -\u03b1, which is also Frechet (but with the mean x scaled by N1 / \u03b1) with the expected value N1 / 1\u03b1 \u2212 \u2212 \u2212 \u2212 \u2212 exp (as long as \u03b1 > 1, otherwise the expectation is infinite). Thus, the regret we receive, O (\u03b1), which we have, is no longer than 2.1."}, {"heading": "D.3 Weibull distribution", "text": "Weibull's CDF is 1 \u2212 exp (\u2212 xk) for x > 0 (and otherwise 0), where k > 0 is a form parameter; the density is kxk \u2212 1 exp (\u2212 xk) and the hazard rate is kxk \u2212 1. For k > 1, the hazard rate increases monotonously and is therefore unlimited for large x; if k < 1, the hazard rate for small values is unlimited by x. Note that Weibull is exponential if k = 1.Let k = 1 / m for some positive integers m \u2265 1 and with the conditioning trick includes a modified Weibull with CDF 1 \u2212 exp (\u2212 (x + 1) k + 1) as a special case; the density is k (x + 1) k \u2212 1 exp (\u2212 1) k + 1 exp (\u2212 1) and the hazard is k (x + 1) k \u2212 1, which is limited by k."}], "references": [{"title": "Interior-point methods for full-information and bandit online learning", "author": ["Jacob Abernethy", "Elad Hazan", "Alexander Rakhlin"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Abernethy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2012}, {"title": "Online linear optimization via smoothing", "author": ["Jacob Abernethy", "Chansoo Lee", "Abhinav Sinha", "Ambuj Tewari"], "venue": "In COLT,", "citeRegEx": "Abernethy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2014}, {"title": "Minimax policies for adversarial and stochastic bandits", "author": ["Jean-Yves Audibert", "S\u00e9bastien Bubeck"], "venue": "In COLT,", "citeRegEx": "Audibert and Bubeck.,? \\Q2009\\E", "shortCiteRegEx": "Audibert and Bubeck.", "year": 2009}, {"title": "Minimax policies for combinatorial prediction games", "author": ["Jean-Yves Audibert", "S\u00e9bastien Bubeck", "G\u00e1bor Lugosi"], "venue": "In COLT,", "citeRegEx": "Audibert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2011}, {"title": "Using confidence bounds for exploitation-exploration trade-offs", "author": ["Peter Auer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Auer.,? \\Q2003\\E", "shortCiteRegEx": "Auer.", "year": 2003}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Paul Fischer"], "venue": "Machine learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire"], "venue": "SIAM Journal of Computuataion,", "citeRegEx": "Auer et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2003}, {"title": "Stochastic optimization problems with nondifferentiable cost functionals", "author": ["Dimitri P. Bertsekas"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "Bertsekas.,? \\Q1973\\E", "shortCiteRegEx": "Bertsekas.", "year": 1973}, {"title": "Regret analysis of stochastic and nonstochastic multiarmed bandit problems", "author": ["S\u00e9bastien Bubeck", "Nicolo Cesa-Bianchi"], "venue": "arXiv preprint arXiv:1204.5721,", "citeRegEx": "Bubeck and Cesa.Bianchi.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Cesa.Bianchi.", "year": 2012}, {"title": "Prediction, Learning, and Games", "author": ["Nicol\u00f2 Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Robbing the bandit: less regret in online geometric optimization against an adaptive adversary", "author": ["V. Dani", "T.P. Hayes"], "venue": "In SODA,", "citeRegEx": "Dani and Hayes.,? \\Q2006\\E", "shortCiteRegEx": "Dani and Hayes.", "year": 2006}, {"title": "The price of bandit information for online optimization", "author": ["Varsha Dani", "Thomas Hayes", "Sham Kakade"], "venue": "In NIPS,", "citeRegEx": "Dani et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dani et al\\.", "year": 2008}, {"title": "Prediction by random-walk perturbation", "author": ["Luc Devroye", "G\u00e1bor Lugosi", "Gergely Neu"], "venue": "In Conference on Learning Theory,", "citeRegEx": "Devroye et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Devroye et al\\.", "year": 2013}, {"title": "Reliability Engineering. Wiley Series in Systems", "author": ["E.A. Elsayed"], "venue": "Engineering and Management. Wiley,", "citeRegEx": "Elsayed.,? \\Q2012\\E", "shortCiteRegEx": "Elsayed.", "year": 2012}, {"title": "Modelling Extremal Events", "author": ["P. Embrechts", "C. Kl\u00fcppelberg", "T. Mikosch"], "venue": "For Insurance and Finance. Applications of mathematics. Springer,", "citeRegEx": "Embrechts et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Embrechts et al\\.", "year": 1997}, {"title": "Online convex optimization in the bandit setting: gradient descent without a gradient", "author": ["Abraham D. Flaxman", "Adam Tauman Kalai", "H. Brendan McMahan"], "venue": "In SODA,", "citeRegEx": "Flaxman et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Flaxman et al\\.", "year": 2005}, {"title": "Quantitative methods in the planning of pharmaceutical research", "author": ["John Gittins"], "venue": "Drug Information Journal,", "citeRegEx": "Gittins.,? \\Q1996\\E", "shortCiteRegEx": "Gittins.", "year": 1996}, {"title": "Multi-armed bandit allocation indices", "author": ["John Gittins", "Kevin Glazebrook", "Richard Weber"], "venue": null, "citeRegEx": "Gittins et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gittins et al\\.", "year": 2011}, {"title": "Approximation to bayes risk in repeated play", "author": ["J. Hannan"], "venue": "Contributions to the Theory of Games,", "citeRegEx": "Hannan.,? \\Q1957\\E", "shortCiteRegEx": "Hannan.", "year": 1957}, {"title": "Efficient algorithms for online decision problems", "author": ["Adam Kalai", "Santosh Vempala"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Kalai and Vempala.,? \\Q2005\\E", "shortCiteRegEx": "Kalai and Vempala.", "year": 2005}, {"title": "Efficient learning by implicit exploration in bandit problems with side observations", "author": ["Tom\u00e1\u0161 Koc\u00e1k", "Gergely Neu", "Michal Valko", "Remi Munos"], "venue": "In NIPS,", "citeRegEx": "Koc\u00e1k et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Koc\u00e1k et al\\.", "year": 2014}, {"title": "On following the perturbed leader in the bandit setting", "author": ["Jussi Kujala", "Tapio Elomaa"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Kujala and Elomaa.,? \\Q2005\\E", "shortCiteRegEx": "Kujala and Elomaa.", "year": 2005}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "Herbert Robbins"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "Lai and Robbins.,? \\Q1985\\E", "shortCiteRegEx": "Lai and Robbins.", "year": 1985}, {"title": "The weighted majority algorithm", "author": ["Nick Littlestone", "Manfred K. Warmuth"], "venue": "Information and Computation,", "citeRegEx": "Littlestone and Warmuth.,? \\Q1994\\E", "shortCiteRegEx": "Littlestone and Warmuth.", "year": 1994}, {"title": "Online geometric optimization in the bandit setting against an adaptive adversary", "author": ["H. Brendan McMahan", "Avrim Blum"], "venue": "In COLT,", "citeRegEx": "McMahan and Blum.,? \\Q2004\\E", "shortCiteRegEx": "McMahan and Blum.", "year": 2004}, {"title": "An efficient algorithm for learning with semi-bandit feedback", "author": ["Gergely Neu", "G\u00e1bor Bart\u00f3k"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Neu and Bart\u00f3k.,? \\Q2013\\E", "shortCiteRegEx": "Neu and Bart\u00f3k.", "year": 2013}, {"title": "Hyperparameter tuning in bandit-based adaptive operator selection", "author": ["Maciej Pacula", "Jason Ansel", "Saman Amarasinghe", "Una-May OReilly"], "venue": "In Applications of Evolutionary Computation,", "citeRegEx": "Pacula et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pacula et al\\.", "year": 2012}, {"title": "Sub-hessians, super-hessians and conjugation", "author": ["Jean-Paul Penot"], "venue": "Nonlinear Analysis: Theory, Methods & Applications,", "citeRegEx": "Penot.,? \\Q1994\\E", "shortCiteRegEx": "Penot.", "year": 1994}, {"title": "Relax and randomize: From value to algorithms", "author": ["Sasha Rakhlin", "Ohad Shamir", "Karthik Sridharan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Rakhlin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2012}, {"title": "Some aspects of the sequential design of experiments", "author": ["Herbert Robbins"], "venue": "Bull. Amer. Math. Soc.,", "citeRegEx": "Robbins.,? \\Q1952\\E", "shortCiteRegEx": "Robbins.", "year": 1952}, {"title": "Possible generalization of boltzmann-gibbs statistics", "author": ["Constantino Tsallis"], "venue": "Journal of Statistical Physics,", "citeRegEx": "Tsallis.,? \\Q1988\\E", "shortCiteRegEx": "Tsallis.", "year": 1988}, {"title": "Monte-carlo tree search in poker using expected reward distributions", "author": ["Guy Van den Broeck", "Kurt Driessens", "Jan Ramon"], "venue": "In Advances in Machine Learning,", "citeRegEx": "Broeck et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Broeck et al\\.", "year": 2009}, {"title": "Follow the leader with dropout perturbations", "author": ["Tim Van Erven", "Wojciech Kotlowski", "Manfred K Warmuth"], "venue": "In COLT,", "citeRegEx": "Erven et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Erven et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 16, "context": "The MAB framework is not only mathematically elegant, but useful for a wide range of applications including medical experiments design (Gittins, 1996), automated poker playing strategies (Van den Broeck et al.", "startOffset": 135, "endOffset": 150}, {"referenceID": 26, "context": ", 2009), and hyperparameter tuning (Pacula et al., 2012).", "startOffset": 35, "endOffset": 56}, {"referenceID": 17, "context": ", IID) on the loss sequence (Gittins et al., 2011; Lai and Robbins, 1985; Auer et al., 2002).", "startOffset": 28, "endOffset": 92}, {"referenceID": 22, "context": ", IID) on the loss sequence (Gittins et al., 2011; Lai and Robbins, 1985; Auer et al., 2002).", "startOffset": 28, "endOffset": 92}, {"referenceID": 5, "context": ", IID) on the loss sequence (Gittins et al., 2011; Lai and Robbins, 1985; Auer et al., 2002).", "startOffset": 28, "endOffset": 92}, {"referenceID": 23, "context": "As researchers began to establish non-stochastic, worst-case guarantees for sequential decision problems such as prediction with expert advice (Littlestone and Warmuth, 1994), a natural question arose as to whether similar guarantees were possible for the bandit setting.", "startOffset": 143, "endOffset": 174}, {"referenceID": 24, "context": "Attention later turned to the bandit version of online linear optimization, and several associated guarantees were published the following decade (McMahan and Blum, 2004; Flaxman et al., 2005; Dani and Hayes, 2006; Dani et al., 2008; Abernethy et al., 2012).", "startOffset": 146, "endOffset": 257}, {"referenceID": 15, "context": "Attention later turned to the bandit version of online linear optimization, and several associated guarantees were published the following decade (McMahan and Blum, 2004; Flaxman et al., 2005; Dani and Hayes, 2006; Dani et al., 2008; Abernethy et al., 2012).", "startOffset": 146, "endOffset": 257}, {"referenceID": 10, "context": "Attention later turned to the bandit version of online linear optimization, and several associated guarantees were published the following decade (McMahan and Blum, 2004; Flaxman et al., 2005; Dani and Hayes, 2006; Dani et al., 2008; Abernethy et al., 2012).", "startOffset": 146, "endOffset": 257}, {"referenceID": 11, "context": "Attention later turned to the bandit version of online linear optimization, and several associated guarantees were published the following decade (McMahan and Blum, 2004; Flaxman et al., 2005; Dani and Hayes, 2006; Dani et al., 2008; Abernethy et al., 2012).", "startOffset": 146, "endOffset": 257}, {"referenceID": 0, "context": "Attention later turned to the bandit version of online linear optimization, and several associated guarantees were published the following decade (McMahan and Blum, 2004; Flaxman et al., 2005; Dani and Hayes, 2006; Dani et al., 2008; Abernethy et al., 2012).", "startOffset": 146, "endOffset": 257}, {"referenceID": 15, "context": "The classic multi-armed bandit (MAB) problem, generally attributed to the early work of Robbins (1952), poses a generic online decision scenario in which an agent must make a sequence of choices from a fixed set of options.", "startOffset": 88, "endOffset": 103}, {"referenceID": 2, "context": ", 2011; Lai and Robbins, 1985; Auer et al., 2002). As researchers began to establish non-stochastic, worst-case guarantees for sequential decision problems such as prediction with expert advice (Littlestone and Warmuth, 1994), a natural question arose as to whether similar guarantees were possible for the bandit setting. The pioneering work of Auer, Cesa-Bianchi, Freund, and Schapire (2003) answered this in the affirmative by showing that their algorithm EXP3 possesses nearly-optimal regret bounds with matching lower bounds.", "startOffset": 31, "endOffset": 394}, {"referenceID": 4, "context": "For example, the EXP3 algorithm (Auer, 2003) regularizes with the entropy function and achieves a nearly optimal regret bound when K is the probability sim-", "startOffset": 32, "endOffset": 44}, {"referenceID": 0, "context": "For a general convex set, however, other regularizers such as self-concordant barrier functions (Abernethy et al., 2012) have tighter regret bounds.", "startOffset": 96, "endOffset": 120}, {"referenceID": 19, "context": "Another class of algorithms for the full information setting is Follow the Perturbed Leader (FTPL) (Kalai and Vempala, 2005) whose foundations date back to the earliest work in adversarial online learning (Hannan, 1957).", "startOffset": 99, "endOffset": 124}, {"referenceID": 18, "context": "Another class of algorithms for the full information setting is Follow the Perturbed Leader (FTPL) (Kalai and Vempala, 2005) whose foundations date back to the earliest work in adversarial online learning (Hannan, 1957).", "startOffset": 205, "endOffset": 219}, {"referenceID": 12, "context": "For every different choice of D, an entirely new set of techniques had to be developed (Devroye et al., 2013; Van Erven et al., 2014).", "startOffset": 87, "endOffset": 133}, {"referenceID": 10, "context": "For every different choice of D, an entirely new set of techniques had to be developed (Devroye et al., 2013; Van Erven et al., 2014). Rakhlin et al. (2012) and Abernethy et al.", "startOffset": 88, "endOffset": 157}, {"referenceID": 0, "context": "(2012) and Abernethy et al. (2014) made some progress towards unifying the analysis framework.", "startOffset": 11, "endOffset": 35}, {"referenceID": 2, "context": "We show that regularization via the Tsallis entropy leads to the state-of-the-art adversarial MAB algorithm, matching the minimax regret rate of Audibert and Bubeck (2009) with a tighter constant.", "startOffset": 145, "endOffset": 172}, {"referenceID": 6, "context": "Framework 1 may appear restrictive but it has served as the basis for much of the published work on adversarial MAB algorithms (Auer et al., 2003; Kujala and Elomaa, 2005; Neu and Bart\u00f3k, 2013) mainly for two reasons.", "startOffset": 127, "endOffset": 193}, {"referenceID": 21, "context": "Framework 1 may appear restrictive but it has served as the basis for much of the published work on adversarial MAB algorithms (Auer et al., 2003; Kujala and Elomaa, 2005; Neu and Bart\u00f3k, 2013) mainly for two reasons.", "startOffset": 127, "endOffset": 193}, {"referenceID": 25, "context": "Framework 1 may appear restrictive but it has served as the basis for much of the published work on adversarial MAB algorithms (Auer et al., 2003; Kujala and Elomaa, 2005; Neu and Bart\u00f3k, 2013) mainly for two reasons.", "startOffset": 127, "endOffset": 193}, {"referenceID": 1, "context": "First, the GBPA framework encompasses all FTRL and FTPL algorithms, which are the core techniques for sequential prediction algorithms (Abernethy et al., 2014).", "startOffset": 135, "endOffset": 159}, {"referenceID": 0, "context": "We study a particular algorithmic template described in Framework 1, which is a slight variation of the Gradient Based Prediction Algorithm (GBPA) of Abernethy et al. (2014). Note that the algorithm (i) maintains an unbiased estimate of the cumulative losses \u011ct, (ii) updates \u011ct by adding a singleround estimate \u011dt, and (iii) uses the gradient of a convex function \u03a6\u0303 as sampling distribution pt.", "startOffset": 150, "endOffset": 174}, {"referenceID": 0, "context": "Abernethy et al. (2014) proved that a uniform bound the norm of\u2207\u03a6\u0303 directly gives a regret guarantee for the full-information setting.", "startOffset": 0, "endOffset": 24}, {"referenceID": 3, "context": "A few years later, Audibert and Bubeck (2009) resolved this gap with Implicitly Normalized Forecaster (INF), which later was shown to be equivalent to Mirror Descent (Audibert et al., 2011) on the probability simplex.", "startOffset": 166, "endOffset": 189}, {"referenceID": 2, "context": "A few years later, Audibert and Bubeck (2009) resolved this gap with Implicitly Normalized Forecaster (INF), which later was shown to be equivalent to Mirror Descent (Audibert et al.", "startOffset": 19, "endOffset": 46}, {"referenceID": 30, "context": "Now we will replace the Shannon entropy with the Tsallis entropy1 (Tsallis, 1988), defined as:", "startOffset": 66, "endOffset": 81}, {"referenceID": 27, "context": "Following the setup of Penot (1994), \u2207S\u03b1(p) is a sub-hessian of \u015c\u03b1(p).", "startOffset": 23, "endOffset": 36}, {"referenceID": 7, "context": "By Bertsekas (1973), we can swap the order of differentiation and expectation: \u2207\u03a6\u0303(G;D) = E Z1,.", "startOffset": 3, "endOffset": 20}, {"referenceID": 20, "context": "Indeed, an early result of Kujala and Elomaa (2005) showed that a near-optimal MAB strategy comes about through the use of exponentially-distributed noise, and the same perturbation strategy has more recently been utilized in the work of Neu and Bart\u00f3k (2013) and Koc\u00e1k et al.", "startOffset": 27, "endOffset": 52}, {"referenceID": 20, "context": "Indeed, an early result of Kujala and Elomaa (2005) showed that a near-optimal MAB strategy comes about through the use of exponentially-distributed noise, and the same perturbation strategy has more recently been utilized in the work of Neu and Bart\u00f3k (2013) and Koc\u00e1k et al.", "startOffset": 27, "endOffset": 260}, {"referenceID": 20, "context": "Indeed, an early result of Kujala and Elomaa (2005) showed that a near-optimal MAB strategy comes about through the use of exponentially-distributed noise, and the same perturbation strategy has more recently been utilized in the work of Neu and Bart\u00f3k (2013) and Koc\u00e1k et al. (2014). However, a more general understanding of perturbation methods has remained elusive.", "startOffset": 264, "endOffset": 284}, {"referenceID": 19, "context": "In fact, this is equivalent to Follow the Perturbed Leader Algorithm (FTPL) (Kalai and Vempala, 2005) applied to the bandit setting.", "startOffset": 76, "endOffset": 101}, {"referenceID": 19, "context": "In fact, this is equivalent to Follow the Perturbed Leader Algorithm (FTPL) (Kalai and Vempala, 2005) applied to the bandit setting. On the other hand, the estimation step is hard because generally there is no closed-form expression for\u2207\u03a6\u0303. To address this issue, Neu and Bart\u00f3k (2013) proposed Geometric Resampling (GR).", "startOffset": 77, "endOffset": 286}, {"referenceID": 14, "context": "We derive the third column of the table in Appendix C using Extreme Value Theory (Embrechts et al., 1997).", "startOffset": 81, "endOffset": 105}], "year": 2015, "abstractText": "We define a novel family of algorithms for the adversarial multi-armed bandit problem, and provide a simple analysis technique based on convex smoothing. We prove two main results. First, we show that regularization via the Tsallis entropy, which includes EXP3 as a special case, achieves the \u0398( \u221a TN) minimax regret. Second, we show that a wide class of perturbation methods achieve a near-optimal regret as low as O( \u221a TN logN) if the perturbation distribution has a bounded hazard rate. For example, the Gumbel, Weibull, Frechet, Pareto, and Gamma distributions all satisfy this key property.", "creator": "LaTeX with hyperref package"}}}