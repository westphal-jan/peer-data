{"id": "1512.04466", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Dec-2015", "title": "Semisupervised Autoencoder for Sentiment Analysis", "abstract": "In this paper, we investigate the usage of autoencoders in modeling textual data. Traditional autoencoders suffer from at least two aspects: scalability with the high dimensionality of vocabulary size and dealing with task-irrelevant words. We address this problem by introducing supervision via the loss function of autoencoders. In particular, we first train a linear classifier on the labeled data, then define a loss for the autoencoder with the weights learned from the linear classifier. To reduce the bias brought by one single classifier, we define a posterior probability distribution on the weights of the classifier, and derive the marginalized loss of the autoencoder with Laplace approximation. We show that our choice of loss function can be rationalized from the perspective of Bregman Divergence, which justifies the soundness of our model. We evaluate the effectiveness of our model on six sentiment analysis datasets, and show that our model significantly outperforms all the competing methods with respect to classification accuracy. We also show that our model is able to take advantage of unlabeled dataset and get improved performance. We further show that our model successfully learns highly discriminative feature maps, which explains its superior performance.", "histories": [["v1", "Mon, 14 Dec 2015 19:09:53 GMT  (25kb)", "http://arxiv.org/abs/1512.04466v1", "To appear in AAAI 2016"]], "COMMENTS": "To appear in AAAI 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shuangfei zhai", "zhongfei zhang"], "accepted": true, "id": "1512.04466"}, "pdf": {"name": "1512.04466.pdf", "metadata": {"source": "CRF", "title": "Semisupervised Autoencoder for Sentiment Analysis", "authors": ["Shuangfei Zhai", "Zhongfei (Mark) Zhang"], "emails": ["szhai2@binghamton.edu", "zhongfei@cs.binghamton.edu"], "sections": [{"heading": null, "text": "ar Xiv: 151 2.04 466v 1 [cs.L G] 14 Dec 2"}, {"heading": "Introduction", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Denoising Autoencoders", "text": "In fact, it is the case that most of them are able to abide by the rules that they have imposed on themselves. (...) Most of them are able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...)"}, {"heading": "Loss Function as Bregman Divergence", "text": "To be specific, given two data points x, x and a convex function f (x) defined on Rd, the Bregman divergence of x with respect to f must be described as follows: Df (x) = f (x) + f (x) T (x). (3) The Bregman divergence measures the distance between two points x, x as a deviation between the function value f and the linear approximation from f to x. (2) The most commonly used loss functions for autoencoders are the square distance and the elementary KL divergence."}, {"heading": "Semisupervised Autoencoder with Bregman Divergence", "text": "= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "The Bayesian Marginalization", "text": "In principle, equation (4) can be applied directly as a loss function instead of the squared Euclidean distance and can form an autoencoder, but this could introduce a bias caused by a single classifier. \u2212 As a remedy, we resort to the Bayean approach, which defines a probability distribution over the following distribution patterns. Although SVM2 is not a probable classifier like logistic regression, we can avoid the idea of the energy-based model (Bengio 2009) and the use of L (\u03b8) as a negative log probability of the following distribution: p (\u03b8) = exp (\u2212 \u03b2L))). \u2212 exp (\u2212 \u03b2L)). x) d\u03b8 (9), where \u03b2 > 0 is the temperature parameter that controls the shape of the distribution p. Note that the greater size is the sharper p. In an extreme case, p (\u03b8) will be x."}, {"heading": "Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Datasets", "text": "We evaluate our model using six sentiment analysis benchmarks, the first being IMDB dataset 1 (Maas et al. 2011), which consists of IMDB film reviews; the IMDB dataset is one of the largest publicly available Sentiment Analysis datasets; it also includes an unlabeled set that allows us to evaluate semi-supervised learning methods; the remaining five datasets are all from Amazon 2 (Blitzer, Dredze and Pereira 2007), which corresponds to the ratings of five different products: books, DVDs, music, electronics, kitchen utensils; all six datasets are already labeled as Unigram or Bi-gram characteristics; for arithmetic reasons, we only select the words that appear in at least 30 training examples."}, {"heading": "Methods", "text": "In fact, it is so that it is able to hide in a position to erenie.n the aforementioned lcihsrcnlrVo"}, {"heading": "Results", "text": "First, our model consistently beats BoW by a margin, and it achieves the best results on four (larger) records out of six. On the other hand, DAE, DAE + and NN all fail to surpass BoW, even though they share the same architecture as nonlinear classifiers. This suggests that SBDAE will be able to learn a much better nonlinear function transformation function by training with a more informed goal (than that of DAE). In addition, also note that finetuning on labeled record (DAE +) significantly improves the performance of DAE, which ultimately stands on a par with the formation of a neural network with random initialization (NN). However, finetuning offers little help to SBDAE as it is already implicitly passed by labels during training. LrDrop is the second best method we have tested."}, {"heading": "Related Work and Discussion", "text": "In fact, the fact is that most of them are able to move, to move and to move."}, {"heading": "Conclusion", "text": "In this paper, we proposed a novel extension for autoencoders to learn task-specific representations of text data. We generalized traditional autoencoders by loosening their loss function compared to Bregman divergence, and then derived a discriminatory loss function from the label information. Experiments with text classification benchmarks have shown that our model performs significantly better than Bag of Words, traditional denoising autoencoders, and other competing methods. We also visualized qualitatively that our model successfully learns discriminatory features, which unattended methods fail to do."}, {"heading": "Acknowledgments", "text": "This work is partially supported by the NSF (CCF-1017828)."}, {"heading": "2011, 27-31 July 2011, John McIntyre Conference Centre, Edinburgh, UK, A meeting of SIGDAT, a Special Interest", "text": "Group of ACL, 151-161. [Srivastava et al. 2014] Srivastava, N.; Hinton, G. E.; Krizhevsky, A.; Sutskever, I.; and Salakhutdinov, R. 2014. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research 15 (1): 1929-1958. [Turney and Pantel 2010] Turney, P.; Larochelle, H.; Bengio, Y.; and Manzagol, P. 2008. Extracting and composing robust features with denoising autoencoders. (JAIR) 37: 141-188. [Vincent et al. 2008] Vincent, P.; Bengio, Y.; and Manzagol, P. 2008."}], "references": [{"title": "Clustering with bregman divergences", "author": ["Banerjee"], "venue": "In Proceedings of the Fourth SIAM International Conference on Data Mining, Lake Buena Vista, Florida,", "citeRegEx": "Banerjee,? \\Q2004\\E", "shortCiteRegEx": "Banerjee", "year": 2004}, {"title": "Latent dirichlet allocation", "author": ["Ng Blei", "D.M. Jordan 2003] Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research 3:993\u20131022", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification", "author": ["Dredze Blitzer", "J. Pereira 2007] Blitzer", "M. Dredze", "F. Pereira"], "venue": "ACL", "citeRegEx": "Blitzer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2007}, {"title": "Indexing by latent semantic analysis", "author": ["Deerwester"], "venue": "JASIS", "citeRegEx": "Deerwester,? \\Q1990\\E", "shortCiteRegEx": "Deerwester", "year": 1990}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["Bordes Glorot", "X. Bengio 2011] Glorot", "A. Bordes", "Y. Bengio"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Q.V. Mikolov 2014] Le", "T. Mikolov"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Learning word vectors for sentiment analysis. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies", "author": ["Maas"], "venue": "Proceedings of the Conference,", "citeRegEx": "Maas,? \\Q2011\\E", "shortCiteRegEx": "Maas", "year": 2011}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mikolov,? \\Q2013\\E", "shortCiteRegEx": "Mikolov", "year": 2013}, {"title": "The manifold tangent classifier", "author": ["Rifai"], "venue": "In Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems", "citeRegEx": "Rifai,? \\Q2011\\E", "shortCiteRegEx": "Rifai", "year": 2011}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["Rifai"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Rifai,? \\Q2011\\E", "shortCiteRegEx": "Rifai", "year": 2011}, {"title": "Semisupervised recursive autoencoders for predicting sentiment distributions", "author": ["Socher"], "venue": "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Socher,? \\Q2011\\E", "shortCiteRegEx": "Socher", "year": 2011}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Srivastava"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Srivastava,? \\Q2014\\E", "shortCiteRegEx": "Srivastava", "year": 2014}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Turney", "P.D. Pantel 2010] Turney", "P. Pantel"], "venue": "J. Artif. Intell. Res", "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Vincent"], "venue": "In Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML", "citeRegEx": "Vincent,? \\Q2008\\E", "shortCiteRegEx": "Vincent", "year": 2008}, {"title": "Dropout training as adaptive regularization", "author": ["Wang Wager", "S. Liang 2013] Wager", "S.I. Wang", "P. Liang"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Wager et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wager et al\\.", "year": 2013}, {"title": "Document clustering based on non-negative matrix factorization", "author": ["Liu Xu", "W. Gong 2003] Xu", "X. Liu", "Y. Gong"], "venue": "In SIGIR 2003: Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "Xu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2003}], "referenceMentions": [], "year": 2015, "abstractText": "In this paper, we investigate the usage of autoencoders in modeling textual data. Traditional autoencoders suffer from at least two aspects: scalability with the high dimensionality of vocabulary size and dealing with task-irrelevant words. We address this problem by introducing supervision via the loss function of autoencoders. In particular, we first train a linear classifier on the labeled data, then define a loss for the autoencoder with the weights learned from the linear classifier. To reduce the bias brought by one single classifier, we define a posterior probability distribution on the weights of the classifier, and derive the marginalized loss of the autoencoder with Laplace approximation. We show that our choice of loss function can be rationalized from the perspective of Bregman Divergence, which justifies the soundness of our model. We evaluate the effectiveness of our model on six sentiment analysis datasets, and show that our model significantly outperforms all the competing methods with respect to classification accuracy. We also show that our model is able to take advantage of unlabeled dataset and get improved performance. We further show that our model successfully learns highly discriminative feature maps, which explains its superior performance.", "creator": "LaTeX with hyperref package"}}}