{"id": "1512.04860", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Dec-2015", "title": "Increasing the Action Gap: New Operators for Reinforcement Learning", "abstract": "This paper introduces new optimality-preserving operators on Q-functions. We first describe an operator for tabular representations, the consistent Bellman operator, which incorporates a notion of local policy consistency. We show that this local consistency leads to an increase in the action gap at each state; increasing this gap, we argue, mitigates the undesirable effects of approximation and estimation errors on the induced greedy policies. This operator can also be applied to discretized continuous space and time problems, and we provide empirical results evidencing superior performance in this context. Extending the idea of a locally consistent operator, we then derive sufficient conditions for an operator to preserve optimality, leading to a family of operators which includes our consistent Bellman operator. As corollaries we provide a proof of optimality for Baird's advantage learning algorithm and derive other gap-increasing operators with interesting properties. We conclude with an empirical study on 60 Atari 2600 games illustrating the strong potential of these new operators.", "histories": [["v1", "Tue, 15 Dec 2015 17:13:49 GMT  (717kb,D)", "http://arxiv.org/abs/1512.04860v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["marc g bellemare", "georg ostrovski", "arthur guez", "philip s thomas", "r\u00e9mi munos"], "accepted": true, "id": "1512.04860"}, "pdf": {"name": "1512.04860.pdf", "metadata": {"source": "CRF", "title": "Increasing the Action Gap: New Operators for Reinforcement Learning", "authors": ["Marc G. Bellemare", "Georg Ostrovski", "Arthur Guez", "Philip S. Thomas", "R\u00e9mi Munos"], "emails": ["bellemare@google.com;", "ostrovski@google.com;", "aguez@google.com;", "munos@google.com;", "philipt@cs.cmu.edu"], "sections": [{"heading": null, "text": "In fact, it is as if most people are able to survive themselves, and that they feel able to survive themselves. (...) It is not as if they would survive themselves. (...) It is not as if they would survive themselves. (...) It is as if they would survive themselves. (...) It is as if they would survive themselves. (...) It is as if they would survive themselves. (...) It is as if they would survive themselves. (...) It is as if they would survive themselves. (...) It is as if they would survive themselves. (...) It is as if they would survive themselves. (...) It is as if they would survive themselves. (...) It is as if they would survive themselves. (...). (...) It is as if they would survive themselves. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (... It is. (...) It is. It is. (...) It is. (...) It is. (... It is. It is. () It is. (...) It is. (... It is. It is. (...). It is. It is. It is. It is. It is. It is. (...). It is. It is. (...). It is. It is. It is. It is. (). It is. It is. (). (... It is. It is. (). It is. It is. It is. (). It is. It is. (). It is. (). It is. (). It is. (It is. (). (). It is. It is. (). It is. (). It is. (). It is. (). It is. (). It is. (It is. (). It is. (It is."}, {"heading": "Background", "text": "We consider a Markov decision-making process M: = (X, A, P, R, Q), in which Q = Q = Q = Q-Q-Q-Q, A = finite scope for action, P is the transition probability core, R is the reward function that maps state action pairs to a limited subset of R, and Q = Q [0, 1) is the discount factor. After Q: = QX, A and V: = VX, we designate the space of limited real action functions via X \u00b7 A and X, respectively. For Q-Q, we write V: = maxaQ (x, a), and follow this convention for related quantities (V \u00b2 for Q \u00b2, V \u00b2 for Q \u00b2, etc.) whenever comfortable and unambiguous action functions have X. In the context of a specific (x, a) x \u00d7 A, we continue to write EP: = Ex \u00b2 P (x, a) in order to assign the expectation in relation to (x, the \"x,\" the next convention x, \"x,\" x)."}, {"heading": "The Consistent Bellman Operator", "text": "It is generally known (and implicitly included in our notation) that the optimal policy \u03c0 for Q = Q = Q = Q = Q = Q = Q Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q ="}, {"heading": "Aggregation Methods", "text": "At first glance, the use of an indicator function in (5) may seem limiting: Q = Q = Q = Q (x | x, a) can be zero or close to zero anywhere, or the state can be described by features that exclude meaningful identity testing. However, there is a meaningful family of value functions that have \"tabular\" properties: aggregation programs (Bertsekas 2011). As we now show, the consistent operator Bellman is well defined for all aggregation processes. An aggregation plan for M is a tuple (Z, A, D) where Z is a set of aggregate states, A is a mapping of X to distributions over Z, and D is a mapping of Z to distributions over X. For z, x \"X,\" we leave the operator ED: = = QZ \"and EA: Ez.\""}, {"heading": "Experiments on the Bicycle Domain", "text": "We are now examining the behavior of our new operators on the bicycle domain (Randlov and Alstrom 1998), in which the agent must simultaneously balance a simulated bicycle and bring it to a destination 1 km north of its starting point. Each time step consists of one hundredth of a second, with a successful episode that usually takes 50,000 or more steps. The driving aspect of this problem is particularly challenging for value-oriented methods, since each step does little to achieve ultimate success, and the \"Curse of Dimensionality\" (Bellman 1957) excludes a fine representation of state-space. In this setting, our consistent operator offers significantly improved performance and stability. We approached value functions using multilinear interpolation on a uniform basis of 10 x \u00b7 \u00b7 10 grids via a 6-dimensional vector: = (between us and others, between us, between us and us, between us, between us and us, between us and us)."}, {"heading": "A Family of Convergent Operators", "text": "Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q Q = Q = Q = Q Q Q Q = Q = Q = Q = Q Q Q = Q = Q = Q Q Q Q = Q Q Q Q = Q = Q = Q Q = Q Q = Q Q Q Q Q Q Q = Q Q Q Q = Q Q = Q = Q Q Q = Q = Q Q Q Q Q Q = Q Q Q Q Q Q = Q Q Q Q Q Q Q Q Q Q Q Q Q"}, {"heading": "Baird\u2019s Advantage Learning", "text": "The method of learning advantages was proposed by Baird (1999) to widen the gap between the optimal and sub-optimal actions in the context of residual algorithms applied to continuous time problems.4 The corresponding operator is T \u2032 Q (x, a) = K \u2212 1 [R (x, a) + \u03b3t EP V (x \u2032) + (K \u2212 1) V (x)], 3If two or more actions are optimal, we can only guarantee that one of them will ultimately be evaluated correctly. The \"1-lazy\" operator described below illustrates this possibility. 4Updating the advantage, also by Baird, is a popular but different idea, in which an actor maintains both V and A: = Q \u2212 V. It is a time constant and K: = C \u0445 t with C > 0. Taking into account that t = 1 and \u03b1: = 1 \u2212 K, we define a new operator with the same fixed point, but not an ALTQ \u2212 V, but that ALTQ = Q \u2212 x is a familiar operator."}, {"heading": "Persistent Advantage Learning", "text": "We define an operator that prefers repeated actions: TPALQ (x, a): = max {TALQ (x, a), R (x, a) + \u03b3EP Q (x \u2032, a)}. Note that the second term of the maximum can also be written as T Q (x, a) \u2212 \u03b3EP [V (x \u2032) \u2212 Q (x \u2032, a)."}, {"heading": "The Lazy Operator", "text": "As a curiosity, consider the following operator with \u03b1 [0, 1): T \u2032 Q (x, a): = Q (x, a) if Q (x, a) \u2264 T Q (x, a) and T Q (x, a) \u2264 \u03b1V (x) + (1 \u2212 \u03b1) Q (x, a), T Q (x, a) otherwise. This \u03b1-lazy operator updates Q values only if this would affect greedy politics. Nevertheless, Theorem 1 applies! Therefore, T \u2032 is optimal-preserving and gap-increasing, although there are a number of fixed points in Q. Remarkably, Theorem 1 does not apply to the 1-lazy operator, but the latter is also optimal-preserving; in this case, however, we are only guaranteed that an optimal action remains optimal."}, {"heading": "Experimental Results on Atari 2600", "text": "We evaluated our new operators on the Arcade Learning Environment (ALE; Bellemare et al. 2013), a reinforcement learning interface to Atari 2600 games. In ALE, a frame takes 1 / 60th of a second, with actions that are typically selected every four frames. Intuitively, the ALE setting is related to continuous domains such as the bicycle domain examined above, in the sense that each individual action has little impact on the game. For our evaluation, we have trained agents based on the Deep Q Network (DQN) architecture by Mnih et al. (2015). DQN operates according to a -greedy policy via a learned neural network Q function. DQN uses an experience mechanism to train this Q function to perform the gradient pedigree on the sample of queer errors. (x, a) 2, in which we learn."}, {"heading": "Open Questions", "text": "Theorem 1 and our empirical results suggest that there are many practical operators who do not maintain sub-optimal Q values. Of course, maintaining the optimum value function V itself is unnecessary as long as the iterates converge to a Q function for which arg maxa Q (x, a) = \u03c0 (x). It may well be that even weaker conditions for optimality exist than those required by Theorem 1. However, at present, our method of proof does not seem to extend to this case. Statistical efficiency of new operators. Advantage learning (as given by our redefinition) can be considered a generalization of the consistent Bellman operator when P (\u00b7 | x, a) is unknown or irrelevant. In this light, we ask: Is there a probable interpretation to the advantage of new operators? We continue to wonder incorrectly whether the statistical efficiency of the consistent Bellman operator, when this operator is less effective, is always taking into account the differences in the usual efficiency of the operator."}, {"heading": "Concluding Remarks", "text": "At the heart of our efforts was the desire to widen the action gap; we have demonstrated through experiments that this gap plays a central role in implementing greedy strategies over approximate value functions, and how significantly increased performance could be achieved by simply modifying the Bellman operator. We believe that our work highlights the inadequacy of the classical Q function in creating reliable strategies in practice, challenges the traditional policy-value relationship in value-based reinforcement learning, and illustrates how fruitful a review of the concept of value itself can be."}, {"heading": "Acknowledgments", "text": "The authors thank Michael Bowling, Csaba Szepesva \u0301 ri, Craig Boutilier, Dale Schuurmans, Marty Zinkevich, Lihong Li, Thomas Degris and Joseph Modayil for useful discussions and the anonymous reviewers for their excellent feedback."}, {"heading": "Appendix", "text": "This appendix is divided into three sections. In the first section we present the evidence of our theoretical results, in the second we present experimental details and additional results for the bicycle area. In the last section we present details of our experiments on the arcade learning environment, including the results of 60 games."}, {"heading": "Theoretical Results", "text": "Let us be an operator with the properties for all x x x, an x x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x, an x."}, {"heading": "Experimental Details: Bicycle", "text": "We used the bicycle simulator described by Randlov and Alstrom (1998) with a reward function Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q"}, {"heading": "Experimental Details: ALE", "text": "We are dealing here with a very well-functioning system in which people are able to assert their interests and to assert their interests."}, {"heading": "Parameter Selection", "text": "We used five training games (ASTERIX, BEAM RIDER, PONG, SEAQUEST, SPACE INVADERS) to select the \u03b1 parameter for our two operators. Specifically, we trained agents on the basis of our second test setup with the parameters \u03b2 {0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0}, evaluated them according to the highest score achieved and manually selected the \u03b1 value that seemed to achieve the best performance. Note that \u03b1 = 0.0 corresponds to DQN in both cases. Figure 7 shows the results of this parameter oscillation."}], "references": [{"title": "Using locally weighted regression for robot learning", "author": ["C.G. Atkeson"], "venue": "Proceedings of 1991 IEEE International Conference on Robotics and Automation, 958\u2013963.", "citeRegEx": "Atkeson,? 1991", "shortCiteRegEx": "Atkeson", "year": 1991}, {"title": "Speedy Q-learning", "author": ["M.G. Azar", "R. Munos", "M. Gavamzadeh", "H.J. Kappen"], "venue": "Advances in Neural Information Processing Systems 24.", "citeRegEx": "Azar et al\\.,? 2011", "shortCiteRegEx": "Azar et al\\.", "year": 2011}, {"title": "Reinforcement learning through gradient descent", "author": ["L.C. Baird"], "venue": "Ph.D. Dissertation, Carnegie Mellon University.", "citeRegEx": "Baird,? 1999", "shortCiteRegEx": "Baird", "year": 1999}, {"title": "The Arcade Learning Environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research 47:253\u2013279.", "citeRegEx": "Bellemare et al\\.,? 2013", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Dynamic programming", "author": ["R.E. Bellman"], "venue": "Princeton, NJ: Princeton University Press.", "citeRegEx": "Bellman,? 1957", "shortCiteRegEx": "Bellman", "year": 1957}, {"title": "Neuro-Dynamic Programming", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": "Athena Scientific.", "citeRegEx": "Bertsekas and Tsitsiklis,? 1996", "shortCiteRegEx": "Bertsekas and Tsitsiklis", "year": 1996}, {"title": "Q-learning and enhanced policy iteration in discounted dynamic programming", "author": ["D.P. Bertsekas", "H. Yu"], "venue": "Mathematics of Operations Research 37(1):66\u201394.", "citeRegEx": "Bertsekas and Yu,? 2012", "shortCiteRegEx": "Bertsekas and Yu", "year": 2012}, {"title": "Approximate policy iteration: A survey and some new methods", "author": ["D.P. Bertsekas"], "venue": "Journal of Control Theory and Applications 9(3):310\u2013335.", "citeRegEx": "Bertsekas,? 2011", "shortCiteRegEx": "Bertsekas", "year": 2011}, {"title": "PILCO: A model-based and data-efficient approach to policy search", "author": ["M.P. Deisenroth", "C.E. Rasmussen"], "venue": "Proceedings of the International Conference on Machine Learning.", "citeRegEx": "Deisenroth and Rasmussen,? 2011", "shortCiteRegEx": "Deisenroth and Rasmussen", "year": 2011}, {"title": "Tree-based batch mode reinforcement learning", "author": ["D. Ernst", "P. Geurts", "L. Wehenkel"], "venue": "Journal of Machine Learning Research 6:503\u2013556.", "citeRegEx": "Ernst et al\\.,? 2005", "shortCiteRegEx": "Ernst et al\\.", "year": 2005}, {"title": "Action-gap phenomenon in reinforcement learning", "author": ["A. Farahmand"], "venue": "Advances in Neural Information Processing Systems 24.", "citeRegEx": "Farahmand,? 2011", "shortCiteRegEx": "Farahmand", "year": 2011}, {"title": "Stable function approximation in dynamic programming", "author": ["G. Gordon"], "venue": "Proceedings of the Twelfth International Conference on Machine Learning.", "citeRegEx": "Gordon,? 1995", "shortCiteRegEx": "Gordon", "year": 1995}, {"title": "System identification of post stall aerodynamics for UAV perching", "author": ["W. Hoburg", "R. Tedrake"], "venue": "Proceedings of the AIAA Infotech Aerospace Conference.", "citeRegEx": "Hoburg and Tedrake,? 2009", "shortCiteRegEx": "Hoburg and Tedrake", "year": 2009}, {"title": "Optimal hour ahead bidding in the real time electricity market with battery storage using approximate dynamic programming", "author": ["D.R. Jiang", "W.B. Powell"], "venue": "INFORMS Journal on Computing 27(3):525 \u2013 543.", "citeRegEx": "Jiang and Powell,? 2015", "shortCiteRegEx": "Jiang and Powell", "year": 2015}, {"title": "Numerical methods for stochastic control problems in continuous time", "author": ["H. Kushner", "P.G. Dupuis"], "venue": "Springer.", "citeRegEx": "Kushner and Dupuis,? 2001", "shortCiteRegEx": "Kushner and Dupuis", "year": 2001}, {"title": "Biascorrected Q-learning to control max-operator bias in Qlearning", "author": ["D. Lee", "B. Defourny", "W.B. Powell"], "venue": "Symposium on Adaptive Dynamic Programming And Reinforcement Learning.", "citeRegEx": "Lee et al\\.,? 2013", "shortCiteRegEx": "Lee et al\\.", "year": 2013}, {"title": "Humanlevel control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G Ostrovski"], "venue": "Nature", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Barycentric interpolators for continuous space & time reinforcement learning", "author": ["R. Munos", "A. Moore"], "venue": "Advances in Neural Information Processing Systems 11.", "citeRegEx": "Munos and Moore,? 1998", "shortCiteRegEx": "Munos and Moore", "year": 1998}, {"title": "Variable resolution discretization in optimal control", "author": ["R. Munos", "A. Moore"], "venue": "Machine learning 49(23):291\u2013323.", "citeRegEx": "Munos and Moore,? 2002", "shortCiteRegEx": "Munos and Moore", "year": 2002}, {"title": "Kernel-based reinforcement learning", "author": ["D. Ormoneit", "\u015a. Sen"], "venue": "Machine learning 49(2-3):161\u2013178.", "citeRegEx": "Ormoneit and Sen,? 2002", "shortCiteRegEx": "Ormoneit and Sen", "year": 2002}, {"title": "Learning to drive a bicycle using reinforcement learning and shaping", "author": ["J. Randlov", "P. Alstrom"], "venue": "Proceedings of the Fifteenth International Conference on Machine Learning.", "citeRegEx": "Randlov and Alstrom,? 1998", "shortCiteRegEx": "Randlov and Alstrom", "year": 1998}, {"title": "Reinforcement learning for robot soccer", "author": ["M. Riedmiller", "T. Gabel", "R. Hafner", "S. Lange"], "venue": "Autonomous Robots 27(1):55\u201373.", "citeRegEx": "Riedmiller et al\\.,? 2009", "shortCiteRegEx": "Riedmiller et al\\.", "year": 2009}, {"title": "On-line Qlearning using connectionist systems", "author": ["G.A. Rummery", "M. Niranjan"], "venue": "Technical report, Cambridge University Engineering Department.", "citeRegEx": "Rummery and Niranjan,? 1994", "shortCiteRegEx": "Rummery and Niranjan", "year": 1994}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT Press.", "citeRegEx": "Sutton and Barto,? 1998", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction", "author": ["R. Sutton", "J. Modayil", "M. Delp", "T. Degris", "P. Pilarski", "A. White", "D. Precup"], "venue": "Proceedings of the Tenth International Conference on Autonomous Agents and Multiagents", "citeRegEx": "Sutton et al\\.,? 2011", "shortCiteRegEx": "Sutton et al\\.", "year": 2011}, {"title": "Learning to predict by the methods of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine Learning 3(1):9\u201344.", "citeRegEx": "Sutton,? 1988", "shortCiteRegEx": "Sutton", "year": 1988}, {"title": "Generalization in reinforcement learning: Successful examples using sparse coarse coding", "author": ["R.S. Sutton"], "venue": "Advances in Neural Information Processing Systems 8, 1038\u2013 1044.", "citeRegEx": "Sutton,? 1996", "shortCiteRegEx": "Sutton", "year": 1996}, {"title": "Temporal difference learning and TDGammon", "author": ["G. Tesauro"], "venue": "Communications of the ACM 38(3).", "citeRegEx": "Tesauro,? 1995", "shortCiteRegEx": "Tesauro", "year": 1995}, {"title": "Deep reinforcement learning with double Q-learning", "author": ["H. van Hasselt", "A. Guez", "D. Silver"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (to appear)", "citeRegEx": "Hasselt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2016}, {"title": "Compress and control", "author": ["J. Veness", "M.G. Bellemare", "M. Hutter", "A. Chua", "G. Desjardins"], "venue": "Proceedings of the AAAI Conference on Artificial Intelligence.", "citeRegEx": "Veness et al\\.,? 2015", "shortCiteRegEx": "Veness et al\\.", "year": 2015}, {"title": "Learning From Delayed Rewards", "author": ["C. Watkins"], "venue": "Ph.D. Dissertation, Cambridge University, Cambridge, England.", "citeRegEx": "Watkins,? 1989", "shortCiteRegEx": "Watkins", "year": 1989}], "referenceMentions": [{"referenceID": 4, "context": "In particular, value iteration (Bellman 1957) directly computes the value function through the iterated evaluation of Bellman\u2019s equation, either exactly or from samples (e.", "startOffset": 31, "endOffset": 45}, {"referenceID": 10, "context": "Increasing the action gap is advantageous in the presence of approximation or estimation error (Farahmand 2011), and may be crucial for systems operating at a fine time scale such as video games (Togelius et al.", "startOffset": 95, "endOffset": 111}, {"referenceID": 3, "context": "Increasing the action gap is advantageous in the presence of approximation or estimation error (Farahmand 2011), and may be crucial for systems operating at a fine time scale such as video games (Togelius et al. 2009; Bellemare et al. 2013), real-time markets (Jiang and Powell 2015), and robotic platforms (Riedmiller et al.", "startOffset": 195, "endOffset": 240}, {"referenceID": 13, "context": "2013), real-time markets (Jiang and Powell 2015), and robotic platforms (Riedmiller et al.", "startOffset": 25, "endOffset": 48}, {"referenceID": 21, "context": "2013), real-time markets (Jiang and Powell 2015), and robotic platforms (Riedmiller et al. 2009; Hoburg and Tedrake 2009; Deisenroth and Rasmussen 2011; Sutton et al. 2011).", "startOffset": 72, "endOffset": 172}, {"referenceID": 12, "context": "2013), real-time markets (Jiang and Powell 2015), and robotic platforms (Riedmiller et al. 2009; Hoburg and Tedrake 2009; Deisenroth and Rasmussen 2011; Sutton et al. 2011).", "startOffset": 72, "endOffset": 172}, {"referenceID": 8, "context": "2013), real-time markets (Jiang and Powell 2015), and robotic platforms (Riedmiller et al. 2009; Hoburg and Tedrake 2009; Deisenroth and Rasmussen 2011; Sutton et al. 2011).", "startOffset": 72, "endOffset": 172}, {"referenceID": 24, "context": "2013), real-time markets (Jiang and Powell 2015), and robotic platforms (Riedmiller et al. 2009; Hoburg and Tedrake 2009; Deisenroth and Rasmussen 2011; Sutton et al. 2011).", "startOffset": 72, "endOffset": 172}, {"referenceID": 2, "context": "In fact, the idea of devaluating suboptimal actions underpins Baird\u2019s advantage learning (Baird 1999), designed for continuous time control, and occurs naturally when considering the discretized solution of continuous time and space MDPs (e.", "startOffset": 89, "endOffset": 101}, {"referenceID": 14, "context": "Munos and Moore 1998; 2002), whose limit is the Hamilton-Jacobi-Bellman equation (Kushner and Dupuis 2001).", "startOffset": 81, "endOffset": 106}, {"referenceID": 20, "context": "Our empirical results on the bicycle domain (Randlov and Alstrom 1998) show a marked increase in performance from using the consistent Bellman operator.", "startOffset": 44, "endOffset": 70}, {"referenceID": 6, "context": "While numerous alternatives to the Bellman operator have been put forward (e.g. recently Azar et al. 2011; Bertsekas and Yu 2012), we believe our work to be the first to propose such a major departure from the canonical fixed-point condition required from an optimality-preserving operator.", "startOffset": 74, "endOffset": 129}, {"referenceID": 3, "context": "results on the Arcade Learning Environment (Bellemare et al. 2013).", "startOffset": 43, "endOffset": 66}, {"referenceID": 3, "context": "results on the Arcade Learning Environment (Bellemare et al. 2013). We consider the Deep Q-Network (DQN) architecture of Mnih et al. (2015), replacing only its learning rule with one of our operators.", "startOffset": 44, "endOffset": 140}, {"referenceID": 5, "context": "T is a contraction mapping in supremum norm (Bertsekas and Tsitsiklis 1996) whose unique fixed point is the optimal Q-function", "startOffset": 44, "endOffset": 75}, {"referenceID": 22, "context": "We emphasize that while we focus on the Bellman operator, our results easily extend to its variations such as SARSA (Rummery and Niranjan 1994), policy evaluation (Sutton 1988), and fitted Q-iteration (Ernst, Geurts, and Wehenkel 2005).", "startOffset": 116, "endOffset": 143}, {"referenceID": 25, "context": "We emphasize that while we focus on the Bellman operator, our results easily extend to its variations such as SARSA (Rummery and Niranjan 1994), policy evaluation (Sutton 1988), and fitted Q-iteration (Ernst, Geurts, and Wehenkel 2005).", "startOffset": 163, "endOffset": 176}, {"referenceID": 4, "context": "We emphasize that while we focus on the Bellman operator, our results easily extend to its variations such as SARSA (Rummery and Niranjan 1994), policy evaluation (Sutton 1988), and fitted Q-iteration (Ernst, Geurts, and Wehenkel 2005). In particular, our new operators all have a sample-based form, i.e., an analogue to the Q-Learning rule of Watkins (1989). a1", "startOffset": 40, "endOffset": 359}, {"referenceID": 7, "context": "There is, however, one important family of value functions which have \u201ctabular-like\u201d properties: aggregation schemes (Bertsekas 2011).", "startOffset": 117, "endOffset": 133}, {"referenceID": 19, "context": ", multilinear interpolation, illustrated in Figure 2) and kernel-based methods (Ormoneit and Sen 2002).", "startOffset": 79, "endOffset": 102}, {"referenceID": 23, "context": "If A also corresponds to the identity and X is finite, TA reduces to the Bellman operator (1) and we recover the familiar tabular representation (Sutton and Barto 1998).", "startOffset": 145, "endOffset": 168}, {"referenceID": 20, "context": "We now study the behaviour of our new operators on the bicycle domain (Randlov and Alstrom 1998).", "startOffset": 70, "endOffset": 96}, {"referenceID": 4, "context": "The driving aspect of this problem is particularly challenging for value-based methods, since each step contributes little to an eventual success and the \u201ccurse of dimensionality\u201d (Bellman 1957) precludes a fine representation of the state-space.", "startOffset": 180, "endOffset": 194}, {"referenceID": 4, "context": "Policies derived from our consistent operator can safely balance the bicycle earlier on, and also reach the goal earlier than policies derived from the Bellman operator. Note, in particular, the striking difference in the trajectories followed by the resulting policies. The effect is even more pronounced when using a 8 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 8 grid (results provided in the appendix). Effectively, by decreasing suboptimal Q-values at grid points we produce much better policies within the grid cells. This phenomenon is consistent with the theoretical results of Farahmand (2011) relating the size of action gaps to the quality of derived greedy policies.", "startOffset": 152, "endOffset": 570}, {"referenceID": 26, "context": "One may ask whether it is possible to extend the consistent Bellman operator to Q-value approximation schemes which lack a probabilistic interpretation, such as linear approximation (Sutton 1996), locally weighted regression (Atkeson 1991), neural networks (Tesauro 1995), or even informationtheoretic methods (Veness et al.", "startOffset": 182, "endOffset": 195}, {"referenceID": 0, "context": "One may ask whether it is possible to extend the consistent Bellman operator to Q-value approximation schemes which lack a probabilistic interpretation, such as linear approximation (Sutton 1996), locally weighted regression (Atkeson 1991), neural networks (Tesauro 1995), or even informationtheoretic methods (Veness et al.", "startOffset": 225, "endOffset": 239}, {"referenceID": 27, "context": "One may ask whether it is possible to extend the consistent Bellman operator to Q-value approximation schemes which lack a probabilistic interpretation, such as linear approximation (Sutton 1996), locally weighted regression (Atkeson 1991), neural networks (Tesauro 1995), or even informationtheoretic methods (Veness et al.", "startOffset": 257, "endOffset": 271}, {"referenceID": 29, "context": "One may ask whether it is possible to extend the consistent Bellman operator to Q-value approximation schemes which lack a probabilistic interpretation, such as linear approximation (Sutton 1996), locally weighted regression (Atkeson 1991), neural networks (Tesauro 1995), or even informationtheoretic methods (Veness et al. 2015).", "startOffset": 310, "endOffset": 330}, {"referenceID": 2, "context": "Theorem 1 is our main result; one corollary is a convergence proof for Baird\u2019s advantage learning (Baird 1999).", "startOffset": 98, "endOffset": 110}, {"referenceID": 2, "context": "The method of advantage learning was proposed by Baird (1999) as a means of increasing the gap between the optimal and suboptimal actions in the context of residual algorithms applied to continuous time problems.", "startOffset": 49, "endOffset": 62}, {"referenceID": 3, "context": "We evaluated our new operators on the Arcade Learning Environment (ALE; Bellemare et al. 2013), a reinforcement learning interface to Atari 2600 games.", "startOffset": 66, "endOffset": 94}, {"referenceID": 3, "context": "We evaluated our new operators on the Arcade Learning Environment (ALE; Bellemare et al. 2013), a reinforcement learning interface to Atari 2600 games. In the ALE, a frame lasts 1/60 of a second, with actions typically selected every four frames. Intuitively, the ALE setting is related to continuous domains such as the bicycle domain studied above, in the sense that each individual action has little effect on the game. For our evaluation, we trained agents based on the Deep Q-Network (DQN) architecture of Mnih et al. (2015). DQN acts according to an -greedy policy over a learned neuralnetwork Q-function.", "startOffset": 72, "endOffset": 530}, {"referenceID": 16, "context": "For comparison, we also trained agents using the Original DQN setting (Mnih et al. 2015), in particular using a longer 200 million frames of training.", "startOffset": 70, "endOffset": 88}, {"referenceID": 4, "context": "We trained each agent for 100 million frames using either regular Bellman updates, advantage learning (A.L.), or persistent advantage learning (P.A.L.). We optimized the \u03b1 parameters over 5 training games and tested our algorithms on 55 more games using 10 independent trials each. For each game, we performed a paired t-test (99% C.I.) on the post-training evaluation scores obtained by our algorithms and DQN. A.L. and P.A.L. are statistically better than DQN on 37 and 35 out of 60 games, respectively; both perform worse on one (ATLANTIS, JAMES BOND). P.A.L. often achieves higher scores than A.L., and is statistically better on 16 games and worse on 6. These results are especially remarkable given that the only difference between DQN and our operators is a simple modification to the update rule. For comparison, we also trained agents using the Original DQN setting (Mnih et al. 2015), in particular using a longer 200 million frames of training. Figure 4 depicts learning curves for two games, ASTERIX and SPACE INVADERS. These curves are representative of our results, rather than exceptional: on most games, advantage learning outperforms Bellman updates, and persistent advantage learning further improves on this result. Across games, the median score improvement over DQN is 8.4% for A.L. and 9.1% for P.A.L., while the average score improvement is respectively 27.0% and 32.5%. Full experimental details are provided in the appendix. The learning curve for ASTERIX illustrates the poor performance of DQN on certain games. Recently, van Hasselt, Guez, and Silver (2016) argued that this poor performance", "startOffset": 66, "endOffset": 1586}, {"referenceID": 1, "context": "observed a similar effect on the value estimates when replacing the Bellman updates with Double Q-Learning updates, one of many solutions recently proposed to mitigate the negative impact of statistical bias in value function estimation (van Hasselt 2010; Azar et al. 2011; Lee, Defourny, and Powell 2013).", "startOffset": 237, "endOffset": 305}], "year": 2015, "abstractText": "This paper introduces new optimality-preserving operators on Q-functions. We first describe an operator for tabular representations, the consistent Bellman operator, which incorporates a notion of local policy consistency. We show that this local consistency leads to an increase in the action gap at each state; increasing this gap, we argue, mitigates the undesirable effects of approximation and estimation errors on the induced greedy policies. This operator can also be applied to discretized continuous space and time problems, and we provide empirical results evidencing superior performance in this context. Extending the idea of a locally consistent operator, we then derive sufficient conditions for an operator to preserve optimality, leading to a family of operators which includes our consistent Bellman operator. As corollaries we provide a proof of optimality for Baird\u2019s advantage learning algorithm and derive other gap-increasing operators with interesting properties. We conclude with an empirical study on 60 Atari 2600 games illustrating the strong potential of these new operators. Value-based reinforcement learning is an attractive solution to planning problems in environments with unknown, unstructured dynamics. In its canonical form, value-based reinforcement learning produces successive refinements of an initial value function through repeated application of a convergent operator. In particular, value iteration (Bellman 1957) directly computes the value function through the iterated evaluation of Bellman\u2019s equation, either exactly or from samples (e.g. Q-Learning, Watkins 1989). In its simplest form, value iteration begins with an initial value function V0 and successively computes Vk+1 := T Vk, where T is the Bellman operator. When the environment dynamics are unknown, Vk is typically replaced by Qk, the state-action value function, and T is approximated by an empirical Bellman operator. The fixed point of the Bellman operator, Q\u2217, is the optimal state-action value function or optimal Q-function, from which an optimal policy \u03c0\u2217 can be recovered. In this paper we argue that the optimal Q-function is inconsistent, in the sense that for any action a which is subop\u2217Now at Carnegie Mellon University. Copyright c \u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. timal in state x, Bellman\u2019s equation for Q\u2217(x, a) describes the value of a nonstationary policy: upon returning to x, this policy selects \u03c0\u2217(x) rather than a. While preserving global consistency appears impractical, we propose a simple modification to the Bellman operator which provides us a with a first-order solution to the inconsistency problem. Accordingly, we call our new operator the consistent Bellman operator. We show that the consistent Bellman operator generally devalues suboptimal actions but preserves the set of optimal policies. As a result, the action gap \u2013 the value difference between optimal and second best actions \u2013 increases. Increasing the action gap is advantageous in the presence of approximation or estimation error (Farahmand 2011), and may be crucial for systems operating at a fine time scale such as video games (Togelius et al. 2009; Bellemare et al. 2013), real-time markets (Jiang and Powell 2015), and robotic platforms (Riedmiller et al. 2009; Hoburg and Tedrake 2009; Deisenroth and Rasmussen 2011; Sutton et al. 2011). In fact, the idea of devaluating suboptimal actions underpins Baird\u2019s advantage learning (Baird 1999), designed for continuous time control, and occurs naturally when considering the discretized solution of continuous time and space MDPs (e.g. Munos and Moore 1998; 2002), whose limit is the Hamilton-Jacobi-Bellman equation (Kushner and Dupuis 2001). Our empirical results on the bicycle domain (Randlov and Alstrom 1998) show a marked increase in performance from using the consistent Bellman operator. In the second half of this paper we derive novel sufficient conditions for an operator to preserve optimality. The relative weakness of these new conditions reveal that it is possible to deviate significantly from the Bellman operator without sacrificing optimality: an optimality-preserving operator needs not be contractive, nor even guarantee convergence of the Q-values for suboptimal actions. While numerous alternatives to the Bellman operator have been put forward (e.g. recently Azar et al. 2011; Bertsekas and Yu 2012), we believe our work to be the first to propose such a major departure from the canonical fixed-point condition required from an optimality-preserving operator. As proof of the richness of this new operator family we describe a few practical instantiations with unique properties. We use our operators to obtain state-of-the-art empirical ar X iv :1 51 2. 04 86 0v 1 [ cs .A I] 1 5 D ec 2 01 5 results on the Arcade Learning Environment (Bellemare et al. 2013). We consider the Deep Q-Network (DQN) architecture of Mnih et al. (2015), replacing only its learning rule with one of our operators. Remarkably, this one-line change produces agents that significantly outperform the original DQN. Our work, we believe, demonstrates the potential impact of rethinking the core components of value-based reinforcement learning.", "creator": "TeX"}}}