{"id": "1512.04906", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Dec-2015", "title": "Strategies for Training Large Vocabulary Neural Language Models", "abstract": "Training neural network language models over large vocabularies is still computationally very costly compared to count-based models such as Kneser-Ney. At the same time, neural language models are gaining popularity for many applications such as speech recognition and machine translation whose success depends on scalability. We present a systematic comparison of strategies to represent and train large vocabularies, including softmax, hierarchical softmax, target sampling, noise contrastive estimation and self normalization. We further extend self normalization to be a proper estimator of likelihood and introduce an efficient variant of softmax. We evaluate each method on three popular benchmarks, examining performance on rare words, the speed/accuracy trade-off and complementarity to Kneser-Ney.", "histories": [["v1", "Tue, 15 Dec 2015 19:29:01 GMT  (252kb,D)", "http://arxiv.org/abs/1512.04906v1", "12 pages; journal paper; under review"]], "COMMENTS": "12 pages; journal paper; under review", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["wenlin chen", "david grangier", "michael auli"], "accepted": true, "id": "1512.04906"}, "pdf": {"name": "1512.04906.pdf", "metadata": {"source": "CRF", "title": "Strategies for Training Large Vocabulary Neural Language Models", "authors": ["Wenlin Chen", "David Grangier", "Michael Auli"], "emails": ["wenlinchen@wustl.edu", "grangier@fb.com", "michaelauli@fb.com"], "sections": [{"heading": null, "text": "At the same time, neural language models are gaining popularity for many applications such as speech recognition and machine translation, whose success depends on scalability. We present a systematic comparison of strategies for displaying and developing large vocabulary, including Softmax, Hierarchical Softmax, Target Scanning, Contrasting Noise Estimation, and Self-Normalization. We expand Self-Normalization further to be a reasonable estimate of probability, and introduce an efficient variant of Softmax. We evaluate each method against three popular benchmarks, examining the performance of rare words, the trade-off between speed and accuracy, and complementarity with Kneser-Ney."}, {"heading": "1 Introduction", "text": "We have gained popularity for tasks such as automatic speech recognition (Arisoy et al., 2012) and statistical machine translation (Schwenk et al., 2012; Vaswani et al., 2013). Furthermore, models similar in architecture to neural language models have been proposed for translation (Le et al., 2012; Devlin et al., 2014; Bahdanau et al., 2015) and linguistic generation (Sordoni et al., 2015). \u00b2 Work, while Wenlin assigns a probability to an internal model on Facebook.Language models for a word that has preceded, and possibly subsequent words. The model architecture determines how the context is represented and there are several choices, including recurrent neural networks (Mikolov et al., 2010), or log-bilinear models (Mnih and Hinrich, 2010)."}, {"heading": "2 Modeling Large Vocabularies", "text": "We first present our basic language model architecture with a classic Softmax and then describe various other methods, including a novel variant of Softmax."}, {"heading": "2.1 Softmax Neural Language Model", "text": "It is not only a matter of time, but also a matter of time until a solution is found. (...) It is a matter of time before a solution is found. (...) It is a matter of time before a solution is found. (...) It is a matter of time before a solution is found. (...) It is a matter of time before a solution is found. (...) It is a matter of time before a solution is found. (...) It is a matter of time before a solution is found. (...) It is a matter of time before a solution is found. (...) It is a matter of time before a solution is found. (...) It is a matter of time before a solution is found. (...) It is a matter of time before a solution is found. (...) It is a matter of time until a solution is found. (...) It is a matter of time before a solution is found. (...) It is a matter of time before a solution is found. (...) It is a matter of time before a solution is found."}, {"heading": "2.2 Hierarchical Softmax", "text": "The hierarchical Softmax (HSM) divides the output vocabulary into a tree in which the leaves are the words and the internodes are latent variables or classes (Morin and Bengio, 2005). The tree has potentially many levels and there is a unique path from the root to each word. The probability of a word is the product of the probabilities of the latent variables along the path from the root to the leaf, including the probability of the leaf. If the tree is perfectly balanced, this can reduce the complexity from O (V) to O (log V). We are experimenting with a version that follows Goodman (2001) and was used in Mikolov et al. (2011b). Goodman proposed a two-tiered tree that predicts first the class of the next word ct and then the actual word wt given word wt (n) to O (log V)."}, {"heading": "2.3 Differentiated Softmax", "text": "This section introduces a novel variation of softmax that allocates variable capacity per word in the output layer. The weight matrix of the last layer W k + 1 \u0445 Rdk \u00b7 V stores output embeddings of the size dk for the V-words that the language model can predict: W k + 11;..; W k + 1 V. Differentiated Softmax (DSoftmax) varies the dimension of output embeddings dk between words depending on how much model capacity is considered appropriate for a particular word. In particular, it makes sense to assign more parameters to frequent words than to rare words. By definition, frequent words in the training data occur more than ten times as rare words and thus allow to adjust more parameters. In particular, we define partitions of the output word based on word frequency and the words in each partition share the same embedsize. For example, we can specify the frequency of ordered group of output partitions {as IDs = 1."}, {"heading": "2.4 Target Sampling", "text": "Sample-based methods approach Softmax normalization (Eq.2) by selecting a number of stackers rather than using all the results, which can greatly speed up each training repetition depending on the size of the stacker. We follow Jean et al. (2014), who, as stackers, select all the positive examples in a mini-stack as well as a subset of the remaining words. This subset is uniformly scanned and its size is cross-validated. One disadvantage of the sample is that the (sampled) final weight matrix W k + 1 (Eq.1) constantly changes between mini-stacks. This is mathematically costly, and the success of the sample depends on estimating a good model and keeping the number of samples small."}, {"heading": "2.5 Noise Contrastive Estimation", "text": "Another sample-based method (Hyva \ufffd rinen, 2010; Mnih and Teh, 2012) is Contrastive Noise Estimation (NCE), which, unlike target sampling, does not directly maximize the probability of training data. Instead, it solves a two-tier problem of distinguishing real data from sound samples. The training algorithm randomly selects a word w against the preceding context x from a mixture P (w | x) = 1 k + 1 Ptrain (w | x) + k + 1 Pnoise (w | x) wherePtrain is the empirical distribution of the training set, and Pnoise is a known sound distribution that is typically a context-independent unigram distribution that fits the training set. The training algorithm matches the P (w | x) model to determine whether a mixed sample comes from the data or the sound distribution."}, {"heading": "2.6 Infrequent Normalization", "text": "Andreas and Klein (2015) also propose to relax the normalization of the score. Their strategy (here referred to as WeaknormSQ) combines unnormalized probability maximization with a punitive term that favors normalized predictions, resulting in the following loss compared to the training set TL (2) \u03b1 = \u2212 \u2211 (w, x) \u2022 Ts (w | x) + \u03b1 (w, x) \u2022 T (logZ (x) \u2022 2where s (w | x) refers to the unnormalized score of the word w in the given context x and Z (x) = \u2211 w exp (s (w | x))) refers to the partition function for the context x. For efficient training, the second term can be scanned downwards: L (2) \u03b1, \u03b3 (w, x) \u2022 move s (w | x) = negative move (w, x)."}, {"heading": "2.7 Other Methods", "text": "Fast local hashing was used to approximate the Dot product between the final activation of the hidden layer hk and the output word embedding (Vijayanarasimhan et al., 2014). However, during the training, a high effort for re-indexing the embedding and test time acceleration virtually disappeared as the batch size increased due to the efficiency of matrix matrix products."}, {"heading": "3 Experimental Setup", "text": "It is the time in which we set out to find what surrounds us, according to what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us, what drives us"}, {"heading": "4 Results", "text": "Looking at the test results (Table 2) and learning paths on the validation sets (Figures 2, 3 and 4), one can see a clear trend: softmax's competitiveness decreases with vocabulary size. Softmax performs very well with the small vocabulary Penn Treebank corpus, but very poorly with the larger vocabulary BillionW corpus. Faster methods such as sampling, hierarchical Softmax and rare normalization (Weaknorm and WeaknormalSQ) are very good with the large vocabulary of BillionW.D Softmax corpus. D-Softmax performs very well with all datasets and shows that assigning higher capacity where it benefits most leads to better models. Target sampling performs worse than Softmax with the gigaword, but better with BillionW. Hierarchical Softmax methods perform very poorly with Penn Treebank, which contrasts sharply with BillionW, where it performs very well."}, {"heading": "4.1 Softmax", "text": "Although Softmax is our starting point, it is one of the most accurate methods on the PTB and is the second best method on the Gigaword after D-Softmax (with a similar performance from WeaknormSQ). For BillionW, the extremely large vocabulary makes Softmax training too slow to compete with faster alternatives. Softmax, however, has the easiest implementation of all methods and no additional hyperparameters compared to other methods."}, {"heading": "4.2 Target Sampling", "text": "Figure 5 shows that the target scanning is most accurate when the distractor set represents a large part of the vocabulary, i.e. more than 30% on gigaword (the best setting is even higher at 50%).The target scanning is asymptotically faster and therefore performs more iterations than Softmax over the same time. However, it makes less progress in terms of per-iteration perplexity reduction compared to Softmax. Overall, it is not much better than Softmax. One reason might be that the scanning method selects distractors regardless of context or current model performance, which does not favor the scanning of distractors that the model erroneously considers probable in the current context. These distractors would yield a high gradient that could accelerate the model's progress."}, {"heading": "4.3 Hierarchical Softmax", "text": "Hierarchical softmax is very efficient for large vocabulary and it is the best method on billionW. On the other hand, HSM intersects with small vocabulary as on Penn Treebank. We found that a good word cluster structure helps with learning: If each cluster contains words that occur in similar contexts, the cluster probability is easier to learn. If the cluster structure is uninformative, cluster probabilities converge with the even distribution. This has a negative effect on accuracy, since words can never have a higher probability than their clusters (see Equation 3). Our experiments group words into a two-step hierarchy and compare four cluster strategies over billionW and gigaword (\u00a7 2.2). Random cluster formation mixes the vocabulary and divides it into equally large partitions. Frequency-based cluster formation initially means that the frequency of cluster formation is based on the frequency of cluster formation and each cluster has the same percentage of cluster formation, representing one cluster word."}, {"heading": "4.4 Differentiated Softmax", "text": "D-Softmax is the best technology on Gigaword and the second best on BillionW after HSM. At the PTB, it is one of the best techniques whose helplessness cannot be reliably distinguished. In contrast to hierarchical Softmax, NCE or Weaknorm, D-Softmax can assign large embeddings to frequent words, while the computing complexity remains manageable by small embeddings of rare words. In contrast to hierarchical Softmax, NCE or Weaknorm, the computing advantage of D-Softmax remains at the test date (Table 3). D-Softmax is the fastest technology at the test date and is one of the most accurate methods. This speed advantage is due to the low dimensional representation of rare words that negatively affects the model accuracy of these words (Table 5)."}, {"heading": "4.5 Noise Contrastive Estimation", "text": "In order to work with large neural networks and large vocabularies, we had to separate the number of noise samples from the noise ratio data in the modelled mix. For example, a data noise ratio of 1 / 50 provides good performance in our experiments, but estimating only 50 noise samples per data point is wasteful given the cost of network evaluation. Furthermore, this setting does not allow for frequent sampling of every word in a large vocabulary. Our setting takes more noise samples into account and evaluates the data sample, making it possible to determine the data noise ratio independently of the number of noise samples. Overall, the NCE results are better than Softmax only for billions of W, a setting for which Softmax is very slow due to its very large vocabulary. Why does NCE perform so poorly? Figure 6 shows entropy on the validation quantity compared to the NCE loss for several models. The results clearly show that similar NCE values are not very useful under other validation techniques."}, {"heading": "4.6 Infrequent Normalization", "text": "Rare normalization (Weaknorm and WeaknormSQ) is better than softmax at billionW and comparable to softmax at Penn Treebank and Gigaword (Table 2). Acceleration by skipping partition function calculations is significant, with WeaknormSQ evaluating the partition at billionW only at 10% of the examples. In one week, the model is evaluated and updated to 868M tokens (with 86.8M partition ratings), compared to 156M tokens for softmax.Although it is described in the literature as self-normalizing (Andreas and Klein, 2015), the trained models still need to be normalized after training, the partition cannot be considered constant and differs greatly between the data samples. At millionW, the 10th to 90th percentile range is an irregular variant of normalization (9.4 to 10.3 on the natural log scale, i.e. a ratio of 2.5 to WeaknormSQ)."}, {"heading": "5 Analysis", "text": "This section discusses model capacity, model initialization, training set size, and rare word performance."}, {"heading": "5.1 Model Capacity", "text": "The learning curves on Gigaword and BillionW indicate that most models are still progressing after a week, so training time must be taken into account when considering increasing capacity. Figure 7 shows the validation perplexity compared to the number of iterations for a training week. This figure shows that a Softmax model with 1024 hidden units could perform better in the last shift than the 512 hidden unit model with a longer training horizon. However, in the time allocated, 512 hidden units perform the best validation performance. D-Softmax shows that it is possible to selectively increase capacity, i.e. assign more hidden units to the representation of the most common words at the expense of more selective words. This captures the greatest benefit of a larger Softmax model while maintaining a reasonable training budget."}, {"heading": "5.2 Effect of Initialization", "text": "Our experiments use Hellinger PCA (Lebret and Collobert, 2014), motivated by its simplicity: it can be calculated in a few minutes and requires only the implementation of parallel coexistence counting and rapid randomized PCA. We look at the initialization of both the input-word embedding and the output matrix of PCA embedding. Figure 8 shows that PCA is better than random to initialize both input and output word representations; the initialization of both from PCA is even better. Results show that even after one week of training, initial conditions still influence validation perplexity. This trend is not specific to Softmax and similar results have been observed in other strategies. After one week of training, we observe only for HSM that the random initialization of the output matrix can achieve comparable performance."}, {"heading": "5.3 Training Set Size", "text": "Large training sets and a fixed training time result in competition between slower models with more capacity and the observation of more training data. This trade-off applies only to iterative SGD optimization and not to classic numbered models that visit the training set once and then solve the training in closed form. We compare Kneser-Ney and Softmax, which are trained for one week, with Gigaword on varying subsets of training data. For each setting, we make sure to include all data from the smaller subsets. Figure 9 shows that the performance of the neural model barely improves with more than 500 million tokens. To benefit from the full training set, we need a much higher training budget, faster hardware or parallelization. Scaling the training to large data sets can have a significant impact on perplexity, even if the data from the distribution of interests is limited."}, {"heading": "5.4 Rare Words", "text": "How well do neural models with rare words work? To answer this question, we calculated entropy using word frequency bands of vocabulary for Kneser-Ney and neural models, that is, we report entropy for the 4000 most common words, then for the next 16,000 words, etc. Table 5 shows that Kneser-Ney is very competitive on rare words, contrary to the general assumption that neural models are better on rare words. Neural models are equal to or better than Kneser-Ney on frequent words. This underlines that the two approaches complement each other, as observed in our combination experiments (Table 2). Among neural strategies, D-Softmax excels on frequent words, but bad on rare words. This is because D-Softmax assigns more common words at the expense of rare words. Overall, hierarchical Softmax is the best neural technique for rare words, as each Imax is very fast as the other Iteration is very technical."}, {"heading": "6 Conclusions", "text": "This paper presents the first comprehensive analysis of strategies for developing large vocabulary in neural language models. Large vocabulary poses a challenge for neural networks as they have to calculate the partition function across the entire vocabulary during each evaluation. We compared classical softmax with hierarchical softmax, target sampling, noise contrast estimation and rare normalization, commonly referred to as self-normalization. In addition, we expand rare normalization or self-normalization to be an appropriate estimate of probability, and we introduce differentiated softmax, a novel variant of softmax that allocates less capacity to rare words in order to reduce computation.Our results show that methods that are effective for small vocabularies are not necessarily the best for large vocabularies. In our environment, target sampling and noise-contrast assessments are not able to exceed the softmax baseline."}, {"heading": "7 Acknowledgments", "text": "Do not pay the receipt section and do not insert this section when submitting your work for review."}], "references": [{"title": "When and why are log-linear models selfnormalizing", "author": ["Andreas", "Klein2015] Jacob Andreas", "Dan Klein"], "venue": "In Proc. of NAACL", "citeRegEx": "Andreas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Andreas et al\\.", "year": 2015}, {"title": "Brian Kingsbury", "author": ["Ebru Arisoy", "Tara N. Sainath"], "venue": "and Bhuvana Ramabhadran.", "citeRegEx": "Arisoy et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Kyunghyun Cho", "author": ["Dzmitry Bahdanau"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bahdanau et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Pascal Vincent", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme"], "venue": "and Christian Jauvin.", "citeRegEx": "Bengio et al.2003", "shortCiteRegEx": null, "year": 2003}, {"title": "and Jenifer C", "author": ["Peter F. Brown", "Peter V. deSouza", "Robert L. Mercer", "Vincent J. Della Pietra"], "venue": "Lai.", "citeRegEx": "Brown et al.1992", "shortCiteRegEx": null, "year": 1992}, {"title": "Phillipp Koehn", "author": ["Ciprian Chelba", "Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants"], "venue": "and Tony Robinson.", "citeRegEx": "Chelba et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Alexander M", "author": ["Sumit Chopra", "Jason Weston"], "venue": "Rush.", "citeRegEx": "Chopra et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Richard Schwartz", "author": ["Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar"], "venue": ", and John Makhoul.", "citeRegEx": "Devlin et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Classes for Fast Maximum Entropy Training", "author": ["Joshua Goodman"], "venue": "In Proc. of ICASSP", "citeRegEx": "Goodman.,? \\Q2001\\E", "shortCiteRegEx": "Goodman.", "year": 2001}, {"title": "KenLM: Faster and Smaller Language Model Queries", "author": ["Kenneth Heafield"], "venue": "In Workshop on Statistical Machine Translation,", "citeRegEx": "Heafield.,? \\Q2011\\E", "shortCiteRegEx": "Heafield.", "year": 2011}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["Michael Gutmann Aapo Hyv\u00e4rinen"], "venue": "In Proc. of AISTATS", "citeRegEx": "Hyv\u00e4rinen.,? \\Q2010\\E", "shortCiteRegEx": "Hyv\u00e4rinen.", "year": 2010}, {"title": "Roland Memisevic", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho"], "venue": "and Yoshua Bengio.", "citeRegEx": "Jean et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Alexandre Allauzen", "author": ["Hai-Son Le"], "venue": "and Fran\u00e7ois Yvon.", "citeRegEx": "Le et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Genevieve Orr", "author": ["Yann LeCun", "Leon Bottou"], "venue": "and Klaus-Robert Mueller.", "citeRegEx": "LeCun et al.1998", "shortCiteRegEx": null, "year": 1998}, {"title": "Mary Ann Marcinkiewicz", "author": ["Mitchell P. Marcus"], "venue": "and Beatrice Santorini.", "citeRegEx": "Marcus et al.1993", "shortCiteRegEx": null, "year": 1993}, {"title": "2010", "author": ["Tom\u00e1\u0161 Mikolov", "Karafi\u00e1t Martin", "Luk\u00e1\u0161 Burget", "Jan Cernock\u00fd", "Sanjeev Khudanpur"], "venue": "Recurrent Neural Network based Language Model. In Proc. of INTERSPEECH, pages 1045\u2013", "citeRegEx": "Mikolov et al.2010", "shortCiteRegEx": null, "year": 1048}, {"title": "Extensions of Recurrent Neural Network Language Model", "author": ["Stefan Kombrink", "Luk\u00e1\u0161 Burget", "Jan Cernock\u00fd", "Sanjeev Khudanpur"], "venue": "In Proc. of ICASSP,", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Greg Corrado", "author": ["Tom\u00e1\u0161 Mikolov", "Kai Chen"], "venue": "and Jeffrey Dean.", "citeRegEx": "Mikolov et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "A Scalable Hierarchical Distributed Language Model", "author": ["Mnih", "Hinton2010] Andriy Mnih", "Geoffrey E. Hinton"], "venue": "In Proc. of NIPS", "citeRegEx": "Mnih et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2010}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["Mnih", "Teh2012] Andriy Mnih", "Yee Whye Teh"], "venue": "In Proc. of ICML", "citeRegEx": "Mnih et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2012}, {"title": "Hierarchical Probabilistic Neural Network Language Model", "author": ["Morin", "Bengio2005] Frederic Morin", "Yoshua Bengio"], "venue": "In Proc. of AISTATS", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "Ke Chen", "author": ["Robert Parker", "David Graff", "Junbo Kong"], "venue": "and Kazuaki Maeda.", "citeRegEx": "Parker et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Richard Socher", "author": ["Jeffrey Pennington"], "venue": "and Christopher D Manning.", "citeRegEx": "Pennington et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Anthony Rousseau", "author": ["Holger Schwenk"], "venue": "and Mohammed Attik.", "citeRegEx": "Schwenk et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Jian-Yun Nie1", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell"], "venue": "Jianfeng Gao, and Bill Dolan.", "citeRegEx": "Sordoni et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Oriol Vinyals", "author": ["Ilya Sutskever"], "venue": "and Quoc Le.", "citeRegEx": "Sutskever et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Victoria Fossum", "author": ["Ashish Vaswani", "Yinggong Zhao"], "venue": "and David Chiang.", "citeRegEx": "Vaswani et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Rajat Monga", "author": ["Sudheendra Vijayanarasimhan", "Jonathon Shlens"], "venue": "and Jay Yagnik.", "citeRegEx": "Vijayanarasimhan et al.2014", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [], "year": 2015, "abstractText": "Training neural network language models over large vocabularies is still computationally very costly compared to count-based models such as Kneser-Ney. At the same time, neural language models are gaining popularity for many applications such as speech recognition and machine translation whose success depends on scalability. We present a systematic comparison of strategies to represent and train large vocabularies, including softmax, hierarchical softmax, target sampling, noise contrastive estimation and self normalization. We further extend self normalization to be a proper estimator of likelihood and introduce an efficient variant of softmax. We evaluate each method on three popular benchmarks, examining performance on rare words, the speed/accuracy trade-off and complementarity to Kneser-Ney.", "creator": "LaTeX with hyperref package"}}}