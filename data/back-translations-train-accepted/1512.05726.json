{"id": "1512.05726", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2015", "title": "Semi-supervised Question Retrieval with Gated Convolutions", "abstract": "Question answering forums are rapidly growing in size with no automated ability to refer to and reuse existing answers. In this paper, we develop a methodology for finding semantically related questions. The task is difficult since 1) key pieces of information are often buried in extraneous detail in the question body and 2) available annotations are scarce and fragmented, driven by participants. We design a novel combination of recurrent and convolutional models (gated convolutions) to effectively map questions to their semantic representations. The models are pre-trained within an encoder-decoder framework (from body to title) on the basis of the entire raw corpus, and fine-tuned discriminatively from limited annotations. Our evaluation demonstrates that our model yields 10\\% gain over a standard IR baseline, and 6\\% over standard neural network architectures (including CNNs and LSTMs) trained analogously.", "histories": [["v1", "Thu, 17 Dec 2015 19:14:20 GMT  (75kb,D)", "http://arxiv.org/abs/1512.05726v1", null], ["v2", "Mon, 4 Apr 2016 00:29:15 GMT  (503kb,D)", "http://arxiv.org/abs/1512.05726v2", "NAACL 2016"]], "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["tao lei", "hrishikesh joshi", "regina barzilay", "tommi s jaakkola", "kateryna tymoshenko", "alessandro moschitti", "llu\u00eds m\u00e0rquez"], "accepted": true, "id": "1512.05726"}, "pdf": {"name": "1512.05726.pdf", "metadata": {"source": "CRF", "title": "Denoising Bodies to Titles: Retrieving Similar Questions with Recurrent Convolutional Models", "authors": ["Tao Lei", "Hrishikesh Joshi", "Regina Barzilay", "Tommi Jaakkola", "Katerina Tymoshenko", "Alessandro Moschitti", "Llu\u0131\u0301s M\u00e0rquez"], "emails": ["tommi}@csail.mit.edu", "tymoshenko@disi.unitn.it", "lmarquez}@qf.org.qa"], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "2 Related Work", "text": "Given the growing popularity of community QA forums, retrieving questions has become an important area of research. Previous work on query has modeled this task using machine translation, topic modeling, and knowledge graph-based approaches (Jeon et al., 2005; Li and Manandhar, 2011; Duan et al., 2008; Zhou et al., 2013). Recent work relies on presentation learning to go beyond word-based methods and capture semantic representations for more refined mappings. Thus, Zhou et al. (2015) learn word embedding using category-based metadata information for questions. They define each question as a distribution that generates each word (embedding) independently, and use Fisher kernel to investigate question similarities. Dos Santos et al. (2015) we propose an approach that combines a revolutionary neural network of questions (as opposed to questions from Shou et al.) In contrast to questions from Shou et al."}, {"heading": "3 Question Retrieval Setup", "text": "We start with the introduction of the basic discriminatory setting for retrieving similar questions. Let q be a query question, which usually consists of both a title sentence and a body part. For efficiency reasons, we do not compare q against all other queries in the database. Instead, we first pick up a smaller set of related questions Q (q) and then apply the more complex models only to this reduced sentence. The aim is to classify the candidate questions in Q (q) so that all similar questions to q are ranked above the unequal questions. To do this, we define a similarity value s (q, p; \u03b8) with parameters where similarity measures how closely the candidate p-Q (q) interacts with the query q. The method of comparison can include the use of the title and body of each question.The scoring function s (\u00b7; answer) is a correct function that we can optimize based on commented data."}, {"heading": "4 Recurrent Convolutional Networks", "text": "We describe here our encoder model, i.e., the method of mapping the question title and the body into a vector representation mode =. Our approach is inspired by temporal Convolutionary Neural Networks (LeCun et al., 1998) and, in particular, its current refinement (Lei et al., 2015), tailored to capture long-distance, non-sequential patterns in a weighted manner. Such models can be used to integrate the appearance of patterns in text and aggregate into a vector representation. However, the summary is not selective as all patterns are counted, weighted according to how contiguous (not sequential) they are used. In our problem, the questioner tends to be very long and full of irrelevant words and fragments. Therefore, we believe that the interpretation of the interpretation of the interrogator requires a more selective approach to patterns."}, {"heading": "4.1 Pre-training Using the Entire Corpus", "text": "The number of questions in the AskUbuntu corpus far exceeds the user's comments on pairs of similar questions. We can use this larger body in two different ways. First, because models take word embeddings as input, we can tailor the embeddings to the specific vocabulary and expressions in that corpus. To this end, we run word2vec (Mikolov et al., 2013) in addition to the Wikipedia dump on the body. Second, and more importantly, we use individual questions as training examples for an auto encoder that is constructed by linking the encoder model (RCNN) with the corresponding decoder. The resulting encoder decoder architecture is similar to those used in machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) and Summary (Rush et al., 2015)."}, {"heading": "5 Alternative models", "text": "To determine whether RCNNs are necessary for good performance, we also need to train two alternative benchmark encoders (LSTMs and CNNs) for mapping questions to vector representations. LSTM-based encoders can be pre-trained and refined in a discriminatory manner similar to RCNNs, but CNN encoders are trained only in a discriminatory way. While plausible, no alternative achieves the same level of performance as our pre-trained RCNN.LSTMs LSTM cells (Hochreiter and Schmidhuber, 1997) to capture semantic information across a wide range of applications, including machine translation and entailment recognition (Bahdanau et al., 2014; Bowman et al., 2015). Their success can be attributed neural to gates that are adaptively read or discarded information on / from internal storage states.In our context, LSTM can be used much like RCNN."}, {"heading": "6 Experimental Setup", "text": "We have written to the flags that we will be able to get a grip on the problems of the world, \"he told the German Press Agency in an interview with\" Welt am Sonntag. \"\" We have managed to get a grip on the world, \"he told the\" Welt am Sonntag. \"\" We have done it. \"\" We have done it. \"\" We have done it. \"\" We have done it. \"\" \"We have done it.\" \"\" We have done it. \"\" \"We have done it.\" \"\" \"We have done it.\" \"\" \"\" We have done it. \".\" \"\" \"\" \"We have done it.\" \".\" \"\" \"\". \"\" \"\". \"\" \"\" \".\" \"\" \"\". \"\" \"\" \".\" \"\" \".\" \"\". \"\" \"\". \"\" \".\" \"\". \"\" \".\" \".\" \"\" \".\" \".\" \".\" \"\" \".\". \"\". \"\" \".\" \".\" \".\". \"\". \"\". \"\" \"\". \"\". \".\". \".\" \"\". \".\". \"\". \"\". \".\". \".\". \".\" \".\". \".\". \".\". \".\". \".\". \".\". \".\" \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\".. \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \"..\". \".\". \".\"......................................................."}, {"heading": "7 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Overall Performance", "text": "Results show that our full model, pre-schooled RCNNs, achieves the best performance across all metrics on both the development and test sets, achieving a P @ 1 of 64.5% on the test set and outperforming the word-based BM25 method by over 10%. In addition, our RCNN model effectively outperforms other neural encoder models and baselines in all metrics. The ability of the RCNN model to outperform other models suggests that the use of non-consecutive filters and a varying decay factor effectively improves performance beyond traditional neural network models. Table 3 also shows the performance gain from pre-schooling the RCNN encoder. The RCNN model, when preschooled across the entire corpus, consistently delivers better results across all metrics."}, {"heading": "7.2 Discussion", "text": "As shown in Table 4, our RCNN model significantly exceeds the performance of the TF-IDF base and the RCNN model."}, {"heading": "8 Conclusion", "text": "In this paper, we used gated convolutions to map questions to their semantic representations and demonstrated their effectiveness in the task of question recovery in the community QA forums. This architecture allows the model to extract key information from lengthy, detailed user questions. Pre-training within an encoder decoder framework (from body to title) based on the entire body is critical to the success of the model. In future work, we plan to extend this model to the task of answer selection. As this task presents similar challenges to question recovery, both pre-training and gated convolutions should benefit overall performance. In addition, we plan to use pre-prepared representations for the task of title generation."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Dkpro similarity: An open source framework for text similarity", "author": ["B\u00e4r et al.2013] Daniel B\u00e4r", "Torsten Zesch", "Iryna Gurevych"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations,", "citeRegEx": "B\u00e4r et al\\.,? \\Q2013\\E", "shortCiteRegEx": "B\u00e4r et al\\.", "year": 2013}, {"title": "A large annotated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326", "author": ["Gabor Angeli", "Christopher Potts", "Christopher D Manning"], "venue": null, "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning hybrid representations to retrieve semantically equivalent questions", "author": ["Luciano Barbosa", "Dasha Bogdanova", "Bianca Zadrozny"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Searching questions by identifying question topic and question focus", "author": ["Duan et al.2008] Huizhong Duan", "Yunbo Cao", "Chin-Yew Lin", "Yong Yu"], "venue": "In ACL,", "citeRegEx": "Duan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2008}, {"title": "Applying deep learning to answer selection: A study and an open task. arXiv preprint arXiv:1508.01585", "author": ["Feng et al.2015] Minwei Feng", "Bing Xiang", "Michael R Glass", "Lidan Wang", "Bowen Zhou"], "venue": null, "citeRegEx": "Feng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2015}, {"title": "Modeling interestingness with deep neural networks", "author": ["Gao et al.2014] Jianfeng Gao", "Patrick Pantel", "Michael Gamon", "Xiaodong He", "Li Deng", "Yelong Shen"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Gao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2014}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580", "author": ["Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Finding similar questions in large question and answer archives", "author": ["Jeon et al.2005] Jiwoon Jeon", "W Bruce Croft", "Joon Ho Lee"], "venue": "In Proceedings of the 14th ACM international conference on Information and knowledge management,", "citeRegEx": "Jeon et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Jeon et al\\.", "year": 2005}, {"title": "Optimizing search engines using clickthrough data", "author": ["T. Joachims"], "venue": "In ACM SIGKDD KDD", "citeRegEx": "Joachims.,? \\Q2002\\E", "shortCiteRegEx": "Joachims.", "year": 2002}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "A convolutional neural network for modelling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": "In Proceedings of the 52th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Characteraware neural language models", "author": ["Kim et al.2015] Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": "arXiv preprint arXiv:1508.06615", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun et al.1998] Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Molding cnns for text: non-linear, non-consecutive convolutions", "author": ["Lei et al.2015] Tao Lei", "Regina Barzilay", "Tommi Jaakkola"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Lei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lei et al\\.", "year": 2015}, {"title": "Improving question recommendation by exploiting information need", "author": ["Li", "Manandhar2011] Shuguang Li", "Suresh Manandhar"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Efficient estimation of word representations in vector space. CoRR", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "The probabilistic relevance framework: BM25 and beyond", "author": ["Robertson", "Zaragoza2009] Stephen Robertson", "Hugo Zaragoza"], "venue": null, "citeRegEx": "Robertson et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Robertson et al\\.", "year": 2009}, {"title": "Reasoning about entailment with neural attention", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1509.06664", "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Sumit Chopra", "Jason Weston"], "venue": "arXiv preprint arXiv:1509.00685", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Learning to rank short text pairs with convolutional deep neural networks", "author": ["Severyn", "Moschitti2015] Aliaksei Severyn", "Alessandro Moschitti"], "venue": null, "citeRegEx": "Severyn et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Severyn et al\\.", "year": 2015}, {"title": "Word embedding based correlation model for question/answer matching", "author": ["Shen et al.2015] Yikang Shen", "Wenge Rong", "Nan Jiang", "Baolin Peng", "Jie Tang", "Zhang Xiong"], "venue": "arXiv preprint arXiv:1511.04646", "citeRegEx": "Shen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104\u20133112", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Lstm-based deep learning models for non-factoid answer selection. arXiv preprint arXiv:1511.04108", "author": ["Tan et al.2015] Ming Tan", "Bing Xiang", "Bowen Zhou"], "venue": null, "citeRegEx": "Tan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2015}, {"title": "A long short-term memory model for answer sentence selection in question answering", "author": ["Wang", "Nyberg2015] Di Wang", "Eric Nyberg"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Text understanding from scratch. arXiv preprint arXiv:1502.01710", "author": ["Zhang", "LeCun2015] Xiang Zhang", "Yann LeCun"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Improving question retrieval in community question answering using world knowledge", "author": ["Zhou et al.2013] Guangyou Zhou", "Yang Liu", "Fang Liu", "Daojian Zeng", "Jun Zhao"], "venue": "In Proceedings of the TwentyThird international joint conference on Artificial Intel-", "citeRegEx": "Zhou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2013}, {"title": "Learning continuous word embedding with metadata for question retrieval in community question answering", "author": ["Zhou et al.2015] Guangyou Zhou", "Tingting He", "Jun Zhao", "Po Hu"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computa-", "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 31, "context": "The task of automatically retrieving similar questions to a given user\u2019s question has recently attracted significant attention and has become a testbed for various representation learning approaches (Zhou et al., 2015; dos Santos et al., 2015).", "startOffset": 199, "endOffset": 243}, {"referenceID": 4, "context": ", 2015; dos Santos et al., 2015). However, the task has proven to be quite challenging\u2014for instance, dos Santos et al. (2015) report a 22.", "startOffset": 12, "endOffset": 126}, {"referenceID": 18, "context": "In particular, we incorporate adaptive gating in non-consecutive CNNs (Lei et al., 2015) so as to focus temporal averaging in these models on key pieces of the questions.", "startOffset": 70, "endOffset": 88}, {"referenceID": 26, "context": "The methodology is reminiscent of recent encoder-decoder networks in machine translation and document summarization (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Rush et al., 2015).", "startOffset": 116, "endOffset": 209}, {"referenceID": 3, "context": "The methodology is reminiscent of recent encoder-decoder networks in machine translation and document summarization (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Rush et al., 2015).", "startOffset": 116, "endOffset": 209}, {"referenceID": 23, "context": "The methodology is reminiscent of recent encoder-decoder networks in machine translation and document summarization (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Rush et al., 2015).", "startOffset": 116, "endOffset": 209}, {"referenceID": 10, "context": "Previous work on question retrieval has modeled this task using machine translation, topic modeling and knowledge graph-based approaches (Jeon et al., 2005; Li and Manandhar, 2011; Duan et al., 2008; Zhou et al., 2013).", "startOffset": 137, "endOffset": 218}, {"referenceID": 5, "context": "Previous work on question retrieval has modeled this task using machine translation, topic modeling and knowledge graph-based approaches (Jeon et al., 2005; Li and Manandhar, 2011; Duan et al., 2008; Zhou et al., 2013).", "startOffset": 137, "endOffset": 218}, {"referenceID": 30, "context": "Previous work on question retrieval has modeled this task using machine translation, topic modeling and knowledge graph-based approaches (Jeon et al., 2005; Li and Manandhar, 2011; Duan et al., 2008; Zhou et al., 2013).", "startOffset": 137, "endOffset": 218}, {"referenceID": 31, "context": "In contrast to (Zhou et al., 2015), our model treats each question as a word sequence as opposed to a bag of words, and we apply a recurrent convolutional model as opposed to the traditional CNN model used by dos Santos et al.", "startOffset": 15, "endOffset": 34}, {"referenceID": 4, "context": ", 2005; Li and Manandhar, 2011; Duan et al., 2008; Zhou et al., 2013). More recent work relies on representation learning to go beyond word-based methods, aiming to capture semantic representations for more refined mappings. For instance, Zhou et al. (2015) learn word embeddings using category-based metadata information for questions.", "startOffset": 32, "endOffset": 258}, {"referenceID": 4, "context": "Dos Santos et al. (2015) propose an approach which combines a convolutional neural network (CNN) and a bag-of-words representation for comparing questions.", "startOffset": 4, "endOffset": 25}, {"referenceID": 4, "context": "Dos Santos et al. (2015) propose an approach which combines a convolutional neural network (CNN) and a bag-of-words representation for comparing questions. In contrast to (Zhou et al., 2015), our model treats each question as a word sequence as opposed to a bag of words, and we apply a recurrent convolutional model as opposed to the traditional CNN model used by dos Santos et al. (2015) to map questions into meaning representations.", "startOffset": 4, "endOffset": 390}, {"referenceID": 25, "context": "Recent work on answer selection on community QA forums, similar to our task of question retrieval, has also involved the use of neural network architectures (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015).", "startOffset": 157, "endOffset": 265}, {"referenceID": 6, "context": "Recent work on answer selection on community QA forums, similar to our task of question retrieval, has also involved the use of neural network architectures (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015).", "startOffset": 157, "endOffset": 265}, {"referenceID": 27, "context": "Recent work on answer selection on community QA forums, similar to our task of question retrieval, has also involved the use of neural network architectures (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015).", "startOffset": 157, "endOffset": 265}, {"referenceID": 6, "context": ", 2015; Feng et al., 2015; Tan et al., 2015). Similar to our work, these approaches apply neural network techniques, but focus on improving various other aspects of the model. For instance, Feng et al. (2015) explore different similarity measures beyond cosine similarity, and Tan et al.", "startOffset": 8, "endOffset": 209}, {"referenceID": 6, "context": ", 2015; Feng et al., 2015; Tan et al., 2015). Similar to our work, these approaches apply neural network techniques, but focus on improving various other aspects of the model. For instance, Feng et al. (2015) explore different similarity measures beyond cosine similarity, and Tan et al. (2015) adopt the neural attention mechanism over RNNs to generate better answer representations given the questions as context.", "startOffset": 8, "endOffset": 295}, {"referenceID": 17, "context": "Our approach is inspired by temporal convolutional neural networks (LeCun et al., 1998) and, in particular, its recent refinement (Lei et al.", "startOffset": 67, "endOffset": 87}, {"referenceID": 18, "context": ", 1998) and, in particular, its recent refinement (Lei et al., 2015), tailored to capture longerrange, non-consecutive patterns in a weighted manner.", "startOffset": 50, "endOffset": 68}, {"referenceID": 18, "context": "Our approach builds on (Lei et al., 2015), thus we begin by briefly outlining it.", "startOffset": 23, "endOffset": 41}, {"referenceID": 18, "context": "Our approach builds on (Lei et al., 2015), thus we begin by briefly outlining it. Let W1 and W2 denote filter matrices (as parameters) for pattern size n = 2. Lei et al. (2015) generate a sequence of states in response to tokens according to", "startOffset": 24, "endOffset": 177}, {"referenceID": 20, "context": "To this end, we run word2vec (Mikolov et al., 2013) on the raw corpus in addition to the Wikipedia dump.", "startOffset": 29, "endOffset": 51}, {"referenceID": 26, "context": "The resulting encoder-decoder architecture is akin to those used in machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) and summarization (Rush et al.", "startOffset": 88, "endOffset": 162}, {"referenceID": 3, "context": "The resulting encoder-decoder architecture is akin to those used in machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) and summarization (Rush et al.", "startOffset": 88, "endOffset": 162}, {"referenceID": 23, "context": ", 2014) and summarization (Rush et al., 2015).", "startOffset": 26, "endOffset": 45}, {"referenceID": 0, "context": "LSTMs LSTM cells (Hochreiter and Schmidhuber, 1997) have been used to capture semantic information across a wide range of applications, including machine translation and entailment recognition (Bahdanau et al., 2014; Bowman et al., 2015; Rockt\u00e4schel et al., 2015).", "startOffset": 193, "endOffset": 263}, {"referenceID": 2, "context": "LSTMs LSTM cells (Hochreiter and Schmidhuber, 1997) have been used to capture semantic information across a wide range of applications, including machine translation and entailment recognition (Bahdanau et al., 2014; Bowman et al., 2015; Rockt\u00e4schel et al., 2015).", "startOffset": 193, "endOffset": 263}, {"referenceID": 22, "context": "LSTMs LSTM cells (Hochreiter and Schmidhuber, 1997) have been used to capture semantic information across a wide range of applications, including machine translation and entailment recognition (Bahdanau et al., 2014; Bowman et al., 2015; Rockt\u00e4schel et al., 2015).", "startOffset": 193, "endOffset": 263}, {"referenceID": 17, "context": "CNNs Convolutional neural networks (LeCun et al., 1998) have also been successfully applied to various NLP tasks (Kalchbrenner et al.", "startOffset": 35, "endOffset": 55}, {"referenceID": 13, "context": ", 1998) have also been successfully applied to various NLP tasks (Kalchbrenner et al., 2014; Kim, 2014; Kim et al., 2015; Zhang and LeCun, 2015; Gao et al., 2014).", "startOffset": 65, "endOffset": 162}, {"referenceID": 15, "context": ", 1998) have also been successfully applied to various NLP tasks (Kalchbrenner et al., 2014; Kim, 2014; Kim et al., 2015; Zhang and LeCun, 2015; Gao et al., 2014).", "startOffset": 65, "endOffset": 162}, {"referenceID": 14, "context": ", 1998) have also been successfully applied to various NLP tasks (Kalchbrenner et al., 2014; Kim, 2014; Kim et al., 2015; Zhang and LeCun, 2015; Gao et al., 2014).", "startOffset": 65, "endOffset": 162}, {"referenceID": 7, "context": ", 1998) have also been successfully applied to various NLP tasks (Kalchbrenner et al., 2014; Kim, 2014; Kim et al., 2015; Zhang and LeCun, 2015; Gao et al., 2014).", "startOffset": 65, "endOffset": 162}, {"referenceID": 11, "context": "\u2022 SVM, we trained a re-ranker using SVM-Light (Joachims, 2002) with a linear kernel incorporating various similarity measures from the DKPro similarity package (B\u00e4r et al.", "startOffset": 46, "endOffset": 62}, {"referenceID": 1, "context": "\u2022 SVM, we trained a re-ranker using SVM-Light (Joachims, 2002) with a linear kernel incorporating various similarity measures from the DKPro similarity package (B\u00e4r et al., 2013).", "startOffset": 160, "endOffset": 178}, {"referenceID": 8, "context": "We optimized other hyper-parameters with the following range of values: learning rate \u2208 {1e\u2212 3, 3e\u2212 4}, dropout (Hinton et al., 2012) probability \u2208 {0.", "startOffset": 112, "endOffset": 133}, {"referenceID": 20, "context": "Word Vectors We ran word2vec (Mikolov et al., 2013) to obtain 200-dimensional word embeddings using all Stack Exchange data (excluding StackOverflow) and a large Wikipedia corpus.", "startOffset": 29, "endOffset": 51}], "year": 2017, "abstractText": "Question answering forums are rapidly growing in size with no automated ability to refer to and reuse existing answers. In this paper, we develop a methodology for finding semantically related questions. The task is difficult since 1) key pieces of information are often buried in extraneous detail in the question body and 2) available annotations are scarce and fragmented, driven by participants. We design a novel combination of recurrent and convolutional models (gated convolutions) to effectively map questions to their semantic representations. The models are pre-trained within an encoder-decoder framework (from body to title) on the basis of the entire raw corpus, and fine-tuned discriminatively from limited annotations. Our evaluation demonstrates that our model yields 10% gain over a standard IR baseline, and 6% over standard neural network architectures (including CNNs and LSTMs) trained analogously.1", "creator": "LaTeX with hyperref package"}}}