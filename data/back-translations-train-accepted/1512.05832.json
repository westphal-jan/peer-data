{"id": "1512.05832", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Dec-2015", "title": "Learning the Preferences of Ignorant, Inconsistent Agents", "abstract": "An important use of machine learning is to learn what people value. What posts or photos should a user be shown? Which jobs or activities would a person find rewarding? In each case, observations of people's past choices can inform our inferences about their likes and preferences. If we assume that choices are approximately optimal according to some utility function, we can treat preference inference as Bayesian inverse planning. That is, given a prior on utility functions and some observed choices, we invert an optimal decision-making process to infer a posterior distribution on utility functions. However, people often deviate from approximate optimality. They have false beliefs, their planning is sub-optimal, and their choices may be temporally inconsistent due to hyperbolic discounting and other biases. We demonstrate how to incorporate these deviations into algorithms for preference inference by constructing generative models of planning for agents who are subject to false beliefs and time inconsistency. We explore the inferences these models make about preferences, beliefs, and biases. We present a behavioral experiment in which human subjects perform preference inference given the same observations of choices as our model. Results show that human subjects (like our model) explain choices in terms of systematic deviations from optimal behavior and suggest that they take such deviations into account when inferring preferences.", "histories": [["v1", "Fri, 18 Dec 2015 00:24:08 GMT  (145kb,D)", "http://arxiv.org/abs/1512.05832v1", "AAAI 2016"]], "COMMENTS": "AAAI 2016", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["owain evans", "andreas stuhlm\u00fcller", "noah d goodman"], "accepted": true, "id": "1512.05832"}, "pdf": {"name": "1512.05832.pdf", "metadata": {"source": "CRF", "title": "Learning the Preferences of Ignorant, Inconsistent Agents", "authors": ["Owain Evans", "Andreas Stuhlm\u00fcller", "Noah D. Goodman"], "emails": [], "sections": [{"heading": null, "text": "Keywords: Bayesian learning, cognitive bias, preference conclusion"}, {"heading": "Introduction", "text": "The problem of learning a person from the observations of their decisions is that they are preferred in economics (Hausman 2011), cognitive science (Baker, Saxe and Tenenbaum 2011; Ullman et al.), and applied machine learning (Jannach et al. 2014). To give just one example, we use a person's behavior to choose which stories, complaints, and potential contacts to show. A promising approach to learning preferences from observed decisions is that of developing artificial intelligence (www.aaaai.org), which implements a model of rational decision-making that has a real function (Russell and Norvig 1995). This approach is known as Inverse Reinforcement Learning (NG and Russell 2000)."}, {"heading": "Computational Framework", "text": "The key question for this project is: How do our observations of an agent's behavior relate to the agent's preferences? Technically speaking, what generative model (Tenenbaum et al. 2011) best describes the agent's approximate sequential planning taking into account a benefit function? Given such a model and a previous benefit function, we could \"reverse\" it (by executing complete Bayesian conclusions) to calculate a subset of what the agent evaluates. The following section describes the class of models we examine in this paper. We first take an informal look at the specific deviations from the optimality contained in our agent model. Then, we formally define the model and show our implementation as a probabilistic program, an approach that clarifies our assumptions and allows easy exploration of deviations from optimal planning."}, {"heading": "Deviations from optimality", "text": "In fact, we will be able to look for a solution that is capable of finding a solution that meets the needs of the individual."}, {"heading": "Formal model definition", "text": "First of all, we define an agent with full knowledge and without temporal inconsistency, 2 and then we generalise to agents that deviate from the optimality. We refer to states that have a \"A,\" a \"A,\" a \"S,\" a deterministic service functionU: S \"A\" R, \"a stochastic action selection function C: S\" A \"and a stochastic transition function T: S\" A \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S"}, {"heading": "Agents as probabilistic programs", "text": "We have implemented the model described above in the probabilistic programming language WebPPL (Goodman and Stuhlmu M\u00fcller 2014). WebPPL provides automated inferences for functional programs that include recursion, which means that we can translate the above recursions directly into programs that represent an agent and the world simulation used for expected benefit calculations. All of the above agents can be captured in a concise functional program that can be easily expanded to capture other types of suboptimal planning. Figure 2 shows a simplified example (including hyperbolic discounting, but not uncertainty about the state).For Bayesian inference, which corresponds to Equation 1, we use a discrete grid approach for continuous variables (i.e. U, p (s), k, and \u03b1) and perform exact inferences by enumeration with dynamic programming."}, {"heading": "Model inferences", "text": "We show that the model described above can infer preferences, false beliefs, and temporal inconsistencies, which are mainly due to simple sequences of actions that often occur in everyday life. We confirm this intuition later in our experiments, where we show that human subjects draw conclusions about behavior similar to those of our model. Example 1: Inference with full knowledge We have previously seen how the modeling of agents such as Naive and Sophisticated may predict the action sequences shown in Figures1a and 1b. We are now looking at the problem of what can be inferred about the agent. We assume that the agent has exact beliefs about the restaurants and that the two donut stores D1 and D2 are identical (with D1 closer to the starting point). 4 We model each restaurant as an immediate service (received upon arrival at the restaurant) and a delayed utility (received one step after)."}, {"heading": "Experiments with Human Subjects", "text": "We have shown that, given the short plot sequences, our model can infer whether (and how) an agent is time consistent while collectively inferring appropriate tools. We claim that this type of inference is familiar from everyday life and is therefore intuitively plausible. This section provides support for this assertion by collecting data on the conclusions of human subjects. In our first two experiments, we ask the subjects to explain the behavior in Figures 1a and 1b. This section not only examines their conclusions about preferences and false beliefs that may have influenced the choice of the agent. Experiment 1: Conclusion with full knowledge Experiment 1 corresponds to Example 1 in the previous section (where the agent is assumed to have full knowledge). Two groups of subjects were shown Figures 1a and 1b, which have already shown evidence of a preference for the Vegetarian Cafe over the other restaurants."}, {"heading": "Conclusion", "text": "AI systems have the potential to improve our lives by helping us make decisions that involve the integration of vast amounts of information or that require long and sophisticated plans. In order for these systems to deliver on their promise, we must be willing to delegate some of our decisions to them - that is, we need such systems to reliably act in accordance with our preferences and values. It can be difficult to formalise our preferences in complex areas; instead, it is desirable for systems to learn our preferences, just as learning in other areas is often preferable to manual specifications. This learning requires us to build assumptions about how our preferences relate to the observations of the AI system. As a starting point, we can assume that our decisions result from optimal rational planning, which has a latent benefit function. However, as our experiments with human subjects show, this assumption does not align with the intuitions of the chosen systems that we most support in terms of the characteristics chosen."}, {"heading": "Acknowledgments", "text": "This work was supported by the Future of Life Institute grant 2015-144846 (all authors) and the ONR grant N00014-13-10788 (NG).This material is based on research sponsored by DARPA under contract number FA8750-14-2-0009 (AS and NG).The U.S. government is authorized to reproduce and distribute reproductions for government purposes, regardless of the copyright notices contained therein. The views and conclusions contained herein are those of the authors and should not necessarily be interpreted as representing the official guidelines or notices of DARPA or the U.S. government."}], "references": [{"title": "J", "author": ["C.L. Baker", "Tenenbaum"], "venue": "B.", "citeRegEx": "Baker and Tenenbaum 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "J", "author": ["C.L. Baker", "R. Saxe", "Tenenbaum"], "venue": "B.", "citeRegEx": "Baker. Saxe. and Tenenbaum 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "J", "author": ["C.L. Baker", "R.R. Saxe", "Tenenbaum"], "venue": "B.", "citeRegEx": "Baker. Saxe. and Tenenbaum 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning large-scale dynamic discrete choice models of spatio-temporal preferences with application to migratory pastoralism in east africa", "author": ["Ermon"], "venue": null, "citeRegEx": "Ermon,? \\Q2014\\E", "shortCiteRegEx": "Ermon", "year": 2014}, {"title": "and Stuhlm\u00fcller", "author": ["N.D. Goodman"], "venue": "A.", "citeRegEx": "Goodman and Stuhlm\u00fcller 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "D", "author": ["Hausman"], "venue": "M.", "citeRegEx": "Hausman 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Recommender systems: an introduction", "author": ["Jannach"], "venue": null, "citeRegEx": "Jannach,? \\Q2010\\E", "shortCiteRegEx": "Jannach", "year": 2010}, {"title": "A", "author": ["L.P. Kaelbling", "M.L. Littman", "Cassandra"], "venue": "R.", "citeRegEx": "Kaelbling. Littman. and Cassandra 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "and Tversky", "author": ["D. Kahneman"], "venue": "A.", "citeRegEx": "Kahneman and Tversky 1979", "shortCiteRegEx": null, "year": 1979}, {"title": "Inverse reinforcement learning for micro-turn management", "author": ["Kim"], "venue": "In Proceedings of the Annual Conference of the International Speech Communication Association,", "citeRegEx": "Kim,? \\Q2014\\E", "shortCiteRegEx": "Kim", "year": 2014}, {"title": "S", "author": ["A.Y. Ng", "Russell"], "venue": "J.", "citeRegEx": "Ng and Russell 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "Doing it now or later", "author": ["O\u2019Donoghue", "T. Rabin 1999] O\u2019Donoghue", "M. Rabin"], "venue": "American Economic Review", "citeRegEx": "O.Donoghue et al\\.,? \\Q1999\\E", "shortCiteRegEx": "O.Donoghue et al\\.", "year": 1999}, {"title": "The economics of immediate gratification", "author": ["O\u2019Donoghue", "T. Rabin 2000] O\u2019Donoghue", "M. Rabin"], "venue": "Journal of Behavioral Decision Making", "citeRegEx": "O.Donoghue et al\\.,? \\Q2000\\E", "shortCiteRegEx": "O.Donoghue et al\\.", "year": 2000}, {"title": "and Gmytrasiewicz", "author": ["A. Panella"], "venue": "P.", "citeRegEx": "Panella and Gmytrasiewicz 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Norvig", "author": ["S. Russell"], "venue": "P.", "citeRegEx": "Russell and Norvig 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "N", "author": ["J.B. Tenenbaum", "C. Kemp", "T.L. Griffiths", "Goodman"], "venue": "D.", "citeRegEx": "Tenenbaum et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "2009", "author": ["T. Ullman", "C. Baker", "O. Macindoe", "O. Evans", "N. Goodman", "J. B Tenenbaum"], "venue": "Help or hinder: Bayesian models of social goal inference. In Advances in neural information processing systems, 1874\u2013", "citeRegEx": "Ullman et al. 2009", "shortCiteRegEx": null, "year": 1882}, {"title": "L", "author": ["J. Zheng", "S. Liu", "Ni"], "venue": "M.", "citeRegEx": "Zheng. Liu. and Ni 2014", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [], "year": 2015, "abstractText": "An important use of machine learning is to learn what people value. What posts or photos should a user be shown? Which jobs or activities would a person find rewarding? In each case, observations of people\u2019s past choices can inform our inferences about their likes and preferences. If we assume that choices are approximately optimal according to some utility function, we can treat preference inference as Bayesian inverse planning. That is, given a prior on utility functions and some observed choices, we invert an optimal decision-making process to infer a posterior distribution on utility functions. However, people often deviate from approximate optimality. They have false beliefs, their planning is sub-optimal, and their choices may be temporally inconsistent due to hyperbolic discounting and other biases. We demonstrate how to incorporate these deviations into algorithms for preference inference by constructing generative models of planning for agents who are subject to false beliefs and time inconsistency. We explore the inferences these models make about preferences, beliefs, and biases. We present a behavioral experiment in which human subjects perform preference inference given the same observations of choices as our model. Results show that human subjects (like our model) explain choices in terms of systematic deviations from optimal behavior and suggest that they take such deviations into account when inferring preferences.", "creator": "LaTeX with hyperref package"}}}