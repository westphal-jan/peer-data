{"id": "1512.06110", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Dec-2015", "title": "Morphological Inflection Generation Using Character Sequence to Sequence Learning", "abstract": "Morphological inflection generation is the task of generating the inflected form of a given lemma corresponding to a particular linguistic transformation. We model the problem of inflection generation as a character sequence to sequence learning problem and present a variant of the neural encoder-decoder model for solving it. Our model is language independent and can be trained in both supervised and semi-supervised settings. We evaluate our system on seven datasets of morphologically rich languages and achieve either better or comparable results to existing state-of-the-art models of inflection generation.", "histories": [["v1", "Fri, 18 Dec 2015 20:48:26 GMT  (103kb,D)", "http://arxiv.org/abs/1512.06110v1", null], ["v2", "Thu, 31 Dec 2015 17:23:32 GMT  (103kb,D)", "http://arxiv.org/abs/1512.06110v2", null], ["v3", "Tue, 22 Mar 2016 01:02:01 GMT  (103kb,D)", "http://arxiv.org/abs/1512.06110v3", "Proceedings of NAACL 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["manaal faruqui", "yulia tsvetkov", "graham neubig", "chris dyer"], "accepted": true, "id": "1512.06110"}, "pdf": {"name": "1512.06110.pdf", "metadata": {"source": "CRF", "title": "Morphological Inflection Generation Using Character Sequence to Sequence Learning", "authors": ["Manaal Faruqui", "Yulia Tsvetkov", "Graham Neubig", "Chris Dyer"], "emails": ["mfaruqui@cs.cmu.edu", "ytsvetko@cs.cmu.edu", "cdyer@cs.cmu.edu", "neubig@is.naist.jp"], "sections": [{"heading": null, "text": "Morphological diffraction generation is the task of generating the flexed form of a given problem that corresponds to a particular linguistic transformation. We model the problem of diffraction generation as a string for sequencing learning problems and present a variant of the neural encoder decoder model to solve this problem. Our model is language-independent and can be trained in both supervised and semi-supervised environments. We evaluate our system using seven data sets of morphologically rich languages and achieve either better or comparable results with existing state-of-the-art models of diffraction generation."}, {"heading": "1 Introduction", "text": "In fact, the fact is that most of them will be able to move to another world, in which they are able, in which they are able to integrate, and in which they are able, in which they are able to change the world."}, {"heading": "2 Inflection Generation: Background", "text": "In fact, it is a way in which most people are able to determine for themselves how they want to behave and how they want to behave."}, {"heading": "3 Neural Encoder-Decoder Models", "text": "Here we briefly describe the underlying framework of our inflection generation model, called a recurrent encoder decoder (RNN) (Cho et al., 2014; Sutskever et al., 2014), which is used to convert an input sequence ~ x into an output sequence ~ y.We represent an item after x, an item sequence after ~ x, vectors after x, matrices after X, and vector sequences after ~ x."}, {"heading": "3.1 Formulation", "text": "Within the encoder decoder framework, an encoder reads a variable length input sequence, a vector sequence ~ x = < x1, \u00b7 \u00b7, xT > (according to a sequence of input symbols ~ x = < x1, \u00b7 \u00b7, xT >) and generates a fixed-dimensional vector representation of the sequence. xt-Rl is an input vector of length. The most common approach is to use an RNN in such a way that: ht = f (ht \u2212 1, xt) (1) (1), where ht-Rn is a hidden state at the time t, and f is generally a nonlinear transformation that produces e: = hT + 1 as an input representation. The decoder is trained to get the next output yt (ht \u2212 1) into the encoded input vector e and all previously predicted outputs < yt, \u00b7 \u00b7 yst, < n 1, other words: \u00b7 \u00b7 or \u00b7 \u00b7 n =."}, {"heading": "3.2 Long Short-Term Memory (LSTM)", "text": "In principle, RNNs allow the withholding of information from far-distant time steps, but the nonlinear \"squashing\" functions used in the calculation of each ht result in a decay of the error signal used in reverse propagation training. LSTMs are a variant of RNNNs designed to solve this problem of \"disappearing gradient\" with the help of an additional memory cell (Hochreiter and Schmidhuber, 1997; Graves, 2013). Previous work explains the calculation within an LSTM by metaphors of deciding how much of the current input into the memory (it) should pass over or (ft). We refer interested readers to the original documentation and present only the recursive equations that update the memory cell ct and the hidden state refers to the given text, the previous hidden state ht \u2212 1, and the memory cell ct \u2212 1: it = the original documents and present only the recursive equations that refresh the memory wave ct and the hidden state ht \u2212 1 \u2212 the hidden state ht \u2212 1 \u2212 the given text, the previous hidden state ht \u2212 1, and the memory cell ct \u2212 1: it = the wixxt + wixt \u2212 wihate \u2212 1 \u2212 wiht \u2212 1 \u2212 wiht \u2212 1 \u2212 the \u2212 wiht \u2212 1 \u2212 xt \u2212 1 \u2212 and \u2212 \u2212 \u2212 \u2212 1 \u2212 the memory cell is \u2212 1 \u2212 1."}, {"heading": "4 Inflection Generation Model", "text": "The standard encoder decoder models are designed for machine translation, where the goal is to translate a sentence (word sequence) from one language into a semantically equivalent sentence (word sequence) in another language. However, our problem differs from the above setting in two ways: (1) The input and output character sequences are largely similar, except for the inflections; (2) the input and output character sequences have different semantics. In terms of the first difference, the word play differs from the above setting in two ways: (1) The input and output character sequences are largely similar, but the input and output character sequences are different."}, {"heading": "4.1 Supervised Learning", "text": "The parameters of our model are the set of character vectors, the transformation parameters (Wtrans, btrans) and the parameters of the encoder and decoder LSTMs (\u00a7 3.2). We use a negative logarithmetic of the output character sequence as a loss function: \u2212 log p (~ y | ~ x) = \u2212 \u2211 T \u2032 t = 1 log p (yt | e, ~ y < t) (7) We minimize the loss due to stochastic updates with AdaDelta (Zeiler, 2012). This is our purely supervised model for the generation of deflections and we evaluate it in two different settings as determined by previous work: Factored Model. In the first setting we learn a separate model for each type of deflection independent of the other possible deflections. For example, with German nouns we learn the deflection of deflection, and with German verbs we learn 27 individual encoders of deflection models."}, {"heading": "4.2 Semi-supervised Learning", "text": "The model we have described so far is entirely based on the availability of pairs of root forms and inflected word forms for learning to generate diffractions. Although such monitored models can be used to obtain diffraction models (Durrett and DeNero, 2013; Ahlberg et al., 2015), it has been shown that unlabeled data can generally improve the performance of such systems (Ahlberg et al., 2014; Nicolai et al., 2015). We use this language model to make predictions about the next character in the sequence of what correct strings in a language look like. Thus, we learn a language model of the strings in a vocabulary extracted from a large unlabeled corpus. We use this language model to predict the next character in the sequence, with the previous characters predetermined in the following two settings."}, {"heading": "4.3 Ensembling", "text": "It has been shown that using an ensemble of models initialized differently and trained independently of each other leads to improved performance (Hansen and Salamon, 1990; Collobert et al., 2011). Therefore, for each type of model used in this work, we report results obtained using an ensemble of models. Thus, while we calculate the probability of emitting a character as an expert product of the individual models within the ensemble, pens (yt | \u00b7) = 1Z-k = 1 pi (yt | \u00b7) 1 k, where pi (yt | \u00b7) is the probability according to the i-th model and Z is the normalization factor."}, {"heading": "5 Experiments", "text": "We are now conducting experiments with the described models. Note that not all previously published models show results for all settings, so we compare our results with them wherever appropriate. Hyperparameters. In all models described in this paper, we use the following hyperparameters. In both the encoder and decoder models, we use single-layer LSTMs with the hidden vector of length 100. The length of the character vectors corresponds to the size of the character vocabulary depending on the dataset. The parameters are regulated with '2, with the regularization constant 10 \u2212 5. The number of models for similarity is k = 5."}, {"heading": "5.1 Data", "text": "Durrett and DeNero (2013) published the Wiktionary Inflection Dataset with training, development and test columns. The development and test sets each contain 200 diffraction tables and the training tables consist of the remaining data. This data set contains inflections for German, Finnish and Spanish. This data set has been extended by Dutch verbs extracted from the lexical database CELEX (Baayen et al., 1995), French verbs from verbs, a French online conjugation dictionary, and Czech nouns and verbs from the Prague Dependnecy Treebank (Hajic \u03a4et al., 2001). As the Czech data set contains many incomplete tables, we do not use it for our experiments. These data sets come with pre-specified training / developer / test columns that we use for each of these sets."}, {"heading": "5.2 Results", "text": "In fact, we are able to go in search of a solution that will enable us to find a solution that will enable us to find a solution, that will enable us to find a solution and that will enable us to find a solution that will enable us to find a solution, that will enable us to find a solution that will enable us to find a solution and that will enable us to find a solution that will enable us to find a solution."}, {"heading": "6 Analysis", "text": "In Figure 4, we show how the predictive accuracy of a deflected shape varies with respect to the length of the correct deflected shape. To get stable estimates, we use the in-3The mapping of accuracy against the length of the root shape follows a similar pattern, and we leave it here for Brewity.deflected shapes according to their length: < 5, [5, 10), [10, 15) and \u2265 15. The accuracy for each trash is macro averaged over 6 datasets4 for our factor model and the best models of DDN13 and NCK15. Our model consistently shows an improvement in performance as word length and is significantly better than DDN13 for words longer than 20 and is roughly the same as NCK15. In words of length < 5, we perform worse than DDN13 but better than NCK15."}, {"heading": "7 Related Work", "text": "In addition to the relevant work we have mentioned in the background (\u00a7 2) and throughout the work, we now briefly describe other areas of related work. Generating inflection morphology was particularly useful in the statistical machine4We remove DE-N as the smallest and show a high variance in results.5The total number of cases of vocal harmony in FI-NA is 4620 translations, both in translations from morphologically rich languages (Goldwater and McClosky, 2005) and into morphologically richer languages (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modelling the morphological structure of a word has also shown that the quality of word clusters (Clark et al, 2003) and word vector representations (Cotterell et al, 2015) is improved."}, {"heading": "8 Conclusion", "text": "We have presented a model that generates flexed shapes of a particular root shape by using a sequence of neural networks to sequence string converters. Our model obtains state-of-the-art results and works on seven different datasets at eye level or better than existing diffraction generation models. Our model is able to learn extensive dependencies within strings to generate diffraction, making it particularly suitable for morphologically rich languages."}], "references": [{"title": "Markus Forsberg", "author": ["Malin Ahlberg"], "venue": "and Mans Hulden.", "citeRegEx": "Ahlberg et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Markus Forsberg", "author": ["Malin Ahlberg"], "venue": "and Mans Hulden.", "citeRegEx": "Ahlberg et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Richard Piepenbrock", "author": ["Harald R. Baayen"], "venue": "and Leon Gulikers.", "citeRegEx": "Baayen et al.1995", "shortCiteRegEx": null, "year": 1995}, {"title": "Kyunghyun Cho", "author": ["Dzmitry Bahdanau"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bahdanau et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Data-driven sentence generation with non-isomorphic trees", "author": ["Bernd Bohnet", "Simon Mille", "Leo Wanner"], "venue": "In Proc. of NAACL", "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Improved transitionbased parsing by modeling characters instead of words with lstms", "author": ["Chris Dyer", "Noah A. Smith"], "venue": "In Proc. of EMNLP", "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "John G Cleary", "author": ["Timothy C Bell"], "venue": "and Ian H Witten.", "citeRegEx": "Bell et al.1990", "shortCiteRegEx": null, "year": 1990}, {"title": "Harri Hakonen", "author": ["Lasse Bergroth"], "venue": "and Timo Raita.", "citeRegEx": "Bergroth et al.2000", "shortCiteRegEx": null, "year": 2000}, {"title": "Translating into morphologically rich languages with synthetic phrases", "author": ["Eva Schlinger", "Noah A. Smith", "Chris Dyer"], "venue": "In Proc. of EMNLP", "citeRegEx": "Chahuneau et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chahuneau et al\\.", "year": 2013}, {"title": "Knowledge-rich morphological priors for bayesian language models", "author": ["Noah A Smith", "Chris Dyer"], "venue": "In Proc. of NAACL", "citeRegEx": "Chahuneau et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chahuneau et al\\.", "year": 2013}, {"title": "Holger Schwenk", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares"], "venue": "and Yoshua Bengio.", "citeRegEx": "Cho et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "The Sound Pattern of English", "author": ["Chomsky", "Halle1968] N. Chomsky", "M. Halle"], "venue": null, "citeRegEx": "Chomsky et al\\.,? \\Q1968\\E", "shortCiteRegEx": "Chomsky et al\\.", "year": 1968}, {"title": "Combining distributional and morphological information for part of speech induction", "author": ["Alexander Clark"], "venue": "In Proc. of EACL", "citeRegEx": "Clark.,? \\Q2003\\E", "shortCiteRegEx": "Clark.", "year": 2003}, {"title": "Combining morpheme-based machine translation with post-processing morpheme prediction", "author": ["Clifton", "Sarkar2011] Ann Clifton", "Anoop Sarkar"], "venue": "In Proc. of ACL", "citeRegEx": "Clifton et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Clifton et al\\.", "year": 2011}, {"title": "Koray Kavukcuoglu", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen"], "venue": "and Pavel Kuksa.", "citeRegEx": "Collobert et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Morphological word-embeddings", "author": ["Cotterell", "Sch\u00fctze2015] Ryan Cotterell", "Hinrich Sch\u00fctze"], "venue": "In Proc. of NAACL", "citeRegEx": "Cotterell et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cotterell et al\\.", "year": 2015}, {"title": "Discovering morphological paradigms from plain text using a dirichlet process mixture model", "author": ["Dreyer", "Eisner2011] Markus Dreyer", "Jason Eisner"], "venue": "In Proc. of EMNLP", "citeRegEx": "Dreyer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dreyer et al\\.", "year": 2011}, {"title": "Supervised learning of complete morphological paradigms", "author": ["Durrett", "DeNero2013] Greg Durrett", "John DeNero"], "venue": "In Proc. of NAACL", "citeRegEx": "Durrett et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Durrett et al\\.", "year": 2013}, {"title": "Parameter estimation for probabilistic finite-state transducers", "author": ["Jason Eisner"], "venue": "In Proc. of ACL", "citeRegEx": "Eisner.,? \\Q2002\\E", "shortCiteRegEx": "Eisner.", "year": 2002}, {"title": "Aoife Cahill", "author": ["Alexander Fraser", "Marion Weller"], "venue": "and Fabienne Cap.", "citeRegEx": "Fraser et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Schraudolph", "author": ["Felix A. Gers", "Nicol N"], "venue": "and J\u00fcrgen Schmidhuber.", "citeRegEx": "Gers et al.2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Improving statistical MT through morphological analysis", "author": ["Goldwater", "McClosky2005] Sharon Goldwater", "David McClosky"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Goldwater et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Goldwater et al\\.", "year": 2005}, {"title": "Santiago Fern\u00e1ndez", "author": ["Alex Graves"], "venue": "and J\u00fcrgen Schmidhuber.", "citeRegEx": "Graves et al.2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Generating sequences with recurrent neural networks. CoRR, abs/1308.0850", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Jan Koutn\u0131\u0301k", "author": ["Klaus Greff", "Rupesh Kumar Srivastava"], "venue": "Bas R. Steunebrink, and J\u00fcrgen Schmidhuber.", "citeRegEx": "Greff et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Barbora Vidov\u00e1-Hladk\u00e1", "author": ["Jan Haji\u010d"], "venue": "and Petr Pajas.", "citeRegEx": "Haji\u010d et al.2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Neural network ensembles", "author": ["Hansen", "Salamon1990] Lars Kai Hansen", "Peter Salamon"], "venue": "In Proc. of PAMI", "citeRegEx": "Hansen et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Hansen et al\\.", "year": 1990}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Tuning as ranking", "author": ["Hopkins", "May2011] Mark Hopkins", "Jonathan May"], "venue": "In Proc. of EMNLP", "citeRegEx": "Hopkins et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hopkins et al\\.", "year": 2011}, {"title": "Generalizing inflection tables into paradigms with finite state operations", "author": ["Mans Hulden"], "venue": "In Proc. of the Joint Meeting of SIGMORPHON and SIGFSM", "citeRegEx": "Hulden.,? \\Q2014\\E", "shortCiteRegEx": "Hulden.", "year": 2014}, {"title": "Regular models of phonological rule systems", "author": ["Kaplan", "Kay1994] Ronald M Kaplan", "Martin Kay"], "venue": "Computational linguistics,", "citeRegEx": "Kaplan et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Kaplan et al\\.", "year": 1994}, {"title": "Twolevel morphology: A general computational model for word-form recognition and production", "author": ["Kimmo Koskenniemi"], "venue": "University of Helsinki", "citeRegEx": "Koskenniemi.,? \\Q1983\\E", "shortCiteRegEx": "Koskenniemi.", "year": 1983}, {"title": "Tiago Lu\u0131\u0301s", "author": ["Wang Ling"], "venue": "Lu\u0131\u0301s Marujo, R\u00e1mon Fernandez Astudillo, Silvio Amir, Chris Dyer, Alan W Black, and Isabel Trancoso.", "citeRegEx": "Ling et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Christopher D", "author": ["Thang Luong", "Hieu Pham"], "venue": "Manning.", "citeRegEx": "Luong et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Alicia Burga", "author": ["Simon Mille"], "venue": "and Leo Wanner.", "citeRegEx": "Mille et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Kristina Toutanova", "author": ["Einat Minkov"], "venue": "and Hisami Suzuki.", "citeRegEx": "Minkov et al.2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Colin Cherry", "author": ["Garrett Nicolai"], "venue": "and Grzegorz Kondrak.", "citeRegEx": "Nicolai et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "\u00d6zlem \u00e7etino\u011flu", "author": ["Kemal Oflazer"], "venue": "and Bilge Say.", "citeRegEx": "Oflazer et al.2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Error-tolerant finitestate recognition with applications to morphological analysis and spelling correction", "author": ["Kemal Oflazer"], "venue": "Computational Linguistics,", "citeRegEx": "Oflazer.,? \\Q1996\\E", "shortCiteRegEx": "Oflazer.", "year": 1996}, {"title": "Learning stochastic edit distance: Application in handwritten character recognition", "author": ["Oncina", "Sebban2006] Jose Oncina", "Marc Sebban"], "venue": "Pattern recognition,", "citeRegEx": "Oncina et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Oncina et al\\.", "year": 2006}, {"title": "Todd Ward", "author": ["Kishore Papineni", "Salim Roukos"], "venue": "and Wei-Jing Zhu.", "citeRegEx": "Papineni et al.2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["Santos", "Zadrozny2014] Cicero D. Santos", "Bianca Zadrozny"], "venue": "In Proc. of ICML", "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Semi-markov conditional random fields for information extraction", "author": ["Sarawagi", "Cohen2004] Sunita Sarawagi", "William W Cohen"], "venue": "In Proc. of NIPS", "citeRegEx": "Sarawagi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Sarawagi et al\\.", "year": 2004}, {"title": "Srilm-an extensible language modeling toolkit", "author": ["Andreas Stolcke"], "venue": "In Proc. of Interspeech", "citeRegEx": "Stolcke.,? \\Q2002\\E", "shortCiteRegEx": "Stolcke.", "year": 2002}, {"title": "Oriol Vinyals", "author": ["Ilya Sutskever"], "venue": "and Quoc VV Le.", "citeRegEx": "Sutskever et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Hisami Suzuki", "author": ["Kristina Toutanova"], "venue": "and Achim Ruopp.", "citeRegEx": "Toutanova et al.2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Visualizing Data using t-SNE", "author": ["van der Maaten", "Geoffrey Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Multilingual noise-robust supervised morphological analysis using the wordframe model", "author": ["Richard Wicentowski"], "venue": "In Proc. of SIGPHON", "citeRegEx": "Wicentowski.,? \\Q2004\\E", "shortCiteRegEx": "Wicentowski.", "year": 2004}, {"title": "Minimally supervised morphological analysis by multimodal alignment", "author": ["Yarowsky", "Wicentowski2000] David Yarowsky", "Richard Wicentowski"], "venue": "In Proc. of ACL", "citeRegEx": "Yarowsky et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Yarowsky et al\\.", "year": 2000}, {"title": "Adadelta: An adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [], "year": 2017, "abstractText": "Morphological inflection generation is the task of generating the inflected form of a given lemma corresponding to a particular linguistic transformation. We model the problem of inflection generation as a character sequence to sequence learning problem and present a variant of the neural encoder-decoder model for solving it. Our model is language independent and can be trained in both supervised and semi-supervised settings. We evaluate our system on seven datasets of morphologically rich languages and achieve either better or comparable results to existing state-of-the-art models of inflection generation.", "creator": "LaTeX with hyperref package"}}}