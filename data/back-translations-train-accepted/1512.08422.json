{"id": "1512.08422", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Dec-2015", "title": "Natural Language Inference by Tree-Based Convolution and Heuristic Matching", "abstract": "Recognizing entailment and contradiction between two sentences has wide applications in NLP. Traditional methods include feature-rich classifiers or formal reasoning. However, they are usually limited in terms of accuracy and scope. Recently, the renewed prosperity neural networks has made many improvements in a variety of NLP tasks. In particular, the tree-based convolutional neural network (TBCNN) proposed in our previous work has achieved high performance in several sentence-level classification tasks. But whether TBCNN is applicable to recognize entailment and contradiction between two sentences remains unknown. In this paper, we propose TBCNN-pair model for entailment/contradiction recognition. Experimental results in a large dataset verifies the rationale of using TBCNN as the sentence-level model; leveraging additional heuristics like element-wise product/difference further improves the result, and outperforms published results by a large margin.", "histories": [["v1", "Mon, 28 Dec 2015 14:28:21 GMT  (225kb,D)", "https://arxiv.org/abs/1512.08422v1", null], ["v2", "Fri, 22 Apr 2016 17:49:46 GMT  (238kb,D)", "http://arxiv.org/abs/1512.08422v2", "Accepted by ACL'16 as a short paper"], ["v3", "Fri, 13 May 2016 16:24:56 GMT  (238kb,D)", "http://arxiv.org/abs/1512.08422v3", "Accepted by ACL'16 as a short paper"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["lili mou", "rui men", "ge li", "yan xu", "lu zhang 0023", "rui yan", "zhi jin"], "accepted": true, "id": "1512.08422"}, "pdf": {"name": "1512.08422.pdf", "metadata": {"source": "CRF", "title": "Natural Language Inference by Tree-Based Convolution and Heuristic Matching", "authors": ["Lili Mou", "Rui Men", "Ge Li", "Yan Xu", "Lu Zhang", "Rui Yan", "Zhi Jin"], "emails": ["doublepower.mou@gmail.com", "menruimr@gmail.com", "lige@sei.pku.edu.cn", "xuyan14@sei.pku.edu.cn", "zhanglu@sei.pku.edu.cn", "zhijin@sei.pku.edu.cn", "yanrui02@baidu.com"], "sections": [{"heading": "1 Introduction", "text": "Even when it comes to the question of whether the hypothesis can actually be true with respect to the individual sentences, the hypothesis cannot be true (contradiction). Several examples are illustrated in Table 1.NLI, which is based on the core of understanding in language and has broad ranges of application, e.g. answering questions (Harabagiu and Hickl, 2006) and automatic summary (Lacatusu et al., 2006). Yan et al., 2011a and al. They also relate to other tasks of the sentence pair, including the recognition of paraphrases (Hu et al., 2014), the recognition of discourses (Liu et al., 2016), etc. Traditional approaches to which I have mainly represented two groups."}, {"heading": "2 Related Work", "text": "Most neural networks in this area involve a set-level model, followed by one or a few matching levels. They are sometimes referred to as \"Siamese\" architectures (Bromley et al., 1993).Hu et al. (2014) and Yin and Protectors (2015) apply Convolutionary Neural Networks (CNNs) as an individual set model, in which a series of feature detectors are designed using consecutive words to extract local characteristics. Wan et al. (2015) build set-pair models on recurrent neural networks (RNNs) on iteratively integrated information along a set. Socher et al al al al al al al al al al al al. (2011a) dynamically construct tree structures (analogous to analyze trees) using recursive autocoders to detect paraphrase between two sets. As shown, innate structural information in sentences is often important for understanding natural language."}, {"heading": "3 Our Approach", "text": "We follow the \"Siamese\" architecture (like most of the work in Section 2) and follow a two-step strategy for classifying the relationship between two sentences. Specifically, our model consists of two parts: \u2022 A tree-based Convolutionary Neural Network models each sentence (Figure 1a). Note that the two sentences, premise and hypothesis, share the same TBCNN model (with the same parameters), as this part aims to capture the general semantics of sentences. \u2022 A matching layer combines the creation of two sentences through heuristics (Figure 1b). According to individual sentence models, we design a sentence match layer to aggregate information. We use simple heuristics, including concentrate nation, elementary products and differences, which are effective and efficient. Finally, we add a Softmax layer for the output. The training goal is entropy loss, and we use stochastic descent in minibatch, which is backaggregated."}, {"heading": "3.1 Tree-Based Convolution", "text": "In this article, the tree-based confrontation with the process of confrontation is described in detail. TBCNN's basic idea is to design a series of subranges that defy the parser tree of a sentence. Every node in the definition of a dependency tree applies to a dependency tree. In this paper, we prefer tree-based confrontation for its efficiency and compact expression. Specifically, a sentence is transformed into a dependence on parser judgments. Each node in the definition corresponds to a word in the sentence; an edge in the confrontation is preferred for its efficiency and compact expression."}, {"heading": "3.2 Matching Heuristics", "text": "In this part, we will present how vector representations of individual propositions are combined to grasp the relationship between the premise and hypothesis. As the data set is large, we prefer O (1) matching operations for efficiency reasons. Specifically, we have three identical heuristics: \u2022 concatenation of the two proposition vectors, \u2022 element-wise product and \u2022 element-wise difference. The first heuristics follows the most common approach of the \"Siamese\" architectures, while the latter two are specific measures of \"similarity\" or \"proximity.\" These matching layers are further concatenated (Figure 1b), where bym = [h1; h2; h1 \u2212 h2; h2 \u2012 h2] where h1 Rnc and h2 Rnc are the proposition vectors of the premise and hypothesis that reflect the difference between the premises and the hypotheses (Figure 1b)."}, {"heading": "4 Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Dataset", "text": "To evaluate our TBCNN pair model, we used the newly published dataset from Stanford Natural Language Inference (SNLI) (Bowman et al., 2015).4 The dataset consists of crowdsourcing efforts, with each set of people written. In addition, the SNLI dataset is many times larger than previous resources, making it particularly suitable for comparing neural models. Target labels include three classes: Adapt, Contradict, and Neutral (two irrelevant sets).We applied the standard train / validation / test split, pitting 550k, 10k, and 10k samples, respectively. Figure 2 shows 4http: / / nlp.stanford.edu / projects / snli / additional dataset statistics, especially those relevant to dependency savings agreements.5"}, {"heading": "4.2 Hyperparameter Settings", "text": "All of our neural layers, including embedding, are set to 300 dimensions. the model is usually robust when the dimension is large, e.g. several hundred (Collobert and Weston, 2008). Word embedding was pre-trained by word2vec on the English Wikipedia corpus and refined during training as part of model parameters. We applied '2 penalty of 3 x 10 \u2212 4; dropouts were selected by validation with a granularity of 0.1 (Figure 2). We see that a large failure rate (\u2265 0.3) affects the performance of such a large dataset (and also slows training), as opposed to small datasets in other tasks (Peng et al., 2015)."}, {"heading": "4.3 Performance", "text": "Table 3 compares our model with previous results. As can be seen, the TBCNN sentence pair model, followed by simple concatenation alone, outperforms existing sentence coding-based approaches (without prior training), including a feature-rich method that removes 6 sets of human traits, long-term short-term memory, and traditional CNNs. We used broken dependency trees in which prepositions and conjunctions are noted on the dependency relationships, but these auxiliary words are removed themselves. (LSTM) -based RNNNNs and traditional CNNs. This confirms the logic for using tree-based folding as a neural sentence model for NLI. Table 4 compares different heuristics of matching. First, we analyze each heuristic product individually: the use of elementary products alone is significantly worse than concatenation or elementary difference; the latter two are comparable to any other. Combining different matching heuristics results in higher accuracy than the result of the single pair: the difference between the TCNN and the pair model."}, {"heading": "4.4 Complexity Concerns", "text": "For most sentence models, including TBCNN, the overall complexity is at least O (n). Nevertheless, an efficient matching approach is still important, especially for call-and-restore systems (Yan et al., 2016; Li et al., 2016). For example, in a call-based question or call system, we can reduce response time by sentence matching based on candidate embedding. In contrast, context-sensitive matching approaches as described in Section 2 involve processing each candidate given a new custom query, which is time consuming for most industry products.In our experiments, the matching part (Figure 1b) accounts for 1.71% of the total time during the prediction (single CPU, C + + implementation), demonstrating the potential applications of our approach to efficiently query semantically related sentences."}, {"heading": "5 Conclusion", "text": "In this paper, we proposed the TBCNN pair model for natural language conclusions. Our model relies on the tree-based Convolutionary Neural Network (TBCNN) to capture the semantics of the sentence plane; then the information of two sentences is combined by multiple heuristics, including concatenation, elementary product, and difference. Experimental results on a large dataset show high performance of our TBCNN pair model with low complexity order."}, {"heading": "Acknowledgments", "text": "We thank all anonymous reviewers for their constructive comments, especially on complexity issues. We also thank Sam Bowman, Edward Grefenstette and Tim Rockta \ufffd schel for their discussion. This research was supported by the National Basic Research Programme of China (the 973 Programme) under grant number 2015CB352201 and the National Science Foundation of China under grant numbers 61232015, 61421091 and 61502014."}], "references": [{"title": "Combining shallow and deep NLP methods for recognizing textual entailment", "author": ["Bos", "Markert2005] Johan Bos", "Katja Markert"], "venue": "In Proceedings of the First PASCAL Challenges Workshop on Recognising Textual Entailment,", "citeRegEx": "Bos et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bos et al\\.", "year": 2005}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Gabor Angeli", "Christopher Potts", "Christopher D. Manning"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Signature verification using a \u201cSiamese", "author": ["Bromley et al.1993] Jane Bromley", "James W Bentz", "L\u00e9on Bottou", "Isabelle Guyon", "Yann LeCun", "Cliff Moore", "Eduard S\u00e4ckinger", "Roopak Shah"], "venue": null, "citeRegEx": "Bromley et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Bromley et al\\.", "year": 1993}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th International Conference on Machine learning,", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Generating typed dependency parses from phrase structure parses", "author": ["Bill MacCartney", "Christopher D Manning"], "venue": "In Proceedings of the Language Resource and Evaluation Conference,", "citeRegEx": "Marneffe et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2006}, {"title": "Methods for using textual entailment in open-domain question answering", "author": ["Harabagiu", "Hickl2006] Sanda Harabagiu", "Andrew Hickl"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual", "citeRegEx": "Harabagiu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Harabagiu et al\\.", "year": 2006}, {"title": "Negation, contrast and contradiction in text processing", "author": ["Andrew Hickl", "Finley Lacatusu"], "venue": "In Proceedings of AAAI Conference on Artificial Intelligence,", "citeRegEx": "Harabagiu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Harabagiu et al\\.", "year": 2006}, {"title": "Multi-perspective sentence similarity modeling with convolutional neural networks", "author": ["He et al.2015] Hua He", "Kevin Gimpel", "Jimmy Lin"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Hu et al.2014] Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "A convolutional neural network for modelling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": null, "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "LCCs GISTexter at DUC 2006: Multi-strategy multidocument summarization", "author": ["Andrew Hickl", "Kirk Roberts", "Ying Shi", "Jeremy Bensley", "Bryan Rink", "Patrick Wang", "Lara Taylor"], "venue": "Proceedings of DUC", "citeRegEx": "Lacatusu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lacatusu et al\\.", "year": 2006}, {"title": "StalemateBreaker: A proactive content-introducing approach to automatic humancomputer conversation", "author": ["Li et al.2016] Xiang Li", "Lili Mou", "Rui Yan", "Ming Zhang"], "venue": "In Proceedings of the 25th International Joint Conference on Artificial Intelli-", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Implicit discourse relation classification via multi-task neural networks", "author": ["Liu et al.2016] Yang Liu", "Sujian Li", "Xiaodong Zhang", "Zhifang Sui"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Learning to recognize features of valid textual entailments", "author": ["Trond Grenager", "Marie-Catherine de Marneffe", "Daniel Cer", "Christopher D. Manning"], "venue": "In Proceedings of the Human Language Technology", "citeRegEx": "MacCartney et al\\.,? \\Q2006\\E", "shortCiteRegEx": "MacCartney et al\\.", "year": 2006}, {"title": "Natural Language Inference", "author": ["Bill MacCartney"], "venue": "Ph.D. thesis,", "citeRegEx": "MacCartney.,? \\Q2009\\E", "shortCiteRegEx": "MacCartney.", "year": 2009}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Wen-tau Yih", "Geoffrey Zweig"], "venue": "In NAACL-HLT,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Discriminative neural sentence modeling by tree-based convolution", "author": ["Mou et al.2015] Lili Mou", "Hao Peng", "Ge Li", "Yan Xu", "Lu Zhang", "Zhi Jin"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Mou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2015}, {"title": "Convolutional neural networks over tree structures for programming language processing", "author": ["Mou et al.2016] Lili Mou", "Ge Li", "Lu Zhang", "Tao Wang", "Zhi Jin"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence", "citeRegEx": "Mou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2016}, {"title": "A comparative study on regularization strategies for embedding-based neural networks", "author": ["Peng et al.2015] Hao Peng", "Lili Mou", "Ge Li", "Yunchuan Chen", "Yangyang Lu", "Zhi Jin"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods", "citeRegEx": "Peng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2015}, {"title": "Reasoning about entailment with neural attention", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Phil Blunsom"], "venue": "In Proceedings of the International Conference on Learning", "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2016}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Eric H Huang", "Jeffrey Pennin", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Advances in Neural Information Processing Sys-", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Jeffrey Pennington", "Eric H Huang", "Andrew Y Ng", "Christopher D Manning"], "venue": "In Proceedings of the Conference on Empiri-", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "A deep architecture for semantic matching with multiple positional sentence representations. arXiv preprint arXiv:1511.08277", "author": ["Wan et al.2015] Shengxian Wan", "Yanyan Lan", "Jiafeng Guo", "Jun Xu", "Liang Pang", "Xueqi Cheng"], "venue": null, "citeRegEx": "Wan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2015}, {"title": "Classifying relations via long short term memory networks along shortest dependency paths", "author": ["Xu et al.2015] Yan Xu", "Lili Mou", "Ge Li", "Yunchuan Chen", "Hao Peng", "Zhi Jin"], "venue": "In Proceedings of Conference on Empirical Methods in Natural Language", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Improved relation classification by deep recurrent neural networks with data augmentation", "author": ["Xu et al.2016] Yan Xu", "Ran Jia", "Lili Mou", "Ge Li", "Yunchuan Chen", "Yangyang Lu", "Zhi Jin"], "venue": "arXiv preprint arXiv:1601.03651", "citeRegEx": "Xu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2016}, {"title": "Timeline generation through evolutionary trans-temporal summarization", "author": ["Yan et al.2011a] Rui Yan", "Liang Kong", "Congrui Huang", "Xiaojun Wan", "Xiaoming Li", "Yan Zhang"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural", "citeRegEx": "Yan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2011}, {"title": "Evolutionary timeline summarization: A balanced optimization framework via iterative substitution", "author": ["Yan et al.2011b] Rui Yan", "Xiaojun Wan", "Jahna Otterbacher", "Liang Kong", "Xiaoming Li", "Yan Zhang"], "venue": null, "citeRegEx": "Yan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2011}, {"title": "Learning to respond with deep neural networks for retrieval based human-computer conversation system", "author": ["Yan et al.2016] Rui Yan", "Yiping Song", "Hua Wu"], "venue": "In Proceedings of the 39th International ACM SIGIR Conference on Research and De-", "citeRegEx": "Yan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2016}, {"title": "Convolutional neural network for paraphrase identification", "author": ["Yin", "Sch\u00fctze2015] Wenpeng Yin", "Hinrich Sch\u00fctze"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association", "citeRegEx": "Yin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2015}, {"title": "Shallow convolutional neural network for implicit discourse relation recognition", "author": ["Zhang et al.2015] Biao Zhang", "Jinsong Su", "Deyi Xiong", "Yaojie Lu", "Hong Duan", "Junfeng Yao"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 14, "context": "tween two sentences (called a premise and a hypothesis) is known as natural language inference (NLI) in MacCartney (2009). Provided with a premise sentence, the task is to judge whether the hypothesis can be inferred (entailment), or the", "startOffset": 104, "endOffset": 122}, {"referenceID": 10, "context": ", question answering (Harabagiu and Hickl, 2006) and automatic summarization (Lacatusu et al., 2006; Yan et al., 2011a; Yan et al., 2011b).", "startOffset": 77, "endOffset": 138}, {"referenceID": 8, "context": "Moreover, NLI is also related to other tasks of sentence pair modeling, including paraphrase detection (Hu et al., 2014), relation recognition of discourse units (Liu et al.", "startOffset": 103, "endOffset": 120}, {"referenceID": 12, "context": ", 2014), relation recognition of discourse units (Liu et al., 2016), etc.", "startOffset": 49, "endOffset": 67}, {"referenceID": 13, "context": "(MacCartney et al., 2006; Harabagiu et al., 2006).", "startOffset": 0, "endOffset": 49}, {"referenceID": 5, "context": "(MacCartney et al., 2006; Harabagiu et al., 2006).", "startOffset": 0, "endOffset": 49}, {"referenceID": 9, "context": "The renewed prosperity of neural networks has made significant achievements in various NLP applications, including individual sentence modeling (Kalchbrenner et al., 2014; Mou et al., 2015) as well as sentence matching (Hu et al.", "startOffset": 144, "endOffset": 189}, {"referenceID": 17, "context": "The renewed prosperity of neural networks has made significant achievements in various NLP applications, including individual sentence modeling (Kalchbrenner et al., 2014; Mou et al., 2015) as well as sentence matching (Hu et al.", "startOffset": 144, "endOffset": 189}, {"referenceID": 2, "context": "A typical neural architecture to model sentence pairs is the \u201cSiamese\u201d structure (Bromley et al., 1993), which involves an underlying sentence model and a matching layer to determine the relationship between two sentences.", "startOffset": 81, "endOffset": 103}, {"referenceID": 9, "context": "Prevailing sentence models include convolutional networks (Kalchbrenner et al., 2014) and recurrent/recursive networks (Socher et al.", "startOffset": 58, "endOffset": 85}, {"referenceID": 17, "context": "in two sentence classification tasks (Mou et al., 2015).", "startOffset": 37, "endOffset": 55}, {"referenceID": 2, "context": "They are sometimes called \u201cSiamese\u201d architectures (Bromley et al., 1993).", "startOffset": 50, "endOffset": 72}, {"referenceID": 21, "context": "Wan et al. (2015) build sentence pair models upon recurrent neural networks (RNNs) to iteratively integrate information along a sentence.", "startOffset": 0, "endOffset": 18}, {"referenceID": 21, "context": "Socher et al. (2011a) dynamically construct tree structures (analogous to parse trees) by recursive autoencoders to detect paraphrase between two sentences.", "startOffset": 0, "endOffset": 22}, {"referenceID": 24, "context": "sification (Xu et al., 2015; Xu et al., 2016).", "startOffset": 11, "endOffset": 45}, {"referenceID": 25, "context": "sification (Xu et al., 2015; Xu et al., 2016).", "startOffset": 11, "endOffset": 45}, {"referenceID": 7, "context": "He et al. (2015) apply additional heuristics, namely Euclidean distance, cosine measure, and elementwise absolute difference.", "startOffset": 0, "endOffset": 17}, {"referenceID": 21, "context": "(2014) (Arc-II) concatenate two words\u2019 vectors (after convolution), Socher et al. (2011a) compute Euclidean distance, and Wan et al.", "startOffset": 68, "endOffset": 90}, {"referenceID": 21, "context": "(2014) (Arc-II) concatenate two words\u2019 vectors (after convolution), Socher et al. (2011a) compute Euclidean distance, and Wan et al. (2015) apply tensor product.", "startOffset": 68, "endOffset": 140}, {"referenceID": 20, "context": "Recently, Rockt\u00e4schel et al. (2016) introduce several context-aware methods for sentence matching.", "startOffset": 10, "endOffset": 36}, {"referenceID": 28, "context": "In some scenarios like sentence pair re-ranking (Yan et al., 2016), it", "startOffset": 48, "endOffset": 66}, {"referenceID": 20, "context": "Rockt\u00e4schel et al. (2016) further develop a word-by-word attention mechanism and obtain a higher accuracy with a complexity order ofO(n2).", "startOffset": 0, "endOffset": 26}, {"referenceID": 18, "context": "The tree-based convolutoinal neural network (TBCNN) is first proposed in our previous work (Mou et al., 2016)2 to classify program source code; later, we further propose TBCNN variants", "startOffset": 91, "endOffset": 109}, {"referenceID": 17, "context": "to model sentences (Mou et al., 2015).", "startOffset": 19, "endOffset": 37}, {"referenceID": 8, "context": "A dynamic pooling layer is applied to aggregate information along different parts of the tree, serving as a way of semantic compositionality (Hu et al., 2014).", "startOffset": 141, "endOffset": 158}, {"referenceID": 17, "context": "TBCNN is effective and efficient in learning such structural information (Mou et al., 2015).", "startOffset": 73, "endOffset": 91}, {"referenceID": 7, "context": "Although element-wise distance is used to detect paraphrase in He et al. (2015), it mainly reflects \u201csimilarity\u201d information.", "startOffset": 63, "endOffset": 80}, {"referenceID": 1, "context": "To evaluate our TBCNN-pair model, we used the newly published Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015).", "startOffset": 113, "endOffset": 134}, {"referenceID": 19, "context": "3) hurts the performance (and also makes training slow) for such a large dataset as opposed to small datasets in other tasks (Peng et al., 2015).", "startOffset": 125, "endOffset": 144}, {"referenceID": 28, "context": "However, an efficient matching approach is still important, especially to retrieval-and-reranking systems (Yan et al., 2016; Li et al., 2016).", "startOffset": 106, "endOffset": 141}, {"referenceID": 11, "context": "However, an efficient matching approach is still important, especially to retrieval-and-reranking systems (Yan et al., 2016; Li et al., 2016).", "startOffset": 106, "endOffset": 141}], "year": 2016, "abstractText": "In this paper, we propose the TBCNNpair model to recognize entailment and contradiction between two sentences. In our model, a tree-based convolutional neural network (TBCNN) captures sentencelevel semantics; then heuristic matching layers like concatenation, element-wise product/difference combine the information in individual sentences. Experimental results show that our model outperforms existing sentence encoding-based approaches by a large margin.", "creator": "LaTeX with hyperref package"}}}