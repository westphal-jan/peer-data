{"id": "1512.08787", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Dec-2015", "title": "Matrix Completion Under Monotonic Single Index Models", "abstract": "Most recent results in matrix completion assume that the matrix under consideration is low-rank or that the columns are in a union of low-rank subspaces. In real-world settings, however, the linear structure underlying these models is distorted by a (typically unknown) nonlinear transformation. This paper addresses the challenge of matrix completion in the face of such nonlinearities. Given a few observations of a matrix that are obtained by applying a Lipschitz, monotonic function to a low rank matrix, our task is to estimate the remaining unobserved entries. We propose a novel matrix completion method that alternates between low-rank matrix estimation and monotonic function estimation to estimate the missing matrix elements. Mean squared error bounds provide insight into how well the matrix can be estimated based on the size, rank of the matrix and properties of the nonlinear transformation. Empirical results on synthetic and real-world datasets demonstrate the competitiveness of the proposed approach.", "histories": [["v1", "Tue, 29 Dec 2015 20:52:41 GMT  (148kb,D)", "http://arxiv.org/abs/1512.08787v1", "21 pages, 5 figures, 1 table. Accepted for publication at NIPS 2015"]], "COMMENTS": "21 pages, 5 figures, 1 table. Accepted for publication at NIPS 2015", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["ravi sastry ganti mahapatruni", "laura balzano", "rebecca willett"], "accepted": true, "id": "1512.08787"}, "pdf": {"name": "1512.08787.pdf", "metadata": {"source": "CRF", "title": "Matrix Completion Under Monotonic Single Index Models", "authors": ["Ravi Ganti", "Laura Balzano"], "emails": ["gantimahapat@wisc.edu", "girasole@umich.edu", "rmwillett@wisc.edu"], "sections": [{"heading": "1 Introduction", "text": "In matrix completion, you have access to a matrix with only a few observed entries, and the task is to estimate the entire matrix from the entries observed. (Matrix completion has been well studied in machine learning, and we now know how to restore certain matrices if you start from some observed entries in the matrix [3, 4, 6, 7, 8] if you start from a low rank. While the recent work focuses on loosening the incoherence and sampling conditions of matrix completion under which matrix completion succeeds, there is little work for matrix completion if the underlying matrix is of high rank."}, {"heading": "1.1 Our Model and contributions", "text": "In this book, we are able to line up in search of a solution that could enable us to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position."}, {"heading": "2 Related work", "text": "The recovery techniques proposed in these papers solve a convex optimization problem, which has the nuclear standard of the matrix under the condition that the matrix reaches a low rank. B) The matrix is incoherent or not very pointed, and c) the escapes are observed randomly. Literature based on the nuclear standard guarantees the recovery of the matrix under the condition that the matrix is low."}, {"heading": "3 Algorithms for matrix completion", "text": "Our goal is to estimate g? and z? based on the model in equations (3-4). We approach this problem by mathematical optimization. Before discussing our algorithms, we will briefly mention an algorithm for the problem of learning Lipschitz, monotonous functions in the 1-dimension. This algorithm is used to learn the link function in MMC. The LPAV algorithm: Let's say we get data (p1, y1),. (pn, yn), where p1 \u2264 p2.... \u2264 pn and y1,. yn are real numbers. Let's say G def = {g: R \u2192 R, g is L-Lipschitz and monoton. the LPAV-4 algorithm introduced in [11] gives the best function g in G, the n = 1 (pi) def = 1 (pi) \u2212 yi) 2. To do this, the LPAV-4 algorithm first solves the following problem: The best algorithm in G: The optimization."}, {"heading": "3.1 Squared loss minimization", "text": "To do this, we must solve the following optimization problem: min g, z \u00b2 (g (Zi, j) \u2212 Xi, j) 2g: R \u2192 R is L-Lipschitz and monotonous rank (Z) \u2264 r. (7) The problem is a non-convex optimization problem individually in the parameters g, Z. A reasonable approach to solving this optimization problem would be to optimize w.r.t. Each variable is fixed while retaining the other variable. For example, in the iteration t, while the estimate of Z is a fixed, say gt \u2212 1, and then a projected gradient departure w.r.t. Z. This leads to the following updates for Z: Zti, j \u00b2 Zt \u2212 1i, j \u2212 Pt \u2212 1 (g \u2212 1i, j \u2212 Xi, j \u2212 Projected gradient Descent w.r.t.)."}, {"heading": "3.2 Minimization of a calibrated loss function and the MMC algorithm.", "text": "Let us understand the MMC model in Equation (3) as a minimizer of the function L (Z). (3) Let us understand the function of the function L (Z). (3) Let us understand the function L (Z). (4) Let us consider the function Z (Z). (4) Let us be satisfied with the function Z (Z). (4) LPAV stands for Lipschitz Pool Adjacent ViolatorThe above loss function is convex in Z, because it is a convex. (4) Differentiation of the expression on the R.H.S. equation 10 w.r.t. Z, and the setting to 0, we get it. (i) LPAV (Zi, j) \u2212 EXi, j = 0. (11) The MMC model shown in Equation (3) fulfills the function L (Z) and is therefore a minimizer of the function L (Z)."}, {"heading": "4 MSE Analysis of MMC", "text": "We will analyze our algorithms, MMC, for the case of T = 1, under the premise of M = 1, which is shown in equations (4) and (3). In addition, we will assume that the matrices Z? and M? are limited in absolute values by 1. (18) g) If T = 1, the MMC algorithm is shown in Equation (17), we can find in Equation (18) that M + 1, j = g = 2, which is shown in the equation (17) with Z. This allows us, M + 1, j = g, which is defined in Equation (18)."}, {"heading": "5 Experimental results", "text": "We compare the performance of MMC \u2212 1, MMC \u2212 c, MMC-LS and nuclear-based matrix completion (LRMC) [4] on different synthetic and real datasets. The objective metric we use to compare different algorithms is the mean square error (RMSE) of algorithms on unobserved test indices of the incomplete matrix."}, {"heading": "5.1 Efficient implementation of MMC", "text": "The MMC algorithm consists of three main steps. A gradient gradation problem follows the set of the above three steps, and a QP optimization package is coupled to the correct values for the precedence r, and the step size is used in step 4 of the MMC. However, a straight-line implementation of the above three steps can be coupled with a grid search over r and \u03b7. Such an implementation would be inefficient and unscalable beyond small matrices. Since r is assumed to need to find only the top few vectors of the intermediate matrices. We use PROPACK 5 for efficient SVD implementation. Rather than searching for r on a grid, we use an increasing ranking method to estimate r. The increasing ranking procedure starts from a small value for r, say rmin, and increases the current estimate of r, through rinc, whenever there is no major progress in iterates."}, {"heading": "5.2 Synthetic experiments", "text": "For our synthetic experiments, we created a random 30 \u00b7 20 matrix Z? of rank 5 by creating the product of two random Gaussian matrices of size n \u00b7 r and r \u00b7 m, with n = 30, m = 20, r = 5. Matrix M? was created using the function, g? (M? i, j) = 1 / (1 + exp (\u2212 cZ? i, j)), where c > 0. By increasing the Lipschitz constant of function g?, the matrix completion task is made more difficult. For large enough c, j \u2248 sgn (Zi, j). We consider the silent version of the problem where X = M?. Each entry in the MatrixX threshold was scanned with the probability p, and the scanned centers are observed."}, {"heading": "5.3 Experiments on real datasets", "text": "We performed experimental comparisons to four real datasets: paper recommendation, Jester3, ML-100k, cameraman. The source of our datasets is listed in the appendix. All of the above datasets, with the exception of the Cameraman dataset, are evaluation datasets in which users evaluated some of the different items. For the Jester-3 dataset, we used 5 randomly selected ratings for each user for training, 5 randomly selected ratings for validation and the rest for the test. ML-100k comes with its own training and test dataset. We used 20% of the training data for validation. For the camera and the paper recommendation dataset, 20% of the data were used for training, 20% for validation and the rest for the test. The baseline algorithm chosen for the low-level matrix completion is LMaFit-A [15] For each data set of the MMC, we used the MSE \u2212 MSE \u2212 all the MSE \u2212 MMC datasets, the very small datasets \u2212 MMC \u2212 all the MSE \u2212 MSE \u2212 MSE \u2212 MSE \u2212 MSE \u2212 MMC datasets \u2212 MMC \u2212 all the data sets."}, {"heading": "6 Conclusions and future work", "text": "We have explored a new framework for high-level matrix completion problems known as monotonous matrix completion. We have proposed and studied an algorithm called MMC based on minimizing a calibrated loss function. In the future, we would like to explore whether the technical assumptions that accompany determining our theoretical results could be loosened."}, {"heading": "A Error Analysis of Monotonic Matrix Completion", "text": "We analyze our algorithm, MMC \u2212 c, for the case of T = 1. Since for T = 1, MMC \u2212 c and MMC \u2212 LS are the same, we will use the word MMC to refer to both algorithms if T = 1. For T = 1, we have Z = Pr (mnXB) (33) g = LPAV (Z, XB) (34) M-i, j = g (Z-i, j), i = [m], j = [n], (35) Finally, we define the mean square error (MSE) of our estimate M-i can be defined as MSE (M-i) = E [1mn n n-i = 1 m-j = 1 (M-i, j-Mi, j) 2]. (36) We are interested in analyzing the MSE of M output by MMC for T = 1. We will make the following assumptions:"}, {"heading": "B MMC model and technical assumptions", "text": "The MMC model assumes the following assumptions: (Z? i), j = g (Z? i), j = g (Z? i), j = g (n), j (n), j (n), j (n), j (n), j (n), j (m), j (m), j (m), j (m), j (m), j (m), j (m), j (m), j (m), j (m), j (m), j), j (n), j (m), j (m), j (m), j (m)."}, {"heading": "D Source for datasets", "text": "Here you can download the real-world datasets on which all of our experiments.1. Recommendation dataset for paper documents: http: / / www.comp.nus.edu.sg / \u02dc sugiyama / SchPaperRecData.html.2. Jester dataset: http: / / goldberg.berkeley.edu / jester-data /.3. Dataset for film lenses: http: / / grouplens.org / datasets / movielens / 4. dataset for cameramen: http: / / www.utdallas.edu / \u02dc cxc123730 / mh _ bcs _ spl.html"}, {"heading": "E RMSE plots with iterations", "text": "In Figure (3) we show how the RMSE of the MMC \u2212 c algorithm changes with iterations. These diagrams were created on the synthetic datasets used in our experiments, and the value of p was set to 0.35. As you can see, on average there is a decreasing trend in RMSE. This decrease is almost linear at small values of c and sublinear at larger values of c."}], "references": [{"title": "Recommender systems", "author": ["Prem Melville", "Vikas Sindhwani"], "venue": "In Encyclopedia of machine learning. Springer,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Graph realization and low-rank matrix completion", "author": ["Mihai Cucuringu"], "venue": "PhD thesis, Princeton University,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "A simpler approach to matrix completion", "author": ["Benjamin Recht"], "venue": "JMLR, 12:3413\u20133430,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Exact matrix completion via convex optimization. FOCM", "author": ["Emmanuel J Cand\u00e8s", "Benjamin Recht"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Matrix completion with noise", "author": ["Emmanuel J Candes", "Yaniv Plan"], "venue": "Proceedings of the IEEE,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Restricted strong convexity and weighted matrix completion: Optimal bounds with noise", "author": ["Sahand Negahban", "Martin J Wainwright"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Matrix completion from a few entries", "author": ["Raghunandan H Keshavan", "Andrea Montanari", "Sewoong Oh"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Recovering low-rank matrices from few coefficients in any basis", "author": ["David Gross"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Convex optimization & Euclidean distance geometry", "author": ["Jon Dattorro"], "venue": "Lulu. com,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "The isotron algorithm: High-dimensional isotonic regression", "author": ["Adam Tauman Kalai", "Ravi Sastry"], "venue": "In COLT,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Efficient learning of generalized linear and single index models with isotonic regression", "author": ["Sham M Kakade", "Varun Kanade", "Ohad Shamir", "Adam Kalai"], "venue": "In NIPS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Low-rank matrix completion by riemannian optimization", "author": ["Bart Vandereycken"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Riemannian pursuit for big matrix recovery", "author": ["Mingkui Tan", "Ivor W Tsang", "Li Wang", "Bart Vandereycken", "Sinno J Pan"], "venue": "In ICML,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Rank-one matrix pursuit for matrix completion", "author": ["Zheng Wang", "Ming-Jun Lai", "Zhaosong Lu", "Wei Fan", "Hasan Davulcu", "Jieping Ye"], "venue": "In ICML,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Solving a low-rank factorization model for matrix completion by a nonlinear successive over-relaxation algorithm", "author": ["Zaiwen Wen", "Wotao Yin", "Yin Zhang"], "venue": "Mathematical Programming Computation,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "High-rank matrix completion", "author": ["Brian Eriksson", "Laura Balzano", "Robert Nowak"], "venue": "In AISTATS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Sparse subspace clustering with missing entries", "author": ["Congyuan Yang", "Daniel Robinson", "Rene Vidal"], "venue": "In ICML,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "A geometric analysis of subspace clustering with outliers", "author": ["Mahdi Soltanolkotabi", "Emmanuel J Candes"], "venue": "The Annals of Statistics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Sparse subspace clustering: Algorithm, theory, and applications", "author": ["Ehsan Elhamifar", "Rene Vidal"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Completion of high-rank ultrametric matrices using selective entries", "author": ["Aarti Singh", "Akshay Krishnamurthy", "Sivaraman Balakrishnan", "Min Xu"], "venue": "In SPCOM,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Retargeted matrix factorization for collaborative filtering", "author": ["Oluwasanmi Koyejo", "Sreangsu Acharyya", "Joydeep Ghosh"], "venue": "In Proceedings of the 7th ACM conference on Recommender systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "1-bit matrix completion", "author": ["Mark A Davenport", "Yaniv Plan", "Ewout van den Berg", "Mary Wootters"], "venue": "Information and Inference,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Semiparametric least squares (sls) and weighted sls estimation of single-index models", "author": ["Hidehiko Ichimura"], "venue": "Journal of Econometrics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1993}, {"title": "Direct semiparametric estimation of single-index models with discrete covariates", "author": ["Joel L Horowitz", "Wolfgang H\u00e4rdle"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1996}, {"title": "Least squares revisited: Scalable approaches for multi-class prediction", "author": ["Alekh Agarwal", "Sham Kakade", "Nikos Karampatziakis", "Le Song", "Gregory Valiant"], "venue": "In ICML,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["Roman Vershynin"], "venue": "arXiv preprint arXiv:1011.3027,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["Stephen Boyd", "Neal Parikh", "Eric Chu", "Borja Peleato", "Jonathan Eckstein"], "venue": "Foundations and Trends R  \u00a9 in Machine Learning,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Tfocs: Flexible first-order methods for rank minimization", "author": ["Stephen Becker", "E Candes", "M Grant"], "venue": "In Low-rank Matrix Optimization Symposium, SIAM Conference on Optimization,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "Smoothness, low noise and fast rates", "author": ["Nathan Srebro", "Karthik Sridharan", "Ambuj Tewari"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["Peter L Bartlett", "Shahar Mendelson"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2003}, {"title": "Prediction, learning, and games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2006}, {"title": "Matrix estimation by universal singular value thresholding", "author": ["Sourav Chatterjee"], "venue": "The Annals of Statistics,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "This problem has a plethora of applications such as collaborative filtering, recommender systems [1] and sensor networks [2].", "startOffset": 97, "endOffset": 100}, {"referenceID": 1, "context": "This problem has a plethora of applications such as collaborative filtering, recommender systems [1] and sensor networks [2].", "startOffset": 121, "endOffset": 124}, {"referenceID": 2, "context": "Matrix completion has been well studied in machine learning, and we now know how to recover certain matrices given a few observed entries of the matrix [3, 4] when it is assumed to be low rank.", "startOffset": 152, "endOffset": 158}, {"referenceID": 3, "context": "Matrix completion has been well studied in machine learning, and we now know how to recover certain matrices given a few observed entries of the matrix [3, 4] when it is assumed to be low rank.", "startOffset": 152, "endOffset": 158}, {"referenceID": 2, "context": "Typical work in matrix completion assumes that the matrix to be recovered is incoherent, low rank, and entries are sampled uniformly at random [3, 4, 5, 6, 7, 8].", "startOffset": 143, "endOffset": 161}, {"referenceID": 3, "context": "Typical work in matrix completion assumes that the matrix to be recovered is incoherent, low rank, and entries are sampled uniformly at random [3, 4, 5, 6, 7, 8].", "startOffset": 143, "endOffset": 161}, {"referenceID": 4, "context": "Typical work in matrix completion assumes that the matrix to be recovered is incoherent, low rank, and entries are sampled uniformly at random [3, 4, 5, 6, 7, 8].", "startOffset": 143, "endOffset": 161}, {"referenceID": 5, "context": "Typical work in matrix completion assumes that the matrix to be recovered is incoherent, low rank, and entries are sampled uniformly at random [3, 4, 5, 6, 7, 8].", "startOffset": 143, "endOffset": 161}, {"referenceID": 6, "context": "Typical work in matrix completion assumes that the matrix to be recovered is incoherent, low rank, and entries are sampled uniformly at random [3, 4, 5, 6, 7, 8].", "startOffset": 143, "endOffset": 161}, {"referenceID": 7, "context": "Typical work in matrix completion assumes that the matrix to be recovered is incoherent, low rank, and entries are sampled uniformly at random [3, 4, 5, 6, 7, 8].", "startOffset": 143, "endOffset": 161}, {"referenceID": 8, "context": "The Euclidean distance matrix is a low-rank matrix [9].", "startOffset": 51, "endOffset": 54}, {"referenceID": 9, "context": "We believe that the proposed ADMM algorithm is useful in its own right and can be used in general isotonic regression problems elsewhere [10, 11].", "startOffset": 137, "endOffset": 145}, {"referenceID": 10, "context": "We believe that the proposed ADMM algorithm is useful in its own right and can be used in general isotonic regression problems elsewhere [10, 11].", "startOffset": 137, "endOffset": 145}, {"referenceID": 2, "context": "Classical matrix completion with and without noise has been investigated by several authors [3, 4, 5, 6, 7, 8].", "startOffset": 92, "endOffset": 110}, {"referenceID": 3, "context": "Classical matrix completion with and without noise has been investigated by several authors [3, 4, 5, 6, 7, 8].", "startOffset": 92, "endOffset": 110}, {"referenceID": 4, "context": "Classical matrix completion with and without noise has been investigated by several authors [3, 4, 5, 6, 7, 8].", "startOffset": 92, "endOffset": 110}, {"referenceID": 5, "context": "Classical matrix completion with and without noise has been investigated by several authors [3, 4, 5, 6, 7, 8].", "startOffset": 92, "endOffset": 110}, {"referenceID": 6, "context": "Classical matrix completion with and without noise has been investigated by several authors [3, 4, 5, 6, 7, 8].", "startOffset": 92, "endOffset": 110}, {"referenceID": 7, "context": "Classical matrix completion with and without noise has been investigated by several authors [3, 4, 5, 6, 7, 8].", "startOffset": 92, "endOffset": 110}, {"referenceID": 11, "context": "Progress has also been made on designing efficient algorithms to solve the ensuing convex optimization problem [12, 13, 14, 15].", "startOffset": 111, "endOffset": 127}, {"referenceID": 12, "context": "Progress has also been made on designing efficient algorithms to solve the ensuing convex optimization problem [12, 13, 14, 15].", "startOffset": 111, "endOffset": 127}, {"referenceID": 13, "context": "Progress has also been made on designing efficient algorithms to solve the ensuing convex optimization problem [12, 13, 14, 15].", "startOffset": 111, "endOffset": 127}, {"referenceID": 14, "context": "Progress has also been made on designing efficient algorithms to solve the ensuing convex optimization problem [12, 13, 14, 15].", "startOffset": 111, "endOffset": 127}, {"referenceID": 15, "context": "[16] suggested looking at the neighbourhood of each incomplete point for completion, [17] used a combination of spectral clustering techniques as done in [18, 19] along with learning sparse representations via convex optimization to estimate the incomplete matrix.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[16] suggested looking at the neighbourhood of each incomplete point for completion, [17] used a combination of spectral clustering techniques as done in [18, 19] along with learning sparse representations via convex optimization to estimate the incomplete matrix.", "startOffset": 85, "endOffset": 89}, {"referenceID": 17, "context": "[16] suggested looking at the neighbourhood of each incomplete point for completion, [17] used a combination of spectral clustering techniques as done in [18, 19] along with learning sparse representations via convex optimization to estimate the incomplete matrix.", "startOffset": 154, "endOffset": 162}, {"referenceID": 18, "context": "[16] suggested looking at the neighbourhood of each incomplete point for completion, [17] used a combination of spectral clustering techniques as done in [18, 19] along with learning sparse representations via convex optimization to estimate the incomplete matrix.", "startOffset": 154, "endOffset": 162}, {"referenceID": 19, "context": "[20] consider a certain specific class of high-rank matrices that are obtained from ultra-metrics.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "In [21] the authors consider a model similar to ours, but instead of learning a single monotonic function, they learn multiple monotonic functions, one for each row of the matrix.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "Davenport et al [22] studied the one-bit matrix completion problem.", "startOffset": 16, "endOffset": 20}, {"referenceID": 9, "context": "The MMC model is inspired by the single-index model (SIM) that has been studied both in statistics [10, 11] and econometrics for regression problems [23, 24].", "startOffset": 99, "endOffset": 107}, {"referenceID": 10, "context": "The MMC model is inspired by the single-index model (SIM) that has been studied both in statistics [10, 11] and econometrics for regression problems [23, 24].", "startOffset": 99, "endOffset": 107}, {"referenceID": 22, "context": "The MMC model is inspired by the single-index model (SIM) that has been studied both in statistics [10, 11] and econometrics for regression problems [23, 24].", "startOffset": 149, "endOffset": 157}, {"referenceID": 23, "context": "The MMC model is inspired by the single-index model (SIM) that has been studied both in statistics [10, 11] and econometrics for regression problems [23, 24].", "startOffset": 149, "endOffset": 157}, {"referenceID": 10, "context": "The LPAV 4 algorithm introduced in [11] outputs the best function \u011d in G that minimizes \u2211n i=1(g(pi)\u2212 yi) .", "startOffset": 35, "endOffset": 39}, {"referenceID": 24, "context": "The idea of using calibrated loss functions was first introduced for learning single index models [25].", "startOffset": 98, "endOffset": 102}, {"referenceID": 25, "context": "39 in [26]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 3, "context": "We compare the performance of MMC\u22121, MMC\u2212 c, MMC- LS, and nuclear norm based low-rank matrix completion (LRMC) [4] on various synthetic and real world datasets.", "startOffset": 111, "endOffset": 114}, {"referenceID": 14, "context": "The increasing rank procedure was inspired by the work of [15], where they demonstrated that such procedures are suitable for low-rank approximation problems.", "startOffset": 58, "endOffset": 62}, {"referenceID": 26, "context": "Instead we use the Alternating Direction Method of Multipliers (ADMM) algorithm for our problem [27] which allows us to exploit the rich structure present in problem (22).", "startOffset": 96, "endOffset": 100}, {"referenceID": 26, "context": "In our practical implementations we follow the advice as mentioned in [27] and set abs and rel to 10\u22122.", "startOffset": 70, "endOffset": 74}, {"referenceID": 14, "context": "For our implementations we assume that r is unknown, and estimate it either (i) via the use of a dedicated validation set in the case of MMC \u2212 1 or (ii) adaptively, where we progressively increase the estimate of our rank until a sufficient decrease in error over the training set is achieved [15].", "startOffset": 293, "endOffset": 297}, {"referenceID": 27, "context": "For an implementation of the LRMC algorithm we used a standard off-the-shelf implementation from TFOCS [28].", "startOffset": 103, "endOffset": 107}, {"referenceID": 14, "context": "The baseline algorithm chosen for low rank matrix completion is LMaFit-A [15] 7.", "startOffset": 73, "endOffset": 77}], "year": 2015, "abstractText": "Most recent results in matrix completion assume that the matrix under consideration is low-rank or that the columns are in a union of low-rank subspaces. In real-world settings, however, the linear structure underlying these models is distorted by a (typically unknown) nonlinear transformation. This paper addresses the challenge of matrix completion in the face of such nonlinearities. Given a few observations of a matrix that are obtained by applying a Lipschitz, monotonic function to a low rank matrix, our task is to estimate the remaining unobserved entries. We propose a novel matrix completion method that alternates between lowrank matrix estimation and monotonic function estimation to estimate the missing matrix elements. Mean squared error bounds provide insight into how well the matrix can be estimated based on the size, rank of the matrix and properties of the nonlinear transformation. Empirical results on synthetic and real-world datasets demonstrate the competitiveness of the proposed approach.", "creator": "LaTeX with hyperref package"}}}