{"id": "1512.08849", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Dec-2015", "title": "Learning Natural Language Inference with LSTM", "abstract": "Natural language inference (NLI) is a fundamentally important task in natural language processing that has many applications. The recently released Stanford Natural Language Inference (SNLI) corpus has made it possible to develop and evaluate learning-centered methods such as deep neural networks for the NLI task. In this paper, we propose a special long short-term memory (LSTM) architecture for NLI. Our model builds on top of a recently proposed neutral attention model for NLI but is based on a significantly different idea. Instead of deriving sentence embeddings for the premise and the hypothesis to be used for classification, our solution uses a matching-LSTM that performs word-by-word matching of the hypothesis with the premise. This LSTM is able to place more emphasis on important word-level matching results. In particular, we observe that this LSTM remembers important mismatches that are critical for predicting the contradiction or the neutral relationship label. Our experiments on the SNLI corpus show that our model outperforms the state of the art, achieving an accuracy of 86.1% on the test data.", "histories": [["v1", "Wed, 30 Dec 2015 05:02:53 GMT  (694kb,D)", "http://arxiv.org/abs/1512.08849v1", "10 pages, 4 figures"], ["v2", "Thu, 10 Nov 2016 11:54:29 GMT  (367kb,D)", "http://arxiv.org/abs/1512.08849v2", "10 pages, 2 figures"]], "COMMENTS": "10 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.NE", "authors": ["shuohang wang", "jing jiang"], "accepted": true, "id": "1512.08849"}, "pdf": {"name": "1512.08849.pdf", "metadata": {"source": "CRF", "title": "Learning Natural Language Inference with LSTM", "authors": ["Shuohang Wang", "Jing Jiang"], "emails": ["shwang.2014@phdis.smu.edu.sg", "jingjiang@smu.edu.sg"], "sections": [{"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 Model", "text": "In this section we present our mLSTM architecture for inferences about natural language."}, {"heading": "2.1 Background", "text": "We first present the model of Rockta \u00bc schel et al. (2015) because our model is based on the top of them. (2015) Your model uses LSTM to process the premise and the hypothesis. (2015) Your model uses LSTM to process the premise and the hypothesis. (2015) LSTM uses a few gate vectors at each position to control the transmission of information along the sequence, thus improving the modelling of long-distance dependencies. (While there are various variations of LSTMs, here we present the one assumed sequence in (2015). Specifically, let us use X = (x1, x2,.) to denote an input sequence."}, {"heading": "2.2 Our Model", "text": "Although the neural attention model of Rockta \u00bc schel et al. (2015) We have better results than Bowman et al. (2015), we see two limitations. (2015) We also have two limitations. (2015) We have two limitations. (2015) We have only two limitations. (2015) We have only two limitations. (2015) There are only two limitations. (2015) There are only two limitations. (2015) There are only two limitations. (2015) We can achieve better quality by using each of the two terms we take in both positions to understand how good the overall agreement of the two sentences is. (In the end, the RNN will produce a single vector that represents the assignment of the two complete sentences.) The second limitation is that the model of Rockta \u00bc schel et al. (2015) does not allow us to place more emphasis on the more important match of the two sentences. (2015) We will produce a single vector that represents the two sentences."}, {"heading": "2.3 Implementation Details", "text": "In addition to the mLSTM architecture, which is the main difference between our model and the Rockta \ufffd schel et al. (2015) model, we are also introducing a few other modifications. First, we insert a special word NULL into the premise and allow words in the hypothesis to be aligned with this NULL. This hs0 stands for NULL and is used along with other hsj to derive attention vectors {ak} Nk = 1.Second, we use word embeddings trained by GloVe (Pennington et al., 2014) instead of word2vec vectors, the main reason being that GloVe word embeddings cover more words in the SNLI corpus than word2vec3.Third, for words that do not have pre-formed word embeddings."}, {"heading": "3 Experiments", "text": "In this section, we present the evaluation of our model. First, we perform a quantitative evaluation and compare our model with the model of Rockta \ufffd schel et al. (2015). Then, we perform some qualitative analyses to understand how our mLSTM model works when it meets the premise and hypothesis."}, {"heading": "3.1 Experiment Settings", "text": "In fact, it is so that it will be able to erenie.n the aforementioned lcihsrcehnlrc\u00fceS"}, {"heading": "3.2 Quantitative Results", "text": "Table 1 compares the performance of the various models we tested with some previously reported results. We have the following observations: (1) First of all, we can see that if we set d to 300, our model achieves an accuracy of 86.1% on the test data, which, to the best of our knowledge, is the highest in this data set. (2) If we compare our mLSTM model with our implementation of the word-for-word attention model by Rockta \ufffd schel et al. (2015) with the same setting with d = 150, we can see that our performance on the test data (85.7%) is still higher than that of the other model (82.6%). We also tested the statistical significance and found that the improvement at the level of 0.001 is statistically significant. (3) The performance of mLSTM with bi-LSTM set modeling compared to the model with standard LSTM set set to M set the statistical significance, and found that the measurement at the level of 0.001 is statistically significant."}, {"heading": "3.3 Qualitative Analyses", "text": "(...). (.). (.). (.). (.). (.). (.). (.). (.).). (.). (.).). (.).). (.).). (.).). (.). (.). (.). (.). (.). (.). (.).). (.).). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.).). (.). (.). (.). (.).). (.). (.). (.).). (.). (.). (.). (.). (.).). (.). (.). (.). (.).). (.). (.). (.).). (.). (.).). (.).). (.). (.). (.).). (.). (.). (.).). (.). (.).). (.). (.). (.). (.). (.).). (.). (.).). (. (.). (.).). (.).). (.). (.). (.).). (.). (.).). (.). (.). (.).). (.).). (.). (.).). (.). (.). (.).).). (.). (.). (.)..).). (.).).).).). (. (.). (.).).). (. (.). (.).). (.).).).).).).."}, {"heading": "4 Related Work", "text": "For example, Bowman et al. (2015) experimented with a lexicalized classifier-based method that uses only lexical information to extract features used by a classifier, and the method achieves an accuracy of 78.2% on the SNLI corpus. More advanced methods use syntactic structures of the sentences to help them assign attributes. For example, Mehdad et al. (2009) syntactic-semantic tree cores were used to detect textual entanglements. Since conclusions are essentially a logical problem, methods based on formal logic (Clark and Harrison, 2009) or natural logic (MacCartney, 2009) are also proposed. A comprehensive review of existing work on natural linguistic conclusions can be found in (Sammons et al., 2011)."}, {"heading": "5 Conclusions", "text": "In this paper, we proposed a specific LSTM architecture for the task of natural language reasoning. Based on a recent paper by Rockta \ufffd schel et al. (2015), we first used neural attention models to derive attention-weighted vector representations of the premise, then designed a match-LSTM that processes the hypothesis word for word while trying to align the hypothesis with the premise. Specifically, the mLSTM takes into account both the current hidden state of the hypothesis and an attention-weighted representation of the premise, generating an output that represents the agreement between premise and hypothesis to its current position. Consequently, the last hidden state of the mLSTM can be used to predict the relationship between premise and hypothesis. Experiments on the SNLI corpus showed that the mLSTM model exceeded the performance previously presented on this dataset."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "HyungHyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the International Conference on Learning Representations.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "An inferencebased approach to recognizing entailment", "author": ["Peter Clark", "Phil Harrison."], "venue": "Proceedings of the Text Analysis Conference.", "citeRegEx": "Clark and Harrison.,? 2009", "shortCiteRegEx": "Clark and Harrison.", "year": 2009}, {"title": "The PASCAL Recognising Textual Entailment Challenge", "author": ["Ido Dagan", "Oren Glickman", "Bernardo Magnini."], "venue": "Proceedings of the PASCAL Challenges Workshop on Recognizing Textual Entailment.", "citeRegEx": "Dagan et al\\.,? 2005", "shortCiteRegEx": "Dagan et al\\.", "year": 2005}, {"title": "Web based probabilistic textual entailment", "author": ["Oren Glickman", "Ido Dagan", "Moshe Koppel."], "venue": "Proceedings of the PASCAL Challenges Workshop on Recognizing Textual Entailment.", "citeRegEx": "Glickman et al\\.,? 2005", "shortCiteRegEx": "Glickman et al\\.", "year": 2005}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems, pages 1684\u2013", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "Advances in Neural Information Processing Systems, pages 3276\u20133284.", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "A phrase-based alignment model for natural language inference", "author": ["Bill MacCartney", "Michel Galley", "Christopher D Manning."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "MacCartney et al\\.,? 2008", "shortCiteRegEx": "MacCartney et al\\.", "year": 2008}, {"title": "Natural Language Inference", "author": ["Bill MacCartney."], "venue": "Ph.D. thesis, Stanford University.", "citeRegEx": "MacCartney.,? 2009", "shortCiteRegEx": "MacCartney.", "year": 2009}, {"title": "SemKer: Syntactic/semantic kernels for recognizing textual entailment", "author": ["Yashar Mehdad", "Alessandro Moschitti", "Fabio Massiomo Zanzotto"], "venue": "In Proceedings of the Text Analysis Conference", "citeRegEx": "Mehdad et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mehdad et al\\.", "year": 2009}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Reasoning about entailment with neural attention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Phil Blunsom."], "venue": "arXiv preprint arXiv:1509.06664.", "citeRegEx": "Rockt\u00e4schel et al\\.,? 2015", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston."], "venue": "arXiv preprint arXiv:1509.00685.", "citeRegEx": "Rush et al\\.,? 2015", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Recognizing textual entailment", "author": ["Mark Sammons", "VG Vinod Vydiswaran", "Dan Roth."], "venue": "Multilingual Natural Language Applications: From Theory to Practice. Prentice Hall, Jun.", "citeRegEx": "Sammons et al\\.,? 2011", "shortCiteRegEx": "Sammons et al\\.", "year": 2011}, {"title": "Order-embeddings of images and language", "author": ["Ivan Vendrov", "Ryan Kiros", "Sanja Fidler", "Raquel Urtasun."], "venue": "arXiv preprint arXiv:1511.06361.", "citeRegEx": "Vendrov et al\\.,? 2015", "shortCiteRegEx": "Vendrov et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 10, "context": "can infer another sentenceH (MacCartney, 2009).", "startOffset": 28, "endOffset": 46}, {"referenceID": 4, "context": "There has been much interest in NLI in the past decade, especially surrounding the PASCAL Recognizing Textual Entailment (RTE) Challenge (Dagan et al., 2005).", "startOffset": 137, "endOffset": 157}, {"referenceID": 5, "context": "Existing solutions to NLI range from shallow approaches based on lexical similarities (Glickman et al., 2005) to advanced methods that consider syntax (Mehdad et al.", "startOffset": 86, "endOffset": 109}, {"referenceID": 11, "context": ", 2005) to advanced methods that consider syntax (Mehdad et al., 2009), perform explicit sen-", "startOffset": 49, "endOffset": 70}, {"referenceID": 9, "context": "tence alignment (MacCartney et al., 2008) or use formal logic (Clark and Harrison, 2009).", "startOffset": 16, "endOffset": 41}, {"referenceID": 3, "context": ", 2008) or use formal logic (Clark and Harrison, 2009).", "startOffset": 28, "endOffset": 54}, {"referenceID": 2, "context": "Recently, Bowman et al. (2015) released the Stanford Natural Language Inference (SNLI) corpus for the purpose of encouraging more learningcentered approaches to NLI.", "startOffset": 10, "endOffset": 31}, {"referenceID": 2, "context": "Bowman et al. (2015)", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "Bowman et al. (2015) achieved an accuracy of 77.", "startOffset": 0, "endOffset": 21}, {"referenceID": 13, "context": "A more recent work by Rockt\u00e4schel et al. (2015) improved the performance by applying a neural attention model.", "startOffset": 22, "endOffset": 48}, {"referenceID": 2, "context": "Different from the models in (Bowman et al., 2015) and (Rockt\u00e4schel et al.", "startOffset": 29, "endOffset": 50}, {"referenceID": 13, "context": ", 2015) and (Rockt\u00e4schel et al., 2015), our prediction is not based on whole sentence embed-", "startOffset": 12, "endOffset": 38}, {"referenceID": 13, "context": "We first present the model by Rockt\u00e4schel et al. (2015) because our model builds on top of theirs.", "startOffset": 30, "endOffset": 56}, {"referenceID": 13, "context": "While there are different variations of LSTMs, here we present the one adopted in (Rockt\u00e4schel et al., 2015).", "startOffset": 82, "endOffset": 108}, {"referenceID": 13, "context": "Rockt\u00e4schel et al. (2015) further built an", "startOffset": 0, "endOffset": 26}, {"referenceID": 13, "context": "Rockt\u00e4schel et al. (2015) then used this hN , which represents the whole premise, together with hN , which is an aggregated representation of the whole hypothesis2, to predict the label y.", "startOffset": 0, "endOffset": 26}, {"referenceID": 13, "context": "Strictly speaking, in (Rockt\u00e4schel et al., 2015), hN encodes both the premise and the hypothesis because the two sentences are chained.", "startOffset": 22, "endOffset": 48}, {"referenceID": 11, "context": "We present the word-by-word attention model by Rockt\u00e4schel et al. (2015) in a different way but the underlying model is the same.", "startOffset": 47, "endOffset": 73}, {"referenceID": 0, "context": "Our presentation is close to the one by Bahdanau et al. (2015), with our attention vectors a corresponding to the context vectors c in their paper.", "startOffset": 40, "endOffset": 63}, {"referenceID": 12, "context": "Although the neural attention model by Rockt\u00e4schel et al. (2015) achieved better results than Bowman et al.", "startOffset": 39, "endOffset": 65}, {"referenceID": 2, "context": "(2015) achieved better results than Bowman et al. (2015), we see two limitations.", "startOffset": 36, "endOffset": 57}, {"referenceID": 13, "context": "The second limitation is that the model by Rockt\u00e4schel et al. (2015) does not explicitly allow us to place more emphasis on the more important", "startOffset": 43, "endOffset": 69}, {"referenceID": 13, "context": "Figure 1 gives an overview of our model in contrast to the model by Rockt\u00e4schel et al. (2015).", "startOffset": 68, "endOffset": 94}, {"referenceID": 13, "context": "First, similar to Rockt\u00e4schel et al. (2015), we process the premise and the hypothesis using two LSTMs, but we do not feed the last cell state of the premise to the LSTM of the hypothesis.", "startOffset": 18, "endOffset": 44}, {"referenceID": 13, "context": "Figure 1: The top figure depicts the model by Rockt\u00e4schel et al. (2015) and the bottom figure depicts our model.", "startOffset": 46, "endOffset": 72}, {"referenceID": 13, "context": "Figure 1: The top figure depicts the model by Rockt\u00e4schel et al. (2015) and the bottom figure depicts our model. Here Hs represents all the hidden states hj . We can see that in the model by Rockt\u00e4schel et al. (2015), each hk represents a weighted version of the premise only, while in our", "startOffset": 46, "endOffset": 217}, {"referenceID": 13, "context": "Besides the mLSTM architecture, which is the main difference of our model from the model by Rockt\u00e4schel et al. (2015), we also introduce a few", "startOffset": 92, "endOffset": 118}, {"referenceID": 12, "context": "Second, we use word embeddings trained from GloVe (Pennington et al., 2014) instead of", "startOffset": 50, "endOffset": 75}, {"referenceID": 13, "context": "Although this is a very crude approximation, it reduces the number of parameters we need to update, and as it turns out, we can still achieve better performance than Rockt\u00e4schel et al. (2015).", "startOffset": 166, "endOffset": 192}, {"referenceID": 2, "context": "Model d |\u03b8|W+M |\u03b8|M Train Dev Test LSTM [Bowman et al. (2015)] 100 10M 221K 84.", "startOffset": 41, "endOffset": 62}, {"referenceID": 2, "context": "Model d |\u03b8|W+M |\u03b8|M Train Dev Test LSTM [Bowman et al. (2015)] 100 10M 221K 84.4 - 77.6 Classifier [Bowman et al. (2015)] - - 99.", "startOffset": 41, "endOffset": 121}, {"referenceID": 2, "context": "Model d |\u03b8|W+M |\u03b8|M Train Dev Test LSTM [Bowman et al. (2015)] 100 10M 221K 84.4 - 77.6 Classifier [Bowman et al. (2015)] - - 99.7 - 78.2 LSTM shared [Rockt\u00e4schel et al. (2015)] 159 3.", "startOffset": 41, "endOffset": 177}, {"referenceID": 2, "context": "Model d |\u03b8|W+M |\u03b8|M Train Dev Test LSTM [Bowman et al. (2015)] 100 10M 221K 84.4 - 77.6 Classifier [Bowman et al. (2015)] - - 99.7 - 78.2 LSTM shared [Rockt\u00e4schel et al. (2015)] 159 3.9M 252K 84.4 83.0 81.4 Word-by-word attention [Rockt\u00e4schel et al. (2015)] 100 3.", "startOffset": 41, "endOffset": 257}, {"referenceID": 2, "context": "This follows the same data partition used by Bowman et al. (2015) in their experiments.", "startOffset": 45, "endOffset": 66}, {"referenceID": 7, "context": "Parameters: We use the Adam method (Kingma and Ba, 2014) with hyperparameters \u03b21 set to 0.", "startOffset": 35, "endOffset": 56}, {"referenceID": 13, "context": "Methods for comparison: We mainly want to compare our model with the word-by-word attention model by Rockt\u00e4schel et al. (2015) because this model achieved the state-of-the-art performance on the SNLI corpus.", "startOffset": 101, "endOffset": 127}, {"referenceID": 13, "context": "ported by Rockt\u00e4schel et al. (2015), we also reimplemented their word-by-word attention model ourselves and report the performance of our implementation.", "startOffset": 10, "endOffset": 36}, {"referenceID": 13, "context": "\u2022 Word-by-word attention (d = 150): This is our implementation of the word-by-word attention model by Rockt\u00e4schel et al. (2015),", "startOffset": 102, "endOffset": 128}, {"referenceID": 13, "context": "The differences between our implementation and the original implementation by Rockt\u00e4schel et al. (2015) are the following: (1) We also add a NULL token to the premise for matching.", "startOffset": 78, "endOffset": 104}, {"referenceID": 13, "context": "we compare our mLSTM model with our implementation of the word-by-word attention model by Rockt\u00e4schel et al. (2015) under the same setting with d = 150, we can see that our performance on the test data (85.", "startOffset": 90, "endOffset": 116}, {"referenceID": 2, "context": "For example, Bowman et al. (2015) experimented with a lexicalized classifier-based method, which only uses lexical information to extract features used by a classifier, and the method achieves an accuracy of 78.", "startOffset": 13, "endOffset": 34}, {"referenceID": 2, "context": "For example, Bowman et al. (2015) experimented with a lexicalized classifier-based method, which only uses lexical information to extract features used by a classifier, and the method achieves an accuracy of 78.2% on the SNLI corpus. More advanced methods use syntactic structures of the sentences to help matching them. For example, Mehdad et al. (2009) applied syntactic-semantic tree kernels for recognizing textual entailment.", "startOffset": 13, "endOffset": 355}, {"referenceID": 3, "context": "ence is essentially a logic problem, methods based on formal logic (Clark and Harrison, 2009) or natural logic (MacCartney, 2009) have also been proposed.", "startOffset": 67, "endOffset": 93}, {"referenceID": 10, "context": "ence is essentially a logic problem, methods based on formal logic (Clark and Harrison, 2009) or natural logic (MacCartney, 2009) have also been proposed.", "startOffset": 111, "endOffset": 129}, {"referenceID": 15, "context": "A comprehensive review on existing work on natural language inference can be found in (Sammons et al., 2011).", "startOffset": 86, "endOffset": 108}, {"referenceID": 0, "context": "Neural attention models have recently been applied to some natural language processing tasks including machine translation (Bahdanau et al., 2014), abstractive summarization (Rush et al.", "startOffset": 123, "endOffset": 146}, {"referenceID": 14, "context": ", 2014), abstractive summarization (Rush et al., 2015) and question an-", "startOffset": 35, "endOffset": 54}, {"referenceID": 6, "context": "swering (Hermann et al., 2015).", "startOffset": 8, "endOffset": 30}, {"referenceID": 6, "context": "swering (Hermann et al., 2015). Rockt\u00e4schel et al. (2015) showed that the neural attention model could help derive a better representation of the premise to be used to match the hypothesis, whereas in our work we also use it to derive representations of the premise that are used to sequentially match the words in the hypothesis.", "startOffset": 9, "endOffset": 58}, {"referenceID": 2, "context": "Besides the work by Bowman et al. (2015) themselves and by Rockt\u00e4schel et al.", "startOffset": 20, "endOffset": 41}, {"referenceID": 2, "context": "Besides the work by Bowman et al. (2015) themselves and by Rockt\u00e4schel et al. (2015), there is another study by Vendrov et al.", "startOffset": 20, "endOffset": 85}, {"referenceID": 2, "context": "Besides the work by Bowman et al. (2015) themselves and by Rockt\u00e4schel et al. (2015), there is another study by Vendrov et al. (2015) in which a Skip-Thought model by Kiros et al.", "startOffset": 20, "endOffset": 134}, {"referenceID": 2, "context": "Besides the work by Bowman et al. (2015) themselves and by Rockt\u00e4schel et al. (2015), there is another study by Vendrov et al. (2015) in which a Skip-Thought model by Kiros et al. (2015) was applied to the NLI task.", "startOffset": 20, "endOffset": 187}, {"referenceID": 2, "context": "Besides the work by Bowman et al. (2015) themselves and by Rockt\u00e4schel et al. (2015), there is another study by Vendrov et al. (2015) in which a Skip-Thought model by Kiros et al. (2015) was applied to the NLI task. The authors reported an accuracy of 81.5% on the data set. Because this accuracy is lower than the best performance reported by Rockt\u00e4schel et al. (2015) and because our main focus was to examine the effectiveness of our match-LSTM compared with the model by Rockt\u00e4schel et al.", "startOffset": 20, "endOffset": 370}, {"referenceID": 2, "context": "Besides the work by Bowman et al. (2015) themselves and by Rockt\u00e4schel et al. (2015), there is another study by Vendrov et al. (2015) in which a Skip-Thought model by Kiros et al. (2015) was applied to the NLI task. The authors reported an accuracy of 81.5% on the data set. Because this accuracy is lower than the best performance reported by Rockt\u00e4schel et al. (2015) and because our main focus was to examine the effectiveness of our match-LSTM compared with the model by Rockt\u00e4schel et al. (2015), we did not include their study for comparison in our experiments.", "startOffset": 20, "endOffset": 501}, {"referenceID": 13, "context": "Based on a recent work by Rockt\u00e4schel et al. (2015), we first used neural attention models to derive attention-weighted vector representations of the premise.", "startOffset": 26, "endOffset": 52}], "year": 2015, "abstractText": "Natural language inference (NLI) is a fundamentally important task in natural language processing that has many applications. The recently released Stanford Natural Language Inference (SNLI) corpus has made it possible to develop and evaluate learning-centered methods such as deep neural networks for the NLI task. In this paper, we propose a special long short-term memory (LSTM) architecture for NLI. Our model builds on top of a recently proposed neutral attention model for NLI but is based on a significantly different idea. Instead of deriving sentence embeddings for the premise and the hypothesis to be used for classification, our solution uses a matching-LSTM that performs word-by-word matching of the hypothesis with the premise. This LSTM is able to place more emphasis on important word-level matching results. In particular, we observe that this LSTM remembers important mismatches that are critical for predicting the contradiction or the neutral relationship label. Our experiments on the SNLI corpus show that our model outperforms the state of the art, achieving an accuracy of 86.1% on the test data.", "creator": "TeX"}}}