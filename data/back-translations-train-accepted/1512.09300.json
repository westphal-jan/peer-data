{"id": "1512.09300", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Dec-2015", "title": "Autoencoding beyond pixels using a learned similarity metric", "abstract": "We present an autoencoder that leverages the power of learned representations to better measure similarities in data space. By combining a variational autoencoder (VAE) with a generative adversarial network (GAN) we can use learned feature representations in the GAN discriminator as basis for the VAE reconstruction objective. Thereby, we replace element-wise errors with feature-wise errors that better capture the data distribution while offering invariance towards e.g. translation. We apply our method to images of faces and show that our method outperforms VAEs with element-wise similarity measures in terms of visual fidelity. Moreover, we show that our method learns an embedding in which high-level abstract visual features (e.g. wearing glasses) can be modified using simple arithmetic.", "histories": [["v1", "Thu, 31 Dec 2015 14:53:39 GMT  (1033kb,D)", "http://arxiv.org/abs/1512.09300v1", null], ["v2", "Wed, 10 Feb 2016 21:18:27 GMT  (1305kb,D)", "http://arxiv.org/abs/1512.09300v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV stat.ML", "authors": ["anders boesen lindbo larsen", "s\u00f8ren kaae s\u00f8nderby", "hugo larochelle", "ole winther"], "accepted": true, "id": "1512.09300"}, "pdf": {"name": "1512.09300.pdf", "metadata": {"source": "META", "title": "Autoencoding beyond pixels using a learned similarity metric", "authors": ["Anders Boesen", "Lindbo Larsen", "S\u00f8ren Kaae S\u00f8nderby", "Ole Winther"], "emails": ["ABLL@DTU.DK", "SKAAESONDERBY@GMAIL.DK", "OLWI@DTU.DK"], "sections": [{"heading": "1. Introduction", "text": "In this paper, we show that currently used similarity measurements represent a barrier to learning good generative models, and that we can improve a generative model by applying learned similarity measurement. If learning models such as the varying autoencoder (UAE) (Kingma & Welling, 2014; Rezende et al., 2014), the choice of similarity measurements is central, as they provide the main part of the training signal via the reconstruction error target. For this task, the elementary Euclidean process is most welcome. Comments and suggestions are most welcome, while the choice of similarity measurements is central as they represent the main part of the training signal via the reconstruction error object."}, {"heading": "1.1. Contributions", "text": "\u2022 We combine VAEs and GANs to form an unattended generative model that simultaneously learns to encode, generate, and compare datasets. \u2022 We show that a model trained with learned error metrics produces better image samples. \u2022 We show that unattended training leads to latent imaging with untangled variation factors (Bengio et al., 2013)."}, {"heading": "2. Autoencoding with learned similarity", "text": "In this section, we provide background information on VAEs and GANs and introduce a minor modification of GANs to make it easier to combine VAEs and GANs. In addition, we replace the expected log probability in the UAE with a reconstruction error caused by the GAN.Variational Autoencoder A VAE consists of two networks that encrypt a data sample x to a latent representation z and decode the latent representation back into the data space, or: z \ufffd Enc (x) = q (z | x), x \ufffd Dec (z) Dec (x | z). (1) The VAE regulates the encoder by imposing a prior distribution of the encryption p (z). Typically, z \ufffd N (0, I) is chosen. The VAE loss is minus the sum of the expected log probability (the reconstruction error) and a preceding regulation term: z \ufffd N (0, I)."}, {"heading": "2.1. Beyond element-wise reconstruction error", "text": "Since elementary reconstruction errors are not adequate for images and other signals with invariances, we propose to replace the UAE reconstruction (expected log probability) with error terms from Eq. \u2212 To achieve this, we must mark the hidden representation of the Lth level of the discriminator. \u2212 We present a Gaussian observation model for Disl (x) in which the sample from the decoder of x (x) and identity covariance: p (x) | Disl (x) | Disl (x), I), where x (z) is the sample from the decoder of x. We can now use the VAE error Eq."}, {"heading": "3. Experiments", "text": "Measuring the quality of generative models is a challenge, as current evaluation methods do not scale to natural images. As the probability of logging proves insoluble, it is often estimated using a parzen window approach (Breuleux et al., 2011), but even small images of 6 x 6 pixels show that they are distorted in favor of exaggerated methods (Theis et al., 2015).In this paper, we use images of 64x64 size and perform a qualitative assessment only because current evaluation methods are not feasible for our problem size. We leave it to future work to evaluate the method with more comparable measurements. We examine the performance of different generative models: \u2022 Plain VAE with an elemental Gaussian detection model, which corresponds to a reconstruction error with a euclidean distance. \u2022 VAE with an acquired distance (VAEDisc constellation). We first form a GAN network and use the network as a comparator learning a size."}, {"heading": "3.1. Face images", "text": "We apply our methods to images from the CelebA dataset2 (Liu et al., 2015). This dataset consists of 202,599 images annotated with 40 binary attributes such as glasses, bangs, pale skin, etc. We scale and crop the images to 64 x 64 pixels and use only the images (not the attributes) for unattended training. After training, we extract samples from p (z) and propagate them through Dec to create new images that are shown in Fig. 3. The simple VAE is able to focus the frontal part of the face, but outside the center, the images become blurred. This is because the datasets align faces with frontal landmarks. If we move too far away from the aligned parts, the recognition model collapses because pixel correspondence cannot be accepted. VAEl produces sharper images even outside the center because the reconstruction error is canceled across pixels."}, {"heading": "3.1.1. VISUAL ATTRIBUTE VECTORS", "text": "Inspired by attempts to learn embeddings in which semantic concepts can be expressed using simple arithmetic (Mikolov et al., 2013), we examine the latent space of a trained AE / GAN model. The idea is to find directions in latent space that correspond to certain visual characteristics in the image space. We use the binary attributes of the dataset to extract visual attribute vectors. For all images, we use the encoder to calculate latent vector representations. For each attribute, we calculate the middle vector for images with the attribute and the middle vector for images without the attribute. We then calculate the visual attribute vector as the difference between the two middle vectors. This is a very simple method for calculating visual attribute vectors that have problems with strongly correlated visual attributes such as strong make-up and wearing lipstick, after we have added different visual attributes to each other, even though we have different face constructs in both."}, {"heading": "4. Related work", "text": "In the Computer Vision community, image pre-processing is a widely used solution to improve the robustness of certain disturbances. Examples of pre-processing include normalizing contrast, working with gradient images or pixel statistics collected in histograms. We consider these operations to be a form of metric engineering to account for the shortcomings of simple elementary distance measurements. A more detailed discussion of the topic is provided by Wang & Bovik (2009). Neural networks have been applied to metric learning in the form of Siamese architecture (Bromley et al al., 1993; Chopra et al., 2005). Distance measurements learned are minimized for similar examples and maximized for unequal samples with maximum margin costs. However, as Siamese networks are trained in a supervised setup, we cannot apply them directly to our problem. For autocoder (AE), several attempts have been suggested to soften the distances."}, {"heading": "5. Discussion", "text": "The problems with elementary distance measurements are well known in the literature, and many attempts have been made to go beyond pixels - typically by craft. In the spirit of deep learning, we argue that similarity measurement is another component that can be replaced by a learned model that produces images of unparalleled visual accuracy as shown in our experiments. In addition, we show that our network is capable of unraveling variation factors in the input data distribution and discovering visual attributes in the high-level representation of latent space. In principle, this allows us to use a large number of unlabeled images for training and the use of a small set of labeled images to discover features in latent space."}, {"heading": "Acknowledgements", "text": "We would like to thank S\u00f8ren Hauberg, Casper Kaae S\u00f8nderby and Lars Maal\u00f8e for their insightful discussions, Nvidia for donating GPUs for experiments, and the authors of DeepPy3 and CUDArray (Larsen, 2014) for the software frameworks used to implement our model."}], "references": [{"title": "Representation learning: A review and new perspectives", "author": ["Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pierre"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Quickly generating representative samples from an rbmderived process", "author": ["Breuleux", "Olivier", "Bengio", "Yoshua", "Vincent", "Pascal"], "venue": "Neural Computation,", "citeRegEx": "Breuleux et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Breuleux et al\\.", "year": 2011}, {"title": "Learning a similarity metric discriminatively, with application to face verification", "author": ["S. Chopra", "R. Hadsell", "Y. LeCun"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Chopra et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chopra et al\\.", "year": 2005}, {"title": "Learning to generate chairs with convolutional neural networks", "author": ["Dosovitskiy", "Alexey", "Springenberg", "Jost Tobias", "Brox", "Thomas"], "venue": "In IEEE International Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Dosovitskiy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dosovitskiy et al\\.", "year": 2015}, {"title": "A neural algorithm of artistic style", "author": ["Gatys", "Leon A", "Ecker", "Alexander S", "Bethge", "Matthias"], "venue": "CoRR, abs/1508.06576,", "citeRegEx": "Gatys et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gatys et al\\.", "year": 2015}, {"title": "Auto-encoding variational Bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "In Proceedings of the International Conference on Learning Representations,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Deep convolutional inverse graphics", "author": ["Kulkarni", "Tejas D", "Whitney", "Will", "Kohli", "Pushmeet", "Tenenbaum", "Joshua B"], "venue": "network. CoRR,", "citeRegEx": "Kulkarni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2015}, {"title": "CUDArray: CUDA-based NumPy", "author": ["Larsen", "Anders Boesen Lindbo"], "venue": "Technical Report DTU Compute 2014-21,", "citeRegEx": "Larsen and Lindbo.,? \\Q2014\\E", "shortCiteRegEx": "Larsen and Lindbo.", "year": 2014}, {"title": "Deep learning face attributes in the wild", "author": ["Liu", "Ziwei", "Luo", "Ping", "Wang", "Xiaogang", "Tang", "Xiaoou"], "venue": "In Proceedings of International Conference on Computer Vision (ICCV),", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Generating images from captions with attention", "author": ["Mansimov", "Elman", "Parisotto", "Emilio", "Ba", "Lei Jimmy", "Salakhutdinov", "Ruslan"], "venue": "CoRR, abs/1511.02793,", "citeRegEx": "Mansimov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mansimov et al\\.", "year": 2015}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Radford", "Alec", "Metz", "Luke", "Chintala", "Soumith"], "venue": "CoRR, abs/1511.06434,", "citeRegEx": "Radford et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Learning to generate images with perceptual similarity", "author": ["Ridgeway", "Karl", "Snell", "Jake", "Roads", "Brett", "Zemel", "Richard S", "Mozer", "Michael C"], "venue": "metrics. CoRR,", "citeRegEx": "Ridgeway et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ridgeway et al\\.", "year": 2015}, {"title": "A note on the evaluation of generative models", "author": ["L. Theis", "A. van den Oord", "M. Bethge"], "venue": "CoRR, abs/1511.01844,", "citeRegEx": "Theis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2015}, {"title": "Mean squared error: Love it or leave it? a new look at signal fidelity measures", "author": ["Wang", "Zhou", "A.C. Bovik"], "venue": "Signal Processing Magazine,", "citeRegEx": "Wang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2009}, {"title": "Attribute2Image: Conditional Image Generation from Visual Attributes", "author": ["X. Yan", "J. Yang", "K. Sohn", "H. Lee"], "venue": "CoRR, abs/1512.00570,", "citeRegEx": "Yan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "When learning models such as the variational autoencoder (VAE) (Kingma & Welling, 2014; Rezende et al., 2014), the choice of similarity metric is central as it provides the main part of the training signal via the reconstruction error objective.", "startOffset": 63, "endOffset": 109}, {"referenceID": 0, "context": "\u2022 We demonstrate that unsupervised training result in a latent image representation with disentangled factors of variation (Bengio et al., 2013).", "startOffset": 123, "endOffset": 144}, {"referenceID": 4, "context": "LGAN which we regard as a style error in addition to the reconstruction error which can be interpreted as a content error using the terminology from Gatys et al. (2015).", "startOffset": 149, "endOffset": 169}, {"referenceID": 1, "context": "Because the log-likelihood is intractable, it is often estimated using a Parzen window approach (Breuleux et al., 2011), but even for small images of 6\u00d76 pixels this is shown to be biased in favor of methods that overfit (Theis et al.", "startOffset": 96, "endOffset": 119}, {"referenceID": 13, "context": ", 2011), but even for small images of 6\u00d76 pixels this is shown to be biased in favor of methods that overfit (Theis et al., 2015).", "startOffset": 109, "endOffset": 129}, {"referenceID": 10, "context": "This modes has recently been shown capable of generating high-quality images (Denton et al., 2015; Radford et al., 2015).", "startOffset": 77, "endOffset": 120}, {"referenceID": 8, "context": "We apply our methods to face images from the CelebA dataset2 (Liu et al., 2015).", "startOffset": 61, "endOffset": 79}, {"referenceID": 2, "context": "Neural networks have been applied to metric learning in form of the Siamese architecture (Bromley et al., 1993; Chopra et al., 2005).", "startOffset": 89, "endOffset": 132}, {"referenceID": 11, "context": "Ridgeway et al. (2015) apply the structural similarity index as reconstruction metric for grey-scale images.", "startOffset": 0, "endOffset": 23}, {"referenceID": 11, "context": "Ridgeway et al. (2015) apply the structural similarity index as reconstruction metric for grey-scale images. Yan et al. (2015) let the AE output two additional images to learn shape and edge structures more explicitly.", "startOffset": 0, "endOffset": 127}, {"referenceID": 9, "context": "Mansimov et al. (2015) append a GAN-based sharpening step to their decoder network.", "startOffset": 0, "endOffset": 23}, {"referenceID": 10, "context": "(2015); Radford et al. (2015).", "startOffset": 8, "endOffset": 30}, {"referenceID": 3, "context": "First, Dosovitskiy et al. (2015) used supervised training to train convolutional network to generate chairs given highlevel information about the desired chair.", "startOffset": 7, "endOffset": 33}, {"referenceID": 3, "context": "First, Dosovitskiy et al. (2015) used supervised training to train convolutional network to generate chairs given highlevel information about the desired chair. Later, Kulkarni et al. (2015); Yan et al.", "startOffset": 7, "endOffset": 191}, {"referenceID": 3, "context": "First, Dosovitskiy et al. (2015) used supervised training to train convolutional network to generate chairs given highlevel information about the desired chair. Later, Kulkarni et al. (2015); Yan et al. (2015); Reed et al.", "startOffset": 7, "endOffset": 210}, {"referenceID": 3, "context": "First, Dosovitskiy et al. (2015) used supervised training to train convolutional network to generate chairs given highlevel information about the desired chair. Later, Kulkarni et al. (2015); Yan et al. (2015); Reed et al. (2015) have demonstrated encoder-decoder architectures with disentangled feature representations, but their training schemes rely on supervised information.", "startOffset": 7, "endOffset": 230}, {"referenceID": 3, "context": "First, Dosovitskiy et al. (2015) used supervised training to train convolutional network to generate chairs given highlevel information about the desired chair. Later, Kulkarni et al. (2015); Yan et al. (2015); Reed et al. (2015) have demonstrated encoder-decoder architectures with disentangled feature representations, but their training schemes rely on supervised information. Radford et al. (2015) inspect the latent space of a GAN after training and find directions corresponding to eyeglasses and smiles.", "startOffset": 7, "endOffset": 402}, {"referenceID": 3, "context": "First, Dosovitskiy et al. (2015) used supervised training to train convolutional network to generate chairs given highlevel information about the desired chair. Later, Kulkarni et al. (2015); Yan et al. (2015); Reed et al. (2015) have demonstrated encoder-decoder architectures with disentangled feature representations, but their training schemes rely on supervised information. Radford et al. (2015) inspect the latent space of a GAN after training and find directions corresponding to eyeglasses and smiles. As they rely on pure GANs, however, they cannot encode images making it challenging to explore the latent space. Our idea of a learned similarity metric is partly motivated by the neural artistic style network of Gatys et al. (2015) who demonstrate the representational power of deep convolutional features.", "startOffset": 7, "endOffset": 744}], "year": 2015, "abstractText": "We present an autoencoder that leverages the power of learned representations to better measure similarities in data space. By combining a variational autoencoder (VAE) with a generative adversarial network (GAN) we can use learned feature representations in the GAN discriminator as basis for the VAE reconstruction objective. Thereby, we replace element-wise errors with feature-wise errors that better capture the data distribution while offering invariance towards e.g. translation. We apply our method to images of faces and show that our method outperforms VAEs with element-wise similarity measures in terms of visual fidelity. Moreover, we show that our method learns an embedding in which high-level abstract visual features (e.g. wearing glasses) can be modified using simple arithmetic.", "creator": "LaTeX with hyperref package"}}}