{"id": "1601.00318", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jan-2016", "title": "A Unified Approach for Learning the Parameters of Sum-Product Networks", "abstract": "We present a unified approach for learning the parameters of Sum-Product networks (SPNs). We prove that any complete and decomposable SPN is equivalent to a mixture of trees where each tree corresponds to a product of univariate distributions. Based on the mixture model perspective, we characterize the objective function when learning SPNs based on the maximum likelihood estimation (MLE) principle and show that the optimization problem can be formulated as a signomial program. Both the projected gradient descent (PGD) and the exponentiated gradient (EG) in this setting can be viewed as first order approximations of the signomial program after proper transformation of the objective function. Based on the signomial program formulation, we construct two parameter learning algorithms for SPNs by using sequential monomial approximations (SMA) and the concave-convex procedure (CCCP), respectively. The two proposed methods naturally admit multiplicative updates, hence effectively avoiding the projection operation. With the help of the a unified framework, we also show an intrinsic connection between CCCP and Expectation Maximization (EM), where EM turns out to be another relaxation of the signomial program. Extensive experiments on 20 data sets demonstrate the effectiveness and efficiency of the two proposed approaches for learning SPNs. We also show that the proposed methods can improve the performance of structure learning and yield state-of-the-art results.", "histories": [["v1", "Sun, 3 Jan 2016 18:11:14 GMT  (447kb,D)", "https://arxiv.org/abs/1601.00318v1", null], ["v2", "Wed, 13 Jan 2016 06:49:49 GMT  (448kb,D)", "http://arxiv.org/abs/1601.00318v2", null], ["v3", "Sat, 30 Jan 2016 19:32:33 GMT  (456kb,D)", "http://arxiv.org/abs/1601.00318v3", null], ["v4", "Fri, 26 Aug 2016 18:10:50 GMT  (474kb,D)", "http://arxiv.org/abs/1601.00318v4", "NIPS 2016"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["han zhao", "pascal poupart", "geoffrey j gordon"], "accepted": true, "id": "1601.00318"}, "pdf": {"name": "1601.00318.pdf", "metadata": {"source": "CRF", "title": "A Unified Approach for Learning the Parameters of Sum-Product Networks", "authors": ["Han Zhao", "Pascal Poupart"], "emails": ["han.zhao@cs.cmu.edu", "ppoupart@uwaterloo.ca", "ggordon@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, it is at an all-time high in the history of the European Union."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Sum-Product Networks", "text": "To simplify the discussion on the main idea of our unified framework, we will focus our attention on SPNs about Boolean random variables. However, the framework presented here is general and can easily be extended to other discrete and continuous random variables. We will first define the term network polynomia. We will use Ix to denote an indicator variable that yields 1 if X = x and 0 otherwise. Definition 1 (network polynomial [4]). Let f (\u00b7) \u2265 0 indicate an unnormalized probability distribution via a Boolean random vector X1: N. The network polynomial of f (\u00b7) is a multilinear function [x f (x) \u0445N = 1 Ixn of indicator variables where summing over all possible instantiations of the Boolean random vector X1: N."}, {"heading": "2.2 Signomial Programming (SP)", "text": "Before introducing SP, we first introduce geometric programming (GP), which is a strict subclass of SP. A monome is defined as a function h: Rn + + 7 \u2192 R: h (x) = dxa11 xa22 \u00b7 \u00b7 \u00b7 xann, where the domain is limited to being the positive orthant (Rn + +), the coefficient d is positive and the exponents ai: h (x) = dxa11 xa22 \u00b7 \u00b7 \u00b7 xann. A posynomial is a sum of monomials: g (x) = \u2211 K = 1 dkx a1k 1 x a2k 2 \u00b7 \u00b7 xankn. One of the key properties of posynomials is positivity, which allows us to transform each posynomial into the log domain. A GP in standard form is defined as an optimization problem, where both the objective function and the inequality constraints are posynomic and the constraints are monomial."}, {"heading": "3 Unified Approach for Learning", "text": "In this section, we will show that the parameter learning problem of SPNs based on the MLE principle can be formulated as SP. We will use a sequence of optimal monomial approximations combined with traceability line search and concave-convex method to tackle SP. For space reasons, we refer interested readers to the supplementary material for all the evidence details."}, {"heading": "3.1 Sum-Product Networks as a Mixture of Trees", "text": "It is not a matter of whether or not it is a matter of whether or not it is a matter of whether or not it is a matter of whether or not it is a matter of whether or not it is a matter of whether or not it is a matter of whether or not it is a matter of whether or not it is a matter of whether or not it is a matter of whether or not it is a matter of whether or not it is a matter of whether or not it is a matter of whether or not it is a matter of whether or not it is a matter of whether or not it is a matter of whether or not it is a matter of whether or not it is a matter of whether or not it is a matter of whether or not it is a matter of whether or not it is a matter of whether or not it is a matter of whether or not it is a matter of whether or not it is a matter of whether or not it is a matter of whether or not it is a matter of whether or not it is a matter of whether or not it is a matter of whether or not, whether or not it is a matter of whether or not it is a matter of whether or not, whether or not it is a matter of whether or not, whether or not it is not, whether or not it is a matter of whether or not it is a matter of whether or not it is a matter of whether or not it is a matter of whether or not it is a matter of whether or not it is a matter of whether or not it is a matter of whether or not a matter of whether or not it is a matter of whether or not it is a matter of whether or not it is a matter of whether or not a matter of whether or not it is a matter of whether or not it is a matter of whether or not it is a matter of whether or not a matter of whether or not it is a matter of whether or not a matter of whether or not it is a matter of whether or not it is a matter of whether or not it is a matter of whether or not it is a matter of whether or not it is a matter of whether or not a matter of whether or not it is a matter of whether or not a matter of whether or not it is a matter of whether or not a matter of whether or not it is a matter of whether or not a matter of whether or not a matter of whether or not a matter of whether or not a matter of whether or not"}, {"heading": "3.2 Maximum Likelihood Estimation as SP", "text": "It is the most probable function calculated by a SPN S via N binary random variables with model parameters w and input vector x 0, 1, N. The probability distribution generated by S can be of PrS (x-w), fS (x-w), fS (x-w), fS (x-w), fS (x-w), fS (x-W), fS (x-W), fS (x-W), fS (x-W), fS (x-W), fS (x-W), fS (x-W), fS (x-W), fS (x-W), fS (x-W), fS (x-W)."}, {"heading": "3.3 Difference of Convex Functions", "text": "Although (1) is a signomial program, its objective function is expressed as the ratio of two posynomes. Therefore, we can still apply the logarithmic transformation trick applied in geometric programming to its objective function and to the variables to be optimized. Specifically, allow wd = exp (yd) and take the protocol of the objective function; it becomes synonymous with maximizing the next new object without any limitation to y: maximizing the protocol (x) x = 1 exp (D-d = 1 ydIyd-Tt) \u2212 log (D-p = 1 exp (D-d = 1 ydIyd-Tt))). (2) Note that in the first term of Equation 2 the upper index point (x) is the consequence (fS problem (1) hexp (D-T = 1 exp = 1 exp (D-D = 1 function yd-Tt), the natural transformation of the Tx)."}, {"heading": "3.3.1 Sequential Monomial Approximation", "text": "Consider the linearisation of both terms in Equation 2 to apply first-order methods in transformed space. To calculate the gradient in relation to different components of y, we consider each node of an SPN to be an intermediate function of the network polynomial and apply the chain rule to propagate the gradient back. Differentiation of fS (x | w) in relation to the root node of the network is set to 1. Differentiation of the network polynomial in relation to a sub-function at each node can then be calculated in two passes of the network: The bottom-up pass evaluates the values of all sub-functions in relation to the current input x and the top-down pass differentiates the network polynomial in relation to each sub-function. Following the evaluation differentiation, the gradient of the objective function runs in (2) the gradient of the objective function in (2) for the calculation of O (S) |)."}, {"heading": "3.3.2 Concave-convex Procedure", "text": "In many cases, the maximum of a concave replacement function can only depend on other convex solvers and, as a result of the efficiency of CCCP, to a large extent on the choice of convex solvers. However, we show that by a suitable transformation of the mesh, we can calculate the maximum of the convex replacement function in closed form, which is linear in network size, resulting in a very efficient algorithm development for learning SPNs."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Experimental Setting", "text": "We conduct experiments with 20 benchmark datasets from different domains to compare and evaluate the convergence performance of the four algorithms PGD, EG, SMA and CCCP (EM). These 20 datasets are commonly used in [7, 15] to evaluate different SPNs for density estimation. All characteristics in the 20 datasets are binary characteristics. All SPNs used for comparing PGD, EG, SMA and CCCP are trained with LearnSPN [7]. We discard the weights returned by LearnSPN and use random weights as starting parameters. Random weights are determined by the same random starting value in all four algorithms. Detailed information about these 20 datasets and the SPNs used in the experiments are provided in the supplementary material."}, {"heading": "4.2 Parameter Learning", "text": "We implement all four algorithms in C + +. For each algorithm, we set the maximum number of iterations to 50. However, if the absolute difference in the training protocol probability in two consecutive steps is less than 0.001, the algorithms are stopped. However, for PGD, EG and SMA, we combine each of them with the backtracking line search and use a weight shrinkage coefficient of 0.8. Learning rates are initialized to 1.0 for all three methods. For PGD, we set the projection span to 0.01. There is no learning rate and no backtracking row search in CCCP. We set the smoothing parameters to 0.001 in CCCP to avoid numerical problems. In Fig. 2, we show the average log likelihood scores to 20 training data to evaluate the convergence speed and stability of the PGD."}, {"heading": "4.3 Fine Tuning", "text": "We combine CCCP as a \"fine-tuning\" method with the LearnSPN structure learning algorithm and compare it with the state-of-the-art structure learning algorithm ID-SPN [15]. Specifically, we maintain the model parameters learned from LearnSPN and use them to initialize CCCP. We then update the model parameters globally with CCCP as a fine-tuning technique. This usually helps to obtain a better generative model, as the original parameters are learned greedily and locally during the structure learning algorithm. We use the Log-Likelihood Score validation set to avoid match. The algorithm provides the set of parameters that achieve the best validation set Log-Likelihood Score as output. For LearnSPN and ID-SPN, we use their publicly available implementations provided by the original authors, and the standard SPN parameter settings to improve the LearnSPN learning algorithm. Experiments are presented in the LearnSPN LearnSPN table, using LearnSPN to optimize the LearnSPN learning results according to the use of CCnSPP algorithms."}, {"heading": "5 Conclusion", "text": "We show that the network polynomial of an SPN is a posynomic function of the model parameters and that learning the parameter with maximum probability results in a signomial program. We propose two convex relaxations to solve the SP. We analyze the convergence properties of CCCP to learn SPNs. Extensive experiments are conducted to evaluate the proposed approaches and current methods. We also recommend combining CCCP with current structure learning algorithms to increase modeling accuracy."}, {"heading": "Acknowledgments", "text": "HZ and GG are grateful for the support of the ONR contract N000141512365. HZ also thanks Ryan Tibshirani for the helpful discussion about CCCP."}, {"heading": "A Proof of SPNs as Mixture of Trees", "text": "This means that there are at least two parents in T, namely two parents in T, p1, p1,. v and R, q1,......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "B MLE as Signomial Programming", "text": "Proposition 6: The MLE problem for SPNs is a signomial problem. Proof. On the basis of the definition of Pr (x | w) and Corollary 5, let \u03c4 = fS (1 | 1), the MLE problem can be rewritten asmaximizew fS (x | w) fS (1 | w) = \u2211 t = 1 \u00b2 N = 1 I (t) xn \u00b2 D = 1 wIwd \u00b2 Tt \u00b2 T = 1 \u00b2 D = 1 w Iwd \u00b2 Tt \u00b2 T + + (8), which we claim is equivalent to: minimizew, z \u00b2 T = 1 wIwd \u00b2 Tt \u00b2 T = 1 N \u00b2 N \u00b2 N = 1 I (t) xn D \u00b2 D = 1 wd \u00b2 T \u00b2 T + + + (8), of which we claim that it is equivalent to: minimizew, z \u00b2 Z \u00b2 Z \u00b2 Z \u00b2 Z \u00b2 Z \u00b2 Z \u00b2 Z \u00b2 Z \u00b2 Z \u00b2 Z \u00b2 Z \u00b2 Z \u00b2 Z \u00b2 function can also be verified as a function N \u00b2 Z \u00b2 Z \u00b2 Z \u00b2 function, and as a function 9 wd \u00b2 Z \u00b2 Z \u00b2 Z \u00b2 Z \u00b2 Z \u00b2 Z \u00b2 Z \u00b2 Z \u00b2 Z \u00b2 Z \u00b2 Z \u00b2 Z \u00b2 Z \u00b2 Z \u00b2 Z \u00b2 Z is simple as a function."}, {"heading": "C Convergence of CCCP for SPNs", "text": "We discussed previously that the sequence of functional values (f (k)), (k), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, (c), (c), (c), (c), (c), (c), (c, (c), (c), (c), (c, (c), (c), (c), (c, (c), (c), (c), (c, (c), (c), (c), (c, c, (c, c, c, c), (c, c, c), (c, c, c, c, c, c, c), (c, c, c, c, c, c, c, (c, c, c, c, c, c, c, c, c, (c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, (c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, (c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c,"}, {"heading": "D Experiment Details", "text": "D.1 Methods We will briefly review the current approach to training SPNs using the Projected Gradient Descent (PGD). Another related approach is the use of the Exposed Gradient (EG) [10] for optimization (8).The PGD optimizes the log probability by projecting the intermediate solution back to the positive orthane after each gradient update. Since the constraint in (8) is an open sentence, we must manually create a closed sentence on which the projection process can be well defined. A viable option is to project onto RD, assuming > 0 to be very small. To avoid the projection, a direct solution is to use the Exposed Gradient Method (EG) [10], which was first applied in an online environment and the latter successfully extended to batch settings when training is performed with Convex Model Pets."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "We present a unified approach for learning the parameters of Sum-Product networks<lb>(SPNs). We prove that any complete and decomposable SPN is equivalent to a<lb>mixture of trees where each tree corresponds to a product of univariate distributions.<lb>Based on the mixture model perspective, we characterize the objective function<lb>when learning SPNs based on the maximum likelihood estimation (MLE) principle<lb>and show that the optimization problem can be formulated as a signomial program.<lb>We construct two parameter learning algorithms for SPNs by using sequential<lb>monomial approximations (SMA) and the concave-convex procedure (CCCP),<lb>respectively. The two proposed methods naturally admit multiplicative updates,<lb>hence effectively avoiding the projection operation. With the help of the unified<lb>framework, we also show that, in the case of SPNs, CCCP leads to the same<lb>algorithm as Expectation Maximization (EM) despite the fact that they are different<lb>in general.", "creator": "LaTeX with hyperref package"}}}