{"id": "1601.00706", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jan-2016", "title": "Weakly-supervised Disentangling with Recurrent Transformations for 3D View Synthesis", "abstract": "An important problem for both graphics and vision is to synthesize novel views of a 3D object from a single image. This is particularly challenging due to the partial observability inherent in projecting a 3D object onto the image space, and the ill-posedness of inferring object shape and pose. However, we can train a neural network to address the problem if we restrict our attention to specific object categories (in our case faces and chairs) for which we can gather ample training data. In this paper, we propose a novel recurrent convolutional encoder-decoder network that is trained end-to-end on the task of rendering rotated objects starting from a single image. The recurrent structure allows our model to capture long-term dependencies along a sequence of transformations. We demonstrate the quality of its predictions for human faces on the Multi-PIE dataset and for a dataset of 3D chair models, and also show its ability to disentangle latent factors of variation (e.g., identity and pose) without using full supervision.", "histories": [["v1", "Tue, 5 Jan 2016 00:08:09 GMT  (6511kb,D)", "http://arxiv.org/abs/1601.00706v1", "This was published in NIPS 2015 conference"]], "COMMENTS": "This was published in NIPS 2015 conference", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV", "authors": ["jimei yang", "scott e reed", "ming-hsuan yang 0001", "honglak lee"], "accepted": true, "id": "1601.00706"}, "pdf": {"name": "1601.00706.pdf", "metadata": {"source": "CRF", "title": "Weakly-supervised Disentangling with Recurrent Transformations for 3D View Synthesis", "authors": ["Jimei Yang", "Scott Reed", "Ming-Hsuan Yang", "Honglak Lee"], "emails": ["mhyang}@ucmerced.edu", "honglak}@umich.edu", "jimyang@adobe.com"], "sections": [{"heading": "1 Introduction", "text": "There are a number of approaches aimed at gaining abstract representations of objects contained in images, such as classification [11], segmentation of images that are automatically synchronized, and labeling of image formats. These approaches typically aim to derive abstract representations from raw image pixels. However, it is a tedious problem to automatically apply novel images for both graphics and vision, using intrinsic transformations (e.g. 3D rotation and deformation) to decipher the subject of an input-image perspective. This can be seen as an answer to questions about the appearance of objects, or enlightenment. These synthesized images can then be perceived by humans in image processing [15] or by other systems."}, {"heading": "2 Related Work", "text": "In fact, most of them are not a political party, but a party that is able to establish itself."}, {"heading": "3 Recurrent Convolutional Encoder-Decoder Network", "text": "In this section, we describe our model formulation. Faced with an image of the 3D object, our goal is to synthesize its rotated views. Inspired by the recent success of Constitutional Networks (CNNs) in mapping images to high-level abstract representations [17] and synthesizing images from graphic codes [8], we are based on profound Convolutionary Encoder Decoder Networks. An example of the network structure is shown in Figure 1. The encoder network uses 5 Konvolution-relu layers with step width 2 and 2-pixel padding, so that the dimension is halved at each convolution layer, followed by two fully connected layers. In the narrowly designed layer, we define a group of units to represent the pose (pose units) in which the desired transformations can be applied. The other group of units represent what does not change during transformations, and are referred to as identity units."}, {"heading": "3.1 Curriculum Training", "text": "We trained the network parameters using back propagation through time and the ADAM optimization method [3]. In order to train our relapsing network effectively, we found it beneficial to learn curricula [4] in which we gradually increase the difficulty of the training by increasing the trajectory length, which also seems to be useful for predicting sequences with relapsing networks in other areas [22, 29]. In Section 4, we show that increasing the training sequence length improves both the model's image prediction performance and the position-invariant recognition performance of identity traits. In addition, longer training sequences force the identity units to unravel better from the pose. If the same identity units have to be used to predict both a 15-rotated and a 120-rotated image during training, these units cannot capture position-related information. In this way, our model can learn unraveled features (i.e., we cannot perform identity-invariance effects without identifying them), and vice versa."}, {"heading": "4 Experiments", "text": "We are conducting experiments to achieve the following objectives: First, we are investigating the ability of our model to synthesize high-quality images of both face and complex 3D objects (chairs) at a wide range of rotation angles; second, we are evaluating the discriminatory performance of disentangled identity units by cross-sectional object recognition; third, we are demonstrating the ability to create and rotate new object classes by interpolating identity units from query objects."}, {"heading": "4.1 Datasets", "text": "The Multi-PIE [12] dataset consists of 754,204 facial images of 337 people. Images are taken from 15 viewing angles under 20 lighting conditions in different sessions. To evaluate our rotating face model, we select a subset of Multi-PIE that covers 7 viewpoints evenly from \"45\" to 45 \"under neutral lighting. Each facial image is aligned to the eyes, nose and mouth corners by manually annotated landmarks and then cut to 80,000 60\u04453 pixels. We use the images of the first 200 people for training and the remaining 137 people for testing. This dataset contains 1393 chair CAD models made available to the public by Aubry et al. [2]. Each chair model is rendered from 31 azimuth angles (with steps of 11 \u2212 or 12 \u2212) and 2 elevation angles at a fixed distance from the virtual camera."}, {"heading": "4.2 Network Architectures and Training Details", "text": "The number of identity and pose units is 512 and 128, respectively. The decoder network is symmetrical to the encoder. The curriculum training procedure begins with the single-stage rotation model we use RN1. We prepare the training samples by taking facial images of the same person in the same session with adjacent camera points. For example, xpiq at \"30\" and \"ypiq at\" 15, \"r001s; xpiq at\" 15 \"and\" 15, \"we mapped facial images of the same person taken in the same session with adjacent camera points."}, {"heading": "4.3 3D View Synthesis of Novel Objects", "text": "It is a question of whether the naming of a name is a 'typical' name or a 'typical' name, a 'typical' name, a 'typical' name, a 'typical' name, a 'typical' name, a 'typical' name, a 'typical' name, a 'typical' name, a name, a name, a name, a name, a name, a name, a name, a name, a first name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a name, a"}, {"heading": "4.4 Cross-View Object Recognition", "text": "In this experiment, we examine and compare the discriminatory performance of unbundled representations by mutual recognition of objects and their layers. Multi-PIE. We create 7 galleries / probe splits from the test set. In each split, the facial images of the same view, e.g. \"45,\" are collected as a gallery and the rest of other views as probes. We extract 512-d features from the identity units of RNNNs for all test images, so that the probes of the gallery are matched by their cosine distance. It is considered a success if the customized gallery image has the same identity with a probe. We also categorize the probes in each split by measuring their angle distances from the gallery, so that the angle distances range from 15 to 90. Detection difficulties increase with angle distances. To demonstrate the discriminatory performance of our learned representations, we also implement a conventional network classifier 200. The CNN code architecture is set up through the encoding of our identity encodings."}, {"heading": "4.5 Class Interpolation and View Synthesis", "text": "In this experiment, we demonstrate the ability of our RNN model to generate new chairs by interpolating between two existing chairs. In view of two chair images of the same view from different instances, the encoder network is used to calculate their identity units z1id, z 2 id and pose units z1pose and z 2 pose, respectively, the interpolation is fed by zid \"\u03b2z1id\" p1 \"\u03b2qz2id and zpose\" \u03b2z1pose \"p1\" \u03b2qz2pose, where \u03b2 \"r0.0, 0.2, 0.4, 0.6, 0.8, 1.0. The interpolated cide and Zpose are then fed into the recursive decoder network to render its rotated views. Example interpolations between four chair instances are shown in Figure 9. The interpolated chairs show gentle stylistic transformations between any input classes (each line in Figure 9), and their unique stylistic properties are also well preserved in each of their 9 anrounded views."}, {"heading": "5 Conclusion", "text": "In this work, we develop a recurring convolutionary encoder decoder network and demonstrate its effectiveness for the synthesis of 3D views of invisible object instances. Based on the Multi-PIE dataset and a database of 3D CAD models for the chair, the model predicts precise renderings of the trajectories of repeated rotations. The proposed curriculum training by gradually increasing the trajectory length of the training sequences results in both a better image appearance and more differentiated features for detecting poseinvariant movements. We also show that a trained model can interpolate the identity diversity of chairs in fixed pose, traversing the pose in manifold ways. This generative untangling of chair identity and pose originated from our recurring rotation prediction goal, although we do not explicitly regulate the hidden units that are to be untangled by handling tasks that are included in many of our proposed rotation actions other than the one."}, {"heading": "5.1 Training the chair model with mask stream.", "text": "When developing the stool model, we decipher not only the rotated stool images, but also their binary masks. The layered structure is shown in Figure 10. As a simpler predictive object than the image, the binary mask ensures proper regulation of the network and greatly improves prediction performance. We compare the learning curves with and without mask current to form RNN1 in Figure 11. It is noteworthy that the training loss for the network with mask current decreases faster and its test loss tends to converge better."}, {"heading": "5.2 Results on cars.", "text": "We use the auto-CAD models collected by [9]. For each of the 183 CAD models, we generate 64x64 grayscale renderings from 24 azimuth angles, offset by 15 degrees and 4 vertical angles [0,6,12,18]. The renderings from the first 150 models are used for training and the remaining 33 models for testing. In this experiment, the same network structure as the chair model in Figure 10 is used, except that both input and output layers are single-channel for grayscale images. The curriculum training also follows the procedure in the chair experiment. We train RNN1, RNN2, RN4, RNN8 and RNN16 sequentially. Note that we train the network only for azimuth rotation."}], "references": [{"title": "Understanding deep features with computer-generated imagery", "author": ["M. Aubry", "B.C. Russell"], "venue": "ICCV,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Seeing 3D chairs: exemplar part-based 2D-3D alignment using a large dataset of CAD models", "author": ["M. Aubry", "D. Maturana", "A.A. Efros", "B. Russell", "J. Sivic"], "venue": "CVPR,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["J. Ba", "D. Kingma"], "venue": "ICLR,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Curriculum learning", "author": ["Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston"], "venue": "ICML,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "A morphable model for the synthesis of 3D faces", "author": ["V. Blanz", "T. Vetter"], "venue": "SIGGRAPH,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1999}, {"title": "Discovering hidden factors of variation in deep networks", "author": ["B. Cheung", "J. Livezey", "A. Bansal", "B. Olshausen"], "venue": "ICLR,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Mental rotation by optimizing transforming distance", "author": ["W. Ding", "G. Taylor"], "venue": "NIPS Deep Learning and Representation Learning Workshop,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning to generate chairs with convolutional neural networks", "author": ["A. Dosovitskiy", "J. Springenberg", "T. Brox"], "venue": "CVPR,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "3D object detection and viewpoint estimation with a deformable 3D cuboid model", "author": ["S. Fidler", "S. Dickinson", "R. Urtasun"], "venue": "NIPS,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Deepstereo: Learning to predict new views from the world\u2019s imagery", "author": ["J. Flynn", "I. Neulander", "J. Philbin", "N. Snavely"], "venue": "arXiv preprint arXiv:1506.06825,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast R-CNN", "author": ["R. Girshick"], "venue": "ICCV,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-PIE", "author": ["R. Gross", "I. Matthews", "J. Cohn", "T. Kanade", "S. Baker"], "venue": "Image and Vision Computing, 28 (5):807\u2013813, May", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Transforming auto-encoders", "author": ["G.E. Hinton", "A. Krizhevsky", "S.D. Wang"], "venue": "ICANN,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "3D object manipulation in a single photograph using stock 3D models", "author": ["N. Kholgade", "T. Simon", "A. Efros", "Y. Sheikh"], "venue": "SIGGRAPH,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Auto-encoding variational Bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "ICLR,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep convolutional inverse graphics network", "author": ["T.D. Kulkarni", "W. Whitney", "P. Kohli", "J.B. Tenenbaum"], "venue": "NIPS,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CVPR,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Modeling deep temporal dependencies with recurrent \u201cgrammar cells", "author": ["V. Michalski", "R. Memisevic", "K. Konda"], "venue": "NIPS,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "NIPS Deep Learning Workshop,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Action-conditional video prediction using deep networks in atari games", "author": ["J. Oh", "X. Guo", "H. Lee", "R. Lewis", "S. Singh"], "venue": "NIPS,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to disentangle factors of variation with manifold interaction", "author": ["S. Reed", "K. Sohn", "Y. Zhang", "H. Lee"], "venue": "ICML,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Mental rotation of three dimensional objects", "author": ["R.N. Shepard", "J. Metzler"], "venue": "Science, 171(3972):701\u2013703,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1971}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Separating style and content with bilinear models", "author": ["J.B. Tenenbaum", "W.T. Freeman"], "venue": "Neural Computation, 12(6):1247\u20131283,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2000}, {"title": "Optimizing neural networks that generate images", "author": ["T. Tieleman"], "venue": "PhD thesis, University of Toronto,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CVPR,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to execute", "author": ["W. Zaremba", "I. Sutskever"], "venue": "arXiv preprint arXiv:1410.4615,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "High-fidelity pose and expression normalization for face recognition in the wild", "author": ["X. Zhu", "Z. Lei", "J. Yan", "D. Yi", "S.Z. Li"], "venue": "CVPR,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-view perceptron: a deep model for learning face identity and view representations", "author": ["Z. Zhu", "P. Luo", "X. Wang", "X. Tang"], "venue": "NIPS,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 16, "context": "At the same time, recent advances in vision algorithms enable computers to gain some form of understanding of objects contained in images, such as classification [17], detection [11], segmentation [19], and caption generation [28], to name a few.", "startOffset": 162, "endOffset": 166}, {"referenceID": 10, "context": "At the same time, recent advances in vision algorithms enable computers to gain some form of understanding of objects contained in images, such as classification [17], detection [11], segmentation [19], and caption generation [28], to name a few.", "startOffset": 178, "endOffset": 182}, {"referenceID": 18, "context": "At the same time, recent advances in vision algorithms enable computers to gain some form of understanding of objects contained in images, such as classification [17], detection [11], segmentation [19], and caption generation [28], to name a few.", "startOffset": 197, "endOffset": 201}, {"referenceID": 27, "context": "At the same time, recent advances in vision algorithms enable computers to gain some form of understanding of objects contained in images, such as classification [17], detection [11], segmentation [19], and caption generation [28], to name a few.", "startOffset": 226, "endOffset": 230}, {"referenceID": 14, "context": "These synthesized images may then be perceived by humans in photo editing [15], or evaluated by other machine vision systems, such as the game playing agent with vision-based reinforcement learning [21, 22].", "startOffset": 74, "endOffset": 78}, {"referenceID": 20, "context": "These synthesized images may then be perceived by humans in photo editing [15], or evaluated by other machine vision systems, such as the game playing agent with vision-based reinforcement learning [21, 22].", "startOffset": 198, "endOffset": 206}, {"referenceID": 21, "context": "These synthesized images may then be perceived by humans in photo editing [15], or evaluated by other machine vision systems, such as the game playing agent with vision-based reinforcement learning [21, 22].", "startOffset": 198, "endOffset": 206}, {"referenceID": 4, "context": ", faces [5].", "startOffset": 8, "endOffset": 11}, {"referenceID": 23, "context": "Shepard and Metzler in their mental rotation experiments [24] found that the time taken for humans to match 3D objects from two different views increased proportionally with the angular rotational difference between them.", "startOffset": 57, "endOffset": 61}, {"referenceID": 16, "context": "The network consists of four components: a deep convolutional encoder [17], shared identity units, recurrent pose units with rotation action inputs, and a deep convolutional decoder [8].", "startOffset": 70, "endOffset": 74}, {"referenceID": 7, "context": "The network consists of four components: a deep convolutional encoder [17], shared identity units, recurrent pose units with rotation action inputs, and a deep convolutional decoder [8].", "startOffset": 182, "endOffset": 185}, {"referenceID": 28, "context": "To improve the ease of training, we employed curriculum learning, similar to that used in other sequence prediction problems [29].", "startOffset": 125, "endOffset": 129}, {"referenceID": 12, "context": "2 Related Work The transforming autoencoder [13] introduces the notion of capsules in deep networks, which tracks both the presence and position of visual features in the input image.", "startOffset": 44, "endOffset": 48}, {"referenceID": 19, "context": "The Predictive Gating Pyramid [20] is developed for time-series prediction and can learn image transformations including shifts and rotation over multiple time steps.", "startOffset": 30, "endOffset": 34}, {"referenceID": 6, "context": "Ding and Taylor [7] proposed a gating network to directly model mental rotation by optimizing transforming distance.", "startOffset": 16, "endOffset": 19}, {"referenceID": 26, "context": "The problem of training neural networks that generate images is studied in [27].", "startOffset": 75, "endOffset": 79}, {"referenceID": 7, "context": "[8] proposed a convolutional network mapping shape, pose and transformation labels to images for generating chairs.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "Contemporary to our work, the Inverse Graphics Network (IGN) [18] also adds an encoding function to learn graphics codes of images, along with a decoder similar to that in the chair generating network.", "startOffset": 61, "endOffset": 65}, {"referenceID": 15, "context": "Our model differs in that 1) we train a recurrent network to perform trajectories of multiple transformations, 2) we add control signal input at each step, and 3) we use deterministic feed-forward training rather than the variational auto-encoder (VAE) framework [16] (although our approach could be extended to a VAE version).", "startOffset": 263, "endOffset": 267}, {"referenceID": 25, "context": "Bilinear models for separating style and content are developed in [26], and are shown to be capable of separating handwriting style and character identity, and also separating face identity and pose.", "startOffset": 66, "endOffset": 70}, {"referenceID": 22, "context": "The disentangling Boltzmann Machine (disBM) [23] applies this idea to augment the Restricted Boltzmann Machine by partitioning its hidden state into distinct factors of variation and modeling their higher-order interaction.", "startOffset": 44, "endOffset": 48}, {"referenceID": 30, "context": "The multi-view perceptron [31] employs a stochastic feedforward network to disentangle the identity and pose factors of face images in order to achieve view-invariant recognition.", "startOffset": 26, "endOffset": 30}, {"referenceID": 5, "context": "In [6], the (potentially unknown) latent factors of variation are both discovered and disentangled using a novel hidden unit regularizer.", "startOffset": 3, "endOffset": 6}, {"referenceID": 9, "context": "Our work is also loosely related to the \u201cDeepStereo\" algorithm [10] that synthesizes novel views of scenes from multiple images using deep convolutional networks.", "startOffset": 63, "endOffset": 67}, {"referenceID": 16, "context": "Inspired by recent success of convolutional networks (CNNs) in mapping images to high-level abstract representations [17] and synthesizing images from graphics codes [8], we base our model on deep convolutional encoder-decoder networks.", "startOffset": 117, "endOffset": 121}, {"referenceID": 7, "context": "Inspired by recent success of convolutional networks (CNNs) in mapping images to high-level abstract representations [17] and synthesizing images from graphics codes [8], we base our model on deep convolutional encoder-decoder networks.", "startOffset": 166, "endOffset": 169}, {"referenceID": 7, "context": "To increase dimensionality we use fixed upsampling as in [8].", "startOffset": 57, "endOffset": 60}, {"referenceID": 1, "context": ", [2 0 0] for two-step clockwise rotation, the pose units will fall off the manifold resulting in bad predictions.", "startOffset": 2, "endOffset": 9}, {"referenceID": 2, "context": "1 Curriculum Training We trained the network parameters using backpropagation through time and the ADAM optimization method [3].", "startOffset": 124, "endOffset": 127}, {"referenceID": 3, "context": "To effectively train our recurrent network, we found it beneficial to use curriculum learning [4], in which we gradually increase the difficulty of training by increasing the trajectory length.", "startOffset": 94, "endOffset": 97}, {"referenceID": 21, "context": "This appears to be useful for sequence prediction with recurrent networks in other domains as well [22, 29].", "startOffset": 99, "endOffset": 107}, {"referenceID": 28, "context": "This appears to be useful for sequence prediction with recurrent networks in other domains as well [22, 29].", "startOffset": 99, "endOffset": 107}, {"referenceID": 11, "context": "The Multi-PIE [12] dataset consists of 754,204 face images from 337 people.", "startOffset": 14, "endOffset": 18}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] in order to remove nearduplicate models (e.", "startOffset": 0, "endOffset": 3}, {"referenceID": 29, "context": "Figure 4: Comparing face pose normalization results with 3D morphable model [30].", "startOffset": 76, "endOffset": 80}, {"referenceID": 7, "context": "We adopted this idea from the generative CNN [8] and found it beneficial to training efficiency and image synthesis quality.", "startOffset": 45, "endOffset": 48}, {"referenceID": 13, "context": "We carry out experiments using Caffe [14] on Nvidia K40c and Titan X GPUs.", "startOffset": 37, "endOffset": 41}, {"referenceID": 29, "context": "We also compare our RNN model with a state-of-the-art 3D morphable model for face pose normalization [30] in Figure 4.", "startOffset": 101, "endOffset": 105}, {"referenceID": 24, "context": "We first extract the CNN features \u201cfc7\u201d from VGG-16 net [25] for all the chair images.", "startOffset": 56, "endOffset": 60}, {"referenceID": 0, "context": "The retrieved top-K images are expected to be similar to the query in terms of both style and pose [1].", "startOffset": 99, "endOffset": 102}, {"referenceID": 0, "context": "We tune the K value in [1,3,5,7], namely KNN1, KNN3, KNN5 and KNN7 to achieve the best performance.", "startOffset": 23, "endOffset": 32}, {"referenceID": 2, "context": "We tune the K value in [1,3,5,7], namely KNN1, KNN3, KNN5 and KNN7 to achieve the best performance.", "startOffset": 23, "endOffset": 32}, {"referenceID": 4, "context": "We tune the K value in [1,3,5,7], namely KNN1, KNN3, KNN5 and KNN7 to achieve the best performance.", "startOffset": 23, "endOffset": 32}, {"referenceID": 6, "context": "We tune the K value in [1,3,5,7], namely KNN1, KNN3, KNN5 and KNN7 to achieve the best performance.", "startOffset": 23, "endOffset": 32}, {"referenceID": 24, "context": "We also compare our model against CNN, but instead of training CNN from scratch we use the pre-trained VGG-16 net [25] to extract the 4096-d \u201cfc7\u201d features for chair matching.", "startOffset": 114, "endOffset": 118}], "year": 2016, "abstractText": "An important problem for both graphics and vision is to synthesize novel views of a 3D object from a single image. This is particularly challenging due to the partial observability inherent in projecting a 3D object onto the image space, and the ill-posedness of inferring object shape and pose. However, we can train a neural network to address the problem if we restrict our attention to specific object categories (in our case faces and chairs) for which we can gather ample training data. In this paper, we propose a novel recurrent convolutional encoder-decoder network that is trained end-to-end on the task of rendering rotated objects starting from a single image. The recurrent structure allows our model to capture long-term dependencies along a sequence of transformations. We demonstrate the quality of its predictions for human faces on the Multi-PIE dataset and for a dataset of 3D chair models, and also show its ability to disentangle latent factors of variation (e.g., identity and pose) without using full supervision.", "creator": "LaTeX with hyperref package"}}}