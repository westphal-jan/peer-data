{"id": "1601.00710", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jan-2016", "title": "Multi-Source Neural Translation", "abstract": "We build a multi-source machine translation model and train it to maximize the probability of a target English string given French and German sources. Using the neural encoder-decoder framework, we explore several combination methods and report up to +4.8 Bleu increases on top of a very strong attention-based neural translation model.", "histories": [["v1", "Tue, 5 Jan 2016 00:49:22 GMT  (259kb,D)", "http://arxiv.org/abs/1601.00710v1", "5 pages, 6 figures"]], "COMMENTS": "5 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["barret zoph", "kevin knight"], "accepted": true, "id": "1601.00710"}, "pdf": {"name": "1601.00710.pdf", "metadata": {"source": "CRF", "title": "Multi-Source Neural Translation", "authors": ["Barret Zoph"], "emails": ["zoph@isi.edu", "knight@isi.edu"], "sections": [{"heading": "1 Introduction", "text": "Kay (2000) points out that once a document is translated, it is likely to be translated into other languages again and again. (This leads to an interesting idea: a person does the first translation by hand, then the rest turns to machine translation (MT). The translation system now has two strings as input, which can reduce ambiguity by \"triangulation\" (Kay's term). For example, the normally ambiguous English word \"bank\" can be translated more easily into French in the presence of a second German input string containing the word \"river bank.\" Och and Ney (2001) describe such a multisource MT system. They first form separate bilingual MT systems F \u2192 E, etc. At runtime, they translate input strings f and g into candidate target strings e1 and e2, then choose the best one of the two."}, {"heading": "2 Multi-Source Neural MT", "text": "Within the neural encoder decoder system for MT (Neco and Forcada, 1997; Castan o and Casacuberta, 1997; Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015b), we use a recursive Xiv: 160 1.00 710v 1 [cs.C L] 5J an2 016rent neural network (encoder) to convert a source set into a dense fixed-length vector. We then use another recursive network (decoder) to convert this vector into a target set. 1In this paper, we use a four-layer encoder decoder system (Figure 1) with long-term memory units (LSTM) (Hochreiter and Schmidhuber, 1997) trained for maximum probability (via a softmax layer) with backward propagation over time (Werbos, 1990)."}, {"heading": "2.1 Basic Combination Method", "text": "The Basic method works by concatenating the two hidden states of the source encoders, applying a linear transformation Wc (size 2000 x 1000) and then sending its output by a tanh nonlinearity. This operation is represented by the equation: h = tanh (Wc [h1; h2]) (1) Wc and all other weights in the network are1We follow previous authors in representing the source set to the encoder in reverse order. Learned from string triples from a trilingual training corpus. The new cell state is simply the sum of the two cell states from the encoders. c = c1 + c2 (2) We have also tried to concatenate cell states and apply a linear transformation, but the formation differs from each other due to large cell values."}, {"heading": "2.2 Child-Sum Method", "text": "Our second combination method is inspired by the Child-Sum Tree-LSTMs of Tai et al. (2015). Here we use an LSTM variant to combine the two hidden states and cells. Specifically: i = sigmoid (W i1h1 + W i 2h2) (3) f = sigmoid (W fi hi) (4) o = sigmoid (W o1h1 + W o 2h2) (5) u = tanh (W u1 h1 + W u 2 h2) (6) c = if uf + f1 c1 + f2 c2 (7) h = of tanh (cf) (8) This method uses eight new matrices (the W's in the above equations), with each 1000 OS symbol representing a 1000 Y equation."}, {"heading": "2.3 Multi-Source Attention", "text": "Our single attention model is modeled from the local p-attention model using the feed input from Luong et al. (2015b), with hidden states from the top decoder layer looking back at the top hidden states from the encoder. The hidden state of the top decoder is combined with a weighted sum of the hidden states of the encoder layer to create a better hidden state vector (h-t) that is passed on to the softmax output layer. By input feeding, the hidden state from the attention model is sent to the bottom decoder layer at the next step. The local p-attention model from Luong et al. (2015b) works as follows: A position that can be viewed in the source code is predicted by Equation 9: pt = S \u00b7 sigmoid (vTp tanh) (vvt) (9) is the source set, where p and vp are the source set and 1000."}, {"heading": "3 Experiments", "text": "We use English, French and German data from a subset of the WMT 2014 data set (Bojar et al., 2014). Figure 3 shows statistics for our training. For the development we use the 3000 sets supplied by WMT. For the exam we use a 1503-line trilingual subset of the WMT testset.For the single-source models we follow the training procedure used in Luong et al. (2015b), but with 15 epochs and halving the learning rate after the 10th epoch. We also rescale the normalized slope when the standard is > 5. For the training we use a minibatch size of 128, a hidden state size of 1000, and dropout as in Zaremba et al. (2014) The dropout rate rate is 0.2, the initial parameter range is [-0.1, + 0.1], and the learning rate is 1.0."}, {"heading": "4 Conclusion", "text": "We describe a neural multi-source MT system that gains up to + 4.8 bleu from a source via a very strong attention-based baseline. We achieve this result through a novel encoder-vector combination method and a novel multi-attention system. We publish the code for these experiments at https: / / github.com / isi-nlp / Zoph _ RNN."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio."], "venue": "Proc. ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Co-training for statistical machine translation", "author": ["C. Callison-Burch."], "venue": "Master\u2019s thesis, School of Informatics, University of Edinburgh.", "citeRegEx": "Callison.Burch.,? 2002", "shortCiteRegEx": "Callison.Burch.", "year": 2002}, {"title": "A connectionist approach to machine translation", "author": ["M.A. Casta\u00f1o", "F. Casacuberta."], "venue": "EUROSPEECH.", "citeRegEx": "Casta\u00f1o and Casacuberta.,? 1997", "shortCiteRegEx": "Casta\u00f1o and Casacuberta.", "year": 1997}, {"title": "How to make a Frenemy: Multitape FSTs for portmanteau generation", "author": ["A. Deri", "K. Knight."], "venue": "Proc. NAACL.", "citeRegEx": "Deri and Knight.,? 2015", "shortCiteRegEx": "Deri and Knight.", "year": 2015}, {"title": "Multi-task learning for multiple language translation", "author": ["D. Dong", "H. Wu", "W. he", "D. Yu", "H. Wang."], "venue": "Proc. ACL.", "citeRegEx": "Dong et al\\.,? 2015", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "On relations defined by generalized finite automata", "author": ["C. Elgot", "J. Mezei."], "venue": "IBM Journal of Research and Development, 9(1):47\u201368.", "citeRegEx": "Elgot and Mezei.,? 1965", "shortCiteRegEx": "Elgot and Mezei.", "year": 1965}, {"title": "A program for aligning sentences in bilingual corpora", "author": ["W. A Gale", "K. W Church."], "venue": "Computational linguistics, 19(1):75\u2013102.", "citeRegEx": "Gale and Church.,? 1993", "shortCiteRegEx": "Gale and Church.", "year": 1993}, {"title": "On the use of median string for multi-source translation", "author": ["J. Gonz\u00e1lez-Rubio", "F. Casacuberta."], "venue": "Proc. ICPR.", "citeRegEx": "Gonz\u00e1lez.Rubio and Casacuberta.,? 2010", "shortCiteRegEx": "Gonz\u00e1lez.Rubio and Casacuberta.", "year": 2010}, {"title": "Long shortterm memory", "author": ["S. Hochreiter", "J. Schmidhuber."], "venue": "Neural Computation, 9(8).", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Regular models of phonological rule systems", "author": ["R. Kaplan", "M. Kay."], "venue": "Computational Linguistics, 20(3):331\u2013378.", "citeRegEx": "Kaplan and Kay.,? 1994", "shortCiteRegEx": "Kaplan and Kay.", "year": 1994}, {"title": "Triangulation in translation", "author": ["M. Kay."], "venue": "Keynote at MT 2000 Conference, University of Exeter.", "citeRegEx": "Kay.,? 2000", "shortCiteRegEx": "Kay.", "year": 2000}, {"title": "Multi-task sequence to sequence learning", "author": ["M. Luong", "Q.V. Le", "I. Sutskever", "O. Vinyals", "L. Kaiser."], "venue": "arXiv. http://arxiv.org/abs/1511.06114.", "citeRegEx": "Luong et al\\.,? 2015a", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["M. Luong", "H. Pham", "C. Manning."], "venue": "Proc. EMNLP.", "citeRegEx": "Luong et al\\.,? 2015b", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment", "author": ["E. Matusov", "N. Ueffing", "H. Ney."], "venue": "Proc. EACL.", "citeRegEx": "Matusov et al\\.,? 2006", "shortCiteRegEx": "Matusov et al\\.", "year": 2006}, {"title": "Contrastive lexical evaluation of machine translation", "author": ["A. Max", "J. Crego", "F. Yvon."], "venue": "Proc. LREC.", "citeRegEx": "Max et al\\.,? 2010", "shortCiteRegEx": "Max et al\\.", "year": 2010}, {"title": "Asynchronous translations with recurrent neural nets", "author": ["R. Neco", "M. Forcada."], "venue": "International Conf. on Neural Networks, volume 4, pages 2535\u2013 2540.", "citeRegEx": "Neco and Forcada.,? 1997", "shortCiteRegEx": "Neco and Forcada.", "year": 1997}, {"title": "Statistical multi-source translation", "author": ["F.J. Och", "H. Ney."], "venue": "Proc. MT Summit.", "citeRegEx": "Och and Ney.,? 2001", "shortCiteRegEx": "Och and Ney.", "year": 2001}, {"title": "Word lattices for multi-source translation", "author": ["J. Schroeder", "T. Cohn", "P. Koehn."], "venue": "Proc. EACL.", "citeRegEx": "Schroeder et al\\.,? 2009", "shortCiteRegEx": "Schroeder et al\\.", "year": 2009}, {"title": "Multi-source translation methods", "author": ["L. Schwartz."], "venue": "Proc. AMTA.", "citeRegEx": "Schwartz.,? 2008", "shortCiteRegEx": "Schwartz.", "year": 2008}, {"title": "Text-translation alignment: Three languages are better than two", "author": ["M. Simard."], "venue": "Proc. EMNLP/VLC.", "citeRegEx": "Simard.,? 1999", "shortCiteRegEx": "Simard.", "year": 1999}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le."], "venue": "Proc. NIPS.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["K.S. Tai", "R. Socher", "C. Manning."], "venue": "Proc. ACL.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["P.J. Werbos."], "venue": "Proceedings of the IEEE, 78(10):1550\u20131560.", "citeRegEx": "Werbos.,? 1990", "shortCiteRegEx": "Werbos.", "year": 1990}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals."], "venue": "CoRR, abs/1409.2329.", "citeRegEx": "Zaremba et al\\.,? 2014", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 13, "context": "These methods include confusion networks (Matusov et al., 2006; Schroeder et al., 2009), source-side string combination (Schroeder et al.", "startOffset": 41, "endOffset": 87}, {"referenceID": 17, "context": "These methods include confusion networks (Matusov et al., 2006; Schroeder et al., 2009), source-side string combination (Schroeder et al.", "startOffset": 41, "endOffset": 87}, {"referenceID": 17, "context": ", 2009), source-side string combination (Schroeder et al., 2009), and median strings (Gonz\u00e1lez-Rubio and Casacuberta, 2010).", "startOffset": 40, "endOffset": 64}, {"referenceID": 7, "context": ", 2009), and median strings (Gonz\u00e1lez-Rubio and Casacuberta, 2010).", "startOffset": 28, "endOffset": 66}, {"referenceID": 6, "context": "This follows early work in sentence alignment (Gale and Church, 1993) and word alignment (Simard, 1999), which exploited trilingual text, but did not build trilingual models.", "startOffset": 46, "endOffset": 69}, {"referenceID": 19, "context": "This follows early work in sentence alignment (Gale and Church, 1993) and word alignment (Simard, 1999), which exploited trilingual text, but did not build trilingual models.", "startOffset": 89, "endOffset": 103}, {"referenceID": 5, "context": "We view this as a kind of multi-tape transduction (Elgot and Mezei, 1965; Kaplan and Kay, 1994; Deri and Knight, 2015) with two input tapes and one output tape.", "startOffset": 50, "endOffset": 118}, {"referenceID": 9, "context": "We view this as a kind of multi-tape transduction (Elgot and Mezei, 1965; Kaplan and Kay, 1994; Deri and Knight, 2015) with two input tapes and one output tape.", "startOffset": 50, "endOffset": 118}, {"referenceID": 3, "context": "We view this as a kind of multi-tape transduction (Elgot and Mezei, 1965; Kaplan and Kay, 1994; Deri and Knight, 2015) with two input tapes and one output tape.", "startOffset": 50, "endOffset": 118}, {"referenceID": 4, "context": "We are able to achieve these results using the framework of neural encoder-decoder models, where multi-target MT (Dong et al., 2015) and multi-source, cross-modal mappings have been explored (Luong et al.", "startOffset": 113, "endOffset": 132}, {"referenceID": 11, "context": ", 2015) and multi-source, cross-modal mappings have been explored (Luong et al., 2015a).", "startOffset": 66, "endOffset": 87}, {"referenceID": 1, "context": "Callison-Burch (2002) exploits hypothesis selection in multi-source MT to expand available corpora, via co-training.", "startOffset": 0, "endOffset": 22}, {"referenceID": 15, "context": "In the neural encoder-decoder framework for MT (Neco and Forcada, 1997; Casta\u00f1o and Casacuberta, 1997; Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015b), we use a recurar X iv :1 60 1.", "startOffset": 47, "endOffset": 170}, {"referenceID": 2, "context": "In the neural encoder-decoder framework for MT (Neco and Forcada, 1997; Casta\u00f1o and Casacuberta, 1997; Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015b), we use a recurar X iv :1 60 1.", "startOffset": 47, "endOffset": 170}, {"referenceID": 20, "context": "In the neural encoder-decoder framework for MT (Neco and Forcada, 1997; Casta\u00f1o and Casacuberta, 1997; Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015b), we use a recurar X iv :1 60 1.", "startOffset": 47, "endOffset": 170}, {"referenceID": 0, "context": "In the neural encoder-decoder framework for MT (Neco and Forcada, 1997; Casta\u00f1o and Casacuberta, 1997; Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015b), we use a recurar X iv :1 60 1.", "startOffset": 47, "endOffset": 170}, {"referenceID": 12, "context": "In the neural encoder-decoder framework for MT (Neco and Forcada, 1997; Casta\u00f1o and Casacuberta, 1997; Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015b), we use a recurar X iv :1 60 1.", "startOffset": 47, "endOffset": 170}, {"referenceID": 20, "context": "Figure 1: The encoder-decoder framework for neural machine translation (NMT) (Sutskever et al., 2014).", "startOffset": 77, "endOffset": 101}, {"referenceID": 8, "context": "1 In this paper, we use a four-layer encoderdecoder system (Figure 1) with long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997) trained for maximum likelihood (via a softmax layer) with back-propagation through time (Werbos, 1990).", "startOffset": 111, "endOffset": 145}, {"referenceID": 22, "context": "1 In this paper, we use a four-layer encoderdecoder system (Figure 1) with long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997) trained for maximum likelihood (via a softmax layer) with back-propagation through time (Werbos, 1990).", "startOffset": 234, "endOffset": 248}, {"referenceID": 8, "context": "1 In this paper, we use a four-layer encoderdecoder system (Figure 1) with long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997) trained for maximum likelihood (via a softmax layer) with back-propagation through time (Werbos, 1990). For our baseline singlesource MT system we use two different models, one of which implements the local attention plus feed-input model from Luong et al. (2015b). Figure 2 shows our approach to multi-source MT.", "startOffset": 112, "endOffset": 411}, {"referenceID": 21, "context": "Our second combination method is inspired by the Child-Sum Tree-LSTMs of Tai et al. (2015). Here, we use an LSTM variant to combine the two hidden states and cells.", "startOffset": 73, "endOffset": 91}, {"referenceID": 11, "context": "Our single-source attention model is modeled off the local-p attention model with feed input from Luong et al. (2015b), where hidden states from the top decoder layer can look back at the top hidden states from the encoder.", "startOffset": 98, "endOffset": 119}, {"referenceID": 11, "context": "Our single-source attention model is modeled off the local-p attention model with feed input from Luong et al. (2015b), where hidden states from the top decoder layer can look back at the top hidden states from the encoder. The top decoder hidden state is combined with a weighted sum of the encoder hidden states, to make a better hidden state vector (h\u0303t), which is passed to the softmax output layer. With input-feeding, the hidden state from the attention model is sent down to the bottom decoder layer at the next time step. The local-p attention model from Luong et al. (2015b) works as follows.", "startOffset": 98, "endOffset": 584}, {"referenceID": 11, "context": "For the single-source models, we follow the training procedure used in Luong et al. (2015b), but with 15 epochs and halving the learning rate every full epoch after the 10th epoch.", "startOffset": 71, "endOffset": 92}, {"referenceID": 11, "context": "For the single-source models, we follow the training procedure used in Luong et al. (2015b), but with 15 epochs and halving the learning rate every full epoch after the 10th epoch. We also rescale the normalized gradient when norm> 5. For training, we use a minibatch size of 128, a hidden state size of 1000, and dropout as in Zaremba et al. (2014). The dropout rate is 0.", "startOffset": 71, "endOffset": 350}], "year": 2016, "abstractText": "We build a multi-source machine translation model and train it to maximize the probability of a target English string given French and German sources. Using the neural encoder-decoder framework, we explore several combination methods and report up to +4.8 Bleu increases on top of a very strong attention-based neural translation model.", "creator": "TeX"}}}