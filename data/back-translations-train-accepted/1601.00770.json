{"id": "1601.00770", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jan-2016", "title": "End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures", "abstract": "We present a novel end-to-end neural model to extract entities and relations between them. Our recurrent neural network based model stacks bidirectional sequential LSTM-RNNs and bidirectional tree-structured LSTM-RNNs to capture both word sequence and dependency tree substructure information. This allows our model to jointly represent both entities and relations with shared parameters. We further encourage detection of entities during training and use of entity information in relation extraction via curriculum learning and scheduled sampling. Our model improves over the state-of-the-art feature-based model on end-to-end relation extraction, achieving 3.5% and 4.8% relative error reductions in F-score on ACE2004 and ACE2005, respectively. We also show improvements over the state-of-the-art convolutional neural network based model on nominal relation classification (SemEval-2010 Task 8), with 2.5% relative error reduction in F-score.", "histories": [["v1", "Tue, 5 Jan 2016 08:53:05 GMT  (91kb,D)", "http://arxiv.org/abs/1601.00770v1", "10 pages, 1 figure, 5 tables"], ["v2", "Sat, 19 Mar 2016 02:23:01 GMT  (94kb,D)", "http://arxiv.org/abs/1601.00770v2", "12 pages, 1 figure, 5 tables"], ["v3", "Wed, 8 Jun 2016 01:08:08 GMT  (94kb,D)", "http://arxiv.org/abs/1601.00770v3", "Accepted for publication at the Association for Computational Linguistics (ACL), 2016. 13 pages, 1 figure, 6 tables"]], "COMMENTS": "10 pages, 1 figure, 5 tables", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["makoto miwa", "mohit bansal"], "accepted": true, "id": "1601.00770"}, "pdf": {"name": "1601.00770.pdf", "metadata": {"source": "CRF", "title": "End-to-end Relation Extraction using LSTMs on Sequences and Tree Structures", "authors": ["Makoto Miwa", "Mohit Bansal"], "emails": ["makoto-miwa@toyota-ti.ac.jp", "mbansal@ttic.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people who work for the rights of women and men are not in a position to exercise their rights and obligations, but rather to exercise and exercise their rights and obligations themselves."}, {"heading": "2 Related Work", "text": "LSTM-RNNs have been widely used for sequential labeling, such as clause identification (Hammerton, 2001), phonetic labeling (Graves and Schmidhuber, 2005), and NER (Hammerton, 2003). Recently, Huang et al. (2015) showed that building a conditional random field (CRF) layer on top of the bidirectional LSTM-RNNs structural approaches (Zelenko et al., 2003; Bunescu and Mooney, 2005), several neural models were proposed in SemEval2010 Task 8 (Hendrickx et al., 2010), including embedding-based models (Hashimoto et al., 2015), CNN-based models."}, {"heading": "3 Model", "text": "We design our model with LSTM RNNs, which represent both word sequences and dependency tree structures, and perform end-to-end extraction of relations between entities on these RNNs. Fig. 1 illustrates the overview of the model. The model is essentially made up of three representation layers: a word embedding layer, a word sequence-based LSTM RNN layer, and finally a dependency subtree-based LSTM RNN layer."}, {"heading": "3.1 Embedding Layer", "text": "At the embedding level, word embedding representations are manageed. nw, np, nd and one-dimensional vectors v (w), v (p), v (d) and v (e) are embedded in words, POS tags, dependency types and entity labels."}, {"heading": "3.2 Sequence Layer", "text": "The sequence layer represents words in linear order using representations from the embedding layer. This layer represents sentential context information and obtains entities as shown in the lower left part of the figure. 1.We use bidirectional LSTM RNs (Zaremba and Sutskever, 2014) to represent the word sequence in a sentence. The LSTM unit on the tenth word consists of a collection of d-dimensional vectors: an input gate, a forget gate ft, an output gate ot, a memory cell and a hidden state ht. The unit receives an n-dimensional input vector xt, the previous hidden state ht \u2212 1 and the memory cell ct \u2212 1 and computes the new vectors using the following equations: it = circular shape (W (i) xt + U (t) ht + U (t) ht = unit, h (v) t = unit (t \u2212 h), W = t \u2212 h (W), W = \u2212 t \u2212 h \u2212 t \u2212 t \u2212 h, W = \u2212 t \u2212 t \u2212 h \u2212 t \u2212 t \u2212 t \u2212 h."}, {"heading": "3.3 Entity Detection", "text": "We assign an entity tag to each word, with each entity tag representing the entity type and the position of a word within the entity type. For example, in Fig. 1, we assign B-PER and L-PER to each word in Sidney Yates (each representing the first and last words of a person entity type) to make this phrase a PER (person) entity type. We implement the entity detection on the top of the sequence layer. We use a two-layer NN with a three-dimensional hidden layer h (e) and a soft maxoutput layer for entity detection. h (e) t = tanh (W (eh) [st; v (e) t \u2212 1] + b (eh)."}, {"heading": "3.4 Dependency Layer", "text": "The dependency layer represents a relationship between two target words in the dependency structure and is responsible for the relationship between the two target structures as represented in the top right-hand layer of the figure. This layer focuses mainly on the shortest path between two target words in the dependency structure (i.e., the path between the smallest common node and the two target words), as these paths are effective in relation to their relationship (Xu et al., 2015a). For example, we show the shortest path between Yates and Chicago in the bottom of Figure 1, and this path also captures the key phrase of their relationship, i.e. we employ bidirectional tree-structured LSTMRNs (i.e., bottom-up and top-down) to represent a relationship between the dependency structures by capturing the target word pairs around the target pairs. This bidirectional structure propagates not only the information from the sheets, but also information from the root."}, {"heading": "3.5 Stacking Sequence and Dependency Layers", "text": "We stack the dependency layer on the sequence layer to include both word sequence and dependency tree information in the output; the dependency layer LSTM unit on the t-thword receives xt = [st; v (d) t; v (e) t], i.e. the concatenation of its corresponding hidden state vectors st in the sequence layer, the dependence type embedding v (d) t (denotes the type of dependency on the parent3) and the label embedding v (e) t (corresponds to the predicted entity designation); next, the output reference vector, which is passed on to the subsequent relation classification Softmax layer, is constructed as the concatenation dp = [\u2191 hpA; \u2193 hp1; \u2193 hp2], where \u2191 hpA is the hidden state vector of the uppermost LSTM unit in the second lower STM pair, where the STP is the smallest STP."}, {"heading": "3.6 Relation Classification", "text": "We build relation candidates using the last words of the entities, i.e. words with L or U names in the BILOU scheme. For example, we build a relation candidate using Yates with an L-PER label and Chicago with a U-LOC label. For each relation candidate, we get the output of the dependency tree layer dp (as described above), which corresponds to the path between the word pair p in the candidate, and forecast its relation label. 5 Similar to entity capture, we use a two-layer NN with an hr-dimensional hidden layer h (r) and a softmax output layer (with weight matrices W, bias vectors b).h (r) p = tanh (W (rh) dp + b (rh).yp = softmax (W (ry) h (r) t + b (ry).ecels)."}, {"heading": "3.7 Training", "text": "We update the model parameters including weights, distortions and embedding by back propagation over time (BPTT) and Adam (Kingma and Ba, 2015) with gradient clipping, parameter determination and L2 regularization (we regulate weights W and U, not bias terms b). We also apply suspensions (Srivastava et al., 2014) to the embedding layer and to the last hidden layers for the detection and classification of entities. As far as I am concerned, we choose the inverse sigmoid decay rate i = k / (k + exp (i / k) as a prediction of the probability of i, which depends on the number of epochs i during the training, if the gold labels are legal. As far as I am concerned, we choose the inverse sigmoid decay rate i = k / (k + exp (i / k), whereby benchmark \u2265 1 is a correlation between the training and the fit."}, {"heading": "4 Results and Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Data and Task Settings", "text": "We evaluate an entity using three sets of data: ACE05 and ACE04 for end-to-end relation extraction and SemEval2010 Task 8 for relation classification. We use the first two sets as our primary goal and use the last one to thoroughly analyze and abbreviate the portion of the relation classification of our model. 6Note that we do not display this pair in Fig.1 for simplicity reasons. ACE05 defines 7 coarse-grained entity types 7 and 6 coarse-grained relationship types between entities.8 We use the same data splits and pre-processing as Li and Ji (2014).9 We report on micro-precision, recall, and F-scores for both entity types and relationship extractions to better explain model performance. We treat an entity as correct if its type and region of its head are correct, and we treat a relationship as correct if its type and its arguments are correct."}, {"heading": "4.2 Experimental Settings", "text": "We implemented our model with the cnn library.13 We analyzed the texts with the Stanford Neural Dependency Parser (Chen and Manning, 2014) with the original Stanford Dependencies. Based on prelim-7Facility (FAC), Geo-Political Entities (GPE), Location (LOC), Organization (ORG), Person (PER), Vehicle (VEH), and Weapon (WEA).8Artifact (ART), Gen-Affiliation (GEN-AFF), OrgAffiliation (ORG-AFF), Part-Whole (PART-WHOLE), PersonSocial (PER-SOC), and Physical (PHYS).9We removed the cts, un subsets, and used a 351 / 80 / dev / test. We removed duplicated entities and relationships, and solved nested entities."}, {"heading": "4.3 End-to-end Relation Extraction Results", "text": "Table 1 compares our model to the state-of-the-art function-based model of Li and Ji (2014) on final test kits and shows that our model performs better than the state-of-the-art model. To analyze the contributions and effects of the various components of our end-to-end relationship extraction model, we perform ablation tests on the ACE05 development set (Table 2). Performance deteriorates slightly without curriculum learning or scheduled sampling, and performance deteriorates significantly if we remove both components (p < 0.05). This is reasonable because the model can only perform relationship tests when both entities are found, and without these improvements it may be too late to find some correlations. Removal of label embedding does not affect entity detection performance, but this impairs extraction memory."}, {"heading": "4.4 Relation Classification Analysis Results", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "5 Conclusion", "text": "We presented an end-to-end relation extraction model using bidirectional sequential and bidirectional tree-structured LSTM RNNNs to represent both word sequences and dependency tree structures, which enabled us to represent both entities and relationships in a single model, giving us advantages over the modern, function-based end-to-end relationship extraction system (ACE04 and ACE05) and newer CNN-based models based on nominal relation classification (SemEval-2010 task 8). Our evaluation and ablation led to three key results: firstly, the use of both word sequences and dependency tree structures is effective; secondly, training with common parameters improves the accuracy of relation extraction, especially when applied with curriculum learning, scheduled sampling and label embedding. Finally, the shortest route to modelling of the relationship structure is also widely used in STM."}, {"heading": "Acknowledgments", "text": "We thank Qi Li and Kevin Gimpel for details of the records and helpful discussions."}], "references": [{"title": "Curriculum learning", "author": ["Bengio et al.2009] Yoshua Bengio", "J\u00e9r\u00f4me Louradour", "Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Bengio et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks. arXiv preprint arXiv:1506.03099", "author": ["Bengio et al.2015] Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "A shortest path dependency kernel for relation extraction", "author": ["Bunescu", "Mooney2005] Razvan C Bunescu", "Raymond Mooney"], "venue": "In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,", "citeRegEx": "Bunescu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bunescu et al\\.", "year": 2005}, {"title": "Exploiting syntactico-semantic structures for relation extraction", "author": ["Chan", "Roth2011] Yee Seng Chan", "Dan Roth"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Chan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2011}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "The automatic content extraction (ace) program \u2013 tasks, data, and evaluation", "author": ["Alexis Mitchell", "Mark Przybocki", "Lance Ramshaw", "Stephanie Strassel", "Ralph Weischedel"], "venue": "In Proceedings of the Fourth International", "citeRegEx": "Doddington et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Doddington et al\\.", "year": 2004}, {"title": "Classifying relations by ranking with convolutional neural networks", "author": ["Bing Xiang", "Bowen Zhou"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Inter-", "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Graves", "Schmidhuber2005] Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "Clause identification with long short-term memory", "author": ["James Hammerton"], "venue": "In Proceedings of the 2001 workshop on Computational Natural Language Learning-Volume", "citeRegEx": "Hammerton.,? \\Q2001\\E", "shortCiteRegEx": "Hammerton.", "year": 2001}, {"title": "Named entity recognition with long short-term memory", "author": ["James Hammerton"], "venue": "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL", "citeRegEx": "Hammerton.,? \\Q2003\\E", "shortCiteRegEx": "Hammerton.", "year": 2003}, {"title": "Task-oriented learning of word embeddings for semantic relation classification", "author": ["Pontus Stenetorp", "Makoto Miwa", "Yoshimasa Tsuruoka"], "venue": "In Proceedings of the Nineteenth Conference on Computational Natural", "citeRegEx": "Hashimoto et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2015}, {"title": "Semeval-2010 task 8: Multi-way classification of semantic relations", "author": ["Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid \u00d3 S\u00e9aghdha", "Sebastian Pad\u00f3", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz"], "venue": null, "citeRegEx": "Hendrickx et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hendrickx et al\\.", "year": 2010}, {"title": "Bidirectional lstm-crf models for sequence tagging", "author": ["Huang et al.2015] Zhiheng Huang", "Wei Xu", "Kai Yu"], "venue": "arXiv preprint arXiv:1508.01991", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Joint entity and relation extraction using card-pyramid parsing", "author": ["Kate", "Mooney2010] Rohit J. Kate", "Raymond Mooney"], "venue": "In Proceedings of the Fourteenth Conference on Computational Natural Language Learning,", "citeRegEx": "Kate et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kate et al\\.", "year": 2010}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Ba2015] Diederik Kingma", "Jimmy Ba"], "venue": "ICLR", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Incremental joint extraction of entity mentions and relations", "author": ["Li", "Ji2014] Qi Li", "Heng Ji"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "When are tree structures necessary for deep learning of representations", "author": ["Li et al.2015] Jiwei Li", "Thang Luong", "Dan Jurafsky", "Eduard Hovy"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Modeling joint entity and relation extraction with table representation", "author": ["Miwa", "Sasaki2014] Makoto Miwa", "Yutaka Sasaki"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Miwa et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Miwa et al\\.", "year": 2014}, {"title": "A survey of named entity recognition and classification", "author": ["Nadeau", "Sekine2007] David Nadeau", "Satoshi Sekine"], "venue": "Lingvisticae Investigationes,", "citeRegEx": "Nadeau et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Nadeau et al\\.", "year": 2007}, {"title": "ComputerIntensive Methods for Testing Hypotheses : An Introduction", "author": ["Eric W. Noreen"], "venue": null, "citeRegEx": "Noreen.,? \\Q1989\\E", "shortCiteRegEx": "Noreen.", "year": 1989}, {"title": "Global belief recursive neural networks", "author": ["Paulus et al.2014] Romain Paulus", "Richard Socher", "Christopher D Manning"], "venue": null, "citeRegEx": "Paulus et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Paulus et al\\.", "year": 2014}, {"title": "Design challenges and misconceptions in named entity recognition", "author": ["Ratinov", "Roth2009] Lev Ratinov", "Dan Roth"], "venue": "In Proceedings of the Thirteenth Conference on Computational Natural Language Learning", "citeRegEx": "Ratinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ratinov et al\\.", "year": 2009}, {"title": "Global Inference for Entity and Relation Identification via a Linear Programming Formulation", "author": ["Roth", "Yih2007] Dan Roth", "Wen-Tau Yih"], "venue": null, "citeRegEx": "Roth et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Roth et al\\.", "year": 2007}, {"title": "Joint inference of entities, relations, and coreference", "author": ["Singh et al.2013] Sameer Singh", "Sebastian Riedel", "Brian Martin", "Jiaping Zheng", "Andrew McCallum"], "venue": "In Proceedings of the 2013 workshop on Automated knowledge base construction,", "citeRegEx": "Singh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2013}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Brody Huval", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Tai et al.2015] Kai Sheng Tai", "Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Semantic relation classification via convolutional neural networks with simple negative sampling", "author": ["Xu et al.2015a] Kun Xu", "Yansong Feng", "Songfang Huang", "Dongyan Zhao"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Classifying relations via long short term memory networks along shortest dependency paths", "author": ["Xu et al.2015b] Yan Xu", "Lili Mou", "Ge Li", "Yunchuan Chen", "Hao Peng", "Zhi Jin"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Joint inference for fine-grained opinion extraction", "author": ["Yang", "Cardie2013] Bishan Yang", "Claire Cardie"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "Yang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2013}, {"title": "Jointly identifying entities and extracting relations in encyclopedia text via a graphical model approach", "author": ["Yu", "Lam2010] Xiaofeng Yu", "Wai Lam"], "venue": "In Coling 2010: Posters,", "citeRegEx": "Yu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2010}, {"title": "Learning to execute. arXiv preprint arXiv:1410.4615", "author": ["Zaremba", "Sutskever2014] Wojciech Zaremba", "Ilya Sutskever"], "venue": null, "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Kernel methods for relation extraction", "author": ["Chinatsu Aone", "Anthony Richardella"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Zelenko et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zelenko et al\\.", "year": 2003}, {"title": "Exploring various knowledge in relation extraction", "author": ["Zhou et al.2005] GuoDong Zhou", "Jian Su", "Jie Zhang", "Min Zhang"], "venue": "In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Zhou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 33, "context": ", named entity recognition (NER) (Nadeau and Sekine, 2007; Ratinov and Roth, 2009) and relation extraction (Zelenko et al., 2003; Zhou et al., 2005), but recent studies show that end-to-end (joint) modeling of entity and relation is important for high performance (Li and Ji, 2014; Miwa and Sasaki, 2014) since relations interact closely with entity information.", "startOffset": 107, "endOffset": 148}, {"referenceID": 34, "context": ", named entity recognition (NER) (Nadeau and Sekine, 2007; Ratinov and Roth, 2009) and relation extraction (Zelenko et al., 2003; Zhou et al., 2005), but recent studies show that end-to-end (joint) modeling of entity and relation is important for high performance (Li and Ji, 2014; Miwa and Sasaki, 2014) since relations interact closely with entity information.", "startOffset": 107, "endOffset": 148}, {"referenceID": 8, "context": ", word sequences (Hammerton, 2001) and constituent/dependency trees (Tai et al.", "startOffset": 17, "endOffset": 34}, {"referenceID": 27, "context": ", word sequences (Hammerton, 2001) and constituent/dependency trees (Tai et al., 2015).", "startOffset": 68, "endOffset": 86}, {"referenceID": 16, "context": "Despite this representation ability, for relation classification tasks, the previously reported performance using long short-term memory (LSTM) based RNNs (Xu et al., 2015b; Li et al., 2015) is worse than one using CNNs (dos Santos et al.", "startOffset": 155, "endOffset": 190}, {"referenceID": 34, "context": "Many traditional, feature-based relation classification models extract features from both sequences and parse trees (Zhou et al., 2005).", "startOffset": 116, "endOffset": 135}, {"referenceID": 25, "context": "However, previous RNN-based models focus on only one of these linguistic structures (Socher et al., 2012).", "startOffset": 84, "endOffset": 105}, {"referenceID": 0, "context": "Our model also incorporates curriculum learning (Bengio et al., 2009) and scheduled sampling (Bengio et al.", "startOffset": 48, "endOffset": 69}, {"referenceID": 1, "context": ", 2009) and scheduled sampling (Bengio et al., 2015) to alleviate the problem of lowperformance entity detection in early stages of training, as well as to allow entity information to further help downstream relation extraction.", "startOffset": 31, "endOffset": 52}, {"referenceID": 8, "context": "LSTM-RNNs have been widely used for sequential labeling, such as clause identification (Hammerton, 2001), phonetic labeling (Graves and Schmidhuber, 2005), and NER (Hammerton, 2003).", "startOffset": 87, "endOffset": 104}, {"referenceID": 9, "context": "LSTM-RNNs have been widely used for sequential labeling, such as clause identification (Hammerton, 2001), phonetic labeling (Graves and Schmidhuber, 2005), and NER (Hammerton, 2003).", "startOffset": 164, "endOffset": 181}, {"referenceID": 8, "context": "LSTM-RNNs have been widely used for sequential labeling, such as clause identification (Hammerton, 2001), phonetic labeling (Graves and Schmidhuber, 2005), and NER (Hammerton, 2003). Recently, Huang et al. (2015) showed that building a conditional random field (CRF) layer on the top of bidirectional LSTM-RNNs performs comparably to the state-of-the-art methods in the part-of-speech (POS) tagging, chunking, and NER.", "startOffset": 88, "endOffset": 213}, {"referenceID": 33, "context": "For relation classification, in addition to traditional feature/kernel-based approaches (Zelenko et al., 2003; Bunescu and Mooney, 2005), several neural models have been proposed in the SemEval2010 Task 8 (Hendrickx et al.", "startOffset": 88, "endOffset": 136}, {"referenceID": 11, "context": ", 2003; Bunescu and Mooney, 2005), several neural models have been proposed in the SemEval2010 Task 8 (Hendrickx et al., 2010), including embedding-based models (Hashimoto et al.", "startOffset": 102, "endOffset": 126}, {"referenceID": 10, "context": ", 2010), including embedding-based models (Hashimoto et al., 2015), CNN-based models (dos Santos et al.", "startOffset": 42, "endOffset": 66}, {"referenceID": 25, "context": ", 2015), and RNN-based models (Socher et al., 2012).", "startOffset": 30, "endOffset": 51}, {"referenceID": 6, "context": ", 2015), CNN-based models (dos Santos et al., 2015), and RNN-based models (Socher et al., 2012). Recently, Xu et al. (2015a) and Xu et al.", "startOffset": 31, "endOffset": 125}, {"referenceID": 6, "context": ", 2015), CNN-based models (dos Santos et al., 2015), and RNN-based models (Socher et al., 2012). Recently, Xu et al. (2015a) and Xu et al. (2015b) showed that the shortest dependency paths between relation arguments, which were used in feature/kernelbased systems (Bunescu and Mooney, 2005), are also useful in NN-based models.", "startOffset": 31, "endOffset": 147}, {"referenceID": 6, "context": ", 2015), CNN-based models (dos Santos et al., 2015), and RNN-based models (Socher et al., 2012). Recently, Xu et al. (2015a) and Xu et al. (2015b) showed that the shortest dependency paths between relation arguments, which were used in feature/kernelbased systems (Bunescu and Mooney, 2005), are also useful in NN-based models. Xu et al. (2015b) also showed that LSTM-RNNs are useful for relation classification, but the performance was worse than CNN-based models.", "startOffset": 31, "endOffset": 346}, {"referenceID": 6, "context": ", 2015), CNN-based models (dos Santos et al., 2015), and RNN-based models (Socher et al., 2012). Recently, Xu et al. (2015a) and Xu et al. (2015b) showed that the shortest dependency paths between relation arguments, which were used in feature/kernelbased systems (Bunescu and Mooney, 2005), are also useful in NN-based models. Xu et al. (2015b) also showed that LSTM-RNNs are useful for relation classification, but the performance was worse than CNN-based models. Li et al. (2015) compared separate sequence-based and tree-structured LSTMRNNs on relation classification, using basic RNN model structures.", "startOffset": 31, "endOffset": 483}, {"referenceID": 27, "context": "Research on tree-structured LSTM-RNNs (Tai et al., 2015) fixes the direction of information propagation from bottom to top, and also cannot handle an arbitrary number of typed children as in a typed dependency tree.", "startOffset": 38, "endOffset": 56}, {"referenceID": 24, "context": "Such models include structured prediction (Li and Ji, 2014; Miwa and Sasaki, 2014), integer linear programming (Roth and Yih, 2007; Yang and Cardie, 2013), card-pyramid parsing (Kate and Mooney, 2010), and global probabilistic graphical models (Yu and Lam, 2010; Singh et al., 2013).", "startOffset": 244, "endOffset": 282}, {"referenceID": 27, "context": "2 Note that the two variants of tree-structured LSTM-RNNs by Tai et al. (2015) are not able to represent our target structures which have a variable number of typed children: the Child-Sum Tree-LSTM does not deal with", "startOffset": 61, "endOffset": 79}, {"referenceID": 21, "context": "We also tried to use one LSTM-RNN by connecting the root (Paulus et al., 2014), but preparing two LSTM-RNNs showed slightly better performance in our initial experiments.", "startOffset": 57, "endOffset": 78}, {"referenceID": 28, "context": "When the predicted labels are inconsistent, we select the positive and more confident label, similar to Xu et al. (2015a).", "startOffset": 104, "endOffset": 122}, {"referenceID": 26, "context": "We also apply dropout (Srivastava et al., 2014) to the embedding layer and to the final hidden layers for entity detection and relation classification.", "startOffset": 22, "endOffset": 47}, {"referenceID": 1, "context": "We employ scheduled sampling (Bengio et al., 2015) in entity detection.", "startOffset": 29, "endOffset": 50}, {"referenceID": 0, "context": "We also incorporate curriculum learning (Bengio et al., 2009), where we pretrain the entity detection model using the training data to encourage building positive relation instances from the detected entities in training.", "startOffset": 40, "endOffset": 61}, {"referenceID": 5, "context": "ACE04 defines the same 7 coarse-grained entity types as ACE05 (Doddington et al., 2004), but defines 7 coarse-grained relation types.", "startOffset": 62, "endOffset": 87}, {"referenceID": 11, "context": "SemEval-2010 Task 8 defines 9 relation types between nominals12 and a tenth type Other when two nouns have none of these relations (Hendrickx et al., 2010).", "startOffset": 131, "endOffset": 155}, {"referenceID": 5, "context": "ACE04 defines the same 7 coarse-grained entity types as ACE05 (Doddington et al., 2004), but defines 7 coarse-grained relation types.10 We follow the cross-validation setting of Chan and Roth (2011) and Li and Ji (2014)11, and the preprocessing and evaluation metrics of ACE05.", "startOffset": 63, "endOffset": 199}, {"referenceID": 5, "context": "ACE04 defines the same 7 coarse-grained entity types as ACE05 (Doddington et al., 2004), but defines 7 coarse-grained relation types.10 We follow the cross-validation setting of Chan and Roth (2011) and Li and Ji (2014)11, and the preprocessing and evaluation metrics of ACE05.", "startOffset": 63, "endOffset": 220}, {"referenceID": 17, "context": "We initialized word vectors via word2vec (Mikolov et al., 2013) trained on Wikipedia14 and randomly initialized all other parameters.", "startOffset": 41, "endOffset": 63}, {"referenceID": 20, "context": "15 Our statistical significance results are based on the Approximate Randomization (AR) test (Noreen, 1989).", "startOffset": 93, "endOffset": 107}, {"referenceID": 6, "context": "845 dos Santos et al. (2015) 0.", "startOffset": 8, "endOffset": 29}, {"referenceID": 6, "context": "845 dos Santos et al. (2015) 0.841 Xu et al. (2015a) 0.", "startOffset": 8, "endOffset": 53}, {"referenceID": 27, "context": "We also compare our tree-structured LSTMRNN (SPTree) with the Child-Sum tree-LSTM on the shortest path of Tai et al. (2015). Child-Sum performs worse than our SPTree model, but not with as big of a decrease as above.", "startOffset": 106, "endOffset": 124}, {"referenceID": 28, "context": "SPXu is our adaptation of the shortest path LSTM-RNN proposed by Xu et al. (2015b) to match our sequence-layer based model.", "startOffset": 65, "endOffset": 83}, {"referenceID": 28, "context": "Treating the inverse relation candidate as a negative instance (Negative sampling) also performed comparably to other generation methods in our model (unlike Xu et al. (2015a), which showed a significance improvement over generating only left-to-right candidates).", "startOffset": 158, "endOffset": 176}], "year": 2017, "abstractText": "We present a novel end-to-end neural model to extract entities and relations between them. Our recurrent neural network based model stacks bidirectional sequential LSTM-RNNs and bidirectional tree-structured LSTMRNNs to capture both word sequence and dependency tree substructure information. This allows our model to jointly represent both entities and relations with shared parameters. We further encourage detection of entities during training and use of entity information in relation extraction via curriculum learning and scheduled sampling. Our model improves over the state-of-the-art feature-based model on end-to-end relation extraction, achieving 3.5% and 4.8% relative error reductions in F-score on ACE2004 and ACE2005, respectively. We also show improvements over the state-of-the-art convolutional neural network based model on nominal relation classification (SemEval-2010 Task 8), with 2.5% relative error reduction in F-score.", "creator": "LaTeX with hyperref package"}}}