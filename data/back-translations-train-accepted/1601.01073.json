{"id": "1601.01073", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jan-2016", "title": "Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism", "abstract": "We propose multi-way, multilingual neural machine translation. The proposed approach enables a single neural translation model to translate between multiple languages, with a number of parameters that grows only linearly with the number of languages. This is made possible by having a single attention mechanism that is shared across all language pairs. We train the proposed multi-way, multilingual model on ten language pairs from WMT'15 simultaneously and observe clear performance improvements over models trained on only one language pair. In particular, we observe that the proposed model significantly improves the translation quality of low-resource language pairs.", "histories": [["v1", "Wed, 6 Jan 2016 04:00:50 GMT  (77kb,D)", "http://arxiv.org/abs/1601.01073v1", null]], "reviews": [], "SUBJECTS": "cs.CL stat.ML", "authors": ["orhan firat", "kyunghyun cho", "yoshua bengio"], "accepted": true, "id": "1601.01073"}, "pdf": {"name": "1601.01073.pdf", "metadata": {"source": "CRF", "title": "Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism", "authors": ["Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio"], "emails": ["orhan.firat@ceng.metu.edu.tr"], "sections": [{"heading": "1 Introduction", "text": "This year it is so far that it will only be a matter of time before an agreement is reached."}, {"heading": "2 Background: Attention-based Neural Machine Translation", "text": "This is largely due to the fact that the encoder of this basic approach has to compress an entire set of sources into a single vector. Here, we describe attention-based neural machine translation, which aims to build a single neural network that takes a source sequence as input., xTx) and generalizes a corresponding translation Y = (y1, yTy). Each symbol in both source and target sentences, xt or yt, is an integral index of the symbol in a vocabulary."}, {"heading": "3 Multi-Way, Multilingual Translation", "text": "In this section, we discuss problems and our solutions in extending conventional, attention-based neural machine translation into a multi-page, multilingual model.Problem definition We assume that N > 1 source languages {X1, X2,.., XN} and M > 1 tar-get languages {Y 1, Y 2,..., YM} and the availability of L \u2264 M \u00b7 N bilingual parallel corpus.For each parallel corpus l, we can directly use the log probability function of Equation (7) to specify a pair-specific log probability model Ls (Dl), t (Dl), target languages of the l-th parallel corpus.Then, the goal of multilingual neural corpus translation is to build a model that maximizes the probability from L (1) to L (1)."}, {"heading": "3.1 Existing Approaches", "text": "The authors have expanded the basic encoder decoder network for multitask neural machine translation. As they expand the basic encoder decoder network, their model effectively becomes a series of encoders and decoders, with each of the encoders projecting a source set into a common vector space. The key difference between (Luong et al., 2015a) and our work is that we are expanding the attention-based encoder decoder instead of the basic model. This is an important contribution, since attention-based machine translation has become the de facto standard in neural translation literature (Jean et al., 2014; Jean et al., 2015; Luong et al., 2015b; Sennrich et al., 2015a), as opposed to the basic English translation literature, they are just a coder."}, {"heading": "3.2 Challenges", "text": "As described in the introduction, the basic idea is simple. We assign a separate encoder to each source language and a separate decoder to each target language. The encoder will project a source sentence in its own language into a common language agnostic space from which the decoder will generate a translation in its own language. In this case, as opposed to multiple, a pair of comprehensive neural translation models, the encoder and decoder are shared across multiple pairs, which is mathematically advantageous because the number of parameters only grows linearly relative to the number of languages (O (L)), as opposed to separate single-pair models of the language, the number of parameters in this case being square (O (L2). The attention mechanism that was originally called the soft-alignment model."}, {"heading": "4 Multi-Way, Multilingual Model", "text": "We describe in this section a multi-layered, multilingual attention to the different types of languages that we have proposed in terms of the number of recurrent units.) We describe in this section a multi-layered, multilingual processing of the different types of encoders that we propose in terms of the number of recurrent units (in terms of the number of recurrent units) or a different architecture (in terms of the number of recurrent languages). (The number of recurrent units that we propose in terms of the number of recurrent units (in terms of the number of recurrent units) or the type of recurrent units (in terms of the number of recurrent languages), we can use for each type of encoder encoder language a different type of encoder language. (In terms of the number of recurrent units) or a different architecture (in terms of the number of recurrent units). (In terms of the number of recurrent units we can use the different types of encoder language (in terms of the languages)."}, {"heading": "5 Experiment Settings", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Datasets", "text": "We evaluate the proposed multipage multilingual translation model based on all available pairs from WMT '15-English (En) \u2194 French (Fr), Czech (Cs), German (De), Russian (Ru) and Finnish (Fi) - a total of ten directed pairs. For each pair, we link all available parallel corpora from WMT' 15. We use newtest-2013 as development kit and newtest-2015 as test kit, in all other pairs as Fi-En. In the case of Fi-En, we use newsdev-2015 and newtest-2015 as development kit and test kit, respectiv.Data Preprocessing Each training corpus is symbolized by the tokenizer script from the Moses decoder. The tokenized training corpus is cleaned according to the procedure in (Jean et al., 2015). Instead of using metrically separated tools or words, we use subword units from the Moses decoder decoder."}, {"heading": "5.2 Two Scenarios", "text": "First, we examine the impact of the proposed multipage, multilingual model on the translation of language pairs with limited resources. Among the five languages of WMT '15, we select En, De and Fi as source languages and En and De as target languages. We control the proportion of the parallel corpus of each pair from three to 5%, 10%, 20% and 40% of the original corpus. In other words, we train four models with different sizes of the parallel corpus for each language pair (EnDe, De-En, Fi-En.) as the starting point we train a one-pair model for each multiple, multilingual model. We further fine-tune the one-pair model in order to integrate the target-side monolingual corpus, which consists of the entire target-side monolingual corpus, with the entire target text consisting of the target text."}, {"heading": "5.3 Model Architecture", "text": "Each symbol, either source or target, is projected onto a 620-dimensional space. The encoder is a bidirectional recursive neural network with 1,000 gated recurrent units (GRU) in each direction, and the decoder is a recursive neural network with 1,000 GRU as well. (7) The output function of the gk decoder is a feedback network with 1,000 tanh hidden units. (11) The dimensions of the context vector are all set to 1,200. We use the same encoder type and decoder type for each target language. (9) The only difference between the monopair models and the proposed multilingual models is the number of the multilingual pairs of the transcodes or the 11."}, {"heading": "5.4 Training", "text": "We use the initial learning rate of 2 \u00b7 10 \u2212 4 and leave all other hyperparameters as suggested in (Kingma and Ba, 2015). Each SGD update is calculated with a minibatch of 80 examples, unless the model is paralleled via two GPUs, in which case we use a minibatch of 60 examples. We only use sets of up to 50 symbols. We cut the standard of gradient to no more than 1 (Pascanu et al., 2012). All training runs are stopped early on the basis of the BLEU development group. As we have observed in preliminary experiments, there are better values for development when we multiply the common parameters and output layers of the Ecoder based on Ecoders. We do this for all multilingual models."}, {"heading": "6 Results and Analysis", "text": "Table 2 clearly shows that the proposed model (Multi) exceeds the single-pair model (Single) in all cases, even if the single-pair model is strengthened by a target-side monolingual corpus (Single + DF). This suggests that the advantage of generalizing by multilingual models goes beyond the advantage of a simpler monolingual corpus. The performance gap grows as the size of the parallel target corpus (Single + DF) increases. Large-scale translation In Table 3, we find that the proposed multilingual model exceeds or is comparable to the single-pair model in the majority of the ten pairs / directions considered, both in terms of BLEU and the average likelihood of logging. This is encouraging considering that there are twice as many parameters in the totality of monolingual models as in the multilingual models considered. It is worth noting that the benefit is more obvious if the single-pair model translates from a target language (either to an increase in the target language) to a better strategy of English."}, {"heading": "7 Conclusion", "text": "The proposed approach allows us to build a single neural network that can handle multiple source and target languages at the same time. The proposed model is a step forward from recent work on multilingual neural translation in the sense that we support the attention mechanism compared to (Luong et al., 2015a) and multilingual translation compared to (Dong et al., 2015). Furthermore, we evaluate the proposed model in large-scale experiments using the full set of parallel corpora in large-scale experiments, empirically evaluating the proposed model in large-scale experiments using all five languages from WMT '15 with the full set of parallel corpora and also in constellations with artificially controlled set of parallel corpus. In both constellations, we observed the benefit of the proposed multilingual neural translation model over the use of an entire set of parallel models."}, {"heading": "Acknowledgments", "text": "The authors thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012) and Blocks (van Merrie \ufffd nboer et al., 2015) for the support of the following research funding and computer support organizations: NSERC, Samsung, IBM, Calcul Que \ufffd bec, Compute Canada, the Canada Research Chairs, CIFAR and TUBITAK-2214a."}], "references": [{"title": "2014", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "Neural machine translation by jointly learning to align and translate. In ICLR", "citeRegEx": "Bahdanau et al.2014", "shortCiteRegEx": null, "year": 2015}, {"title": "Nicolas Bouchard", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bastien et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "David WardeFarley", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bergstra et al.2010", "shortCiteRegEx": null, "year": 2010}, {"title": "1997", "author": ["L\u00e9on Bottou", "Yoshua Bengio", "Yann Le Cun"], "venue": "Global training of document processing systems using graph transformer networks. In Computer Vision and Pattern Recognition,", "citeRegEx": "Bottou et al.1997", "shortCiteRegEx": null, "year": 1997}, {"title": "On the properties of neural machine translation: Encoder\u2013 Decoder approaches", "author": ["Cho et al.2014a] Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "In Eighth Workshop on Syntax,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine", "author": ["Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Merri\u00ebnboer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2014}, {"title": "Attention-based models for speech recognition", "author": ["Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "2015.,? \\Q2015\\E", "shortCiteRegEx": "2015.", "year": 2015}, {"title": "Koray Kavukcuoglu", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen"], "venue": "and Pavel Kuksa.", "citeRegEx": "Collobert et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "and Graham W", "author": ["Weiguang Ding", "Ruoyan Wang", "Fei Mao"], "venue": "Taylor.", "citeRegEx": "Ding et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Dianhai Yu", "author": ["Daxiang Dong", "Hua Wu", "Wei He"], "venue": "and Haifeng Wang.", "citeRegEx": "Dong et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Faustino Gomez", "author": ["Alex Graves", "Santiago Fern\u00e1ndez"], "venue": "and J\u00fcrgen Schmidhuber.", "citeRegEx": "Graves et al.2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Horst Bunke", "author": ["Alex Graves", "Marcus Liwicki", "Santiago Fern\u00e1ndez", "Roman Bertolami"], "venue": "and J\u00fcrgen Schmidhuber.", "citeRegEx": "Graves et al.2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Holger Schwenk", "author": ["Caglar Gulcehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Loic Barrault", "HueiChi Lin", "Fethi Bougares"], "venue": "and Yoshua Bengio.", "citeRegEx": "Gulcehre et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "2014", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "On using very large target vocabulary for neural machine translation. In ACL", "citeRegEx": "Jean et al.2014", "shortCiteRegEx": null, "year": 2015}, {"title": "Roland Memisevic", "author": ["S\u00e9bastien Jean", "Orhan Firat", "Kyunghyun Cho"], "venue": "and Yoshua Bengio.", "citeRegEx": "Jean et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Ba2015] Diederik Kingma", "Jimmy Ba"], "venue": "The International Conference on Learning Representations (ICLR)", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Hieu Pham", "Christopher D Manning"], "venue": "arXiv preprint arXiv:1508.04025", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Tomas Mikolov", "author": ["Razvan Pascanu"], "venue": "and Yoshua Bengio.", "citeRegEx": "Pascanu et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Barry Haddow", "Alexandra Birch"], "venue": "arXiv preprint arXiv:1511.06709", "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909", "author": ["Barry Haddow", "Alexandra Birch"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Oriol Vinyals", "author": ["Ilya Sutskever"], "venue": "and Quoc VV Le.", "citeRegEx": "Sutskever et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Jan Chorowski", "author": ["Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Vincent Dumoulin", "Dmitriy Serdyuk", "David Warde-Farley"], "venue": "and Yoshua Bengio.", "citeRegEx": "van Merri\u00ebnboer et al.2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "We propose multi-way, multilingual neural machine translation. The proposed approach enables a single neural translation model to translate between multiple languages, with a number of parameters that grows only linearly with the number of languages. This is made possible by having a single attention mechanism that is shared across all language pairs. We train the proposed multiway, multilingual model on ten language pairs from WMT\u201915 simultaneously and observe clear performance improvements over models trained on only one language pair. In particular, we observe that the proposed model significantly improves the translation quality of low-resource language pairs.", "creator": "LaTeX with hyperref package"}}}