{"id": "1601.01085", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jan-2016", "title": "Incorporating Structural Alignment Biases into an Attentional Neural Translation Model", "abstract": "Neural encoder-decoder models of machine translation have achieved impressive results, rivalling traditional translation models. However their modelling formulation is overly simplistic, and omits several key inductive biases built into traditional models. In this paper we extend the attentional neural translation model to include structural biases from word based alignment models, including positional bias, Markov conditioning, fertility and agreement over translation directions. We show improvements over a baseline attentional model and standard phrase-based model over several language pairs, evaluating on difficult languages in a low resource setting.", "histories": [["v1", "Wed, 6 Jan 2016 06:03:17 GMT  (1408kb,D)", "http://arxiv.org/abs/1601.01085v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["trevor cohn", "cong duy vu hoang", "ekaterina vymolova", "kaisheng yao", "chris dyer", "gholamreza haffari"], "accepted": true, "id": "1601.01085"}, "pdf": {"name": "1601.01085.pdf", "metadata": {"source": "CRF", "title": "Incorporating Structural Alignment Biases into an Attentional Neural Translation Model", "authors": ["Trevor Cohn", "Kaisheng Yao", "Gholamreza Haffari"], "emails": ["tcohn@unimelb.edu.au", "vhoang2@student.unimelb.edu.au", "evylomova@student.unimelb.edu.au", "kaisheng.YAO@microsoft.com", "cdyer@cmu.edu", "gholamreza.haffari@monash.edu"], "sections": [{"heading": "1 Introduction", "text": "Recently, models of end-to-end machine translation based on the classification of neural networks have been shown to produce excellent translations that compete with, or in some cases exceed, traditional statistical systems (Kalchburner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), despite neural approaches that use an overall simpler model, with fewer assumptions about learning and prediction problems. Generally, neural approaches are based on the idea of an encoder decoder (Sutskever et al., 2014) in which the source language is encoded into a distributed representation, followed by a decoding step that generates the target translation. We focus on the attention-oriented translation model (Bahdanau et al., 2015), which uses a dynamic representation of the source set while the decoder takes care of different parts of the source, as it generates the target sentence."}, {"heading": "2 The attentional model of translation", "text": "In fact, it is a question of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of temptation, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error, of error,"}, {"heading": "3 Incorporating Structural Biases", "text": "The attention model, as described above, is a powerful and elegant translation model in which alignments between starting and target words are learned through the implicit conditional context of the attention mechanism. Despite its elegance, the attention model omits several key components of traditional alignment models such as the IBM models (Brown et al., 1993) and Vogel's hidden Markov model (Vogel et al., 1996), as implemented in the toolkit GIZA + + (Och and Ney, 2003). Combining the strengths of this highly successful research body in a neural model of machine translation holds potential to further improve the modeling accuracy of neural techniques. Below, we explain methods for incorporating these factors as structural distortions into the attention model."}, {"heading": "3.1 Position bias", "text": "First, we consider the position distortion, based on the observation that a word at a given relative position in the source tends to align with a word at a similar relative position in the target, iI \u2248 j J (IBM 2). Related, alignments tend to occur near the diagonal (Dyer et al., 2013) when we consider the alignments as a binary I \u00b7 J matrix (illustrated in Figure 1), where the cell at (i, j) ei + W (ah) gj \u2212 1 + W (ap). (j, i, I)) (6), where the position distortion consists of a redefinition of the pre-normalized attention scalars fji in Eq 5 as: fji = v > tanh (W (ae) gj \u2212 1 + W (ap). (j, i, I))), where the new component in the input is a simple feature function of the positions of the source length (1), inefficient (1), (1, and inefficient (1), (i), (i), (i), where the infinite component in the input is (1), infinite (1, inefficient (1), (1, inefficient (1), (i), (i), (i), (i), (i), (i), (i), where the new component in the infinite function of the source length is complete, (1, (1), (i), (i), (i), infinite (1, infinite (i), (1, infinite (1), (1, 1), infinite (1), infinite (1, (1), inefficient (1, 1), (1, 1), and 1, (i), (i), (i)."}, {"heading": "3.2 Markov condition", "text": "The HMM translation model (Vogel et al., 1996) is based on a Markov condition about random variables for alignment so that the model can learn local effects, e.g. if i \u2190 j is aligned, then it is likely that i + 1 \u2190 j + 1 or i \u2190 j + 1 correspond to local diagonal alignments or one-to-many alignments. Generally, there are many correlations between the alignments of a word and the word immediately to the left. Markov conditioning can also be incorporated into the position preference in a similar way by extending the attention entry from Equation 5 and 6 to include: fji = v > tanh (.. + W (am) 1 (1 \u2212 1; i)) (7), where.. the egg, gj \u2212 1 and components from Equation 6 and vice versa are abbreviated: fji = v > tanh (."}, {"heading": "3.3 Fertility", "text": "Fertility is a central component in the IBM models 3-5 (Brown et al., 1993). However, the inclusion of fertility in the attention-oriented model is a little more involved, and we present two techniques to do so. Local fertility First, we consider a feature-based technique that includes the following features: 2 (\"\u03b1 < j; i) = [1] = [2] = [3] = [3] = [3] = [4] = [4] = [4] = [5] = [5] = [6] [6]\" (\"\" \"\" \")\" (\"\" 7] \"[8]\" [8] \"[\"] \"[8]\" [8] \"[8]\" [8] \"[8]\" [8] \"[8]\" (\"8]]: (\" 8] 8 \"8\" 8 \"8\" [\"] 8\" [8 \"] 8\" (\"] 8\" 8 \"8\" [\"] 8\" [\"] 8\" (\"])."}, {"heading": "3.4 Bilingual Symmetry", "text": "So far, we have considered a conditional model of the target in view of the source by modeling p (t | s), but it has been proven for most decoders (Koehn et al., 2005) and also by explicit common educational goals (Liang et al., 2006; Ganchev et al., 2008) that both models make reasonably independent errors, so that an ensemble can benefit from the reduction of variance. We propose a method for the joint formation of two directional models, as shown in Figure 2. Training partner models is about optimizing L = \u2212 log p (t | s) \u2212 log p (s | t) + \u03b3B, where, as before, we only look at a single sentence pair, for reasons of simplicity of notation."}, {"heading": "4 Experiments", "text": "We conducted our experiments with four language pairs and translated between English \u2194 Romanian, Estonian, Russian and Chinese. These languages were selected to represent a number of translation difficulties, including languages with significant morphological complexity (Estonian, Russian). We are focusing on a (simulated) setting of low resources for which only a limited amount of training data is available. This serves to demonstrate the robustness and generalization of our model on sparse data - something that has not yet been established for neural models with millions of parameters with excessive potential for excessive customization.Table 1 shows the statistics of training sets.6 For the Chinese-English model, the data comes from the BTEC corpus, where the number of training sets has not yet been determined. We used \"devset1\" and \"devset 3\" as development and test models, and in both cases we used the first reference models for evaluation."}, {"heading": "4.1 Analysis of Alignment Biases", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "4.2 Full Results", "text": "The confusion results of the neural models for the two translation directions across the four language pairs are presented in Tables 3.a and 3.b. In all cases, due to linguistic constraints, we achieve less perplexity compared to the vanilla attention model and the encoder decoder architecture. Table 4 presents the BLEU values for the reranking setting for the English translation from our four languages. We compare the rankings based on the protocol probabilities generated by our model as additional features, compared to the use of the protocol probabilities from the vanilla attention model and the encoder decoder. The rankings based on our model are significantly better than the rest for Chinese and Estonian, and for Russian and Romanian \u2192 English on an equal footing with the others. In all cases, our model performs at least 1 BLEU point better than the baseline-based phrase system."}, {"heading": "5 Related Work", "text": "Kalchbrenner and Blunsom (2013) were the first to propose a complete neural translation model, in which a revolutionary network as source coder fol-9We used the results of 6-12 models trained in both directions, using different orientation and fertility options and a smaller dimensionality than before (100 embedding, 100 hidden and 50 attention dimensions). This was made possible by an RNN decoder (2015), which introduced the term \"attention\" into the model, whereby the source context can change dynamically during the decryption process to take care of the most relevant parts of the source set Luong et al. (2015)."}, {"heading": "6 Conclusion", "text": "We have shown that the attention-oriented translation model does not capture many well-known characteristics of traditional word-based translation models, and have suggested several ways to impose them as structural distortions on the model. We show improvements in several challenging language pairs in a low-resource environment, both in perplexity and reevaluation. In future work, we intend to examine the model's performance on larger datasets and incorporate additional linguistic information such as morphological representations."}, {"heading": "Acknowledgements", "text": "The work reported here was started at JSALT 2015 at UW, Seattle, and supported by JHU with grants from NSF (IIS), DARPA (LORELEI), Google, Microsoft, Amazon, Mitsubishi Electric, and MERL. Dr. Cohn was supported by ARC (Future Fellowship)."}], "references": [{"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["Brown et al.1993] Peter E. Brown", "Stephen A. Della Pietra", "Vincent J. Della Pietra", "Robert L. Mercer"], "venue": null, "citeRegEx": "Brown et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Agreement-based joint training for bidirectional attention-based neural machine translation", "author": ["Cheng et al.2015] Yong Cheng", "Shiqi Shen", "Zhongjun Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "venue": null, "citeRegEx": "Cheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2015}, {"title": "On the properties of neural machine translation", "author": ["K. Cho", "B. van Merrienboer", "D. Bahdanau", "Y. Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A simple, fast, and effective reparameterization of ibm model 2", "author": ["Dyer et al.2013] Chris Dyer", "Victor Chahuneau", "Noah A. Smith"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics:", "citeRegEx": "Dyer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Better alignments = better translations", "author": ["Jo\u00e3o V. Gra\u00e7a", "Ben Taskar"], "venue": "In Proceedings of ACL-08: HLT,", "citeRegEx": "Ganchev et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ganchev et al\\.", "year": 2008}, {"title": "KenLM: faster and smaller language model queries", "author": ["Kenneth Heafield"], "venue": "In Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation,", "citeRegEx": "Heafield.,? \\Q2011\\E", "shortCiteRegEx": "Heafield.", "year": 2011}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Statistical phrase-based translation", "author": ["Koehn et al.2003] Philipp Koehn", "Franz Josef Och", "Daniel Marcu"], "venue": "In Proceedings of North American Chapter of the Association for Computational Linguistics on Human Language Technology,", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Edinburgh system description for the 2005 IWSLT speech translation evaluation", "author": ["Koehn et al.2005] Philipp Koehn", "Amittai Axelrod", "Alexandra Birch", "Chris Callison-Burch", "Miles Osborne", "David Talbot", "Michael White"], "venue": "In IWSLT,", "citeRegEx": "Koehn et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2005}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": "In Proc. ACL Interactive Poster and Demonstration Sessions,", "citeRegEx": "Bertoldi et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bertoldi et al\\.", "year": 2007}, {"title": "Europarl: A Parallel Corpus for Statistical Machine Translation", "author": ["Philipp Koehn"], "venue": "In Conference Proceedings: the tenth Machine Translation Summit,", "citeRegEx": "Koehn.,? \\Q2005\\E", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Model invertibility regularization: Sequence alignment with or without parallel data", "author": ["Ashish Vaswani", "David Chiang"], "venue": "In Proceedings of the North American Chapter of the Association", "citeRegEx": "Levinboim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levinboim et al\\.", "year": 2015}, {"title": "Alignment by agreement", "author": ["Liang et al.2006] Percy Liang", "Ben Taskar", "Dan Klein"], "venue": "In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL),", "citeRegEx": "Liang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2006}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Hieu Pham", "Christopher D. Manning"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Neural reranking improves subjective quality of machine translation: NAIST at WAT2015", "author": ["Neubig et al.2015] Graham Neubig", "Makoto Morishita", "Satoshi Nakamura"], "venue": "In Proceedings of the 2nd Workshop on Asian Translation", "citeRegEx": "Neubig et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neubig et al\\.", "year": 2015}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Och", "Ney2003] Franz Josef Och", "Hermann Ney"], "venue": "Computational Linguistics,", "citeRegEx": "Och et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Och et al\\.", "year": 2003}, {"title": "Bleu: A method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th Annual Meeting on Association", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "HMM-based word alignment in statistical translation", "author": ["Vogel et al.1996] Stephan Vogel", "Hermann Ney", "Christoph Tillmann"], "venue": "In Proceedings of the International Conference on Computational Linguistics (COLING),", "citeRegEx": "Vogel et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Vogel et al\\.", "year": 1996}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu et al.2015] Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 18, "context": "Recently, models of end-to-end machine translation based on neural network classification have been shown to produce excellent translations, rivalling or in some cases surpassing traditional statistical machine translation systems (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 231, "endOffset": 310}, {"referenceID": 18, "context": "Broadly, neural approaches are based around the notion of an encoder-decoder (Sutskever et al., 2014), in which the source language is encoded into a distributed representation, followed by a decoding step which generates the target translation.", "startOffset": 77, "endOffset": 101}, {"referenceID": 0, "context": "The attentional model raises intriguing opportunities, given the correspondence between the notions of attention and alignment in traditional word-based machine translation models (Brown et al., 1993).", "startOffset": 180, "endOffset": 200}, {"referenceID": 3, "context": ", IBM Model 2 and (Dyer et al., 2013)), fertility whereby each instance of a source word type tends to be translated into a consistent number of target tokens (e.", "startOffset": 18, "endOffset": 37}, {"referenceID": 19, "context": ", IBM Model 4, 5 and HMM-based Alignment (Vogel et al., 1996)), and alignment consistency whereby the attention in both translation directions are encourged to agree (e.", "startOffset": 41, "endOffset": 61}, {"referenceID": 13, "context": "symmetrization heuristics (Och and Ney, 2003) or joint modelling (Liang et al., 2006; Ganchev et al., 2008)).", "startOffset": 65, "endOffset": 107}, {"referenceID": 4, "context": "symmetrization heuristics (Och and Ney, 2003) or joint modelling (Liang et al., 2006; Ganchev et al., 2008)).", "startOffset": 65, "endOffset": 107}, {"referenceID": 2, "context": "The presentation above assumes a simple RNN is used to define the recurrence over hidden states, however we can easily use alternative formulations of recurrent networks including multiplelayer RNNs, gated recurrent units (GRU; Cho et al. (2014)), or long short-term memory (LSTM; Hochreiter and Schmidhuber (1997)) units.", "startOffset": 228, "endOffset": 246}, {"referenceID": 2, "context": "The presentation above assumes a simple RNN is used to define the recurrence over hidden states, however we can easily use alternative formulations of recurrent networks including multiplelayer RNNs, gated recurrent units (GRU; Cho et al. (2014)), or long short-term memory (LSTM; Hochreiter and Schmidhuber (1997)) units.", "startOffset": 228, "endOffset": 315}, {"referenceID": 0, "context": "Despite its elegance, the attentional model omits several key components of a traditional alignment models such as the IBM models (Brown et al., 1993) and Vogel\u2019s hidden Markov Model (Vogel et al.", "startOffset": 130, "endOffset": 150}, {"referenceID": 19, "context": ", 1993) and Vogel\u2019s hidden Markov Model (Vogel et al., 1996) as implemented in the GIZA++ toolkit (Och and Ney, 2003).", "startOffset": 40, "endOffset": 60}, {"referenceID": 3, "context": "Related, alignments tend to occur near the diagonal (Dyer et al., 2013), when considering the alignments as a binary I \u00d7 J matrix (illustrated in Figure 1), where the cell at (i, j) denotes whether an alignment exists between source word i and target word j.", "startOffset": 52, "endOffset": 71}, {"referenceID": 19, "context": "The HMM model of translation (Vogel et al., 1996) is based on a Markov condition over alignment random variables, to allow the model to learn local effects such as when i \u2190 j is aligned then it is likely that i + 1 \u2190 j + 1 or i \u2190 j + 1.", "startOffset": 29, "endOffset": 49}, {"referenceID": 0, "context": "Fertility is a central component in the IBM models 3\u20135 (Brown et al., 1993).", "startOffset": 55, "endOffset": 75}, {"referenceID": 20, "context": "Initially we considered a soft constraint based on the approach in (Xu et al., 2015), where an image captioning model was biased to attend to every pixel in the image exactly once.", "startOffset": 67, "endOffset": 84}, {"referenceID": 0, "context": "Compared to the fertility model in IBM 3\u20135 (Brown et al., 1993), ours uses many fewer parameters through working over vector embeddings, and moreover, the BiRNN encoding of the source means that we learn context-dependent fertilities, which can be useful for dealing with fixed syntactic patterns or multi-word expressions.", "startOffset": 43, "endOffset": 63}, {"referenceID": 8, "context": "3 Modern decoders (Koehn et al., 2003) often impose the restriction of each word being translated exactly once, however this is tempered by their use of phrases as translation units rather than words, which allow for higher fertility in contiguous translation chunks.", "startOffset": 18, "endOffset": 38}, {"referenceID": 9, "context": "However it is well established for latent variable translation models that the alignments improve if p(s|t) is also modelled and the inferences of both directional models are combined \u2013 evidenced by the symmetrisation heuristics used in most decoders (Koehn et al., 2005), and also by explicit joint agreement training objectives (Liang et al.", "startOffset": 251, "endOffset": 271}, {"referenceID": 13, "context": ", 2005), and also by explicit joint agreement training objectives (Liang et al., 2006; Ganchev et al., 2008).", "startOffset": 66, "endOffset": 108}, {"referenceID": 4, "context": ", 2005), and also by explicit joint agreement training objectives (Liang et al., 2006; Ganchev et al., 2008).", "startOffset": 66, "endOffset": 108}, {"referenceID": 12, "context": "To achieve this, we use a \u2018trace bonus\u2019, inspired by (Levinboim et al., 2015), formulated as", "startOffset": 53, "endOffset": 77}, {"referenceID": 11, "context": "For other language pairs, the data come from the Europarl corpus (Koehn, 2005), where we used 100K sentence pairs for training, and 3K for development and 2K for testing.", "startOffset": 65, "endOffset": 78}, {"referenceID": 18, "context": ", 2015) and encoder-decoder architecture (Sutskever et al., 2014).", "startOffset": 41, "endOffset": 65}, {"referenceID": 5, "context": "We used KenLM (Heafield, 2011) to create 3-gram language models with Kneser-Ney smoothing on the target side of the bilingual training corpora.", "startOffset": 14, "endOffset": 30}, {"referenceID": 18, "context": "Following previous work (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Neubig et al., 2015), we evaluated all neural models using test set perplexities and in a re-ranking setting, using BLEU (Papineni et al.", "startOffset": 24, "endOffset": 124}, {"referenceID": 15, "context": "Following previous work (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Neubig et al., 2015), we evaluated all neural models using test set perplexities and in a re-ranking setting, using BLEU (Papineni et al.", "startOffset": 24, "endOffset": 124}, {"referenceID": 17, "context": ", 2015), we evaluated all neural models using test set perplexities and in a re-ranking setting, using BLEU (Papineni et al., 2002) measure.", "startOffset": 108, "endOffset": 131}, {"referenceID": 18, "context": "For comparison, we report the results of an encoderdecoder-based neural translation model (Sutskever et al., 2014) as the baseline.", "startOffset": 90, "endOffset": 114}, {"referenceID": 14, "context": "To leverage the attention history, (Luong et al., 2015) made use of the attention vector of the previous position when generating the attention vector for the next position, similar in spirit to our method for incorporating alignment structural biases.", "startOffset": 35, "endOffset": 55}, {"referenceID": 16, "context": "This was extended in Sutskever et al. (2014), who replaced the source encoder with an RNN using a Long Short-Term Memory (LSTM), and Bahdanau et al.", "startOffset": 21, "endOffset": 45}, {"referenceID": 16, "context": "This was extended in Sutskever et al. (2014), who replaced the source encoder with an RNN using a Long Short-Term Memory (LSTM), and Bahdanau et al. (2015) who introduced the notion of \u201cattention\u201d to the model, whereby the source context can dynamically change during the decoding process to attend to the most relevant parts of the source sentence Luong et al.", "startOffset": 21, "endOffset": 156}, {"referenceID": 13, "context": "(2015) who introduced the notion of \u201cattention\u201d to the model, whereby the source context can dynamically change during the decoding process to attend to the most relevant parts of the source sentence Luong et al. (2015) refined the attention mechanism to be more local, by constraining attention to a text span, whose words\u2019 representations are averaged.", "startOffset": 200, "endOffset": 220}, {"referenceID": 1, "context": "Concurrent with our work, Cheng et al. (2015) proposed a similar agreementbased joint training for bidirectional attention-based neural machine translation, and showed significant improvement in the BLEU score for the large data French\u2194English translation.", "startOffset": 26, "endOffset": 46}], "year": 2016, "abstractText": "Neural encoder-decoder models of machine translation have achieved impressive results, rivalling traditional translation models. However their modelling formulation is overly simplistic, and omits several key inductive biases built into traditional models. In this paper we extend the attentional neural translation model to include structural biases from word based alignment models, including positional bias, Markov conditioning, fertility and agreement over translation directions. We show improvements over a baseline attentional model and standard phrase-based model over several language pairs, evaluating on difficult languages in a low resource setting.", "creator": "LaTeX with hyperref package"}}}