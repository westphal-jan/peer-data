{"id": "1601.01272", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jan-2016", "title": "Recurrent Memory Networks for Language Modeling", "abstract": "Recurrent Neural Networks (RNN) have obtained excellent result in many natural language processing (NLP) tasks. However, understanding and interpreting the source of this success remains a challenge. In this paper, we propose Recurrent Memory Network (RMN), a novel RNN architecture, that not only amplifies the power of RNN but also facilitates our understanding of its internal functioning and allows us to discover underlying patterns in data. We demonstrate the power of RMN on language modeling and sentence completion tasks. On language modeling, RMN outperforms Long Short-Term Memory (LSTM) network on three large German, Italian, and English dataset. Additionally we perform in-depth analysis of various linguistic dimensions that RMN captures. On Sentence Completion Challenge, for which it is essential to capture sentence coherence, our RMN obtains 69.2% accuracy, surpassing the previous state-of-the-art by a large margin.", "histories": [["v1", "Wed, 6 Jan 2016 18:44:07 GMT  (662kb,D)", "http://arxiv.org/abs/1601.01272v1", "8 pages, 6 figures"], ["v2", "Fri, 22 Apr 2016 11:13:11 GMT  (1036kb,D)", "http://arxiv.org/abs/1601.01272v2", "8 pages, 6 figures. Accepted at NAACL 2016"]], "COMMENTS": "8 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ke m tran", "arianna bisazza", "christof monz"], "accepted": true, "id": "1601.01272"}, "pdf": {"name": "1601.01272.pdf", "metadata": {"source": "CRF", "title": "Recurrent Memory Network for Language Modeling", "authors": ["Ke Tran", "Arianna Bisazza", "Christof Monz"], "emails": ["m.k.tran@uva.nl", "a.bisazza@uva.nl", "c.monz@uva.nl"], "sections": [{"heading": "1 Introduction", "text": "This year, it has come to the point where it only takes one year to get to the next round."}, {"heading": "2 Recurrent Neural Networks", "text": "We have shown a variety of sequential modeling tasks due to their ability to encode unlimited input situations. However, the formation of simple RNNs is difficult due to the disappearing and exploding gradient problems (Bengio et al., 1994; Pascanu et al., 2013). Several variants of RNs have been proposed as a simple and effective solution to exploding gradients, including Long Short-Term Memory (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (Cho et al., 2014), widely regarded as the most successful variants to deal with the more difficult problem of disappearing gradients. In this work, we focus on LSTMs because they have been shown to outperform GRUs in linguistic modeling tasks (Jo)."}, {"heading": "3 Recurrent Memory Network", "text": "Due to this difficulty, we propose a novel RNN architecture called Recurrent Memory Network (RMN). In terms of linguistic data, the RMN allows us not only to qualify what linguistic information is preserved over time and why it is preserved, but also to discover dependencies within the data (\u00a7 5). Our RMN consists of two components: an LSTM and a Memory Block (MB) (\u00a7 3.1). The MB records the hidden state of the LSTM and compares it with recent inputs using an attention mechanism (Gregor et al., 2015; Bahdanau et al., 2014; Graves et al., 2014)."}, {"heading": "3.1 Memory Block", "text": "In fact, it is as if it is a kind of conspiracy that sees itself in a position to destroy the world. (...) It is as if the world in which it is situated were to put the world in order. (...) It is as if the world were in order. (...) It is as if the world were in order. (...) It is as if the world were in order. (...) It is as if the world was in order. (...) It is as if the world was in order. (...) It is as if the world was in order. (...) It is as if the world was in order. (...) It is as if the world is in order. (...) It is as if the world is in order. (...) It is as if the world is in order. (...) It is as if the world is in order. (...) It is as if the world is in order."}, {"heading": "3.2 RMN Architectures", "text": "As explained above, our proposed MB receives the hidden state of the LSTM as one of its inputs. This results in an intuitive combination of the two units by stacking the MB on the LSTM. We call this architecture Recurrent Memory (RM). However, the RM architecture does not allow interaction between memory blocks in different time steps. To enable this interaction, we can stack another LSTM layer on top of the RM. We call this architecture Recurrent Memory Recurrent (RMR)."}, {"heading": "4 Language Model Experiments", "text": "Language models play a crucial role in many NLP applications such as machine translation and speech recognition. Language modeling also serves as a standard test bed for newly proposed models (Sukhbaatar et al., 2015; Kalchbrenner et al., 2015). We suspect that by explicitly accessing historical words, RMN will provide better predictive power than existing recurring architectures. We therefore evaluate our RMN architectures against the state of the art in terms of perplexity."}, {"heading": "4.1 Data", "text": "We evaluate our models using three languages: English, German and Italian. We are particularly interested in German and Italian because of their larger vocabulary and complex match patterns. Table 1 summarizes the data used in our experiments. The training data correspond to approximately 1M sentences in each language. For English, we use all news comment data (8M tokens) and 18M tokens from News Crawl 2014 for training. Development and test data are randomly drawn from the concatenation of news comment data from 2009-2014 with the test records from WMT 2009-2014 (Bojar et al., 2015). For German, we use the first 6M tokens from the news comment data and 16M tokens from News Crawl 2014 for training. For development and test data, we use the remaining part of the news comment data associated with the test records from WMT 2009-2014 (Bojar et al., 2015). And finally, for Italian, we use a selection of 29M SA tokens from the Pus Korus (Wikipedia, 2014 and smaller documents)."}, {"heading": "4.2 Setup", "text": "Our baselines are a 5 gram language model with Kneser-Ney smoothing, a Memory Network (MemN), a single-layer vanilla LSTM model, and two stacked LSTMs with two and three levels, respectively. N gram models have been widely used for their excellent performance and fast training in many applications. (2015) Chen et al. (2015) show that the n gram model outperforms a popular feed language model (Bengio et al., 2003) to a billion word benchmark (Chelba et al., 2013). If we need longer time to train, RNN is superior to the n gram model. We compare these baselines with our two model architectures: RMR et al and RM. For each of our models, we experiment with two settings: with or without a time matrix (+ tM or -tM), and the linear vs. gating composition function."}, {"heading": "4.3 Results", "text": "The best overall results are achieved by RM with gating composition. Generally, all our RMN models outperform all competitive baselines. The results also reflect that the gating composition is critical for our models. Our results agree with the hypothesis of mitigating prediction errors by explicitly using the last n words in RNNs (Karpathy et al., 2015). We also observe that using the time matrix only benefits the RM architecture. This can be explained by considering RM as a principled way to interpolate an LSTM and a neural n-gram model. In contrast, RMR works better without a time matrix, but its overall performance is not as good as RM. This suggests that we need a better mechanism to address the interaction between MBs that we leave to future work."}, {"heading": "5 Attention Analysis", "text": "The goal of our RMN design is twofold: (1) to improve predictive power and (2) to facilitate understanding of the model and the discovery of patterns in data. In Section 4, we validated the predictive power of the RMN, now we are investigating the source of this power based on linguistic assumptions about word coherence and dependency structures."}, {"heading": "5.1 Positional and lexical analysis", "text": "This year it is more than ever before."}, {"heading": "5.2 Syntactic analysis", "text": "In fact, most of them are able to play by the rules that they have set themselves, and they are able to play by the rules that they have set themselves in order to play by the rules."}, {"heading": "6 Sentence Completion Challenge", "text": "We choose this task to demonstrate the effectiveness of our RMN in capturing sentence coherence. The test set consists of 1,040 sentences selected from five Sherlock Holmes novels by Conan Doyle. The best reported result is 58.9% accuracy (Mikolov et al., 2013) 3, which is far from human performance of 91% (Branch and Burges, 2012).Our baseline is insoluble for local language models such as n-gram. The best reported result is accuracy (Mikolov et al., 2013) 3, which is far from human performance of 91% and Burges. Our baseline is a stacked three-layer LSTM. Our models are two variants of RM (+ tM-g), each consisting of three LSTM layers followed by one MB."}, {"heading": "7 Conclusion", "text": "We have proposed the Recurrent Memory Network (RMN), a novel recurrent architecture for speech modeling. RMN surpasses LSTM in terms of the perplexity of three large data sets and allows us to analyze their behavior from a linguistic perspective. We note that RMN learns important coexistence, regardless of their distance. More interestingly, RMN implicitly detects certain dependency types that are important for word prediction, even though they are trained without syntactical information. Finally, RMN achieves outstanding performance in modeling sentence coherence, which represents a new state of the art in a demanding sentence completion task."}], "references": [{"title": "Accurate dependency parsing with a stacked multilayer perceptron. In Proceedings of Evalita\u201909, Evaluation of NLP and Speech Tools for Italian, Reggio Emilia, Italy", "author": ["Felice Dell\u2019Orletta", "Maria Simi", "Joseph Turian"], "venue": null, "citeRegEx": "Attardi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Attardi et al\\.", "year": 2009}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "ICLR", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio et al.1994] Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "Transaction on Neural Networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Treestructured composition in neural networks without tree-structured architectures", "author": ["Christopher D. Manning", "Christopher Potts"], "venue": "In Proceedings of Proceedings of the NIPS 2015 Workshop on Cognitive", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson"], "venue": null, "citeRegEx": "Chelba et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2013}, {"title": "Strategies for Training Large Vocabulary Neural Language Models", "author": ["Chen et al.2015] Welin Chen", "David Grangier", "Michael Auli"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "On the properties of neural machine translation: Encoder\u2013 decoder approaches", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "In Proceedings of SSST-8,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Finding structure in time", "author": ["Jeffrey L. Elman"], "venue": "Cognitive Science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Sentence compression by deletion with lstms", "author": ["Enrique Alfonseca", "Carlos A. Colmenares", "Lukasz Kaiser", "Oriol Vinyals"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Filippova et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Filippova et al\\.", "year": 2015}, {"title": "Eine umfassende Constraint-Dependenz-Grammatik des Deutschen. Fachbereich Informatik", "author": ["Kilian A. Foth"], "venue": null, "citeRegEx": "Foth.,? \\Q2006\\E", "shortCiteRegEx": "Foth.", "year": 2006}, {"title": "LSTM: A search space odyssey", "author": ["Greff et al.2015] Klaus Greff", "Rupesh Kumar Srivastava", "Jan Koutn\u0131\u0301k", "Bas R. Steunebrink", "J\u00fcrgen Schmidhuber"], "venue": null, "citeRegEx": "Greff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["Gregor et al.2015] Karol Gregor", "Ivo Danihelka", "Alex Graves", "Danilo Jimenez Rezende", "Daan Wierstra"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Training and analysing deep recurrent neural networks", "author": ["Hermans", "Schrauwen2013] Michiel Hermans", "Benjamin Schrauwen"], "venue": null, "citeRegEx": "Hermans et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hermans et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "J\u00f3zefowicz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "J\u00f3zefowicz et al\\.", "year": 2015}, {"title": "Visualizing and understanding recurrent networks. CoRR, abs/1506.02078", "author": ["Justin Johnson", "Fei-Fei Li"], "venue": null, "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "The pais\u00c0 corpus of italian web texts", "author": ["Lyding et al.2014] Verena Lyding", "Egon Stemle", "Claudia Borghetti", "Marco Brunello", "Sara Castagnoli", "Felice Dell\u2019Orletta", "Henrik Dittmann", "Alessandro Lenci", "Vito Pirrelli"], "venue": "In Proceedings of the 9th Web as Corpus", "citeRegEx": "Lyding et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lyding et al\\.", "year": 2014}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Luk\u00e1s Burget", "Jan Cernock\u00fd", "Sanjeev Khudanpur"], "venue": "INTERSPEECH", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space. In ICLR", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["Mnih", "Kavukcuoglu2013] Andriy Mnih", "Koray Kavukcuoglu"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Tomas Mikolov", "Yoshua Bengio"], "venue": "JMLR Proceedings,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Dropout improves recurrent neural networks for handwriting recognition", "author": ["Pham et al.2014] Vu Pham", "Christopher Bluche", "Th\u00e9odore Kermorvant", "J\u00e9r\u00f4me Louradour"], "venue": "In International Conference on Frontiers in Handwriting Recognition (ICFHR),", "citeRegEx": "Pham et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2014}, {"title": "Exploiting synergies between open resources for german dependency parsing, pos-tagging, and morphological analysis", "author": ["Martin Volk", "Gerold Schneider"], "venue": "In Recent Advances in Natural Language Processing (RANLP", "citeRegEx": "Sennrich et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2013}, {"title": "Less is more? towards a reduced inventory of categories for training a parser for the italian stanford dependencies", "author": ["Simi et al.2014] Maria Simi", "Cristina Bosco", "Simonetta Montemagni"], "venue": null, "citeRegEx": "Simi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simi et al\\.", "year": 2014}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning"], "venue": "In Proceedings of the Conference on Empirical Meth-", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "End-toend memory networks", "author": ["Arthur Szlam", "Jason Weston", "Rob Fergus"], "venue": "Advances in Neural Information Processing", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A challenge set for advancing language modeling", "author": ["Zweig", "Burges2012] Geoffrey Zweig", "Chris J.C. Burges"], "venue": "In Proceedings of the NAACL-HLT", "citeRegEx": "Zweig et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zweig et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 9, "context": "Recurrent Neural Networks (RNNs) (Elman, 1990; Mikolov et al., 2010) are remarkably powerful models for sequential data.", "startOffset": 33, "endOffset": 68}, {"referenceID": 19, "context": "Recurrent Neural Networks (RNNs) (Elman, 1990; Mikolov et al., 2010) are remarkably powerful models for sequential data.", "startOffset": 33, "endOffset": 68}, {"referenceID": 16, "context": "Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), a specific architecture of RNNs, has a track record of success in many natural language processing tasks such as language modeling (J\u00f3zefowicz et al., 2015), dependency parsing (Dyer et al.", "startOffset": 197, "endOffset": 222}, {"referenceID": 8, "context": ", 2015), dependency parsing (Dyer et al., 2015), sentence compression (Filippova et al.", "startOffset": 28, "endOffset": 47}, {"referenceID": 10, "context": ", 2015), sentence compression (Filippova et al., 2015), and machine translation (Sutskever et al.", "startOffset": 30, "endOffset": 54}, {"referenceID": 29, "context": ", 2015), and machine translation (Sutskever et al., 2014).", "startOffset": 33, "endOffset": 57}, {"referenceID": 4, "context": "Evidence supporting this assumption mainly comes from evaluating LSTMs in downstream applications: Bowman et al. (2015) carefully design two artificial datasets where sentences have explicit recursive structures.", "startOffset": 99, "endOffset": 120}, {"referenceID": 4, "context": "Evidence supporting this assumption mainly comes from evaluating LSTMs in downstream applications: Bowman et al. (2015) carefully design two artificial datasets where sentences have explicit recursive structures. They show empirically that while processing the input linearly, LSTMs can implicitly exploit recursive structures of languages. Filippova et al. (2015) find that using explicit syntactic features within LSTMs in their sentence compression model hurts the performance of overall system.", "startOffset": 99, "endOffset": 365}, {"referenceID": 17, "context": "To our knowledge, the only attempt to better understand the reasons of an LSTM\u2019s performance and limitations is the work of Karpathy et al. (2015) by means of visualization experiments and cell activation statistics in the context of character-level language modeling.", "startOffset": 124, "endOffset": 147}, {"referenceID": 28, "context": "We propose Recurrent Memory Network (RMN), a novel RNN architecture that combines the strengths of both LSTM and Memory Network (Sukhbaatar et al., 2015).", "startOffset": 128, "endOffset": 153}, {"referenceID": 2, "context": "However, training simple RNNs is difficult because of the vanishing and exploding gradient problems (Bengio et al., 1994; Pascanu et al., 2013).", "startOffset": 100, "endOffset": 143}, {"referenceID": 22, "context": "However, training simple RNNs is difficult because of the vanishing and exploding gradient problems (Bengio et al., 1994; Pascanu et al., 2013).", "startOffset": 100, "endOffset": 143}, {"referenceID": 7, "context": "Among them, Long Short-Term Memory (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (Cho et al., 2014) are widely regarded as the most successful variants.", "startOffset": 95, "endOffset": 113}, {"referenceID": 16, "context": "In this work, we focus on LSTMs because they have been shown to outperform GRUs on language modeling tasks (J\u00f3zefowicz et al., 2015).", "startOffset": 107, "endOffset": 132}, {"referenceID": 2, "context": "However, training simple RNNs is difficult because of the vanishing and exploding gradient problems (Bengio et al., 1994; Pascanu et al., 2013). A simple and effective solution for exploding gradients is gradient clipping proposed by Pascanu et al. (2013). To deal with the more challenging problem of vanishing gradients, several variants of RNNs have been proposed.", "startOffset": 101, "endOffset": 256}, {"referenceID": 12, "context": "Despite the popularity of LSTM in sequential modeling, its design is not straightforward to justify and understanding why it works remains a challenge (Hermans and Schrauwen, 2013; Chung et al., 2014; Greff et al., 2015; J\u00f3zefowicz et al., 2015; Karpathy et al., 2015).", "startOffset": 151, "endOffset": 268}, {"referenceID": 16, "context": "Despite the popularity of LSTM in sequential modeling, its design is not straightforward to justify and understanding why it works remains a challenge (Hermans and Schrauwen, 2013; Chung et al., 2014; Greff et al., 2015; J\u00f3zefowicz et al., 2015; Karpathy et al., 2015).", "startOffset": 151, "endOffset": 268}, {"referenceID": 17, "context": "Despite the popularity of LSTM in sequential modeling, its design is not straightforward to justify and understanding why it works remains a challenge (Hermans and Schrauwen, 2013; Chung et al., 2014; Greff et al., 2015; J\u00f3zefowicz et al., 2015; Karpathy et al., 2015).", "startOffset": 151, "endOffset": 268}, {"referenceID": 12, "context": ", 2014; Greff et al., 2015; J\u00f3zefowicz et al., 2015; Karpathy et al., 2015). There have been few recent attempts to understand the components of an LSTM from an empirical point of view: Greff et al. (2015) carry out a large-scale experiment of eight LSTM variants.", "startOffset": 8, "endOffset": 206}, {"referenceID": 12, "context": ", 2014; Greff et al., 2015; J\u00f3zefowicz et al., 2015; Karpathy et al., 2015). There have been few recent attempts to understand the components of an LSTM from an empirical point of view: Greff et al. (2015) carry out a large-scale experiment of eight LSTM variants. The results from their 5,400 experimental runs suggest that forget gates and output gates are the most critical components of LSTMs. J\u00f3zefowicz et al. (2015) conduct and evaluate over ten thousand RNN architectures and find that the initialization of the forget gate bias is crucial to the LSTM\u2019s performance.", "startOffset": 8, "endOffset": 423}, {"referenceID": 26, "context": "(2015) show that a vanilla LSTM, such as described above, performs reasonably well compared to a recursive neural network (Socher et al., 2011) that explicitly exploits tree structures on two artificial datasets.", "startOffset": 122, "endOffset": 143}, {"referenceID": 13, "context": "The MB takes the hidden state of the LSTM and compares it to the most recent inputs using an attention mechanism (Gregor et al., 2015; Bahdanau et al., 2014; Graves et al., 2014).", "startOffset": 113, "endOffset": 178}, {"referenceID": 1, "context": "The MB takes the hidden state of the LSTM and compares it to the most recent inputs using an attention mechanism (Gregor et al., 2015; Bahdanau et al., 2014; Graves et al., 2014).", "startOffset": 113, "endOffset": 178}, {"referenceID": 28, "context": "The Memory Block (Figure 1) is a variant of Memory Network (Sukhbaatar et al., 2015) with one hop (or a single-layer Memory Network).", "startOffset": 59, "endOffset": 84}, {"referenceID": 7, "context": "Our gating unit is a form of Gated Recurrent Unit (Cho et al., 2014; Chung et al., 2014)", "startOffset": 50, "endOffset": 88}, {"referenceID": 27, "context": "Instead of using a simple addition function g(st,ht) = st + ht as in Sukhbaatar et al. (2015), we propose to use a gating unit that decides how much it should trust the hidden state ht and context st at time step t.", "startOffset": 69, "endOffset": 94}, {"referenceID": 28, "context": "Language modeling also serves as a standard test bed for newly proposed models (Sukhbaatar et al., 2015; Kalchbrenner et al., 2015).", "startOffset": 79, "endOffset": 131}, {"referenceID": 18, "context": "Finally, for Italian, we use a selection of 29M tokens from the PAIS\u00c0 corpus (Lyding et al., 2014), mainly including Wikipedia pages and, to a minor extent, Wikibooks and Wikinews documents.", "startOffset": 77, "endOffset": 98}, {"referenceID": 3, "context": "(2015) show that n-gram model outperforms a popular feed-forward language model (Bengio et al., 2003) on a one billion word benchmark (Chelba et al.", "startOffset": 80, "endOffset": 101}, {"referenceID": 5, "context": ", 2003) on a one billion word benchmark (Chelba et al., 2013).", "startOffset": 40, "endOffset": 61}, {"referenceID": 3, "context": "Chen et al. (2015) show that n-gram model outperforms a popular feed-forward language model (Bengio et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 16, "context": "The bias of the LSTM\u2019s forget gate is initialized to 1 (J\u00f3zefowicz et al., 2015) while all other parameters are initialized uniformly in (\u22120.", "startOffset": 55, "endOffset": 80}, {"referenceID": 22, "context": "During training, we rescale the gradients whenever their norm is greater than 5 (Pascanu et al., 2013).", "startOffset": 80, "endOffset": 102}, {"referenceID": 17, "context": "Our results agree with the hypothesis of mitigating prediction error by explicitly using the last n words in RNNs (Karpathy et al., 2015).", "startOffset": 114, "endOffset": 137}, {"referenceID": 17, "context": "Previous work (Hermans and Schrauwen, 2013; Karpathy et al., 2015) studied this property of LSTM by analyzing simple cases of closing parenthesis and brace.", "startOffset": 14, "endOffset": 66}, {"referenceID": 24, "context": "We produce dependency parses for our test sets using (Sennrich et al., 2013) for German and (Attardi et al.", "startOffset": 53, "endOffset": 76}, {"referenceID": 0, "context": ", 2013) for German and (Attardi et al., 2009) for Italian.", "startOffset": 23, "endOffset": 45}, {"referenceID": 25, "context": "The German and Italian dependency tag sets are explained in (Simi et al., 2014) and (Foth, 2006) respectively.", "startOffset": 60, "endOffset": 79}, {"referenceID": 11, "context": ", 2014) and (Foth, 2006) respectively.", "startOffset": 12, "endOffset": 24}, {"referenceID": 20, "context": "9% accuracy (Mikolov et al., 2013)3 which is far from of the human performance of 91% (Zweig and Burges, 2012).", "startOffset": 12, "endOffset": 34}, {"referenceID": 27, "context": "Moreover, for regularization, we place dropout (Srivastava et al., 2014) after each LSTM layer as suggested in (Pham et al.", "startOffset": 47, "endOffset": 72}, {"referenceID": 23, "context": ", 2014) after each LSTM layer as suggested in (Pham et al., 2014).", "startOffset": 46, "endOffset": 65}], "year": 2016, "abstractText": "Recurrent Neural Networks (RNN) have obtained excellent result in many natural language processing (NLP) tasks. However, understanding and interpreting the source of this success remains a challenge. In this paper, we propose Recurrent Memory Network (RMN), a novel RNN architecture, that not only amplifies the power of RNN but also facilitates our understanding of its internal functioning and allows us to discover underlying patterns in data. We demonstrate the power of RMN on language modeling and sentence completion tasks. On language modeling, RMN outperforms Long Short-Term Memory (LSTM) network on three large German, Italian, and English dataset. Additionally we perform indepth analysis of various linguistic dimensions that RMN captures. On Sentence Completion Challenge, for which it is essential to capture sentence coherence, our RMN obtains 69.2% accuracy, surpassing the previous stateof-the-art by a large margin.", "creator": "LaTeX with hyperref package"}}}