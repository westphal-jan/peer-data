{"id": "1601.01280", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jan-2016", "title": "Language to Logical Form with Neural Attention", "abstract": "Semantic parsing aims at mapping natural language to machine interpretable meaning representations. Traditional approaches rely on high-quality lexicons, manually-built templates, and linguistic features which are either domain- or representation-specific. In this paper, we present a general method based on an attention-enhanced sequence-to-sequence model. We encode input sentences into vector representations using recurrent neural networks, and generate their logical forms by conditioning the output on the encoding vectors. The model is trained in an end-to-end fashion to maximize the likelihood of target logical forms given the natural language inputs. Experimental results on four datasets show that our approach performs competitively without using hand-engineered features and is easy to adapt across domains and meaning representations.", "histories": [["v1", "Wed, 6 Jan 2016 19:13:12 GMT  (437kb,D)", "http://arxiv.org/abs/1601.01280v1", null], ["v2", "Mon, 6 Jun 2016 21:06:55 GMT  (887kb,D)", "http://arxiv.org/abs/1601.01280v2", "Accepted by ACL-16"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["li dong", "mirella lapata"], "accepted": true, "id": "1601.01280"}, "pdf": {"name": "1601.01280.pdf", "metadata": {"source": "CRF", "title": "Language to Logical Form with Neural Attention", "authors": ["Li Dong"], "emails": ["li.dong@ed.ac.uk", "mlap@inf.ed.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Semantic parsing is the task of translating text into a formal representation of meaning, such as logical forms or structured queries. Recently, interest has increased in the development of machine learning methods for semantic parsing (see references in Section 2), due in part to the existence of corporas that contain statements commented on with formal meaning representations. Figure 1 shows an example of a question (left) and its annotated logical form (right), which come from JOBS (Tang and Mooney, 2001), a well-known semantic parsing benchmark. To predict the correct logical form for a given input utterance, most previous systems rely on predefined templates and manually designed features that render parsers domain-specific or representation-specific. In this work, we aim to use a simple but effective method to bridge the gap between natural language and logical form with minimal domain knowledge."}, {"heading": "2 Related Work", "text": "Our work synthesizes two strands of research, namely semantic parsing and the encoder-decoder architecture with neural networks, and the problem of semantic parsers has received significant attention dating back to Woods (1973).A large number of approaches learn from sentences paired with logical forms that follow different modeling strategies, such as the use of parsing models (Miller et al., 1996; Ge and Mooney, 2005; Lu et al., 2008; Zhao and Huang, 2015), inductive logic programming (Cell and Mooney, 1996; Tang and Mooney, 2000; Thomspon and Mooney, 2003), probabilistic automata (He and Young, 2006), string / tree-to-tree transformation rules (Kate et al., 2005), classifiers based on string water (Kate and Mooney, 2006), machine translation (Wong and Mooney, 2006)."}, {"heading": "3 Model", "text": "Our goal is to learn a model that maps the input of natural language q = (x1, \u00b7 \u00b7, x | q |) to a logical representation of its meaning a = (y1, \u00b7 \u00b7 \u00b7, y | a |). We consider both input q and output a as sequences and would like to estimate the conditional probability p (a | q), which is divided as follows: p (a | q) = | a | q = 1 p (yt | y < t, q) (1), where y < t = (y1, \u00b7 \u00b7, yt \u2212 1). Below we describe the details of how p (a | q) is calculated."}, {"heading": "3.1 Sequence-to-Sequence Model", "text": "Our model consists of an encoder that encodes the input of natural language q into a vector representation, and a decoder that learns to condition y1, \u00b7 \u00b7 \u00b7, y | on the encoding vector.As shown in Figure 2, the encoder and the decoder belong to two different L-layer recursive neural networks with Long Short-Term Memory (LSTM) units that follow recursively, the first | q time steps belong to the encoder, while the following | a time steps belong to the decoder. Let hold that a n-dimensional hidden vector is in time step t and layer L. hlt is replaced by: hlt = f (hlt \u2212 1, h \u2212 1 t) (2), where h0t is the word vector of the current vector x and the previously predicted word for the decoder."}, {"heading": "3.2 Attention Mechanism", "text": "As shown in Equation (4), the hidden vectors of the input sequence are not directly used in the decoding process. However, it is intuitively useful to consider relevant information from the input to better predict the current token. Following this idea, various techniques have been proposed to integrate encoder-side information (in the form of a context vector) into each time step of the decoder (Bahdanau et al., 2015; Luong et al., 2015b; Xu et al., 2015; Hermann et al., 2015). We use the global attention architecture described in Luong et al. (2015b). As shown in Figure 3, we calculate its attention value using the k-th hidden state in the encoder as: s | t t k = exp = exp {hLk)."}, {"heading": "3.3 Model Training", "text": "Our goal is to maximize the probability of generated logical forms by using natural linguistic expressions as input. Therefore, the objective function is defined as min \u2212 \u2211 (q, a) \u0394Dlog p (a | q) (9), where D is the set of all natural linguistic form training pairs and p (a | q) is calculated as shown in Equation (1). To solve this non-convex optimization problem, the RMSProp algorithm (Tieleman and Hinton, 2012) is used. Furthermore, the dropout is used to regulate the model, as proposed in Zaremba et al. (2015). Specifically, dropout operators are used between different LSTM layers and for the hidden layers in front of the Softmax classifiers. This technique can significantly reduce overpass, especially for small-size datasets."}, {"heading": "3.4 Inference", "text": "At test date, the model predicts the logical form of an input expression q, which maximizes the conditional probability: a = argmax a \"p (a\" | q) (10), where a \"represents a logical form sequence of the candidate. However, it is not practical to iterate the total possible sequences in order to obtain the optimal prediction. According to Equation (1), we break down the probability p (a | q) so that we can predict tokens successively by bar search or greedy search."}, {"heading": "3.5 Handling Rare Words", "text": "Conventional sequence-to-sequence models replace rare words with the unknown word symbol (Luong et al., 2015a; Jean et al., 2015). However, this strategy would adversely affect the accuracy of our model, since a predictive result is correct only if all tokens are analyzed correctly. Our solution is to link units and numbers in utterances with their corresponding logical constants and replace them with their type designations and unique IDs. For example, we edit the training example \"Jobs with a salary of 40,000\" and its logical form \"Job (ANS), salary greater than (ANS, 40,000, year)\" as \"Jobs with a salary of num0\" and \"Job (ANS), salary greater than (ANS, num0, year).\" We use the pre-processed examples as training data. At the same time, we mask entities and type designations with their respective step types to obtain the corresponding IDs."}, {"heading": "4 Experiments", "text": "We compare our method with several previous systems on four datasets. We describe these datasets below and present our experimental settings and results. Finally, we perform model analyses to understand what the model learns."}, {"heading": "4.1 Datasets", "text": "Our model was trained using the following data sets, which cover different areas and use different meanings. Examples for each domain are shown in Table 1.JOBS. This benchmark data set contains 640 queries to a database of job vacancies. Specifically, questions are paired with prologue-style queries. We used the same training test split as Zettlemoyerand Collins (2005), which contains 500 queries to a database of US geography. Values for the variables company, degree, language, platform, location, workspace and number are considered to be rare words.GEO This is a standard semantic benchmark, containing 880 queries to a database of US geography. GEO has 880 sentence logic pairs divided into a training set of 680 training examples and 200 test examples (Zettlemoyer and Collins, 2005). We used the same meaning representation based on Lambda calculations as Katwikowski (2011)."}, {"heading": "4.2 Settings", "text": "We converted natural language sets to lowercase letters and corrected misspelled words using a dictionary based on Wikipedia's list of the most common spelling errors. To reduce sparseness, words were derived using the snowball stamper within the NLTK (Bird et al., 2009).For IFTTT, we filtered tokens, channels, and functions that appeared less than five times in the training set. For the other records, we filtered input words that did not occur at least twice in the training set, but kept all tokens in the logical forms. Simple string matching was used to identify entities for handling rare words, as described in Section 3.5. More sophisticated approaches could be used, but we left this future work.Model hyper parameters were cross-validated on the training set for JOBS and GEO. We used the standard development sets for ATIFTI and batch algorithm mini-trained."}, {"heading": "4.3 Results", "text": "In fact, most of them are able to survive themselves if they don't put themselves in a position to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves. Most of them are able to survive themselves, and most of them are not able to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves."}, {"heading": "4.4 Model Analysis", "text": "Figure 4 illustrates examples of alignments generated by our model. Attention value matrices are calculated using Equation (5) and displayed in grayscale. Aligned input words and logical form predicates are enclosed in (same-colored) rectangles whose overlapping ranges contain attention values. Input sentences are reversed (Sutskever et al., 2014) to achieve better performance. Also, note that attention values are calculated using hidden LSTM vectors that encode context information and not just the words in their current position. The examples show that the attention mechanism learns soft alignments between input sets and output logical shapes, performing surprisingly well, even in cases of reordering (Figure 4b, 4c, and 4d) and one-to-many alignments (Figure 4c). We also examine whether the length of natural language descriptions and logical forms influences performance, such as the variability of the AT5 shows with different length mapping."}, {"heading": "5 Conclusions", "text": "Our experimental results show that improving the model through an attention mechanism improves performance across the board, especially for long sequences. We also show that our method of handling rare words (through masking and post-processing) is effective and increases performance. Extensive comparisons with previous approaches show that our model is competitive without recourse to domain or representation-specific characteristics. Instructions for future work are manifold. For example, it would be interesting to learn a model from pairs of questions without access to logical forms. We would also like to use the model to answer questions about a knowledge base, for example by ordering predicate paths between entities (which emerge in the queries) and candidates."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Semantic parsing aims at mapping natural<lb>language to machine interpretable meaning<lb>representations. Traditional approaches rely<lb>on high-quality lexicons, manually-built tem-<lb>plates, and linguistic features which are either<lb>domainor representation-specific. In this pa-<lb>per, we present a general method based on<lb>an attention-enhanced sequence-to-sequence<lb>model. We encode input sentences into vec-<lb>tor representations using recurrent neural net-<lb>works, and generate their logical forms by<lb>conditioning the output on the encoding vec-<lb>tors. The model is trained in an end-to-end<lb>fashion to maximize the likelihood of target<lb>logical forms given the natural language in-<lb>puts. Experimental results on four datasets<lb>show that our approach performs competi-<lb>tively without using hand-engineered features<lb>and is easy to adapt across domains and mean-<lb>ing representations.", "creator": "TeX"}}}