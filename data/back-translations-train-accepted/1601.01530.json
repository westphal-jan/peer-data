{"id": "1601.01530", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jan-2016", "title": "Leveraging Sentence-level Information with Encoder LSTM for Semantic Slot Filling", "abstract": "Recurrent Neural Network (RNN) and one of its specific architectures, Long Short-Term Memory (LSTM), have been widely used for sequence labeling. In this paper, we first enhance LSTM-based sequence labeling to explicitly model label dependencies. Then we propose another enhancement to incorporate the global information spanning over the whole input sequence. The latter proposed method, encoder-labeler LSTM, first encodes the whole input sequence into a fixed length vector with the encoder LSTM, and then uses this encoded vector as the initial state of another LSTM for sequence labeling. Combining these methods, we can predict the label sequence with considering label dependencies and information of whole input sequence. In the experiments of a slot filling task, which is an essential component of natural language understanding, with using the standard ATIS corpus, we achieved the state-of-the-art F1-score of 95.66%.", "histories": [["v1", "Thu, 7 Jan 2016 13:32:31 GMT  (157kb)", "http://arxiv.org/abs/1601.01530v1", null], ["v2", "Tue, 23 Aug 2016 02:06:45 GMT  (605kb)", "http://arxiv.org/abs/1601.01530v2", null], ["v3", "Mon, 29 Aug 2016 00:41:29 GMT  (603kb)", "http://arxiv.org/abs/1601.01530v3", "Accepted in EMNLP 2016"], ["v4", "Wed, 31 Aug 2016 00:30:10 GMT  (512kb)", "http://arxiv.org/abs/1601.01530v4", "Accepted in EMNLP 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["gakuto kurata", "bing xiang", "bowen zhou", "mo yu"], "accepted": true, "id": "1601.01530"}, "pdf": {"name": "1601.01530.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["gakuto@jp.ibm.com", "bingxia@us.ibm.com", "zhou@us.ibm.com", "gflfof@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 160 1,01 530v 1 [cs.C L] 7J on"}, {"heading": "1 Introduction", "text": "In fact, we are focused on fulfilling tasks in this area, especially since the 1980s, when most of the semantic labeling problems were set in motion. (DARPA) The labeling system has begun to develop semantic labeling for each word in the given word sequence. Filling labeling is a traditional task and a huge effort, especially since the 1980s, when the Defense Advanced Research Program Agency (DARPA) pushed forward airline labeling of labeling. (DARPA) The labeling system of labeling has begun (DIS). The success of deep learning (Hinton et al., 2006), Recurrent Neurrent Neural Network (RNN) (Elman, 1990; Jordan, 1997) and one of its specific architectures (LSTM)."}, {"heading": "2 Proposed Method", "text": "We first check the LSTM for slot filling and extend it to explicitly model label dependencies, and then explain the proposed LSTM encoder labeler."}, {"heading": "2.1 LSTM for Slot Filling", "text": "LSTM is a specific RNN architecture and is easier to train thanks to its internal memory cells and gates. Figure 1 (a) shows a typical LSTM for filling slots, and we call this as the labeler LSTM (W), where words are fed to the LSTM (Yao et al., 2014a). Slit labeling is a sequential labeling task to map a sequence of T-words xT1 to a sequence of T-words yT1. Instead of simply feeding Ext into the LSTM, context window is a widely used technique where V is the word size and is transferred into the de-dimensional continuous space by the word embedding of the matrix E-R de-V as an Ext."}, {"heading": "2.2 Explicit Modeling of Label Dependency", "text": "One shortcoming of the LSTM (W) labeler is that it does not take label dependencies into account. To explicitly model label dependencies, we are introducing a new architecture, the LSTM (W + L) labeler, as shown in Figure 1 (b), where the output label of the previous time step is fed together with words into the hidden state of the current time step, as Mesnil et al. (2015) and Liu and Lane (2015) have tried with RNN. For model training, a hot vector of the ground truth label of the previous time step is fed into the hidden state of the current time step, and a left-right beam search is used for evaluation."}, {"heading": "2.3 Encoder-labeler LSTM for Slot Filling", "text": "We propose two types of encoder labeler LSTM, which uses the labeler LSTM (W) and the labeler LSTM (W + L). Figure 1 (d) shows the encoder labeler LSTM (W). The encoder LSTM, to the left of the dashed line, reads the input set backwards. Its last hidden state contains the encoded information of the input set. The labeler LSTM (W), to the right of the dotted line, is identical to the labeler LSTM (W), which is explained in Section 2.1, except that its hidden state is initialized with the last hidden state of the encoder LSTM. The labeler LSTM (W) predicts the labeler LSTM (W) conditional labeler LSTM (W), which means that the labeler is labeled by the encoder LSTM, whereby the filler M is encoded."}, {"heading": "3 Experiments", "text": "We first explain the experimental setup, then report on the results to confirm the improvement with the proposed encoder labeler LSTM. Finally, we compare our results with the published results while discussing the corresponding work."}, {"heading": "3.1 Experimental Setup", "text": "We used the ATIS corpus, which is widely used as a benchmark for NLU (Price, 1990; Dahl et al., 1994; Wang et al., 2006; Tur et al., 2010). Figure 2 shows a sample sentence and its semantic slot labels in In-Out Begin (IOB) representation. The task in filling the slot labels was to predict the ratio of correct labels in the output of the system and recall the ratio of correct labels in the basic truth of the evaluation data (van Rijsbergen, 1979). The ATIS corpus contains the training data of 4,978 sets and evaluation data of 893 sets. The unique number of slot labels is 127 and the following Vocabulary size of the following experiments in 2014."}, {"heading": "3.2 Improvement by Encoder-labeler LSTM", "text": "We conducted experiments to compare the labellers LSTM (W) (Section 2.1), the labellers LSTM (W + L) (Section 2.2) and the encoder labellers LSTM (Section 2.3). As for yet another baseline, we tried the encoder decoders LSTM (W + L) as shown in Figure 1 (c) 1. For all laboratories, we set the initial learning rate to 0.001 (K and Ba, 2014) and the dimension of the word embeddings to de = 30. We changed the number of hidden units in the LSTM, ie, 200, 300} 2, and the size of the context window, k, k {0, 2} 3. We used the backwards coding for the encoder decoders LSTM laboratory LSTM as suggested in Sutskever et al."}, {"heading": "3.3 Comparison with Published Results", "text": "Table 2 summarizes the recently published results on the ATIS slot filling task and compares them with the results of the proposed methods.Recent research focuses on RNN and its extensions. Yao et al. (2013) used RNN and exceeded methods that did not use neural networks, such as SVM (Raymond and Riccardi, 2007) and CRF (Deng et al., 2012).Mesnil et al. (2015) tried bi-directional RNN, but reported a deterioration compared to their unilateral RNN (94.98%).Bi-directional RNN laboratories were not effective after using context windows in this task. Yao et al. (2014a) introduced LSTM and deep LSTM and achieved an improvement over RNN. Peng and Yao (2015) suggested RNN-EM an external memory architecture to propose the memory capability of RNN characteristics, which many studies were also carried out to expose and expose CRN dependencies."}, {"heading": "4 Conclusion", "text": "Vukotic et al. (2015) achieved 96.16% F1 score by using the so-called entity (NE) database in the evaluation of word embedding. Yao et al. (2013) and Yao et al. (2014a) used NE features in addition to word features and made improvements in both RNN and LSTM. In a slot filling task using the standard ATIS corpus, we obtained the status-of-the-art F1 score. Our future work includes the evaluation of other data sets."}], "references": [{"title": "Learning deep architectures for ai. Foundations and trends R", "author": ["Yoshua Bengio"], "venue": null, "citeRegEx": "Bengio.,? \\Q2009\\E", "shortCiteRegEx": "Bengio.", "year": 2009}, {"title": "Random search for hyper-parameter optimization", "author": ["Bergstra", "Bengio2012] James Bergstra", "Yoshua Bengio"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bergstra et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2012}, {"title": "Expanding the scope of the ATIS task: The ATIS-3 corpus", "author": ["Dahl et al.1994] Deborah A Dahl", "Madeleine Bates", "Michael Brown", "William Fisher", "Kate Hunicke-Smith", "David Pallett", "Christine Pao", "Alexander Rudnicky", "Elizabeth Shriberg"], "venue": "In Proc. HLT,", "citeRegEx": "Dahl et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 1994}, {"title": "Spoken language understanding", "author": ["Fr\u00e9d\u00e9ric Bechet", "Dilek Hakkani-Tur", "Michael McTear", "Giuseppe Riccardi", "Gokhan Tur"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Mori et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mori et al\\.", "year": 2008}, {"title": "Use of kernel deep convex networks and end-to-end learning for spoken language understanding", "author": ["Deng et al.2012] Li Deng", "Gokhan Tur", "Xiaodong He", "Dilek Hakkani-Tur"], "venue": "In Proc. SLT,", "citeRegEx": "Deng et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2012}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman"], "venue": "Cognitive science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Bengio2010] Xavier Glorot", "Yoshua Bengio"], "venue": "In Proc AISTATS,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Simon Osindero", "Yee-Whye Teh"], "venue": "Neural computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Serial order: A parallel distributed processing approach", "author": ["Michael I Jordan"], "venue": "Advances in psychology,", "citeRegEx": "Jordan.,? \\Q1997\\E", "shortCiteRegEx": "Jordan.", "year": 1997}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "In Proc. ICML,", "citeRegEx": "Jozefowicz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "ADAM: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Recurrent neural network structured output prediction for spoken language understanding", "author": ["Liu", "Lane2015] Bing Liu", "Ian Lane"], "venue": "In Proc. NIPS Workshop on Machine Learning for Spoken Language Understanding and Interactions", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Using recurrent neural networks for slot filling in spoken language", "author": ["Yann Dauphin", "Kaisheng Yao", "Yoshua Bengio", "Li Deng", "Dilek Hakkani-Tur", "Xiaodong He", "Larry Heck", "Gokhan Tur", "Dong Yu"], "venue": null, "citeRegEx": "Mesnil et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mesnil et al\\.", "year": 2015}, {"title": "Recurrent neural networks with external memory for language understanding", "author": ["Peng", "Yao2015] Baolin Peng", "Kaisheng Yao"], "venue": "arXiv preprint arXiv:1506.00195", "citeRegEx": "Peng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2015}, {"title": "Evaluation of spoken language systems: The ATIS domain", "author": ["Patti Price"], "venue": "In Proc. DARPA Speech and Natural Language Workshop,", "citeRegEx": "Price.,? \\Q1990\\E", "shortCiteRegEx": "Price.", "year": 1990}, {"title": "Generative and discriminative algorithms for spoken language understanding", "author": ["Raymond", "Riccardi2007] Christian Raymond", "Giuseppe Riccardi"], "venue": "In Proc. INTERSPEECH,", "citeRegEx": "Raymond et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Raymond et al\\.", "year": 2007}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": "In Proc. NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "What is left to be understood in ATIS", "author": ["Tur et al.2010] Gokhan Tur", "Dilek Hakkani-Tur", "Larry Heck"], "venue": "In Proc. SLT,", "citeRegEx": "Tur et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tur et al\\.", "year": 2010}, {"title": "Is it time to switch to word embedding and recurrent neural networks for spoken language understanding", "author": ["Christian Raymond", "Guillaume Gravier"], "venue": "In Proc. INTERSPEECH,", "citeRegEx": "Vukotic et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vukotic et al\\.", "year": 2015}, {"title": "Combining statistical and knowledge-based spoken language understanding in conditional models", "author": ["Wang et al.2006] Ye-Yi Wang", "Alex Acero", "Milind Mahajan", "John Lee"], "venue": "In Proc. COLING-ACL,", "citeRegEx": "Wang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2006}, {"title": "An efficient gradient-based algorithm for on-line training of recurrent network trajectories", "author": ["Williams", "Peng1990] Ronald J Williams", "Jing Peng"], "venue": "Neural Computation,", "citeRegEx": "Williams et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Williams et al\\.", "year": 1990}, {"title": "Convolutional neural network based triangular", "author": ["Xu", "Sarikaya2013] Puyang Xu", "Ruhi Sarikaya"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2013}, {"title": "Sequence-to-sequence neural net models for grapheme-to-phoneme conversion", "author": ["Yao", "Zweig2015] Kaisheng Yao", "Geoffrey Zweig"], "venue": "Proc. INTERSPEECH,", "citeRegEx": "Yao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2015}, {"title": "Recurrent neural networks for language understanding", "author": ["Yao et al.2013] Kaisheng Yao", "Geoffrey Zweig", "Mei-Yuh Hwang", "Yangyang Shi", "Dong Yu"], "venue": "In Proc. INTERSPEECH,", "citeRegEx": "Yao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2013}, {"title": "Spoken language understanding using long short-term memory neural networks", "author": ["Yao et al.2014a] Kaisheng Yao", "Baolin Peng", "Yu Zhang", "Dong Yu", "Geoffrey Zweig", "Yangyang Shi"], "venue": "In Proc. SLT,", "citeRegEx": "Yao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2014}, {"title": "Recurrent conditional random field for language understanding", "author": ["Yao et al.2014b] Kaisheng Yao", "Baolin Peng", "Geoffrey Zweig", "Dong Yu", "Xiaolong Li", "Feng Gao"], "venue": "In Proc. ICASSP,", "citeRegEx": "Yao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["Ilya Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1409.2329", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 15, "context": "Slot filling is a traditional task and tremendous efforts have been done, especially since the 1980s when the Defense Advanced Research Program Agency (DARPA) Airline Travel Information System (ATIS) projects started (Price, 1990).", "startOffset": 217, "endOffset": 230}, {"referenceID": 7, "context": "Following the success of deep learning (Hinton et al., 2006; Bengio, 2009), Recurrent Neural Network (RNN) (Elman, 1990; Jordan, 1997) and one of its specific architectures, Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), have been widely used since they can capture temporal dependencies (Yao et al.", "startOffset": 39, "endOffset": 74}, {"referenceID": 0, "context": "Following the success of deep learning (Hinton et al., 2006; Bengio, 2009), Recurrent Neural Network (RNN) (Elman, 1990; Jordan, 1997) and one of its specific architectures, Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), have been widely used since they can capture temporal dependencies (Yao et al.", "startOffset": 39, "endOffset": 74}, {"referenceID": 5, "context": ", 2006; Bengio, 2009), Recurrent Neural Network (RNN) (Elman, 1990; Jordan, 1997) and one of its specific architectures, Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), have been widely used since they can capture temporal dependencies (Yao et al.", "startOffset": 54, "endOffset": 81}, {"referenceID": 9, "context": ", 2006; Bengio, 2009), Recurrent Neural Network (RNN) (Elman, 1990; Jordan, 1997) and one of its specific architectures, Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), have been widely used since they can capture temporal dependencies (Yao et al.", "startOffset": 54, "endOffset": 81}, {"referenceID": 25, "context": ", 2006; Bengio, 2009), Recurrent Neural Network (RNN) (Elman, 1990; Jordan, 1997) and one of its specific architectures, Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), have been widely used since they can capture temporal dependencies (Yao et al., 2013; Yao et al., 2014a; Mesnil et al., 2015).", "startOffset": 254, "endOffset": 312}, {"referenceID": 13, "context": ", 2006; Bengio, 2009), Recurrent Neural Network (RNN) (Elman, 1990; Jordan, 1997) and one of its specific architectures, Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), have been widely used since they can capture temporal dependencies (Yao et al., 2013; Yao et al., 2014a; Mesnil et al., 2015).", "startOffset": 254, "endOffset": 312}, {"referenceID": 18, "context": "In the field of machine translation, an encoder-decoder LSTM has been gaining attention (Sutskever et al., 2014), where the encoder LSTM encodes the global information spanning over the whole input sentence in its last hidden state.", "startOffset": 88, "endOffset": 112}, {"referenceID": 13, "context": "In this paper, we first enhance the LSTM-based slot filling to explicitly model label dependencies by feeding the output label of the previous time step to the hidden state of the current time step, as Mesnil et al. (2015) and Liu and Lane (2015) tried with RNN.", "startOffset": 202, "endOffset": 223}, {"referenceID": 13, "context": "In this paper, we first enhance the LSTM-based slot filling to explicitly model label dependencies by feeding the output label of the previous time step to the hidden state of the current time step, as Mesnil et al. (2015) and Liu and Lane (2015) tried with RNN.", "startOffset": 202, "endOffset": 247}, {"referenceID": 10, "context": "The LSTM has the architecture based on Jozefowicz et al. (2015) that does not have peephole connections and yields the hidden state sequence hT1 .", "startOffset": 39, "endOffset": 64}, {"referenceID": 13, "context": "To explicitly model label dependencies, we introduce a new architecture, labeler LSTM (W+L), as shown in Figure 1(b), where the output label of previous time step is fed to the hidden state of current time step, jointly with words, as Mesnil et al. (2015) and Liu and Lane (2015) tried with RNN.", "startOffset": 235, "endOffset": 256}, {"referenceID": 13, "context": "To explicitly model label dependencies, we introduce a new architecture, labeler LSTM (W+L), as shown in Figure 1(b), where the output label of previous time step is fed to the hidden state of current time step, jointly with words, as Mesnil et al. (2015) and Liu and Lane (2015) tried with RNN.", "startOffset": 235, "endOffset": 280}, {"referenceID": 18, "context": "This encoder-labeler LSTM is motivated by the encoder-decoder LSTM that has been applied to machine translation (Sutskever et al., 2014), graphemeto-phoneme conversion (Yao and Zweig, 2015), and so on.", "startOffset": 112, "endOffset": 136}, {"referenceID": 15, "context": "1 Experimental Setup We used the ATIS corpus, which has been widely used as the benchmark for NLU (Price, 1990; Dahl et al., 1994; Wang et al., 2006; Tur et al., 2010).", "startOffset": 98, "endOffset": 167}, {"referenceID": 2, "context": "1 Experimental Setup We used the ATIS corpus, which has been widely used as the benchmark for NLU (Price, 1990; Dahl et al., 1994; Wang et al., 2006; Tur et al., 2010).", "startOffset": 98, "endOffset": 167}, {"referenceID": 21, "context": "1 Experimental Setup We used the ATIS corpus, which has been widely used as the benchmark for NLU (Price, 1990; Dahl et al., 1994; Wang et al., 2006; Tur et al., 2010).", "startOffset": 98, "endOffset": 167}, {"referenceID": 19, "context": "1 Experimental Setup We used the ATIS corpus, which has been widely used as the benchmark for NLU (Price, 1990; Dahl et al., 1994; Wang et al., 2006; Tur et al., 2010).", "startOffset": 98, "endOffset": 167}, {"referenceID": 13, "context": "In the following experiments, we randomly selected 80% of the original training data to train the model and used the remaining 20% as the heldout data (Mesnil et al., 2015).", "startOffset": 151, "endOffset": 172}, {"referenceID": 17, "context": "5 (Srivastava et al., 2014; Zaremba et al., 2014).", "startOffset": 2, "endOffset": 49}, {"referenceID": 28, "context": "5 (Srivastava et al., 2014; Zaremba et al., 2014).", "startOffset": 2, "endOffset": 49}, {"referenceID": 18, "context": "For the encoder-decoder LSTM, labeler LSTM(W+L), and encoder-labeler LSTM(W+L), we used the left-to-right beam search decoder (Sutskever et al., 2014) with beam sizes of 1, 2, 4, and 8 for evaluation where the best F1-score was reported.", "startOffset": 126, "endOffset": 150}, {"referenceID": 18, "context": "We used backward encoding for the encoder-decoder LSTM and the encoder-labeler LSTM as suggested in Sutskever et al. (2014). For the encoder-decoder LSTM, labeler LSTM(W+L), and encoder-labeler LSTM(W+L), we used the left-to-right beam search decoder (Sutskever et al.", "startOffset": 100, "endOffset": 124}, {"referenceID": 4, "context": "(2013) used RNN and outperformed methods that did not use neural networks, such as SVM (Raymond and Riccardi, 2007) and CRF (Deng et al., 2012).", "startOffset": 124, "endOffset": 143}, {"referenceID": 22, "context": "Yao et al. (2013) used RNN and outperformed methods that did not use neural networks, such as SVM (Raymond and Riccardi, 2007) and CRF (Deng et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 4, "context": "(2013) used RNN and outperformed methods that did not use neural networks, such as SVM (Raymond and Riccardi, 2007) and CRF (Deng et al., 2012). Mesnil et al. (2015) tried bi-directional RNN, but reported degradation comparing with their single-directional RNN (94.", "startOffset": 125, "endOffset": 166}, {"referenceID": 4, "context": "(2013) used RNN and outperformed methods that did not use neural networks, such as SVM (Raymond and Riccardi, 2007) and CRF (Deng et al., 2012). Mesnil et al. (2015) tried bi-directional RNN, but reported degradation comparing with their single-directional RNN (94.98%). Bi-directional RNN was not effective after using context window in this task. Yao et al. (2014a) introduced LSTM and deep LSTM and obtained improvement over RNN.", "startOffset": 125, "endOffset": 368}, {"referenceID": 4, "context": "(2013) used RNN and outperformed methods that did not use neural networks, such as SVM (Raymond and Riccardi, 2007) and CRF (Deng et al., 2012). Mesnil et al. (2015) tried bi-directional RNN, but reported degradation comparing with their single-directional RNN (94.98%). Bi-directional RNN was not effective after using context window in this task. Yao et al. (2014a) introduced LSTM and deep LSTM and obtained improvement over RNN. Peng and Yao (2015) proposed RNN-EM that used an external memory architecture to improve the memory capability of RNN.", "startOffset": 125, "endOffset": 453}, {"referenceID": 25, "context": "Liu and Lane (2015) used the output label for the previous word to model label dependencies F1-score RNN (Yao et al., 2013) 94.", "startOffset": 105, "endOffset": 123}, {"referenceID": 13, "context": "35 Bi-directional RNN (Mesnil et al., 2015) 94.", "startOffset": 22, "endOffset": 43}, {"referenceID": 13, "context": "89 Hybrid RNN (Mesnil et al., 2015) 95.", "startOffset": 14, "endOffset": 35}, {"referenceID": 11, "context": "Mesnil et al. (2015) used hybrid RNN that combined Elman-type and Jordan-type RNNs.", "startOffset": 0, "endOffset": 21}, {"referenceID": 5, "context": "(2015) used hybrid RNN that combined Elman-type and Jordan-type RNNs. Liu and Lane (2015) used the output label for the previous word to model label dependencies F1-score RNN (Yao et al.", "startOffset": 37, "endOffset": 90}, {"referenceID": 20, "context": "Vukotic et al. (2015) achieved 96.", "startOffset": 0, "endOffset": 22}, {"referenceID": 20, "context": "Vukotic et al. (2015) achieved 96.16% F1-score by using the named entity (NE) database when estimating word embeddings. Yao et al. (2013) and Yao et al.", "startOffset": 0, "endOffset": 138}, {"referenceID": 20, "context": "Vukotic et al. (2015) achieved 96.16% F1-score by using the named entity (NE) database when estimating word embeddings. Yao et al. (2013) and Yao et al. (2014a) used NE features in addition to word features and obtained improvement with both the RNN and LSTM.", "startOffset": 0, "endOffset": 161}], "year": 2016, "abstractText": "Recurrent Neural Network (RNN) and one of its specific architectures, Long Short-Term Memory (LSTM), have been widely used for sequence labeling. In this paper, we first enhance LSTM-based sequence labeling to explicitly model label dependencies. Then we propose another enhancement to incorporate the global information spanning over the whole input sequence. The latter proposed method, encoder-labeler LSTM, first encodes the whole input sequence into a fixed length vector with the encoder LSTM, and then uses this encoded vector as the initial state of another LSTM for sequence labeling. Combining these methods, we can predict the label sequence with considering label dependencies and information of whole input sequence. In the experiments of a slot filling task, which is an essential component of natural language understanding, with using the standard ATIS corpus, we achieved the state-of-the-art F1score of 95.66%.", "creator": "LaTeX with hyperref package"}}}