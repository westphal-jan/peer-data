{"id": "1601.01705", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jan-2016", "title": "Learning to Compose Neural Networks for Question Answering", "abstract": "We describe a question answering model that applies to both images and structured knowledge bases. The model uses natural language strings to automatically assemble neural networks from a collection of composable modules. Parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning, with only (world, question, answer) triples as supervision. Our approach, which we term a dynamic neural model network, achieves state-of-the-art results on benchmark datasets in both visual and structured domains.", "histories": [["v1", "Thu, 7 Jan 2016 21:21:59 GMT  (4810kb,D)", "http://arxiv.org/abs/1601.01705v1", null], ["v2", "Wed, 1 Jun 2016 18:20:37 GMT  (5947kb,D)", "http://arxiv.org/abs/1601.01705v2", null], ["v3", "Mon, 6 Jun 2016 01:44:25 GMT  (7152kb,D)", "http://arxiv.org/abs/1601.01705v3", null], ["v4", "Tue, 7 Jun 2016 23:25:51 GMT  (7293kb,D)", "http://arxiv.org/abs/1601.01705v4", null]], "reviews": [], "SUBJECTS": "cs.CL cs.CV cs.NE", "authors": ["jacob andreas", "marcus rohrbach", "trevor darrell", "dan klein"], "accepted": true, "id": "1601.01705"}, "pdf": {"name": "1601.01705.pdf", "metadata": {"source": "CRF", "title": "Learning to Compose Neural Networks for Question Answering", "authors": ["Jacob Andreas"], "emails": ["jda@eecs.berkeley.edu", "rohrbach@eecs.berkeley.edu", "trevor@eecs.berkeley.edu", "klein@eecs.berkeley.edu"], "sections": [{"heading": "1 Introduction", "text": "This paper presents a compositional, attentive model for answering questions about a variety of world representations, including images and structured knowledge databases. The model translates questions into dynamically composed neural networks and then applies these networks to world representations (images or knowledge databases) to produce answers. We use two largely independent workspaces: on the one hand, our model utilizes the best aspects of both linguistic composition and continuous representation. Our model consists of two components that are trained together: first, a collection of neural \"modules\" that can be freely composed (Figure 1b); second, a network layout predictor that assembles modules into complete deep networks tailored to each question (Figure 1a)."}, {"heading": "2 Deep networks as functional programs", "text": "In fact, most of them will be able to orient themselves in a direction in which they are in a position in which they are in."}, {"heading": "3 Related work", "text": "There is an extensive literature on answering questions in the database, in which strings are mapped to logical forms, which are then evaluated by a black box execution model to produce answers. Monitoring can be done either by commented logical forms (Wong and Mooney, 2007; Kwiatkowski et al., 2013) or by (World, Question, Answer) triples alone (Liang et al., 2011; Pasupat and Liang, 2015). Generally, the amount of primitive functions from which these logical forms can be composed is fixed, but a current work line focuses on generating new predictive functions, either by perceptual features (Krishnamurthy and Liang, 2013) or by the underlying scheme (Kwiatkowski et al., 2013). The model we describe in this paper has a uniform framework for dealing with both perspective and schematic cases, and differs from existing work."}, {"heading": "4 Model", "text": "This process includes the following variables: 1. w a world representation 2. x a question 3. y an answer 4. z a network layout 5. \u03b8 a collection of model parameters. Our model is based on two distributions: a layout model p (z | x; \u03b8 '), which selects a layout for a set, and an execution model pz (y | w; \u03b8e), which parameterizes the network specified by z on w.For easier representation, we present these models in reverse order. We first imagine that z is always observed, and describe in Section 4.1 how modules parameterized by phenomena with fixed structures are evaluated and learned. In Section 4.2, we move on to the real scenario, where z is unknown. We describe how to predict and learn layouts from questions without overseeing the layout."}, {"heading": "4.1 Evaluating modules", "text": "The question we have to ask ourselves is whether we are able to find a solution that relates to the world in a certain way and in a different way: we then define the execution model: pz (y) = (JzKw) y (1) (this assumes that the root module of z produces distribution via labels y.) The set of possible layouts z is constrained by module type constraints: some modules (as above) operate directly on the input representation, while others (as described above) depend on specific earlier modules."}, {"heading": "4.2 Assembling networks", "text": "Next, we describe the layout model p (z; x). First, we use a fixed syntactic approach to generate a small set of candidate layouts, analogous to the way a semantic grammar generates candidate semantic parses in previous work (Berant and Liang, 2014). A semantic approach differs from a syntactic approach in two ways. First, lexical elements must be mapped to a (possibly smaller) set of semantic primitives. Second, these semantic primitives must be combined into a structure that is narrow but not ex-accurate, the structure provided by syntax. For example, states and provinces must be identified with the same field, while all states have a capital that needs to be identified with the correct (in situ) quantifier scopy. While we cannot get around the structural selection problem, continuous representations simplify the lexical selection problem."}, {"heading": "5 Experiments", "text": "The framework described in this paper is general, and we are interested in how well it performs on datasets of different domains, sizes, and linguistic complexity. To this end, we evaluate our model for tasks at opposite extremes of these two criteria: a large visual dataset to answer questions, and a small collection of more structured geographical questions."}, {"heading": "5.1 Questions about images", "text": "Our first task is the recently introduced Visual Question Answering Challenge (VQA) (Antol et al., 2015). The VQA dataset consists of more than 200,000 images paired with human annotated questions and answers, as shown in Figure 4.We use the VQA 1.0 version, using the development set for model selection and hyperparameter tuning, and the final results from the evaluation server are set to the test standard. For the experiments described in this section, the input feature representations are pooled from the fifth revolutionary layer of a 16-layer VGNet (Simonyan and Zisserman, 2014), and input images are scaled to 448 \u00d7 448 before their representations are calculated. We found that the performance on this task was best when the candidate layouts were relatively simple, and you will find modules used, and layouts included in most of the two conjunctions."}, {"heading": "5.2 Questions about geography", "text": "The next set of experiments we are looking at focuses on GeoQA, a geographical task to answer questions first introduced by Krishnamurthy and Kollar (2013), which was originally paired with a visual question to answer the task, which is much simpler than the one just discussed and attractive for a number of reasons. In contrast to the VQA dataset, GeoQA is quite small and contains only 263 examples. Two basic principles are available: one using a classical semantic parser supported by a database, and another inducing logical predicates with linear classifiers on spatial and distributional characteristics, which allows us to evaluate the quality of our model relative to other perceptively grounded logical semantics and strictly logical approaches. GeoQA domains consist of a number of units (e.g. states, cities, parks) that participate in different relationships (e.g. north of, capital)."}, {"heading": "6 Conclusion", "text": "We have introduced a new model, the dynamic neural module network, for answering questions about both structured and unstructured information sources. As the model only provides triples as training data (question, world, answer), it learns to spontaneously assemble neural networks from an inventory of neural models, while simultaneously learning weights for these modules so that they can be assembled into novel structures. Our approach achieves state-of-the-art results in two tasks. We believe that the success of this work stems from two factors: Continuous representations improve the expressiveness and learnability of semantic parsers: By replacing discrete predicates with differentiable neural network fragments, we circumvent the challenging combinatory optimization problem associated with the induction of a semantic lexicon. In structured world representations, neural predicate representations allow the model to invent reusable attributes and relationships that are not expressed in the scheme of complex questions that may be even more complex in practice."}], "references": [{"title": "Semantic parsing as machine translation", "author": ["Jacob Andreas", "Andreas Vlachos", "Stephen Clark."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics, Sofia, Bulgaria.", "citeRegEx": "Andreas et al\\.,? 2013", "shortCiteRegEx": "Andreas et al\\.", "year": 2013}, {"title": "Deep compositional question answering with neural module networks", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein."], "venue": "arXiv:1511.02799.", "citeRegEx": "Andreas et al\\.,? 2015", "shortCiteRegEx": "Andreas et al\\.", "year": 2015}, {"title": "VQA: Visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C Lawrence Zitnick", "Devi Parikh."], "venue": "Proceedings of the International Conference on Computer Vision.", "citeRegEx": "Antol et al\\.,? 2015", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "Semantic parsing via paraphrasing", "author": ["Jonathan Berant", "Percy Liang."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics, volume 7, page 92.", "citeRegEx": "Berant and Liang.,? 2014", "shortCiteRegEx": "Berant and Liang.", "year": 2014}, {"title": "Question answering with subgraph embeddings", "author": ["Antoine Bordes", "Sumit Chopra", "Jason Weston."], "venue": "arXiv preprint arXiv:1406.3676.", "citeRegEx": "Bordes et al\\.,? 2014", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "From machine learning to machine reasoning", "author": ["L\u00e9on Bottou."], "venue": "Machine learning, 94(2):133\u2013149.", "citeRegEx": "Bottou.,? 2014", "shortCiteRegEx": "Bottou.", "year": 2014}, {"title": "The Stanford typed dependencies representation", "author": ["Marie-Catherine De Marneffe", "Christopher D Manning."], "venue": "Proceedings of the International Conference on Computational Linguistics, pages 1\u20138.", "citeRegEx": "Marneffe and Manning.,? 2008", "shortCiteRegEx": "Marneffe and Manning.", "year": 2008}, {"title": "Towards a formal distributional semantics: Simulating logical calculi with tensors", "author": ["Edward Grefenstette."], "venue": "arXiv preprint arXiv:1304.5823.", "citeRegEx": "Grefenstette.,? 2013", "shortCiteRegEx": "Grefenstette.", "year": 2013}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems, pages 1684\u20131692.", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "A neural network for factoid question answering over paragraphs", "author": ["Mohit Iyyer", "Jordan Boyd-Graber", "Leonardo Claudino", "Richard Socher", "Hal Daum\u00e9 III."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Iyyer et al\\.,? 2014", "shortCiteRegEx": "Iyyer et al\\.", "year": 2014}, {"title": "Jointly learning to parse and perceive: connecting natural language to the physical world", "author": ["Jayant Krishnamurthy", "Thomas Kollar."], "venue": "Transactions of the Association for Computational Linguistics.", "citeRegEx": "Krishnamurthy and Kollar.,? 2013", "shortCiteRegEx": "Krishnamurthy and Kollar.", "year": 2013}, {"title": "Vector space semantic parsing: A framework for compositional vector space models", "author": ["Jayant Krishnamurthy", "Tom Mitchell"], "venue": null, "citeRegEx": "Krishnamurthy and Mitchell.,? \\Q2013\\E", "shortCiteRegEx": "Krishnamurthy and Mitchell.", "year": 2013}, {"title": "Inducing probabilistic CCG grammars from logical form with higherorder unification", "author": ["Tom Kwiatkowski", "Luke Zettlemoyer", "Sharon Goldwater", "Mark Steedman."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Kwiatkowski et al\\.,? 2010", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2010}, {"title": "Scaling semantic parsers with onthe-fly ontology matching", "author": ["Tom Kwiatkowski", "Eunsol Choi", "Yoav Artzi", "Luke Zettlemoyer."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Kwiatkowski et al\\.,? 2013", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2013}, {"title": "Learning dependency-based compositional semantics", "author": ["Percy Liang", "Michael Jordan", "Dan Klein."], "venue": "Proceedings of the Human Language Technology Conference of the Association for Computational Linguistics, pages 590\u2013599, Portland, Oregon.", "citeRegEx": "Liang et al\\.,? 2011", "shortCiteRegEx": "Liang et al\\.", "year": 2011}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["Mateusz Malinowski", "Marcus Rohrbach", "Mario Fritz."], "venue": "Proceedings of the International Conference on Computer Vision, 12.", "citeRegEx": "Malinowski et al\\.,? 2015", "shortCiteRegEx": "Malinowski et al\\.", "year": 2015}, {"title": "A joint model of language and perception for grounded attribute learning", "author": ["Cynthia Matuszek", "Nicholas FitzGerald", "Luke Zettlemoyer", "Liefeng Bo", "Dieter Fox."], "venue": "arXiv preprint arXiv:1206.6423.", "citeRegEx": "Matuszek et al\\.,? 2012", "shortCiteRegEx": "Matuszek et al\\.", "year": 2012}, {"title": "Image question answering using convolutional neural network with dynamic parameter prediction", "author": ["Hyeonwoo Noh", "Paul Hongsuck Seo", "Bohyung Han."], "venue": "arXiv preprint arXiv:1511.05756.", "citeRegEx": "Noh et al\\.,? 2015", "shortCiteRegEx": "Noh et al\\.", "year": 2015}, {"title": "Compositional semantic parsing on semi-structured tables", "author": ["Panupong Pasupat", "Percy Liang."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Pasupat and Liang.,? 2015", "shortCiteRegEx": "Pasupat and Liang.", "year": 2015}, {"title": "Image question answering: A visual semantic embedding model and a new dataset", "author": ["Mengye Ren", "Ryan Kiros", "Richard S. Zemel."], "venue": "CoRR, abs/1505.02074.", "citeRegEx": "Ren et al\\.,? 2015", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K Simonyan", "A Zisserman."], "venue": "CoRR, abs/1409.1556.", "citeRegEx": "Simonyan and Zisserman.,? 2014", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Parsing with compositional vector grammars", "author": ["Richard Socher", "John Bauer", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams."], "venue": "Machine learning, 8(3-4):229\u2013256.", "citeRegEx": "Williams.,? 1992", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Learning synchronous grammars for semantic parsing with lambda calculus", "author": ["Yuk Wah Wong", "Raymond J. Mooney."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics, volume 45, page 960.", "citeRegEx": "Wong and Mooney.,? 2007", "shortCiteRegEx": "Wong and Mooney.", "year": 2007}, {"title": "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering", "author": ["Huijuan Xu", "Kate Saenko."], "venue": "arXiv preprint arXiv:1511.05234.", "citeRegEx": "Xu and Saenko.,? 2015", "shortCiteRegEx": "Xu and Saenko.", "year": 2015}, {"title": "Show, attend and tell: neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1502.03044.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Smola."], "venue": "arXiv preprint arXiv:1511.02274.", "citeRegEx": "Yang et al\\.,? 2015", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Neural enquirer: Learning to query tables", "author": ["Pengcheng Yin", "Zhengdong Lu", "Hang Li", "Ben Kao."], "venue": "arXiv preprint arXiv:1512.00965.", "citeRegEx": "Yin et al\\.,? 2015", "shortCiteRegEx": "Yin et al\\.", "year": 2015}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["Matthew D Zeiler."], "venue": "arXiv preprint arXiv:1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Simple baseline for visual question answering", "author": ["Bolei Zhou", "Yuandong Tian", "Sainbayar Sukhbaatar", "Arthur Szlam", "Rob Fergus."], "venue": "arXiv preprint arXiv:1512.02167.", "citeRegEx": "Zhou et al\\.,? 2015", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "Previous work has used manually-specified modular structures for visual learning (Andreas et al., 2015).", "startOffset": 81, "endOffset": 103}, {"referenceID": 25, "context": "This operation is commonly referred to as the attention mechanism, and is a standard tool for manipulating images (Xu et al., 2015) and text representations (Hermann et al.", "startOffset": 114, "endOffset": 131}, {"referenceID": 8, "context": ", 2015) and text representations (Hermann et al., 2015)", "startOffset": 33, "endOffset": 55}, {"referenceID": 0, "context": "Andreas et al. (2015) use hand-written rules to deterministically transform dependency trees into layouts, and restricted to producing simple structures like the above for non-synthetic data.", "startOffset": 0, "endOffset": 22}, {"referenceID": 23, "context": "Supervision may be provided either by annotated logical forms (Wong and Mooney, 2007; Kwiatkowski et al., 2010; Andreas et al., 2013) or from (world, question, answer) triples alone (Liang et al.", "startOffset": 62, "endOffset": 133}, {"referenceID": 12, "context": "Supervision may be provided either by annotated logical forms (Wong and Mooney, 2007; Kwiatkowski et al., 2010; Andreas et al., 2013) or from (world, question, answer) triples alone (Liang et al.", "startOffset": 62, "endOffset": 133}, {"referenceID": 0, "context": "Supervision may be provided either by annotated logical forms (Wong and Mooney, 2007; Kwiatkowski et al., 2010; Andreas et al., 2013) or from (world, question, answer) triples alone (Liang et al.", "startOffset": 62, "endOffset": 133}, {"referenceID": 14, "context": ", 2013) or from (world, question, answer) triples alone (Liang et al., 2011; Pasupat and Liang, 2015).", "startOffset": 56, "endOffset": 101}, {"referenceID": 18, "context": ", 2013) or from (world, question, answer) triples alone (Liang et al., 2011; Pasupat and Liang, 2015).", "startOffset": 56, "endOffset": 101}, {"referenceID": 10, "context": "In general the set of primitive functions from which these logical forms can be assembled is fixed, but one recent line of work focuses on inducing new predicates functions automatically, either from perceptual features (Krishnamurthy and Kollar, 2013) or the underlying schema (Kwiatkowski et al.", "startOffset": 220, "endOffset": 252}, {"referenceID": 13, "context": "In general the set of primitive functions from which these logical forms can be assembled is fixed, but one recent line of work focuses on inducing new predicates functions automatically, either from perceptual features (Krishnamurthy and Kollar, 2013) or the underlying schema (Kwiatkowski et al., 2013).", "startOffset": 278, "endOffset": 304}, {"referenceID": 9, "context": "These include approaches that model the task directly as a multiclass classification problem (Iyyer et al., 2014), models that attempt to embed questions and answers in a shared vector space (Bordes et al.", "startOffset": 93, "endOffset": 113}, {"referenceID": 4, "context": ", 2014), models that attempt to embed questions and answers in a shared vector space (Bordes et al., 2014) and attentional models that select words from documents sources (Hermann et al.", "startOffset": 85, "endOffset": 106}, {"referenceID": 8, "context": ", 2014) and attentional models that select words from documents sources (Hermann et al., 2015).", "startOffset": 72, "endOffset": 94}, {"referenceID": 4, "context": ", 2014), models that attempt to embed questions and answers in a shared vector space (Bordes et al., 2014) and attentional models that select words from documents sources (Hermann et al., 2015). Such approaches generally require that answers can be retrieved directly based on surface linguistic features, without requiring intermediate computation. A more structured approach described by Yin et al. (2015) learns a query execution model for database tables without any natural language component.", "startOffset": 86, "endOffset": 408}, {"referenceID": 4, "context": ", 2014), models that attempt to embed questions and answers in a shared vector space (Bordes et al., 2014) and attentional models that select words from documents sources (Hermann et al., 2015). Such approaches generally require that answers can be retrieved directly based on surface linguistic features, without requiring intermediate computation. A more structured approach described by Yin et al. (2015) learns a query execution model for database tables without any natural language component. Previous efforts toward unifying formal logic and representation learning include those of Grefenstette (2013) and Krishnamurthy and Mitchell (2013).", "startOffset": 86, "endOffset": 610}, {"referenceID": 4, "context": ", 2014), models that attempt to embed questions and answers in a shared vector space (Bordes et al., 2014) and attentional models that select words from documents sources (Hermann et al., 2015). Such approaches generally require that answers can be retrieved directly based on surface linguistic features, without requiring intermediate computation. A more structured approach described by Yin et al. (2015) learns a query execution model for database tables without any natural language component. Previous efforts toward unifying formal logic and representation learning include those of Grefenstette (2013) and Krishnamurthy and Mitchell (2013).", "startOffset": 86, "endOffset": 648}, {"referenceID": 20, "context": "The visually-grounded component of this work relies on recent advances in convolutional networks for computer vision (Simonyan and Zisserman, 2014), and in particular the fact that late convolutional layers in networks trained for image recognition contain rich features useful for other downstream vision tasks, while preserving spatial information.", "startOffset": 117, "endOffset": 147}, {"referenceID": 25, "context": "These features have been used for both image captioning (Xu et al., 2015) and visual question answering (Yang et al.", "startOffset": 56, "endOffset": 73}, {"referenceID": 26, "context": ", 2015) and visual question answering (Yang et al., 2015).", "startOffset": 38, "endOffset": 57}, {"referenceID": 19, "context": "Most previous approaches to visual question answering either apply a recurrent model to deep representations of both the image and the question (Ren et al., 2015; Malinowski et al., 2015), or use the question to compute an attention over the input image, and then answer based on both the question and the image features attended to (Yang et al.", "startOffset": 144, "endOffset": 187}, {"referenceID": 15, "context": "Most previous approaches to visual question answering either apply a recurrent model to deep representations of both the image and the question (Ren et al., 2015; Malinowski et al., 2015), or use the question to compute an attention over the input image, and then answer based on both the question and the image features attended to (Yang et al.", "startOffset": 144, "endOffset": 187}, {"referenceID": 26, "context": ", 2015), or use the question to compute an attention over the input image, and then answer based on both the question and the image features attended to (Yang et al., 2015; Xu and Saenko, 2015).", "startOffset": 153, "endOffset": 193}, {"referenceID": 24, "context": ", 2015), or use the question to compute an attention over the input image, and then answer based on both the question and the image features attended to (Yang et al., 2015; Xu and Saenko, 2015).", "startOffset": 153, "endOffset": 193}, {"referenceID": 12, "context": ", 2015; Malinowski et al., 2015), or use the question to compute an attention over the input image, and then answer based on both the question and the image features attended to (Yang et al., 2015; Xu and Saenko, 2015). Other approaches include the simple classification model described by Zhou et al. (2015) and the dynamic parameter prediction network described by Noh et al.", "startOffset": 8, "endOffset": 309}, {"referenceID": 12, "context": ", 2015; Malinowski et al., 2015), or use the question to compute an attention over the input image, and then answer based on both the question and the image features attended to (Yang et al., 2015; Xu and Saenko, 2015). Other approaches include the simple classification model described by Zhou et al. (2015) and the dynamic parameter prediction network described by Noh et al. (2015). All of these models assume that a fixed computation can be performed on the image and question to compute the answer, rather than adapting the structure of the computation to the question.", "startOffset": 8, "endOffset": 385}, {"referenceID": 0, "context": "As noted, Andreas et al. (2015) previously considered a simple generalization of these attentional approaches in which small variations in the network structure per-question were permitted, with the structure chosen by (deterministic) syntactic processing of questions.", "startOffset": 10, "endOffset": 32}, {"referenceID": 0, "context": "As noted, Andreas et al. (2015) previously considered a simple generalization of these attentional approaches in which small variations in the network structure per-question were permitted, with the structure chosen by (deterministic) syntactic processing of questions. Other approaches in this general family include the \u201cuniversal parser\u201d sketched by Bottou (2014), and the recursive neural networks of Socher et al.", "startOffset": 10, "endOffset": 367}, {"referenceID": 0, "context": "As noted, Andreas et al. (2015) previously considered a simple generalization of these attentional approaches in which small variations in the network structure per-question were permitted, with the structure chosen by (deterministic) syntactic processing of questions. Other approaches in this general family include the \u201cuniversal parser\u201d sketched by Bottou (2014), and the recursive neural networks of Socher et al. (2013), which use a fixed tree structure to perform further linguistic analysis without any external world representation.", "startOffset": 10, "endOffset": 426}, {"referenceID": 3, "context": "We first use a fixed syntactic parse to generate a small set of candidate layouts, analogously to the way a semantic grammar generates candidate semantic parses in previous work (Berant and Liang, 2014).", "startOffset": 178, "endOffset": 202}, {"referenceID": 10, "context": "As our approach includes both categories, relations and simple quantification, the range of phenomena considered is generally broader than previous perceptually-grounded QA work (Krishnamurthy and Kollar, 2013; Matuszek et al., 2012).", "startOffset": 178, "endOffset": 233}, {"referenceID": 16, "context": "As our approach includes both categories, relations and simple quantification, the range of phenomena considered is generally broader than previous perceptually-grounded QA work (Krishnamurthy and Kollar, 2013; Matuszek et al., 2012).", "startOffset": 178, "endOffset": 233}, {"referenceID": 22, "context": "Williams (1992) showed that the gradient of the reward surface J with respect to the parameters of the policy is", "startOffset": 0, "endOffset": 16}, {"referenceID": 28, "context": "\u03b8e and \u03b8` are optimized using ADADELTA (Zeiler, 2012) with \u03c1 = 0.", "startOffset": 39, "endOffset": 53}, {"referenceID": 2, "context": "Our first task is the recently-introduced Visual Question Answering challenge (VQA) (Antol et al., 2015).", "startOffset": 84, "endOffset": 104}, {"referenceID": 0, "context": "NMN is the parameter-tying model from Andreas et al. (2015), while NMN* is a reimplementation using the same image processing pipeline as D-NMN.", "startOffset": 38, "endOffset": 60}, {"referenceID": 20, "context": "(Simonyan and Zisserman, 2014).", "startOffset": 0, "endOffset": 30}, {"referenceID": 29, "context": "We achieve state-of-theart results on this task, outperforming a highly effective visual bag-of-words model (Zhou et al., 2015), a model with dynamic network parameter prediction (but fixed network structure) (Noh et al.", "startOffset": 108, "endOffset": 127}, {"referenceID": 17, "context": ", 2015), a model with dynamic network parameter prediction (but fixed network structure) (Noh et al., 2015), and a previous approach using neural module networks with no structure prediction (Andreas et al.", "startOffset": 89, "endOffset": 107}, {"referenceID": 1, "context": ", 2015), and a previous approach using neural module networks with no structure prediction (Andreas et al., 2015).", "startOffset": 91, "endOffset": 113}, {"referenceID": 26, "context": "A more conventional attentional model has also been applied to this task (Yang et al., 2015); while we also What is in the sheep\u2019s ear? What color is she wearing? What is the man dragging?", "startOffset": 73, "endOffset": 92}, {"referenceID": 10, "context": "The next set of experiments we consider focuses on GeoQA, a geographical question-answering task first introduced by Krishnamurthy and Kollar (2013). This task was originally paired with a visual question answering task much simpler than the one just discussed, and is appealing for a number of reasons.", "startOffset": 117, "endOffset": 149}, {"referenceID": 10, "context": "Our dynamic model (D-NMN) outperforms both the logical (LSP-F) and perceptual models (LSP-W) described by (Krishnamurthy and Kollar, 2013), as well as a fixed-structure neural module net (NMN).", "startOffset": 106, "endOffset": 138}], "year": 2016, "abstractText": "We describe a question answering model that applies to both images and structured knowledge bases.The model uses natural language strings to automatically assemble neural networks from a collection of composable modules. Parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning, with only (world, question, answer) triples as supervision. Our approach, which we term a dynamic neural module network, achieves state-of-the-art results on benchmark datasets in both visual and structured domains.", "creator": "LaTeX with hyperref package"}}}