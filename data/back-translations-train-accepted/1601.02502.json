{"id": "1601.02502", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jan-2016", "title": "Trans-gram, Fast Cross-lingual Word-embeddings", "abstract": "We introduce Trans-gram, a simple and computationally-efficient method to simultaneously learn and align wordembeddings for a variety of languages, using only monolingual data and a smaller set of sentence-aligned data. We use our new method to compute aligned wordembeddings for twenty-one languages using English as a pivot language. We show that some linguistic features are aligned across languages for which we do not have aligned data, even though those properties do not exist in the pivot language. We also achieve state of the art results on standard cross-lingual text classification and word translation tasks.", "histories": [["v1", "Mon, 11 Jan 2016 16:12:32 GMT  (315kb)", "http://arxiv.org/abs/1601.02502v1", "EMNLP 2015"]], "COMMENTS": "EMNLP 2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jocelyn coulmance", "jean-marc marty", "guillaume wenzek", "amine benhalloum"], "accepted": true, "id": "1601.02502"}, "pdf": {"name": "1601.02502.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Jean-Marc Marty", "Guillaume Wenzek"], "emails": ["joc@proxem.com", "jmm@proxem.com", "guw@proxem.com", "aba@proxem.com"], "sections": [{"heading": null, "text": "ar Xiv: 160 1.02 502v 1 [cs.C L] 11. Jan 2016We introduce Trans-gram, a simple and computationally efficient method of learning and aligning word beds for a variety of languages simultaneously, using only monolingual data and a smaller set of sentence beds. We use our new method to calculate aligned word beds for twenty-one languages using English as the pivot language. We show that some linguistic features are aligned across languages for which we do not have aligned data, although these characteristics do not exist in the pivot language."}, {"heading": "1 Introduction", "text": "Word embedding is a representation of words with specified vectors. It is a distributed representation (Hinton, 1984) in the sense that there is not necessarily a 1: 1 correspondence between vector dimensions and linguistic properties, and the linguistic properties are distributed along the dimensions of the space.A popular method for calculating word embedding is the Skip-gram model (Mikolov et al., 2013a), which learns high-quality word embedding with much less computational effort than previous methods, allowing the processing of very important data sets. For example, a 1.6 billion word dataset can be created in less than one day.Several authors have come up with different methods to align word embedding in two languages (Klementiev et al., 2012; Mikolov et al., 2013b; Lauly et al., 2014; Gouws et al., 2015)."}, {"heading": "2 Review of Previous Work", "text": "These methods have two objectives: first, to assign similar representations (i.e. spatially close) to similar words (i.e. \"semantically close\") within each language - but this is the monolingual goal; second, to assign similar representations across languages - this is the lingual goal. The simplest approach is to separate the monolingual optimization task from the cross-border optimization task, as is the case, for example, in (Mikolov et al., 2013b). The idea is to train two sets of word embeddings for each language separately and then make a parametric estimate of the allocation between word embeddings across languages. This method has been expanded by (Faruqui and Dyer, 2014)."}, {"heading": "3 From Skip-Gram to Trans-Gram", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Skip-gram", "text": "We will briefly introduce the Skip-gram algorithm, as we will need it for further explanations. Skipgram allows to train word embedding for a language using monolingual data. This method uses a dual representation for words. Each word w has two embedding: a target vector, ~ w (\u0435RD), and a context vector, ~ w (\u0435RD). The algorithm attempts to estimate the probability that a word w will appear in the context of a word c. More precisely, we learn the embedding ~ w, ~ c, so that: \u03c3 (~ w \u00b7 ~ c) = P (w | c), where \u03c3 is the sigmoid function. A simplified version of the loss function minimized by Skip-gram is the following: J =, s, \"s, C\" s, s, s, s, c, [w \u2212 l: w \u2212 l] s, [w \u2212 l] s, [w \u2212 l] s, now [w] l \u2212 l], the phrase that is \u2212 logging, (w), and \u2212 c, where \u2212 is a phrase."}, {"heading": "3.2 Trans-gram", "text": "In this section, we present a new method for calculating aligned word embedding for a variety of languages. Our method will minimize the sum of monolingual losses and cross-border losses. As in BilBOWA (Gouws et al., 2015), we use Skipgram as a monolingual loss. Suppose we are trying to learn aligned word vectors for languages e (e.g. English) and f (e.g. French), we note Je and Jf the bilingual losses. In BilBOWA, the cross-border loss function is a distance between the representations of terms of two aligned sentences. But as we showed (Levy and Goldberg, 2014), the Skipgram loss function extracts interesting linguistic features, we wanted to use a loss function for the cross-border f-language target."}, {"heading": "4 Implementation", "text": "In our experiments we used the corpora aligned with Europarl (Koehn, 2005). Europarl-v7 has two peculiarities: First, the corpora is aligned at sentence level; second, each language pair contains English as one of its members: For example, there is no French-Italian pair. In other words, English is used as a pivot language; neither bilingual lexicographs nor other bilingual datasets aligned at word level were used. Using only the Europarl-v7 texts as monolingual and bilingual data, it took 10 minutes, two languages and two and a half hours to align the 21 languages of the corpus in a 40-dimensional space on a 6-core computer."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Reuters Cross-lingual Document Classification", "text": "We used a subset of the English and German sections of the Reuters RCV1 / RCV2 Corpora (Lewis and Li, 2004) (10,000 documents each), as in (Klementiev et al., 2012), and we replicated the experimental setting. In the English dataset, there are four topics: CCAT (Corporate / Industrial), ECAT (Economics), GCAT (Government / Social) and MCAT (Markets). We used these topics as our labels and selected only documents labeled with a single topic. We trained our classifier on the articles of a language in which each document was represented with a weighted sum of the vectors of its words, and then tested it on the articles of the other language. The classifier used an average perceptron, and we used the implementation of (Klementiev et al., 2012) The word vectors were compiled on the Europarl-v7 parallel model."}, {"heading": "5.2 P@k Word Translation", "text": "Next, we evaluated our method on a word translation task that was introduced in (Mikolov et al., 2013b) and used in (Gouws et al., 2015). The words were extracted from the publicly available WMT112 corpus and translated with Google Translate. The experiments were performed for two groups of translations: English to Spanish and Spanish to English. (Mikolov et al., 2013b) The experiments were extracted for the top 6K most common words and translated with Google Translate. They used the top 5K pairs to train a translation matrix: English to Spanish to English and Spanish to English. (Mikolov et al., 2013b) The experiments were extracted for the top 6K most common words and translated with Google Translate. They used the top 5K pairs of EEEEK to train a translation matrix, and evaluated their method on the remaining 1K. As our English and1Thanks to S. Gouws for providing this implementation 2http: / www.statmt.org / wmt11 / EVEVt11 / Method En En VEn \u2192 En EVEn EVEVEVEVEVEVEVEVEVEVEVEVEVEVEEVEVEVEVEVEVEVEVEVEVEVEVEVEVEVEVEVEVEVEVEVEVEVEVEVEVEVEVEVEVEVEEV77EV71.1% EV71.1% EVEV71.1% EVEVEVEVEVEVEVEVEV71.1% EVEVEVEVEVEVEVEVEV71.1% EVEVEVEVEVEVEVEVEVEVEVEVEVEVEVEV71.1% EVEVEVEVEVEVEVEVEVEVEVEVEVEVEVEVEVEVEVEVEVEVEVEVEVEV"}, {"heading": "6 Interesting properties", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Cross-lingual disambiguation", "text": "The idea of our method is to look for a language in which the undesirable senses are represented by unambiguous words, and then perform some arithmetic operations. Let's illustrate the process with a concrete example: Consider the French word \"train,\" trainfr. The three Polish words closest to the word \"trainfr\" translate in English into \"now,\" \"a train\" and \"when.\" This seems to be a bad match. In fact, trainfr is ambiguous. It can name a number of railway carriages, but is also used to form progressive tenses. French \"Il est en train de manger\" translates into \"he is eating,\" or in Italian \"sta mangiando.\" Just as the Italian word \"rail\" is used to form progressive trains, it is a good candidate to transform the train into a better sense."}, {"heading": "6.2 Transfer of linguistic features", "text": "Another interesting characteristic of the vectors generated by Trans-gram is the transmission of linguistic characteristics by a pivot language that does not possess these characteristics. Let us clarify this by focusing on Latin languages that have some characteristics that English does not possess, such as rich conjugations. For example, in French and Italian, the infinitives of \"essen\" are mangerfr and mangiamoit, and the first plural persons are mangeonsfr and mangiamoit. In fact, we observe the following orientations in our models: ~ mangerfr \u2248 ~ mangiareit and ~ mangeonsfr \u2248 ~ mangiamoit. It is therefore noteworthy that characteristics that do not exist in English coincide in languages aligned by English as the only pivot language. We also found similar translations for the genders of adjectives and are currently investigating other similar characteristics that are captured by Trans-gram."}, {"heading": "7 Conclusion", "text": "In this paper, we have presented the following contributions: Trans-gram, a new method for calculating cross-language word embedding in a single word space; state-of-the-art in cross-language NLP tasks; a sketch of a lingual calculation to clearly define multilingual words; the issuance of linguistic character transfers by a key language that does not possess these features. We are still in the process of researching promising properties of the generated vectors and their application in other NLP tasks (Sentiment Analysis, NER...)."}], "references": [{"title": "Polyglot: Distributed word representations for multilingual nlp", "author": ["Al-Rfou et al.2013] Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena"], "venue": "arXiv preprint arXiv:1307.1662", "citeRegEx": "Al.Rfou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2013}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["Faruqui", "Dyer2014] Manaal Faruqui", "Chris Dyer"], "venue": "Association for Computational Linguistics", "citeRegEx": "Faruqui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2014}, {"title": "Bilbowa: Fast bilingual distributed representations without word alignments", "author": ["Gouws et al.2015] Stephan Gouws", "Yoshua Bengio", "Greg Corrado"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Gouws et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gouws et al\\.", "year": 2015}, {"title": "Multilingual distributed representations without word alignment", "author": ["Hermann", "Blunsom2013] Karl Moritz Hermann", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1312.6173", "citeRegEx": "Hermann et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2013}, {"title": "Inducing crosslingual distributed representations of words", "author": ["Ivan Titov", "Binod Bhattarai"], "venue": null, "citeRegEx": "Klementiev et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "A parallel corpus for statistical machine translation", "author": ["Philipp Koehn"], "venue": "In MT Summit", "citeRegEx": "Koehn.,? \\Q2005\\E", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "An autoencoder approach to learning bilingual word representations", "author": ["Hugo Larochelle", "Mitesh Khapra", "Balaraman Ravindran", "Vikas C Raykar", "Amrita Saha"], "venue": "In Advances in Neural Information Processing Sys-", "citeRegEx": "Lauly et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lauly et al\\.", "year": 2014}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Levy", "Goldberg2014] Omer Levy", "Yoav Goldberg"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["Lewis", "Li2004] Y", "Rose T", "D. D", "Yang", "F. Li"], "venue": "In Journal of Machine Learning Research", "citeRegEx": "Lewis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2004}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Exploiting similarities among languages for machine translation", "author": ["Quoc V Le", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1309.4168", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 4, "context": "Several authors came up with different methods to align word-embeddings across two languages (Klementiev et al., 2012; Mikolov et al., 2013b; Lauly et al., 2014; Gouws et al., 2015).", "startOffset": 93, "endOffset": 181}, {"referenceID": 6, "context": "Several authors came up with different methods to align word-embeddings across two languages (Klementiev et al., 2012; Mikolov et al., 2013b; Lauly et al., 2014; Gouws et al., 2015).", "startOffset": 93, "endOffset": 181}, {"referenceID": 2, "context": "Several authors came up with different methods to align word-embeddings across two languages (Klementiev et al., 2012; Mikolov et al., 2013b; Lauly et al., 2014; Gouws et al., 2015).", "startOffset": 93, "endOffset": 181}, {"referenceID": 6, "context": "This was explored in (Hermann and Blunsom, 2013; Lauly et al., 2014) where every couple of aligned sentences is transformed into two fixed-size vectors.", "startOffset": 21, "endOffset": 68}, {"referenceID": 4, "context": "(Klementiev et al., 2012) proved this approach to be useful by obtaining state-of-the-art results on several tasks.", "startOffset": 0, "endOffset": 25}, {"referenceID": 2, "context": "(Gouws et al., 2015) extends their work with a more computationally-efficient implementation.", "startOffset": 0, "endOffset": 20}, {"referenceID": 2, "context": "Like in BilBOWA (Gouws et al., 2015), we use Skipgram as a mono-lingual loss.", "startOffset": 16, "endOffset": 36}, {"referenceID": 5, "context": "In our experiments, we used the Europarl (Koehn, 2005) aligned corpora.", "startOffset": 41, "endOffset": 54}, {"referenceID": 0, "context": "We also computed 300 dimensions vectors using the Wikipedia extracts provided by (Al-Rfou et al., 2013) as monolingual data for each language.", "startOffset": 81, "endOffset": 103}, {"referenceID": 4, "context": "We used a subset of the English and German sections of the Reuters RCV1/RCV2 corpora (Lewis and Li, 2004) (10000 documents each), as in (Klementiev et al., 2012), and we replicated the experimental setting.", "startOffset": 136, "endOffset": 161}, {"referenceID": 4, "context": "The classifier used was an averaged perceptron, and we used the implementation from (Klementiev et al., 2012)1.", "startOffset": 84, "endOffset": 109}, {"referenceID": 2, "context": "The previous state of the art results were detained (Gouws et al., 2015) with BilBOWA and (Lauly et al.", "startOffset": 52, "endOffset": 72}, {"referenceID": 6, "context": ", 2015) with BilBOWA and (Lauly et al., 2014) with their Bilingual Autoencoder model.", "startOffset": 25, "endOffset": 45}, {"referenceID": 2, "context": ", 2013b) and used in (Gouws et al., 2015).", "startOffset": 21, "endOffset": 41}], "year": 2016, "abstractText": "We introduce Trans-gram, a simple and computationally-efficient method to simultaneously learn and align wordembeddings for a variety of languages, using only monolingual data and a smaller set of sentence-aligned data. We use our new method to compute aligned wordembeddings for twenty-one languages using English as a pivot language. We show that some linguistic features are aligned across languages for which we do not have aligned data, even though those properties do not exist in the pivot language. We also achieve state of the art results on standard cross-lingual text classification and word translation tasks.", "creator": "LaTeX with hyperref package"}}}