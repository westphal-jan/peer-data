{"id": "1601.03855", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2016", "title": "A Relative Exponential Weighing Algorithm for Adversarial Utility-based Dueling Bandits", "abstract": "We study the K-armed dueling bandit problem which is a variation of the classical Multi-Armed Bandit (MAB) problem in which the learner receives only relative feedback about the selected pairs of arms. We propose a new algorithm called Relative Exponential-weight algorithm for Exploration and Exploitation (REX3) to handle the adversarial utility-based formulation of this problem. This algorithm is a non-trivial extension of the Exponential-weight algorithm for Exploration and Exploitation (EXP3) algorithm. We prove a finite time expected regret upper bound of order O(sqrt(K ln(K)T)) for this algorithm and a general lower bound of order omega(sqrt(KT)). At the end, we provide experimental results using real data from information retrieval applications.", "histories": [["v1", "Fri, 15 Jan 2016 09:50:07 GMT  (1637kb,D)", "http://arxiv.org/abs/1601.03855v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["pratik gajane", "tanguy urvoy", "fabrice cl\u00e9rot"], "accepted": true, "id": "1601.03855"}, "pdf": {"name": "1601.03855.pdf", "metadata": {"source": "META", "title": "A Relative Exponential Weighing Algorithm for Adversarial Utility-based Dueling Bandits (extended version)", "authors": ["Pratik Gajane", "Tanguy Urvoy", "Fabrice Cl\u00e9rot"], "emails": ["PRATIK.GAJANE@ORANGE.COM", "TANGUY.URVOY@ORANGE.COM", "FABRICE.CLEROT@ORANGE.COM"], "sections": [{"heading": null, "text": "\u221a K ln (K) T) for this algorithm and a general lower limit of the order \u0438 (\u221a KT). Finally, we provide experimental results using real data from information gathering applications."}, {"heading": "1. Introduction", "text": "The K-armed dueling bandit problem is a variation of the classic Multi-Armed Bandit (MAB) problem introduced by Yue and Joachims (2009) to formalize the exploration / exploitation dilemma in learning from preference feedback. In its use-based formulation, the learner sees only the result of the duel between the selected arms (i.e. the feedback indicates which of the selected arms has a better value) and receives the average of the rewards of the selected arms. The difficulty of this problem stems from the fact that the learning algorithms have no way to directly observe the reward of their actions, which is a perfect example of a partial surveillance problem as in Piccoli and Schindelhauer (2001)."}, {"heading": "2. Previous Work and Notations", "text": "The conventional MAB problem has been well studied in both stochastic and (forgotten) adversarial environments (cf. Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012) These MAB algorithms are designed to optimize exploration and exploitation to control the cumulative regret that represents the difference between winning a reference strategy and actually winning the algorithm."}, {"heading": "2.1. Exponential-weight algorithm for Exploration and Exploitation", "text": "Of particular interest is the Exponential Weight Algorithm for Exploration and Exploitation (EXP3) and its variants for the opposing bandit environment presented by Auer et al. (2002b). For a fixed horizon T and K weapons, the EXP3 algorithm delivers an expected cumulative regret limit of the order O (\u221a K ln (K) T) against the best single-armed strategy. This algorithm is indeed counterproductive because it does not require stochastic assumption of the rewards, but this is not always the case because it requires knowledge of the horizon T to function properly. A \"doubling trick\" solution is proposed by Auer et al. (2002b) to maintain the limit of regret when T is unknown. It consists of executing the EXP3 in a carefully designed sequence of increasing epochs. Another elegant solution was proposed later by Rare (2012) for the same purpose al."}, {"heading": "2.2. Previous work on stochastic dueling bandits", "text": "The question of the way in which people have to deal with the question, how they should deal with the question, how they should deal with the question, how they should deal with the question, how they should deal with the question, how they should deal with the question, how they should deal with the question, how they should deal with the question, how they should deal with the question, how they should deal with the question, how they should deal with the question, how they should deal with the question, how they should deal with the question, how they should deal with the question, how they should deal with the question, how they should deal with the question, how they should deal with the question, how they should deal with the question, how they should deal with the question, whether they should deal with the question, how they should deal with the question, how they should deal with the question, how they should deal with the question, how they should deal with the question, how they should deal with the question, how they should deal with the question, how they should deal with the question, how they should deal with the question, how they should deal with the question, whether they should deal with the question, how they should deal with the question, whether they should deal with the question, how should deal with the question, how should deal with the question, whether they should deal with the question, whether they should deal with the question, whether they should deal with the question, whether they should deal with the question, whether they should deal with the question, how should deal with the question, how should deal with the question, how should deal with the question, how they should deal with the question, how they should deal with the question, how should deal with the question, whether they should deal with the question, whether they should ask themselves, whether they should ask the question, whether they should ask themselves, whether they should ask themselves, whether they should ask themselves, whether they should ask themselves whether they should ask themselves, whether they should ask themselves whether they are asking themselves, whether they are asking themselves whether they are asking themselves, whether they are asking themselves are asking themselves whether they are asking themselves, whether they are asking themselves whether they are asking themselves, whether they are asking themselves are asking themselves, whether they are asking themselves, whether they are asking themselves are asking themselves, whether they are asking themselves, whether they are asking themselves are asking themselves, whether they are asking whether they are asking themselves, whether they are"}, {"heading": "2.3. Adversarial dueling bandits", "text": "The results of the dueling bandits, however, are only partially discernible. (...) In this context, as in the classical comparison, the environment is mainly focused on the monitoring of utility and reward vectors x (t),.., xK (t)).,.,.,.,.,.,.,.,.,.,.,.,.,.,..,...,...,......,......,......,...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "3. Relative Exponential-weight Algorithm for Exploration and Exploitation", "text": "The pseudo-code for the algorithm we propose is given in algorithm 1 (this becomes a problem). As explained earlier in Section 2.3, this algorithm is designed to be applied to opposing service-based duels with bandits. \u2212 \u2212 pK (t) This is a mixture of a standardized weighting of arms wi / p and a uniform distribution 1 / K. As in EXP3, this uniform probability is introduced to ensure a minimum exploration of all weapons. \u2212 pK (t) The algorithms draw two arms a and b independently after p (t). In step 8, the algorithm gets a uniform distribution 1 / K. As in EXP3, this uniform probability is introduced to ensure a minimum exploration of all weapons. In step 7, the algorithm will draw two arms a and b independently of each other."}, {"heading": "4. Analysis", "text": "For analysis, we focus on the simple case where the identity (t + 1) is the identity. It provides ternary win / tie / loss feedback (if we assume binary rewards).The main difference between EXP3 and our algorithm is in steps 10 and 11 of algorithm 1, where we update weights according to the outcome of the duel: the winning arm is satisfied while the loser is punished.This \"punitive\" approach of exponential balancing starts from EXP3 and other balancing algorithms that satisfy the most rewarding arms, while we kindly ignore the non-rewarding ones (Freund und Schapire, 1999; Cesa-Bianchi and Lugosi, 2006).4.1 Limit for REX3In this section, we offer a finite horizon without stochastic upper limit for the expected regret against the best individual action policy.The steps 10-11 on algorithm 1 are equivalent to any form of updating (+ 1)."}, {"heading": "4.2. Lower bound for dueling bandits algorithms", "text": "To create a lower limit on the regret of all dueling bandits, we use a reduction to the classic MAB problem proposed by Ailon et al. (2014). Algorithm 2 reduction to the classic MAB 1: DBA.init () 2: Set t = 1 3: repeat 4: (at, bt + 1) \u2190 DBA.decide () 5: xat \u2190 CBE.get reward () 6: xbt + 1 \u2190 CBE.get reward () 7: DBA.update (((at, bt + 1), (xat \u2212 xbt + 1)) 8: t = t + 2 9: until t \u2265 TAlgorithm 2 gives an explicit formulation of this reduction by using a generic dueling bandit algorithm (DBA) as a black box with the following public subroutures: init (), decide () and update () bandit environment (CBE)."}, {"heading": "5. Experiments", "text": "To evaluate REX3 and other dueling bandit algorithms, we applied them to the online comparison of search engine rankings by interleaved filtering (Radlinski and Joachims, 2007). A search engine ranking is a function that ranks a collection of documents according to their relevance to a given search query by the user. By linking the results of two rankings together and tracking which ranking the user clicked on, we are able to get an unbiased feedback on the relative quality of these two rankings. Given the K-ranking, the problem of finding the best ranking is actually a K-armed duel between bandits. In order to obtain reproducible and comparable results, we adopted the stochastic matrix-based test setup already used by Yue and Joachims (2011); Zoghi et al al al. (2014a; b; 2015) with both cumulative matrices as defined by Yue al."}, {"heading": "5.1. Empirical validation of Corollary 1", "text": "We used LETOR NP2004 and MSLR30K datasets (traced back to 64 ranks) to compare the average condorcet regret of 100 passes of REX3 with T = 105 with the corresponding halved 3 theoretical limits of sequence 1 for different values of \u03b3. Results of this experiment are Summa-3 As mentioned at the end of Section 2.2, the benefit-based regret of the bandit is actually twice as great as the condorcet regret according to Figure 1. We drew two theoretical curves: one with conservative Gmax = T / 2 and a riskier one with Gmax = T / 4. This experiment illustrates the double effect of the \u03b3 parameter on the exploration / exploitation compromise: a low value reduces both the exploration and the reactivity of the algorithm to unexpected feedback and a high value tends to unify the exploration while increasing the mutability of the upper limit (even for a theoretical upper limit of 6)."}, {"heading": "5.2. Interleave filtering simulations", "text": "For our experiments, we considered the following state-of-the-art algorithms: BTM (Yue and Joachims, 2011) with \u03b3 = 1.1 and \u03b4 = 1 / T (exploration-then-exploit setting), Condorcet-SAVAGE (Urvoy et al., 2013) with \u03b4 = 1 / T Reue, RUCB (Zoghi et al., 2014a) with \u03b1 = 0.51, and SPARRING coupled with EXP3 (Ailon et al., 2014). We also took as a basis the uniform sampling strategy RANDOM. We consider three versions of REX3: two versions of REX3 not always available, in which the optimal GOP is calculated in advance, corresponding to (6) with Gmax Set or T / 10 and a version at any time in which we are recalculated (6)."}, {"heading": "6. Conclusion", "text": "We proposed REX3, an exponential weighing algorithm for dueling bandits based on opposing utilities. We provided both an upper and a lower limit for the expected cumulative regret. These two limits correspond to the original limits of the classic EXP3 algorithm. A thorough empirical study of several sets of data for obtaining information confirmed the validity of these theoretical results. Furthermore, it was shown that REX3, and in particular its readily available version with adaptive \u03b3, are competitive solutions for dueling bandits, even when compared with stochastically setting algorithms in a stochastic environment."}, {"heading": "A. Proof Sketch for Theorem 1", "text": "The general structure of the evidence is similar to that of (Auer et al., 2002b, Section 3), but, as explained above, the estimator we use differs from that of EXP3 because it contains an instant estimate of regret rather than an absolute estimate of reward. An unknown reader can therefore refer to the extended version for step-by-step details."}, {"heading": "Acknowledgments", "text": "We thank the reviewers for their constructive comments and Masrour Zoghi, who kindly sent us his experiment data."}, {"heading": "B. Detailed Proof of Theorem 1", "text": "For better readability we simply write a, b instead of a, b instead of a, bt = a, b = a, b = a, b =, b =, b =, c =, c =, c =, c =, c =, c =, c =, c (t), c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c = t), c (t), c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c =, c ="}, {"heading": "C. Further Experiments", "text": "We give here the simulation results that could not match the core article: Figure 4 provides results for a smaller number of anchors on the NP2004 dataset, and Figure 5 completes the experiments on the MSLR30K dataset. On Figure 6, we added an experiment that we performed with sparring in conjunction with UCB. We also included two artificial matrices: SAVAGE and BVS. The 30-30-SAVAGE matrix, defined by Pi, j = 12 + j / (2K) for i < j as described in (Urvoy et al., 2013). The 20-20-20-BVS matrix is defined by: P1, j = 0.51 for each j > 1 and Pi, j = 1 for each 1 < i < j. Your Condorcet winner has a low Borda value (9.69 vs. 18.49 for the Borda winner), which makes it difficult for algorithms to find."}], "references": [{"title": "Reducing dueling bandits to cardinal bandits", "author": ["N. Ailon", "Z.S. Karnin", "T. Joachims"], "venue": "In ICML 2014,", "citeRegEx": "Ailon et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ailon et al\\.", "year": 2014}, {"title": "Finitetime analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": null, "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "A near-optimal algorithm for finite partial-monitoring games against adversarial opponents", "author": ["G. Bart\u00f3k"], "venue": "In Proc. COLT", "citeRegEx": "Bart\u00f3k,? \\Q2013\\E", "shortCiteRegEx": "Bart\u00f3k", "year": 2013}, {"title": "Partial monitoring - classification, regret bounds, and algorithms", "author": ["G. Bart\u00f3k", "D.P. Foster", "D. P\u00e1l", "A. Rakhlin", "C. Szepesv\u00e1ri"], "venue": null, "citeRegEx": "Bart\u00f3k et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bart\u00f3k et al\\.", "year": 2014}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S. Bubeck", "N. Cesa-Bianchi"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bubeck and Cesa.Bianchi,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Cesa.Bianchi", "year": 2012}, {"title": "A survey of preference-based online learning with bandit algorithms", "author": ["R. Busa-Fekete", "E. H\u00fcllermeier"], "venue": "ALT", "citeRegEx": "Busa.Fekete and H\u00fcllermeier,? \\Q2014\\E", "shortCiteRegEx": "Busa.Fekete and H\u00fcllermeier", "year": 2014}, {"title": "Preference-based rank elicitation using statistical models: The case of mallows", "author": ["R. Busa-Fekete", "E. H\u00fcllermeier", "B. Sz\u00f6r\u00e9nyi"], "venue": "ICML", "citeRegEx": "Busa.Fekete et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Busa.Fekete et al\\.", "year": 2014}, {"title": "Top-k selection based on adaptive sampling of noisy preferences", "author": ["R. Busa-Fekete", "B. Sz\u00f6r\u00e9nyi", "W. Cheng", "P. Weng", "E. H\u00fcllermeier"], "venue": "In ICML 2013,", "citeRegEx": "Busa.Fekete et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Busa.Fekete et al\\.", "year": 2013}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi", "year": 2006}, {"title": "Combinatorial bandits", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": "COLT", "citeRegEx": "Cesa.Bianchi and Lugosi,? \\Q2009\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi", "year": 2009}, {"title": "Large-scale validation and analysis of interleaved search evaluation", "author": ["O. Chapelle", "T. Joachims", "F. Radlinski", "Y. Yue"], "venue": "ACM Trans. Inf. Syst.,", "citeRegEx": "Chapelle et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2012}, {"title": "An updated survey on the linear ordering problem for weighted or unweighted tournaments", "author": ["I. Charon", "O. Hudry"], "venue": "Annals OR,", "citeRegEx": "Charon and Hudry,? \\Q2010\\E", "shortCiteRegEx": "Charon and Hudry", "year": 2010}, {"title": "An efficient boosting algorithm for combining preferences", "author": ["Y. Freund", "R. Iyer", "R.E. Schapire", "Y. Singer"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Freund et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Freund et al\\.", "year": 2003}, {"title": "Adaptive game playing using multiplicative weights", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Games and Economic Behavior,", "citeRegEx": "Freund and Schapire,? \\Q1999\\E", "shortCiteRegEx": "Freund and Schapire", "year": 1999}, {"title": "Evaluating the accuracy of implicit feedback from clicks and query reformulations in web search", "author": ["T. Joachims", "L. Granka", "B. Pan", "H. Hembrooke", "F. Radlinski", "G. Gay"], "venue": "ACM Trans. Inf. Syst.,", "citeRegEx": "Joachims et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Joachims et al\\.", "year": 2007}, {"title": "Noisy binary search and its applications", "author": ["R.M. Karp", "R. Kleinberg"], "venue": "SODA", "citeRegEx": "Karp and Kleinberg,? \\Q2007\\E", "shortCiteRegEx": "Karp and Kleinberg", "year": 2007}, {"title": "Learning to rank for information retrieval", "author": ["Liu", "T.-Y"], "venue": "Found. Trends Inf. Retr.,", "citeRegEx": "Liu and T..Y.,? \\Q2009\\E", "shortCiteRegEx": "Liu and T..Y.", "year": 2009}, {"title": "LETOR: Benchmark dataset for research on learning to rank for information retrieval", "author": ["Liu", "T.-Y", "J. Xu", "T. Qin", "W. Xiong", "H. Li"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2007}, {"title": "Discrete prediction games with arbitrary feedback and loss", "author": ["A. Piccolboni", "C. Schindelhauer"], "venue": "In COLT/EuroCOLT,", "citeRegEx": "Piccolboni and Schindelhauer,? \\Q2001\\E", "shortCiteRegEx": "Piccolboni and Schindelhauer", "year": 2001}, {"title": "Active exploration for learning rankings from clickthrough data", "author": ["F. Radlinski", "T. Joachims"], "venue": "KDD", "citeRegEx": "Radlinski and Joachims,? \\Q2007\\E", "shortCiteRegEx": "Radlinski and Joachims", "year": 2007}, {"title": "Evaluation and analysis of the performance of the exp3 Algorithm in stochastic environments", "author": ["Y. Seldin", "C. Szepesv\u00e1ri", "P. Auer", "Y. Abbasi-Yadkori"], "venue": "In EWRL, volume 24 of JMLR Proceedings,", "citeRegEx": "Seldin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Seldin et al\\.", "year": 2012}, {"title": "Generic exploration and K-armed voting bandits", "author": ["T. Urvoy", "F. Clerot", "R. F\u00e9raud", "S. Naamane"], "venue": "In ICML 2013,", "citeRegEx": "Urvoy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Urvoy et al\\.", "year": 2013}, {"title": "The k-armed dueling bandits problem", "author": ["Y. Yue", "J. Broder", "R. Kleinberg", "T. Joachims"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "Yue et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2012}, {"title": "Interactively optimizing information retrieval systems as a dueling bandits problem", "author": ["Y. Yue", "T. Joachims"], "venue": "ICML", "citeRegEx": "Yue and Joachims,? \\Q2009\\E", "shortCiteRegEx": "Yue and Joachims", "year": 2009}, {"title": "Beat the mean bandit", "author": ["Y. Yue", "T. Joachims"], "venue": "ICML", "citeRegEx": "Yue and Joachims,? \\Q2011\\E", "shortCiteRegEx": "Yue and Joachims", "year": 2011}, {"title": "MergeRUCB: A method for large-scale online ranker evaluation", "author": ["M. Zoghi", "S. Whiteson", "M. de Rijke"], "venue": "WSDM", "citeRegEx": "Zoghi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zoghi et al\\.", "year": 2015}, {"title": "Relative upper confidence bound for the karmed dueling bandit problem", "author": ["M. Zoghi", "S. Whiteson", "R. Munos", "M. de Rijke"], "venue": "In ICML 2014,", "citeRegEx": "Zoghi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zoghi et al\\.", "year": 2014}, {"title": "Relative confidence sampling for efficient online ranker evaluation", "author": ["M. Zoghi", "S.A. Whiteson", "M. de Rijke", "R. Munos"], "venue": "WSDM", "citeRegEx": "Zoghi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zoghi et al\\.", "year": 2014}, {"title": "Further Experiments We give here the simulation results that could not fit on the core article: Figure 4 gives results for smaller number of rankers on NP2004 dataset and Figure 5 complete the experiments on MSLR30K dataset. On Figure 6 we added an experiment we made with Sparring coupled with UCB", "author": [], "venue": "SAVAGE", "citeRegEx": "C.,? \\Q2004\\E", "shortCiteRegEx": "C.", "year": 2004}], "referenceMentions": [{"referenceID": 20, "context": "The accuracy of this interleave filtering method was highlighted in several experimental studies (Radlinski and Joachims, 2007; Joachims et al., 2007; Chapelle et al., 2012).", "startOffset": 97, "endOffset": 173}, {"referenceID": 15, "context": "The accuracy of this interleave filtering method was highlighted in several experimental studies (Radlinski and Joachims, 2007; Joachims et al., 2007; Chapelle et al., 2012).", "startOffset": 97, "endOffset": 173}, {"referenceID": 11, "context": "The accuracy of this interleave filtering method was highlighted in several experimental studies (Radlinski and Joachims, 2007; Joachims et al., 2007; Chapelle et al., 2012).", "startOffset": 97, "endOffset": 173}, {"referenceID": 20, "context": "Introduction The K-armed dueling bandit problem is a variation of the classical Multi-Armed Bandit (MAB) problem introduced by Yue and Joachims (2009) to formalize the exploration/exploitation dilemma in learning from preference feedback.", "startOffset": 127, "endOffset": 151}, {"referenceID": 17, "context": "This is a perfect example of partial monitoring problem as defined in Piccolboni and Schindelhauer (2001). Proceedings of the 32 International Conference on Machine Learning, Lille, France, 2015.", "startOffset": 70, "endOffset": 106}, {"referenceID": 17, "context": "This is a perfect example of partial monitoring problem as defined in Piccolboni and Schindelhauer (2001). Proceedings of the 32 International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s). Relative feedback is naturally suited to many practical applications like user-perceived product preferences, where a relative perception: \u201cA is better than B\u201d is easier to obtain than its absolute counterpart: \u201cA value is 42, B is worth 33\u201d. Another important application of dueling bandits comes from information retrieval systems where users provide implicit feedback about the provided results. This implicit feedback is collected in various ways e.g. a click on a link, a tap, or any monitored action of the user. In all these ways however, this kind of feedback is often strongly biased by the model itself (the user cannot click on a link which was not proposed). To remove this bias in search engines, Radlinski and Joachims (2007) propose to interleave the outputs of different ranking models: the model which scores a click wins the duel.", "startOffset": 70, "endOffset": 991}, {"referenceID": 0, "context": "We prove a finite time expected regret upper bound of order O( \u221a K ln(K)T ) and develop an argument initially proposed by Ailon et al. (2014) to exhibit a general lower bound of order \u03a9( \u221a KT ) for this problem.", "startOffset": 122, "endOffset": 142}, {"referenceID": 5, "context": "Previous Work and Notations The conventional MAB problem has been well studied in the stochastic setting as well as the (oblivious) adversarial setting (see Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012).", "startOffset": 152, "endOffset": 218}, {"referenceID": 13, "context": "This problem also falls under the framework of preference learning (Freund et al., 2003; Liu, 2009; F\u00fcrnkranz and H\u00fcllermeier, 2010) which deals with learning of (predictive) preference models from observed (or extracted) preference information i.", "startOffset": 67, "endOffset": 132}, {"referenceID": 1, "context": "Exponential-weight algorithm for Exploration and Exploitation Of particular interest are the Exponential-weight algorithm for Exploration and Exploitation (EXP3) and its variants presented by Auer et al. (2002b) for the adversarial bandit setting.", "startOffset": 192, "endOffset": 212}, {"referenceID": 1, "context": "Exponential-weight algorithm for Exploration and Exploitation Of particular interest are the Exponential-weight algorithm for Exploration and Exploitation (EXP3) and its variants presented by Auer et al. (2002b) for the adversarial bandit setting. For a fixed horizon T and K arms, the EXP3 algorithm provides an expected cumulative regret bound of order O( \u221a K ln(K)T ) against the best single-arm strategy. This algorithm is indeed adversarial because it does not require a stochastic assumption on the rewards. It is although not anytime because it requires the knowledge of the horizon T to run properly. A \u201cdoubling trick\u201d solution is proposed by Auer et al. (2002b) to preserve the regret bound when T is unknown.", "startOffset": 192, "endOffset": 672}, {"referenceID": 1, "context": "Exponential-weight algorithm for Exploration and Exploitation Of particular interest are the Exponential-weight algorithm for Exploration and Exploitation (EXP3) and its variants presented by Auer et al. (2002b) for the adversarial bandit setting. For a fixed horizon T and K arms, the EXP3 algorithm provides an expected cumulative regret bound of order O( \u221a K ln(K)T ) against the best single-arm strategy. This algorithm is indeed adversarial because it does not require a stochastic assumption on the rewards. It is although not anytime because it requires the knowledge of the horizon T to run properly. A \u201cdoubling trick\u201d solution is proposed by Auer et al. (2002b) to preserve the regret bound when T is unknown. It consists of running EXP3 in a carefully designed sequence of increasing epochs. Another elegant solution was later proposed by Seldin et al. (2012) for the same purpose.", "startOffset": 192, "endOffset": 871}, {"referenceID": 1, "context": "Exponential-weight algorithm for Exploration and Exploitation Of particular interest are the Exponential-weight algorithm for Exploration and Exploitation (EXP3) and its variants presented by Auer et al. (2002b) for the adversarial bandit setting. For a fixed horizon T and K arms, the EXP3 algorithm provides an expected cumulative regret bound of order O( \u221a K ln(K)T ) against the best single-arm strategy. This algorithm is indeed adversarial because it does not require a stochastic assumption on the rewards. It is although not anytime because it requires the knowledge of the horizon T to run properly. A \u201cdoubling trick\u201d solution is proposed by Auer et al. (2002b) to preserve the regret bound when T is unknown. It consists of running EXP3 in a carefully designed sequence of increasing epochs. Another elegant solution was later proposed by Seldin et al. (2012) for the same purpose. The notation \u00d5(\u00b7) hides logarithmic factors. 2.2. Previous work on stochastic dueling bandits The dueling bandits problem is recent, although related to previous works on computing with noisy comparison (see for instance Karp and Kleinberg, 2007). This problem also falls under the framework of preference learning (Freund et al., 2003; Liu, 2009; F\u00fcrnkranz and H\u00fcllermeier, 2010) which deals with learning of (predictive) preference models from observed (or extracted) preference information i.e. relative feedback which specifies which of the chosen alternatives is preferred. Most of the articles hitherto published on dueling bandits consider the problem under a stochastic assumption. Yue and Joachims (2009) propose an algorithm called Dueling Bandit Gradient Descent (DBGD) to solve a version of the dueling bandits problem where context information is provided.", "startOffset": 192, "endOffset": 1607}, {"referenceID": 1, "context": "Exponential-weight algorithm for Exploration and Exploitation Of particular interest are the Exponential-weight algorithm for Exploration and Exploitation (EXP3) and its variants presented by Auer et al. (2002b) for the adversarial bandit setting. For a fixed horizon T and K arms, the EXP3 algorithm provides an expected cumulative regret bound of order O( \u221a K ln(K)T ) against the best single-arm strategy. This algorithm is indeed adversarial because it does not require a stochastic assumption on the rewards. It is although not anytime because it requires the knowledge of the horizon T to run properly. A \u201cdoubling trick\u201d solution is proposed by Auer et al. (2002b) to preserve the regret bound when T is unknown. It consists of running EXP3 in a carefully designed sequence of increasing epochs. Another elegant solution was later proposed by Seldin et al. (2012) for the same purpose. The notation \u00d5(\u00b7) hides logarithmic factors. 2.2. Previous work on stochastic dueling bandits The dueling bandits problem is recent, although related to previous works on computing with noisy comparison (see for instance Karp and Kleinberg, 2007). This problem also falls under the framework of preference learning (Freund et al., 2003; Liu, 2009; F\u00fcrnkranz and H\u00fcllermeier, 2010) which deals with learning of (predictive) preference models from observed (or extracted) preference information i.e. relative feedback which specifies which of the chosen alternatives is preferred. Most of the articles hitherto published on dueling bandits consider the problem under a stochastic assumption. Yue and Joachims (2009) propose an algorithm called Dueling Bandit Gradient Descent (DBGD) to solve a version of the dueling bandits problem where context information is provided. They approach (contextual) dueling bandits as an on-line convex optimization problem. Yue et al. (2012) propose an algorithm called Interleaved Flitering (IF).", "startOffset": 192, "endOffset": 1867}, {"referenceID": 1, "context": "Exponential-weight algorithm for Exploration and Exploitation Of particular interest are the Exponential-weight algorithm for Exploration and Exploitation (EXP3) and its variants presented by Auer et al. (2002b) for the adversarial bandit setting. For a fixed horizon T and K arms, the EXP3 algorithm provides an expected cumulative regret bound of order O( \u221a K ln(K)T ) against the best single-arm strategy. This algorithm is indeed adversarial because it does not require a stochastic assumption on the rewards. It is although not anytime because it requires the knowledge of the horizon T to run properly. A \u201cdoubling trick\u201d solution is proposed by Auer et al. (2002b) to preserve the regret bound when T is unknown. It consists of running EXP3 in a carefully designed sequence of increasing epochs. Another elegant solution was later proposed by Seldin et al. (2012) for the same purpose. The notation \u00d5(\u00b7) hides logarithmic factors. 2.2. Previous work on stochastic dueling bandits The dueling bandits problem is recent, although related to previous works on computing with noisy comparison (see for instance Karp and Kleinberg, 2007). This problem also falls under the framework of preference learning (Freund et al., 2003; Liu, 2009; F\u00fcrnkranz and H\u00fcllermeier, 2010) which deals with learning of (predictive) preference models from observed (or extracted) preference information i.e. relative feedback which specifies which of the chosen alternatives is preferred. Most of the articles hitherto published on dueling bandits consider the problem under a stochastic assumption. Yue and Joachims (2009) propose an algorithm called Dueling Bandit Gradient Descent (DBGD) to solve a version of the dueling bandits problem where context information is provided. They approach (contextual) dueling bandits as an on-line convex optimization problem. Yue et al. (2012) propose an algorithm called Interleaved Flitering (IF). Their formulation is stochastic and matrixbased: for each pair (i, j) of arms, there is an unknown probability Pi,j for i to win against j. This preference matrix P of size K\u00d7K must satisfy the following symetry property: \u2200i, j \u2208 {1, . . . ,K}, Pi,j + Pj,i = 1 (1) Hence on the diagonal: Pi,i = 12 \u2200i \u2208 {1, . . . ,K}. Let i\u2217 be the \u201cbest arm\u201d (as we will see later, this best arm coincides with the notion of Condorcet winner). Yue et al. (2012) define the regret incurred at the time instant t when arms a and b are pulled as: r\u2032 a,b = Pi\u2217,a + Pi\u2217,b \u2212 1 2 \u2208 (0, 1 2 ) (2)", "startOffset": 192, "endOffset": 2369}, {"referenceID": 23, "context": "Yue and Joachims (2011) introduce Beat The Mean (BTM), an algorithm which proceeds by successive elimination of arms.", "startOffset": 0, "endOffset": 24}, {"referenceID": 22, "context": "Urvoy et al. (2013) propose a generic algorithm called SAVAGE (for Sensitivity Analysis of VAriables for Generic Exploration) which does away with several assumptions made in the previous algorithms e.", "startOffset": 0, "endOffset": 20}, {"referenceID": 12, "context": "The key notions they introduce for dueling bandits are the Copeland, Borda and Condorcet scores (Charon and Hudry, 2010).", "startOffset": 96, "endOffset": 120}, {"referenceID": 9, "context": "The key notions they introduce for dueling bandits are the Copeland, Borda and Condorcet scores (Charon and Hudry, 2010). The Borda score of an arm i on a preference matrix P is \u2211K j=1 Pi,j and its Copeland score is \u2211K j=1JPi,j > 1 2K (We use J . . .K to denote the indicator function). If an arm has a Copeland score of K \u2212 1, which means that it defeats all the other arms in the long run, it is called a Condorcet winner. The existence of a Condorcet winner is the minimum assumption required for the Condorcet regret as defined on equation (2) to be applicable. There exists however some datasets like MSLR30K (2012) where this Condorcet condition is not satisfied.", "startOffset": 97, "endOffset": 621}, {"referenceID": 9, "context": "The key notions they introduce for dueling bandits are the Copeland, Borda and Condorcet scores (Charon and Hudry, 2010). The Borda score of an arm i on a preference matrix P is \u2211K j=1 Pi,j and its Copeland score is \u2211K j=1JPi,j > 1 2K (We use J . . .K to denote the indicator function). If an arm has a Copeland score of K \u2212 1, which means that it defeats all the other arms in the long run, it is called a Condorcet winner. The existence of a Condorcet winner is the minimum assumption required for the Condorcet regret as defined on equation (2) to be applicable. There exists however some datasets like MSLR30K (2012) where this Condorcet condition is not satisfied. Zoghi et al. (2014a) extend the Upper Confidence Bound (UCB) algorithm (Auer et al.", "startOffset": 97, "endOffset": 691}, {"referenceID": 0, "context": "Ailon et al. (2014) propose three methods (DOUBLER, MULTISMB, and SPARRING) to reduce the stochastic utility-based dueling bandits problem to the conventional MAB problem.", "startOffset": 0, "endOffset": 20}, {"referenceID": 8, "context": "We can cite (Busa-Fekete et al., 2013; 2014; Zoghi et al., 2014b; 2015).", "startOffset": 12, "endOffset": 71}, {"referenceID": 6, "context": "See also (Busa-Fekete and H\u00fcllermeier, 2014) for an extensive survey of this domain.", "startOffset": 9, "endOffset": 44}, {"referenceID": 19, "context": "As mentionned earlier, the dueling bandits problem is a special instance of a partial monitoring game (Piccolboni and Schindelhauer, 2001; Cesa-Bianchi and Lugosi, 2009; Bart\u00f3k et al., 2014).", "startOffset": 102, "endOffset": 190}, {"referenceID": 10, "context": "As mentionned earlier, the dueling bandits problem is a special instance of a partial monitoring game (Piccolboni and Schindelhauer, 2001; Cesa-Bianchi and Lugosi, 2009; Bart\u00f3k et al., 2014).", "startOffset": 102, "endOffset": 190}, {"referenceID": 4, "context": "As mentionned earlier, the dueling bandits problem is a special instance of a partial monitoring game (Piccolboni and Schindelhauer, 2001; Cesa-Bianchi and Lugosi, 2009; Bart\u00f3k et al., 2014).", "startOffset": 102, "endOffset": 190}, {"referenceID": 0, "context": "A utility-based formulation of the problem is however proposed in Ailon et al. (2014, section 6). In this setting, as in classical adversarial MAB, the environment chooses beforehand an horizon T and a sequence of utility/reward vectors x(t) = (x1(t), . . . , xK(t)) \u2208 [0, 1] for t = 1, . . . , T . The learning algorithm aims at controling the bandit regret against the best single-arm strategy, as defined in (3), by choosing properly the pairs of arms (i, j) to be compared. To tackle this problem, Ailon et al. (2014) suggest to apply the SPARRING reduction algorithm, although originally designed for stochastic settings, with an adversarial bandit algorithm like EXP3 as a black-box MAB.", "startOffset": 66, "endOffset": 522}, {"referenceID": 3, "context": "If we except GLOBALEXP3 (Bart\u00f3k, 2013) which tries to capture more finely the structure of the games, these algorithms only focus on the time bound and perform inefficiently when the number of actions grows.", "startOffset": 24, "endOffset": 38}, {"referenceID": 14, "context": "This \u2018punitive\u2019 approach of exponential weighing departs from EXP3 and other weighing algorithms which gratify the most rewarding arms while kindly ignoring the nonrewarding ones (Freund and Schapire, 1999; Cesa-Bianchi and Lugosi, 2006).", "startOffset": 179, "endOffset": 237}, {"referenceID": 9, "context": "This \u2018punitive\u2019 approach of exponential weighing departs from EXP3 and other weighing algorithms which gratify the most rewarding arms while kindly ignoring the nonrewarding ones (Freund and Schapire, 1999; Cesa-Bianchi and Lugosi, 2006).", "startOffset": 179, "endOffset": 237}, {"referenceID": 0, "context": "Lower bound for dueling bandits algorithms To provide a lower bound on the regret of any dueling bandits algorithm, we use a reduction to the classical MAB problem suggested by Ailon et al. (2014).", "startOffset": 177, "endOffset": 197}, {"referenceID": 20, "context": "Experiments To evaluate REX3 and other dueling bandits algorithms, we have applied them to the online comparison of rankers for search engines by interleaved filtering (Radlinski and Joachims, 2007).", "startOffset": 168, "endOffset": 198}, {"referenceID": 25, "context": "We used several preference matrices issued from namely: ARXIV dataset (Yue and Joachims, 2011), LETOR NP2004 dataset (Liu et al.", "startOffset": 70, "endOffset": 94}, {"referenceID": 18, "context": "We used several preference matrices issued from namely: ARXIV dataset (Yue and Joachims, 2011), LETOR NP2004 dataset (Liu et al., 2007), and MSLR30K dataset.", "startOffset": 117, "endOffset": 135}, {"referenceID": 19, "context": "Experiments To evaluate REX3 and other dueling bandits algorithms, we have applied them to the online comparison of rankers for search engines by interleaved filtering (Radlinski and Joachims, 2007). A search-engine ranker is a function that orders a collection of documents according to their relevancy to a given user search query. By interleaving the output of two rankers and tracking on which ranker\u2019s output the user did click, we are able to get an unbiased feedback about the relative quality of these two rankers. Given K rankers, the problem of finding the best ranker is indeed a K-armed dueling bandits. In order to obtain reproducible and comparable results, we adopted the stochastic matrix-based experiment setup already employed by Yue and Joachims (2011); Zoghi et al.", "startOffset": 169, "endOffset": 772}, {"referenceID": 19, "context": "Experiments To evaluate REX3 and other dueling bandits algorithms, we have applied them to the online comparison of rankers for search engines by interleaved filtering (Radlinski and Joachims, 2007). A search-engine ranker is a function that orders a collection of documents according to their relevancy to a given user search query. By interleaving the output of two rankers and tracking on which ranker\u2019s output the user did click, we are able to get an unbiased feedback about the relative quality of these two rankers. Given K rankers, the problem of finding the best ranker is indeed a K-armed dueling bandits. In order to obtain reproducible and comparable results, we adopted the stochastic matrix-based experiment setup already employed by Yue and Joachims (2011); Zoghi et al. (2014a;b; 2015) with both the cumulative Condorcet regret as defined by Yue et al. (2012) and the accuracy i.", "startOffset": 169, "endOffset": 876}, {"referenceID": 18, "context": "We used several preference matrices issued from namely: ARXIV dataset (Yue and Joachims, 2011), LETOR NP2004 dataset (Liu et al., 2007), and MSLR30K dataset. The last dataset distinguishes three kinds of queries: informational, navigational and perfecthit navigational (MSLR30K, 2012). These matrices are courtesy of Zoghi et al. (2014b)\u2019s authors.", "startOffset": 118, "endOffset": 338}, {"referenceID": 25, "context": "Interleave filtering simulations For our experiments we have considered the following state of the art algorithms: BTM (Yue and Joachims, 2011) with \u03b3 = 1.", "startOffset": 119, "endOffset": 143}, {"referenceID": 22, "context": "1 and \u03b4 = 1/T (explore-then-exploit setting), Condorcet-SAVAGE (Urvoy et al., 2013) with \u03b4 = 1/T , RUCB (Zoghi et al.", "startOffset": 63, "endOffset": 83}, {"referenceID": 0, "context": "51, and SPARRING coupled with EXP3 (Ailon et al., 2014).", "startOffset": 35, "endOffset": 55}], "year": 2016, "abstractText": "We study the K-armed dueling bandit problem which is a variation of the classical Multi-Armed Bandit (MAB) problem in which the learner receives only relative feedback about the selected pairs of arms. We propose an efficient algorithm called Relative Exponential-weight algorithm for Exploration and Exploitation (REX3) to handle the adversarial utility-based formulation of this problem. We prove a finite time expected regret upper bound of order O( \u221a K ln(K)T ) for this algorithm and a general lower bound of order \u03a9( \u221a KT ). At the end, we provide experimental results using real data from information retrieval applications.", "creator": "LaTeX with hyperref package"}}}