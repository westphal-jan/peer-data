{"id": "1601.03916", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2016", "title": "Multimodal Pivots for Image Caption Translation", "abstract": "We present an approach to improve statistical machine translation of image descriptions by multimodal pivots defined in visual space. Image similarity is computed by a convolutional neural network and incorporated into a target-side translation memory retrieval model where descriptions of most similar images are used to rerank translation outputs. Our approach does not depend on the availability of in-domain parallel data and achieves improvements of 1.4 BLEU over strong baselines.", "histories": [["v1", "Fri, 15 Jan 2016 13:42:04 GMT  (547kb,D)", "https://arxiv.org/abs/1601.03916v1", "in submission"], ["v2", "Mon, 21 Mar 2016 13:47:26 GMT  (461kb,D)", "http://arxiv.org/abs/1601.03916v2", "domain adaptation, VGG16 visual features"], ["v3", "Mon, 13 Jun 2016 16:52:09 GMT  (464kb,D)", "http://arxiv.org/abs/1601.03916v3", "Final version, accepted at ACL 2016. New section on Human Evaluation"]], "COMMENTS": "in submission", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["julian hitschler", "shigehiko schamoni", "stefan riezler"], "accepted": true, "id": "1601.03916"}, "pdf": {"name": "1601.03916.pdf", "metadata": {"source": "CRF", "title": "Multimodal Pivots for Image Caption Translation", "authors": ["Julian Hitschler", "Shigehiko Schamoni", "Stefan Riezler"], "emails": ["hitschler@cl.uni-heidelberg.de", "schamoni@cl.uni-heidelberg.de", "riezler@cl.uni-heidelberg.de"], "sections": [{"heading": "1 Introduction", "text": "The multimodal data, which consists of images and natural language descriptions (henceforth referred to as captions), is a rich source of information that has led to a renewed increase in research results integrating language and vision. Recently, the aspect of multilingualism was included in multimodal language processing in a joint task at the WMT16 conference. [There is clearly also a practical demand for multilingual captions, as they would allow e-commerce transactions to automatically expand into international markets. However, while datasets of images and monolingual captions already contain million1http: / www.statmt.org / wmt16 / multimodal-task.htmlof tuples (Ferraro et al, 2015), the largest multilingual captions and monolingual captions, the well-known authors are known in the German language."}, {"heading": "2 Related Work", "text": "In this context, it is also worth mentioning the fact that the approaches we have mentioned are also a kind of image editing, which is directed at the way in which it has taken place over the past few years, the way in which it has been perceived by people. (...) It is not as if they have behaved in the way in which they have done it. (...) It is as if they have behaved in the way in which they have done it. (...) It is as if they have behaved in the way in which they have done it. (...) It is as if they have done it. \"(...) It is as if they have done it.\" (...) It is as if they have done it. \"(...) It is as if they have done it.\" (...) It is as if they have done it. \"(...) It is like this.\" (...)"}, {"heading": "3 Models", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Overview", "text": "Following the basic approach established by Wa \u00b6 schle and Riezler (2015), we use a crosslingual scoring model to find sentences in a target language and use it to regenerate target language translations in a source category.The systems described in our paper differ from those of Wa \u00b6 schle and Riezler (2015) in a number of aspects. Instead of a two-step architecture of coarse and fine-grained retrieval, our system uses relevance scoring functions in the document collection C, and for4https: / images.google.com / reranking of translation candidates based on inverse document frequency of terms, our system uses relevance scoring functions in the document collection C, and for4https: \"We have a schematic overview of our approach in Figure 1. It consists of the following components: input category, image, image, image, image, image, image, image, image, image, image."}, {"heading": "3.2 Target Side Retrieval Models", "text": "It is not the first time that the US government has followed the rules used in the US (and in Europe) in such a way. (It is the first time that the US government (and the EU Commission) has followed such a strategy. (It is the second time that the EU (and the EU) has followed such a strategy. (It is the first time that the EU (and the EU) has followed such a strategy.) (It is the second time that the EU (and the EU) has followed such a strategy. (It is the second time that the EU (and the EU) has followed such a strategy.) (It is the second time that the EU (and the EU) has followed such a strategy.) (It is the second time that the EU (and the EU) has followed such a strategy. (It is the second time that the EU (and the EU) has followed such a strategy. () It is the second time that the EU (and the EU), the EU (and the EU), the EU (and the US) and the US () and the US () and the second time that the US (and) have followed such a strategy."}, {"heading": "3.3 Translation Candidate Re-scoring", "text": "The relevance value F (r, Mfi) used in the reranking model was calculated as follows for all three models: F (r, Mfi) = ZMfi = ZMfi \u2211 m-Mfi \u2211 wm-type (m) \u2211 wr-tok (r) \u03b4 (wm, wr) idf (wm) with the normalization term ZMfi = (\u2211 m-Mfi | tok (m) |) \u2212 1, where r is a translation candidate and Mfi is a list of km-top destination page matches. As the model should provide a value that reflects the relevance of r with respect to Mfi, regardless of the length of Mfi, a normalization with respect to the symbol number of Mfi is necessary. ZMfi serves this purpose."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Bilingual Image-Caption Data", "text": "We created a German-English parallel dataset based on the MS COCO image corpus (Lin et al., 2014). 1,000 images were randomly selected from training section 10, and in a second step, one of their five English captions was randomly selected. This caption was then translated into German by a native German speaker. Note that our experiments were conducted with German as the source and English as the target language, so our reference data was not created by a single speaker, but broadly reflects the heterogeneity of the MS COCO dataset. Data was divided into a development set of 250 captions, a development set of 250 captions to review ongoing work, and a test set of 500 captions. For our retrievable experiments, we used only images and captions that were not included in the development, development testing, or test decc data, a total of 81,822 images with 5 English captions per image. All data was tokenized and converted into lowercase captions, using the 11."}, {"heading": "4.2 Translation Baselines", "text": "We compare our approach with two basic machine translation systems, one trained exclusively on external domain data and one domain-customized system. Table 2 provides an overview of the training data for machine translation systems. Out-of-Domain Baseline. Our baseline SMT framework is hierarchical phrase-based translation using synchronous context-free grammars (Chiang, 2007) implemented by the cdec decoder (Dyer et al., 2010). Data from Europarl (Koehn, 2005), News Commentary and Common Crawl corpora (Smith et al., 2013) as used in the WMT15 workshop were used to train the translation model with German as the source language and English as the target language. As the retrieval dataset dataset, training, development and test data were converted to lowercase, using the same cdec tools."}, {"heading": "4.3 Optimization of TSR Hyperparameters", "text": "For each of our retrieval models, we performed a step-by-step exhaustive search of the hyperparameter space using the four system hyperparameters for IBM BLEU on the development set: the length of the kn-best list, whose entries are used as queries for the retrieval; the number of km best matching subtitles retrieved; the length of the final kr-best list used in the reranking; the interpolation weight of the relevance value F relative to the translation hypothesis log probability supplied by the decoder. The parameter ranges to be examined were determined manually by examining the system output for prototypical examples. Table 3 provides an overview of the obtained hyperparameter values. For TSR-CNN, we first set the intersection distance d to 90.0 after manually checking the sets of the closest neighbors for various maximum distance values. After optimizing the retrieval parameters, we performed an exhaustive search by setting the other values from 8d to 100.0, confirming all of the steps."}, {"heading": "4.4 Significance Testing", "text": "Significance tests of differences in translation quality were conducted using the approximate randomization technique for measuring the performance differences of machine translation systems described in Riezler and Maxwell (2005) and implemented by Clark et al. (2011) as part of the Multeval toolkity.1414https: / / github.com / jhclark / multeval"}, {"heading": "4.5 Experimental Results", "text": "Table 4 summarizes the results for all models based on an invisible test set of 500 captions. Domain matching resulted in a significant improvement of + 4.1 BLEU and major improvements in METEOR and Translation Edit Rate (TER). We found that the target-side retrieval model, enhanced by multimodal pans from a deep Convolutionary Neural Network, consistently exceeded TSR-CNN and TSR-HCA, both the domain-customized cdec baseline and the text-based target retrieval model TSR-TXT. Thus, these models achieve performance gains that go beyond the effect of generic domain matching. Performance gains for TSR-CNN and TSRHCA were significant for both p < 0.05 for BLEU, METEOR and TER TSR baseline data."}, {"heading": "4.6 Human Evaluation", "text": "The in-domain Baseline and TSR-CNN differed in their output in 169 out of 500 cases of the test set. These 169 cases were presented to a human judge in a double-blind, pair-by-pair preference ranking experiment alongside the German captions. The order of presentation was randomized for the two systems and the judge was asked to independently evaluate fluctuation and accuracy of translations, the results are shown in Figure 2. Overall, there was a clear preference for TSRCNN output."}, {"heading": "4.7 Examples", "text": "Table 5 shows sample translations created by the two cdec baselines TSR-TXT, TSR-CNN and TSR-HCA, together with image and reference translations. The visual information generated by the captions of the pivot images on the target page allows a clear distinction between translation alternatives such as \"rock\" versus \"rock (music)\" for the German \"rock,\" \"pole\" versus \"mast\" for the German \"mast\" and is capable of repairing mistranslations such as \"foot\" instead of \"mouth\" for the German \"mouth.\""}, {"heading": "5 Conclusions and Further Work", "text": "We have shown that the inclusion of multimodal pivot points in a target-side retrieval model improves SMT performance compared to a strong in-domain baseline in terms of BLEU, METEOR, and TER on our parallel dataset derived from MS COCO. Performance gain was comparable between a distance measurement based on a deep Constitutional network and one based on annotations of the human object category, demonstrating the effectiveness of CNN-derived distance measurement. With our approach, SMT can benefit from multimodal context information in certain cases. Crucially, this is possible without using large amounts of parallel text data, but instead using large amounts of monolingual images that are more readily available."}, {"heading": "Acknowledgments", "text": "This research was supported in part by DFG funding RI-2221 / 2-1 \"Grounding Statistical Machine Translation in Perception and Action\" and an Amazon Academic Research Award (AARA) \"Multimodal Pivots for Low Resource Machine Translation in E-Commerce Localization.\""}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR), San Diego, California, USA.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Learning bilingual lexicons using the visual similarity of labeled web images", "author": ["Shane Bergsma", "Benjamin Van Durme."], "venue": "Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), Barcelona, Spain.", "citeRegEx": "Bergsma and Durme.,? 2011", "shortCiteRegEx": "Bergsma and Durme.", "year": 2011}, {"title": "Images as context in statistical machine translation", "author": ["Iacer Calixto", "Te\u00f3filo de Compos", "Lucia Specia."], "venue": "Proceedings of the Workshop on Vision and Language (VL), Sheffield, England, United Kingdom.", "citeRegEx": "Calixto et al\\.,? 2012", "shortCiteRegEx": "Calixto et al\\.", "year": 2012}, {"title": "Hierarchical phrase-based translation", "author": ["David Chiang."], "venue": "Computational Linguistics, 33(2):201\u2013228.", "citeRegEx": "Chiang.,? 2007", "shortCiteRegEx": "Chiang.", "year": 2007}, {"title": "Better hypothesis testing for statistical https://commons.wikimedia.org/wiki/ Main_Page", "author": ["Jonathan Clark", "Chris Dyer", "Alon Lavie", "Noah Smith"], "venue": null, "citeRegEx": "Clark et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2011}, {"title": "Ultraconservative online algorithms for multiclass problems", "author": ["Koby Crammer", "Yoram Singer."], "venue": "Journal of Machine Learning Research, 3:951\u2013991.", "citeRegEx": "Crammer and Singer.,? 2003", "shortCiteRegEx": "Crammer and Singer.", "year": 2003}, {"title": "cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models", "author": ["Chris Dyer", "Adam Lopez", "Juri Ganitkevitch", "Johnathan Weese", "Ferhan Ture", "Phil Blunsom", "Hendra Setiawan", "Vladimir Eidelman", "Philip Resnik"], "venue": null, "citeRegEx": "Dyer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2010}, {"title": "Using a maximum entropy model to build segmentation lattices for mt", "author": ["Chris Dyer."], "venue": "Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT), Boul-", "citeRegEx": "Dyer.,? 2009", "shortCiteRegEx": "Dyer.", "year": 2009}, {"title": "Multi-language image description with neural sequence models", "author": ["Desmond Elliott", "Stella Frank", "Eva Hasler."], "venue": "CoRR, abs/1510.04709.", "citeRegEx": "Elliott et al\\.,? 2015", "shortCiteRegEx": "Elliott et al\\.", "year": 2015}, {"title": "From captions to visual concepts and back", "author": ["Hao Fang", "Li Deng", "Margaret Mitchell", "Saurabh Gupta", "Piotr Dollar", "John C. Platt", "Forrest Iandola", "Jianfeng Gao", "C. Lawrence Zitnick", "Rupesh K. Srivastava", "Xiaodeng He", "Geoffrey Zweit."], "venue": "In Proceedings", "citeRegEx": "Fang et al\\.,? 2015", "shortCiteRegEx": "Fang et al\\.", "year": 2015}, {"title": "A survey of current datasets for vision and language research", "author": ["Francis Ferraro", "Nasrin Mostafazadeh", "Ting-Hao (Kenneth) Huang", "Lucy Vanderwende", "Jacob Devlin", "Michel Galley", "Margaret Mitchell"], "venue": "In Proceedings of the Conference on Em-", "citeRegEx": "Ferraro et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ferraro et al\\.", "year": 2015}, {"title": "Imagemediated learning for zero-shot cross-lingual document retrieval", "author": ["Ruka Funaki", "Hideki Nakayama."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Lisbon, Portugal.", "citeRegEx": "Funaki and Nakayama.,? 2015", "shortCiteRegEx": "Funaki and Nakayama.", "year": 2015}, {"title": "The IAPR TC-12 benchmark: A new evaluatioin resource for visual information systems", "author": ["Michael Grubinger", "Paul Clough", "Henning M\u00fcller", "Thomas Deselaers."], "venue": "In Proceedings of LREC, Genova, Italy.", "citeRegEx": "Grubinger et al\\.,? 2006", "shortCiteRegEx": "Grubinger et al\\.", "year": 2006}, {"title": "Scalable modified Kneser-Ney language model estimation", "author": ["Kenneth Heafield", "Ivan Pouzyrevsky", "Jonathan H. Clark", "Philipp Koehn."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL), Sofia,", "citeRegEx": "Heafield et al\\.,? 2013", "shortCiteRegEx": "Heafield et al\\.", "year": 2013}, {"title": "KenLM: faster and smaller language model queries", "author": ["Kenneth Heafield."], "venue": "Proceedings of the Sixth Workshop on Statistical Machine Translation (WMT), Edinburgh, Scotland, United Kingdom.", "citeRegEx": "Heafield.,? 2011", "shortCiteRegEx": "Heafield.", "year": 2011}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["Micah Hodosh", "Peter Young", "Julia Hockenmaier."], "venue": "Journal of Artificial Intelligence Research, 47:853\u2013899.", "citeRegEx": "Hodosh et al\\.,? 2013", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "Deep visualsemantic alignments for generating image descriptions", "author": ["Andrey Karpathy", "Li Fei-Fei."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Boston, Massachusetts, USA.", "citeRegEx": "Karpathy and Fei.Fei.,? 2015", "shortCiteRegEx": "Karpathy and Fei.Fei.", "year": 2015}, {"title": "Visual bilingual lexicon induction with transferred convnet features", "author": ["Douwe Kiela", "Ivan Vuli\u0107", "Stephen Clark."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Lisbon, Portugal.", "citeRegEx": "Kiela et al\\.,? 2015", "shortCiteRegEx": "Kiela et al\\.", "year": 2015}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Philipp Koehn."], "venue": "Proceedings of the Machine Translation Summit, Phuket, Thailand.", "citeRegEx": "Koehn.,? 2005", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Baby talk: Understanding and generating image descriptions", "author": ["Girish Kulkarni", "Visruth Premraj", "Sagnik Dhar", "Siming Li", "Yejin Choi", "Alexander C Berg", "Tamara L Berg."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-", "citeRegEx": "Kulkarni et al\\.,? 2011", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2011}, {"title": "Microsoft COCO: common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "Lubomir D. Bourdev", "Ross B. Girshick", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C. Lawrence Zitnick."], "venue": "Computing Research Repos-", "citeRegEx": "Lin et al\\.,? 2014", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ard", "WeiJing Zhu."], "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, Pennsylva-", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Collecting image annotations using amazon\u2019s mechanical turk", "author": ["Cyrus Rashtchian", "Peter Young", "Micah Hodosh", "Julia Hockenmaier."], "venue": "Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechan-", "citeRegEx": "Rashtchian et al\\.,? 2010", "shortCiteRegEx": "Rashtchian et al\\.", "year": 2010}, {"title": "On some pitfalls in automatic evaluation and significance testing for mt", "author": ["Stefan Riezler", "John Maxwell."], "venue": "Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Methods for MT and Summarization (MTSE) at the 43rd Annual Meeting of", "citeRegEx": "Riezler and Maxwell.,? 2005", "shortCiteRegEx": "Riezler and Maxwell.", "year": 2005}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael S. Bernstein", "Alexander C. Berg", "Fei-Fei Li."], "venue": "Computing", "citeRegEx": "Russakovsky et al\\.,? 2014", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2014}, {"title": "Learning grounded meaning representations with autoencoders", "author": ["Carina Silberer", "Mirella Lapata."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL), Baltimore, Maryland, USA.", "citeRegEx": "Silberer and Lapata.,? 2014", "shortCiteRegEx": "Silberer and Lapata.", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR), San Diego, CA.", "citeRegEx": "Simonyan and Zisserman.,? 2015", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2015}, {"title": "Dirt cheap web-scale parallel text from the Common Crawl", "author": ["Jason Smith", "Herve Saint-Amand", "Magdalena Plamada", "Philipp Koehn", "Chris Callison-Burch", "Adam Lopez."], "venue": "Proceedings of the Conference of the Association for Computational", "citeRegEx": "Smith et al\\.,? 2013", "shortCiteRegEx": "Smith et al\\.", "year": 2013}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Richard Socher", "Andrej Karpathy", "Quoc V. Le", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Socher et al\\.,? 2014", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "A statistical interpretation of term specificity and its application in retrieval", "author": ["Karen Sp\u00e4rck Jones."], "venue": "Journal of Documentation, 28:11\u201321.", "citeRegEx": "Jones.,? 1972", "shortCiteRegEx": "Jones.", "year": 1972}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Boston,Massachusetts, USA.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Learning fine-grained image similarity with deep ranking", "author": ["Jiang Wang", "Yang Song", "Thomas Leung", "Chuck Rosenberg", "Jingbin Wang", "James Philbin", "Bo Chen", "Ying Wu."], "venue": "Proceedings of the Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Wang et al\\.,? 2014", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Scalable similarity learning using large margin neighborhood embedding", "author": ["Zhaowen Wang", "Jianchao Yang", "Zhe Lin", "Jonathan Brandt", "Shiyu Chang", "Thomas Huang."], "venue": "Proceedings of the IEEE Winter Conference on Applications of Com-", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Integrating a large, monolingual corpus as translation memory into statistical machine translation", "author": ["Katharina W\u00e4schle", "Stefan Riezler."], "venue": "Proceedings of the 18th Annual Conference of the European Association for Machine Translation (EAMT), An-", "citeRegEx": "W\u00e4schle and Riezler.,? 2015", "shortCiteRegEx": "W\u00e4schle and Riezler.", "year": 2015}, {"title": "Online multimodal deep similarity learning with application to image retrieval", "author": ["Pengcheng Wu", "Steven C.H. Hoi", "Hao Xia", "Peilin Zhao", "Dayong Wang", "Chunyan Miao."], "venue": "Proceedings of the 21st ACM International Conference on Multimedia, Barcelona,", "citeRegEx": "Wu et al\\.,? 2013", "shortCiteRegEx": "Wu et al\\.", "year": 2013}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio."], "venue": "Proceedings of the International Confer-", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 10, "context": "html of tuples (Ferraro et al., 2015), the largest multilingual datasets of images and captions known to the authors contain 20,000 (Grubinger et al.", "startOffset": 15, "endOffset": 37}, {"referenceID": 12, "context": ", 2015), the largest multilingual datasets of images and captions known to the authors contain 20,000 (Grubinger et al., 2006) or 30,0002 triples of images with German and English descriptions.", "startOffset": 102, "endOffset": 126}, {"referenceID": 19, "context": "In contrast to prior work on generating captions directly from images (Kulkarni et al. (2011), Karpathy and Fei-", "startOffset": 71, "endOffset": 94}, {"referenceID": 30, "context": "Fei (2015), Vinyals et al. (2015), inter alia), our goal is to integrate visual information into an SMT pipeline.", "startOffset": 12, "endOffset": 34}, {"referenceID": 15, "context": ", the identification of relevant captions for i (Hodosh et al., 2013) is not itself part of the task of caption translation.", "startOffset": 48, "endOffset": 69}, {"referenceID": 22, "context": "The dataset used at the WMT16 shared task is based on translations of Flickr30K captions (Rashtchian et al., 2010).", "startOffset": 89, "endOffset": 114}, {"referenceID": 9, "context": "Recent approaches also employ reranking of image captions by measuring similarity between image and text using deep representations (Fang et al., 2015).", "startOffset": 132, "endOffset": 151}, {"referenceID": 28, "context": "We rely on the CNN framework (Socher et al., 2014; Simonyan and Zisserman, 2015) to solve semantic classification and disambiguation tasks in NLP with the help of supervision signals from visual feedback.", "startOffset": 29, "endOffset": 80}, {"referenceID": 26, "context": "We rely on the CNN framework (Socher et al., 2014; Simonyan and Zisserman, 2015) to solve semantic classification and disambiguation tasks in NLP with the help of supervision signals from visual feedback.", "startOffset": 29, "endOffset": 80}, {"referenceID": 15, "context": "(2011), Karpathy and Fei-Fei (2015), Vinyals et al.", "startOffset": 8, "endOffset": 36}, {"referenceID": 15, "context": "(2011), Karpathy and Fei-Fei (2015), Vinyals et al. (2015), inter alia).", "startOffset": 8, "endOffset": 59}, {"referenceID": 32, "context": "In the area of SMT, W\u00e4schle and Riezler (2015) presented a framework for integrating a large, indomain, target-side monolingual corpus into machine translation by making use of techniques from crosslingual information retrieval.", "startOffset": 20, "endOffset": 47}, {"referenceID": 2, "context": "Calixto et al. (2012) suggest using images as supplementary context information for statistical machine translation.", "startOffset": 0, "endOffset": 22}, {"referenceID": 8, "context": "In parallel to our work, Elliott et al. (2015) addressed the problem of caption transla-", "startOffset": 25, "endOffset": 47}, {"referenceID": 24, "context": "For example, Silberer and Lapata (2014) show that distributional word embeddings grounded in visual representations outperform competitive baselines on term similarity scoring and word categorization tasks.", "startOffset": 13, "endOffset": 40}, {"referenceID": 17, "context": "The orthogonality of visual feedback has previously been exploited in a multilingual setting by Kiela et al. (2015) (relying on previous work by Bergsma and Van Durme (2011)), who induce a bilingual lexicon using term-specific multimodal representations obtained by querying the Google image", "startOffset": 96, "endOffset": 116}, {"referenceID": 17, "context": "The orthogonality of visual feedback has previously been exploited in a multilingual setting by Kiela et al. (2015) (relying on previous work by Bergsma and Van Durme (2011)), who induce a bilingual lexicon using term-specific multimodal representations obtained by querying the Google image", "startOffset": 96, "endOffset": 174}, {"referenceID": 8, "context": "We replicated the results of Elliott et al. (2015) on the IAPR TC-12 data.", "startOffset": 29, "endOffset": 51}, {"referenceID": 11, "context": "4 Funaki and Nakayama (2015)", "startOffset": 2, "endOffset": 29}, {"referenceID": 11, "context": "based visual similarity information is used as a \u201chub\u201d (Funaki and Nakayama, 2015) or pivot connecting corpora in two natural languages which lack direct parallelism, a strategy which we apply to the problem of caption translation.", "startOffset": 55, "endOffset": 82}, {"referenceID": 33, "context": "Following the basic approach set out by W\u00e4schle and Riezler (2015), we use a crosslingual retrieval model to find sentences in a target language document collection C, and use these to rerank target language translations e of a source caption f .", "startOffset": 40, "endOffset": 67}, {"referenceID": 33, "context": "The systems described in our work differ from that of W\u00e4schle and Riezler (2015) in a number of aspects.", "startOffset": 54, "endOffset": 81}, {"referenceID": 33, "context": "This function is purely text-based and does not make use of multimodal context information (as such, it comes closest to the models used for target-side retrieval in W\u00e4schle and Riezler (2015)).", "startOffset": 166, "endOffset": 193}, {"referenceID": 20, "context": "This function makes use of the object annotations available for the MS COCO corpus (Lin et al., 2014) to give an indication of the effectiveness of our automatically extracted visual similarity metric.", "startOffset": 83, "endOffset": 101}, {"referenceID": 18, "context": "Term frequencies were computed on monolingual data from Europarl (Koehn, 2005) and the News Commentary and News Discussions English datasets provided for the WMT15 workshop.", "startOffset": 65, "endOffset": 78}, {"referenceID": 24, "context": "Our visual distance measure v was computed using the VGG16 deep convolutional model of Simonyan and Zisserman (2015), which was pretrained on ImageNet (Russakovsky et al., 2014).", "startOffset": 151, "endOffset": 177}, {"referenceID": 25, "context": "Our visual distance measure v was computed using the VGG16 deep convolutional model of Simonyan and Zisserman (2015), which was pretrained on ImageNet (Russakovsky et al.", "startOffset": 87, "endOffset": 117}, {"referenceID": 20, "context": "We constructed a German-English parallel dataset based on the MS COCO image corpus (Lin et al., 2014).", "startOffset": 83, "endOffset": 101}, {"referenceID": 7, "context": "performed compound-splitting using the method described by Dyer (2009), as implemented by the cdec utility compound-split.", "startOffset": 59, "endOffset": 71}, {"referenceID": 3, "context": "using synchronous context free grammars (Chiang, 2007), as implemented by the cdec decoder (Dyer et al.", "startOffset": 40, "endOffset": 54}, {"referenceID": 6, "context": "using synchronous context free grammars (Chiang, 2007), as implemented by the cdec decoder (Dyer et al., 2010).", "startOffset": 91, "endOffset": 110}, {"referenceID": 18, "context": "Data from the Europarl (Koehn, 2005), News Commentary and Common Crawl corpora (Smith et al.", "startOffset": 23, "endOffset": 36}, {"referenceID": 27, "context": "Data from the Europarl (Koehn, 2005), News Commentary and Common Crawl corpora (Smith et al., 2013) as provided for", "startOffset": 79, "endOffset": 99}, {"referenceID": 13, "context": "monolingual data from Europarl, as well as the News Crawl and News Discussions English datasets provided for the WMT15 workshop (the same data as was used for estimating term frequencies for the retrieval models) with the KenLM toolkit (Heafield et al., 2013; Heafield, 2011).", "startOffset": 236, "endOffset": 275}, {"referenceID": 14, "context": "monolingual data from Europarl, as well as the News Crawl and News Discussions English datasets provided for the WMT15 workshop (the same data as was used for estimating term frequencies for the retrieval models) with the KenLM toolkit (Heafield et al., 2013; Heafield, 2011).", "startOffset": 236, "endOffset": 275}, {"referenceID": 21, "context": "BLEU (Papineni et al., 2002) using the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003).", "startOffset": 5, "endOffset": 28}, {"referenceID": 5, "context": ", 2002) using the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003).", "startOffset": 58, "endOffset": 84}, {"referenceID": 22, "context": "The English captions in this dataset belong to the Flickr30k corpus (Rashtchian et al., 2010) and are very similar to those of the MS COCO corpus.", "startOffset": 68, "endOffset": 93}, {"referenceID": 22, "context": "Significance tests on the differences in translation quality were performed using the approximate randomization technique for measuring performance differences of machine translation systems described in Riezler and Maxwell (2005) and implemented by Clark et al.", "startOffset": 204, "endOffset": 231}, {"referenceID": 4, "context": "Significance tests on the differences in translation quality were performed using the approximate randomization technique for measuring performance differences of machine translation systems described in Riezler and Maxwell (2005) and implemented by Clark et al. (2011) as part of the Multeval toolkit.", "startOffset": 250, "endOffset": 270}, {"referenceID": 33, "context": "This finding is actually consistent with W\u00e4schle and Riezler (2015) who report performance gains for text-based, target side retrieval", "startOffset": 41, "endOffset": 68}, {"referenceID": 34, "context": "Learning semantically informative distance metrics using deep learning techniques is an area under active investigation (Wu et al., 2013; Wang et al., 2014; Wang et al., 2015).", "startOffset": 120, "endOffset": 175}, {"referenceID": 31, "context": "Learning semantically informative distance metrics using deep learning techniques is an area under active investigation (Wu et al., 2013; Wang et al., 2014; Wang et al., 2015).", "startOffset": 120, "endOffset": 175}, {"referenceID": 32, "context": "Learning semantically informative distance metrics using deep learning techniques is an area under active investigation (Wu et al., 2013; Wang et al., 2014; Wang et al., 2015).", "startOffset": 120, "endOffset": 175}, {"referenceID": 15, "context": "This problem is aggravated in the multimodal case, since the relevance of captions with respect to images varies greatly between different corpora (Hodosh et al., 2013).", "startOffset": 147, "endOffset": 168}, {"referenceID": 7, "context": "A further avenue of future research is improving models such as that presented in Elliott et al. (2015) by crucial components of neural MT such as \u201cattention mechanisms\u201d.", "startOffset": 82, "endOffset": 104}, {"referenceID": 0, "context": "For example, the attention mechanism of Bahdanau et al. (2015) serves as a soft alignment that helps to guide the translation process by influencing the sequence in which source tokens are translated.", "startOffset": 40, "endOffset": 63}, {"referenceID": 0, "context": "For example, the attention mechanism of Bahdanau et al. (2015) serves as a soft alignment that helps to guide the translation process by influencing the sequence in which source tokens are translated. A similar mechanism is used in Xu et al. (2015) to decide which part of the image should influence which part of the generated caption.", "startOffset": 40, "endOffset": 249}], "year": 2016, "abstractText": "We present an approach to improve statistical machine translation of image descriptions by multimodal pivots defined in visual space. The key idea is to perform image retrieval over a database of images that are captioned in the target language, and use the captions of the most similar images for crosslingual reranking of translation outputs. Our approach does not depend on the availability of large amounts of in-domain parallel data, but only relies on available large datasets of monolingually captioned images, and on state-ofthe-art convolutional neural networks to compute image similarities. Our experimental evaluation shows improvements of 1 BLEU point over strong baselines.", "creator": "TeX"}}}