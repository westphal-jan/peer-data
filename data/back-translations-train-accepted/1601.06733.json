{"id": "1601.06733", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jan-2016", "title": "Long Short-Term Memory-Networks for Machine Reading", "abstract": "Teaching machines to process text with psycholinguistics insights is a challenging task. We propose an attentive machine reader that reads text from left to right, whilst linking the word at the current fixation point to previous words stored in the memory as a kind of implicit information parsing to facilitate understanding. The reader is equipped with a Long Short-Term Memory architecture, which, different from previous work, has a memory tape (instead of a memory cell) to store the past information and adaptively use them without severe information compression. We demonstrate the excellent performance of the machine reader in language modeling as well as the downstream sentiment analysis and natural language inference.", "histories": [["v1", "Mon, 25 Jan 2016 19:25:48 GMT  (737kb,D)", "http://arxiv.org/abs/1601.06733v1", null], ["v2", "Tue, 26 Jan 2016 20:48:02 GMT  (675kb,D)", "http://arxiv.org/abs/1601.06733v2", null], ["v3", "Mon, 1 Feb 2016 14:29:04 GMT  (675kb,D)", "http://arxiv.org/abs/1601.06733v3", null], ["v4", "Thu, 17 Mar 2016 13:28:16 GMT  (1354kb,D)", "http://arxiv.org/abs/1601.06733v4", null], ["v5", "Thu, 7 Apr 2016 09:53:49 GMT  (1354kb,D)", "http://arxiv.org/abs/1601.06733v5", null], ["v6", "Wed, 1 Jun 2016 12:27:42 GMT  (1623kb,D)", "http://arxiv.org/abs/1601.06733v6", "Fixed the incomparable parameters in experiments to previous work; fixed a few typos"], ["v7", "Tue, 20 Sep 2016 21:20:09 GMT  (1618kb,D)", "http://arxiv.org/abs/1601.06733v7", "Published as a conference paper at EMNLP 2016"]], "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["jianpeng cheng 0001", "li dong", "mirella lapata"], "accepted": true, "id": "1601.06733"}, "pdf": {"name": "1601.06733.pdf", "metadata": {"source": "CRF", "title": "Long Short-Term Memory-Networks for Machine Reading", "authors": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata"], "emails": ["jianpeng.cheng@ed.ac.uk", "li.dong@ed.ac.uk", "mlap@inf.ed.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Related Work", "text": "Most of the research to date in this field analyzes texts with traditional NLP pipelines, such as marking and parrying, which are substantially different from human reading behavior. These systems require a considerable amount of manual and labeled memory, which limits their scope to the specific task and distribution of data. Indeed, machine reading is by its nature an unmanageable task that has implicitly come across the fly while reading (Etzioni et al., 2006). While it is a few recent papers that are being reciprocated, a relationship has indeed come to be implicitly encountered by the fly, the relationships become implicit with the fly (Etzioni et al.)."}, {"heading": "3 The Machine Reader", "text": "In this section, we propose a novel machine reader inspired by psycholinguistics. At the core of the reader is a recursive neural long-term memory with an extended memory band that explicitly simulates human memory span. The reader performs implicit dependency analyses with an attention memory addressing mechanism at each input step. In the following, we will first consider the standard unit for short-term memory."}, {"heading": "3.1 Long Short-Term Memory", "text": "A long short-term memory (LSTM) recurrent neural network processes a variable sequence of length x = (x1, x2, \u00b7 \u00b7, xn) by gradually adding new content into a single slot of maintained memory, with gates controlling the extent to which new content should be stored, old content should be deleted, and current content should be uncovered. In due course, the memory and the hidden state are updated with the following equation ot-c-t. Compared to the standard RNN, LSTM separates memory c from the hidden state h, which actually interacts with the environment when the output is compressed. This network essentially stores long vocabulary from which future predictions can be made."}, {"heading": "3.2 Long Short-Term Memory-Network", "text": "To this end, we modify the standard LSTM structure by replacing the memory cell with a storage network whose size grows with the input sequence. (The resulting Long Short-Term Memory Network (LSTMN) stores the input at each time step with a unique memory space, bypassing the problem of information compression, while allowing adaptive modulation within the memory. (While it is feasible to apply both read and write operations to the storage network, based on the current input tag, we focus only on the read operation - as a way to carefully link the current input tag to previous content in the memory and select appropriately useful memory contents when processing the current model.) Although it is not the focus of this work, the importance of the write operation can similarly be justified - as a way to gradually update the previous memory to get the correct attention for processing garden examples."}, {"heading": "3.3 Deep LSTMNs", "text": "It is possible to construct deep LSTMNs by stacking several hidden layers on top of each other, similar to a stacked LSTM (Graves, 2013), or from another perspective, a multi-hop storage network (Sukhbaatar et al., 2015), by feeding the attention vector of the k layer as input into the (k + 1) th layer."}, {"heading": "4 LSTMNs for Dual Sequence Modeling", "text": "At the heart of these tasks is a dual sequence processor (e.g. an encoder decoder), in which the second sequence (i.e. the target) is conditioned on the first (i.e., source). In this section, we draw connections between the inherent attention mechanism of the LSTMN and the widely used sequence range between two sequences. We then explain how an LSTMN can be used in dual sequence processing. Generally, the intra-attention within a sequence and the inter-attention between two sequences will complement each other. While inter-attention derives the alignment between source and target, intra-attention provides implicit dependency analysis within a sequence, which could lead to improved memories favoring the subsequent inter-alignment. Below, we present two ways of using the LSTMN in a dual architecture, shown in Figure A and 3b."}, {"heading": "5 Experiments", "text": "In this section, we describe a series of experiments that have been conducted to evaluate the performance of the LSTMN machine reader. We begin by specifying general aspects of the implementation."}, {"heading": "5.1 Implementation Details", "text": "All experiments are performed on a Nvidia GTX 980 graphics card. Size of word vectors, hidden vectors and memory vectors is set to 300. We initialized model weights with an even distribution between [-0.05, 0.05]."}, {"heading": "5.2 Language Modeling", "text": "Following the pre-processing by Mikolov et al. (2010), we use splits 0-20 for training, 21-22 for validation, and 23-24 for the test. The entire data set contains about 1 million tokens and a vocabulary of size 10k. As in the standard language that models the task, we use as a benchmark the perplexity PPPL = exp (NLL / T), where NLL is the negative log probability of the entire test set and T is the corresponding number of tokens. We use the stochastic gradient lineage for optimization with an initial learning rate of 0.65, which decreases by a factor of 0.85 percent if no significant improvement in validation has been observed. We re-normalize the gradient if its standard is greater than 5.We compare LSTMN and the stacked LSTMN with a variety of basic terms."}, {"heading": "5.3 Sentiment Analysis", "text": "The second experiment concerns the prediction of sentiment labels of sets. We used the Stanford Sentiment Treebank (Socher et al., 2013), which contains 11,855 sets and their 5x fine-grained sentiment labels (very negative, negative, neutral, positive, very positive). According to previous experiments on this data set, we use 8,544 sets for training, 1,101 for validation, and 2,210 for the test. In addition, it is possible to perform another binary classification task by removing the neutral label, resulting in 6,920 sets for training, 872 for validation, and 1,821 for the test. We report the results for both the fine-grained and binary classification tasks. As shown in Table 2, we compare our models with 7 existing systems and 2 LSTM baselines. For the LSTMNs, we predict the sentential sentiment labels for training based on the average hidden vector we pass on to the activator two-layer activator."}, {"heading": "5.4 Natural Language Inference", "text": "In this task, we used the Stanford Natural Language Inference (SNLI) data set approach (Bowman et al., 2015), which includes presumption-hypotheses pairs and target markers that indicate their relationship. Vocabulary size is 37,249. Recent approaches use two sequential LSTMs to encode the premise and hypotheses, and apply neural attention to their logical relationship. It was found in Rockta \u00bc schel et al. (2015) that hypotheses are conditioned in a significant increase in performance, making the entire system non-standard."}, {"heading": "6 Discussion and Future Work", "text": "We propose a novel machine reader that reads a sequence from left to right while implicitly addressing its internal structures directly on reading. The reader uses a long-term memory architecture with an extended memory band and explicitly stores all past input information without recursive memory compression. This architecture allows for implicit dependency analysis and adaptive memory modulation driven by an internal attention mechanism. While the idea is general in terms of internal attention, many components of the model are flexible in design. For example, the internal attention mechanism can be applied analogously in a gated recurrent unit (Cho et al., 2014). The way the current input interacts with the memories can also vary. It is also useful to introduce a memory update mechanism that can be modeled as a reading process with other attention parameters. This is potentially helpful for the machine reader to integrate the more general information into the network, so that it would be more interesting to integrate the machine reader into the past."}], "references": [{"title": "A computational model of human parsing", "author": ["Steven P Abney"], "venue": "Journal of psycholinguistic Research,", "citeRegEx": "Abney.,? \\Q1989\\E", "shortCiteRegEx": "Abney.", "year": 1989}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio et al.1994] Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "Neural Networks, IEEE Transactions", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "A convolutional neural network for modelling sentences", "author": ["Blunsom et al.2014] Phil Blunsom", "Edward Grefenstette", "Nal Kalchbrenner"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association", "citeRegEx": "Blunsom et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Blunsom et al\\.", "year": 2014}, {"title": "A large annotated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326", "author": ["Gabor Angeli", "Christopher Potts", "Christopher D Manning"], "venue": null, "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder", "author": ["Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Merri\u00ebnboer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2014}, {"title": "Gated feedback recurrent neural networks. arXiv preprint arXiv:1502.02367", "author": ["Chung et al.2015] Junyoung Chung", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Chung et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "Learning context-free grammars: Capabilities and limitations of a recurrent neural network with an external stack memory", "author": ["Das et al.1992] Sreerupa Das", "C Lee Giles", "GuoZheng Sun"], "venue": null, "citeRegEx": "Das et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Das et al\\.", "year": 1992}, {"title": "Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Learning to transduce", "author": ["Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom"], "venue": null, "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Untersuchungen zu dynamischen neuronalen netzen. Diploma, Technische Universit\u00e4t M\u00fcnchen", "author": ["Sepp Hochreiter"], "venue": null, "citeRegEx": "Hochreiter.,? \\Q1991\\E", "shortCiteRegEx": "Hochreiter.", "year": 1991}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["Irsoy", "Cardie2014] Ozan Irsoy", "Claire Cardie"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Irsoy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Irsoy et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882", "author": ["Yoon Kim"], "venue": null, "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "A clockwork rnn", "author": ["Klaus Greff", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "arXiv preprint arXiv:1402.3511", "citeRegEx": "Koutn\u0131\u0301k et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Koutn\u0131\u0301k et al\\.", "year": 2014}, {"title": "Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053", "author": ["Le", "Mikolov2014] Quoc V Le", "Tomas Mikolov"], "venue": null, "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "A deep memory-based architecture for sequence-to-sequence learning", "author": ["Meng et al.2015] Fandong Meng", "Zhengdong Lu", "Zhaopeng Tu", "Hang Li", "Qun Liu"], "venue": "arXiv preprint arXiv:1506.06442", "citeRegEx": "Meng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Meng et al\\.", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH 2010,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Learning longer memory in recurrent neural networks. arXiv preprint arXiv:1412.7753", "author": ["Armand Joulin", "Sumit Chopra", "Michael Mathieu", "Marc\u2019Aurelio Ranzato"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2014}, {"title": "On the difficulty of training recurrent neural networks. arXiv preprint arXiv:1211.5063", "author": ["Tomas Mikolov", "Yoshua Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Machine reading: A\u201d killer app", "author": ["Poon", "Domingos2010] Hoifung Poon", "Pedro Domingos"], "venue": null, "citeRegEx": "Poon et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Poon et al\\.", "year": 2010}, {"title": "Reasoning about entailment with neural attention", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1509.06664", "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Sumit Chopra", "Jason Weston"], "venue": "arXiv preprint arXiv:1509.00685", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "On the computational power of neural nets", "author": ["Siegelmann", "Sontag1995] Hava T Siegelmann", "Eduardo D Sontag"], "venue": "Journal of computer and system sciences,", "citeRegEx": "Siegelmann et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Siegelmann et al\\.", "year": 1995}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Eric H Huang", "Jeffrey Pennin", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank. Citeseer", "author": ["Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "End-to-end memory networks", "author": ["Jason Weston", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075", "author": ["Tai et al.2015] Kai Sheng Tai", "Richard Socher", "Christopher D Manning"], "venue": null, "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Recurrent memory network for language modeling", "author": ["Tran et al.2016] Ke Tran", "Arianna Bisazza", "Christof Monz"], "venue": "arXiv preprint arXiv:1601.01272", "citeRegEx": "Tran et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tran et al\\.", "year": 2016}, {"title": "Learning natural language inference with lstm", "author": ["Wang", "Jiang2015] Shuohang Wang", "Jing Jiang"], "venue": "arXiv preprint arXiv:1512.08849", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Depth-gated recurrent neural networks. arXiv preprint arXiv:1508.03790", "author": ["Yao et al.2015] Kaisheng Yao", "Trevor Cohn", "Katerina Vylomova", "Kevin Duh", "Chris Dyer"], "venue": null, "citeRegEx": "Yao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2015}, {"title": "Learning to execute. arXiv preprint arXiv:1410.4615", "author": ["Zaremba", "Sutskever2014] Wojciech Zaremba", "Ilya Sutskever"], "venue": null, "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 11, "context": "One concerns training and has to do with the vanishing and exploding gradient problem (Hochreiter, 1991; Bengio et al., 1994), which can be ameliorated with gated activation functions, such as the Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), and the gradient clipping trick (Pascanu et al.", "startOffset": 86, "endOffset": 125}, {"referenceID": 2, "context": "One concerns training and has to do with the vanishing and exploding gradient problem (Hochreiter, 1991; Bengio et al., 1994), which can be ameliorated with gated activation functions, such as the Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), and the gradient clipping trick (Pascanu et al.", "startOffset": 86, "endOffset": 125}, {"referenceID": 20, "context": ", 1994), which can be ameliorated with gated activation functions, such as the Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), and the gradient clipping trick (Pascanu et al., 2012).", "startOffset": 177, "endOffset": 199}, {"referenceID": 27, "context": "There have been a few recent papers on storing and querying sequence information with external memories (Weston et al., 2014; Sukhbaatar et al., 2015; Grefenstette et al., 2015).", "startOffset": 104, "endOffset": 177}, {"referenceID": 9, "context": "There have been a few recent papers on storing and querying sequence information with external memories (Weston et al., 2014; Sukhbaatar et al., 2015; Grefenstette et al., 2015).", "startOffset": 104, "endOffset": 177}, {"referenceID": 18, "context": "cent works leverage recurrent neural networks to understand text from scratch (Mikolov et al., 2010; Bahdanau et al., 2014).", "startOffset": 78, "endOffset": 123}, {"referenceID": 1, "context": "cent works leverage recurrent neural networks to understand text from scratch (Mikolov et al., 2010; Bahdanau et al., 2014).", "startOffset": 78, "endOffset": 123}, {"referenceID": 1, "context": "The later has assumed several guises in tasks like machine translation (Bahdanau et al., 2014), sentence", "startOffset": 71, "endOffset": 94}, {"referenceID": 23, "context": "compression (Rush et al., 2015), program execution (Zaremba and Sutskever, 2014), to name just a few.", "startOffset": 12, "endOffset": 31}, {"referenceID": 2, "context": "In practice, training RNNs to capture long term dependencies is challenging due to the wellknown exploding or vanishing gradient problem (Bengio et al., 1994).", "startOffset": 137, "endOffset": 158}, {"referenceID": 15, "context": ", 2014), and more advanced architectures that enhance the information flow within the network (Koutn\u0131\u0301k et al., 2014; Chung et al., 2015).", "startOffset": 94, "endOffset": 137}, {"referenceID": 6, "context": ", 2014), and more advanced architectures that enhance the information flow within the network (Koutn\u0131\u0301k et al., 2014; Chung et al., 2015).", "startOffset": 94, "endOffset": 137}, {"referenceID": 1, "context": "Since the model recursively combines all inputs into a single memory which is typically too small, RNNs are known to have difficulty in memorizing sequences well (Zaremba and Sutskever, 2014), which becomes a bottle neck for downstream tasks (Bahdanau et al., 2014).", "startOffset": 242, "endOffset": 265}, {"referenceID": 1, "context": "In the encoder-decoder architecture, this problem can be sidestepped with an attention mechanism to learn a soft alignment between the encoding and decoding states (Bahdanau et al., 2014).", "startOffset": 164, "endOffset": 187}, {"referenceID": 27, "context": "This model can be trained end-to-end with a memory addressing mechanism closely related to soft attention (Sukhbaatar et al., 2015).", "startOffset": 106, "endOffset": 131}, {"referenceID": 7, "context": "This idea dates back to an early work of Das et al. (1992), who connect a recurrent neural network state controller with an external stack memory for learning context free grammars.", "startOffset": 41, "endOffset": 59}, {"referenceID": 7, "context": "This idea dates back to an early work of Das et al. (1992), who connect a recurrent neural network state controller with an external stack memory for learning context free grammars. Very recently, Weston et al. (2014) propose Memory Networks to explicitly segregate the memory storage from the computation of a neural network.", "startOffset": 41, "endOffset": 218}, {"referenceID": 9, "context": "Grefenstette et al. (2015) define a set of differentiable data structures (stacks, queues and dequeues) as memories controlled by a recurrent neural network.", "startOffset": 0, "endOffset": 27}, {"referenceID": 29, "context": "Concurrent to our work is the pre-print Recurrent Memory Networks (Tran et al., 2016), which equips an LSTM with an external memory block interacting with the hidden state of the LSTM.", "startOffset": 66, "endOffset": 85}, {"referenceID": 8, "context": "It is possible to construct deep LSTMNs by stacking multiple hidden layers on top of each other, resembling a stacked LSTM (Graves, 2013), or from", "startOffset": 123, "endOffset": 137}, {"referenceID": 27, "context": "another perspective, a multi-hop memory network (Sukhbaatar et al., 2015).", "startOffset": 48, "endOffset": 73}, {"referenceID": 8, "context": "We find skip-connections (Graves, 2013) across layers important for easing", "startOffset": 25, "endOffset": 39}, {"referenceID": 1, "context": "Meanwhile, we apply inter attention between them at every time step when the target sequence is analyzed, same as the RNNSearch (Bahdanau et al., 2014).", "startOffset": 128, "endOffset": 151}, {"referenceID": 18, "context": "lowing the preprocessing of Mikolov et al. (2010), we use splits 0-20 for training, 21-22 for validation, and 23-24 for test.", "startOffset": 28, "endOffset": 50}, {"referenceID": 6, "context": "We also implemented more sophisticated LSTM variations, including the stacked (multilayer) LSTM (sLSTM), the gated-feedback LSTM (gLSTM) (Chung et al., 2015) and the depth-gated LSTM (dLSTM) (Yao et al.", "startOffset": 137, "endOffset": 157}, {"referenceID": 31, "context": ", 2015) and the depth-gated LSTM (dLSTM) (Yao et al., 2015).", "startOffset": 41, "endOffset": 59}, {"referenceID": 15, "context": "The gatedfeedback LSTM is a generalization of the clockwork RNN (Koutn\u0131\u0301k et al., 2014), with feedback", "startOffset": 64, "endOffset": 87}, {"referenceID": 18, "context": "Both numbers for the KN-5 and RNN are copied from Mikolov et al. (2014). As we can see, the single-layer LSTMN outperformed the standard baselines with a significant margin.", "startOffset": 50, "endOffset": 72}, {"referenceID": 26, "context": "We used the Stanford Sentiment Treebank (Socher et al., 2013), which contains 11,855 sentences and their 5-way fine-grained sentiment labels (very negative, negative, neutral, positive, very positive).", "startOffset": 40, "endOffset": 61}, {"referenceID": 25, "context": "Models Fine-grained Binary RAE (Socher et al., 2011) 43.", "startOffset": 31, "endOffset": 52}, {"referenceID": 26, "context": "4 RNTN (Socher et al., 2013) 45.", "startOffset": 7, "endOffset": 28}, {"referenceID": 3, "context": "4 DCNN (Blunsom et al., 2014) 48.", "startOffset": 7, "endOffset": 29}, {"referenceID": 13, "context": "8 CNN (Kim, 2014) 48.", "startOffset": 6, "endOffset": 17}, {"referenceID": 28, "context": "6 CT-LSTM (Tai et al., 2015) 51.", "startOffset": 10, "endOffset": 28}, {"referenceID": 28, "context": "0 LSTM (Tai et al., 2015) 46.", "startOffset": 7, "endOffset": 25}, {"referenceID": 28, "context": "3 2-layer LSTM (Tai et al., 2015) 46.", "startOffset": 15, "endOffset": 33}, {"referenceID": 4, "context": "In this task we used the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015), which contains premise-hypothesis pairs and target labels indicating their relation (entailment, contradiction or neutral).", "startOffset": 76, "endOffset": 97}, {"referenceID": 22, "context": "respectively, and apply neural attention to reason about their logical relationship (Rockt\u00e4schel et al., 2015; Wang and Jiang, 2015).", "startOffset": 84, "endOffset": 132}, {"referenceID": 22, "context": "respectively, and apply neural attention to reason about their logical relationship (Rockt\u00e4schel et al., 2015; Wang and Jiang, 2015). It has been found in Rockt\u00e4schel et al. (2015) that processing the hypothesis conditioned on the premise results in a", "startOffset": 85, "endOffset": 181}, {"referenceID": 22, "context": "In the second approach we apply word-by-word neural attention (Rockt\u00e4schel et al., 2015) in a way that the alignments between words in the two sentences are implicitly addressed.", "startOffset": 62, "endOffset": 88}, {"referenceID": 22, "context": "In the second approach we apply word-by-word neural attention (Rockt\u00e4schel et al., 2015) in a way that the alignments between words in the two sentences are implicitly addressed. At each time step of the decoding, we compute the attention over Y = [\u03b31, \u00b7 \u00b7 \u00b7 , \u03b3m] given ht followed by a weighted average to get an alignment vector \u03b3\u0304t. Different from Rockt\u00e4schel et al. (2015), we did not feed the previous alignment vector to the attention scorer, as a simplified version of their model.", "startOffset": 63, "endOffset": 378}, {"referenceID": 4, "context": "8 LSTM (Bowman et al., 2015) 84.", "startOffset": 7, "endOffset": 28}, {"referenceID": 4, "context": "6 Hardcrafted features (Bowman et al., 2015) 99.", "startOffset": 23, "endOffset": 44}, {"referenceID": 22, "context": "2 LSTM shared (Rockt\u00e4schel et al., 2015) 84.", "startOffset": 14, "endOffset": 40}, {"referenceID": 22, "context": "4 LSTM attention (Rockt\u00e4schel et al., 2015) 85.", "startOffset": 17, "endOffset": 43}], "year": 2017, "abstractText": "Teaching machines to process text with psycholinguistics insights is a challenging task. We propose an attentive machine reader that reads text from left to right, whilst linking the word at the current fixation point to previous words stored in the memory as a kind of implicit information parsing to facilitate understanding. The reader is equipped with a Long ShortTerm Memory architecture, which, different from previous work, has a memory tape (instead of a memory cell) to store the past information and adaptively use them without severe information compression. We demonstrate the excellent performance of the machine reader in language modeling as well as the downstream sentiment analysis and natural language inference.", "creator": "LaTeX with hyperref package"}}}