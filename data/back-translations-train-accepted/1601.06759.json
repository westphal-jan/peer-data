{"id": "1601.06759", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jan-2016", "title": "Pixel Recurrent Neural Networks", "abstract": "Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.", "histories": [["v1", "Mon, 25 Jan 2016 20:34:24 GMT  (3500kb,D)", "http://arxiv.org/abs/1601.06759v1", null], ["v2", "Mon, 29 Feb 2016 15:32:16 GMT  (2385kb,D)", "http://arxiv.org/abs/1601.06759v2", null], ["v3", "Fri, 19 Aug 2016 14:10:16 GMT  (2647kb,D)", "http://arxiv.org/abs/1601.06759v3", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["a\u00e4ron van den oord", "nal kalchbrenner", "koray kavukcuoglu"], "accepted": true, "id": "1601.06759"}, "pdf": {"name": "1601.06759.pdf", "metadata": {"source": "META", "title": "Pixel Recurrent Neural Networks", "authors": ["A\u00e4ron van den Oord", "Nal Kalchbrenner", "Koray Kavukcuoglu"], "emails": ["AVDNOORD@GOOGLE.COM", "NALK@GOOGLE.COM", "KORAYK@GOOGLE.COM"], "sections": [{"heading": "1. Introduction", "text": "In fact, most people who work for the rights of women and men have to burden themselves and their rights and obligations."}, {"heading": "2. Model", "text": "Figure 2 illustrates this process: common distribution across pixels is factored into a product of conditional distributions, and the parameters used in the predictions are shared across all pixel positions in the image. To capture the generation process, Theis & Bethge (2015) suggest using a two-dimensional LSTM network (Graves & Schmidhuber, 2009) that starts at the top left pixel and progresses toward the bottom right pixel. The advantage of the LSTM network is that it effectively handles extensive dependencies that are central to understanding objects and scenes, and the two-dimensional structure ensures that signals are well propagated in both the lower and lower right pixels."}, {"heading": "2.1. Generating an Image Pixel by Pixel", "text": "The goal is to assign a probability p (x) to each image consisting of n \u00b7 n pixels. To estimate the common distribution p (x), we write it as the product of the conditional distributions over the pixels: p (x) = n2, taking pixels in rows from the image. To estimate the common distribution p (x), we write it as the product of the conditional distributions over the pixels: p (x) = n2, i = 1 p (xi | x1,..., xi \u2212 1) (1) The value p (xi | x1,..., xi \u2212 1) is the probability of the i-th pixel xi when specifying all preceding pixels x1,..., xi \u2212 1. The generation is line by line and pixel by pixel. Figure 2 illustrates the conditioning scheme. Each pixel xi is in turn determined collectively by three values, one for each of the color channels red, green, and blue (RGB). We write the lt. < < < < < R < (x; the following product &p; <"}, {"heading": "2.2. Pixels as Discrete Variables", "text": "Previous approaches have used a continuous distribution for the values of the pixels in the image (e.g. Uria et al. (2014); Theis & Bethge (2015)). In contrast, we model p (x) as a discrete distribution, with each conditional distribution in Equation 2 being a multinomic distribution modelled with a softmax layer. Each channel variable xi simply takes one of 256 different values; the discrete distribution is representatively simple and has the advantage of being arbitrarily multimodal without first determining the shape; we also find experimentally that the discrete distribution is easy to learn and performs better than a continuous distribution (Section 5)."}, {"heading": "3. Pixel Recurrent Neural Networks", "text": "In this section, we describe the architectural components that make up the PixelRNN. In sections 3.1 and 3.2, we describe the two types of LSTM layers that coils use to calculate the states along one of the spatial dimensions at once. In section 3.3, we describe how to build residual joints to improve the formation of a PixelRNN with many LSTM layers. In section 3.4, we describe the Softmax layer, which calculates the discrete joint distribution of colors, and the masking technique that ensures the correct conditioning scheme. In section 3.5, we describe the PixelCNN architecture. Finally, in section 3.6, we describe the multi-scale architecture."}, {"heading": "3.1. Row LSTM", "text": "The line LSTM is a unidirectional layer that processes the image line by line from top to bottom ~ GI = hello GI row itself is used \u00b7 the calculation functions for an entire line at once; the calculation is performed with a one-dimensional fold. For one pixel xi, the layer captures a roughly triangular context above the pixel as shown in Figure 2 (center).The core of the one-dimensional fold is the size k \u00b7 1, where k \u2265 3; the greater the value of k is the broader context that is captured. The weight distribution in the fold ensures the translation invariance of the calculated properties along each line. The calculation proceeds as follows: An LSTM layer has an input-to-state component and a recursive state-to-state component that together determine the four gates within the LSTM core. To improve the parallelization in the line LSTM component, the state-to-input component is."}, {"heading": "3.2. Diagonal BiLSTM", "text": "The Diagonal BiLSTM is designed to both parallelise the calculation and capture the entire available context for each image size. Each of the two directions of the plane scans the image diagonally, starting from a corner at the top and ending at the opposite corner at the bottom. At the same time, each step in the calculation computes the LSTM state along a diagonal in the image. Figure 2 (right) illustrates the calculation and the resulting receiver field. The diagonal calculation proceeds as follows: We first sketch the input map into a space that makes it easy to apply the contours along the diagonals. The sketching operation shifts each line of the input map by one position relative to the previous line, as shown in Figure 3, resulting in a map of size n \u00d7 (2n \u2212 1) that compares the constellation of the input-to-state constellation and state-to-state components of the diagonal constellation."}, {"heading": "3.3. Residual Connections", "text": "In order to both increase the convergence speed and transmit signals more directly through the network, we use residual connections (He et al., 2015) from one LSTM layer to the next. Figure 4 (left) shows a diagram of the connections. The input map to the LSTM layer has 2h characteristics. The input-to-state component reduces the number of features by generating h-characteristics per gate. After applying the recursive layer, the output map is extrapolated via a 1 \u00d7 1 fold to 2h characteristics per position and the input map is added to the output map. This method is related to earlier approaches where gating is used along the depth of the recursive network (Kalchbrenner et al., 2015; Zhang et al., 2016), but has the advantage that no additional gates are required. Apart from residual connections, one can also use learnable skip connections from each layer to evaluate the residual connection effectiveness."}, {"heading": "3.4. Masked Convolution", "text": "The h-characteristics for each input position at each level in the network are divided into three parts, each corresponding to one of the RGB channels. In predicting the R channel for the current pixel xi, only the generated pixels on the left and above of xi can be used as context. In predicting the G channel, the value of the R channel can be used as context in addition to the previously generated pixels. Likewise, the values of both the R and G channels can be used for the B channel. To limit the connections in the network to these dependencies, we apply a mask to the input state volumes and other purely revolutionary layers in a PixelRNN. We use two types of masks, which we specify with Mask A and Mask B, as shown in Figure 4. Mask A is applied only to the first revolutionary layer in a PixelRNN and limits the connections to the adjacent pixels and to the current colors."}, {"heading": "3.5. PixelCNN", "text": "The line and diagonal LSTM layers have a potentially unlimited range of dependencies within their receiver field, which involves a lot of calculation because each state needs to be calculated sequentially. A simple solution is to make the receiver field large but not unlimited. We can use standard coil layers to capture a limited receiver field and calculate characteristics for all pixel positions at once. PixelCNN uses multiple coil layers that maintain spatial resolution; pooling layers are not used. Masks are used in the coils so as not to see the future context. Note that the advantage of parallelizing the PixelCNN over the PixelRNN is only available during training or during the evaluation of test images. The image generation process is sequential for both types of networks, as each sampled pixel must be returned to the network as input."}, {"heading": "3.6. Multi-Scale PixelRNN", "text": "The multidimensional PixelRNN consists of an unconditional PixelRNN and one or more conditional PixelRNN. The unconditional network first generates a smaller s x s image, just like a standard PixelRNN described above, where s is a smaller integer that divides n. The conditional network then takes the s x s image as an additional input and generates a larger n x n image. The conditional network resembles a standard PixelRNN, but each of its layers is distorted with an upsampled version of the small s x s s image. Upsampling and biasing processes are defined as follows. In the upsampling process, a conditional network is used with deconditional layers to construct an enlarged feature card of the size c x n x x x n, where c is the number of features in the output card of the upsampling network. In the upsampling process, a configuration 1 layer of 1 x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "4. Specifications of Models", "text": "In this section we specify the specifications of the PixelRNNs used in the experiments. We have four types of networks: the PixelRNN based on Row LSTM, which is based on Diagonal BiLSTM, the fully Convolutionary and the MultiScale one.Table 1 specifies each layer in the one-dimensional networks; the first layer is a 7 x 7 folding, which uses the Type A mask, which prevents the network from seeing (as input) the color it is supposed to predict; the two types of LSTM networks then use a variable number of recurring layers; the input-to-state folding in this layer uses a Type B mask, while the Imageto-State folding is not masked; the PixelCNN uses volumes of size 3 x 3 with a Type B mask; the uppermost feature card is then traversed by a few layers of Rowlater type rowaves, which run through a Reflected Linear and Unix (1)."}, {"heading": "5. Experiments", "text": "In this section we describe our experiments and results. We start by describing the way we evaluate and compare our results. In Section 5.2 we give details about the training. Then we give results on the relative effectiveness of architectural components and our best results on the data sets MNIST, CIFAR-10 and ImageNet."}, {"heading": "5.1. Evaluation", "text": "Although natural image data are normally modeled using continuous distributions using density functions, we can compare our results with previous work in the following way: In the literature, adding real noise to pixel values to dequantify the data using density functions is currently the best practice (Uria et al., 2013). If we add the values from the discrete distribution (with values in the interval [0, 1]), then the log probabilities of continuous and discrete models are directly comparable (Theis et al., 2015). In our case, we can use the values from the discrete distribution as a piecework-uniform continuous function that has a constant value for each interval [i, i + 1], i = 1, 2,.. 256. This corresponding distribution will have the same log probability (on data with additional noise) as the original discrete distribution (on discrete data)."}, {"heading": "5.2. Training Details", "text": "For smaller datasets like MNIST and CIFAR-10, we use smaller batch sizes of 16 images, as this seems to regulate the models. In ImageNet, we use as large a batch size as the GPU memory allows; this corresponds to 64 images / stacks for 32 x 32 ImageNet and 32 images / stacks for 64 x 64 ImageNet. Apart from scaling and centering the images at the input of the network, we do not use any other pre-processing or augmentation. For the multinomic loss function, we use the raw pixel color values as categories. For all PixelRNN models, we learn the initial state of the network."}, {"heading": "5.3. Discrete Softmax Distribution", "text": "In addition to being intuitive and easy to implement, we find that using a Softmax on discrete pixel values rather than a mixture density approach on continuous pixel values delivers better results; for the Row LSTM model with a Softmax output distribution, we get 3.06 bits / dim on the CIFAR10 validation set; for the same model with a Mixture of Conditional Gaussian Scale Mixtures (MCGSM) (Theis & Bethge, 2015), we get 3.22 bits / dimm.Figure 5 shows some Softmax activations from the model. Although we do not embed any prior information about the meaning or relationships of the 256 color categories, such as that the pixel values are 51 and 52 neighbors, the distributions predicted by the model are significant and can be multimodal, distorted, pointed, or dull."}, {"heading": "5.4. Residual Connections", "text": "Another core component of the networks is residual connections. In Table 2 we show the results of residual connections, standard skip connections or both in the 12-layer CIFAR-10 Row LSTM model. We see that using residual connections is just as effective as using skip connections; using both is also effective and protects the benefit. If we use both the residual and skip connections, we see in Table 3 that the performance of the Row LSTM improves with increasing depth, up to the 12 LSTM layers we have tried."}, {"heading": "5.5. MNIST", "text": "Although the goal of our work was to model natural images on a large scale, we have also tried our model on the binary version (Salakhutdinov & Murray, 2008) of MNIST (LeCun et al., 1998) because it is a good test of reason and there is a lot of previous work on this dataset to compare with. In Table 4 we report on the performance of the Diagonal BiLSTM model and previous published results."}, {"heading": "5.6. CIFAR-10", "text": "Next, we will test our models using the CIFAR-10 dataset (Krizhevsky, 2009).Table 5 lists the results of our models and those of previously published approaches. Of the proposed networks, the Diagonal BiLSTM performs best, followed by the Row LSTM and the Pixel CNN. This is consistent with the size of the respective receptive fields: the Diagonal BiLSTM has a global view, the Row LSTM a partially hidden view, and the PixelCNN sees the fewest pixels in context, indicating that effective coverage of a large receptive field is important. Figure 6 (left) shows CIFAR-10 samples generated from the Diagonal BiLSTM."}, {"heading": "5.7. ImageNet", "text": "Although, to our knowledge, there are no published results in the ILSVRC ImageNet dataset (Russakovsky et al., 2015) with which we can compare our models, we give our ImageNet log probability performance in Table 6. On ImageNet, the current PixelRNs do not seem to overmatch, as we have seen that their validation performance has improved with size and depth. Currently, the main obstacle to model size is computation time and GPU memory. Note that ImageNet models are generally less compressible than the CIFAR-10 images. ImageNet has more image diversity and the images have most likely been reduced by a different algorithm than the one we use for imaging. ImageNet images are less blurry, which means that adjacent pixels are less correlated to each other and therefore less predictable. As the downsampling method affects the compression performance, we will be able to release the images we have ampled."}, {"heading": "6. Conclusions", "text": "We have described novel two-dimensional LSTM layers: the Row LSTM and the Diagonal BiLSTM, which are easier to scale to larger datasets; the models have been trained to model the RGB pixel values of images; we have treated the pixel values as discrete random variables by using a Softmax layer in the conditional distributions; we have shown that the pixel RNNNs greatly improve the state of the art on the binary MNIST and CIFAR-10 datasets; we are also providing new benchmarks for generative image modeling on the ImageNet dataset; and based on the samples and completions from the larger models that we can improve in terms of spatial and spatial conditions, we are likely to provide these datasets."}, {"heading": "Acknowledgements", "text": "The authors thank Shakir Mohamed and Guillaume Desjardins for their helpful contributions to this paper and Lucas Theis, Alex Graves, Karen Simonyan, Lasse Espeholt, Danilo Rezende, Karol Gregor and Ivo Danihelka for their insightful discussions."}], "references": [{"title": "NICE: Non-linear independent components estimation", "author": ["Dinh", "Laurent", "Krueger", "David", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1410.8516,", "citeRegEx": "Dinh et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dinh et al\\.", "year": 2014}, {"title": "MADE: Masked autoencoder for distribution estimation", "author": ["Germain", "Mathieu", "Gregor", "Karol", "Murray", "Iain", "Larochelle", "Hugo"], "venue": "arXiv preprint arXiv:1502.03509,", "citeRegEx": "Germain et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Germain et al\\.", "year": 2015}, {"title": "Generating sequences with recurrent neural networks", "author": ["Graves", "Alex"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves and Alex.,? \\Q2013\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2013}, {"title": "Deep autoregressive networks", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Mnih", "Andriy", "Blundell", "Charles", "Wierstra", "Daan"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Gregor et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2014}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Wierstra", "Daan"], "venue": "Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Nal", "Blunsom", "Phil"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Grid long short-term memory", "author": ["Kalchbrenner", "Nal", "Danihelka", "Ivo", "Graves", "Alex"], "venue": "arXiv preprint arXiv:1507.01526,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2015}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex"], "venue": null, "citeRegEx": "Krizhevsky and Alex.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Alex.", "year": 2009}, {"title": "The neural autoregressive distribution estimator", "author": ["Larochelle", "Hugo", "Murray", "Iain"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Larochelle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2011}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Evaluating probabilities under high-dimensional latent variable models", "author": ["Murray", "Iain", "Salakhutdinov", "Ruslan R"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Murray et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Murray et al\\.", "year": 2009}, {"title": "Connectionist learning of belief networks", "author": ["Neal", "Radford M"], "venue": "Artificial intelligence,", "citeRegEx": "Neal and M.,? \\Q1992\\E", "shortCiteRegEx": "Neal and M.", "year": 1992}, {"title": "Iterative neural autoregressive distribution estimator NADE-k", "author": ["Raiko", "Tapani", "Li", "Yao", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Raiko et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Raiko et al\\.", "year": 2014}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo J", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Deep boltzmann machines", "author": ["Salakhutdinov", "Ruslan", "Hinton", "Geoffrey E"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2009}, {"title": "On the quantitative analysis of deep belief networks", "author": ["Salakhutdinov", "Ruslan", "Murray", "Iain"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2008}, {"title": "Markov chain monte carlo and variational inference: Bridging the gap", "author": ["Salimans", "Tim", "Kingma", "Diederik P", "Welling", "Max"], "venue": "Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Salimans et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2015}, {"title": "Deep unsupervised learning using nonequilibrium thermodynamics", "author": ["Sohl-Dickstein", "Jascha", "Weiss", "Eric A", "Maheswaranathan", "Niru", "Ganguli", "Surya"], "venue": "Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Sohl.Dickstein et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sohl.Dickstein et al\\.", "year": 2015}, {"title": "Parallel multi-dimensional lstm, with application to fast biomedical volumetric image segmentation", "author": ["Stollenga", "Marijn F", "Byeon", "Wonmin", "Liwicki", "Marcus", "Schmidhuber", "Juergen"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Stollenga et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Stollenga et al\\.", "year": 2015}, {"title": "Generating text with recurrent neural networks", "author": ["Sutskever", "Ilya", "Martens", "James", "Hinton", "Geoffrey E"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Generative image modeling using spatial LSTMs", "author": ["Theis", "Lucas", "Bethge", "Matthias"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Theis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2015}, {"title": "A note on the evaluation of generative models", "author": ["Theis", "Lucas", "van den Oord", "A\u00e4ron", "Bethge", "Matthias"], "venue": "arXiv preprint arXiv:1511.01844,", "citeRegEx": "Theis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2015}, {"title": "RNADE: The real-valued neural autoregressive densityestimator", "author": ["Uria", "Benigno", "Murray", "Iain", "Larochelle", "Hugo"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Uria et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Uria et al\\.", "year": 2013}, {"title": "Factoring variations in natural images with deep gaussian mixture models", "author": ["van den Oord", "A\u00e4ron", "Schrauwen", "Benjamin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Oord et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2014}, {"title": "The student-t mixture as a natural image patch prior with application to image compression", "author": ["van den Oord", "A\u00e4ron", "Schrauwen", "Benjamin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Oord et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2014}, {"title": "Highway long shortterm memory RNNs for distant speech recognition", "author": ["Zhang", "Yu", "Chen", "Guoguo", "Dong", "Yao", "Kaisheng", "Khudanpur", "Sanjeev", "Glass", "James"], "venue": "In Proceedings of the International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 16, "context": "Most work focuses on stochastic latent variable models (Rezende et al., 2014; Kingma & Welling, 2013) that aim to extract meaningful representations, but often come with an intractable inference step that can hinder their performance.", "startOffset": 55, "endOffset": 101}, {"referenceID": 22, "context": "RNNs have been shown to excel at hard sequence problems ranging from handwriting generation (Graves, 2013), to character prediction (Sutskever et al., 2011) and to machine translation (Kalchbrenner & Blunsom, 2013).", "startOffset": 132, "endOffset": 156}, {"referenceID": 21, "context": "The first type is the Row LSTM layer where the convolution is applied along each row; a similar technique is described in (Stollenga et al., 2015).", "startOffset": 122, "endOffset": 146}, {"referenceID": 5, "context": "The networks also incorporate residual connections (He et al., 2015) around LSTM layers; we observe that this helps with training of the PixelRNN for up to twelve layers of depth.", "startOffset": 51, "endOffset": 68}, {"referenceID": 3, "context": ", Theis & Bethge (2015); Gregor et al. (2014)), we model the pixels as discrete values using a multinomial distribution implemented with a simple softmax layer.", "startOffset": 25, "endOffset": 46}, {"referenceID": 25, "context": "Uria et al. (2014); Theis & Bethge (2015)).", "startOffset": 0, "endOffset": 19}, {"referenceID": 25, "context": "Uria et al. (2014); Theis & Bethge (2015)).", "startOffset": 0, "endOffset": 42}, {"referenceID": 5, "context": "As a means to both increase convergence speed and propagate signals more directly through the network, we deploy residual connections (He et al., 2015) from one LSTM layer to the next.", "startOffset": 134, "endOffset": 151}, {"referenceID": 8, "context": "This method is related to previous approaches that use gating along the depth of the recurrent network (Kalchbrenner et al., 2015; Zhang et al., 2016), but has the advantage of not requiring additional gates.", "startOffset": 103, "endOffset": 150}, {"referenceID": 28, "context": "This method is related to previous approaches that use gating along the depth of the recurrent network (Kalchbrenner et al., 2015; Zhang et al., 2016), but has the advantage of not requiring additional gates.", "startOffset": 103, "endOffset": 150}, {"referenceID": 3, "context": "Similar masks have also been used in (variational) autoencoders (Gregor et al., 2014; Germain et al., 2015).", "startOffset": 64, "endOffset": 107}, {"referenceID": 1, "context": "Similar masks have also been used in (variational) autoencoders (Gregor et al., 2014; Germain et al., 2015).", "startOffset": 64, "endOffset": 107}, {"referenceID": 25, "context": "In the literature it is currently best practice to add realvalued noise to the pixel values to dequantize the data when using density functions (Uria et al., 2013).", "startOffset": 144, "endOffset": 163}, {"referenceID": 23, "context": "When uniform noise is added (with values in the interval [0, 1]), then the log-likelihoods of continuous and discrete models are directly comparable (Theis et al., 2015).", "startOffset": 149, "endOffset": 169}, {"referenceID": 23, "context": "These numbers are interpretable as the number of bits that a compression scheme based on this model would need to compress every RGB color value (van den Oord & Schrauwen, 2014b; Theis et al., 2015); in practice there is also a small overhead due to arithmetic coding.", "startOffset": 145, "endOffset": 198}, {"referenceID": 12, "context": "Although the goal of our work was to model natural images on a large scale, we also tried our model on the binary version (Salakhutdinov & Murray, 2008) of MNIST (LeCun et al., 1998) as it is a good sanity check and there is a lot of previous art on this dataset to compare with.", "startOffset": 162, "endOffset": 182}, {"referenceID": 15, "context": ", 2014), [4] (Raiko et al., 2014), [5] (Rezende et al.", "startOffset": 13, "endOffset": 33}, {"referenceID": 16, "context": ", 2014), [5] (Rezende et al., 2014), [6] (Salimans et al.", "startOffset": 13, "endOffset": 35}, {"referenceID": 19, "context": ", 2014), [6] (Salimans et al., 2015), [7] (Gregor et al.", "startOffset": 13, "endOffset": 36}, {"referenceID": 3, "context": ", 2015), [7] (Gregor et al., 2014), [8] (Germain et al.", "startOffset": 13, "endOffset": 34}, {"referenceID": 1, "context": ", 2014), [8] (Germain et al., 2015), [9] (Gregor et al.", "startOffset": 13, "endOffset": 35}, {"referenceID": 4, "context": ", 2015), [9] (Gregor et al., 2015).", "startOffset": 13, "endOffset": 34}, {"referenceID": 0, "context": "[1] (Dinh et al., 2014), [2] (Sohl-Dickstein et al.", "startOffset": 4, "endOffset": 23}, {"referenceID": 20, "context": ", 2014), [2] (Sohl-Dickstein et al., 2015), [3] (van den Oord & Schrauwen, 2014a).", "startOffset": 13, "endOffset": 42}], "year": 2016, "abstractText": "Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast twodimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.", "creator": "LaTeX with hyperref package"}}}