{"id": "1602.00133", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2016", "title": "SCOPE: Scalable Composite Optimization for Learning on Spark", "abstract": "Many machine learning models, such as logistic regression~(LR) and support vector machine~(SVM), can be formulated as composite optimization problems. Recently, many distributed stochastic optimization~(DSO) methods have been proposed to solve the large-scale composite optimization problems, which have shown better performance than traditional batch methods. However, most of these DSO methods are not scalable enough. In this paper, we propose a novel DSO method, called \\underline{s}calable \\underline{c}omposite \\underline{op}timization for l\\underline{e}arning~({SCOPE}), and implement it on the fault-tolerant distributed platform \\mbox{Spark}. SCOPE is both computation-efficient and communication-efficient. Theoretical analysis shows that SCOPE is convergent with linear convergence rate when the objective function is convex. Furthermore, empirical results on real datasets show that SCOPE can outperform other state-of-the-art distributed learning methods on Spark, including both batch learning methods and DSO methods.", "histories": [["v1", "Sat, 30 Jan 2016 16:11:53 GMT  (574kb,D)", "https://arxiv.org/abs/1602.00133v1", null], ["v2", "Sun, 7 Feb 2016 07:07:56 GMT  (597kb,D)", "http://arxiv.org/abs/1602.00133v2", null], ["v3", "Wed, 1 Jun 2016 07:50:39 GMT  (366kb,D)", "http://arxiv.org/abs/1602.00133v3", null], ["v4", "Thu, 2 Jun 2016 07:01:25 GMT  (366kb,D)", "http://arxiv.org/abs/1602.00133v4", null], ["v5", "Sun, 11 Dec 2016 16:10:37 GMT  (503kb,D)", "http://arxiv.org/abs/1602.00133v5", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["shen-yi zhao", "ru xiang", "ying-hao shi", "peng gao", "wu-jun li"], "accepted": true, "id": "1602.00133"}, "pdf": {"name": "1602.00133.pdf", "metadata": {"source": "META", "title": "SCOPE: Scalable Composite Optimization for Learning on Spark", "authors": ["Shen-Yi Zhao", "Ru Xiang", "Ying-Hao Shi", "Peng Gao", "Wu-Jun Li"], "emails": ["zhaosy@lamda.nju.edu.cn,", "xiangr@lamda.nju.edu.cn,", "shiyh@lamda.nju.edu.cn,", "gaop@lamda.nju.edu.cn,", "liwujun@nju.edu.cn"], "sections": [{"heading": "Introduction", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "SCOPE", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Framework of SCOPE", "text": "SCOPE is based on a distributed master-slave framework shown in Figure 1. Specifically, there is a master machine (called a master) and p (p \u2265 1) slave machines (called workers) in the cluster. These workers are called Worker 1, Worker 2, \u00b7 \u00b7 \u00b7 or Worker p, respectively."}, {"heading": "Data Partition and Parameter Storage", "text": "As a matter of fact, it is a way in which most people are able to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight."}, {"heading": "Communication Cost", "text": "Traditional mini-batch-based distributed SGD methods, such as DisSVRG in the appendix, need to transfer the parameters w and stochastic gradients frequently between workers and masters. The number of communication times is O (Tn) for DisSVRG. Other traditional mini-batch-based distributed SGD methods have the same number of communication times. Typically, traditional mini-batch-based methods are O (Tn) number of communication times, which can lead to high communication costs.Most of the training (computation) loads of the SCOPE come from the internal loop of algorithm 2, which is performed in local workers without any communication whatsoever. It is easy to find that the number of communication times in SCOPE O (T) is dramatically less than O (Tn) of traditional mini-batch-based distributed SGD or distributed SVRG methods. In the following section we will prove that SCOPE has linear convergence, which is optimal when the number of convergences is not great in terms of the number of results."}, {"heading": "SCOPE on Spark", "text": "s basic programming model is MapReduce, which is actually a BSP model. In SCOPE, the task of workers calculating the local gradient sum zk and the training procedure in the inner loop of Algorithm 2 can be considered a map process, as both use only local data. Master's task, which calculates the average for both full gradient z and wt + 1, can be considered a reduction process. MapReduce's programming model is essentially a synchronous model that requires some synchronization costs. Fortunately, the number of synchronization times mentioned above is very low. Therefore, both the communication costs and the waiting time for SCOPE are very short. In this essay, we implement our SCOPE on Spark, as Spark is widely used in the industry for big data applications and our data pipeline is easily integrated by SCOPE."}, {"heading": "Convergence of SCOPE", "text": "In this section, we will demonstrate the convergence of SCOPE if the objective functions are strongly convex = = b > b). We list only a few terms and theorems, the detailed proof of which can be found in the appendices (Zhao et al. 2016). For convenience, we use \"w\" to denote the optimal solution. In practice, we cannot necessarily guarantee that these values are the same. However, it is easy to guarantee that each worker has the same number of training instances and D1 | = = Dp | = q. In practice, we cannot necessarily guarantee that these values are the same."}, {"heading": "Impact of Parameter c", "text": "In Algorithm 2 we need the parameter c to determine the convergence of SCOPE (k = k) k (k = k) k (k = k) k (c > L \u2212 \u00b5 to Lemma 1).We first start from c = 0 and try to find out whether algorithm 2 converges or not. It means that in the following derivative we always define another local function: F (t) k (w) = Fk (w) + (z \u2212 Fk (wt))) T (w) and denote w (k) k = arg min w (t) k (w).Let vk, m = fik, m) \u2212 fik, m \u2212 fik, m (wt) + c (uk + c (m \u2212 wt).If c = 0, vk, m = W (k).we"}, {"heading": "Separating Data Uniformly", "text": "If we separate the data uniformly, which means that the local data distribution for each worker is similar to the global data distribution, then we have Ak \u2248 A and HI \u2212 1 p \u2211 p i = 1 A \u2212 1 k A \u0445 \u2248 0. From (1) we can conclude that c = 0 can make the SCOPE converge for this particular case."}, {"heading": "Experiment", "text": "We choose logistic regression (LR) with an L2norm regularization term to evaluate SCOPE and baselines. Therefore, P (w) is defined as P (w) = 1 n \u2211 n i = 1 [log (1 + e \u2212 yix T i w) + \u03bb2 \u0445w \u0445 2] The code can be downloaded from the following address: https: / / github.com / LIBBLE / LIBBLE-Spark /."}, {"heading": "Dataset", "text": "We use four data sets for evaluation: MNIST-8M, epsilon, KDD12 and Data-A. The first two data sets can be downloaded from the LibSVM website3. MNIST-8M contains 8,100,000 handwritten digits. We set the instances of the digits 5 to 9 as positive and set the instances of the digits 0 to 4 as negative. KDD12 is the data set of Track 1 for the KDD Cup 2012, which can be downloaded from the KDD Cup website4. Data-A is a data set from a data mining contest 5. The information about these data sets is in Table 2. All data is standardized before training. The regulation hyperparameter \u03bb is set to 10 \u2212 4 for the first three data sets, which are relatively small, and to 10 \u2212 4 for the largest data set Data-A. A similar phenomenon can be observed in others, which are not considered due to space constraints."}, {"heading": "Experimental Setting and Baseline", "text": "This year, the number of participants in international competitions has tripled compared to previous years, and the number of participants in international competitions has tripled in recent years."}, {"heading": "Efficiency Comparison with Baselines", "text": "We compare SCOPE with other baselines on the four sets of data. The result is in Figure 2. Each highlighted point on the curves denotes an update for w by the master, which typically corresponds to an iteration in the outer loop. For SCOPE, good convergence results can be achieved with a number of updates (i.e. the T in algorithm 1) of less than five. We can find that Splash vibrates on some sets of data because it introduces variance in the training process. On the contrary, SCOPE are stable, which means that SCOPE is a variance reduction method like SVRG. It is easy to see that SCOPE has a linear convergence rate that is also consistent with our theoretical analysis. In addition, SCOPE is much faster than all other baselines. SCOPE can surpass SVRGfoR (Konecny \u0301, McMahan and Ramage 2015) and DisSVRG. An experimental comparison is available in the Appendix 2016 (to be found)."}, {"heading": "Speedup", "text": "We use the data set MNIST-8M to accelerate the evaluation of SCOPE. Per machine, two cores are used. We evaluate the acceleration by increasing the number of machines. If the gap between the objective functional value and the optimal value is less than 10 \u2212 10, the training process is terminated as follows: speedup = 7https: / / www.csie.ntu.edu.tw / \u0445 cjlin / liblinear / 8http: / / zhangyuc.github.io / splash 9https: / / github.com / gingsmith / coa10https: / / github.com / gingsmith / coatime with 16 cores by SCOPE time with 2\u03c0 cores, where the number of machines is, and we opt for the SCOP setting = 8, 16, 24, 32. The experiments are executed five times faster and the average time will explain the final synchronization result as SCOPE, where SCOP is higher."}, {"heading": "Conclusion", "text": "In this paper, we propose a novel DSO method called SCOPE for distributed machine learning on Spark. Theoretical analyses show that SCOPE converges at linear convergence rate in strongly convex cases. Empirical results show that SCOPE can outperform other state-of-the-art distributed methods on Spark."}, {"heading": "Acknowledgements", "text": "This work is partially supported by the \"DengFeng\" project of Nanjing University."}, {"heading": "Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "SVRG and Mini-Batch based Distributed SVRG", "text": "The sequential SVRG is outlined in algorithm 3, which corresponds to the original SVRG in (Johnson and Zhang 2013). (Algorithm 3 Sequential SVRG initialization: initialize w0, \u03b7; for t = 0, 1, 2,..., T do u0 = wt; Calculate the complete gradient z = 1n, i = 1, fi (u0); for m = 0 to M \u2212 1 do random in {1,..., n}; um + 1 = um \u2212 \u03b7 (um) fim (u0) + z); end for Take wt + 1 to be uM or the average of {um}; end forThe mini-batch based distributed SVRG (called DisSVRG) is completed in algorithm 4 and algorithm Sum 5, with algorithm fum Sm."}, {"heading": "Proof of Lemma 1", "text": "We define the local stochastic gradient in algorithm 2 as follows: vk, m = bay, m (uk, m) \u2212 fik, m (wt) + z + c (uk, m \u2212 wt).Then the update rule can be rewritten for local workers as follows: uk, m + 1 = uk, m \u2212 \u03b7vk, m (2) First, we specify the expectation and variation properties of vk, m in Lemma 2 and Lemma 3. Lemma 2. Conditional expectation of the local stochastic gradient vk, m in uk, m in uk, m isE [vk, m | uk, m] = Fk (m) \u2212 Fk (wt) + z + c (uk, m \u2212 wt).Algorithm 5 task of the workers in DisSVRG For the worker k: for t = 0, 1, 2,., T doit until it receives the newest parameter from the master; assigned for the local gradient in Sollyk, the Sollyk, the Sollyk, the Sollyk in the Sollyk, the Sollyk, the Sollyk in the Sollyk, the Sollyk, the Sollyk in the Sollyk, the Sollyk in the Sollyk, the Sollyk in the Sollyk, the Sollyk in the Sollyk, the Sollyk in the Sollyk, the Sollyk in the Sollyk, the Sollyk in the Sollyk, the Sollyk in the Sollyk, the Sollyk, the Sollyk in the Sollyk in the Sollyk, the Sollyk in the Sollyk, the Sollyk in the Sollyk, the Sollyk in the Sollyk, the Sollyk in the Sollyk, the Sollyk the Sollyk, the Sollyk in the Sollyk, the Sollyk the Sollyk in the Sollyk, the Sollyk the Sollyk, the Sollyk in the Solly"}, {"heading": "Proof.", "text": "E [vk, m | uk, m] = 1q \u00b2 i-Dk [\u0441fi (uk, m) \u2212 \u0441fi (uk, m \u2212 wt) + z + c (uk, m \u2212 wt)] = \u0435Fk (uk, m) \u2212 \u0430 Fk (wt) + z + c (uk, m \u2212 wt) Lemma 3. The variance of vk, m has the following property: E [\u0102vk, m, m, m-2 | uk, m] \u2264 3 (L2 + c2) \u0441uk, m \u2212 wt-2 + 3L2 \u0441wt \u2212 w-2."}, {"heading": "Proof.", "text": "Fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth"}, {"heading": "Proof of Theorem 1", "text": "Evidence. According to Lemma 1 we have a value of \u03b1\u03b3m \u2212 1 \u2264 (\u03b1m + \u03b2 1 \u2212 \u03b1) \u03b30Since we proceed from wt + 1 = 1p p k = 1 uk, M we have a value of E-wt + 1 \u2212 w = E-1p \u0445k = 1 uk, M-w = 1 E-k, M-w = 1 E-uk, M-w \u2264 (\u03b1M + 1 \u2212 w = E-wt \u2212 w = E-1p \u0445k, M-w = 1 p k = 1 E-k, M-w = 1 M \u2264 (\u03b1M + 1 \u2212 w = E-wt) E-wt \u2212 wt = E-wt = E-wt = E-wt = E-wt = E-wt = E-wt = E-wt = E-wt = E-wt = E-wt = E-wt = E-wt = E-wt = E-wt = E-wt = E-wt = E-wt = E-w = E-wt = E-wt = E-wt = E-w = E-wt = E-wt = E-w = E-wt = E-wt = E-wt = E-w = E-wt = E-w = E-wt = E-w = E-w = E-wt = E-wn"}, {"heading": "Proof of Theorem 2", "text": "Proof. According to Lemma 1 we have + 1 + (1 \u2212 \u03b1) \u03b3m \u2264 \u03b3m + \u03b2\u03b30Summed up m from 1 to M, we have + 1 + (1 \u2212 \u03b1) M \u0445 m = 1\u03b3m \u2264 (1 + M\u03b2) \u03b30Since we take gew + 1 = 1pM = 1pM + M = 1 \u0445 p k = 1 uk, m, then we have E-wt + 1 \u2212 w-w-2 = E-1pM M M-m = 1 p-k = 1 uk, M-w-m-2 \u2264 1 pM-k = 1-uk, M-w-w-p-2 \u2264 (1 (1 \u2212 \u03b1) + \u03b2 1 \u2212 \u03b1) E-wt \u2212 w-2"}, {"heading": "Efficiency Comparison with DisSVRG", "text": "In this section, we compare SCOPE with the distributed minibatch-based SVRG (referred to as DisSVRG) in algorithm 4 and algorithm 5 using the MNIST-8M dataset. The result is in Figure 4. The x-axis is the CPU time, which contains both computation and synchronization time, where the unit is millisecond. The y-axis is the objective functional value minus the optimal value in a protocol scale. In this essay, the optimal value is the minimum value achieved by running all baselines and SCOPE for a large number of iterations. It is easy to see that DisSVRG is much slower than our SCOPE, which means that the traditional minibatch-based DSO strategy is not scalable due to huge communication costs."}, {"heading": "Efficiency Comparison with SVRGfoR", "text": "SVRGfoR (Konecny \u0301, McMahan, and Ramage 2015) is proposed for cases where the number of workers is relatively large, the data partitions are unbalanced, and the characteristics are sparse. We use the KDD12 dataset with sparse features for evaluation. We form a large cluster of 1600 workers. In addition, we partition the data in an unbalanced way. The largest number of data points of a worker is 423954, and the smallest number of data points of a worker is 28. We adjust several increments for SVRGfoR to achieve the best performance. Experimental results are shown in Figure 5."}, {"heading": "Synchronization Cost", "text": "One shortcoming of the synchronous framework of SCOPE is that, in addition to the computing costs, there are synchronization costs. Synchronization costs include both communication time and waiting time. Fortunately, the synchronization costs of SCOPE are low, as most calculations are performed locally and only a small number of synchronization times are required. We use experiments to verify this. We use the MNIST-8M data set for the evaluation. The result is shown in Figure 6. the x-axis is the number of cores, the y-axis is the CPU time (in milliseconds) per iteration, which is calculated as a division of the total time by the number of iterations T. Please note that the CPU time includes both computation time and synchronization time (costs)."}], "references": [{"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "In International Conference on Computational Statistics", "citeRegEx": "Bottou,? \\Q2010\\E", "shortCiteRegEx": "Bottou", "year": 2010}, {"title": "Efficient distributed SGD with variance reduction", "author": ["S. De", "T. Goldstein"], "venue": "In IEEE International Conference on Data Mining", "citeRegEx": "De and Goldstein,? \\Q2016\\E", "shortCiteRegEx": "De and Goldstein", "year": 2016}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J.C. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research 12:2121\u20132159", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Passcode: Parallel asynchronous stochastic dual co-ordinate descent", "author": ["C.-J. Hsieh", "H.-F. Yu", "I.S. Dhillon"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Hsieh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hsieh et al\\.", "year": 2015}, {"title": "On variance reduction in stochastic gradient descent and its asynchronous variants", "author": ["S.J. Reddi", "A. Hefny", "S. Sra", "B. Poczos", "A.J. Smola"], "venue": "In Neural Information Processing Systems", "citeRegEx": "Reddi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Reddi et al\\.", "year": 2015}, {"title": "Communication-efficient distributed dual coordinate ascent", "author": ["M. Jaggi", "V. Smith", "M. Takac", "J. Terhorst", "S. Krishnan", "T. Hofmann", "M.I. Jordan"], "venue": "In Neural Information Processing Systems", "citeRegEx": "Jaggi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaggi et al\\.", "year": 2014}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["R. Johnson", "T. Zhang"], "venue": "In Neural Information Processing Systems", "citeRegEx": "Johnson and Zhang,? \\Q2013\\E", "shortCiteRegEx": "Johnson and Zhang", "year": 2013}, {"title": "Federated optimization: Distributed optimization beyond the datacenter", "author": ["J. Konecn\u00fd", "B. McMahan", "D. Ramage"], "venue": null, "citeRegEx": "Konecn\u00fd et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Konecn\u00fd et al\\.", "year": 2015}, {"title": "Distributed stochastic variance reduced gradient methods and a lower bound for communication complexity. arXiv:1507.07595v2", "author": ["J.D. Lee", "Q. Lin", "T. Ma", "T. Yang"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Scaling distributed machine learning with the parameter server", "author": ["M. Li", "D.G. Andersen", "J.W. Park", "A.J. Smola", "A. Ahmed", "V. Josifovski", "J. Long", "E.J. Shekita", "B. Su"], "venue": "In USENIX Symposium on Operating Systems Design and Implementation", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Large-scale logistic regression and linear support vector machines using spark", "author": ["C.-Y. Lin", "C.-H. Tsai", "C.-P. Lee", "C.-J. Lin"], "venue": "In IEEE International Conference on Big Data", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "An accelerated proximal coordinate gradient method", "author": ["Q. Lin", "Z. Lu", "L. Xiao"], "venue": "In Neural Information Processing Systems", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "An asynchronous parallel stochastic coordinate descent algorithm", "author": ["J. Liu", "S.J. Wright", "C. R\u00e9", "V. Bittorf", "S. Sridhar"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Liu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "Adding vs. averaging in distributed primal-dual optimization", "author": ["C. Ma", "V. Smith", "M. Jaggi", "M.I. Jordan", "P. Richt\u00e1rik", "M. Tak\u00e1c"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Ma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Perturbed iterate analysis for asynchronous stochastic optimization. arXiv:1507.06970", "author": ["H. Mania", "X. Pan", "D.S. Papailiopoulos", "B. Recht", "K. Ramchandran", "M.I. Jordan"], "venue": null, "citeRegEx": "Mania et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mania et al\\.", "year": 2015}, {"title": "Mllib: Machine learning in apache spark", "author": ["X. Meng", "J. Bradley", "B. Yavuz", "E. Sparks", "S. Venkataraman", "D. Liu", "J. Freeman", "D. Tsai", "M. Amde", "S. Owen", "D. Xin", "R. Xin", "M.J. Franklin", "R. Zadeh", "M. Zaharia", "A. Talwalkar"], "venue": "Journal of Machine Learning Research 17(34):1\u20137", "citeRegEx": "Meng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Meng et al\\.", "year": 2016}, {"title": "Stochastic proximal gradient descent with acceleration techniques", "author": ["A. Nitanda"], "venue": "In Neural Information Processing Systems", "citeRegEx": "Nitanda,? \\Q2014\\E", "shortCiteRegEx": "Nitanda", "year": 2014}, {"title": "Hogwild!: A lock-free approach to parallelizing stochastic gradient descent", "author": ["B. Recht", "C. Re", "S.J. Wright", "F. Niu"], "venue": "In Neural Information Processing Systems", "citeRegEx": "Recht et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Recht et al\\.", "year": 2011}, {"title": "Scaling structure learning of probabilistic logic programs by mapreduce", "author": ["F. Riguzzi", "E. Bellodi", "R. Zese", "G. Cota", "E. Lamma"], "venue": "In European Conference on Artificial Intelligence", "citeRegEx": "Riguzzi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Riguzzi et al\\.", "year": 2016}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["M.W. Schmidt", "N.L. Roux", "F.R. Bach"], "venue": "CoRR abs/1309.2388", "citeRegEx": "Schmidt et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Schmidt et al\\.", "year": 2013}, {"title": "Stochastic dual coordinate ascent methods for regularized loss", "author": ["S. Shalev-Shwartz", "T. Zhang"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Shalev.Shwartz and Zhang,? \\Q2013\\E", "shortCiteRegEx": "Shalev.Shwartz and Zhang", "year": 2013}, {"title": "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization", "author": ["S. Shalev-Shwartz", "T. Zhang"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Shalev.Shwartz and Zhang,? \\Q2014\\E", "shortCiteRegEx": "Shalev.Shwartz and Zhang", "year": 2014}, {"title": "Dual averaging method for regularized stochastic learning and online optimization", "author": ["L. Xiao"], "venue": "In Neural Information Processing Systems", "citeRegEx": "Xiao,? \\Q2009\\E", "shortCiteRegEx": "Xiao", "year": 2009}, {"title": "Petuum: A new platform for distributed machine learning on big data", "author": ["E.P. Xing", "Q. Ho", "W. Dai", "J.K. Kim", "J. Wei", "S. Lee", "X. Zheng", "P. Xie", "A. Kumar", "Y. Yu"], "venue": "In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining", "citeRegEx": "Xing et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xing et al\\.", "year": 2015}, {"title": "Trading computation for communication: Distributed stochastic dual coordinate ascent", "author": ["T. Yang"], "venue": "In Neural Information Processing Systems", "citeRegEx": "Yang,? \\Q2013\\E", "shortCiteRegEx": "Yang", "year": 2013}, {"title": "Distributed stochastic ADMM for matrix factorization", "author": ["Z.-Q. Yu", "X.-J. Shi", "L. Yan", "W.-J. Li"], "venue": "In International Conference on Conference on Information and Knowledge Management", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Spark: Cluster computing with working sets", "author": ["M. Zaharia", "M. Chowdhury", "M.J. Franklin", "S. Shenker", "I. Stoica"], "venue": "In USENIX Workshop on Hot Topics in Cloud Computing", "citeRegEx": "Zaharia et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zaharia et al\\.", "year": 2010}, {"title": "Splash: User-friendly programming interface for parallelizing stochastic algorithms. CoRR abs/1506.07552", "author": ["Y. Zhang", "M.I. Jordan"], "venue": null, "citeRegEx": "Zhang and Jordan,? \\Q2015\\E", "shortCiteRegEx": "Zhang and Jordan", "year": 2015}, {"title": "Asynchronous distributed ADMM for consensus optimization", "author": ["R. Zhang", "J.T. Kwok"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Zhang and Kwok,? \\Q2014\\E", "shortCiteRegEx": "Zhang and Kwok", "year": 2014}, {"title": "Deep learning with elastic averaging SGD", "author": ["S. Zhang", "A. Choromanska", "Y. LeCun"], "venue": "In Neural Information Processing Systems", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Linear convergence with condition number independent access of full gradients", "author": ["L. Zhang", "M. Mahdavi", "R. Jin"], "venue": "In Neural Information Processing Systems", "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}, {"title": "Communication-efficient algorithms for statistical optimization", "author": ["Y. Zhang", "M.J. Wainwright", "J.C. Duchi"], "venue": "In Neural Information Processing Systems", "citeRegEx": "Zhang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2012}, {"title": "Asynchronous distributed semi-stochastic gradient optimization", "author": ["R. Zhang", "S. Zheng", "J.T. Kwok"], "venue": "In AAAI Conference on Artificial Intelligence", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Fast asynchronous parallel stochastic gradient descent: A lock-free approach with convergence guarantee", "author": ["Zhao", "S.-Y", "Li", "W.-J"], "venue": "In AAAI Conference on Artificial Intelligence", "citeRegEx": "Zhao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2016}, {"title": "Accelerated mini-batch randomized block coordinate descent method", "author": ["T. Zhao", "M. Yu", "Y. Wang", "R. Arora", "H. Liu"], "venue": "In Neural Information Processing Systems", "citeRegEx": "Zhao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2014}, {"title": "SCOPE: scalable composite optimization for learning on Spark", "author": ["S.-Y. Zhao", "R. Xiang", "Y.-H. Shi", "P. Gao", "W.-J. Li"], "venue": "CoRR abs/1602.00133", "citeRegEx": "Zhao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2016}, {"title": "Parallelized stochastic gradient descent", "author": ["M. Zinkevich", "M. Weimer", "L. Li", "A.J. Smola"], "venue": "In Neural Information Processing Systems", "citeRegEx": "Zinkevich et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zinkevich et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 6, "context": "The first category is stochastic gradient descent (SGD) and its variants, such as stochastic average gradient (SAG) (Schmidt, Roux, and Bach 2013) and stochastic variance reduced gradient (SVRG) (Johnson and Zhang 2013), which try to perform optimization on the primal problem.", "startOffset": 195, "endOffset": 219}, {"referenceID": 20, "context": "The second category, such as stochastic dual coordinate ascent (SDCA) (Shalev-Shwartz and Zhang 2013), tries to perform optimization with the dual formulation.", "startOffset": 70, "endOffset": 101}, {"referenceID": 17, "context": "Hence, recent progress of PSO mainly focuses on designing asynchronous or lock-free optimization strategies (Recht et al. 2011; Liu et al. 2014; Hsieh, Yu, and Dhillon 2015; J. Reddi et al. 2015; Zhao and Li 2016).", "startOffset": 108, "endOffset": 213}, {"referenceID": 12, "context": "Hence, recent progress of PSO mainly focuses on designing asynchronous or lock-free optimization strategies (Recht et al. 2011; Liu et al. 2014; Hsieh, Yu, and Dhillon 2015; J. Reddi et al. 2015; Zhao and Li 2016).", "startOffset": 108, "endOffset": 213}, {"referenceID": 36, "context": "Representative distributed SGD methods include PSGD (Zinkevich et al. 2010), BAVGM (Zhang, Wainwright, and Duchi 2012) ar X iv :1 60 2.", "startOffset": 52, "endOffset": 75}, {"referenceID": 27, "context": "and Splash (Zhang and Jordan 2015).", "startOffset": 11, "endOffset": 34}, {"referenceID": 24, "context": "Representative distributed dual formulations include DisDCA (Yang 2013), CoCoA (Jaggi et al.", "startOffset": 60, "endOffset": 71}, {"referenceID": 5, "context": "Representative distributed dual formulations include DisDCA (Yang 2013), CoCoA (Jaggi et al. 2014) and CoCoA+ (Ma et al.", "startOffset": 79, "endOffset": 98}, {"referenceID": 13, "context": "2014) and CoCoA+ (Ma et al. 2015).", "startOffset": 17, "endOffset": 33}, {"referenceID": 26, "context": "In this paper, we propose a novel DSO method, called scalable composite optimization for learning (SCOPE), and implement it on the fault-tolerant distributed platform Spark (Zaharia et al. 2010).", "startOffset": 173, "endOffset": 194}, {"referenceID": 9, "context": "Please note that some asynchronous methods or systems, such as Parameter Server (Li et al. 2014), Petuum (Xing et al.", "startOffset": 80, "endOffset": 96}, {"referenceID": 23, "context": "2014), Petuum (Xing et al. 2015) and the methods in (Zhang and Kwok 2014; Zhang, Zheng, and Kwok 2016), have also been proposed for distributed learning with promising performance.", "startOffset": 14, "endOffset": 32}, {"referenceID": 28, "context": "2015) and the methods in (Zhang and Kwok 2014; Zhang, Zheng, and Kwok 2016), have also been proposed for distributed learning with promising performance.", "startOffset": 25, "endOffset": 75}, {"referenceID": 15, "context": "This is similar to most existing distributed learning frameworks like MLlib (Meng et al. 2016), Splash, Parameter Server, and CoCoA and so on.", "startOffset": 76, "endOffset": 94}, {"referenceID": 6, "context": "SCOPE is inspired by SVRG (Johnson and Zhang 2013) which tries to utilize full gradient to speed up the convergence of stochastic optimization.", "startOffset": 26, "endOffset": 50}, {"referenceID": 6, "context": "However, the original SVRG in (Johnson and Zhang 2013) is sequential.", "startOffset": 30, "endOffset": 54}, {"referenceID": 34, "context": "To design a distributed SVRG method, one natural strategy is to adapt the mini-batch SVRG (Zhao et al. 2014) to distributed settings, which is a typical strategy in most distributed SGD frameworks like Parameter Server (Li et al.", "startOffset": 90, "endOffset": 108}, {"referenceID": 9, "context": "2014) to distributed settings, which is a typical strategy in most distributed SGD frameworks like Parameter Server (Li et al. 2014) and Petuum (Xing et al.", "startOffset": 116, "endOffset": 132}, {"referenceID": 23, "context": "2014) and Petuum (Xing et al. 2015).", "startOffset": 17, "endOffset": 35}, {"referenceID": 33, "context": "All the appendices and proofs of this paper can be found in the arXiv version of this paper (Zhao et al. 2016).", "startOffset": 92, "endOffset": 110}, {"referenceID": 8, "context": "Besides the above mini-batch based strategy (DisSVRG) for distributed SVRG, there also exist some other distributed SVRG methods, including DSVRG (Lee et al. 2016), KroMagnon (Mania et al.", "startOffset": 146, "endOffset": 163}, {"referenceID": 14, "context": "2016), KroMagnon (Mania et al. 2015), SVRGfoR (Konecn\u00fd, McMahan, and Ramage 2015) and the distributed SVRG in (De and Goldstein 2016).", "startOffset": 17, "endOffset": 36}, {"referenceID": 1, "context": "2015), SVRGfoR (Konecn\u00fd, McMahan, and Ramage 2015) and the distributed SVRG in (De and Goldstein 2016).", "startOffset": 79, "endOffset": 102}, {"referenceID": 1, "context": "The distributed SVRG in (De and Goldstein 2016) cannot be guaranteed to converge because it is similar to the version of SCOPE with c = 0.", "startOffset": 24, "endOffset": 47}, {"referenceID": 18, "context": "Local learning strategy is also adopted in other problems like probabilistic logic programs (Riguzzi et al. 2016).", "startOffset": 92, "endOffset": 113}, {"referenceID": 33, "context": "We only list some Lemmas and Theorems, the detailed proof of which can be found in the appendices (Zhao et al. 2016).", "startOffset": 98, "endOffset": 116}, {"referenceID": 27, "context": "Please note that these assumptions are weaker than those in (Zhang and Jordan 2015; Ma et al. 2015; Jaggi et al. 2014), since we do not need each fi(w) to be convex and we do not make any assumption about the Hessian matrices either.", "startOffset": 60, "endOffset": 118}, {"referenceID": 13, "context": "Please note that these assumptions are weaker than those in (Zhang and Jordan 2015; Ma et al. 2015; Jaggi et al. 2014), since we do not need each fi(w) to be convex and we do not make any assumption about the Hessian matrices either.", "startOffset": 60, "endOffset": 118}, {"referenceID": 5, "context": "Please note that these assumptions are weaker than those in (Zhang and Jordan 2015; Ma et al. 2015; Jaggi et al. 2014), since we do not need each fi(w) to be convex and we do not make any assumption about the Hessian matrices either.", "startOffset": 60, "endOffset": 118}, {"referenceID": 15, "context": "More specifically, we adopt the following baselines for comparison: \u2022 MLlib6 (Meng et al. 2016): MLlib is an open source library for distributed machine learning on Spark.", "startOffset": 77, "endOffset": 95}, {"referenceID": 10, "context": "\u2022 LibLinear7 (Lin et al. 2014): LibLinear is a distributed Newton method, which is also a batch learning method.", "startOffset": 13, "endOffset": 30}, {"referenceID": 27, "context": "\u2022 Splash8 (Zhang and Jordan 2015): Splash is a distributed SGD method by using the local learning strategy to reduce communication cost (Zhang, Wainwright, and Duchi 2012), which is different from the mini-batch based distributed SGD method.", "startOffset": 10, "endOffset": 33}, {"referenceID": 5, "context": "\u2022 CoCoA9 (Jaggi et al. 2014): CoCoA is a distributed dual coordinate ascent method by using local learning strategy to reduce communication cost, which is formulated from the dual problem.", "startOffset": 9, "endOffset": 28}, {"referenceID": 13, "context": "\u2022 CoCoA+10 (Ma et al. 2015): CoCoA+ is an improved version of CoCoA.", "startOffset": 11, "endOffset": 27}, {"referenceID": 33, "context": "Experimental comparison can be found in appendix (Zhao et al. 2016).", "startOffset": 49, "endOffset": 67}, {"referenceID": 25, "context": "This might be reasonable due to the higher cache hit ratio with more machines (Yu et al. 2014).", "startOffset": 78, "endOffset": 94}, {"referenceID": 33, "context": "We also do experiments to show the low synchronization cost of SCOPE, which can be found in the appendix (Zhao et al. 2016).", "startOffset": 105, "endOffset": 123}], "year": 2016, "abstractText": "Many machine learning models, such as logistic regression (LR) and support vector machine (SVM), can be formulated as composite optimization problems. Recently, many distributed stochastic optimization (DSO) methods have been proposed to solve the large-scale composite optimization problems, which have shown better performance than traditional batch methods. However, most of these DSO methods might not be scalable enough. In this paper, we propose a novel DSO method, called scalable composite optimization for learning (SCOPE), and implement it on the fault-tolerant distributed platform Spark. SCOPE is both computationefficient and communication-efficient. Theoretical analysis shows that SCOPE is convergent with linear convergence rate when the objective function is strongly convex. Furthermore, empirical results on real datasets show that SCOPE can outperform other state-of-the-art distributed learning methods on Spark, including both batch learning methods and DSO methods.", "creator": "TeX"}}}