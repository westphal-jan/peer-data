{"id": "1602.00287", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jan-2016", "title": "Additive Approximations in High Dimensional Nonparametric Regression via the SALSA", "abstract": "High dimensional nonparametric regression is an inherently difficult problem with known lower bounds depending exponentially in dimension. A popular strategy to alleviate this curse of dimensionality has been to use additive models of \\emph{first order}, which model the regression function as a sum of independent functions on each dimension. Though useful in controlling the variance of the estimate, such models are often too restrictive in practical settings. Between non-additive models which often have large variance and first order additive models which have large bias, there has been little work to exploit the trade-off in the middle via additive models of intermediate order. In this work, we propose SALSA, which bridges this gap by allowing interactions between variables, but controls model capacity by limiting the order of interactions. SALSA minimises the residual sum of squares with squared RKHS norm penalties. Algorithmically, it can be viewed as Kernel Ridge Regression with an additive kernel. When the regression function is additive, the excess risk is only polynomial in dimension. Using the Girard-Newton formulae, we efficiently sum over a combinatorial number of terms in the additive expansion. Via a comparison on $16$ real datasets, we show that our method is competitive against $21$ other alternatives.", "histories": [["v1", "Sun, 31 Jan 2016 17:32:51 GMT  (2678kb,D)", "https://arxiv.org/abs/1602.00287v1", null], ["v2", "Sun, 20 Mar 2016 23:11:13 GMT  (2020kb,D)", "http://arxiv.org/abs/1602.00287v2", null], ["v3", "Tue, 24 May 2016 23:15:24 GMT  (3375kb,D)", "http://arxiv.org/abs/1602.00287v3", "International Conference on Machine Learning (ICML) 2016"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["kirthevasan kandasamy", "yaoliang yu"], "accepted": true, "id": "1602.00287"}, "pdf": {"name": "1602.00287.pdf", "metadata": {"source": "CRF", "title": "Additive Approximations in High Dimensional Nonparametric Regression via the SALSA", "authors": ["Kirthevasan Kandasamy", "Yaoliang Yu"], "emails": ["KANDASAMY@CS.CMU.EDU", "YAOLIANG@CS.CMU.EDU"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is as if they are able to trump themselves. (...) It is as if they are able to trump themselves. (...) It is as if they are able to trump themselves. (...) It is as if they are able to trump themselves. (...) It is as if they are able to trump themselves. (...) It is as if they are able to trump themselves. (...) It is as if they are able to trump themselves. (...) It is as if they are able to trump themselves. (...) It is as if they are able to trump themselves. (...) It is as if they are able to trump themselves. (...) It is as if they are able to trump themselves. (...) It is as if they are able to trump themselves."}, {"heading": "Related Work", "text": "One of the most popular techniques is the back-fitting algorithm (Hastie et al., 2001), which approaches iteratively the sum of the additive functions of the first order. However, some variants, such as RODEO (Lafferty & Wasserman, 2005) and SpAM (Ravikumar et al., 2009) examine first-order models in variable selection / sparsity settings. MARS (Friedman, 1991) uses a sum of splines on individual dimensions, but allows interactions between variables via products of hinge functions at selected nodes. Lou et al. (2013) examine additive models of the first order plus a sparse collection of paired interactions. However, limiting to a sparse collection of second-order interactions may be too distorted in practice."}, {"heading": "2. Preliminaries", "text": "& & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & & # 10; & & # 10; & & # 10; & & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; &"}, {"heading": "3. SALSA", "text": "To extend KRR to additive models, we first define cores k (j), which act on each subset x (j). Then we optimize the fol-staining target together via f (j), j (j), j (j), j = 1., Md. {f (j)} Mdj = 1 = argmin f (j), j (j), i),..., Md\u03bb Md \u00b2 j = 1 (j), 2H k (j) + 1n n n \u00b2 i = 1 (Yi \u2212 Md \u00b2 j = 1 f (j) (j) i)) 2. (3) Our estimate for f \u00b2 (\u00b7) = Xi \u00b2 f (j) (j), 2H \u00b2 k (j) (j) + 1n \u00b2). At first glance, this seems problematic, as it requires an optimization via nMd \u00b2 (l)."}, {"heading": "3.1. The ESP Kernel", "text": "While the above formula reduces the number of optimization parameters, the kernel still has a combinatorial number of terms that can be expensive to calculate. While this applies to any decisions for k (j) s, we can efficiently calculate k under certain constraints, using the same trick used by Shawe-Taylor & Christianini (2004) and Duvenaud et al. (2011). First, let's consider a set of base kernels that work on each dimension k1, k2,...., kD. Define k (j) to be the product core of all kernels that act on each coordinate - k (j) (j), x (j), x (j) \u2032) \u2032. We can ki1 (xi1) ki2 (xi2, x \u2032 i2) ki2 (xi2) kid (xid, x \u2032 id). Then the additive kernel (x \u2032) becomes the elementary symmetric class ki."}, {"heading": "3.2. Theoretical Analysis", "text": "We consider first the setting, when f (j). \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D"}, {"heading": "3.3. Practical Considerations", "text": "This is hardly the case in reality and they need to be properly selected for good empirical performance. Validation is not feasible here because there are too many hyper parameters. However, in our experiments we set every Ki to be a Gaussian kernel (xi, x \"i) = \u03c3Y exp (\u2212 x\" i) 2 / 2h2i) with bandwidth hi = c\u03c3in \u2212 1 / 5. Here the standard deviation of the ith covariate and \u03c3i is the standard deviation of Y the standard deviation of Y. The choice of bandwidth was inspired by several other kernel methods using bandwidths in the order - 1 / 5 (Ravikumar et al., 2009; Tsybakov, 2008).The constant c was manually tuned - we found this performance to be between 5 and 60. In our experiments we use c = 20. c1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h1h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h2h"}, {"heading": "4. Experiments", "text": "We compare SALSA with the following non-parametric models: Kernel Ridge Regression (KRR) (KRR), k-Nearest Neighbors (kNN), Nadaraya Watson (NW), Locally Linear / Quadratic Interpolation (LL, LQ), -Support Vector Regression (\u2212 SVR), \u03bd-Support Vector Regression (\u03bd \u2212 SVR), Gaussian Process Regression (GP), Regression Trees (RT), Gradient Boosted Regression Trees (GBRT) (Friedman, 2000), RBF Interpolation (RBFI), M5 'Model Trees (M5') (Wang & Witten, 1997) and Shepard Interpolation (SI). Non-parametric additive models: Additive Trees (GBRT) (Back-fitting with cubic splines (BF), Hastie & Tibirani, 1990), Multivate OS (Selection), SCOS (Selection), Selection)."}, {"heading": "4.1. Synthetic Experiments", "text": "We start with a series of synthetic examples. We compare SALSA with some non-additive methods to give intuition about our additive model. First, we create a low order synthetic function d = 3 in D = 15 dimensions. We do this by creating a d-dimensional function fd and adding this function across all (D d) coordinate combinations. We compare SALSA with order 3 and compare with others. The results are given in Figure 1 (a). This setting is tailored to the assumptions of our method and, unsurprisingly, surpasses all alternatives. Next, we demonstrate the distortion variance in the use of additive approximations for non-additive functions. We created a 15-dimensional (non-additive) function and fitted a SALSA model with d = 1, 2, 4, 8, 15 for difference decisions from n. The results are given in Figure 1 (b). The interesting observation is that in small samples d works best."}, {"heading": "4.2. Real Datasets", "text": "Finally, we compare SALSA with the other 16 data sets listed above. Data sets come from the UCI Repository, Bristol Multilevel Modeling and the following sources: (Guillame-Bert et al., 2014; Just et al., 2010; Paschou et al., 2007; Tegmark et al., 2006; Tu, 2012; Wehbe et al., 2014) Table 1 indicates the average square error of a test set. For the Speech data set, we have specified the training time (including cross validation for selecting hyperparameters) for each method. For SALSA, we have also specified the order d chosen by cross validation. See the caption below the table for more details. SALSA performs best (or is very close to the best) in 5 data sets. In addition, it falls into the top 5 in all but two data sets, placing sixth in both cases."}, {"heading": "5. Conclusion", "text": "SALSA finds additive approaches to the regression function in high dimensions. It has less bias than first-order models and less variance than non-additive methods. Algorithmically, it requires attaching an additive core to KRR. In calculating the core, we use the Girard-Newton formulas to sum up efficiently over a combinatorial number of terms. Our theorems show that the excessive risk depends only on D if f x is additive, significantly better than the usual exponential dependence of non-parametric methods, albeit under stronger assumptions. Our analysis of the agnostic environment provides intuitions about the changing ones. We demonstrate the effectiveness of SALSA through a comprehensive empirical evaluation. In the future, we would like to use techniques from scalable core methods to deal with large data sets. Theorems 3,4 show polynomic dependence on D if f is additive."}, {"heading": "Acknowledgements", "text": "We thank Calvin McCarter, Ryan Tibshirani, and Larry Wasserman for the insightful discussions and feedback on the paper, and Madalina Fiterau for providing data sets. This work was partly funded by the DOE grant DESC0011114."}, {"heading": "Appendix", "text": "A. Proof of Theorem 3: Convergence of SALSA Our analysis here is a rough generalization of the analysis in Zhang et al. (2013). We will treat the additive case using ideas from Aronszajn (1950). As such, we will try to stick to the same notation. Some interim results can be obtained directly from Zhang et al. (2013), but we will repeat them (or provide a sketch) here for the sake of completeness. In addition to the definitions presented in the main text, we will also need the following quantities, \u03b2 (j) t = number of results. \"(j), (j), (j) = number of results.\" (n), q) = max (q) max. (q), log t), max. (q), log t (t), log t (t) n1 / q)."}, {"heading": "A.1. Set up", "text": "We first define the following functional class of the product of all RKHS, F = Hk (1) \u00b7 Hk (2) \u00b7 \u00b7 \u00b7 Hk (Md) = (1),. < Xi (1) 2 > Hk (1) + < f (Md) 2 > Hk (j) 2 > Hk (Md) 2 > Hk (Md) 2 > Hk (Md) 2 > Hk (Md) 2 > Hk (Md) 2 > Hk (Md) 2 > Hk (Md) 2 > Hk (Md) 2 > Hk (Md).Here f (j) 1 are the elements of f1 and < \u00b7 Hk (j) 2 > Hk (j) the inner product of Hk (j)."}, {"heading": "Proofs of Technical Lemmas", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.2.1. PROOF OF LEMMA 7", "text": "Lemma 7 is straightforward: \"Q \u2212 1M \u2212 1\u03b8\" 22 = \"M\" j = 1 \"Q (j) \u2212 1\" M (j) \u2212 1 \"M\" (j) \"2\" 2 \"= 1\" M \"(j)\" (M (j) 2 + \"M\" (j) \u2212 1 \"M\" (j) \"\u2264 M\" j \"= 1\" A \"(j) >\" M \"(j) \u2212 1\" M \"(j)\" \u2193 = 1 \"M\" j = 1 \"1\" 2 \"(j)\" \u2264 1 \"f\" 2F"}, {"heading": "A.2.2. PROOF OF LEMMA 8", "text": "We decompile the LHS as follows: (1), (1), (1), (1), (2), (2), (2), (2), (2), (3), (4), (4), (4), (4), (4), (4), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5, (5), (5), (5), (5), (5, (5), (5), (5, (5), (5), (5), (5, (5), (5), (5), (5, (5), (5), (5), (5), (5), (5), (5), (5),"}, {"heading": "A.2.3. PROOF OF LEMMA 9", "text": "Define \u03c0 (j) (j) (j) (j) (j) (q) (q) (q) (q) (1) i; \u03c0 (M) i) i (p) i i (p) i i (q) i (q) i (q) i (q) i (q) i (max) i (p) i (p) i (p) i (p) i (p) i (p) i (p) i (p) i i (p) i) i (p) i) i (p) i (p) i) i (p) i) i (p) i i (p) i (p) i) i (p) i i (p) i i) (p) i (p) i) i (p) i) (i) (p) i) (i) (p) i) (i) (p) i) (i) (p) i) (i) (i) (p) i) (i) (i) (i) (p) i) (i) (i) (p) i) (i) (i) (i) (p) i) (i) (i) (i) (p) i) (i) (i) (i) (p) (i) (i) (i) (i) (p) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (p) (i) (i) (i) (i) (i) (i) (i (i) (i) (i) (i (i) (i) (i) (i (i) (i) (i) (i (i) (i) (p) (i) (i (i) (i) (p) (i (i) (i) (i) (i) (i) (p) (i (i) (i) (i) (p) (i (i) (i (i) (i) (i) (i) (p) (i (i) (i) (p) (i) (i) (i (i) (i) (p) (i) (i (i) (i) (i"}, {"heading": "Proofs of Technical Lemmas", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.3.1. PROOF OF LEMMA 10", "text": "Note that according to an argumentation similar to Equation (25) in Lemma 8, it is sufficient, E (M1) > M1 (M2) > M2 (M2) = M2 (M2) = M2 (M2) = M2 (M2) = M2 (M2) = M2 (M2) = M2 (M2) = M2 (M2) = M2 (M2) = M2 (M2) = (M2) (M2) (M2) = M2 (M2) = M2 (M2) = M2 (M2) (M2) = M2 (M2) = M2 (M2) (M2) (M2) = M2 (M2) (M2) (M2) = M2 (M2) (M2) (M2) (M2) = M2 (M2) (M2) (M2) = M2 (M2) (M2) (M2) = M2 (M2) (M2) (M2) = M2 (M2) (M2) = M2 (M2)."}, {"heading": "A.3.2. PROOF OF LEMMA 11", "text": "We extend the LHS as follows in order to obtain the result: E [\u0435\u0442 1 n Q \u2212 1\u0445 > \u0430 2] = 1 n2 M \u0445 j = 1 t \u2211 '= 1 n \u2211 i = 111 + \u03bb / \u00b5 (j)' E [\u03c6 (j) '(Xi) 2 2i] \u2264 \u03c32n M \u2211 j = 1 \u03b3 (j) (\u03bb) = \u03c32 n \u03b3k (\u03bb) The first step is merely an extension of the matrix. In the second step we have used the definitions of \u03b3 (j) (\u03bb) and \u03b3k (\u03bb) = E [\u03c6 (j)' (Xi) 2 E [2i | Xi]] \u2264 \u03c32 since E [\u03c6 (j) '(X) 2] = 1."}, {"heading": "B. Proof of Theorem 4: Rate of Convergence in Different RKHSs", "text": "Our strategy will be to choose \u03bb to limit the dependence on n in the first two terms of the RHS of the limit in theorem 3. Proof of theorem 4-1. Polynomial decay: The number \u03b3k (\u03bb) can be limited by Md \u00b2 s \"= 1 1 / (1 + \u03bb / \u00b5). If we set \u03bb = n \u2212 2s 2s + d, there is a risk. \u2212 2s \u2212 2s + d \u00b2 s 2s + n \u2212 2s 2s + d \u00b2 s 2s + d \u00b2 s 2s + d \u00b2 s 2s + d \u00b2 s 2s + d \u00b2 s risk."}, {"heading": "C. Proof of Theorem 5: Analysis in the Agnostic Setting", "text": "As before, we generalize the analysis of Zhang et al. (2013) to the tuple RKHS F. We begin by making the following crucial observation about the population (26). To prove this, we consider any g = 1 g (j), using the fact that R (g) = R (f) = R (f). G (f)."}, {"heading": "Proofs of Technical Lemmas", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C.1. Proof of Lemma 13", "text": "Since f \u00b2 is the minimizer of the empirical target, it is found that E \u00b2 f \u00b2 2F \u00b2 Xn1 \u00b2 E \u00b2 M \u00b2 J = 1 \u00b2 F (j) 2H k (j) + 1 n \u00b2 i = 1 M \u00b2 J = 1 f (j) (X (j) i) \u2212 Yi 2 = 1 f (j) i) - Yi 2 = 1 f (j) i = 1 H k (j) + 1 n (Xi) i = 1 M \u00b2 J = 1 f (j) (X (j) i) \u2212 Yi 2 = 1 f (j) i) - Yi 2 = 1 x (f) - 2F + 1n (i) i = 1 x (Xi) 2k (Xi) i = 1 x (Xi) 2F \u00b2 F - 2F (2F) (2F) 2F - 2F (2F) 2F (2F) 2F) (2F) 2F (2F) 2F (2F) 2F (2F) - 2F (2F)"}, {"heading": "C.2. Proof of Lemma 12", "text": "\"First, with Jensen's inequality twice we have E [4\u03bb (X) = 4\u043c (X) = 4\u043c (F) = 4\u043c (E) [(Y \u2212 f\u03bb (X))) 2 | X] \u2264 E [(Y \u2212 f\u03bb (X))) 4] \u2264 8E [f4\u03bb (X) + 8E [Y] (29) Consider any f (j) \u03bb, f (j) \u03bb (x) = 1 \u03b8 (j) '(j)' (x) (b) (b) \u2264 (Y) (1) (j) '1 / 3 \u03b8 (j) 3 / 4 \u0432 (j)' (j) = 1 \u03b8 (j) '(x)' 4 \u00b5 (j) '(j) (b) (j) j) = 1 \u00b5 (j) = 1 \u00b5 (j) + 8E (j) (j) (j) 1 / 2) and we (j) have none (j)."}, {"heading": "C.3. Proof of Lemma 14", "text": "(1) (1) (2) (1) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2)) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2 (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2 (2) (2) (2) (2) (2) (2 (2) (2) (2) (2) (2) (2) (2) (2 (2) (2) (2) (2) (2) (2) (2) (2 (2) (2 (2) (2) (2) (2) (2) (2) (2) (2) (2 (2) (2) (2) (2) (2) (2) (2) (2) (2 (2) (2) (2) (2) ("}, {"heading": "C.4. Proof of Lemma 15", "text": "The first remark is that we can write the LHS of the problem as follows: E [1] \"E\" (1) \"E\" (1) \"- 1\" n \"(1)\" i \"(1)\" i) 2 \"(X)\" i) 2 \"To limit the inner expectation, we use the optimal conditions of the population minimizer (7). We have, 2E\" (M) \"j\" = 1 \"f\" (j) - (X) i \"i\" (j) - (j) - Xi + 2f (j) - (j) Xi i \"=\" f \"(j) -\" E \"(j)\" (j) - (j) i \"- (j) - (j) - (j). (30) In the last step, we have the F inner product with (0)."}, {"heading": "D. Some Details on Experimental Setup", "text": "The function fd that is used in Figure 1 (a) is the logbook of three Gaussern. \"We,\" according to the authors, \"we.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"\" We. \"\" \"\" We. \"\" \"\" We. \"\" \".\" \"\" \"We.\" \"\" \"\" \"\". \"\" \"\" \"\". \"\" \"\" \".\" \"\" \".\" \"\". \"\". \".\" \".\". \".\". \"\" \".\" \"\" \"\". \"\" \"\" \".\" \"\" \"\". \"\" \"\". \".\". \".\" \".\". \".\". \"\". \".\". \".\". \"\" \"\". \".\" \".\". \".\" \"\" \".\". \".\" \".\". \"\" \".\". \"\". \"\" \".\". \"\" \".\" \".\" \"\". \".\". \"\" \".\". \"\". \"\". \"\". \"\". \"\". \"\". \".\" \"\". \".\" \".\". \"\" \"\". \".\". \"\". \".\" \"\" \".\" \".\". \".\" \".\". \".\" \".\". \"\" \".\". \"\". \"\". \".\" \".\". \"\". \"\" \".\". \".\". \".\" \"\". \".\". \".\" \".\". \".\" \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \"\". \".\". \".\". \"\". \".\". \"\". \".\" \".\" \".\" \".\". \".\" \".\". \".\". \"\". \"\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \""}], "references": [{"title": "Consistency of the Group Lasso and Multiple Kernel Learning", "author": ["Bach", "Francis R"], "venue": null, "citeRegEx": "Bach and R.,? \\Q2008\\E", "shortCiteRegEx": "Bach and R.", "year": 2008}, {"title": "Reproducing kernel Hilbert spaces in Probability and Statistics", "author": ["Berlinet", "Alain", "Thomas-Agnan", "Christine"], "venue": "Kluwer Academic,", "citeRegEx": "Berlinet et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Berlinet et al\\.", "year": 2004}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chang", "Chih-Chung", "Lin", "Chih-Jen"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "Chang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2011}, {"title": "Scalable Kernel Methods via Doubly Stochastic Gradients", "author": ["Dai", "Bo", "Xie", "He", "Niao", "Liang", "Yingyu", "Raj", "Anant", "Balcan", "Maria-Florina F", "Song", "Le"], "venue": "In NIPS,", "citeRegEx": "Dai et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2014}, {"title": "Additive Gaussian Processes", "author": ["Duvenaud", "David K", "Nickisch", "Hannes", "Rasmussen", "Carl Edward"], "venue": "In NIPS,", "citeRegEx": "Duvenaud et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duvenaud et al\\.", "year": 2011}, {"title": "Least Angle Regression", "author": ["Efron", "Bradley", "Hastie", "Trevor", "Johnstone", "Iain", "Tibshirani", "Robert"], "venue": "Annals of Statistics,", "citeRegEx": "Efron et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Efron et al\\.", "year": 2004}, {"title": "Multivariate Adaptive Regression Splines", "author": ["Friedman", "Jerome H"], "venue": "Ann. Statist.,", "citeRegEx": "Friedman and H.,? \\Q1991\\E", "shortCiteRegEx": "Friedman and H.", "year": 1991}, {"title": "Greedy Function Approximation: A Gradient Boosting Machine", "author": ["Friedman", "Jerome H"], "venue": "Annals of Statistics,", "citeRegEx": "Friedman and H.,? \\Q2000\\E", "shortCiteRegEx": "Friedman and H.", "year": 2000}, {"title": "Multiple Kernel Learning Algorithms", "author": ["G\u00f6nen", "Mehmet", "Alpaydin", "Ethem"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "G\u00f6nen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "G\u00f6nen et al\\.", "year": 2011}, {"title": "Utility of Empirical Models of Hemorrhage in Detecting and Quantifying Bleeding", "author": ["M Guillame-Bert", "A Dubrawski", "L Chen", "A Holder", "MR Pinsky", "G. Clermont"], "venue": "In Intensive Care Medicine,", "citeRegEx": "Guillame.Bert et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Guillame.Bert et al\\.", "year": 2014}, {"title": "A Distribution Free Theory of Nonparametric Regression", "author": ["Gy\u00f6rfi", "L\u00e1szl\u00f3", "Kohler", "Micael", "Krzyzak", "Adam", "Walk", "Harro"], "venue": "Springer Series in Statistics,", "citeRegEx": "Gy\u00f6rfi et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Gy\u00f6rfi et al\\.", "year": 2002}, {"title": "Computationally efficient regression on a dependency graph for human pose estimation", "author": ["Hara", "Kentaro", "Chellappa", "Rama"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Hara et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hara et al\\.", "year": 2013}, {"title": "Generalized Additive Models", "author": ["T.J. Hastie", "R.J. Tibshirani"], "venue": null, "citeRegEx": "Hastie and Tibshirani,? \\Q1990\\E", "shortCiteRegEx": "Hastie and Tibshirani", "year": 1990}, {"title": "Open source regression software for Matlab/Octave", "author": ["Jakabsons", "Gints"], "venue": null, "citeRegEx": "Jakabsons and Gints.,? \\Q2015\\E", "shortCiteRegEx": "Jakabsons and Gints.", "year": 2015}, {"title": "A neurosemantic theory of concrete noun representation based on the underlying brain", "author": ["Just", "Marcel Adam", "Cherkassky", "Vladimir L", "S Aryal", "Mitchell", "Tom M", "Aryal", "Esh"], "venue": "codes. PLoS ONE,", "citeRegEx": "Just et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Just et al\\.", "year": 2010}, {"title": "Rodeo: Sparse Nonparametric Regression in High Dimensions", "author": ["Lafferty", "John D", "Wasserman", "Larry A"], "venue": "In NIPS,", "citeRegEx": "Lafferty et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2005}, {"title": "Fastfood Approximating Kernel Expansions in Loglinear Time", "author": ["Le", "Quoc", "Sarlos", "Tamas", "Smola", "Alex"], "venue": "In 30th International Conference on Machine Learning (ICML),", "citeRegEx": "Le et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Le et al\\.", "year": 2013}, {"title": "Component selection and smoothing in smoothing spline analysis of variance models", "author": ["Lin", "Yi", "Zhang", "Hao Helen"], "venue": "Annals of Statistics,", "citeRegEx": "Lin et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2006}, {"title": "Accurate Intelligible Models with Pairwise Interactions", "author": ["Lou", "Yin", "Caruana", "Rich", "Gehrke", "Johannes", "Hooker", "Giles"], "venue": "In KDD,", "citeRegEx": "Lou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lou et al\\.", "year": 2013}, {"title": "Symmetric functions and Hall polynomials", "author": ["Macdonald", "Ian Grant"], "venue": null, "citeRegEx": "Macdonald and Grant.,? \\Q1995\\E", "shortCiteRegEx": "Macdonald and Grant.", "year": 1995}, {"title": "PCA-correlated SNPs for Structure Identification", "author": ["P. Paschou"], "venue": "PLoS Genetics,", "citeRegEx": "Paschou,? \\Q2007\\E", "shortCiteRegEx": "Paschou", "year": 2007}, {"title": "Accuracy versus Interpretability in flexible modeling:implementing a tradeoff using Gaussian process models", "author": ["Plate", "Tony A"], "venue": "Behaviourmetrika, Interpreting Neural Network Models\u201d,", "citeRegEx": "Plate and A.,? \\Q1999\\E", "shortCiteRegEx": "Plate and A.", "year": 1999}, {"title": "Random Features for Large-Scale Kernel Machines", "author": ["Rahimi", "Ali", "Recht", "Benjamin"], "venue": "In NIPS,", "citeRegEx": "Rahimi et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Rahimi et al\\.", "year": 2007}, {"title": "Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning", "author": ["Rahimi", "Ali", "Recht", "Benjamin"], "venue": "In NIPS,", "citeRegEx": "Rahimi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Rahimi et al\\.", "year": 2009}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": null, "citeRegEx": "Rasmussen and Williams,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen and Williams", "year": 2006}, {"title": "Sparse Additive Models", "author": ["Ravikumar", "Pradeep", "Lafferty", "John", "Liu", "Han", "Wasserman", "Larry"], "venue": "Journal of the Royal Statistical Society: Series B,", "citeRegEx": "Ravikumar et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ravikumar et al\\.", "year": 2009}, {"title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond", "author": ["Sch\u00f6lkopf", "Bernhard", "Smola", "Alexander J"], "venue": null, "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2001}, {"title": "Kernel Methods for Pattern Analysis", "author": ["Shawe-Taylor", "John", "Cristianini", "Nello"], "venue": "Cambridge university press,", "citeRegEx": "Shawe.Taylor et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Shawe.Taylor et al\\.", "year": 2004}, {"title": "Optimal Rates for Regularized Least Squares Regression", "author": ["Steinwart", "Ingo", "Hush", "Don R", "Scovel", "Clint"], "venue": "In COLT,", "citeRegEx": "Steinwart et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Steinwart et al\\.", "year": 2009}, {"title": "Cosmological Constraints from the SDSS Luminous Red Galaxies", "author": ["M. Tegmark et al"], "venue": "Physical Review,", "citeRegEx": "al,? \\Q2006\\E", "shortCiteRegEx": "al", "year": 2006}, {"title": "Regression Shrinkage and Selection Via the Lasso", "author": ["Tibshirani", "Robert"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "Tibshirani and Robert.,? \\Q1994\\E", "shortCiteRegEx": "Tibshirani and Robert.", "year": 1994}, {"title": "Introduction to Nonparametric Estimation", "author": ["Tsybakov", "Alexandre B"], "venue": null, "citeRegEx": "Tsybakov and B.,? \\Q2008\\E", "shortCiteRegEx": "Tsybakov and B.", "year": 2008}, {"title": "Integrative Analysis of a cross-locci regulation Network identifies App as a Gene regulating Insulin Secretion from Pancreatic Islets", "author": ["Tu", "Zhidong"], "venue": "PLoS Genetics,", "citeRegEx": "Tu and Zhidong.,? \\Q2012\\E", "shortCiteRegEx": "Tu and Zhidong.", "year": 2012}, {"title": "Inducing Model Trees for Continuous Classes", "author": ["Wang", "Yong", "Witten", "Ian H"], "venue": "In ECML,", "citeRegEx": "Wang et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Wang et al\\.", "year": 1997}, {"title": "Simultaneously uncovering the patterns of brain regions involved in different story reading", "author": ["L. Wehbe", "B. Murphy", "P. Talukdar", "A. Fyshe", "A. Ramdas", "T. Mitchell"], "venue": "PLoS ONE,", "citeRegEx": "Wehbe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wehbe et al\\.", "year": 2014}, {"title": "Generalization Performance of Regularization Networks and Support Vector Machines via Entropy Numbers of Compact Operators", "author": ["Williamson", "Robert C", "Smola", "Alex J", "Sch\u00f6lkopf", "Bernhard"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Williamson et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Williamson et al\\.", "year": 2001}, {"title": "Simple and Efficient Multiple Kernel Learning by Group Lasso", "author": ["Xu", "Zenglin", "Jin", "Rong", "Yang", "Haiqin", "King", "Irwin", "Lyu", "Michael R"], "venue": "In ICML,", "citeRegEx": "Xu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2010}, {"title": "Learning Bounds for Kernel Regression Using Effective Data Dimensionality", "author": ["Zhang", "Tong"], "venue": "Neural Computation,", "citeRegEx": "Zhang and Tong.,? \\Q2005\\E", "shortCiteRegEx": "Zhang and Tong.", "year": 2005}, {"title": "Divide and Conquer Kernel Ridge Regression", "author": ["Zhang", "Yuchen", "Duchi", "John C", "Wainwright", "Martin J"], "venue": "In COLT,", "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}, {"title": "Shrunk Additive Least Squares Approximation Appendix A. Proof of Theorem 3: Convergence of SALSA Our analysis here is a brute force generalisation of the analysis", "author": ["Zhang"], "venue": null, "citeRegEx": "Zhang,? \\Q2013\\E", "shortCiteRegEx": "Zhang", "year": 2013}, {"title": "but we repeat them (or provide an outline) here for the sake of completeness", "author": ["Zhang"], "venue": null, "citeRegEx": "Zhang,? \\Q2013\\E", "shortCiteRegEx": "Zhang", "year": 2013}, {"title": "Shrunk Additive Least Squares Approximation A.3", "author": ["Zhang"], "venue": "Variance (Proof of Bound", "citeRegEx": "Zhang,? \\Q2013\\E", "shortCiteRegEx": "Zhang", "year": 2013}, {"title": "\u03c7(k) can be shown to be low order by choosing t = n which results in \u03bc\u0303t+1, \u03b2t \u2208 O(n\u22124). C. Proof of Theorem 5: Analysis in the Agnostic Setting As before, we generalise the analysis by Zhang et al. (2013) to the tuple RKHS F . We begin by making the following crucial observation about the population", "author": ["d \u03c0\u0303/n"], "venue": null, "citeRegEx": "\u03c0\u0303.n..,? \\Q2013\\E", "shortCiteRegEx": "\u03c0\u0303.n..", "year": 2013}, {"title": "R are constant vectors. For figures 1(b)-1(f) we used fD where D is given in the figures. In all experiments, we used a test set of 2000 points and plot the mean squared test error", "author": ["\u221a d"], "venue": null, "citeRegEx": "d,? \\Q2000\\E", "shortCiteRegEx": "d", "year": 2000}, {"title": "Predictor: Story feature at a given time step Features: Other attributes", "author": ["Brain: (From Wehbe"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}], "referenceMentions": [{"referenceID": 10, "context": "Current lower bounds (Gy\u00f6rfi et al., 2002) suggest that this dependence is unavoidable.", "startOffset": 21, "endOffset": 42}, {"referenceID": 25, "context": "In this light, a common simplification has been to assume that f\u2217 decomposes into the additive form f\u2217(x) = f (1) \u2217 (x1)+f (2) \u2217 (x2)+\u00b7 \u00b7 \u00b7+f (D) \u2217 (xD) (Hastie & Tibshirani, 1990; Lafferty & Wasserman, 2005; Ravikumar et al., 2009).", "startOffset": 153, "endOffset": 232}, {"referenceID": 25, "context": "Some variants such as RODEO (Lafferty & Wasserman, 2005) and SpAM (Ravikumar et al., 2009) study first order models in variable selection/sparsity settings.", "startOffset": 66, "endOffset": 90}, {"referenceID": 36, "context": "A large line of work, in what has to come to be known as Multiple Kernel Learning (MKL), focuses on precisely this problem (Bach, 2008; G\u00f6nen & Alpaydin, 2011; Xu et al., 2010).", "startOffset": 123, "endOffset": 176}, {"referenceID": 4, "context": "Additive models have also been studied in Gaussian process literature via additive kernels (Duvenaud et al., 2011; Plate, 1999).", "startOffset": 91, "endOffset": 127}, {"referenceID": 17, "context": "Lou et al. (2013) model f\u2217 as a first order model plus a sparse collection of pairwise interactions.", "startOffset": 0, "endOffset": 18}, {"referenceID": 10, "context": "It is well known thatR is minimised by the regression function f\u2217(\u00b7) = EXY [Y |X = \u00b7] and the excess risk for any f is R(f)\u2212R(f\u2217) = \u2016f \u2212 f\u2217\u20162 (Gy\u00f6rfi et al., 2002).", "startOffset": 142, "endOffset": 163}, {"referenceID": 28, "context": "KRR has been analysed extensively under different assumptions on f\u2217; see (Steinwart & Christmann, 2008; Steinwart et al., 2009; Zhang, 2005) and references therein.", "startOffset": 73, "endOffset": 140}, {"referenceID": 4, "context": "We circumvent this bottleneck using two strategems: a classical result from RKHS theory, and a computational trick using elementary symmetric polynomials used before by Duvenaud et al. (2011); Shawe-Taylor & Cristianini (2004) in the kernel literature for additive kernels.", "startOffset": 169, "endOffset": 192}, {"referenceID": 4, "context": "We circumvent this bottleneck using two strategems: a classical result from RKHS theory, and a computational trick using elementary symmetric polynomials used before by Duvenaud et al. (2011); Shawe-Taylor & Cristianini (2004) in the kernel literature for additive kernels.", "startOffset": 169, "endOffset": 227}, {"referenceID": 43, "context": "At first, this appears troublesome since it requres optimising over nMd parameters (\u03b1 i ), j = 1, . . . ,Md, i = 1, . . . , n. However, from the work of Aronszajn (1950), we know that the solution of (3) lies in the RKHS of the sum kernel k", "startOffset": 70, "endOffset": 170}, {"referenceID": 28, "context": "The ESP Kernel While the above formulation reduces the number of optimisation parameters, the kernel still has a combinatorial number of terms which can be expensive to compute. While this is true for arbitrary choices for k\u2019s, under some restrictions we can efficiently compute k. For this, we use the same trick used by Shawe-Taylor & Cristianini (2004) and Duvenaud et al.", "startOffset": 124, "endOffset": 356}, {"referenceID": 4, "context": "For this, we use the same trick used by Shawe-Taylor & Cristianini (2004) and Duvenaud et al. (2011). First consider a set of base kernels acting on each dimension k1, k2, .", "startOffset": 78, "endOffset": 101}, {"referenceID": 38, "context": "Similar assumptions are made in (Zhang et al., 2013) and are satisfied for a large range of kernels including those in Theorem 4.", "startOffset": 32, "endOffset": 52}, {"referenceID": 38, "context": "The first term is known as the effective data dimensionality of k (Zhang, 2005; Zhang et al., 2013) and captures the statistical difficulty of estimating a function inHk(j) .", "startOffset": 66, "endOffset": 99}, {"referenceID": 29, "context": "Our proof technique generalises the analysis of Zhang et al. (2013) for KRR to the additive case.", "startOffset": 25, "endOffset": 68}, {"referenceID": 29, "context": "Our proof technique generalises the analysis of Zhang et al. (2013) for KRR to the additive case. We use ideas from Aronszajn (1950) to handle sum RKHSs.", "startOffset": 25, "endOffset": 133}, {"referenceID": 35, "context": "An example of the second eigendecay is the Gaussian kernel with \u03c0\u0303 = \u221a 2\u03c0 (Williamson et al., 2001).", "startOffset": 74, "endOffset": 99}, {"referenceID": 10, "context": "Current lower bounds suggest that the exponential dependence is unavoidable (Gy\u00f6rfi et al., 2002; Tsybakov, 2008).", "startOffset": 76, "endOffset": 113}, {"referenceID": 28, "context": "The proof, given in Appendix C, also follows the template in Zhang et al. (2013). Loosely, we may interpret AE and EE as the approximation and estimation errors1.", "startOffset": 32, "endOffset": 81}, {"referenceID": 25, "context": "The choice of bandwidth was inspired by several other kernel methods which use bandwidths on the order \u03c3in (Ravikumar et al., 2009; Tsybakov, 2008).", "startOffset": 107, "endOffset": 147}, {"referenceID": 3, "context": "(Dai et al., 2014; Le et al., 2013; Rahimi & Recht, 2007; 2009; Zhang et al., 2013) can be explored to scale SALSA with n.", "startOffset": 0, "endOffset": 83}, {"referenceID": 16, "context": "(Dai et al., 2014; Le et al., 2013; Rahimi & Recht, 2007; 2009; Zhang et al., 2013) can be explored to scale SALSA with n.", "startOffset": 0, "endOffset": 83}, {"referenceID": 38, "context": "(Dai et al., 2014; Le et al., 2013; Rahimi & Recht, 2007; 2009; Zhang et al., 2013) can be explored to scale SALSA with n.", "startOffset": 0, "endOffset": 83}, {"referenceID": 25, "context": "Nonparametric additive models: Back-fitting with cubic splines (BF) (Hastie & Tibshirani, 1990), Multivariate Adaptive Regression Splines (MARS) (Friedman, 1991), Component Selection and Smoothing (COSSO) (Lin & Zhang, 2006), Sparse Additive Models (SpAM) (Ravikumar et al., 2009) and Additive Gaussian Processes (AddGP) (Duvenaud et al.", "startOffset": 256, "endOffset": 280}, {"referenceID": 4, "context": ", 2009) and Additive Gaussian Processes (AddGP) (Duvenaud et al., 2011).", "startOffset": 48, "endOffset": 71}, {"referenceID": 5, "context": "Parametric models: Ridge Regression (RR), Least Absolute Shrinkage and Selection (LASSO) (Tibshirani, 1994) and Least Angle Regression (LAR) (Efron et al., 2004).", "startOffset": 141, "endOffset": 161}, {"referenceID": 9, "context": "The datasets were taken from the UCI repository, Bristol Multilevel Modeling and the following sources: (Guillame-Bert et al., 2014; Just et al., 2010; Paschou, 2007; Tegmark et al, 2006; Tu, 2012; Wehbe et al., 2014).", "startOffset": 104, "endOffset": 217}, {"referenceID": 14, "context": "The datasets were taken from the UCI repository, Bristol Multilevel Modeling and the following sources: (Guillame-Bert et al., 2014; Just et al., 2010; Paschou, 2007; Tegmark et al, 2006; Tu, 2012; Wehbe et al., 2014).", "startOffset": 104, "endOffset": 217}, {"referenceID": 20, "context": "The datasets were taken from the UCI repository, Bristol Multilevel Modeling and the following sources: (Guillame-Bert et al., 2014; Just et al., 2010; Paschou, 2007; Tegmark et al, 2006; Tu, 2012; Wehbe et al., 2014).", "startOffset": 104, "endOffset": 217}, {"referenceID": 34, "context": "The datasets were taken from the UCI repository, Bristol Multilevel Modeling and the following sources: (Guillame-Bert et al., 2014; Just et al., 2010; Paschou, 2007; Tegmark et al, 2006; Tu, 2012; Wehbe et al., 2014).", "startOffset": 104, "endOffset": 217}, {"referenceID": 29, "context": "Proof of Theorem 3: Convergence of SALSA Our analysis here is a brute force generalisation of the analysis in Zhang et al. (2013). We handle the additive case using ideas from Aronszajn (1950).", "startOffset": 47, "endOffset": 130}, {"referenceID": 29, "context": "Proof of Theorem 3: Convergence of SALSA Our analysis here is a brute force generalisation of the analysis in Zhang et al. (2013). We handle the additive case using ideas from Aronszajn (1950). As such we will try and stick to the same notation.", "startOffset": 47, "endOffset": 193}, {"referenceID": 29, "context": "Proof of Theorem 3: Convergence of SALSA Our analysis here is a brute force generalisation of the analysis in Zhang et al. (2013). We handle the additive case using ideas from Aronszajn (1950). As such we will try and stick to the same notation. Some intermediate technical results can be obtained directly from Zhang et al. (2013) but we repeat them (or provide an outline) here for the sake of completeness.", "startOffset": 47, "endOffset": 332}, {"referenceID": 43, "context": "The key observation is that we only need to consider n (and not nMd) parameters even though we are optimising overMd RKHSs. The reasoning uses a powerful result from Aronszajn (1950). Consider the class of functionsH\u2032 = {f = \u2211 j f ; f (j) \u2208 Hk(j)}.", "startOffset": 39, "endOffset": 183}, {"referenceID": 29, "context": "We will need the following technical lemmas. The proofs are given at the end of this section. These results correspond to Lemma 5 in Zhang et al. (2013).", "startOffset": 34, "endOffset": 153}, {"referenceID": 38, "context": "The proof mimics Lemma 6 in (Zhang et al., 2013) by performing essentially the same steps over F instead of the usual Hilbert space.", "startOffset": 28, "endOffset": 48}, {"referenceID": 29, "context": "Variance (Proof of Bound (10)) Once again, we follow Zhang et al. (2013). The tricks we use to generalise it to the additive case (i.", "startOffset": 62, "endOffset": 73}, {"referenceID": 29, "context": "Proof of Theorem 5: Analysis in the Agnostic Setting As before, we generalise the analysis by Zhang et al. (2013) to the tuple RKHS F .", "startOffset": 22, "endOffset": 114}, {"referenceID": 29, "context": "In all experiments, we used a test set of 2000 points and plot the mean squared test error. For the real datasets, we normalised the training data so that the X, y values have zero mean and unit variance along each dimensions. We split the given dataset roughly equally to form a training set and testing set. We tuned hyper-parameters via 5-fold cross validation on the training set and report the mean squared error on the test set. For some datasets the test prediction error is larger than 1. Such datasets turned out to be quite noisy. In fact, when we used a constant predictor at 0 (i.e. the mean of the training instances) the mean squared error on the test set was typically much larger than 1. Below, we list details on the dataset: the source, the used predictor and features. 1. Housing: (UCI), Predictor: CRIM Features: All other attributes except CHAS which is a binary feature. 2. Galaxy: (SDSS data on Luminous Red Galaxies from Tegmark et al (2006)), Predictor: Baryonic Density Features: All other attributes.", "startOffset": 3, "endOffset": 966}, {"referenceID": 14, "context": "fMRI: (From (Just et al., 2010)), Predictor: Noun representation Features: Voxel Intensities.", "startOffset": 12, "endOffset": 31}, {"referenceID": 9, "context": "Bleeding: (From (Guillame-Bert et al., 2014)), Predictor: Given output Features: Given features reduced to 100 dimensions via a random projection.", "startOffset": 16, "endOffset": 44}, {"referenceID": 9, "context": "Bleeding: (From (Guillame-Bert et al., 2014)), Predictor: Given output Features: Given features reduced to 100 dimensions via a random projection. We got this dataset from a private source and don\u2019t know much about its attributes. We used the given features and labels. 10. Speech: (Parkinson Speech dataset from UCI), Predictor: Median Pitch Features: All other attributes except the mean pitch, standard deviation, minimum pitch and maximum pitches which are not actual features but statistics of the pitch. 11. Music: (UCI), Predictor: Year of production Features: All other attributes: 12 timbre average and 78 timbre covariance 12. Telemonit: (Parkinson\u2019s Telemonitoring dataset from UCI), Predictor: total-UPDRS Features: All other features except subject-id and motor-UPDRS (since it was too correlated with total-UPDRS). We only consider the female subjects in the dataset. 13. Propulsion: (Naval Propulsion Plant dataset from UCI), Predictor: Lever Position Features: All other attributes. We picked a random attribute as the predictor since no clear predictor was specifified. 14. Airfoil*: (Airfoil Self-Noise dataset from UCI), Predictor: Sound Pressure Level Features: The other 5 features and 35 random features. 15. Forestfires: (UCI), Predictor: DC Features: All other attributes. We picked a random attribute as the predictor since no clear predictor was specifified. 16. Brain: (From Wehbe et al. (2014)), Predictor: Story feature at a given time step Features: Other attributes Some experimental details: GP is the Bayesian interpretation of KRR.", "startOffset": 17, "endOffset": 1422}], "year": 2016, "abstractText": "High dimensional nonparametric regression is an inherently difficult problem with known lower bounds depending exponentially in dimension. A popular strategy to alleviate this curse of dimensionality has been to use additive models of first order, which model the regression function as a sum of independent functions on each dimension. Though useful in controlling the variance of the estimate, such models are often too restrictive in practical settings. Between non-additive models which often have large variance and first order additive models which have large bias, there has been little work to exploit the trade-off in the middle via additive models of intermediate order. In this work, we propose SALSA, which bridges this gap by allowing interactions between variables, but controls model capacity by limiting the order of interactions. SALSA minimises the residual sum of squares with squared RKHS norm penalties. Algorithmically, it can be viewed as Kernel Ridge Regression with an additive kernel. When the regression function is additive, the excess risk is only polynomial in dimension. Using the Girard-Newton formulae, we efficiently sum over a combinatorial number of terms in the additive expansion. Via a comparison on 15 real datasets, we show that our method is competitive against 21 other alternatives.", "creator": "TeX"}}}