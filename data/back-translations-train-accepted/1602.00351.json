{"id": "1602.00351", "review": {"conference": "aaai", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Feb-2016", "title": "Adaptive Subgradient Methods for Online AUC Maximization", "abstract": "Learning for maximizing AUC performance is an important research problem in Machine Learning and Artificial Intelligence. Unlike traditional batch learning methods for maximizing AUC which often suffer from poor scalability, recent years have witnessed some emerging studies that attempt to maximize AUC by single-pass online learning approaches. Despite their encouraging results reported, the existing online AUC maximization algorithms often adopt simple online gradient descent approaches that fail to exploit the geometrical knowledge of the data observed during the online learning process, and thus could suffer from relatively larger regret. To address the above limitation, in this work, we explore a novel algorithm of Adaptive Online AUC Maximization (AdaOAM) which employs an adaptive gradient method that exploits the knowledge of historical gradients to perform more informative online learning. The new adaptive updating strategy of the AdaOAM is less sensitive to the parameter settings and maintains the same time complexity as previous non-adaptive counterparts. Additionally, we extend the algorithm to handle high-dimensional sparse data (SAdaOAM) and address sparsity in the solution by performing lazy gradient updating. We analyze the theoretical bounds and evaluate their empirical performance on various types of data sets. The encouraging empirical results obtained clearly highlighted the effectiveness and efficiency of the proposed algorithms.", "histories": [["v1", "Mon, 1 Feb 2016 00:25:18 GMT  (4125kb,D)", "http://arxiv.org/abs/1602.00351v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yi ding", "peilin zhao", "steven c h hoi", "yew-soon ong"], "accepted": true, "id": "1602.00351"}, "pdf": {"name": "1602.00351.pdf", "metadata": {"source": "CRF", "title": "Adaptive Subgradient Methods for Online AUC Maximization", "authors": ["Yi Ding", "Peilin Zhao", "Steven C.H. Hoi", "Yew-Soon Ong"], "emails": ["dingy@uchicago.edu.", "chhoi@smu.edu.sg."], "sections": [{"heading": null, "text": "Index terms - AUC maximization, second-order online learning, adaptive gradient, high-dimensional, economical."}, {"heading": "1 INTRODUCTION", "text": "In fact, it is the case that most of them are able to survive themselves if they do not go into another world, in which they go into another world, in which they go into another world, in which they go into another world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in fact, in which they, in fact, in which they, in which they, in which they, in fact, in which they, in which they, in fact, live, in which they, live, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, in which they, in fact, in which they, in fact, in a fact, in a different world,"}, {"heading": "2 RELATED WORK", "text": "This year it is so far that it will only take a few more days to reach an agreement."}, {"heading": "3 ADAPTIVE SUBGRADIENT METHODS FOR OAM", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Problem Setting", "text": "We aim to learn a linear classification model that maximizes the AUC for a binary classification problem. Without loss of generality, we assume that the positive class is smaller than the negative class. Denote (xt, yt) as the training instance obtained in the t-test, with xt, Rd and yt, and wt, Rd being the weight vector learned so far. In view of this setting, we define the AUC measurement [1] for the binary classification task. Considering a data set D = {(xi, yi), the indicator Rd \u00b7 {\u2212 1, + 1} | i, in which [n] = {1, 2,. \u2212 n} we naturally divide it into two sets: the set of positive instances D + = {(x + i, + 1) | i, [n +] the indicator x, and the set of negative instances, D + n = the series \u2212 n in relation to the D."}, {"heading": "3.2 Adaptive Online AUC Maximization", "text": "This is not the case. (...) [...] (...] (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) () (...) () () () () () () (...) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () ()) () () () () () () ()) () () () ()) () () () () ()) () () () ()) () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () () () () () (() () () () () () () () (() () () (() () () () () () () (() () () (() () () () ("}, {"heading": "3.3 Fast AdaOAM Algorithm for High-dimensional Sparse Data", "text": "One feature of the proposed AdaOAM algorithms is that they exploit the full features for weight-based classification = = = = \"The number of features is large, but there is no information on the detection task.\" Research in [30] has shown that classification performance with dozens of features out of tens of thousands of features can be sufficient to improve the efficiency and scalability of AdaOAM algorithms (SAdaOAM algorithms), which learn a frugal linear classification that includes a limited size of active features. (Specifically, SAdaOAM addresses the issue of thrift in the learned model)"}, {"heading": "4 THEORETICAL ANALYSIS", "text": "In this section, we outline the limits of regret for the proposed set of AdaOAM algorithms for handling both normal and high-dimensional sparse data."}, {"heading": "4.1 Regret Bounds with Regular Data", "text": "First, we present two lemmas that are used to facilitate our subsequent analyses: < p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p p = p = p = p = p p p = p"}, {"heading": "4.2 Regret Bounds with High-dimensional Sparse Data", "text": "Theorem 2: \"We assume that we will not be able to learn more than we are able to learn.\" Theorem 2: \"We assume that we are not able to be able to.\" Theorem 2: \"We do not regret.\" Theorem 2: \"We do not regret.\" Theorem 2: \"We do not regret.\" Theorem 2: \"We do not regret.\" Theorem 2: \"We do not regret.\" Theorem 2: \"We regret.\" We regret that we are able to be able. \"Theorem 2:\" We do not regret. \"We regret that we are able to be able.\" Theorem 1: \"We do not regret.\" Theorem 2: \"We regret that we are able to be able.\" Theorem 2: \"We do not regret.\""}, {"heading": "5 EXPERIMENTAL RESULTS", "text": "In this section we evaluate the proposed set of AdaOAM algorithms in terms of AUC performance, convergence rate and their parameter sensitivity. The main framework of the experiments is based on LIBOL, an open source library for online learning algorithms 1 [32]."}, {"heading": "5.1 Comparison Algorithms", "text": "We conduct extensive empirical studies by comparing the proposed algorithms with various AUC optimization algorithms for online and batch scenarios. Specifically, the algorithms considered in our experiments include: \u2022 Online Uni-Exp: An online learning algorithm that optimizes (weighted) univariate exponential loss [33]; \u2022 Online Uni-Log: An online learning algorithm that optimizes (weighted) univariate logistic loss [33]; \u2022 OAMseq: The OAM algorithm with reservoir sampling and sequential update method [6]; \u2022 OAMgra: The OAM algorithm with reservoir sampling and online gradient update method [6]; \u2022 OPAUC: The one-pass AUC optimization algorithm with square loss function [7]; SVM-perf: A batch algorithm that directly optimizes AUC basic data."}, {"heading": "5.2 Experimental Testbed and Setup", "text": "In order to investigate the performance of the proposed AdaOAM compared to existing state-of-the-art methods, we are conducting extensive experiments on sixteen benchmark datasets, maintaining consistency with previous studies on online AUC maximization [6], [7]. Table 1 shows the details of 16 binary class datasets in our experiments. All of these datasets can be downloaded from LIBSVM 2 and the UCI Machine Learning Repository 3. Note that several datasets (svmguide4, vehicle) are originally multi-level datasets that were converted to class unbalanced binary datasets for the purpose of our experimental studies. \u2212 In the experiments, the features were fairly normalized, i.e., xt \u00b2 xt \u00b2 xt / xt \u00b2 xt xt, which is reasonable since instances are received sequentially in online learning environments."}, {"heading": "5.3 Evaluation of AdaOAM on Benchmark Datasets", "text": "In this table we have the following results in Table 2, which we have classified as insufficient in most cases: \"A,\" \"A,\" \"A,\" \"A,\" \"O,\" \"O,\" \"O,\" \"O,\" \"O,\" \"O,\" \"O,\" \"O,\" \"O,\" \"O,\" \"O,\" \"O,\" \"O,\" \"O,\" \"\" O, \"\" \"O,\" \"\" O, \"\" \"O,\" \"\" O, \"\" \"O,\" \"\" O, \"\" \"O,\" \"O,\" \"O,\" \"O,\" O, \"O,\" O, \"\" O, \"\" O, \"\" O, \"O,\" O, \"O,\" O, \"O,\" O, \"O,\" O, \"O,\" O, \"O,\" O, \"O,\" O, \"O,\" O, \"O,\" O, \"O,\" O, \"O,\" O, \"O,\" O, \"O,\" O, \"O,\" O, \"O,\" O, \"O,\" O, \"O,\" O, \"O,\" O, \"O,\" O, \"O,\" O, \"O,\" O, \"O,\" O, \"O,\" O, \"O,\" O, \"O,\" O, \"O,\" O, \"O,\" O, \"O,\" O, \"O,\" O, \"O,\" O, \"O,\" O, \"O,\" O, \"O,\" O, \"O,\" O, \"O,\" O, O, \"O,\" O, \"O, O, O,\" O, O, \"O, O, O,\" O, O, O, \"O, O, O,\" O, O, O, \"O, O, O,\" O, \"O, O,\" O, \"O,\" O, O, \"O, O, O,\" O, O, \"O,\" O, O, \"O, O, O,\" O, O, \"O,\" O, \"O,\" O, \"O,\" O, \"O,\" O, \"O,"}, {"heading": "5.4 Evaluation of Online Performance", "text": "Next, we examine the online performance of AdaOAM compared to the other online learning algorithms and highlight six sets of data for illustration. Specifically, Figure 3 (a) - (h) shows the average AUC performance (over 20 independent runs) of the online learning model on the test data sets. Results show that AdaOAM is once again 9Fig. 1 runtime (in milliseconds) of AdaOAM and the other online learning algorithms on all 16 benchmark datasets. Note that the y-axis is shown in the log scale.10 that it significantly outperforms all other three equivalents in the online learning process, which is consistent with our theoretical analysis that AdaOAM can leverage second-order information more effectively to achieve improved limits of regret and robust performance."}, {"heading": "5.5 Evaluation of Parameter Sensitivity", "text": "In our study, we experimented with AdaOAM with a range of different learning rates in the wide range \u03b7 2 [\u2212 8: 4]. AdaOAM's average AUC test results within the broad range of learning rates after a single run through the training data of the respective benchmark datasets are then summarized in Figure 4 (a) - (h). Due to spatial limitations, the results of 8 datasets are presented here for illustration. As the AdaOAM algorithm provides a function-dependent adaptive learning rate at each iteration, it is less sensitive to the learning rate itself than the standard SGD. In [7], the authors claimed that OPAUC is insensitive to parameter settings. Figure 4 indicates that AdaOAM is significantly more robust or less sensitive to learning rate than the OPAUC."}, {"heading": "5.6 Evaluation of SAdaOAM on High-dimensional Sparse Datasets", "text": "The fact is that most of them will be able to assert themselves in the way they do it: in the way they do it, in the way they do it, in the way they do it, in the way they do it, in the way they do it, in the way they do it, in the way they do it, in the way they do it. \""}, {"heading": "5.7 Evaluation of SAdaOAM on Sparsity-AUC Tradeoffs", "text": "To achieve this, we set the regularization parameter for \"1 with the range of non-zeros in the final weight solution after a single run through the training data. These experimental settings result in the final model solutions ranging from an almost dense weight to an almost zero. We randomly select three sets of data for this set of experiments and show the results in Figure 5.1112From this figure, we can conclude that there are indeed trade-offs between the degree of scarcity and AUC performance. At high regularization parameters, the SAdaOAM model performs poorly as expected, as the weight vector is too sparse and generalizes poorly."}, {"heading": "5.8 Application to Real World Online Anomaly Detection Task", "text": "In this subsection, we will show an application of the proposed algorithm, namely AdaOAM, to solve online anomaly detection tasks. Specifically, we will begin with an introduction of the applications followed by a presentation of the empirical results. To be precise, we will consider the following four domains: Webspam: We will use AdaOAM to detect malicious websites using the \"Webspam u\" dataset from the subsection of the Pascal Large Scale Learning Challenge. [37] Sensor Errors: We will use AdaOAM to identify sensor errors in buildings using the \"smartBuilding\" dataset, where the sensors monitor the concentration of contaminant of interest (e.g. CO2) in different zones in a building."}, {"heading": "6 CONCLUSION", "text": "In this paper, we have proposed two adaptive subgradient online AUC maximization approaches for dealing with regular and high-dimensional sparse data that take into account historical component-wise gradient information for more efficient and adaptive learning. Our proposed algorithms use second-order information to accelerate online AUC maximization, and are less sensitive to parameter setting than the simple SGD strategy. Theoretically, we have derived and analyzed the limitations of regretting AdaOAM's online adaptive AUC maximization approaches and 13TABLE 8 AUC performance assessment (mean \u00b1 hour) compared to other online data set anomalgorithms."}, {"heading": "ACKNOWLEDGMENTS", "text": "This research is supported in part by the MultiplAtform Game Innovation Centre (MAGIC) at Nanyang Technological University. MAGIC is funded by the Interactive Digital Media Programme Office (IDMPO), which is operated by the Media Development Authority of Singapore. IDMPO was established in 2006 on behalf of the National Research Foundation to deepen Singapore's research capacities in the field of interactive digital media (IDM), drive innovation and shape the future of media. Furthermore, the last author is grateful for the support of the Singapore MOE tier 1 research grant (C220 / MSS14C003)."}], "references": [{"title": "An introduction to roc analysis", "author": ["T. Fawcett"], "venue": "Pattern Recogn. Lett., vol. 27, pp. 861\u2013874, 2006.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Auc optimization vs. error rate minimization", "author": ["C. Cortes", "M. Mohri"], "venue": "NIPS, 2003.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Efficient auc optimization for classification", "author": ["T. Calders", "S. Jaroszewicz"], "venue": "PKDD, 2007, pp. 42\u201353.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "A support vector method for multivariate performance measures", "author": ["T. Joachims"], "venue": "ICML, 2005, pp. 377\u2013384.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Margin-based ranking and an equivalence between adaboost and rankboost", "author": ["C. Rudin", "R.E. Schapire"], "venue": "Journal of Machine Learning Research, vol. 10, pp. 2193\u20132232, 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Online AUC maximization", "author": ["P. Zhao", "S.C.H. Hoi", "R. Jin", "T. Yang"], "venue": "Proceedings of the 28th International Conference on Machine Learning, ICML 2011, Bellevue, Washington, USA, June 28 - July 2, 2011, 2011, pp. 233\u2013240.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "One-pass auc optimization", "author": ["W. Gao", "R. Jin", "S. Zhu", "Z.-H. Zhou"], "venue": "ICML (3), 2013, pp. 906\u2013914.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "On the generalization ability of online learning algorithms for pairwise loss functions", "author": ["P. Kar", "B.K. Sriperumbudur", "P. Jain", "H. Karnick"], "venue": "Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, 2013, pp. 441\u2013449.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "On the consistency of auc optimization", "author": ["W. Gao", "Z.-H. Zhou"], "venue": "arXiv preprint arXiv:1208.0645, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J.C. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research, vol. 12, pp. 2121\u20132159, 2011.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Prediction, learning, and games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Online passive-aggressive algorithms", "author": ["K. Crammer", "O. Dekel", "J. Keshet", "S. Shalev-Shwartz", "Y. Singer"], "venue": "Journal of Machine Learning Research, vol. 7, pp. 551\u2013585, 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Double updating online learning", "author": ["P. Zhao", "S.C.H. Hoi", "R. Jin"], "venue": "Journal of Machine Learning Research, vol. 12, pp. 1587\u20131615, 2011.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Online multiple kernel classification", "author": ["S.C.H. Hoi", "R. Jin", "P. Zhao", "T. Yang"], "venue": "Machine Learning, vol. 90, no. 2, pp. 289\u2013316, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Online transfer learning", "author": ["P. Zhao", "S.C.H. Hoi", "J. Wang", "B. Li"], "venue": "Artif. Intell., vol. 216, pp. 76\u2013102, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "The perceptron: a probabilistic model for information storage and organization in the brain.", "author": ["F. Rosenblatt"], "venue": "Psychological review,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1958}, {"title": "Confidence-weighted linear classification", "author": ["M. Dredze", "K. Crammer", "F. Pereira"], "venue": "Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML 2008), Helsinki, Finland, June 5-9, 2008, 2008, pp. 264\u2013271.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Adaptive regularization of weight vectors", "author": ["K. Crammer", "A. Kulesza", "M. Dredze"], "venue": "Advances in Neural Information Processing Systems 22: 23rd Annual Conference on Neural Information Processing Systems 2009. Proceedings of a meeting held 7-10 December 2009, Vancouver, British Columbia, Canada., 2009, pp. 414\u2013422.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "New adaptive algorithms for online classification", "author": ["F. Orabona", "K. Crammer"], "venue": "Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010. Proceedings of a meeting held 6-9 December 2010, Vancouver, British Columbia, Canada., 2010, pp. 1840\u20131848.  14", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Exact soft confidence-weighted learning", "author": ["J. Wang", "P. Zhao", "S.C.H. Hoi"], "venue": "Proceedings of the 29th International Conference on Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012, 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Cost-sensitive online classification", "author": ["\u2014\u2014"], "venue": "IEEE Trans. Knowl. Data Eng., vol. 26, no. 10, pp. 2425\u20132438, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Cost-sensitive online active learning with application to malicious URL detection", "author": ["P. Zhao", "S.C.H. Hoi"], "venue": "The 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 2013, Chicago, IL, USA, August 11-14, 2013, 2013, pp. 919\u2013927.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Cost-sensitive double updating online learning and its application to online anomaly detection", "author": ["S.C.H. Hoi", "P. Zhao"], "venue": "Proceedings of the 13th SIAM International Conference on Data Mining, May 2-4, 2013. Austin, Texas, USA., 2013, pp. 207\u2013215.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Sparse online learning via truncated gradient", "author": ["J. Langford", "L. Li", "T. Zhang"], "venue": "Journal of Machine Learning Research, vol. 10, pp. 777\u2013801, 2009.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficient online and batch learning using forward backward splitting", "author": ["J.C. Duchi", "Y. Singer"], "venue": "Journal of Machine Learning Research, vol. 10, pp. 2899\u20132934, 2009.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Dual averaging methods for regularized stochastic learning and online optimization", "author": ["L. Xiao"], "venue": "Journal of Machine Learning Research, vol. 11, pp. 2543\u20132596, 2010.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Stochastic methods for l1-regularized loss minimization", "author": ["S. Shalev-Shwartz", "A. Tewari"], "venue": "Journal of Machine Learning Research, vol. 12, pp. 1865\u20131892, 2011.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1865}, {"title": "Primal-dual subgradient methods for convex problems", "author": ["Y. Nesterov"], "venue": "Math. Program., vol. 120, no. 1, pp. 221\u2013259, 2009.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "Machine Learning, Proceedings of the Twentieth International Conference (ICML 2003), August 21-24, 2003, Washington, DC, USA, 2003, pp. 928\u2013936.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2003}, {"title": "Spam email classification using an adaptive ontology", "author": ["S. Youn", "D. McLeod"], "venue": "JSW, vol. 2, no. 3, pp. 43\u201355, 2007.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "On accelerated proximal gradient methods for convex-concave optimization", "author": ["P. Tseng"], "venue": "Technical report, Department of Mathematics, University of Washington, 2008.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "LIBOL: a library for online learning algorithms", "author": ["S.C.H. Hoi", "J. Wang", "P. Zhao"], "venue": "Journal of Machine Learning Research, vol. 15, no. 1, pp. 495\u2013499, 2014.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Bipartite ranking through minimization of univariate loss", "author": ["W. Kotlowski", "K. Dembczynski", "E. H\u00fcllermeier"], "venue": "Proceedings of the 28th International Conference on Machine Learning, ICML 2011, Bellevue, Washington, USA, June 28 - July 2, 2011, 2011, pp. 1113\u20131120.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient optimization of performance measures by classifier adaptation", "author": ["N. Li", "I.W. Tsang", "Z. Zhou"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 6, pp. 1370\u20131382, 2013.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Core vector machines: Fast SVM training on very large data sets", "author": ["I.W. Tsang", "J.T. Kwok", "P. Cheung"], "venue": "Journal of Machine Learning Research, vol. 6, pp. 363\u2013392, 2005.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2005}, {"title": "Active learning using on-line algorithms", "author": ["C. Mesterharm", "M.J. Pazzani"], "venue": "In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD, 2011.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Evolutionary study of web spam: Webb spam corpus 2011 versus webb spam corpus 2006", "author": ["D. Wang", "D. Irani", "C. Pu"], "venue": "8th International Conference on Collaborative Computing: Networking, Applications and Worksharing, CollaborateCom 2012, Pittsburgh, PA, USA, October 14- 17, 2012, 2012, pp. 40\u201349.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "SNAP: fault tolerant event location estimation in sensor networks using binary data", "author": ["M.P. Michaelides", "C.G. Panayiotou"], "venue": "IEEE Trans. Computers, vol. 58, no. 9, pp. 1185\u20131197, 2009.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2009}, {"title": "Dissecting android malware: Characterization and evolution", "author": ["Y. Zhou", "X. Jiang"], "venue": "IEEE Symposium on Security and Privacy, 2012, pp. 95\u2013109.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "Using probabilistic generative models for ranking risks of android apps", "author": ["H. Peng", "C.S. Gates", "B.P. Sarma", "N. Li", "Y. Qi", "R. Potharaju", "C. Nita- Rotaru", "I. Molloy"], "venue": "ACM Conference on Computer and Communications Security, 2012, pp. 241\u2013252.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "AUC (Area Under ROC curve) [1] is an important measure for characterizing machine learning performances in many realworld applications, such as ranking, and anomaly detection tasks, especially when misclassification costs are unknown.", "startOffset": 27, "endOffset": 30}, {"referenceID": 1, "context": "Many efforts have been devoted recently to developing efficient AUC optimization algorithms for both batch and online learning tasks [2], [3], [4], [5], [6], [7].", "startOffset": 133, "endOffset": 136}, {"referenceID": 2, "context": "Many efforts have been devoted recently to developing efficient AUC optimization algorithms for both batch and online learning tasks [2], [3], [4], [5], [6], [7].", "startOffset": 138, "endOffset": 141}, {"referenceID": 3, "context": "Many efforts have been devoted recently to developing efficient AUC optimization algorithms for both batch and online learning tasks [2], [3], [4], [5], [6], [7].", "startOffset": 143, "endOffset": 146}, {"referenceID": 4, "context": "Many efforts have been devoted recently to developing efficient AUC optimization algorithms for both batch and online learning tasks [2], [3], [4], [5], [6], [7].", "startOffset": 148, "endOffset": 151}, {"referenceID": 5, "context": "Many efforts have been devoted recently to developing efficient AUC optimization algorithms for both batch and online learning tasks [2], [3], [4], [5], [6], [7].", "startOffset": 153, "endOffset": 156}, {"referenceID": 6, "context": "Many efforts have been devoted recently to developing efficient AUC optimization algorithms for both batch and online learning tasks [2], [3], [4], [5], [6], [7].", "startOffset": 158, "endOffset": 161}, {"referenceID": 5, "context": "The first framework is based on the idea of buffer sampling [6], [8], which stores some randomly sampled historical examples in a buffer to represent the observed data for calculating the pairwise loss functions.", "startOffset": 60, "endOffset": 63}, {"referenceID": 7, "context": "The first framework is based on the idea of buffer sampling [6], [8], which stores some randomly sampled historical examples in a buffer to represent the observed data for calculating the pairwise loss functions.", "startOffset": 65, "endOffset": 68}, {"referenceID": 6, "context": "The other framework focuses on one-pass AUC optimization [7], where the algorithm", "startOffset": 57, "endOffset": 60}, {"referenceID": 8, "context": "The benefit of one-pass AUC optimization lies in the use of squared loss to represent the AUC loss function while providing proofs on its consistency with the AUC measure [9].", "startOffset": 171, "endOffset": 174}, {"referenceID": 9, "context": "Although these algorithms have been shown to be capable of achieving fairly good AUC performances, they share a common trait of employing the online gradient descent technique, which fail to take advantage of the geometrical property of the data observed from the online learning process, while recent studies have shown the importance of exploiting this information for online optimization [10].", "startOffset": 391, "endOffset": 395}, {"referenceID": 9, "context": "To achieve this purpose, we propose the AdaOAM algorithm by adopting the adaptive gradient updating framework proposed by [10] to control the learning rates for different features.", "startOffset": 122, "endOffset": 126}, {"referenceID": 10, "context": "Online learning has been extensively studied in the machine learning communities [11], [12], [13], [14], [15], mainly due to its high efficiency and scalability to large-scale learning tasks.", "startOffset": 81, "endOffset": 85}, {"referenceID": 11, "context": "Online learning has been extensively studied in the machine learning communities [11], [12], [13], [14], [15], mainly due to its high efficiency and scalability to large-scale learning tasks.", "startOffset": 87, "endOffset": 91}, {"referenceID": 12, "context": "Online learning has been extensively studied in the machine learning communities [11], [12], [13], [14], [15], mainly due to its high efficiency and scalability to large-scale learning tasks.", "startOffset": 93, "endOffset": 97}, {"referenceID": 13, "context": "Online learning has been extensively studied in the machine learning communities [11], [12], [13], [14], [15], mainly due to its high efficiency and scalability to large-scale learning tasks.", "startOffset": 99, "endOffset": 103}, {"referenceID": 14, "context": "Online learning has been extensively studied in the machine learning communities [11], [12], [13], [14], [15], mainly due to its high efficiency and scalability to large-scale learning tasks.", "startOffset": 105, "endOffset": 109}, {"referenceID": 15, "context": "A number of first-order algorithms have been proposed including the well-known Perceptron algorithm [16] and the Passive-Aggressive (PA) algorithm [12].", "startOffset": 100, "endOffset": 104}, {"referenceID": 11, "context": "A number of first-order algorithms have been proposed including the well-known Perceptron algorithm [16] and the Passive-Aggressive (PA) algorithm [12].", "startOffset": 147, "endOffset": 151}, {"referenceID": 16, "context": "In order to address this issue, recent years have witnessed some second-order online learning algorithms [17], [18], [19], [20], which apply parameter confidence information to improve online learning performance.", "startOffset": 105, "endOffset": 109}, {"referenceID": 17, "context": "In order to address this issue, recent years have witnessed some second-order online learning algorithms [17], [18], [19], [20], which apply parameter confidence information to improve online learning performance.", "startOffset": 111, "endOffset": 115}, {"referenceID": 18, "context": "In order to address this issue, recent years have witnessed some second-order online learning algorithms [17], [18], [19], [20], which apply parameter confidence information to improve online learning performance.", "startOffset": 117, "endOffset": 121}, {"referenceID": 19, "context": "In order to address this issue, recent years have witnessed some second-order online learning algorithms [17], [18], [19], [20], which apply parameter confidence information to improve online learning performance.", "startOffset": 123, "endOffset": 127}, {"referenceID": 20, "context": "Further, in order to solve the costsensitive classification tasks on-the-fly, online learning researchers have also proposed a few novel online learning algorithms to directly optimize some more meaningful cost-sensitive metrics [21], [22], [23].", "startOffset": 229, "endOffset": 233}, {"referenceID": 21, "context": "Further, in order to solve the costsensitive classification tasks on-the-fly, online learning researchers have also proposed a few novel online learning algorithms to directly optimize some more meaningful cost-sensitive metrics [21], [22], [23].", "startOffset": 235, "endOffset": 239}, {"referenceID": 22, "context": "Further, in order to solve the costsensitive classification tasks on-the-fly, online learning researchers have also proposed a few novel online learning algorithms to directly optimize some more meaningful cost-sensitive metrics [21], [22], [23].", "startOffset": 241, "endOffset": 245}, {"referenceID": 1, "context": "Recently, many algorithms have been developed to optimize AUC directly [2], [3], [4], [6], [7].", "startOffset": 71, "endOffset": 74}, {"referenceID": 2, "context": "Recently, many algorithms have been developed to optimize AUC directly [2], [3], [4], [6], [7].", "startOffset": 76, "endOffset": 79}, {"referenceID": 3, "context": "Recently, many algorithms have been developed to optimize AUC directly [2], [3], [4], [6], [7].", "startOffset": 81, "endOffset": 84}, {"referenceID": 5, "context": "Recently, many algorithms have been developed to optimize AUC directly [2], [3], [4], [6], [7].", "startOffset": 86, "endOffset": 89}, {"referenceID": 6, "context": "Recently, many algorithms have been developed to optimize AUC directly [2], [3], [4], [6], [7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 3, "context": "In [4], the author firstly presented a general framework for optimizing multivariate nonlinear performance measures such as the AUC, F1, etc.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "The first framework is based on the idea of buffer sampling [6], [8], which employed a fixed-size buffer to represent the observed data for calculating the pairwise loss functions.", "startOffset": 60, "endOffset": 63}, {"referenceID": 7, "context": "The first framework is based on the idea of buffer sampling [6], [8], which employed a fixed-size buffer to represent the observed data for calculating the pairwise loss functions.", "startOffset": 65, "endOffset": 68}, {"referenceID": 5, "context": "A representative study is available in [6], which leveraged the reservoir sampling technique to represent the observed data instances by a fixed-size buffer where notable theoretical and empirical results have been reported.", "startOffset": 39, "endOffset": 42}, {"referenceID": 7, "context": "Then, [8] studied the improved generalization capability of online learning algorithms for pairwise loss functions with the framework of buffer sampling.", "startOffset": 6, "endOffset": 9}, {"referenceID": 6, "context": "The other framework which takes a different perspective was presented by [7].", "startOffset": 73, "endOffset": 76}, {"referenceID": 23, "context": "Recent years have witnessed extensive research studies on sparse online learning [24], [25], [26], [27], which aim to learn sparse classifiers by limiting the number of active features.", "startOffset": 81, "endOffset": 85}, {"referenceID": 24, "context": "Recent years have witnessed extensive research studies on sparse online learning [24], [25], [26], [27], which aim to learn sparse classifiers by limiting the number of active features.", "startOffset": 87, "endOffset": 91}, {"referenceID": 25, "context": "Recent years have witnessed extensive research studies on sparse online learning [24], [25], [26], [27], which aim to learn sparse classifiers by limiting the number of active features.", "startOffset": 93, "endOffset": 97}, {"referenceID": 26, "context": "Recent years have witnessed extensive research studies on sparse online learning [24], [25], [26], [27], which aim to learn sparse classifiers by limiting the number of active features.", "startOffset": 99, "endOffset": 103}, {"referenceID": 24, "context": "Taking the FOBOS algorithm [25] as an example, which is based on the Forward-Backward Splitting method to solve the sparse online learning problem by alternating between two phases: (i) an unconstraint stochastic subgradient descent step with respect to the loss function, and (ii) an instantaneous optimization for a tradeoff between keeping close proximity to the result of the first step and minimizing `1 regularization term.", "startOffset": 27, "endOffset": 31}, {"referenceID": 23, "context": "Following this strategy, [24] argues that the truncation at each step is too aggressive and thus proposes the Truncated Gradient (TG) method, which alleviates the updates by truncating the coefficients at every K steps when they are lower than a predefined threshold.", "startOffset": 25, "endOffset": 29}, {"referenceID": 27, "context": "The second category of methods are mainly motivated by the dual averaging method [28].", "startOffset": 81, "endOffset": 85}, {"referenceID": 25, "context": "The most popular method in this category is the Regularized Dual Averaging (RDA) [26], which solves the optimization problem by using the running average of all past subgradients of the loss functions and the whole regularization term instead of the subgradient.", "startOffset": 81, "endOffset": 85}, {"referenceID": 0, "context": "Given this setting, let us define the AUC measurement [1] for binary classification task.", "startOffset": 54, "endOffset": 57}, {"referenceID": 6, "context": ", the square loss from [7] due to its consistency with AUC [9]", "startOffset": 23, "endOffset": 26}, {"referenceID": 8, "context": ", the square loss from [7] due to its consistency with AUC [9]", "startOffset": 59, "endOffset": 62}, {"referenceID": 6, "context": "Following the similar approach in [7], we modify the loss function L(w) in (1) as a sum of losses", "startOffset": 34, "endOffset": 37}, {"referenceID": 28, "context": "Upon obtaining gradient gt = \u2207Lt(wt), a simple solution is to move the weight wt in the opposite direction of gt, while keeping \u2016wt+1\u2016 \u2264 1/ \u221a \u03bb via the projected gradient update [29]", "startOffset": 178, "endOffset": 182}, {"referenceID": 9, "context": ", Adaptive Gradient Updating strategy, as inspired by [10].", "startOffset": 54, "endOffset": 58}, {"referenceID": 29, "context": "The research work in [30] has shown that the classification performance saturates with dozens of features out of tens of thousands of features.", "startOffset": 21, "endOffset": 25}, {"referenceID": 30, "context": "In order to optimize this objective function, we apply the composite mirror descent method [31] that is able to achieve a trade-off between the immediate adaptive gradient term gt and the regularizer \u03c6(w).", "startOffset": 91, "endOffset": 95}, {"referenceID": 9, "context": "Following the framework of the composite mirror descent update in [10], the update needed to solve is", "startOffset": 66, "endOffset": 70}, {"referenceID": 9, "context": "From the Algorithm 2, it is observed that we perform \u201clazy\u201d computation when the gradient vectors are sparse [10].", "startOffset": 109, "endOffset": 113}, {"referenceID": 9, "context": "These two lemmas are actually the Lemma 4 and Corollary 1 in the paper [10].", "startOffset": 71, "endOffset": 75}, {"referenceID": 9, "context": "derivation results of [10] and attain the following regret bound", "startOffset": 22, "endOffset": 26}, {"referenceID": 9, "context": "From [10], we have", "startOffset": 5, "endOffset": 9}, {"referenceID": 31, "context": "The main framework of the experiments is based on the LIBOL, an open-source library for online learning algorithms 1 [32].", "startOffset": 117, "endOffset": 121}, {"referenceID": 32, "context": "\u2022 Online Uni-Exp: An online learning algorithm which optimizes the (weighted) univariate exponential loss [33]; \u2022 Online Uni-Log: An online learning algorithm which optimizes the (weighted) univariate logistic loss [33]; \u2022 OAMseq: The OAM algorithm with reservoir sampling and sequential updating method [6]; \u2022 OAMgra: The OAM algorithm with reservoir sampling and online gradient updating method [6]; \u2022 OPAUC: The one-pass AUC optimization algorithm with square loss function [7]; \u2022 SVM-perf: A batch algorithm which directly optimizes AUC [4]; \u2022 CAPO: A batch algorithm which trains nonlinear auxiliary classifiers first and then adapts auxiliary classifiers for specific performance measures [34]; \u2022 Batch Uni-Log: A batch algorithm which optimizes the (weighted) univariate logistic loss [33]; \u2022 Batch Uni-Squ: A batch algorithm which optimizes the (weighted) univariate square loss; \u2022 AdaOAM: The proposed adaptive gradient method for online AUC maximization.", "startOffset": 106, "endOffset": 110}, {"referenceID": 32, "context": "\u2022 Online Uni-Exp: An online learning algorithm which optimizes the (weighted) univariate exponential loss [33]; \u2022 Online Uni-Log: An online learning algorithm which optimizes the (weighted) univariate logistic loss [33]; \u2022 OAMseq: The OAM algorithm with reservoir sampling and sequential updating method [6]; \u2022 OAMgra: The OAM algorithm with reservoir sampling and online gradient updating method [6]; \u2022 OPAUC: The one-pass AUC optimization algorithm with square loss function [7]; \u2022 SVM-perf: A batch algorithm which directly optimizes AUC [4]; \u2022 CAPO: A batch algorithm which trains nonlinear auxiliary classifiers first and then adapts auxiliary classifiers for specific performance measures [34]; \u2022 Batch Uni-Log: A batch algorithm which optimizes the (weighted) univariate logistic loss [33]; \u2022 Batch Uni-Squ: A batch algorithm which optimizes the (weighted) univariate square loss; \u2022 AdaOAM: The proposed adaptive gradient method for online AUC maximization.", "startOffset": 215, "endOffset": 219}, {"referenceID": 5, "context": "\u2022 Online Uni-Exp: An online learning algorithm which optimizes the (weighted) univariate exponential loss [33]; \u2022 Online Uni-Log: An online learning algorithm which optimizes the (weighted) univariate logistic loss [33]; \u2022 OAMseq: The OAM algorithm with reservoir sampling and sequential updating method [6]; \u2022 OAMgra: The OAM algorithm with reservoir sampling and online gradient updating method [6]; \u2022 OPAUC: The one-pass AUC optimization algorithm with square loss function [7]; \u2022 SVM-perf: A batch algorithm which directly optimizes AUC [4]; \u2022 CAPO: A batch algorithm which trains nonlinear auxiliary classifiers first and then adapts auxiliary classifiers for specific performance measures [34]; \u2022 Batch Uni-Log: A batch algorithm which optimizes the (weighted) univariate logistic loss [33]; \u2022 Batch Uni-Squ: A batch algorithm which optimizes the (weighted) univariate square loss; \u2022 AdaOAM: The proposed adaptive gradient method for online AUC maximization.", "startOffset": 304, "endOffset": 307}, {"referenceID": 5, "context": "\u2022 Online Uni-Exp: An online learning algorithm which optimizes the (weighted) univariate exponential loss [33]; \u2022 Online Uni-Log: An online learning algorithm which optimizes the (weighted) univariate logistic loss [33]; \u2022 OAMseq: The OAM algorithm with reservoir sampling and sequential updating method [6]; \u2022 OAMgra: The OAM algorithm with reservoir sampling and online gradient updating method [6]; \u2022 OPAUC: The one-pass AUC optimization algorithm with square loss function [7]; \u2022 SVM-perf: A batch algorithm which directly optimizes AUC [4]; \u2022 CAPO: A batch algorithm which trains nonlinear auxiliary classifiers first and then adapts auxiliary classifiers for specific performance measures [34]; \u2022 Batch Uni-Log: A batch algorithm which optimizes the (weighted) univariate logistic loss [33]; \u2022 Batch Uni-Squ: A batch algorithm which optimizes the (weighted) univariate square loss; \u2022 AdaOAM: The proposed adaptive gradient method for online AUC maximization.", "startOffset": 397, "endOffset": 400}, {"referenceID": 6, "context": "\u2022 Online Uni-Exp: An online learning algorithm which optimizes the (weighted) univariate exponential loss [33]; \u2022 Online Uni-Log: An online learning algorithm which optimizes the (weighted) univariate logistic loss [33]; \u2022 OAMseq: The OAM algorithm with reservoir sampling and sequential updating method [6]; \u2022 OAMgra: The OAM algorithm with reservoir sampling and online gradient updating method [6]; \u2022 OPAUC: The one-pass AUC optimization algorithm with square loss function [7]; \u2022 SVM-perf: A batch algorithm which directly optimizes AUC [4]; \u2022 CAPO: A batch algorithm which trains nonlinear auxiliary classifiers first and then adapts auxiliary classifiers for specific performance measures [34]; \u2022 Batch Uni-Log: A batch algorithm which optimizes the (weighted) univariate logistic loss [33]; \u2022 Batch Uni-Squ: A batch algorithm which optimizes the (weighted) univariate square loss; \u2022 AdaOAM: The proposed adaptive gradient method for online AUC maximization.", "startOffset": 477, "endOffset": 480}, {"referenceID": 3, "context": "\u2022 Online Uni-Exp: An online learning algorithm which optimizes the (weighted) univariate exponential loss [33]; \u2022 Online Uni-Log: An online learning algorithm which optimizes the (weighted) univariate logistic loss [33]; \u2022 OAMseq: The OAM algorithm with reservoir sampling and sequential updating method [6]; \u2022 OAMgra: The OAM algorithm with reservoir sampling and online gradient updating method [6]; \u2022 OPAUC: The one-pass AUC optimization algorithm with square loss function [7]; \u2022 SVM-perf: A batch algorithm which directly optimizes AUC [4]; \u2022 CAPO: A batch algorithm which trains nonlinear auxiliary classifiers first and then adapts auxiliary classifiers for specific performance measures [34]; \u2022 Batch Uni-Log: A batch algorithm which optimizes the (weighted) univariate logistic loss [33]; \u2022 Batch Uni-Squ: A batch algorithm which optimizes the (weighted) univariate square loss; \u2022 AdaOAM: The proposed adaptive gradient method for online AUC maximization.", "startOffset": 541, "endOffset": 544}, {"referenceID": 33, "context": "\u2022 Online Uni-Exp: An online learning algorithm which optimizes the (weighted) univariate exponential loss [33]; \u2022 Online Uni-Log: An online learning algorithm which optimizes the (weighted) univariate logistic loss [33]; \u2022 OAMseq: The OAM algorithm with reservoir sampling and sequential updating method [6]; \u2022 OAMgra: The OAM algorithm with reservoir sampling and online gradient updating method [6]; \u2022 OPAUC: The one-pass AUC optimization algorithm with square loss function [7]; \u2022 SVM-perf: A batch algorithm which directly optimizes AUC [4]; \u2022 CAPO: A batch algorithm which trains nonlinear auxiliary classifiers first and then adapts auxiliary classifiers for specific performance measures [34]; \u2022 Batch Uni-Log: A batch algorithm which optimizes the (weighted) univariate logistic loss [33]; \u2022 Batch Uni-Squ: A batch algorithm which optimizes the (weighted) univariate square loss; \u2022 AdaOAM: The proposed adaptive gradient method for online AUC maximization.", "startOffset": 695, "endOffset": 699}, {"referenceID": 32, "context": "\u2022 Online Uni-Exp: An online learning algorithm which optimizes the (weighted) univariate exponential loss [33]; \u2022 Online Uni-Log: An online learning algorithm which optimizes the (weighted) univariate logistic loss [33]; \u2022 OAMseq: The OAM algorithm with reservoir sampling and sequential updating method [6]; \u2022 OAMgra: The OAM algorithm with reservoir sampling and online gradient updating method [6]; \u2022 OPAUC: The one-pass AUC optimization algorithm with square loss function [7]; \u2022 SVM-perf: A batch algorithm which directly optimizes AUC [4]; \u2022 CAPO: A batch algorithm which trains nonlinear auxiliary classifiers first and then adapts auxiliary classifiers for specific performance measures [34]; \u2022 Batch Uni-Log: A batch algorithm which optimizes the (weighted) univariate logistic loss [33]; \u2022 Batch Uni-Squ: A batch algorithm which optimizes the (weighted) univariate square loss; \u2022 AdaOAM: The proposed adaptive gradient method for online AUC maximization.", "startOffset": 792, "endOffset": 796}, {"referenceID": 5, "context": "To examine the performance of the proposed AdaOAM in comparison to the existing state-of-the-art methods, we conduct extensive experiments on sixteen benchmark datasets by maintaining consistency to the previous studies on online AUC maximization [6], [7].", "startOffset": 247, "endOffset": 250}, {"referenceID": 6, "context": "To examine the performance of the proposed AdaOAM in comparison to the existing state-of-the-art methods, we conduct extensive experiments on sixteen benchmark datasets by maintaining consistency to the previous studies on online AUC maximization [6], [7].", "startOffset": 252, "endOffset": 255}, {"referenceID": 5, "context": "For OAMgra and OAMseq , the buffer size is fixed at 100 as suggested in [6].", "startOffset": 72, "endOffset": 75}, {"referenceID": 34, "context": "The reason is that \u201ca2a\u201d dataset being a highly sparse dataset is a clear advantage for the CAPO since it operates under the framework of Core Vector Machine method [35], which is a very fast batch algorithm for training SVM model involving sparse data.", "startOffset": 165, "endOffset": 169}, {"referenceID": 6, "context": "In [7], the authors claimed that OPAUC was insensitive to the parameter settings.", "startOffset": 3, "endOffset": 6}, {"referenceID": 35, "context": "The farm ads dataset is generated based on the text ads found on twelve websites that deal with various farm animal related topics [36] and downloaded from UCI Machine Learning Repository.", "startOffset": 131, "endOffset": 135}, {"referenceID": 36, "context": "To be precise, consider the following four domains: Webspam: We apply the AdaOAM to detect malicious web pages using the \u201cwebspam-u\u201d dataset with unigram format from the subset used in the Pascal Large Scale Learning Challenge [37]; Sensor Faults: We apply the AdaOAM to identify sensor faults in buildings with the \u201csmartBuilding\u201d dataset [38], where the sensors monitor the concentration of the contaminant of interest (such as CO2) in different zones in a building; Malware App: We apply the AdaOAM to detect mobile malware app with a \u201cmalware\u201d app permission dataset, which is built from the Android Malware Genome Project 10 [39].", "startOffset": 227, "endOffset": 231}, {"referenceID": 37, "context": "To be precise, consider the following four domains: Webspam: We apply the AdaOAM to detect malicious web pages using the \u201cwebspam-u\u201d dataset with unigram format from the subset used in the Pascal Large Scale Learning Challenge [37]; Sensor Faults: We apply the AdaOAM to identify sensor faults in buildings with the \u201csmartBuilding\u201d dataset [38], where the sensors monitor the concentration of the contaminant of interest (such as CO2) in different zones in a building; Malware App: We apply the AdaOAM to detect mobile malware app with a \u201cmalware\u201d app permission dataset, which is built from the Android Malware Genome Project 10 [39].", "startOffset": 340, "endOffset": 344}, {"referenceID": 38, "context": "To be precise, consider the following four domains: Webspam: We apply the AdaOAM to detect malicious web pages using the \u201cwebspam-u\u201d dataset with unigram format from the subset used in the Pascal Large Scale Learning Challenge [37]; Sensor Faults: We apply the AdaOAM to identify sensor faults in buildings with the \u201csmartBuilding\u201d dataset [38], where the sensors monitor the concentration of the contaminant of interest (such as CO2) in different zones in a building; Malware App: We apply the AdaOAM to detect mobile malware app with a \u201cmalware\u201d app permission dataset, which is built from the Android Malware Genome Project 10 [39].", "startOffset": 630, "endOffset": 634}, {"referenceID": 39, "context": "org/ preprocessed by [40] after data cleansing and duplication removal; Bioinformatics: We apply the AdaOAM to solve a bioinformatics problem with the \u201cprotein-h\u201d dataset from the prediction task of the KDD Cup 2004 [41].", "startOffset": 21, "endOffset": 25}], "year": 2016, "abstractText": "Learning for maximizing AUC performance is an important research problem in Machine Learning and Artificial Intelligence. Unlike traditional batch learning methods for maximizing AUC which often suffer from poor scalability, recent years have witnessed some emerging studies that attempt to maximize AUC by single-pass online learning approaches. Despite their encouraging results reported, the existing online AUC maximization algorithms often adopt simple online gradient descent approaches that fail to exploit the geometrical knowledge of the data observed during the online learning process, and thus could suffer from relatively larger regret. To address the above limitation, in this work, we explore a novel algorithm of Adaptive Online AUC Maximization (AdaOAM) which employs an adaptive gradient method that exploits the knowledge of historical gradients to perform more informative online learning. The new adaptive updating strategy of the AdaOAM is less sensitive to the parameter settings and maintains the same time complexity as previous non-adaptive counterparts. Additionally, we extend the algorithm to handle high-dimensional sparse data (SAdaOAM) and address sparsity in the solution by performing lazy gradient updating. We analyze the theoretical bounds and evaluate their empirical performance on various types of data sets. The encouraging empirical results obtained clearly highlighted the effectiveness and efficiency of the proposed algorithms.", "creator": "LaTeX with hyperref package"}}}