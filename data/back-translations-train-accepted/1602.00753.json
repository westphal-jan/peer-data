{"id": "1602.00753", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Feb-2016", "title": "Are Elephants Bigger than Butterflies? Reasoning about Sizes of Objects", "abstract": "Human vision greatly benefits from the information about sizes of objects. The role of size in several visual reasoning tasks has been thoroughly explored in human perception and cognition. However, the impact of the information about sizes of objects is yet to be determined in AI. We postulate that this is mainly attributed to the lack of a comprehensive repository of size information. In this paper, we introduce a method to automatically infer object sizes, leveraging visual and textual information from web. By maximizing the joint likelihood of textual and visual observations, our method learns reliable relative size estimates, with no explicit human supervision. We introduce the relative size dataset and show that our method outperforms competitive textual and visual baselines in reasoning about size comparisons.", "histories": [["v1", "Tue, 2 Feb 2016 00:16:39 GMT  (3363kb,D)", "http://arxiv.org/abs/1602.00753v1", "To appear in AAAI 2016"]], "COMMENTS": "To appear in AAAI 2016", "reviews": [], "SUBJECTS": "cs.AI cs.CV", "authors": ["hessam bagherinezhad", "hannaneh hajishirzi", "yejin choi", "ali farhadi"], "accepted": true, "id": "1602.00753"}, "pdf": {"name": "1602.00753.pdf", "metadata": {"source": "CRF", "title": "Are Elephants Bigger than Butterflies? Reasoning about Sizes of Objects", "authors": ["Hessam Bagherinezhad", "Hannaneh Hajishirzi", "Yejin Choi", "Ali Farhadi"], "emails": ["ali}@washington.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2 Related Work", "text": "A few researchers (Prager et al. 2003; Chu-carroll et al. 2003) use manually curated common sense knowledge bases such as OpenCyc (Lenat 1995) to answer questions about numerical information. These knowledge resources (e.g. ConceptNet (Havasi, Speer, and Alonso 2007) usually consist of taxonomic assertions or generic relationships, but do not include size information. Manual annotations of this knowledge are not scalable. Our efforts result in size information populating such knowledge bases (esp. ConceptNet) with size information at scale. Identify numerical attributes about objects recently addressed in NLP. The common theme in recent work (Aramaki et al. 2007; Davidov and Rappoport 2010; Iftene and Morus 2010; de Melo, and Weikum 2014; Narisawa et al.)."}, {"heading": "3 Overview of Our Method", "text": "Problem Overview: In this paper we deal with the problem of identifying the sizes of physical objects with visual and textual information. Our objectives are (a) to collect visual observation about the relative sizes of objects, (b) to collect textual observations about the absolute sizes of objects, and (c) to develop a method to make sense of huge amounts of visual and textual observations and estimate object sizes. We evaluate our method by answering questions about the size comparisons: if object A is larger than object B for all two objects A and B in our datasets. Algorithm 1 The overview of our method. 1: Presentation: Construct Size Graph (Section 4.1)."}, {"heading": "4 Representation: Size Graph", "text": "It is not scalable to capture visual observations for all pairs of objects. In addition, for some pairs such as \"airplane\" and \"apple\" it is noisy (if at all possible) to capture visual observations directly. We introduce size charts as compact, well-connected, sparse graph representation (Section 4.1), whose nodes are distributions of the actual size of objects (Section 4.2). The properties of the size chart allow us to collect enough visual and textual data to model size distributions."}, {"heading": "4.1 Graph Construction", "text": "It is about the question to what extent it is a question of a manner in which it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what it is about the question, to what extent it is about the question is about the question, to what is about the question, to what is about the question, to what is about the question to what is about the question, to what is about the question, to what extent it is about the question to what is about the question, to what is about the question to what is about the question, to what is about the question, to what is to what is about the question to what is about the question, to what is about the question to what is about the question to what is about the question, to what is to what extent it is about the question, to what is to what is the question, to what is to what extent it is about the question, to what is about the question, to what extent it is about the question to what is about the question, to what extent to what extent it is about the question, to what is about the question, to what is to what is to what is to what is to what is the question, to what is to what is the question, to what is to what is to what is the question, to what is to what is"}, {"heading": "4.2 Log-normal Sizes", "text": "In this paper, we argue that the sizes of the object instances are taken from a protocol standard distribution specific to the object type, i.e., the logarithm of the sizes is taken from a normal distribution, which differs from what was used in the previous work in NLP (Davidov and Rappoport 2010), where the sizes of the objects are taken from a normal distribution. Suppose the actual size of an apple is derived from a normal distribution with \u00b5 = 5 and \u03c3 = 1. The first problem is a non-zero Pdf for x \u2264 0, but physical objects cannot have negative sizes (loss of probability mass). The second problem is that the probability of finding an apple with a size of less than 0.1 (150 of an average apple) is greater than an apple with a size of more than 10 (twice the size of an average apple), which is intuitively wrong."}, {"heading": "5 Learning Object Sizes", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Collecting Observations", "text": "For each edge e = (Oi, Oj) in the size chart, we download several images from Flickr marked with both Oi and Oj and execute the corresponding object detectors, which are trained by a webly-monitored algorithm (Divvala, Farhadi and Guestrin 2014) to maintain scalability. Let box1 and box2 be the top predicted Bounding boxes for the first and second object, respectively. If the score of both predictions is above the default threshold of each detector, we capture r = area (box1) area (box2) \u00d7 depth (box2) 2 depth (box2) 2, as an observation for the relative size (Oi) size (Oj) size (Oj)."}, {"heading": "5.2 Learning", "text": "As discussed in Section 4.2, we assume that the log of object sizes comes from a normal distribution, i.e., gi = log size (Oi) \u0445 N (\u00b5i, \u03c32i). The aim of the learning step is to find parameters \u00b5i and \u03c3i for each object Oi that maximize the probability of observations (Oi). We define variables y (r) ij = log x (r) ij and y (r) i = logarithms of observations x (r) ij and x (r) i = logical textual observation for the size (Oi). We define variables y (r) ij = log x (r) ij and y (r) ij = logarithms of observations x (r) ij and x (r) ij = logarithms of observations i (r) ij and x (r) ij = logarithms of observations i (r) ij and x (r) i = logarithms of observations i, respectively. This implies that yij (y) and ygi (\u2212 gi)."}, {"heading": "5.3 Inference", "text": "After learning the parameters \u00b5i and \u03c3i for all objects in our test set, we can deduce from the probability distributions of object sizes whether the object Oi is larger than Oj. Any linear combination of normal distributions is also a normal distribution; hence: P (size (Oi) > size (Oj)) = P (log size (Oi) \u2212 log size (Oj) > 0) = P (gij > 0 | gij \u0445 N (\u00b5i \u2212 \u00b5j, \u03c32i + \u03c32j)) = 1 \u2212 \u03a6 (\u00b5j \u2212 \u00b5i \u221a \u03c32i + \u03c3 2 j) \u0445 (x) is the cumulative distribution function of the standard normal distribution and can be approximated numerically (Hart 1978; Marsaglia 2004)."}, {"heading": "6 Experiments", "text": "We use the Flickr 100M dataset (Thomee et al. 2015) as the source of tag lists required to create the size chart (Section 4.1). We model the size chart as a two-edged contiguous subgraph because it is still sparse, the total cost of edges is low and it is not disconnected from removing an edge. For each edge (Oi, Oj) in the size chart, we collect a maximum of 100 images from Flickr. We collect visual observations from the retrieved images and cut the outliers. To collect textual observations for the nodes, we run our sample sets on the Google Custom Search Engine (Section 5.1). The code, data and results can be found on the project page at http: / / grail.cs.washington.edu / projects / size."}, {"heading": "6.1 Dataset", "text": "It is difficult, if possible, to evaluate our model with absolute size object categories, as there is no single absolute size for a category (i.e. the size of the car varies from the smallest minicars to the largest SUVs), so we have compiled a dataset of size comparisons between different physical objects, the dataset includes annotations for a number of pairs of objects (Oi, Oj) for which people agree on this size (Oi) > size (Oj).The list of objects is selected from the 4869 detectors in LEVAN (Divvala, Farhadi and Guestrin 2014), which correspond to 41 physical objects. To comment on the size comparisons.To comment on the size comparisons.In comments, we used a web page and asked commenters to answer questions on the form \"Which is bigger, Oi or Oj?,\" and possible answers include three choices of Oi, Oj, Oj, or \"not obvious\" comparisons with other objects that are not obvious. \""}, {"heading": "6.2 Comparisons", "text": "Language-only baseline: We implement (Davidov and Rappoport 2010; Aramaki et al. 2007) by building and executing search engine queries with the size patterns mentioned in Section 5.1. For each query, we record a magnitude value after scaling the numerical results relative to their units; the size of each object is then modeled with a normal distribution across observations. 2Our model (textual only): This is a variant of our model that uses only textual observations; this model maximizes the second completion date of the log probability (Equation 2). Vision-only baseline: This is based on the use of the relative size comparisons taken directly from the visual data. For each edge in the full diagram, we collect visual observations and set its relative size as the geometric mean of all observations. To calculate the size between each pair of objects, we multiply all relative sizes of the object pairs."}, {"heading": "6.3 Results", "text": "Overall accuracy in size comparisons: We report on the accuracy of our model to derive size comparisons in our datasets in Figure 3a. To draw conclusions from this, we calculate P (size (A) > size (B)))) (Section 5.3) and conclude that A is greater than B if and only if P (size (A) > size (B) > size (B) > 0.5. Accuracy is the number of correctly derived pairs across all the pairs in the dataset.Our model achieves a significant improvement over all other models. The results confirm that visual and textual information are complementary and our model can use both modalities. In addition, our model (textually only) achieves significantly higher performance compared to the pure baseline. This supports the superiority of our representation, which is represented by log-normal distributions. Finally, our model achieves significantly higher accuracy (visually only) compared to the pure baseline."}, {"heading": "7 Conclusion", "text": "We evaluated our method by estimating the relative size of objects and show significant increases over competitive textual and visual baselines. We introduced size charts and demonstrated their advantages in using the transitive character of the size problem. Future work will include applying derived size information to image and diagram object recognition (Seo et al. 2014), estimating image depth in frames, and building a sound knowledge base. This paper is a step toward the important problem of deriving size information and can confidently declare that elephants are larger than butterflies! Recognition: This work was partially supported by ONR N00014-13-1-0720, NSF IIS-1218683, NSF IIS1338054, and Allen Distinguished Investigator Award."}], "references": [{"title": "Uth: Svm-based semantic relation classification using physical sizes", "author": ["Aramaki"], "venue": "In SemEval Workshop. ACL", "citeRegEx": "Aramaki,? \\Q2007\\E", "shortCiteRegEx": "Aramaki", "year": 2007}, {"title": "Neil: Extracting visual knowledge from web data", "author": ["Shrivastava Chen", "X. Gupta 2013] Chen", "A. Shrivastava", "A. Gupta"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Hybridization in question answering systems. In Directions in Question Answering", "author": ["Chu-carroll"], "venue": null, "citeRegEx": "Chu.carroll,? \\Q2003\\E", "shortCiteRegEx": "Chu.carroll", "year": 2003}, {"title": "and Rappoport", "author": ["D. Davidov"], "venue": "A.", "citeRegEx": "Davidov and Rappoport 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "A", "author": ["E. Delage", "H. Lee", "Ng"], "venue": "Y.", "citeRegEx": "Delage. Lee. and Ng 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "S", "author": ["Divvala"], "venue": "K.; Farhadi, A.; and Guestrin, C.", "citeRegEx": "Divvala. Farhadi. and Guestrin 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Depth map prediction from a single image using a multi-scale deep", "author": ["Puhrsch Eigen", "D. Fergus 2014] Eigen", "C. Puhrsch", "R. Fergus"], "venue": null, "citeRegEx": "Eigen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Eigen et al\\.", "year": 2014}, {"title": "D", "author": ["M.R. Garey", "Johnson"], "venue": "S.", "citeRegEx": "Garey and Johnson 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "J", "author": ["Hart"], "venue": "F.", "citeRegEx": "Hart 1978", "shortCiteRegEx": null, "year": 1978}, {"title": "Conceptnet: A lexical resource for common sense knowledge. Recent advances in natural language processing V: selected papers from RANLP 309:269", "author": ["Speer Havasi", "C. Alonso 2007] Havasi", "R. Speer", "J. Alonso"], "venue": null, "citeRegEx": "Havasi et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Havasi et al\\.", "year": 2007}, {"title": "Recovering the spatial layout of cluttered rooms", "author": ["Hoiem Hedau", "V. Forsyth 2009] Hedau", "D. Hoiem", "D. Forsyth"], "venue": null, "citeRegEx": "Hedau et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hedau et al\\.", "year": 2009}, {"title": "Learning knowledge graphs for question answering through conversational dialog", "author": ["Clark Hixon", "B. Hajishirzi 2015] Hixon", "P. Clark", "H. Hajishirzi"], "venue": null, "citeRegEx": "Hixon et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hixon et al\\.", "year": 2015}, {"title": "E", "author": ["A.H. Holway", "Boring"], "venue": "G.", "citeRegEx": "Holway and Boring 1941", "shortCiteRegEx": null, "year": 1941}, {"title": "and Moruz", "author": ["A. Iftene"], "venue": "M.-A.", "citeRegEx": "Iftene and Moruz 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "W", "author": ["Ittelson"], "venue": "H.", "citeRegEx": "Ittelson 1951", "shortCiteRegEx": null, "year": 1951}, {"title": "S", "author": ["H. Izadinia", "F. Sadeghi", "Divvala"], "venue": "K.; Hajishirzi, H.; Choi, Y.; and Farhadi, A.", "citeRegEx": "Izadinia et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Oliva", "author": ["T. Konkle"], "venue": "A.", "citeRegEx": "Konkle and Oliva 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "and Oliva", "author": ["T. Konkle"], "venue": "A.", "citeRegEx": "Konkle and Oliva 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Pulling things out of perspective", "author": ["Shi Ladicky", "L. Pollefeys 2014] Ladicky", "J. Shi", "M. Pollefeys"], "venue": null, "citeRegEx": "Ladicky et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ladicky et al\\.", "year": 2014}, {"title": "D", "author": ["Lenat"], "venue": "B.", "citeRegEx": "Lenat 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "Single image depth estimation from predicted semantic labels", "author": ["Gould Liu", "B. Koller 2010] Liu", "S. Gould", "D. Koller"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2010}, {"title": "On the use of size modifiers when referring to visible objects", "author": ["van Deemter Mitchell", "M. Reiter 2011] Mitchell", "K. van Deemter", "E. Reiter"], "venue": "In Annual Conference of the Cognitive Science Society", "citeRegEx": "Mitchell et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2011}, {"title": "Is a 204 cm man tall or small ? acquisition of numerical common sense from the web", "author": ["Narisawa"], "venue": null, "citeRegEx": "Narisawa,? \\Q2013\\E", "shortCiteRegEx": "Narisawa", "year": 2013}, {"title": "L", "author": ["Pero"], "venue": "D.; Bowdish, J.; Fried, D.; Kermgard, B.; Hartley, E.; and Barnard, K.", "citeRegEx": "Pero et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "C", "author": ["J.M. Prager", "J. Chu-Carroll", "K. Czuba", "Welty"], "venue": "A.; Ittycheriah, A.; and Mahindru, R.", "citeRegEx": "Prager et al. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "A", "author": ["A. Saxena", "S.H. Chung", "Ng"], "venue": "Y.", "citeRegEx": "Saxena. Chung. and Ng 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "M", "author": ["Seo"], "venue": "J.; Hajishirzi, H.; Farhadi, A.; and Etzioni, O.", "citeRegEx": "Seo et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Solving geometry problems: Combining text and diagram interpretation", "author": ["Seo"], "venue": null, "citeRegEx": "Seo,? \\Q2015\\E", "shortCiteRegEx": "Seo", "year": 2015}, {"title": "Acquiring comparative commonsense knowledge from the web", "author": ["de Melo Tandon", "N. Weikum 2014] Tandon", "G. de Melo", "G. Weikum"], "venue": null, "citeRegEx": "Tandon et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tandon et al\\.", "year": 2014}, {"title": "D", "author": ["Thomee, B.", "Shamma"], "venue": "A.; Friedland, G.; Elizalde, B.; Ni, K.; Poland, D.; Borth, D.; and Li, L.-J.", "citeRegEx": "Thomee et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Predicting failures of vision systems", "author": ["Zhang"], "venue": null, "citeRegEx": "Zhang,? \\Q2014\\E", "shortCiteRegEx": "Zhang", "year": 2014}], "referenceMentions": [], "year": 2016, "abstractText": "Human vision greatly benefits from the information about sizes of objects. The role of size in several visual reasoning tasks has been thoroughly explored in human perception and cognition. However, the impact of the information about sizes of objects is yet to be determined in AI. We postulate that this is mainly attributed to the lack of a comprehensive repository of size information. In this paper, we introduce a method to automatically infer object sizes, leveraging visual and textual information from web. By maximizing the joint likelihood of textual and visual observations, our method learns reliable relative size estimates, with no explicit human supervision. We introduce the relative size dataset and show that our method outperforms competitive textual and visual baselines in reasoning about size comparisons.", "creator": "LaTeX with hyperref package"}}}