{"id": "1602.01198", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Feb-2016", "title": "k-variates++: more pluses in the k-means++", "abstract": "k-means++ seeding has become a de facto standard for hard clustering algorithms. In this paper, our first contribution is a two-way generalisation of this seeding, k-variates++, that includes the sampling of general densities rather than just a discrete set of Dirac densities anchored at the point locations, and a generalisation of the well known Arthur-Vassilvitskii (AV) approximation guarantee, in the form of a bias+variance approximation bound of the global optimum. This approximation exhibits a reduced dependency on the \"noise\" component with respect to the optimal potential --- actually approaching the statistical lower bound. We show that k-variates++ reduces to efficient (biased seeding) clustering algorithms tailored to specific frameworks; these include distributed, streaming and on-line clustering, with direct approximation results for these algorithms. Finally, we present a novel application of k-variates++ to differential privacy. For either the specific frameworks considered here, or for the differential privacy setting, there is little to no prior results on the direct application of k-means++ and its approximation bounds --- state of the art contenders appear to be significantly more complex and / or display less favorable (approximation) properties. We stress that our algorithms can still be run in cases where there is \\textit{no} closed form solution for the population minimizer. We demonstrate the applicability of our analysis via experimental evaluation on several domains and settings, displaying competitive performances vs state of the art.", "histories": [["v1", "Wed, 3 Feb 2016 06:31:09 GMT  (3242kb,D)", "https://arxiv.org/abs/1602.01198v1", null], ["v2", "Sat, 13 Feb 2016 00:36:41 GMT  (3242kb,D)", "http://arxiv.org/abs/1602.01198v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["richard nock", "rapha\u00ebl canyasse", "roksana boreli", "frank nielsen"], "accepted": true, "id": "1602.01198"}, "pdf": {"name": "1602.01198.pdf", "metadata": {"source": "CRF", "title": "k-variates++: more pluses in the k-means++", "authors": ["Richard Nock", "Roksana Boreli"], "emails": ["richard.nock@nicta.com.au", "raphael.canyasse@polytechnique.edu", "roksana.boreli@nicta.com.au", "Frank.Nielsen@acm.org"], "sections": [{"heading": null, "text": "ar Xiv: 160 2.01 19"}, {"heading": "1 Introduction", "text": "Arthur-Vassilvitskii et al., 2009; Liberty Means + + algorithm, however, has been widely used to address the problem of hard cluster membership, due to its simplicity, experimental performance, and guaranteed approximation to the global optimum; the goal is to k-partition a dataset to minimize the sum of distances recorded within a cluster to the cluster center (Arthur & Vassilvitskii, 2007), i.e., a central theme or a population minimizer (Nock et al, 2016).The k Means + non-uniform seeding approach has also been used in more complex settings, including tensor clustering, distributed, data stream, and parallel clustering, with non-metric distortions, and even clusters with distortions that do not allow population distortions, in closed form (Ailon et al, 2009; Balcan et al.)."}, {"heading": "5 Experiments", "text": "The experiments we carry out are carried out extensively in the appendix (starting on page 44). > q means + vs-k means + + remarkable k means + + (Bahmani et al., 2012) To address algorithms that can be reduced by k variates + + (section 3), we have k means + + state-of-the-art tested k means + to be fair with Dk means + + +, we use k means + + seding as a recluding algorithm in k means (section 3). The parameters are in line with (Bahmani et al., 2012). To control the propagation of the Forgy nodes, we can use the initial data of each peer from points in a random hyperrectangle in a space of d = 50 (expected number of peers points mi = 500, we sample4\u0445) in (Nissim et al., 2007)."}, {"heading": "6 Discussion and Conclusion", "text": "We first show in this paper that the Nielsen mean + + analysis by Arthur and Vassilvitskii can indeed be carried out on a much more general scale by aggregating various cluster frameworks of interest for which no trivial adjustment of k-means + + was previously known. Our contributions are on two levels: (i) we provide the \"meta\" algorithm, k-varies + +, and two key results, one on the approximation capability of the global optimum and one on the probability ratio of the centers it delivers. We expect further applications of these results, in particular to address several other central cluster problems: stability, generalization and smoothed analysis (Arthur et al., 2011; von Luxburg, 2010); (ii) we provide two application examples. The first is a reduction technology of k-varies + +, which shows that the application techniques of k-varies + + + the application techniques of cluster types are obtained from the application techniques of k-varies and the application techniques of the other minimized ones."}, {"heading": "7 Acknowledgments", "text": "Thanks are due to Stephen Hardy, Guillaume Smith, Wilko Henecka and Max Ott for stimulating discussions and feedback on this topic. Nicta is funded by the Australian Government through the Department of Communications and the Australian Research Council through the ICT Center of Excellence Program."}, {"heading": "Appendix \u2014 Table of contents", "text": "Appendix to the Proofs p. 19 Proof of the Theorem 2 p. 19 Proof of the Theorem 3 p. 22 Notes to Table 1 p. 22 Proof of the Theorems 4, 5 and 6 p. 23 \u2192 Proof of the Theorem 4 p. 23 \u2192 Proof of the Theorem 5 p. 25 \u2192 Proof the Theorem 6 p. 26 Proof of the Theorem 9 p. 28 Proof the Theorem 10 p. 34 Proof the Theorem 12 p. 41 Extension to Nonmetric Spaces p. 42"}, {"heading": "Appendix on experiments Pg 44", "text": "Experiments on Theorem 12 and the sublinear noise regime p. 44 Experiments with Dk-Means + +, k-Means + + and k-Means \u2022 p. 47 Experiments with k-Variables + + and GUPT p. 51"}, {"heading": "8 Appendix on Proofs", "text": "Several proofs are based on properties of the k-means + + algorithm, which are not used in the proof (Arthur & Vassilvitskii, 2007). Here we presuppose the basic knowledge of the proof technique (Arthur & Vassilvitskii, 2007)."}, {"heading": "Proof of Theorem 2", "text": "It is well known that c (A) = arg mina (A) defines a cluster in the optimal cluster formation. We also define the noisy potential of A (A). We also define the noisy potential of A (A) as: \"A.\" = \"A\" (A): \"A\" (A): \"A\" (A): \"A\" (A): \"A\" (A): \"A\" (A): \"A\" (A): \"A\" (A): \"A\" (A): \"A\" (A): \"A\" (A): \"A\" (A): \"A\" (A): \"A\" (A): \"A\" (A): \"A\" (A). \"(A):\" A. \"(A):\" A. \"(A):\" A. \"(A):\" A. \"(A):\" A. \"(A):\" A. \"(A):\" A. \"(A):\" A. \"(A.\" (A): \"A.\" (A): \"A.\" (A): \"A.\" (A. \"(A):\" A. \"(A):\" (A): \"A.\" (A. \"):\" (A): \"A.\" (A. (A): \"A.\" (A): \"A. (A.\"): \"(A):\" A. (A. \"):\" (A. (A): \"):\" (A): \"A. (A. (A. (A):\" A. \"):\" A. (A. (A): \"):\" A. (A. (A. \"):\"): A. (A. (A): \"A. (A. (A):\"): \"A. (A. (A):\" A. (A). (A): A. (A). (A): \"). (A. (A. (A). (A):\"): \"): A. (A. (A.\"): A. (A. (A. \"): A. (A."}, {"heading": "Proof of Lemma 3", "text": "Consider the simple case k = 1 and a spherical Gaussian noise for p with a single point in A. Renormalize both sides of (7) by m = | A | so that (1 / m) \u2211 a \u00b2 A tr = tr (\u03a3). We see that the left side of ineq. (7) is only an estimator of the variance of pa, which must be at least the inversion of the Fisher information due to the Fre \u00b2 chet-Darmois-Crame \u00b2 r-Rao boundary, i.e. in this case the trace of the covariance matrix, i.e. tr (\u03a3)."}, {"heading": "Comments on Table 1", "text": "(Wang et al., 2015) deal with the approximation of clustering in subspace and therefore use a very different potential function, which lies between two subspaces S and S. To get an idea of the approximation of the problem of K-mean clustering that their technique produces, we calculate on the basis of the fact that U (or U) is an orthonormal basis for S (or S). To get an idea of the approximation of the k-mean clustering problem that results from their technique, we calculate on the basis of the fact that due to the triangular inequality and the fact that projections are linear and do not increase norms, an approximation of the k-mean clustering problem that is approaching (a)."}, {"heading": "Proofs of Theorems 4, 5 and 6", "text": "The proof for these theorems uses a reduction of k variables + + to the corresponding algorithms, which means that there are certain probe functions and densities for which the centers supplied by k variables + + are the same as those supplied by the corresponding algorithms. Definition 15 Let H (parameters omitted) be any k clustering algorithms for hard affiliation. We point out that k variations + + are reduced to H if there are data, densities, and probe functions that depend on the instance of H, so that in anticipation of the internal randomization of H, the centers supplied by H are the same as those supplied by k variables + +. We note that itk variations + + H. (40) Thus, if k variations + + H vary, Theorem 2 immediately gives a guarantee for the approximation of the global optimum in anticipation of H, but this requires the translation of the parameters that are involved in this problem (7) to solve only."}, {"heading": "Proof of Theorem 4", "text": "Figure 4 represents the architecture of the message defined in the Dk mean + + / PDk mean + + frame. We focus first on the protected scheme, Dk mean + +. We reduce k variables + + to algorithm1 by using identity probe functions. The trick in the reduction is based on the densities. We let p\u00b5a, \u03b8a be uniform over the subset Ai to which a belongs. Therefore, density support is discrete, and C is a subset of A; moreover, the probability qt (a) that an \"Ai\" is chosen in the iteration t in k variables + + is actually simplified to a convenient expression: qt (a) = q D ti \u00b7 ui, (41) where we remember that qDti. = {Dt (Ai) \u00b7 (n) \u00b7 c Dt (Aj) \u2212 1 if t > 1 (1 / n)."}, {"heading": "Proof of Theorem 5", "text": "The proof proceeds in the same way as for theorem 4. The probe function (the same for each iteration) is already defined in the statement of theorem 5 from the definition of synopses. The distributions p\u00b5a, \u03b8a are diracs anchored at the places of the probe (synopses). Therefore, the centers chosen in k variations + + are synopses, and it is not difficult to verify whether the probability of selecting a synopse sj at iteration t factors is in the same way as in the definition of qSt in eq. (9). We therefore obtain the equivalence between algorithm 2 and k variations + + as inevitable. Lemma 17 For data, densities and probes defined as before, k variations + + Sk means + +. The proof of the approximation property of Sk means + + + is then derived from the fact that an occurrence is hered as 0 (dirads and a probe)."}, {"heading": "Proof of Theorem 6", "text": "The reduction of k-varied points to OLK means proceeds in the same way as for Theorem 4. The reduction of k-varied points to OLK means depends on two things: firstly, the uniform choice of the first center in k-means + + can be replaced by selecting the center uniformly in each subset of data: nothing changes in the expected approximation properties of the algorithm (this comes from Lemma 3.4 in (Arthur & Vassilvitskii, 2007); secondly, a specific probe must be developed, which is outlined in Figure 6. Basically, all probe functions of a minibatch are the same: each point in the minibatch is projected for itself, while points outside the minibatch are projected onto its nearest center."}, {"heading": "Proof of Theorem 9", "text": "To simplify the notations in the proof, we leave pa (x) the value of the density p (a) = q (a) q (a) q (a) q (a) q (a) q (a) q (a) q (a) q (a) q (a) q (a) q (a) q (b) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x (c) x) x (c) x (c) x (c) x) x (c) x) x (c) x (c) x x) x (c) x (c) x) x (c) x (c) x (c) x) x (c) x (x) x) x (x) x) x (c) x (x) x) x (c) x) x) x (c) x) x) x (c) x) x (c) x) x) x (c) x) x) x (c) x) x) x (c) x) x (c) x) x (c) x) x) x (x) x) x (x) x) x) x (x) x) x (x) x) x) x (x) x) x) x (x) x) x (x) x) x) x (x) x) x (x) x)."}, {"heading": "Here, 0 \u2264 \u03b7 \u2264 3 is a constant.", "text": "The proof is directly derived from the following example. \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212 Si \u2212"}, {"heading": "Proof of Theorem 10", "text": "Suppose that Density D contains a minimum value for the probability of escaping from the neighborhoods of N, where the number of points in Group D depends on the number of points in Group D (0, R). Suppose that Density D has a minimum value for the number of points in Group D (0, R). Suppose that Density D contains a minimum value for the number of points in Group D (0, R). Suppose that Density D (0, R) has a minimum value for the number of points in Group D. Suppose that Density D (0, R) has a minimum value for the number of points in Group D. Suppose that Density D (0, R) is a minimum value for the number of points in Group D. Suppose that Density D (0, R) is a minimum value for the number of points in Group D."}, {"heading": "Proof of Theorem 12", "text": "If p (\u00b5a, \u03b8a) is a product of the Laplace distributions Lap (b) (b) the scale parameter of the distribution (Dwork & Roth, 2014), the condition in ineq. (13) becomes: p (\u00b5a, \"\u03b8a\") (x) p (\u00b5a, \u03b8a) (x) \u2264 exp (\u0432 a. \"1 b) = exp (\u221a a.\" 1) \u2264 exp (2. \"2.\" B1 \"(0.\" B2 \"(0.\" R) (the L2-Ball). \"x,\" (142) assuming A. \"B1\" (0. \"R). Now let us fix% (R). = exp (2.\" R \"). Since B1.\" B2 \"(0.\" R \") (the L2-Ball), we want (1.\" R) (the L2-Ball) that we (1. \"2) direct returns (2.\" D). \""}, {"heading": "Extension to non-metric spaces", "text": "Since its introduction, the k-means + + seeding technique has been successfully adapted to various distortion measures D (\u00b7 \u00b7) to handle non-Euclidean characteristics (Jegelka et al., 2009; Nock et al., 2008, 2016). Similarly, our advanced seeding technique can be adapted to these scenarios: this amounts to setting the distortion as a free parameter of the algorithm and replacing Dt (a) (eq. (1)) with Dt (a). = mina \"PD (a) (a). For example, noting that the crossed Euclidean distance is only an example of Bregman divergences (the well-known canonical divergences in the information geometry of dual flat spaces), k-varies + + can be extended to this family of dissimilarities (Nock et al., 2008). But more interesting examples now appear that build on limitations that must satisfy certain problems such as the satisfaction of practice."}, {"heading": "9 Appendix on Experiments", "text": "\"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \".\" \"\" We. \"\" \"\" We. \"\" \"\" We. \"\" \"\" We. \"\" \"We.\" \"\" \"We.\" \"\" \".\" \"We.\" \"\" \"\". \"\" We. \"\" \"\" \"We.\". \"\" \"We.\" \"\". \"\" We. \"\" \".\" We. \"\". \"\" We. \".\" \"\" We. \".\" \"\" We. \"\" \".\" We. \"\". \"We.\" \"\" We. \"\" \"\" We. \"\" \"\" We. \"\" \"We.\". \"We.\" \"\" We. \"\" \"\" We.. \"\" We.. \"\" We. \"\" \"We..\" \"\" We.. \"\""}], "references": [{"title": "Bregman divergences and triangle inequality", "author": ["S. Acharyya", "A. Banerjee", "D. Boley"], "venue": "In Proc. of the 13 SIAM International Conference on Data Mining,", "citeRegEx": "Acharyya et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Acharyya et al\\.", "year": 2013}, {"title": "Streamkm++: A clustering algorithms for data streams", "author": ["Ackermann", "M.-R", "C. Lammersen", "M. M\u00e4rtens", "C. Raupach", "C. Sohler", "K. Swierkot"], "venue": "ALENEX,", "citeRegEx": "Ackermann et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ackermann et al\\.", "year": 2010}, {"title": "Streaming k-means approximation", "author": ["N. Ailon", "R. Jaiswal", "C. Monteleoni"], "venue": "In NIPS*22,", "citeRegEx": "Ailon et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ailon et al\\.", "year": 2009}, {"title": "Scalable k-means++", "author": ["B. Bahmani", "B. Moseley", "A. Vattani", "R. Kumar", "S. Vassilvitskii"], "venue": "VLDB, pp", "citeRegEx": "Bahmani et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bahmani et al\\.", "year": 2012}, {"title": "Distributed k-means and k-median clustering on general communication topologies", "author": ["Balcan", "M.-F", "S. Ehrlich", "Y. Liang"], "venue": "In NIPS*26,", "citeRegEx": "Balcan et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 1995}, {"title": "Differentially private empirical risk", "author": ["K. Chaudhuri", "C. Monteleoni", "Sarwate", "A.-D"], "venue": "minimization. JMLR,", "citeRegEx": "Chaudhuri et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2011}, {"title": "Adaptive noisy clustering", "author": ["M. Chichignoud", "S. Lousteau"], "venue": "IEEE Trans. IT,", "citeRegEx": "Chichignoud and Lousteau,? \\Q2014\\E", "shortCiteRegEx": "Chichignoud and Lousteau", "year": 2014}, {"title": "The algorithmic foudations of differential privacy", "author": ["C. Dwork", "A. Roth"], "venue": "Found. & Trends in TCS,", "citeRegEx": "Dwork and Roth,? \\Q2014\\E", "shortCiteRegEx": "Dwork and Roth", "year": 2014}, {"title": "Calibrating noise to sensitivity in private data analysis", "author": ["C. Dwork", "F. McSherry", "K. Nissim", "A. Smith"], "venue": "TCC,", "citeRegEx": "Dwork et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2006}, {"title": "The noisy power method: a meta algorithm with applications", "author": ["M. Hardt", "E. Price"], "venue": "In NIPS*27,", "citeRegEx": "Hardt and Price,? \\Q2014\\E", "shortCiteRegEx": "Hardt and Price", "year": 2014}, {"title": "Composable core-sets for diversity and coverage maximization", "author": ["P. Indyk", "S. Mahabadi", "M. Mahdian", "Mirrokni", "V.-S"], "venue": "ACM PODS,", "citeRegEx": "Indyk et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Indyk et al\\.", "year": 2014}, {"title": "Efficient algorithms for online decision problems", "author": ["A. Kalai", "S. Vempala"], "venue": "J. Comp. Syst. Sc.,", "citeRegEx": "Kalai and Vempala,? \\Q2005\\E", "shortCiteRegEx": "Kalai and Vempala", "year": 2005}, {"title": "An algorithm for online k-means clustering", "author": ["E. Liberty", "R. Sriharsha", "M. Sviridenko"], "venue": "CoRR, abs/1412.5721,", "citeRegEx": "Liberty et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liberty et al\\.", "year": 2014}, {"title": "Shape retrieval using hierarchical total bregman soft clustering", "author": ["M. Liu", "Vemuri", "B.-C", "S. .i Amari", "F. Nielsen"], "venue": "IEEE Trans. PAMI,", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Privacy integrated queries: an extensible platform for privacy-preserving data analysis", "author": ["F. McSherry"], "venue": "Communications of the ACM,", "citeRegEx": "McSherry,? \\Q2010\\E", "shortCiteRegEx": "McSherry", "year": 2010}, {"title": "GUPT: privacy preserving data analysis made easy", "author": ["P. Mohan", "A. Thakurta", "E. Shi", "D. Song", "Culler", "D.-E"], "venue": "ACM SIGMOD,", "citeRegEx": "Mohan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mohan et al\\.", "year": 2012}, {"title": "Smooth sensitivity and sampling in private data analysis", "author": ["K. Nissim", "S. Raskhodnikova", "A. Smith"], "venue": "ACM STOC, pp", "citeRegEx": "Nissim et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Nissim et al\\.", "year": 2007}, {"title": "Mixed Bregman clustering with approximation guarantees", "author": ["R. Nock", "P. Luosto", "J. Kivinen"], "venue": "ECML, pp", "citeRegEx": "Nock et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Nock et al\\.", "year": 2008}, {"title": "On conformal divergences and their population minimizers", "author": ["R. Nock", "F. Nielsen", "Amari", "S.-I"], "venue": "IEEE Trans. IT,", "citeRegEx": "Nock et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nock et al\\.", "year": 2016}, {"title": "Fast and accurate k-means for large datasets", "author": ["M. Shindler", "A. Wong", "A. Meyerson"], "venue": "In NIPS*24,", "citeRegEx": "Shindler et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shindler et al\\.", "year": 2011}, {"title": "Clustering stability: an overview", "author": ["U. von Luxburg"], "venue": "Found. & Trends in ML,", "citeRegEx": "Luxburg,? \\Q2010\\E", "shortCiteRegEx": "Luxburg", "year": 2010}, {"title": "Differentially private subspace clustering", "author": ["Y. Wang", "Wang", "Y.-X", "A. Singh"], "venue": "In NIPS*28,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "2016). We give an example of the extension of k-variates++to the total", "author": ["Nock"], "venue": null, "citeRegEx": "2015 and Nock,? \\Q2016\\E", "shortCiteRegEx": "2015 and Nock", "year": 2016}], "referenceMentions": [{"referenceID": 18, "context": ", a centroid or a population minimizer (Nock et al., 2016).", "startOffset": 39, "endOffset": 58}, {"referenceID": 2, "context": "The k-means++ non-uniform seeding approach has also been utilized in more complex settings, including tensor clustering, distributed, data stream, on-line and parallel clustering, clustering with non-metric distortions and even clustering with distortions not allowing population minimizers in closed form (Ailon et al., 2009; Balcan et al., 2013; Jegelka et al., 2009; Liberty et al., 2014; Nock et al., 2008; Nielsen & Nock, 2015).", "startOffset": 306, "endOffset": 432}, {"referenceID": 12, "context": "The k-means++ non-uniform seeding approach has also been utilized in more complex settings, including tensor clustering, distributed, data stream, on-line and parallel clustering, clustering with non-metric distortions and even clustering with distortions not allowing population minimizers in closed form (Ailon et al., 2009; Balcan et al., 2013; Jegelka et al., 2009; Liberty et al., 2014; Nock et al., 2008; Nielsen & Nock, 2015).", "startOffset": 306, "endOffset": 432}, {"referenceID": 17, "context": "The k-means++ non-uniform seeding approach has also been utilized in more complex settings, including tensor clustering, distributed, data stream, on-line and parallel clustering, clustering with non-metric distortions and even clustering with distortions not allowing population minimizers in closed form (Ailon et al., 2009; Balcan et al., 2013; Jegelka et al., 2009; Liberty et al., 2014; Nock et al., 2008; Nielsen & Nock, 2015).", "startOffset": 306, "endOffset": 432}, {"referenceID": 16, "context": ", there is limited prior work in a differentially private setting (Nissim et al., 2007; Wang et al., 2015).", "startOffset": 66, "endOffset": 106}, {"referenceID": 21, "context": ", there is limited prior work in a differentially private setting (Nissim et al., 2007; Wang et al., 2015).", "startOffset": 66, "endOffset": 106}, {"referenceID": 16, "context": "We use it directly in a differential privacy setting, addressing a conjecture of (Nissim et al., 2007) with weaker assumptions.", "startOffset": 81, "endOffset": 102}, {"referenceID": 2, "context": "This simple reduction technique allows us to analyze lightweight algorithms that compare favorably to the state of the art in the related domains (Ailon et al., 2009; Balcan et al., 2013; Liberty et al., 2014), from the approximation, assumptions and / or complexity aspects.", "startOffset": 146, "endOffset": 209}, {"referenceID": 12, "context": "This simple reduction technique allows us to analyze lightweight algorithms that compare favorably to the state of the art in the related domains (Ailon et al., 2009; Balcan et al., 2013; Liberty et al., 2014), from the approximation, assumptions and / or complexity aspects.", "startOffset": 146, "endOffset": 209}, {"referenceID": 18, "context": "2 k-variates++ We consider the hard clustering problem (Banerjee et al., 2005; Nock et al., 2016): given set A \u2282 R and integer k > 0, find centers C \u2282 R which minimizes the L2 potential to the centers (here, c(a) .", "startOffset": 55, "endOffset": 97}, {"referenceID": 3, "context": "3 Reductions from k-variates++ Despite tremendous advantages, k-means++ has a serious downside: it is difficult to parallelize, distribute or stream it under relevant communication, space, privacy and/or time resource constraints (Bahmani et al., 2012).", "startOffset": 230, "endOffset": 252}, {"referenceID": 1, "context": "Although extending k-means clustering to these settings has been a major research area in recent years, there has been no obvious solution to tailoring kmeans++ (Ackermann et al., 2010; Ailon et al., 2009; Bahmani et al., 2012; Balcan et al., 2013; Liberty et al., 2014; Shindler et al., 2011) (and others).", "startOffset": 161, "endOffset": 293}, {"referenceID": 2, "context": "Although extending k-means clustering to these settings has been a major research area in recent years, there has been no obvious solution to tailoring kmeans++ (Ackermann et al., 2010; Ailon et al., 2009; Bahmani et al., 2012; Balcan et al., 2013; Liberty et al., 2014; Shindler et al., 2011) (and others).", "startOffset": 161, "endOffset": 293}, {"referenceID": 3, "context": "Although extending k-means clustering to these settings has been a major research area in recent years, there has been no obvious solution to tailoring kmeans++ (Ackermann et al., 2010; Ailon et al., 2009; Bahmani et al., 2012; Balcan et al., 2013; Liberty et al., 2014; Shindler et al., 2011) (and others).", "startOffset": 161, "endOffset": 293}, {"referenceID": 12, "context": "Although extending k-means clustering to these settings has been a major research area in recent years, there has been no obvious solution to tailoring kmeans++ (Ackermann et al., 2010; Ailon et al., 2009; Bahmani et al., 2012; Balcan et al., 2013; Liberty et al., 2014; Shindler et al., 2011) (and others).", "startOffset": 161, "endOffset": 293}, {"referenceID": 19, "context": "Although extending k-means clustering to these settings has been a major research area in recent years, there has been no obvious solution to tailoring kmeans++ (Ackermann et al., 2010; Ailon et al., 2009; Bahmani et al., 2012; Balcan et al., 2013; Liberty et al., 2014; Shindler et al., 2011) (and others).", "startOffset": 161, "endOffset": 293}, {"referenceID": 3, "context": "Property Them Us (1) (Bahmani et al., 2012) Communication complexity O(n` \u00b7 log \u03c61) (expected) O(nk) (2) (Bahmani et al.", "startOffset": 21, "endOffset": 43}, {"referenceID": 3, "context": ", 2012) Communication complexity O(n` \u00b7 log \u03c61) (expected) O(nk) (2) (Bahmani et al., 2012) # data to compute one center m \u2264 maxi\u2208[n](m/mi) (3) (Bahmani et al.", "startOffset": 69, "endOffset": 91}, {"referenceID": 3, "context": ", 2012) # data to compute one center m \u2264 maxi\u2208[n](m/mi) (3) (Bahmani et al., 2012) Data points shared O(` \u00b7 log \u03c61) (expected) k (4) (Bahmani et al.", "startOffset": 60, "endOffset": 82}, {"referenceID": 3, "context": ", 2012) Data points shared O(` \u00b7 log \u03c61) (expected) k (4) (Bahmani et al., 2012) Approximation bound O((log k) \u00b7 \u03c6opt) (2 + log k) \u00b7 ( 10\u03c6opt + 6\u03c6 F s )", "startOffset": 58, "endOffset": 80}, {"referenceID": 2, "context": "(i) (Ailon et al., 2009) Time complexity (outer loop) \u2014 identical \u2014 (ii) (Ailon et al.", "startOffset": 4, "endOffset": 24}, {"referenceID": 2, "context": ", 2009) Time complexity (outer loop) \u2014 identical \u2014 (ii) (Ailon et al., 2009) Approximation bound (2 + log k)(1 + \u03b7) \u00b7 32\u03c6opt (2 + log k) \u00b7 ((8 + 4\u03b7)\u03c6opt + 2\u03c6s ) (a) (Liberty et al.", "startOffset": 56, "endOffset": 76}, {"referenceID": 12, "context": ", 2009) Approximation bound (2 + log k)(1 + \u03b7) \u00b7 32\u03c6opt (2 + log k) \u00b7 ((8 + 4\u03b7)\u03c6opt + 2\u03c6s ) (a) (Liberty et al., 2014) Knowledge required Lowerbound \u03c6\u2217 \u2264 \u03c6opt None (b) (Liberty et al.", "startOffset": 96, "endOffset": 118}, {"referenceID": 12, "context": ", 2014) Knowledge required Lowerbound \u03c6\u2217 \u2264 \u03c6opt None (b) (Liberty et al., 2014) Approximation bound O(logm \u00b7 \u03c6opt) (2 + log k) \u00b7 ( 4 + (32/\u03c2) ) \u03c6opt (A) (Nissim et al.", "startOffset": 57, "endOffset": 79}, {"referenceID": 16, "context": ", 2014) Approximation bound O(logm \u00b7 \u03c6opt) (2 + log k) \u00b7 ( 4 + (32/\u03c2) ) \u03c6opt (A) (Nissim et al., 2007) Knowledge required \u03bb(\u03c6opt) None (B) (Nissim et al.", "startOffset": 81, "endOffset": 102}, {"referenceID": 16, "context": ", 2007) Knowledge required \u03bb(\u03c6opt) None (B) (Nissim et al., 2007) Noise variance (\u03c3) O(\u03bbkR/ ) O(R/( + logm)) (C) (Nissim et al.", "startOffset": 44, "endOffset": 65}, {"referenceID": 16, "context": ", 2007) Noise variance (\u03c3) O(\u03bbkR/ ) O(R/( + logm)) (C) (Nissim et al., 2007) Approximation bound O(\u03c6opt +m\u03bbkR/ ) O(log k(\u03c6opt +mR/( + logm))) (\u03b1) (Wang et al.", "startOffset": 55, "endOffset": 76}, {"referenceID": 21, "context": ", 2007) Approximation bound O(\u03c6opt +m\u03bbkR/ ) O(log k(\u03c6opt +mR/( + logm))) (\u03b1) (Wang et al., 2015) Assumptions on \u03c6opt Several (separability, size of clusters, etc.", "startOffset": 77, "endOffset": 96}, {"referenceID": 21, "context": ") None (\u03b2) (Wang et al., 2015) Approximation bound O(\u03c6opt + km log(m)R/ ) O(log k(\u03c6opt +mR/( + logm)))", "startOffset": 11, "endOffset": 30}, {"referenceID": 3, "context": "\u03c61 is the expected potential of a clustering with a single cluster over the whole data and ` is in general \u03a9(k) (Bahmani et al., 2012).", "startOffset": 112, "endOffset": 134}, {"referenceID": 2, "context": "\u03b7 is the approximation factor of the optimum in (Ailon et al., 2009).", "startOffset": 48, "endOffset": 68}, {"referenceID": 16, "context": "1 in (Nissim et al., 2007).", "startOffset": 5, "endOffset": 26}, {"referenceID": 3, "context": "Distributed clustering We consider horizontally partitioned data among peers, in line with (Bahmani et al., 2012), and a setting significantly more restrictive than theirs: each peer can only locally run the standard operations of Forgy initialisation (that is, uniform random seeding) on its own data, unlike for example the biased distributions of (Bahmani et al.", "startOffset": 91, "endOffset": 113}, {"referenceID": 3, "context": ", 2012), and a setting significantly more restrictive than theirs: each peer can only locally run the standard operations of Forgy initialisation (that is, uniform random seeding) on its own data, unlike for example the biased distributions of (Bahmani et al., 2012).", "startOffset": 244, "endOffset": 266}, {"referenceID": 2, "context": "We note that none of the algorithms of (Ailon et al., 2009; Balcan et al., 2013; Bahmani et al., 2012) would be applicable to this setting without non-trivial modifications affecting their properties.", "startOffset": 39, "endOffset": 102}, {"referenceID": 3, "context": "We note that none of the algorithms of (Ailon et al., 2009; Balcan et al., 2013; Bahmani et al., 2012) would be applicable to this setting without non-trivial modifications affecting their properties.", "startOffset": 39, "endOffset": 102}, {"referenceID": 2, "context": "We authorise the computation / output of the clustering at the end of the stream, but the memory n allowed for all operations satisfies n < m, such as n = m with \u03b1 < 1 in (Ailon et al., 2009).", "startOffset": 171, "endOffset": 191}, {"referenceID": 10, "context": "It relies on the standard \u201ctrick\u201d of summarizing massive datasets via compact representations (synopses) before processing them (Indyk et al., 2014).", "startOffset": 128, "endOffset": 148}, {"referenceID": 2, "context": "(Proof in page 25) It is not surprising to see that Sk-means++ looks like a generalization of (Ailon et al., 2009) and almost matches it (up to the number of centers delivered) when k\u2032 k synopses are learned from k\u2032-means#.", "startOffset": 94, "endOffset": 114}, {"referenceID": 2, "context": "Table 1 compares properties of Sk-means++ to (Ailon et al., 2009) (\u03b7 relates to approximation of the k-means objective in inner loop).", "startOffset": 45, "endOffset": 65}, {"referenceID": 12, "context": "Here, points arrive in a sequence, finite, but of unknown size and too large to fit in memory (Liberty et al., 2014).", "startOffset": 94, "endOffset": 116}, {"referenceID": 12, "context": "In (Liberty et al., 2014), the clustering algorithm is required to have space and time at most polylog in the length of the stream.", "startOffset": 3, "endOffset": 25}, {"referenceID": 12, "context": "Table 1 compares properties of OLk-means++ to (Liberty et al., 2014) (we picked the fully on-line, non-heuristic algorithm).", "startOffset": 46, "endOffset": 68}, {"referenceID": 5, "context": "Examples abound (Hardt & Price, 2014; Kalai & Vempala, 2005; Chaudhuri et al., 2011; Chichignoud & Lousteau, 2014), etc.", "startOffset": 16, "endOffset": 114}, {"referenceID": 16, "context": "Few approaches are related to clustering, yet noise injected is big \u2014 the existence of a smaller, sufficient noise, was conjectured in (Nissim et al., 2007) \u2014 and approaches rely on a variety of assumptions or knowledge about the optimum (See Table 1) (Nissim et al.", "startOffset": 135, "endOffset": 156}, {"referenceID": 16, "context": ", 2007) \u2014 and approaches rely on a variety of assumptions or knowledge about the optimum (See Table 1) (Nissim et al., 2007; Wang et al., 2015).", "startOffset": 103, "endOffset": 143}, {"referenceID": 21, "context": ", 2007) \u2014 and approaches rely on a variety of assumptions or knowledge about the optimum (See Table 1) (Nissim et al., 2007; Wang et al., 2015).", "startOffset": 103, "endOffset": 143}, {"referenceID": 8, "context": "Following is the definition of differential privacy (Dwork et al., 2006), tailored for conciseness to our clustering problem.", "startOffset": 52, "endOffset": 72}, {"referenceID": 8, "context": "We refer to (Dwork et al., 2006) for details, and assume from now on that data belong to a L1 ball B1(0, R).", "startOffset": 12, "endOffset": 32}, {"referenceID": 16, "context": "\u03a62) compare with each other, and how do they compare to the state of the art (Nissim et al., 2007; Wang et al., 2015) (we only consider methods with provable approximation bounds of the global optimum).", "startOffset": 77, "endOffset": 117}, {"referenceID": 21, "context": "\u03a62) compare with each other, and how do they compare to the state of the art (Nissim et al., 2007; Wang et al., 2015) (we only consider methods with provable approximation bounds of the global optimum).", "startOffset": 77, "endOffset": 117}, {"referenceID": 16, "context": "Table 1 compares k-variates++ to (Nissim et al., 2007; Wang et al., 2015) in this large sample regime, which is actually a prerequisite for (Nissim et al.", "startOffset": 33, "endOffset": 73}, {"referenceID": 21, "context": "Table 1 compares k-variates++ to (Nissim et al., 2007; Wang et al., 2015) in this large sample regime, which is actually a prerequisite for (Nissim et al.", "startOffset": 33, "endOffset": 73}, {"referenceID": 16, "context": ", 2015) in this large sample regime, which is actually a prerequisite for (Nissim et al., 2007; Wang et al., 2015).", "startOffset": 74, "endOffset": 114}, {"referenceID": 21, "context": ", 2015) in this large sample regime, which is actually a prerequisite for (Nissim et al., 2007; Wang et al., 2015).", "startOffset": 74, "endOffset": 114}, {"referenceID": 21, "context": "NotationO\u2217 removes all dependencies in their model parameters (assumptions, model parameters, and \u03b4 for the ( , \u03b4)-DP in (Wang et al., 2015)), and \u03bb is the separability assumption parameter (Nissim et al.", "startOffset": 121, "endOffset": 140}, {"referenceID": 16, "context": ", 2015)), and \u03bb is the separability assumption parameter (Nissim et al., 2007)4.", "startOffset": 57, "endOffset": 78}, {"referenceID": 16, "context": "The approximation bounds in (Nissim et al., 2007) consider Wasserstein distance between (estimated / optimal) centers, and not the potential involving data points like us.", "startOffset": 28, "endOffset": 49}, {"referenceID": 3, "context": "Dk-means++ vs k-means++ and k-means\u2016 (Bahmani et al., 2012) To address algorithms that can be reduced from k-variates++ (Section 3), we have tested Dk-means++ vs state of the art approach k-means\u2016; to be fair with Dk-means++, we use k-means++ seeding as the reclustering algorithm in k-means\u2016.", "startOffset": 37, "endOffset": 59}, {"referenceID": 3, "context": "Parameters are in line with (Bahmani et al., 2012).", "startOffset": 28, "endOffset": 50}, {"referenceID": 16, "context": "We sample 4\u03bb is named \u03c6 in (Nissim et al., 2007).", "startOffset": 27, "endOffset": 48}, {"referenceID": 15, "context": "k-variates++ vs Forgy-DP and GUPT To address algorithms that can be obtained via a direct use of k-variates++ (Section 4), we have tested it in a differential privacy framework vs state of the art approach GUPT (Mohan et al., 2012).", "startOffset": 211, "endOffset": 231}, {"referenceID": 2, "context": "The first is a reduction technique from k-variates++, which shows a way to obtain straight approximabilty results for other clustering algorithms, some being efficient proxies for the generalisation of existing approaches (Ailon et al., 2009).", "startOffset": 222, "endOffset": 242}, {"referenceID": 16, "context": "The second is a direct application of k-variates++ to differential privacy, exhibiting a noise component significantly better than existing approaches (Nissim et al., 2007; Wang et al., 2015).", "startOffset": 151, "endOffset": 191}, {"referenceID": 21, "context": "The second is a direct application of k-variates++ to differential privacy, exhibiting a noise component significantly better than existing approaches (Nissim et al., 2007; Wang et al., 2015).", "startOffset": 151, "endOffset": 191}, {"referenceID": 0, "context": "One example are Bregman divergences, that fail simple metric transforms (Acharyya et al., 2013).", "startOffset": 72, "endOffset": 95}, {"referenceID": 18, "context": "Another example are total divergences, that fail the simple computation of the population minimizers (Nock et al., 2016; Liu et al., 2012).", "startOffset": 101, "endOffset": 138}, {"referenceID": 13, "context": "Another example are total divergences, that fail the simple computation of the population minimizers (Nock et al., 2016; Liu et al., 2012).", "startOffset": 101, "endOffset": 138}, {"referenceID": 21, "context": "Comments on Table 1 (Wang et al., 2015) are concerned with approximating subspace clustering, and so they are using a very different potential function, which is, between two subspaces S and S\u2032, d(S, S\u2032) = \u2016UU> \u2212 UU\u2016F , where U (resp.", "startOffset": 20, "endOffset": 39}, {"referenceID": 8, "context": "The statements for \u03c32 and \u03a62 are direct applications of the Laplace mechanism properties (Dwork & Roth, 2014; Dwork et al., 2006).", "startOffset": 89, "endOffset": 129}, {"referenceID": 17, "context": "For example, by noticing that the squared Euclidean distance is merely an example of Bregman divergences (the well-known canonical divergences in information geometry of dually flat spaces), k-variates++ can be been extended to that family of dissimilarities (Nock et al., 2008).", "startOffset": 259, "endOffset": 278}, {"referenceID": 18, "context": "Such hard distortions include the skew Jeffreys \u03b1-centroids (Nock et al., 2016).", "startOffset": 60, "endOffset": 79}, {"referenceID": 18, "context": "This also include the recent class of total Bregman/Jensen divergences that are examples of conformal divergences (Nielsen & Nock, 2015; Nock et al., 2016).", "startOffset": 114, "endOffset": 155}, {"referenceID": 3, "context": "We have compared Dk-means++ to k-means++ and k-means\u2016 (Bahmani et al., 2012).", "startOffset": 54, "endOffset": 76}, {"referenceID": 3, "context": "We also pick ` = 2k, considering that it is a value which gives some of the best experimental results in (Bahmani et al., 2012).", "startOffset": 105, "endOffset": 127}, {"referenceID": 14, "context": "Experiments with k-variates++ and GUPT Among the state-of-the-art approaches against which we could compare k-variates++, there are two major contenders, PINQ (McSherry, 2010) and GUPT (Mohan et al.", "startOffset": 159, "endOffset": 175}, {"referenceID": 15, "context": "Experiments with k-variates++ and GUPT Among the state-of-the-art approaches against which we could compare k-variates++, there are two major contenders, PINQ (McSherry, 2010) and GUPT (Mohan et al., 2012).", "startOffset": 185, "endOffset": 205}, {"referenceID": 15, "context": "k-means implemented in the GUPT proceeds the following way: the dataset is cut in a certain number of blocks ` (following (Mohan et al., 2012), we fix ` = m in our experiments), the usual k-means algorithm is performed on each block.", "startOffset": 122, "endOffset": 142}, {"referenceID": 15, "context": "Note that we disregard the fact that our data are multidimensional, which should require a finer-grained tuning of `, and choose to rely on the ` = m suggestion from (Mohan et al., 2012).", "startOffset": 166, "endOffset": 186}], "year": 2016, "abstractText": "k-means++ seeding has become a de facto standard for hard clustering algorithms. In this paper, our first contribution is a two-way generalisation of this seeding, k-variates++, that includes the sampling of general densities rather than just a discrete set of Dirac densities anchored at the point locations, and a generalisation of the well known Arthur-Vassilvitskii (AV) approximation guarantee, in the form of a bias+variance approximation bound of the global optimum. This approximation exhibits a reduced dependency on the \u201dnoise\u201d component with respect to the optimal potential \u2014 actually approaching the statistical lower bound. We show that k-variates++ reduces to efficient (biased seeding) clustering algorithms tailored to specific frameworks; these include distributed, streaming and on-line clustering, with direct approximation results for these algorithms. Finally, we present a novel application of k-variates++ to differential privacy. For either the specific frameworks considered here, or for the differential privacy setting, there is little to no prior results on the direct application of k-means++ and its approximation bounds \u2014 state of the art contenders appear to be significantly more complex and / or display less favorable (approximation) properties. We stress that our algorithms can still be run in cases where there is no closed form solution for the population minimizer. We demonstrate the applicability of our analysis via experimental evaluation on several domains and settings, displaying competitive performances vs state of the art. 1 ar X iv :1 60 2. 01 19 8v 2 [ cs .L G ] 1 3 Fe b 20 16", "creator": "LaTeX with hyperref package"}}}