{"id": "1602.01407", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Feb-2016", "title": "A Kronecker-factored approximate Fisher matrix for convolution layers", "abstract": "Second-order optimization methods such as natural gradient descent have the potential to speed up training of neural networks by correcting for the curvature of the loss function. Unfortunately, the exact natural gradient is impractical to compute for large models, and most approximations either require an expensive iterative procedure or make crude approximations to the curvature. We present Kronecker Factors for Convolution (KFC), a tractable approximation to the Fisher matrix for convolutional networks based on a structured probabilistic model for the distribution over backpropagated derivatives. Similarly to the recently proposed Kronecker-Factored Approximate Curvature (K-FAC), each block of the approximate Fisher matrix decomposes as the Kronecker product of small matrices, allowing for efficient inversion. KFC captures important curvature information while still yielding comparably efficient updates to stochastic gradient descent (SGD). We show that the updates are invariant to commonly used reparameterizations, such as centering of the activations. In our experiments, approximate natural gradient descent with KFC was able to train convolutional networks several times faster than carefully tuned SGD. Furthermore, it was able to train the networks in 10-20 times fewer iterations than SGD, suggesting its potential applicability in a distributed setting.", "histories": [["v1", "Wed, 3 Feb 2016 18:45:07 GMT  (1802kb,D)", "http://arxiv.org/abs/1602.01407v1", null], ["v2", "Mon, 23 May 2016 22:44:56 GMT  (1802kb,D)", "http://arxiv.org/abs/1602.01407v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["roger b grosse", "james martens"], "accepted": true, "id": "1602.01407"}, "pdf": {"name": "1602.01407.pdf", "metadata": {"source": "CRF", "title": "A Kronecker-factored approximate Fisher matrix for convolution layers", "authors": ["Roger Grosse"], "emails": ["rgrosse@cs.toronto.edu", "jmartens@cs.toronto.edu"], "sections": [{"heading": "1 Introduction", "text": "Although most neural networks are still formed with variants of stochastic gradient systems that exhibit high yield, most neural networks are still formed with variants of stochastic gradient lineage (SGD), while their dynamics could greatly accelerate natural gradient lineage (Amari, 1998), because they take into account the geometry of the optimization landscape and exhibit desirable invariance characteristics. (See Martens (2014) for a review.) Unfortunately, the calculation of natural gradient descent is unavoidable for large networks, as it requires a large linear system whose dimension is the number of parameters (potentially tens of millions for modern architectures).Approximations on the natural gradients usually either impose very restrictive structures. (e.g. LeCun et al, 1998; Le Roux et al.) or require costly iterative procedures to perform any update, quasi-Newton-like methods (e.g. 2010)."}, {"heading": "2 Background", "text": "In this section we outline the K-FAC method, as previously formulated for standard fully connected forward meshes without weight distribution (Martens & Grosse, 2015), each layer of a fully connected network calculates activations as: s' = W'a \"\u2212 1 (1) a\" = \u03c6 \"(s\"), (2) where \"Rumi {1,.., L\" denotes the activation of the layer, a \"denotes the activations, W\" = (b'W \") denotes the matrix of distortions and weights, a\" (1 a > \") > the activation is appended with a homogeneous dimension, and \u03c6\" a nonlinear activation function (normally applied in coordination). (In this work we assume that the index 0 for all homogeneous coordinates x.) We refer to the values s \"as pre-activations."}, {"heading": "2.1 Second-order optimization of neural networks", "text": "This can also be seen as an approximate solution for a linear system, in which the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way,"}, {"heading": "2.2 Kronecker-factored approximate curvature", "text": "\"It is not that we would be able to see a complete description of the complete algorithm in Appendix A.2.The block diagonal version of K-FAC (which is the simpler of the two versions) is what we will present here) relies on two approaches to F-FAC, which together make it tractable to invert it. First, it is assumed that gradings in different layers are uncorrelated to what F-FAC is what F-FAC is."}, {"heading": "2.3 Convolutional networks", "text": "In our case, we are interested in calculating the correlations of derivatives that exacerbate the notation problems. In this section, we summarize the notation we use. (Table 1 lists all the Convolutionary Network Nodes used in this essay.) In sections that focus on a single level of the network, we specify the explicit level. (Here, T is the set of spatial locations that is typically a 2-D grid. For simplicity, we assume that the conversion is carried out with an increment of 1 and the addition equal to R, so that the set of spatial locations is divided between the input and output characteristic maps.) This layer is divided by a series of weights wi, j, j, and bi-i indexes."}, {"heading": "2.3.1 Efficient implementation and vectorized notation", "text": "It is important that our proposed algorithms can be efficient by using the same memory layouts (shuffling operations are extremely expensive). As a bonus, these vectorized operations provide a convenient high-level notation system that we will use around the world."}, {"heading": "3 Kronecker factorization for convolution layers", "text": "We begin by adopting a block diagonal approach to the Fisher matrix, like the K-FAC method we used in 2014. Each block contains all the relevant parameters for a layer (see Section 2.2). (Remember that these blocks are typically too large to be inverted or even explicitly represented, which is why the further Kronecker approach is required.) Kronecker factorization of K-FAC applies only to fully connected layers. However, Constitutional networks present several types of layers that are not found in fully connected lining nets: convolution, pooling, and reaction to normalization. Since pooling and reaction normalization layers have no traceable weights, they are not included in the Fisher Matrix layers. In this section, we present our main contribution, an approximate Kronecker factorization for the blocks of the F-FAC layers."}, {"heading": "3.1 Estimating the factors", "text": "Since the true covariance statistics are unknown, we estimate them empirically by starting from the distribution of the model, similar to Martens & Grosse (2015).To stamp derivatives from the distribution of the model, we select a mini-batch, stamp the results from the predictive distribution of the model and propagate the derivatives back. We have to estimate the Kronecker factors, the \"L-1\" = 0 and \"L\" = 1. Since these matrices are defined in relation to the auto-covariance functions, it seems natural to estimate these functions empirically."}, {"heading": "3.2 Using KFC in optimization", "text": "So far, we have defined an approach to the Fisher matrix F that can be reversed in any way in the context of optimization, most easily by using methods that are applied in the context of the full K-FAC algorithm, or as a prerequisite for second-order iterative methods (Martens, 2010; Vinyals & Povey, 2012; Sohl-Dickstein et al., 2014). In our experiments, we have examined two particular instances of KFC in optimization algorithms to provide the most direct comparison with standardized SGD-based optimizations."}, {"heading": "4 Theoretical analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Invariance", "text": "The natural gradient is partly motivated by its invariance to repair parameterization (which is considered a group), regardless of how the model is parameterized, the updates are synonymous with the first order. Approaches to the natural gradient do not suffice for full activation invariance, but certain approaches have proven invariant to be more limited but still fairly broad types of transformations. Ollivier (2015) showed that such an approach is invariable for (invariant) affine transformations of individual activations, a class of transformations that includes the substitution of sigmoial by concrete activation functions, as well as the centering transformations discussed in the next section. Martens & Grosse (2015) showed that K-FAC is invariant for a broader class of repair ametrizations: affine2Our SGD baseline used momentum and parameter averaging as well."}, {"heading": "4.2 Relationship with other algorithms", "text": "The most commonly used methods are algorithms that attempt to adjust learning rates for individual parameters based on the variance of gradients (LeCun et al., 1998; Duchi et al., 2011; Tieleman & Hinton, 2012; Zeiler, 2013; Kingma & Ba, 2015), which can be regarded as diagonal approximations to the Hessian or Fisher matrix, some of which use the empirical Fisher matrix, which differs from the proper Fisher matrix in that the targets are taken from the training data and not from the predictive distribution of the model. The empirical Fisher matrix is less closely related to curvature than the correct one. (Martens, 2014) Another class of approaches attempts to repair a network that its activations have zero mean and unit variance, with the objectives of avoiding variation and improving the conditioning of change."}, {"heading": "5 Experiments", "text": "We evaluated our method using two standardized image recognition benchmark datasets: CIFAR-10 (Krizhevsky, 2009) and Street View Housing Numbers (SVHN; Netzer et al., 2011). Our goal is not to achieve state-of-the-art performance, but to evaluate KFC's ability to optimize already published architectures. We first examine the likely assumptions and then present optimization results. For CIFAR-10, we used the architecture of cuda-convennet4, which scored an 18% error in 20 minutes. This network consists of three folding layers and a fully bonded layer. (While cuda-convennet offers some more powerful architectures, we could not use them because they include locally bonded layers that KFC cannot handle.) For SVHN, we used the architecture of Srivastava (2013), which consists of three convolutionary layers followed by three fully bonded layers, and failure layers were used for each of these two GDA architectures."}, {"heading": "5.1 Evaluating the probabilistic modeling assumptions", "text": "In the definition of such terms, we have linked three probable assumptions: the termination of such terms, which in most cases turn out to be insufficient, and the termination of such terms, which in most cases turn out to be unavoidable, as unavoidable, as unavoidable, as unavoidable, as unavoidable, as unavoidable, as unavoidable, as unavoidable, as unavoidable, as unavoidable, as unavoidable, as unavoidable, as unavoidable, as unavoidable, as unavoidable, as unavoidable, as unavoidable, as unavoidable, as unavoidable, as unavoidable, as unavoidable, as unavoidable, as unavoidable, as unavoidable, as, as, as, as, as, as, as, as, as, as, as, unavoidable, as, as, as, as,"}, {"heading": "5.2 Optimization performance", "text": "This year it is more than ever before."}, {"heading": "5.3 Potential for distributed implementation", "text": "Much work has recently been devoted to highly parallel or distributed implementations of neural network optimization (e.g. Dean et al. (2012). Synchronous SGD allows one to efficiently use very large mini-batches, which helps optimization by partially mitigating the variance in the stochastic gradient parameters as soon as it becomes available, but limiting the gradations with the silent parameters."}, {"heading": "Dean, J., Corrado, G. S., Monga, R., Chen, K., Devin, M., Le, Q. V., Mao, M. Z., Ranzato, M., Senior,", "text": "A., Tucker, P., Yang, K., and Ng, A. Y. Large scale distributed deep networks, 2012.9Both SGD and KFC-pre reached a little-minibatch experiments of the previous section. This is because large mini-batches lose the regulatory advantage of stochastic gradients. One would have to adjust the regulator to achieve good generalization performance in this environment. Demmel, J. W. Applied Numerical Linear Algebra. SIAM, 1997.Desjardins, G., Simonyan, K., Pascanu, R., and Kavukcuoglu, K. Natural neural networks. arXiv: 1507.00210, 2015.Duchi, J., Krizan, E., and Singer, Y."}, {"heading": "Le Roux, Nicolas, Manzagol, Pierre-antoine, and Bengio, Yoshua. Topmoumoute online natural gradient", "text": "Algorithm. In Advances in Neural Information Processing Systems 20, pp. 849-856. MIT Press, 2008."}, {"heading": "LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D.", "text": "In fact, in recent years, the number of people who are able to stay in Germany has drastically decreased, many times over, \"he said.\" We have it in our hands, \"he said,\" but we have it in our hands. \"He added,\" It's not that we are able to put the world in order. \"He added,\" It's not as if the world is in order. \"He added,\" It's not as if the world is in order, as if the world is in order, to put the world in order. \""}, {"heading": "Tieleman, T. and Hinton, G. Lecture 6.5, RMSProp. In Coursera course Neural Networks for Machine", "text": "Learning, 2012.Vatanen, Tommi, Raiko, Tapani, Valpola, Harri, and LeCun, Yann. Stochastic Gradient Towards Second Order Methods - Backpropagation Learning with transformations in nonlinearities. 2013.Vinyals, O. and Povey, D. Krylov subspace descent for deep learning. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2012.Yang, Z., Moczulski, M., Denil, M., de Freitas, N., Smola, A., Song, L., and Wang, Z. Deep fried Convets. arXiv: 1412.7149, 2014.Zeiler, Matthew D. ADADADELTA: An adaptive learning rate method. 2013."}, {"heading": "A Optimization methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 KFC as a preconditioner for SGD", "text": "The first optimization method we used in our experiments was a generic natural gradient descent approach, in which F-value (\u03b3) was used to approximate F. This method is like SGD with dynamics, except that the Euclidean gradient is replaced by B-value. It can also be considered a pre-conditioned SGD method, in which F-value (\u03b3) is used as a prerequisite. To distinguish this optimization method from the KFC approach itself, we call it the KFC value. Our method is perhaps closer to earlier Kronecker product-based natural gradient approaches (Heskes, 2000; Povey et al., 2015) than to the K-FAC approach itself. In addition, we have used a variant of the gradient section (Pascanu et al., 2013) to avoid instability."}, {"heading": "A.2 Kronecker-factored approximate curvature", "text": "The central idea of K-FAC is to combine approaches to the Fisher matrix, which is described in Section 2.2. (While one could potentially perform a natural gradient descent using the approximate natural gradient, possibly associated with a fixed learning rate and fixed square models, Martens & Grosse (2015) is the most effective way to apply this method within a robust 2nd order optimization framework based on adaptively attenuated square models, similar to the one used in HF (Martens, 2010). In this section, we will describe the K-FAC method in detail, while omitting certain aspects of the method that we do not use, such as the block diagonal inverse approximation."}, {"heading": "A.3 Efficient implementation", "text": "We are based on the Toronto Deep Learning ConvNet (TDLCN) package (Srivastava, 2015), which is a python wrapper around the CUDA cores. We therefore had to write a handful of additional cores: \u2022 a kernel for calculating TDLCN (Eqn. 32) \u2022 a kernel that provides automatic forward differentiation for the maximum pooling and response normalization layers. \u2022 Most other operations for KFC could be performed on the GPU using cores provided by TDLCN. (The only exception is the calculation of inverse (n.) \"The only exception is the calculation of inverse (\u03b3) layers.\" \u2212 1} L \u2212 1 \"= 0 and {1} L.\" \u2212 L \"which was performed on the CPU. (Forward mode is used only in update clipping; as mentioned above, this step can be partially eliminated in the practice of copying a KPU's sources)."}, {"heading": "B Proofs of theorems", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1 Proofs for Section 3", "text": "Lemma 1. Taking into account the approximation IAD, E [Dwi, j, \u03b4Dwi, j, \"j,\" g, \"g,\" g, \"g,\" g, \"g,\" g, \"g,\" g, \"g,\" g \"(45) E [Dwi, j, g,\" g, t, \"t,\" t, \"t\" (47) Proof. We prove the first equality; the rest is analog.E [Dwi, j, \"j,\" j, \"g,\" g, \"t\"] (46) E [DbiDbi \"] = | T | E [Dsi, tDsi,\" t \"(47) Proof. We prove the first equality; the rest is analog.E [Dwi, j,\" j \"Dwi,\" g, \"g,\" g, \"g,\" t, \"t\" (47) E [DbiDbi \"] = T, t,\" t, \"t,\" t, \"t,\" t, \"t,\" t, \"(47),\" t."}, {"heading": "Proof.", "text": "E [Dwi, j, j'aj, j'aj, i'aj, i'aj, i'aj, i'aj, i'aj, i'aj, i'aj, i'aj, i'aj, i'si, t'i, i'si, t'i, i'i, i'\u00ed, i'\u00ed, t'\u00ed, t'\u00ed, t'\u00ed, t'\u00ed, i'\u00ed, t'\u00ed, t'\u00ed, t'\u00ed, t'\u00ed, t'\u00ed, t'\u00ed, t'\u00ed, t'\u00ed, t'\u00ed, T'T, i'\u00ed, T, i'T, i'T, i'T, i't, i't'i, i't'i, i'i'i, i'i'i, i's, i'i'i't. \""}, {"heading": "B.2 Proofs for Section 4", "text": "In the discussion of invariances it will be convenient to add homogeneous coordinates to different matrices = > \"(A '] H, (1 A ') (75) [S '] H, (1 S') (76) [W\"] H, (1 b'W \") (77) We also define the activation function\" H \"to ignore the homogeneous column so that we can write the effect of affine transformations on the pre-activations and activations: [S'H) = \u03c6 (YES '\u2212 1K'H) H) H. (78) With the homogeneous coordinate notation we can determine the effect of affine transformations on the pre-activations and activations."}], "references": [{"title": "Natural gradient works efficiently in learning", "author": ["Amari", "Shun-Ichi"], "venue": "Neural Computation,", "citeRegEx": "Amari and Shun.Ichi.,? \\Q1998\\E", "shortCiteRegEx": "Amari and Shun.Ichi.", "year": 1998}, {"title": "High performance convolutional neural networks for document", "author": ["S. Puri", "P. Simard"], "venue": "Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "K. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "K. et al\\.", "year": 2011}, {"title": "Enhanced gradient for training restricted Boltzmann machines", "author": ["T. Raiko", "A. Ilin"], "venue": "Workshop on Frontiers in Handwriting Recognition,", "citeRegEx": "K. et al\\.,? \\Q2006\\E", "shortCiteRegEx": "K. et al\\.", "year": 2006}, {"title": "Applied Numerical Linear Algebra", "author": ["J.W. Demmel"], "venue": null, "citeRegEx": "Demmel,? \\Q1997\\E", "shortCiteRegEx": "Demmel", "year": 1997}, {"title": "Adaptive subgradient methods for online learning and stochastic", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": null, "citeRegEx": "Duchi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2015}, {"title": "Scaling up natural gradient by sparsely factorizing the inverse", "author": ["Roger", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Roger et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Roger et al\\.", "year": 2011}, {"title": "Long short-term memory", "author": ["J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "S. and Schmidhuber,? \\Q2000\\E", "shortCiteRegEx": "S. and Schmidhuber", "year": 2000}, {"title": "ImageNet classification with deep convolutional neural", "author": ["Toronto", "A. 2009. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": null, "citeRegEx": "Toronto et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Toronto et al\\.", "year": 2009}, {"title": "Topmoumoute online natural gradient", "author": ["Le Roux", "Nicolas", "Manzagol", "Pierre-antoine", "Bengio", "Yoshua"], "venue": "Neural Information Processing Systems,", "citeRegEx": "Roux et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Roux et al\\.", "year": 2012}, {"title": "Deep learning via Hessian-free optimization", "author": ["J. Martens"], "venue": "In Proceedings of the 27th International Conference", "citeRegEx": "Martens,? \\Q1998\\E", "shortCiteRegEx": "Martens", "year": 1998}, {"title": "Optimizing neural networks with Kronecker-factored approximate curvature", "author": ["R. Grosse"], "venue": "Machine Learning (ICML),", "citeRegEx": "J. and Grosse,? \\Q2010\\E", "shortCiteRegEx": "J. and Grosse", "year": 2010}, {"title": "Riemannian metrics for neural networks I: feedforward networks", "author": ["Y. Ollivier"], "venue": "Information and Inference,", "citeRegEx": "Ollivier,? \\Q2015\\E", "shortCiteRegEx": "Ollivier", "year": 2015}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "On the saddle point problem for non-convex optimization", "author": ["R. Pascanu", "Y.N. Dauphin", "S. Ganguli", "Y. Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2014}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["B.T. Polyak", "A.B. Juditsky"], "venue": "SIAM Journal of Control and Optimization,", "citeRegEx": "Polyak and Juditsky,? \\Q1992\\E", "shortCiteRegEx": "Polyak and Juditsky", "year": 1992}, {"title": "Parallel training of DNNs with natural gradient and parameter averaging", "author": ["Povey", "Daniel", "Zhang", "Xiaohui", "Khudanpur", "Sanjeev"], "venue": "In International Conference on Learning Representations: Workshop track,", "citeRegEx": "Povey et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Povey et al\\.", "year": 2015}, {"title": "Modeling pixel means and covariances using factorized third-order Boltzmann machines", "author": ["M. Ranzato", "G.E. Hinton"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Ranzato and Hinton,? \\Q2010\\E", "shortCiteRegEx": "Ranzato and Hinton", "year": 2010}, {"title": "Learning representations by back-propagating", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "errors. Nature,", "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Fast curvature matrix-vector products for second-order gradient descent", "author": ["Schraudolph", "Nicol N"], "venue": "Neural Computation,", "citeRegEx": "Schraudolph and N.,? \\Q2002\\E", "shortCiteRegEx": "Schraudolph and N.", "year": 2002}, {"title": "Natural image statistics and neural representation", "author": ["E.P. Simoncelli", "B.A. Olshausen"], "venue": "Annual Review of Neuroscience,", "citeRegEx": "Simoncelli and Olshausen,? \\Q2001\\E", "shortCiteRegEx": "Simoncelli and Olshausen", "year": 2001}, {"title": "Fast large-scale optimization by unifying stochastic gradient and quasi-Newton methods", "author": ["J. Sohl-Dickstein", "B. Poole", "S. Ganguli"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Sohl.Dickstein et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sohl.Dickstein et al\\.", "year": 2014}, {"title": "Improving neural networks with dropout", "author": ["N. Srivastava"], "venue": "Master\u2019s thesis, University of Toronto,", "citeRegEx": "Srivastava,? \\Q2013\\E", "shortCiteRegEx": "Srivastava", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V.V. Le"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A tutorial on stochastic approximation algorithms for training restricted Boltzmann machines and deep belief nets", "author": ["K. Swersky", "Chen", "Bo", "B. Marlin", "N. de Freitas"], "venue": "In Information Theory and Applications Workshop (ITA),", "citeRegEx": "Swersky et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Swersky et al\\.", "year": 2010}, {"title": "Lecture 6.5, RMSProp", "author": ["T. Tieleman", "G. Hinton"], "venue": "In Coursera course Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton", "year": 2012}, {"title": "Pushing stochastic gradient towards second-order methods \u2013 backpropagation learning with transformations in nonlinearities", "author": ["Vatanen", "Tommi", "Raiko", "Tapani", "Valpola", "Harri", "LeCun", "Yann"], "venue": null, "citeRegEx": "Vatanen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Vatanen et al\\.", "year": 2013}, {"title": "Krylov subspace descent for deep learning", "author": ["O. Vinyals", "D. Povey"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Vinyals and Povey,? \\Q2012\\E", "shortCiteRegEx": "Vinyals and Povey", "year": 2012}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["Zeiler", "Matthew D"], "venue": null, "citeRegEx": "Zeiler and D.,? \\Q2013\\E", "shortCiteRegEx": "Zeiler and D.", "year": 2013}, {"title": "particular, each time we adapt \u03b3, we compute \u2207\u0302h for three different values \u03b3\u2212 < \u03b3 < \u03b3+. We choose whichever of the three values results in the lowest value of M(\u03b8 + v)", "author": ["Martens", "Grosse"], "venue": null, "citeRegEx": "Martens and Grosse,? \\Q2015\\E", "shortCiteRegEx": "Martens and Grosse", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "(See Martens (2014) for a review.", "startOffset": 5, "endOffset": 20}, {"referenceID": 22, "context": ", 2012) and recurrent neural networks (Hochreiter & Schmidhuber, 1997; Sutskever et al., 2014).", "startOffset": 38, "endOffset": 94}, {"referenceID": 20, "context": "It could also potentially be used as a pre-conditioner for iterative second-order methods (Martens, 2010; Vinyals & Povey, 2012; Sohl-Dickstein et al., 2014).", "startOffset": 90, "endOffset": 157}, {"referenceID": 17, "context": ") For each case, \u2207\u03b8h is usually computed using automatic-differentiation aka backpropagation (Rumelhart et al., 1986; LeCun et al., 1998), which can be thought of as comprising two steps: first computing the pre-activation derivatives \u2207s`h for each layer, and then computing \u2207W`h = (\u2207s`h)\u0101`\u22121.", "startOffset": 93, "endOffset": 137}, {"referenceID": 8, "context": "Martens, 2010; Vinyals & Povey, 2012). F is an n \u00d7 n matrix, where n is the number of parameters and can be in the tens of millions for modern deep architectures. Therefore, it is impractical to represent F explicitly in memory, let alone solve the linear system exactly. There are two general strategies one can take to find a good search direction. First, one can impose a structure on F enabling tractable inversion; for instance LeCun et al. (1998) approximates it as a diagonal matrix, TONGA (Le Roux et al.", "startOffset": 0, "endOffset": 453}, {"referenceID": 8, "context": "(1998) approximates it as a diagonal matrix, TONGA (Le Roux et al., 2008) uses a more flexible low-rank-within-block-diagonal structure, and factorized natural gradient (Grosse & Salakhutdinov, 2015) imposes a directed Gaussian graphical model structure. Another strategy is to approximately minimize the quadratic approximation to the objective using an iterative procedure such as conjugate gradient; this is the approach taken in Hessianfree optimization (Martens, 2010), a type of truncated Newton method (e.g. Nocedal & Wright, 2006). Conjugate gradient (CG) is defined in terms of matrix-vector products Fv, which can be computed efficiently and exactly using the method outlined by Schraudolph (2002). While iterative approaches can produce high quality search directions, they can be very expensive in practice, as each update may require tens or even hundreds of CG iterations to reach an acceptable quality, and each of these iterations is comparable in cost to an SGD update.", "startOffset": 55, "endOffset": 708}, {"referenceID": 9, "context": "Martens & Grosse (2015) pointed out that F\u0302` + (\u03bb+ \u03b3)I \u2248 ( \u03a8`\u22121 + \u03c0` \u221a \u03bb+ \u03b3 I ) \u2297 ( \u0393` + 1 \u03c0` \u221a \u03bb+ \u03b3 I ) .", "startOffset": 0, "endOffset": 24}, {"referenceID": 9, "context": "Mathematically, \u03c0` can be any positive scalar, but Martens & Grosse (2015) suggest the formula", "startOffset": 51, "endOffset": 75}, {"referenceID": 9, "context": "The algorithm as presented by Martens & Grosse (2015) has many additional elements which are orthogonal to the contributions of this paper.", "startOffset": 30, "endOffset": 54}, {"referenceID": 21, "context": "ConvNet (TDLCN) package (Srivastava, 2015), whose convolution kernels we use in our experiments. Like many modern implementations, this implementation follows the approach of Chellapilla et al. (2006), which reduces the convolution operations to large matrix-vector products in order to exploit memory locality and efficient parallel BLAS operators.", "startOffset": 25, "endOffset": 201}, {"referenceID": 9, "context": "1 Estimating the factors Since the true covariance statistics are unknown, we estimate them empirically by sampling from the model\u2019s distribution, similarly to Martens & Grosse (2015). To sample derivatives from the model\u2019s distribution, we select a mini-batch, sample the outputs from the model\u2019s predictive distribution, and backpropagate the derivatives.", "startOffset": 160, "endOffset": 184}, {"referenceID": 20, "context": "Alternatively, we could use it in the context of the full K-FAC algorithm, or as a preconditioner for iterative second-order methods (Martens, 2010; Vinyals & Povey, 2012; Sohl-Dickstein et al., 2014).", "startOffset": 133, "endOffset": 200}, {"referenceID": 9, "context": "Alternatively, we could use it in the context of the full K-FAC algorithm, or as a preconditioner for iterative second-order methods (Martens, 2010; Vinyals & Povey, 2012; Sohl-Dickstein et al., 2014). In our experiments, we explored two particular instantiations of KFC in optimization algorithms. First, in order to provide as direct a comparison as possible to standard SGD-based optimization, we used \u2207\u0302h in the context of a generic approximate natural gradient descent procedure; this procedure is like SGD, except that \u2207\u0302h is substituted for the Euclidean gradient. Additionally, we used momentum, update clipping, and parameter averaging \u2014 all standard techniques in the context of stochastic optimization. One can also view this as a preconditioned SGD method, where F\u0302 is used as the preconditioner. Therefore, we refer to this method in our experiments as KFC-pre (to distinguish it from the KFC approximation itself). This method is spelled out in detail in Appendix A.1. We also explored the use of F\u0302 in the context of K-FAC, which (in addition to the techniques of Section 2.2), includes methods for adaptively changing the learning rate, momentum, and damping parameters over the course of optimization. The full algorithm is given in Appendix A.2. Our aim was to measure how KFC can perform in the context of a sophisticated and well-tuned second-order optimization procedure. We found that the adaptation methods tended to choose stable values for the learning rate, momentum, and damping parameters, suggesting that these could be replaced with fixed values (as in KFC-pre). Since both methods performed similarly, we report results only for KFC-pre. We note that this finding stands in contrast with the autoencoder experiments of Martens & Grosse (2015), where the adapted parameters varied considerably over the course of optimization.", "startOffset": 134, "endOffset": 1774}, {"referenceID": 10, "context": "Ollivier (2015) showed that one such approximation was invariant to (invertible) affine transformations of individual activations.", "startOffset": 0, "endOffset": 16}, {"referenceID": 9, "context": "Martens & Grosse (2015) showed that K-FAC is invariant to a broader class of reparameterizations: affine 2Our SGD baseline used momentum and parameter averaging as well.", "startOffset": 0, "endOffset": 24}, {"referenceID": 9, "context": "Invariance to affine transformations also implies approximate invariance to smooth nonlinear transformations; see Martens (2014) for further discussion.", "startOffset": 114, "endOffset": 129}, {"referenceID": 25, "context": "Another class of approaches attempts to reparameterize a network such that its activations have zero mean and unit variance, with the goals of preventing covariate shift and improving the conditioning of the curvature (Cho et al., 2013; Vatanen et al., 2013; Ioffe & Szegedy, 2015).", "startOffset": 218, "endOffset": 281}, {"referenceID": 21, "context": ") For SVHN, we used the architecture of Srivastava (2013). This architecture consists of three convolutional layers followed by three fully connected layers, and uses dropout for regularization.", "startOffset": 40, "endOffset": 58}, {"referenceID": 9, "context": "As discussed above, IAD is the same approximation made by standard K-FAC, and it was investigated in detail both theoretically and empirically by Martens & Grosse (2015). One implicitly 4https://code.", "startOffset": 146, "endOffset": 170}, {"referenceID": 4, "context": "As baselines, we also tried Adagrad (Duchi et al., 2011), RMSProp (Tieleman & Hinton, 2012), and Adam (Kingma & Ba, 2015), but none of these approaches outperformed carefully tuned SGD with momentum. This is consistent with the observations of Kingma & Ba (2015). Figure 3(a,b) shows the optimization performance on the CIFAR-10 dataset, in terms of wall clock time.", "startOffset": 37, "endOffset": 263}], "year": 2017, "abstractText": "Second-order optimization methods such as natural gradient descent have the potential to speed up training of neural networks by correcting for the curvature of the loss function. Unfortunately, the exact natural gradient is impractical to compute for large models, and most approximations either require an expensive iterative procedure or make crude approximations to the curvature. We present Kronecker Factors for Convolution (KFC), a tractable approximation to the Fisher matrix for convolutional networks based on a structured probabilistic model for the distribution over backpropagated derivatives. Similarly to the recently proposed Kronecker-Factored Approximate Curvature (K-FAC), each block of the approximate Fisher matrix decomposes as the Kronecker product of small matrices, allowing for efficient inversion. KFC captures important curvature information while still yielding comparably efficient updates to stochastic gradient descent (SGD). We show that the updates are invariant to commonly used reparameterizations, such as centering of the activations. In our experiments, approximate natural gradient descent with KFC was able to train convolutional networks several times faster than carefully tuned SGD. Furthermore, it was able to train the networks in 10-20 times fewer iterations than SGD, suggesting its potential applicability in a distributed setting.", "creator": "LaTeX with hyperref package"}}}