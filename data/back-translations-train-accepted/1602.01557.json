{"id": "1602.01557", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2016", "title": "An ensemble diversity approach to supervised binary hashing", "abstract": "Binary hashing is a well-known approach for fast approximate nearest-neighbor search in information retrieval. Much work has focused on affinity-based objective functions involving the hash functions or binary codes. These objective functions encode neighborhood information between data points and are often inspired by manifold learning algorithms. They ensure that the hash functions differ from each other through constraints or penalty terms that encourage codes to be orthogonal or dissimilar across bits, but this couples the binary variables and complicates the already difficult optimization. We propose a much simpler approach: we train each hash function (or bit) independently from each other, but introduce diversity among them using techniques from classifier ensembles. Surprisingly, we find that not only is this faster and trivially parallelizable, but it also improves over the more complex, coupled objective function, and achieves state-of-the-art precision and recall in experiments with image retrieval.", "histories": [["v1", "Thu, 4 Feb 2016 04:59:54 GMT  (615kb)", "http://arxiv.org/abs/1602.01557v1", "17 pages, 5 figures"]], "COMMENTS": "17 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CV math.OC stat.ML", "authors": ["miguel \u00e1 carreira-perpi\u00f1\u00e1n", "ramin raziperchikolaei"], "accepted": true, "id": "1602.01557"}, "pdf": {"name": "1602.01557.pdf", "metadata": {"source": "CRF", "title": "An Ensemble Diversity Approach to Supervised Binary Hashing", "authors": ["Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n", "Ramin Raziperchikolaei"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 160 2.01 557v 1 [cs.L G] 4F eb"}, {"heading": "1 Introduction and related work", "text": "The information contained in the retrieval measurement, such as the search for a query image or a document in a database, is essentially oriented toward neighborly search (Shakhnarovich et al., 2006). If the dimensionality of the query and the size of the database are large, an approximate search is necessary., We focus on binary hashing (Grauman and Fergus, 2013), where the query and the database are mapped to low-dimensional binary vectors, where the search is performed, at two speeds: computing distances (with hardware support) are much faster than computing distances between high-dimensional floating point vectors; and the entire database is much smaller, so that it is located in quick memory as a disk (for example, a database of 1 billion real vectors of the dimension 500 takes 2 TB in floating points, but 8 GB as 64-bit codes).Constructing functions, such as measurement eval and trieval, are made well."}, {"heading": "2 Ideas from learning classifier ensembles", "text": "At first glance, optimizing (3) without limitations does not seem like a good idea: because the classification of data extends across the B-bits, we independently obtain identical goals, one across each hash function, and so they all have the same global optimum. And if all hash functions are the same, they are equivalent to using only one of them, resulting in much less precision / recall. In fact, the same problem arises when training an ensemble of classifiers (Dietterich, 2000; Zhou, 2012; Kuncheva, 2014). Here, we have a training set of input vectors and output-class labels, and we want to train several classifiers whose outputs are then combined (usually by majority vote). If the classifiers are all the same, we gain nothing over a single classifier. Therefore, it is necessary to introduce diversity among classifiers so that they do not match in their predictions."}, {"heading": "3 Independent Laplacian Hashing (ILH) with diversity", "text": "The connection of the binary hashing function with the method of auxiliary coordinates; carpin-pin-ei-ei-n essentially. (\"base learner\"), binary hashing function (single-bit) objective function, optimization algorithm and diversity mechanism. (\"base learner\"), binary hashing function (single-bit) objective function, optimization algorithm and diversity mechanism. (\"single bit\") In this paper we focus on the following options. (We use linear and kernel SVMs as hash functions.) Without loss of generality (see later) we use the laplacian target (3), which we use for a single bit the formE (z) = 1ynm = 1ynm (zm \u2212 zm) 2 {zn = h (xn). (1) n = 1,., N. (4) To optimize it, where we first optimize the auxiliary function, then we can optimize the N by coordinating the hash function (it is also possible to fix the N)."}, {"heading": "4 Experiments", "text": "In fact, it is the case that most people are able to outdo themselves and that they are able to outdo themselves. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "5 Discussion", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "6 Conclusion", "text": "Surprisingly, we have shown that training hash functions independently of each other is not only easier, faster, and more parallel, but can also achieve better retrieval quality as long as diversity is introduced into the objective function of each hash function, which connects with ensemble learning and enables techniques to be borrowed from it. We have shown that it works well when each hash function optimizes a laptop target on a subset of data and facilitates the selection of bits to use. Although our evidence is largely empirical, the intuition behind it is solid and coincides with the many (also predominantly empirical) results that demonstrate the power of ensemble classics. The perspective of ensemble learning suggests many ideas for future work, such as circumcision of a large ensemble or the use of other diversity techniques. It may also be possible theoretically to characterize the accuracy of hash functions depending on the variety of hash functions."}, {"heading": "A Equivalence of objective functions in the single-bit case: proofs", "text": "In the main, we believe that the individual bit cases (b = 1), the individual functions (b = 1), the KSH and BRE loss functions can be written via the z vector of binary code for each data point, in the form of a binary quadratic function without a linear term (or a quadratic potentiality only): \"It is not possible for the two diagrams to be able to set a constant to E (z).\" We can assume that the two diagrams are able to be the diagrams of A (z) and B (z), the diagrams of A (z) and B (z), the diagrams of A (z) and B (z), the diagrams of A (z) and B (z)."}, {"heading": "B Orthogonality measure: proofs", "text": "This year we are going to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to put ourselves in a position, to be able to put ourselves in a position, to be able to put ourselves in a position, to be able to put ourselves in a position, to be able to put ourselves in a position to be able to put ourselves in a position to be able to put ourselves in a position. \""}, {"heading": "C Additional experiments", "text": "In Fig. 5, we also include the results for an additional unattended dataset, the Flickr 1 million dataset (Huiskes et al., 2010). For Flickr, we randomly select 2,000 images for testing and the rest for training. We use D = 150 MPEG-7 edge histogram characteristics. As no labels are available, we create pseudo labels ynm for xn, indicating as similar points its 100 true closest neighbors (using Euclidean distance) and as unequal points a random subset of 100 points among the remaining points. As a ground truth, we use the K = 10 000 closest neighbors of the query in Euclidean. All hash functions are trained on the basis of 5 000 points. Accessed: k nearest neighbors of the query point in the hamming room, for a random subset of 100 points below the remaining points. As a reason, we use the K = 10 000 closest neighbors of the query in the Euclidean area."}, {"heading": "Acknowledgments", "text": "Work supported by the NSF Prize IIS-1423515."}], "references": [{"title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "author": ["A. Andoni", "P. Indyk"], "venue": "Comm. ACM,", "citeRegEx": "Andoni and Indyk.,? \\Q2008\\E", "shortCiteRegEx": "Andoni and Indyk.", "year": 2008}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural Computation,", "citeRegEx": "Belkin and Niyogi.,? \\Q2003\\E", "shortCiteRegEx": "Belkin and Niyogi.", "year": 2003}, {"title": "Pseudo-boolean optimization", "author": ["E. Boros", "P.L. Hammer"], "venue": "Discrete Applied Math.,", "citeRegEx": "Boros and Hammer.,? \\Q2002\\E", "shortCiteRegEx": "Boros and Hammer.", "year": 2002}, {"title": "Fast approximate energy minimization via graph cuts", "author": ["Y. Boykov", "O. Veksler", "R. Zabih"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Boykov et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Boykov et al\\.", "year": 2001}, {"title": "The elastic embedding algorithm for dimensionality reduction", "author": ["M.\u00c1. Carreira-Perpi\u00f1\u00e1n"], "venue": "Proc. of the 27th Int. Conf. Machine Learning (ICML", "citeRegEx": "Carreira.Perpi\u00f1\u00e1n.,? \\Q2010\\E", "shortCiteRegEx": "Carreira.Perpi\u00f1\u00e1n.", "year": 2010}, {"title": "Hashing with binary autoencoders", "author": ["M.\u00c1. Carreira-Perpi\u00f1\u00e1n", "R. Raziperchikolaei"], "venue": "In Proc. of the 2015 IEEE Computer Society Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Carreira.Perpi\u00f1\u00e1n and Raziperchikolaei.,? \\Q2015\\E", "shortCiteRegEx": "Carreira.Perpi\u00f1\u00e1n and Raziperchikolaei.", "year": 2015}, {"title": "Distributed optimization of deeply nested systems", "author": ["M.\u00c1. Carreira-Perpi\u00f1\u00e1n", "W. Wang"], "venue": "Proc. of the 17th Int. Conf. Artificial Intelligence and Statistics (AISTATS", "citeRegEx": "Carreira.Perpi\u00f1\u00e1n and Wang.,? \\Q2014\\E", "shortCiteRegEx": "Carreira.Perpi\u00f1\u00e1n and Wang.", "year": 2014}, {"title": "Introduction to Algorithms", "author": ["T.H. Cormen", "C.E. Leiserson", "R.L. Rivest", "C. Stein"], "venue": null, "citeRegEx": "Cormen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cormen et al\\.", "year": 2009}, {"title": "Ensemble methods in machine learning", "author": ["T.G. Dietterich"], "venue": "In Multiple Classifier Systems,", "citeRegEx": "Dietterich.,? \\Q2000\\E", "shortCiteRegEx": "Dietterich.", "year": 2000}, {"title": "Solving multi-class learning problems via error-correcting output codes", "author": ["T.G. Dietterich", "G. Bakiri"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Dietterich and Bakiri.,? \\Q1995\\E", "shortCiteRegEx": "Dietterich and Bakiri.", "year": 1995}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin"], "venue": "J. Machine Learning Research,", "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Computers and Intractability: A Guide to the Theory of NP-Completeness", "author": ["M.R. Garey", "D.S. Johnson"], "venue": "W.H. Freeman,", "citeRegEx": "Garey and Johnson.,? \\Q1979\\E", "shortCiteRegEx": "Garey and Johnson.", "year": 1979}, {"title": "Graph cuts for supervised binary coding", "author": ["T. Ge", "K. He", "J. Sun"], "venue": "In Proc. 13th European Conf. Computer Vision (ECCV\u201914),", "citeRegEx": "Ge et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ge et al\\.", "year": 2014}, {"title": "Neural networks and the bias/variance dilemma", "author": ["S. Geman", "E. Bienenstock", "R. Doursat"], "venue": "Neural Computation,", "citeRegEx": "Geman et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Geman et al\\.", "year": 1992}, {"title": "Iterative quantization: A Procrustean approach to learning binary codes for large-scale image retrieval", "author": ["Y. Gong", "S. Lazebnik", "A. Gordo", "F. Perronnin"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Gong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2013}, {"title": "Learning binary hash codes for large-scale image search", "author": ["K. Grauman", "R. Fergus"], "venue": "Machine Learning for Computer Vision,", "citeRegEx": "Grauman and Fergus.,? \\Q2013\\E", "shortCiteRegEx": "Grauman and Fergus.", "year": 2013}, {"title": "Exact maximum a posteriori estimation for binary images", "author": ["D.M. Greig", "B.T. Porteous", "A.H. Seheult"], "venue": "Journal of the Royal Statistical Society, B,", "citeRegEx": "Greig et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Greig et al\\.", "year": 1989}, {"title": "New trends and ideas in visual concept detection: The MIR Flickr Retrieval Evaluation Initiative", "author": ["M.J. Huiskes", "B. Thomee", "M.S. Lew"], "venue": "In Proc. ACM Int. Conf. Multimedia Information Retrieval,", "citeRegEx": "Huiskes et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Huiskes et al\\.", "year": 2010}, {"title": "The error coding method and PICTs", "author": ["G. James", "T. Hastie"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "James and Hastie.,? \\Q1998\\E", "shortCiteRegEx": "James and Hastie.", "year": 1998}, {"title": "What energy functions can be minimized via graph cuts", "author": ["V. Kolmogorov", "R. Zabih"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Kolmogorov and Zabih.,? \\Q2003\\E", "shortCiteRegEx": "Kolmogorov and Zabih.", "year": 2003}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Master\u2019s thesis, Dept. of Computer Science, University of Toronto, Apr", "citeRegEx": "Krizhevsky.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky.", "year": 2009}, {"title": "Neural network ensembles, cross validation, and active learning", "author": ["A. Krogh", "J. Vedelsby"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Krogh and Vedelsby.,? \\Q1995\\E", "shortCiteRegEx": "Krogh and Vedelsby.", "year": 1995}, {"title": "Learning to hash with binary reconstructive embeddings", "author": ["B. Kulis", "T. Darrell"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Kulis and Darrell.,? \\Q2009\\E", "shortCiteRegEx": "Kulis and Darrell.", "year": 2009}, {"title": "Combining Pattern Classifiers: Methods and Algorithms", "author": ["L.I. Kuncheva"], "venue": null, "citeRegEx": "Kuncheva.,? \\Q2014\\E", "shortCiteRegEx": "Kuncheva.", "year": 2014}, {"title": "Learning binary codes with bagging PCA", "author": ["C. Leng", "J. Cheng", "T. Yuan", "X. Bai", "H. Lu"], "venue": "Proc. of the 25th European Conf. Machine Learning", "citeRegEx": "Leng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Leng et al\\.", "year": 2014}, {"title": "Geodesic distance function learning via heat flows on vector fields", "author": ["B. Lin", "J. Yang", "X. He", "J. Ye"], "venue": "Proc. of the 31st Int. Conf. Machine Learning (ICML", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "A general two-step approach to learning-based hashing", "author": ["G. Lin", "C. Shen", "D. Suter", "A. van den Hengel"], "venue": "In Proc. 14th Int. Conf. Computer Vision (ICCV\u201913),", "citeRegEx": "Lin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2013}, {"title": "Fast supervised hashing with decision trees for high-dimensional data", "author": ["G. Lin", "C. Shen", "Q. Shi", "A. van den Hengel", "D. Suter"], "venue": "In Proc. of the 2014 IEEE Computer Society Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Hashing with graphs", "author": ["W. Liu", "J. Wang", "S. Kumar", "S.-F. Chang"], "venue": "Proc. of the 28th Int. Conf. Machine Learning (ICML", "citeRegEx": "Liu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2011}, {"title": "Supervised hashing with kernels", "author": ["W. Liu", "J. Wang", "R. Ji", "Y.-G. Jiang", "S.-F. Chang"], "venue": "In Proc. of the 2012 IEEE Computer Society Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Training invariant support vector machines using selective sampling", "author": ["G. Loosli", "S. Canu", "L. Bottou"], "venue": "Large Scale Kernel Machines, Neural Information Processing Series,", "citeRegEx": "Loosli et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Loosli et al\\.", "year": 2007}, {"title": "Minimal loss hashing for compact binary codes", "author": ["M. Norouzi", "D. Fleet"], "venue": "Proc. of the 28th Int. Conf. Machine Learning (ICML", "citeRegEx": "Norouzi and Fleet.,? \\Q2011\\E", "shortCiteRegEx": "Norouzi and Fleet.", "year": 2011}, {"title": "Modeling the shape of the scene: A holistic representation of the spatial envelope", "author": ["A. Oliva", "A. Torralba"], "venue": "Int. J. Computer Vision,", "citeRegEx": "Oliva and Torralba.,? \\Q2001\\E", "shortCiteRegEx": "Oliva and Torralba.", "year": 2001}, {"title": "Computationally bounded retrieval", "author": ["M. Rastegari", "C. Keskin", "P. Kohli", "S. Izadi"], "venue": "In Proc. of the 2015 IEEE Computer Society Conf. Computer Vision and Pattern Recognition (CVPR\u201915),", "citeRegEx": "Rastegari et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rastegari et al\\.", "year": 2015}, {"title": "Learning hashing with affinity-based loss functions using auxiliary coordinates", "author": ["R. Raziperchikolaei", "M.\u00c1. Carreira-Perpi\u00f1\u00e1n"], "venue": "[cs.LG], Jan", "citeRegEx": "Raziperchikolaei and Carreira.Perpi\u00f1\u00e1n.,? \\Q2015\\E", "shortCiteRegEx": "Raziperchikolaei and Carreira.Perpi\u00f1\u00e1n.", "year": 2015}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science,", "citeRegEx": "Roweis and Saul.,? \\Q2000\\E", "shortCiteRegEx": "Roweis and Saul.", "year": 2000}, {"title": "Nearest-Neighbor Methods in Learning and Vision. Neural Information Processing Series", "author": ["G. Shakhnarovich", "P. Indyk", "T. Darrell", "editors"], "venue": null, "citeRegEx": "Shakhnarovich et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Shakhnarovich et al\\.", "year": 2006}, {"title": "Visualizing data using t-SNE", "author": ["L.J.P. van der Maaten", "G.E. Hinton"], "venue": "J. Machine Learning Research,", "citeRegEx": "Maaten and Hinton.,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Semi-supervised hashing for large scale search", "author": ["J. Wang", "S. Kumar", "S.-F. Chang"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Spectral hashing", "author": ["Y. Weiss", "A. Torralba", "R. Fergus"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Weiss et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2009}, {"title": "Sparse projections for high-dimensional binary codes", "author": ["Y. Xia", "K. He", "P. Kohli", "J. Sun"], "venue": "In Proc. of the 2015 IEEE Computer Society Conf. Computer Vision and Pattern Recognition (CVPR\u201915),", "citeRegEx": "Xia et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xia et al\\.", "year": 2015}, {"title": "Multiclass spectral clustering", "author": ["S.X. Yu", "J. Shi"], "venue": "In Proc. 9th Int. Conf. Computer Vision (ICCV\u201903),", "citeRegEx": "Yu and Shi.,? \\Q2003\\E", "shortCiteRegEx": "Yu and Shi.", "year": 2003}, {"title": "Self-taught hashing for fast similarity search", "author": ["D. Zhang", "J. Wang", "D. Cai", "J. Lu"], "venue": "In Proc. of the 33rd ACM Conf. Research and Development in Information Retrieval (SIGIR", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}, {"title": "Ensemble Methods: Foundations and Algorithms. Chapman & Hall/CRC Machine Learning and Pattern Recognition Series", "author": ["Z.-H. Zhou"], "venue": "CRC Publishers,", "citeRegEx": "Zhou.,? \\Q2012\\E", "shortCiteRegEx": "Zhou.", "year": 2012}], "referenceMentions": [{"referenceID": 36, "context": "Information retrieval tasks such as searching for a query image or document in a database are essentially a nearest-neighbor search (Shakhnarovich et al., 2006).", "startOffset": 132, "endOffset": 160}, {"referenceID": 15, "context": "We focus on binary hashing (Grauman and Fergus, 2013), where the query and database are mapped onto low-dimensional binary vectors, where the search is performed.", "startOffset": 27, "endOffset": 53}, {"referenceID": 29, "context": "Examples of these objective functions are Supervised Hashing with Kernels (KSH) (Liu et al., 2012), Binary Reconstructive Embeddings", "startOffset": 80, "endOffset": 98}, {"referenceID": 22, "context": "(BRE) (Kulis and Darrell, 2009) and the binary Laplacian loss (an extension of the Laplacian Eigenmaps objective; Belkin and Niyogi, 2003):", "startOffset": 6, "endOffset": 31}, {"referenceID": 1, "context": "(BRE) (Kulis and Darrell, 2009) and the binary Laplacian loss (an extension of the Laplacian Eigenmaps objective; Belkin and Niyogi, 2003):", "startOffset": 62, "endOffset": 138}, {"referenceID": 35, "context": "Other examples of these objective functions include models developed for dimension reduction, be they spectral such as Locally Linear Embedding (Roweis and Saul, 2000) or Anchor Graphs (Liu et al.", "startOffset": 144, "endOffset": 167}, {"referenceID": 28, "context": "Other examples of these objective functions include models developed for dimension reduction, be they spectral such as Locally Linear Embedding (Roweis and Saul, 2000) or Anchor Graphs (Liu et al., 2011), or nonlinear such as the Elastic Embedding (Carreira-Perpi\u00f1\u00e1n, 2010) or t-SNE (van der Maaten and Hinton, 2008); as well as objective functions designed specifically for binary hashing, such as Semi-supervised sequential Projection Learning Hashing (SPLH) (Wang et al.", "startOffset": 185, "endOffset": 203}, {"referenceID": 4, "context": ", 2011), or nonlinear such as the Elastic Embedding (Carreira-Perpi\u00f1\u00e1n, 2010) or t-SNE (van der Maaten and Hinton, 2008); as well as objective functions designed specifically for binary hashing, such as Semi-supervised sequential Projection Learning Hashing (SPLH) (Wang et al.", "startOffset": 52, "endOffset": 77}, {"referenceID": 38, "context": ", 2011), or nonlinear such as the Elastic Embedding (Carreira-Perpi\u00f1\u00e1n, 2010) or t-SNE (van der Maaten and Hinton, 2008); as well as objective functions designed specifically for binary hashing, such as Semi-supervised sequential Projection Learning Hashing (SPLH) (Wang et al., 2012).", "startOffset": 265, "endOffset": 284}, {"referenceID": 39, "context": "For example, in the Laplacian loss (3) we can encourage codes to be orthogonal through a constraint ZZ = NI (Weiss et al., 2009) or a penalty term \u2016ZTZ\u2212NI\u2016 (the latter requiring a hyperparameter that controls the weight of the penalty) (Ge et al.", "startOffset": 108, "endOffset": 128}, {"referenceID": 12, "context": ", 2009) or a penalty term \u2016ZTZ\u2212NI\u2016 (the latter requiring a hyperparameter that controls the weight of the penalty) (Ge et al., 2014), although this generates dense matrices of N \u00d7N .", "startOffset": 115, "endOffset": 132}, {"referenceID": 41, "context": "Most papers ignore the binary nature of the Z codes and optimize over them as real values, then binarize them by truncation (possibly with an optimal rotation; Yu and Shi, 2003; Gong et al., 2013), and finally fit a classifier (e.", "startOffset": 124, "endOffset": 196}, {"referenceID": 14, "context": "Most papers ignore the binary nature of the Z codes and optimize over them as real values, then binarize them by truncation (possibly with an optimal rotation; Yu and Shi, 2003; Gong et al., 2013), and finally fit a classifier (e.", "startOffset": 124, "endOffset": 196}, {"referenceID": 1, "context": "For example, for the Laplacian loss with constraints this involves solving an eigenproblem on Z as in Laplacian eigenmaps (Belkin and Niyogi, 2003; Weiss et al., 2009; Zhang et al., 2010), or approximated using landmarks (Liu et al.", "startOffset": 122, "endOffset": 187}, {"referenceID": 39, "context": "For example, for the Laplacian loss with constraints this involves solving an eigenproblem on Z as in Laplacian eigenmaps (Belkin and Niyogi, 2003; Weiss et al., 2009; Zhang et al., 2010), or approximated using landmarks (Liu et al.", "startOffset": 122, "endOffset": 187}, {"referenceID": 42, "context": "For example, for the Laplacian loss with constraints this involves solving an eigenproblem on Z as in Laplacian eigenmaps (Belkin and Niyogi, 2003; Weiss et al., 2009; Zhang et al., 2010), or approximated using landmarks (Liu et al.", "startOffset": 122, "endOffset": 187}, {"referenceID": 28, "context": ", 2010), or approximated using landmarks (Liu et al., 2011).", "startOffset": 41, "endOffset": 59}, {"referenceID": 3, "context": "Some recent papers try to respect the binary nature of the codes during their optimization, using techniques such as alternating optimization, min-cut and GraphCut (Boykov et al., 2001; Lin et al., 2014b; Ge et al., 2014) or others (Lin et al.", "startOffset": 164, "endOffset": 221}, {"referenceID": 12, "context": "Some recent papers try to respect the binary nature of the codes during their optimization, using techniques such as alternating optimization, min-cut and GraphCut (Boykov et al., 2001; Lin et al., 2014b; Ge et al., 2014) or others (Lin et al.", "startOffset": 164, "endOffset": 221}, {"referenceID": 26, "context": ", 2014) or others (Lin et al., 2013), and then fit the classifiers, or use alternating optimization directly on the hash function parameters (Liu et al.", "startOffset": 18, "endOffset": 36}, {"referenceID": 29, "context": ", 2013), and then fit the classifiers, or use alternating optimization directly on the hash function parameters (Liu et al., 2012).", "startOffset": 112, "endOffset": 130}, {"referenceID": 12, "context": "Even more recently, one can optimize jointly over the binary codes and hash functions (Ge et al., 2014; Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei, 2015; Raziperchikolaei and Carreira-Perpi\u00f1\u00e1n, 2015).", "startOffset": 86, "endOffset": 195}, {"referenceID": 5, "context": "Even more recently, one can optimize jointly over the binary codes and hash functions (Ge et al., 2014; Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei, 2015; Raziperchikolaei and Carreira-Perpi\u00f1\u00e1n, 2015).", "startOffset": 86, "endOffset": 195}, {"referenceID": 34, "context": "Even more recently, one can optimize jointly over the binary codes and hash functions (Ge et al., 2014; Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei, 2015; Raziperchikolaei and Carreira-Perpi\u00f1\u00e1n, 2015).", "startOffset": 86, "endOffset": 195}, {"referenceID": 8, "context": "In fact, the very same issue arises when training an ensemble of classifiers (Dietterich, 2000; Zhou, 2012; Kuncheva, 2014).", "startOffset": 77, "endOffset": 123}, {"referenceID": 43, "context": "In fact, the very same issue arises when training an ensemble of classifiers (Dietterich, 2000; Zhou, 2012; Kuncheva, 2014).", "startOffset": 77, "endOffset": 123}, {"referenceID": 23, "context": "In fact, the very same issue arises when training an ensemble of classifiers (Dietterich, 2000; Zhou, 2012; Kuncheva, 2014).", "startOffset": 77, "endOffset": 123}, {"referenceID": 3, "context": "We approximately optimize it using a min-cut algorithm (as implemented by Boykov et al., 2001) applied in alternating fashion to submodular blocks as described in Lin et al. (2014a). This first partitions the N points into disjoint groups containing only nonnegative weights.", "startOffset": 74, "endOffset": 182}, {"referenceID": 11, "context": "This problem is NPcomplete in general (Garey and Johnson, 1979; Boros and Hammer, 2002; Kolmogorov and Zabih, 2003), when A has both positive and negative elements, as well as zeros.", "startOffset": 38, "endOffset": 115}, {"referenceID": 2, "context": "This problem is NPcomplete in general (Garey and Johnson, 1979; Boros and Hammer, 2002; Kolmogorov and Zabih, 2003), when A has both positive and negative elements, as well as zeros.", "startOffset": 38, "endOffset": 115}, {"referenceID": 19, "context": "This problem is NPcomplete in general (Garey and Johnson, 1979; Boros and Hammer, 2002; Kolmogorov and Zabih, 2003), when A has both positive and negative elements, as well as zeros.", "startOffset": 38, "endOffset": 115}, {"referenceID": 2, "context": "It is submodular if A has only nonpositive elements, in which case it is equivalent to a min-cut/max-flow problem and it can be solved in polynomial time (Boros and Hammer, 2002; Greig et al., 1989).", "startOffset": 154, "endOffset": 198}, {"referenceID": 16, "context": "It is submodular if A has only nonpositive elements, in which case it is equivalent to a min-cut/max-flow problem and it can be solved in polynomial time (Boros and Hammer, 2002; Greig et al., 1989).", "startOffset": 154, "endOffset": 198}, {"referenceID": 3, "context": "This is the case for the best practical GraphCut (Boykov et al., 2001) and max-flow/min-cut algorithms (Cormen et al.", "startOffset": 49, "endOffset": 70}, {"referenceID": 7, "context": ", 2001) and max-flow/min-cut algorithms (Cormen et al., 2009).", "startOffset": 40, "endOffset": 61}, {"referenceID": 20, "context": "We use the following labeled datasets (all using the Euclidean distance in feature space): (1) CIFAR (Krizhevsky, 2009) contains 60 000 images in 10 classes.", "startOffset": 101, "endOffset": 119}, {"referenceID": 32, "context": "We use D = 320 GIST features (Oliva and Torralba, 2001) from each image.", "startOffset": 29, "endOffset": 55}, {"referenceID": 30, "context": "(2) Infinite MNIST (Loosli et al., 2007).", "startOffset": 19, "endOffset": 40}, {"referenceID": 22, "context": "Because of the computational cost of affinity-based methods, previous work has used training sets limited to a few thousand points (Kulis and Darrell, 2009; Liu et al., 2012; Lin et al., 2013; Ge et al., 2014).", "startOffset": 131, "endOffset": 209}, {"referenceID": 29, "context": "Because of the computational cost of affinity-based methods, previous work has used training sets limited to a few thousand points (Kulis and Darrell, 2009; Liu et al., 2012; Lin et al., 2013; Ge et al., 2014).", "startOffset": 131, "endOffset": 209}, {"referenceID": 26, "context": "Because of the computational cost of affinity-based methods, previous work has used training sets limited to a few thousand points (Kulis and Darrell, 2009; Liu et al., 2012; Lin et al., 2013; Ge et al., 2014).", "startOffset": 131, "endOffset": 209}, {"referenceID": 12, "context": "Because of the computational cost of affinity-based methods, previous work has used training sets limited to a few thousand points (Kulis and Darrell, 2009; Liu et al., 2012; Lin et al., 2013; Ge et al., 2014).", "startOffset": 131, "endOffset": 209}, {"referenceID": 10, "context": "As hash functions (for each bit), we use linear SVMs (trained with LIBLINEAR; Fan et al., 2008) and kernel SVMs (with 500 basis functions centered at a random subset of training points).", "startOffset": 53, "endOffset": 95}, {"referenceID": 29, "context": "As baseline coupled objective, we use KSH (Liu et al., 2012) but using the same two-step training as ILH: first we find the codes using the alternating min-cut method described earlier (initialized from an all-ones code, and running one iteration of alternating min-cut) and then we fit the classifiers.", "startOffset": 42, "endOffset": 60}, {"referenceID": 23, "context": "Linear SVMs are very stable classifiers known to benefit less from ensembles than less stable classifiers such as decision trees or neural nets (Kuncheva, 2014).", "startOffset": 144, "endOffset": 160}, {"referenceID": 24, "context": "Bagging tPCA (Leng et al., 2014) does make tPCA improve monotonically with b, but the result is still far from competitive.", "startOffset": 13, "endOffset": 32}, {"referenceID": 29, "context": "Comparison with other binary hashing methods We compare with both the original KSH (Liu et al., 2012) and its min-cut optimization KSHcut (Lin et al.", "startOffset": 83, "endOffset": 101}, {"referenceID": 22, "context": ", 2014b), and a representative subset of affinitybased and unsupervised hashing methods: Supervised Binary Reconstructive Embeddings (BRE) (Kulis and Darrell, 2009), Supervised Self-Taught Hashing (STH) (Zhang et al.", "startOffset": 139, "endOffset": 164}, {"referenceID": 42, "context": ", 2014b), and a representative subset of affinitybased and unsupervised hashing methods: Supervised Binary Reconstructive Embeddings (BRE) (Kulis and Darrell, 2009), Supervised Self-Taught Hashing (STH) (Zhang et al., 2010), Spectral Hashing (SH) (Weiss et al.", "startOffset": 203, "endOffset": 223}, {"referenceID": 39, "context": ", 2010), Spectral Hashing (SH) (Weiss et al., 2009), Iterative Quantization (ITQ) (Gong et al.", "startOffset": 31, "endOffset": 51}, {"referenceID": 14, "context": ", 2009), Iterative Quantization (ITQ) (Gong et al., 2013), Binary Autoencoder (BA) (Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei, 2015), thresholded PCA (tPCA), and Locality-Sensitive Hashing (LSH) (Andoni and Indyk, 2008).", "startOffset": 38, "endOffset": 57}, {"referenceID": 5, "context": ", 2013), Binary Autoencoder (BA) (Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei, 2015), thresholded PCA (tPCA), and Locality-Sensitive Hashing (LSH) (Andoni and Indyk, 2008).", "startOffset": 33, "endOffset": 79}, {"referenceID": 0, "context": ", 2013), Binary Autoencoder (BA) (Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei, 2015), thresholded PCA (tPCA), and Locality-Sensitive Hashing (LSH) (Andoni and Indyk, 2008).", "startOffset": 142, "endOffset": 166}, {"referenceID": 24, "context": "There has been a prior attempt to use bagging (bootstrapped samples) with truncated PCA (Leng et al., 2014).", "startOffset": 88, "endOffset": 107}, {"referenceID": 29, "context": "Some supervised binary hashing work (Liu et al., 2012; Wang et al., 2012) has proposed to learn the b hash functions sequentially, where the ith function has an orthogonality-like constraint to force it to differ from the previous functions.", "startOffset": 36, "endOffset": 73}, {"referenceID": 38, "context": "Some supervised binary hashing work (Liu et al., 2012; Wang et al., 2012) has proposed to learn the b hash functions sequentially, where the ith function has an orthogonality-like constraint to force it to differ from the previous functions.", "startOffset": 36, "endOffset": 73}, {"referenceID": 21, "context": "This means that well-known error decompositions such as the error-ambiguity decomposition (Krogh and Vedelsby, 1995) and the bias-variance decomposition (Geman et al.", "startOffset": 90, "endOffset": 116}, {"referenceID": 13, "context": "This means that well-known error decompositions such as the error-ambiguity decomposition (Krogh and Vedelsby, 1995) and the bias-variance decomposition (Geman et al., 1992) do not apply.", "startOffset": 153, "endOffset": 173}, {"referenceID": 9, "context": "In this respect, there is also a relation with error-correcting output codes (ECOC) (Dietterich and Bakiri, 1995), an approach for multiclass classification.", "startOffset": 84, "endOffset": 113}, {"referenceID": 18, "context": "(James and Hastie, 1998).", "startOffset": 0, "endOffset": 24}, {"referenceID": 33, "context": "Some binary hashing methods optimize an objective essentially of the following form (Rastegari et al., 2015; Xia et al., 2015):", "startOffset": 84, "endOffset": 126}, {"referenceID": 40, "context": "Some binary hashing methods optimize an objective essentially of the following form (Rastegari et al., 2015; Xia et al., 2015):", "startOffset": 84, "endOffset": 126}, {"referenceID": 25, "context": "This fact, already noted by Lin et al. (2013), is because a function of 2 binary variables f(x, y) can take 4 different values:", "startOffset": 28, "endOffset": 46}, {"referenceID": 31, "context": "For example, the loss function for Minimal Loss Hashing (Norouzi and Fleet, 2011):", "startOffset": 56, "endOffset": 81}, {"referenceID": 17, "context": "5 we also include results for an additional, unsupervised dataset, the Flickr 1 million image dataset (Huiskes et al., 2010).", "startOffset": 102, "endOffset": 124}, {"referenceID": 39, "context": "In addition to the methods we used in the supervised datasets, we compare ILHt with Spectral Hashing (SH) (Weiss et al., 2009), Iterative Quantization (ITQ) (Gong et al.", "startOffset": 106, "endOffset": 126}, {"referenceID": 14, "context": ", 2009), Iterative Quantization (ITQ) (Gong et al., 2013), Binary Autoencoder (BA) (Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei, 2015), thresholded PCA (tPCA), and Locality-Sensitive Hashing (LSH) (Andoni and Indyk, 2008).", "startOffset": 38, "endOffset": 57}, {"referenceID": 5, "context": ", 2013), Binary Autoencoder (BA) (Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei, 2015), thresholded PCA (tPCA), and Locality-Sensitive Hashing (LSH) (Andoni and Indyk, 2008).", "startOffset": 33, "endOffset": 79}, {"referenceID": 0, "context": ", 2013), Binary Autoencoder (BA) (Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei, 2015), thresholded PCA (tPCA), and Locality-Sensitive Hashing (LSH) (Andoni and Indyk, 2008).", "startOffset": 142, "endOffset": 166}], "year": 2016, "abstractText": "Binary hashing is a well-known approach for fast approximate nearest-neighbor search in information retrieval. Much work has focused on affinity-based objective functions involving the hash functions or binary codes. These objective functions encode neighborhood information between data points and are often inspired by manifold learning algorithms. They ensure that the hash functions differ from each other through constraints or penalty terms that encourage codes to be orthogonal or dissimilar across bits, but this couples the binary variables and complicates the already difficult optimization. We propose a much simpler approach: we train each hash function (or bit) independently from each other, but introduce diversity among them using techniques from classifier ensembles. Surprisingly, we find that not only is this faster and trivially parallelizable, but it also improves over the more complex, coupled objective function, and achieves state-of-the-art precision and recall in experiments with image retrieval.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}