{"id": "1602.01582", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2016", "title": "SDCA without Duality, Regularization, and Individual Convexity", "abstract": "Stochastic Dual Coordinate Ascent is a popular method for solving regularized loss minimization for the case of convex losses. We describe variants of SDCA that do not require explicit regularization and do not rely on duality. We prove linear convergence rates even if individual loss functions are non-convex, as long as the expected loss is strongly convex.", "histories": [["v1", "Thu, 4 Feb 2016 08:14:06 GMT  (11kb,D)", "https://arxiv.org/abs/1602.01582v1", null], ["v2", "Sat, 21 May 2016 12:33:05 GMT  (25kb)", "http://arxiv.org/abs/1602.01582v2", "ICML 2016"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shai shalev-shwartz"], "accepted": true, "id": "1602.01582"}, "pdf": {"name": "1602.01582.pdf", "metadata": {"source": "META", "title": "SDCA without Duality, Regularization, and Individual Convexity", "authors": ["Shai Shalev-Shwartz"], "emails": ["SHAIS@CS.HUJI.AC.IL"], "sections": [{"heading": null, "text": "ar Xiv: 160 2.01 582v 2 [cs.L G] 21 May 201 6"}, {"heading": "1. Introduction", "text": "We look at the following loss minimization problem: min w. \"RdF (w): 1nn\" i = 1fi (w).An important subclass of problems is when every single case can be written as fi (w).( SDCA) and (Shalev-Shwartz & Zhang, 2013) has the convergence rate of O. \"(Lmax / n) Log (1 / n) Log (1 / n) Log (1 / n) Log (1 / n) Log (1 / 2), where Lmax = maxi Li.As its name suggests, SDCA is derived from a double problem. (In this paper we consider the possibility of applying SDCA to problems where individual fi do not necessarily have the form of SDCA (w) + 2, and may even be non-convex."}, {"heading": "2. SDCA without Duality", "text": "We begin the section by describing a variant of SDCA that is not based on duality. To simplify the presentation, we start with problems of regulated loss minimization in Section 2.2, we deal with the unregulated case and in Section 2.3, we deal with the non-convex case. We remember the following basic definitions: A (differentiable) function f is convex when for each u, while we f (w) \u2212 f (u) f (u) f (u) (w \u2212 u) (w \u2212 u) (w \u2212 u) \u00b2 w \u2212 u 2. We say that f is convex when it is 0-strong convex. We say that f L is smooth when it is f (w) \u2212 f (u) w \u2212 u w \u2212 u. It is well known that smoothness and convexity also imply that f (w) \u2212 f (u) \u2212 f (u) \u2264 f (u \u2212 u)."}, {"heading": "2.1. Regularized problems", "text": "In regularized problems, each fi can be called fi (w) = \u03c6i (w) + \u03bb 2 (q) =. Similar to the original SDCA algorithm, we will call the vectors \u03b11,. \u2212 \u2212 n, each of these vectors being called pseudo-dual vectors (w). The algorithm is described below: Number of iterations T, Number of iterations T \u2212 \u2212 \u2212 Number of iterations L1 \u2212 Number of iterations L1 \u2212 Number of iterations L1 \u2212., Ln Initialize: w (0) = 1n Number of vectors i (w) + Number of iterations F, Number of iterations T \u2212 Number of iterations L1 \u2212,., Ln Initialize: w (0) = 1 Number of iterations n (0) i (0) = Number of iterations F (0) = (0) 1) 1, Number of iterations T, Number of iterations, Number of iterations T, Number of itterations, Number of itterations, Number of itterations (T), Number of itterts (number)."}, {"heading": "2.2. SDCA without regularization", "text": "Turning now to the case where the target is not explicitly regulated, the following algorithm solves this problem by reducing it to the regularized case. In particular, we artificially add regulation to the target and compensate it by adding another loss function that cancels the regularized term. Although the additional function is not convex (in fact it is concave), we prove that the same convergence rate applies due to the special structure of the additional loss function. Algorithm 2: Dual-Free SDCA for Non-Regularized ObjectivesGoal: Minimize F (w) = 1n \u2211 ni = 1 fi (w) Input: Objective F, Number of iterations T, Step Size T, Strong Conveity Parameter \u03bb, Flatness Parameter L1,..., LnDefine: For all i [n], sp."}, {"heading": "2.3. The non-convex case", "text": "For the sake of simplicity, we will focus on the regularized setting. In the non-regularized setting, we can simply replace each fi with \u03c6i (w) = fi (w) \u2212 \u03bb2-w-2 and apply the regularized setting. Note that this does not significantly change the smoothness (because \u03bb is typically much smaller than the average smoothness of fi). We can apply algorithm 1 to the non-convex case, and the only change is the choice of \u03b7, as reflected in the following theorem.Theorem 3 If we consider the execution of algorithm 1 on F, which is p-strongly convex, we assume that each \u03c6i is Li-smooth, and vice versa is \u2264 (1 \u2212 \u041at] \u2264 (1 \u2212 \u041at) t t C0, where Ct is defined as in (1)."}, {"heading": "2.4. Acceleration", "text": "Accelerated SDCA (Shalev-Shwartz & Zhang, 2015) is achieved by solving (with SDCA) a sequence of problems (with SDCA), with each iteration adding an artificial regularization of form (Lin et al., 2015) so that the inner solver can be any algorithm. For completeness, we offer the pseudo-code of the \"catalyst\" algorithm of (Lin et al., 2015) and its analyses.Algorithm 3: AccelerationGoal: Minimizing a strongly convex function F (w) Parameter: D, T-Alize: Initial solution w (0).t. \u00b7 F (w) \u2212 F (w) \u2212 F (w)."}, {"heading": "3. Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Proof of Theorem 1", "text": "Note that 0 = 1 \u2212 2 \u2212 3 \u2212 3 \u2212 4 \u2212 4 \u2212 4 \u2212 4 \u2212 4 \u2212 4 \u2212 4 \u2212 4 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 0 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 0 \u2212 5 \u2212 5 \u2212 5 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5"}, {"heading": "3.2. Proof of Lemma 1", "text": "We have: E [2] w (t) -w (t \u2212 1) 2] = 1) i (w (t \u2212 1)) + 1 (t \u2212 1) i (3) 2 \u2264 3 (2) 2n2) i1 qi (w (t \u2212 1) + 2 (t \u2212 1) i + 1 (t \u2212 1) i \u2212 2) (triangular inequality) = 3 (2n2) i (1qi (w (t (t \u2212 1))) \u2212 2 (w (t \u2212 1) i (w) i (2 + 1qi (t \u2212 1)) 2 + 1qi (t \u2212 1) i (1) i \u2212 2) \u2264 3 (2) 2n2 (2 \u2212 1) \u2212 w (t \u2212 1) 2 + 1qi (t \u2212 1) i \u2212 2) (smooth and (15) \u2264 3 (1 2 (w (t \u2212 1) \u2212 w (2 + Ct \u2212 1) (1)."}, {"heading": "3.3. Proof of Theorem 2", "text": "The beginning of the evidence is identical to the proof of theory 1. The change begins in (13), where we cannot apply (12) to (1) because it is not convex. To overcome this, we first apply (12) to (1),.., and obtain that we (11) cannot apply (12) to (1) because it is not convex. In order to overcome this, we apply (12) to (1). (2) (2) (2) (2) (2) (2) (2) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4)) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4"}, {"heading": "3.4. Proof of Theorem 3", "text": "The beginning of the proof is identical to the proof of theorem 1 to (9). We will choose the parameters \u03b7, ca, cb so that the term in (9) is not negative. Next, due to the strong convexity of F, we have that (w (t \u2212 1) -w, F (w (t \u2212 1))) -F (t \u2212 1) -F (w) -w, 2) -w, w (t \u2212 1) -w, 2. Therefore, E [Ct \u2212 1 \u2212 Ct] = Ct] = Cqi (t \u2212 1)) -F (t \u2212 1) -F (w) + qi, 2 (t \u2212 1) -w, 2) -w, L (t \u2212 2) -w, so we have the definition of E, E [Ct \u2212 1 \u2212 Ct] = Ct \u2212 1 \u2212 Ct that we have the definition of L."}, {"heading": "4. Proof of Theorem 4", "text": "Evidence Each iteration of algorithm 3 requires the minimization of the accuracy of Gt as a function of the accuracy of T \u2264 t \u2264 O (1) (1 \u2212 \u03c1) t, where \u03c1 = 0.9 \u221a q. If t \u2264 T is defined as in Lemma 2, then we have the, \u2212 t log (1 \u2212 \u03c1) \u2264 \u2212 T log (1 \u2212 \u03c1) = \u2212 log (1 \u2212 \u03c1) \u03c1 log (800q) using Lemma 4, \u2212 log (1 \u2212 \u03c1) \u03c1 \u2264 2 for each determination (0, 1 / 2). In our case, it is indeed in (0, 1 / 2) due to the definition of Even and our assumption that (L \u00b2 / \u03bb) 2 \u2265 3n. Consequently, we obtain the number of iterations required in each application of the algorithm 3, O (Log (((((E) / (E))))))). Combine this with theorem 3, and using the definition of Gt, so that we get the number of iterations required in each application of the algorithm 3, the number of Item \u00b2 (O) is required in each application (N) by (1)."}, {"heading": "4.1. Technical Lemmas", "text": "Lemma 3 Suppose \u03c6 L-smooth and convex, then for each w and u there is proof that g (w) = \u03c6 (w) \u2212 \u03c6 (u) \u2212 \u03c6 (w) \u2212 \u03c6 (u) \u2212 \u03c6 (u) \u2212 \u03c6 (u) (w \u2212 u). The proof for each i, defineg (w) = \u03c6 (w) \u2212 \u03c6 (u) \u2212 \u03c6 (u) \u2212 \u03c6 (u) results in g not being negative and smooth and thus constituting a self-limitation (see1While theorem 3 limits the expected sub-optimality, it can similarly be transformed by techniques (Shalev-Shwartz & Zhang, 2015) into a limit with a high probability. Section 12,1,3 in (Shalev-Shwartz Ben-David, 2014): Similar techniques (Shalev-Shwartz & Zhang, 2015) allow it to be transformed into a limit with a high probability."}, {"heading": "5. Summary", "text": "Our analysis shows a linear convergence rate for all of these cases. Two immediate open questions are whether greater dependence on the number of conditions for the non-convex result is necessary for the non-convex case and whether the factor n3 / 4 in Theorem 4 can be reduced to n1 / 2: In an earlier draft of this paper, the limit was for the non-convex case n5 / 4 + n3 / 4 \u221a L /. We thank Ohad Shamir for showing us how to improve the limit of + n3 / 4 VVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV"}], "references": [{"title": "A lower bound for the optimization of finite sums", "author": ["Agarwal", "Alekh", "Bottou", "Leon"], "venue": "In ICML,", "citeRegEx": "Agarwal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2014}, {"title": "Univr: A universal variance reduction framework for proximal stochastic gradient method", "author": ["Allen-Zhu", "Zeyuan", "Yuan", "Yang"], "venue": "arXiv preprint arXiv:1506.01972,", "citeRegEx": "Allen.Zhu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Allen.Zhu et al\\.", "year": 2015}, {"title": "On lower and upper bounds for smooth and strongly convex optimization problems", "author": ["Arjevani", "Yossi", "Shalev-Shwartz", "Shai", "Shamir", "Ohad"], "venue": "arXiv preprint arXiv:1503.06833,", "citeRegEx": "Arjevani et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Arjevani et al\\.", "year": 2015}, {"title": "Primal method for erm with flexible mini-batching schemes and nonconvex losses", "author": ["Csiba", "Dominik", "Richt\u00e1rik", "Peter"], "venue": "arXiv preprint arXiv:1506.02227,", "citeRegEx": "Csiba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Csiba et al\\.", "year": 2015}, {"title": "New Optimisation Methods for Machine Learning", "author": ["Defazio", "Aaron"], "venue": "PhD thesis, Australian National Univer- sity,", "citeRegEx": "Defazio and Aaron.,? \\Q2014\\E", "shortCiteRegEx": "Defazio and Aaron.", "year": 2014}, {"title": "Saga: A fast incremental gradient method with support for non-strongly convex composite objectives", "author": ["Defazio", "Aaron", "Bach", "Francis", "Lacoste-Julien", "Simon"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Defazio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Defazio et al\\.", "year": 2014}, {"title": "Finito: A faster, permutable incremental gradient method for big data problems", "author": ["Defazio", "Aaron J", "Caetano", "Tib\u00e9rio S", "Domke", "Justin"], "venue": "arXiv preprint arXiv:1407.2710,", "citeRegEx": "Defazio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Defazio et al\\.", "year": 2014}, {"title": "Dual free sdca for empirical risk minimization with adaptive probabilities", "author": ["He", "Xi", "Tak\u00e1\u010d", "Martin"], "venue": "arXiv preprint arXiv:1510.06684,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Robust shift-and-invert preconditioning: Faster and more sample efficient algorithms for eigenvector computation", "author": ["Jin", "Chi", "Kakade", "Sham M", "Musco", "Cameron", "Netrapalli", "Praneeth", "Sidford", "Aaron"], "venue": "arXiv preprint arXiv:1510.08896,", "citeRegEx": "Jin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2015}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Johnson", "Rie", "Zhang", "Tong"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Johnson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2013}, {"title": "Semi-stochastic gradient descent methods", "author": ["Kone\u010dn\u1ef3", "Jakub", "Richt\u00e1rik", "Peter"], "venue": "arXiv preprint arXiv:1312.1666,", "citeRegEx": "Kone\u010dn\u1ef3 et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kone\u010dn\u1ef3 et al\\.", "year": 2013}, {"title": "A stochastic gradient method with an exponential convergence rate for finite training sets", "author": ["Le Roux", "Nicolas", "Schmidt", "Mark", "Bach", "Francis"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Roux et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Roux et al\\.", "year": 2012}, {"title": "A universal catalyst for first-order optimization", "author": ["Lin", "Hongzhou", "Mairal", "Julien", "Harchaoui", "Zaid"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization", "author": ["S. Shalev-Shwartz", "T. Zhang"], "venue": "Mathematical Programming SERIES A and B (to appear),", "citeRegEx": "Shalev.Shwartz and Zhang,? \\Q2015\\E", "shortCiteRegEx": "Shalev.Shwartz and Zhang", "year": 2015}, {"title": "Sdca without duality", "author": ["Shalev-Shwartz", "Shai"], "venue": "arXiv preprint arXiv:1502.06177,", "citeRegEx": "Shalev.Shwartz and Shai.,? \\Q2015\\E", "shortCiteRegEx": "Shalev.Shwartz and Shai.", "year": 2015}, {"title": "Understanding Machine Learning: From Theory to Algorithms", "author": ["Shalev-Shwartz", "Shai", "Ben-David"], "venue": "Cambridge university press,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2014}, {"title": "Stochastic dual coordinate ascent methods for regularized loss minimization", "author": ["Shalev-Shwartz", "Shai", "Zhang", "Tong"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2013}, {"title": "A stochastic pca and svd algorithm with an exponential convergence rate", "author": ["Shamir", "Ohad"], "venue": "In ICML,", "citeRegEx": "Shamir and Ohad.,? \\Q2015\\E", "shortCiteRegEx": "Shamir and Ohad.", "year": 2015}, {"title": "A proximal stochastic gradient method with progressive variance reduction", "author": ["Xiao", "Lin", "Zhang", "Tong"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Xiao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": ", deep learning optimization problems, or problems arising in fast calculation of the top singular vectors (Jin et al., 2015)).", "startOffset": 107, "endOffset": 125}, {"referenceID": 2, "context": "Lower bounds have been derived in (Arjevani et al., 2015; Agarwal & Bottou, 2014).", "startOffset": 34, "endOffset": 81}, {"referenceID": 12, "context": "Applying an acceleration technique ((Shalev-Shwartz & Zhang, 2015; Lin et al., 2015)) we obtain the convergence rate \u00d5(n \u221a", "startOffset": 36, "endOffset": 84}, {"referenceID": 8, "context": "Using SVRG for non-convex individual functions has been recently studied in (Shamir, 2015; Jin et al., 2015), in the context of fast computation of the top singular vectors of a matrix.", "startOffset": 76, "endOffset": 108}, {"referenceID": 12, "context": "The algorithm has been generalized in (Lin et al., 2015) to allow the inner solver to be any algorithm.", "startOffset": 38, "endOffset": 56}, {"referenceID": 12, "context": "For completeness, we provide the pseudo-code of the \u201cCatalyst\u201d algorithm of (Lin et al., 2015) and its analysis.", "startOffset": 76, "endOffset": 94}, {"referenceID": 12, "context": "1 of (Lin et al., 2015) by observing that Algorithm 3 is a specification of Algorithm 1 in (Lin et al.", "startOffset": 5, "endOffset": 23}, {"referenceID": 12, "context": ", 2015) by observing that Algorithm 3 is a specification of Algorithm 1 in (Lin et al., 2015) with \u03b10 = \u221a q (which implies that \u03b1t = \u03b10 for every t), with \u01ebt = \u01eb0(1\u2212 \u03c1), and with \u03c1 = 0.", "startOffset": 75, "endOffset": 93}], "year": 2016, "abstractText": "Stochastic Dual Coordinate Ascent is a popular method for solving regularized loss minimization for the case of convex losses. We describe variants of SDCA that do not require explicit regularization and do not rely on duality. We prove linear convergence rates even if individual loss functions are non-convex, as long as the expected loss is strongly convex.", "creator": "LaTeX with hyperref package"}}}