{"id": "1602.01690", "review": {"conference": "icml", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2016", "title": "Minimizing the Maximal Loss: How and Why?", "abstract": "A commonly used learning rule is to approximately minimize the \\emph{average} loss over the training set. Other learning algorithms, such as AdaBoost and hard-SVM, aim at minimizing the \\emph{maximal} loss over the training set. The average loss is more popular, particularly in deep learning, due to three main reasons. First, it can be conveniently minimized using online algorithms, that process few examples at each iteration. Second, it is often argued that there is no sense to minimize the loss on the training set too much, as it will not be reflected in the generalization loss. Last, the maximal loss is not robust to outliers. In this paper we describe and analyze an algorithm that can convert any online algorithm to a minimizer of the maximal loss. We prove that in some situations better accuracy on the training set is crucial to obtain good performance on unseen examples. Last, we propose robust versions of the approach that can handle outliers.", "histories": [["v1", "Thu, 4 Feb 2016 14:32:23 GMT  (47kb,D)", "https://arxiv.org/abs/1602.01690v1", null], ["v2", "Sun, 22 May 2016 15:13:56 GMT  (307kb,D)", "http://arxiv.org/abs/1602.01690v2", "ICML 2016"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shai shalev-shwartz", "yonatan wexler"], "accepted": true, "id": "1602.01690"}, "pdf": {"name": "1602.01690.pdf", "metadata": {"source": "META", "title": "Minimizing the Maximal Loss: How and Why", "authors": ["Shai Shalev-Shwartz", "Yonatan Wexler"], "emails": ["SHAIS@CS.HUJI.AC.IL", "YONATAN.WEXLER@ORCAM.COM"], "sections": [{"heading": "1. Introduction", "text": "In a typical learning scenario, we have the ability to communicate in a way we have experienced in recent years. < # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "2. How", "text": "In this section we describe and analyze an algorithmic system for optimization, which in (2).Denote of Sm = [0, 1] m: [0, 1] m. \"(0, 1). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (1). (1). (1). (1). (). (1). (). (1). (). (1). (). (1). ()."}, {"heading": "2.1. Classification", "text": "In the classification, \"(w, x, y) is the zero-one loss, namely, it is equal to zero if hw (x) = y and it is equal to 1 if hw (x) 6 = y. We assume that each number is strictly less than 1 / 2, say 0.499.Note that theorem 1 tells us that the average loss of the classifiers wt1,.., wtk is less than = 0.499. Since the values of the loss are either 1 or 0, it means that the loss of more than 1 / 2 is the classifier 0, which implies that the majority classifier has zero losses. Consequence 1 Let's assume that\" (w, x, y) is the zero-one loss function, namely \"(w, x, y) = 1 [hw (x) 6 = y]."}, {"heading": "2.2. Convex Losses", "text": "Let us now consider the case where \"(w, x, y) has the form \u03c6y (hw (x)), where for each y there is a convex function. Note that this assumption alone does not imply that\" w is a convex function (this only applies if hw (x) is an affine function). In the case of convex \u03c6y, where theorem 1 is combined with Jensen's inequality, we obtain: Conclusion 2 Under the assumptions of theorem 1, if \"(w, x, y) has the form \u03c6y (hw (x), where for each y is a convex function, then the predictor h (x) = 1 k-j = 1 hwtj (x), then we also have this i-y (h (xi). If we continue to assume that hw (x) is an affine function of w, and if we leave w = 1k-j = 1, then we also have xi-y (x)."}, {"heading": "2.3. Pseudo-code", "text": "In the following we describe a pseudo-code of the algorithm. We rely on a tree data structure to maintain the probability of the p-player. It is easy to verify the correctness of the implementation.Note that the runtime of each iteration is the time required to perform one step of the online learner plusO (log (m)) for scanning pt and updating the tree structure.Note that the runtime of each iteration is the time required to perform one step of the online learner plusO (log (m) -loss function ': W \u00b7 X \u00b7 Y [0, 1] parameter, T \u2212 k Oracle access the online learning algorithm OLA initialization: Tree.initialize (m) (see the Tree pseudo code) w1 = OLA.initialize () Output / Output / Output / Output / - / - / -"}, {"heading": "2.4. Related Work", "text": "As already mentioned, our algorithm is a variant of the approach outlined in Auer et al. (2002, Section 9), but has the advantage that updating the p-player on each iteration scale with log (m) and not with m. This work focuses on the specific case of binary classification with a linear predictor, namely that they tackle the problem minw-Rd. (Clarkson et al., 2012) The problem minw-Rd-2 \u2264 1 maxp-Sm-i pi < w, xi >. Taking the setup of Example 1, (Clarkson et al., 2012) presents an algorithm that finds a consistent hypothesis in the runtime of O-Rd (m + d) \u00b7 C (m + d) \u00b7 For the same problem, our algorithm (with the perception tron as weak learner) can be found."}, {"heading": "3. Why", "text": "In this section, we address the \"why\" hypothesis, namely why we should prefer to minimize the maximum loss rather than the average loss? (For the sake of simplicity of presentation, in this section we deal with binary classification problems, in the realizable situation.) In this context, minimizing the maximum loss of accuracy of < 1 example leads to a consistent hypothesis and is not satisfied with a hypothesis with Lavg (h) \u2264 1 / m. In the usual PAC learning model (see Shalev-Shwartz & Ben-David, 2014) for an overview, there is a distribution D over X \u00d7 Y and the training examples are assumed to be a consistent hypothesis over a consistent hypothesis."}, {"heading": "3.1. A Simple Example of a Gap", "text": "Consider the following distribution: First, let us leave z1 = (\u03b1, 1) and z2 = (\u03b1, \u2212 2\u03b1) for a few small \u03b1 > 0. To generate an example (x, y) \u0445 D, sample a label y from {\u00b1 1}, then we set x = yz1 with probability 1 - and set x = yz2 with probability. The hypotheses class consists of half spaces: H = {x \u2192 characters (< w, x >): w = R2}. The following three lemmats, the proofs of which are given in the appendix, determine the gap between the different approaches. Lemma 1 For each other consideration (0, 1), if m \u2265 2 log (4 / 3) then with probability of at least 1 \u2212 3% over the choice of the training set, S \u2012 Dm, each hypothesis in the ERM (S) will have a generalization error of 0, Lemma 2 soup."}, {"heading": "3.2. Typical vs. Rare Distributions", "text": "To motivate the learning situation, we look at the problem of facial recognition, where the goal is to perform image editing and determine whether or not it is an image of a face. Therefore, a picture of typical positive and negative examples is given in Figure 1 (top row). However, by having enough examples for training, we can learn that the distinction between face and non-face is based on a few features such as \"a face,\" \"eyes,\" \"nose\" and \"mouth.\" From the typical examples, it is difficult to tell whether or not a picture of a watermelon is a face - it has the elliptical shape of a face and something that looks like eyes, but it has no nose or mouth. The bottom row of Figure 1 shows some additional \"examples.\" Such a phenomenon can be formally described as follows. There are two distributions over the examples, D1 and D2. Our goal is to have an error in most distributions, namely we would like to find such."}, {"heading": "4. Robustness", "text": "In the previous section, we have shown cases where minimizing Lmax is better than minimizing Lavg. However, in the presence of outliers, minimizing Lmax could lead to meaningless results - a single outlier can change the value of Lmax and lead to a trivial, uninteresting solution. In this section, we describe two tricks to solve this problem: the first trick replaces the original sample with a new sample, the examples of which can be taken from the original sample; the second trick is based on gap variables. We point out that these tricks are not new and come in various forms in the literature. See, for example (Huber & Ronchetti, 2009; Maronna et al., 2006). The aim of this section is merely to show how to apply known tricks to the maximum loss problem. Remember that in the previous section, we have shown that a small number of \"rare\" examples can have dramatic effects on the performance of the algorithm on the \"rare\" if the number of allergy is greater than the number of allergy sufferers we can hope."}, {"heading": "4.1. Sub-sampling with repetitions", "text": "The first trick we are considering is simply to take a new sample of n examples, in which each example in the new sample is sampled regardless of the even distribution across the original m examples. Then, we run our algorithm based on the obtained sample of n examples. If there are k outliers and the size of the new sample is significantly smaller than m / k, then there is a good chance that no outliers will fall into the new sample. On the other hand, we want enough \"rare\" examples to fall into the new sample. Suppose that m \u2265 10k, the proof of which can be found in the appendix, shows for which values k and m2 this is possible. Theorem 3 Let k be the number of outliers, m2 be the number of rare examples, m the size of the original sample, and n the size of the new sample. Let us assume that m \u2265 10k. Then the probability that the new sample contains outliers and / or not at least m2 / 2 are rare examples, maximum 0.01 + 9.99% of the new sample size and the new sample size."}, {"heading": "4.2. Slack variables", "text": "Another common trick commonly used in the SVM literature is to introduce a vector of slack variables so that the number of outliers is limited to a maximum of K. Then we can write the following optimization problem: min w-W, 0-Rm max i-K. That is, we allow the algorithm to refer to most K examples as outsiders. (w, xi, yi) s.t.2, 0, 1 m, 1-K. This optimization problem minimizes the maximum loss over a subset of examples of size with leastm \u2212 K. That is, we allow the algorithm to refer to most K examples as outliers. Note that the above problem can be written as max loss minimization."}, {"heading": "5. Experiments", "text": "In this section, we show several advantages of our approach to the well-studied problem of facial recognition, so that we can focus on different methods rather than the way they are often expected to detect a few needles in a haystack. In addition, a mixture of typical and rare distributions is to be expected. For example, smartphone users will not be on the same continent as the manufacturers collecting data for the training. This domain requires a weighting of examples and is therefore a good starting point to examine our algorithms. In order to create a dataset that we have marked with \"face,\" we then applied an off-the-shelf face detector, and it found 32 k rectangles that are aligned to faces. This was the basis of our positive examples. For negative examples that we happened to have 250k rectangles in the same images that are not overlapped. Each rectangle was cropped and scaled to 28 x pixels."}, {"heading": "A. Proof of Theorem 1", "text": "(1)......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "B. Proofs of Lemmas in Section 3.1", "text": "Proof [proof of Lemma 1] There are only 4 possible examples, so an ERM will have a generalization error of 0 (assuming we see all 4 examples. By a simple direct calculation together with the union bound by the 4 examples, it is easy to verify that the probability that we do not see all examples is at most 4 (1 \u2212 / 2) m \u2264 4e \u2212 m / 2, and the assertion follows. Proof [proof of Lemma 2] For SGD we can (due to symmetry) assume that y always 1. Therefore, there are only two possible examples z1, z2. With probability 1 \u2212 the first examples are z1 \u2212 the probability is z1. Also, wT always has the formatting (kz1 + rz2) = (k + r) \u03b1, k \u2212 2r\u03b1), where k is the number of times we had a margin and r the number of times we had a margin."}, {"heading": "C. Proof of Theorem 2", "text": "We can imagine the ERM algorithm as follows: First, we sample (i1,.., in), where P [ir = j] = \u03bbj. Let m1 be the number of indices for which ir = 1 is and let m2 = m \u2212 m1. Second, we sample S1 \u0445 Dm11, and define H \u0445 1 as all hypotheses in H that are consistent with S1. Finally, we sample S2 \u0445 Dm22 and define the starting hypotheses as hypotheses in H-1 that are consistent with S2. The proof is based on the following three claims in which we use C to designate a universal constant: \u2022 Claim 1: With probability of at least 1 \u2212 Dm22 and 3 by selecting (i1,..) we have both m1 and m2 in line with S2 and m2. \u2022 Claim 2: Claim of at least 1 \u2212 Dm22 and 3 by selecting (i1,..) we have that both m1 and m2 are consistent with S2 and m2."}, {"heading": "D. Proof of Theorem 3", "text": "The probability that not all outliers will fall into the sample of n examples is (1 \u2212 k / m) n \u2265 0.99 e \u2212 kn / m. Therefore, the probability that at least one outlier will fall into the sample is at most 1 \u2212 0.99 e \u2212 kn / m \u2264 1 \u2212 0.99 (1 \u2212 kn / m) = 0.01 + 0.99 kn / mOn the other hand, the expected number of rare examples in the sample is at most exp (\u2212 0.1nm2 / m) and at Chernoff's limit, the probability that less than half of the rare examples will fall into the sample is at most exp (\u2212 0.1nm2 / m)."}], "references": [{"title": "Even faster accelerated coordinate descent using non-uniform sampling", "author": ["Allen-Zhu", "Zeyuan", "Yuan", "Yang"], "venue": "arXiv preprint arXiv:1512.09103,", "citeRegEx": "Allen.Zhu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Allen.Zhu et al\\.", "year": 2015}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Auer", "Peter", "Cesa-Bianchi", "Nicolo", "Freund", "Yoav", "Schapire", "Robert E"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Adaptive importance sampling to accelerate training of a neural probabilistic language model", "author": ["Bengio", "Yoshua", "Sen\u00e9cal", "Jean-S\u00e9bastien"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2008}, {"title": "Accelerating stochastic gradient descent via online learning to sample", "author": ["Bouchard", "Guillaume", "Trouillon", "Th\u00e9o", "Perez", "Julien", "Gaidon", "Adrien"], "venue": "arXiv preprint arXiv:1506.09016,", "citeRegEx": "Bouchard et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bouchard et al\\.", "year": 2015}, {"title": "Theory of classification: A survey of some recent advances", "author": ["Boucheron", "St\u00e9phane", "Bousquet", "Olivier", "Lugosi", "G\u00e1bor"], "venue": "ESAIM: probability and statistics,", "citeRegEx": "Boucheron et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Boucheron et al\\.", "year": 2005}, {"title": "The tradeoffs of large scale learning", "author": ["Bousquet", "Olivier", "Bottou", "L\u00e9on"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Bousquet et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bousquet et al\\.", "year": 2008}, {"title": "Sublinear optimization for machine learning", "author": ["Clarkson", "Kenneth L", "Hazan", "Elad", "Woodruff", "David P"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Clarkson et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Clarkson et al\\.", "year": 2012}, {"title": "Efficient projections onto the l 1-ball for learning in high dimensions", "author": ["Duchi", "John", "Shalev-Shwartz", "Shai", "Singer", "Yoram", "Chandra", "Tushar"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Duchi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2008}, {"title": "Hoeffding\u2019s inequality for supermartingales", "author": ["Fan", "Xiequan", "Grama", "Ion", "Liu", "Quansheng"], "venue": "Stochastic Processes and their Applications,", "citeRegEx": "Fan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2012}, {"title": "A desicion-theoretic generalization of on-line learning and an application to boosting", "author": ["Freund", "Yoav", "Schapire", "Robert E"], "venue": "In Computational learning theory,", "citeRegEx": "Freund et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Freund et al\\.", "year": 1995}, {"title": "Rigorous learning curve bounds from statistical mechanics", "author": ["Haussler", "David", "Kearns", "Michael", "Seung", "H Sebastian", "Tishby", "Naftali"], "venue": "Machine Learning,", "citeRegEx": "Haussler et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Haussler et al\\.", "year": 1996}, {"title": "Beating sgd: Learning svms in sublinear time", "author": ["Hazan", "Elad", "Koren", "Tomer", "Srebro", "Nati"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hazan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2011}, {"title": "Robust Statistics (second edition)", "author": ["Huber", "Peter J", "Ronchetti", "Elvezio M"], "venue": "J. Wiley,", "citeRegEx": "Huber et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Huber et al\\.", "year": 2009}, {"title": "Exponentiated gradient versus gradient descent for linear predictors", "author": ["J. Kivinen", "M. Warmuth"], "venue": "Information and Computation,", "citeRegEx": "Kivinen and Warmuth,? \\Q1997\\E", "shortCiteRegEx": "Kivinen and Warmuth", "year": 1997}, {"title": "Online learning and online convex optimization", "author": ["Shalev-Shwartz", "Shai"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Shalev.Shwartz and Shai.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz and Shai.", "year": 2011}, {"title": "Understanding Machine Learning: From Theory to Algorithms", "author": ["Shalev-Shwartz", "Shai", "Ben-David"], "venue": "Cambridge university press,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2014}, {"title": "On the equivalence of weak learnability and linear separability: New relaxations and efficient boosting algorithms", "author": ["Shalev-Shwartz", "Shai", "Singer", "Yoram"], "venue": "Machine learning,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2010}, {"title": "Svm optimization: inverse dependence on training set size", "author": ["Shalev-Shwartz", "Shai", "Srebro", "Nathan"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2008}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["Shalev-Shwartz", "Shai", "Singer", "Yoram", "Srebro", "Nathan", "Cotter", "Andrew"], "venue": "Mathematical programming,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2011}, {"title": "Stochastic optimization with importance sampling", "author": ["Zhao", "Peilin", "Zhang", "Tong"], "venue": "arXiv preprint arXiv:1401.2753,", "citeRegEx": "Zhao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2014}, {"title": "AT be a martingale difference sequence w.r.t", "author": ["Hazan"], "venue": "Lemma C.3) and Fan et al. (2012,", "citeRegEx": "Hazan,? \\Q2011\\E", "shortCiteRegEx": "Hazan", "year": 2011}, {"title": "The algorithm that generates the above sequence is known as the EG algorithm (Kivinen", "author": ["\u2208 Sm"], "venue": "(Shalev-Shwartz,", "citeRegEx": "Sm.,? \\Q1997\\E", "shortCiteRegEx": "Sm.", "year": 1997}], "referenceMentions": [{"referenceID": 1, "context": "For the p player, we use the seminal work of (Auer et al., 2002).", "startOffset": 45, "endOffset": 64}, {"referenceID": 1, "context": "Crucially, the work of (Auer et al., 2002) does not assume that \u039b(wt) are sampled from a fixed distribution, but rather the vectors \u039b(wt) can be chosen by an adversary.", "startOffset": 23, "endOffset": 42}, {"referenceID": 1, "context": "In (Auer et al., 2002) it is proposed to rely on the algorithm EXP3.", "startOffset": 3, "endOffset": 22}, {"referenceID": 6, "context": "Phrasing the maxloss minimization as a two players game has also been proposed by (Clarkson et al., 2012; Hazan et al., 2011).", "startOffset": 82, "endOffset": 125}, {"referenceID": 11, "context": "Phrasing the maxloss minimization as a two players game has also been proposed by (Clarkson et al., 2012; Hazan et al., 2011).", "startOffset": 82, "endOffset": 125}, {"referenceID": 6, "context": "Assuming the setup of Example 1, (Clarkson et al., 2012) presents an algorithm that finds a consistent hypothesis in runtime of \u00d5((m + d) \u00b7 C).", "startOffset": 33, "endOffset": 56}, {"referenceID": 6, "context": "In any case, our bound is sometimes better and sometimes worse than the one in (Clarkson et al., 2012).", "startOffset": 79, "endOffset": 102}, {"referenceID": 3, "context": "See for example (Bengio & Sen\u00e9cal, 2008; Bouchard et al., 2015; Zhao & Zhang, 2014; Allen-Zhu & Yuan, 2015).", "startOffset": 16, "endOffset": 107}, {"referenceID": 10, "context": "Second, we apply an analysis of the sample complexity similar to the \u201cshell analysis\u201d of (Haussler et al., 1996), and assume that the error of all hypotheses in H1,\u270f on D2 is either smaller than \u270f or larger than c, where we would like to think of c as being significantly larger than \u270f.", "startOffset": 89, "endOffset": 112}, {"referenceID": 10, "context": "Second, we apply an alysis of the sample complexity similar to he \u201cshell analysis\u201d of (Haussler et al., 1996), and assume that the erro f all hypotheses in H1, on D2 is either smaller than or large than c, wher we would like to think of c as being sign ficantly larger than .", "startOffset": 86, "endOffset": 109}, {"referenceID": 7, "context": "For efficient implementations of this projection see for example (Duchi et al., 2008).", "startOffset": 65, "endOffset": 85}, {"referenceID": 18, "context": "We can further replace the constraint \u2016\u03be\u20161 \u2264 K with a constraint of \u2016\u03be\u20162 \u2264 K, because projection onto the Euclidean ball is a simple scaling, and the operation can be done efficiently with an adequate data structure (as described, for example, in (Shalev-Shwartz et al., 2011)).", "startOffset": 247, "endOffset": 276}], "year": 2016, "abstractText": "A commonly used learning rule is to approximately minimize the average loss over the training set. Other learning algorithms, such as AdaBoost and hard-SVM, aim at minimizing the maximal loss over the training set. The average loss is more popular, particularly in deep learning, due to three main reasons. First, it can be conveniently minimized using online algorithms, that process few examples at each iteration. Second, it is often argued that there is no sense to minimize the loss on the training set too much, as it will not be reflected in the generalization loss. Last, the maximal loss is not robust to outliers. In this paper we describe and analyze an algorithm that can convert any online algorithm to a minimizer of the maximal loss. We prove that in some situations better accuracy on the training set is crucial to obtain good performance on unseen examples. Last, we propose robust versions of the approach that can handle outliers.", "creator": "LaTeX with hyperref package"}}}