{"id": "1602.01783", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2016", "title": "Asynchronous Methods for Deep Reinforcement Learning", "abstract": "We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task involving finding rewards in random 3D mazes using a visual input.", "histories": [["v1", "Thu, 4 Feb 2016 18:38:41 GMT  (1848kb,D)", "http://arxiv.org/abs/1602.01783v1", null], ["v2", "Thu, 16 Jun 2016 16:38:45 GMT  (1883kb,D)", "http://arxiv.org/abs/1602.01783v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["volodymyr mnih", "adri\u00e0 puigdom\u00e8nech badia", "mehdi mirza", "alex graves", "timothy p lillicrap", "tim harley", "david silver", "koray kavukcuoglu"], "accepted": true, "id": "1602.01783"}, "pdf": {"name": "1602.01783.pdf", "metadata": {"source": "CRF", "title": "Asynchronous Methods for Deep Reinforcement Learning", "authors": ["Volodymyr Mnih", "Adri\u00e0 Puigdom\u00e8nech Badia", "Mehdi Mirza", "Alex Graves", "Tim Harley", "Timothy P. Lillicrap", "David Silver", "Koray Kavukcuoglu"], "emails": ["vmnih@google.com", "adriap@google.com", "mirzamom@iro.umontreal.ca", "gravesa@google.com", "tharley@google.com", "countzero@google.com", "davidsilver@google.com", "korayk@google.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, the fact is that we will be able to look for a solution that is capable, that we are able, that we are able, that we are able to find a solution, \"he said.\" We have never found a solution, \"he said."}, {"heading": "2 Related Work", "text": "The general Reinforcement Learning Architecture (Gorila) by Nair et al. [2015] performs asynchronous training of reinforcement learning agents in a distributed environment. In Gorila, each process includes an actor acting in his own copy of the environment, a separate replay memory and a learner collecting data from the replay memory and calculating gradients of DQN loss [Mnih et al., 2015] in terms of policy parameters. Gradations are sent asynchronously to a central parameter server, which updates a central copy of the model. Updated policy parameters are sent to the actor learners at fixed intervals. By using 100 separate actor learning processes and 30 parameter server instances, for a total of 130 CPU corners, Gorila was able to significantly surpass DQN over 49 Atari games. In many games, Gorila achieved the score, which was reached by DN 20 times faster than DN over DN."}, {"heading": "3 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Reinforcement Learning", "text": "We consider the standard amplification learning setting, in which an agent with an environment E is defined over a number of discrete time steps Q = Q = Q. At each time step Q, the agent receives a state st and selects an action on a number of possible actions A according to its policy \u03c0, where \u03c0 is a mapping of states st to actions at. In return, the agent receives the next state + 1 and receives a scalar reward rt. The process continues until the agent reaches a final state after which the process begins anew. The return Rt = \u2211 k = 0 \u03b3krt + k is the total accumulated return from the time step t with discount factor \u03b3 (0, 1]. The objective of the agent is to maximize the expected return from each state.The action value Q\u03c0 (a) = E [Rt | st = s, a] is the expected return for the selection of actions in a state s and the following policy."}, {"heading": "3.2 Deep Q Networks", "text": "The recently introduced variant of Q-Learning to train Deep Q Networks [Mnih et al., 2015] used two techniques to avoid such discrepancies in practice. First, an experiential replay storage mechanism based on Lin [1993] was used to perform Q-Learning updates on random samples of past experiences rather than the most recent examples. Experiential replay reduces the correlations between successive updates applied to the network, making the training data less stationary. Second, the network used to calculate Q-Learning targets was fixed for intervals of a few thousand updates, after which it would be updated with the current weights of Q (s, a). This technique of using a target network reduces the correlations between the target and the predicted Q values, making the training problem less stationary. Loss of Q (s, a) function is lessened by experience (a)."}, {"heading": "4 Asynchronous Lock-Free Reinforcement Learning", "text": "The goal in designing these methods was to find RL algorithms that could reliably train deep strategies for neural networks without large resource requirements. While the underlying RL methods are quite different, with actuator-critical methods being a method of political search and Q-learning used outside politics, we are using two main ideas to make all four algorithms practicable in line with our design goal. First, we are using asynchronous actor-learners, as proposed in the Gorila framework [Nair et al., 2015], but instead of using separate machines and a parameter server, we are using multiple threads on a single machine. If we keep the learners on a single machine, we shift the communication costs by sending gradients and parameters and enabling us to use Hogwild. [Recht et al, 2011] Style updates for the training of controllers. Second, we are observing that multiple actors are being executed."}, {"heading": "4.1 Asynchronous one-step Q-learning", "text": "Pseudo-code for our variant of Q-Learning, which we call Asynchronous One-Step Q-Learning, is shown in Algorithm 1. Each thread interacts with its own copy of the environment and calculates a gradient of Q-Learning loss at each step. We use a common and slowly changing target network to calculate Q-Learning loss, as suggested in the DQN Training Method. We also accumulate gradients over several periods of time before they are applied, which is similar to the use of minibatches, reducing the likelihood of multi-actor learners overwriting each other's updates in the Hogwild! setting. Accumulating updates over multiple steps also provides some ability to trade computing efficiency for data efficiency.Finally, we found that it helps give each thread a different exploration policy to improve robustness."}, {"heading": "4.2 Asynchronous one-step Sarsa", "text": "The asynchronous single-stage Sarsa algorithm corresponds to the asynchronous single-stage Q-learning as specified in algorithm 1, except that it uses a different target value for Q (s, a), the target value used by a single-stage Sarsa isy = {r for terminal s (s) + \u03b3Q (s) for non-terminal s (6), where a \"is the action performed in state s (Sutton and Barto, 1998). We again use a target network and updates accumulated over multiple periods to stabilize learning."}, {"heading": "4.3 Asynchronous n-step Q-learning", "text": "The pseudo-code for our variant of multi-level Q-learning is shown in Algorithm 2. The algorithm is somewhat unusual because it works in forward view by explicitly calculating n-step returns, as opposed to the more frequent backward view used by techniques such as permission tracing [Sutton and Barto, 1998]. We found that using forward view is easier when neural networks are trained with impulse-based methods and backward propagation over time. To calculate a single update, the algorithm first selects actions with its exploration policy for up to tmax steps or until reaching a terminal state. This process results in the agent receiving up to tmax rewards from the environment since its last update. The algorithm then calculates gradients for n-level Q-learning updates for each of the status pairs encountered since the last update, leading to a total two-level upgrade, which leads to a total two-level return applied to the longest possible update."}, {"heading": "4.4 Asynchronous advantage actor-critic", "text": "Our asynchronous variant of the Actor-Critic is presented in Algorithm 3. The algorithm we call asynchronous Advantage Critic (A3C) maintains a policy \u03c0 (at | st; \u03b8) and an estimate of the value function V (st; empirical). Like our variant of the n-step Q-Learning, our variant of the Actor-Critic also operates in the forward perspective and uses the same mixture of n-step returns to update both the policy and the value function. Politics and the value function are updated after each tmax action or when a final state is reached. The update performed by the algorithm can be regarded as a log (at | st; empirical) A (at; empirical, empirical, empirical, empirical, empirical) where A (st, at; empirical, empirical) is an estimate of the benefit function we give."}, {"heading": "4.5 Optimization", "text": "We examined two different optimization algorithms using our asynchronous frame - stochastic gradient descent and RMSProp. Our implementations of these algorithms do not use a latch to maximize throughput when a large number of threads is used. Momentum SGD: The implementation of SGD in an asynchronous environment is relatively simple and well studied [Recht et al., 2011]. Let us be the parameter vector shared across all threads, and let us show the cumulative gradients of loss in terms of parameters calculated by the thread count i. Each thread independently applies the default moment SGD update mi = \u03b1mi + (1 \u2212 \u03b1), followed by the thread elements that divide each other with the learning rate, dynamic \u03b1 and without locks. Note that in this setting each thread has its own gradient and the dynamics Provector.RSSSp: While we are optimizing the RM\u03b1, we are: While we are splitting the torque dynamics."}, {"heading": "5 Experiments", "text": "We use four different platforms to evaluate the properties of the proposed framework. First, the Arcade Learning Environment [Bellemare et al., 2012], which provides a simulator for Atari 2600 games. This is one of the most commonly used benchmark environments for RL algorithms. We compare with current results in this environment, as by Van Hasselt et al. [2015], Wang et al. [2015], Schaul et al. [2015], Nair et al. [2015] and Mnih et al. [2015]. The second environment we use is the TORCS car racing simulator [Wymann et al., 2013]. TORCS is a 3D simulator where the graphics are more realistic than in Atari and in addition understanding the physics of the car is an important component. The third environment we use to report results is the MuJoCo [Todorov et al., 2015] physics simulator, in which the graphics are more realistic than in Atari and in which, in addition, understanding the physics of the car is an important component. The third environment we use to report results is the MuJoCo [Todorov et al., 2015] physics simulator in which the graphics are more realistic than in Atari and in which the graphics are an important component."}, {"heading": "5.1 Experimental Setup", "text": "The experiments performed on a subset of Atari games (Figures 1, 6 \u03b2, 7 and Table 2), as well as the TORCS experiments (Figure 2) used the following setup: Each experiment used 16 actor-learner threads running on a single machine and without GPUs; all methods performed updates after 5 actions each (tmax = 5 and IUpdate = 5) and shared RMSProp was used for optimization; the three asynchronous value-based methods used a common target network that was updated every 40,000 frames; the Atari experiments used the same input preprocessing as Mnih et al. [2015] and an action repetition of 4; the agents used the network architecture of Mnih et al. [2013]; the network used a revolutionary layer of 16 filters of size 8 x 8 with step 4, followed by a constellation rate, followed by a continuous layer of 32 with a unificity layer followed by 25x with a unificity layer of size 4."}, {"heading": "5.2 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.2.1 Atari 2600 Games", "text": "We first present the results on a subset of Atari 2600 games to demonstrate the training speed of the new methods. Figure 1 compares the learning speed of the DQN algorithm on an Nvidia K40 GPU with the asynchronous methods trained with 16 CPU cores on five Atari 2600 games. Results show that all four asynchronous methods we have presented can successfully train neural network controllers on the Atari domain. The asynchronous methods tend to learn faster than DQN, with significantly faster learning on some games, while training on only 16 CPU cores. Furthermore, the results suggest that n-step methods are actually trained faster than one-step methods. Overall, the policy-based advantage actorcritical method significantly outperforms all three value-based methods. We then rated asynchronous advantage actor-critics on only 16 CPU cores."}, {"heading": "5.2.2 TORCS Car Racing Simulator", "text": "We also compared the four asynchronous methods of the car racing game TORCS 3D [Wymann et al., 2013]. TORCS not only has more realistic graphics than Atari 2600 games, but also requires the agent to learn the dynamics of the car he controls. At each step, an agent received only one visual input in the form of an RGB image of the current frame, as well as a reward that is proportional to the speed of the agent along the middle of the track in the current position of the agent. This reward structure is very different from most Atari games, where the rewards are usually very sparse. We used the same neural network architecture as in the Atari experiments according to Section 5.1. We conducted experiments with four different settings - the agent driving a slow car with and without opposing bots, and the agent driving a fast car with and without opposing bots. The results for the different game configurations are shown, all in four algorithms."}, {"heading": "5.2.3 Continuous Action Control Using the MuJoCo Physics Simulator", "text": "Finally, we examined a number of tasks where the action space is continuous. In particular, we look at a number of rigid body physics domains with contact dynamics, in which the tasks include a variety of examples of manipulation and locomotion, which were simulated in the Mujoco physics engine, although the action space for the Atari domains is of course discrete and TORCS requires a small discretion of the action space, which is straightforward and has proven successful. However, there are many problems for which the discretion of the action space is unlikely to be a good strategy. For example, if a problem requires the control of a system with 10 independently controlled common torques, then even a very rough discretion of the action space in 5 values for each joint guidance to 510 discrete actions is necessary. Due to this fact, the DQN algorithm (or any algorithm that relies on a maximum operator via actions) cannoteasy can be applied to continuous control problems with even moderately large action spaces."}, {"heading": "5.2.4 Labyrinth", "text": "At the beginning of each episode, the agent was placed in a new randomly generated labyrinth of rooms and corridors. Each labyrinth contained two types of objects that the agent rewarded for finding - apples and portals. Finding the agent resulted in a reward of 1. Entering a portal resulted in a reward of 10 points, after which the agent reappeared in a new random place in the labyrinth and all previously collected apples were regenerated. An episode ended after 60 seconds, whereupon a new episode would begin. The agent's goal is to collect as many points as possible within the time limit, and the optimal strategy includes first finding the portal and then returning there after each repetition. This task is much more challenging than the TORCS driving domain, because the agent is faced with a new labyrinth and the optimal strategy is to first explore the portal and then to consider the best one after each repetition."}, {"heading": "5.3 Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.3.1 Scalability and Data Efficiency", "text": "We are now analyzing the effectiveness of our proposed framework by looking at how training time and data efficiency changes with the number of parallel actor-learner levels. Therefore, if we deploy multiple workers in parallel and update a common model, one would ideally expect the total number of training steps to reach a safe point with different numbers of workers to remain the same. Therefore, the advantage would be solely in the system's ability to consume more data in the same amount of wall clock time and possibly improved exploration. Table 2 shows the training speed achieved through the use of an increasing number of parallel actor-learners on average over seven Atari games. These results show that all four methods achieve considerable acceleration through the use of multiple threads, with 16 threads leading to at least an order of speed acceleration, confirming that our proposed framework scales well with the number of people working in parallel, requiring efficient use of resources."}, {"heading": "5.3.2 Robustness and Stability", "text": "Smarts1, Smarts1, Smarts2, Smarts2, Smarts2, Smarts2, Smarts2, Smarts2, Smarts2, Smarts2, Smarts2, Smarts2, Smarts2, Smarts2, Smarts2, Smarts2, Smarts2, Smarts2, Smarts2, Smarts2, Smarts2, Smarts2, Smarts2, Smarts2, Smarts2, Smarts2, Smarts2, Smarts2, Smarts2, Smarts2, Smarts2, Smarts2, Smarts2, Smarts2, Smarts2, Smarts2, Smarts2, Smarts2, Smarts2, Smarts2, Smarts2"}, {"heading": "6 Conclusions and Discussion", "text": "In his opinion, it is important that we are able to establish ourselves in the region, and that we are able to establish ourselves in the region. \"In his opinion, it is important that we are able to establish ourselves in the region, and that we are able to establish ourselves in the region."}, {"heading": "Acknowledgments", "text": "We thank Thomas Degris, Remi Munos, Marc Lanctot, Sasha Vezhnevets and Joseph Modayil for many helpful discussions, suggestions and comments on the essay. We also thank the DeepMind evaluation team for setting up the environments in which the agents in the essay are evaluated."}], "references": [{"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["Marc G Bellemare", "Yavar Naddaf", "Joel Veness", "Michael Bowling"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bellemare et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2012}, {"title": "Increasing the action gap: New operators for reinforcement learning", "author": ["Marc G. Bellemare", "Georg Ostrovski", "Arthur Guez", "Philip S. Thomas", "R\u00e9mi Munos"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,", "citeRegEx": "Bellemare et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2016}, {"title": "Distributed dynamic programming", "author": ["Dimitri P Bertsekas"], "venue": "Automatic Control, IEEE Transactions on,", "citeRegEx": "Bertsekas.,? \\Q1982\\E", "shortCiteRegEx": "Bertsekas.", "year": 1982}, {"title": "Distributed deep q-learning", "author": ["Kevin Chavez", "Hao Yi Ong", "Augustus Hong"], "venue": "Technical report,", "citeRegEx": "Chavez et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chavez et al\\.", "year": 2015}, {"title": "Model-free reinforcement learning with continuous action in practice", "author": ["Thomas Degris", "Patrick M Pilarski", "Richard S Sutton"], "venue": "In American Control Conference (ACC),", "citeRegEx": "Degris et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Degris et al\\.", "year": 2012}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Parallel reinforcement learning with linear function approximation. In Proceedings of the 5th, 6th and 7th European Conference on Adaptive and Learning Agents and Multi-agent Systems: Adaptation and Multi-agent Learning, pages 60\u201374", "author": ["Matthew Grounds", "Daniel Kudenko"], "venue": null, "citeRegEx": "Grounds and Kudenko.,? \\Q2008\\E", "shortCiteRegEx": "Grounds and Kudenko.", "year": 2008}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "On the convergence of stochastic iterative dynamic programming algorithms", "author": ["Tommi Jaakkola", "Michael I Jordan", "Satinder P Singh"], "venue": "Neural computation,", "citeRegEx": "Jaakkola et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Jaakkola et al\\.", "year": 1994}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Evolving deep unsupervised convolutional networks for vision-based reinforcement learning", "author": ["Jan Kout\u0144\u0131k", "J\u00fcrgen Schmidhuber", "Faustino Gomez"], "venue": "In Proceedings of the 2014 conference on Genetic and evolutionary computation,", "citeRegEx": "Kout\u0144\u0131k et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kout\u0144\u0131k et al\\.", "year": 2014}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Sergey Levine", "Chelsea Finn", "Trevor Darrell", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1504.00702,", "citeRegEx": "Levine et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2015}, {"title": "Mapreduce for parallel reinforcement learning", "author": ["Yuxi Li", "Dale Schuurmans"], "venue": "EWRL", "citeRegEx": "Li and Schuurmans.,? \\Q2011\\E", "shortCiteRegEx": "Li and Schuurmans.", "year": 2011}, {"title": "Continuous control with deep reinforcement learning", "author": ["Timothy P Lillicrap", "Jonathan J Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Reinforcement learning for robots using neural networks", "author": ["Long-Ji Lin"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Lin.,? \\Q1993\\E", "shortCiteRegEx": "Lin.", "year": 1993}, {"title": "Playing atari with deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller"], "venue": "In NIPS Deep Learning Workshop", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A. Rusu", "Joel Veness", "Marc G. Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K. Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Massively parallel methods for deep reinforcement learning", "author": ["Arun Nair", "Praveen Srinivasan", "Sam Blackwell", "Cagdas Alcicek", "Rory Fearon", "Alessandro De Maria", "Vedavyas Panneershelvam", "Mustafa Suleyman", "Charles Beattie", "Stig Petersen", "Shane Legg", "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver"], "venue": "In ICML Deep Learning Workshop", "citeRegEx": "Nair et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2015}, {"title": "Incremental multi-step q-learning", "author": ["Jing Peng", "Ronald J Williams"], "venue": "Machine Learning,", "citeRegEx": "Peng and Williams.,? \\Q1996\\E", "shortCiteRegEx": "Peng and Williams.", "year": 1996}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["Benjamin Recht", "Christopher Re", "Stephen Wright", "Feng Niu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Recht et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Recht et al\\.", "year": 2011}, {"title": "Neural fitted q iteration\u2013first experiences with a data efficient neural reinforcement learning method", "author": ["Martin Riedmiller"], "venue": "In Machine Learning: ECML", "citeRegEx": "Riedmiller.,? \\Q2005\\E", "shortCiteRegEx": "Riedmiller.", "year": 2005}, {"title": "On-line q-learning using connectionist systems", "author": ["Gavin A Rummery", "Mahesan Niranjan"], "venue": null, "citeRegEx": "Rummery and Niranjan.,? \\Q1994\\E", "shortCiteRegEx": "Rummery and Niranjan.", "year": 1994}, {"title": "Prioritized experience replay", "author": ["Tom Schaul", "John Quan", "Ioannis Antonoglou", "David Silver"], "venue": "arXiv preprint arXiv:1511.05952,", "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Trust region policy optimization", "author": ["John Schulman", "Sergey Levine", "Philipp Moritz", "Michael I Jordan", "Pieter Abbeel"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Highdimensional continuous control using generalized advantage estimation", "author": ["John Schulman", "Philipp Moritz", "Sergey Levine", "Michael Jordan", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1506.02438,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Convergence results for single-step on-policy reinforcement-learning algorithms", "author": ["Satinder Singh", "Tommi Jaakkola", "Michael L Littman", "Csaba Szepesv\u00e1ri"], "venue": "Machine Learning,", "citeRegEx": "Singh et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2000}, {"title": "Reinforcement Learning: an Introduction", "author": ["R. Sutton", "A. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton.", "year": 2012}, {"title": "MuJoCo: Modeling, Simulation and Visualization of Multi-Joint Dynamics with Contact (ed 1.0)", "author": ["E Todorov"], "venue": "Roboti Publishing,", "citeRegEx": "Todorov.,? \\Q2015\\E", "shortCiteRegEx": "Todorov.", "year": 2015}, {"title": "Parallel and distributed evolutionary algorithms: A review", "author": ["Marco Tomassini"], "venue": "Technical report,", "citeRegEx": "Tomassini.,? \\Q1999\\E", "shortCiteRegEx": "Tomassini.", "year": 1999}, {"title": "An analysis of temporal-difference learning with function approximation", "author": ["J. Tsitsiklis", "B. Van Roy"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Tsitsiklis and Roy.,? \\Q1997\\E", "shortCiteRegEx": "Tsitsiklis and Roy.", "year": 1997}, {"title": "Asynchronous stochastic approximation and q-learning", "author": ["John N Tsitsiklis"], "venue": "Machine Learning,", "citeRegEx": "Tsitsiklis.,? \\Q1994\\E", "shortCiteRegEx": "Tsitsiklis.", "year": 1994}, {"title": "Deep reinforcement learning with double q-learning", "author": ["Hado Van Hasselt", "Arthur Guez", "David Silver"], "venue": "arXiv preprint arXiv:1509.06461,", "citeRegEx": "Hasselt et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2015}, {"title": "True Online Temporal-Difference Learning", "author": ["H. van Seijen", "A. Rupam Mahmood", "P.M. Pilarski", "M.C. Machado", "R.S. Sutton"], "venue": null, "citeRegEx": "Seijen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Seijen et al\\.", "year": 2015}, {"title": "Dueling Network Architectures for Deep Reinforcement Learning", "author": ["Z. Wang", "N. de Freitas", "M. Lanctot"], "venue": "ArXiv e-prints,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Learning from delayed rewards", "author": [], "venue": "PhD thesis,", "citeRegEx": "Watkins.,? \\Q1989\\E", "shortCiteRegEx": "Watkins.", "year": 1989}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine Learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Function optimization using connectionist reinforcement learning algorithms", "author": ["Ronald J Williams", "Jing Peng"], "venue": "Connection Science,", "citeRegEx": "Williams and Peng.,? \\Q1991\\E", "shortCiteRegEx": "Williams and Peng.", "year": 1991}, {"title": "Adadelta: An adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 17, "context": ", 2015] or massively distributed architectures [Nair et al., 2015], our experiments run on a single machine with a standard multi-core CPU.", "startOffset": 47, "endOffset": 66}, {"referenceID": 16, "context": "In Gorila, each process contains an actor that acts in its own copy of the environment, a separate replay memory, and a learner that samples data from the replay memory and computes gradients of the DQN loss [Mnih et al., 2015] with respect to the policy parameters.", "startOffset": 208, "endOffset": 227}, {"referenceID": 12, "context": "The General Reinforcement Learning Architecture (Gorila) of Nair et al. [2015] performs asynchronous training of reinforcement learning agents in a distributed setting.", "startOffset": 60, "endOffset": 79}, {"referenceID": 3, "context": "We also note that a similar way of parallelizing DQN was proposed by Chavez et al. [2015]. In earlier work, Li and Schuurmans [2011] applied the Map Reduce framework to parallelizing batch reinforcement learning methods with linear function approximation.", "startOffset": 69, "endOffset": 90}, {"referenceID": 3, "context": "We also note that a similar way of parallelizing DQN was proposed by Chavez et al. [2015]. In earlier work, Li and Schuurmans [2011] applied the Map Reduce framework to parallelizing batch reinforcement learning methods with linear function approximation.", "startOffset": 69, "endOffset": 133}, {"referenceID": 3, "context": "We also note that a similar way of parallelizing DQN was proposed by Chavez et al. [2015]. In earlier work, Li and Schuurmans [2011] applied the Map Reduce framework to parallelizing batch reinforcement learning methods with linear function approximation. Parallelism was used to speed up large matrix operations but not to parallelize the collection of experience or stabilize learning. Grounds and Kudenko [2008] proposed a parallel version of the Sarsa algorithm that uses multiple separate actor-learners to accelerate training.", "startOffset": 69, "endOffset": 415}, {"referenceID": 29, "context": "Another related area of work is in evolutionary methods, which are often straightforward to parallelize by distributing fitness evaluations over multiple machines or threads [Tomassini, 1999].", "startOffset": 174, "endOffset": 191}, {"referenceID": 2, "context": "Even earlier, Bertsekas [1982] studied the related problem of distributed dynamic programming.", "startOffset": 14, "endOffset": 31}, {"referenceID": 2, "context": "Even earlier, Bertsekas [1982] studied the related problem of distributed dynamic programming. Another related area of work is in evolutionary methods, which are often straightforward to parallelize by distributing fitness evaluations over multiple machines or threads [Tomassini, 1999]. Such parallel evolutionary approaches have recently been applied to some visual reinforcement learning tasks. In one example, Kout\u0144\u0131k et al. [2014] evolved convolutional neural network controllers for the TORCS driving simulator by performing fitness evaluations on 8 CPU cores in parallel.", "startOffset": 14, "endOffset": 436}, {"referenceID": 36, "context": "It is possible to reduce the variance of this estimate while keeping it unbiased by subtracting a learned function of the state bt(st), known as a baseline [Williams, 1992], from the return \u2207\u03b8 log \u03c0(at|st; \u03b8) (Rt \u2212 bt(st)) .", "startOffset": 156, "endOffset": 172}, {"referenceID": 36, "context": "One example of such a method is the REINFORCE family of algorithms due to Williams [1992]. Standard REINFORCE updates the policy parameters \u03b8 in the direction \u2207\u03b8 log \u03c0(at|st; \u03b8)Rt, which is an unbiased estimate of \u2207\u03b8E[Rt].", "startOffset": 74, "endOffset": 90}, {"referenceID": 30, "context": "Temporal difference learning methods, such as Q-learning, have been known to diverge when used with nonlinear function approximators [Tsitsiklis and Roy, 1997].", "startOffset": 133, "endOffset": 159}, {"referenceID": 16, "context": "The recently introduced variant of Q-learning for training Deep Q Networks [Mnih et al., 2015] made use of two techniques for avoiding such divergences in practice.", "startOffset": 75, "endOffset": 94}, {"referenceID": 14, "context": "First, an experience replay memory mechanism due to Lin [1993] was used to perform Q-learning updates on random samples of past experience instead on the most recent samples of experience.", "startOffset": 52, "endOffset": 63}, {"referenceID": 17, "context": "First, we use asynchronous actor-learners as proposed in the Gorila framework [Nair et al., 2015], but instead of using separate machines and a parameter server, we use multiple threads on a single machine.", "startOffset": 78, "endOffset": 97}, {"referenceID": 19, "context": "Keeping the learners on a single machine removes the communication costs incurred by sending gradients and parameters and enables us to use Hogwild! [Recht et al., 2011] style updates for training the controllers.", "startOffset": 149, "endOffset": 169}, {"referenceID": 26, "context": "where a\u2032 is the action taken in state s\u2032 [Sutton and Barto, 1998].", "startOffset": 41, "endOffset": 65}, {"referenceID": 26, "context": "The algorithm is somewhat unusual because it operates in the forward view by explicitly computing n-step returns, as opposed to the more common backward view used by techniques like eligibility traces [Sutton and Barto, 1998].", "startOffset": 201, "endOffset": 225}, {"referenceID": 37, "context": "This technique was originally proposed by [Williams and Peng, 1991], who found that it was particularly helpful on tasks requiring hierarchical behavior.", "startOffset": 42, "endOffset": 67}, {"referenceID": 19, "context": "Momentum SGD: The implementation of SGD in an asynchronous setting is relatively straightforward and well studied [Recht et al., 2011].", "startOffset": 114, "endOffset": 134}, {"referenceID": 27, "context": "RMSProp: While RMSProp [Tieleman and Hinton, 2012] has been widely used in the deep learning literature, it has not been extensively studied in the asynchronous optimization setting.", "startOffset": 23, "endOffset": 50}, {"referenceID": 0, "context": "First, the Arcade Learning Environment [Bellemare et al., 2012] that provides a simulator for Atari 2600 games.", "startOffset": 39, "endOffset": 63}, {"referenceID": 28, "context": "The third environment we use to report results is the MuJoCo [Todorov, 2015] physics simulator for evaluating agents on continuous motor control tasks with contact dynamics.", "startOffset": 61, "endOffset": 76}, {"referenceID": 0, "context": "First, the Arcade Learning Environment [Bellemare et al., 2012] that provides a simulator for Atari 2600 games. This is one of the most commonly used benchmark environments for RL algorithms. We compare against state of the art results on this environment as reported by Van Hasselt et al. [2015], Wang et al.", "startOffset": 40, "endOffset": 297}, {"referenceID": 0, "context": "First, the Arcade Learning Environment [Bellemare et al., 2012] that provides a simulator for Atari 2600 games. This is one of the most commonly used benchmark environments for RL algorithms. We compare against state of the art results on this environment as reported by Van Hasselt et al. [2015], Wang et al. [2015], Schaul et al.", "startOffset": 40, "endOffset": 317}, {"referenceID": 0, "context": "First, the Arcade Learning Environment [Bellemare et al., 2012] that provides a simulator for Atari 2600 games. This is one of the most commonly used benchmark environments for RL algorithms. We compare against state of the art results on this environment as reported by Van Hasselt et al. [2015], Wang et al. [2015], Schaul et al. [2015], Nair et al.", "startOffset": 40, "endOffset": 339}, {"referenceID": 0, "context": "First, the Arcade Learning Environment [Bellemare et al., 2012] that provides a simulator for Atari 2600 games. This is one of the most commonly used benchmark environments for RL algorithms. We compare against state of the art results on this environment as reported by Van Hasselt et al. [2015], Wang et al. [2015], Schaul et al. [2015], Nair et al. [2015] and Mnih et al.", "startOffset": 40, "endOffset": 359}, {"referenceID": 0, "context": "First, the Arcade Learning Environment [Bellemare et al., 2012] that provides a simulator for Atari 2600 games. This is one of the most commonly used benchmark environments for RL algorithms. We compare against state of the art results on this environment as reported by Van Hasselt et al. [2015], Wang et al. [2015], Schaul et al. [2015], Nair et al. [2015] and Mnih et al. [2015]. The second environment we use is the TORCS car racing simulator [Wymann et al.", "startOffset": 40, "endOffset": 382}, {"referenceID": 15, "context": "The Atari experiments used the same input preprocessing as Mnih et al. [2015] and an action repeat of 4.", "startOffset": 59, "endOffset": 78}, {"referenceID": 15, "context": "The Atari experiments used the same input preprocessing as Mnih et al. [2015] and an action repeat of 4. The agents used the network architecture from Mnih et al. [2013]. The network used a convolutional layer with 16 filters of size 8 \u00d7 8 with stride 4, followed by a convolutional layer with with 32 filters of size 4 \u00d7 4 with stride 2, followed by a fully connected layer with 256 hidden units.", "startOffset": 59, "endOffset": 170}, {"referenceID": 32, "context": "In order to compare with the state of the art in Atari game playing, we largely followed the training and evaluation protocol of Van Hasselt et al. [2015]. Specifically, we tuned hyperparameters", "startOffset": 133, "endOffset": 155}, {"referenceID": 17, "context": "Method Training Time Mean Median DQN (from [Nair et al., 2015]) 8 days on GPU 121.", "startOffset": 43, "endOffset": 62}, {"referenceID": 17, "context": "5% Gorila [Nair et al., 2015] 4 days, 100 machines 215.", "startOffset": 10, "endOffset": 29}, {"referenceID": 34, "context": "9% Dueling Double DQN [Wang et al., 2015] 8 days on GPU 343.", "startOffset": 22, "endOffset": 41}, {"referenceID": 22, "context": "1% Prioritized DQN [Schaul et al., 2015] 8 days on GPU 463.", "startOffset": 19, "endOffset": 40}, {"referenceID": 7, "context": ", 2015] as well as a recurrent agent with an additional 256 LSTM [Hochreiter and Schmidhuber, 1997] cells after the final hidden layer.", "startOffset": 65, "endOffset": 99}, {"referenceID": 34, "context": ", 2015] and Dueling Double DQN [Wang et al., 2015] can be incorporated to 1-step Q and n-step Q methods presented in this work with similar potential improvements.", "startOffset": 31, "endOffset": 50}, {"referenceID": 0, "context": "We additionally used the final network weights for evaluation to make the results more comparable to the original results from Bellemare et al. [2012]. We trained our agents for four days using 16 CPU cores, while the other agents were trained for 8 to 10 days on Nvidia K40 GPUs.", "startOffset": 127, "endOffset": 151}, {"referenceID": 13, "context": "the physics models and task objectives) are near identical to the tasks examined in [Lillicrap et al., 2015].", "startOffset": 84, "endOffset": 108}, {"referenceID": 5, "context": "Although there are many extensions such as ADAGRAD [Duchi et al., 2011],", "startOffset": 51, "endOffset": 71}, {"referenceID": 38, "context": "ADADELTA [Zeiler, 2012], RMSProp [Tieleman and Hinton, 2012] and ADAM [Kingma and Ba, 2014], there is no consensus as to which method is the best.", "startOffset": 9, "endOffset": 23}, {"referenceID": 27, "context": "ADADELTA [Zeiler, 2012], RMSProp [Tieleman and Hinton, 2012] and ADAM [Kingma and Ba, 2014], there is no consensus as to which method is the best.", "startOffset": 33, "endOffset": 60}, {"referenceID": 9, "context": "ADADELTA [Zeiler, 2012], RMSProp [Tieleman and Hinton, 2012] and ADAM [Kingma and Ba, 2014], there is no consensus as to which method is the best.", "startOffset": 70, "endOffset": 91}, {"referenceID": 26, "context": "in the forward view [Sutton and Barto, 1998] by using corrected n-step returns directly as targets, it has been more common to use the backward view to implicitly combine different returns through eligibility traces [Watkins, 1989, Sutton and Barto, 1998, Peng and Williams, 1996].", "startOffset": 20, "endOffset": 44}, {"referenceID": 15, "context": "in the forward view [Sutton and Barto, 1998] by using corrected n-step returns directly as targets, it has been more common to use the backward view to implicitly combine different returns through eligibility traces [Watkins, 1989, Sutton and Barto, 1998, Peng and Williams, 1996]. The asynchronous advantage actor-critic method could be potentially improved by using other ways of estimating the advantage function, such as generalized advantage estimation of Schulman et al. [2015b]. All of the value-based methods we investigated could benefit from different ways of reducing over-estimation bias of Q-values [Van Hasselt et al.", "startOffset": 256, "endOffset": 485}, {"referenceID": 0, "context": ", 2015, Bellemare et al., 2016]. Yet another, more speculative, direction is to try and combine the recent work on true online temporal difference methods [van Seijen et al., 2015] with nonlinear function approximation. In addition to these algorithmic improvements, a number of complementary improvements to the neural network architecture are possible. The dueling architecture of Wang et al. [2015] has been shown to produce more accurate estimates of Q-values by including separate streams for the state value and advantage in the network.", "startOffset": 8, "endOffset": 402}, {"referenceID": 0, "context": ", 2015, Bellemare et al., 2016]. Yet another, more speculative, direction is to try and combine the recent work on true online temporal difference methods [van Seijen et al., 2015] with nonlinear function approximation. In addition to these algorithmic improvements, a number of complementary improvements to the neural network architecture are possible. The dueling architecture of Wang et al. [2015] has been shown to produce more accurate estimates of Q-values by including separate streams for the state value and advantage in the network. The spatial softmax proposed by Levine et al. [2015] could improve both value-based and policy-based methods by making it easier for the network to represent feature coordinates.", "startOffset": 8, "endOffset": 597}, {"referenceID": 17, "context": "DQN scores taken from Nair et al. [2015]. Double DQN scores taken from Van Hasselt et al.", "startOffset": 22, "endOffset": 41}, {"referenceID": 17, "context": "DQN scores taken from Nair et al. [2015]. Double DQN scores taken from Van Hasselt et al. [2015], Dueling scores from Wang et al.", "startOffset": 22, "endOffset": 97}, {"referenceID": 17, "context": "DQN scores taken from Nair et al. [2015]. Double DQN scores taken from Van Hasselt et al. [2015], Dueling scores from Wang et al. [2015] and Prioritized scores taken from Schaul et al.", "startOffset": 22, "endOffset": 137}, {"referenceID": 17, "context": "DQN scores taken from Nair et al. [2015]. Double DQN scores taken from Van Hasselt et al. [2015], Dueling scores from Wang et al. [2015] and Prioritized scores taken from Schaul et al. [2015]", "startOffset": 22, "endOffset": 192}], "year": 2016, "abstractText": "We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task involving finding rewards in random 3D mazes using a visual input.", "creator": "LaTeX with hyperref package"}}}