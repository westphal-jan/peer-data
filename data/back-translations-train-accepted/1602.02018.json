{"id": "1602.02018", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Feb-2016", "title": "Compressive Spectral Clustering", "abstract": "Spectral clustering has become a popular technique due to its high performance in many contexts. It comprises three main steps: create a similarity graph between N objects to cluster, compute the first k eigenvectors of its Laplacian matrix to define a feature vector for each object, and run k-means on these features to separate objects into k classes. Each of these three steps becomes computationally intensive for large N and/or k. We propose to speed up the last two steps based on recent results in the emerging field of graph signal processing: graph filtering of random signals, and random sampling of bandlimited graph signals. We prove that our method, with a gain in computation time that can reach several orders of magnitude, is in fact an approximation of spectral clustering, for which we are able to control the error. We test the performance of our method on artificial and real-world network data.", "histories": [["v1", "Fri, 5 Feb 2016 13:42:27 GMT  (113kb,D)", "http://arxiv.org/abs/1602.02018v1", "15 pages, 2 figures"], ["v2", "Mon, 23 May 2016 13:21:56 GMT  (133kb,D)", "http://arxiv.org/abs/1602.02018v2", "12 pages, 2 figures"]], "COMMENTS": "15 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.DS cs.LG stat.ML", "authors": ["nicolas tremblay", "gilles puy", "r\u00e9mi gribonval", "pierre vandergheynst"], "accepted": true, "id": "1602.02018"}, "pdf": {"name": "1602.02018.pdf", "metadata": {"source": "CRF", "title": "Compressive spectral clustering", "authors": ["Nicolas Tremblay", "Gilles Puy", "R\u00e9mi Gribonval", "Pierre Vandergheynst"], "emails": [], "sections": [{"heading": null, "text": "Its laplac matrix defines a characteristic vector for each object, and perform k-means on these characteristics to separate objects into k-classes. Each of these three steps will require computational effort for large N and / or k. We propose to accelerate the last two steps based on the latest results in the emerging field of graph signal processing: graph filtering of random signals and random sampling of band-delimited graph signals. We test the performance of our method on artificial and real network data. 1. Introduction Spectral Clustering (SC) is a basic tool in data mining [1]. Given a set of N data points for which we are able to control the error. We test the performance of our method on artificial and real network data. 1. Introduction Spectral Clustering (SC) is a basic tool in data mining."}, {"heading": "1.1. Related work", "text": "Bypassing these bottlenecks has aroused considerable interest over the past decade. Several authors have proposed ideas to address the bottleneck in self-decomposition, e.g. by the power method [8, 9], by carefully optimizing the diagonalization algorithms in the context of SK [10], or by matrix column subsampling as in the Nystro subsampling method [11], the nSPEC and cSPEC methods of [12], or in [13, 14]. All of these methods aim to quickly calculate feature vectors, but the K means are still applied to N feature vectors. Other authors have suggested circumventing k means in high dimensions by applying some data from the available samples to SK's reduced similarity curve and interpolating the results to the full dataset."}, {"heading": "1.2. Contribution: compressive clustering", "text": "In this paper, inspired by recent advances in graph signal processing [20, 21], we circumvent the last two bottlenecks and detail a fast approximate spectral clustering method for large data sets as well as the supporting theory. We assume that the laplac matrix L-RN-N of G. Our method consists of two ingredients. The first ingredient builds on recent work [22,23], which avoids the costly calculation of the eigenvectors of L by filtering O- (log (k)))) random signals on G, which then serve as feature vectors to perform clustering. In this paper, we show how to integrate the effects of non-ideal but compressively efficient graph filters on the quality of the characteristic vectors used for clustering. The second ingredient uses a newer sample theory of limited-band graph signals [24] to reduce the complexity of notation."}, {"heading": "2.1. Graph signal processing", "text": "Let us make G = (V, E, W) an undirected graph with V the set of N nodes, E the set of edges, and W the weighted adjacence matrix, so that the weight of the edge between the nodes i and j.The graph Fourier matrix. Consider the strength of the graph i. L = I \u2212 D \u2212 1 / 2WD \u2212 1 / 2, where I am the identity matrix in dimension N, and D is diagonally with Dii = 6 = i Wij the strength of the graph i. L is a real symmetrical and positive semi-definite, therefore diagonal basis. Its spectrum is composed of its set of assorted eigenvalues 0 = 1."}, {"heading": "2.2. Spectral clustering", "text": "We choose here Ng et al.'s method [2] based on the normalized Laplacian as our standard SC method. The input is the adjacency matrix W, which represents the pair similarity of all N objects to Cluster1. After calculating his Laplacian L, follow Alg. 1 to find k classes. 3. Principles of CSCCompressive Spectral Clustering (CSC) bypasses two of the bottlenecks of SC, the partial diagonalization of the Laplacian and the high-dimensional k averages, thanks to the following ideas.1) Perform a controlled estimate D-ij of the spectral clustering distance Dij (see Eq (2)), without partially diagnosing the Laplacian, by fast filtering a few random signals with the polynomic approximation h-fine, which filter the optimal low-pass filter horizontally (see Eq."}, {"heading": "3.1. Ideal filtering of random signals", "text": "Definition 3.1 (Local cumulative coherence) Definition 3.1 (Local cumulative coherence) Definition 3.1 (Local cumulative coherence) Definition 3.1 (Local cumulative coherence) Definition 3.1 (Local cumulative coherence) Definition of the local cumulative coherence of the order k at the node i is2 vk (i) Definition of the diagonal matrix: Vk (i, i) Definition of the diagonal matrix: Vk (i, i) Definition of the diagonal matrix: Vk (i) Definition of the diagonal matrix: Vk (1) Definition of the matrix R = (1) Definition of the matrix R = (2) Definition of the matrix R = (i) Definition of the diagonal matrix: Vi)."}, {"heading": "3.2. Downsampling and interpolation", "text": "For j = 1,.., k, let us denote the basic truth indicator of the cluster Cj, i.e. (cj) i: = {1, if i-Cj, 0 otherwise, \u0441i-i-1,..., N}, by cj. To estimate cj, we could run k averages on the N attribute vectors {f-1,..., f-N}, as happened in [22, 23]. Nevertheless, this is still inefficient for large N. To further reduce computing costs, we propose to use k averages only on a small subset of n attribute vectors. The goal is then to derive the designations of all N nodes from the labels of the n sampled nodes. To this end, we need 1) a low-dimensional model that captures the regularity of the vectors cj, 2) to ensure that after sampling enough information is preserved to quickly restore the vectors, and to estimate their accuracy through an algorithm algorithm."}, {"heading": "3.2.1. The low-dimensional model", "text": "For a simple regular (with nodes of the same degree) graph of k-separated clusters, it is easy to verify that {c1,.., ck} form a set of orthogonal eigenvectors of L with eigenvalue 0. Therefore, all indicator vectors cj live in a span (UK). For general graphs, we assume that the indicator vectors cj live near a span (UK), i.e. the difference between any cj and its orthogonal projection to a span (UK) is small. Experiments in section 5 will confirm that it is a good model for recovering the cluster indicator vectors. In signal processing terms, cj can be said to be roughly k-limited, i.e. its k first graph Fourier coefficient carries most of its energy. Recently, there has been a sharp increase in interest in adapting classical sampling bandwidth theories to such limited graph signals [29]."}, {"heading": "3.2.2. Sampling and interpolation", "text": "The subset of attribute vectors is selected by setting up n-indices: = {\u03c91,..., \u03c9n} uniformly according to the random principle of {1,.., N} without substitution. The execution k-means on the subset of the characteristics {f-\u03c91,.., f-\u03c9n} thus results in clustering the n sampled nodes in k-clusters. We denote by crj-Rn the resulting low-dimensional indicator vectors. Our goal now is to recover cj from c-j. Consider that k-means is able to c1,..., ck-RN with the original series of characteristics {f1,.., fN} using the SC algorithm (otherwise CSC is doomed to failure from the outset). The results in [22,23] show that k-means are also able to identify the attribute vectors."}, {"heading": "3.2.3. How many features to sample?", "text": "It is not as if we were able to find such a solution. (...) It is not as if there were such a solution. (...) It is not as if there were such a solution. (...) It is not as if there were such a solution. (...). (...). (...). (...). (...). (...). (...). It is as if there were such a solution. (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...). (...).). (...). (...).). (...). (...). (...).). (...). (...).). (...). (...).). (...).). (...). (...).). (...). (...).). (...). (...).). (...). (...). (...).). (...). (...). (...).). (...).). (...). (...). (...). (...). (...).). (...).). (...). (...). (...). (...). (...). (...). (...). (...).).). (...). (...).). (...)."}, {"heading": "4.1. Algorithm", "text": "As for SC (see paragraph 2.2), the algorithm begins with the adjacence matrix W of a graph G. After calculating its laplac L, the CSC algorithm is rendered in Alg. 2. The output c-j (i) is non-binary and actually quantifies how many nodes i belong to cluster j, which is useful for a blurred partitioning. To obtain an exact partition of the nodes, we normalize each indicator vector c-j and assign the node i to the cluster j, for which c-j (i) / c-j-j-y is maximum. This is the procedure we follow in the experiments of Sec-5."}, {"heading": "4.2. Non-ideal filtering of random signals", "text": "In this section we improve theorem 3.2 by examining how the error of polynomial approximation extends to the spectral distance and taking into account the fact that k-means are executed on the reduced series of features (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p) p (p) p (f) p (p) p (p) p (f) p (p) p (p) p) p (p) p (f) p (p) p (p) p (p) p (f) p (p) p (p) p (p) p (p) p (f) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p) p (p) p (p) p (p) p (f) p (p) p (p) p (p) p (p) p (f) p (p (p) p (p) p) p (p (p (p) p (p (p) p (p) p) p (p (p) p (p (p) p (f) p (p) p (p (p) p (p (p) p (p (p) p) p (p (p) p (p (p) p (p) p (p) p (p (p) p (p) p (p (p) p) p (p (p) p (p (p) p (p) p (p) p (p (p) p (p) p (p) p (p (p) p (p) p) p (p (p) p (p (f) p) p (p (p) p (p) p (p)"}, {"heading": "4.3. Polynomial approximation and estimation of \u03bbk", "text": "In order to have such control in practice, one would have to use rational filters (ratio of two polynomials) to achieve an approximate efficiency. Such filters were introduced in the graph context [33], but they include another optimization step that would burden our main message. We prefer to simplify our analysis by using polynomials for which only the maximum error can be controlled. We write (10) em: = max (e1, e2) = sup \u03bb, [2]. The maximum (1, e2) effectiveness of polynomials is controlled by the use of polynomials."}, {"heading": "4.4. Complexity considerations", "text": "The complexity of steps 2, 3 and 5 of Alg. 2 are not detailed because they are insignificant compared to the others. First, it should be noted that the fast filtering of a graph signal costs O (p # E). Step 1 costs O (p # E logN) per iteration of the dichotomy, and step 4 costs O (p # E log n) (as d = O (log n). Step 7 requires the solution of Eq. (7) with the polynomial approximation of g1 (\u03bb), and step 4 costs O (\u03bb). If solved, then by conjugation gradients or gradient descent this step becomes a quick filter operation per4Recall, the p is the sequence of polynomia."}, {"heading": "5.1. The Stochastic Block Model", "text": "What distinguishes the SBM from the Erdos-Renyi graphs is that the probability of a connection between two nodes i and j is not uniform, but from the common denomination of i and j. Specifically, the probability of a connection between the nodes i and j is equal to q1 if they are in the same community, and q2 if not. In a first approach, we look at graphs with k communities, all of which have the same size N / k. Furthermore, instead of considering the probabilities, one can fully characterize an SBM by indicating its ratio = q2q1 and the average degree s of the graph. The larger, the more difficult it is to detect the common structure. In fact, Decelle et al. [41] show that there is a critical value c above which it is impossible to detect collectively at the large N boundary: c = (s \u2212 \u221a s) / (s \u2212 s (k \u2212 1)))."}, {"heading": "5.2. Performance results", "text": "In this scenario, there is no theoretical value beyond which recovery in the large N limit is impossible. Instead, we compare the recovery performance of CSC versus SC for various parameters. Performance is measured by the Adjusted Rand similarity index [42] between the SBM's basic truth and the partitions obtained. The higher it is, the better the reconstruction. These figures show that the performance of CSC is saturated by the default values of n, d, p and g (see above of Alg. 2). We also conduct experiments on an SBM with N = 103, k = 20, s = 16 and heterogeneous community sizes. More specifically, the list of community sizes is chosen to be: 5, 10, 15, 25, 40, 45, 50, 55, 60, 65, 75, 80, 90 and 95 nodes. In this scenario, there is no theoretical value beyond which recovery is impossible."}], "references": [{"title": "Spectral methods for graph clustering a survey", "author": ["M. Nascimento", "A. de Carvalho"], "venue": "European Journal of Operational Research, vol. 211, no. 2, pp. 221 \u2013 231, 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["A. Ng", "M. Jordan", "Y. Weiss"], "venue": "Advances in Neural Information Processing Systems 14, T. Dietterich, S. Becker, and Z. Ghahramani, Eds. MIT Press, 2002, pp. 849\u2013856.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2002}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 22, no. 8, pp. 888\u2013905, 2000.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural computation, vol. 15, no. 6, pp. 1373\u20131396, 2003.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Self-tuning spectral clustering", "author": ["L. Zelnik-Manor", "P. Perona"], "venue": "Advances in neural information processing systems, 2004, pp. 1601\u20131608.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Algebraic connectivity of graphs", "author": ["M. Fiedler"], "venue": "Czechoslovak mathematical journal, vol. 23, no. 2, pp. 298\u2013305, 1973.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1973}, {"title": "A spectral clustering approach to finding communities in graph.", "author": ["S. White", "P. Smyth"], "venue": "in SDM,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Spectral clustering via the power method - provably.", "author": ["C.K.P. Boutsidis", "A. Gittens"], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML-15),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Power iteration clustering.", "author": ["F. Lin", "W.W. Cohen"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), Haifa, Israel,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Fast large-scale spectral clustering by sequential shrinkage optimization", "author": ["T.-Y. Liu", "H.-Y. Yang", "X. Zheng", "T. Qin", "W.-Y. Ma"], "venue": "Advances in Information Retrieval, 2007, pp. 319\u2013330.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Spectral grouping using the nystrom method", "author": ["C. Fowlkes", "S. Belongie", "F. Chung", "J. Malik"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 26, no. 2, pp. 214\u2013225, 2004.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "Approximate spectral clustering", "author": ["L. Wang", "C. Leckie", "K. Ramamohanarao", "J. Bezdek"], "venue": "Advances in Knowledge Discovery and Data Mining, 2009, pp. 134\u2013146.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Large scale spectral clustering with landmark-based representation.", "author": ["X. Chen", "D. Cai"], "venue": "Proceedings of the 25th AAAI Conference on Artificial Intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Fast spectral clustering with random projection and sampling", "author": ["T. Sakai", "A. Imiya"], "venue": "Machine Learning and Data Mining in Pattern Recognition, 2009, pp. 372\u2013384.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast approximate spectral clustering", "author": ["D. Yan", "L. Huang", "M. Jordan"], "venue": "Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD \u201909, New York, NY, USA, 2009, pp. 907\u2013916.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Weighted graph cuts without eigenvectors a multilevel approach", "author": ["I. Dhillon", "Y. Guan", "B. Kulis"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 29, no. 11, pp. 1944\u20131957, 2007.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1944}, {"title": "A survey of kernel and spectral methods for clustering", "author": ["M. Filippone", "F. Camastra", "F. Masulli", "S. Rovetta"], "venue": "Pattern Recognition, vol. 41, no. 1, pp. 176 \u2013 190, 2008.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Stochastic dimensionality reduction for k-means clustering", "author": ["C. Boutsidis", "A. Zouzias", "M.W. Mahoney", "P. Drineas"], "venue": "arXiv, abs/1110.2897, 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Dimensionality reduction for k-means clustering and low rank approximation", "author": ["M.B. Cohen", "S. Elder", "C. Musco", "C. Musco", "M. Persu"], "venue": "Proceedings of the 47th Annual ACM on Symposium on Theory of Computing. ACM, 2015, pp. 163\u2013172.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains", "author": ["D. Shuman", "S. Narang", "P. Frossard", "A. Ortega", "P. Vandergheynst"], "venue": "Signal Processing Magazine, IEEE, vol. 30, no. 3, pp. 83\u201398, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Big data analysis with signal processing on graphs: Representation and processing of massive data sets with irregular structure", "author": ["A. Sandryhaila", "J. Moura"], "venue": "Signal Processing Magazine, IEEE, vol. 31, no. 5, pp. 80\u201390, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Accelerated spectral clustering using graph filtering of random signals", "author": ["N. Tremblay", "G. Puy", "P. Borgnat", "R. Gribonval", "P. Vandergheynst"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), IEEE International Conference on, 2016, accepted.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Compressive spectral embedding: sidestepping the SVD", "author": ["D. Ramasamy", "U. Madhow"], "venue": "Advances in Neural Information Processing Systems 28, 2015, pp. 550\u2013558.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Random sampling of bandlimited signals on graphs", "author": ["G. Puy", "N. Tremblay", "R. Gribonval", "P. Vandergheynst"], "venue": "arXiv, vol. abs/1511.05118, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Spectral graph theory", "author": ["F. Chung"], "venue": "Amer Mathematical Society,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1997}, {"title": "Discrete signal processing on graphs", "author": ["A. Sandryhaila", "J. Moura"], "venue": "Signal Processing, IEEE Transactions on, vol. 61, no. 7, pp. 1644\u20131656, 2013.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "A tutorial on spectral clustering", "author": ["U. von Luxburg"], "venue": "Statistics and Computing, vol. 17, no. 4, pp. 395\u2013416, 2007.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Database-friendly random projections: Johnson-lindenstrauss with binary coins", "author": ["D. Achlioptas"], "venue": "Journal of Computer and System Sciences, vol. 66, no. 4, pp. 671 \u2013 687, 2003.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2003}, {"title": "Discrete signal processing on graphs: Sampling theory", "author": ["S. Chen", "R. Varma", "A. Sandryhaila", "J. Kovacevic"], "venue": "Signal Processing, IEEE Transactions on, vol. 63, no. 24, pp. 6510\u20136523, 2015.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient sampling set selection for bandlimited graph signals using graph spectral proxies", "author": ["A. Anis", "A. Gadde", "A. Ortega"], "venue": "arXiv, vol. abs/1510.00297, 2015.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Signals on graphs: Uncertainty principle and sampling", "author": ["M. Tsitsvero", "S. Barbarossa", "P.D. Lorenzo"], "venue": "arXiv, vol. abs/1507.08822, 2015.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Sampling of graph signals with successive local aggregations", "author": ["A. Marques", "S. Segarra", "G. Leus", "A. Ribeiro"], "venue": "Signal Processing, IEEE Transactions on, vol. PP, no. 99, pp. 1\u20131, 2015.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Infinite impulse response graph filters in wireless sensor networks", "author": ["X. Shi", "H. Feng", "M. Zhai", "T. Yang", "B. Hu"], "venue": "Signal Processing Letters, IEEE, vol. 22, no. 8, pp. 1113\u20131117, 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Chebyshev polynomial approximation for distributed signal processing", "author": ["D. Shuman", "P. Vandergheynst", "P. Frossard"], "venue": "Distributed Computing in Sensor Systems and Workshops (DCOSS), International Conference on, 2011, pp. 1\u20138.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient estimation of eigenvalue counts in an interval", "author": ["E.D. Napoli", "E. Polizzi", "Y. Saad"], "venue": "arXiv, vol. abs/1308.4275, 2013.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Randomized algorithms for estimating the trace of an implicit symmetric positive semidefinite matrix", "author": ["H. Avron", "S. Toledo"], "venue": "J. ACM, vol. 58, no. 2, pp. 8:1\u20138:34, 2011.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Parallel spectral clustering in distributed systems", "author": ["W.-Y. Chen", "Y. Song", "H. Bai", "L.C.-J", "E. Chang"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 33, no. 3, pp. 568\u2013586, 2011.  Compressive spectral clustering  15", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "Consistency of spectral clustering in stochastic block models", "author": ["J. Lei", "A. Rinaldo"], "venue": "Ann. Statist., vol. 43, no. 1, pp. 215\u2013237, 2015.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Gspbox: A toolbox for signal processing on graphs", "author": ["N. Perraudin", "J. Paratte", "D. Shuman", "V. Kalofolias", "P. Vandergheynst", "D. Hammond"], "venue": "arXiv, vol. abs/1408.5781, 2014.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications", "author": ["A. Decelle", "F. Krzakala", "C. Moore", "L. Zdeborov\u00e1"], "venue": "Phys. Rev. E, vol. 84, p. 066106, 2011.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2011}, {"title": "Comparing partitions", "author": ["L. Hubert", "P. Arabie"], "venue": "Journal of classification, vol. 2, no. 1, pp. 193\u2013218, 1985.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1985}, {"title": "Defining and evaluating network communities based on ground-truth", "author": ["J. Yang", "J. Leskovec"], "venue": "Knowledge and Information Systems, vol. 42, no. 1, pp. 181\u2013213, 2015.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "Finding and evaluating community structure in networks", "author": ["M.E.J. Newman", "M. Girvan"], "venue": "Phys. Rev. E, vol. 69, p. 026113, 2004.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2004}, {"title": "Stable community cores in complex networks", "author": ["M. Seifi", "I. Junier", "J.-B. Rouquier", "S. Iskrov", "J.-L. Guillaume"], "venue": "Complex Networks, 2013, pp. 87\u201398.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Spectral clustering (SC) is a fundamental tool in data mining [1].", "startOffset": 62, "endOffset": 65}, {"referenceID": 1, "context": ", [2\u20135], but all follow the same scheme.", "startOffset": 2, "endOffset": 7}, {"referenceID": 2, "context": ", [2\u20135], but all follow the same scheme.", "startOffset": 2, "endOffset": 7}, {"referenceID": 3, "context": ", [2\u20135], but all follow the same scheme.", "startOffset": 2, "endOffset": 7}, {"referenceID": 4, "context": ", [2\u20135], but all follow the same scheme.", "startOffset": 2, "endOffset": 7}, {"referenceID": 5, "context": "This k-way scheme is a generalisation of Fiedler\u2019s pioneering work [6].", "startOffset": 67, "endOffset": 70}, {"referenceID": 6, "context": ", concentric circles) for which naive k-means clustering fails; 2) if the input data is directly a graph G modeling a network [7], such as social, neuronal, or transportation networks.", "startOffset": 126, "endOffset": 129}, {"referenceID": 7, "context": ", via the power method [8, 9], via a careful optimisation of diagonalisation algorithms in the context of SC [10], or via matrix column-subsampling such as in the Nystr\u00f6m method [11], the nSPEC and cSPEC methods of [12], or in [13, 14].", "startOffset": 23, "endOffset": 29}, {"referenceID": 8, "context": ", via the power method [8, 9], via a careful optimisation of diagonalisation algorithms in the context of SC [10], or via matrix column-subsampling such as in the Nystr\u00f6m method [11], the nSPEC and cSPEC methods of [12], or in [13, 14].", "startOffset": 23, "endOffset": 29}, {"referenceID": 9, "context": ", via the power method [8, 9], via a careful optimisation of diagonalisation algorithms in the context of SC [10], or via matrix column-subsampling such as in the Nystr\u00f6m method [11], the nSPEC and cSPEC methods of [12], or in [13, 14].", "startOffset": 109, "endOffset": 113}, {"referenceID": 10, "context": ", via the power method [8, 9], via a careful optimisation of diagonalisation algorithms in the context of SC [10], or via matrix column-subsampling such as in the Nystr\u00f6m method [11], the nSPEC and cSPEC methods of [12], or in [13, 14].", "startOffset": 178, "endOffset": 182}, {"referenceID": 11, "context": ", via the power method [8, 9], via a careful optimisation of diagonalisation algorithms in the context of SC [10], or via matrix column-subsampling such as in the Nystr\u00f6m method [11], the nSPEC and cSPEC methods of [12], or in [13, 14].", "startOffset": 215, "endOffset": 219}, {"referenceID": 12, "context": ", via the power method [8, 9], via a careful optimisation of diagonalisation algorithms in the context of SC [10], or via matrix column-subsampling such as in the Nystr\u00f6m method [11], the nSPEC and cSPEC methods of [12], or in [13, 14].", "startOffset": 227, "endOffset": 235}, {"referenceID": 13, "context": ", via the power method [8, 9], via a careful optimisation of diagonalisation algorithms in the context of SC [10], or via matrix column-subsampling such as in the Nystr\u00f6m method [11], the nSPEC and cSPEC methods of [12], or in [13, 14].", "startOffset": 227, "endOffset": 235}, {"referenceID": 14, "context": "One can find such methods in [15] and [12]\u2019s eSPEC proposition, where two different interpolation methods are used.", "startOffset": 29, "endOffset": 33}, {"referenceID": 11, "context": "One can find such methods in [15] and [12]\u2019s eSPEC proposition, where two different interpolation methods are used.", "startOffset": 38, "endOffset": 42}, {"referenceID": 15, "context": "Also, let us mention [16] that circumvents both the eigendecomposition and the k-means bottlenecks: the authors reduce the graph\u2019s size by successive aggregation of nodes, apply SC on this small graph, and propagate the results on the complete graph using kernel k-means to control interpolation errors.", "startOffset": 21, "endOffset": 25}, {"referenceID": 16, "context": "The kernel is computed so that kernel k-means and SC share the same objective function [17].", "startOffset": 87, "endOffset": 91}, {"referenceID": 17, "context": "works [18, 19] that concentrate on reducing the feature vectors\u2019 dimension in the k-means problem, but do not sidestep the eigendecomposition nor the large N issues.", "startOffset": 6, "endOffset": 14}, {"referenceID": 18, "context": "works [18, 19] that concentrate on reducing the feature vectors\u2019 dimension in the k-means problem, but do not sidestep the eigendecomposition nor the large N issues.", "startOffset": 6, "endOffset": 14}, {"referenceID": 19, "context": "In this work, inspired by recent advances in the emerging field of graph signal processing [20, 21], we circumvent SC\u2019s last two bottlenecks and detail a fast approximate spectral clustering method for large datasets, as well as the supporting theory.", "startOffset": 91, "endOffset": 99}, {"referenceID": 20, "context": "In this work, inspired by recent advances in the emerging field of graph signal processing [20, 21], we circumvent SC\u2019s last two bottlenecks and detail a fast approximate spectral clustering method for large datasets, as well as the supporting theory.", "startOffset": 91, "endOffset": 99}, {"referenceID": 21, "context": "The first ingredient builds upon recent works [22,23] that avoid the costly computation of the eigenvectors of L by filtering O(log(k)) random signals on G that will then serve as feature vectors to perform clustering.", "startOffset": 46, "endOffset": 53}, {"referenceID": 22, "context": "The first ingredient builds upon recent works [22,23] that avoid the costly computation of the eigenvectors of L by filtering O(log(k)) random signals on G that will then serve as feature vectors to perform clustering.", "startOffset": 46, "endOffset": 53}, {"referenceID": 23, "context": "The second ingredient uses a recent sampling theory of bandlimited graph-signals [24] to reduce the computational complexity of k-means.", "startOffset": 81, "endOffset": 85}, {"referenceID": 24, "context": "6 \u03bbN 6 2 [25], and of the orthonormal matrix U := (u1|u2| .", "startOffset": 9, "endOffset": 13}, {"referenceID": 19, "context": "By analogy to the continuous Laplacian operator whose eigenfunctions are the classical Fourier modes and eigenvalues their squared frequencies, the columns of U are considered as the graph\u2019s Fourier modes, and { \u221a \u03bbl}l as its set of associated \u201cfrequencies\u201d [20].", "startOffset": 258, "endOffset": 262}, {"referenceID": 25, "context": ", [26], but in order to exhibit the link between graph signal processing and SC (that partially diagonalizes the Laplacian matrix), the Laplacian-based Fourier matrix appears more natural.", "startOffset": 2, "endOffset": 6}, {"referenceID": 1, "context": "Given a continuous filter function h defined on [0, 2], its associated graph filter operator H \u2208 RN\u00d7N is defined as H := h(L) = Uh(\u039b)UT, where h(\u039b) := diag(h(\u03bb1), h(\u03bb2), \u00b7 \u00b7 \u00b7 , h(\u03bbN )).", "startOffset": 48, "endOffset": 54}, {"referenceID": 1, "context": "In the following, we consider ideal low-pass filters, denoted by h\u03bbc , that satisfy, for all \u03bb \u2208 [0, 2], h\u03bbc(\u03bb) = 1, if \u03bb 6 \u03bbc, and h\u03bbc(\u03bb) = 0, if not.", "startOffset": 97, "endOffset": 103}, {"referenceID": 1, "context": "Algorithm 1 Spectral Clustering [2]", "startOffset": 32, "endOffset": 35}, {"referenceID": 1, "context": "l=0 \u03b1l\u03bb l ' h(\u03bb) for all \u03bb \u2208 [0, 2], where \u03b11, .", "startOffset": 29, "endOffset": 35}, {"referenceID": 1, "context": "\u2019s method [2] based on the normalized Laplacian as our standard SC method.", "startOffset": 10, "endOffset": 13}, {"referenceID": 21, "context": "A theorem recently published independently by two teams [22, 23] shows that this is possible when there is no normalisation step (step 2 in Alg.", "startOffset": 56, "endOffset": 64}, {"referenceID": 22, "context": "A theorem recently published independently by two teams [22, 23] shows that this is possible when there is no normalisation step (step 2 in Alg.", "startOffset": 56, "endOffset": 64}, {"referenceID": 23, "context": "To guarantee robust reconstruction, we take advantage of recent results of [24] on random sampling of", "startOffset": 75, "endOffset": 79}, {"referenceID": 26, "context": "See [27] for several choices of similarity measure s and several ways to create W from the s(xi,xj).", "startOffset": 4, "endOffset": 8}, {"referenceID": 27, "context": "1 of [28]) random variables.", "startOffset": 5, "endOffset": 9}, {"referenceID": 27, "context": "1 of [28] (an instance of the Johnson-Lindenstrauss lemma) to \u2016RUk(fi \u2212 fj)\u2016, the following holds.", "startOffset": 5, "endOffset": 9}, {"referenceID": 21, "context": ", f\u0303N} , as done in [22, 23].", "startOffset": 20, "endOffset": 28}, {"referenceID": 22, "context": ", f\u0303N} , as done in [22, 23].", "startOffset": 20, "endOffset": 28}, {"referenceID": 28, "context": "There has been recently a surge of interest around adapting classical sampling theorems to such bandlimited graph signals [29\u201332].", "startOffset": 122, "endOffset": 129}, {"referenceID": 29, "context": "There has been recently a surge of interest around adapting classical sampling theorems to such bandlimited graph signals [29\u201332].", "startOffset": 122, "endOffset": 129}, {"referenceID": 30, "context": "There has been recently a surge of interest around adapting classical sampling theorems to such bandlimited graph signals [29\u201332].", "startOffset": 122, "endOffset": 129}, {"referenceID": 31, "context": "There has been recently a surge of interest around adapting classical sampling theorems to such bandlimited graph signals [29\u201332].", "startOffset": 122, "endOffset": 129}, {"referenceID": 23, "context": "We rely here on the random sampling strategy proposed in [24] to select a subset of n nodes.", "startOffset": 57, "endOffset": 61}, {"referenceID": 21, "context": "Results in [22,23] show that k-means is also able to identify the clusters using the feature vectors {f\u03031, .", "startOffset": 11, "endOffset": 18}, {"referenceID": 22, "context": "Results in [22,23] show that k-means is also able to identify the clusters using the feature vectors {f\u03031, .", "startOffset": 11, "endOffset": 18}, {"referenceID": 23, "context": "[24] show that the solution to the optimisation problem", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "3precise error bounds are provided in [24].", "startOffset": 38, "endOffset": 42}, {"referenceID": 23, "context": "It is shown in [24] that \u03bdk \u2208 [k, N].", "startOffset": 15, "endOffset": 19}, {"referenceID": 23, "context": "4 ( [24]).", "startOffset": 4, "endOffset": 8}, {"referenceID": 23, "context": "Indeed, it is proved in [24] that, whatever the graph G, there always exists an optimal sampling distribution such that n = O(k log k) samples are sufficient to satisfy Eq.", "startOffset": 24, "endOffset": 28}, {"referenceID": 23, "context": "This distribution depends on the profile of the local cumulative coherence and can be estimated rapidly (see [24] for more details).", "startOffset": 109, "endOffset": 113}, {"referenceID": 1, "context": "\u2200\u03bb \u2208 [0, 2], e(\u03bb) := h\u0303\u03bbk(\u03bb)\u2212 h\u03bbk(\u03bb).", "startOffset": 5, "endOffset": 11}, {"referenceID": 27, "context": "1 in [28].", "startOffset": 5, "endOffset": 9}, {"referenceID": 23, "context": "4 of [24] showing that vk(i) = \u2016Uk\u03b4i\u2016 \u2248 \u2016(H\u03bbkR)\u03b4i\u2016.", "startOffset": 5, "endOffset": 9}, {"referenceID": 32, "context": "Such filters have been introduced in the graph context [33], but they involve another optimisation step that would burden our main message.", "startOffset": 55, "endOffset": 59}, {"referenceID": 33, "context": "In our experiments, we could follow [34] and use truncated Chebychev polynomials to approximate the ideal filter, as these polynomials are known to require a small degree to ensure a given tolerated maximal error em.", "startOffset": 36, "endOffset": 40}, {"referenceID": 34, "context": "We prefer to follow [35] who suggest to use Jackson-Chebychev polynomials: Chebychev polynomials to which are added damping multipliers to alleviate the unwanted Gibbs oscillations around the cut-off frequency \u03bbk.", "startOffset": 20, "endOffset": 24}, {"referenceID": 23, "context": "1 in [24], where an estimation of \u03bbk is obtained as a by-product.", "startOffset": 5, "endOffset": 9}, {"referenceID": 35, "context": "3 of [36] shows that this eigencount estimator yields the exact result (i.", "startOffset": 5, "endOffset": 9}, {"referenceID": 34, "context": "In practice, experiments in [35] show that fewer random signals are sufficient in some cases.", "startOffset": 28, "endOffset": 32}, {"referenceID": 23, "context": "Inspired by these results and as in [24], we choose 2 logN random signals for this estimation task.", "startOffset": 36, "endOffset": 40}, {"referenceID": 36, "context": "2 of [37]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 37, "context": "We first perform well-controlled experiments on the Stochastic Block Model (SBM), a model of random graphs with community structures, that was showed suitable as a benchmark for SC in [38].", "startOffset": 184, "endOffset": 188}, {"referenceID": 38, "context": "The fast filtering part of CSC uses the gsp cheby op function of the GSP toolbox [39].", "startOffset": 81, "endOffset": 85}, {"referenceID": 39, "context": "[41] show that a critical value c exists above which community detection is impossible at the large N limit: c = (s\u2212 \u221a s)/(s+ \u221a s(k \u2212 1)).", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "The performance is measured by the Adjusted Rand similarity index [42] between the SBM\u2019s ground truth and the obtained partitions.", "startOffset": 66, "endOffset": 70}, {"referenceID": 7, "context": "We compare the recovery performance and the time of computation of CSC, SC and Boutsidis\u2019 power method [8].", "startOffset": 103, "endOffset": 106}, {"referenceID": 41, "context": "We finally compare CSC and SC on a real-world dataset: the Amazon co-purchasing network [43].", "startOffset": 88, "endOffset": 92}, {"referenceID": 42, "context": "As there is no clear ground truth in this case, we use the modularity [44] to measure the algorithm\u2019s clustering performance, a well-known cost function that measures how well a given partition separates a network in different communities.", "startOffset": 70, "endOffset": 74}, {"referenceID": 43, "context": "To improve the clustering result of the reduced set of nodes, one could build upon the concept of community cores [45].", "startOffset": 114, "endOffset": 118}, {"referenceID": 26, "context": "1) is no longer required [27].", "startOffset": 25, "endOffset": 29}, {"referenceID": 2, "context": "On the other hand, for the version of SC based on the random walk Laplacian L = I\u2212 D\u22121W [3], the Fourier matrix is no longer orthogonal, its inverse is no longer equal to its transpose, and the sampling theorems of [24] need yet to be extended to this case.", "startOffset": 88, "endOffset": 91}, {"referenceID": 23, "context": "On the other hand, for the version of SC based on the random walk Laplacian L = I\u2212 D\u22121W [3], the Fourier matrix is no longer orthogonal, its inverse is no longer equal to its transpose, and the sampling theorems of [24] need yet to be extended to this case.", "startOffset": 215, "endOffset": 219}], "year": 2017, "abstractText": "Spectral clustering has become a popular technique due to its high performance in many contexts. It comprises three main steps: create a similarity graph between N objects to cluster, compute the first k eigenvectors of its Laplacian matrix to define a feature vector for each object, and run k-means on these features to separate objects into k classes. Each of these three steps becomes computationally intensive for large N and/or k. We propose to speed up the last two steps based on recent results in the emerging field of graph signal processing: graph filtering of random signals, and random sampling of bandlimited graph signals. We prove that our method, with a gain in computation time that can reach several orders of magnitude, is in fact an approximation of spectral clustering, for which we are able to control the error. We test the performance of our method on artificial and real-world network data.", "creator": "LaTeX with hyperref package"}}}