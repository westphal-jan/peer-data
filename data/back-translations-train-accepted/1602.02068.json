{"id": "1602.02068", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Feb-2016", "title": "From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification", "abstract": "We propose sparsemax, a new activation function similar to the traditional softmax, but able to output sparse probabilities. After deriving its properties, we show how its Jacobian can be efficiently computed, enabling its use in a network trained with backpropagation. Then, we propose a new smooth and convex loss function which is the sparsemax analogue of the logistic loss. We reveal an unexpected connection between this new loss and the Huber classification loss. We obtain promising empirical results in multi-label classification problems and in attention-based neural networks for natural language inference. For the latter, we achieve a similar performance as the traditional softmax, but with a selective, more compact, attention focus.", "histories": [["v1", "Fri, 5 Feb 2016 15:49:02 GMT  (1062kb,D)", "http://arxiv.org/abs/1602.02068v1", null], ["v2", "Mon, 8 Feb 2016 09:41:36 GMT  (1062kb,D)", "http://arxiv.org/abs/1602.02068v2", "Minor corrections"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG stat.ML", "authors": ["andr\u00e9 f t martins", "ram\u00f3n fern\u00e1ndez astudillo"], "accepted": true, "id": "1602.02068"}, "pdf": {"name": "1602.02068.pdf", "metadata": {"source": "META", "title": "From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification", "authors": ["Andr\u00e9 F. T. Martins", "Ram\u00f3n F. Astudillo"], "emails": ["ANDRE.MARTINS@UNBABEL.COM", "RAMON@UNBABEL.COM"], "sections": [{"heading": "1. Introduction", "text": "The softmax transformation is a key component of several statistical learning models, which include a simple evaluation: it is 2001 (McCullagh & Nelder, 1989), action selection in enhancing learning (Sutton & Barto, 1998), and neural networks for multi-class classification (Bridle, 1990; Goodfellow et al., 2016); more recently, it has also been used to design attention mechanisms in neural networks, with important achievements in machine translation (Bahdanau et al., 2015), caption generation (Xu et al., 2015), speech recognition (Chorowski et al., 2015), memory networks (Sukhbaatar et al., 2015), and various tasks in understanding natural language (Hermann et al., 2015; Rockta \ufffd schel et al., 2015; Rush et al., 2015) and computation learning (Graves et al., 2014; Grefenstet al). There are a number of reasons why max transformation is so attractive."}, {"heading": "2. The Sparsemax Transformation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Definition", "text": "We are interested in functions that map vectors in RK to probability distributions in \u2206 K \u2212 1. Such functions are useful for converting a vector of real weights (e.g. labels) into a probability distribution (e.g. posterior probabilities of labels).The classic example is the Softmax function, which is defined as: Softmaxi (z) = exp (zi) = j exp (zj). (1) One limitation of the Softmax transformation is that the resulting probability distribution always has full support, i.e., Softmaxi (z) 6 = 0 for each z and i. This is a disadvantage in applications where a sparse probability distribution is desired, in which case it is customary to define a threshold below which small probability values are reduced to spareness."}, {"heading": "2.2. Closed-Form Solution", "text": "Projecting onto the simplex is a well-researched problem for which linear time algorithms are available (Michelot, 1986; Pardalos & Kovoor, 1990; Duchi et al., 2008). We begin by remembering the well-known result that such projections correspond to a soft threshold operation. In the following, we will use the notation [K]: = {1,.,., K} and [t] +: = max {0, t}. Suggestion 1 The solution of Equation 2 is of the form: Sparsemaxi (z) = [zi \u2212 \u03c4 (z)] +, (3) where RK \u2212 R is the (unique) function that fulfils the threshold of j (z) + = 1 for each z threshold. In addition, we support the algorithm 1 Sparsemax Evaluation Input: z Sort z as z (1)."}, {"heading": "2.3. Basic Properties", "text": "s z (1): = maxk zk, and use A (z): = {k [K] | zk = z (1)} to denote the set of the largest components of z. We define the indicator vector 1A (z), whose kth component is 1 if k \u00b2 A (z), and 0 otherwise. We also use g \u00b2 (z): = z (1) \u2212 maxk / \u00b2 A (z) zk to denote the gap between the largest components of z and the second largest. We leave 0 and 1 to be vectors of zeros and ones, correspondingly. Proposition 2 The following properties apply to \"softmax,\" sparsemax. \"zi (0) = 1 / K and lim\" \u2192 0 + \u03c1 \"(\u2212 1z) = 1A (z) / | A (even distribution of z)."}, {"heading": "2.4. Two and Three-Dimensional Cases", "text": "For the two-class case, it is known that the Softmax activation becomes a logistic (sigmoid) function. Specifically, if z = (t, 0), then Softmax1 (z) = \u03c3 (t): = (1 + exp (\u2212 t) \u2212 1. Next, we show that the analog in Sparsemax is the \"hard\" version of the sigmoid. In fact, we have this when using Prop. 1, Equation 4 for z = (t, 0), \u03c4 (z) = t \u2212 1, if t > 1 (t \u2212 1) / 2, if \u2212 1 \u2264 t \u2264 1 \u2212 1, if t < \u2212 1, (5) and then for Parsemax1 (z) = 1, if t > 1 (t + 1) / 2, if \u2212 1 \u2264 t \u2264 1 0, if t < \u2212 1. (6) Figure 1 returns a figure for the two and three-dimensional cases. For the latter function, we can (1) and for the function (tender, tender), (tender (), (soft) (and ()."}, {"heading": "2.5. Jacobian of Sparsemax", "text": "We must remember what the Jacobian of softmax (z) looks like: (7), where the Kronecker delta, which evaluates the Kronecker delta, is 1 if i = j and 0 otherwise. (2), the full Jacobian (z), which can be written in the matrix notation asJsoftmax (z). (2), the full Jacobian (z), which can be written in the matrix notation asJsoftmax (z) = Diag (p) \u2212 pp (8), where the Diag (p) -pp >, where the Diag (p) is a matrix. (z), which can be written in the matrix notation asJsoftmax (z). (z), which we have in the matrix notation asJsoftmax (z) = Diag (p) -pp >, where the Diag (p) is a matrix."}, {"heading": "3. A Loss Function for Sparsemax", "text": "Now that we have defined the Sparsemax transformation and defined its main characteristics, we will show how to use this transformation to design a new loss function similar to logistical loss but capable of generating sparse posterior distributions. Later (in sections 4.1-4.2) we will apply this loss to the estimation of the label share and the classification of multiple labels."}, {"heading": "3.1. Logistic Loss", "text": "Consider a dataset D: = {(xi, yi)} Ni = 1, where each xi-RD is an input vector and each yi-RD is a target output label. We consider regulated empirical risk minimization problems of the formality minimization function, where L is a loss function, W is a matrix of weights, and b is a distortion vector. The loss function associated with the Softmax is the logistic loss (or negative log probability): Lsoftmax (z; k) = \u2212 log softmaxk (z) = \u2212 zk + log j exp (zj), (16) where z = Wxi + b, and k = yi is the \"gold distribution.\" The grave of this loss, if it leads to a loss (z), is the result (zq = 7), i.e. the probability that there is a change (zk)."}, {"heading": "3.2. Sparsemax Loss", "text": "A nice aspect of the log probability (equation 16) is that the addition of loss terms for multiple examples, assuming i.i.d, yields the log probability of the complete training data. Unfortunately, this idea cannot be applied to Sparsemax: now, some labels may have exactly the probability q zero, so any model that assigns the maximum probability to a gold label would set the probability of the entire training sample to zero. Of course, this is highly undesirable. However, one possible workaround is that the property sparsemax (z; k) = -log + Sparsemaxk (z) 1 + K (18), where a small constant is, and + sparsemaxk (z) 1 + K is a \"disturbing\" Sparsemax. However, this loss is not convex, as opposed to that in equation. 16.Another possibility we are exploring here is to construct an alternative loss function whose gradients resemble one in equation."}, {"heading": "3.3. Relation to the Huber Loss", "text": "Coincidentally, as we will show next, the slight loss in the binary case is reduced to the loss of the Huber classification, an important loss function in robust statistics (Huber, 1964). First, let us note that if | S (z) | = 1, Lsparsemax (z; k) = \u2212 zk + z (1), (21) and if | S (z) | = 2, Lsparsemax (z; k) = \u2212 zk) = \u2212 zk + (z (1) \u2212 z (2) 2 4 + z (1) + z (2) 2, (22), if z (1) \u2265 z (2)."}, {"heading": "3.4. Generalization to Multi-Label Classification", "text": "= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "4. Experiments", "text": "Next, we evaluate empirically sparsemax's ability to address two classes of problems: 3Not to be confused with the \"multiclass classification,\" which identifies problems where Y = [K] and K > 2.4This scenario is also relevant for \"learning with a probabilistic teacher\" (Agrawala, 1970) and semi-supervised learning (Chapelle et al., 2006), as it can model marker uncertainty. 1. Marker proportion determination and multi-class classification on sparsemax loss in equation 26 (\u00a7 4.1-4.2).2. Attention-based neural networks on sparsemax transformation of equation 2 (\u00a7 4.3)."}, {"heading": "4.1. Label Proportion Estimation", "text": "Since sparsemax q = q can predict sparse distributions, we expect their superiority in this task. We created datasets with 1,200 training examples and 1,000 test examples. Each example emulates a \"multi-described document\": a variable sequence of word symbols associated with several themes (labels). We select the number of labels N = 1,., K} by sampling from a Poisson distribution with repulsion samples and extract the N labels from a multinomial. Then we select a document length from a Poisson and repeatedly sample its words from the mixture of N-label-specific multinomials. We experimented with two settings: uniform mixtures (qkn = 1 / N for the N-active labels k1,......, kN, and random labels)."}, {"heading": "4.2. Multi-Label Classification on Benchmark Datasets", "text": "Next, we conducted experiments in five benchmark classification datasets with multiple labels: the four small-scale datasets, those of Koyejo et al. (2015), 7, and the much larger Reuters RCV15Note that the problem with uniform mixtures essentially becomes multi-level classification.6Note that KL divergence is not an appropriate metric here, since the sparseness of q and p could lead to \u2212 higher values.7Obected from http: / / mulan.sourceforge.net / datasets-mlc.html.v2 datasets by Lewis et al (2004).8 For all datasets, we removed examples without labels (i.e. Y = \u2205), but for all Reuters datasets we normalized the characteristics to have zero mean and unit variance. Statistics for these datasets are presented in Table 1.Recent work."}, {"heading": "4.3. Neural Networks with Attention Mechanisms", "text": "We are now evaluating the suitability of the Sparsemax system to construct a \"sparse\" neural attention system close to the realization of reactionary actions. (D) We have engaged in the question of the meaning of the system we are in. (D) We have the system we see in Fig. (2015) The architecture of our system shown in Fig. 4 is the same as that of Rockta. (2015) We are comparing the performance of four systems: NOATTENTION, a (gated) RNN-based system similar to Bowman et al. (2015); LOGISTICATTENTION, an attention-based system with independent logistical activations; SOFTTENTION, a, a (gated) RNN-based system similar to Bowman et al. (2015); LOGISTENTION."}, {"heading": "5. Conclusions", "text": "We introduced the sparsemax transformation, which has similar characteristics to the traditional softmax, but seems capable of producing sparse probability distributions. We derived a closed form of expression for its jacobian, which is required for the back-propagation algorithm, and we proposed a novel \"sparsemax loss\" function, a sparse analogue of logistic loss that is smooth and convex. Empirical results in multi-label classification and in attention networks for natural language inference confirm the validity of our approach. However, the link between sparse modelling and interpretability is the key in signal processing (Hastie et al., 2015). Our approach is distinctive: it is not the model that is assumed to be sparse, but the posterior label that parameterizes the model. Thrift is also a desirable (and biologically plausible) property of neural networks that are present in fictitious units."}, {"heading": "Acknowledgements", "text": "We would like to thank Tim Rockta \ufffd schel for answering various implementation questions, and Ma \ufffd rio Figueiredo and Chris Dyer for helpful comments on a draft of this report, which was partially supported by Fundac, an Institute of Corporate Governance and Technology (FCT) through the UID / EEA / 50008 / 2013 and UID / CEC / 50021 / 2013 treaties."}, {"heading": "A. Proofs", "text": "The reason for this development is the following: The optimal one (p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p)."}], "references": [{"title": "Learning with a probabilistic teacher", "author": ["Agrawala", "Ashok K"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Agrawala and K.,? \\Q1970\\E", "shortCiteRegEx": "Agrawala and K.", "year": 1970}, {"title": "Fast k-selection algorithms for graphics processing units", "author": ["Alabi", "Tolu", "Blanchard", "Jeffrey D", "Gordon", "Bradley", "Steinbach", "Russel"], "venue": "Journal of Experimental Algorithmics (JEA),", "citeRegEx": "Alabi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Alabi et al\\.", "year": 2012}, {"title": "Bayesian analysis of binary and polychotomous response data", "author": ["Albert", "James H", "Chib", "Siddhartha"], "venue": "Journal of the American statistical Association,", "citeRegEx": "Albert et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Albert et al\\.", "year": 1993}, {"title": "Learning word representations from scarce and noisy data with embedding sub-spaces", "author": ["Astudillo", "Ramon F", "Amir", "Silvio", "Lin", "Wang", "Silva", "M\u00e1rio", "Trancoso", "Isabel"], "venue": "In Proc. of the Association for Computational Linguistics (ACL), Beijing, China,", "citeRegEx": "Astudillo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Astudillo et al\\.", "year": 2015}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Time bounds for selection", "author": ["Blum", "Manuel", "Floyd", "Robert W", "Pratt", "Vaughan", "Rivest", "Ronald L", "Tarjan", "Robert E"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Blum et al\\.,? \\Q1973\\E", "shortCiteRegEx": "Blum et al\\.", "year": 1973}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Bowman", "Samuel R", "Angeli", "Gabor", "Potts", "Christopher", "Manning", "Christopher D"], "venue": "In Proc. of Empirical Methods in Natural Language Processing,", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Rank analysis of incomplete block designs: The method of paired comparisons", "author": ["Bradley", "Ralph Allan", "Terry", "Milton E"], "venue": null, "citeRegEx": "Bradley et al\\.,? \\Q1952\\E", "shortCiteRegEx": "Bradley et al\\.", "year": 1952}, {"title": "Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition", "author": ["Bridle", "John S"], "venue": "In Neurocomputing,", "citeRegEx": "Bridle and S.,? \\Q1990\\E", "shortCiteRegEx": "Bridle and S.", "year": 1990}, {"title": "Semi-Supervised Learning", "author": ["Chapelle", "Olivier", "Sch\u00f6lkopf", "Bernhard", "Zien", "Alexander"], "venue": null, "citeRegEx": "Chapelle et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2006}, {"title": "Attention-based models for speech recognition", "author": ["Chorowski", "Jan K", "Bahdanau", "Dzmitry", "Serdyuk", "Dmitriy", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Chorowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2015}, {"title": "Optimization and Nonsmooth Analysis", "author": ["Clarke", "Frank H"], "venue": null, "citeRegEx": "Clarke and H.,? \\Q1983\\E", "shortCiteRegEx": "Clarke and H.", "year": 1983}, {"title": "An exploration of softmax alternatives belonging to the spherical loss family", "author": ["de Br\u00e9bisson", "Alexandre", "Vincent", "Pascal"], "venue": "arXiv preprint arXiv:1511.05042,", "citeRegEx": "Br\u00e9bisson et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Br\u00e9bisson et al\\.", "year": 2015}, {"title": "Efficient projections onto the L1-ball for learning in high dimensions", "author": ["J. Duchi", "S. Shalev-Shwartz", "Y. Singer", "T. Chandra"], "venue": "In Proc. of International Conference of Machine Learning,", "citeRegEx": "Duchi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2008}, {"title": "On the consistency of multilabel learning", "author": ["Gao", "Wei", "Zhou", "Zhi-Hua"], "venue": "Artificial Intelligence,", "citeRegEx": "Gao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2013}, {"title": "Deep Sparse Rectifier Neural Networks", "author": ["Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Deep learning. Book in preparation for MIT Press, 2016", "author": ["Goodfellow", "Ian", "Bengio", "Yoshua", "Courville", "Aaron"], "venue": "URL http://goodfeli.github.io/dlbook/", "citeRegEx": "Goodfellow et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2016}, {"title": "Learning to Transduce with Unbounded Memory", "author": ["Grefenstette", "Edward", "Hermann", "Karl Moritz", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Statistical learning with sparsity: the lasso and generalizations", "author": ["Hastie", "Trevor", "Tibshirani", "Robert", "Wainwright", "Martin"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2015}, {"title": "Teaching Machines to Read and Comprehend", "author": ["Hermann", "Karl Moritz", "Kocisky", "Tomas", "Grefenstette", "Edward", "Espeholt", "Lasse", "Kay", "Will", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Robust estimation of a location parameter", "author": ["Huber", "Peter J"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Huber and J.,? \\Q1964\\E", "shortCiteRegEx": "Huber and J.", "year": 1964}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "In Proc. of International Conference on Learning Representations,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Consistent multilabel classification", "author": ["Koyejo", "Sanmi", "Natarajan", "Nagarajan", "Ravikumar", "Pradeep K", "Dhillon", "Inderjit S"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Koyejo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Koyejo et al\\.", "year": 2015}, {"title": "RCV1: a new benchmark collection for text categorization research", "author": ["Lewis", "David D", "Yang", "Yiming", "Rose", "Tony G", "Li", "Fan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Lewis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2004}, {"title": "On the limited memory bfgs method for large scale optimization", "author": ["Liu", "Dong C", "Nocedal", "Jorge"], "venue": "Mathematical programming,", "citeRegEx": "Liu et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Liu et al\\.", "year": 1989}, {"title": "Generalized Linear Models, volume 37", "author": ["McCullagh", "Peter", "Nelder", "John A"], "venue": "CRC press,", "citeRegEx": "McCullagh et al\\.,? \\Q1989\\E", "shortCiteRegEx": "McCullagh et al\\.", "year": 1989}, {"title": "A bradley\u2013terry artificial neural network model for individual ratings in group competitions", "author": ["Menke", "Joshua E", "Martinez", "Tony R"], "venue": "Neural computing and Applications,", "citeRegEx": "Menke et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Menke et al\\.", "year": 2008}, {"title": "A finite algorithm for finding the projection of a point onto the canonical simplex of R", "author": ["C. Michelot"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "Michelot,? \\Q1986\\E", "shortCiteRegEx": "Michelot", "year": 1986}, {"title": "A method of solving a convex programming problem with convergence rate O(1/k)", "author": ["Y. Nesterov"], "venue": "Soviet Math. Doklady,", "citeRegEx": "Nesterov,? \\Q1983\\E", "shortCiteRegEx": "Nesterov", "year": 1983}, {"title": "Riemannian metrics for neural networks", "author": ["Ollivier", "Yann"], "venue": "arXiv preprint arXiv:1303.0818,", "citeRegEx": "Ollivier and Yann.,? \\Q2013\\E", "shortCiteRegEx": "Ollivier and Yann.", "year": 2013}, {"title": "An algorithm for a singly constrained class of quadratic programs subject to upper and lower bounds", "author": ["Pardalos", "Panos M", "Kovoor", "Naina"], "venue": "Mathematical Programming,", "citeRegEx": "Pardalos et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Pardalos et al\\.", "year": 1990}, {"title": "Glove: Global vectors for word representation", "author": ["Pennington", "Jeffrey", "Socher", "Richard", "Manning", "Christopher D"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Conciliating generalized derivatives", "author": ["Penot", "Jean-Paul"], "venue": "Constructive Nonsmooth Analysis and Related Topics,", "citeRegEx": "Penot and Jean.Paul.,? \\Q2014\\E", "shortCiteRegEx": "Penot and Jean.Paul.", "year": 2014}, {"title": "Reasoning about Entailment with Neural Attention", "author": ["Rockt\u00e4schel", "Tim", "Grefenstette", "Edward", "Hermann", "Karl Moritz", "Ko\u010disk\u1ef3", "Tom\u00e1\u0161", "Blunsom", "Phil"], "venue": "arXiv preprint arXiv:1509.06664,", "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Rush", "Alexander M", "Chopra", "Sumit", "Weston", "Jason"], "venue": "Proc. of Empirical Methods in Natural Language Processing,", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "End-to-End Memory Networks", "author": ["Sukhbaatar", "Sainbayar", "Szlam", "Arthur", "Weston", "Jason", "Fergus", "Rob"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": "MIT press Cambridge,", "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Efficient exact gradient update for training deep networks with very large sparse targets", "author": ["Vincent", "Pascal"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Vincent and Pascal.,? \\Q2015\\E", "shortCiteRegEx": "Vincent and Pascal.", "year": 2015}, {"title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "author": ["Xu", "Kelvin", "Ba", "Jimmy", "Kiros", "Ryan", "Courville", "Aaron", "Salakhutdinov", "Ruslan", "Zemel", "Richard", "Bengio", "Yoshua"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Reducing multiclass to binary by coupling probability estimates", "author": ["Zadrozny", "Bianca"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zadrozny and Bianca.,? \\Q2001\\E", "shortCiteRegEx": "Zadrozny and Bianca.", "year": 2001}, {"title": "A review on multilabel learning algorithms. Knowledge and Data Engineering", "author": ["Zhang", "Min-Ling", "Zhou", "Zhi-Hua"], "venue": "IEEE Transactions on,", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Statistical behavior and consistency of classification methods based on convex risk minimization", "author": ["Zhang", "Tong"], "venue": "Annals of Statistics,", "citeRegEx": "Zhang and Tong.,? \\Q2004\\E", "shortCiteRegEx": "Zhang and Tong.", "year": 2004}, {"title": "The margin vector, admissible loss and multi-class marginbased classifiers", "author": ["Zou", "Hui", "Zhu", "Ji", "Hastie", "Trevor"], "venue": "Technical report,", "citeRegEx": "Zou et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 16, "context": "The softmax transformation is a key component of several statistical learning models, encompassing multinomial logistic regression (McCullagh & Nelder, 1989), action selection in reinforcement learning (Sutton & Barto, 1998), and neural networks for multi-class classification (Bridle, 1990; Goodfellow et al., 2016).", "startOffset": 277, "endOffset": 316}, {"referenceID": 4, "context": "Recently, it has also been used to design attention mechanisms in neural networks, with important achievements in machine translation (Bahdanau et al., 2015), image caption generation (Xu et al.", "startOffset": 134, "endOffset": 157}, {"referenceID": 38, "context": ", 2015), image caption generation (Xu et al., 2015), speech recognition (Chorowski et al.", "startOffset": 34, "endOffset": 51}, {"referenceID": 10, "context": ", 2015), speech recognition (Chorowski et al., 2015), memory networks (Sukhbaatar et al.", "startOffset": 28, "endOffset": 52}, {"referenceID": 35, "context": ", 2015), memory networks (Sukhbaatar et al., 2015), and various tasks in natural language understanding (Hermann et al.", "startOffset": 25, "endOffset": 50}, {"referenceID": 19, "context": ", 2015), and various tasks in natural language understanding (Hermann et al., 2015; Rockt\u00e4schel et al., 2015; Rush et al., 2015) and computation learning (Graves et al.", "startOffset": 61, "endOffset": 128}, {"referenceID": 33, "context": ", 2015), and various tasks in natural language understanding (Hermann et al., 2015; Rockt\u00e4schel et al., 2015; Rush et al., 2015) and computation learning (Graves et al.", "startOffset": 61, "endOffset": 128}, {"referenceID": 34, "context": ", 2015), and various tasks in natural language understanding (Hermann et al., 2015; Rockt\u00e4schel et al., 2015; Rush et al., 2015) and computation learning (Graves et al.", "startOffset": 61, "endOffset": 128}, {"referenceID": 17, "context": ", 2015) and computation learning (Graves et al., 2014; Grefenstette et al., 2015).", "startOffset": 33, "endOffset": 81}, {"referenceID": 27, "context": "Projecting onto the simplex is a well studied problem, for which linear-time algorithms are available (Michelot, 1986; Pardalos & Kovoor, 1990; Duchi et al., 2008).", "startOffset": 102, "endOffset": 163}, {"referenceID": 13, "context": "Projecting onto the simplex is a well studied problem, for which linear-time algorithms are available (Michelot, 1986; Pardalos & Kovoor, 1990; Duchi et al., 2008).", "startOffset": 102, "endOffset": 163}, {"referenceID": 5, "context": "More elaborate O(K) algorithms exist based on linear-time selection (Blum et al., 1973; Pardalos & Kovoor, 1990).", "startOffset": 68, "endOffset": 112}, {"referenceID": 28, "context": "However, unlike the hinge loss, Lsparsemax is everywhere differentiable, hence amenable to smooth optimization methods such as L-BFGS or accelerated gradient descent (Liu & Nocedal, 1989; Nesterov, 1983).", "startOffset": 166, "endOffset": 203}, {"referenceID": 42, "context": "This loss is a variant of the Huber loss adapted for classification, and has been called \u201cmodified Huber loss\u201d by Zhang (2004); Zou et al. (2006).", "startOffset": 128, "endOffset": 146}, {"referenceID": 9, "context": "This scenario is also relevant for \u201clearning with a probabilistic teacher\u201d (Agrawala, 1970) and semi-supervised learning (Chapelle et al., 2006), as it can model label uncertainty.", "startOffset": 121, "endOffset": 144}, {"referenceID": 22, "context": "Next, we ran experiments in five benchmark multi-label classification datasets: the four small-scale datasets used by Koyejo et al. (2015),7 and the much larger Reuters RCV1 Note that, with uniform mixtures, the problem becomes essentially multi-label classification.", "startOffset": 118, "endOffset": 139}, {"referenceID": 22, "context": "Recent work has investigated the consistency of multi-label classifiers for various micro and macro-averaged metrics (Gao & Zhou, 2013; Koyejo et al., 2015), among which a plug-in classifier that trains independent binary logistic regressors on each label, and then tunes a probability threshold \u03b4 \u2208 [0, 1] on validation data.", "startOffset": 117, "endOffset": 156}, {"referenceID": 22, "context": "v2 dataset of Lewis et al. (2004).8 For all datasets, we removed examples without labels (i.", "startOffset": 14, "endOffset": 34}, {"referenceID": 6, "context": "0 corpus (Bowman et al., 2015), a collection of 570,000 human-written English sentence pairs.", "startOffset": 9, "endOffset": 30}, {"referenceID": 32, "context": "4, is the same as the one proposed by Rockt\u00e4schel et al. (2015). We compare the performance of four systems: NOATTENTION, a (gated) RNN-based system similar to Bowman et al.", "startOffset": 38, "endOffset": 64}, {"referenceID": 6, "context": "We compare the performance of four systems: NOATTENTION, a (gated) RNN-based system similar to Bowman et al. (2015); LOGISTICATTENTION, an attention-based system with independent logistic activations; SOFTATTENTION, a near-reproduction of the Rockt\u00e4schel et al.", "startOffset": 95, "endOffset": 116}, {"referenceID": 6, "context": "We compare the performance of four systems: NOATTENTION, a (gated) RNN-based system similar to Bowman et al. (2015); LOGISTICATTENTION, an attention-based system with independent logistic activations; SOFTATTENTION, a near-reproduction of the Rockt\u00e4schel et al. (2015)\u2019s attention-based system; and SPARSEATTENTION, which replaces the latter softmax-activated attention mechanism by a sparsemax activation.", "startOffset": 95, "endOffset": 269}, {"referenceID": 31, "context": "We represent the words in the premise and in the hypothesis with 300-dimensional GloVe vectors (Pennington et al., 2014), not optimized during training, which we linearly project onto a D-dimensional subspace (Astudillo et al.", "startOffset": 95, "endOffset": 120}, {"referenceID": 33, "context": "Instead of long short-term memories, as Rockt\u00e4schel et al. (2015), we used gated recurrent units (GRUs, Cho et al.", "startOffset": 40, "endOffset": 66}, {"referenceID": 33, "context": "We tuned a `2-regularization coefficient in {0, 10\u22124, 3 \u00d7 10\u22124, 10\u22123} and, as Rockt\u00e4schel et al. (2015), a dropout probability of 0.", "startOffset": 78, "endOffset": 104}, {"referenceID": 33, "context": "10 Table 4 shows examples of sentence pairs, highlighting the premise words selected by the SPARSEATTENTION mechRockt\u00e4schel et al. (2015) report scores slightly above ours: they reached a test accuracy of 82.", "startOffset": 112, "endOffset": 138}, {"referenceID": 33, "context": "10 Table 4 shows examples of sentence pairs, highlighting the premise words selected by the SPARSEATTENTION mechRockt\u00e4schel et al. (2015) report scores slightly above ours: they reached a test accuracy of 82.3% for their implementation of SOFTATTENTION, and 83.5% with their best system, a more elaborate word-by-word attention model. Differences may be due to distinct word vectors and the use of LSTMs instead of GRUs. Table 4. Examples of sparse attention for the natural language inference task. Nonzero attention coefficients are marked in bold. Our system classified all four examples correctly. The examples were picked from Rockt\u00e4schel et al. (2015).", "startOffset": 112, "endOffset": 658}, {"referenceID": 18, "context": "The connection between sparse modeling and interpretability is key in signal processing (Hastie et al., 2015).", "startOffset": 88, "endOffset": 109}, {"referenceID": 15, "context": "Sparsity is also a desirable (and biologically plausible) property in neural networks, present in rectified units (Glorot et al., 2011) and maxout nets (Goodfellow et al.", "startOffset": 114, "endOffset": 135}, {"referenceID": 17, "context": "The ability of sparsemax-activated attention to select only a few variables to attend makes it potentially relevant to neural architectures with random access memory (Graves et al., 2014; Grefenstette et al., 2015; Sukhbaatar et al., 2015), since it offers a compromise between soft and hard operations, maintaining differentiability.", "startOffset": 166, "endOffset": 239}, {"referenceID": 35, "context": "The ability of sparsemax-activated attention to select only a few variables to attend makes it potentially relevant to neural architectures with random access memory (Graves et al., 2014; Grefenstette et al., 2015; Sukhbaatar et al., 2015), since it offers a compromise between soft and hard operations, maintaining differentiability.", "startOffset": 166, "endOffset": 239}, {"referenceID": 1, "context": "There is, however, recent work providing efficient implementations of these algorithms on GPUs (Alabi et al., 2012).", "startOffset": 95, "endOffset": 115}, {"referenceID": 37, "context": "tention are often useful, arising as word alignments in machine translation pipelines, or latent variables as in Xu et al. (2015). Sparsemax is also appealing for hierarchical attention: if we define a top-down product of distributions along the hierarchy, the sparse distributions produced by sparsemax will automatically prune the hierarchy, leading to computational savings.", "startOffset": 113, "endOffset": 130}], "year": 2017, "abstractText": "We propose sparsemax, a new activation function similar to the traditional softmax, but able to output sparse probabilities. After deriving its properties, we show how its Jacobian can be efficiently computed, enabling its use in a network trained with backpropagation. Then, we propose a new smooth and convex loss function which is the sparsemax analogue of the logistic loss. We reveal an unexpected connection between this new loss and the Huber classification loss. We obtain promising empirical results in multi-label classification problems and in attention-based neural networks for natural language inference. For the latter, we achieve a similar performance as the traditional softmax, but with a selective, more compact, attention focus.", "creator": "LaTeX with hyperref package"}}}