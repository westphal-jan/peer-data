{"id": "1602.02101", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Feb-2016", "title": "Variance-Reduced and Projection-Free Stochastic Optimization", "abstract": "The Frank-Wolfe optimization algorithm has recently regained popularity for machine learning applications due to its projection-free property and its ability to handle structured constraints. However, in the stochastic learning setting, it is still relatively understudied compared to the gradient descent counterpart. In this work, leveraging a recent variance reduction technique, we propose two stochastic Frank-Wolfe variants which substantially improve previous results in terms of the number of stochastic gradient evaluations needed to achieve $1-\\epsilon$ accuracy. For example, we improve from $O(\\frac{1}{\\epsilon})$ to $O(\\ln\\frac{1}{\\epsilon})$ if the objective function is smooth and strongly convex, and from $O(\\frac{1}{\\epsilon^2})$ to $O(\\frac{1}{\\epsilon^{1.5}})$ if the objective function is smooth and Lipschitz. The theoretical improvement is also observed in experiments on real-world datasets for a multiclass classification application.", "histories": [["v1", "Fri, 5 Feb 2016 17:14:59 GMT  (46kb,D)", "http://arxiv.org/abs/1602.02101v1", null], ["v2", "Thu, 14 Sep 2017 00:03:37 GMT  (53kb,D)", "http://arxiv.org/abs/1602.02101v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["elad hazan", "haipeng luo"], "accepted": true, "id": "1602.02101"}, "pdf": {"name": "1602.02101.pdf", "metadata": {"source": "CRF", "title": "Variance-Reduced and Projection-Free Stochastic Optimization", "authors": ["Elad Hazan", "Haipeng Luo"], "emails": ["ehazan@cs.princeton.edu", "haipengl@cs.princeton.edu"], "sections": [{"heading": null, "text": ") to O (ln 1), if the objective function is smooth and strongly convex, and from O (1 2) to O (1 1.5), if the objective function is smooth and tip of the lip."}, {"heading": "1 Introduction", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "2 Preliminary and Related Work", "text": "We assume that any function is fi convex and L-smooth, that is, for all W, V, W, W, W, FI, W, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI, FI"}, {"heading": "2.1 Example Application: Multiclass Classification", "text": "Consider a multi-class classification problem where a series of training examples (ei, yi) i = 1,..., n is given beforehand. Here, ei-Rm is a feature vector and yi-Rh {1,.., h} is the label. Our goal is to find an accurate linear predictor, a matrixw = [w > 1;.., w > h] that predicts argmax'w > \"e, for example e. Note that here the dimensionality is d hm. Previous work (Dudik et al., 2012; Zhang et al., 2012) has found that the search w by minimizing a regularized multivariate logistic loss yields a very precise predictor in general. Specifically, the target in our note may be fi (w) = log (1 + 6) = yi exp (w > chaiei) n that yields an optimization of time that is not linear."}, {"heading": "2.2 Detailed Efficiency Comparisons", "text": "In fact, it is so that it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way in which it is about a way, and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way and a way in which it is about a way and a way in which it is about a way and a way and a way in which it is about a way and a way and a way in which it is about a way and a way and a way in which it is about a way and a way and a way in which it is about a way and a way and a way in which it is about a way and a way and a way it is about a way and a way in which it is about a way and a way and a way and a way it is about a way and a way in which it is about a way and a way and a way and a way and a way it is about a way and a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way and a way and a way and a way and a way it is about a way and a way and a way in which it is about a way and a way and a way and a way and"}, {"heading": "2.3 Variance-Reduced Stochastic Gradients", "text": ".. \"It is not as if we would be able to find a solution.\" \"It is not as if.\" \"It is as if.\" \"It is as if.\" \"It is as if.\" \"It is as if.\" \"It is as if.\" \"It is as if.\" \"It is as if.\" \"It is as if.\" \"It is as if.\" \"\" It is as if. \"\" \"It is as if.\" \"\" It is as if. \"\" \"It is as if.\" \"\" It is as if. \"\" It is as if. \"\" \"\" It is as if. \"\" \"It is as if.\" \"\" It is as if. \"\" \"It is as if.\" \"\" It is as if. \"\".. \"\" \"..\" \"..\" \"\".. \"\".. \"\".. \".\". \"\" It is as if.. \"\" \"\".. \"\" \"..\". \".\" \"..\". \".\" \"..\". \".\". \"It is so, as if.\" \"..\".. \".\". \"\".. \".\". \".\". \"..\". \".\". \".\" \".\".. \".\". \".\". \".\". \"..\". \".\". \"..\".. \".\". \"\". \".\""}, {"heading": "3 Stochastic Variance-Reduced Frank-Wolfe", "text": "With the previous discussion, our first algorithm requires 1 O (ln D 2 exact evaluations): compared to the standard Frank-Wolfe (1), we simply replace the exact gradient with the average of a stochastic gradient reduced in variance (1), the pseudo-code of which is represented in Alg 1."}, {"heading": "4 Stochastic Variance-Reduced Conditional Gradient Sliding", "text": "s algorithms, is obtained only once in a while. (See pseudocode in Alg 2 for detailed optimization, so it is close to xk \u2212 1 (line 11). This step does not require additional gradients of f or fi, and is performed by executing the standard Frank Wolfe algorithms (q. (3) until the duality gap has most of a certain value."}, {"heading": "5 Experiments", "text": "To support our theory, we conduct experiments in the multiclass classification problem mentioned in Sec 2.1. Three sets of data are selected from the LIBSVM repository4 with relatively large number of characteristics, categories and examples, summarized in Table 3.Recall that the loss function is multivariate logistic loss and is the set of matrices with limited trace norm. We focus on how quickly the loss decreases instead of the final test error rate, so that the match of \u03c4 is less important, and is fixed at 50 passes. We compare six algorithms. Four of them (SFW, SCGS, SVRF, STORC) are projection free as discussed, and the other two are standard projected stochastic gradient descent (SGD) and its reduced variance version (SVRG & Zhang, 2013), both of which require expensive projections."}, {"heading": "6 Omitted Proofs", "text": "This section contains some previously omitted evidence."}, {"heading": "6.1 Proof of Lemma 1", "text": "The proof. Let Ei denotes the conditional expectation given in the past, except for the realization of i. We have Ei (w; w; w0) \u2212 no (w) \u2212 no (w) \u2212 no (w) \u2212 no (w) \u2212 no (w) \u2212 no (w) no (w) no (w) no (w) no (w) no (w) no (w) no (w) no (w) no (w) no (w) no (w) no (w) no (w) no (w) no (w) no (w) no (w) no (w) no (w) no (w) no (w) no (w) \u2212 no (w) no (w) no (w) no (w) no (w) no (w) no (w) no (w)."}, {"heading": "6.2 Proof of Lemma 2", "text": "The proof: For all s \u2264 k, by smoothness \u2212 s we have f (xs) \u2264 f (xs \u2212 1) + f (xs \u2212 1) > (xs \u2212 1) + f \u2212 s (xs \u2212 1) > (vs \u2212 xs \u2212 1) > (vs \u2212 xs \u2212 1) + f (s) x (s) (xs \u2212 1) \u2264 f (xs \u2212 1) + f \u2212 s (vs \u2212 xs \u2212 1) > (vs \u2212 xs \u2212 1) + f \u2212 s (vs \u2212 1) + l (s) 2 s (vs \u2212 s) (f \u2212 s) (f \u2212 s) > s (f \u2212 s) > f (f \u2212 s) > f (f \u2212 s) > f (f \u2212 s) > (f \u2212 s) > (f \u2212 s) > s (f \u2212 s) (f \u2212 s) > s (f \u2212 s) > f (f \u2212 s) > f (f \u2212 s) > f (f \u2212 s)."}, {"heading": "7 Conclusion and Open Problems", "text": "We conclude that the method of reducing variance, which has so far proved extremely useful for gradient descending variants, can also be very helpful in accelerating projection-free algorithms. In the strongly convex case, the most important open question is whether the number of stochastic gradients for STORC can be improved from O (\u00b52 ln 1) to O (\u00b5 ln 1), which is typical for gradient descending methods, and whether the number of linear optimizations can be improved from O (1) to O (ln 1)."}, {"heading": "B Analysis for SFW", "text": "The specific update of SFW isvk \u2212 \u2212 vk \u2212 vk = (1 \u2212 vk) vk (1 \u2212 vk) vk = (1 \u2212 vk) vk (1 \u2212 vk) vk (1 \u2212 vk) vk (vk) vk (vk) vk (vk) vk (vk) vk (vk) vk (vk) vk (vk) vk (vk) vk (vk) vk (vk) vk (vk) vk (vk) vk (vk) vk (vk) vk (vk) vk (vk) vk (vk) vk (vk) vk (k) vk) vk (k) vk (k) vk (k) vk (k) vk (k) vk (k) vk (k) vk (k) vk (k) vk (k) vk (k) vk (k) vk (k) vk (k) vk (k) vk (k) vk (k) vk (k) vk (k) vk (k) vk (k) vk (vk) vk (vk) vk (vk) vk (vk) vk (vk) vk) vk (vk) vk (vk) vk (vk) vk (vk) vk (vk) vk (vk) vk (vk) vk) vk (vk (vk) vk (vk) vk (vk) vk (vk) vk (vk) vk (vk (vk) vk) vk (vk (vk) vk (vk) vk (vk) vk (vk) vk (vk (vk) vk (vk) vk (vk) vk (vk) vk (vk) vk) vk (vk (vk) vk (vk) vk (vk (vk) vk (vk) vk (vk (vk) vk) vk (v"}, {"heading": "C Proof of Lemma 3", "text": "The proof. Allow it. \u2212 f (s) \u2212 f (s) \u2212 f (s). \u2212 f (s). \u2212 f (s). \u2212 f (s). \u2212 f (s). \u2212 f (s). \u2212 f (s). \u2212 f (s). \u2212 f (s). \u2212 f (s). \u2212 f (s). \u2212 f (s). \u2212 f (s). \u2212 f (s). \u2212 f (s). \u2212 f (s). \u2212 f (s). \u2212 f (s). \u2212 f (s). \u2212 f (s). \u2212 f (s). \u2212 f (s). \u2212 f (s). \u2212 f (s). (f (s). \u2212 f. (s). (f (s). \u2212 f. \u2212.). \u2212 f. \u2212 f. \u2212. \u2212 f (s). \u2212 f. (s). \u2212 f. (s). \u2212 f (s). \u2212 f. (s). \u2212 f. (s). \u2212 f. (s). \u2212 f. (s). \u2212 f. \u2212 f. (s). \u2212 f. (s). \u2212 (s). \u2212 f (s). \u2212 f. \u2212 f. (s). \u2212 f. \u2212 f. \u2212 f. (s). \u2212 f. \u2212 f. \u2212 f. \u2212 f. \u2212 f. \u2212 f. \u2212. \u2212 f. \u2212 f. \u2212. \u2212. (s. \u2212 f. \u2212 f. \u2212 f. \u2212 f. \u2212 f. \u2212. \u2212. \u2212. \u2212 f. \u2212. \u2212 f. \u2212 f. (s. \u2212. \u2212. \u2212 f. \u2212 f. \u2212. \u2212. \u2212 f. (s). \u2212 f. (f. \u2212 f. \u2212. \u2212 f. (f (f. \u2212 f (s)."}], "references": [{"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "Li", "Li-Jia", "Kai", "Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Lifted coordinate descent for learning with trace-norm regularization", "author": ["Dudik", "Miro", "Harchaoui", "Zaid", "Malick", "J\u00e9r\u00f4me"], "venue": "In Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Dudik et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dudik et al\\.", "year": 2012}, {"title": "An algorithm for quadratic programming", "author": ["Frank", "Marguerite", "Wolfe", "Philip"], "venue": "Naval research logistics quarterly,", "citeRegEx": "Frank et al\\.,? \\Q1956\\E", "shortCiteRegEx": "Frank et al\\.", "year": 1956}, {"title": "Competing with the empirical risk minimizer in a single pass", "author": ["Frostig", "Roy", "Ge", "Rong", "Kakade", "Sham M", "Sidford", "Aaron"], "venue": "In Proceedings of the 28th Annual Conference on Learning Theory,", "citeRegEx": "Frostig et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Frostig et al\\.", "year": 2015}, {"title": "A linearly convergent conditional gradient algorithm with applications to online and stochastic optimization", "author": ["Garber", "Dan", "Hazan", "Elad"], "venue": "arXiv preprint arXiv:1301.4666,", "citeRegEx": "Garber et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Garber et al\\.", "year": 2013}, {"title": "Conditional gradient algorithms for norm-regularized smooth convex optimization", "author": ["Harchaoui", "Zaid", "Juditsky", "Anatoli", "Nemirovski", "Arkadi"], "venue": "Mathematical Programming,", "citeRegEx": "Harchaoui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Harchaoui et al\\.", "year": 2015}, {"title": "Projection-free online learning", "author": ["Hazan", "Elad", "Kale", "Satyen"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Hazan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2012}, {"title": "Near-optimal algorithms for online matrix prediction", "author": ["Hazan", "Elad", "Kale", "Satyen", "Shalev-Shwartz", "Shai"], "venue": "In COLT 2012 - The 25th Annual Conference on Learning Theory, June", "citeRegEx": "Hazan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2012}, {"title": "Revisiting frank-wolfe: Projection-free sparse convex optimization", "author": ["Jaggi", "Martin"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Jaggi and Martin.,? \\Q2013\\E", "shortCiteRegEx": "Jaggi and Martin.", "year": 2013}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Johnson", "Rie", "Zhang", "Tong"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Johnson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2013}, {"title": "On the global linear convergence of frank-wolfe optimization variants", "author": ["Lacoste-Julien", "Simon", "Jaggi", "Martin"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Lacoste.Julien et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lacoste.Julien et al\\.", "year": 2015}, {"title": "Conditional gradient sliding for convex optimization", "author": ["Lan", "Guanghui", "Zhou", "Yi"], "venue": "Optimization-Online preprint (4605),", "citeRegEx": "Lan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lan et al\\.", "year": 2014}, {"title": "Mixed optimization for smooth functions", "author": ["Mahdavi", "Mehrdad", "Zhang", "Lijun", "Jin", "Rong"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mahdavi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mahdavi et al\\.", "year": 2013}, {"title": "A linearly-convergent stochastic l-bfgs algorithm", "author": ["Moritz", "Philipp", "Nishihara", "Robert", "Jordan", "Michael I"], "venue": "In Proceedings of the Nineteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Moritz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Moritz et al\\.", "year": 2016}, {"title": "Accelerated training for matrix-norm regularization: A boosting approach", "author": ["Zhang", "Xinhua", "Schuurmans", "Dale", "Yu", "Yao-liang"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Zhang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 6, "context": "Examples of such problem include multiclass classification, multitask learning, recommendation systems, matrix learning and many more (see for example (Hazan & Kale, 2012; Hazan et al., 2012; Jaggi, 2013; Dudik et al., 2012; Zhang et al., 2012; Harchaoui et al., 2015)).", "startOffset": 151, "endOffset": 268}, {"referenceID": 1, "context": "Examples of such problem include multiclass classification, multitask learning, recommendation systems, matrix learning and many more (see for example (Hazan & Kale, 2012; Hazan et al., 2012; Jaggi, 2013; Dudik et al., 2012; Zhang et al., 2012; Harchaoui et al., 2015)).", "startOffset": 151, "endOffset": 268}, {"referenceID": 14, "context": "Examples of such problem include multiclass classification, multitask learning, recommendation systems, matrix learning and many more (see for example (Hazan & Kale, 2012; Hazan et al., 2012; Jaggi, 2013; Dudik et al., 2012; Zhang et al., 2012; Harchaoui et al., 2015)).", "startOffset": 151, "endOffset": 268}, {"referenceID": 5, "context": "Examples of such problem include multiclass classification, multitask learning, recommendation systems, matrix learning and many more (see for example (Hazan & Kale, 2012; Hazan et al., 2012; Jaggi, 2013; Dudik et al., 2012; Zhang et al., 2012; Harchaoui et al., 2015)).", "startOffset": 151, "endOffset": 268}, {"referenceID": 12, "context": "In this work, we thus try to answer the following question: how fast can a projection-free algorithm achieve in terms of the number of stochastic gradient evaluations and the number of linear optimizations needed to achieve a certain accuracy? Utilizing Nesterov\u2019s acceleration technique (Nesterov, 1983) and the recent variance reduction idea (Johnson & Zhang, 2013; Mahdavi et al., 2013), we propose two new algorithms that are substantially faster than previous work.", "startOffset": 344, "endOffset": 389}, {"referenceID": 1, "context": "Previous work (Dudik et al., 2012; Zhang et al., 2012) found that finding w by minimizing a regularized multivariate logistic loss gives a very accurate predictor in general.", "startOffset": 14, "endOffset": 54}, {"referenceID": 14, "context": "Previous work (Dudik et al., 2012; Zhang et al., 2012) found that finding w by minimizing a regularized multivariate logistic loss gives a very accurate predictor in general.", "startOffset": 14, "endOffset": 54}, {"referenceID": 0, "context": "The number of examples n can be prohibitively large for non-stochastic methods (for instance, tens of millions for the ImageNet dataset (Deng et al., 2009)), which makes stochastic optimization necessary.", "startOffset": 136, "endOffset": 155}, {"referenceID": 12, "context": "The key idea of our algorithms is to combine the variance reduction technique proposed in (Johnson & Zhang, 2013; Mahdavi et al., 2013) with some of the above-mentioned algorithms.", "startOffset": 90, "endOffset": 135}, {"referenceID": 12, "context": "3 Variance-Reduced Stochastic Gradients Originally proposed in (Johnson & Zhang, 2013) and independently in (Mahdavi et al., 2013), the idea of variancereduced stochastic gradients is proven to be highly useful and has been extended to various different algorithms (such as (Frostig et al.", "startOffset": 108, "endOffset": 130}, {"referenceID": 3, "context": ", 2013), the idea of variancereduced stochastic gradients is proven to be highly useful and has been extended to various different algorithms (such as (Frostig et al., 2015; Moritz et al., 2016)).", "startOffset": 151, "endOffset": 194}, {"referenceID": 13, "context": ", 2013), the idea of variancereduced stochastic gradients is proven to be highly useful and has been extended to various different algorithms (such as (Frostig et al., 2015; Moritz et al., 2016)).", "startOffset": 151, "endOffset": 194}, {"referenceID": 3, "context": ", 2013), the idea of variancereduced stochastic gradients is proven to be highly useful and has been extended to various different algorithms (such as (Frostig et al., 2015; Moritz et al., 2016)). A variance-reduced stochastic gradient at some point w \u2208 \u03a9 with some snapshot w0 \u2208 \u03a9 is defined as \u2207\u0303f(w;w0) = \u2207fi(w)\u2212 (\u2207fi(w0)\u2212\u2207f(w0)), where i is again picked from {1, . . . , n} uniformly at random. The snapshotw0 is usually a decision point from some previous iteration of the algorithm and its exact gradient \u2207f(w0) has been pre-computed before, so that computing \u2207\u0303f(w;w0) only requires two standard stochastic gradient evaluations: \u2207fi(w) and \u2207fi(w0). 2See also recent follow up work Lacoste-Julien & Jaggi (2015). 3The first result comes from the setting where the online loss functions are stochastic, and the second one comes from a completely online setting with the standard online-to-batch conversion.", "startOffset": 152, "endOffset": 718}], "year": 2016, "abstractText": "The Frank-Wolfe optimization algorithm has recently regained popularity for machine learning applications due to its projection-free property and its ability to handle structured constraints. However, in the stochastic learning setting, it is still relatively understudied compared to the gradient descent counterpart. In this work, leveraging a recent variance reduction technique, we propose two stochastic Frank-Wolfe variants which substantially improve previous results in terms of the number of stochastic gradient evaluations needed to achieve 1 \u2212 accuracy. For example, we improve from O( 1 ) to O(ln 1 ) if the objective function is smooth and strongly convex, and from O( 1 2 ) to O( 1 1.5 ) if the objective function is smooth and Lipschitz. The theoretical improvement is also observed in experiments on real-world datasets for a multiclass classification application.", "creator": "LaTeX with hyperref package"}}}