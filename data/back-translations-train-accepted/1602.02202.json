{"id": "1602.02202", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2016", "title": "Efficient Second Order Online Learning by Sketching", "abstract": "We propose Sketched Online Newton (SON), an online second order learning algorithm that enjoys substantially improved regret guarantees for ill-conditioned data. SON is an enhanced version of the Online Newton Step, which, via sketching techniques enjoys a linear running time. We further improve the computational complexity to linear in the number of nonzero entries by creating sparse forms of the sketching methods (such as Oja's rule) for top eigenvector extraction. Together, these algorithms eliminate all computational obstacles in previous second order online learning approaches.", "histories": [["v1", "Sat, 6 Feb 2016 02:33:53 GMT  (51kb,D)", "https://arxiv.org/abs/1602.02202v1", null], ["v2", "Sat, 21 May 2016 15:10:31 GMT  (65kb,D)", "http://arxiv.org/abs/1602.02202v2", null], ["v3", "Tue, 4 Oct 2016 15:22:07 GMT  (85kb,D)", "http://arxiv.org/abs/1602.02202v3", "NIPS2016 camera-ready version"], ["v4", "Tue, 17 Oct 2017 05:01:59 GMT  (85kb,D)", "http://arxiv.org/abs/1602.02202v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["haipeng luo", "alekh agarwal", "nicol\u00f2 cesa-bianchi", "john langford"], "accepted": true, "id": "1602.02202"}, "pdf": {"name": "1602.02202.pdf", "metadata": {"source": "CRF", "title": "Efficient Second Order Online Learning by Sketching", "authors": ["Haipeng Luo", "Alekh Agarwal"], "emails": ["haipengl@cs.princeton.edu", "alekha@microsoft.com", "nicolo.cesa-bianchi@unimi.it", "jcl@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Setup and an Optimal Algorithm", "text": ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"}, {"heading": "3 Efficiency via Sketching", "text": "In this section, we will show how we can achieve almost all of the above limits while maintaining the calculation within a constant factor of the methods of the first order. Let's write a matrix so that the t-th line is g > t, in which we define the g-th line. (The idea of the sketch is to maintain an approximation of Gt designated by St-Rm-d, where m is called a small constant.) If m is chosen so that S > t St approaches G > t, we can redefine the Gt. (The idea of the sketch is an approximation of Gt cited by St-Rm, where m d is a small constant."}, {"heading": "4 Sparse Implementation", "text": "In many examples (and thus inclines) one can orient oneself in Appendix in this way by the two following examples: \"There are only two possibilities.\" \"There are only two possibilities.\" \"There is only one way.\" \"There is only one way.\" \"There is no way.\" \"There is no way.\" \"There is no way.\" \"There is no way.\" \"\" There is no way. \"\" \"There is no way.\" \"\" There is no way. \"\" \"\" There is no way. \"\" \"\" There is no way. \"\" \"\" There is no way. \"\" \"\" There is no way. \"\" \"\" \"There is no way.\" \"\" \"\" There is no way. \"\" \"\" \"\" There is no way. \"\" \"\" \"..\" \"\" \"\" There is no way. \"\" \"\" \"\".. \"\" \"\" \"There is no way......\" \"\" \"\" \"There is no way.......\" \"\" \"\" \"\" There is no way....... \"\" \"\" \"\" There is no way......... \"\" \"\" \"\" There is no way......... \"\" \"\" \"There is no way..........\" \"\" There is no way....... \"There is no way.\""}, {"heading": "5 Experiments", "text": "Initial experiments have shown that Oja's sketch of our two sketch options generally performs better (see Appendix H.) For a more thorough evaluation, we have the sparse version of Oja-SON in Vowpal Wabbit.5 We compare it to ADAGRAD [5, 22] on both synthetic and real datasets. Each algorithm assumes a step-by-step parameter: 1\u03b1 serves as a step value for Oja-SON and as a scaling constant on the gradient matrix for ADAGRAD. We try both methods with the parameter set 2j for j = \u2212 3, \u2212 2,...,..., 6 and report the best results. We consistently record the step-by-step matrix in Oja-SON as T = 1t Im."}, {"heading": "5.1 Synthetic Datasets", "text": "To investigate the performance of Oja-SON in the environment for which it is really designed, we created a series of synthetic, poorly conditioned data sets as follows: We chose a random Gaussian matrix Z \u0445 RT \u00b7 d (T = 10,000 and d = 100) and a random orthonormal base V \u0445 Rd \u00b7 d. We chose a specific spectrum in which the first d \u2212 10 coordinates are 1 and the rest rises linearly to a certain parameter of the fixed conditional number. We created 20 such data sets with the same Z, V and designations y, but different values of the same sample matrix {10, 20,.., 200} and created a binary classification problem with labels y = characters (\u03b8 > x) in which the condition \"Rd\" is a random vector."}, {"heading": "5.2 Real-world Datasets", "text": "In fact, the majority of them are in a position to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "A Proof of Theorem 1", "text": "Assuming T is a multiple of d without loss of universality, we select xt from the base vectors {e1,.., ed} so that every egg T / d appears (in an arbitrary order). Note that K is now only a hypercube: K = {w: | w > xt | \u2264 C,.., ed} = {w:. For a scalar function, we define the loss function 7 't (\u03b8) = (\u0394tL), so that assumptions 1 and 2 are clearly satisfied with \u0441t = 0. We show that for each online algorithm E [RT] = E [T: t = 1' t (w > t xt) \u2212 inf = 1 't \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t =."}, {"heading": "B Projection", "text": "We prove a more general version of Lemma 1, which presupposes no immutability of the matrix A = > > > Problem A here. Lemma 2. For any x 6 = 0, u Rd \u00b7 1 and positive semi-definitive matrix A Rd \u00b7 d, we have a similar effect on Argmin w: | w > x | \u2264 C-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W"}, {"heading": "C Proof of Theorem 2", "text": "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +"}, {"heading": "D A Truly Invariant Algorithm", "text": "In this section we will discuss how to transform our adaptation to the Newton algorithm into the range in which the data is actually transformed. In this section we will discuss how to transform our adaptation to the Newton algorithm into the range. (1) To achieve this, we set A \u2212 1t with the MoorePenrose pseudo-inverse A \u2020 t: 8ut + 1 = wt \u2212 A \u2020 tgt, wt + 1 = argminw + 1 = argminw + 1 = argminw. (7) When we are written in this form, it is not immediately clear that the algorithm has the invariable property. However, you can rewrite the algorithm in a mirror form: wt + 1 = argmin w = argmin w = argmin w = argmin w. (2)."}, {"heading": "E Proof of Theorem 3", "text": "Proof. Once again, we first apply Proposition 1 (remember the notation RG and RD = > V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V \u2212 V \u2212 V \u2212 V = V \u2212 V \u2212 V = V = V \u2212 V = V \u2212 V \u2212 V = V = V \u2212 V \u2212 V = V \u2212 V = V \u2212 V \u2212 V \u2212 V = V = V \u2212 V \u2212 V = V = V = V \u2212 V \u2212 V \u2212 V = V \u2212 V \u2212 V \u2212 V = V = V \u2212 V \u2212 V \u2212 V = V = V \u2212 V \u2212 V = V \u2212 V = V = V = V \u2212 V \u2212 V \u2212 V \u2212 V \u2212 V = V = V \u2212 V \u2212 V \u2212 V = V \u2212 V \u2212 V \u2212 V \u2212 V \u2212 V \u2212 V \u2212 V \u2212 V \u2212 V \u2212 V \u2212 V = V \u2212 V = V = V = V = V = V = V = V = V = V = V = V = V = V = V \u2212 V = V \u2212 V \u2212 V \u2212 V \u2212 V \u2212 V \u2212 V \u2212 V \u2212 V \u2212 V \u2212 V \u2212 V \u2212 V \u2212 V \u2212 V \u2212 V \u2212 V \u2212 V \u2212 V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V \u2212 V = V = V = V = V = V = V = V = V \u2212 V = V \u2212 V = V = V \u2212 V = V \u2212 V = V = V = V = V = V = V \u2212 V = V = V \u2212 V = V = V = V = V = V = V = V \u2212 V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V V"}, {"heading": "F Sparse updates for FD sketch", "text": "The sparse version of our algorithm is represented by two parts: the top part (DV) comes from the lowest level. We start with a detour and the introduction of a fast and era-based variant of the system (up to 2 million Euros).The idea is that we aim for an original composition immediately after the insertion of a new original composition (up to 2 million Euros).The advantage of this variant is that it can be implemented without complicated ranking only at the end of each round. The advantage of this variant is that it can be implemented directly in O (md) time without achieving a complicated ranking, while still guaranteeing the exact same guarantee with the only price of doubling the sketch."}, {"heading": "G Details for sparse Oja\u2019s algorithm", "text": "Since we have already discussed the updates for w-t and bt in section 4, we only need to describe how the updates for Ft and Zt work. Remember that the dense Ojas updates can be written in terms F and Z. (9) Here, the update for eigenvalues is straightforward. (9) For the update of eigenvectors, the update for eigenvalues is straightforward. (9) For the update of eigenvalues, the update for eigenvalues is straightforward. (9) For the update of eigenvectors, the update for eigenvalues is straightforward. (9) The update for eigenvalues is ineffective. (9) The update for eigenvalues is ineffective. (9) The update for eigenvalues is straightforward.) For the update of eigenvectors is ineffective. (9) The update of eigenvalues is ineffective."}, {"heading": "H Experiment Details", "text": "This section reports on some detailed experimental results omitted from Section 5.2. Table 1 provides a description of benchmark data sets; Table 2 reports on error rates for relatively small data sets to show that Oja-SON generally performs better; Table 3 reports on concrete error rates for the experiments described in Section 5.2; and finally Table 4 shows that Oja's algorithm accurately estimates eigenvalues. As mentioned in Section 5.2, we see significant improvements to the splice data set when we use Oja's sketch even after the diagonal fit. We verify that the conditional number for this data set is very close to each other before and after the diagonal fit (682 and 668, respectively), which explains why a big improvement can be seen from the Oja sketch. Figure 4 shows the decrease in error rates as Oja-SON sees more examples with different sketch sizes."}], "references": [{"title": "The fast convergence of incremental pca", "author": ["A. Balsubramani", "S. Dasgupta", "Y. Freund"], "venue": "NIPS,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "A stochastic quasi-newton method for large-scale optimization", "author": ["R.H. Byrd", "S. Hansen", "J. Nocedal", "Y. Singer"], "venue": "SIAM Journal on Optimization, 26:1008\u20131031,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": "Cambridge University Press,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "A second-order perceptron algorithm", "author": ["N. Cesa-Bianchi", "A. Conconi", "C. Gentile"], "venue": "SIAM Journal on Computing, 34(3):640\u2013668,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "JMLR, 12:2121\u20132159,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Convergence rates of sub-sampled newton methods", "author": ["M.A. Erdogdu", "A. Montanari"], "venue": "NIPS,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "An algorithm for quadratic programming", "author": ["M. Frank", "P. Wolfe"], "venue": "Naval research logistics quarterly, 3 (1-2):95\u2013110,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1956}, {"title": "One-pass auc optimization", "author": ["W. Gao", "R. Jin", "S. Zhu", "Z.-H. Zhou"], "venue": "ICML,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "A linearly convergent conditional gradient algorithm with applications to online and stochastic optimization", "author": ["D. Garber", "E. Hazan"], "venue": "SIAM Journal on Optimization, 26:1493\u20131528,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Online learning of eigenvectors", "author": ["D. Garber", "E. Hazan", "T. Ma"], "venue": "ICML,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Frequent directions: Simple and deterministic matrix sketching", "author": ["M. Ghashami", "E. Liberty", "J.M. Phillips", "D.P. Woodruff"], "venue": "SIAM Journal on Computing, 45:1762\u20131792,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient frequent directions algorithm for sparse matrices", "author": ["M. Ghashami", "E. Liberty", "J.M. Phillips"], "venue": "KDD,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Faster sgd using sketched conditioning", "author": ["A. Gonen", "S. Shalev-Shwartz"], "venue": "arXiv:1506.02649,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Solving ridge regression using sketched preconditioned svrg", "author": ["A. Gonen", "F. Orabona", "S. Shalev-Shwartz"], "venue": "ICML,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "The noisy power method: A meta algorithm with applications", "author": ["M. Hardt", "E. Price"], "venue": "NIPS,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Projection-free online learning", "author": ["E. Hazan", "S. Kale"], "venue": "ICML,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["E. Hazan", "A. Agarwal", "S. Kale"], "venue": "Machine Learning, 69(2-3):169\u2013192,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Revisiting frank-wolfe: Projection-free sparse convex optimization", "author": ["M. Jaggi"], "venue": "ICML,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Rivalry of two families of algorithms for memory-restricted streaming pca", "author": ["C.-L. Li", "H.-T. Lin", "C.-J. Lu"], "venue": "arXiv:1506.01490,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Simple and deterministic matrix sketching", "author": ["E. Liberty"], "venue": "KDD,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "On the limited memory bfgs method for large scale optimization", "author": ["D.C. Liu", "J. Nocedal"], "venue": "Mathematical programming, 45(1-3):503\u2013528,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1989}, {"title": "Adaptive bound optimization for online convex optimization", "author": ["H.B. McMahan", "M. Streeter"], "venue": "COLT,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Global convergence of online limited memory bfgs", "author": ["A. Mokhtari", "A. Ribeiro"], "venue": "JMLR, 16:3151\u20133181,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "A linearly-convergent stochastic l-bfgs algorithm", "author": ["P. Moritz", "R. Nishihara", "M.I. Jordan"], "venue": "AISTATS,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Simplified neuron model as a principal component analyzer", "author": ["E. Oja"], "venue": "Journal of mathematical biology, 15 (3):267\u2013273,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1982}, {"title": "On stochastic approximation of the eigenvectors and eigenvalues of the expectation of a random matrix", "author": ["E. Oja", "J. Karhunen"], "venue": "Journal of mathematical analysis and applications, 106(1):69\u201384,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1985}, {"title": "Scale-free algorithms for online linear optimization", "author": ["F. Orabona", "D. P\u00e1l"], "venue": "ALT,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "A generalized online mirror descent with applications to classification and regression", "author": ["F. Orabona", "K. Crammer", "N. Cesa-Bianchi"], "venue": "Machine Learning, 99(3):411\u2013435,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Newton sketch: A linear-time optimization algorithm with linearquadratic convergence", "author": ["M. Pilanci", "M.J. Wainwright"], "venue": "arXiv:1505.02250,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Normalized online learning", "author": ["S. Ross", "P. Mineiro", "J. Langford"], "venue": "UAI,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "A stochastic quasi-newton method for online convex optimization", "author": ["N.N. Schraudolph", "J. Yu", "S. G\u00fcnter"], "venue": "AISTATS,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "Fast large-scale optimization by unifying stochastic gradient and quasi-newton methods", "author": ["J. Sohl-Dickstein", "B. Poole", "S. Ganguli"], "venue": "ICML,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Sketching as a tool for numerical linear algebra", "author": ["D.P. Woodruff"], "venue": "Foundations and Trends in Machine Learning, 10(1-2):1\u2013157,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 16, "context": "Second order algorithms such as Online Newton Step [17] have the attractive property of being invariant to linear transformations of the data, but typically require space and update time quadratic in the number of dimensions.", "startOffset": 51, "endOffset": 55}, {"referenceID": 29, "context": "We study an invariant learning setting similar to the paper [30] which compares the learner to a benchmark only constrained to generate bounded predictions on the sequence of examples.", "startOffset": 60, "endOffset": 64}, {"referenceID": 16, "context": "We show that a variant of the Online Newton Step [17], while quadratic in computation, stays regret-optimal with a nearly matching lower bound in this more general setting.", "startOffset": 49, "endOffset": 53}, {"referenceID": 32, "context": "While the idea of data sketching is widely studied [33], as far as we know our work is the first one to apply it to a general adversarial", "startOffset": 51, "endOffset": 55}, {"referenceID": 10, "context": "Two different sketching methods are considered: Frequent Directions [11, 20] and Oja\u2019s algorithm [25, 26], both of which allow linear running time per round.", "startOffset": 68, "endOffset": 76}, {"referenceID": 19, "context": "Two different sketching methods are considered: Frequent Directions [11, 20] and Oja\u2019s algorithm [25, 26], both of which allow linear running time per round.", "startOffset": 68, "endOffset": 76}, {"referenceID": 24, "context": "Two different sketching methods are considered: Frequent Directions [11, 20] and Oja\u2019s algorithm [25, 26], both of which allow linear running time per round.", "startOffset": 97, "endOffset": 105}, {"referenceID": 25, "context": "Two different sketching methods are considered: Frequent Directions [11, 20] and Oja\u2019s algorithm [25, 26], both of which allow linear running time per round.", "startOffset": 97, "endOffset": 105}, {"referenceID": 9, "context": "[10]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Empirically, we evaluate our algorithm using the sparse Oja sketch (called Oja-SON) against first order methods such as diagonalized ADAGRAD [5, 22] on both ill-conditioned synthetic and a suite of real-world datasets.", "startOffset": 141, "endOffset": 148}, {"referenceID": 21, "context": "Empirically, we evaluate our algorithm using the sparse Oja sketch (called Oja-SON) against first order methods such as diagonalized ADAGRAD [5, 22] on both ill-conditioned synthetic and a suite of real-world datasets.", "startOffset": 141, "endOffset": 148}, {"referenceID": 29, "context": "Related work Our online learning setting is closest to the one proposed in [30], which studies scale-invariant algorithms, a special case of the invariance property considered here (see also [28, Section 5]).", "startOffset": 75, "endOffset": 79}, {"referenceID": 26, "context": "Orabona and P\u00e1l [27] study unrelated notions of invariance.", "startOffset": 16, "endOffset": 20}, {"referenceID": 7, "context": "[8] study a specific randomized sketching method for a special online learning setting.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "The L-BFGS algorithm [21] has recently been studied in the stochastic setting2 [2, 23, 24, 31, 32], but has strong assumptions with pessimistic rates in theory and reliance on the use of large mini-batches empirically.", "startOffset": 21, "endOffset": 25}, {"referenceID": 1, "context": "The L-BFGS algorithm [21] has recently been studied in the stochastic setting2 [2, 23, 24, 31, 32], but has strong assumptions with pessimistic rates in theory and reliance on the use of large mini-batches empirically.", "startOffset": 79, "endOffset": 98}, {"referenceID": 22, "context": "The L-BFGS algorithm [21] has recently been studied in the stochastic setting2 [2, 23, 24, 31, 32], but has strong assumptions with pessimistic rates in theory and reliance on the use of large mini-batches empirically.", "startOffset": 79, "endOffset": 98}, {"referenceID": 23, "context": "The L-BFGS algorithm [21] has recently been studied in the stochastic setting2 [2, 23, 24, 31, 32], but has strong assumptions with pessimistic rates in theory and reliance on the use of large mini-batches empirically.", "startOffset": 79, "endOffset": 98}, {"referenceID": 30, "context": "The L-BFGS algorithm [21] has recently been studied in the stochastic setting2 [2, 23, 24, 31, 32], but has strong assumptions with pessimistic rates in theory and reliance on the use of large mini-batches empirically.", "startOffset": 79, "endOffset": 98}, {"referenceID": 31, "context": "The L-BFGS algorithm [21] has recently been studied in the stochastic setting2 [2, 23, 24, 31, 32], but has strong assumptions with pessimistic rates in theory and reliance on the use of large mini-batches empirically.", "startOffset": 79, "endOffset": 98}, {"referenceID": 5, "context": "Recent works [6, 14, 13, 29] employ sketching in stochastic optimization, but do not provide sparse implementations or extend in an obvious manner to the online setting.", "startOffset": 13, "endOffset": 28}, {"referenceID": 13, "context": "Recent works [6, 14, 13, 29] employ sketching in stochastic optimization, but do not provide sparse implementations or extend in an obvious manner to the online setting.", "startOffset": 13, "endOffset": 28}, {"referenceID": 12, "context": "Recent works [6, 14, 13, 29] employ sketching in stochastic optimization, but do not provide sparse implementations or extend in an obvious manner to the online setting.", "startOffset": 13, "endOffset": 28}, {"referenceID": 28, "context": "Recent works [6, 14, 13, 29] employ sketching in stochastic optimization, but do not provide sparse implementations or extend in an obvious manner to the online setting.", "startOffset": 13, "endOffset": 28}, {"referenceID": 6, "context": "The FrankWolfe algorithm [7, 18] is also invariant to linear transformations, but with worse regret bounds [16] without further assumptions and modifications [9].", "startOffset": 25, "endOffset": 32}, {"referenceID": 17, "context": "The FrankWolfe algorithm [7, 18] is also invariant to linear transformations, but with worse regret bounds [16] without further assumptions and modifications [9].", "startOffset": 25, "endOffset": 32}, {"referenceID": 15, "context": "The FrankWolfe algorithm [7, 18] is also invariant to linear transformations, but with worse regret bounds [16] without further assumptions and modifications [9].", "startOffset": 107, "endOffset": 111}, {"referenceID": 8, "context": "The FrankWolfe algorithm [7, 18] is also invariant to linear transformations, but with worse regret bounds [16] without further assumptions and modifications [9].", "startOffset": 158, "endOffset": 161}, {"referenceID": 11, "context": "For an invariant update, Recent work by [12] also studies sparse updates for a more complicated variant of Frequent Directions which is randomized and incurs extra approximation error.", "startOffset": 40, "endOffset": 44}, {"referenceID": 29, "context": "This relaxation is similar to the comparator set considered in [30].", "startOffset": 63, "endOffset": 67}, {"referenceID": 29, "context": "[30] are recovered.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "We study a choice of At that is similar to the Online Newton Step (ONS) [17] (though with different projections):", "startOffset": 72, "endOffset": 76}, {"referenceID": 16, "context": "extending the O(d lnT ) results in [17] to the weaker Assumption 2 and a larger comparator set K.", "startOffset": 35, "endOffset": 39}, {"referenceID": 10, "context": "Frequent Directions sketch [11, 20] is a deterministic sketching method.", "startOffset": 27, "endOffset": 35}, {"referenceID": 19, "context": "Frequent Directions sketch [11, 20] is a deterministic sketching method.", "startOffset": 27, "endOffset": 35}, {"referenceID": 10, "context": "The sketch update works inO(md) time (see [11] and Appendix F) so the total running time is O(md) per round.", "startOffset": 42, "endOffset": 46}, {"referenceID": 24, "context": "Oja\u2019s algorithm [25, 26] is not usually considered as a sketching algorithm but seems very natural here.", "startOffset": 16, "endOffset": 24}, {"referenceID": 25, "context": "Oja\u2019s algorithm [25, 26] is not usually considered as a sketching algorithm but seems very natural here.", "startOffset": 16, "endOffset": 24}, {"referenceID": 14, "context": "To improve the running time to O(md), one can only update the sketch every m rounds (similar to the block power method [15, 19]).", "startOffset": 119, "endOffset": 127}, {"referenceID": 18, "context": "To improve the running time to O(md), one can only update the sketch every m rounds (similar to the block power method [15, 19]).", "startOffset": 119, "endOffset": 127}, {"referenceID": 0, "context": "[1, 19]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 18, "context": "[1, 19]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 4, "context": "5 We compare it with ADAGRAD [5, 22] on both synthetic and real-world datasets.", "startOffset": 29, "endOffset": 36}, {"referenceID": 21, "context": "5 We compare it with ADAGRAD [5, 22] on both synthetic and real-world datasets.", "startOffset": 29, "endOffset": 36}, {"referenceID": 29, "context": "[30].", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "References [1] A.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] R.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] N.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] N.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] W.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] F.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] F.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] N.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "2 in [3]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 10, "context": "1 of [11]): \u2211T t=1 \u03c1t \u2264 \u03a9k m\u2212k .", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "We begin by taking a detour and introducing a fast and epoch-based variant of the Frequent Directions algorithm proposed in [11].", "startOffset": 124, "endOffset": 128}], "year": 2017, "abstractText": "We propose Sketched Online Newton (SON), an online second order learning algorithm that enjoys substantially improved regret guarantees for ill-conditioned data. SON is an enhanced version of the Online Newton Step, which, via sketching techniques enjoys a running time linear in the dimension and sketch size. We further develop sparse forms of the sketching methods (such as Oja\u2019s rule), making the computation linear in the sparsity of features. Together, the algorithm eliminates all computational obstacles in previous second order online learning approaches.", "creator": "LaTeX with hyperref package"}}}