{"id": "1602.02218", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2016", "title": "Strongly-Typed Recurrent Neural Networks", "abstract": "Recurrent neural networks are increasing popular models for sequential learning. Unfortunately, although the most effective RNN architectures are perhaps excessively complicated, extensive searches have not found simpler alternatives. This paper imports ideas from physics and functional programming into RNN design to provide guiding principles. From physics we introduce type constraints, analogous to the constraints that disqualify adding meters to seconds in physics. From functional programming, we require that strongly-typed architectures factorize into stateless learnware and state-dependent firmware, thereby ameliorating the impact of side-effects. The features learned by strongly-typed nets have a simple semantic interpretation via dynamic average-pooling on one-dimensional convolutions. We also show that strongly-typed gradients are better behaved than in classical architectures, and characterize the representational power of strongly-typed nets. Finally, experiments show that, despite being more constrained, strongly-typed architectures achieve lower training error and comparable generalization error to classical architectures.", "histories": [["v1", "Sat, 6 Feb 2016 05:34:03 GMT  (25kb)", "http://arxiv.org/abs/1602.02218v1", "9 pages"], ["v2", "Tue, 24 May 2016 21:35:23 GMT  (26kb)", "http://arxiv.org/abs/1602.02218v2", "10 pages, final version, ICML 2016"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["david balduzzi", "muhammad ghifary"], "accepted": true, "id": "1602.02218"}, "pdf": {"name": "1602.02218.pdf", "metadata": {"source": "META", "title": "Strongly-Typed Recurrent Neural Networks", "authors": ["David Balduzzi"], "emails": ["DBALDUZZI@GMAIL.COM", "MGHIFARY@GMAIL.COM"], "sections": [{"heading": null, "text": "ar Xiv: 160 2.02 218v 1 [cs.L G] 6F eb2 01"}, {"heading": "1. Introduction", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2. Strongly-Typed Features", "text": "A variety of type systems have been developed for mathematical logic and language design (Reynolds, 1974; Girard, 1989; Pierce, 2002). We are introducing a type system based on linear algebra that is suitable for deep learning. Informally, a type is a vector space on an orthogonal basis. A more precise definition and rules for manipulating types are given below. Section 2.2 provides examples; Section 2.3 uses types to identify design errors in classical RNNs."}, {"heading": "2.1. Strongly-Typed Quasi-Linear Algebra", "text": "Quasi-linear algebra is a linear algebra supplemented by non-linear functions that act in coordination. Definition 1. A type T = (V, < \u2022, \u2022 >, {ti} d i = 1) is a ddimensional vector space equipped with an internal product and an orthogonal base, so < ti, tj > = 1 [i = j].Definition 2. The following operations are permitted: T1. Uniform operations on one type: T \u2192 T-tuple viavT \u2194 (v1,.,., vd), where vi: = < v, ti >.Definition 2. The following operations are permitted: T1."}, {"heading": "2.2. Motivating examples", "text": "s leave X-Rn \u00b7 d n datapoints {x (1),.., x-Rd \u00b7 PCA factorizes X-DP, where P is an (d \u00b7 d) -orthogonal matrix, and D-Diag (d) contains the eigenvalues of X XA common application of PCA dimensionality reduction. From a type perspective, this consists of: T {ek} P \u2212 \u2192 (i) T {pk} Proj \u2212 \u2212 \u2212 \u2212 pk} P (iiii) T {ek}, (i) transforms the standard orthogonal base {ek} dk = 1 of Rd into the latent type of P."}, {"heading": "2.3. Incoherent features in classical RNNs", "text": "In fact, most of them are unable to abide by the rules they have imposed on themselves."}, {"heading": "3. Recurrent Neural Networks", "text": "We present three highly typed RNNs that intentionally mimic classic RNNs as accurately as possible. Perhaps surprisingly, the optimizations presented below have profound structural implications and produce architectures that are much easier to understand, see Sections 3.3 and 3.4."}, {"heading": "3.1. Weakly-Typed RNNs", "text": "SCRNs or Structurally Constrained Recurrent Networks (Mikolov et al., 2015) add a type-consistent state layer: st = \u03b1 \u00b7 st \u2212 1 + (1 \u2212 \u03b1) \u00b7 Wsxt, where \u03b1 is a scale. In MUT1, the best functioning architecture in (Jozefowicz et al., 2015), the behavior of z and h is well typed, although the gating by r is not. Finally, IRNs initialize their identity connections as recursive (2015)."}, {"heading": "3.2. Strongly-Typed RNNs", "text": "T-RNN (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T-RNN (1) T (1) T (1) T (1) T (1) T-RNN (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1 T) T (1 T) T (1) T (1 T) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1) T (1 (1) T (1) T (1) T (1 (1) T (1) T (1) T (1) T (1) T (1 (1) T (1) T (1) T (1) T (1 (1) T (1 (1) T (1) T (1) T (1 (1) T (1) T (1 (1) T (1) T (1) T (1 (1) T (1) T (1 (1) T (1) T (1 (1) T (1) T (1) T (1 (1 (1) T (1) T (1) T (1 (1 (1) T (1) T (1 (1) T (1 (1) T (1) T (1 (1 (1) T (1) T (1"}, {"heading": "3.3. Feature Semantics", "text": "The results of the study are: \"The results of the study show that people in developing countries are not able to identify themselves.\" The results of the study show that people in developing countries are not able to integrate. \"\" The results of the study show that people in developing countries are not able to integrate themselves. \"\" The results of the study show that people in developing countries are not able to integrate themselves. \"\" The results of the study show that people in developing countries are not able to integrate themselves. \"\" The results of the study show that people in developing countries are not able to integrate themselves. \"\" The results of the study show that people in developing countries are not able to integrate themselves. \"The results of the study show that people in developing countries are not able to integrate themselves in developing countries.\" The results of the study show that people in developing countries are not able to integrate themselves. \"The results of the study show that people in developing countries are not able to integrate themselves in developing countries.\" The results of the study show that people in developing countries are not able to integrate themselves in developing countries. \"The results of the study show that people in developing countries are not able to integrate themselves.\" The results of the study show that people in developing countries are not able to integrate themselves in developing countries. \""}, {"heading": "3.4. Feature Algebra", "text": "It's not just the question of whether it's a way of dealing with it, but also the question of whether it's a way of dealing with it, the way it's dealt with in a way, the way it's dealt with in a way, the way it's dealt with in a way, the way it's dealt with in a way, the way it's dealt with in a way, the way it's dealt with in a way, the way it's dealt with in a way, the way it's dealt with in a way, the way it's dealt with in a way, the way it's dealt with in a way, the way it's dealt with in a way, the way it's dealt with in a way, the way it's dealt with in a way, the way it's dealt with in a way, the way it's dealt with in a way, and the way it's dealt with in a way, the way it's dealt with in a way, the way it's dealt with in a way it's dealt with in a way, the way it's dealt with in a way it's dealt with in a way, the way it's dealt with in a way it's dealt with in a way, the way it's dealt with in a manner, the way it's dealt with in a way it's dealt with in a way, the way it's dealt with in a way it's dealt with in a way, the way it's dealt with in a way it's dealt with in a manner, and dealt with in a manner it's dealt with in a way it's dealt with in a manner, and dealt with in a manner it's dealt with in a manner, and dealt with in a manner it's dealt with in a manner its dealt with in a manner, and dealt with in a manner its dealt with in a manner and dealt with in a manner its dealt with in a manner and dealt with in a manner its dealt with in a manner and dealt with in a manner and dealt with in a manner, and dealt with in a manner its dealt with in a manner and dealt with in a manner and dealt with in a manner and dealt with in a manner that's dealt with by the manner and dealt with by the dealt with and dealt with and dealt with by the dealt with by the dealt with by the manner and dealt with by the dealt with by the dealt with by the dealt with by the dealt with by the dealt with and dealt with by the dealt with"}, {"heading": "4. Experiments", "text": "We investigated the empirical performance of highly typed recurrent networks for sequence learning, evaluating performance at the character and text level. We conducted a series of proof-of-concept experiments, so the goal is not to compete with previous work or find the most powerful model under a specific hyperparameter setting, but rather to examine how the two architecture classes behave over a series of settings."}, {"heading": "4.1. Character-level Text Generation", "text": "The first task is to generate text from a sequence of characters by predicting the next character in a sequence. We used Leo Tolstoy's War and Peace (WP) dataset (Karpathy et al., 2015), which consists of 3,258,246 characters of English text, divided into train / valve / test sets with a ratio of 80 / 10 / 10. The characters are encoded in K-dimensional one-dimensional hot vectors, where K is the size of the vocabulary. We follow the basic experimental setting proposed in (Karpathy et al., 2015). Results are reported for two configurations: \"64\" and \"256,\" which refer to models that contain an equivalent number of parameters to a 1-layer LSTM with 64 and 256 cells per layer, respectively. Dropout regulation was applied only to the \"256\" models. The drop-out rate was taken from {0.1, based on a 1-layer LSTM containing 64 and 256 cells per layer, respectively."}, {"heading": "4.2. Word-level Text Generation", "text": "The second task was to perform word-level text generation by predicting the next word from a sequence of words. We used the Penn Treebank (PTB) data set (Marcus et al., 1993), which consists of 929K training words, 73K validation words and 82K test words, with a word size of 10K words. PTB data set is publicly available on the Internet. 3We followed the experimental setting in (Zaremba et al., 3 http: / www.fit.vutbr.cz / \u02dc imikolov / rnnlm / simple-examples.tgz2015) and compared the performance of the \"small\" and \"medium\" models. The parameter size of the \"small\" models corresponds to the 2 layers of the 200-cell LSTM, while the parameter size of the \"medium\" models is the same as the 2 layers of the 650-cell LSTM models."}, {"heading": "5. Conclusions", "text": "In fact, it is so that most people who are able to help themselves, to help themselves, to help themselves and to help themselves, to help themselves and to help themselves."}], "references": [{"title": "On the information-theoretic structure of distributed measurements", "author": ["D. Balduzzi"], "venue": "Elect. Proc. in Theor. Comp. Sci.,", "citeRegEx": "Balduzzi,? \\Q2012\\E", "shortCiteRegEx": "Balduzzi", "year": 2012}, {"title": "Evolving memory cell structures for sequence learning", "author": ["Bayer", "Justin", "Wierstra", "Daan", "Togelius", "Julian", "Schmidhuber", "Juergen"], "venue": "In ICANN,", "citeRegEx": "Bayer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bayer et al\\.", "year": 2009}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio", "Yoshua", "P Simard", "P. Frasconi"], "venue": "IEEE Trans. Neur. Net.,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "From machine learning to machine reasoning: An essay", "author": ["Bottou", "L\u00e9on"], "venue": "Machine Learning,", "citeRegEx": "Bottou and L\u00e9on.,? \\Q2014\\E", "shortCiteRegEx": "Bottou and L\u00e9on.", "year": 2014}, {"title": "Dimensional analysis", "author": ["Bridgman", "P W"], "venue": null, "citeRegEx": "Bridgman and W.,? \\Q1922\\E", "shortCiteRegEx": "Bridgman and W.", "year": 1922}, {"title": "Inventing Temperature: Measurement and Scientific Progress", "author": ["Chang", "Hasok"], "venue": null, "citeRegEx": "Chang and Hasok.,? \\Q2004\\E", "shortCiteRegEx": "Chang and Hasok.", "year": 2004}, {"title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation", "author": ["K Cho", "B van Merri\u00ebnboer", "C Gulcehre", "D Bahdanau", "F Bougares", "H Schwenk", "Y. Bengio"], "venue": "In EMNLP,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Proofs and Types", "author": ["Girard", "Jean-Yves"], "venue": null, "citeRegEx": "Girard and Jean.Yves.,? \\Q1989\\E", "shortCiteRegEx": "Girard and Jean.Yves.", "year": 1989}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves", "Alex", "A Mohamed", "Hinton", "GE"], "venue": "In ICASSP,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Neural Turing Machines", "author": ["Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo"], "venue": "In arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Learning to Transduce with Unbounded Memory", "author": ["Grefenstette", "Edward", "Hermann", "Karl Moritz", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "In Adv in Neural Information Processing Systems (NIPS),", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "LSTM: A Search Space Odyssey", "author": ["Greff", "Klaus", "Srivastava", "Rupesh Kumar", "Koutn\u0131\u0301k", "Jan", "Steunebrink", "Bas R", "Schmidhuber", "Juergen"], "venue": "In arXiv:1503.04069,", "citeRegEx": "Greff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "DRAW: A Recurrent Neural Network For Image Generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Rezende", "Danilo Jimenez", "Wierstra", "Daan"], "venue": "In ICML,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Multidimensional Analysis: Algebras and Systems for Science and Engineering", "author": ["Hart", "George W"], "venue": null, "citeRegEx": "Hart and W.,? \\Q1995\\E", "shortCiteRegEx": "Hart and W.", "year": 1995}, {"title": "Long Short-Term Memory", "author": ["S Hochreiter", "J. Schmidhuber"], "venue": "Neural Comp,", "citeRegEx": "Hochreiter and Schmidhuber,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Untersuchungen zu dynamischen neuronalen Netzen", "author": ["Hochreiter", "Sepp"], "venue": "Master\u2019s thesis, Tech. Universita\u0308t Mu\u0308nchen,", "citeRegEx": "Hochreiter and Sepp.,? \\Q1991\\E", "shortCiteRegEx": "Hochreiter and Sepp.", "year": 1991}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "In arXiv:1502.03167,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "An Empirical Exploration of Recurrent Network Architectures", "author": ["Jozefowicz", "Rafal", "Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "In ICML,", "citeRegEx": "Jozefowicz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "Deep Visual-Semantic Alignments for Generating Image Descriptions", "author": ["Karpathy", "Andrej", "Fei-Fei", "Li"], "venue": "In CVPR,", "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Visualizing and understanding recurrent neural networks", "author": ["Karpathy", "Andrej", "Johnson", "Justin", "Fei-Fei", "Li"], "venue": "In arXiv:1506.02078,", "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Batch Normalized Recurrent Neural Networks", "author": ["C Laurent", "G Pereyra", "P Brakel", "Y Zhang", "Bengio", "Yoshua"], "venue": "In arXiv:1510.01378,", "citeRegEx": "Laurent et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Laurent et al\\.", "year": 2015}, {"title": "A Simple Way to Initialize Recurrent Networks of Rectified Linear Units", "author": ["Le", "Quoc", "Jaitly", "Navdeep", "Hinton", "Geoffrey"], "venue": "In arXiv:1504.00941,", "citeRegEx": "Le et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Multilayer Feedforward Networks With a Nonpolynomial Activation Function Can Approximate Any Function", "author": ["Leshno", "Moshe", "Lin", "Vladimir Ya", "Pinkus", "Allan", "Schocken", "Shimon"], "venue": "Neural Networks,", "citeRegEx": "Leshno et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Leshno et al\\.", "year": 1993}, {"title": "Typing linear algebra: A biproduct-oriented approach", "author": ["Macedo", "Hugo Daniel", "Oliveira", "Jos\u00e9 Nuno"], "venue": "Science of Computer Programming,", "citeRegEx": "Macedo et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Macedo et al\\.", "year": 2013}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Marcus", "Mitchell P", "Marcinkiewics", "Mary Ann", "Santorini", "Beatrice"], "venue": "Comp. Linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Marc\u2019Aurelio. Learning Longer Memory in Recurrent Neural Networks", "author": ["Mikolov", "Tomas", "Joulin", "Armand", "Chopra", "Sumit", "Mathieu", "Michael", "Ranzato"], "venue": "In ICLR,", "citeRegEx": "Mikolov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2015}, {"title": "ACDC: A Structured Efficient Linear Layer", "author": ["Moczulski", "Marin", "Denil", "Misha", "Appleyard", "Jeremy", "de Freitas", "Nando"], "venue": "In arXiv:1511.05946,", "citeRegEx": "Moczulski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Moczulski et al\\.", "year": 2015}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Pascanu", "Razvan", "Mikolov", "Tomas", "Bengio", "Yoshua"], "venue": "In ICML,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "How to Construct Deep Recurrent Networks", "author": ["Pascanu", "Razvan", "Gulcehre", "Caglar", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In ICLR,", "citeRegEx": "Pascanu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2014}, {"title": "Types and Programming Languages", "author": ["Pierce", "Benjamin C"], "venue": null, "citeRegEx": "Pierce and C.,? \\Q2002\\E", "shortCiteRegEx": "Pierce and C.", "year": 2002}, {"title": "Towards a theory of type structure", "author": ["Reynolds", "J C"], "venue": "In Paris colloquium on programming,", "citeRegEx": "Reynolds and C.,? \\Q1974\\E", "shortCiteRegEx": "Reynolds and C.", "year": 1974}, {"title": "On the Computational Power of Neural Nets", "author": ["Siegelmann", "Hava", "Sontag", "Eduardo"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Siegelmann et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Siegelmann et al\\.", "year": 1995}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["I Sutskever", "O Vinyals", "Q. Le"], "venue": "In Adv in Neural Information Processing Systems (NIPS),", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["O Vinyals", "A Toshev", "S Bengio", "D. Erhan"], "venue": "In CVPR,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Recurrent Neural Network Regularization", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya", "Vinyals", "Oriol"], "venue": "In arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "Applications include speech recognition (Graves et al., 2013), image generation (Gregor et al.", "startOffset": 40, "endOffset": 61}, {"referenceID": 12, "context": ", 2013), image generation (Gregor et al., 2015), machine translation (Sutskever et al.", "startOffset": 26, "endOffset": 47}, {"referenceID": 32, "context": ", 2015), machine translation (Sutskever et al., 2014) and image captioning (Vinyals et al.", "startOffset": 29, "endOffset": 53}, {"referenceID": 33, "context": ", 2014) and image captioning (Vinyals et al., 2015; Karpathy & Fei-Fei, 2015).", "startOffset": 29, "endOffset": 77}, {"referenceID": 2, "context": "Training large RNNs can be difficult due to exploding and vanishing gradients (Hochreiter, 1991; Bengio et al., 1994; Pascanu et al., 2013).", "startOffset": 78, "endOffset": 139}, {"referenceID": 27, "context": "Training large RNNs can be difficult due to exploding and vanishing gradients (Hochreiter, 1991; Bengio et al., 1994; Pascanu et al., 2013).", "startOffset": 78, "endOffset": 139}, {"referenceID": 6, "context": "Researchers have therefore developed gradient-stabilizing architectures such as Long Short-Term Memories or LSTMs (Hochreiter & Schmidhuber, 1997) and Gated Recurrent Units or GRUs (Cho et al., 2014).", "startOffset": 181, "endOffset": 199}, {"referenceID": 1, "context": "Extensive searches (Bayer et al., 2009; Jozefowicz et al., 2015; Greff et al., 2015) have not yielded significant improvements.", "startOffset": 19, "endOffset": 84}, {"referenceID": 17, "context": "Extensive searches (Bayer et al., 2009; Jozefowicz et al., 2015; Greff et al., 2015) have not yielded significant improvements.", "startOffset": 19, "endOffset": 84}, {"referenceID": 11, "context": "Extensive searches (Bayer et al., 2009; Jozefowicz et al., 2015; Greff et al., 2015) have not yielded significant improvements.", "startOffset": 19, "endOffset": 84}, {"referenceID": 34, "context": "Two recent papers provide empirical evidence that recurrent (horizontal) connections are problematic even after gradients are stabilized: (Zaremba et al., 2015) find that Dropout performs better when restricted to vertical connections and (Laurent et al.", "startOffset": 138, "endOffset": 160}, {"referenceID": 20, "context": ", 2015) find that Dropout performs better when restricted to vertical connections and (Laurent et al., 2015) find that Batch Normalization fails unless restricted to vertical connections (Ioffe & Szegedy, 2015).", "startOffset": 86, "endOffset": 108}, {"referenceID": 20, "context": "More precisely, (Laurent et al., 2015) find that Batch Normalization improves training but not test error when restricted to vertical connections; it fails completely when also applied to horizontal connections.", "startOffset": 16, "endOffset": 38}, {"referenceID": 26, "context": "Finally, (Moczulski et al., 2015) propose to accelerate matrix computations in feedforward nets by interleaving diagonal matrices, A and D, with the orthogonal discrete cosine transform, C.", "startOffset": 9, "endOffset": 33}, {"referenceID": 25, "context": "SCRNs, or Structurally Constrained Recurrent Networks (Mikolov et al., 2015), add a type-consistent state layer:", "startOffset": 54, "endOffset": 76}, {"referenceID": 17, "context": "In MUT1, the best performing architecture in (Jozefowicz et al., 2015), the behavior of z and h is well-typed, although the gating by r is not.", "startOffset": 45, "endOffset": 70}, {"referenceID": 21, "context": "Finally, IRNNs initialize their recurrent connections as the identity matrix (Le et al., 2015).", "startOffset": 77, "endOffset": 94}, {"referenceID": 11, "context": "We drop the input gate from the updates for simplicity; see (Greff et al., 2015).", "startOffset": 60, "endOffset": 80}, {"referenceID": 22, "context": "A vanilla RNN can approximate any continuous state update ht = g(xt,ht\u22121) since span{s(wx) |w \u2208 R} is dense in continuous functions C(R) on R if s is a nonpolynomial nonlinear function (Leshno et al., 1993).", "startOffset": 185, "endOffset": 206}, {"referenceID": 34, "context": "Evidence that side-effects are a problem for LSTMs can be found in (Zaremba et al., 2015) and (Laurent et al.", "startOffset": 67, "endOffset": 89}, {"referenceID": 20, "context": ", 2015) and (Laurent et al., 2015), which show that Dropout and Batch Normalization respectively need to be restricted to vertical connections.", "startOffset": 12, "endOffset": 34}, {"referenceID": 22, "context": "Combining (Leshno et al., 1993) with the observation that a\u03c1(bh + z + c) = \u03c1(abh + az + ac) for a > 0 implies that span{\u03c1(b \u00b7 ht\u22121 + zt) | b, c \u2208 R} = C(R).", "startOffset": 10, "endOffset": 31}, {"referenceID": 22, "context": "Finally, vertical connections can approximate any set of features (Leshno et al., 1993).", "startOffset": 66, "endOffset": 87}, {"referenceID": 18, "context": "We used Leo Tolstoy\u2019s War and Peace (WP) dataset (Karpathy et al., 2015) which consists of 3,258,246 characters of English text, split into train/val/test sets with 80/10/10 ratios.", "startOffset": 49, "endOffset": 72}, {"referenceID": 18, "context": "We follow the basic experimental setting proposed in (Karpathy et al., 2015).", "startOffset": 53, "endOffset": 76}, {"referenceID": 18, "context": "However, our results (for both classical and typed models) fail to match those reported in (Karpathy et al., 2015), where a more extensive parameter search was performed.", "startOffset": 91, "endOffset": 114}, {"referenceID": 24, "context": "We used the Penn Treebank (PTB) dataset (Marcus et al., 1993), which consists of 929K training words, 73K validation words, and 82K test words, with vocabulary size of 10K words.", "startOffset": 40, "endOffset": 61}, {"referenceID": 34, "context": "82 medium, with dropout LSTM (Zaremba et al., 2015) 48.", "startOffset": 29, "endOffset": 51}, {"referenceID": 34, "context": "The dropout-regularized \u201cmedium\u201d T-LSTM matches the LSTM performance reported in (Zaremba et al., 2015).", "startOffset": 81, "endOffset": 103}, {"referenceID": 1, "context": "The complicated structure of LSTMs and GRUs has led to searches for simpler alternatives with limited success (Bayer et al., 2009; Greff et al., 2015; Jozefowicz et al., 2015; Le et al., 2015; Mikolov et al., 2015).", "startOffset": 110, "endOffset": 214}, {"referenceID": 11, "context": "The complicated structure of LSTMs and GRUs has led to searches for simpler alternatives with limited success (Bayer et al., 2009; Greff et al., 2015; Jozefowicz et al., 2015; Le et al., 2015; Mikolov et al., 2015).", "startOffset": 110, "endOffset": 214}, {"referenceID": 17, "context": "The complicated structure of LSTMs and GRUs has led to searches for simpler alternatives with limited success (Bayer et al., 2009; Greff et al., 2015; Jozefowicz et al., 2015; Le et al., 2015; Mikolov et al., 2015).", "startOffset": 110, "endOffset": 214}, {"referenceID": 21, "context": "The complicated structure of LSTMs and GRUs has led to searches for simpler alternatives with limited success (Bayer et al., 2009; Greff et al., 2015; Jozefowicz et al., 2015; Le et al., 2015; Mikolov et al., 2015).", "startOffset": 110, "endOffset": 214}, {"referenceID": 25, "context": "The complicated structure of LSTMs and GRUs has led to searches for simpler alternatives with limited success (Bayer et al., 2009; Greff et al., 2015; Jozefowicz et al., 2015; Le et al., 2015; Mikolov et al., 2015).", "startOffset": 110, "endOffset": 214}, {"referenceID": 0, "context": "We draw on two disparate intuitions that turn out to be surprisingly compatible: (i) that neural networks are analogous to measuring devices (Balduzzi, 2012) and (ii) that training an RNN is analogous to writing code.", "startOffset": 141, "endOffset": 157}, {"referenceID": 28, "context": "It was pointed out in (Pascanu et al., 2014) that unfolding horizontal connections over time implies the concept of depth is not straightforward in classical RNNs.", "startOffset": 22, "endOffset": 44}, {"referenceID": 9, "context": "Indeed, neural Turing machines (Graves et al., 2014) are harder to train than more constrained architectures such as neural queues and deques (Grefenstette et al.", "startOffset": 31, "endOffset": 52}, {"referenceID": 10, "context": ", 2014) are harder to train than more constrained architectures such as neural queues and deques (Grefenstette et al., 2015).", "startOffset": 97, "endOffset": 124}], "year": 2017, "abstractText": "Recurrent neural networks are increasing popular models for sequential learning. Unfortunately, although the most effective RNN architectures are perhaps excessively complicated, extensive searches have not found simpler alternatives. This paper imports ideas from physics and functional programming into RNN design to provide guiding principles. From physics we introduce type constraints, analogous to the constraints that disqualify adding meters to seconds in physics. From functional programming, we require that strongly-typed architectures factorize into stateless learnware and statedependent firmware, thereby ameliorating the impact of side-effects. The features learned by strongly-typed nets have a simple semantic interpretation via dynamic average-pooling on onedimensional convolutions. We also show that strongly-typed gradients are better behaved than in classical architectures, and characterize the representational power of strongly-typed nets. Finally, experiments show that, despite being more constrained, strongly-typed architectures achieve lower training error and comparable generalization error to classical architectures.", "creator": "LaTeX with hyperref package"}}}