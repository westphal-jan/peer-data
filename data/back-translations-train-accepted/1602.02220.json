{"id": "1602.02220", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2016", "title": "Improved Dropout for Shallow and Deep Learning", "abstract": "Dropout has been witnessed with great success in training deep neural networks by independently zeroing out the outputs of neurons at random. It has also received a surge of interest for shallow learning, e.g., logistic regression. However, the independent sampling for dropout could be suboptimal for the sake of convergence. In this paper, we propose to use multinomial sampling for dropout, i.e., sampling features or neurons according to a multinomial distribution with different probabilities for different features/neurons. To exhibit the optimal dropout probabilities, we analyze the shallow learning with multinomial dropout and establish the risk bound for stochastic optimization. By minimizing a sampling dependent factor in the risk bound, we obtain a distribution-dependent dropout with sampling probabilities dependent on the second order statistics of the data distribution. To tackle the issue of evolving distribution of neurons in deep learning, we propose an efficient adaptive dropout (named \\textbf{evolutional dropout}) that computes the sampling probabilities on-the-fly from a mini-batch of examples. Empirical studies on several benchmark datasets demonstrate that the proposed dropouts achieve not only much faster convergence and but also a smaller testing error than the standard dropout. For example, on the CIFAR-100 data, the evolutional dropout achieves relative improvements over 10\\% on the prediction performance and over 50\\% on the convergence speed compared to the standard dropout.", "histories": [["v1", "Sat, 6 Feb 2016 05:41:57 GMT  (145kb,D)", "http://arxiv.org/abs/1602.02220v1", null], ["v2", "Sun, 4 Dec 2016 05:31:19 GMT  (454kb)", "http://arxiv.org/abs/1602.02220v2", "In NIPS 2016"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["zhe li", "boqing gong", "tianbao yang"], "accepted": true, "id": "1602.02220"}, "pdf": {"name": "1602.02220.pdf", "metadata": {"source": "META", "title": "Improved Dropout for Shallow and Deep Learning ", "authors": ["Zhe Li", "Boqing Gong", "Tianbao Yang"], "emails": ["ZHE-LI-1@UIOWA.EDU", "BGONG@CRCV.UCF.EDU", "TIANBAO-YANG@UIOWA.EDU"], "sections": [{"heading": "1. Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2. Related Work", "text": "In this section, we review some of the related work on dropout, optimization algorithms for deep learning and distribution-dependent or sampling-dependent fault bores. Dropout is a simple but effective method to prevent the proliferation of deep neural networks (Srivastava et al., 2014). However, it has received much attention from researchers to investigate its practical and theoretical properties, especially the simple form of dropout is to multiply hidden units by i.d Bernoulli noise. Several recent work has also found that it works with different types of noise than Bernoulli noise (e.g., Gaussian noise), which could lead to a better approximation of marginalized losses (Wang & Manning, 2015)."}, {"heading": "3. Preliminaries", "text": "In this section, we introduce some precursor models, including the framework of risk minimisation in machine learning and learning with failures. (...) We also ask ourselves how we share the common distribution of (...), (...) and (...) (...) (...) (...) (...) and (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (... (...) (...) (...) (... (...) (...) (...) (...) (... (...) (...) (... (...) (...) (...) (... (...) (...) (...) (...) (... (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (... (...) (...) (...) (...) (... (...) (... (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (..."}, {"heading": "4. Learning with Multinomial Dropout", "text": "In this section, we analyze a stochastic optimization approach to minimize the risk of default in (2). Suppose the probabilities of selection are known. (3) We first get a risk associated with a multinomial risk of default. (2) Then, we try to minimize the risk factors that depend on the probabilities of selection. (3) We would like to emphasize that our goal here is not to show that the use of a default risk would represent a lesser risk than without the use of default risks, but rather to focus on the impact of different probabilities of selection on risk. (3) Let the initial solution be w1."}, {"heading": "4.1. Distribution Dependent Dropout", "text": "Next, we look at the sampling-dependent factors in the risk ranges. From theorem 1, we can see that there are two terms that depend on the sampling probabilities, i.e., B2 - the upper limit of ED [2], and RD, M (w), M (w), ED (w), M (w). We note that the second term also depends on w (w) and w (n), which is more difficult to optimize. We first try ED (b), ED (b), ED (2) and present the discussion about minimizing RD, M (w) later. From theorem 1, we can see that minimizing ED (2) would not only lead to a smaller risk (given the same number of overall examples, smaller ED)."}, {"heading": "4.2. Evolutional Dropout for Deep Learning", "text": "Next, we will discuss how to implement the distribution-dependent dropouts for deep learning. In deep neural network formation, the dropout is usually added to the intermediate layers (e.g., fully connected layers and revolutionary layers). Let xl = (xl1,.., x l d) denote the results of the l-th layer (with the index of omitted data). Adding dropouts to this layer is equivalent to multiplying xl by a dropout vector l, i.e., feeding x = xl \"l as input to the next layer. Inspired by the data-dependent dropout step, we can generate l according to a distribution calculated in definition 1 with sampling probabilities pli., which we derive from {xl1,., xln} similar to the (9). However, in-depth learning is usually optimized with large data and a deep neural network."}, {"heading": "5. Experimental Results", "text": "In the section, we present some experimental results to justify the proposed dropouts. In all experiments, we put \u03b4 = 0.5 in the standard dropout and k = 0.5d in the proposed dropouts for a fair comparison, where d represents the number of characteristics or neurons of the layer to which the dropout is applied. For clarity, we divided the experiments into three parts. In the first part, we compare the performance of the data-dependent dropout (d-dropouts) with the standard dropout (s-dropouts) for logistic regression. In the second part, we compare the performance of the evolutionary dropout (e-dropouts) with the standard dropout for the formation of deep, revolutionary neural networks. Finally, we compare e-dropouts with the batch normalization."}, {"heading": "5.1. Shallow Learning", "text": "In order to evaluate the performance of data-dependent dropouts for shallow learning, we used the three data sets: Real-Sim, News20 and RCV1 3. Table 1 summarizes the statistics of the three data sets. In this experiment, we use a fixed step size and adjust the step size in [0,1, 0,05, 0,01, 0,0005, 0,0001] and report the best results in terms of the convergence speed of training data for both standard dropouts and data-dependent dropouts. Figure 2 shows the results obtained from these three data sets. In each image, we show both the training error and the test error. We can see that both the training error and the test error decrease much faster when using the proposed data-dependent dropout than when using the standard dropout, and also a smaller test error is achieved by using the data-dependent dropout."}, {"heading": "5.2. Evolutional Dropout for Deep Learning", "text": "We would like to emphasize that we are not striving for better forecasting performance by trying different net 3https: / / www.csie.ntu.edu.tw / \u02dc cjlin / libsvmtools / datasets / work structures and various technical tricks such as data augmentation, whitening, etc., but rather focusing on comparing the proposed dropout with the standard dropout using Bernoulli noise on the same network structure. In the next three subsections, we present the results using three benchmark data sets to compare E-Dropout and s-Dropout: MNIST (LeCun et al., 1998), CIFAR-10 and CIFAR-100 (Krizhevsky & Hinton, 2009). We are using the same or similar network structure as in the literature for the three data sets described separately for different data sets."}, {"heading": "5.2.1. MNIST", "text": "The MNIST dataset (LeCun et al., 1998) has 60,000 training images and 10,000 test images. Each 28 x 28 image in this dataset represents a handwritten digits4https: / / code.google.com / archive / p / cuda-convennet / 0 x 9. The task is to classify each image in the test dataset into 10 categories. We used the similar structure of neural networks to (Wan et al., 2013): two folding layers, two fully connected layers, a Softmax layer, and a cost layer at the end. The dropout is added to the first fully connected layer. The initial learning rate for standard dropouts is set to 0.01 (Wan et al., 2013), and for evolutionary dropouts to 0.1. Figure 3 (a) shows the training error and test error in MNIST data based on the standard dropout and the evolutionary dropout error. We can see that the dropout error is much faster than the dropout error."}, {"heading": "5.2.2. CIFAR-10", "text": "The CIFAR-10 dataset (Krizhevsky & Hinton, 2009) includes 50,000 training images and 10,000 test images belonging to one of 10 classes, each with a total of 6,000 images. Each image in this dataset is in RGB format and has a size of 32 x 32. The structure of the neural network has been adopted from (Krizhevsky & Hinton, 2009), which consists of 2 folding layers, 2 max pooling layers, 2 local response normalization layers, 2 locally connected layers, 2 fully connected layers and a softmax and a cost layer. Dropout is added to the first fully connected layer. The initial learning rate for the standard dropout is set to 0.001, and that for the evolutionary dropout to 0.01. Figure 3 (b) shows the training and testing errors on CIFAR-10 data using the standard dropout and the evolutionary dropout."}, {"heading": "5.2.3. CIFAR-100", "text": "CIFAR-100 (Krizhevsky & Hinton, 2009) dataset is similar to CIFAR-10 except that there are 100 classes. The network structure for this dataset is similar to CIFAR-10, expect the size of input and output to be different in the last fully connected layer and the remaining layers. The initial learning rate for the standard dropout is 0.001 similar to CIFAR-10, and that for the evolutionary dropout is set to 0.01. Figure 3 (c) shows the training and test errors on the CIFAR-100 dataset. Results show that the evolutionary dropout not only significantly accelerates convergence, but also significantly reduces the test error."}, {"heading": "5.3. Comparison with the Batch Normalization (BN)", "text": "For batch normalization, we use the implementation in Caffe 5. We compare the evolutionary dropout with the batch normalization on the CIFAR 10 dataset. The network structure comes from the Caffe package and can be found in the appendix, which is different from the one used in the previous experiment. It contains three revolutionary layers and a completely connected layer. Each revolutionary layer is followed by a pooling layer. We compare three methods: (i) No BN and No Dropout - without the use of batch normalization and dropout; (ii) BN; (ii) Evolutional Dropout. For BN, three batch normalization layers are inserted before or after each pooling layer, following the architecture of the Caffe package."}, {"heading": "6. Conclusion", "text": "Theoretically, we have proven that the new breaker achieves a lower risk and faster convergence. Based on the distribution-dependent breaker, we developed an efficient evolutionary breaker for the formation of deep neural networks that adapts the sampling probabilities to the evolving distributions of the layer results. Experimental results from various data sets confirmed that the proposed breakers can dramatically improve convergence and also reduce the test error."}, {"heading": "A. Proof of Theorem 1", "text": "The updated version of wt + 1 = wt > t (w > t =), yt) can be considered the stochastic gradient lineage (SGD). \u2212 n [t] Define gt = [t] (w > (w > (x), y)] Define gt = (w > t) (w > t (xt), yt) = \"t\" (t) (w > t). \u2212 t [t) Define gt = \"t\" (w > w). (w > t) Define gt = \"t\" (w > t). (t) Definition of \"t.\" (t) Definition of \"t.\" (z, y) Definition of \"t.\" (t) Definition of \"t.\" Since the loss function G-Lipschitz is ongoing, therefore \"t.\" (t). (t) Definition of \"t.\" (t) Definition of \"t.\" (t) Definition of \"t.\""}, {"heading": "B. Proof of Proposition 4", "text": "From Eqn. (4) in the work we have Eqn. (5) in the work, we have Eqn. (5) in the work, we have Eqn. (d) in the work, we have Eqn. (d) in the work, we have Eqn. (d) in the work, we have Eqn. (w > 5) in the work. (c) This results in a narrow boundary of R. D, M (w), i.e., R. D, M (w). 18k. (d). (d). i = 1 w2 ED [d]. ix 2 i pi \u2212 (w > x). (b > x) 2). By minimizing the upper boundary above Pi we get the following probabilities."}, {"heading": "C. Neural Network Structures", "text": "Tables 3, 4, and 5 show the neural network structures and the number of filters, filter size, padding, and step parameters for MNIST, CIFAR-10, and CIFAR-100, respectively, and Tables 6 and 7 show the network structures of various methods in Section 5.3 of the essay. Note that in Tables 4 and 5, the rnorm layer is the local response normalization layer, and the local layer is the locally connected layer with undivided weights. The layer pool (ave) in Tables 6 and 7 represents the average pool layer."}], "references": [{"title": "Understanding dropout", "author": ["Baldi", "Pierre", "Sadowski", "Peter J"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Baldi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Baldi et al\\.", "year": 2013}, {"title": "Dropout training for support vector machines", "author": ["Chen", "Ning", "Zhu", "Jun", "Jianfei", "Zhang", "Bo"], "venue": "In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Efficient batchwise dropout training using submatrices", "author": ["Graham", "Benjamin", "Reizenstein", "Jeremy", "Robinson", "Leigh"], "venue": "CoRR, abs/1502.02478,", "citeRegEx": "Graham et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Graham et al\\.", "year": 2015}, {"title": "On the inductive bias of dropout", "author": ["Helmbold", "David P", "Long", "Philip M"], "venue": "CoRR, abs/1412.4736,", "citeRegEx": "Helmbold et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Helmbold et al\\.", "year": 2014}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik P", "Ba", "Jimmy"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Variational dropout and the local reparameterization trick", "author": ["Kingma", "Diederik P", "Salimans", "Tim", "Welling", "Max"], "venue": "CoRR, abs/1506.02557,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Learning multiple layers of features from tiny", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Attribute efficient linear regression with distribution-dependent sampling", "author": ["Kukliansky", "Doron", "Shamir", "Ohad"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning (ICML),", "citeRegEx": "Kukliansky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kukliansky et al\\.", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Optimizing neural networks with kronecker-factored approximate curvature", "author": ["Martens", "James", "Grosse", "Roger"], "venue": "arXiv preprint arXiv:1503.05671,", "citeRegEx": "Martens et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Martens et al\\.", "year": 2015}, {"title": "Path-sgd: Path-normalized optimization in deep neural networks", "author": ["Neyshabur", "Behnam", "Salakhutdinov", "Ruslan R", "Srebro", "Nati"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Neyshabur et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neyshabur et al\\.", "year": 2015}, {"title": "Factored 3-way restricted boltzmann machines for modeling natural images", "author": ["Ranzato", "Marc\u2019Aurelio", "Krizhevsky", "Alex", "Hinton", "Geoffrey E"], "venue": "In AISTATS,", "citeRegEx": "Ranzato et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2010}, {"title": "Stochastic convex optimization", "author": ["Shalev-Shwartz", "Shai", "Shamir", "Ohad", "Srebro", "Nathan", "Sridharan", "Karthik"], "venue": "In The 22nd Conference on Learning Theory (COLT),", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2009}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Sutskever", "Ilya", "Martens", "James", "Dahl", "George", "Hinton", "Geoffrey"], "venue": "In Proceedings of the 30th international conference on machine learning", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Dropout training as adaptive regularization", "author": ["Wager", "Stefan", "Wang", "Sida", "Liang", "Percy S"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Wager et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wager et al\\.", "year": 2013}, {"title": "Altitude training: Strong bounds for singlelayer dropout", "author": ["Wager", "Stefan", "Fithian", "William", "Wang", "Sida", "Liang", "Percy S"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Wager et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wager et al\\.", "year": 2014}, {"title": "Regularization of neural networks using dropconnect", "author": ["Wan", "Li", "Zeiler", "Matthew", "Zhang", "Sixin", "Cun", "Yann L", "Fergus", "Rob"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}, {"title": "Fast dropout training", "author": ["Wang", "Sida", "Manning", "Christopher"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "An explicit sampling dependent spectral error bound for column subset selection", "author": ["Yang", "Tianbao", "Zhang", "Lijun", "Jin", "Rong", "Zhu", "Shenghuo"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning (ICML),", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Deep learning with elastic averaging sgd", "author": ["Zhang", "Sixin", "Choromanska", "Anna", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1412.6651,", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Adaptive dropout rates for learning with corrupted features", "author": ["Zhuo", "Jingwei", "Zhu", "Jun", "Zhang", "Bo"], "venue": "In IJCAI, pp", "citeRegEx": "Zhuo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhuo et al\\.", "year": 2015}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["Zinkevich", "Martin"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Zinkevich and Martin.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich and Martin.", "year": 2003}], "referenceMentions": [{"referenceID": 9, "context": "Dropout has been widely used to avoid overfitting of deep neural networks with a large number of parameters (Krizhevsky et al., 2012; Srivastava et al., 2014), which usually identically and independently at random samples neurons and sets their outputs to be zeros.", "startOffset": 108, "endOffset": 158}, {"referenceID": 4, "context": "Extensive experiments (Hinton et al., 2012) have shown that dropout can help obtain the state-of-the-art performance on a range of benchmark data sets.", "startOffset": 22, "endOffset": 43}, {"referenceID": 18, "context": ", equivalent to a form of data-dependent regularizer) (Wager et al., 2013).", "startOffset": 54, "endOffset": 74}, {"referenceID": 7, "context": ", Gaussian noise), which could lead to a better approximation of the marginalized loss (Wang & Manning, 2013; Kingma et al., 2015).", "startOffset": 87, "endOffset": 130}, {"referenceID": 24, "context": "Some works tried to optimize the hyper-parameters that define the noise level in a Bayesian framework (Zhuo et al., 2015; Kingma et al., 2015).", "startOffset": 102, "endOffset": 142}, {"referenceID": 7, "context": "Some works tried to optimize the hyper-parameters that define the noise level in a Bayesian framework (Zhuo et al., 2015; Kingma et al., 2015).", "startOffset": 102, "endOffset": 142}, {"referenceID": 19, "context": "Other studies focus on shallow learning with dropout noise (Wager et al., 2014; Helmbold & Long, 2014; Chen et al., 2014).", "startOffset": 59, "endOffset": 121}, {"referenceID": 1, "context": "Other studies focus on shallow learning with dropout noise (Wager et al., 2014; Helmbold & Long, 2014; Chen et al., 2014).", "startOffset": 59, "endOffset": 121}, {"referenceID": 12, "context": "Dropout is a simple yet effective technique to prevent overfitting in training deep neural networks (Srivastava et al., 2014). It has received much attention recently from researchers to study its practical and theoretical properties. Notably, Wager et al. (2013); Baldi & Sadowski (2013) have analyzed the dropout from a theoretical viewpoint and found that dropout is equivalent to a data-dependent regularizer.", "startOffset": 101, "endOffset": 264}, {"referenceID": 12, "context": "Dropout is a simple yet effective technique to prevent overfitting in training deep neural networks (Srivastava et al., 2014). It has received much attention recently from researchers to study its practical and theoretical properties. Notably, Wager et al. (2013); Baldi & Sadowski (2013) have analyzed the dropout from a theoretical viewpoint and found that dropout is equivalent to a data-dependent regularizer.", "startOffset": 101, "endOffset": 289}, {"referenceID": 1, "context": "Graham et al. (2015) used the same noise across a batch of examples in order to speed-up the computation.", "startOffset": 0, "endOffset": 21}, {"referenceID": 17, "context": "Recently, there emerge a battery of studies trying to accelearte the optimization of deep learning (Sutskever et al., 2013; Neyshabur et al., 2015; Zhang et al., 2014; Martens & Grosse, 2015; Ioffe & Szegedy, 2015; Kingma & Ba, 2014), which tackle the problem from", "startOffset": 99, "endOffset": 233}, {"referenceID": 13, "context": "Recently, there emerge a battery of studies trying to accelearte the optimization of deep learning (Sutskever et al., 2013; Neyshabur et al., 2015; Zhang et al., 2014; Martens & Grosse, 2015; Ioffe & Szegedy, 2015; Kingma & Ba, 2014), which tackle the problem from", "startOffset": 99, "endOffset": 233}, {"referenceID": 23, "context": "Recently, there emerge a battery of studies trying to accelearte the optimization of deep learning (Sutskever et al., 2013; Neyshabur et al., 2015; Zhang et al., 2014; Martens & Grosse, 2015; Ioffe & Szegedy, 2015; Kingma & Ba, 2014), which tackle the problem from", "startOffset": 99, "endOffset": 233}, {"referenceID": 22, "context": "Yang et al. (2015) developed sampling-dependent approximation error bound for column subset selection problem and tried to optimize the sampling-dependent error bound to obtain optimal sampling probabilities.", "startOffset": 0, "endOffset": 19}, {"referenceID": 22, "context": "Yang et al. (2015) developed sampling-dependent approximation error bound for column subset selection problem and tried to optimize the sampling-dependent error bound to obtain optimal sampling probabilities. Kukliansky & Shamir (2015) proposed a similar distributiondependent sampling for attribute-efficient learning in linear regression.", "startOffset": 0, "endOffset": 236}, {"referenceID": 18, "context": "In standard dropout (Wager et al., 2013; Hinton et al., 2012), the entries of the noise vector are sampled independently according to Pr( j = 0) = \u03b4 and Pr( j = 1 1\u2212\u03b4 ) = 1 \u2212 \u03b4, i.", "startOffset": 20, "endOffset": 61}, {"referenceID": 4, "context": "In standard dropout (Wager et al., 2013; Hinton et al., 2012), the entries of the noise vector are sampled independently according to Pr( j = 0) = \u03b4 and Pr( j = 1 1\u2212\u03b4 ) = 1 \u2212 \u03b4, i.", "startOffset": 20, "endOffset": 61}, {"referenceID": 18, "context": "Dropout is a data-dependent regularizer Dropout as a regularizer has been studied in (Wager et al., 2013; Baldi & Sadowski, 2013) for logistic regression, which is stated in the following proposition for ease of discussion later.", "startOffset": 85, "endOffset": 129}, {"referenceID": 18, "context": "Using the second order Taylor expansion, (Wager et al., 2013) showed that the following approximation of RD,M(w) is easy to manipulate and understand:", "startOffset": 41, "endOffset": 61}, {"referenceID": 14, "context": "In this sense, our theoretical analysis also explains why Z-normalization usually speeds up the training (Ranzato et al., 2010).", "startOffset": 105, "endOffset": 127}, {"referenceID": 11, "context": "In the next three subsections, we present the results on three benchmark data sets for comparing e-dropout and s-dropout: MNIST (LeCun et al., 1998), CIFAR-10 and CIFAR-100 (Krizhevsky & Hinton, 2009).", "startOffset": 128, "endOffset": 148}, {"referenceID": 9, "context": "The training procedure is similar to (Krizhevsky et al., 2012), that is using mini-batch SGD with momentum (0.", "startOffset": 37, "endOffset": 62}, {"referenceID": 9, "context": ", step size) is decreased after a number of epochs similar to what was done in previous works (Krizhevsky et al., 2012).", "startOffset": 94, "endOffset": 119}, {"referenceID": 11, "context": "The MNIST (LeCun et al., 1998) data set has 60,000 training images and 10,000 testing images.", "startOffset": 10, "endOffset": 30}, {"referenceID": 20, "context": "We used the similar neural network structure to (Wan et al., 2013): two convolution layers, two fully connected layers, a softmax layer and a cost layer at the end.", "startOffset": 48, "endOffset": 66}, {"referenceID": 20, "context": "01 the same to (Wan et al., 2013), and that for evolutional dropout is set to 0.", "startOffset": 15, "endOffset": 33}], "year": 2017, "abstractText": "Dropout has been witnessed with great success in training deep neural networks by independently zeroing out the outputs of neurons at random. It has also received a surge of interest for shallow learning, e.g., logistic regression. However, the independent sampling for dropout could be suboptimal for the sake of convergence. In this paper, we propose to use multinomial sampling for dropout, i.e., sampling features or neurons according to a multinomial distribution with different probabilities for different features/neurons. To exhibit the optimal dropout probabilities, we analyze the shallow learning with multinomial dropout and establish the risk bound for stochastic optimization. By minimizing a sampling dependent factor in the risk bound, we obtain a distribution-dependent dropout with sampling probabilities dependent on the second order statistics of the data distribution. To tackle the issue of evolving distribution of neurons in deep learning, we propose an efficient adaptive dropout (named evolutional dropout) that computes the sampling probabilities on-the-fly from a mini-batch of examples. Empirical studies on several benchmark datasets demonstrate that the proposed dropouts achieve not only much faster convergence and but also a smaller testing error than the standard dropout. For example, on the CIFAR-100 data, the evolutional dropout achieves relative improvements over 10% on the prediction performance and over 50% on the convergence speed compared to the standard dropout.", "creator": "LaTeX with hyperref package"}}}