{"id": "1602.02262", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2016", "title": "Recovery guarantee of weighted low-rank approximation via alternating minimization", "abstract": "Many applications require recovering a ground truth low-rank matrix from noisy observations of the entries. In practice, this is typically formulated as weighted low-rank approximation problem and solved using non-convex optimization heuristics such as alternating minimization. Such non-convex techniques have little guarantees. Even worse, weighted low-rank approximation is NP-hard for even the most simple case when the ground truth is a rank-1 matrix.", "histories": [["v1", "Sat, 6 Feb 2016 14:55:12 GMT  (429kb)", "https://arxiv.org/abs/1602.02262v1", null], ["v2", "Thu, 8 Dec 2016 17:05:41 GMT  (600kb)", "http://arxiv.org/abs/1602.02262v2", "40 pages. Updated with the ICML 2016 camera ready version, together with an additional algorithm which needs less assumptions in Appendix C"]], "reviews": [], "SUBJECTS": "cs.LG cs.DS stat.ML", "authors": ["yuanzhi li", "yingyu liang", "andrej risteski"], "accepted": true, "id": "1602.02262"}, "pdf": {"name": "1602.02262.pdf", "metadata": {"source": "CRF", "title": "Recovery guarantee of weighted low-rank approximation via alternating minimization", "authors": ["Yuanzhi Li", "Yingyu Liang", "Andrej Risteski"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 160 2.02 262v 2 [cs.L G] 8D ec2 01The central technical challenge is that for non-binary deterministic weights, five alternating steps destroy the incoherence and spectral properties of the intermediate solutions necessary to make progress towards soil truth. We show that the properties only need to be maintained in the average sense and can be achieved through the clipping step. We also provide an alternating algorithm that uses a whitening step that maintains the properties by means of SDP and Rademacher rounding and therefore requires weaker assumptions. This technique can potentially be used in some other applications and is of independent interest."}, {"heading": "1 Introduction", "text": "In this year it is so far that it concerns a pure \"yes,\" a \"yes,\" a \"yes,\" a \"no,\" a \"yes,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"\" no, \"a\" \"no,\" a \"no,\" a \"a\" yes, \"a\", \"a\" a \"yes,\" a \"a\" yes, \"a\" yes, \"a\" yes, \"a\" yes, \"a\" a \"yes,\" a \"a\" a \"yes,\" a \"a\" a \"a\" \"\" \"a\" \"\" \"yes,\" a \"a\" a \",\" a \"a\" a \"a\" \"\" \"\" \"\" a \"\" \"\" \"\", \"a\" a \"a\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" a \",\" a \"a\" a \"\" a \"a\" \"\" \"\" \"\" \"\" \",\" a \"a\" a \"a\" \"\" \"\" a \"\" \"\" a \",\" a \"a\" a \"\" a \"\" \"a\", \"a\" a \"\" a \"\" \"a\" \",\" a \"a\" \"a\" a \"\" \"\" a \"a\" \"\" \",\" a \"a\" a \"a\", \"a\" a \"a\" \"\" a \"\" \"\" \",\" a \"a\" a \"a\" \"\" \"a\" \"\" \"\", \"a\" \"a\" \"a\" \"\" a \"\" a \"\" \"\" \"\" a \"\", \"a\" \"a\" \"a\" \"\" a \"\" \"\" \",\" a \"\" a \"\" \"a\" \"\" \"\" a \",\" a \"\" \"\" a \"\" a \"\" \"\" \"\" \",\" a \"a\" \"a\""}, {"heading": "2 Related work", "text": "In fact, it is so that most people are able to know themselves and to understand how they have behaved. (...) In fact, it is so that they are able to surpass themselves. \"(...)\" It is not as if it were to exaggerate. \"(...)\" It is not as if. \"(...)\" It is as if. \"(...)\" It is as if it were. \"(...)\" It is as if \"(...)\" (...) \"It is as if\" (...) \"(...)\" It is as if. \"(...)\" (\")\" It is as if. \"(...)\" (\")\" (\")\" ((\")\" ((\")\" (\") ((()\" (\") ((\") ((\") (() (\" () (\"(\") (\") (\" (\") (\") (\"(\") (\") (\") (\"(\") (\") (\") (\") (\" (\") (\") (\") (\" (\") (\") (\") (\" (\") (\") (\") () (\" ()."}, {"heading": "3 Problem definition and assumptions", "text": "For a matrix A, let Ai write her i-th column, Aj her j-th line, and Ai, j the element in the i-th line and j-th column. Let's note M = M + N, where N is a noise matrix, so we can get the down-to-earth truth M + 1 by solving the weighted approximation problems for M and a non-negative weight matrix W: min M - M + N, where Rk is the set of rank-k n of n matrices, and n matrices, and j Wi, jA 2 i, j is the weighted Frobenius norm."}, {"heading": "4 Algorithm and results", "text": "We prove guarantees for vanilla minimization with a simple clipping step starting from either SVD initialization or random initialization. (The algorithms are defined in algorithm 1.) Overall, it follows the usual alternating algorithm 1 (ALT) or Y1 (RANDINITIAL 2: for t = 1,..., T do 3: X: X + 1 = argminX = argminX. (The algorithms 2: 1). The algorithms 2: X + 1 (the algorithms 1: 2), T do 3: X + 1 = argminX. (The algorithms 2: XY + 1). (The algorithms 2: X + 1). (The algorithms 2: X + 1). (The algorithms 2: X + 1). (XY + 1: X + 1). (XT + 1). (XY + 1: Y + 1). (XY + 1 + 1)."}, {"heading": "The running time is polynomial in n and log(1/\u01eb).", "text": "To emphasize the dependence on matrix quantity n, precedence k and incoherence \u00b5, we can consider a specific range of parameter values where the other parameters (spectral limits, condition number, D1 / n) are constants. These parameter values are also typical of matrix completion, which facilitates our comparison in the next subsection.Corollary 2: Adopted quantities are all constants, D1 = number (n), and T = O (log (1 / 2)). Moreover, these parameter values are typical of matrix completion, which facilitates our comparison in the next subsection."}, {"heading": "The running time is polynomial in n and log(1/\u01eb).", "text": "Compared to SVD initialization, we need somewhat stronger assumptions for random initialization to work. There is an additional 1 / (\u00b51 / 2k1 / 2) in the request for the spectral parameter \u03b3. We note that the same error limit is reached when random initialization is used. Roughly speaking, this is due to the fact that our analysis shows that the updates can make improvements under rather weak requirements that random initialization can fulfill, and after the first step, the other updates make the same progress as in the case of SVD initialization."}, {"heading": "4.1 Comparison with prior work", "text": "In order to restore the basic truths, we will make a more detailed comparison with the results of the matrix study, in which we have emphasized the dependence on n, k, and p, as well as on other parameters as constants. (...) First of all, we will find that the m entries observed on the n matrix, which has the corresponding binary weight matrix, can only be insufficiently evaluated. (...) We see that we have a worse dependence on parameters such as those of convex relativization, but a slightly better dependence than those of alternative minimizations. The comparison is shown in Table 1.The seminal paper [Cande] s und Recht, 2009] that nuclear norm relaxation can restore the fundamental truth. (...)"}, {"heading": "5 Proof sketch", "text": "This year, it has come to the point where it has never happened before."}, {"heading": "5.1 Update", "text": "We would like to show that the new matrix X is already invading the correct space. (X, U). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D)."}, {"heading": "5.2 Proofs of main results", "text": "We just need to show that we can bring an initialization close enough to the basic truth to be able to apply the above analysis to the update. In the case of SVD initialization [X, \u03a3, Y] = rank-k SVD (W-M * + W-N). Since the SVD initialization can be considered small, the idea is to show that W-M * comes close to the spectral standard M * and then apply Wedin's theorem [Wedin, 1972]. We show this by looking at the spectral gap property of W and the incoherent display property of U, V. In the case of random initialization, the evidence is only a slight modification of that for SVD initialization, since the update requires rather mild conditions for initialization, so that even the random initialization is sufficient (with slightly worse parameters)."}, {"heading": "6 Conclusion", "text": "In this paper, we presented the first guarantee for restoring a weighted low-value matrix approximation by alternating minimization. Our work generalized previous work on matrix completion and revealed technical obstacles to the analysis of alternating minimization, i.e. the incoherence and spectral properties of the intermediate iteration rate must be preserved. We addressed the obstacles through a simple clipping step that led to a very simple algorithm that is almost identical to practical heuristics."}, {"heading": "Acknowledgements", "text": "This work was partially supported by NSF grants CCF-1527371, DMS-1317308, Simons Investigator Award, Simons Collaboration Grant and ONR-N00014-16-1-2329."}, {"heading": "A Preliminaries about subspace distance", "text": "Before turning to the proofs, we will define some simple preliminaries over partial space angles / distances (Q = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y. Definition (A = Y, V) = Y \u2212 V \u2212 V = Y \u2212 V = Y \u2212 V = Y \u2212 V, Y = Y = Y, Y = Y = Y, Y = Y = Y, Y = Y = Y, Y = Y = Y, Y = Y = Y \u00b2, V = Y = Y, Y = Y = Y, Y = Y = Y, Y = Y = Y, Y = Y = Y, Y = Y = Y, Y = Y = Y, Y = V = Y, Y = Y = Y, Y = V = Y = Y, Y = Y = Y, Y = Y = Y \u00b2, Y = Y = Y \u00b2, Y = Y = Y = Y = Y = Y = Y = Y = Y, Y = Y = Y = Y = Y = Y = Y = Y = V, Y = Y = Y = Y = Y = Y = V = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y, Y = Y = Y = Y = Y = Y \u00b2, Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y, Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y, Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y, Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y ="}, {"heading": "B Proofs for alternating minimization with clipping", "text": "In this section we will show the results for our algorithm, which is based on alternating minimization with a cutout step. Organization is as follows: In section B.1 we will present the necessary terms for initialization, in section B.3 we will show the reduction of the potential function after an update step, and in section B.4 we will put everything together and prove our main theory. Before we start with the proofs, we will make a remark that will simplify the exposure. Without loss of generality, we can assume that????????????????????????????????????????????? (B.1) Otherwise we can issue the 0 matrix and the warranty of all our theories would be fulfilled in a vacuum."}, {"heading": "B.1 SVD-based initialization", "text": "We want to show that after initialization, the matrices X, Y are close to the ground truth matrix U, V, and V. According to our assumptions, we will know that we are close to U, V, and V. We show that W and M will be able to detect a non-negative matrix on the order of Rn in the spectral property of W and the incoherence property of U, V.Lemma 5 (spectral mma)."}, {"heading": "B.2 Random initialization", "text": "With respect to random initialization, the following dilemma arises: Lemma 9 (random initialization). Let Y be a random matrix in Rn \u00b7 k, which is generated as Yi, j \u2212 bi, j \u2212 n, where bi, j are independent, uniform {\u2212 1, 1} variables. Also, allow the variables in Rn \u00b7 k to be generated with a probability of at least 1 \u2212 1n2 over the drawing of Y, i \u2212 min (Y DiY) \u2265 14\u03bbk\u00b5.Proof Lemma 9. Note that Y DiY = j (Y \u2212 j) (Di) jYj \u2212 n and each of the terms (Yj \u2212 n) (Di) jYYj, n (Y DiY) (Di) j (Yj j 2) = 1n (Di) j \u2212 kjj \u2212 n, and each of the terms (Di) jYj \u2212 n, is independent. Furthermore, it is easy to see that E (j) Yj (Yj) Yj (Yj) Yj (Yj) (Yj) Vn (Yj)"}, {"heading": "B.3 Update", "text": "We prove the two most important technical lemmas (Lemma 10 and Lemma 11) and then use them to prove that the updates are making progress toward ground truth. We prove them for Yt and use them to show that they are improving, while completely analog arguments apply even when we swap the role of the two iterates. Note that we measure the distance between Yt and V using these two lemmas, we leave Yo = YtQ (Yt, V) = minQ (YtQ) \u2212 V), where Ok \u00b7 k \u00b2 k \u00b2 k \u00b2 k \u00b2 k \u00b2 k \u00b2 k \u00b2 n matrices are set. For simplicity of notations, in these two lemmas, we leave Yo = YtQ \u00b2 s (YtQ), where Q \u00b2 k \u00b2 k \u00b2 k \u00b2 ytQ \u2212 V \u00b2. We first show that there can be only a few i's that the spectral property of Y \u00b2 o DiYo (DiYo) can be bad when Yo (Yo \u00b2 is in the vicinity of Yo) (Di \u00b2)."}, {"heading": "The running time is polynomial in n and log(1/\u01eb).", "text": "Proof of Theorem 1, and distc (Yt, U). \u2212 Proof of the Theorem 1, and distc (Yt, U). \u2212 Proof of the Theorem 1, and distc (Yt, U). \u2212 Proof of the Theorem 2, and distc 2, and distc (Yt, U). \u2212 Proof the Theorem 2, and distc 2, and distc (V, Y1). \u2212 Proof the Theorem 2, and distc 2, and distc (V, Y)."}, {"heading": "B.5 Estimating \u03c3max(M\u2217)", "text": "Finally, we show that we can estimate the number of (M) to a very good accuracy, so that we can apply our main laws to matrices with any (M) values. \u2212 This is fairly simple: The estimation is simple. \u2212 W (1) -W (1) -W (1) -W (1) -W (1) -W (1) -W (1) -W (1) -W (1) -W (1) -W (1) -W (1) -W (1) -W (1) -W (1) -W (1) -W (1) -W (1) -W (1) -W (1) -W (1) -W (1) -W (1) -W (1) -W) -W (1 W) -W (1 W) -W (1 W) (1 W) 1 W) (1 W) (1 W) -W (1 W) (1 W) (1 W) -W (1 W) (1 W) (1 W) (1 W) (1 W) W (1 W) (1 W) -W (1 W) (1 W) (1 W) (1 W) -W (1 W) (1 W) -W (1 W) (1 W) -W (1 W) -W (1 W) -W (1 W) -W (1 W) -W (1 W (1 W)) -W (1 W (1 W)) -W (1 W (1 W) -W (1 W) -W (1 W) -W (1 W (1 W)) -W (1 W (1 W) 1 W (1 W) 1 W (1 W) 1 W (1 W (1 W) 1 W (1 W (1 W) 1 W) 1 W (1 W (1 W)) 1 W (1 W (1 W) 1 W (1 W) 1 W (1 W (1 W) 1 W (1 W) 1 W (1 W) 1 W (1 W) 1 W (1 W (1 W) 1 W (1 W) 1 W (1 W) 1 W (1 W (1 W) 1 W (1 W) 1 W (1 W (1 W (1 W) 1 W"}, {"heading": "C An alternative approach: alternating minimization with SDP whitening", "text": "Our main results are based on the understanding that the spectral property only needs to be kept in an average sense. However, we can even ensure that the spectral property is kept in a strict sense at each step by a whitening step using SDP and Rademacher Rounding. This is an earlier version of the paper, and we keep this result here because it can potentially be applied in some other applications where similar spectral properties are required and therefore of independent interest.The whitening step (see Algorithm 6) is a convex (actually semi-defined) relaxation followed by a randomized rounding procedure. We explain each of the constraints in the semi-defined program, with the first three constraints controlling the spectral distance between X and X. The next two constraints control the incoherence, and the rest are for the spectral rativector which is the wheel sector. The relaxation solution is then used to control the mean and the despecification of a wheel variant."}, {"heading": "C.1 Update", "text": "We want to show that after each round of ALT, we move our current matrices close to the optimum. (...) We will show that after each round of ALT, we have moved our current matrices close to the optimum. (...) We will show that, when we are small, we are still small. (...) We will show that we are still small. (...) Then, to show that Lemcos is small. (...) We must make sure that we start from a good Y position. (...) We will show that when G is small, we still have the tan position. (...) We must make sure that we start from a good Y position. (...) We must make sure that we start from a good Y position when G is small."}, {"heading": "C.2 Whitening", "text": "What remains is to show that the wear step can ensure that Y has a good incoherence and that Oi has the desired spectral property. (Let's remember that the wear step consists of an SDP relaxation and a new rounding scheme to fix Y so that we have the incoherence of R near \u00b5 (V), which is limited by the SDP relaxation. (Note: It cannot simply be said when to approach the Y-DiY distribution.) This is because Di 2 can be as large as npoly (log) in our case."}, {"heading": "C.3 Final result", "text": "Theorem 20. If M *, W fulfil assumptions (A1) - (A3), and | | W | \u221e = O (\u03bb2nk2\u00b5\u03bb logn), \u03b3 = O (\u03bb\u03c3min (M *) k3\u00b5 \u221a logn), then algorithm 5 after O (log (1 / 2)) rounds outputs a matrix M * that fulfils with probability \u2265 1 \u2212 1 / n | | | M *."}, {"heading": "The running time is polynomial in n and log(1/\u01eb).", "text": "To emphasize the dependence on matrix size n, precedence k and incoherence p, we can consider a certain range of parameter values in which the other parameters (the lower / upper spectral boundary, the conditional number of M \u00b2) are constants that are easier to analyze. Moreover, these parameter values show that we can handle a wider range of parameters than the simple algorithm with clipping as a brightening step. Corolla number 21. Suppose and minimum requirements (M \u00b2) are all constants, and T = O (log (1 / 2). Furthermore, we can handle a wider range of parameters than the simple algorithm with clipping as a brightening step. Corolla setting 21. Suppose and minimum constants (M \u00b2) are all constants, and T = O (log (1 / 2)."}, {"heading": "D Empirical verification of the spectral gap property", "text": "Therefore, we will focus on verifying the key threshold, i.e. the spectral gap property of the weight matrix (assumption (A2)). Here, we will consider the application of the calculation of word embedding by factoring the co-occurrence matrix between words, which is one of the state-of-the-art techniques for mapping words to low-dimensional vectors (about 300 dimensions). There are many variants (e.g. [Levy and Goldberg, 2014, Pennington et al., 2014, Arora et al., 2016]); we will consider the following simple approach. Let's be the co-occurrence matrix, where Xi, j is the number of times in which word i and word j appear together within a window of small size (we are the size of the JP)."}], "references": [{"title": "A latent variable model approach to pmi-based word embeddings", "author": ["Sanjeev Arora", "Yuanzhi Li", "Yingyu Liang", "Tengyu Ma", "Andrej Risteski"], "venue": "To appear in Transactions of the Association for Computational Linguistics,", "citeRegEx": "Arora et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2016}, {"title": "Universal matrix completion", "author": ["Srinadh Bhojanapalli", "Prateek Jain"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Bhojanapalli and Jain.,? \\Q2014\\E", "shortCiteRegEx": "Bhojanapalli and Jain.", "year": 2014}, {"title": "Tighter low-rank approximation via sampling the leveraged element", "author": ["Srinadh Bhojanapalli", "Prateek Jain", "Sujay Sanghavi"], "venue": "In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Bhojanapalli et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bhojanapalli et al\\.", "year": 2015}, {"title": "Dropping convexity for faster semi-definite optimization", "author": ["Srinadh Bhojanapalli", "Anastasios Kyrillidis", "Sujay Sanghavi"], "venue": "arXiv preprint arXiv:1509.03917,", "citeRegEx": "Bhojanapalli et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bhojanapalli et al\\.", "year": 2015}, {"title": "N-gram counts and language models from the common crawl", "author": ["Christian Buck", "Kenneth Heafield", "Bas van Ooyen"], "venue": "In Proceedings of the Language Resources and Evaluation Conference,", "citeRegEx": "Buck et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Buck et al\\.", "year": 2014}, {"title": "Matrix completion with noise", "author": ["Emmanuel J Candes", "Yaniv Plan"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Candes and Plan.,? \\Q2010\\E", "shortCiteRegEx": "Candes and Plan.", "year": 2010}, {"title": "Exact matrix completion via convex optimization", "author": ["Emmanuel J Cand\u00e8s", "Benjamin Recht"], "venue": "Foundations of Computational mathematics,", "citeRegEx": "Cand\u00e8s and Recht.,? \\Q2009\\E", "shortCiteRegEx": "Cand\u00e8s and Recht.", "year": 2009}, {"title": "The power of convex relaxation: Near-optimal matrix completion", "author": ["Emmanuel J Cand\u00e8s", "Terence Tao"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Cand\u00e8s and Tao.,? \\Q2010\\E", "shortCiteRegEx": "Cand\u00e8s and Tao.", "year": 2010}, {"title": "Phase retrieval via wirtinger flow: Theory and algorithms", "author": ["Emmanuel J Candes", "Xiaodong Li", "Mahdi Soltanolkotabi"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Candes et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Candes et al\\.", "year": 1985}, {"title": "Efficient computation of robust weighted low-rank matrix approximations using the l 1 norm", "author": ["Anders Eriksson", "Anton van den Hengel"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Eriksson and Hengel.,? \\Q2012\\E", "shortCiteRegEx": "Eriksson and Hengel.", "year": 2012}, {"title": "Spectral techniques applied to sparse random graphs", "author": ["Uriel Feige", "Eran Ofek"], "venue": "Random Structures & Algorithms,", "citeRegEx": "Feige and Ofek.,? \\Q2005\\E", "shortCiteRegEx": "Feige and Ofek.", "year": 2005}, {"title": "Nuclear magnetic resonance and its applications to living systems", "author": ["David G Gadian"], "venue": null, "citeRegEx": "Gadian.,? \\Q1982\\E", "shortCiteRegEx": "Gadian.", "year": 1982}, {"title": "Low-rank matrix approximation with weights or missing data is np-hard", "author": ["Nicolas Gillis", "Fran\u00e7ois Glineur"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Gillis and Glineur.,? \\Q2011\\E", "shortCiteRegEx": "Gillis and Glineur.", "year": 2011}, {"title": "Recovering low-rank matrices from few coefficients in any basis", "author": ["David Gross"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Gross.,? \\Q2011\\E", "shortCiteRegEx": "Gross.", "year": 2011}, {"title": "Understanding alternating minimization for matrix completion", "author": ["Marcus Hardt"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "Hardt.,? \\Q2014\\E", "shortCiteRegEx": "Hardt.", "year": 2014}, {"title": "Deterministic algorithms for matrix completion", "author": ["Eyal Heiman", "Gideon Schechtman", "Adi Shraibman"], "venue": "Random Structures & Algorithms,", "citeRegEx": "Heiman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Heiman et al\\.", "year": 2014}, {"title": "Low-rank matrix completion using alternating minimization", "author": ["Prateek Jain", "Praneeth Netrapalli", "Sujay Sanghavi"], "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,", "citeRegEx": "Jain et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2013}, {"title": "Matrix completion from noisy entries", "author": ["Raghunandan Keshavan", "Andrea Montanari", "Sewoong Oh"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Keshavan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Keshavan et al\\.", "year": 2009}, {"title": "Matrix completion from any given set of observations", "author": ["Troy Lee", "Adi Shraibman"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Lee and Shraibman.,? \\Q2013\\E", "shortCiteRegEx": "Lee and Shraibman.", "year": 2013}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Omer Levy", "Yoav Goldberg"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Levy and Goldberg.,? \\Q2014\\E", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Improving one-class collaborative filtering by incorporating rich user information", "author": ["Yanen Li", "Jia Hu", "ChengXiang Zhai", "Ye Chen"], "venue": "In Proceedings of the 19th ACM international conference on Information and knowledge management,", "citeRegEx": "Li et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "Weighted low-rank approximation of general complex matrices and its application in the design of 2-d digital filters", "author": ["W-S Lu", "S-C Pei", "P-H Wang"], "venue": "Circuits and Systems I: Fundamental Theory and Applications, IEEE Transactions on,", "citeRegEx": "Lu et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Lu et al\\.", "year": 1997}, {"title": "Restricted strong convexity and weighted matrix completion: Optimal bounds with noise", "author": ["Sahand Negahban", "Martin J Wainwright"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Negahban and Wainwright.,? \\Q2012\\E", "shortCiteRegEx": "Negahban and Wainwright.", "year": 2012}, {"title": "Orthogonal representations over finite fields and the chromatic number", "author": ["Ren\u00e9 Peeters"], "venue": "of graphs. Combinatorica,", "citeRegEx": "Peeters.,? \\Q1996\\E", "shortCiteRegEx": "Peeters.", "year": 1996}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Weighted low rank approximations with provable guarantees", "author": ["Ilya Razenshteyn", "Zhao Song", "David Woodruff"], "venue": "In Proceedings of the 48th Annual Symposium on the Theory of Computing,", "citeRegEx": "Razenshteyn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Razenshteyn et al\\.", "year": 2016}, {"title": "A simpler approach to matrix completion", "author": ["Benjamin Recht"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Recht.,? \\Q2011\\E", "shortCiteRegEx": "Recht.", "year": 2011}, {"title": "Weighted low-rank approximations", "author": ["Nathan Srebro", "Tommi Jaakkola"], "venue": "In Proceedings of the 20th International Conference on Machine Learning", "citeRegEx": "Srebro and Jaakkola.,? \\Q2003\\E", "shortCiteRegEx": "Srebro and Jaakkola.", "year": 2003}, {"title": "Guaranteed matrix completion via nonconvex factorization", "author": ["Ruoyu Sun", "Zhi-Quan Luo"], "venue": "In IEEE 56th Annual Symposium on Foundations of Computer Science,", "citeRegEx": "Sun and Luo.,? \\Q2015\\E", "shortCiteRegEx": "Sun and Luo.", "year": 2015}, {"title": "Perturbation bounds in connection with singular value decomposition", "author": ["Per-\u00c5ke Wedin"], "venue": "BIT Numerical Mathematics,", "citeRegEx": "Wedin.,? \\Q1972\\E", "shortCiteRegEx": "Wedin.", "year": 1972}, {"title": "Maximum likelihood multivariate calibration", "author": ["Peter D Wentzell", "Darren T Andrews", "Bruce R Kowalski"], "venue": "Analytical chemistry,", "citeRegEx": "Wentzell et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Wentzell et al\\.", "year": 1997}, {"title": "2016]); we consider the following simple approach", "author": ["Arora"], "venue": "Let X be the co-occurrence matrix,", "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 19, "context": ", [Levy and Goldberg, 2014]).", "startOffset": 2, "endOffset": 27}, {"referenceID": 27, "context": "Even for collaborative filtering, which is typically modeled as a matrix completion problem that assigns weight 1 on sampled entries and 0 on non-sampled entries, one can achieve better results when allowing non-binary weights [Srebro and Jaakkola, 2003].", "startOffset": 227, "endOffset": 254}, {"referenceID": 12, "context": "Moreover, general weighted low-rank approximation is NP-hard, even when the ground truth is a rank-1 matrix [Gillis and Glineur, 2011].", "startOffset": 108, "endOffset": 134}, {"referenceID": 1, "context": "In this line of work the state-of-the-art is [Bhojanapalli and Jain, 2014], who proved recovery guarantees under the assumptions that the ground truth has a strong version of incoherence and the weight matrix has a sufficiently large spectral gap.", "startOffset": 45, "endOffset": 74}, {"referenceID": 1, "context": "In this paper, we provide the first theoretical guarantee for weighted low-rank approximation via alternating minimization, under assumptions generalizing those in [Bhojanapalli and Jain, 2014].", "startOffset": 164, "endOffset": 193}, {"referenceID": 28, "context": "Furthermore, combining our insight that the spectral property only need to hold in an average sense with the framework in [Sun and Luo, 2015], one can show provable guarantees for the family of algorithms analyzed there, including stochastic gradient descent.", "startOffset": 122, "endOffset": 141}, {"referenceID": 12, "context": "On the other hand, weighted low-rank approximation is NP-hard in the worst case, even when the ground truth is a rank-1 matrix [Gillis and Glineur, 2011].", "startOffset": 127, "endOffset": 153}, {"referenceID": 25, "context": "On the theoretical side, the only result we know of is [Razenshteyn et al., 2016], who provide a fixed-parameter tractability result when additionally the weight matrix is low-rank.", "startOffset": 55, "endOffset": 81}, {"referenceID": 23, "context": "It is known that matrix completion is NP-hard in the case when the k = 3 [Peeters, 1996].", "startOffset": 73, "endOffset": 88}, {"referenceID": 1, "context": "In this line the state-of-the-art is [Bhojanapalli and Jain, 2014], where the support of the observation is a d-regular expander such that the weight matrix has a sufficiently large spectral gap.", "startOffset": 37, "endOffset": 66}, {"referenceID": 22, "context": "We also mention [Negahban and Wainwright, 2012] who consider random sampling, but one that is not uniformly random across the entries.", "startOffset": 16, "endOffset": 47}, {"referenceID": 2, "context": "Assuming that the matrix is incoherent and the observed entries are chosen uniformly at random, Cand\u00e8s and Recht [2009] showed that nuclear norm convex relation can recover an n\u00d7n rank-k matrix using m = O(nk log(n)) entries.", "startOffset": 96, "endOffset": 120}, {"referenceID": 2, "context": "Candes and Plan [2010] relaxed the assumption to tolerate noise and showed the nuclear norm convex relaxation can lead to a solution such that the Frobenius norm of the error matrix is bounded by O( \u221a n3/m) times that of the noise matrix.", "startOffset": 0, "endOffset": 23}, {"referenceID": 1, "context": ", 2014, Lee and Shraibman, 2013, Bhojanapalli and Jain, 2014]. In this line the state-of-the-art is [Bhojanapalli and Jain, 2014], where the support of the observation is a d-regular expander such that the weight matrix has a sufficiently large spectral gap. However, it only works for binary weights, and is for a nuclear norm convex relaxation and does not incorporate noise. Recently, there is an increasing interest in analyzing non-convex optimization techniques for matrix completion. In two seminal papers [Jain et al., 2013, Hardt, 2014], it was shown that with an appropriate SVD-based initialization, the alternating minimization algorithm (with a few modifications) recovers the ground-truth. These results are for random binary weight matrix and crucially rely on re-sampling (i.e., using independent samples at each iteration), which is inherently not possible for the setting studied in this paper. More recently, Sun and Luo [2015] proved recovery guarantees for a family of algorithms including alternating minimization on matrix completion without re-sampling.", "startOffset": 33, "endOffset": 947}, {"referenceID": 1, "context": "Our assumption is also a generalization of the one in [Bhojanapalli and Jain, 2014], which requires W to be d-regular expander-like (i.", "startOffset": 54, "endOffset": 83}, {"referenceID": 1, "context": "The final assumption (A3) is a generalization of the assumption A2 in [Bhojanapalli and Jain, 2014] that, intuitively, requires the singular vectors to satisfy RIP (restricted isometry property).", "startOffset": 70, "endOffset": 99}, {"referenceID": 7, "context": "They viewed it as a stronger version of incoherence, discussed the necessity and showed that it is implied by the strong incoherence property assumed in [Cand\u00e8s and Tao, 2010].", "startOffset": 153, "endOffset": 175}, {"referenceID": 5, "context": "5} exact recovery ours (SVD init) real yes yes yes 1 \u03bc3/2k2 \u2016\u2206\u20162 = O (k) \u2016W \u2299N\u20162 + \u01eb ours (random init) real yes yes yes 1 \u03bc2k5/2 \u2016\u2206\u20162 = O (k) \u2016W \u2299N\u20162 + \u01eb Table 1: Comparison with related work on matrix completion: (1) [Candes and Plan, 2010]; (2) [Keshavan et al.", "startOffset": 219, "endOffset": 242}, {"referenceID": 17, "context": "5} exact recovery ours (SVD init) real yes yes yes 1 \u03bc3/2k2 \u2016\u2206\u20162 = O (k) \u2016W \u2299N\u20162 + \u01eb ours (random init) real yes yes yes 1 \u03bc2k5/2 \u2016\u2206\u20162 = O (k) \u2016W \u2299N\u20162 + \u01eb Table 1: Comparison with related work on matrix completion: (1) [Candes and Plan, 2010]; (2) [Keshavan et al., 2009]; (3) [Bhojanapalli and Jain, 2014]; (4) [Hardt, 2014].", "startOffset": 248, "endOffset": 271}, {"referenceID": 1, "context": ", 2009]; (3) [Bhojanapalli and Jain, 2014]; (4) [Hardt, 2014].", "startOffset": 13, "endOffset": 42}, {"referenceID": 14, "context": ", 2009]; (3) [Bhojanapalli and Jain, 2014]; (4) [Hardt, 2014].", "startOffset": 48, "endOffset": 61}, {"referenceID": 28, "context": "(5) [Sun and Luo, 2015].", "startOffset": 4, "endOffset": 23}, {"referenceID": 10, "context": ", [Feige and Ofek, 2005]).", "startOffset": 2, "endOffset": 24}, {"referenceID": 6, "context": "The seminal paper [Cand\u00e8s and Recht, 2009] showed that a nuclear norm convex relaxation approach can recover the ground truth matrix usingm = O(nk log n) entries chosen uniformly at random and without noise.", "startOffset": 18, "endOffset": 42}, {"referenceID": 7, "context": "The sample size was improved to O(nk log n) in [Cand\u00e8s and Tao, 2010] and then O(nk logn) in subsequent papers.", "startOffset": 47, "endOffset": 69}, {"referenceID": 4, "context": "Candes and Plan [2010] generalized the result to the case with noise: the same convex program using m = O(nk log n) entries recovers a matrix M\u0303 s.", "startOffset": 0, "endOffset": 23}, {"referenceID": 4, "context": "Candes and Plan [2010] generalized the result to the case with noise: the same convex program using m = O(nk log n) entries recovers a matrix M\u0303 s.t. \u2016M\u0303\u2212M\u2217\u2016F \u2264 (2 + 4 \u221a (2 + p)n/p)\u2016N\u03a9\u2016F where p = m/n and N\u03a9 is the noise projected on the observed entries. Keshavan et al. [2009] showed that withm = O(n\u03bck logn), one can recover a matrix M\u0303 such that \u2225\u2225M\u2217 \u2212 M\u0303 \u2225\u2225 F = O ( n \u221a k m \u2016N\u03a9\u20162 ) by an optimization over a Grassmanian manifold.", "startOffset": 0, "endOffset": 279}, {"referenceID": 1, "context": "Bhojanapalli and Jain [2014] relaxed the assumption that the entries are randomly sampled.", "startOffset": 0, "endOffset": 29}, {"referenceID": 14, "context": "Hardt [2014] showed that with an appropriate initialization alternating minimization recovers the ground truth approximately.", "startOffset": 0, "endOffset": 13}, {"referenceID": 14, "context": "Hardt [2014] showed that with an appropriate initialization alternating minimization recovers the ground truth approximately. Precisely, they assumed N satisfies: (1). \u03bc(N) . \u03c3min(M\u2217)2;(2). \u2016N\u2016\u221e \u2264 \u03bc n\u2016M\u2016F . Then, he shows that log(n\u01eb logn) alternating minimization steps recover a matrix M\u0303 such that \u2016M\u0303\u2212M\u2217\u2016F \u2264 \u01eb\u2016M\u2016F provided that pn \u2265 k(k + log(n/\u01eb))\u03bc \u00d7 ( \u2016M\u2016F+\u2016N\u2016F /\u01eb \u03c3k )2 ( 1\u2212 \u03c3k+1 \u03c3k )5 where \u03c3k is the k-th singular value of the groundtruth matrix. The parameter \u03b3 corresponding to the case considered there would be roughly O( 1 k \u221a \u03bc logn ). While their algorithm has a good tolerance to noise, N is assumed to have special structure for him that we do not assume in our setting. Sun and Luo [2015] proved recovery guarantees for a family of algorithms including alternating minimization on matrix completion.", "startOffset": 0, "endOffset": 708}, {"referenceID": 28, "context": "We note that [Sun and Luo, 2015] only needs sampling before the algorithm starts and does not need re-sampling in different iterations, but still relies on the randomness in the sampled entries.", "startOffset": 13, "endOffset": 32}, {"referenceID": 13, "context": "Keshavan et al. [2009] analyzed optimization over a Grassmanian manifold, which uses the fact that E[W \u2299 S] = S for any matrix S.", "startOffset": 0, "endOffset": 23}, {"referenceID": 29, "context": "Since ||W \u2299 N||2 \u2264 \u03b4 can be regarded as small, the idea is to show that W \u2299M\u2217 is close to M\u2217 in spectral norm and then apply Wedin\u2019s theorem [Wedin, 1972].", "startOffset": 141, "endOffset": 154}, {"referenceID": 29, "context": "By our assumptions we know that ||W \u2299N||2 \u2264 \u03b4 which we are thinking of as small, so the idea is to show that W \u2299M\u2217 is close to M\u2217 in spectral norm, then by Wedin\u2019s theorem [Wedin, 1972] we will have X,Y are close to U,V.", "startOffset": 172, "endOffset": 185}, {"referenceID": 29, "context": "Lemma 6 (Wedin\u2019s Theorem [Wedin, 1972]).", "startOffset": 25, "endOffset": 38}, {"referenceID": 4, "context": "We consider two large corpora (Wikipedia corpus [Wikimedia, 2012], about 3G tokens; a subset of Commoncrawl corpus [Buck et al., 2014], about 20G tokens).", "startOffset": 115, "endOffset": 134}], "year": 2016, "abstractText": "Many applications require recovering a ground truth low-rank matrix from noisy observations of the entries, which in practice is typically formulated as a weighted low-rank approximation problem and solved by non-convex optimization heuristics such as alternating minimization. In this paper, we provide provable recovery guarantee of weighted low-rank via a simple alternating minimization algorithm. In particular, for a natural class of matrices and weights and without any assumption on the noise, we bound the spectral norm of the difference between the recovered matrix and the ground truth, by the spectral norm of the weighted noise plus an additive error that decreases exponentially with the number of rounds of alternating minimization, from either initialization by SVD or, more importantly, random initialization. These provide the first theoretical results for weighted low-rank via alternating minimization with non-binary deterministic weights, significantly generalizing those for matrix completion, the special case with binary weights, since our assumptions are similar or weaker than those made in existing works. Furthermore, this is achieved by a very simple algorithm that improves the vanilla alternating minimization with a simple clipping step. The key technical challenge is that under non-binary deterministic weights, na\u0131\u0308ve alternating steps will destroy the incoherence and spectral properties of the intermediate solutions, which are needed for making progress towards the ground truth. We show that the properties only need to hold in an average sense and can be achieved by the clipping step. We further provide an alternating algorithm that uses a whitening step that keeps the properties via SDP and Rademacher rounding and thus requires weaker assumptions. This technique can potentially be applied in some other applications and is of independent interest.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}