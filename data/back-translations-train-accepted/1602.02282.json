{"id": "1602.02282", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2016", "title": "Ladder Variational Autoencoders", "abstract": "Variational autoencoders are a powerful framework for unsupervised learning. However, previous work has been restricted to shallow models with one or two layers of fully factorized stochastic latent variables, limiting the flexibility of the latent representation. We propose three advances in training algorithms of variational autoencoders, for the first time allowing to train deep models of up to five stochastic layers, (1) using a structure similar to the Ladder network as the inference model, (2) warm-up period to support stochastic units staying active in early training, and (3) use of batch normalization. Using these improvements we show state-of-the-art log-likelihood results for generative modeling on several benchmark datasets.", "histories": [["v1", "Sat, 6 Feb 2016 17:32:48 GMT  (4324kb,D)", "http://arxiv.org/abs/1602.02282v1", null], ["v2", "Tue, 24 May 2016 10:41:45 GMT  (5047kb,D)", "http://arxiv.org/abs/1602.02282v2", null], ["v3", "Fri, 27 May 2016 09:05:10 GMT  (5047kb,D)", "http://arxiv.org/abs/1602.02282v3", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["casper kaae s\u00f8nderby", "tapani raiko", "lars maal\u00f8e", "s\u00f8ren kaae s\u00f8nderby", "ole winther"], "accepted": true, "id": "1602.02282"}, "pdf": {"name": "1602.02282.pdf", "metadata": {"source": "META", "title": "How to Train Deep Variational Autoencoders and Probabilistic Ladder Networks", "authors": ["Casper Kaae S\u00f8nderby", "Tapani Raiko", "Lars Maal\u00f8e", "S\u00f8ren Kaae S\u00f8nderby", "Ole Winther"], "emails": ["CASPERKAAE@GMAIL.COM", "TAPANI.RAIKO@AALTO.FI", "LARSMA@DTU.DK", "SKAAESONDERBY@GMAIL.COM", "OLWI@DTU.DK"], "sections": [{"heading": "1. Introduction", "text": "This year, we have reached the point where we feel we are in a position to take the lead without being in a position to try to find a solution."}, {"heading": "2. Methods", "text": "Variational information flows in the inference and generative models in a probable network stability network. Variational data flows simultaneously form a generative model p\u03b8 (x, z) = p\u03b8 (x | z) p\u03b8 (z) for data x x (x, z) latent variables z and an inference model q\u03c6 (z | x) 1 bound to the probability threshold by optimizing a variational lower limit (x). The inference models are also referred to as the recognition model or the encoder, and the generative model as decoder.zdzza) b) nnn \"Likelihood\" Deterministicbottom up pathwayStochastic up pathway \"Prior\" Copy \"Top down pathwaythrough KL-divergencesin generative modelBottom up pathway in Inference modellation modelBottom up way in Indirect topdown information through priorDirect flow of information z nGenerativemodel\" Figure 2. \"Copy Flow probable information in the generative network.\""}, {"heading": "2.1. Probabilistic Ladder Network", "text": "We propose a new follow-up model using the structure of the conductor network (Valpola, 2014; Rasmus et al., 2015) as the conclusion model of a UAE, as shown in Figure 1. The generative model is the same as before. The conclusion is constructed to first achieve a deterministic upward trend: d1 = MLP (x) (13) \u00b5d, i = Linear (di), i = 1.. L (14) \u03c32d, i = Softplus (Linear (di)), i = 1. L (15) di = MLP (\u00b5d, i \u2212 1), i = 2.. L (16) followed by a stochastic downpendant phenomenon: q\u03c6 (zL | x) = N (\u00b5d, L, \u03c3 2 d) (17) ti = MLP (zi + 1), i \u2212 1 \u2212 b \u2212 t, i = Linear (ti) (x)."}, {"heading": "2.2. Warm-up from deterministic to variational autoencoder", "text": "The variable training criterion in Equation (11) contains the reconstruction term p\u03b8 (x | z) and the variational regularization term. The variable regularization term causes some of the latent units to become inactive during the training (MacKay, 2001) because the approximate back part of the unit k, q (zi, k |...) is regulated in the direction of its own previous p (zi, k |...), a phenomenon also recognized in the UAE setting (Burda et al., 2015). However, this can be seen as a virtue of automatic relevance determination, but also as a problem if many units are cut back in the training before they have learned a useful representation. We observed that such units remain inactive for the rest of the training, presumably trapped in a local minimum or saddle point at KL (qi, k | pi, k)."}, {"heading": "2.3. Batch Normalization", "text": "Batch normalization (Ioffe & Szegedy, 2015) is a newer innovation that improves and stabilizes the convergence velocity. The idea was rejected at the time as it could entail considerable theoretical complications."}, {"heading": "3. Experiments", "text": "The largest models were implemented with a hierarchy of five layers of latent variables of size 64, 32, 16, and 4, going from bottom to top. We implemented all cards with two-layer MLPs. In all models, the MLP between x and z1 was associated with the size of MLP."}, {"heading": "4. Results & Discussion", "text": "This year, it is so far that it will only take a few days to achieve an outcome in which everyone else will participate."}, {"heading": "5. Conclusion", "text": "We presented three methods for optimizing VAEs using deep hierarchies of latent stochastic variables. Using these models, we demonstrated the current generative performance of the MNIST and OMNIGLOT datasets and demonstrated that a new parameterization, the probabilistic conductor network, works better or better than current models. Furthermore, we demonstrated that VAEs can be trained with up to five hierarchies of active stochastic units and that the new probabilistic conductor network in particular is capable of utilizing many stochastic units."}, {"heading": "Acknowledgments", "text": "This research was supported by the Novo Nordisk Foundation, the Danish Innovation Foundation and the NVIDIA Corporation with a donation of TITAN X and Tesla K40 GPUs."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Warde-Farley", "David", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1211.5590,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Importance weighted autoencoders", "author": ["Burda", "Yuri", "Grosse", "Roger", "Salakhutdinov", "Ruslan"], "venue": "arXiv preprint arXiv:1509.00519,", "citeRegEx": "Burda et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Burda et al\\.", "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Semi-supervised learning with deep generative models", "author": ["Kingma", "Diederik P", "Mohamed", "Shakir", "Rezende", "Danilo Jimenez", "Welling", "Max"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "One-shot learning by inverting a compositional causal process", "author": ["Lake", "Brenden M", "Salakhutdinov", "Ruslan R", "Tenenbaum", "Josh"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Lake et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2013}, {"title": "Learning methods for generic object recognition with invariance to pose and lighting", "author": ["LeCun", "Yann", "Huang", "Fu Jie", "Bottou", "Leon"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "LeCun et al\\.,? \\Q2004\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2004}, {"title": "Improving semisupervised learning with auxiliary deep generative models", "author": ["Maaloee", "Lars P", "S\u00f8nderby", "Casper Kaae", "S\u00f8ren Kaae", "Winther", "Ole"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Maaloee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Maaloee et al\\.", "year": 2016}, {"title": "Local minima, symmetry-breaking, and model pruning in variational free energy minimization", "author": ["MacKay", "David JC"], "venue": null, "citeRegEx": "MacKay and JC.,? \\Q2001\\E", "shortCiteRegEx": "MacKay and JC.", "year": 2001}, {"title": "Pixel recurrent neural networks", "author": ["Oord", "Aaron van den", "Kalchbrenner", "Nal", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1601.06759,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Building blocks for variational bayesian learning of latent variable models", "author": ["Raiko", "Tapani", "Valpola", "Harri", "Harva", "Markus", "Karhunen", "Juha"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Raiko et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Raiko et al\\.", "year": 2007}, {"title": "Iterative neural autoregressive distribution estimator nade-k", "author": ["Raiko", "Tapani", "Li", "Yao", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Raiko et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Raiko et al\\.", "year": 2014}, {"title": "Semi-supervised learning with ladder networks", "author": ["Rasmus", "Antti", "Berglund", "Mathias", "Honkala", "Mikko", "Valpola", "Harri", "Raiko", "Tapani"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Rasmus et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rasmus et al\\.", "year": 2015}, {"title": "Variational inference with normalizing flows", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir"], "venue": "arXiv preprint arXiv:1505.05770,", "citeRegEx": "Rezende et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1401.4082,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Variational gaussian process", "author": ["Tran", "Dustin", "Ranganath", "Rajesh", "Blei", "David M"], "venue": "arXiv preprint arXiv:1511.06499,", "citeRegEx": "Tran et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tran et al\\.", "year": 2015}, {"title": "From neural pca to deep unsupervised learning", "author": ["Valpola", "Harri"], "venue": "arXiv preprint arXiv:1411.7783,", "citeRegEx": "Valpola and Harri.,? \\Q2014\\E", "shortCiteRegEx": "Valpola and Harri.", "year": 2014}, {"title": "What auto-encoders could learn from brains - generation as feedback in unsupervised deep learning and inference, 2016", "author": ["van den Broeke", "Gerben"], "venue": null, "citeRegEx": "Broeke and Gerben.,? \\Q2016\\E", "shortCiteRegEx": "Broeke and Gerben.", "year": 2016}], "referenceMentions": [{"referenceID": 15, "context": "The recently introduced variational autoencoder (VAE) (Kingma & Welling, 2013; Rezende et al., 2014) provides a framework for deep generative models (DGM).", "startOffset": 54, "endOffset": 100}, {"referenceID": 3, "context": "DGMs have later been shown to be a powerful framework for semi-supervised learning (Kingma et al., 2014; Maaloee et al., 2016).", "startOffset": 83, "endOffset": 126}, {"referenceID": 8, "context": "DGMs have later been shown to be a powerful framework for semi-supervised learning (Kingma et al., 2014; Maaloee et al., 2016).", "startOffset": 83, "endOffset": 126}, {"referenceID": 13, "context": "Another line of research starting from denoising autoencoders introduced the Ladder network for unsupervised learning (Valpola, 2014) which have also been shown to perform very well in the semi-supervised setting (Rasmus et al., 2015).", "startOffset": 213, "endOffset": 234}, {"referenceID": 16, "context": "We first show that these models have competitive generative performance, measured in terms of test log likelihood, when compared to equally or more complicated methods for creating flexible variational distributions such as the Variational Gaussian Processes (Tran et al., 2015) Normalizing Flows (Rezende & Mohamed, 2015) or Importance Weighted Autoencoders (Burda et al.", "startOffset": 259, "endOffset": 278}, {"referenceID": 1, "context": ", 2015) Normalizing Flows (Rezende & Mohamed, 2015) or Importance Weighted Autoencoders (Burda et al., 2015).", "startOffset": 88, "endOffset": 108}, {"referenceID": 1, "context": "A strictly tighter bound on the likelihood may be obtained at the expense of a K-fold increase of samples by using the importance weighted bound (Burda et al., 2015):", "startOffset": 145, "endOffset": 165}, {"referenceID": 15, "context": "(11) using stochastic gradient descent where we use the reparametrization trick for stochastic backpropagation through the Gaussian latent variables (Kingma & Welling, 2013; Rezende et al., 2014).", "startOffset": 149, "endOffset": 195}, {"referenceID": 12, "context": "A further extension could be to make the inference in k steps over an iterative inference procedure (Raiko et al., 2014).", "startOffset": 100, "endOffset": 120}, {"referenceID": 1, "context": "), a phenomenon also recognized in the VAE setting (Burda et al., 2015).", "startOffset": 51, "endOffset": 71}, {"referenceID": 6, "context": "To test our models we use the standard benchmark datasets MNIST, OMNIGLOT (Lake et al., 2013) and NORB (LeCun et al.", "startOffset": 74, "endOffset": 93}, {"referenceID": 7, "context": ", 2013) and NORB (LeCun et al., 2004).", "startOffset": 17, "endOffset": 37}, {"referenceID": 0, "context": "The models were implemented in the Theano (Bastien et al., 2012), Lasagne (Dieleman et al.", "startOffset": 42, "endOffset": 64}, {"referenceID": 0, "context": "(12) with 5000 importance weighted samples as in Burda et al. (2015). The models were implemented in the Theano (Bastien et al.", "startOffset": 49, "endOffset": 69}, {"referenceID": 1, "context": "Similarly to Burda et al. (2015) we resample the binarized training values from the real-valued images using a Bernoulli distribution after each epoch which prevents the models from over-fitting.", "startOffset": 13, "endOffset": 33}, {"referenceID": 1, "context": "The OMNIGLOT data was partitioned and preprocessed as in Burda et al. (2015), https://github.", "startOffset": 57, "endOffset": 77}, {"referenceID": 1, "context": "Comparing the results obtained here with current state-ofthe art results on permutation invariant MNIST, Burda et al. (2015) report a test-set performance of\u221282.", "startOffset": 105, "endOffset": 125}, {"referenceID": 1, "context": "Comparing the results obtained here with current state-ofthe art results on permutation invariant MNIST, Burda et al. (2015) report a test-set performance of\u221282.90 using 50 IW samples and a two layer model with 100-50 latent units. Using a similar model Tran et al. (2015) achieves \u221281.", "startOffset": 105, "endOffset": 273}, {"referenceID": 1, "context": "26, is higher than the best results from Burda et al. (2015) at \u2212103.", "startOffset": 41, "endOffset": 61}, {"referenceID": 1, "context": "26, is higher than the best results from Burda et al. (2015) at \u2212103.38, which were obtained using more latent variables (100-50 vs 64-32-16) and 50 importance weighted samples for training. We tested the models using a continuous Gaussian observation model on the NORB dataset consisting of gray-scale images of 5 different toy objects under different illuminations and observation angles. As seen in Table 3 we again find that bath normalization and warm-up increases performance. However here we do not see a gain in performance for L > 2 layers of latent stochastic variables. We found the Gaussian observation models to be harder to train requiring lower learning rates and hyperbolic tangent instead of leaky rectifiers as nonlinearities for stable training. The probabilistic ladder network were sometimes unstable during training explaining the high variance in the results. The harder optimization of the Gaussian observation model, also recognized in Oord et al. (2016), might explain the lower utilization of the topmost latent layers in these models.", "startOffset": 41, "endOffset": 980}, {"referenceID": 11, "context": "However, as previously identified (Raiko et al., 2007; Burda et al., 2015), the latent representation is often overly sparse with few stochastic latent variables propagating useful information.", "startOffset": 34, "endOffset": 74}, {"referenceID": 1, "context": "However, as previously identified (Raiko et al., 2007; Burda et al., 2015), the latent representation is often overly sparse with few stochastic latent variables propagating useful information.", "startOffset": 34, "endOffset": 74}], "year": 2016, "abstractText": "Variational autoencoders are a powerful framework for unsupervised learning. However, previous work has been restricted to shallow models with one or two layers of fully factorized stochastic latent variables, limiting the flexibility of the latent representation. We propose three advances in training algorithms of variational autoencoders, for the first time allowing to train deep models of up to five stochastic layers, (1) using a structure similar to the Ladder network as the inference model, (2) warm-up period to support stochastic units staying active in early training, and (3) use of batch normalization. Using these improvements we show state-of-the-art log-likelihood results for generative modeling on several benchmark datasets.", "creator": "LaTeX with hyperref package"}}}