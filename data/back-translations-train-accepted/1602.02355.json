{"id": "1602.02355", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Feb-2016", "title": "Hyperparameter optimization with approximate gradient", "abstract": "Most models in machine learning contain at least one hyperparameter to control for model complexity. Choosing an appropriate set of hyperparameters is both crucial in terms of model accuracy and computationally challenging. In this work we propose an algorithm for the optimization of continuous hyperparameters using inexact gradient information. An advantage of this method is that hyperparameters can be updated before model parameters have fully converged. We also give sufficient conditions for the global convergence of this method, based on regularity conditions of the involved functions and summability of errors. Finally, we validate the empirical performance of this method on the estimation of regularization constants of L2-regularized logistic regression and kernel Ridge regression. Empirical benchmarks indicate that our approach is highly competitive with respect to state of the art methods.", "histories": [["v1", "Sun, 7 Feb 2016 10:37:13 GMT  (240kb,D)", "http://arxiv.org/abs/1602.02355v1", null], ["v2", "Tue, 9 Feb 2016 11:24:08 GMT  (240kb,D)", "http://arxiv.org/abs/1602.02355v2", null], ["v3", "Fri, 27 May 2016 09:50:52 GMT  (430kb,D)", "http://arxiv.org/abs/1602.02355v3", "Published at Proceedings of the International conference on Machine Learning (ICML)"], ["v4", "Thu, 9 Jun 2016 15:42:04 GMT  (338kb,D)", "http://arxiv.org/abs/1602.02355v4", "Proceedings of the International conference on Machine Learning (ICML)"], ["v5", "Sun, 26 Jun 2016 01:02:54 GMT  (255kb,D)", "http://arxiv.org/abs/1602.02355v5", "Proceedings of the International conference on Machine Learning (ICML)"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG math.OC", "authors": ["fabian pedregosa"], "accepted": true, "id": "1602.02355"}, "pdf": {"name": "1602.02355.pdf", "metadata": {"source": "META", "title": "Hyperparameter optimization with approximate gradient", "authors": ["Fabian Pedregosa"], "emails": ["F@BIANP.NET"], "sections": [{"heading": "1. Introduction", "text": "This year it is more than ever before."}, {"heading": "1.1. Problem setting", "text": "As mentioned in the introduction, the goal of hyperparameter optimization is to choose the hyperparameter problem that optimizes some criteria, such as a cross-validation loss or a SURE / AIC / BIC problem. We will describe these criteria by f: Rs \u2192 R, where s is the number of hyperparameters. In its simplest form, the hyperparameter optimization problem can be considered a problem of minimizing the cost function f over a domain D. Some approaches, such as sequential model-based optimization, require only functional evaluations of this cost function. However, in order to access the local information of this cost function, we will further investigate this function.The cost function f depends on the model parameters that we will name by X. These are usually not available in closed form, but implicitly as the minimizers of a cost function that we will evaluate."}, {"heading": "2. HOAG: Hyperparameter optimization with approximate gradient", "text": "As we have seen in the previous section, the calculation of an exact f gradient can be mathematically demanding. (PD) In this section, we present an algorithm that uses an approximation instead of the true gradient to estimate the optimal hyperparameters. This approach results in a compromise between speed and accuracy: a loose approximation can be calculated faster, but could lead to slow convergence, or the algorithm may even diverge. In the k \u2212 gradient iteration, this trade-off is balanced by the tolerance parameter \u03b5k. The sequence of tolerance parameters {\u03b51, \u03b52,} will turn out to be an important role in the convergence of the algorithm, although the time we use is treated as a free parameter. We will now describe our main contribution, the HOAG algorithm: Algorithm 1 (HOAG) qk = 1, \u03b52,."}, {"heading": "2.1. Related work", "text": "There is a wide variety of hyperparameter optimization methods, and a full review of this literature would be out of the scope of this work. Below we will comment on the relationship between HOAG and some of the most closely related methods. This approach consists in deriving an implicit equation for the gradient and iterative differentiation depending on how the gradient is compatible with the hyperparameters. Implicitly, it differentiates itself in deriving the optimality conditions for the inner optimization problem (as we have done in Eq)."}, {"heading": "3. Analysis", "text": "The analysis of this algorithm is inspired by the work of d'Aspremont (2008); Schmidt et al. (2011); Friedlander et al. (2012) on inexact gradient algorithms for convex optimization. We assume that the first derivatives of g and the second derivatives of h enumerate the continuous functions that we assume for the hyperparameter optimization problem. The following conditions are accepted by the section: \u2022 (A1) L-Smoothness. We assume that the first derivatives of g and the second derivatives of h are the continuous functions that we assume for the hyperparameter optimization problem."}, {"heading": "4. Experiments", "text": "In this section we compare the empirical performance of the HOAG. We start by discussing some implementation details such as the choice of step size, then compare the convergence of different tolerance acceptance strategies proposed by the theoretical analysis. In a third part we compare the performance of the HOAG against other hyperparameter optimization methods. Our algorithm relies on the knowledge of the Lipschitz constant L for the cost function, but in practice this is not known in advance. As the cost function is costly, it is not practical to perform traceability line search. To overcome this, we use a method in which the step size is corrected according to the gain from the previous step. In the experiments we use this technique, although we do not have a formal analysis of the algorithms for this choice of step size."}, {"heading": "4.1. Tolerance decrease sequence", "text": "In Figure 2, we report on the convergence of different tolerance reduction strategies. According to Theorem 4, the only condition for these sequences is that they are summable. Three notable examples of summable sequences are the square, cubic and exponential sequences. Therefore, we choose a representative of each of these strategies. Specifically, the acceptance sequences we select are a quadratic decrease of Form \u03b5k = 0.1 \u00b7 k \u2212 2, a cubic decrease of Form \u03b5k = 0.1 \u00b7 k \u2212 3, and an exponential decrease of Form \u03b5k = 0.1 \u00b7 (0.5k). The value assumed as the true minimum of the hyperparameter optimization problem is calculated by the minimum of 10 randomly initialized cases of HOAG with guaranteed acceptance tolerance."}, {"heading": "4.2. Comparison with other hyperparameter optimization methods", "text": "We compare ourselves to other hyperparameter optimization methods. The methods against which we compare are: \u2022 HOAG. The method we present in this paper is comparable to an exponentially decreasing tolerance sequence. \u2022 This method is simply to split the domain of the hyperparameter into a simultaneous grid. \u2022 We divide the interval [12, 12] into a grid of 10 values. \u2022 This is the random search method (Bergstra and Bengio, 2012) Examples of the hyperparameters of a predefined distribution. We choose the examples of uniform distribution in the interval [12, 12] SMBO. Sequential-based optimization with Gaussian Process. We used the implementation of the Bayesian Optimization4 package."}, {"heading": "5. Discussion and future work", "text": "In previous sections, we have presented and discussed several aspects of the HOAG algorithm. Finally, we outline some future directions that we consider worth exploring. Given the success of recent stochastic optimization techniques (Schmidt et al., 2013; Johnson and Zhang, 2013), it seems natural to study a stochastic variant of this algorithm, i.e. one in which the updates in the internal and external optimization programs incur costs that are independent of the number of samples. In fact, the adoption (A3) is almost exclusively introduced to guarantee the presence of solutions. At the same time, recent progress in defining image restoration, which can be considered a partial problem of the hyperparameter optimization problem (HO), ensures the adoption (A3) of solutions."}, {"heading": "Acknowledgments", "text": "I would also like to thank Gabriel Peyre for numerous discussions, suggestions and pointers. I would also like to thank the developers of scikit-learn for discussions and their continuous efforts in creating high quality tools, and Justin Domke for publishing the code of his iterative differentiation method. The author thanks the \"Chaire Economie des Nouvelles Donne \u0301 es\" under the auspices of the Louis Bachelier Institute, Havas-Media and the Universite \u0301 Paris-Dauphine for the financial support."}, {"heading": "6. Proofs", "text": "For sufficiently large k, the error in the course curve is limited by a constant factor of \u03b5k. This means that the Hessian optimization function is roughly evaluated on the model parameters and on the k-th iteration. This means that Ak = 21h (note, X (note, X)) and A-k approximate the Hessian optimization function on the model parameters and on the k-th iteration. This means that Ak = 21h (note, X (note, X)) and A-k approximately evaluate the Hessian optimization function on the model parameters and on the k-th. Similarly, we define bk, b-k and Dk, D-k: bk = 1g (note, X) and b-k (note, X)."}, {"heading": "7. Experiments", "text": "The derivation of this procedure is based on the L-smooth assumption and theorem 3. The L-smooth assumption implies that the following inequality is verified for all \u03b1, \u03b2 in the domain (\u03b2) \u2264 f (\u03b1) + f (\u03b1) T (\u03b2 \u2212 \u03b1) + L-smooth assumption \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p - p \u2212 p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p -"}], "references": [{"title": "Gradient-based optimization of hyperparameters", "author": ["Yoshua Bengio"], "venue": "Neural computation,", "citeRegEx": "Bengio.,? \\Q2000\\E", "shortCiteRegEx": "Bengio.", "year": 2000}, {"title": "Random search for hyper-parameter optimization", "author": ["James Bergstra", "Yoshua Bengio"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bergstra and Bengio.,? \\Q2012\\E", "shortCiteRegEx": "Bergstra and Bengio.", "year": 2012}, {"title": "Algorithms for hyper-parameter optimization", "author": ["James S. Bergstra", "R\u00e9mi Bardenet", "Yoshua Bengio", "Bal\u00e1zs K\u00e9gl"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Bergstra et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2011}, {"title": "A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning", "author": ["Eric Brochu", "Vlad M Cora", "Nando De Freitas"], "venue": "arXiv preprint arXiv:1012.2599,", "citeRegEx": "Brochu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Brochu et al\\.", "year": 2010}, {"title": "Bilevel approaches for learning of variational imaging models", "author": ["Luca Calatroni", "Cao Chung", "Juan Carlos De Los Reyes", "Carola-Bibiane Sch\u00f6nlieb", "Tuomo Valkonen"], "venue": "arXiv preprint arXiv:1505.02120,", "citeRegEx": "Calatroni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Calatroni et al\\.", "year": 2015}, {"title": "Choosing multiple parameters for support vector machines", "author": ["Olivier Chapelle", "Vladimir Vapnik", "Olivier Bousquet", "Sayan Mukherjee"], "venue": "Machine learning,", "citeRegEx": "Chapelle et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2002}, {"title": "Smooth optimization with approximate gradient", "author": ["Alexandre d\u2019Aspremont"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "d.Aspremont.,? \\Q2008\\E", "shortCiteRegEx": "d.Aspremont.", "year": 2008}, {"title": "Stein unbiased gradient estimator of the risk (SUGAR) for multiple parameter selection", "author": ["Charles-Alban Deledalle", "Samuel Vaiter", "Jalal Fadili", "Gabriel Peyr\u00e9"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "Deledalle et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Deledalle et al\\.", "year": 2014}, {"title": "Generic methods for optimization-based modeling", "author": ["Justin Domke"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Domke.,? \\Q2012\\E", "shortCiteRegEx": "Domke.", "year": 2012}, {"title": "Efficient multiple hyperparameter learning for log-linear models", "author": ["Chuan-Sheng Foo", "Chuong B. Do", "Andrew Y. Ng"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Foo et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Foo et al\\.", "year": 2008}, {"title": "Hybrid deterministic-stochastic methods for data fitting", "author": ["Michael Friedlander", "Mark Schmidt"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Friedlander and Schmidt.,? \\Q2012\\E", "shortCiteRegEx": "Friedlander and Schmidt.", "year": 2012}, {"title": "Accuracy and stability of numerical algorithms", "author": ["Nicholas J Higham"], "venue": null, "citeRegEx": "Higham.,? \\Q2002\\E", "shortCiteRegEx": "Higham.", "year": 2002}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Johnson and Zhang.,? \\Q2013\\E", "shortCiteRegEx": "Johnson and Zhang.", "year": 2013}, {"title": "A bilevel optimization approach for parameter learning in variational models", "author": ["Karl Kunisch", "Thomas Pock"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "Kunisch and Pock.,? \\Q2013\\E", "shortCiteRegEx": "Kunisch and Pock.", "year": 2013}, {"title": "Sequential model-based ensemble optimization", "author": ["Alexandre Lacoste", "Hugo Larochelle", "Fran\u00e7ois Laviolette", "Mario Marchand"], "venue": "arXiv preprint arXiv:1402.0796,", "citeRegEx": "Lacoste et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lacoste et al\\.", "year": 2014}, {"title": "Design and regularization of neural networks: the optimal use of a validation set", "author": ["Jan Larsen", "Lars Kai Hansen", "Claus Svarer", "M Ohlsson"], "venue": "In Neural Networks for Signal Processing", "citeRegEx": "Larsen et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Larsen et al\\.", "year": 1996}, {"title": "Adaptive regularization in neural network modeling", "author": ["Jan Larsen", "Claus Svarer", "Lars Nonboe Andersen", "Lars Kai Hansen"], "venue": "In Neural Networks: Tricks of the Trade. Springer,", "citeRegEx": "Larsen et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Larsen et al\\.", "year": 1998}, {"title": "Trust region newton method for logistic regression", "author": ["Chih-Jen Lin", "Ruby C Weng", "S Sathiya Keerthi"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Lin et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2008}, {"title": "On the limited memory bfgs method for large scale optimization", "author": ["Dong C Liu", "Jorge Nocedal"], "venue": "Mathematical programming,", "citeRegEx": "Liu and Nocedal.,? \\Q1989\\E", "shortCiteRegEx": "Liu and Nocedal.", "year": 1989}, {"title": "Parametric or nonparametric? a parametricness index for model selection", "author": ["Wei Liu", "Yuhong Yang"], "venue": "The Annals of Statistics,", "citeRegEx": "Liu and Yang.,? \\Q2011\\E", "shortCiteRegEx": "Liu and Yang.", "year": 2011}, {"title": "Gradient-based hyperparameter optimization through reversible learning", "author": ["Dougal Maclaurin", "David Duvenaud", "Ryan P. Adams"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Maclaurin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Maclaurin et al\\.", "year": 2015}, {"title": "Introductory lectures on convex optimization", "author": ["Yurii Nesterov"], "venue": "Springer Science & Business Media,", "citeRegEx": "Nesterov.,? \\Q2004\\E", "shortCiteRegEx": "Nesterov.", "year": 2004}, {"title": "Fast exact multiplication by the hessian", "author": ["Barak A Pearlmutter"], "venue": "Neural computation,", "citeRegEx": "Pearlmutter.,? \\Q1994\\E", "shortCiteRegEx": "Pearlmutter.", "year": 1994}, {"title": "Convergence rates of inexact proximal-gradient methods for convex optimization", "author": ["Mark Schmidt", "Nicolas Le Roux", "Francis R Bach"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Schmidt et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Schmidt et al\\.", "year": 2011}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["Mark Schmidt", "Nicolas Le Roux", "Francis Bach"], "venue": "arXiv preprint arXiv:1309.2388,", "citeRegEx": "Schmidt et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Schmidt et al\\.", "year": 2013}, {"title": "Cross-validation optimization for large scale structured classification kernel methods", "author": ["Matthias W Seeger"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Seeger.,? \\Q2008\\E", "shortCiteRegEx": "Seeger.", "year": 2008}, {"title": "Estimation of the mean of a multivariate normal distribution", "author": ["Charles M Stein"], "venue": "The annals of Statistics,", "citeRegEx": "Stein.,? \\Q1981\\E", "shortCiteRegEx": "Stein.", "year": 1981}, {"title": "Freeze-thaw bayesian optimization", "author": ["Kevin Swersky", "Jasper Snoek", "Ryan Prescott Adams"], "venue": "arXiv preprint arXiv:1406.3896,", "citeRegEx": "Swersky et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Swersky et al\\.", "year": 2014}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Robert Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Tibshirani.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani.", "year": 1996}, {"title": "Accurate telemonitoring of parkinson\u2019s disease progression by noninvasive speech tests", "author": ["Athanasios Tsanas", "Max A Little", "Patrick E McSharry", "Lorraine O Ramig"], "venue": "Biomedical Engineering, IEEE Transactions on,", "citeRegEx": "Tsanas et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tsanas et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 28, "context": "Among its most well-known instances are the LASSO (Tibshirani, 1996), in which `1 regularization is added to a squared loss to encourage sparsity in the solutions, or `2-regularized logistic regression, in which squared `2 regularization (known as weight decay in the context of neural networks) is added to obtain solutions with small euclidean norm.", "startOffset": 50, "endOffset": 68}, {"referenceID": 26, "context": "This criterion can be a goodness of fit on unseen data, such as a cross-validation loss, or some criteria of model quality on the train set such as SURE (Stein, 1981), AIC/BIC (Liu and Yang, 2011) or Mallows Cp (Mallows, 1973), to name a few.", "startOffset": 153, "endOffset": 166}, {"referenceID": 19, "context": "This criterion can be a goodness of fit on unseen data, such as a cross-validation loss, or some criteria of model quality on the train set such as SURE (Stein, 1981), AIC/BIC (Liu and Yang, 2011) or Mallows Cp (Mallows, 1973), to name a few.", "startOffset": 176, "endOffset": 196}, {"referenceID": 2, "context": "Random search (Bergstra et al., 2011) has been proven to yield a faster exploration of the hyperparameter space than grid search, specially in spaces with multiple hyperparameters.", "startOffset": 14, "endOffset": 37}, {"referenceID": 3, "context": "(Brochu et al., 2010) for an review on current methodologies).", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "This probabilistic model typically relies on a Gaussian process regressor but other approaches exist using trees (Bergstra et al., 2011) or ensemble methods (Lacoste et al.", "startOffset": 113, "endOffset": 136}, {"referenceID": 14, "context": ", 2011) or ensemble methods (Lacoste et al., 2014).", "startOffset": 28, "endOffset": 50}, {"referenceID": 15, "context": "This approach, which we will refer to as implicit differentiation (Larsen et al., 1996; Bengio, 2000; Foo et al., 2008), relies on the observation that under some regularity conditions it is possible to replace the inner optimization problem by an implicit equation.", "startOffset": 66, "endOffset": 119}, {"referenceID": 0, "context": "This approach, which we will refer to as implicit differentiation (Larsen et al., 1996; Bengio, 2000; Foo et al., 2008), relies on the observation that under some regularity conditions it is possible to replace the inner optimization problem by an implicit equation.", "startOffset": 66, "endOffset": 119}, {"referenceID": 9, "context": "This approach, which we will refer to as implicit differentiation (Larsen et al., 1996; Bengio, 2000; Foo et al., 2008), relies on the observation that under some regularity conditions it is possible to replace the inner optimization problem by an implicit equation.", "startOffset": 66, "endOffset": 119}, {"referenceID": 17, "context": "Efficient schemes for multiplication by the Hessian can be derived for least squares, logistic regression (Lin et al., 2008) and other general loss functions (Pearlmutter, 1994).", "startOffset": 106, "endOffset": 124}, {"referenceID": 22, "context": ", 2008) and other general loss functions (Pearlmutter, 1994).", "startOffset": 41, "endOffset": 60}, {"referenceID": 15, "context": "Originally motivated by the problem of setting the regularization parameter in the context of neural networks (Larsen et al., 1996; 1998; Bengio, 2000), has also been applied to the problem of selecting kernel parameters (Chapelle et al.", "startOffset": 110, "endOffset": 151}, {"referenceID": 0, "context": "Originally motivated by the problem of setting the regularization parameter in the context of neural networks (Larsen et al., 1996; 1998; Bengio, 2000), has also been applied to the problem of selecting kernel parameters (Chapelle et al.", "startOffset": 110, "endOffset": 151}, {"referenceID": 5, "context": ", 1996; 1998; Bengio, 2000), has also been applied to the problem of selecting kernel parameters (Chapelle et al., 2002; Seeger, 2008) or multiple regularization parameters in log-linear models (Foo et al.", "startOffset": 97, "endOffset": 134}, {"referenceID": 25, "context": ", 1996; 1998; Bengio, 2000), has also been applied to the problem of selecting kernel parameters (Chapelle et al., 2002; Seeger, 2008) or multiple regularization parameters in log-linear models (Foo et al.", "startOffset": 97, "endOffset": 134}, {"referenceID": 9, "context": ", 2002; Seeger, 2008) or multiple regularization parameters in log-linear models (Foo et al., 2008).", "startOffset": 81, "endOffset": 99}, {"referenceID": 13, "context": "This approach has also been successfully applied to the problem of image reconstruction (Kunisch and Pock, 2013; Calatroni et al., 2015), in which case the simplicity of the cost function function allows for a particularly simple expression of the gradient with respect to hyperparameters.", "startOffset": 88, "endOffset": 136}, {"referenceID": 4, "context": "This approach has also been successfully applied to the problem of image reconstruction (Kunisch and Pock, 2013; Calatroni et al., 2015), in which case the simplicity of the cost function function allows for a particularly simple expression of the gradient with respect to hyperparameters.", "startOffset": 88, "endOffset": 136}, {"referenceID": 0, "context": ", 1996; 1998; Bengio, 2000), has also been applied to the problem of selecting kernel parameters (Chapelle et al., 2002; Seeger, 2008) or multiple regularization parameters in log-linear models (Foo et al., 2008). This approach has also been successfully applied to the problem of image reconstruction (Kunisch and Pock, 2013; Calatroni et al., 2015), in which case the simplicity of the cost function function allows for a particularly simple expression of the gradient with respect to hyperparameters. Iterative differentiation. In this approach, the gradient with respect to hyperparameters is computed by differentiating each step of the inner optimization algorithm and then using the chain rule to aggregate the results. Since the gradient is computed after a finite number of steps of the inner optimization routine, the estimated gradient is naturally an approximation to the true gradient. This method was first proposed by Domke (2012) and later extended to the setting of stochastic gradient descent by Maclaurin et al.", "startOffset": 14, "endOffset": 946}, {"referenceID": 0, "context": ", 1996; 1998; Bengio, 2000), has also been applied to the problem of selecting kernel parameters (Chapelle et al., 2002; Seeger, 2008) or multiple regularization parameters in log-linear models (Foo et al., 2008). This approach has also been successfully applied to the problem of image reconstruction (Kunisch and Pock, 2013; Calatroni et al., 2015), in which case the simplicity of the cost function function allows for a particularly simple expression of the gradient with respect to hyperparameters. Iterative differentiation. In this approach, the gradient with respect to hyperparameters is computed by differentiating each step of the inner optimization algorithm and then using the chain rule to aggregate the results. Since the gradient is computed after a finite number of steps of the inner optimization routine, the estimated gradient is naturally an approximation to the true gradient. This method was first proposed by Domke (2012) and later extended to the setting of stochastic gradient descent by Maclaurin et al. (2015). We note also that contrary to the implicit differentiation approach, this method can be applied to problems with non-smooth cost functions (Deledalle et al.", "startOffset": 14, "endOffset": 1038}, {"referenceID": 27, "context": "Swersky et al. (2014) proposes an approach in which the inner optimization is \u201cfreezed\u201d whenever the method decides that the current hyperparameter values are not promising.", "startOffset": 0, "endOffset": 22}, {"referenceID": 6, "context": "The analysis of this algorithm is inspired by the work of d\u2019Aspremont (2008); Schmidt et al.", "startOffset": 58, "endOffset": 77}, {"referenceID": 6, "context": "The analysis of this algorithm is inspired by the work of d\u2019Aspremont (2008); Schmidt et al. (2011); Friedlander and Schmidt (2012) on inexact-gradient algorithms for convex optimization.", "startOffset": 58, "endOffset": 100}, {"referenceID": 6, "context": "The analysis of this algorithm is inspired by the work of d\u2019Aspremont (2008); Schmidt et al. (2011); Friedlander and Schmidt (2012) on inexact-gradient algorithms for convex optimization.", "startOffset": 58, "endOffset": 132}, {"referenceID": 9, "context": "To overcome this and following (Foo et al., 2008), we use the logistic loss as the validation loss.", "startOffset": 31, "endOffset": 49}, {"referenceID": 18, "context": "The solver used for the inner optimization problem of the logistic regression problem is L-BFGS (Liu and Nocedal, 1989), while for Ridge regression we used a linear conjugate descent method.", "startOffset": 96, "endOffset": 119}, {"referenceID": 29, "context": "This dataset is composed of a range of biomedical voice measurements from 42 people with early-stage Parkinson\u2019s disease(Tsanas et al., 2010).", "startOffset": 120, "endOffset": 141}, {"referenceID": 1, "context": "This is the random search method (Bergstra and Bengio, 2012) samples the hyperparameters from a predefined distribution.", "startOffset": 33, "endOffset": 60}, {"referenceID": 8, "context": "This is the iterative differentiation approach from (Domke, 2012), using the same inneroptimization algorithm as HOAG.", "startOffset": 52, "endOffset": 65}, {"referenceID": 24, "context": "Given the success of recent stochastic optimization techniques (Schmidt et al., 2013; Johnson and Zhang, 2013) it seems natural to study a stochastic variant of this algorithm, that is, one in which the updates in the inner and outer optimization schemes have a cost that is independent of the number of samples.", "startOffset": 63, "endOffset": 110}, {"referenceID": 12, "context": "Given the success of recent stochastic optimization techniques (Schmidt et al., 2013; Johnson and Zhang, 2013) it seems natural to study a stochastic variant of this algorithm, that is, one in which the updates in the inner and outer optimization schemes have a cost that is independent of the number of samples.", "startOffset": 63, "endOffset": 110}], "year": 2017, "abstractText": "Most models in machine learning contain at least one hyperparameter to control for model complexity. Choosing an appropriate set of hyperparameters is both crucial in terms of model accuracy and computationally challenging. In this work we propose an algorithm for the optimization of continuous hyperparameters using inexact gradient information. An advantage of this method is that hyperparameters can be updated before model parameters have fully converged. We also give sufficient conditions for the global convergence of this method, based on regularity conditions of the involved functions and summability of errors. Finally, we validate the empirical performance of this method on the estimation of regularization constants of `2-regularized logistic regression and kernel Ridge regression. Empirical benchmarks indicate that our approach is highly competitive with respect to state of the art methods.", "creator": "LaTeX with hyperref package"}}}