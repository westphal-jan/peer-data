{"id": "1602.02373", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Feb-2016", "title": "Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings", "abstract": "One-hot CNN (convolutional neural network) has been shown to be effective for text categorization in our previous work. We view it as a special case of a general framework which jointly trains a linear model with a non-linear feature generator consisting of `text region embedding + pooling'. Under this framework, we explore a more sophisticated region embedding method using Long Short-Term Memory (LSTM). LSTM can embed text regions of variable (and possibly large) sizes, whereas the region size needs to be fixed in a CNN. We seek the best use of LSTM for the purpose in the supervised and semi-supervised settings, starting with the idea of one-hot LSTM, which eliminates the customarily used word embedding layer. Our results indicate that on this task, embeddings of text regions, which can convey higher concepts than single words in isolation, are more useful than word embeddings. We report performances exceeding the previous best results on four benchmark datasets.", "histories": [["v1", "Sun, 7 Feb 2016 14:05:58 GMT  (206kb,D)", "http://arxiv.org/abs/1602.02373v1", null], ["v2", "Thu, 26 May 2016 15:26:34 GMT  (229kb,D)", "http://arxiv.org/abs/1602.02373v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.CL cs.LG", "authors": ["rie johnson", "tong zhang 0001"], "accepted": true, "id": "1602.02373"}, "pdf": {"name": "1602.02373.pdf", "metadata": {"source": "CRF", "title": "Supervised and Semi-Supervised Text Categorization using One-Hot LSTM for Region Embeddings", "authors": ["Rie Johnson", "Tong Zhang"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "It is the problem written in a natural language, and there are numerous examples of use which relate to answering questions relating to answering questions relating to answering questions.) The answer to questions relating to answering questions relating to answering questions which relate to answering questions which relate to answering questions relating to answering questions which relate to answering questions which relate to answering questions which relate to answering questions which relate to answering questions which relate to answering questions which relate to answering questions which relate to answering questions which relate to answering questions which relate to answering questions which relate to answering questions which relate to answering questions which answer to answering questions which relate to answering questions which relate to answering to answering questions which relate to answering questions which relate to answering questions which relate to answering questions which answer to answering answers to answering questions which relate to answering questions which answer to answering to answering questions which relate to answering questions which answer to answering to answering questions which relate to answering to answering questions which answer which relate to answering to answering to answering questions which relate to answering to answering questions which relate to ansving to answering to answering questions which relate to ansving to the answer which relate to answering to answering to answering questions which relate to answering questions which relate to answering to answering to answering questions which answer to answering questions which answer to answering to the answer to answering which answer"}, {"heading": "1.1 Preliminary", "text": "Based on the following LSTM formulation used in LSTM training, we have used the following LSTM formulations as the basis for our work; it has also been used to introduce short sentences mostly used for sentiment analysis, and some of them rely on syntactic parse trees; see, for example, (Zhu et al., 2015; Tai et al., 2015; Le & Zuidema, 2015). Unlike these studies, this work, like JZ15, focuses on the classification of generic complete documents without specific linguistic knowledge. Similarly, DL15 (Dai & Le, 2015) is applied to the categorization of generic complete documents. Therefore, our empirical comparisons are focused on DL15 and JZ15, both of which are reported to be new states of art. Let us first introduce the general LSTM formulation and then briefly describe the DL15 model of how to base the challenges of using LSTM for these multiple STM formulations as we work."}, {"heading": "2 One-hot LSTM for text categorization", "text": "Within the framework of \"Region Embedding + Pooling\" for text categorization, we strive for an effective and efficient use of the LSTM as a new method for regional embedding, starting with the idea of a uniform LSTM."}, {"heading": "2.1 One-hot LSTM: why is it a good idea?", "text": "We eliminate the word embedding layer used by wv-LSTM (as well as most of the previous LSTMs on text) and feed directly one-sided hot vectors to LSTM, which we call One-Hot-LSTM. Here, we focus on an end-to-end system in a supervised environment so that there is no additional data (e.g. unlabeled data) or additional algorithms (e.g. for learning word embedding). In this environment, a word embedding layer, if contained, can be randomly initialized and trained (Sutskever et al., 2014) suggested making each minibatch of sequences of similar lengths, but we found that this strategy slows down word embedding by hindering the stochastic nature of the SGD part of the model. As presented later, our experiments show that the embedding layer of a word in this setting is degraded model accuracy."}, {"heading": "2.2 More simplifications", "text": "In fact, it is so that it is a matter of a way in which people move in the most diverse living worlds in which they move and in which they move. (...) It is not so that they see themselves as being able to surpass themselves. (...) It is as if they were able to change the world. (...) It is as if they were able to change the world. (...) It is as if they were able to change the world. (...) It is as if they were able to change the world. (...) It is as if they were in the world of the world of the world, the world of the world, the world of the world, in the world of the world, in the world of the world, in the world of the world, in the world of the world, in the world of the world, in the world, in the world of the world, in the world, in the world, in the world, in the world in the world in the world, in the world in the world in the world, in the world in the world in the world in the world, in the world in the world in the world in the world, in the world in the world in the world in the world in the world, in the world in the world in the world in the world in the world in the world, in the world in the world in the world in the world in the world in the world, in the world in the world in the world in the world in the world in the world, in the world in the world in the world in the world in the world in and in the world in the world, in the world in the world in the world in the world in the world in and in the world in the world in and in the world in the world in the world in the world, in the world in the world in and in the world in and in the world in the world in and in the world in the world in the world in and in the world in the world, in the world in and in the world in and in the world in the world in the world, in and in the world in the world in and in the world in the world in and in the world in and in the world in the world in and in the world in the world in the world, and in the world in the world in and in the world in the world in and in the world in and in the world"}, {"heading": "2.3 Experiments (supervised)", "text": "We used four datasets: IMDB3, Elec4, RCV1 (second-level topics) and 20-newsgroup (20NG) 5 to facilitate direct comparison with JZ15 and DL15. The first three were used in JZ15. IMDB and 20NG of this version, however, were used in DL15. Datasets are summarized in Table 2.3. http: / / ana.cachopo.org / datasets-for-single-label-text-categorization"}, {"heading": "3 Semi-supervised one-hot LSTM", "text": "To use unlabeled data as an additional resource for LSTM, we use a non-linear extension of two-view feature learning, the linear version of which appeared in our previous paper (Ando & Zhang, 2005; 2007), which was used in JZ15b to learn from unlabeled data a region embodied by a folding layer, in which we use it to learn a region embodied by a unified LSTM. Let's start with a brief overview of two-view nonlinear feature learning."}, {"heading": "3.1 Two-view embedding (tv-embedding) [JZ15b]", "text": "A rough sketch is this: Consider two views of the input. An embedding is referred to as TV embedding if the embedded view is as good as the original view for the purpose of predicting the other view. If the two views and the labels (classification goals) are related only by some hidden states, then the view embedded on the TV is as good for the purpose of classification as the original view. Such an embedding is useful provided that its dimensionality is much smaller than the original view.In JZ15b we applied this idea by treating text regions that are embedded by the folding layer as one view and its surrounding context as another view, and training a TV embedding (embodied by a folding layer) on unlabeled data. The obtained TV embedding was used to generate additional inputs in a monitored region of a hot CNN, resulting in higher accuracy."}, {"heading": "3.2 Learning one-hot LSTM tv-embeddings", "text": "A hot vectorsLSTMA good buy! Top layergood buy! buy! A good buy! Top layerA good A good buyFigure 5: Training LSTM tv-embedings on unlabeled dataWe get a tv-embedding in the form of LSTM from unlabeled data as follows. At any given time, we consider the following two views: the words we have already seen in the document (view-1), and the next few words (view-2). The task of tv-embedding learning is to predict view-2 based on view-1. We train uniform LSTMs in both directions, as in Figure 5, on unlabeled data. To this end, we use the input and output gates as well as the get gate, as we have found them useful. The theory of tv-embedding states that the regional embedding obtained in this way are useful for the task of interest if the two views are found with each other through the concepts found to reduce the relevant relationships between b."}, {"heading": "3.3 Combining LSTM tv-embeddings and CNN tv-embeddings", "text": "It is easy to see that the above set S can be extended with any TV embedding, not only in the form of LSTM (LSTM tv embedding), but also with TV embedding in the form of folding layers (CNN tv embedding) as obtained in JZ15b. Likewise, it is possible to use LSTM tv embedding to produce additional input for CNN. While both LSTM tv embedding and CNN tv embedding are regional embedding, their formulations differ greatly from each other; therefore, we expect them to complement each other and, in combination, bring further performance improvements. We will empirically confirm these assumptions in the following experiments."}, {"heading": "3.4 Semi-supervised experiments", "text": "We used IMDB, Elec, and RCV1 for our semi-supervised experiments # 5 bedeled units this is excluded due to the standard unlabeled data. Table 4 summarizes the unlabeled data.To experiment with LSTM tv-embeddings, although we used two LSTMs (forward and backward) with 100 units each on unlabeled data. The training goal was to predict the next words represented by a bag-of-word vector (i.e., position insensitive), where we were set to 20 for RCV1 and 5 for others. Similar to CNN tv-embedding experiments in JZ15b, we used the square loss with negative sampling for speed-up and with vocabulary control for reducing undesirable relationships between views that set the target vocabulary (i.e., the k words) to the 30K most frequently excluding the 30K most frequently excluding the words, the functional words (c1 or RCV1)."}, {"heading": "3.5 Experiments combining CNN tv-embeddings and LSTM tv-embeddings", "text": "In Section 3.3, we found that LSTM tv-embeddings and CNN tv-embeddings can be combined in a natural way. We experimented with this idea in the following two settings. In one setting, oh-2LSTMp takes additional input from five embeddings: two LSTM tv-embeddings used in Table 5 and three CNN tv-embeddings from JZ15b obtained through three different combinations of training targets and input representations provided publicly. 6 These CNN tv-embeddings were trained to be applied to text regions of size k at any location where arc embedding is performed, with k 5 on IMDB / Elec and 20 on RCV1. We connect each CNN tv-embeddings with an LSTM by aligning the centers of the regions of the former with the LSTM line; e.g. CNN tv-embeddings is passed on to the first five STM words."}, {"heading": "3.6 Comparison with the previous best results", "text": "Other results from previous semi-monitored models can be found in JZ15b, which all fall well short of the semi-monitored uniform CNN of Table 7. Best monitored results on IMDB / Elec of JZ15a can be found in the front row, which is achieved by integrating a document embedding layer into a unified CNN. A unified LSTM could benefit from similar integration, although we have not tried it. Many more6 http: / / riejohnson.com / cnn _ download.htmlof previous results on IMDB can be found in (Le & Mikolov, 2014), which are all above 10% except 8.78 by bi-gram NBSVM (Wang & Manning, 2012). 7.42 by sales vectors (Le & Mikolov, 2014) and 6.51 by JZ15b were considered major improvements."}, {"heading": "4 Conclusion", "text": "In the general framework of \"Region Embedding + Pooling\" for text categorization, we examined regional embedding using One-Hot-LSTM. Embedding of One-Hot-LSTM competed with or exceeded CNN's state-of-the-art embedding methods, demonstrating their effectiveness. We also found that these two types of One-Hot models significantly outperformed other methods, including previous best results on benchmark datasets. At a high level, our results suggest that: First, embedding text regions that can communicate concepts at a higher level is more useful than embedding individual words in isolation. Second, useful regional embedding can be found directly through One-Text data that could not be learned with either new labeled vectors or a new direction."}], "references": [{"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["Ando", "Rie K", "Zhang", "Tong"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Ando et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ando et al\\.", "year": 2005}, {"title": "Two-view feature generation model for semi-supervised learning", "author": ["Ando", "Rie K", "Zhang", "Tong"], "venue": "In Proceedings of ICML,", "citeRegEx": "Ando et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ando et al\\.", "year": 2007}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "van Merri\u00ebnboer", "Bart", "Gulcehre", "Caglar", "Bahdanau", "Dzmitry", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Semi-supervised sequence learning", "author": ["Dai", "Andrew M", "Le", "Quoc V"], "venue": "In NIPS,", "citeRegEx": "Dai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2015}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["Gers", "Felix A", "Schmidhuder", "J\u00fcrgen", "Cummins", "Fred"], "venue": "Neural Computation,", "citeRegEx": "Gers et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuder", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Text categorization with support vector machines: Learning with many relevant features", "author": ["Joachims", "Thorsten"], "venue": "In ECML,", "citeRegEx": "Joachims and Thorsten.,? \\Q1998\\E", "shortCiteRegEx": "Joachims and Thorsten.", "year": 1998}, {"title": "Effective use of word order for text categorization with convolutional neural networks", "author": ["Johnson", "Rie", "Zhang", "Tong"], "venue": "In NAACL HLT,", "citeRegEx": "Johnson et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2015}, {"title": "Semi-supervised convolutional neural networks for text categorization via region embedding", "author": ["Johnson", "Rie", "Zhang", "Tong"], "venue": "In NIPS,", "citeRegEx": "Johnson et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Kim", "Yoon"], "venue": "In Proceedings of EMNLP, pp. 1746\u20131751,", "citeRegEx": "Kim and Yoon.,? \\Q2014\\E", "shortCiteRegEx": "Kim and Yoon.", "year": 2014}, {"title": "Recurrent convolutional neural networks for text classification", "author": ["Lai", "Siwei", "Xu", "Liheng", "Liu", "Kang", "Zhao", "Jun"], "venue": "In Proceedings of AAAI,", "citeRegEx": "Lai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lai et al\\.", "year": 2015}, {"title": "Compositional distributional semantics with long short-term memory", "author": ["Le", "Phong", "Zuidema", "Willem"], "venue": "In Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics,", "citeRegEx": "Le et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Quoc", "Mikolov", "Tomas"], "venue": "In Proceedings of ICML,", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "Le\u00f3n", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1986\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1986}, {"title": "RCV1: A new benchmark collection for text categorization research", "author": ["Lewis", "David D", "Yang", "Yiming", "Rose", "Tony G", "Li", "Fan"], "venue": "Journal of Marchine Learning Research,", "citeRegEx": "Lewis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2004}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "In NIPS,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural netowkrs", "author": ["Sutskever", "Hya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Tai", "Kai Sheng", "Socher", "Richard", "Manning", "Christopher D"], "venue": "In Proceedings of ACL,", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Document modeling with gated recurrent neural network for sentiment classification", "author": ["Tang", "Duyu", "Qin", "Bing", "Liu", "Ting"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Tang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tieleman", "Tijman", "Hinton", "Geoffrey"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["Wang", "Sida", "Manning", "Christopher D"], "venue": "In Proceedings of ACL,", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Character-level convolutional networks for text classification", "author": ["Zhang", "Xiang", "Zhao", "Junbo", "LeCunn", "Yann"], "venue": "In NIPS,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Long short-term memory over recursive structures", "author": ["Zhu", "Xiaodan", "Sobhani", "Parinaz", "Guo", "Hongyu"], "venue": "In Proceedings of ICML,", "citeRegEx": "Zhu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 15, "context": ", (Joachims, 1998; Lewis et al., 2004).", "startOffset": 2, "endOffset": 38}, {"referenceID": 22, "context": ", (Dai & Le, 2015; Johnson & Zhang, 2015a;b; Zhang et al., 2015).", "startOffset": 2, "endOffset": 64}, {"referenceID": 14, "context": "A convolutional neural network (CNN) (LeCun et al., 1986) is a feedforward neural network with convolution layers interleaved with pooling layers, originally developed for image processing.", "startOffset": 37, "endOffset": 57}, {"referenceID": 23, "context": ", (Zhu et al., 2015; Tang et al., 2015; Tai et al., 2015; Le & Zuidema, 2015).", "startOffset": 2, "endOffset": 77}, {"referenceID": 19, "context": ", (Zhu et al., 2015; Tang et al., 2015; Tai et al., 2015; Le & Zuidema, 2015).", "startOffset": 2, "endOffset": 77}, {"referenceID": 18, "context": ", (Zhu et al., 2015; Tang et al., 2015; Tai et al., 2015; Le & Zuidema, 2015).", "startOffset": 2, "endOffset": 77}, {"referenceID": 4, "context": "The forget gate ft (Gers et al., 2000) is for resetting the memory cells.", "startOffset": 19, "endOffset": 38}, {"referenceID": 17, "context": "In this setting, a word embedding layer, if included, needs to be initialized randomly and trained as 1 (Sutskever et al., 2014) suggested making each mini-batch consist of sequences of similar lengths, but we found that on our tasks this strategy slows down convergence presumably by hampering the stochastic nature of SGD.", "startOffset": 104, "endOffset": 128}, {"referenceID": 11, "context": "Another related work is (Lai et al., 2015), which combined pooling with non-LSTM recurrent networks and a word embedding.", "startOffset": 24, "endOffset": 42}, {"referenceID": 2, "context": "It is in spirit similar to Gated Recurrent Units (Cho et al., 2014) but simpler, having fewer gates.", "startOffset": 49, "endOffset": 67}, {"referenceID": 5, "context": "In the neural network experiments, vocabulary was reduced to the most frequent 30K words of the training data to reduce computational burden; square loss was minimized with dropout (Hinton et al., 2012) applied to the input to the top layer.", "startOffset": 181, "endOffset": 202}, {"referenceID": 16, "context": "As mentioned earlier, previous studies on LSTM for text typically convert words into word vectors, and word2vec (Mikolov et al., 2013) is a popular choice for this purpose.", "startOffset": 112, "endOffset": 134}], "year": 2016, "abstractText": "One-hot CNN (convolutional neural network) has been shown to be effective for text categorization in our previous work. We view it as a special case of a general framework which jointly trains a linear model with a non-linear feature generator consisting of \u2018text region embedding + pooling\u2019. Under this framework, we explore a more sophisticated region embedding method using Long Short-Term Memory (LSTM). LSTM can embed text regions of variable (and possibly large) sizes, whereas the region size needs to be fixed in a CNN. We seek the best use of LSTM for the purpose in the supervised and semi-supervised settings, starting with the idea of one-hot LSTM, which eliminates the customarily used word embedding layer. Our results indicate that on this task, embeddings of text regions, which can convey higher concepts than single words in isolation, are more useful than word embeddings. We report performances exceeding the previous best results on four benchmark datasets.", "creator": "LaTeX with hyperref package"}}}