{"id": "1602.02450", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2016", "title": "Loss factorization, weakly supervised learning and label noise robustness", "abstract": "We prove that the empirical risk of most well-known loss functions factors into a linear term aggregating all labels with a term that is label free, and can further be expressed by sums of the loss. This holds true even for non-smooth, non-convex losses and in any RKHS. The first term is a (kernel) mean operator --the focal quantity of this work-- which we characterize as the sufficient statistic for the labels. The result tightens known generalization bounds and sheds new light on their interpretation.", "histories": [["v1", "Mon, 8 Feb 2016 01:50:43 GMT  (104kb)", "https://arxiv.org/abs/1602.02450v1", null], ["v2", "Tue, 9 Feb 2016 23:10:23 GMT  (107kb)", "http://arxiv.org/abs/1602.02450v2", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["giorgio patrini", "frank nielsen", "richard nock", "marcello carioni"], "accepted": true, "id": "1602.02450"}, "pdf": {"name": "1602.02450.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Giorgio Patrini"], "emails": ["giorgio.patrini@anu.edu.au", "nielsen@lix.polytechnique.fr", "richard.nock@nicta.com.au", "marcello.carioni@mis.mpg.de"], "sections": [{"heading": null, "text": "ar Xiv: 160 2.02 45Factorization has a direct application to poorly monitored learning. In particular, we show that algorithms such as sgd and proximal methods can be adapted with minimal effort to deal with weak monitoring once the mean operator has been estimated. We apply this idea to learning with asymmetric, loud labels, combine and expand preliminary work. In addition, we show that most losses have a data-dependent (by the mean operator) form of noise immunity, as opposed to known negative results."}, {"heading": "1 Introduction", "text": "Controlled learning is by far the most effective application of the machine learning paradigm. However, there is a growing need to decouple the success of the field from its top framework, which is often unrealistic in practice. In fact, while the amount of data available continues to grow, its relative training labels - often derived from human effort - are rare, and therefore learning is sometimes done with missing, aggregated and / or loud labels. Therefore, weakly supervised learning (wsl) has attracted a lot of research. In this work, we focus on binary classification under weak supervision. Traditionally, wsl problems are attacked by designing ad hoc loss functions and optimization algorithms tied to the learning situation. Instead, we advocate not \"reinventing the wheel\" and present a unifying treatment. In summary, we show that any loss that fully labels a minimizing algorithm over data is a guarantee of sufficient acoustic ability to change into a gerulic situation."}, {"heading": "1.1 Contribution and related work", "text": "We introduce linear-odd losses (pu), a definition that does not require smoothness or convexity, but a loss l is such that l (x) \u2212 l (\u2212 x) is linear. Many losses of practical interest are those, e.g. logistical and quadratic. We prove a theorem reminiscent of Fisher-Neyman's factorization (Lehmann and Casella, 1998) of the exponential family that forms the basis of this work: It shows how empirical risk factors (Figure 1) in another free term with sufficient statistics of labels, the middle operator, the middle operator. The interplay of the two components is still evident on newly derived generalizations that also improve on known factors. [Kakade et al, 2009] The above linearity is known to guarantee certain losses in learning."}, {"heading": "2 Learning setting and background", "text": "We define vectors in bold as x and 1 {p} as an indicator that p is true. We define [m]. = {1,.., m} and [x] +. = max (0, x). In binary classification, a learning sample is S = {(xi, yi), i [m]} a sequence of (observation, designation) pairs, examples of an unknown distribution D over X \u00b7 Y, with X Rd and Y = {\u2212 1, 1}. The expectation (or average) above (x, y) is called ED (ES). In the face of a hypothesis h \u00b2 H, h \u2212 R, a loss is any function l: Y \u00b7 R \u2192 R. A loss results in a penalty l (y, h (x))) when predicting the value h (x) and the designation observed is y."}, {"heading": "2.1 Background: exponential family and logistic loss", "text": "Some background information on the exponential family will follow. < xi \u2212 xi = < \u03b8, yx > \u2212 log \"y exp < \u03b8, yx >), with y random variable. < xi \u2212 xi = < \u03b8, yx > \u2212 log\" y exp < \u03b8, yx >), with y random variable. The two terms in the exponent are the log partitioning function and the sufficient statistical yixi, which fully summarizes an example (x, y).The Fisher-Neyman theorem [Lehmann and Casella, 1998, Theorem 6.5] shows how an adequate and necessary condition for an adequate statistical T (y): the probability distribution factors in two functions that only T interact with the y: p\u03b8 (y) = g\u03b8 (T (y) = negative y \") g\" (y), ltls. \"In our case, g\" (y | x) = 1, T. \"facicization\" and \"p.\""}, {"heading": "3 Loss factorization and sufficiency", "text": "It's not the first time the star has been linked to a number of high-profile women, having been linked to a number of high-profile men in the past, including model Gigi Hadid, who has been linked to a number of high-profile women, including model Gigi Hadid, model Gigi Hadid, model Gigi Hadid, model Gigi Hadid, model Gigi Hadid, model Gigi Hadid, model Gigi Hadid, model Gigi Hadid, model Gigi Hadid, model Gigi Hadid, model Gigi Hadid, model Gigi Hadid, model Gigi Hadid, model Gigi Hadid, model Gigi Hadid, model Gigi Hadid, model Gigi Hadid, model Gigi Hadid, model Gigi Hadid, model Gigi Hadid, model Gigi Hadid, model Gigi Hadid, model Gigi Hadid, model Gigi Hadid, model Gigi Hadid, model Gigi Hadid, model Gigi Hadid, model Gigi Hadid, model Gigi Hadid, model Gigi Hadid, and model Gigi Hadid, and model Gigi Hadid."}, {"heading": "4 Weakly supervised learning", "text": "In the next two sections we will discuss applications for wsl. Let's remember that in this scenario we learn on S-value with partially observable labels, but aim to generalize on D-value. Let's say we know an algorithm that can only learn on S-value. This direction was taken by working on llp [Quadrianto et al., 2009, logistic loss] and [Patrini et al., 2014, spl] and in the creation of noisy labels [van Rooyen et al., 2015, unhinged loss] and [Gao et al., 2016, logistic loss] and [Patrini et al., 2014, spl] and in the creation of noisy labels [van Rooyen et al., 2015, unhinged loss] and [Gao et al., 2016, logistic loss]. The approach is in contrast to ad-hoc losses and optimization processes that often try to latent, the labels, 2012 and EM]."}, {"heading": "5 Asymmetric label noise", "text": "In the field of learning with noise particles, the estimator is a sequence of examples drawn from a distribution. < / p > p > p > p > p > p > p > p > p > p > p > p > p > p > p (xi, \u2212 p) is therefore with a probability of a maximum of 1 / 2 or it is (xi, yi) otherwise. However, the noise rates depend on (p +, p \u2212) p \u2212 p (0, 1 / 2) 2 for both positive and negative examples, that is, asymmetric noise (aln) [Natarajan et al., 2013].Our first result is based on [Natarajan et al., 2013, Lemma 1] which provides a recipe for unbiased estimates of losses. (4) The estimator is an unbiased theorem 3, rather than an estimate of the entire l that we act on the sufficient statistic: \u00b5 S = ES [y \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p."}, {"heading": "5.1 Every lol is approximately noise-robust", "text": "The next result comes in combination with Theorem 8: it holds independent of algorithm and (linear-odd) loss of choice. In particular, we show that each learner possesses a distributional property of robustness against asymmetric noise. (D) There is no estimate of Rooyen et al., 2015] in a weaker sense. (D) While we are independent of the observational results, we will first apply the notion of robustness of [Ghosh et al.), RD (Rooyen et al., 2015] in a weaker sense. (D) While we are independent of the observational results, we will respectively use the minimizers of the (RD) and the minimizers of the (RD, l), Rooyen et al.) in H. l it is said that the Rooyen-aln is robust if for all D, R D, l. s."}, {"heading": "5.2 Experiments", "text": "We assume that we know only a limited number of learning processes. (...) We assume that there will only be a limited number of learning processes. (...) We assume that there will only be a limited number of learning processes. (...) We assume that there will only be a limited number of learning processes. (...) We are not able to compare the actual number of learning processes. (...) We are not able to compare the actual number of learning processes. (...) We are not able to compare the actual number of learning processes. (...) < 0 and l (x) \u2192 0 for x. (...) We are not able to compare all, but there is no inclusion. (...) One example is e \u2212 x.Algorithm 2 \u00b5sgd applied to noisy labelsWe by building a toy on the theorem 10."}, {"heading": "6 Discussion and conclusion", "text": "The intuition behind the relevance of the mean operator becomes clear as soon as we describe it as follows. (Lemma 11 Let \u03c0 +. = ES1 {y > 0} be the positive label proportional to S. Then the CovS [x, y] covariance is what we need to know about the labels for linear learning models. (Proof in A.7) We have come to the unsurprising fact that - if the observations are centered - the CovS covariance [x, y] is what we need to know about the labels for linear learning models. The rest of the loss can be considered a data-dependent regulator. However, note how the condition of \u00b5D [2 = 0] implies CovD [x, y] = 0, which would make linear classification difficult and limit the validity of Theorem 10. A nuclearized version of this Lemma is given in [Song et al, 2009]."}, {"heading": "Acknowledgements", "text": "The authors thank Aditya Menon for her insightful feedback on an earlier draft. NICTA is funded by the Australian government through the Ministry of Communications and the Australian Research Council through the ICT Center of Excellence Program."}, {"heading": "Appendices", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Proof of Lemma 5", "text": "We must show the double implication that defines sufficiency for y. \u21d2) By factorization theorem (3), RS, l (h) \u2212 RS \u2032, l (h) is independent of the label only if the odd part is cancelled. g) If \u00b5S = \u00b5 \u2032 S then RS, l (h) \u2212 RS \u2032, l (h) is independent of the label because the label only appears in the mean operator due to the factorization theory (3)."}, {"heading": "A.2 Proof of Lemma 6", "text": "Consider the class of lols that satisfies l (x) \u2212 l (\u2212 x) = 2ax. For each element of the class, we define le (x) = l (x) \u2212 ax what is currently. In fact, we have (\u2212 x) = l (\u2212 x) + ax = l (x) \u2212 2ax + ax = l (x) \u2212 ax = l (x) \u2212 ax = le (x)."}, {"heading": "A.3 Proof of Theorem 7", "text": "We start with the fact that we identify two auxiliary persons without loss of universality. The next proof establishes a limit on the number of x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "A.5 Proof of Theorem 8", "text": "This theory is a version of Theorem 7, which is applied to the case of asymmetric noise. These results differ in three elements. First, we look at the generalisation property of a minimizer. \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212"}, {"heading": "A.6 Proof of Theorem 10", "text": "We now repeat and prove Theorem 8. The reader could question the limit in such a way that the quantity on the right can change by recalculating \u00b5D by X, i.e. the maximum L2 norm of the observations in space X. Although such a transformation would also affect the L risks on the left, which would compensate for the effect. In this sense, we formulate the result without making explicit dependence on X. Theorem 10 assumes that the L risks on the left would also be affected. Let us leave (1) or the minimizers of (RD, 1), (RD, 1), l (TB) in H. Then every A-Lol is an A-Lol."}, {"heading": "Moreover:", "text": "D D D D D (D D D D D D D (D D D D D D D D D D D D (D D D D D D D D D D D D D D D D (D D D D D D D D D D D D D D D D D D D D D D D D D D (D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D (D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D"}, {"heading": "A.7 Proof of Lemma 11", "text": "CovS [x, y] = ES [yx] \u2212 ES [y] ES [x] = \u00b5S \u2212 1m \u2211 i: yi > 01 \u2212 1 m \u2211 i: yi < 01ES [x] = \u00b5S \u2212 (2\u03c0 + \u2212 1) ES [x].The second statement follows immediately."}, {"heading": "B Factorization of non linear-odd losses", "text": "x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x,"}, {"heading": "C Factorization of square loss for regression", "text": "We have formulated the factorization theorem for classification problems, but a similar property applies to the regression with square loss: f (< \u03b8, xi >, y) = (< \u03b8, xi > \u2212 yi) 2 Factors asES [(< \u03b8, x > \u2212 y) 2] = ES [< \u03b8, x > 2] + ES [y2] \u2212 2 < \u03b8, \u00b5 >.If you take the minimizers on both sides, you get the minimum fraction ES [f (< \u03b8, x >, y)] = argmin \u03b8 ES [< \u03b8, x > 2] \u2212 2 < \u03b8, \u00b5 > = argmin \u03b8 \u2022 X < \u03b8 \u2012 22 \u2212 2 < \u03b8, \u00b5 >."}, {"heading": "D The role of lols in [du Plessis et al., 2015]", "text": "c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c"}, {"heading": "E Additional examples of loss factorization", "text": "\u2212 2 \u2212 1 0 1 2 x \u2212 2 \u2212 10121 {x < 0} le (x) lo (x) (a) 0-1 loss \u2212 2 \u2212 1 0 1 2 x \u2212 20246 \u221a 1 + x2 \u2212 xle (x) lo (x) (b) Matsushita loss \u2212 2 \u2212 1 0 1 2 x \u2212 2024 6 | x | \u2212 x + 1 le (x) lo (x) (c) \u03c1 loss, \u03c1 = 1 \u2212 2 \u2212 1 0 1 2 \u2212 1012 3 2 \u2212 Hinge le (x) lo (x) (d) 2-Hinge le (x) lo (2 \u2212 1 0 1 2 x \u2212 2 \u2212 10123 [0, 1 \u2212 x] + le (x) lo (x) (e) (2 \u2212 1 0 1 2 x \u2212 20246 Huberle (x) (f) Huber loss"}], "references": [{"title": "Theory of point estimation, volume 31", "author": ["E.L. Lehmann", "G. Casella"], "venue": "Springer Science & Business Media,", "citeRegEx": "Lehmann and Casella.,? \\Q1998\\E", "shortCiteRegEx": "Lehmann and Casella.", "year": 1998}, {"title": "On the complexity of linear prediction: Risk bounds, margin bounds, and regularization", "author": ["S.M. Kakade", "K. Sridharan", "A. Tewari"], "venue": "In NIPS*23,", "citeRegEx": "Kakade et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2009}, {"title": "Convex formulation for learning from positive and unlabeled data", "author": ["M C. du Plessis", "G. Niu", "M. Sugiyama"], "venue": "In 32 th ICML,", "citeRegEx": "Plessis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Plessis et al\\.", "year": 2015}, {"title": "Degrees of supervision", "author": ["D. Garc\u0131a-Garc\u0131a", "R.C. Williamson"], "venue": "In NIPS*25 workshop on Relations between machine learning problems,", "citeRegEx": "Garc\u0131a.Garc\u0131a and Williamson.,? \\Q2011\\E", "shortCiteRegEx": "Garc\u0131a.Garc\u0131a and Williamson.", "year": 2011}, {"title": "Weak supervision and other non-standard classification problems: a taxonomy", "author": ["J. Hernandez-Gonzalez", "I. Inza", "J.A. Lozano"], "venue": "In Pattern Recognition Letters,", "citeRegEx": "Hernandez.Gonzalez et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hernandez.Gonzalez et al\\.", "year": 2016}, {"title": "Learning with noisy labels", "author": ["N. Natarajan", "I.S. Dhillon", "P.K. Ravikumar", "A. Tewari"], "venue": "In NIPS*27,", "citeRegEx": "Natarajan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Natarajan et al\\.", "year": 2013}, {"title": "Semi-supervised learning", "author": ["O. Chapelle", "B. Sch\u00f6lkopf", "A. Zien"], "venue": "MIT press Cambridge,", "citeRegEx": "Chapelle et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2006}, {"title": "Solving the multiple instance problem with axis-parallel rectangles", "author": ["T.G. Dietterich", "R.H. Lathrop", "T. Lozano-P\u00e9rez"], "venue": "Artificial Intelligence,", "citeRegEx": "Dietterich et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Dietterich et al\\.", "year": 1997}, {"title": "Estimating labels from label proportions", "author": ["N. Quadrianto", "A.J. Smola", "T.S. Caetano", "Q.V. Le"], "venue": "JMLR, 10:2349\u20132374,", "citeRegEx": "Quadrianto et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Quadrianto et al\\.", "year": 2009}, {"title": "Almost) no label no cry", "author": ["G. Patrini", "R. Nock", "P. Rivera", "T. Caetano"], "venue": "In NIPS*28,", "citeRegEx": "Patrini et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Patrini et al\\.", "year": 2014}, {"title": "Learning with symmetric label noise: The importance of being unhinged", "author": ["B. van Rooyen", "A.K. Menon", "R.C. Williamson"], "venue": "In NIPS*29,", "citeRegEx": "Rooyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rooyen et al\\.", "year": 2015}, {"title": "Risk minimization in the presence of label noise", "author": ["W. Gao", "L. Wang", "Y.-F. Li", "Z.-H Zhou"], "venue": "In Proc. of the 30 AAAI Conference on Artificial Intelligence,", "citeRegEx": "Gao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2016}, {"title": "Random classification noise defeats all convex potential boosters", "author": ["P.M. Long", "R.A. Servedio"], "venue": "Machine learning,", "citeRegEx": "Long and Servedio.,? \\Q2010\\E", "shortCiteRegEx": "Long and Servedio.", "year": 2010}, {"title": "Learning SVMs from sloppily labeled data", "author": ["G. Stempfel", "L. Ralaivola"], "venue": "In Artificial Neural Networks (ICANN),", "citeRegEx": "Stempfel and Ralaivola.,? \\Q2009\\E", "shortCiteRegEx": "Stempfel and Ralaivola.", "year": 2009}, {"title": "On the design of robust classifiers for computer vision", "author": ["H. Masnadi-Shirazi", "V. Mahadevan", "N. Vasconcelos"], "venue": "In Proc. of the 23 IEEE CVPR,", "citeRegEx": "Masnadi.Shirazi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Masnadi.Shirazi et al\\.", "year": 2010}, {"title": "t-logistic regression", "author": ["N. Ding", "S.V.N. Vishwanathan"], "venue": "In NIPS*24,", "citeRegEx": "Ding and Vishwanathan.,? \\Q2010\\E", "shortCiteRegEx": "Ding and Vishwanathan.", "year": 2010}, {"title": "Bridging weak supervision and privacy aware learning via sufficient statistics. NIPS*29, Workshop on learning and privacy with incomplete data and weak supervision", "author": ["G. Patrini", "F. Nielsen", "R. Nock"], "venue": null, "citeRegEx": "Patrini et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Patrini et al\\.", "year": 2015}, {"title": "Sample complexity bounds for differentially private learning", "author": ["K. Chaudhuri", "D. Hsu"], "venue": "In JMLR,", "citeRegEx": "Chaudhuri and Hsu.,? \\Q2011\\E", "shortCiteRegEx": "Chaudhuri and Hsu.", "year": 2011}, {"title": "A Hilbert space embedding for distributions", "author": ["A. Smola", "A. Gretton", "L. Song", "B. Sch\u00f6lkopf"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Smola et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Smola et al\\.", "year": 2007}, {"title": "Bregman divergences and surrogates for learning", "author": ["R. Nock", "F. Nielsen"], "venue": "IEEE Trans.PAMI,", "citeRegEx": "Nock and Nielsen.,? \\Q2009\\E", "shortCiteRegEx": "Nock and Nielsen.", "year": 2009}, {"title": "Bayesian parameter estimation via variational methods", "author": ["T.S. Jaakkola", "M.I. Jordan"], "venue": "Statistics and Computing,", "citeRegEx": "Jaakkola and Jordan.,? \\Q2000\\E", "shortCiteRegEx": "Jaakkola and Jordan.", "year": 2000}, {"title": "On the boosting ability of top-down decision tree learning algorithms", "author": ["M.J. Kearns", "Y. Mansour"], "venue": "In 28 th ACM STOC,", "citeRegEx": "Kearns and Mansour.,? \\Q1996\\E", "shortCiteRegEx": "Kearns and Mansour.", "year": 1996}, {"title": "The design of bayes consistent loss functions for classification", "author": ["H. Masnadi-Shirazi"], "venue": "PhD thesis, University of California at San Diego,", "citeRegEx": "Masnadi.Shirazi.,? \\Q2011\\E", "shortCiteRegEx": "Masnadi.Shirazi.", "year": 2011}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["P.-L. Bartlett", "S. Mendelson"], "venue": "JMLR, 3:463\u2013482,", "citeRegEx": "Bartlett and Mendelson.,? \\Q2002\\E", "shortCiteRegEx": "Bartlett and Mendelson.", "year": 2002}, {"title": "Unifying divergence minimization and statistical inference via convex duality", "author": ["Y. Altun", "A.J. Smola"], "venue": "In 19 th COLT,", "citeRegEx": "Altun and Smola.,? \\Q2006\\E", "shortCiteRegEx": "Altun and Smola.", "year": 2006}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter"], "venue": "Mathematical programming,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2011}, {"title": "Making risk minimization tolerant to label", "author": ["A. Ghosh", "N. Manwani", "P.S. Sastry"], "venue": "noise. Neurocomputing,", "citeRegEx": "Ghosh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ghosh et al\\.", "year": 2015}, {"title": "Learning from noisy labels with deep neural networks", "author": ["S. Sukhbaatar", "R. Fergus"], "venue": null, "citeRegEx": "Sukhbaatar and Fergus.,? \\Q2014\\E", "shortCiteRegEx": "Sukhbaatar and Fergus.", "year": 2014}, {"title": "Label-noise robust logistic regression and its applications", "author": ["J. Bootkrajang", "A. Kab\u00e1n"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Bootkrajang and Kab\u00e1n.,? \\Q2012\\E", "shortCiteRegEx": "Bootkrajang and Kab\u00e1n.", "year": 2012}, {"title": "Classification with noisy labels by importance reweighting", "author": ["T. Liu", "D. Tao"], "venue": null, "citeRegEx": "Liu and Tao.,? \\Q2014\\E", "shortCiteRegEx": "Liu and Tao.", "year": 2014}, {"title": "Learning from corrupted binary labels via class-probability estimation", "author": ["A. Menon", "B. Van Rooyen", "C.S. Ong", "B. Williamson"], "venue": null, "citeRegEx": "Menon et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Menon et al\\.", "year": 2015}, {"title": "A rate of convergence for mixture proportion estimation, with application to learning from noisy labels", "author": ["C. Scott"], "venue": "AISTATS,", "citeRegEx": "Scott.,? \\Q2015\\E", "shortCiteRegEx": "Scott.", "year": 2015}, {"title": "Hilbert space embeddings of conditional distributions with applications to dynamical systems", "author": ["L. Song", "J. Huang", "A.J. Smola", "K. Fukumizu"], "venue": "In 26 th ICML. ACM,", "citeRegEx": "Song et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Song et al\\.", "year": 2009}, {"title": "Learning with kernels: Support vector machines, regularization, optimization, and beyond", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "MIT press,", "citeRegEx": "Sch\u00f6lkopf and Smola.,? \\Q2002\\E", "shortCiteRegEx": "Sch\u00f6lkopf and Smola.", "year": 2002}, {"title": "Learning reductions that really work", "author": ["A. Beygelzimer", "H. Daum\u00e9 III", "J. Langford", "P. Mineiro"], "venue": null, "citeRegEx": "Beygelzimer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2015}, {"title": "Error limiting reductions between classification tasks", "author": ["A. Beygelzimer", "V. Dani", "T. Hayes", "J. Langford", "B. Zadrozny"], "venue": "In 22 th ICML,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2005}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["M. Schmidt", "N.L. Roux", "F. Bach"], "venue": null, "citeRegEx": "Schmidt et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Schmidt et al\\.", "year": 2013}, {"title": "Probabilistic Methods for Algorithmic Discrete Mathematics, pages 1\u201354", "author": ["C. McDiarmid"], "venue": null, "citeRegEx": "McDiarmid.,? \\Q1998\\E", "shortCiteRegEx": "McDiarmid.", "year": 1998}, {"title": "Noise tolerance under risk minimization", "author": ["N. Manwani", "P.S. Sastry"], "venue": "Cybernetics, IEEE Transactions on,", "citeRegEx": "Manwani and Sastry.,? \\Q2013\\E", "shortCiteRegEx": "Manwani and Sastry.", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "We prove a theorem reminiscent of Fisher-Neyman\u2019s factorization [Lehmann and Casella, 1998] of the exponential family which lays the foundation of this work: it shows how empirical l-risk factors (Figure 1) in a label free term with another incorporating a sufficient statistic of the labels, the mean operator.", "startOffset": 64, "endOffset": 91}, {"referenceID": 1, "context": "The interplay of the two components is still apparent on newly derived generalization bounds, that also improve on known ones [Kakade et al., 2009].", "startOffset": 126, "endOffset": 147}, {"referenceID": 5, "context": "For example, labels may be noisy [Natarajan et al., 2013], missing as with semi-supervision [Chapelle et al.", "startOffset": 33, "endOffset": 57}, {"referenceID": 6, "context": ", 2013], missing as with semi-supervision [Chapelle et al., 2006] and pu [du Plessis et al.", "startOffset": 42, "endOffset": 65}, {"referenceID": 7, "context": ", 2015], or aggregated as it happens in multiple instance learning [Dietterich et al., 1997] and learning from label proportions (llp) [Quadrianto et al.", "startOffset": 67, "endOffset": 92}, {"referenceID": 8, "context": ", 1997] and learning from label proportions (llp) [Quadrianto et al., 2009].", "startOffset": 50, "endOffset": 75}, {"referenceID": 12, "context": "Recent results [Long and Servedio, 2010] have shown that requiring the strongest form of robustness \u2013on any possible noisy sample\u2013 rules out most losses commonly used, and have drifted research focus on non-convex [Stempfel and Ralaivola, 2009, Masnadi-Shirazi et al.", "startOffset": 15, "endOffset": 40}, {"referenceID": 16, "context": "Elements of this work appeared in an early version [Patrini et al., 2015], mostly interested in elucidating the connection between loss factorization and \u03b1-label differential privacy [Chaudhuri and Hsu, 2011].", "startOffset": 51, "endOffset": 73}, {"referenceID": 17, "context": ", 2015], mostly interested in elucidating the connection between loss factorization and \u03b1-label differential privacy [Chaudhuri and Hsu, 2011].", "startOffset": 117, "endOffset": 142}, {"referenceID": 9, "context": "We translate this property for classification with erm, by transferring the ideas of sufficiency and factorization to a wide set of losses including the ones of [Patrini et al., 2014].", "startOffset": 161, "endOffset": 183}, {"referenceID": 8, "context": "The name mean operator, or mean map, is borrowed from the theory of Hilbert space embedding [Quadrianto et al., 2009].", "startOffset": 92, "endOffset": 117}, {"referenceID": 18, "context": "Its importance is due to the injectivity of the map \u2013under conditions on the kernel\u2013 which is used in applications such as twosample and independence tests, feature extraction and covariate shift [Smola et al., 2007].", "startOffset": 196, "endOffset": 216}, {"referenceID": 9, "context": "The definition is motivated by the one in Statistics, taking log-odd ratios [Patrini et al., 2014].", "startOffset": 76, "endOffset": 98}, {"referenceID": 19, "context": "Table 1: Factorization of linear-odd losses: spl (including logistic, square and Matsushita) [Nock and Nielsen, 2009], double \u201c2\u201d-hinge and perceptron [du Plessis et al.", "startOffset": 93, "endOffset": 117}, {"referenceID": 25, "context": "For the sake of presentation, we work on a simple version of sgd based on subgradient descent with L2 regularization, inspired by PEGASO [Shalev-Shwartz et al., 2011].", "startOffset": 137, "endOffset": 166}, {"referenceID": 5, "context": "The noise rates are label dependent 2 by (p+, p\u2212) \u2208 [0, 1/2) respectively for positive and negative examples, that is, asymmetric label noise (aln) [Natarajan et al., 2013].", "startOffset": 148, "endOffset": 172}, {"referenceID": 5, "context": "On one hand, the estimators of [Natarajan et al., 2013] may not be convex even when l is so, but this is never the case with lols; in fact, l(x)\u2212 l(\u2212x) = 2ax may be seen as alternative sufficient condition to [Natarajan et al.", "startOffset": 31, "endOffset": 55}, {"referenceID": 12, "context": "[Long and Servedio, 2010] proves that no convex potential is noise tolerant, that is, 0-ALN robust.", "startOffset": 0, "endOffset": 25}, {"referenceID": 26, "context": "Finally, compare our \u01eb-robustness to the one of [Ghosh et al., 2015]: RD,l(\u03b8\u0303 \u2217) \u2264 (1 \u2212 2max(p\u2212, p+))\u22121RD,l(\u03b8\u2217).", "startOffset": 48, "endOffset": 68}, {"referenceID": 27, "context": "[2013], at least for small |Y| [Sukhbaatar and Fergus, 2014].", "startOffset": 31, "endOffset": 60}, {"referenceID": 5, "context": "In principle they can be tuned as hyper-parameters Natarajan et al. [2013], at least for small |Y| [Sukhbaatar and Fergus, 2014].", "startOffset": 51, "endOffset": 75}, {"referenceID": 25, "context": "The other parameters l, T, \u03bb are the same for both algorithms; the learning rate \u03b7 is untouched from [Shalev-Shwartz et al., 2011] and not tuned for \u03bcsgd.", "startOffset": 101, "endOffset": 130}, {"referenceID": 32, "context": "A kernelized version of this Lemma is given in [Song et al., 2009].", "startOffset": 47, "endOffset": 66}, {"referenceID": 33, "context": "To show that, notice that we satisfy all hypotheses of the Representer Theorem [Sch\u00f6lkopf and Smola, 2002].", "startOffset": 79, "endOffset": 106}, {"referenceID": 9, "context": "Beside that, since we reason at the higher level of wsl, we can frame a solution for pu simply calling \u03bcsgd on \u03bc\u0302 defined above or on estimators improved by exploiting results of [Patrini et al., 2014].", "startOffset": 179, "endOffset": 201}, {"referenceID": 34, "context": "Learning reductions Solving a machine learning problem by solutions to other learning problems is a learning reduction [Beygelzimer et al., 2015].", "startOffset": 119, "endOffset": 145}, {"referenceID": 35, "context": "Following [Beygelzimer et al., 2005], we define a wsl task as a triple (K,Y, l), with weakly supervised advice K, predictions space Y and loss l, and we reduce to binary classification (Y,Y, l).", "startOffset": 10, "endOffset": 36}, {"referenceID": 36, "context": "Beyond \u03bcsgd meta-\u03bcsgd is intimately similar to stochastic average gradient (sag) [Schmidt et al., 2013].", "startOffset": 81, "endOffset": 103}, {"referenceID": 12, "context": "A better (?) picture of robustness The data-dependent worst-case result of [Long and Servedio, 2010], like any extreme-case argument, should be handled with care.", "startOffset": 75, "endOffset": 100}], "year": 2016, "abstractText": "We prove that the empirical risk of most well-known loss functions factors into a linear term aggregating all labels with a term that is label free, and can further be expressed by sums of the loss. This holds true even for non-smooth, non-convex losses and in any rkhs. The first term is a (kernel) mean operator \u2013the focal quantity of this work\u2013 which we characterize as the sufficient statistic for the labels. The result tightens known generalization bounds and sheds new light on their interpretation. Factorization has a direct application on weakly supervised learning. In particular, we demonstrate that algorithms like sgd and proximal methods can be adapted with minimal effort to handle weak supervision, once the mean operator has been estimated. We apply this idea to learning with asymmetric noisy labels, connecting and extending prior work. Furthermore, we show that most losses enjoy a data-dependent (by the mean operator) form of noise robustness, in contrast with known negative results.", "creator": "LaTeX with hyperref package"}}}