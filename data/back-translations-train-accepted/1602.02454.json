{"id": "1602.02454", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2016", "title": "Efficient Algorithms for Adversarial Contextual Learning", "abstract": "We provide the first oracle efficient sublinear regret algorithms for adversarial versions of the contextual bandit problem. In this problem, the learner repeatedly makes an action on the basis of a context and receives reward for the chosen action, with the goal of achieving reward competitive with a large class of policies. We analyze two settings: i) in the transductive setting the learner knows the set of contexts a priori, ii) in the small separator setting, there exists a small set of contexts such that any two policies behave differently in one of the contexts in the set. Our algorithms fall into the follow the perturbed leader family \\cite{Kalai2005} and achieve regret $O(T^{3/4}\\sqrt{K\\log(N)})$ in the transductive setting and $O(T^{2/3} d^{3/4} K\\sqrt{\\log(N)})$ in the separator setting, where $K$ is the number of actions, $N$ is the number of baseline policies, and $d$ is the size of the separator. We actually solve the more general adversarial contextual semi-bandit linear optimization problem, whilst in the full information setting we address the even more general contextual combinatorial optimization. We provide several extensions and implications of our algorithms, such as switching regret and efficient learning with predictable sequences.", "histories": [["v1", "Mon, 8 Feb 2016 03:11:39 GMT  (74kb)", "http://arxiv.org/abs/1602.02454v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["vasilis syrgkanis", "akshay krishnamurthy", "robert e schapire"], "accepted": true, "id": "1602.02454"}, "pdf": {"name": "1602.02454.pdf", "metadata": {"source": "META", "title": "Efficient Algorithms for Adversarial Contextual Learning", "authors": ["Vasilis Syrgkanis", "Akshay Krishnamurthy", "Robert E. Schapire"], "emails": ["VASY@MICROSOFT.COM", "AKSHAYKR@CS.CMU.EDU", "SCHAPIRE@MICROSOFT.COM"], "sections": [{"heading": null, "text": "ar Xiv: 160 2.02 454v 1 [cs.L G] 8F eb \u221a K log (N)) in the transductive setting and O (T 2 / 3d3 / 4K \u221a log (N)) in the delimiter setting, where K is the number of actions, N is the number of baseline policies and d is the size of the delimiter. We actually solve the more general context-dependent context-dependent semi-bandit linear optimization problem, while in the full information environment we turn to the even more general context-dependent combinatorial optimization. We offer several enhancements and implications of our algorithms, such as switching regrets and efficient learning with predictable sequences."}, {"heading": "1. Introduction", "text": "We study contextual online learning, a powerful framework that encompasses a wide range of sequential decision-making problems that are, however, extremely expensive. Here, in each round, learners require contextual information that can be used as a tool in selecting an action. In the full-information version of the problem, the learner then observes the loss that would have suffered for each of the possible actions, while in the much more sophisticated bandit version, only the loss that actually occurred for the chosen action is observed. The contextual bandit problem is of particular practical relevance, with applications to personalized recommendations, clinical trials and targeted advertising. Contextual learning algorithms, such as Hedge (Freund & Schapire, 1997) and Exp4 (Auer et al., 1995), are known to have remarkable theoretical properties that are effective even in adversarial, non-stochastic environments and able to function almost as well as a large family."}, {"heading": "2. Online Learning with Oracles", "text": "We begin by analyzing the family of Perturbed Leader algorithms in a very general online learning environment. (Part of this generic formulation follows the current formulation of Daskalakis & Syrgkanis (2015), but we present a more refined analysis that is essential to our contextual learning outcome in the next sections. (Main symbol of this section is essentially a generalization of Theorem 1.1 by Kalai & Vempala (2005).Consider an online learning problem in which at each time step an opponent selects a result yt Y and the algorithm selects a policy that emanates from some political space. (The algorithms receive a loss: \"E, Y, Y) which could refer to the choice of the learner as a policy, for the uniformity of notation with subsequent sections, where the learner chooses a policy that selects cards to action.positive or negative."}, {"heading": "3. Adversarial Contextual Learning", "text": "The first specialization of the general field is focused on contextual online optimization. (...) In this learning environment we are able to analyze the learning algorithms (...) for each learning situation (...). (...) In this learning environment we are able to analyze the learning algorithms (...). (...) In this context we are equal to each coordinate the read-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector"}, {"heading": "4. Linear Losses and Semi-Bandit Feedback", "text": "In this section, we look at contextual learning with a policy of mutual learning and mutual learning. (...) In each stage of this learning process, the learner selects an action that he or she chooses. (...) The learners choose an action that he or she chooses. (...) The learners choose an action that he or she chooses. (...) The learners choose an action by selecting the one-dimensional vectors, then this attitude is equivalent to the well-studied contextual bandit problem (...).Semi-bandit algorithms. (...) Our semi-bandit algorithms proceed as follows: In each iteration, he or she makes a call to CONTEXT-FTPL (...) that returns a policy and sets a selected action. (...) Our semi-bandit algorithms act as follows: In each iteration, he or she makes a call to CONTEXT-FTPL (...)."}, {"heading": "5. Switching Policy Regret", "text": "In this section, we analyze regret in terms of contextual linear optimization, i.e. regret that the best sequence of strategies that change in most k-times cannot be taken into account. (This is the best sequence of strategies that are not computationally efficient for large political spaces, since they have been designed since then (e.g. (Luo & Schapire, 2015))) Our results provide the first computationally efficient switching of regret algorithms that require offline oracle access. For this setting, we assume that the learner knows the exact sequence x1: T of contexts before time and not just the sequence of potential contexts. The extension stems from the realization that we can simply think of time t as part of the context in time-step-t. Thus, the contexts are now of the form x-t = (t, xt)."}, {"heading": "6. Efficient Path Length Regret Bounds", "text": "In this section, we examine a variant of our CONTEXT FTPL algorithm that is efficient and achieves regret limited by the structural properties of the utility sequence; our algorithm is defined in terms of a generic predictor that the learner has access to; and regret is limited by the deviation of the true loss vector from the predictor. (Our approach is generic enough to allow generalizations of variance and path length that import contextual information and can be viewed as an efficient version and generalization of the results of Rakhlin & Sridharan. (2013b) These results have also found applications in learning in game theory environments (Rakhlin & Sridharan, 2013a)."}, {"heading": "7. Discussion", "text": "Our most important algorithmic contribution is a new Follow-The-Perturbed-Leader algorithm that adds disturbed low-dimensional statistics. We polish this algorithm and guarantee sublinear regret for all of these problems. All of our results stand up to adaptive adversaries, with both full and partial feedback. While our algorithms achieve sublinear regret for all of the problems we are looking at, we do not always meet the limits of regret that can be achieved through inefficient alternatives. An interesting direction for future work is whether fully oracle-based algorithms can achieve optimal limits of regret in the environments we are looking at. Another interesting direction focuses on a deeper understanding of the condition of the small separator and whether it enables efficient non-transductive learning in other environments. We look forward to exploring these questions in future work."}, {"heading": "A. Omitted Proofs from Section 2", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1. Proof of Theorem 1", "text": "We prove the theory by analyzing a slightly modified algorithm that is only once at the beginning of the learning process, but otherwise identical. Most of the evidence is devoted to limiting this modified algorithm (see Lemma 12) in order to obtain a regret for algorithm 1 against adaptive opponents. We provide proof of this reduction in Appendix A.2 and proceed with the analysis of the modified algorithm. To tie the regret of the modified algorithm, we consider respecting the algorithm ahead of time, so that in each step t plays the algorithm."}, {"heading": "B. Omitted Proofs from Section 3", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1. Bounding the Laplacian Error", "text": "The upper limit of the ERROR term is identical in all settings, since it only depends on the input noise distribution, which is the same for all variants and for which it does not matter whether X is a set of contexts used either as separators or as separators. In the following sections we will limit the stability of the algorithm upwards in each setting. < < < < p (max) {p (max.) {p (max.) {p) {p). Then: ERROR = E {z} [max."}, {"heading": "B.2. Bounding Stability: Transductive Setting", "text": "We now turn to limiting stability in the transductive combinatorial optimization setting. < < p (0) = p (0) = p (0) = p (0) = p (1) = p (0) = p (0) = p (2) p (1) p (2) p (0) p (0) p (0) p (0) p (2) p (0) p (0) p (0) p (2) p (1) p (0) p (2) p) p (0) p) p (2) p) p (2) p) p (2) p) p (1) p) p (1) p) p) p (2) p (2) p (2) p (2) p) p (p) p \"p\" p (f) p \"p (2) p (p) p\" p (p) p \"p (2) p (p) p\" p (2) p (p) p \"p (2) p (p) p\" p (2) p (p) p \"p (2) p (p) p\" p (2) p (p) p \"p\" p (2) p (p) p \"p (p) p) p\" p (2) p (p) p (p) p \"p\" p (2) p (p) p (p) p \"p\" p (2) p (p) p (p) p \"p\" p (2) p (p) p (p (p) p) p \"p\" p (2) p (p) p (p) p (2) p (p) p) p (2) p (p (p) p) p \"p\" p (2) p (p (p) p (p) p (2) p (p) p \"p (p (2) p (p\" p (p) p \"p (p) p (2) p (p (p) p) p\" p (p) p \"p (2) p (p (p) p\" p (p) p \"p\" p (2) p (p (p) p \"p (p) p (p) p () p\" p \"p\" p ("}, {"heading": "B.3. Bounding Stability: Transductive Setting with Linear Losses", "text": "In the transductive setting with linear losses, we provide a clearly refined stability that enables applications to partial information or Bandit settings. As before, the combination of this stability with the error bound in Lemma 8 and the application of Theorem 1 is the second claim of Theorem 2. Lemma 10 (multiplicative stability). For each sequence y1: T for all contexts and non-negative linear loss functions, results in the stability of CONTEXT-FTPL (X) in the transductive setting, is the upper limit of: E {z} [< empirical; empirical; empirical; empirical; empirical; empirical; empirical; empirical; empirical; empirical; empirical; empirical; empirical; empirical; empirical; empirical; empirical; empirical; empirical; empirically; empirical; empirical; empirical; empirically; empirically; empirically; empirically; empirically; empirically; empirically; empirically; empirically; empirically; empirically; empirically; empirically; empirically; empirically; empirically; empirically; empirically; empirically; empirically; empirically; empirically; empirically; empirically; empirically; empirically; empirically; empirically; empirically; empirically; empirically; empirically; empirically; empirically; empirically; empirically; empirically;"}, {"heading": "B.4. Bounding Stability: Small Separator Setting", "text": "Finally, we prove the third assertion in Theorem 2. This implies a new stability that is limited for the small delimiter."}, {"heading": "C. Omitted Proofs from Section 4", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C.1. Proof of Theorem 3: Transductive Setting", "text": "Consider the expected loss of the Bandit algorithm in the history of a game by the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition"}, {"heading": "C.2. Proof of Theorem 3: Small Separator Setting", "text": "Consider the expected loss of the bandit algorithm in the time step t = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 2 = 1 = 2 = 2 = 2 = 1 = 1 = 2 = 2 = 2 = 2 = 2 = 2 = 1 \u2212 2 (1 \u2212 qt (j) L (29) As observed by (Neu & Barto \"k,\" 2013), the second quantity can be limited upwards by KeL \"t, da q (1 \u2212 q) L\" qe \u2212 Lq. \"Now we observe that: j\" K \"t\" t \"(j) \u00b7 E\" (j \"k\" k \"k\" k \"k\" k \"k,\" 2013) the second quantity can be limited upwards by KeL \"t\" t, da q \"t\" t \"t\" t. \""}, {"heading": "D. Omitted Proofs from Section 6", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "D.1. Proof of Theorem 6", "text": "Similar to the analysis in Section 3, the proof of the theorem is split into two main Qs (> Q = two main Qs); the first is an analogy to Theorem 1 for algorithms that use a predictor. \u2212 \u2212 \u2212 This problem can be formulated in the general online learning environment described in Section 2. The second Lemma is an anaolgue of our multiplicative stability Lemma 10.Let \u03c1t = M ({z). \u2212 Qbt (30) denotes the policy that would have been played in step t if the predictor had been equal to the actual loss vector that occurred in step t. Furthermore, for the success of f-Qs, Qxt does not mark us with at (xt) and bt = Qxt (xt).Lemma 12 (Follow vs Be the Leader with Predictors). A player's regret under the optimistic QPL and in relation to any other QT-Qs is not characterized by Qxt."}, {"heading": "In the small separator setting:", "text": "The second episode follows the same arguments. According to the definition of \"f\" - \"Qt\" = \"p\" - \"p\" - \"p\" - \"p\" - \"p\" - \"p\" - \"p\" - \"p\" - \"p\" - \"p\" - \"p\" - \"p\" - \"p\" - \"p\" - \"p\" - \"p\" - \"p\" - \"p\" - \"p\" - \"p\" - \"p\" - \"p\" - \"p\" - \"p\" p \"-\" p \"p\" - \"p\" p \"p\" - \"p\" p \"p\" - \"p\" p \"p\" - \"p\" p \"p\" - \"p\" p \"p\" - \"p\" p \"p\" p \"-\" p \"p\" p \"p\" - \"p\" p \"p\" p \"-\" p \"p\" p \"p\" p \"-\" p \"p\" p \"p\" - \"p\" p \"p\" p \"-\" p \"p\" p \"-\" p \"p\" p \"-\" p \"p\" p \"-\" p \"p\" p \"-\" p \"p\" p \"-\" p \"p\" - \"p\" p \"p\" - \"p\" p \"-\" p \"p\" p \"-\" p \"p\" - \"p\" p \"-\" p \"p\" p \"-\" p \"p\" - \"p\" p \"-\" p \"p\" - \"p\" p \"p\" - \"p\" p \"-\" p \"p\" - \"p\" p \"p\" - \"p\" p \"-\" p \"p\" p \"-\" p \"-\" p \"p\" - \"p\" p \"p\" p \"p\" p \"-\" p \"p\" - \"p\" p \"p\" - \"p\" p \"p\" - \"p\" p \"p\" p \"-\" p \"p\" p \"p\" - \"p\" p \"p\" p \"-\" p \"-\" p \"p\" p \"p\" - \"p\" p \"p\" - \"p\" p \"p\" p \"p\" p \"-\" p \"p\" - \"p\" p \"p\" - \"p\" p \"p\" p"}], "references": [{"title": "Taming the monster: A fast and simple algorithm for contextual bandits", "author": ["Agarwal", "Alekh", "Hsu", "Daniel", "Kale", "Satyen", "Langford", "John", "Li", "Lihong", "Schapire", "Robert E"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Agarwal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2014}, {"title": "Gambling in a rigged casino: The adversarial multi-armed bandit pproblem", "author": ["Auer", "Peter", "Cesa-Bianchi", "Nicolo", "Freund", "Yoav", "Schapire", "Robert E"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "Auer et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Auer et al\\.", "year": 1995}, {"title": "Online linear optimization and adaptive routing", "author": ["Awerbuch", "Baruch", "Kleinberg", "Robert"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Awerbuch et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Awerbuch et al\\.", "year": 2008}, {"title": "Characterizations of learnability for classes of (0,..., n)-valued functions", "author": ["Ben-David", "Shai", "Cesa-Bianchi", "Nicolo", "Haussler", "David", "Long", "Philip M"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Ben.David et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 1995}, {"title": "Online learning versus offline learning", "author": ["Ben-David", "Shai", "Kushilevitz", "Eyal", "Mansour", "Yishay"], "venue": "Machine Learning,", "citeRegEx": "Ben.David et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 1997}, {"title": "Efficient online learning via randomized rounding", "author": ["Cesa-Bianchi", "Nicolo", "Shamir", "Ohad"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2011}, {"title": "How to use expert advice", "author": ["Cesa-Bianchi", "Nicolo", "Freund", "Yoav", "Haussler", "David", "Helmbold", "David P", "Schapire", "Robert E", "Warmuth", "Manfred K"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 1997}, {"title": "Learning in auctions: Regret is hard, envy is easy", "author": ["Daskalakis", "Constantinos", "Syrgkanis", "Vasilis"], "venue": null, "citeRegEx": "Daskalakis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Daskalakis et al\\.", "year": 2015}, {"title": "Efficient optimal learning for contextual bandits", "author": ["Dud\u0131\u0301k", "Miroslav", "Hsu", "Daniel", "Kale", "Satyen", "Karampatziakis", "Nikos", "Langford", "John", "Reyzin", "Lev", "Zhang", "Tong"], "venue": "In Uncertainty and Artificial Intelligence (UAI),", "citeRegEx": "Dud\u0131\u0301k et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dud\u0131\u0301k et al\\.", "year": 2011}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Freund", "Yoav", "Schapire", "Robert E"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Freund et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Freund et al\\.", "year": 1997}, {"title": "A generalization of sauer\u2019s lemma", "author": ["Haussler", "David", "Long", "Philip M"], "venue": "Journal of Combinatorial Theory,", "citeRegEx": "Haussler et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Haussler et al\\.", "year": 1995}, {"title": "Extracting certainty from uncertainty: regret bounded by variation in costs", "author": ["Hazan", "Elad", "Kale", "Satyen"], "venue": "Machine Learning,", "citeRegEx": "Hazan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2010}, {"title": "Online submodular minimization", "author": ["Hazan", "Elad", "Kale", "Satyen"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Hazan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2012}, {"title": "Tracking the best expert", "author": ["Herbster", "Mark", "Warmuth", "Manfred K"], "venue": "Machine Learning,", "citeRegEx": "Herbster et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Herbster et al\\.", "year": 1998}, {"title": "A generalization of sampling without replacement from a finite universe", "author": ["Horvitz", "Daniel G", "Thompson", "Donovan J"], "venue": "Journal of the American Statistical Association (JASA),", "citeRegEx": "Horvitz et al\\.,? \\Q1952\\E", "shortCiteRegEx": "Horvitz et al\\.", "year": 1952}, {"title": "Adaptive online prediction by following the perturbed leader", "author": ["Hutter", "Marcus", "Poland", "Jan"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Hutter et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Hutter et al\\.", "year": 2005}, {"title": "Online submodular minimization for combinatorial structures", "author": ["Jegelka", "Stefanie", "Bilmes", "Jeff A"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Jegelka et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jegelka et al\\.", "year": 2011}, {"title": "From batch to transductive online learning", "author": ["Kakade", "Sham M", "Kalai", "Adam"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Kakade et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2005}, {"title": "Efficient algorithms for online decision problems", "author": ["Kalai", "Adam", "Vempala", "Santosh"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Kalai et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Kalai et al\\.", "year": 2005}, {"title": "The epoch-greedy algorithm for multi-armed bandits with side information", "author": ["Langford", "John", "Zhang", "Tong"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Langford et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2008}, {"title": "Achieving all with no parameters: Adanormalhedge", "author": ["Luo", "Haipeng", "Schapire", "Robert E"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "Luo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2015}, {"title": "An efficient algorithm for learning with semi-bandit feedback", "author": ["Neu", "Gergely", "Bart\u00f3k", "G\u00e1bor"], "venue": "In Algorithmic Learning Theory (ALT),", "citeRegEx": "Neu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Neu et al\\.", "year": 2013}, {"title": "Optimization, learning, and games with predictable sequences", "author": ["Rakhlin", "Alexander", "Sridharan", "Karthik"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Rakhlin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2013}, {"title": "Online learning with predictable sequences", "author": ["Rakhlin", "Alexander", "Sridharan", "Karthik"], "venue": "In Conference on Learning Theorem (COLT),", "citeRegEx": "Rakhlin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2013}, {"title": "Fast convergence of regularized learning in games", "author": ["Syrgkanis", "Vasilis", "Agarwal", "Alekh", "Luo", "Haipeng", "Schapire", "Robert E"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Syrgkanis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Syrgkanis et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "Algorithms for contextual learning, such as Hedge (Freund & Schapire, 1997; Cesa-Bianchi et al., 1997) and Exp4 (Auer et al.", "startOffset": 50, "endOffset": 102}, {"referenceID": 1, "context": ", 1997) and Exp4 (Auer et al., 1995), are well-known to have remarkable theoretical properties, being effective even in adversarial, non-stochastic environments, and capable of performing almost as well as the best among an exponentially large family of policies, or rules for choosing actions at each step.", "startOffset": 17, "endOffset": 36}, {"referenceID": 1, "context": ", 1997) and Exp4 (Auer et al., 1995), are well-known to have remarkable theoretical properties, being effective even in adversarial, non-stochastic environments, and capable of performing almost as well as the best among an exponentially large family of policies, or rules for choosing actions at each step. However, the space requirements and running time of these algorithms are generally linear in the number of policies, which is far too expensive for a great many applications which call for an extremely large policy space. In this paper, we address this gap between the statistical promise and computational challenge of algorithms for contextual online learning in an adversarial setting. As an approach to solving online learning problems, we posit that the corresponding batch version is solvable. In other words, we assume access to a certain optimization oracle for solving an associated batch-learning problem. Concrete instances of such an oracle include empirical risk minimization procedures for supervised learning, algorithms for the shortest paths problem, and dynamic programming. Such an oracle is central to the Follow-the-PerturbedLeader algorithms of Kalai & Vempala (2005), although these algorithms are not generally efficient since they require separately \u201cperturbing\u201d each policy in the entire", "startOffset": 18, "endOffset": 1198}, {"referenceID": 0, "context": "Oracles of this kind have also been used in designing efficient contextual bandit algorithms (Agarwal et al., 2014; Langford & Zhang, 2008; Dud\u0131\u0301k et al., 2011); however, these require a much more benign setting in which contexts and losses are chosen randomly and independently rather than by an adversary.", "startOffset": 93, "endOffset": 160}, {"referenceID": 8, "context": "Oracles of this kind have also been used in designing efficient contextual bandit algorithms (Agarwal et al., 2014; Langford & Zhang, 2008; Dud\u0131\u0301k et al., 2011); however, these require a much more benign setting in which contexts and losses are chosen randomly and independently rather than by an adversary.", "startOffset": 93, "endOffset": 160}, {"referenceID": 4, "context": "The first is a transductive setting (Ben-David et al., 1997) in which the learner knows the set of arriving contexts a priori, or, less stringently, knows only the set, but not necessarily the actual sequence or multiplicity with which each context arrives.", "startOffset": 36, "endOffset": 60}, {"referenceID": 0, "context": "Oracles of this kind have also been used in designing efficient contextual bandit algorithms (Agarwal et al., 2014; Langford & Zhang, 2008; Dud\u0131\u0301k et al., 2011); however, these require a much more benign setting in which contexts and losses are chosen randomly and independently rather than by an adversary. In this paper, for a wide range of problems, we present computationally efficient algorithms for contextual online learning in an adversarial setting, assuming oracle access. We give results for both the full-information and bandit settings. To the best of our knowledge, these results are the first of their kind at this level of generality. Overview of results. We begin by proposing and analyzing in Section 2 a new and general Follow-the-PerturbedLeader algorithm in the style of Kalai & Vempala (2005). This algorithm only accesses the policy class using the optimization oracle.", "startOffset": 94, "endOffset": 815}, {"referenceID": 0, "context": "Oracles of this kind have also been used in designing efficient contextual bandit algorithms (Agarwal et al., 2014; Langford & Zhang, 2008; Dud\u0131\u0301k et al., 2011); however, these require a much more benign setting in which contexts and losses are chosen randomly and independently rather than by an adversary. In this paper, for a wide range of problems, we present computationally efficient algorithms for contextual online learning in an adversarial setting, assuming oracle access. We give results for both the full-information and bandit settings. To the best of our knowledge, these results are the first of their kind at this level of generality. Overview of results. We begin by proposing and analyzing in Section 2 a new and general Follow-the-PerturbedLeader algorithm in the style of Kalai & Vempala (2005). This algorithm only accesses the policy class using the optimization oracle. We then apply these results in Section 3 to two settings. The first is a transductive setting (Ben-David et al., 1997) in which the learner knows the set of arriving contexts a priori, or, less stringently, knows only the set, but not necessarily the actual sequence or multiplicity with which each context arrives. In the second, small-separator setting, we assume that the policy space admits the existence of a small set of contexts, called a separator, such that any two policies differ on at least one context from the set. The size of the smallest separator for a particular policy class can be viewed as a new measure of complexity, different from the VC dimension, and potentially of independent interest. We study these for a generalized online learning problem called online combinatorial optimization, which includes as special cases transductive contextual experts, online shortest-path routing, online linear optimization (Kalai & Vempala, 2005), and online submodular minimization (Hazan & Kale, 2012). In Section 4, we extend our results to the bandit setting, or in fact, to the more general semi-bandit setting, using a technique of Neu & Bart\u00f3k (2013). Among our main results, we obtain regret bounds for the adversarial contextual bandit problem of O(T 3/4 \u221a", "startOffset": 94, "endOffset": 2063}, {"referenceID": 24, "context": "Such bounds have various applications, for instance, in obtaining better bounds for playing repeated games (Rakhlin & Sridharan, 2013a; Syrgkanis et al., 2015).", "startOffset": 107, "endOffset": 159}, {"referenceID": 22, "context": "Such bounds have various applications, for instance, in obtaining better bounds for playing repeated games (Rakhlin & Sridharan, 2013a; Syrgkanis et al., 2015). Other related work. Contextual, transductive online learning using an optimization oracle was previously studied by Kakade & Kalai (2005), whose work was later extended and improved by Cesa-Bianchi & Shamir (2011) using a generalization of a technique from Cesa-Bianchi et al.", "startOffset": 136, "endOffset": 299}, {"referenceID": 22, "context": "Such bounds have various applications, for instance, in obtaining better bounds for playing repeated games (Rakhlin & Sridharan, 2013a; Syrgkanis et al., 2015). Other related work. Contextual, transductive online learning using an optimization oracle was previously studied by Kakade & Kalai (2005), whose work was later extended and improved by Cesa-Bianchi & Shamir (2011) using a generalization of a technique from Cesa-Bianchi et al.", "startOffset": 136, "endOffset": 375}, {"referenceID": 5, "context": "Contextual, transductive online learning using an optimization oracle was previously studied by Kakade & Kalai (2005), whose work was later extended and improved by Cesa-Bianchi & Shamir (2011) using a generalization of a technique from Cesa-Bianchi et al. (1997). However, these previous results are for binary classification or other convex losses defined on one-dimensional predictions and outcomes; as such, they are special cases of the much more general setting we consider in the present paper.", "startOffset": 237, "endOffset": 264}, {"referenceID": 5, "context": "Contextual, transductive online learning using an optimization oracle was previously studied by Kakade & Kalai (2005), whose work was later extended and improved by Cesa-Bianchi & Shamir (2011) using a generalization of a technique from Cesa-Bianchi et al. (1997). However, these previous results are for binary classification or other convex losses defined on one-dimensional predictions and outcomes; as such, they are special cases of the much more general setting we consider in the present paper. Awerbuch & Kleinberg (2008) present an efficient algorithm for the online shortest paths problem.", "startOffset": 237, "endOffset": 530}, {"referenceID": 5, "context": "Contextual, transductive online learning using an optimization oracle was previously studied by Kakade & Kalai (2005), whose work was later extended and improved by Cesa-Bianchi & Shamir (2011) using a generalization of a technique from Cesa-Bianchi et al. (1997). However, these previous results are for binary classification or other convex losses defined on one-dimensional predictions and outcomes; as such, they are special cases of the much more general setting we consider in the present paper. Awerbuch & Kleinberg (2008) present an efficient algorithm for the online shortest paths problem. This can be viewed as solving an adversarial bandit problem with a very particular optimization oracle over an exponentially large but highly structured space of \u201cpolicies\u201d corresponding to paths in a graph. However, their setting is clearly far more restrictive and structured than ours is. 2. Online Learning with Oracles We start by analyzing the family of Follow the Perturbed Leader algorithms in a very general online learning setting. Parts of this generic formulation follow the recent formulation of Daskalakis & Syrgkanis (2015), but we present a more refined analysis which is essential for our contextual learning result in the next sections.", "startOffset": 237, "endOffset": 1139}, {"referenceID": 5, "context": "Contextual, transductive online learning using an optimization oracle was previously studied by Kakade & Kalai (2005), whose work was later extended and improved by Cesa-Bianchi & Shamir (2011) using a generalization of a technique from Cesa-Bianchi et al. (1997). However, these previous results are for binary classification or other convex losses defined on one-dimensional predictions and outcomes; as such, they are special cases of the much more general setting we consider in the present paper. Awerbuch & Kleinberg (2008) present an efficient algorithm for the online shortest paths problem. This can be viewed as solving an adversarial bandit problem with a very particular optimization oracle over an exponentially large but highly structured space of \u201cpolicies\u201d corresponding to paths in a graph. However, their setting is clearly far more restrictive and structured than ours is. 2. Online Learning with Oracles We start by analyzing the family of Follow the Perturbed Leader algorithms in a very general online learning setting. Parts of this generic formulation follow the recent formulation of Daskalakis & Syrgkanis (2015), but we present a more refined analysis which is essential for our contextual learning result in the next sections. The main theorem of this section is essentially a generalization of Theorem 1.1 of Kalai & Vempala (2005). Consider an online learning problem where at each timestep an adversary picks an outcome y \u2208 Y and the algorithm picks a policy \u03c0 \u2208 \u03a0 from some policy space \u03a0.", "startOffset": 237, "endOffset": 1361}, {"referenceID": 24, "context": "Such results have also found applications in learning in game theoretic environments (Rakhlin & Sridharan, 2013a; Syrgkanis et al., 2015).", "startOffset": 85, "endOffset": 137}, {"referenceID": 3, "context": "The main property we will use about function classes with bounded Natarajan Dimension is the following analog of the Sauer-Shelah Lemma: Lemma 14 (Sauer-Shelah for Natarajan Dimension (Haussler & Long, 1995; Ben-David et al., 1995)).", "startOffset": 184, "endOffset": 231}], "year": 2016, "abstractText": "We provide the first oracle efficient sublinear regret algorithms for adversarial versions of the contextual bandit problem. In this problem, the learner repeatedly makes an action on the basis of a context and receives reward for the chosen action, with the goal of achieving reward competitive with a large class of policies. We analyze two settings: i) in the transductive setting the learner knows the set of contexts a priori, ii) in the small separator setting, there exists a small set of contexts such that any two policies behave differently in one of the contexts in the set. Our algorithms fall into the follow the perturbed leader family (Kalai & Vempala, 2005) and achieve regret O(T 3/4 \u221a K log(N)) in the transductive setting and O(T dK \u221a log(N)) in the separator setting, where K is the number of actions, N is the number of baseline policies, and d is the size of the separator. We actually solve the more general adversarial contextual semi-bandit linear optimization problem, whilst in the full information setting we address the even more general contextual combinatorial optimization. We provide several extensions and implications of our algorithms, such as switching regret and efficient learning with predictable sequences.", "creator": "LaTeX with hyperref package"}}}