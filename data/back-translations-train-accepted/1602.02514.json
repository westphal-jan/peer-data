{"id": "1602.02514", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2016", "title": "Fast k-means with accurate bounds", "abstract": "We propose a novel accelerated exact $k$-means algorithm, which performs better than the current state-of-the-art low-dimensional algorithm in 18 of 22 experiments, running up to 3$\\times$ faster. We also propose a general improvement of existing state-of-the-art accelerated exact $k$-means algorithms through better estimates of the distance bounds used to reduce the number of distance calculations, and get a speedup in 36 of 44 experiments, up to 1.8$\\times$ faster.", "histories": [["v1", "Mon, 8 Feb 2016 10:19:09 GMT  (31kb)", "https://arxiv.org/abs/1602.02514v1", "8 pages + supplementary material"], ["v2", "Wed, 10 Feb 2016 18:51:11 GMT  (31kb,D)", "http://arxiv.org/abs/1602.02514v2", "8 pages + supplementary material"], ["v3", "Thu, 25 Feb 2016 15:18:03 GMT  (32kb,D)", "http://arxiv.org/abs/1602.02514v3", "8 pages + supplementary material"], ["v4", "Mon, 4 Apr 2016 09:12:21 GMT  (32kb,D)", "http://arxiv.org/abs/1602.02514v4", "8 pages + supplementary material v2: mlpack installed with optimisation (previously installed in DEBUG) v3: Annulus -&gt; Annular"], ["v5", "Thu, 28 Apr 2016 12:11:49 GMT  (32kb,D)", "http://arxiv.org/abs/1602.02514v5", "8 pages + supplementary material v2: mlpack installed with optimisation (previously installed in DEBUG) v3: Annulus -&gt; Annular v4: Author affiliation update"], ["v6", "Sun, 11 Sep 2016 14:57:29 GMT  (37kb,D)", "http://arxiv.org/abs/1602.02514v6", "8 pages + supplementary material v2: mlpack installed with optimisation (previously installed in DEBUG) v3: Annulus -&gt; Annular v4: Author affiliation update v5: Synced with version at ICML, now including Suppl. Mat"]], "COMMENTS": "8 pages + supplementary material", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["james newling", "fran\u00e7ois fleuret"], "accepted": true, "id": "1602.02514"}, "pdf": {"name": "1602.02514.pdf", "metadata": {"source": "META", "title": "Fast K-Means with Accurate Bounds", "authors": ["James Newling", "Fran\u00e7ois Fleuret"], "emails": ["JAMES.NEWLING@IDIAP.CH", "FRANCOIS.FLEURET@IDIAP.CH"], "sections": [{"heading": "1. Introduction", "text": "The answer to this question is: \"I believe that most people in the world who live in the world live in the world.\" The answer to this question is: \"I don't think they're in the world.\" The answer to this question is: \"I don't think they're in the world.\" The answer to this question is: \"I don't think they're in the world.\" The answer to this question is: \"I don't think they're in the world.\" The answer to this question is: \"I don't think they're in the world.\" The answer to this question is: \"I don't think they're in the world.\" The answer to this question is: \"I think they're in the world.\" The answer to this question is: \"I think they're in the world.\" The answer to the question about the world is \"I think they're in the world.\" The answer to the question about the world. \"The answer to the question about the world.\" The answer to this question is to the question about the world. \"The answer to this question is to the question about the world.\""}, {"heading": "1.3. Our Contribution", "text": "Our first contribution (\u00a7 3.1) is a new boundary-based accelerated precise k-mean algorithm, the exponion algorithm. Its closest relative is the annular algorithm (Drake, 2013), which is the current state of the art, accelerated exact k-mean algorithms in low dimensions. We show that the exponion algorithm is significantly faster than the annular algorithm on a majority of low-dimensional datasets. Our second contribution (\u00a7 3.2) is a technique to narrow boundaries and eliminate further redundant distance calculations. The technique illustrated in Figure 1 can be applied to all existing boundary-based means algorithms. Finally, we show how certain of the current state-of-the-art algorithms can be accelerated by strict simplifications (\u00a7 2.2 and \u00a7 2.6). Fully parallel implementations of all algorithms are provided under an open source license at https: / thubgip.com / / means"}, {"heading": "2. Notation and baselines", "text": "We describe four accelerated exact k-mean algorithms in the order of the release date (i). For two of these, we propose simplified versions that allow understanding of the complete versions, as well as a faster one (\u00a7 4.1.2). Our notation is based on that of Hamerly (2010), and new notation is introduced only where necessary. We use one (i) for the number of samples and the number of clusters. We always refer to data and cluster indices, each designated with x (i) and the index of the cluster to which it is assigned. A cluster centroid is referred to as c (j). We introduce new notation by n1 (i) denoting the indices of the clusters whose centroids are closest and second highest."}, {"heading": "3. Contributions and New Algorithms", "text": "We are first (\u00a7 3.1) an algorithm that we call \"exponion,\" and then (\u00a7 3.2) an improved approach (2). (2) It is an extension of the system that adds a test to the cost of 6 (i), n2 (i) if the test (8) fails. (3) Unlike the others where the filter is an origin-centered system, exp as a filter has a ball that is larger than w, whence the expected improvement. Define, R (i) = 2u (i) + s (a), J (i) = j (a)."}, {"heading": "3.2. Improving bounds (sn to ns)", "text": "In all the algorithms presented so far, the upper limits (lower limits) are updated in each round (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower limits) (lower (lower limits) (lower limits (lower (lower) (lower) (lower (lower) (lower) (lower) (lower) (lower (lower) (lower) (lower) (lower (lower) (lower) (lower) (lower (lower) (lower) (lower) (lower) (lower) (lower) (lower) (lower) (lower (lower) (lower) (lower) (lower (lower) (lower) (lower) (lower) (lower (lower) (lower) (lower) (lower) (lower) (lower (lower) (lower) (lower) (lower) (lower) (lower) (lower (lower) (lower) (lower) (lower) (lower) (lower) ("}, {"heading": "3.4. Changing Bounds for Other Algorithms", "text": "All sn- to ns- coversions are similar to Section 3.3. We have implemented versions of elk, syin, and exp with ns-bounds, which we call elk-ns, syin-ns, and exp-ns, respectively."}, {"heading": "4. Experiments and Results", "text": "First of all, we find that our implementations of baseline algorithms are as fast or faster than existing implementations. After we have done this, we consider the effects of the presented novel algorithmic contributions, simplification, the exponion algorithm and the limitation. The final set of experiments is conducted on multiple cores and illustrates how all the algorithms presented point to parallels. We compare 23k medium implementations, including our own implementations of all described algorithms, original implementations that accompany the papers (Hamerly, 2010; Drake, 2013; Ding et al., 2015) and implementations in two popular machine learning libraries, VLFeat and mlpack. We use the following notation to refer to implementations: {codesource algorithm}, where codesource is one of the Bay (Hamerly, 2015, mlFeat and mlpack)."}, {"heading": "4.1. Single core experiments", "text": "A complete representation of wall times and number of iterations for all {datasets, implementations, k} triplets is presented on two pages in Tables 9 and 10 (\u00a7 SM-D). At this point we try to summarize our results. We first compare implementations of published algorithms (\u00a7 4.1.1) and then show how Selk and Syin often outperform their more complex counterparts (\u00a7 4.1.2). We show that exp is generally much faster than ann (\u00a7 4.1.3), and finally we show how the use of ns-limits can speed up algorithms (\u00a7 4.1.4)."}, {"heading": "4.1.1. COMPARING IMPLEMENTATIONS OF BASELINES", "text": "There are algorithmic techniques that can accelerate all of the Kmean algorithms discussed in this work. Another, first proposed in Hamerly (2010), is to update the sum of samples by taking into account only those samples whose mapping changed in the previous round. A third optimization technique is to disassemble during loops that contain internal branchings that depend on the density of the upper boundaries into separate while loops, thus avoiding unnecessary comparisons. Finally, while there are no large matrix operations with boundary-based algorithms, high-dimensional distance calculations can be accelerated by using SSE, as in VLFeat, or by fast implementations of BLAS, such as OpenBLAS (Xianyi, 2016). Our careful attention to optimization is reflected in Table 7 (Par. A), where implementations of four lines (hams and associations) are compared."}, {"heading": "4.1.2. BENEFITS OF SIMPLIFICATION", "text": "We compare the published algorithms elk and yin with their simplified counterparts selk and syin. The values in Table 2 are ratios of mean runtimes using simplified and original algorithms, values below 1 mean that the simplified version is faster. We observe that selk is faster than elk and syin in 16 of 18 experiments than elk and syin in 43 of 44 experiments, often dramatically so. It is interesting to ask why the inventors of elk and yin did not opt instead for the selk and syin algorithms. A partial answer could refer to the use of BLAS, since the speed achieved by simplifying yin to syin never exceeds 10% when deactivating BLAS. syin responds to BLAS better than yin, as it has larger matrix multiplications as it does not have a definitive filter."}, {"heading": "4.1.3. FROM ANNULAR TO EXPONION", "text": "We compare the annular algorithm (ann) with the exponion algorithm (exp).The values in Table 3 are ratios of mean runtime (columns qt) and mean number of distance calculations (columns qau).Values below 1 mean better performance with exp. We observe that exp is significantly faster than ann for most low-dimensional datasets and decreases mean runtime by more than 30% in 17 of 22 experiments.The main reason for the acceleration is the reduced number of distance calculations. Table 4 summarizes how often each of the snalgorithms on the 44 {datasets, k} experiments is fastest without ns algorithms.The 13 experiments where exp is fastest are all very low dimensional (d < 5), the 24 experiments where Syin is fastest are average (8 < d < d < 69) and selk or elk are fastest in very high dimensions (d)."}, {"heading": "4.1.4. FROM SN TO NS BOUNDING", "text": "For each of the 44 {dataset, k} experiments, we compare the fastest sn algorithm with its ns variant. The results are given in Table 5. Columns \"x\" denote the fastest Snalgorithm. Values are averages over runs of a certain quantity using the ns and sn- variants. The ratios are qt (runtimes), qa (number of distance calculations in the assignment step) and qau (total number of distance calculations).In all but 8 of 44 experiments (italic), we observe an acceleration by means of ns limitation of up to 45%. As expected, the number of distance calculations in the assignment step is never greater when using ns limits, but the total number of distance calculations is occasionally increased by maintaining initial variables."}, {"heading": "4.2. Multicore experiments", "text": "We have implemented parallel versions of all the algorithms described in this paper, using the thread support library C + + 11. To measure the acceleration when using multiple cores, we compare the runtime when using four threads with that of a thread on a four-core machine without hyperthreading. The results are summarized in Table 6, where nearly quadruple acceleration is observed."}, {"heading": "5. Conclusion and future work", "text": "The experimental results presented show that the nsbounding scheme makes exact k-mean algorithms faster, and that our exposure algorithm is significantly faster than existing state-of-the-art algorithms in low dimensions. Both can be regarded as good standard choices for k-mean indications of large amounts of data. The biggest practical weakness that remains is the necessary prior selection of the algorithm to be used depending on the dimensionality of the problem at hand. This should be solved by an adaptive procedure that is able to automatically select the optimal algorithm through an efficient exploration / exploitation strategy. The second and more promising direction of the work will be to introduce an exchange of information between samples rather than to process them independently."}, {"heading": "Acknowledgements", "text": "James Newling was funded by the Hasler Foundation under the 13018 MASH2.A funding programme."}], "references": [{"title": "Multidimensional binary search trees used for associative searching", "author": ["Bentley", "Jon Louis"], "venue": "Commun. ACM,", "citeRegEx": "Bentley and Louis.,? \\Q1975\\E", "shortCiteRegEx": "Bentley and Louis.", "year": 1975}, {"title": "MLPACK: A scalable C++ machine learning library", "author": ["Curtin", "Ryan R", "Cline", "James R", "Slagle", "Neil P", "March", "William B", "P. Ram", "Mehta", "Nishant A", "Gray", "Alexander G"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Curtin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Curtin et al\\.", "year": 2013}, {"title": "Accelerated k-means with adaptive distance bounds", "author": ["Drake", "Jonathan", "Hamerly", "Greg"], "venue": "In 5th NIPS Workshop on Optimization for Machine Learning,", "citeRegEx": "Drake et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Drake et al\\.", "year": 2012}, {"title": "Using the triangle inequality to accelerate k-means", "author": ["Elkan", "Charles"], "venue": "In Machine Learning, Proceedings of the Twentieth International Conference (ICML", "citeRegEx": "Elkan and Charles.,? \\Q2003\\E", "shortCiteRegEx": "Elkan and Charles.", "year": 2003}, {"title": "A fast kmeans implementation using coresets", "author": ["Frahling", "Gereon", "Sohler", "Christian"], "venue": "In Proceedings of the Twenty-second Annual Symposium on Computational Geometry,", "citeRegEx": "Frahling et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Frahling et al\\.", "year": 2006}, {"title": "Making k-means even faster", "author": ["Hamerly", "Greg"], "venue": "In SDM, pp. 130\u2013140,", "citeRegEx": "Hamerly and Greg.,? \\Q2010\\E", "shortCiteRegEx": "Hamerly and Greg.", "year": 2010}, {"title": "Graphlab: A new parallel framework for machine learning", "author": ["Low", "Yucheng", "Gonzalez", "Joseph", "Kyrola", "Aapo", "Bickson", "Danny", "Guestrin", "Carlos", "Hellerstein", "Joseph M"], "venue": "In Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Low et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Low et al\\.", "year": 2010}, {"title": "Scalable recognition with a vocabulary tree", "author": ["Nister", "David", "Stewenius", "Henrik"], "venue": "In Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 2,", "citeRegEx": "Nister et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Nister et al\\.", "year": 2006}, {"title": "A fast nearest-neighbor search algorithm", "author": ["M.T. Orchard"], "venue": "In Acoustics, Speech, and Signal Processing,", "citeRegEx": "Orchard,? \\Q1991\\E", "shortCiteRegEx": "Orchard", "year": 1991}, {"title": "Object retrieval with large vocabularies and fast spatial matching", "author": ["J. Philbin", "O. Chum", "M. Isard", "J. Sivic", "A. Zisserman"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Philbin et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Philbin et al\\.", "year": 2007}, {"title": "Acceleration of k-means and related clustering algorithms. volume", "author": ["S.J. Phillips"], "venue": "Lecture Notes in Computer Science. Springer,", "citeRegEx": "Phillips,? \\Q2002\\E", "shortCiteRegEx": "Phillips", "year": 2002}, {"title": "Web-scale k-means clustering", "author": ["D. Sculley"], "venue": "In Proceedings of the 19th International Conference on World Wide Web,", "citeRegEx": "Sculley,? \\Q2010\\E", "shortCiteRegEx": "Sculley", "year": 2010}, {"title": "VLFeat: An open and portable library of computer vision algorithms", "author": ["A. Vedaldi", "B. Fulkerson"], "venue": "http: //www.vlfeat.org/,", "citeRegEx": "Vedaldi and Fulkerson,? \\Q2008\\E", "shortCiteRegEx": "Vedaldi and Fulkerson", "year": 2008}], "referenceMentions": [{"referenceID": 11, "context": "Others rely on a relaxation of the update step, for example by using only a subset of data to update centroids (Frahling & Sohler, 2006; Sculley, 2010).", "startOffset": 111, "endOffset": 151}, {"referenceID": 9, "context": "Certain of these rely on a relaxation of the assignment step, for example by only considering certain clusters according to some hierarchical ordering (Nister & Stewenius, 2006), or by using an approximate nearest neighbour search as in Philbin et al. (2007). Others rely on a relaxation of the update step, for example by using only a subset of data to update centroids (Frahling & Sohler, 2006; Sculley, 2010).", "startOffset": 237, "endOffset": 259}, {"referenceID": 8, "context": "Examples are the adaptation of the algorithm of Orchard (1991) in Phillips (2002), and the use of kdtrees (Bentley, 1975) in Kanungo et al.", "startOffset": 48, "endOffset": 63}, {"referenceID": 8, "context": "Examples are the adaptation of the algorithm of Orchard (1991) in Phillips (2002), and the use of kdtrees (Bentley, 1975) in Kanungo et al.", "startOffset": 48, "endOffset": 82}, {"referenceID": 8, "context": "Examples are the adaptation of the algorithm of Orchard (1991) in Phillips (2002), and the use of kdtrees (Bentley, 1975) in Kanungo et al. (2002). These algorithms relied on storing centroids in special data structures, enabling nearest neighbor queries to be processed without computing distances to all k centroids.", "startOffset": 48, "endOffset": 147}, {"referenceID": 8, "context": "Examples are the adaptation of the algorithm of Orchard (1991) in Phillips (2002), and the use of kdtrees (Bentley, 1975) in Kanungo et al. (2002). These algorithms relied on storing centroids in special data structures, enabling nearest neighbor queries to be processed without computing distances to all k centroids. The next big acceleration (Elkan, 2003) came about by maintaing bounds on distances between samples and centroids, frequently resulting in more than 90% of distance calculations being avoided. It was later shown (Hamerly, 2010) that in low-dimensions, it is more effective to keep bounds on distances to only the two nearest centroids, and that in general bounding-based algorithms are significantly faster than tree-based ones. Further bounding-based algorithms were proposed by Drake (2013) and Ding et al.", "startOffset": 48, "endOffset": 812}, {"referenceID": 8, "context": "Examples are the adaptation of the algorithm of Orchard (1991) in Phillips (2002), and the use of kdtrees (Bentley, 1975) in Kanungo et al. (2002). These algorithms relied on storing centroids in special data structures, enabling nearest neighbor queries to be processed without computing distances to all k centroids. The next big acceleration (Elkan, 2003) came about by maintaing bounds on distances between samples and centroids, frequently resulting in more than 90% of distance calculations being avoided. It was later shown (Hamerly, 2010) that in low-dimensions, it is more effective to keep bounds on distances to only the two nearest centroids, and that in general bounding-based algorithms are significantly faster than tree-based ones. Further bounding-based algorithms were proposed by Drake (2013) and Ding et al. (2015), each providing accelerations over their predecessors in certain settings.", "startOffset": 48, "endOffset": 835}, {"referenceID": 1, "context": "We use the following notation to refer to implementations: {codesource-algorithm}, where codesource is one of bay (Hamerly, 2015), mlp (Curtin et al., 2013), pow (Low et al.", "startOffset": 135, "endOffset": 156}, {"referenceID": 6, "context": ", 2013), pow (Low et al., 2010), vlf (Vedaldi & Fulkerson, 2008) and own (our own code), and algorithm is one of the algorithms described.", "startOffset": 13, "endOffset": 31}, {"referenceID": 1, "context": "We use the following notation to refer to implementations: {codesource-algorithm}, where codesource is one of bay (Hamerly, 2015), mlp (Curtin et al., 2013), pow (Low et al., 2010), vlf (Vedaldi & Fulkerson, 2008) and own (our own code), and algorithm is one of the algorithms described. Unless otherwise stated, times are wall times excluding data loading. We impose a time limit of 40 minutes and a memory limit of 4 GB on all {dataset, implementation, k, seed} runs. If a run fails to complete in 40 minutes, the corresponding table entry is \u2018t\u2019. Similarly, failure to execute with 4GB of memory results in a table entry \u2018m\u2019. We confirm that for all {dataset, k, seed} triplets, all implementations which complete within the time and memory constraint take the same number of iterations to converge to a common local minimum, as expected. The implementations are compared over the 22 datasets presented in Table 1, for k \u2208 {100, 1000}, with 10 distinct centroid initialisations (seeds). For all {dataset, k, seed} triplets, the 23 implementations are run serially on a machine with an Intel i7 processor and 8MB of cache memory. All experiments are performed using double precision floating point numbers. Findings in Drake (2013) suggest that the best algorithm to use for a dataset depends primarily on dimension, where in low-dimensions, ham and ann are fastest, in highdimensions elk is fastest, and in intermediate dimensions an approach maintaining a fractional number of bounds, Drake\u2019s algorithm, is fastest.", "startOffset": 136, "endOffset": 1234}], "year": 2016, "abstractText": "We propose a novel accelerated exact k-means algorithm, which outperforms the current stateof-the-art low-dimensional algorithm in 18 of 22 experiments, running up to 3\u00d7 faster. We also propose a general improvement of existing stateof-the-art accelerated exact k-means algorithms through better estimates of the distance bounds used to reduce the number of distance calculations, obtaining speedups in 36 of 44 experiments, of up to 1.8\u00d7. We have conducted experiments with our own implementations of existing methods to ensure homogeneous evaluation of performance, and we show that our implementations perform as well or better than existing available implementations. Finally, we propose simplified variants of standard approaches and show that they are faster than their fully-fledged counterparts in 59 of 62 experiments.", "creator": "LaTeX with hyperref package"}}}