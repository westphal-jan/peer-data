{"id": "1602.02644", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2016", "title": "Generating Images with Perceptual Similarity Metrics based on Deep Networks", "abstract": "Image-generating machine learning models are typically trained with loss functions based on distance in the image space. This often leads to over-smoothed results. We propose a class of loss functions, which we call deep perceptual similarity metrics (DeePSiM), that mitigate this problem. Instead of computing distances in the image space, we compute distances between image features extracted by deep neural networks. This metric better reflects perceptually similarity of images and thus leads to better results. We show three applications: autoencoder training, a modification of a variational autoencoder, and inversion of deep convolutional networks. In all cases, the generated images look sharp and resemble natural images.", "histories": [["v1", "Mon, 8 Feb 2016 16:50:28 GMT  (14140kb,D)", "http://arxiv.org/abs/1602.02644v1", null], ["v2", "Tue, 9 Feb 2016 09:36:36 GMT  (14294kb,D)", "http://arxiv.org/abs/1602.02644v2", "minor corrections"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["alexey dosovitskiy", "thomas brox"], "accepted": true, "id": "1602.02644"}, "pdf": {"name": "1602.02644.pdf", "metadata": {"source": "META", "title": "Generating Images with Perceptual Similarity Metrics based on Deep Networks", "authors": ["Alexey Dosovitskiy", "Thomas Brox"], "emails": ["DOSOVITS@CS.UNI-FREIBURG.DE", "BROX@CS.UNI-FREIBURG.DE"], "sections": [{"heading": "1. Introduction", "text": "Recently, there has been a wave of interest in the formation of neural networks to generate images, which are used for a variety of applications: unattended and semi-supervised learning, generative models, analysis of learned representations, analysis by synthesis, learning 3D representations, future predictions in videos. Nevertheless, there is little work to reconstruct an image of its function. Typically, square Euclidean distances between images are often blurred, see Figur.1b. This is especially the case when there is an inherent uncertainty in the prediction."}, {"heading": "2. Related work", "text": "It is the time when one sees oneself able to surpass oneself, and it is the time that one needs to unfold. It is the time that one needs to unfold. It is the time that one needs to unfold, to unfold. It is the time that one needs to unfold, to unfold."}, {"heading": "3. Model", "text": "Suppose we get a supervised learning task and a training of input-target pairs, which we call new image losses. (This is a specific specification of images with an arbitrary number of channels.) The goal is to learn the parameters of a differentiable generator function. (The goal is to learn the parameters of a differentiable generator function.) Typical decisions are squared euclidean (SE) losses of L2 (x), y), y, y, y, which optimally approximates the input-target dependence according to a loss function. (The goal is a loss function of L (x), y, y, y, it."}, {"heading": "3.1. Architectures", "text": "Generators. We used several different generators in experiments. They are task-specific, so we describe them in corresponding sections below. All the generators tested use up-convolutionary (\"deconvolutionary\") layers, as in Dosovitskiy et al. (2015b). An up-convolutionary layer consists of up-sampling and subsequent convolution. In this paper, we always perform up-sample by a factor of 2 and an \"bed of nails\" upsampling. In all networks we use leaky ReLU nonlinearities, that is, LReLU (x) = max (x, 0) + \u03b1min (x, 0).We used \u03b1 = 0.3 in our experiments linear output layers. We experimented with four comparators: 1. AlexNet (Krizhevsky et al., 2012) is a network with 5 convolutionary and 2 fully connected layers based on image classifications.2. The Netta network 2015 is the network architecture we traced ()."}, {"heading": "3.2. Training details", "text": "For optimization, we used Adam (Kingma & Ba, 2015) with dynamics \u03b21 = 0.9, \u03b22 = 0.999 and an initial learning rate of 0.0002. To prevent the discriminator from overrepeating during opposing training, we temporarily stopped updating when the Ldiscr / Ladv ratio was below a certain threshold (0.1 in most experiments)."}, {"heading": "4. Experiments", "text": "We started with a simple proof-of-concept experiment that showed how DeePSiM can be applied to auto encoder training, then used the proposed loss function within the Variational Autoencoder (UAE) framework. Finally, we applied the method to reverse the representation learned from AlexNet and analyzed some of the characteristics of the method. In quantitative comparisons, we report normalized Euclidean errors | | a \u2212 b | | 2 / N. The normalization coefficient N is the average of the Euclidean distances between all pairs of different samples from the test set. Therefore, the 100% error means that the algorithm does the same thing as if it randomly draws a sample from the test set."}, {"heading": "4.1. Autoencoder", "text": "Here is the objective of the generator coincides with its input (i.e., y = x), and the task of the generator is to encode the input to a compressed hidden representation and then decrypt the image back. Architecture is shown in Table 2. All layers are convolutional or up-convolutional. Hidden representation is an 8-channel function board 8 times smaller than the input image. We experimented with four loss functions: SE and \"1 in the image space, as well as DeePSiM with 96 x 96 pixels. To prevent us augmenting the data through random 64 x 64 patches during the training. We experimented with four loss functions: SE and\" 1 in the image space, as DeePSiM with AlexNet CONV3 or exemplar CNN CONV3 as a comparative model. Qualitative results are shown in Figure. 3, quantitative results in table."}, {"heading": "4.2. Variational autoencoder", "text": "A standard UAE consists of an encoder Enc and a decoder Dec. The encoder maps an input sample x to a distribution over latent variables z \u0445 Enc (x) = q (z | x). The dec maps a distribution over images x \u00b2 Dec (z) = p (x | z) from this latent space. The loss function is \u2211 i \u2212 Eq (z | xi) log p (xi | z) + DKL (q (z | xi) | p (z))), (6) where p (z) is a previous distribution of latent variables and DKL is the Kullback-Leibler divergence. The first term in Eq. 6 is a reconstruction error. If we assume that the decoder predicts a Gaussian distribution on each pixel, then it reduces the difference to quared euclidean errors in the image space. The second term draws the distribution of latent variables in the direction of the previous one."}, {"heading": "4.3. Inverting AlexNet", "text": "One approach is the reversal of representation. This may give insights into the way in which information is preserved in representation and what its default properties are. However, the reversal of a non-trivial property, which was in fact learned by a large revolutionary network, is a difficult, hard-to-work problem. Our proposed approach also shows how DeePSiM is an excellent loss function when dealing with very difficult image repairs. Moreover, we are very rich in information about the image in deep layers of the network and even in the predicted class probabilities. Although it is an interesting result in itself, it shows that DeePSiM is an excellent loss function when dealing with very difficult image repairs, tasksksksksksksksksksksks.Suppose We have a property that we aim at reversal, and an image I. There are two inverse mappings."}, {"heading": "5. Conclusion", "text": "We have proposed a class of loss functions applicable to image generation based on distances in image areas. Applying these functions to three tasks - automatic image encoding, random natural image generation with UAE, and inversion of image areas - shows that our loss is significantly superior to the typical loss in image area. In particular, it allows the reconstruction of perceptual details even from very low-dimensional images. We have evaluated several feature spaces to measure distances. Further studies are needed to find optimal features that can be used depending on the task. To control the degree of realism in generated images, an alternative to contrary training is an approach similar to using feature statistics (Gatys et al., 2015). We consider this to be interesting directions for future work."}, {"heading": "Acknowledgements", "text": "The authors would like to thank Jost Tobias Springenberg and Philipp Fischer for their useful discussions and acknowledge the support of the ERC Starting Grant VideoLearn (279401)."}], "references": [{"title": "Deep generative stochastic networks trainable by backprop", "author": ["Y. Bengio", "E. Laufer", "G. Alain", "J. Yosinski"], "venue": "In ICML,", "citeRegEx": "Bengio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2014}, {"title": "Digital images and human vision. chapter The Visible Differences Predictor: An Algorithm for the Assessment of Image Fidelity, pp. 179\u2013206", "author": ["S. Daly"], "venue": null, "citeRegEx": "Daly.,? \\Q1993\\E", "shortCiteRegEx": "Daly.", "year": 1993}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks", "author": ["E.L. Denton", "S. Chintala", "arthur Szlam", "R. Fergus"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Discriminative unsupervised feature learning with exemplar convolutional neural networks", "author": ["A. Dosovitskiy", "P. Fischer", "J.T. Springenberg", "M. Riedmiller", "T. Brox"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Dosovitskiy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dosovitskiy et al\\.", "year": 2015}, {"title": "Inverting visual representations with convolutional networks", "author": ["A. Dosovitskiy", "T. Brox"], "venue": "arxiv/1506.02753v2,", "citeRegEx": "Dosovitskiy and Brox.,? \\Q2015\\E", "shortCiteRegEx": "Dosovitskiy and Brox.", "year": 2015}, {"title": "Learning to generate chairs with convolutional neural networks", "author": ["A. Dosovitskiy", "J.T. Springenberg", "T. Brox"], "venue": "In CVPR,", "citeRegEx": "Dosovitskiy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dosovitskiy et al\\.", "year": 2015}, {"title": "A neural algorithm of artistic style", "author": ["L.A. Gatys", "A.S. Ecker", "M. Bethge"], "venue": null, "citeRegEx": "Gatys et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gatys et al\\.", "year": 2015}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "In NIPS,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D.J. Rezende", "D. Wierstra"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Reducing the dimensionality of data with neural", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "networks. Science,", "citeRegEx": "Hinton and Salakhutdinov.,? \\Q2006\\E", "shortCiteRegEx": "Hinton and Salakhutdinov.", "year": 2006}, {"title": "Learning and relearning in boltzmann machines. In Parallel Distributed Processing: Volume 1: Foundations, pp. 282\u2013317", "author": ["G.E. Hinton", "T.J. Sejnowski"], "venue": null, "citeRegEx": "Hinton and Sejnowski.,? \\Q1986\\E", "shortCiteRegEx": "Hinton and Sejnowski.", "year": 1986}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural Comput.,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": null, "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "In ICLR,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Semi-supervised learning with deep generative models", "author": ["D. Kingma", "D. Rezende", "S. Mohamed", "M. Welling"], "venue": "In NIPS,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS, pp", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Autoencoding beyond pixels using a learned similarity", "author": ["A.B.L. Larsen", "S.K. S\u00f8nderby", "O. Winther"], "venue": "metric. arxiv:1512.09300,", "citeRegEx": "Larsen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Larsen et al\\.", "year": 2015}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "In ICML, pp", "citeRegEx": "Lee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "Understanding deep image representations by inverting them", "author": ["A. Mahendran", "A. Vedaldi"], "venue": "In CVPR,", "citeRegEx": "Mahendran and Vedaldi.,? \\Q2015\\E", "shortCiteRegEx": "Mahendran and Vedaldi.", "year": 2015}, {"title": "Deep multiscale video prediction beyond mean square error", "author": ["M. Mathieu", "C. Couprie", "Y. LeCun"], "venue": "URL http://arxiv. org/abs/1511.05440", "citeRegEx": "Mathieu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mathieu et al\\.", "year": 2015}, {"title": "Conditional generative adversarial nets", "author": ["M. Mirza", "S. Osindero"], "venue": null, "citeRegEx": "Mirza and Osindero.,? \\Q2014\\E", "shortCiteRegEx": "Mirza and Osindero.", "year": 2014}, {"title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": null, "citeRegEx": "Radford et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Learning to generate images with perceptual similarity", "author": ["K. Ridgeway", "J. Snell", "B. Roads", "R.S. Zemel", "M.C. Mozer"], "venue": "metrics. arxiv:1511.06409,", "citeRegEx": "Ridgeway et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ridgeway et al\\.", "year": 2015}, {"title": "Deep boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "In AISTATS,", "citeRegEx": "Salakhutdinov and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov and Hinton.", "year": 2009}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "In ICLR workshop track,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Information processing in dynamical systems: Foundations of harmony theory", "author": ["P. Smolensky"], "venue": "In Parallel Distributed Processing: Volume 1: Foundations,", "citeRegEx": "Smolensky.,? \\Q1987\\E", "shortCiteRegEx": "Smolensky.", "year": 1987}, {"title": "Perceptual quality measure using a spatio-temporal model of the human visual system", "author": ["C.J. van den Branden Lambrecht", "O. Verscheure"], "venue": "Electronic Imaging: Science & Technology,", "citeRegEx": "Lambrecht and Verscheure.,? \\Q1996\\E", "shortCiteRegEx": "Lambrecht and Verscheure.", "year": 1996}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "In ICML, pp", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Unsupervised learning of visual representations using videos", "author": ["X. Wang", "A. Gupta"], "venue": "In ICCV,", "citeRegEx": "Wang and Gupta.,? \\Q2015\\E", "shortCiteRegEx": "Wang and Gupta.", "year": 2015}, {"title": "Image quality assessment: From error visibility to structural similarity", "author": ["Z. Wang", "A.C. Bovik", "H.R. Sheikh", "E.P. Simoncelli"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Wang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2004}, {"title": "A perceptual distortion metric for digital color images", "author": ["S. Winkler"], "venue": "In in Proc. SPIE,", "citeRegEx": "Winkler.,? \\Q1998\\E", "shortCiteRegEx": "Winkler.", "year": 1998}, {"title": "Understanding neural networks through deep visualization", "author": ["J. Yosinski", "J. Clune", "A. Nguyen", "T. Fuchs", "H. Lipson"], "venue": "In Deep Learning Workshop,", "citeRegEx": "Yosinski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "To this end, we build upon adversarial training as proposed by Goodfellow et al. (2014). We train a discriminator network to distinguish the output of the generator from real images.", "startOffset": 63, "endOffset": 88}, {"referenceID": 26, "context": "A prominent class of probabilistic models of images are restricted Boltzmann machines (Hinton & Sejnowski, 1986; Smolensky, 1987; Hinton & Salakhutdinov, 2006) and their deep variants (Hinton et al.", "startOffset": 86, "endOffset": 159}, {"referenceID": 12, "context": "A prominent class of probabilistic models of images are restricted Boltzmann machines (Hinton & Sejnowski, 1986; Smolensky, 1987; Hinton & Salakhutdinov, 2006) and their deep variants (Hinton et al., 2006; Salakhutdinov & Hinton, 2009; Lee et al., 2009).", "startOffset": 184, "endOffset": 253}, {"referenceID": 18, "context": "A prominent class of probabilistic models of images are restricted Boltzmann machines (Hinton & Sejnowski, 1986; Smolensky, 1987; Hinton & Salakhutdinov, 2006) and their deep variants (Hinton et al., 2006; Salakhutdinov & Hinton, 2009; Lee et al., 2009).", "startOffset": 184, "endOffset": 253}, {"referenceID": 28, "context": "Autoencoders (Hinton & Salakhutdinov, 2006; Vincent et al., 2008) have been widely used for unsupervised learning and generative modeling, too.", "startOffset": 13, "endOffset": 65}, {"referenceID": 0, "context": "Recently, stochastic neural networks (Bengio et al., 2014; Kingma et al., 2014; Gregor et al., 2015) have become popular, and deterministic networks are being used for image generation tasks (Dosovitskiy et al.", "startOffset": 37, "endOffset": 100}, {"referenceID": 15, "context": "Recently, stochastic neural networks (Bengio et al., 2014; Kingma et al., 2014; Gregor et al., 2015) have become popular, and deterministic networks are being used for image generation tasks (Dosovitskiy et al.", "startOffset": 37, "endOffset": 100}, {"referenceID": 9, "context": "Recently, stochastic neural networks (Bengio et al., 2014; Kingma et al., 2014; Gregor et al., 2015) have become popular, and deterministic networks are being used for image generation tasks (Dosovitskiy et al.", "startOffset": 37, "endOffset": 100}, {"referenceID": 18, "context": "By combining convolutions and unpooling (upsampling) layers (Lee et al., 2009; Goodfellow et al., 2014; Dosovitskiy et al., 2015b) these models can be applied to large images.", "startOffset": 60, "endOffset": 130}, {"referenceID": 8, "context": "By combining convolutions and unpooling (upsampling) layers (Lee et al., 2009; Goodfellow et al., 2014; Dosovitskiy et al., 2015b) these models can be applied to large images.", "startOffset": 60, "endOffset": 130}, {"referenceID": 1, "context": "Some prominent examples are the visible differences predictor (Daly, 1993), the spatio-temporal model for moving picture quality assessment (van den Branden Lambrecht & Verscheure, 1996), and the perceptual distortion metric of Winkler (1998).", "startOffset": 62, "endOffset": 74}, {"referenceID": 30, "context": "The most popular perceptual image similarity metric is the structural similarity metric (SSIM) (Wang et al., 2004), which compares the local statistics of image patches.", "startOffset": 95, "endOffset": 114}, {"referenceID": 0, "context": "Recently, stochastic neural networks (Bengio et al., 2014; Kingma et al., 2014; Gregor et al., 2015) have become popular, and deterministic networks are being used for image generation tasks (Dosovitskiy et al., 2015b). In all these models, loss is measured in the image space. By combining convolutions and unpooling (upsampling) layers (Lee et al., 2009; Goodfellow et al., 2014; Dosovitskiy et al., 2015b) these models can be applied to large images. There is a large body of work on assessing the perceptual similarity of images. Some prominent examples are the visible differences predictor (Daly, 1993), the spatio-temporal model for moving picture quality assessment (van den Branden Lambrecht & Verscheure, 1996), and the perceptual distortion metric of Winkler (1998). The most popular perceptual image similarity metric is the structural similarity metric (SSIM) (Wang et al.", "startOffset": 38, "endOffset": 777}, {"referenceID": 0, "context": "Recently, stochastic neural networks (Bengio et al., 2014; Kingma et al., 2014; Gregor et al., 2015) have become popular, and deterministic networks are being used for image generation tasks (Dosovitskiy et al., 2015b). In all these models, loss is measured in the image space. By combining convolutions and unpooling (upsampling) layers (Lee et al., 2009; Goodfellow et al., 2014; Dosovitskiy et al., 2015b) these models can be applied to large images. There is a large body of work on assessing the perceptual similarity of images. Some prominent examples are the visible differences predictor (Daly, 1993), the spatio-temporal model for moving picture quality assessment (van den Branden Lambrecht & Verscheure, 1996), and the perceptual distortion metric of Winkler (1998). The most popular perceptual image similarity metric is the structural similarity metric (SSIM) (Wang et al., 2004), which compares the local statistics of image patches. We are not aware of any work making use of similarity metrics for machine learning, except a recent pre-print of Ridgeway et al. (2015). They train autoencoders by directly maximizing the SSIM similarity of images.", "startOffset": 38, "endOffset": 1084}, {"referenceID": 7, "context": "Generative adversarial networks (GANs) have been proposed by Goodfellow et al. (2014). In theory, this training procedure can lead to a generator that perfectly models the data distribution.", "startOffset": 61, "endOffset": 86}, {"referenceID": 3, "context": "Denton et al. (2015) employ a multi-scale approach, gradually generating higher resolution images.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Denton et al. (2015) employ a multi-scale approach, gradually generating higher resolution images. Radford et al. (2015) make use of a convolutional-deconvolutional architecture and batch normalization.", "startOffset": 0, "endOffset": 121}, {"referenceID": 20, "context": "Recently Mathieu et al. (2015) used GANs for predicting future frames in videos by conditioning on previous frames.", "startOffset": 9, "endOffset": 31}, {"referenceID": 17, "context": "Most related is concurrent work of Larsen et al. (2015). The general idea is the same \u2014 to measure the similarity not in the image space, but rather in a feature space.", "startOffset": 35, "endOffset": 56}, {"referenceID": 17, "context": "Most related is concurrent work of Larsen et al. (2015). The general idea is the same \u2014 to measure the similarity not in the image space, but rather in a feature space. They also use adversarial training to improve the realism of the generated images. However, Larsen et al. (2015) only apply this approach to a variational autoencoder trained on images of faces, and measure the similarity between features extracted from the discriminator.", "startOffset": 35, "endOffset": 282}, {"referenceID": 8, "context": "Instead of manually designing a prior, as in Mahendran & Vedaldi (2015), we learn it with an approach similar to Generative Adversarial Networks (GANs) of Goodfellow et al. (2014). Namely, we introduce a discriminator D\u03c6 which aims to discriminate the generated images from real ones, and which is trained concurrently with the generatorG\u03b8.", "startOffset": 155, "endOffset": 180}, {"referenceID": 16, "context": "AlexNet (Krizhevsky et al., 2012) is a network with 5 convolutional and 2 fully connected layers trained on image classification.", "startOffset": 8, "endOffset": 33}, {"referenceID": 4, "context": "All tested generators make use of up-convolutional (\u2019deconvolutional\u2019) layers, as in Dosovitskiy et al. (2015b). An up-convolutional layer consists of up-sampling and a subsequent convolution.", "startOffset": 85, "endOffset": 112}, {"referenceID": 2, "context": "First, when dealing with large ImageNet (Deng et al., 2009) images we increase the stride in the first layer from 2 to 4.", "startOffset": 40, "endOffset": 59}, {"referenceID": 13, "context": "We modified the caffe (Jia et al., 2014) framework to train the networks.", "startOffset": 22, "endOffset": 40}, {"referenceID": 17, "context": "This is similar to (Larsen et al., 2015), but the comparator does not have to be a part of the discriminator.", "startOffset": 19, "endOffset": 40}, {"referenceID": 15, "context": "Please refer to Kingma et al. (2014) for details.", "startOffset": 16, "endOffset": 37}, {"referenceID": 25, "context": "Mahendran & Vedaldi (2015), as well as Simonyan et al. (2014) and Yosinski et al.", "startOffset": 39, "endOffset": 62}, {"referenceID": 25, "context": "Mahendran & Vedaldi (2015), as well as Simonyan et al. (2014) and Yosinski et al. (2015), apply gradient-based optimization to find an image \u0128 which minimizes the loss", "startOffset": 39, "endOffset": 89}, {"referenceID": 7, "context": "To control the degree of realism in generated images, an alternative to adversarial training is an approach making use of feature statistics, similar to (Gatys et al., 2015).", "startOffset": 153, "endOffset": 173}], "year": 2016, "abstractText": "Image-generating machine learning models are typically trained with loss functions based on distance in the image space. This often leads to over-smoothed results. We propose a class of loss functions, which we call deep perceptual similarity metrics (DeePSiM), that mitigate this problem. Instead of computing distances in the image space, we compute distances between image features extracted by deep neural networks. This metric better reflects perceptually similarity of images and thus leads to better results. We show three applications: autoencoder training, a modification of a variational autoencoder, and inversion of deep convolutional networks. In all cases, the generated images look sharp and resemble natural images.", "creator": "LaTeX with hyperref package"}}}