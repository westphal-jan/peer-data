{"id": "1602.02658", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2016", "title": "Graying the black box: Understanding DQNs", "abstract": "In recent years there is a growing interest in using deep representations for reinforcement learning. In this paper, we present a methodology and tools to analyze Deep Q-networks (DQNs) in a non-blind matter. Using our tools we reveal that the features learned by DQNs aggregate the state space in a hierarchical fashion, explaining its success. Moreover we are able to understand and describe the policies learned by DQNs for three different Atari2600 games and suggest ways to interpret, debug and optimize of deep neural networks in Reinforcement Learning.", "histories": [["v1", "Mon, 8 Feb 2016 17:27:31 GMT  (7148kb,D)", "http://arxiv.org/abs/1602.02658v1", null], ["v2", "Tue, 9 Feb 2016 16:13:00 GMT  (7148kb,D)", "http://arxiv.org/abs/1602.02658v2", null], ["v3", "Wed, 17 Feb 2016 19:15:55 GMT  (7149kb,D)", "http://arxiv.org/abs/1602.02658v3", null], ["v4", "Mon, 24 Apr 2017 09:57:21 GMT  (7660kb,D)", "http://arxiv.org/abs/1602.02658v4", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["tom zahavy", "nir ben-zrihem", "shie mannor"], "accepted": true, "id": "1602.02658"}, "pdf": {"name": "1602.02658.pdf", "metadata": {"source": "META", "title": "Graying the black box: Understanding DQNs", "authors": ["Tom Zahavy", "Nir Ben Zrihem", "Shie Mannor"], "emails": ["TOMZAHAVY@CAMPUS.TECHNION.AC.IL", "BETZINIR@GMAIL.COM", "SHIE@EE.TECHNION.AC.IL"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is as if most of them are able to follow the rules that they have imposed on themselves. (...) In fact, it is as if they are able to determine themselves what they want. (...) It is as if they are able to determine themselves. (...) It is as if they are able to determine themselves. (...) It is as if they are able to determine themselves. (...) It is as if they are able to determine themselves. (...) It is as if they are able to determine themselves. (...) It is as if they are able to determine themselves. (...)"}, {"heading": "2. Related work", "text": "While some approaches examine the type of calculation that is performed at each level as a group (Yosinski et al., 2014), others attempt to explain the function that is computed by each individual neuron (similar to \"Jennifer Aniston Neuron\" (Quiroga et al., 2005).Dataset-centric approaches show images from the training or test set that cause high or low activations for individual units, e.g. the deconvolution method (Zeiler & Fergus, 2014) highlights the parts of a particular image that are responsible for firing each neural unit. Network-centric approaches examine a network directly without data from a dataset, e.g. Erhan et al. (Erhan et al., 2009) synthesized images that cause high activations for certain units. Other work used the input gradient to find images that cause strong activations (e.g. Simonyan & Zisserman, 2014; Qet al., 2009) that synthesize certain units for high activations."}, {"heading": "3. Background", "text": "This section contains relative background information on RL, DQN, and t-SNE."}, {"heading": "3.1. Reinforcement Learning", "text": "The objective of RL agents is to maximize their expected total reward by learning an optimal policy (mapping of states to actions). At one point in time, the agent observes a state st, selects an action and receives a reward rt, following the agent's decision, he observes the next state st + 1. We consider infinite horizontal problems where the cumulative return is discounted by a factor \u03b3 [0, 1] and the return is given at one point t by Rt = \u2211 T \u00b2 = t \u03b3t \u00b2 -trt, where T. The action-value function Q\u03c0 (s, a) measures the expected return after observing the state st (s, a) = E [Rt | st = s, at = a, \u03c0]. The optimal action value obeys a fundamental recursion known as the Bellman equation, Q \u0445 (st, at) = E rt + \u03b3maxa \u00b2 (Q, + 1)]."}, {"heading": "3.2. Deep Q Networks", "text": "The DQN algorithm (Mnih et al., 2015; 2013) approaches the optimal Q function with a Convolutional Neural Network (CNN) by optimizing the network weights to minimize the expected TD error of the optimal Bellman equation: It is an offline algorithm, which means that the tuples {st, at, rt, st + 1, \u03b3 = 0 st are collected from the agents \"experience, stored in the ER and later used for training. DQN maintains two separate Q networks with current parameters and target parameters updated from each fixed number of iterations."}, {"heading": "4. Methods", "text": "Our tool is shown in Figure 1. It contains a visualization of the t-SNE map (left), tools for visualizing global characteristics (top right), tools for visualizing game-specific characteristics (center right) and the visualization of the selected state and the associated gradient image (bottom right)."}, {"heading": "4.1. t-SNE", "text": "To generate the t-SNE numbers, we train a DQN for each Atari2600 game and then let it play with an epsilon-hungry scheme. We collect 120k game states, store the neural activations of the last hidden layer for each, and apply the Barnes Hut t-SNE (Van Der Maaten, 2014), which is useful for large datasets. We also process the data with Principal Component Analysis up to the dimension of 50. All experiments were performed with 3000 iterations of perplexity of 30."}, {"heading": "4.2. Coloring the t-SNE map with different measures", "text": "We assign each state its measurements, such as estimates (value, Q, advantage), generation time, termination (deciding how many GUI and details we use), and use them to color the t-SNE card. We also extract handmade features directly from the emulator's raw frames, such as player position, enemy position, number of lives, etc. We use these features to filter points in the t-SNE image. Filtered images show revealing patterns that are otherwise unseen. In our experience, handmade features are very powerful, but the disadvantage of using them is that they require manual labor. Similar to (Engel & Mannor, 2001) we visualize the dynamics of learned policies. However, we use a 3D t-SNE state representation and point to transitions with arrows."}, {"heading": "4.3. Saliency maps", "text": "We create saliency maps, similar to (Simonyan & Zisserman, 2014), by calculating the Jacobians of the trained value in relation to the input video. Jacobians image is displayed above the input image itself and helps to understand which pixels in the image affect the value prediction most."}, {"heading": "4.4. Analysis", "text": "From the analysis of the agent dynamics between clusters, we can point to a hierarchical aggregation of the state space. We define the clusters, which have a clear entry and exit area, as option clusters and are able to give an interpretation for the agent policy in these clusters. For some options, we are able to derive rules for initiation and termination, e.g. Landmark options (Mann et al., 2015) have a unique termination state."}, {"heading": "5. Experiments", "text": "We applied our methodology to three ATARI games: Breakout, Pacman and Seaquest. For each of these games, we give a brief description of the game, analyze the optimal strategy, detail the characteristics we designed, interpret the guidelines of the DQN and derive conclusions. Finally, we analyze initial and final states and the influence of score pixels."}, {"heading": "5.1. Breakout", "text": "In fact, the player has a movable racket to move the ball up and can keep it in play by touching it on the other side of the screen."}, {"heading": "5.2. Seaquest", "text": "In fact, the fact is that we will be able to be in a position and that we will be able to be able to put ourselves in a position to be in the lead, \"he said."}, {"heading": "5.3. Pacman", "text": "In Pacman, an agent navigates a maze while being chased by two ghosts. The agent is positively rewarded (+ 1) for collecting bricks. An episode ends when a predator catches the agent, or when the agent collects all bricks. There are also 4 bonus bricks, one at each corner of the maze. The bonus bricks offer a greater reward (+ 5), and more importantly, they make the ghosts vulnerable for a short period of time in which they cannot kill the agent. Occasionally, a bonus box appears for a short time that offers a high reward (+ 100) once collected. We extract traits for the player's position, direction of movement, the number of bricks to eat, minimum distance (L1) between the player and the predators, number of lives, ghost mode indicating that the predators are vulnerable, and the bonus box appears when a highly rated box appears."}, {"heading": "5.4. Enviroment modeling", "text": "The DQN algorithm requires a specific treatment for initial (zero-frame) and terminal states (zero-target), but it is not clear how this treatment works well. Therefore, in Figure 10, we show SNE cards for different games with an emphasis on termination and initial states. We can see that all terminal states are successfully mapped into a singular zone, but initial states are also mapped to singular zones and assigned false value forecasts. Following these observations, we propose to investigate various ways to model initial states, i.e. replicate a frame instead of feeding zeros and testing it with our methodology. We also find that terminal states seem to be properly modeled, but there seems to be a better way to model them, for example by setting the target to zero when the next state is terminal."}, {"heading": "5.5. Score pixels", "text": "Some Atari2600 games contain multiple repetitions of the same game. As soon as the agent finishes the first screen, he will be presented with another one that differs only in the score pixels. Therefore, an agent may have trouble generalizing to the new screen if he fits too closely to the score pixels. In Breakout, for example, the current state of the art score reaches about 450 points, while the maximum available points 896 indicate that the agent somehow does not learn to generalize for the new level. We examined the impact that score pixels have on network projections. Figure 11 shows the conspicuity maps of various games that support our claim that DQN bases its estimates on these pixels, and Figure 12 shows that the network cards assign states with different score digits to different zones in the game Seaquest. We suggest to investigate these effects further, for example, we suggest training an agent who does not receive these pixels as input."}, {"heading": "6. Conclusions", "text": "By analyzing the dynamics in and between these clusters, we were able to identify hierarchical structures. In particular, we are able to identify options with defined starting and ending rules. Aggregation of states gives the agent the ability to learn specific strategies for the different regions, thus providing an explanation for the success of DRL agents. Similar to neuroscience, where reverse engineering methods such as fMRI reveal structures in brain activity, we demonstrated how the agent's policy can be described with simple logical rules by processing the neural activity of the network. This is important because people often understand the optimal policy and therefore understand what the agent's weaknesses are. Furthermore, the ability to understand the hierarchical structure of policy can help to distill it into a simpler architecture (Rusu et al., 2015; Parisotto et al., 2015)."}, {"heading": "7.1. Breakout", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7. Suplemantary figures", "text": "7.2. Seawater7.3. Pacman"}], "references": [{"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["Bellemare", "Marc G", "Naddaf", "Yavar", "Veness", "Joel", "Bowling", "Michael"], "venue": "arXiv preprint arXiv:1207.4708,", "citeRegEx": "Bellemare et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2012}, {"title": "Increasing the action gap: New operators for reinforcement learning", "author": ["Bellemare", "Marc G", "Ostrovski", "Georg", "Guez", "Arthur", "Thomas", "Philip S", "Munos", "R\u00e9mi"], "venue": "arXiv preprint arXiv:1512.04860,", "citeRegEx": "Bellemare et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2015}, {"title": "Feudal reinforcement learning. pp. 271\u2013271", "author": ["Dayan", "Peter", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Dayan et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Dayan et al\\.", "year": 1993}, {"title": "Decomposition techniques for planning in stochastic domains", "author": ["Dean", "Thomas", "Lin", "Shieu-Hong"], "venue": "Citeseer,", "citeRegEx": "Dean et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dean et al\\.", "year": 1995}, {"title": "Hierarchical reinforcement learning with the MAXQ value function decomposition", "author": ["Dietterich", "Thomas G"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "Dietterich and G.,? \\Q2000\\E", "shortCiteRegEx": "Dietterich and G.", "year": 2000}, {"title": "Learning embedded maps of markov processes", "author": ["Engel", "Yaakov", "Mannor", "Shie"], "venue": "In in Proceedings of ICML 2001. Citeseer,", "citeRegEx": "Engel et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Engel et al\\.", "year": 2001}, {"title": "Visualizing higher-layer features of a deep network", "author": ["Erhan", "Dumitru", "Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pascal"], "venue": "Dept. IRO, Universite\u0301 de Montre\u0301al, Tech. Rep,", "citeRegEx": "Erhan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2009}, {"title": "Stable function approximation in dynamic programming", "author": ["Gordon", "Geoffrey J"], "venue": null, "citeRegEx": "Gordon and J.,? \\Q1995\\E", "shortCiteRegEx": "Gordon and J.", "year": 1995}, {"title": "Reinforcement learning for robots using neural networks", "author": ["Lin", "Long-Ji"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Lin and Long.Ji.,? \\Q1993\\E", "shortCiteRegEx": "Lin and Long.Ji.", "year": 1993}, {"title": "Manifold-learning-based feature extraction for classification of hyperspectral data: a review of advances in manifold learning", "author": ["Lunga", "Dalton", "Prasad", "Santasriya", "Crawford", "Melba M", "Ersoy", "Ozan"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "Lunga et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lunga et al\\.", "year": 2014}, {"title": "Approximate value iteration with temporally extended actions", "author": ["Mann", "Timothy A", "Mannor", "Shie", "Precup", "Doina"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Mann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mann et al\\.", "year": 2015}, {"title": "Dynamic abstraction in reinforcement learning via clustering", "author": ["Mannor", "Shie", "Menache", "Ishai", "Hoze", "Amit", "Klein", "Uri"], "venue": "In Proceedings of the twenty-first international conference on Machine learning,", "citeRegEx": "Mannor et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mannor et al\\.", "year": 2004}, {"title": "Q-cutdynamic discovery of sub-goals in reinforcement learning", "author": ["Menache", "Ishai", "Mannor", "Shie", "Shimkin", "Nahum"], "venue": "In Machine Learning: ECML", "citeRegEx": "Menache et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Menache et al\\.", "year": 2002}, {"title": "Playing atari with deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Graves", "Alex", "Antonoglou", "Ioannis", "Wierstra", "Daan", "Riedmiller", "Martin"], "venue": "arXiv preprint arXiv:1312.5602,", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["Nguyen", "Anh", "Yosinski", "Jason", "Clune", "Jeff"], "venue": "arXiv preprint arXiv:1412.1897,", "citeRegEx": "Nguyen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2014}, {"title": "Actor-mimic: Deep multitask and transfer reinforcement", "author": ["Parisotto", "Emilio", "Ba", "Jimmy Lei", "Salakhutdinov", "Ruslan"], "venue": null, "citeRegEx": "Parisotto et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Parisotto et al\\.", "year": 2015}, {"title": "Flexible decomposition algorithms for weakly coupled Markov decision problems", "author": ["Parr", "Ronald"], "venue": "In Proceedings of the Fourteenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "Parr and Ronald.,? \\Q1998\\E", "shortCiteRegEx": "Parr and Ronald.", "year": 1998}, {"title": "Invariant visual representation by single neurons in the human", "author": ["Quiroga", "R Quian", "Reddy", "Leila", "Kreiman", "Gabriel", "Koch", "Christof", "Fried", "Itzhak"], "venue": "brain. Nature,", "citeRegEx": "Quiroga et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Quiroga et al\\.", "year": 2005}, {"title": "Neural fitted Q iteration\u2013first experiences with a data efficient neural reinforcement learning method", "author": ["Riedmiller", "Martin"], "venue": "In Machine Learning: ECML", "citeRegEx": "Riedmiller and Martin.,? \\Q2005\\E", "shortCiteRegEx": "Riedmiller and Martin.", "year": 2005}, {"title": "Prioritized experience replay", "author": ["Schaul", "Tom", "Quan", "John", "Antonoglou", "Ioannis", "Silver", "David"], "venue": "arXiv preprint arXiv:1511.05952,", "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Identifying useful subgoals in reinforcement learning by local graph partitioning", "author": ["\u015eim\u015fek", "\u00d6zg\u00fcr", "Wolfe", "Alicia P", "Barto", "Andrew G"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "\u015eim\u015fek et al\\.,? \\Q2005\\E", "shortCiteRegEx": "\u015eim\u015fek et al\\.", "year": 2005}, {"title": "Reinforcement learning with soft state aggregation", "author": ["Singh", "Satinder P", "Jaakkola", "Tommi", "Jordan", "Michael I"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Singh et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Singh et al\\.", "year": 1995}, {"title": "Automated discovery of options in reinforcement learning", "author": ["Stolle", "Martin"], "venue": "PhD thesis, McGill University,", "citeRegEx": "Stolle and Martin.,? \\Q2004\\E", "shortCiteRegEx": "Stolle and Martin.", "year": 2004}, {"title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["Sutton", "Richard S", "Precup", "Doina", "Singh", "Satinder"], "venue": "Artificial intelligence,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Intriguing properties of neural networks", "author": ["Szegedy", "Christian", "Zaremba", "Wojciech", "Sutskever", "Ilya", "Bruna", "Joan", "Erhan", "Dumitru", "Goodfellow", "Ian", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1312.6199,", "citeRegEx": "Szegedy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2013}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["Tenenbaum", "Joshua B", "De Silva", "Vin", "Langford", "John C"], "venue": null, "citeRegEx": "Tenenbaum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2000}, {"title": "Temporal difference learning and TDGammon", "author": ["Tesauro", "Gerald"], "venue": "Communications of the ACM,", "citeRegEx": "Tesauro and Gerald.,? \\Q1995\\E", "shortCiteRegEx": "Tesauro and Gerald.", "year": 1995}, {"title": "Learning metric-topological maps for indoor mobile robot navigation", "author": ["Thrun", "Sebastian"], "venue": "Artificial Intelligence,", "citeRegEx": "Thrun and Sebastian.,? \\Q1998\\E", "shortCiteRegEx": "Thrun and Sebastian.", "year": 1998}, {"title": "An analysis of temporal-difference learning with function approximation", "author": ["Tsitsiklis", "John N", "Van Roy", "Benjamin"], "venue": "Automatic Control, IEEE Transactions on,", "citeRegEx": "Tsitsiklis et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Tsitsiklis et al\\.", "year": 1997}, {"title": "Accelerating t-SNE using treebased algorithms", "author": ["Van Der Maaten", "Laurens"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Maaten and Laurens.,? \\Q2014\\E", "shortCiteRegEx": "Maaten and Laurens.", "year": 2014}, {"title": "Visualizing data using t-SNE", "author": ["Van der Maaten", "Laurens", "Hinton", "Geoffrey"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Deep reinforcement learning with double q-learning", "author": ["Van Hasselt", "Hado", "Guez", "Arthur", "Silver", "David"], "venue": "arXiv preprint arXiv:1509.06461,", "citeRegEx": "Hasselt et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2015}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Wang", "Ziyu", "de Freitas", "Nando", "Lanctot", "Marc"], "venue": "arXiv preprint arXiv:1511.06581,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "How transferable are features in deep neural networks", "author": ["Yosinski", "Jason", "Clune", "Jeff", "Bengio", "Yoshua", "Lipson", "Hod"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Yosinski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2014}, {"title": "Visualizing and understanding convolutional networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "In Computer Vision\u2013 ECCV", "citeRegEx": "Zeiler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 22, "context": "Countless solutions to this problem have been offered including linear function approximators (Tsitsiklis & Van Roy, 1997), hierarchical representations (Dayan & Hinton, 1993) state aggregation (Singh et al., 1995) and options (Sutton et al.", "startOffset": 194, "endOffset": 214}, {"referenceID": 24, "context": ", 1995) and options (Sutton et al., 1999).", "startOffset": 20, "endOffset": 41}, {"referenceID": 0, "context": "Its promise was demonstrated in the Arcade Learning Environment (ALE) (Bellemare et al., 2012), a challenging framework composed of dozens of Atari games used to evaluate general competency in AI.", "startOffset": 70, "endOffset": 94}, {"referenceID": 19, "context": ", to choose the discount factor, the amount of history frames that represent a state, and to choose between the various algorithms and architectures (Nair et al., 2015; Van Hasselt et al., 2015; Schaul et al., 2015; Wang et al., 2015; Bellemare et al., 2015).", "startOffset": 149, "endOffset": 258}, {"referenceID": 33, "context": ", to choose the discount factor, the amount of history frames that represent a state, and to choose between the various algorithms and architectures (Nair et al., 2015; Van Hasselt et al., 2015; Schaul et al., 2015; Wang et al., 2015; Bellemare et al., 2015).", "startOffset": 149, "endOffset": 258}, {"referenceID": 1, "context": ", to choose the discount factor, the amount of history frames that represent a state, and to choose between the various algorithms and architectures (Nair et al., 2015; Van Hasselt et al., 2015; Schaul et al., 2015; Wang et al., 2015; Bellemare et al., 2015).", "startOffset": 149, "endOffset": 258}, {"referenceID": 12, "context": "Such methods decompose the learning task into simpler subtasks using graph partitioning (Menache et al., 2002; Mannor et al., 2004; \u015eim\u015fek et al., 2005) and path processing mechanisms (Stolle, 2004; Thrun, 1998).", "startOffset": 88, "endOffset": 152}, {"referenceID": 11, "context": "Such methods decompose the learning task into simpler subtasks using graph partitioning (Menache et al., 2002; Mannor et al., 2004; \u015eim\u015fek et al., 2005) and path processing mechanisms (Stolle, 2004; Thrun, 1998).", "startOffset": 88, "endOffset": 152}, {"referenceID": 21, "context": "Such methods decompose the learning task into simpler subtasks using graph partitioning (Menache et al., 2002; Mannor et al., 2004; \u015eim\u015fek et al., 2005) and path processing mechanisms (Stolle, 2004; Thrun, 1998).", "startOffset": 88, "endOffset": 152}, {"referenceID": 34, "context": "While some approaches study the type of computation performed at each layer as a group (Yosinski et al., 2014), others try to explain the function computed by each individual neuron (similar to the \u201dJennifer Aniston Neuron\u201d (Quiroga et al.", "startOffset": 87, "endOffset": 110}, {"referenceID": 17, "context": ", 2014), others try to explain the function computed by each individual neuron (similar to the \u201dJennifer Aniston Neuron\u201d (Quiroga et al., 2005)).", "startOffset": 121, "endOffset": 143}, {"referenceID": 6, "context": "(Erhan et al., 2009) synthesized images that cause high activations for particular units.", "startOffset": 0, "endOffset": 20}, {"referenceID": 14, "context": ", (Simonyan & Zisserman, 2014); (Nguyen et al., 2014); (Szegedy et al.", "startOffset": 32, "endOffset": 53}, {"referenceID": 25, "context": ", 2014); (Szegedy et al., 2013)).", "startOffset": 9, "endOffset": 31}, {"referenceID": 33, "context": "(Wang et al., 2015) suggested to use saliency maps and analyzed which pixels are more active at network predications.", "startOffset": 0, "endOffset": 19}, {"referenceID": 6, "context": ", Erhan et al. (Erhan et al., 2009) synthesized images that cause high activations for particular units. Other works used the input gradient to find images that cause strong activations (e.g., (Simonyan & Zisserman, 2014); (Nguyen et al., 2014); (Szegedy et al., 2013)). Since RL research was mostly focused on linear function approximations and policy gradient methods, we are less familiar with visualization techniques and attempts to understand the structure learnt by an agent. Wang et al. (Wang et al., 2015) suggested to use saliency maps and analyzed which pixels are more active at network predications. Using this method they compared between the standard DQN and their dueling network architecture. Engel & Mannor (2001) learnt an embedded map of Markov processes and visualized it on 2 dimensions, their analysis is based on the state transition distribution while we will focus on distances between the features learned by DQN.", "startOffset": 2, "endOffset": 732}, {"referenceID": 26, "context": "It has been proven to outperform linear dimensionality reduction methods and non-linear embedding methods such as ISOMAP (Tenenbaum et al., 2000) in several research fields including machine-learning benchmark datasets and hyper-spectral remote sensing data (Lunga et al.", "startOffset": 121, "endOffset": 145}, {"referenceID": 9, "context": ", 2000) in several research fields including machine-learning benchmark datasets and hyper-spectral remote sensing data (Lunga et al., 2014).", "startOffset": 120, "endOffset": 140}, {"referenceID": 10, "context": ", landmark options (Mann et al., 2015) has unique termination state.", "startOffset": 19, "endOffset": 38}, {"referenceID": 10, "context": "leave them until the tunnel is carved (see Figure 14), thus, we define them as a landmark option (Mann et al., 2015).", "startOffset": 97, "endOffset": 116}, {"referenceID": 15, "context": "Moreover, the ability to understand the hierarchical structure of the policy can help in distilling it into a simpler architecture (Rusu et al., 2015; Parisotto et al., 2015).", "startOffset": 131, "endOffset": 174}], "year": 2017, "abstractText": "In recent years there is a growing interest in using deep representations for reinforcement learning. In this paper, we present a methodology and tools to analyze Deep Q-networks (DQNs) in a non-blind matter. Using our tools we reveal that the features learned by DQNs aggregate the state space in a hierarchical fashion, explaining its success. Moreover we are able to understand and describe the policies learned by DQNs for three different Atari2600 games and suggest ways to interpret, debug and optimize of deep neural networks in Reinforcement Learning.", "creator": "LaTeX with hyperref package"}}}