{"id": "1602.02666", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2016", "title": "A Variational Analysis of Stochastic Gradient Algorithms", "abstract": "Stochastic Gradient Descent (SGD) is an important algorithm in machine learning. With constant learning rates, it is a stochastic process that, after an initial phase of convergence, generates samples from a stationary distribution. We show that SGD with constant rates can be effectively used as an approximate posterior inference algorithm for probabilistic modeling. Specifically, we show how to adjust the tuning parameters of SGD such as to match the resulting stationary distribution to the posterior. This analysis rests on interpreting SGD as a continuous-time stochastic process and then minimizing the Kullback-Leibler divergence between its stationary distribution and the target posterior. (This is in the spirit of variational inference.) In more detail, we model SGD as a multivariate Ornstein-Uhlenbeck process and then use properties of this process to derive the optimal parameters. This theoretical framework also connects SGD to modern scalable inference algorithms; we analyze the recently proposed stochastic gradient Fisher scoring under this perspective. We demonstrate that SGD with properly chosen constant rates gives a new way to optimize hyperparameters in probabilistic models.", "histories": [["v1", "Mon, 8 Feb 2016 17:46:18 GMT  (1543kb,D)", "http://arxiv.org/abs/1602.02666v1", "8 pages, 3 figures"]], "COMMENTS": "8 pages, 3 figures", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["stephan mandt", "matthew d hoffman", "david m blei"], "accepted": true, "id": "1602.02666"}, "pdf": {"name": "1602.02666.pdf", "metadata": {"source": "META", "title": "A Variational Analysis of Stochastic Gradient Algorithms", "authors": ["Stephan Mandt", "Matthew D. Hoffman"], "emails": ["sm3976@columbia.edu", "mathoffm@adobe.com", "david.blei@columbia.edu"], "sections": [{"heading": "1. Introduction", "text": "The question about the way we use this method is that it has been shown to achieve optimum function (or local optimum if it is not constant); current studies examine the merits of adaptive step sizes (Duchi et al., 2011), gradients or iterate averaging (Toulis et al.; De \u0301 s fossez and Bach, 2015), and constant step sizes (Bach and Moulines, 2015); stochastic gradient descension has enabled efficient optimization with massive data.Recently, we have used stochastic gradients (SG) in the services of scalable Bayesian Bayesians."}, {"heading": "2. Continuous-Time Limit Revisited", "text": "Our goal is to characterize the behavior of SGD using a constant step variable. To achieve this, we approach SGD with a continuous stochastic process (Kushner and Yin, 2003; Ljung et al., 2012)."}, {"heading": "2.1. Problem setup", "text": "Consider the loss functions of the following form: L (\u03b8) = 1N \u2211 N = 1'n (\u03b8), g (\u03b8) \u0445 \u03b8L (\u03b8). (2) Such loss functions are widely used in machine learning, where L (\u03b8) \u2261 L (\u03b8, x) is a loss function depending on the data x and the parameters \u03b8. Each \"n\" (\u03b8, xn) is the contribution to the total loss from a single observation. For example, if we find a maximum a-posterior estimate of a model, the contributions to the loss'n (\u03b8) = \u2212 log (xn | \u03b8) + 1 N log p (\u03b8), (3) where p (xn | \u03b8) is the probability and p (\u03b8) the previous estimate. For simpler notation, we will suppress the dependence of the loss on the data. We construct stochastic gradients from this loss."}, {"heading": "2.2. SGD as a Ornstein-Uhlenbeck process", "text": "We assume that the noise covariance is approximately constant. We decompose the constant noise covariance into a product of two constant matrices: C = BB. (7) This assumption is justified if we define noise as significant. (7) We assume that the noise covariance is approximately constant. (7) We assume that the noise covariance is approximately constant. (7) We assume that the noise covariance is approximately constant. (7) We assume that the noise covariance is approximately constant. (8) We assume that the noise covariance is approximately constant. (7) We assume that the noise covariance is approximately constant. (8) We assume that the noise covariance is approximately constant. (7) We assume that the noise covariance is approximately constant."}, {"heading": "3. SGD as Approximate Inference", "text": "We discussed a continuous time interpretation of the SGD with a constant step variable (constant SGD). We will now discuss how to use the constant SGD as an approximate inference algorithm. To repeat the setup from the introduction, we will consider a probabilistic model p (\u03b8, x) with data x and hidden variables \u03b8; our goal is to approximate this loss in Eq. 1.We will put the loss proportional to the negative log joint distribution (Eqs. 2 and 3), which equals the posterior to an additive constant. The classic goal of the SGD is to minimize this loss in Eq, which leads us to a maximum a-posterior point estimation of the parameters. This is how SGD is used in many statistical models, including logistic regression, linear regression, matrix factoring, neural network classifiers and regressors."}, {"heading": "3.1. Constant stochastic gradient descent", "text": "First, we show how to set constant SGD parameters to minimize the KL divergence to the posterior plane (BB divergence to the posterior plane); this is a kind of variable inference (Jordan et al., 1999a). Based on this analysis, we derive three versions of the constant SGD - one with a constant step size, one with a complete preconditioning matrix, and one with a diagonal preconditioning matrix. Finally, we show how to use these algorithms to learn hyperparameters.Assumption 4 from Sec. 2 says that the posterior plane is roughly Gaussian in the region, that the stationary distribution focuses on f (exp)."}, {"heading": "3.2. Stochastic Gradient Fisher Scoring", "text": "We are investigating whether this is a way in which the distribution effects between the individual areas of the distribution effects and the distribution effects between the individual areas of the distribution effects such as the distribution effects between the individual areas of the distribution effects and the distribution effects of the distribution effects such as the distribution effects between the algorithms and the distribution effects. (24) The matrix H is a preconditioner and EW (t) is a Gaussian noise; we control the preconditioner noise and the covariance of noise. (24) The matrix H is a preconditioner and EW (t) is a preconditioner that controls the preconditioner noise and the codification of noise."}, {"heading": "3.3. Implications for hyperparameter optimization", "text": "One of the most important advantages of the Bayesian approach is the ability to adjust hyperparameters to data without expensive cross-validation runs by placing hyperprioritization on these hyperparameters. Empirical Bayesian methods adjust hyperparameters by finding the hyperparameters that maximize the marginal probability of the data by integrating the most important model parameters: \u03bb? = arg max\u03bb log p (y | x, \u03bb) = arg max\u03bb log. If this marginal log probability is intractable, a common approach is to use variable expectation maximization (VEM) (Bishop, 2006), which iteratively optimizes a lower limit on the marginal log probability above \u03bb. If we approximate the posterior log probability p (2001) with some distribution parameters q (TB), then VEM tries to find a value that optimizes above the marginal log probability."}, {"heading": "4. Experiments", "text": "In Section 4.1 we test our theoretical assumptions and find good experimental evidence for their correctness. In this section we compare them with other approximate inference algorithms. In Section 4.2 we show that constant SGD allows us to optimize hyperparameters in a Bayean model."}, {"heading": "4.1. Confirming the stationary distribution\u2019s covariance", "text": "In this section, we confirm empirically that the stationary distributions of SGD with KL-optimal constant learning rates are approximately the same as those of the Ornstein-Uhlenbeck process.Data. First, we consider the following datasets. (1) The Wine Quality Data Set2, which contains N = 4898 instances, 11 features and a holistic output variable (the Wine Rating variable). (2) A dataset of Protein Tertiary Structure3, which contains N = 45730 instances, 8 features and an output variable. (3) The Skin Segmentation Data Set4, which contains N = 245057 instances, 3 features, and a binary output variable. We have set linear regression to datasets 1 and 2 and applied logistic regression to datasets 3. We have set the feature to standard length and a mini-batch of sizes S = 100, S = 10000."}, {"heading": "4.2. Optimizing hyperparameters", "text": "To test the hypothesis of Section 3.3 that constant SGD as a variable algorithm enables gradient-based learning of hyperparameters, we experimented with a Bayean multinomial logistic (also known as softmax) regression model with normal priors. The negative log joint to be optimized is isL \u2261 \u2212 log p (y, \u03b8 | x) = \u03bb2 \u0445 d, k \u03b8 2 dk \u2212 DK 2 log (\u03bb) + DK 2 log 2\u03c0 + \u0445 n log.k exp {\u2211 d xnd\u03b8dk, (27), where n \u00b2 index examples, d \u00b2 index examples, d \u00b2 index examples and k \u00b2 index examples and k \u00b2 index examples, k \u00b2 index examples, k \u00b2 index examples, (1,.) are indexed. K} indexes index classes, xn \u00b2 indices, classes, xn \u00b2 RD is the feature vector for the ninth example yn and \u00b2 hypersity."}, {"heading": "5. Related Work", "text": "Our work relates to Bayesian inference and stochastic optimization.Scalable MCMC (2015). The most recent work in Bayesian statistics focuses on the production of MCMC sampling algorithms that are scalable by using stochastic gradients. In particular, Welling and Teh (2011) develops stochastic gradient Langevin dynamics (SGLD) for a detailed convergence analysis of the algorithm. Although elegant, one drawback of SGLD is that the learning rate must be reduced to achieve the correct sampling regime, and the algorithms may suffer from slow mixing times. Other research results indicate improvements in this area by using Hamilton's Monte-Carlo."}, {"heading": "6. Conclusions", "text": "We analyzed new optimization goals of stochastic gradient descent in the context of statistical machine learning. Instead of lowering the learning rate to zero, we demand optimal constant learning rates to minimize Kullback-Leibler divergence between the stationary distribution of the SGD and the posterior one. This goal leads to criteria for optimal learning rates, minibatch sizes, and pre-conditioning matrices. In order to calculate stationary distributions and KL divergences, we approached the SGD in the sense of a continuous stochastic process over time, leading to the Ornstein-Uhlenbeck process. We also presented a novel analysis of stochastic gradient Fisher scoring. Finally, we demonstrated that a simple SGD algorithm can compete with stochastic variation methods in empirical Bayesian hyperparameter optimization."}, {"heading": "A. Stationary Covariance", "text": "The Ornstein-Uhlenbeck method has an analytical solution in relation to the stochastic integral (Gardiner et al., 1985), \u03b8 (t) = exp (\u2212 At) \u03b8 (0) + \u221a S-t 0 exp [\u2212 A (t \u2212 t \u2032)] BdW (t \u2032) (28). Following Gardiner's book, we derive an algebraic relation for the stationary covariance of the multivariate OrnsteinUhlenbeck process. Let us define \u03a3 = E [\u03b8 (t) \u03b8 (t) >]. Using the formal solution for \u03b8 (t), which is given in the main paper, we find A\u03a3 + \u0441A > = S-t \u2212 \u043c A exp [\u2212 A (t \u2212 t \u2032)] BB > exp [\u2212 A > (t \u2212 t \u2032)] dt > S > (t \u2212 t \u2032) dt \u2032 (\u2212 t > b) (\u2212 t \u00b2) the value of the subordinate p \u2212 p (t \u2212 t)."}, {"heading": "B. Stochastic Gradient Fisher Scoring", "text": "We proceed from the Ornstein-Uhlenbeck procedure, which defines dW (t) = \u2212 HATB (t) dt + H [B / S + E] dW (t) = \u2212 A \u2032 \u03b8 (t) dt + B \u2032 dW (t). (29) We defined A \u2032 \u2261 HA and B \u2032 \u2261 H [B / S + E]. As derived from the paper, the limit of variation (up to a constant) is KL c = N 2 Tr (A\u03a3) \u2212 log (| NA |). (30) To evaluate it, the task is to remove the unknown covariance \u03a3 from the limit. To this end, we use the identity for the stationary covariance A \u00b2 A \u2032 > = B \u2032 B \u2032 >. The criterion for the stationary covariance is equivalent to HA\u03a3 + \u0445AH = HBB > H > H + EZ (T > EZ)."}, {"heading": "C. Square root preconditioning", "text": "Finally, we analyze the case in which we assume a matrix proportional to the square root of the diagonal entries of the noise covariance. We define G = \u221a diag (BB >) (33) as a diagonal matrix containing the square roots of the diagonal elements of the noise covariance. We use an additional scalar learning rate. Let us consider SGD preconditioned with G \u2212 1 as defined above. Under the previous assumptions, the constant learning rate that minimizes the KL divergence between the stationary distribution of this process and the posterior is \u043d = 2DSNTr (BB > G \u2212 1). (34) For proof, we read the corresponding KL divergence from the proof for theorem 2 with G \u2212 1 \u2261 H: KL (q | f) c = N2S Tr (BB > G \u2212 1) \u2212 Tr log (G) + D 2 log S \u2212 1 \u2212 34 | inch less (35)."}], "references": [{"title": "Bayesian posterior sampling via stochastic gradient fisher scoring", "author": ["S. Ahn", "A. Korattikara", "M. Welling"], "venue": "arXiv preprint arXiv:1206.6380.", "citeRegEx": "Ahn et al\\.,? 2012", "shortCiteRegEx": "Ahn et al\\.", "year": 2012}, {"title": "Non-strongly-convex smooth stochastic approximation with convergence rate o (1/n)", "author": ["F. Bach", "E. Moulines"], "venue": "Advances in Neural Information Processing Systems, pages 773\u2013781.", "citeRegEx": "Bach and Moulines,? 2013", "shortCiteRegEx": "Bach and Moulines", "year": 2013}, {"title": "Pattern Recognition and Machine Learning", "author": ["C. Bishop"], "venue": "Springer New York.", "citeRegEx": "Bishop,? 2006", "shortCiteRegEx": "Bishop", "year": 2006}, {"title": "Online learning and stochastic approximations", "author": ["L. Bottou"], "venue": "On-line learning in neural networks, 17(9):25.", "citeRegEx": "Bottou,? 1998", "shortCiteRegEx": "Bottou", "year": 1998}, {"title": "Bridging the gap between stochastic gradient mcmc and stochastic optimization", "author": ["C. Chen", "D. Carlson", "Z. Gan", "C. Li", "L. Carin"], "venue": "arXiv preprint arXiv:1512.07962.", "citeRegEx": "Chen et al\\.,? 2015a", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "On the convergence of stochastic gradient mcmc algorithms with high-order integrators", "author": ["C. Chen", "N. Ding", "L. Carin"], "venue": "Advances in Neural Information Processing Systems, pages 2269\u20132277.", "citeRegEx": "Chen et al\\.,? 2015b", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Stochastic gradient hamiltonian monte carlo", "author": ["T. Chen", "E.B. Fox", "C. Guestrin"], "venue": "arXiv preprint arXiv:1402.4102.", "citeRegEx": "Chen et al\\.,? 2014", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Averaged least-meansquares: Bias-variance trade-offs and optimal sampling distributions", "author": ["A. D\u00e9fossez", "F. Bach"], "venue": "Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, pages 205\u2013213.", "citeRegEx": "D\u00e9fossez and Bach,? 2015", "shortCiteRegEx": "D\u00e9fossez and Bach", "year": 2015}, {"title": "Bayesian sampling using stochastic gradient thermostats", "author": ["N. Ding", "Y. Fang", "R. Babbush", "C. Chen", "R.D. Skeel", "H. Neven"], "venue": "Advances in neural information processing systems, pages 3203\u20133211.", "citeRegEx": "Ding et al\\.,? 2014", "shortCiteRegEx": "Ding et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research, 12:2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "From averaging to acceleration, there is only a step-size", "author": ["N. Flammarion", "F. Bach"], "venue": "arXiv preprint arXiv:1504.01577.", "citeRegEx": "Flammarion and Bach,? 2015", "shortCiteRegEx": "Flammarion and Bach", "year": 2015}, {"title": "Handbook of stochastic methods, volume 4", "author": ["Gardiner", "C. W"], "venue": "Springer Berlin.", "citeRegEx": "Gardiner and W,? 1985", "shortCiteRegEx": "Gardiner and W", "year": 1985}, {"title": "Introduction to variational methods for graphical models", "author": ["M. Jordan", "Z. Ghahramani", "T. Jaakkola", "L. Saul"], "venue": "Machine Learning, 37:183\u2013233.", "citeRegEx": "Jordan et al\\.,? 1999a", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "An introduction to variational methods for graphical models", "author": ["M.I. Jordan", "Z. Ghahramani", "T.S. Jaakkola", "L.K. Saul"], "venue": "Machine learning, 37(2):183\u2013233.", "citeRegEx": "Jordan et al\\.,? 1999b", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Brownian motion in a field of force and the diffusion model of chemical reactions", "author": ["H.A. Kramers"], "venue": "Physica, 7(4):284\u2013304.", "citeRegEx": "Kramers,? 1940", "shortCiteRegEx": "Kramers", "year": 1940}, {"title": "Automatic variational inference in stan", "author": ["A. Kucukelbir", "R. Ranganath", "A. Gelman", "D. Blei"], "venue": "Advances in Neural Information Processing Systems, pages 568\u2013576.", "citeRegEx": "Kucukelbir et al\\.,? 2015", "shortCiteRegEx": "Kucukelbir et al\\.", "year": 2015}, {"title": "Stochastic approximation and recursive algorithms and applications, volume 35", "author": ["H.J. Kushner", "G. Yin"], "venue": "Springer Science & Business Media.", "citeRegEx": "Kushner and Yin,? 2003", "shortCiteRegEx": "Kushner and Yin", "year": 2003}, {"title": "Dynamics of stochastic gradient algorithms. arXiv preprint arXiv:1511.06251", "author": ["Q. Li", "C Tai"], "venue": null, "citeRegEx": "Li and Tai,? \\Q2015\\E", "shortCiteRegEx": "Li and Tai", "year": 2015}, {"title": "Stochastic approximation and optimization of random systems, volume 17", "author": ["L. Ljung", "G.C. Pflug", "H. Walk"], "venue": "Birkh\u00e4user.", "citeRegEx": "Ljung et al\\.,? 2012", "shortCiteRegEx": "Ljung et al\\.", "year": 2012}, {"title": "A fast scoring algorithm for maximum likelihood estimation in unbalanced mixed models with nested random effects", "author": ["N.T. Longford"], "venue": "Biometrika, 74(4):817\u2013827.", "citeRegEx": "Longford,? 1987", "shortCiteRegEx": "Longford", "year": 1987}, {"title": "A complete recipe for stochastic gradient mcmc", "author": ["Ma", "Y.-A.", "T. Chen", "E.B. Fox"], "venue": "arXiv preprint arXiv:1506.04696.", "citeRegEx": "Ma et al\\.,? 2015", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Early stopping is nonparametric variational inference", "author": ["D. Maclaurin", "D. Duvenaud", "R.P. Adams"], "venue": "arXiv preprint arXiv:1504.01344.", "citeRegEx": "Maclaurin et al\\.,? 2015", "shortCiteRegEx": "Maclaurin et al\\.", "year": 2015}, {"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "The annals of mathematical statistics, pages 400\u2013407.", "citeRegEx": "Robbins and Monro,? 1951", "shortCiteRegEx": "Robbins and Monro", "year": 1951}, {"title": "Efficient recursive estimation; application to estimating the parameters of a covariance function", "author": ["D.J. Sakrison"], "venue": "International Journal of Engineering Science, 3(4):461\u2013483.", "citeRegEx": "Sakrison,? 1965", "shortCiteRegEx": "Sakrison", "year": 1965}, {"title": "Approximation analysis of stochastic gradient langevin dynamics by using fokker-planck equation and ito process", "author": ["I. Sato", "H. Nakagawa"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 982\u2013990.", "citeRegEx": "Sato and Nakagawa,? 2014", "shortCiteRegEx": "Sato and Nakagawa", "year": 2014}, {"title": "Lecture 6.5\u2014 RmsProp: Divide the Gradient by a Running Average of its Recent Magnitude. COURSERA: Neural Networks for Machine Learning", "author": ["T. Tieleman", "G. Hinton"], "venue": null, "citeRegEx": "Tieleman and Hinton,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton", "year": 2012}, {"title": "Statistical analysis of stochastic gradient methods for generalized linear models", "author": ["P. Toulis", "E. Airoldi", "J. Rennie"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 667\u2013675.", "citeRegEx": "Toulis et al\\.,? 2014", "shortCiteRegEx": "Toulis et al\\.", "year": 2014}, {"title": "On the theory of the brownian motion", "author": ["G.E. Uhlenbeck", "L.S. Ornstein"], "venue": "Physical review, 36(5):823.", "citeRegEx": "Uhlenbeck and Ornstein,? 1930", "shortCiteRegEx": "Uhlenbeck and Ornstein", "year": 1930}, {"title": "Bayesian learning via stochastic gradient langevin dynamics", "author": ["M. Welling", "Y.W. Teh"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 681\u2013688.", "citeRegEx": "Welling and Teh,? 2011", "shortCiteRegEx": "Welling and Teh", "year": 2011}, {"title": "Adaptive signal processing", "author": ["B. Widrow", "S.D. Stearns"], "venue": "Englewood Cliffs, NJ, Prentice-Hall, Inc., 1985, 491 p., 1.", "citeRegEx": "Widrow and Stearns,? 1985", "shortCiteRegEx": "Widrow and Stearns", "year": 1985}, {"title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "author": ["T. Zhang"], "venue": "Proceedings of the twenty-first international conference on Machine learning, page 116. ACM.", "citeRegEx": "Zhang,? 2004", "shortCiteRegEx": "Zhang", "year": 2004}], "referenceMentions": [{"referenceID": 9, "context": "Recent studies investigate the merits of adaptive step sizes (Duchi et al., 2011; Tieleman and Hinton, 2012), gradient or iterate averaging (Toulis et al.", "startOffset": 61, "endOffset": 108}, {"referenceID": 25, "context": "Recent studies investigate the merits of adaptive step sizes (Duchi et al., 2011; Tieleman and Hinton, 2012), gradient or iterate averaging (Toulis et al.", "startOffset": 61, "endOffset": 108}, {"referenceID": 7, "context": ", 2011; Tieleman and Hinton, 2012), gradient or iterate averaging (Toulis et al.; D\u00e9fossez and Bach, 2015), and constant step-sizes (Bach and Moulines, 2013; Flammarion and Bach, 2015).", "startOffset": 66, "endOffset": 106}, {"referenceID": 1, "context": "; D\u00e9fossez and Bach, 2015), and constant step-sizes (Bach and Moulines, 2013; Flammarion and Bach, 2015).", "startOffset": 52, "endOffset": 104}, {"referenceID": 10, "context": "; D\u00e9fossez and Bach, 2015), and constant step-sizes (Bach and Moulines, 2013; Flammarion and Bach, 2015).", "startOffset": 52, "endOffset": 104}, {"referenceID": 18, "context": "The classical result of Robbins and Monro (1951) is that this procedure provably reaches the optimum of the function (or local optimum, when it is nonconvex).", "startOffset": 24, "endOffset": 49}, {"referenceID": 28, "context": "New scalable MCMC algorithms\u2014such as SG Langevin dynamics (Welling and Teh, 2011), SG Hamiltonian Monte-Carlo (Chen et al.", "startOffset": 58, "endOffset": 81}, {"referenceID": 6, "context": "New scalable MCMC algorithms\u2014such as SG Langevin dynamics (Welling and Teh, 2011), SG Hamiltonian Monte-Carlo (Chen et al., 2014), and SG Fisher scoring (Ahn et al.", "startOffset": 110, "endOffset": 129}, {"referenceID": 0, "context": ", 2014), and SG Fisher scoring (Ahn et al., 2012)\u2014employ stochastic gradients of log p(\u03b8, x) to improve convergence and computation of existing sampling algorithms.", "startOffset": 31, "endOffset": 49}, {"referenceID": 0, "context": ", 2014), and SG Fisher scoring (Ahn et al., 2012)\u2014employ stochastic gradients of log p(\u03b8, x) to improve convergence and computation of existing sampling algorithms. Also see Ma et al. (2015) for a complete classification of these algorithms.", "startOffset": 32, "endOffset": 191}, {"referenceID": 13, "context": "Specifically, in the spirit of variational Bayes (Jordan et al., 1999b), we set those parameters to minimize the Kullback-Leibler (KL) divergence.", "startOffset": 49, "endOffset": 71}, {"referenceID": 27, "context": "Based on its interpretation as a continuous-time stochastic process\u2014specifically a multivariate Ornstein-Uhlenbeck (OU) process (Uhlenbeck and Ornstein, 1930; Gardiner et al., 1985)\u2014we compute stationary distributions for a large class of SGD algorithms, all of which converge to a Gaussian distribution with a non-trivial covariance matrix.", "startOffset": 128, "endOffset": 181}, {"referenceID": 9, "context": "The resulting criteria strongly resemble AdaGrad (Duchi et al., 2011), RMSProp (Tieleman and Hinton, 2012), and classical Fisher scoring (Longford, 1987).", "startOffset": 49, "endOffset": 69}, {"referenceID": 25, "context": ", 2011), RMSProp (Tieleman and Hinton, 2012), and classical Fisher scoring (Longford, 1987).", "startOffset": 17, "endOffset": 44}, {"referenceID": 19, "context": ", 2011), RMSProp (Tieleman and Hinton, 2012), and classical Fisher scoring (Longford, 1987).", "startOffset": 75, "endOffset": 91}, {"referenceID": 0, "context": "Specifically, we use the stochastic process perspective to compute the stationary sampling distribution of stochastic gradient Fisher scoring (Ahn et al., 2012).", "startOffset": 142, "endOffset": 160}, {"referenceID": 0, "context": "In section 3 we present consequences of this perspective: the interpretation of SGD as variational Bayes and results around stochastic gradient Fisher Scoring (Ahn et al., 2012).", "startOffset": 159, "endOffset": 177}, {"referenceID": 16, "context": "To do this, we approximate SGD with a continuous-time stochastic process (Kushner and Yin, 2003; Ljung et al., 2012).", "startOffset": 73, "endOffset": 116}, {"referenceID": 18, "context": "To do this, we approximate SGD with a continuous-time stochastic process (Kushner and Yin, 2003; Ljung et al., 2012).", "startOffset": 73, "endOffset": 116}, {"referenceID": 27, "context": "5 with the continuous-time Ornstein-Uhlenbeck process (Uhlenbeck and Ornstein, 1930).", "startOffset": 54, "endOffset": 84}, {"referenceID": 15, "context": "Rows: full-rank preconditioned constant SGD (top), constant SGD (middle), and BBVI (Kucukelbir et al., 2015) (bottom).", "startOffset": 83, "endOffset": 108}, {"referenceID": 14, "context": "The exit time of a stochastic process is typically exponential in the height of the barriers between minima, which can make local optima very stable even in the presence of noise (Kramers, 1940).", "startOffset": 179, "endOffset": 194}, {"referenceID": 28, "context": "8 SGLD (Welling and Teh, 2011) 2.", "startOffset": 7, "endOffset": 30}, {"referenceID": 0, "context": "5 SGFS-d (Ahn et al., 2012) 12.", "startOffset": 9, "endOffset": 27}, {"referenceID": 0, "context": "4 SGFS-f (Ahn et al., 2012) 0.", "startOffset": 9, "endOffset": 27}, {"referenceID": 15, "context": "3 BBVI (Kucukelbir et al., 2015) 44.", "startOffset": 7, "endOffset": 32}, {"referenceID": 27, "context": "ate Ornstein-Uhlenbeck process (Uhlenbeck and Ornstein, 1930).", "startOffset": 31, "endOffset": 61}, {"referenceID": 0, "context": "Second, we use it to analyze Stochastic Gradient Fisher Scoring (Ahn et al., 2012), both in its exact form and its more efficient approximate form.", "startOffset": 64, "endOffset": 82}, {"referenceID": 12, "context": "First, we show how to tune constant SGD\u2019s parameters to minimize the KL divergence to the posterior; this is a type of variational inference (Jordan et al., 1999a).", "startOffset": 141, "endOffset": 163}, {"referenceID": 29, "context": "Similar preconditioning matrices have been suggested earlier in optimal control theory based on very different arguments, see (Widrow and Stearns, 1985).", "startOffset": 126, "endOffset": 152}, {"referenceID": 9, "context": "Our result also relates to AdaGrad and its relatives (Duchi et al., 2011; Tieleman and Hinton, 2012), which also adjust the preconditioner based on the square root of the diagonal entries of the noise covariance.", "startOffset": 53, "endOffset": 100}, {"referenceID": 25, "context": "Our result also relates to AdaGrad and its relatives (Duchi et al., 2011; Tieleman and Hinton, 2012), which also adjust the preconditioner based on the square root of the diagonal entries of the noise covariance.", "startOffset": 53, "endOffset": 100}, {"referenceID": 0, "context": "We now investigate Stochastic Gradient Fisher Scoring (Ahn et al., 2012), a scalable Bayesian MCMC algorithm.", "startOffset": 54, "endOffset": 72}, {"referenceID": 0, "context": "This solution corresponds to the suggested Fisher Scoring update in the idealized case when the sampling noise distribution is estimated perfectly (Ahn et al., 2012).", "startOffset": 147, "endOffset": 165}, {"referenceID": 0, "context": "This solution corresponds to the suggested Fisher Scoring update in the idealized case when the sampling noise distribution is estimated perfectly (Ahn et al., 2012). Through this update, the algorithm thus generates posterior samples without decreasing the learning rate to zero. (This is in contrast to Stochastic Gradient Langevin Dynamics by Welling and Teh (2011).)", "startOffset": 148, "endOffset": 369}, {"referenceID": 0, "context": "In practice, however, SGFS is often used with a diagonal approximation of the preconditioning matrix (Ahn et al., 2012; Ma et al., 2015).", "startOffset": 101, "endOffset": 136}, {"referenceID": 20, "context": "In practice, however, SGFS is often used with a diagonal approximation of the preconditioning matrix (Ahn et al., 2012; Ma et al., 2015).", "startOffset": 101, "endOffset": 136}, {"referenceID": 0, "context": "This guideline is opposite to the advice of choosing B proportional to E, as suggested by Ahn et al. (2012), but follows naturally from the variational analysis.", "startOffset": 90, "endOffset": 108}, {"referenceID": 2, "context": "When this marginal log-likelihood is intractable, a common approach is to use variational expectation-maximization (VEM) (Bishop, 2006), which iteratively optimizes a variational lower bound on the marginal log-likelihood over \u03bb.", "startOffset": 121, "endOffset": 135}, {"referenceID": 15, "context": "1 shows the sampling distributions of black box variational inference (BBVI) using the reparametrization trick (Kucukelbir et al., 2015).", "startOffset": 111, "endOffset": 136}, {"referenceID": 6, "context": "Other research suggests improvements to this issue, using Hamiltonian Monte-Carlo (Chen et al., 2014) or thermostats (Ding et al.", "startOffset": 82, "endOffset": 101}, {"referenceID": 8, "context": ", 2014) or thermostats (Ding et al., 2014).", "startOffset": 23, "endOffset": 42}, {"referenceID": 22, "context": "In particular, Welling and Teh (2011) developed stochastic gradient Langevin dynamics (SGLD).", "startOffset": 15, "endOffset": 38}, {"referenceID": 19, "context": "Also see Sato and Nakagawa (2014) for a detailed convergence analysis of the algorithm.", "startOffset": 9, "endOffset": 34}, {"referenceID": 4, "context": "Other research suggests improvements to this issue, using Hamiltonian Monte-Carlo (Chen et al., 2014) or thermostats (Ding et al., 2014). Ma et al. (2015) give a complete classification of possible stochastic gradient-based MCMC schemes.", "startOffset": 83, "endOffset": 155}, {"referenceID": 0, "context": "Above, we analyzed properties of stochastic gradient Fisher scoring (Ahn et al., 2012).", "startOffset": 68, "endOffset": 86}, {"referenceID": 30, "context": "Stochastic gradient descent is an active field (Zhang, 2004; Bottou, 1998).", "startOffset": 47, "endOffset": 74}, {"referenceID": 3, "context": "Stochastic gradient descent is an active field (Zhang, 2004; Bottou, 1998).", "startOffset": 47, "endOffset": 74}, {"referenceID": 1, "context": "Bach and Moulines (2013); Flammarion and Bach (2015) discuss convergence rate of averaged gradients with constant step size, while D\u00e9fossez and Bach (2015) analyze sampling distributions using quasi-martingale techniques.", "startOffset": 0, "endOffset": 25}, {"referenceID": 1, "context": "Bach and Moulines (2013); Flammarion and Bach (2015) discuss convergence rate of averaged gradients with constant step size, while D\u00e9fossez and Bach (2015) analyze sampling distributions using quasi-martingale techniques.", "startOffset": 0, "endOffset": 53}, {"referenceID": 1, "context": "Bach and Moulines (2013); Flammarion and Bach (2015) discuss convergence rate of averaged gradients with constant step size, while D\u00e9fossez and Bach (2015) analyze sampling distributions using quasi-martingale techniques.", "startOffset": 0, "endOffset": 156}, {"referenceID": 1, "context": "Bach and Moulines (2013); Flammarion and Bach (2015) discuss convergence rate of averaged gradients with constant step size, while D\u00e9fossez and Bach (2015) analyze sampling distributions using quasi-martingale techniques. Toulis et al. (2014) calculate the asymptotic variance of SGD for the case of decreasing learning rates, assuming that the data is distributed according to the model.", "startOffset": 0, "endOffset": 243}, {"referenceID": 23, "context": "The fact that optimal preconditioning (using a decreasing Robbins-Monro schedule) is achieved by choosing the inverse noise covariance was first shown in (Sakrison, 1965), but here we derive the same result based on different arguments and suggest a scalar prefactor.", "startOffset": 154, "endOffset": 170}, {"referenceID": 29, "context": "Note the optimal scalar learning rate of 2/Tr(BB>) can also be derived based on stability arguments, as was done in the context of least mean square filters (Widrow and Stearns, 1985).", "startOffset": 157, "endOffset": 183}, {"referenceID": 16, "context": "The idea of analyzing stochastic gradient descent with stochastic differential equations is well established in the stochastic approximation literature (Kushner and Yin, 2003; Ljung et al., 2012).", "startOffset": 152, "endOffset": 195}, {"referenceID": 18, "context": "The idea of analyzing stochastic gradient descent with stochastic differential equations is well established in the stochastic approximation literature (Kushner and Yin, 2003; Ljung et al., 2012).", "startOffset": 152, "endOffset": 195}, {"referenceID": 4, "context": "Finally, Chen et al. (2015a) also draw analogies between SGD and scalable MCMC.", "startOffset": 9, "endOffset": 29}, {"referenceID": 4, "context": "Finally, Chen et al. (2015a) also draw analogies between SGD and scalable MCMC. They suggest annealing the posterior over iterations to use scalable MCMC as a tool for global optimization. We follow the opposite idea and suggest to use constant SGD as an approximate sampler by choosing appropriate learning rate and preconditioners. Stochastic differential equations. The idea of analyzing stochastic gradient descent with stochastic differential equations is well established in the stochastic approximation literature (Kushner and Yin, 2003; Ljung et al., 2012). Recent work focuses on dynamical aspects of the algorithm. Li et al. (2015) discuss several one-dimensional cases and momentum.", "startOffset": 9, "endOffset": 642}, {"referenceID": 4, "context": "Finally, Chen et al. (2015a) also draw analogies between SGD and scalable MCMC. They suggest annealing the posterior over iterations to use scalable MCMC as a tool for global optimization. We follow the opposite idea and suggest to use constant SGD as an approximate sampler by choosing appropriate learning rate and preconditioners. Stochastic differential equations. The idea of analyzing stochastic gradient descent with stochastic differential equations is well established in the stochastic approximation literature (Kushner and Yin, 2003; Ljung et al., 2012). Recent work focuses on dynamical aspects of the algorithm. Li et al. (2015) discuss several one-dimensional cases and momentum. Chen et al. (2015b) analyze stochastic gradient MCMC and studied their convergence properties using stochastic differential equations.", "startOffset": 9, "endOffset": 714}], "year": 2016, "abstractText": "Stochastic Gradient Descent (SGD) is an important algorithm in machine learning. With constant learning rates, it is a stochastic process that, after an initial phase of convergence, generates samples from a stationary distribution. We show that SGD with constant rates can be effectively used as an approximate posterior inference algorithm for probabilistic modeling. Specifically, we show how to adjust the tuning parameters of SGD such as to match the resulting stationary distribution to the posterior. This analysis rests on interpreting SGD as a continuoustime stochastic process and then minimizing the Kullback-Leibler divergence between its stationary distribution and the target posterior. (This is in the spirit of variational inference.) In more detail, we model SGD as a multivariate Ornstein-Uhlenbeck process and then use properties of this process to derive the optimal parameters. This theoretical framework also connects SGD to modern scalable inference algorithms; we analyze the recently proposed stochastic gradient Fisher scoring under this perspective. We demonstrate that SGD with properly chosen constant rates gives a new way to optimize hyperparameters in probabilistic models.", "creator": "LaTeX with hyperref package"}}}