{"id": "1602.02722", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2016", "title": "PAC Reinforcement Learning with Rich Observations", "abstract": "We propose and study a new tractable model for reinforcement learning with high-dimensional observation called Contextual-MDPs, generalizing contextual bandits to a sequential decision making setting. These models require an agent to take actions based on high-dimensional observations (features) with the goal of achieving long-term performance competitive with a large set of policies. Since the size of the observation space is a primary obstacle to sample-efficient learning, Contextual-MDPs are assumed to be summarizable by a small number of hidden states. In this setting, we design a new reinforcement learning algorithm that engages in global exploration while using a function class to approximate future performance. We also establish a sample complexity guarantee for this algorithm, proving that it learns near optimal behavior after a number of episodes that is polynomial in all relevant parameters, logarithmic in the number of policies, and independent of the size of the observation space. This represents an exponential improvement on the sample complexity of all existing alternative approaches and provides theoretical justification for reinforcement learning with function approximation.", "histories": [["v1", "Mon, 8 Feb 2016 20:12:50 GMT  (43kb,D)", "http://arxiv.org/abs/1602.02722v1", null], ["v2", "Tue, 1 Mar 2016 15:16:12 GMT  (50kb,D)", "http://arxiv.org/abs/1602.02722v2", null], ["v3", "Tue, 24 May 2016 13:20:29 GMT  (40kb,D)", "http://arxiv.org/abs/1602.02722v3", null], ["v4", "Fri, 28 Oct 2016 15:37:17 GMT  (45kb,D)", "http://arxiv.org/abs/1602.02722v4", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["akshay krishnamurthy", "alekh agarwal", "john langford"], "accepted": true, "id": "1602.02722"}, "pdf": {"name": "1602.02722.pdf", "metadata": {"source": "CRF", "title": "Contextual-MDPs for PAC-Reinforcement Learning with Rich Observations", "authors": ["Akshay Krishnamurthy", "Alekh Agarwal", "John Langford"], "emails": ["akshaykr@cs.cmu.edu", "alekha@microsoft.com", "jcl@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 The Model", "text": "In this section, we present the model that we will examine throughout the essay, which we call episodic contextual MDPs. First, we will establish a basic notation. Let K = | A |. Let us divide S into H resolved groups S1,.., SH, each with size M at most. For a group P, \u2206 (P) stands for the set of distributions over P. Let K = | A |. Let us divide S into H resolved groups S1,.., SH, each with size M maximum."}, {"heading": "2.1 Basic Definitions", "text": "We assume that the process (also known as loop-free or acyclic in literature) is defined in such a way that for a state in which there is a spatial and spatial distribution. (D) We assume that the process requires a spatial and spatial distribution. (D) We assume that the process requires a spatial and spatial distribution. (D) We assume that the process requires a spatial and spatial distribution. (D) We assume that the process requires a spatial and spatial distribution. (D) We assume that the process requires a spatial and spatial distribution. (D) We assume that the process requires a spatial and spatial distribution. (D) We assume that the process is also known as loop-free or acyclic in literature. (D) We assume that the process is described as lop-free or acyclic in literature."}, {"heading": "2.2 The Contextual-MDP Model", "text": "The contextual-MDP is as described above, but with an important limitation on the structure of the Q? function defined in Equation (3). Definition 1 (Contextual-MDP). Let (S, A, X, H, B, D) be a stratified episodic DP. Let Q? be defined accordingly as in Equation 3 and a? (s, x) = argmaxa-AQ? s (x, a). The POMDP is referred to as a contextual-MDP when for two states s, s \"so that Ds (x), Ds\" (x) > 0 we have one? (s, x) = a? (s, x).Restated, a contextual-MDP requires the optimal action to maximize long-term reward, which depends exclusively on observation x, regardless of the state. This is shown in Figure 1, where the optimal action explicitly depends on the current observation."}, {"heading": "2.3 Connections to Other Models", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "2.4 Connections to Other Techniques", "text": "State abstraction: Our work is closely related to the literature on state abstraction (see [16] for an overview), which focuses primarily on understanding what optimization properties are preserved in an MDP after state space has been compressed. Contextual MDPs, however, do not necessarily allow non-trivial state abstraction functions that are easy to discover (i.e., that do not amount to learning optimal behavior), since optimal behavior can depend on observation in an arbitrary manner. Moreover, while there are finite sample results for learning state abstractions, they all make strong assumptions that limit the scope. A recent example is the work of Jiang et al. [7], which finds a good abstraction from a series of successively finer, but cannot look beyond the exponentially many abstraction functions. Functional approximation: Our solution uses functional approximation, but implies the general approximation in Ps."}, {"heading": "3 Our Approach", "text": "In this paper, we consider the task of probably approximate correct (PAC) learning of contextual MDPs. Considering a political class, we say that a PAC algorithm learns a contextual MDP if the algorithm maximizes a policy \u03c0 with V (\u03c0) \u2265 for any, \u03b4 (1, 1), with a probability of at least 1 \u2212 \u03b4. The sample complexity of the algorithm is the number of episodes of the contextual MDP that the algorithm executes before returning a -suboptimal policy. Formally, sample complexity is a function n: (0, 1) 2 \u2192 N, so that for each, \u03b4 (0, 1), the algorithm returns a -suboptimal policy with a probability of at least 1 \u2212 \u03b4, using only n (, \u03b4) episodes using episodes."}, {"heading": "3.1 Additional Assumptions for the Result", "text": "Our algorithm works on the basis of contextual MDPs with two additional assumptions. The first assumption postulates the ability to approach the Q? function (3) well and seems to be essential for functional approximation based on an approach. Assumption 1 (feasibility). We identify our set of strategies with a number of regression functions. We assume that F? s is available to the learner and assume a feasibility assumption, which means that there is a function f? [0, 1]. This means that for each x x x x x x, a? s (x, a). We assume that F? s is available to the learner and make a feasibility assumption, which means that there is a function f? [f]."}, {"heading": "3.2 Algorithm", "text": "We are looking for an algorithm that enables realizable deterministic transitions, and each edge is associated with a consistent transition phase. We are looking for an algorithm that is consistent with realizable deterministic transitions, that ensures consistent complexity with sample complexity, the poly (M, K, H, Log (N), Log (1 / 3), and we point to such an example complexity that is bound up in Section 4. Our focus is on statistical efficiency, so we ignore compressive considerations in the current mode of operation. Before turning to the algorithm, it is worth clarifying some additional notation. As we focus on the deterministic transition phase, it is natural to think about the contextual MDP as an exponentially large search tree with fan-out K and depth H."}, {"heading": "4 Theoretical Analysis", "text": "In this section, we prove a PAC Learning Guarantee for cMDPLearn. (P, F, Test, E, E, E). (P, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E,"}, {"heading": "4.1 Preliminaries for the Proof", "text": "The proof of the Theorem depends on the analysis of the two subroutines. We initially turn to the TD-Elim-Routine to the TD-Elim, for which we have the following guarantee e.Theorem 2nd viewing the TD-Elim execution on the Path p with the Regressoren F, the parameters??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "4.2 Proof of Theorem 1", "text": "For now, we assume that all calls to TD-Elim and StateLearned are successful. Later, we will link the total number of calls and thus the total failure probability. Let's consider the error parameter passed to cMDPLearn, and remember that the bias in all calls to TD-Elim or StateLearned is the precondition. To see this, we must note that the algorithm only calls TD-Elim on the path p after the recursive step, which means that we have returned either TD-Elim or StateLearned to be true. Since both Theorems 2 and 3 Warranty Errors consider the estimate incorrect, the precondition applies to the path p. This argument applies to all paths p that we call TD-Elim, so the estimation requirement is always satisfactory. Next, we analyze the default that we prove by induction for the following claim."}, {"heading": "5 Discussion", "text": "As a first step, we develop cMDPLearn and show that it learns near-optimal behavior in contextual MDPs with polynomial sample complexity. To our knowledge, this is the first polynomial sample complexity related to learning with general functional approximation. However, there are many possibilities for future work: 1. cMDPLearn has two main undesirable properties. First, it requires a deterministic transition model that is unrealistic in some practical environments. Second, the algorithm includes the enumeration of the class of regression functions so that while its sample complexity is logarithmic in the size of the function class, its runtime is linear, which is typically inseparably slow."}, {"heading": "Acknowledgements", "text": "We thank Akshay Balsubramani and Hal Daume \u0301 III for the formative discussions and Tzu-Kuo Huang for the careful reading of an early draft of this essay."}, {"heading": "A Proof of Theorem 2", "text": "The proof of theorem 2 is quite technical, and we divide ourselves into several components. We start with several technical lemmas. We will reproduce the premises of the theorem (which we here) for all f-F and a-A. For all f-F and a-A we have estimates V-F (p-A, f-A) so that V-F (p-A, f-A) so that V-F (p-A) so, f-A (p-A) so, f-A (p-A) so, f-A (p-A, f-A) so, F-A (p-A) so, F-A (p-A) so, F-A (p-A) so, F-A (p-A) so."}, {"heading": "B Proof of Theorem 3", "text": "This result is a simple application of Hoeffding's inequality. We collect ntest observations xi \u00b2 Dp by rolling in p and applying the Monte Carlo estimates, V \u00b2 f (p \u2212 f) = 1st test ntest \u00b2 i = 1 f (p \u2212 f) - V (p \u2212 f). According to Hoeffding's inequality, we have this with a probability of at least 1 \u2212 \u03b4 that our empirical estimates are at most p \u2212 f (p \u2212 f) \u2212 V f (p \u2212 f) - V (p \u2212 f) - 2 log (2N / \u03b4) ntestSetting ntest = 2 log (2N \u2212 f) / \u03c62 that our empirical estimates are at most p \u2212 f (p \u2212 f) distant from the population.Now, for the first assertion that the population versions are already within 2% of each other, then the empirical versions within the population are at least 2 log (2N \u2212 f) - 2 apart, with the empirical versions for the inequality being V \u2212 f \u2212 f (f \u2212 f \u2212 f) -2."}], "references": [{"title": "Online learning in mdps with side information", "author": ["Yasin Abbasi-Yadkori", "Gergely Neu"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Contextual bandit learning with predictable rewards", "author": ["Alekh Agarwal", "Miroslav Dud\u0131\u0301k", "Satyen Kale", "John Langford", "Robert E Schapire"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["Leemon Baird"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1995}, {"title": "R-max-a general polynomial time algorithm for nearoptimal reinforcement learning", "author": ["Ronen I Brafman", "Moshe Tennenholtz"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Sample complexity of episodic fixed-horizon reinforcement learning", "author": ["Christoph Dann", "Emma Brunskill"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Efficient optimal learning for contextual bandits", "author": ["Miroslav Dudik", "Daniel Hsu", "Satyen Kale", "Nikos Karampatziakis", "John Langford", "Lev Reyzin", "Tong Zhang"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Abstraction selection in model-based reinforcement learning", "author": ["Nan Jiang", "Alex Kulesza", "Satinder Singh"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Model-based exploration in continuous state spaces", "author": ["Nicholas K Jong", "Peter Stone"], "venue": "In Abstraction, Reformulation, and Approximation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Approximately optimal approximate reinforcement learning", "author": ["Sham Kakade", "John Langford"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Exploration in metric state spaces", "author": ["Sham Kakade", "Michael Kearns", "John Langford"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["Michael Kearns", "Satinder Singh"], "venue": "Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Approximate planning in large pomdps via reusable trajectories", "author": ["Michael J Kearns", "Yishay Mansour", "Andrew Y Ng"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1999}, {"title": "A sparse sampling algorithm for near-optimal planning in large markov decision processes", "author": ["Michael J. Kearns", "Yishay Mansour", "Andrew Y. Ng"], "venue": "Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "The epoch-greedy algorithm for multi-armed bandits with side information", "author": ["John Langford", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Reducing reinforcement learning to kwik online regression", "author": ["Lihong Li", "Michael L Littman"], "venue": "Annals of Mathematics and Artificial Intelligence,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Towards a unified theory of state abstraction for mdps", "author": ["Lihong Li", "Thomas J Walsh", "Michael L Littman"], "venue": "In International Symposium on Artificial Intelligence and Mathematics (ISAIM),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Predictive representations of state", "author": ["Michael L Littman", "Richard S Sutton", "Satinder P Singh"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2001}, {"title": "Reinforcement learning and mistake bounded algorithms", "author": ["Yishay Mansour"], "venue": "In Conference on Computational Learning Theory (COLT),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}, {"title": "Learning finite-state controllers for partially observable environments", "author": ["Nicolas Meuleau", "Leonid Peshkin", "Kee-Eung Kim", "Leslie Pack Kaelbling"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1999}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "A convergent form of approximate policy iteration", "author": ["Theodore J Perkins", "Doina Precup"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2002}, {"title": "Efficient pac learning for episodic tasks with acyclic state spaces", "author": ["Spyros Reveliotis", "Theologos Bountourelis"], "venue": "Discrete Event Dynamic Systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Predictive state representations: A new theory for modeling dynamical systems. In Uncertainty in Artificial Intelligence (UAI)", "author": ["Satinder Singh", "Michael R James", "Matthew R Rudary"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2004}, {"title": "Pac model-free reinforcement learning", "author": ["Alexander L Strehl", "Lihong Li", "Eric Wiewiora", "John Langford", "Michael L Littman"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Richard S Sutton", "David A McAllester", "Satinder P Singh", "Yishay Mansour"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1999}], "referenceMentions": [{"referenceID": 19, "context": "The Atari Reinforcement Learning research program [20] has highlighted a critical deficiency of reinforcement learning algorithms: they cannot effectively solve problems that require systematic exploration.", "startOffset": 50, "endOffset": 54}, {"referenceID": 10, "context": "How can we construct Reinforcement Learning (RL) algorithms which effectively plan and plan to explore? In RL theory, this is an effectively solved problem for Markov Decision Processes (MDPs) [11, 4, 24].", "startOffset": 193, "endOffset": 204}, {"referenceID": 3, "context": "How can we construct Reinforcement Learning (RL) algorithms which effectively plan and plan to explore? In RL theory, this is an effectively solved problem for Markov Decision Processes (MDPs) [11, 4, 24].", "startOffset": 193, "endOffset": 204}, {"referenceID": 23, "context": "How can we construct Reinforcement Learning (RL) algorithms which effectively plan and plan to explore? In RL theory, this is an effectively solved problem for Markov Decision Processes (MDPs) [11, 4, 24].", "startOffset": 193, "endOffset": 204}, {"referenceID": 12, "context": "Approaches to RL with a weak dependence on these quantities exist [13], but suffer from an exponential dependence on the time horizon\u2014with K actions and a horizon of H , they require K samples.", "startOffset": 66, "endOffset": 70}, {"referenceID": 0, "context": "1 Basic Definitions An episodic Contextual-MDP is defined by the tuple (\u0393H ,\u0393, D) where H \u2208 N is the episode length, \u0393H \u2208 \u2206(SH) denotes a starting state distribution, \u0393 : (S \u00d7 A) \u2192 \u2206(S) denotes the transition dynamics, and D : S \u2192 \u2206(X \u00d7 [0, 1]) associates a distribution over (observation, reward) pairs with each state.", "startOffset": 237, "endOffset": 243}, {"referenceID": 0, "context": "We use Ds \u2208 \u2206(X \u00d7 [0, 1]) to denote the (observation, reward) distribution associated with state s and also the marginal distribution over observations (usage will be clear from context).", "startOffset": 18, "endOffset": 24}, {"referenceID": 0, "context": "We assume that almost surely \u2211H h=1 rh(ah) \u2208 [0, 1] for any action sequence.", "startOffset": 45, "endOffset": 51}, {"referenceID": 13, "context": "3 Connections to Other Models Our model is closely related to several well-studied models in the literature, namely: Contextual Bandits: IfH = 1, then Contextual-MDPs reduce to stochastic contextual bandits [14, 6], a well-studied simplification of the general reinforcement learning problem.", "startOffset": 207, "endOffset": 214}, {"referenceID": 5, "context": "3 Connections to Other Models Our model is closely related to several well-studied models in the literature, namely: Contextual Bandits: IfH = 1, then Contextual-MDPs reduce to stochastic contextual bandits [14, 6], a well-studied simplification of the general reinforcement learning problem.", "startOffset": 207, "endOffset": 214}, {"referenceID": 10, "context": "MDPs with small state spaces can be efficiently solved by tabular approaches that maintain and update statistics about each state [11, 4, 24].", "startOffset": 130, "endOffset": 141}, {"referenceID": 3, "context": "MDPs with small state spaces can be efficiently solved by tabular approaches that maintain and update statistics about each state [11, 4, 24].", "startOffset": 130, "endOffset": 141}, {"referenceID": 23, "context": "MDPs with small state spaces can be efficiently solved by tabular approaches that maintain and update statistics about each state [11, 4, 24].", "startOffset": 130, "endOffset": 141}, {"referenceID": 9, "context": "[10] (see also [8]) that has sample complexity independent of the number of unique states, but assumes the ability to cover the state space in a metric a priori known to the learner.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[10] (see also [8]) that has sample complexity independent of the number of unique states, but assumes the ability to cover the state space in a metric a priori known to the learner.", "startOffset": 15, "endOffset": 18}, {"referenceID": 12, "context": "[13] also implies a sample complexity bound that is independent of the observation space size for episodic MDPs, but grows as O(K).", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "More recently, Abbasi-Yadkori and Neu [1] propose a model for MDPs with side-information, but this model requires mapping side-information to a small-state MDP, rather than mapping an observation to an action as in Contextual-MDPs.", "startOffset": 38, "endOffset": 41}, {"referenceID": 24, "context": "However, these methods use local search techniques and consequently do not achieve global optimality [25, 9] in theory as well as empirically, unlike our algorithm which is guaranteed to find the globally optimal policy.", "startOffset": 101, "endOffset": 108}, {"referenceID": 8, "context": "However, these methods use local search techniques and consequently do not achieve global optimality [25, 9] in theory as well as empirically, unlike our algorithm which is guaranteed to find the globally optimal policy.", "startOffset": 101, "endOffset": 108}, {"referenceID": 18, "context": "While there are POMDP methods for learning reactive policies, or more generally policies with bounded memory [19], they are based on policy gradient techniques, which suffer both theoretical and empirical drawbacks as we mentioned.", "startOffset": 109, "endOffset": 113}, {"referenceID": 11, "context": "There are some sample complexity guarantees for learning in arbitrarily complex POMDPs, but the bounds we are aware of are quite weak as they scale linearly with |\u03a0| [12, 18].", "startOffset": 166, "endOffset": 174}, {"referenceID": 17, "context": "There are some sample complexity guarantees for learning in arbitrarily complex POMDPs, but the bounds we are aware of are quite weak as they scale linearly with |\u03a0| [12, 18].", "startOffset": 166, "endOffset": 174}, {"referenceID": 16, "context": "Predictive State Representations (PSRs): PSRs [17] encode states as a a collection of tests, a test being a sequence of (a, x) pairs observed in the history.", "startOffset": 46, "endOffset": 50}, {"referenceID": 22, "context": "Representationally, PSRs are even more powerful than POMDPs [23] which make them also more general than Contextual-MDPs.", "startOffset": 60, "endOffset": 64}, {"referenceID": 15, "context": "4 Connections to Other Techniques State Abstraction: Our work is closely related to the literature on state abstraction (See [16] for a survey), which primarily focuses on understanding what optimality properties are preserved in an MDP after the state space is compressed.", "startOffset": 125, "endOffset": 129}, {"referenceID": 6, "context": "[7] which finds a good abstraction from a set of successively finer ones, but cannot search over the exponentially many abstractions functions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "Function approximation is the empirical state-of-the-art in reinforcement learning [20], but theoretical analysis has been quite limited.", "startOffset": 83, "endOffset": 87}, {"referenceID": 20, "context": "Several authors have studied linear function approximation (See [26, 21]) but none of these results give finite sample bounds, as they do not address the exploration question.", "startOffset": 64, "endOffset": 72}, {"referenceID": 2, "context": "Baird [3] analyzes more general function approximation for predicting the value function in a Markov Chain, but does not show convergence when the agent is also selecting actions.", "startOffset": 6, "endOffset": 9}, {"referenceID": 14, "context": "More closely to our work, Li and Littman [15] do give finite sample bounds for RL with function approximation, but they assume access to a particular \u201cKnows-what-it-knows\u201d oracle, which cannot exist even for simple problems.", "startOffset": 41, "endOffset": 45}, {"referenceID": 0, "context": "We identify our set of policies \u03a0 with a set of regression functions F \u2282 (X \u00d7A)\u2192 [0, 1].", "startOffset": 81, "endOffset": 87}, {"referenceID": 1, "context": "[2] for contextual bandit learning in the realizable setting.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Assumptions vary with the paper, but broadly prior results establish the sample complexity for learning layered episodic MDPs with deterministic transitions is \u00d5(MKpoly(H)/ ) [5, 22].", "startOffset": 175, "endOffset": 182}, {"referenceID": 21, "context": "Assumptions vary with the paper, but broadly prior results establish the sample complexity for learning layered episodic MDPs with deterministic transitions is \u00d5(MKpoly(H)/ ) [5, 22].", "startOffset": 175, "endOffset": 182}, {"referenceID": 1, "context": "[2] and shows that R\u0303(f) is close to its expectation, which implies that f\u2217 is never eliminated and all surviving regressors have small expected squared error.", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "We propose and study a new tractable model for reinforcement learning with high-dimensional observation called Contextual-MDPs, generalizing contextual bandits to a sequential decision making setting. These models require an agent to take actions based on high-dimensional observations (features) with the goal of achieving long-term performance competitive with a large set of policies. Since the size of the observation space is a primary obstacle to sample-efficient learning, Contextual-MDPs are assumed to be summarizable by a small number of hidden states. In this setting, we design a new reinforcement learning algorithm that engages in global exploration while using a function class to approximate future performance. We also establish a sample complexity guarantee for this algorithm, proving that it learns near optimal behavior after a number of episodes that is polynomial in all relevant parameters, logarithmic in the number of policies, and independent of the size of the observation space. This represents an exponential improvement on the sample complexity of all existing alternative approaches and provides theoretical justification for reinforcement learning with function approximation.", "creator": "LaTeX with hyperref package"}}}