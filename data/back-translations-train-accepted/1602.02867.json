{"id": "1602.02867", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2016", "title": "Value Iteration Networks", "abstract": "We introduce the value iteration network: a fully differentiable neural network with a `planning module' embedded within. Value iteration networks are suitable for making predictions about outcomes that involve planning-based reasoning, such as predicting a desired trajectory from an observation of a map. Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation. We evaluate our value iteration networks on the task of predicting optimal obstacle-avoiding trajectories from an image of a landscape, both on synthetic data, and on challenging raw images of the Mars terrain.", "histories": [["v1", "Tue, 9 Feb 2016 05:44:36 GMT  (309kb,D)", "http://arxiv.org/abs/1602.02867v1", null], ["v2", "Sun, 29 May 2016 18:33:04 GMT  (416kb,D)", "http://arxiv.org/abs/1602.02867v2", "Update: new experiments on continuous domain and language task, more focus on generalization property of value iteration networks"], ["v3", "Sun, 5 Feb 2017 20:06:14 GMT  (1195kb,D)", "http://arxiv.org/abs/1602.02867v3", "NIPS final version"], ["v4", "Mon, 20 Mar 2017 21:41:51 GMT  (1195kb,D)", "http://arxiv.org/abs/1602.02867v4", "Fixed missing table values"]], "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.NE stat.ML", "authors": ["aviv tamar", "sergey levine", "pieter abbeel", "yi wu", "garrett thomas"], "accepted": true, "id": "1602.02867"}, "pdf": {"name": "1602.02867.pdf", "metadata": {"source": "META", "title": "Value Iteration Networks", "authors": ["Aviv Tamar", "Sergey Levine", "Pieter Abbeel"], "emails": ["AVIVT@BERKELEY.EDU", "SLEVINE@EECS.BERKELEY.EDU", "PABBEEL@CS.BERKELEY.EDU"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves. (...) In fact, it is so that they are able to survive themselves. (...) In fact, it is so that they are able to survive themselves. (...) It is so that they are able to survive themselves. (...) It is not so that they are able to survive themselves. (...) It is so that they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...)"}, {"heading": "2. Background", "text": "In this section, we provide background information on planning and Convolutionary Neural Networks and present our problem formulation."}, {"heading": "2.1. Value Iteration", "text": "A standard model for sequential decision-making and planning is the Markov Decision Process (MDP) (Bellman, 1957; Bertsekas, 2012). An MDP consists of the actions of a state, actions of a state, a reward function R (s, a), and a transitional core P (s, a) that encodes the probability of the next state taking into account the current state and action. A policy \u03c0 (a | s) prescribes a distribution of actions for each state. The goal of an MDP is to find a policy that achieves high rewards in the long term. Formally, the value V \u03c0 (s) of a state taking into account the expected discounted sum of rewards emanating from that state and executing the policy V \u03c0 (s). = A policy that achieves high rewards in the long term can be found in an MDP."}, {"heading": "2.2. Convolutional Neural Networks", "text": "Convolutionary neural networks (CNNs; LeCun et al. 1998; Bengio 2009) are neural networks with a specific architecture that has proved useful for computer vision, among other things. A CNN consists of stacked folding and max pooling layers. Input to each folding layer is a three-dimensional signal X, typically an image with l channels, m horizontal pixels and n vertical pixels, and its output h is a l \u00b2 channel convolution of the image with the nuclei W 1,.., W l \u2032, hl \u2032, i \u2032, j \u2032, j \u2032, i \u2032 l, i \u2032 j, i \u2032 j, j \u2032 \u2212 j, (2), which is a non-linear activation function, typically a rectified linear unit (ReLU): gyroscope (x) = max (x, 0).A maxpooling layer selects itself linear for each channel l and gyroscope (max, max-xi), point (xi-xi), point (i) = maximized point (i)."}, {"heading": "2.3. Problem Formulation", "text": "We consider a structured prediction (Bakir et al., 2007; Nowozin & Lampert, 2011) to be a problem where the input consists of a data set of images {xi} i = 1,..., N, States {si} i = 1,..., N, and action titles {ai} i = 1,..., N. We assume that the actions are generated by an optimal policy ai = \u03c0 * (si) in relation to an unknown path to the goal encoded in image xi. We emphasize that the rewards and transitions in the area of path planning are unknown and are not an explicit part of the input in any way. Our goal is to learn a mapping y (x, s) that best predicts the correct action structure for an image and presents a problem. Through this problem, we can incorporate this problem into a network that turns out to be an effective learning process."}, {"heading": "3. The Value Iteration Network Model", "text": "In this section we present the value prediction network - a model that encodes a differentiated planning procedure. Our starting point is the VI algorithm (1). Our main observation is that each iteration of VI can be considered as passing the previous value function Vn and reward function R through a convolution layer and a max pooling layer. In this analogy, each channel in the convolution layer of the image layer can be used for a specific action according to the Q function, and convolution kernel weights correspond to the discounted transition probabilities. Thus, by repeatedly applying a convolution layer K times, K iterations of VI can be perforated. Following this idea, we propose the VI block model as shown in Figure 2. The inputs to the VIN are a \"reward image\" r of the dimensions l, m, n, and a state (is, js)."}, {"heading": "3.1. Hierarchical VI Networks", "text": "The difficulty of training low-recurrence models such as the VI block described above increases with the number of VI iterations K (Pascanu et al., 2012). Unfortunately, for some planning areas, the number of VI iterations required to obtain a reasonable policy is quite large. Consider, for example, a grid world where target L is steps away from some states. Then, at least L iterations of VI are required to convey the reward information from the target to the states, and clearly, any action forecast received to the states with less than L-VI iterations is unaware of the target position and therefore unacceptable. At least potentially, a well-chosen reward design can help mitigate this problem. However, for a localized mapping from the input image to the shaped reward, as indicated by the CNN layers in the VI network above, the reward information problem can be multiplied."}, {"heading": "3.2. Approximate Planning Interpretation", "text": "In this context, the VI network can be considered an approximate value iteration algorithm, with network weights approaching the MDP transitions and the shaped reward function replacing the true reward. Most importantly, in this approximation VI is not iterated to convergence, but only for K iterations. In the planning literature, several studies examined the use of reward formulas (Ng et al., 1999), hierarchical macro actions (Mann & Mannor, 2014), and state aggregation (Taylor et al., 2009; Bertsekas, 2012) to reduce the number of iterations required for VI to converge, with the goal of reducing computing time."}, {"heading": "4. Results", "text": "We have evaluated the VIN model in two areas of path planning: The first domain is a synthetic grid world with obstacles, in which observation is a map of obstacles and destination position; the second domain is a navigation task for a Mars rover. In this domain, observation is an overhead image of the terrain, together with a map of destination position. Our goal in these experiments is twofold: first, we want to show that VIN models are far superior to standard prediction networks for prediction tasks that affect planning; second, we want to show that by embedding a VIN in an image processing network, challenging structured prediction tasks from real images can be successfully accomplished."}, {"heading": "4.1. Synthetic Grid-World Domain", "text": "In fact, it is a very rare disease, which is a very rare disease, which is usually a very rare disease."}, {"heading": "4.2. Mars Rover Navigation", "text": "In fact, it is in such a way that the majority of them are able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process,"}, {"heading": "5. Related Work", "text": "In this context, the transitions underlying the planning process are assumed to be known in the MDP, but the costs must be learned from the data. A similar approach is also used in inverse reinforcement learning (IRL) (Abbeel & Ng, 2004; Ziebart et al., 2008; Levine et al., 2011), and a similar approach is also used in inverse reinforcement learning (Abbeel & Ng, 2004; Ziebart et al., 2008; Levine et al., 2011)."}, {"heading": "6. Conclusion and Outlook", "text": "We have introduced the Value Iteration Network: a fully differentiated neural network with a \"planning module\" embedded in the network. VI networks are particularly well suited to making predictions about the results of a planning process, for example, the trajectory planning as examined in this paper. By creating a VI end-to-end network, we were able to learn predictions over the shortest possible distances directly from the raw images of an area, both on synthetic data and on real overhead images of a Mars terrain. Furthermore, we proposed a hierarchical variant of VI network that is better suited for larger areas. In this work, we focused on planning problems that have a two-dimensional structure in which the state can easily be mapped on images, and the transitions are local. While such problems have important applications in activity forecasting and path planning (Kitani et al., 2012; Ratliet al, 2009), the common usage structure of our problem is different."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["P. Abbeel", "A. Ng"], "venue": "In ICML,", "citeRegEx": "Abbeel and Ng,? \\Q2004\\E", "shortCiteRegEx": "Abbeel and Ng", "year": 2004}, {"title": "Predicting Structured Data (Neural Information Processing)", "author": ["Bakir", "G. H", "T. Hofmann", "B. Sch\u00f6lkopf", "A. Smola", "B. Taskar", "S. Vishwanathan"], "venue": null, "citeRegEx": "Bakir et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bakir et al\\.", "year": 2007}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio"], "venue": "NIPS Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Dynamic Programming", "author": ["R. Bellman"], "venue": null, "citeRegEx": "Bellman,? \\Q1957\\E", "shortCiteRegEx": "Bellman", "year": 1957}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and trends in Machine Learning,", "citeRegEx": "Bengio,? \\Q2009\\E", "shortCiteRegEx": "Bengio", "year": 2009}, {"title": "Dynamic Programming and Optimal Control, Vol II", "author": ["D. Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "Bertsekas,? \\Q2012\\E", "shortCiteRegEx": "Bertsekas", "year": 2012}, {"title": "Pattern Recognition and Machine Learning", "author": ["C. Bishop"], "venue": null, "citeRegEx": "Bishop,? \\Q2006\\E", "shortCiteRegEx": "Bishop", "year": 2006}, {"title": "Learning to search better than your teacher", "author": ["K. Chang", "A. Krishnamurthy", "A. Agarwal", "H. Daum\u00e9 III", "J. Langford"], "venue": "In ICML,", "citeRegEx": "Chang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2015}, {"title": "Learning hierarchical features for scene labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Farabet et al\\.,? \\Q1915\\E", "shortCiteRegEx": "Farabet et al\\.", "year": 1915}, {"title": "Dynamical system modulation for robot learning via kinesthetic demonstrations. Robotics", "author": ["M. Hersch", "F. Guenter", "S. Calinon", "A. Billard"], "venue": "IEEE Transactions on,", "citeRegEx": "Hersch et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hersch et al\\.", "year": 2008}, {"title": "Spatial transformer networks", "author": ["M. Jaderberg", "K. Simonyan", "A. Zisserman"], "venue": "In NIPS,", "citeRegEx": "Jaderberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2015}, {"title": "3d convolutional neural networks for human action recognition", "author": ["S. Ji", "W. Xu", "M. Yang", "K. Yu"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Ji et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Nonlinear inverse reinforcement learning with gaussian processes", "author": ["S. Levine", "Z. Popovic", "V. Koltun"], "venue": "In NIPS,", "citeRegEx": "Levine et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2011}, {"title": "Scaling up approximate value iteration with options: Better policies with fewer iterations", "author": ["T. Mann", "S. Mannor"], "venue": "In ICML,", "citeRegEx": "Mann and Mannor,? \\Q2014\\E", "shortCiteRegEx": "Mann and Mannor", "year": 2014}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Rusu", "J. Veness", "M. Bellemare", "A. Graves", "M. Riedmiller", "A. Fidjeland", "G Ostrovski"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Policy invariance under reward transformations: Theory and application to reward shaping", "author": ["A. Ng", "D. Harada", "S. Russell"], "venue": "In ICML,", "citeRegEx": "Ng et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Ng et al\\.", "year": 1999}, {"title": "Structured learning and prediction in computer vision", "author": ["S. Nowozin", "C. Lampert"], "venue": "Foundations and Trends in Computer Graphics and Vision,", "citeRegEx": "Nowozin and Lampert,? \\Q2011\\E", "shortCiteRegEx": "Nowozin and Lampert", "year": 2011}, {"title": "Actionconditional video prediction using deep networks in atari games", "author": ["J. Oh", "X. Guo", "H. Lee", "R. Lewis", "S. Singh"], "venue": "In NIPS,", "citeRegEx": "Oh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2015}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "arXiv preprint arXiv:1211.5063,", "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Maximum margin planning", "author": ["N. Ratliff", "A. Bagnell", "M. Zinkevich"], "venue": "In ICML,", "citeRegEx": "Ratliff et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2006}, {"title": "Learning to search: Functional gradient techniques for imitation learning", "author": ["N. Ratliff", "D. Silver", "A. Bagnell"], "venue": "Autonomous Robots,", "citeRegEx": "Ratliff et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2009}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["S. Ross", "G. Gordon", "A. Bagnell"], "venue": "In AISTATS,", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Trust region policy optimization", "author": ["J. Schulman", "S. Levine", "P. Abbeel", "M. Jordan", "P. Moritz"], "venue": "In ICML,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Intrinsically motivated reinforcement learning: An evolutionary perspective", "author": ["S. Singh", "R. Lewis", "A. Barto", "J. Sorg"], "venue": "IEEE Transactions on Autonomous Mental Development,", "citeRegEx": "Singh et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2010}, {"title": "Robobarista: Object partbased transfer of manipulation trajectories from crowdsourcing in 3d pointclouds", "author": ["J. Sung", "S. Jin", "A. Saxena"], "venue": "In International Symposium on Robotics Research (ISRR),", "citeRegEx": "Sung et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sung et al\\.", "year": 2015}, {"title": "Bounding performance loss in approximate MDP homomorphisms", "author": ["J. Taylor", "D. Precup", "P. Panagaden"], "venue": "In NIPS,", "citeRegEx": "Taylor et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Taylor et al\\.", "year": 2009}, {"title": "Lecture 6.5\u2014RmsProp: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton", "year": 2012}, {"title": "Learning simple algorithms from examples", "author": ["W. Zaremba", "T. Mikolov", "A. Joulin", "R. Fergus"], "venue": "arXiv preprint arXiv:1511.07275,", "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["B. Ziebart", "A. Maas", "A. Bagnell", "A. Dey"], "venue": "In AAAI,", "citeRegEx": "Ziebart et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ziebart et al\\.", "year": 2008}, {"title": "Reinforcement planning: RL for optimal planners", "author": ["M. Zucker", "A. Bagnell"], "venue": "In International Conference on Robotics and Automation (ICRA),", "citeRegEx": "Zucker and Bagnell,? \\Q2012\\E", "shortCiteRegEx": "Zucker and Bagnell", "year": 2012}], "referenceMentions": [{"referenceID": 12, "context": "In recent years, deep convolutional neural networks (CNNs) have shown remarkable successes in computer vision tasks such as object recognition, action recognition, and scene labeling (Krizhevsky et al., 2012; Farabet et al., 2013; Ji et al., 2013).", "startOffset": 183, "endOffset": 247}, {"referenceID": 11, "context": "In recent years, deep convolutional neural networks (CNNs) have shown remarkable successes in computer vision tasks such as object recognition, action recognition, and scene labeling (Krizhevsky et al., 2012; Farabet et al., 2013; Ji et al., 2013).", "startOffset": 183, "endOffset": 247}, {"referenceID": 1, "context": "(Bakir et al., 2007; Nowozin & Lampert, 2011) are important for learning from demonstration in robotics (Hersch et al.", "startOffset": 0, "endOffset": 45}, {"referenceID": 9, "context": ", 2007; Nowozin & Lampert, 2011) are important for learning from demonstration in robotics (Hersch et al., 2008; Ratliff et al., 2009; Sung et al., 2015), activity forcasting (Kitani et al.", "startOffset": 91, "endOffset": 153}, {"referenceID": 22, "context": ", 2007; Nowozin & Lampert, 2011) are important for learning from demonstration in robotics (Hersch et al., 2008; Ratliff et al., 2009; Sung et al., 2015), activity forcasting (Kitani et al.", "startOffset": 91, "endOffset": 153}, {"referenceID": 26, "context": ", 2007; Nowozin & Lampert, 2011) are important for learning from demonstration in robotics (Hersch et al., 2008; Ratliff et al., 2009; Sung et al., 2015), activity forcasting (Kitani et al.", "startOffset": 91, "endOffset": 153}, {"referenceID": 7, "context": ", 2012), and natural language processing (Chang et al., 2015).", "startOffset": 41, "endOffset": 61}, {"referenceID": 3, "context": "The key to our approach is an observation that the classic value-iteration (VI) planning algorithm (Bellman, 1957; Bertsekas, 2012) may be represented by a specific type of convolutional neural network.", "startOffset": 99, "endOffset": 131}, {"referenceID": 5, "context": "The key to our approach is an observation that the classic value-iteration (VI) planning algorithm (Bellman, 1957; Bertsekas, 2012) may be represented by a specific type of convolutional neural network.", "startOffset": 99, "endOffset": 131}, {"referenceID": 21, "context": "Previous approaches to the structured-prediction problem, such as inverse reinforcement learning (IRL) address the underlying planning process in the learning algorithm (Abbeel & Ng, 2004; Ratliff et al., 2006; Ziebart et al., 2008; Ross et al., 2011).", "startOffset": 169, "endOffset": 251}, {"referenceID": 30, "context": "Previous approaches to the structured-prediction problem, such as inverse reinforcement learning (IRL) address the underlying planning process in the learning algorithm (Abbeel & Ng, 2004; Ratliff et al., 2006; Ziebart et al., 2008; Ross et al., 2011).", "startOffset": 169, "endOffset": 251}, {"referenceID": 23, "context": "Previous approaches to the structured-prediction problem, such as inverse reinforcement learning (IRL) address the underlying planning process in the learning algorithm (Abbeel & Ng, 2004; Ratliff et al., 2006; Ziebart et al., 2008; Ross et al., 2011).", "startOffset": 169, "endOffset": 251}, {"referenceID": 21, "context": "Previous approaches to the structured-prediction problem, such as inverse reinforcement learning (IRL) address the underlying planning process in the learning algorithm (Abbeel & Ng, 2004; Ratliff et al., 2006; Ziebart et al., 2008; Ross et al., 2011). Our contribution is therefore complimentary to these methods, and can be easily incorporated within more advanced learning algorithms, such as those proposed by Ross et al. (2011). We further discuss related work in Section 5.", "startOffset": 189, "endOffset": 433}, {"referenceID": 3, "context": "A standard model for sequential decision making and planning is the Markov decision process (MDP) (Bellman, 1957; Bertsekas, 2012).", "startOffset": 98, "endOffset": 130}, {"referenceID": 5, "context": "A standard model for sequential decision making and planning is the Markov decision process (MDP) (Bellman, 1957; Bertsekas, 2012).", "startOffset": 98, "endOffset": 130}, {"referenceID": 5, "context": "In practice, VI can only be applied for a finite number of iterations, yielding an approximate solution, for which error bounds can be derived (Bertsekas, 2012).", "startOffset": 143, "endOffset": 160}, {"referenceID": 13, "context": "Convolutional neural networks (CNNs; LeCun et al. 1998; Bengio 2009) are neural networks with a particular architecture that has proved useful for computer vision, among other domains.", "startOffset": 30, "endOffset": 68}, {"referenceID": 4, "context": "Convolutional neural networks (CNNs; LeCun et al. 1998; Bengio 2009) are neural networks with a particular architecture that has proved useful for computer vision, among other domains.", "startOffset": 30, "endOffset": 68}, {"referenceID": 1, "context": "We consider a structured prediction (Bakir et al., 2007; Nowozin & Lampert, 2011) problem in which the input consists of a dataset of images { x }", "startOffset": 36, "endOffset": 81}, {"referenceID": 6, "context": "Na\u0131\u0308vely, this problem can be cast as a standard supervised learning problem (Bishop, 2006), by concatenating x and s into a single observation.", "startOffset": 77, "endOffset": 91}, {"referenceID": 10, "context": "Our approach can be extended to general state spaces by adding a transformation layer between the state and the value function (Jaderberg et al., 2015).", "startOffset": 127, "endOffset": 151}, {"referenceID": 17, "context": "In this sense, the the network is learning a reward shaping (Ng et al., 1999) from data.", "startOffset": 60, "endOffset": 77}, {"referenceID": 20, "context": "The difficulty of training deep recurrent models such as the VI block described above increases with the number of VI iterationsK (Pascanu et al., 2012).", "startOffset": 130, "endOffset": 152}, {"referenceID": 17, "context": "In the planning literature, several studies investigated the use of reward-shaping (Ng et al., 1999), hierarchical macro-actions (Mann & Mannor, 2014), and state-aggregation (Taylor et al.", "startOffset": 83, "endOffset": 100}, {"referenceID": 27, "context": ", 1999), hierarchical macro-actions (Mann & Mannor, 2014), and state-aggregation (Taylor et al., 2009; Bertsekas, 2012) for reducing the number of iterations required for VI to converge, with the goal of reducing computation time.", "startOffset": 81, "endOffset": 119}, {"referenceID": 5, "context": ", 1999), hierarchical macro-actions (Mann & Mannor, 2014), and state-aggregation (Taylor et al., 2009; Bertsekas, 2012) for reducing the number of iterations required for VI to converge, with the goal of reducing computation time.", "startOffset": 81, "endOffset": 119}, {"referenceID": 2, "context": "We trained several neural-network based multi-class logistic regression classifiers using stochastic gradient descent, with an RMSProp step size (Tieleman & Hinton, 2012), implemented in the Theano (Bastien et al., 2012) library.", "startOffset": 198, "endOffset": 220}, {"referenceID": 1, "context": "Learning to predict trajectories is an instance of structured prediction problems (Bakir et al., 2007; Nowozin & Lampert, 2011).", "startOffset": 82, "endOffset": 127}, {"referenceID": 21, "context": "In particular, structured prediction strategies such as maximum-margin prediction (Ratliff et al., 2006) have been applied to imitation learning of trajectories from demonstrations.", "startOffset": 82, "endOffset": 104}, {"referenceID": 30, "context": "A similar approach is also used in inverse reinforcement learning (IRL) (Abbeel & Ng, 2004; Ziebart et al., 2008; Levine et al., 2011), which has been used for similar domains of predicting trajectories from overhead images as in this work (Kitani et al.", "startOffset": 72, "endOffset": 134}, {"referenceID": 14, "context": "A similar approach is also used in inverse reinforcement learning (IRL) (Abbeel & Ng, 2004; Ziebart et al., 2008; Levine et al., 2011), which has been used for similar domains of predicting trajectories from overhead images as in this work (Kitani et al.", "startOffset": 72, "endOffset": 134}, {"referenceID": 22, "context": ", 2011), which has been used for similar domains of predicting trajectories from overhead images as in this work (Kitani et al., 2012; Ratliff et al., 2009).", "startOffset": 113, "endOffset": 156}, {"referenceID": 23, "context": "However, our approach can be easily incorporated within a no-regret structured prediction method such DAgger (Ross et al., 2011) to potentially yield improved results.", "startOffset": 109, "endOffset": 128}, {"referenceID": 25, "context": "In a different study, Singh et al. (2010) used evolutionary algorithms to learn a cost function that accelerates learning.", "startOffset": 22, "endOffset": 42}, {"referenceID": 29, "context": ", 2015), and the recurrent networks of Zaremba et al. (2015). Our approach can be seen as a neural-network approximation of dynamic programming (VI), learned from examples.", "startOffset": 39, "endOffset": 61}, {"referenceID": 22, "context": "While such problems have important applications in activity forecasting and path-planning (Kitani et al., 2012; Ratliff et al., 2009), generalizing our approach to different problem structures is important.", "startOffset": 90, "endOffset": 133}, {"referenceID": 10, "context": "First, the mapping from the state input to the shaped-reward map can be generalized by adding a spatial transformation layer (Jaderberg et al., 2015) between them.", "startOffset": 125, "endOffset": 149}, {"referenceID": 19, "context": "conditioning the kernels on the input (Oh et al., 2015) may provide an additional flexibility.", "startOffset": 38, "endOffset": 55}, {"referenceID": 16, "context": "Recent RL advances (Mnih et al., 2015; Schulman et al., 2015) showed that CNN-based policies can be trained successfully in a RL setting.", "startOffset": 19, "endOffset": 61}, {"referenceID": 24, "context": "Recent RL advances (Mnih et al., 2015; Schulman et al., 2015) showed that CNN-based policies can be trained successfully in a RL setting.", "startOffset": 19, "endOffset": 61}], "year": 2016, "abstractText": "We introduce the value iteration network: a fully differentiable neural network with a \u2018planning module\u2019 embedded within. Value iteration networks are suitable for making predictions about outcomes that involve planning-based reasoning, such as predicting a desired trajectory from an observation of a map. Key to our approach is a novel differentiable approximation of the valueiteration algorithm, which can be represented as a convolutional neural network, and trained endto-end using standard backpropagation. We evaluate our value iteration networks on the task of predicting optimal obstacle-avoiding trajectories from an image of a landscape, both on synthetic data, and on challenging raw images of the Mars terrain.", "creator": "LaTeX with hyperref package"}}}