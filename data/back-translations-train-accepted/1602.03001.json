{"id": "1602.03001", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2016", "title": "A Convolutional Attention Network for Extreme Summarization of Source Code", "abstract": "Attention mechanisms in neural networks have proved useful for problems in which the input and output do not have fixed dimension. Often there exist features that are locally translation invariant and would be valuable for directing the model's attention, but previous attentional architectures are not constructed to learn such features specifically. We introduce an attentional neural network that employs convolution on the input tokens to detect local time-invariant and long-range topical attention features in a context-dependent way. We apply this architecture to the problem of extreme summarization of source code snippets into short, descriptive function name-like summaries. Using those features, the model sequentially generates a summary by marginalizing over two attention mechanisms: one that predicts the next summary token based on the attention weights of the input tokens and another that is able to copy a code token as-is directly into the summary. We demonstrate our convolutional attention neural network's performance on 10 popular Java projects showing that it achieves better performance compared to previous attentional mechanisms.", "histories": [["v1", "Tue, 9 Feb 2016 14:36:49 GMT  (57kb)", "http://arxiv.org/abs/1602.03001v1", null], ["v2", "Wed, 25 May 2016 12:18:28 GMT  (67kb)", "http://arxiv.org/abs/1602.03001v2", "Code, data and visualization atthis http URL"]], "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.SE", "authors": ["miltiadis allamanis", "hao peng", "charles a sutton"], "accepted": true, "id": "1602.03001"}, "pdf": {"name": "1602.03001.pdf", "metadata": {"source": "META", "title": "A Convolutional Attention Network  for Extreme Summarization of Source Code", "authors": ["Miltiadis Allamanis", "Hao Peng"], "emails": ["M.ALLAMANIS@ED.AC.UK", "PENGHAO.PKU@GMAIL.COM", "CSUTTON@INF.ED.AC.UK"], "sections": [{"heading": null, "text": "ar Xiv: 160 2.03 001v 1 [cs.L G] 9F eb"}, {"heading": "1. Introduction", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before in the history of the city."}, {"heading": "2. Convolutional Attention Model", "text": "Our Convolutionary Attention Model receives as input a sequence of code subtokens c = [c < S >, c1,.. cN, c < / S >] and prints an extreme summary in the form of a concise method name.The summary is a sequence of subtokens m = [< S >, try, {, return, render, request,...], while the target output is m = [, should, render,].The neural network predicts each combined subtoken one after the other, and models P (mt | m,.) and P (mt \u2212 1, c) become dependent on the previously generated configuration m,."}, {"heading": "2.1. Learning Attention Features", "text": "The basic construction of our model is a Convolutionary Network (LeCun et al., 1990), which we have described in the following pseudo code: Attention, which we have described with the following pseudo codes: Attention, Attention, Attention, Attention, Attention. (D), Attention. (D), Attention. (D), Attention. (D), Attention. (D), Attention. (D), Attention. (D), Attention. (D), Attention. (D), Attention. (D), Attention. (D), Attention. (D), Attention. (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D)."}, {"heading": "2.2. Simple Convolutional Attentional Model", "text": "We now use the components described above as our building block for our extreme summary model. We first build conv _ attention, a revolutionary attention model that uses an attention vector \u03b1, which is calculated from attention weights, to weight the embedded tokens in c and calculate the predicted target weight. The model provides a probability distribution n R | V | across all possible target weights. We train this model by using the maximum probability. This model works as follows: We start with the special m0 = \"subtoken\" and h0 = \"recreated.\" Then we successively create each subtoken mt at any point in time by calling conht _ attention."}, {"heading": "2.3. Copy Convolutional Attentional Model", "text": "We extend conv _ attention by using an additional attention vector \u0394 in a novel copying mechanism, although we can extend conv _ attention and allow a direct copy from the input sequence c to the summary. In our data, a significant proportion of the output subtoken (about 35%) also appears in c. Motivated by this circumstance, we copy conv _ attention and allow a direct copy from the input sequence c into the summary. Now, when predicting mt, the neural network most likely copies a token from c to mt and with probability 1 \u2212 \u03bb copies the target subtoken as in conv _ attention. When copying a subtoken ci, the subtoken capability is likely to be copied equal to the attention weight. \u2212 This process is described by the following pseudocode: copy _ attention (code c, previous state ht \u2212 1) Lfeat. Attention _ features (c, ht \u2212 1) \u03b1 Attention _ weights (Lfeat, Katt)."}, {"heading": "2.4. Predicting Names", "text": "To predict a full method name, we use a hybrid SFSO and a beam search method. We start all predictions with the specific m0 subtoken, while maintaining a (max-) pile of the most likely sub-predictions to date. At each step, we select the most likely prediction and predict their next possible subtoken and push it back onto the pile. If the specific subtoken is created, the proposal is moved to the list of proposals. As we are interested in creating the list of top-k suggestions, we trim sub-suggestions at each point that have a probability of less than the currently best kth-full proposal. To make this process comprehensible, we limit the size of the subsuggestion pile and end iteration after 100 steps."}, {"heading": "3. Evaluation", "text": "We are interested in the extreme summary of the problem, in which we summarize the real source code relationships so that we can turn each individual source code snippet into a short and concise method. Although such a dataset does not exist for arbitrary source code comparisons, it is only natural to take into account existing methods and methods selected by the developers to capture a good quality of the datasets, we have selected 11 open source projects found on GitHub. We have the most popular projects by using the sum of Z-scores from the number of observers and forks of the individual projects using GHousios & Spinellis, 2012). We have selected the top 11 projects, each containing more than 10MB of the source code files and using libgdx as a development set. These projects have thousands of forks and stars widely known among software developers. Projects along short descriptions are shown in Table 1."}, {"heading": "3.1. Quantitative Evaluation", "text": "In fact, the tf-idf algorithm seems to work very well in this domain, while conv _ attention and copy _ attention perform best; the copy mechanism gives a good F1 improvement in rank 1 and a greater improvement in rank 5. Although our Convolutionary Attention Models have an exact match with tf-idf, they achieve a much higher precision than all other algorithms. These differences in data characteristics could be the cause of the low performance achieved by the Bahdanau et al model."}, {"heading": "3.2. Qualitative Evaluation", "text": "Figure 2 shows a visualization of a small method that illustrates how copy _ attention typically works. In the first step, it focuses its attention on the entire method and decidedly on the first subtoken. In a large number of cases, this includes subtokens such as get, set, is, create, etc. In the next steps, the copy mechanism is very confident (\u03bb = 0.97 in Figure 2) and sequentially copies the correct subtokens from the code snippet into the name. We note that in many examples, the copy mechanism tends to have a much more focused attention vector compared to the attention vector. Presumably, this is due to the different training signals of the attention mechanisms. A second example of copy _ attention can be seen in Figure 3. Although this is a relatively short method, it illustrates how the model has learned both time-consuming features and topical features."}, {"heading": "4. Related Work", "text": "Closely related to this is the work of Denil et al. (2014), which learns representations of documents through convolution but uses network activations to summarize a document rather than an attentive model. Rush et al. (2015) use an attention-based encoder to summarize sentences but do not use confusion for their attention mechanisms. Our work also refers to other work on attention mechanisms for text (Hermann et al., 2015) and images (Xu et al., 2015; Mnih et al., al., 2014), which do not use confusion for their attention mechanisms. To all who refer to other work on attention mechanisms for text (Hermann et al., 2015) and images (Xu et al., 2015; Mnih et al., 2014), which do not use confusion for attention values."}, {"heading": "5. Discussion & Conclusions", "text": "Modelling and understanding source code artifacts through machine learning can have a direct impact on software development. The problem of extreme code summarization is a first step toward the more general goal of developing machine learning representations of source code that allow machine learning methods to think more likely about code, leading to useful software engineering tools that support code construction and maintenance. Moreover, source code - and its derived artifacts - represents a new machine learning modality with very different characteristics compared to images and natural language. Therefore, source code models require exploration of new methods that may have interesting parallels to images and natural language. This work is a step in this direction: Our Neural Convolutionary Attention Model attempts to \"understand\" the highly structured source code text by learning both wide-ranging features and localized patterns, thereby achieving the best performance among competing source code methods."}, {"heading": "Acknowledgements", "text": "This work was supported by Microsoft Research through its PhD Fellowship Program. Charles Sutton was supported by the Research Council for Engineering and Physics Sciences [grant number EP / K024043 / 1]. We thank Daniel Tarlow and Krzysztof Geras for their insightful comments and suggestions."}], "references": [{"title": "Learning natural coding conventions", "author": ["Allamanis", "Miltiadis", "Barr", "Earl T", "Bird", "Christian", "Sutton", "Charles"], "venue": "In Symposium on the Foundations of Software Engineering (FSE),", "citeRegEx": "Allamanis et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Allamanis et al\\.", "year": 2014}, {"title": "Suggesting accurate method and class names", "author": ["Allamanis", "Miltiadis", "Barr", "Earl T", "Bird", "Christian", "Sutton", "Charles"], "venue": "In Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering", "citeRegEx": "Allamanis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Allamanis et al\\.", "year": 2015}, {"title": "Bimodal modelling of source code and natural language", "author": ["Allamanis", "Miltiadis", "Tarlow", "Daniel", "Gordon", "Andrew", "Wei", "Yi"], "venue": "In ICML,", "citeRegEx": "Allamanis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Allamanis et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Bengio", "Samy", "Vinyals", "Oriol", "Jaitly", "Navdeep", "Shazeer", "Noam"], "venue": "arXiv preprint arXiv:1506.03099,", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "The impact of identifier style on effort and comprehension", "author": ["Binkley", "Dave", "Davis", "Marcia", "Lawrie", "Dawn", "Maletic", "Jonathan I", "Morrell", "Christopher", "Sharif", "Bonita"], "venue": "Empirical Software Engineering,", "citeRegEx": "Binkley et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Binkley et al\\.", "year": 2013}, {"title": "A convolutional neural network for modelling sentences", "author": ["Blunsom", "Phil", "Grefenstette", "Edward", "Kalchbrenner", "Nal"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Blunsom et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Blunsom et al\\.", "year": 2014}, {"title": "On the properties of neural machine translation: Encoder\u2013decoder approaches. Syntax, Semantics and Structure in Statistical Translation", "author": ["Cho", "Kyunghyun", "van Merri\u00ebnboer", "Bart", "Bahdanau", "Dzmitry", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["Collobert", "Ronan", "Weston", "Jason"], "venue": "In ICML,", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Modelling, visualising and summarising documents with a single convolutional neural network", "author": ["Denil", "Misha", "Demiraj", "Alban", "Kalchbrenner", "Nal", "Blunsom", "Phil", "de Freitas", "Nando"], "venue": "arXiv preprint arXiv:1406.3830,", "citeRegEx": "Denil et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2014}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer", "Chris", "Ballesteros", "Miguel", "Ling", "Wang", "Matthews", "Austin", "Smith", "Noah A"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "GHTorrent: Github\u2019s data from a firehose", "author": ["Gousios", "Georgios", "Spinellis", "Diomidis"], "venue": "In Mining Software Repositories (MSR),", "citeRegEx": "Gousios et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gousios et al\\.", "year": 2012}, {"title": "Neural turing machines", "author": ["Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Learning to transduce with unbounded memory", "author": ["Grefenstette", "Edward", "Hermann", "Karl Moritz", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "In NIPS,", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Delving deep into rectifiers: surpassing humanlevel performance on ImageNet classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "arXiv preprint arXiv:1502.01852,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["Hermann", "Karl Moritz", "Kocisky", "Tomas", "Grefenstette", "Edward", "Espeholt", "Lasse", "Kay", "Will", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "In NIPS,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "On the naturalness of software", "author": ["Hindle", "Abram", "Barr", "Earl T", "Su", "Zhendong", "Gabel", "Mark", "Devanbu", "Premkumar"], "venue": "In International Conference on Software Engineering (ICSE),", "citeRegEx": "Hindle et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hindle et al\\.", "year": 2012}, {"title": "Neural networks for machine learning. Online Course at coursera", "author": ["Hinton", "Geoffrey", "Srivastava", "Nitsh", "Swersky", "Kevin"], "venue": "org, Lecture,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Joulin", "Armand", "Mikolov", "Tomas"], "venue": "arXiv preprint arXiv:1503.01007,", "citeRegEx": "Joulin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2015}, {"title": "Phrase-based statistical translation of programming languages", "author": ["Karaivanov", "Svetoslav", "Raychev", "Veselin", "Vechev", "Martin"], "venue": "In Proceedings of the 2014 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming & Software. ACM,", "citeRegEx": "Karaivanov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karaivanov et al\\.", "year": 2014}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Nips. chapter Handwritten Digit Recognition with a Back-propagation Network", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "R.E. Howard", "W. Habbard", "L.D. Jackel", "D. Henderson"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1990\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1990}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "Leon", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Cognitive perspectives on the role of naming in computer programs", "author": ["Liblit", "Ben", "Begel", "Andrew", "Sweetser", "Eve"], "venue": "In Proceedings of the 18th Annual Psychology of Programming Workshop,", "citeRegEx": "Liblit et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Liblit et al\\.", "year": 2006}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["Maas", "Andrew L", "Hannun", "Awni Y", "Ng", "Andrew Y"], "venue": "In ICML Workshop on Deep Learning for Audio, Speech, and Language Processing (WDLASL),", "citeRegEx": "Maas et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2013}, {"title": "Structured generative models of natural source code", "author": ["Maddison", "Chris", "Tarlow", "Daniel"], "venue": "In ICML,", "citeRegEx": "Maddison et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Maddison et al\\.", "year": 2014}, {"title": "Recurrent models of visual attention", "author": ["Mnih", "Volodymyr", "Heess", "Nicolas", "Graves", "Alex"], "venue": "In NIPS,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Convolutional neural networks over tree structures for programming language processing", "author": ["Mou", "Lili", "Li", "Ge", "Zhang", "Lu", "Wang", "Tao", "Jin", "Zhi"], "venue": "In Proceedings of the 30th AAAI Conference on Artificial Intelligence (to appear),", "citeRegEx": "Mou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2016}, {"title": "Natural language models for predicting programming comments", "author": ["Movshovitz-Attias", "Dana", "WW Cohen", "W. Cohen", "William"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "Movshovitz.Attias et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Movshovitz.Attias et al\\.", "year": 2013}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["Neelakantan", "Arvind", "Le", "Quoc V", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1511.04834,", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Migrating code with statistical machine translation", "author": ["Nguyen", "Anh Tuan", "Tung Thanh", "Tien N"], "venue": "In Companion Proceedings of the 36th International Conference on Software Engineering", "citeRegEx": "Nguyen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2014}, {"title": "Learning to generate pseudo-code from source code using statistical machine translation", "author": ["Oda", "Yusuke", "Fudaba", "Hiroyuki", "Neubig", "Graham", "Hata", "Hideaki", "Sakti", "Sakriani", "Toda", "Tomoki", "Nakamura", "Satoshi"], "venue": "In Automated Software Engineering (ASE),", "citeRegEx": "Oda et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Oda et al\\.", "year": 2015}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Papineni", "Kishore", "Roukos", "Salim", "Ward", "Todd", "Zhu", "Wei-Jing"], "venue": "In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Learning program embeddings to propagate feedback on student code", "author": ["Piech", "Chris", "Huang", "Jonathan", "Nguyen", "Andy", "Phulsuksombati", "Mike", "Sahami", "Mehran", "Guibas", "Leonidas J"], "venue": "In ICML,", "citeRegEx": "Piech et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Piech et al\\.", "year": 2015}, {"title": "Predicting program properties from \u201cbig code", "author": ["Raychev", "Veselin", "Vechev", "Martin", "Krause", "Andreas"], "venue": "In Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages", "citeRegEx": "Raychev et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Raychev et al\\.", "year": 2015}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Rush", "Alexander M", "Chopra", "Sumit", "Weston", "Jason"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["Snoek", "Jasper", "Larochelle", "Hugo", "Adams", "Ryan P"], "venue": "In NIPS,", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey E", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Sutskever", "Ilya", "Martens", "James", "Dahl", "George", "Hinton", "Geoffrey"], "venue": "In ICML,", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "The effects of comments and identifier names on program comprehensibility: an experimental investigation", "author": ["Takang", "Armstrong A", "Grubb", "Penny A", "Macredie", "Robert D"], "venue": "J. Prog. Lang.,", "citeRegEx": "Takang et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Takang et al\\.", "year": 1996}, {"title": "On the localness of software", "author": ["Tu", "Zhaopeng", "Su", "Zhendong", "Devanbu", "Premkumar"], "venue": "In Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering", "citeRegEx": "Tu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2014}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu", "Kelvin", "Ba", "Jimmy", "Kiros", "Ryan", "Cho", "Kyunghyun", "Courville", "Aaron C", "Salakhutdinov", "Ruslan", "Zemel", "Richard S", "Bengio", "Yoshua"], "venue": "In ICML,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Learning to execute", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1410.4615,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 26, "context": "Recent research has tackled these problems using neural models of attention (Mnih et al., 2014), which have had great recent successes in machine translation (Bahdanau et al.", "startOffset": 76, "endOffset": 95}, {"referenceID": 3, "context": ", 2014), which have had great recent successes in machine translation (Bahdanau et al., 2015) and image captioning (Xu et al.", "startOffset": 70, "endOffset": 93}, {"referenceID": 41, "context": ", 2015) and image captioning (Xu et al., 2015).", "startOffset": 29, "endOffset": 46}, {"referenceID": 22, "context": "Convolutional models are a natural choice for learning translation-invariant features while using only a small number of parameters and for this reason have been highly successful in non-attentional models for images (LeCun et al., 1998; Krizhevsky et al., 2012) and text classification (Blunsom et al.", "startOffset": 217, "endOffset": 262}, {"referenceID": 20, "context": "Convolutional models are a natural choice for learning translation-invariant features while using only a small number of parameters and for this reason have been highly successful in non-attentional models for images (LeCun et al., 1998; Krizhevsky et al., 2012) and text classification (Blunsom et al.", "startOffset": 217, "endOffset": 262}, {"referenceID": 6, "context": ", 2012) and text classification (Blunsom et al., 2014).", "startOffset": 32, "endOffset": 54}, {"referenceID": 23, "context": "For these reasons, software engineering research has found that good names are important to developers (Liblit et al., 2006; Takang et al., 1996; Binkley et al., 2013).", "startOffset": 103, "endOffset": 167}, {"referenceID": 39, "context": "For these reasons, software engineering research has found that good names are important to developers (Liblit et al., 2006; Takang et al., 1996; Binkley et al., 2013).", "startOffset": 103, "endOffset": 167}, {"referenceID": 5, "context": "For these reasons, software engineering research has found that good names are important to developers (Liblit et al., 2006; Takang et al., 1996; Binkley et al., 2013).", "startOffset": 103, "endOffset": 167}, {"referenceID": 21, "context": "The basic building block of our model is a convolutional network (LeCun et al., 1990; Collobert & Weston, 2008) for extracting position and context dependent features.", "startOffset": 65, "endOffset": 111}, {"referenceID": 7, "context": "The state is passed through the vector ht \u2208 R2 which is computed using a Gated Recurrent Unit (Cho et al., 2014) i.", "startOffset": 94, "endOffset": 112}, {"referenceID": 4, "context": "For regularization during training, we use a trick similar to Bengio et al. (2015) and with probability equal to the dropout rate we compute the next state as ht = GRU(n\u0302,ht\u22121), where n\u0302 is the predicted embedding.", "startOffset": 62, "endOffset": 83}, {"referenceID": 32, "context": "Using the BLEU score (Papineni et al., 2002) would have been possible, but it would not be different from F1 given the short lengths of our output sequences (3 on average).", "startOffset": 21, "endOffset": 44}, {"referenceID": 36, "context": "We perform hyperparameter optimizations using Bayesian optimization with spearmint (Snoek et al., 2012) maximizing the F1 at rank 5.", "startOffset": 83, "endOffset": 103}, {"referenceID": 3, "context": "We also use the standard attention model of Bahdanau et al. (2015) that uses a biRNN and fully connected components, that has been successfully used in machine translation.", "startOffset": 44, "endOffset": 67}, {"referenceID": 38, "context": "To train conv_attention and copy_attention we optimize the objective using stochastic gradient descent with RMSProp and Nesterov momentum (Sutskever et al., 2013; Hinton et al., 2012).", "startOffset": 138, "endOffset": 183}, {"referenceID": 17, "context": "To train conv_attention and copy_attention we optimize the objective using stochastic gradient descent with RMSProp and Nesterov momentum (Sutskever et al., 2013; Hinton et al., 2012).", "startOffset": 138, "endOffset": 183}, {"referenceID": 37, "context": "We use dropout (Srivastava et al., 2014) on all parameters, parametric leaky RELUs (Maas et al.", "startOffset": 15, "endOffset": 40}, {"referenceID": 24, "context": ", 2014) on all parameters, parametric leaky RELUs (Maas et al., 2013; He et al., 2015) and gradient clipping.", "startOffset": 50, "endOffset": 86}, {"referenceID": 14, "context": ", 2014) on all parameters, parametric leaky RELUs (Maas et al., 2013; He et al., 2015) and gradient clipping.", "startOffset": 50, "endOffset": 86}, {"referenceID": 3, "context": "\u201cStandard Attention\u201d refers to the machine translation model of Bahdanau et al. (2015). The tf-idf algorithm seems to be performing very well, showing that the bag-of-words representation of the input code is a strong indicator of its name.", "startOffset": 64, "endOffset": 87}, {"referenceID": 3, "context": "These differences in the data characteristics could be the cause of the low performance achieved by the model of Bahdanau et al. (2015). Although source code snippets resemble natural language sentences, they are more structured, much longer and vary greatly in length.", "startOffset": 113, "endOffset": 136}, {"referenceID": 3, "context": "Standard attention refers to the model of Bahdanau et al. (2015).", "startOffset": 42, "endOffset": 65}, {"referenceID": 3, "context": "Standard Attention refers to the work of Bahdanau et al. (2015).", "startOffset": 41, "endOffset": 64}, {"referenceID": 3, "context": "The difference of the performance between the copy_attention and the standard attention model of Bahdanau et al. (2015) raises an interesting question.", "startOffset": 97, "endOffset": 120}, {"referenceID": 20, "context": "Convolutional neural networks have been used for image classification with great success (Krizhevsky et al., 2012; Szegedy et al., 2015; LeCun et al., 1990; 1998).", "startOffset": 89, "endOffset": 162}, {"referenceID": 21, "context": "Convolutional neural networks have been used for image classification with great success (Krizhevsky et al., 2012; Szegedy et al., 2015; LeCun et al., 1990; 1998).", "startOffset": 89, "endOffset": 162}, {"referenceID": 6, "context": "More related to this work is the use of convolutional neural networks for text classification (Blunsom et al., 2014).", "startOffset": 94, "endOffset": 116}, {"referenceID": 15, "context": "Our work is also related to other work in attention mechanisms for text (Hermann et al., 2015) and images (Xu et al.", "startOffset": 72, "endOffset": 94}, {"referenceID": 41, "context": ", 2015) and images (Xu et al., 2015; Mnih et al., 2014) that does not use convolution to provide the attention values.", "startOffset": 19, "endOffset": 55}, {"referenceID": 26, "context": ", 2015) and images (Xu et al., 2015; Mnih et al., 2014) that does not use convolution to provide the attention values.", "startOffset": 19, "endOffset": 55}, {"referenceID": 12, "context": "Finally, distantly related to this work is research on creating neural network architectures that learn code-like behaviors (Graves et al., 2014; Zaremba & Sutskever, 2014; Joulin & Mikolov, 2015; Grefenstette et al., 2015; Dyer et al., 2015; Reed & de Freitas, 2015; Neelakantan et al., 2015).", "startOffset": 124, "endOffset": 293}, {"referenceID": 13, "context": "Finally, distantly related to this work is research on creating neural network architectures that learn code-like behaviors (Graves et al., 2014; Zaremba & Sutskever, 2014; Joulin & Mikolov, 2015; Grefenstette et al., 2015; Dyer et al., 2015; Reed & de Freitas, 2015; Neelakantan et al., 2015).", "startOffset": 124, "endOffset": 293}, {"referenceID": 10, "context": "Finally, distantly related to this work is research on creating neural network architectures that learn code-like behaviors (Graves et al., 2014; Zaremba & Sutskever, 2014; Joulin & Mikolov, 2015; Grefenstette et al., 2015; Dyer et al., 2015; Reed & de Freitas, 2015; Neelakantan et al., 2015).", "startOffset": 124, "endOffset": 293}, {"referenceID": 29, "context": "Finally, distantly related to this work is research on creating neural network architectures that learn code-like behaviors (Graves et al., 2014; Zaremba & Sutskever, 2014; Joulin & Mikolov, 2015; Grefenstette et al., 2015; Dyer et al., 2015; Reed & de Freitas, 2015; Neelakantan et al., 2015).", "startOffset": 124, "endOffset": 293}, {"referenceID": 6, "context": "More related to this work is the use of convolutional neural networks for text classification (Blunsom et al., 2014). Closely related is the work of Denil et al. (2014) that learns representations of documents using convolution but uses the network activations to summarize a document rather than an attentional model.", "startOffset": 95, "endOffset": 169}, {"referenceID": 6, "context": "More related to this work is the use of convolutional neural networks for text classification (Blunsom et al., 2014). Closely related is the work of Denil et al. (2014) that learns representations of documents using convolution but uses the network activations to summarize a document rather than an attentional model. Rush et al. (2015) use an attention-based encoder to summarize sentences, but do not use convolution for their attention mechanism.", "startOffset": 95, "endOffset": 338}, {"referenceID": 40, "context": "Research has mostly focused on token-level (Nguyen et al., 2013; Tu et al., 2014) and syntax-level (Maddison & Tarlow, 2014) language models of code and translation between programming languages (Karaivanov et al.", "startOffset": 43, "endOffset": 81}, {"referenceID": 19, "context": ", 2014) and syntax-level (Maddison & Tarlow, 2014) language models of code and translation between programming languages (Karaivanov et al., 2014; Nguyen et al., 2014).", "startOffset": 121, "endOffset": 167}, {"referenceID": 30, "context": ", 2014) and syntax-level (Maddison & Tarlow, 2014) language models of code and translation between programming languages (Karaivanov et al., 2014; Nguyen et al., 2014).", "startOffset": 121, "endOffset": 167}, {"referenceID": 13, "context": "In recent years, thanks to the insight of Hindle et al. (2012) the use of probabilistic models for software engineering applications has grown.", "startOffset": 42, "endOffset": 63}, {"referenceID": 13, "context": "In recent years, thanks to the insight of Hindle et al. (2012) the use of probabilistic models for software engineering applications has grown. Research has mostly focused on token-level (Nguyen et al., 2013; Tu et al., 2014) and syntax-level (Maddison & Tarlow, 2014) language models of code and translation between programming languages (Karaivanov et al., 2014; Nguyen et al., 2014). Movshovitz-Attias et al. (2013) learns to predict code comments using a source code topic model.", "startOffset": 42, "endOffset": 419}, {"referenceID": 0, "context": "Allamanis et al. (2015b) create a generative model of source code snip-", "startOffset": 0, "endOffset": 25}, {"referenceID": 27, "context": "pets given a natural language query and Oda et al. (2015) use machine translation to convert source code into pseudocode.", "startOffset": 40, "endOffset": 58}, {"referenceID": 27, "context": "pets given a natural language query and Oda et al. (2015) use machine translation to convert source code into pseudocode. Closer to our work, Raychev et al. (2015) aim to predict names and types of variables, whereas Allamanis et al.", "startOffset": 40, "endOffset": 164}, {"referenceID": 0, "context": "(2015) aim to predict names and types of variables, whereas Allamanis et al. (2014) and Allamanis et al.", "startOffset": 60, "endOffset": 84}, {"referenceID": 0, "context": "(2015) aim to predict names and types of variables, whereas Allamanis et al. (2014) and Allamanis et al. (2015a) suggest names for variables, methods and classes.", "startOffset": 60, "endOffset": 113}, {"referenceID": 0, "context": "(2015) aim to predict names and types of variables, whereas Allamanis et al. (2014) and Allamanis et al. (2015a) suggest names for variables, methods and classes. Similar to Allamanis et al. (2015a), we aim to predict method names but using only the tokens within a method and no other features (e.", "startOffset": 60, "endOffset": 199}, {"referenceID": 0, "context": "(2015) aim to predict names and types of variables, whereas Allamanis et al. (2014) and Allamanis et al. (2015a) suggest names for variables, methods and classes. Similar to Allamanis et al. (2015a), we aim to predict method names but using only the tokens within a method and no other features (e.g. method signature). Mou et al. (2016) use syntaxlevel convolutional neural networks, ignoring identifiers, to learn a vector representations of code and classify student submissions into their original tasks without considering naming.", "startOffset": 60, "endOffset": 338}, {"referenceID": 0, "context": "(2015) aim to predict names and types of variables, whereas Allamanis et al. (2014) and Allamanis et al. (2015a) suggest names for variables, methods and classes. Similar to Allamanis et al. (2015a), we aim to predict method names but using only the tokens within a method and no other features (e.g. method signature). Mou et al. (2016) use syntaxlevel convolutional neural networks, ignoring identifiers, to learn a vector representations of code and classify student submissions into their original tasks without considering naming. Piech et al. (2015) also learn program embeddings from student submissions using the program state, to assist MOOC students debug their submissions but do not consider naming.", "startOffset": 60, "endOffset": 556}, {"referenceID": 0, "context": "(2015) aim to predict names and types of variables, whereas Allamanis et al. (2014) and Allamanis et al. (2015a) suggest names for variables, methods and classes. Similar to Allamanis et al. (2015a), we aim to predict method names but using only the tokens within a method and no other features (e.g. method signature). Mou et al. (2016) use syntaxlevel convolutional neural networks, ignoring identifiers, to learn a vector representations of code and classify student submissions into their original tasks without considering naming. Piech et al. (2015) also learn program embeddings from student submissions using the program state, to assist MOOC students debug their submissions but do not consider naming. Additionally, compared to Piech et al. (2015) and Mou et al.", "startOffset": 60, "endOffset": 758}, {"referenceID": 0, "context": "(2015) aim to predict names and types of variables, whereas Allamanis et al. (2014) and Allamanis et al. (2015a) suggest names for variables, methods and classes. Similar to Allamanis et al. (2015a), we aim to predict method names but using only the tokens within a method and no other features (e.g. method signature). Mou et al. (2016) use syntaxlevel convolutional neural networks, ignoring identifiers, to learn a vector representations of code and classify student submissions into their original tasks without considering naming. Piech et al. (2015) also learn program embeddings from student submissions using the program state, to assist MOOC students debug their submissions but do not consider naming. Additionally, compared to Piech et al. (2015) and Mou et al. (2016) our work looks into highly diverse, non-student submission code that performs a wide range of real-world tasks.", "startOffset": 60, "endOffset": 780}], "year": 2016, "abstractText": "Attention mechanisms in neural networks have proved useful for problems in which the input and output do not have fixed dimension. Often there exist features that are locally translation invariant and would be valuable for directing the model\u2019s attention, but previous attentional architectures are not constructed to learn such features specifically. We introduce an attentional neural network that employs convolution on the input tokens to detect local time-invariant and long-range topical attention features in a context-dependent way. We apply this architecture to the problem of extreme summarization of source code snippets into short, descriptive function name-like summaries. Using those features, the model sequentially generates a summary by marginalizing over two attention mechanisms: one that predicts the next summary token based on the attention weights of the input tokens and another that is able to copy a code token as-is directly into the summary. We demonstrate our convolutional attention neural network\u2019s performance on 10 popular Java projects showing that it achieves better performance compared to previous attentional mechanisms.", "creator": "LaTeX with hyperref package"}}}