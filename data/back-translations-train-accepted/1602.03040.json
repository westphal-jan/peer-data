{"id": "1602.03040", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2016", "title": "The Structured Weighted Violations Perceptron Algorithm", "abstract": "We present the Structured Weighted Violations Perceptron (SWVP) algorithm, a new perceptron algorithm for structured prediction, that generalizes the Collins Structured Perceptron (CSP, (Collins, 2002)). Unlike CSP, the update rule of SWVP explicitly exploits the internal structure of the predicted labels. We prove that for linearly separable training sets, SWVP converges to a weight vector that separates the data, under certain conditions on the parameters of the algorithm. We further prove bounds for SWVP on: (a) the number of updates in the separable case; (b) mistakes in the non-separable case; and (c) the probability to misclassify an unseen example (generalization), and show that for most SWVP variants these bounds are tighter than those of the CSP special case. In synthetic data experiments where data is drawn from a generative hidden variable model, SWVP provides substantial improvements over CSP.", "histories": [["v1", "Tue, 9 Feb 2016 15:51:19 GMT  (229kb)", "https://arxiv.org/abs/1602.03040v1", null], ["v2", "Mon, 6 Jun 2016 08:00:15 GMT  (261kb,D)", "http://arxiv.org/abs/1602.03040v2", null], ["v3", "Wed, 14 Sep 2016 07:24:10 GMT  (75kb,D)", "http://arxiv.org/abs/1602.03040v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["rotem dror", "roi reichart"], "accepted": true, "id": "1602.03040"}, "pdf": {"name": "1602.03040.pdf", "metadata": {"source": "CRF", "title": "The Structured Weighted Violations Perceptron Algorithm", "authors": ["Rotem Dror"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to go into another world, in which they are able to move, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they"}, {"heading": "2 The Collins Structured Perceptron", "text": "In structured prediction, the task is to find a mapping f: X \u2192 Y = problem where y-y is a structured object and not a scalar, and a feature mapping \u03c6 (x, y): X \u00b7 Y (x) \u2192 Rd is given. In this thesis, we refer to Y (x) = {y-y-y-DY-Lx}, where Lx, a scalar, is the size of the permitted output sequence for an input x and DY is the domain of y. i for each i-solution {1,. Lx}. 1 However, our results also apply in the general case of an output space with variable size vectors. The CSP algorithm (algorithm 1) aims to learn a parameter (or weight) vector w-Rd that separates the training data, i.e. for each training example (x, y) it is considered identical."}, {"heading": "3 The Structured Weighted Violations Perceptron (SWVP)", "text": "SWVP uses the internal structure of a predicted label y \u043a 6 = y for a training example (x, y) by updating the weight vector with respect to substructures of y *. We start by presenting the basic concepts based on our algorithm."}, {"heading": "3.1 Basic Concepts", "text": "We start with two basic definitions: (1) An individual substructure of a structured object (or label) of JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = = = JP = JP = = JP = = JP = JP = = = JP = JP = = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP = JP ="}, {"heading": "3.2 Algorithm", "text": "With these definitions we can present the SWVP algorithm (algorithm 2). SWVP = J signal function is in fact a family of algorithms that differ in relation to two decisions that can be made at each pass over each training example (x, y): the choice of the set JJx and the implementation of the SETGAMMA function.SWVP is very similar to CSP except in the update rule. As in CSP, the algorithm iterates over the training data examples and for each example it first predicts a label according to the current parameter vector w (conclusion discussed in Section 4.2, property 2).The main difference to CSP is the update rule (lines 6-12).Here, for each substructure in the substructure set, J-JJx, the algorithm generates a mixed mJ (lines 7-9).Then it is updated with a weighted sum of mixed assignments (lines 11)."}, {"heading": "4 Theory", "text": "We begin this section with the convergence conditions on the \u03b3 vector, which weighs the mixed allocation updates in the SGP update rule (line 11), and then, using these conditions, describe the relationship between the SGP and the CSP algorithms, and then examine the convergence of the SGP and analyze the derived properties of the algorithm. Our main observation in this section is that SGP converges under two conditions: (a) the training set D is linearly separable; and (b) for each parameter vector w that can be achieved by the algorithm, there is (x, y) D with JJx 2 [Lx], so that for the predicted output y 6 = y, SETGAMMA provides a weight vector that respects the following conditions: Definition 3. The selection conditions for the SVP algorithm are (I = Iv-Inv): (J = Inv-J) as follows selection conditions."}, {"heading": "4.1 Convergence for Linearly Separable Data", "text": "Here we give the theory about the convergence of the SWVP in the separable case. We define first: Definition J = J J = J J = J = J = J = J = J = J = J = J = J = J = J = J J = J = J = J J = J = J J = J = J = J = J J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J"}, {"heading": "4.2 Convergence Properties", "text": "Next, we point out three properties of the SGP algorithm, derived from its convergence proof: Property 1 (tighter iterations bound) The convergence proof of CSP (Collins, 2002) is given for a vector u that separates the data linearly, with margin \u03b4 and for a data radius R. After observation 1, however, he notes that in our case, u also separates the data linearly with respect to mixed assignments with a specified YY and with margin YJ \u2265 \u03b4. Together with the definition of RJJ \u2264 R, we get this: (R YY) 2 (\u03b4YY) 2 \u2264 R2 \u03b4er 2.This means that the limit on the number of updates made by SGP is narrower than the limit of CSPJ. Property 2 (inference) states that any label from which at least one violating MA can be derived by YYYYYY YY is suitable for an update. This is because in such a case, we can choose the SETMA-1 function."}, {"heading": "4.3 Mistake and Generalization Bounds", "text": "The following limits are documented: the number of updates in the separable case (see Theorem 1); the number of errors in the non-separable case (see Appendix B); and the probability of misclassifying an invisible example (see Appendix B). It can be shown that these limits are generally narrower than those of the CSP special case. Next, we will discuss variants of the SGP."}, {"heading": "5 Passive Aggressive SWVP", "text": "Here we present updated rules that can be implemented within the SGP. Such types of rules are defined by: (a) the selection of \"J\" (J), which exploits the selection conditions (see definition 3) and (b) the selection of \"YY\" (Lx), (x, y) and \"D,\" the substructure sets for the selection examples (see definition 3) and (b) the selection of \"YY\" which constitute violations (i.e. for all J), (mJ), (mJ) and \"D,\" (mJ), (mJ), (mJ) = 0). Note that in this case the condition (2) of the selection conditions is trivial as: w \"J,\" which constitute violations (x, y). The only remaining requirement is the condition (1) that \"YY,\" i.e."}, {"heading": "6 Experiments", "text": "Synthetic data we experiment with synthetic data generated by a linear chain, first-5 For the aggressive approach the equations for schemas (1) and (2) are changed such that JJx is placed with I (y *, y, Jx) v.order Hidden Markov Model (HMM, (Rabiner and Juang, 1986) Our learning algorithm is a liner-chain conditional random field (CRF, (Lafferty et al., 2001))): P (y | x) = 1 Z (x), (i = 1: Lxexp (w \u00b7 p) (yi \u2212 1, yi, x)) (where Z (x) is a normalization factor) with binary indicator features {xi, yi \u2212 1, (xi, yi \u2212 1), yi \u2212 1), (yi \u2212 1), (xi, yi \u2212 1). We experiment with the triplet (yi, yi \u2212 1, x).A datet is generated by Items."}, {"heading": "7 Results", "text": "Synthetic data Table 1 presents our results. In all three constellations, an SGP algorithm is \u03b2 times superior. Average accuracy differences between the best performing algorithms and CSP are: 3.72 (B-WMR, (simple (+ +), learnable (+ +)))), 5.29 (B-WM, (simple (+ +), learnable (+)) and 5.18 (A-WM, (simple (+), learnable (+)))). In all constellations, SGP outperforms CSP in averaged performance (except B-WMR for (simple (+), learnable (+), learnable (+)))). In addition, the weighted models are more stable than CSP, as indicated by the lower standard deviation of their accuracy values. Finally, for the simpler and more learnable data sets of the SGP models, we are not able to perform CSP models in the majority of cases (7-10 / 10)."}, {"heading": "8 Conclusions", "text": "We introduced the Structured Weighted Violations Perceptron (SVP) algorithm, a generalization of the Structured Perceptron (CSP) algorithm that explicitly exploits the internal structure of the predicted label in its updating rule. We demonstrated the convergence of the algorithm for linearly separable training sets under certain conditions using its parameters and provided generalizations and error limits. In experiments, we examined only very simple configurations of the SVP parameters - \u03b3 and JJ. Nevertheless, several of our SVP variants performed better in synthetic data experiments than the CSP special case. In dependency spares experiments, SVP showed some improvements over CSP, but these did not generalize well. Although we find these results somewhat encouraging, they emphasize the need to examine the much more flexible selection strategies of SVP and JJ that make SVP (Sec. 4.2) useful in practice."}, {"heading": "A Proof Observation 1.", "text": "Observation 1. In the case of linearly separable data D and a set of YY, each unit vector u separating the data with margin \u03b4 also separates the data relating to mixed assignments with YY, with margin \u03b4YY \u2265 \u03b4. Likewise, RYY \u2264 R.Proof applies. For each training example (x, y) and D, the following applies: Hz-Y (x) mJ (z, y) Y (x). Since u separates the data with margin (x, y, y), the following applies: u \u00b7, 000 (x, y, mJ (z, y)) \u2265 YYYx, Hz-Y (x, y, z). Since the last inequalities for each radius (x, y) and YY = min (x, y, z) -YYYYJ. It also follows from the same considerations that R-J (R, Yy), because the radius are set."}, {"heading": "B Mistake and Generalization Bounds - Non Separable Case", "text": "We start with the following definition 3: 2: Observation: Definition 8: Definition 1: Definition 1: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: Definition 2: 2: Definition 2: 2: Definition 2: Definition 2: Definition 2: 2: Definition 2: Definition 2: 2:"}, {"heading": "Acknowledgments", "text": "The second author was partially supported by a research grant from the GIF Young Scientists' Program (No. I-2388-407.6 / 2015): Syntactic Parsing in Context."}], "references": [{"title": "Incremental parsing with the perceptron algorithm", "author": ["Collins", "Roark2004] Michael Collins", "Brian Roark"], "venue": "In Proc. of ACL", "citeRegEx": "Collins et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2004}, {"title": "Three generative, lexicalised models for statistical parsing", "author": ["Michael Collins"], "venue": "In Proc. of ACL,", "citeRegEx": "Collins.,? \\Q1997\\E", "shortCiteRegEx": "Collins.", "year": 1997}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "author": ["Michael Collins"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Collins.,? \\Q2002\\E", "shortCiteRegEx": "Collins.", "year": 2002}, {"title": "Ultraconservative online algorithms for multiclass problems", "author": ["Crammer", "Singer2003] Koby Crammer", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Crammer et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Crammer et al\\.", "year": 2003}, {"title": "Learning as search optimization: Approximate large margin methods for structured prediction", "author": ["III Daum\u00e9", "III Marcu2005] Hal Daum\u00e9", "Marcu. Daniel"], "venue": "In Proc. of ICML,", "citeRegEx": "Daum\u00e9 et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Daum\u00e9 et al\\.", "year": 2005}, {"title": "Large margin classification using the perceptron algorithm", "author": ["Freund", "Schapire1999] Yoav Freund", "Robert E Schapire"], "venue": "Machine learning,", "citeRegEx": "Freund et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Freund et al\\.", "year": 1999}, {"title": "An efficient algorithm for easyfirst non-directional dependency parsing", "author": ["Goldberg", "Elhadad2010] Yoav Goldberg", "Michael Elhadad"], "venue": "In Proc. of NAACL-HLT", "citeRegEx": "Goldberg et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2010}, {"title": "Knowledge-based weak supervision for information extraction of overlapping relations", "author": ["Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S Weld"], "venue": "In Proc. of ACL", "citeRegEx": "Hoffmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2011}, {"title": "Structured perceptron with inexact search", "author": ["Huang et al.2012] Liang Huang", "Suphan Fayong", "Yang Guo"], "venue": "In Proc. of NAACL-HLT,", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Efficient third-order dependency parsers", "author": ["Koo", "Collins2010] Terry Koo", "Michael Collins"], "venue": "In Proc. of ACL,", "citeRegEx": "Koo et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Koo et al\\.", "year": 2010}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Andrew McCallum", "Fernando CN Pereira"], "venue": "In Proc. of ICML", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Turning on the turbo: Fast third-order non-projective turbo parsers", "author": ["Miguel Almeida", "Noah A Smith"], "venue": "In Prc. of ACL short papers,", "citeRegEx": "Martins et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Martins et al\\.", "year": 2013}, {"title": "Online learning of approximate dependency parsing algorithms", "author": ["McDonald", "Pereira2006] Ryan T McDonald", "Fernando CN Pereira"], "venue": "In Proc. of EACL", "citeRegEx": "McDonald et al\\.,? \\Q2006\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2006}, {"title": "Online largemargin training of dependency parsers", "author": ["Koby Crammer", "Fernando Pereira"], "venue": "In Proc. of ACL,", "citeRegEx": "McDonald et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Nonprojective dependency parsing using spanning tree algorithms", "author": ["Fernando Pereira", "Kiril Ribarov", "Jan Haji\u010d"], "venue": "In Proc. of EMNLP-HLT,", "citeRegEx": "McDonald et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Learning efficiently with approximate inference via dual losses", "author": ["Meshi et al.2010] Ofer Meshi", "David Sontag", "Tommi Jaakkola", "Amir Globerson"], "venue": "In Proc. of ICML", "citeRegEx": "Meshi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Meshi et al\\.", "year": 2010}, {"title": "The conll 2007 shared task on dependency parsing", "author": ["Nilsson et al.2007] Jens Nilsson", "Sebastian Riedel", "Deniz Yuret"], "venue": "In Proceedings of the CoNLL shared task session of EMNLP-CoNLL,", "citeRegEx": "Nilsson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Nilsson et al\\.", "year": 2007}, {"title": "An introduction to hidden markov models", "author": ["Rabiner", "Juang1986] Lawrence Rabiner", "BiingHwang Juang"], "venue": "ASSP Magazine,", "citeRegEx": "Rabiner et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rabiner et al\\.", "year": 1986}, {"title": "Multi event extraction guided by global constraints", "author": ["Reichart", "Barzilay2012] Roi Reichart", "Regina Barzilay"], "venue": "In Proc. of NAACL-HLT", "citeRegEx": "Reichart et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Reichart et al\\.", "year": 2012}, {"title": "Max-margin markov networks", "author": ["Taskar et al.2004] Ben Taskar", "Carlos Guestrin", "Daphne Koller"], "venue": "In Proc. of NIPS", "citeRegEx": "Taskar et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2004}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["Thorsten Joachims", "Thomas Hofmann", "Yasemin Altun"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "Tsochantaridis et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2005}, {"title": "Online learning of relaxed ccg grammars for parsing to logical form", "author": ["Zettlemoyer", "Collins2007] Luke S Zettlemoyer", "Michael Collins"], "venue": "In Proc. of EMNLP-CoNLL,", "citeRegEx": "Zettlemoyer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zettlemoyer et al\\.", "year": 2007}, {"title": "Joint word segmentation and pos tagging using a single perceptron", "author": ["Zhang", "Clark2008] Yue Zhang", "Stephen Clark"], "venue": "In proc. of ACL,", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 2, "context": "We present the Structured Weighted Violations Perceptron (SWVP) algorithm, a new structured prediction algorithm that generalizes the Collins Structured Perceptron (CSP, (Collins, 2002)).", "startOffset": 170, "endOffset": 185}, {"referenceID": 2, "context": "The structured perceptron ((Collins, 2002), henceforth denoted CSP) is a prominent training algorithm for structured prediction models in NLP, due to its effective parameter estimation and simple implementation.", "startOffset": 27, "endOffset": 42}, {"referenceID": 11, "context": "It has been utilized in numerous NLP applications including word segmentation and POS tagging (Zhang and Clark, 2008), dependency parsing (Koo and Collins, 2010; Goldberg and Elhadad, 2010; Martins et al., 2013), semantic parsing (Zettlemoyer and Collins, 2007) and information extraction (Hoffmann et al.", "startOffset": 138, "endOffset": 211}, {"referenceID": 7, "context": ", 2013), semantic parsing (Zettlemoyer and Collins, 2007) and information extraction (Hoffmann et al., 2011; Reichart and Barzilay, 2012), if to name just a few.", "startOffset": 85, "endOffset": 137}, {"referenceID": 19, "context": "structured SVM (Taskar et al., 2004; Tsochantaridis et al., 2005), MIRA (Crammer and Singer, 2003) and LaSo (Daum\u00e9 III and Marcu, 2005)), CSP considers in its update rule the difference between complete predicted and gold standard labels (Sec.", "startOffset": 15, "endOffset": 65}, {"referenceID": 20, "context": "structured SVM (Taskar et al., 2004; Tsochantaridis et al., 2005), MIRA (Crammer and Singer, 2003) and LaSo (Daum\u00e9 III and Marcu, 2005)), CSP considers in its update rule the difference between complete predicted and gold standard labels (Sec.", "startOffset": 15, "endOffset": 65}, {"referenceID": 15, "context": ", 2005a) and dual-loss based methods (Meshi et al., 2010)) it does not exploit the structure of the predicted label.", "startOffset": 37, "endOffset": 57}, {"referenceID": 8, "context": "Our concept of violating assignment is based on Huang et al. (2012) that presented a variant of the CSP algorithm where the argmax inference problem is replaced with a violation finding function.", "startOffset": 48, "endOffset": 68}, {"referenceID": 1, "context": "In the general case Lx is a set of output sizes, which may be finite or infinite (as in constituency parsing (Collins, 1997)).", "startOffset": 109, "endOffset": 124}, {"referenceID": 8, "context": "Violation The next central concept is that of a violation, originally presented by Huang et al. (2012):", "startOffset": 83, "endOffset": 103}, {"referenceID": 8, "context": "This paper therefore extends the observation of Huang et al. (2012) that perceptron parameter update can be performed w.", "startOffset": 48, "endOffset": 68}, {"referenceID": 2, "context": "While the proof of this theorem resembles that of the CSP (Collins, 2002), unlike the CSP proof the SWVP proof relies on the \u03b3 selection conditions presented above and on the Jensen inequality.", "startOffset": 58, "endOffset": 73}, {"referenceID": 2, "context": "Property 1 (tighter iterations bound) The convergence proof of CSP (Collins, 2002) is given for a vector u that linearly separates the data, with margin \u03b4 and for a data radius R.", "startOffset": 67, "endOffset": 82}, {"referenceID": 10, "context": "Our learning algorithm is a liner-chain conditional random field (CRF, (Lafferty et al., 2001)): P (y|x) =", "startOffset": 71, "endOffset": 94}, {"referenceID": 11, "context": "We implemented our algorithms within the TurboParser (Martins et al., 2013).", "startOffset": 53, "endOffset": 75}, {"referenceID": 16, "context": "We experiment with the datasets of the CoNLL 2007 shared task on multilingual dependency parsing (Nilsson et al., 2007), for a total of 9 languages.", "startOffset": 97, "endOffset": 119}, {"referenceID": 2, "context": "The proof for #mistakes-CSP is given at (Collins, 2002).", "startOffset": 40, "endOffset": 55}, {"referenceID": 2, "context": "Freund and Schapire (1999) presented the voted perceptron, a batch variant of the perceptron algorithm, and (Collins, 2002) presented an approximation for this variant called the averaged parameters perceptron that holds the same generalization guarantees.", "startOffset": 108, "endOffset": 123}, {"referenceID": 2, "context": "Note that the adaptation of (Freund and Schapire, 1999) to the original CSP algorithm provided by (Collins, 2002) gives the generalization bound", "startOffset": 98, "endOffset": 113}], "year": 2016, "abstractText": "We present the Structured Weighted Violations Perceptron (SWVP) algorithm, a new structured prediction algorithm that generalizes the Collins Structured Perceptron (CSP, (Collins, 2002)). Unlike CSP, the update rule of SWVP explicitly exploits the internal structure of the predicted labels. We prove the convergence of SWVP for linearly separable training sets, provide mistake and generalization bounds, and show that in the general case these bounds are tighter than those of the CSP special case. In synthetic data experiments with data drawn from an HMM, various variants of SWVP substantially outperform its CSP special case. SWVP also provides encouraging initial dependency parsing results.", "creator": "LaTeX with hyperref package"}}}