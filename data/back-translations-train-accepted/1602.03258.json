{"id": "1602.03258", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2016", "title": "Interactive Bayesian Hierarchical Clustering", "abstract": "Clustering is a powerful tool in data analysis, but it is often difficult to find a grouping that aligns with a user's needs. To address this, several methods incorporate constraints obtained from users into clustering algorithms, but unfortunately do not apply to hierarchical clustering. We design an interactive Bayesian algorithm that incorporates user interaction into hierarchical clustering while still utilizing the geometry of the data by sampling a constrained posterior distribution over hierarchies. We also suggest several ways to intelligently query a user. The algorithm, along with the querying schemes, shows promising results on real data.", "histories": [["v1", "Wed, 10 Feb 2016 03:59:57 GMT  (2050kb,D)", "https://arxiv.org/abs/1602.03258v1", null], ["v2", "Fri, 12 Feb 2016 23:39:15 GMT  (2050kb,D)", "http://arxiv.org/abs/1602.03258v2", null], ["v3", "Wed, 27 Apr 2016 01:36:46 GMT  (2050kb,D)", "http://arxiv.org/abs/1602.03258v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sharad vikram", "sanjoy dasgupta"], "accepted": true, "id": "1602.03258"}, "pdf": {"name": "1602.03258.pdf", "metadata": {"source": "META", "title": "Interactive Bayesian Hierarchical Clustering", "authors": ["Sharad Vikram", "Sanjoy Dasgupta"], "emails": ["SVIKRAM@CS.UCSD.EDU", "DASGUPTA@CS.UCSD.EDU"], "sections": [{"heading": "1. Introduction", "text": "There are a variety of effective algorithms - including those of 2001, EM for Gaussian mixtures and hierarchical agglomeration forms - that are used to discover \"natural\" groups in the data world. Unfortunately, they are not always able to find an arrangement that meets the needs of the user. It is inevitable that there are many different users who set different priorities, and a collection of rocks has no way of grouping by value, or looking for geological characteristics. Should there be animal images that follow the Linnaean taxonomy, or cuteness? Different users have different priorities, and a superior algorithm has no way of guessing theories. As a result, a rich body of work on restricted clusters has emerged."}, {"heading": "1.1. Other related work", "text": "A related problem that has been studied in more detail (Zoller & Buhmann, 2000; Eriksson et al., 2011; Krishnamurthy et al., 2012) is the construction of a hierarchical cluster where the only available information is pairs of similarities between points, but these must first be hidden and queried separately. In another variant of the interactive flat cluster (Balcan & Blum, 2008; Awasthi & Zadeh, 2010; Awasthi et al., 2014), the user can specify that individual clusters must be merged or split. A sequence of such operations can always lead to a target cluster, and a question of interest is how quickly this convergence can be achieved."}, {"heading": "2. Bayesian hierarchical clustering", "text": "The most basic form of hierarchical clustering is a rooted binary tree with the data points on its leaves, sometimes referred to as an additional cladogram. However, very often the tree is adorned with additional information, for example: 1. An arrangement of internal nodes in which the root is assigned the lowest number and each node has a higher number than its parents. This arrangement clearly specifies the induced clustering (for each k): Simply remove the k \u2212 1 lowest-numbered nodes and take the clusters to the leafsets of the resulting subtrees.2. Lengths on the edges. Intuitively, these lengths correspond to the amount of change (for example, time passes) along the corresponding edges. They induce a tree metric on the nodes, and often the leaves are required to be at the same distance from the root. 3. Parameters on internal nodes are sometimes from the same space as the data representing the intermediate values on the processes."}, {"heading": "2.1. The Dirichlet diffusion tree", "text": "The Dirichlet Diffusion Tree (DDT) is a generative model for d-dimensional vectors when different branches are 1, x2, xN. The data is generated sequentially over a continuous time process that lasts from time t = 0 to t = 1, whereupon they reach their final value. The first point, x1, is generated via a Brownian motion starting from the origin, i.e. X1 (t + dt) = X1 (t) + N, whereupon X1 (t) represents the value of x1 at time. The next point, x2, follows the path created by x1 until it finally deviates to a specified acquisition function. If x2 deviates, it creates an internal node in the tree structure that contains both the time and the value of x2 if it deviates."}, {"heading": "3. Adding interaction", "text": "As impressive as the Dirichlet diffusion tree is, there is no reason to believe that it will magically find a tree that meets the user's needs, but a little interaction can be helpful to improve outcomes. Let's call the hierarchical clustering of the target. It's not necessarily the case that the user would be able to explicitly write this down, but this is the tree that captures the distinctions that he can or wants to make. Figure 3 (left) shows an example of a small data set of 5 points. In this case, the user does not want to distinguish between points 1, 2, 3, but wants to place them in a cluster, point 4.We could position our target as exactly restoring. But in many cases, it is good enough to find a tree that captures all distinctions within T, but may also exhibit some minor distinctions, as in the right side of Figure 3.Formally, given the data set, we say X-leaves are a cluster whose cluster is exactly a T-S-pile."}, {"heading": "3.1. Triplets", "text": "The constraint ({a > b}, c) means that the tree should have a cluster containing a and b, but not C. In other words, the lowest common ancestor of a, b should be a strict descendant of the lowest common ancestor of a, b, c.Let \u2206 (T) denotes the set of all right triplets embodied in tree T. If T has n nodes, then it contains a strict descendant of the lowest common ancestor of a, b, c.Let \u2206 (T) denotes the set of all right triplets embodied in tree T. If T has n nodes, then it contains (T) | \u2264 (n 3). For non-binary trees, it will be smaller than this number. Figure 3 (left) means that T does not have triplets containing 1, 2, 3.Refinement."}, {"heading": "3.2. Finding a tree consistent with constraints", "text": "We start with a randomly initialized hierarchy T above our data and show the user an induced subtree T | S, which results in the first triplets. The next step is to construct a new tree that satisfies the triplets, starting with the feedback cycle; a user provides the user with a triplet tree and the triplet group is incorporated into a cluster algorithm that produces a new candidate tree. A starting point is an algorithm that returns a tree that matches a triplet group, or an error if no such tree exists. The simplest algorithm to solve this problem is the BUILD algorithm introduced in Aho et al. (1981), which has a vertex for each data point and an undirected edge {a, b} for each triplet constraint (a, b} if no such tree exists). In BUILD, we first construct the Acomponent Gapho C, where the triplet point splits both."}, {"heading": "3.3. Incorporating triplets into the sampler", "text": "In this sense, it is not surprising that such a development has occurred in recent years. \"We have not made it,\" he says, \"but we have made it.\" \"We have made it.\" \"We have made it.\" \"We have made it.\" \"We have made it.\" \"We have made it.\" \"\" We have made it. \"\" \"We have made it.\" \"\" We have made it. \"\" \"\" We have made it. \"\" \"\" We have made it. \"\" \"\" \"We have made it.\" \"\" \"\" \"We have made it.\" \"\" \"\" \"\" \"We have made it.\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\". \"\" \"\" \"\" \"\". \"\" \"\" \"\". \"\" \"\" \".\" \"\" \"\" \"\" \".\" \"\" \".\" \"\" \"\" \".\" \"\" \".\" \"\" \".\" \"\" \".\" \"\" \".\" \"\" \"\". \"\" \"\" \"\". \"\". \"\" \"\" \".\" \"\" \".\" \".\" \"\". \"\" \"\". \"\". \"\" \"\""}, {"heading": "3.4. Intelligent subset queries", "text": "In fact, it is in such a way that most people are able to feel how they want to and that they are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move"}, {"heading": "4. Experiments", "text": "In an \"intelligent query,\" a user is unrealistically shown the entire candidate tree and reports a violated triplet variance. However, in a \"random query,\" the user is shown a random candidate tree over a random subset of data, which selects the random candidate tree over a random subset of data. In each experiment, the user is shown a high deviation of the subtree using tree removal variance. Finally, in an \"interleaved query,\" the user is shown alternately a random subtree and a high deviation of the subtree. In each experiment, a high deviation of the subtree was simulated, so that the selection of a triplet was violated by the root division of the queried tree, and if no such triplet exists, we recur on a child. Each scheme was evaluated on four different datasets."}, {"heading": "5. Future Work", "text": "We are interested in investigating the unfeasible case, i.e. if there is no tree that meets the triplet set C. We also want to better understand the effects of limitations on the search for Optima using MCMC methods."}, {"heading": "A. Proof Details", "text": "We have to do, therefore, with a node in which children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children of children"}, {"heading": "B. Additional Results", "text": "(a) Zoo (b) 20 newsgroupsFigure S3. The average of four passes with restricted SPR samplers for the Zoo dataset and the 20 newsgroups dataset using 5 different query schemes. One query was performed every 100 iterations."}], "references": [{"title": "Treestructured stick breaking for hierarchical data", "author": ["R.P. Adams", "Z. Ghahramani", "M.I. Jordan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Adams et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Adams et al\\.", "year": 2008}, {"title": "Inferring a tree from lowest common ancestors with an application to the optimization of relational expressions", "author": ["A.V. Aho", "Y. Sagiv", "T.G. Szymanski", "J.D. Ullman"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Aho et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Aho et al\\.", "year": 1981}, {"title": "Probability distributions on cladograms", "author": ["D. Aldous"], "venue": "Random Discrete Structures (IMA Volumes in Mathematics and its Applications", "citeRegEx": "Aldous,? \\Q1995\\E", "shortCiteRegEx": "Aldous", "year": 1995}, {"title": "Supervised clustering", "author": ["P. Awasthi", "R.B. Zadeh"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Awasthi and Zadeh,? \\Q2010\\E", "shortCiteRegEx": "Awasthi and Zadeh", "year": 2010}, {"title": "Local algorithms for interactive clustering", "author": ["P. Awasthi", "Balcan", "M.-F", "K. Voevodski"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Awasthi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Awasthi et al\\.", "year": 2014}, {"title": "Clustering with interactive feedback", "author": ["Balcan", "M.-F", "A. Blum"], "venue": "Notes in Computer Science),", "citeRegEx": "Balcan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2008}, {"title": "Active semisupervision for pairwise constrained clustering", "author": ["S. Basu", "A. Banerjee", "R. Mooney"], "venue": "In SIAM International Conference on Data Mining,", "citeRegEx": "Basu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Basu et al\\.", "year": 2004}, {"title": "Active image clustering with pairwise constraints from humans", "author": ["A. Biswas", "D. Jacobs"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Biswas and Jacobs,? \\Q2014\\E", "shortCiteRegEx": "Biswas and Jacobs", "year": 2014}, {"title": "Modern Multidimensional Scaling: Theory and Applications", "author": ["I. Borg", "P.J.F. Groenen"], "venue": null, "citeRegEx": "Borg and Groenen,? \\Q2005\\E", "shortCiteRegEx": "Borg and Groenen", "year": 2005}, {"title": "The time-marginalized coalescent prior for hierarchical clustering", "author": ["L. Boyles", "M. Welling"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Boyles and Welling,? \\Q2012\\E", "shortCiteRegEx": "Boyles and Welling", "year": 2012}, {"title": "Extension operations on sets of leaf-labeled trees", "author": ["D Bryant", "M. Steel"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "Bryant and Steel,? \\Q1995\\E", "shortCiteRegEx": "Bryant and Steel", "year": 1995}, {"title": "Active clustering: robust and efficient hierarchical clustering using adaptively selected similarities", "author": ["B. Eriksson", "G. Dasarathy", "A. Singh", "R. Nowak"], "venue": "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Eriksson et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Eriksson et al\\.", "year": 2011}, {"title": "Bayesian hierarchical clustering", "author": ["K. Heller", "Z. Ghahramani"], "venue": "In Proceedings of the 22nd International Conference on Machine Learning,", "citeRegEx": "Heller and Ghahramani,? \\Q2005\\E", "shortCiteRegEx": "Heller and Ghahramani", "year": 2005}, {"title": "A probabilistic analysis of the rocchio algorithm with tfidf for text categorization", "author": ["Joachims", "Thorsten"], "venue": "In Proceedings of the Fourteenth International Conference on Machine Learning,", "citeRegEx": "Joachims and Thorsten.,? \\Q1997\\E", "shortCiteRegEx": "Joachims and Thorsten.", "year": 1997}, {"title": "Pitman-Yor diffusion trees for Bayesian hierarchical clustering", "author": ["D.A. Knowles", "Z. Ghahramani"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Knowles and Ghahramani,? \\Q2015\\E", "shortCiteRegEx": "Knowles and Ghahramani", "year": 2015}, {"title": "Efficient active algorithms for hierarchical clustering", "author": ["A. Krishnamurthy", "S. Balakrishnan", "M. Xu", "A. Singh"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Krishnamurthy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krishnamurthy et al\\.", "year": 2012}, {"title": "Semisupervised graph clustering: a kernel approach", "author": ["B. Kulis", "S. Basu", "I. Dhillon", "R. Mooney"], "venue": "In Proceedings of the 22nd International Conference on Machine Learning,", "citeRegEx": "Kulis et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Kulis et al\\.", "year": 2005}, {"title": "Gradient-based learning applied to document recognition", "author": ["Lecun", "Yann", "Bottou", "Lon", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "Lecun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Lecun et al\\.", "year": 1998}, {"title": "Density modeling and clustering using Dirichlet diffusion trees", "author": ["R.M. Neal"], "venue": "Bayesian Statistics", "citeRegEx": "Neal,? \\Q2003\\E", "shortCiteRegEx": "Neal", "year": 2003}, {"title": "Adaptively learning the crowd kernel", "author": ["O. Tamuz", "C. Liu", "S. Belongie", "O. Shamir", "A.T. Kalai"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Tamuz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tamuz et al\\.", "year": 2011}, {"title": "Bayesian agglomerative clustering with coalescents", "author": ["Y.W. Teh", "III", "H. Daume", "D.M. Roy"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Teh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2008}, {"title": "Clustering with instance-level constraints", "author": ["K. Wagstaff", "C. Cardie"], "venue": "In Proceedings of the 17th International Conference on Machine Learning,", "citeRegEx": "Wagstaff and Cardie,? \\Q2000\\E", "shortCiteRegEx": "Wagstaff and Cardie", "year": 2000}, {"title": "Constrained k-means clustering with background knowledge", "author": ["K. Wagstaff", "C. Cardie", "S. Rogers", "S. Schroedl"], "venue": "In Proceedings of the 18th International Conference on Machine Learning,", "citeRegEx": "Wagstaff et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Wagstaff et al\\.", "year": 2001}, {"title": "A MCMC approach to hierarchical mixture modeling", "author": ["C.K.I. Williams"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Williams,? \\Q2000\\E", "shortCiteRegEx": "Williams", "year": 2000}, {"title": "Active learning for hierarchical pairwise data clustering", "author": ["T. Zoller", "J.M. Buhmann"], "venue": "In Proceedings of the 15th International Conference on Pattern Recognition,", "citeRegEx": "Zoller and Buhmann,? \\Q2000\\E", "shortCiteRegEx": "Zoller and Buhmann", "year": 2000}], "referenceMentions": [{"referenceID": 22, "context": "ent flat clustering procedures (Wagstaff et al., 2001; Bansal et al., 2004; Basu et al., 2004; Kulis et al., 2005; Biswas & Jacobs, 2014).", "startOffset": 31, "endOffset": 137}, {"referenceID": 6, "context": "ent flat clustering procedures (Wagstaff et al., 2001; Bansal et al., 2004; Basu et al., 2004; Kulis et al., 2005; Biswas & Jacobs, 2014).", "startOffset": 31, "endOffset": 137}, {"referenceID": 16, "context": "ent flat clustering procedures (Wagstaff et al., 2001; Bansal et al., 2004; Basu et al., 2004; Kulis et al., 2005; Biswas & Jacobs, 2014).", "startOffset": 31, "endOffset": 137}, {"referenceID": 1, "context": "A wealth of research addresses learning taxonomies from triplets alone, mostly in the field of phylogenetics: see Felsenstein (2004) for an overview, and Aho et al. (1981) for a central algorithmic result.", "startOffset": 154, "endOffset": 172}, {"referenceID": 19, "context": "It was pointed out in Tamuz et al. (2011) that roughly n log n ar X iv :1 60 2.", "startOffset": 22, "endOffset": 42}, {"referenceID": 18, "context": "For concreteness, we focus on the Dirichlet diffusion tree (Neal, 2003), which has enjoyed empirical success.", "startOffset": 59, "endOffset": 71}, {"referenceID": 11, "context": "A related problem that has been studied in more detail (Zoller & Buhmann, 2000; Eriksson et al., 2011; Krishnamurthy et al., 2012) is that of building a hierarchical clustering where the only information available is pairwise similarities between points, but these are initially hidden and must be individually queried.", "startOffset": 55, "endOffset": 130}, {"referenceID": 15, "context": "A related problem that has been studied in more detail (Zoller & Buhmann, 2000; Eriksson et al., 2011; Krishnamurthy et al., 2012) is that of building a hierarchical clustering where the only information available is pairwise similarities between points, but these are initially hidden and must be individually queried.", "startOffset": 55, "endOffset": 130}, {"referenceID": 4, "context": "In another variant of interactive flat clustering (Balcan & Blum, 2008; Awasthi & Zadeh, 2010; Awasthi et al., 2014), the user is allowed to specify that individual clusters be merged or split.", "startOffset": 50, "endOffset": 116}, {"referenceID": 2, "context": "Aldous (1995) has defined a one-parameter family of distributions over cladograms, called the beta-splitting model, that includes the uniform and the Yule model as special cases.", "startOffset": 0, "endOffset": 14}, {"referenceID": 18, "context": "Our primary focus is the Dirichlet diffusion tree (Neal, 2003), which is specified by a birth process that we will shortly describe.", "startOffset": 50, "endOffset": 62}, {"referenceID": 17, "context": "Our primary focus is the Dirichlet diffusion tree (Neal, 2003), which is specified by a birth process that we will shortly describe. However, our methodology applies quite generally. Other notable Bayesian approaches to hierarchical clustering include: Williams (2000), in which each node of the tree is annotated with a vector that is sampled from a Gaussian centered at its parent\u2019s vector; Heller & Ghahramani (2005), that defines a distribution over flat clusterings and then specifies an agglomerative scheme for finding a good partition with respect to this distribution; Adams et al.", "startOffset": 51, "endOffset": 269}, {"referenceID": 17, "context": "Our primary focus is the Dirichlet diffusion tree (Neal, 2003), which is specified by a birth process that we will shortly describe. However, our methodology applies quite generally. Other notable Bayesian approaches to hierarchical clustering include: Williams (2000), in which each node of the tree is annotated with a vector that is sampled from a Gaussian centered at its parent\u2019s vector; Heller & Ghahramani (2005), that defines a distribution over flat clusterings and then specifies an agglomerative scheme for finding a good partition with respect to this distribution; Adams et al.", "startOffset": 51, "endOffset": 420}, {"referenceID": 0, "context": "Other notable Bayesian approaches to hierarchical clustering include: Williams (2000), in which each node of the tree is annotated with a vector that is sampled from a Gaussian centered at its parent\u2019s vector; Heller & Ghahramani (2005), that defines a distribution over flat clusterings and then specifies an agglomerative scheme for finding a good partition with respect to this distribution; Adams et al. (2008), in which data points are allowed to reside at internal nodes of the tree; Teh et al.", "startOffset": 395, "endOffset": 415}, {"referenceID": 0, "context": "Other notable Bayesian approaches to hierarchical clustering include: Williams (2000), in which each node of the tree is annotated with a vector that is sampled from a Gaussian centered at its parent\u2019s vector; Heller & Ghahramani (2005), that defines a distribution over flat clusterings and then specifies an agglomerative scheme for finding a good partition with respect to this distribution; Adams et al. (2008), in which data points are allowed to reside at internal nodes of the tree; Teh et al. (2008); Boyles & Welling (2012), in which the distribution over trees is specified by a bottom-up coalescing process; and Knowles & Ghahramani (2015), which generalizes the Dirichlet diffusion trees to allow non-binary splits.", "startOffset": 395, "endOffset": 508}, {"referenceID": 0, "context": "Other notable Bayesian approaches to hierarchical clustering include: Williams (2000), in which each node of the tree is annotated with a vector that is sampled from a Gaussian centered at its parent\u2019s vector; Heller & Ghahramani (2005), that defines a distribution over flat clusterings and then specifies an agglomerative scheme for finding a good partition with respect to this distribution; Adams et al. (2008), in which data points are allowed to reside at internal nodes of the tree; Teh et al. (2008); Boyles & Welling (2012), in which the distribution over trees is specified by a bottom-up coalescing process; and Knowles & Ghahramani (2015), which generalizes the Dirichlet diffusion trees to allow non-binary splits.", "startOffset": 395, "endOffset": 533}, {"referenceID": 0, "context": "Other notable Bayesian approaches to hierarchical clustering include: Williams (2000), in which each node of the tree is annotated with a vector that is sampled from a Gaussian centered at its parent\u2019s vector; Heller & Ghahramani (2005), that defines a distribution over flat clusterings and then specifies an agglomerative scheme for finding a good partition with respect to this distribution; Adams et al. (2008), in which data points are allowed to reside at internal nodes of the tree; Teh et al. (2008); Boyles & Welling (2012), in which the distribution over trees is specified by a bottom-up coalescing process; and Knowles & Ghahramani (2015), which generalizes the Dirichlet diffusion trees to allow non-binary splits.", "startOffset": 395, "endOffset": 651}, {"referenceID": 18, "context": "Each of these choices has a probability associated with it, according to various properties of the tree structure and choice of acquisition function (details can be found in Neal (2003)).", "startOffset": 174, "endOffset": 186}, {"referenceID": 1, "context": "The simplest algorithm to solve this problem is the BUILD algorithm, introduced in Aho et al. (1981). Given a set of triplets C, BUILD will either return a tree that satisfies C, or error if no such tree exists.", "startOffset": 83, "endOffset": 101}, {"referenceID": 17, "context": "The first dataset, MNIST (Lecun et al., 1998), is an 10-way image classification dataset where the data are 28 x 28 images of digits.", "startOffset": 25, "endOffset": 45}], "year": 2016, "abstractText": "Clustering is a powerful tool in data analysis, but it is often difficult to find a grouping that aligns with a user\u2019s needs. To address this, several methods incorporate constraints obtained from users into clustering algorithms, but unfortunately do not apply to hierarchical clustering. We design an interactive Bayesian algorithm that incorporates user interaction into hierarchical clustering while still utilizing the geometry of the data by sampling a constrained posterior distribution over hierarchies. We also suggest several ways to intelligently query a user. The algorithm, along with the querying schemes, shows promising results on real data.", "creator": "LaTeX with hyperref package"}}}