{"id": "1602.03264", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2016", "title": "A Theory of Generative ConvNet", "abstract": "The convolutional neural network (ConvNet or CNN) is a powerful discriminative learning machine. In this paper, we show that a generative random field model that we call generative ConvNet can be derived from the discriminative ConvNet. The probability distribution of the generative ConvNet model is in the form of exponential tilting of a reference distribution. Assuming re-lu non-linearity and Gaussian white noise reference distribution, we show that the generative ConvNet model contains a representational structure with multiple layers of binary activation variables. The model is non-Gaussian, or more precisely, piecewise Gaussian, where each piece is determined by an instantiation of the binary activation variables that reconstruct the mean of the Gaussian piece. The Langevin dynamics for synthesis is driven by the reconstruction error, and the corresponding gradient descent dynamics converges to a local energy minimum that is auto-encoding. As for learning, we show that the contrastive divergence learning tends to reconstruct the observed images. Finally, we show that the maximum likelihood learning algorithm can generate realistic natural images.", "histories": [["v1", "Wed, 10 Feb 2016 04:46:45 GMT  (5030kb,D)", "http://arxiv.org/abs/1602.03264v1", null], ["v2", "Sun, 29 May 2016 05:52:10 GMT  (8236kb,D)", "http://arxiv.org/abs/1602.03264v2", null], ["v3", "Tue, 31 May 2016 06:02:19 GMT  (8236kb,D)", "http://arxiv.org/abs/1602.03264v3", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["jianwen xie", "yang lu", "song-chun zhu", "ying nian wu"], "accepted": true, "id": "1602.03264"}, "pdf": {"name": "1602.03264.pdf", "metadata": {"source": "CRF", "title": "A Theory of Generative ConvNet", "authors": ["Jianwen Xie", "Yang Lu", "Song-Chun Zhu", "Ying Nian Wu"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to survive on their own, and they are able to survive on their own by going in search of their own identity; most of them are able to go in search of their own identity, and most of them are able to survive on their own."}, {"heading": "2 Contributions and Related Work", "text": "In fact, most of them will be able to orient themselves in a different direction than in a different direction, namely the direction in which they are going."}, {"heading": "3 Generative ConvNet", "text": "To fix this, I must define (x) an image defined on the square (or rectangular). (D) We can also treat myself as a vector when we fix an arrangement for the pixel image. (F) We can define the filter response or attribute at position x.A ConvNet is a composition of several layers of linear filtering and elementary nonlinear transformation as expressed by the following recursive formula. (F) k) k Answer or attribute at position x.A ConvNet is a composition of several layers of linear filtering and elementary nonlinear transformation as expressed: [F) k."}, {"heading": "4 Hierarchical FRAME Model", "text": "Model (7) is a Markov random field model (Besag, 1974), in which the clique functions actually contribute to individual functions. (F (L) k functions are [F (L) k functions I] (x), x-k, x. According to the Hammersley Clifford theorem (Besag, 1974), a Markov random filter pattern can be written (I) (I) = 1Z filter (X) (X (C))], (9) where C-D are the cliques, and each clique C consists of pixels that are neighbors of each other according to a predefined neighborhood system. (C) are the intensities of pixels in Clique C, and \u03bbC is the potential function. The challenge in developing a Markov random field model is to specify the clique functions and estimate them from the data. Model (7) solves this problem by adopting the two models (Filter I) (k) (L)."}, {"heading": "5 A Prototype Model", "text": "A similar model has been studied by Xie et al. (2015). Therefore, it is helpful to start from the prototype model by assuming that the image domain D is small (e.g. 10 x 10). We want to learn a dictionary of filters or basic functions from a series of observed image patches. {Im, m = 1, M} we define these filters or basic functions by (wk, k = 1,..., K), where each wk itself is an image patch."}, {"heading": "6 Properties of Generative ConvNet", "text": "To generalize the prototype model (11) to the generative pattern ConvNet (7), we need only add two elements: (1) horizontal unfolding: the filters (wk) convolutional; (2) vertical unfolding: the filters (wk) multilayered or hierarchical; the results we obtained for the prototype model can be unfolded accordingly; (2) vertical unfolding: the use of indexes is minimized (although there are still many remaining); for filters at level l, the Nl filters are compressed using the compact notation F (l) k, k = 1, k = 1, Nl; and the Nl-filtered images or feature cards are compressed by the compact notation F (l)."}, {"heading": "7 Learning Generative ConvNet", "text": "The Learning Effects of Langevin-Dynamics (28): Langevin-Dynamics (28): Langevin-Dynamics (28): Langevin-Dynamics (28): Langevin-Dynamics (28): Langevin-Dynamics (28): Langevin-Dynamics (28): Langevin-Dynamics (28): Langevin-Dynamics (28): Langevin-Dynamics (28): Langevin-Dynamics (28): Langevin-Dynamics (28): Langevin-Dynamics-Dynamics (28): Langevin-Dynamics-Dynamics (28): Langevin-Dynamics-Dynamics (28): Langevin-Dynamics-Dynamics (28): Langevin-Dynamics-Dynamics-Dynamics (28): Langevin-Dynamics-Dynamics (28): Langevin-Dynamics-Dynamics (28): Langevin-Dynamics-Dynamics-Dynamics (28): Langevin-Dynamics-Dynamics (28): Langevin-Dynamics-Dynamics-Dynamics (28): Langevin-Dynamics-Dynamics-Dynamics (28): Langevin-Dynamics-Dynamics-Dynamics (28): Langevin-Dynamics-Dynamics (28): Langevin-Dynamics-Dynamics-Dynamics (28): Langevin-Dynamics-Dynamics-Dynamics-Dynamics (28): Langevin-Dynamics-Dynamics-Dynamics (28): Langevin-Dynamics-Dynamics-Dynamics-Dynamics (28): Langevin-Dynamics-Dynamics (28): Langevin-Dynamics-Dynamics-Dynamics-Dynamics (28): Langevin-Dynamics (28-Dynamics-Dynamics-Dynamics: 28): Langevin-Dynamics-Dynami"}, {"heading": "8 Synthesis and Reconstruction by Generative ConvNet", "text": "We show that the generative ConvNet is able to learn the observed images and generate realistic natural image patterns. Such empirical proof of the concept confirms the generative capacity of the model. We also show that contrastive divergence learning actually reconstructs the observed images and thus Proposition 4. The code in our experiments is based on the MatConvNet package from Vedaldi & Lenc (2014). Unlike Lu et al. (2016), the generative ConvNets are learned from scratch in our experiments without relying on the pre-learned filters from existing ConvNetworks. When learning the generative ConvNet, we grow the layers one after the other. Starting from the first layer, we add the layers one after the other. Each time, we learn the model and generate the synthesized images using algorithm 1. As we learn the new layer of the filters, we can either adjust the lower layers of the filters one after the other and adjust the second layer of the parameters at the same time."}, {"heading": "8.1 Experiment 1: Generating texture patterns", "text": "We learn a 3-layer generative ConvNet. The first layer has 100 15 \u00d7 15 filters with sub-sampling size of 3. The second layer has 64 5 \u00d7 5 filters with sub-sampling size of 1. The third layer has 30 3 \u00d7 3 filters with sub-sampling size of 1. We learn a generative ConvNet for each category from a single training image. Figure 1 shows the results. For each category, the first image is the training image, and the rest are 2 of the images generated by the learning algorithm."}, {"heading": "8.2 Experiment 2: Special case: generating aligned object patterns", "text": "Experiment 1 clearly shows that the generative ConvNet can learn from images without alignment. We can also specialize in learning aligned object patterns by using a single top layer filter that covers the entire image. It is actually a non-stationary FRAME model of shape (10), i.e. a revolutionary filter at a fixed position in front of Re-Lu nonlinearity. We learn a 4-layer generative ConvNet of images of aligned objects. The first layer consists of 100 7 x 7 filters with sub-sampling size of 2. The second layer consists of 64 5 x 5 filters with sub-sampling size of 1. The third layer consists of 20 3 x 3 filters with sub-sampling size of 1. The fourth layer is a completely connected layer with a single filter covering the entire image. As the layers grow, we always retain the top layer with sub-sampling size of 1. We learn the number of convNet, a generative row for each category, and the number of results for the first line are collected."}, {"heading": "8.3 Experiment 3: Contrastive divergence learns to reconstruct", "text": "The structure of ConvNet is the same as in Experiment 1. To increase computing power, we learn all layers of filters simultaneously, the number of learning iterations is T = 1200. Based on the observed images, the number of Langevin iterations is L = 1. Figure 3 shows the results. The first row shows 4 of the training images, and the second row shows the corresponding auto-encoding reconstructions with the learned parameters."}, {"heading": "9 General Non-Linearity", "text": "For general nonlinearity, i.e., h (r) is not necessarily re-lu or piecemeal linear, some of the above results remain. If we let h \u2032 (r) be the derivative of h (r), then the activation variables will generally be non-binary (l) k, x (I; w) = h \u2032 (< w (l) k, x, F (l \u2212 1) \u0445 I > + bl, k), (42), which is non-binary, and the auto-encoding reconstruction Bw, \u03b4 (I; w) = \u2202 I f (I; w), (43), where f (I; w) is defined by (8). Langevin dynamics (28) and the Hopfield auto-encoder (29) in Proposition 3 are still valid, but the piecemeal Gausan form (27) in Proposition 2 is lost."}, {"heading": "10 Conclusion", "text": "This paper derives the generative ConvNet from the discriminatory ConvNet and reveals a representative structure in the generative ConvNet. Some of the results in this paper can be attributed to the back-propagation in the discriminatory ConvNet, but our reinterpretation of its representation is new and rich in expansion. Our paper unites, reconciles or combines the following antagonistic or disparate pairs: (1) discriminatory ConvNet and generative CongNet, (2) supervised learning and unsupervised learning, (3) exponential family models and latent variable models, (4) bottom-up filters (operation) and top-down basic functions (representation), (5) synthesis (dream) and reconstruction (memory), (6) hierarchical probability model and hierarchical auto-encoder, (7) hopfield-attractor network and auto-encoder, (8) contrasting divergent data (learning) and (reconstruction), (and (and) probability models of hierarchical data (and), (and) the generative ConvNet."}, {"heading": "Acknowledgement", "text": "The code in our work is based on the Matlab code of MatConvNet by Vedaldi & Lenc (2014). We thank the authors for publishing their code. We thank Jifeng Dai for his previous work on generative ConvNet. We thank Wenze Hu for his previous work on inhomogeneous frame models. The work is supported by NSF DMS 1310391, ONR MURI N00014-10-1-0933 and DARPA SIMPLEX N6600115-C-4035."}], "references": [{"title": "What regularized auto-encoders learn from the data-generating distribution", "author": ["Alain", "Guillaume", "Bengio", "Yoshua"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Alain et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Alain et al\\.", "year": 2014}, {"title": "Spatial interaction and the statistical analysis of lattice systems", "author": ["Besag", "Julian"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Besag and Julian.,? \\Q1974\\E", "shortCiteRegEx": "Besag and Julian.", "year": 1974}, {"title": "Generative modeling of convolutional neural networks", "author": ["Dai", "Jifeng", "Lu", "Yang", "Wu", "Ying Nian"], "venue": "In ICLR,", "citeRegEx": "Dai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2015}, {"title": "Unsupervised discovery of nonlinear structure using contrastive backpropagation", "author": ["Hinton", "Geoffrey", "Osindero", "Simon", "Welling", "Max", "Teh", "Yee-Whye"], "venue": "Cognitive science,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["Hinton", "Geoffrey E"], "venue": "Neural Computation,", "citeRegEx": "Hinton and E.,? \\Q2002\\E", "shortCiteRegEx": "Hinton and E.", "year": 2002}, {"title": "wake-sleep\u201d algorithm for unsupervised neural networks", "author": ["Hinton", "Geoffrey E", "Dayan", "Peter", "Frey", "Brendan J", "Neal", "Radford M. The"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1995}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Hinton", "Geoffrey E", "Osindero", "Simon", "Teh", "Yee Whye"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Unsupervised learning of compositional sparse code for natural image representation", "author": ["Hong", "Yi", "Si", "Zhangzhang", "Hu", "Wenze", "Zhu", "Song-Chun", "Wu", "YING NIAN"], "venue": "Quarterly of Applied Mathematics,", "citeRegEx": "Hong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hong et al\\.", "year": 2013}, {"title": "Neural networks and physical systems with emergent collective computational abilities", "author": ["Hopfield", "John J"], "venue": "Proceedings of the national academy of sciences,", "citeRegEx": "Hopfield and J.,? \\Q1982\\E", "shortCiteRegEx": "Hopfield and J.", "year": 1982}, {"title": "Estimation of non-normalized statistical models by score matching", "author": ["Hyv\u00e4rinen", "Aapo"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hyv\u00e4rinen and Aapo.,? \\Q2005\\E", "shortCiteRegEx": "Hyv\u00e4rinen and Aapo.", "year": 2005}, {"title": "Connections between score matching, contrastive divergence, and pseudolikelihood for continuous-valued variables", "author": ["Hyv\u00e4rinen", "Aapo"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Hyv\u00e4rinen and Aapo.,? \\Q2007\\E", "shortCiteRegEx": "Hyv\u00e4rinen and Aapo.", "year": 2007}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In NIPS, pp", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "A tutorial on energy-based learning", "author": ["Y. LeCun", "S. Chopra", "R. Hadsell", "M. Ranzato", "F.J. Huang"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q2006\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2006}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Learning frame models using cnn filters", "author": ["Lu", "Yang", "Zhu", "Song-chun", "Wu", "Ying Nian"], "venue": "In Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Lu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "Neural variational inference and learning in belief networks", "author": ["Mnih", "Andriy", "Gregor", "Karol"], "venue": "In ICML,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "On the number of linear regions of deep neural networks", "author": ["Montufar", "Guido F", "Pascanu", "Razvan", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In NIPS,", "citeRegEx": "Montufar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Montufar et al\\.", "year": 2014}, {"title": "Learning deep energy models", "author": ["Ngiam", "Jiquan", "Chen", "Zhenghao", "Koh", "Pang Wei", "Ng", "Andrew Y"], "venue": "In ICML,", "citeRegEx": "Ngiam et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ngiam et al\\.", "year": 2011}, {"title": "Sparse coding with an overcomplete basis set: A strategy employed by v1", "author": ["Olshausen", "Bruno A", "Field", "David J"], "venue": "Vision Research,", "citeRegEx": "Olshausen et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Olshausen et al\\.", "year": 1997}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo J", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "JMLR Workshop and Conference Proceedings,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Fields of experts: A framework for learning image priors", "author": ["Roth", "Stefan", "Black", "Michael J"], "venue": "In CVPR,", "citeRegEx": "Roth et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Roth et al\\.", "year": 2005}, {"title": "Deep boltzmann machines", "author": ["Salakhutdinov", "Ruslan", "Hinton", "Geoffrey E"], "venue": "In AISTATS,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2009}, {"title": "Learning continuous attractors in recurrent networks", "author": ["Seung", "H. Sebastian"], "venue": "In NIPS,", "citeRegEx": "Seung and Sebastian.,? \\Q1998\\E", "shortCiteRegEx": "Seung and Sebastian.", "year": 1998}, {"title": "On autoencoders and score matching for energy based models", "author": ["Swersky", "Kevin", "Ranzato", "Marc\u2019Aurelio", "Buchman", "David", "Marlin", "Benjamin", "Freitas", "Nando"], "venue": "ICML, ICML", "citeRegEx": "Swersky et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Swersky et al\\.", "year": 2011}, {"title": "Matconvnet \u2013 convolutional neural networks for matlab", "author": ["A. Vedaldi", "K. Lenc"], "venue": "CoRR, abs/1412.4564,", "citeRegEx": "Vedaldi and Lenc,? \\Q2014\\E", "shortCiteRegEx": "Vedaldi and Lenc", "year": 2014}, {"title": "A connection between score matching and denoising autoencoders", "author": ["Vincent", "Pascal"], "venue": "Neural computation,", "citeRegEx": "Vincent and Pascal.,? \\Q2011\\E", "shortCiteRegEx": "Vincent and Pascal.", "year": 2011}, {"title": "From information scaling of natural images to regimes of statistical models", "author": ["Wu", "Ying Nian", "Zhu", "Song-Chun", "Guo", "Cheng-En"], "venue": "Quarterly of Applied Mathematics,", "citeRegEx": "Wu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2008}, {"title": "Learning active basis model for object detection and recognitio", "author": ["Wu", "Ying Nian", "Si", "Zhangzhang", "Gong", "Haifeng", "Zhu", "Song-Chun"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Wu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2010}, {"title": "Learning sparse frame models for natural image patterns", "author": ["Xie", "Jianwen", "Hu", "Wenze", "Zhu", "Song-Chun", "Wu", "Ying Nian"], "venue": "International Journal of Computer Vision, pp", "citeRegEx": "Xie et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2014}, {"title": "Inducing wavelets into random fields via generative boosting", "author": ["Xie", "Jianwen", "Lu", "Yang", "Zhu", "Song-Chun", "Wu", "Ying Nian"], "venue": "Journal of Applied and Computational Harmonic Analysis,", "citeRegEx": "Xie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2015}, {"title": "On the convergence of markovian stochastic algorithms with rapidly decreasing ergodicity rates", "author": ["Younes", "Laurent"], "venue": "Stochastics: An International Journal of Probability and Stochastic Processes,", "citeRegEx": "Younes and Laurent.,? \\Q1999\\E", "shortCiteRegEx": "Younes and Laurent.", "year": 1999}, {"title": "Visualizing and understanding convolutional neural networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": null, "citeRegEx": "Zeiler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2014}, {"title": "Adaptive deconvolutional networks for mid and high level feature learning", "author": ["Zeiler", "Matthew D", "Taylor", "Graham W", "Fergus", "Rob"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "Zeiler et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2011}, {"title": "Grade: Gibbs reaction and diffusion equations", "author": ["S.C. Zhu", "D.B. Mumford"], "venue": "In ICCV,", "citeRegEx": "Zhu and Mumford,? \\Q1998\\E", "shortCiteRegEx": "Zhu and Mumford", "year": 1998}, {"title": "A stochastic grammar of images", "author": ["Zhu", "Song Chun", "Mumford", "David"], "venue": "Foundations and Trends in Computer Graphics and Vision,", "citeRegEx": "Zhu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2006}, {"title": "Minimax entropy principle and its application to texture modeling", "author": ["Zhu", "Song Chun", "Wu", "Ying Nian", "Mumford", "David"], "venue": "Neural Computation,", "citeRegEx": "Zhu et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 1997}], "referenceMentions": [{"referenceID": 14, "context": "1 Introduction The convolutional neural network (ConvNet or CNN) (LeCun et al., 1998; Krizhevsky et al., 2012) has proven to be a tremendously successful discriminative or predictive learning machine.", "startOffset": 65, "endOffset": 110}, {"referenceID": 12, "context": "1 Introduction The convolutional neural network (ConvNet or CNN) (LeCun et al., 1998; Krizhevsky et al., 2012) has proven to be a tremendously successful discriminative or predictive learning machine.", "startOffset": 65, "endOffset": 110}, {"referenceID": 36, "context": "The generative ConvNet model can be viewed as a hierarchical version of the FRAME (Filters, Random field, And Maximum Entropy) model (Zhu et al., 1997), as well as the Product of Experts (PoE) (Hinton, 2002) and Field of Experts (FoE) (Roth & Black, 2005) models.", "startOffset": 133, "endOffset": 151}, {"referenceID": 12, "context": ", 1998; Krizhevsky et al., 2012) has proven to be a tremendously successful discriminative or predictive learning machine. Can the discriminative ConvNet be turned into a generative model and an unsupervised learning machine? It would be highly desirable if this can be achieved because generative models and unsupervised learning can be very useful when the training datasets are small or the labeled data are scarce. It would also be extremely satisfying from a conceptual point of view if both discriminative classifier and generative model, and both supervised learning and unsupervised learning, can be treated within a unified framework for ConvNet. In this conceptual paper, we show that a generative random field model, which we call generative ConvNet, can be derived from the commonly used discriminative ConvNet. The probability distribution of the generative ConvNet model is in the form of an exponential tilting of a reference distribution, and the exponential tilting is defined by \u2217Equal contributions. ConvNet that involves multiple layers of liner filtering and non-linear transformation. The generative ConvNet model can be viewed as a hierarchical version of the FRAME (Filters, Random field, And Maximum Entropy) model (Zhu et al., 1997), as well as the Product of Experts (PoE) (Hinton, 2002) and Field of Experts (FoE) (Roth & Black, 2005) models. Being an exponential family model, the generative ConvNet may appear dull and opaque. The main purpose of this article is to show that the contrary is true. Assuming Gaussian white noise reference distribution and re-lu nonlinearity, we discover that the generative ConvNet contains a surprisingly explicit and exquisite representational structure. Specifically, it contains multiple layers of binary activation variables that indicate the presence or absence of the patterns modeled by the multiple layers of filters of the ConvNet. The generative ConvNet model is non-Gaussian, or more precisely, piecewise Gaussian, where each piece is determined by an instantiation of the binary activation variables. These binary variables are computed by a bottom-up process by the multiple layers of filters, and they reconstruct the mean of the Gaussian piece by a top-down process, where the multiple layers of filters serve as multiple layers of basis functions for image reconstruction. The Langevin dynamics can be employed to synthesize images by sampling from the generative ConvNet. Interestingly, the dynamics is driven by the reconstruction error, i.e., the difference between the current image and the reconstruction by the binary activation variables mentioned above. Thus image synthesis and image reconstruction are connected. The deterministic gradient descent counterpart of the Langevin dynamics was employed by Zhu & Mumford (1998) for exploring the local energy minima of the FRAME model.", "startOffset": 8, "endOffset": 2812}, {"referenceID": 13, "context": "The hierarchical energy-based models (LeCun et al., 2006) were studied by the pioneering work of Hinton et al.", "startOffset": 37, "endOffset": 57}, {"referenceID": 36, "context": "A main motivation for this paper is to reconcile the FRAME model (Zhu et al., 1997), where the Gabor wavelets play the role of bottom-up filters, and the Olshausen-Field model (Olshausen & Field, 1997), where the wavelets play the role of top-down basis functions, and unfold these models into a hierarchical sparse compositional model (Hong et al.", "startOffset": 65, "endOffset": 83}, {"referenceID": 7, "context": ", 1997), where the Gabor wavelets play the role of bottom-up filters, and the Olshausen-Field model (Olshausen & Field, 1997), where the wavelets play the role of top-down basis functions, and unfold these models into a hierarchical sparse compositional model (Hong et al., 2013; Xie et al., 2014).", "startOffset": 260, "endOffset": 297}, {"referenceID": 29, "context": ", 1997), where the Gabor wavelets play the role of bottom-up filters, and the Olshausen-Field model (Olshausen & Field, 1997), where the wavelets play the role of top-down basis functions, and unfold these models into a hierarchical sparse compositional model (Hong et al., 2013; Xie et al., 2014).", "startOffset": 260, "endOffset": 297}, {"referenceID": 5, "context": "One way to get around the intractable inference problem mentioned above is to use the wake-sleep algorithm (Hinton et al., 1995) or the variational auto-encoder (Kingma & Welling, 2014; Rezende et al.", "startOffset": 107, "endOffset": 128}, {"referenceID": 20, "context": ", 1995) or the variational auto-encoder (Kingma & Welling, 2014; Rezende et al., 2014; Mnih & Gregor, 2014).", "startOffset": 40, "endOffset": 107}, {"referenceID": 2, "context": "The model in the form of exponential tilting of a reference distribution where the exponential tilting is defined by ConvNet was first proposed by Dai et al. (2015). They learned the model by a non-parametric importance sampling scheme.", "startOffset": 147, "endOffset": 165}, {"referenceID": 2, "context": "The model in the form of exponential tilting of a reference distribution where the exponential tilting is defined by ConvNet was first proposed by Dai et al. (2015). They learned the model by a non-parametric importance sampling scheme. Lu et al. (2016) proposed to learn the FRAME models based on pre-learned filters of existing ConvNets.", "startOffset": 147, "endOffset": 254}, {"referenceID": 2, "context": "The model in the form of exponential tilting of a reference distribution where the exponential tilting is defined by ConvNet was first proposed by Dai et al. (2015). They learned the model by a non-parametric importance sampling scheme. Lu et al. (2016) proposed to learn the FRAME models based on pre-learned filters of existing ConvNets. They did not learn the models from scratch. The hierarchical energy-based models (LeCun et al., 2006) were studied by the pioneering work of Hinton et al. (2006a) and Ngiam et al.", "startOffset": 147, "endOffset": 503}, {"referenceID": 2, "context": "The model in the form of exponential tilting of a reference distribution where the exponential tilting is defined by ConvNet was first proposed by Dai et al. (2015). They learned the model by a non-parametric importance sampling scheme. Lu et al. (2016) proposed to learn the FRAME models based on pre-learned filters of existing ConvNets. They did not learn the models from scratch. The hierarchical energy-based models (LeCun et al., 2006) were studied by the pioneering work of Hinton et al. (2006a) and Ngiam et al. (2011). However, their models do not correspond directly to modern ConvNet.", "startOffset": 147, "endOffset": 527}, {"referenceID": 2, "context": "The model in the form of exponential tilting of a reference distribution where the exponential tilting is defined by ConvNet was first proposed by Dai et al. (2015). They learned the model by a non-parametric importance sampling scheme. Lu et al. (2016) proposed to learn the FRAME models based on pre-learned filters of existing ConvNets. They did not learn the models from scratch. The hierarchical energy-based models (LeCun et al., 2006) were studied by the pioneering work of Hinton et al. (2006a) and Ngiam et al. (2011). However, their models do not correspond directly to modern ConvNet. Compared to the above mentioned papers, we would like to emphasize the conceptual novelty of our work. Starting from a prototype model and then unfolding it, our work reveals a curious representational structure contained in the model that involves multiple layers of activation variables. Such a representational structure is unexpected for the exponential family models, and was not studied by the papers cited above. A main motivation for this paper is to reconcile the FRAME model (Zhu et al., 1997), where the Gabor wavelets play the role of bottom-up filters, and the Olshausen-Field model (Olshausen & Field, 1997), where the wavelets play the role of top-down basis functions, and unfold these models into a hierarchical sparse compositional model (Hong et al., 2013; Xie et al., 2014). The generative ConvNet may help solve this problem. The representational structure in the generative ConvNet is similar to but subtly different from the deconvolution network of Zeiler et al. (2011). The top-down process of the generative ConvNet is controlled by multiple layers of binary activation variables computed by the bottom-up process.", "startOffset": 147, "endOffset": 1590}, {"referenceID": 12, "context": "We take h(r) = max(r, 0), the rectified linear unit (re-lu), that is commonly adopted in modern ConvNet (Krizhevsky et al., 2012).", "startOffset": 104, "endOffset": 129}, {"referenceID": 14, "context": "The discriminative ConvNet is a multinomial logistic regression (or soft-max) that is commonly used for classification (LeCun et al., 1998; Krizhevsky et al., 2012).", "startOffset": 119, "endOffset": 164}, {"referenceID": 12, "context": "The discriminative ConvNet is a multinomial logistic regression (or soft-max) that is commonly used for classification (LeCun et al., 1998; Krizhevsky et al., 2012).", "startOffset": 119, "endOffset": 164}, {"referenceID": 2, "context": "The model was first proposed by Dai et al. (2015).", "startOffset": 32, "endOffset": 50}, {"referenceID": 2, "context": "Result (a) has already been explained in Dai et al. (2015). Result (b) is stronger and is new.", "startOffset": 41, "endOffset": 59}, {"referenceID": 36, "context": "The first model in the literature that represents the clique functions by non-linear transformations of linear filter responses is the FRAME (Filters, Random field, And Maximum Entropy) model (Zhu et al., 1997).", "startOffset": 192, "endOffset": 210}, {"referenceID": 29, "context": "Based on filters {F (l\u22121) i ,\u2200i} at layer l \u2212 1, each filter F (l) k at layer l corresponds to a non-stationary FRAME model (Xie et al., 2014; Lu et al., 2016) of an image patch defined on the support of the filter, Sl, and centered at x:", "startOffset": 124, "endOffset": 159}, {"referenceID": 15, "context": "Based on filters {F (l\u22121) i ,\u2200i} at layer l \u2212 1, each filter F (l) k at layer l corresponds to a non-stationary FRAME model (Xie et al., 2014; Lu et al., 2016) of an image patch defined on the support of the filter, Sl, and centered at x:", "startOffset": 124, "endOffset": 159}, {"referenceID": 15, "context": "The model is also a generative ConvNet model, and it can generate vivid object patterns (Lu et al., 2016).", "startOffset": 88, "endOffset": 105}, {"referenceID": 15, "context": "See Lu et al. (2016) for details.", "startOffset": 4, "endOffset": 21}, {"referenceID": 29, "context": "A similar model was studied by Xie et al. (2015). The generative ConvNet can be obtained from the prototype model by unfolding the latter both convolutionally and hierarchically, but with much more involved notation that is in danger of obscuring the key ideas.", "startOffset": 31, "endOffset": 49}, {"referenceID": 27, "context": "Another justification for the Gaussian white noise distribution is that it is the limiting distribution if we zoom out a texture image due to the central limit theorem, a phenomenon called information scaling by Wu et al. (2008). The exponential tilting is to recover the non-Gaussian distribution before the central limit theorem takes effect.", "startOffset": 212, "endOffset": 229}, {"referenceID": 17, "context": "See Montufar et al. (2014) for an analysis of the number of linear pieces.", "startOffset": 4, "endOffset": 27}, {"referenceID": 30, "context": "Sparsifying the connections will make the compositions more explicit and meaningful (Xie et al., 2015; Wu et al., 2010).", "startOffset": 84, "endOffset": 119}, {"referenceID": 28, "context": "Sparsifying the connections will make the compositions more explicit and meaningful (Xie et al., 2015; Wu et al., 2010).", "startOffset": 84, "endOffset": 119}, {"referenceID": 24, "context": "The relationship between score matching and autoencoder was discovered by Vincent (2011) and Swersky et al. (2011). Our work can be considered a sharpened specialization of the above mentioned connection and relationship, where the piecewise linear structure in ConvNet greatly simplifies the matter by getting rid of the complicated second derivative terms, so that the contrastive divergence gradient becomes exactly proportional to the gradient of the reconstruction error, which is not the case in general score matching estimator.", "startOffset": 93, "endOffset": 115}, {"referenceID": 24, "context": "The relationship between score matching and autoencoder was discovered by Vincent (2011) and Swersky et al. (2011). Our work can be considered a sharpened specialization of the above mentioned connection and relationship, where the piecewise linear structure in ConvNet greatly simplifies the matter by getting rid of the complicated second derivative terms, so that the contrastive divergence gradient becomes exactly proportional to the gradient of the reconstruction error, which is not the case in general score matching estimator. Also, our work gives a novel hierarchical realization of the relationship between probability model and auto-encoder, as well as an explicit hierarchical realization of auto-encoder based sampling of Alain & Bengio (2014). The connection with Hopfied network also appears new.", "startOffset": 93, "endOffset": 758}, {"referenceID": 15, "context": "Unlike Lu et al. (2016), the generative ConvNets in our experiments are learned from scratch without relying on the pre-learned filters of existing ConvNets.", "startOffset": 7, "endOffset": 24}], "year": 2016, "abstractText": "The convolutional neural network (ConvNet or CNN) is a powerful discriminative learning machine. In this paper, we show that a generative random field model that we call generative ConvNet can be derived from the discriminative ConvNet. The probability distribution of the generative ConvNet model is in the form of exponential tilting of a reference distribution. Assuming re-lu non-linearity and Gaussian white noise reference distribution, we show that the generative ConvNet model contains a representational structure with multiple layers of binary activation variables. The model is nonGaussian, or more precisely, piecewise Gaussian, where each piece is determined by an instantiation of the binary activation variables that reconstruct the mean of the Gaussian piece. The Langevin dynamics for synthesis is driven by the reconstruction error, and the corresponding gradient descent dynamics converges to a local energy minimum that is auto-encoding. As for learning, we show that the contrastive divergence learning tends to reconstruct the observed images. Finally, we show that the maximum likelihood learning algorithm can generate realistic natural images.", "creator": "LaTeX with hyperref package"}}}