{"id": "1602.03476", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2016", "title": "Conditional Dependence via Shannon Capacity: Axioms, Estimators and Applications", "abstract": "We conduct an axiomatic study of the problem of estimating the strength of a known causal relationship between a pair of variables. We propose that an estimate of causal strength should be based on the conditional distribution of the effect given the cause (and not on the driving distribution of the cause), and study dependence measures on conditional distributions. Shannon capacity, appropriately regularized, emerges as a natural measure under these axioms. We examine the problem of calculating Shannon capacity from the observed samples and propose a novel fixed-$k$ nearest neighbor estimator, and demonstrate its consistency. Finally, we demonstrate an application to single-cell flow-cytometry, where the proposed estimators significantly reduce sample complexity.", "histories": [["v1", "Wed, 10 Feb 2016 18:27:04 GMT  (89kb,D)", "https://arxiv.org/abs/1602.03476v1", "41 pages, 3 figures"], ["v2", "Fri, 12 Feb 2016 19:32:09 GMT  (89kb,D)", "http://arxiv.org/abs/1602.03476v2", "41 pages, 3 figures, typos fixed and references added"], ["v3", "Thu, 2 Jun 2016 15:55:46 GMT  (295kb,D)", "http://arxiv.org/abs/1602.03476v3", "43 pages, 3 figures"]], "COMMENTS": "41 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.IT cs.LG math.IT stat.ML", "authors": ["weihao gao", "sreeram kannan", "sewoong oh", "pramod viswanath"], "accepted": true, "id": "1602.03476"}, "pdf": {"name": "1602.03476.pdf", "metadata": {"source": "CRF", "title": "Conditional Dependence via Shannon Capacity: Axioms, Estimators and Applications\u2217", "authors": ["Weihao Gao", "Sreeram Kannan", "Sewoong Oh", "Pramod Viswanath"], "emails": [], "sections": [{"heading": null, "text": "In this paper, we examine the relatively unexplored terrain of measures that depend only on the conditional distribution of PY. We are motivated to investigate conditional dependency measures on a problem of causal strength. In this paper, we are interested in a fundamental problem in many areas of scientific learning, where the cause-effect relationship is usually determined using interventions or sometimes directly from observational data. [Pea09, RE15, MPJ + 15]. In this paper, we are interested in an even simpler question: How to measure the strength of the relationship, for example, one can know causal genetic pathways, but only a subset of these potentially active tissues or organs that deduces how much influence a causal relationship exerts."}], "references": [{"title": "IEEE Transactions on", "author": ["Suguru Arimoto. An algorithm for computing the capacity of arbitrary discrete memoryless channels. Information Theory"], "venue": "18(1):14\u201320,", "citeRegEx": "Ari72", "shortCiteRegEx": null, "year": 1972}, {"title": "Nonparametric entropy estimation: An overview", "author": ["Jan Beirlant", "Edward J Dudewicz", "L\u00e1szl\u00f3 Gy\u00f6rfi", "Edward C Van der Meulen"], "venue": "International Journal of Mathematical and Statistical Sciences, 6(1):17\u201339,", "citeRegEx": "BDGVdM97", "shortCiteRegEx": null, "year": 1997}, {"title": "IEEE Transactions on", "author": ["Richard E Blahut. Computation of channel capacity", "rate-distortion functions. Information Theory"], "venue": "18(4):460\u2013473,", "citeRegEx": "Bla72", "shortCiteRegEx": null, "year": 1972}, {"title": "Scandinavian Journal of Statistics", "author": ["Jean Cornuet", "Jean-Michel Marin", "Antonietta Mira", "Christian P Robert. Adaptive multiple importance sampling"], "venue": "39(4):798\u2013812,", "citeRegEx": "CMMR12", "shortCiteRegEx": null, "year": 2012}, {"title": "Information theory and statistics: A tutorial", "author": ["Imre Csisz\u00e1r", "Paul C Shields"], "venue": "Now Publishers Inc,", "citeRegEx": "Csisz\u00e1r and Shields.,? \\Q2004\\E", "shortCiteRegEx": "Csisz\u00e1r and Shields.", "year": 2004}, {"title": "Generalized cutoff rates and renyi\u2019s information measures", "author": ["Imre Csisz\u00e1r"], "venue": "Information Theory, IEEE Transactions on, 41(1):26\u201334,", "citeRegEx": "Csi95", "shortCiteRegEx": null, "year": 1995}, {"title": "Entropy", "author": ["Imre Csisz\u00e1r. Axiomatic characterizations of information measures"], "venue": "10(3):261\u2013273,", "citeRegEx": "Csi08", "shortCiteRegEx": null, "year": 2008}, {"title": "Elements of information theory", "author": ["Thomas M Cover", "Joy A Thomas"], "venue": "John Wiley & Sons,", "citeRegEx": "CT12", "shortCiteRegEx": null, "year": 2012}, {"title": "The Annals of Statistics", "author": ["Luc Devroye", "Clark S Penrod. The consistency of automatic kernel density estimates"], "venue": "pages 1231\u20131249,", "citeRegEx": "DP84", "shortCiteRegEx": null, "year": 1984}, {"title": "Demystifying fixed k-nearest neighbor information estimators", "author": ["Weihao Gao", "Sewoong Oh", "Pramod Viswanath"], "venue": "arXiv preprint arXiv:1604.03006,", "citeRegEx": "GOV16", "shortCiteRegEx": null, "year": 2016}, {"title": "Efficient estimation of mutual information for strongly dependent variables", "author": ["Shuyang Gao", "Greg Ver Steeg", "Aram Galstyan"], "venue": "arXiv preprint arXiv:1411.2003,", "citeRegEx": "GSG14", "shortCiteRegEx": null, "year": 2014}, {"title": "Estimating mutual information by local gaussian approximation", "author": ["Shuyang Gao", "Greg Ver Steeg", "Aram Galstyan"], "venue": "arXiv preprint arXiv:1508.00536,", "citeRegEx": "GSG15", "shortCiteRegEx": null, "year": 2015}, {"title": "Locally parametric nonparametric density estimation", "author": ["Nils Lid Hjort", "MC Jones"], "venue": "The Annals of Statistics,", "citeRegEx": "Hjort and Jones.,? \\Q1996\\E", "shortCiteRegEx": "Hjort and Jones.", "year": 1996}, {"title": "Convexity/concavity of renyi entropy and \u03b1-mutual information", "author": ["Siu-Wai Ho", "Sergio Verd\u00fa"], "venue": "Information Theory (ISIT), 2015 IEEE International Symposium on, pages 745\u2013749. IEEE,", "citeRegEx": "HV15", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["Dominik Janzing", "David Balduzzi", "Moritz Grosse-Wentrup", "Bernhard Sch\u00f6lkopf"], "venue": "Quantifying causal influences. The Annals of Statistics, 41(5):2324\u20132358,", "citeRegEx": "JBGW13", "shortCiteRegEx": null, "year": 2013}, {"title": "Justifying Information-Geometric Causal Inference", "author": ["D. Janzing", "B. Steudel", "N. Shajarisales", "B. Sch\u00f6lkopf"], "venue": "chapter 18, pages 253\u2013265. Springer International Publishing", "citeRegEx": "JSSS15", "shortCiteRegEx": null, "year": 2015}, {"title": "Physical Review E", "author": ["Shiraj Khan", "Sharba Bandyopadhyay", "Auroop R Ganguly", "Sunil Saigal", "David J Erickson III", "Vladimir Protopopescu", "George Ostrouchov. Relative performance of mutual information estimation methods for quantifying the dependence among short", "noisy data"], "venue": "76(2):026209,", "citeRegEx": "KBG07", "shortCiteRegEx": null, "year": 2007}, {"title": "divergences and mutual informations", "author": ["Kirthevasan Kandasamy", "Akshay Krishnamurthy", "Barnabas Poczos", "Larry Wasserman. Nonparametric von mises estimators for entropies"], "venue": "Advances in Neural Information Processing Systems, pages 397\u2013405,", "citeRegEx": "KKPW15", "shortCiteRegEx": null, "year": 2015}, {"title": "Problemy Peredachi Informatsii", "author": ["LF Kozachenko", "Nikolai N Leonenko. Sample estimate of the entropy of a random vector"], "venue": "23(2):9\u201316,", "citeRegEx": "KL87", "shortCiteRegEx": null, "year": 1987}, {"title": "Estimating mutual information", "author": ["A. Kraskov", "H. St\u00f6gbauer", "P. Grassberger"], "venue": "Physical review E, 69(6):066138", "citeRegEx": "KSG04", "shortCiteRegEx": null, "year": 2004}, {"title": "Dana Pe\u2019er", "author": ["Smita Krishnaswamy", "Matthew H Spitzer", "Michael Mingueneau", "Sean C Bendall", "Oren Litvin", "Erica Stone"], "venue": "and Garry P Nolan. Conditional density-based analysis of t cell signaling in single-cell data. Science, 346(6213):1250689,", "citeRegEx": "KSM14", "shortCiteRegEx": null, "year": 2014}, {"title": "Local likelihood density estimation", "author": ["Clive R Loader"], "venue": "The Annals of Statistics,", "citeRegEx": "Loader,? \\Q1996\\E", "shortCiteRegEx": "Loader", "year": 1996}, {"title": "Nonparametric k-nearest-neighbor entropy estimator", "author": ["Damiano Lombardi", "Sanjay Pant"], "venue": "Physical Review E,", "citeRegEx": "Lombardi and Pant.,? \\Q2016\\E", "shortCiteRegEx": "Lombardi and Pant.", "year": 2016}, {"title": "Modeling trends in distributions", "author": ["Jonas Mueller", "Tommi Jaakkola", "David Gifford"], "venue": "arXiv preprint arXiv:1511.04486,", "citeRegEx": "MJG15", "shortCiteRegEx": null, "year": 2015}, {"title": "Signal processing", "author": ["Rudy Moddemeijer. On estimation of entropy", "mutual information of continuous distributions"], "venue": "16(3):233\u2013248,", "citeRegEx": "Mod89", "shortCiteRegEx": null, "year": 1989}, {"title": "Distinguishing cause from effect using observational data: methods and benchmarks", "author": ["J.M. Mooij", "J. Peters", "D. Janzing", "J. Zscheischler", "B. Sch\u00f6lkopf"], "venue": "Journal of Machine Learning Research", "citeRegEx": "MPJ15", "shortCiteRegEx": null, "year": 2015}, {"title": "Monte Carlo theory", "author": ["Art B. Owen"], "venue": "methods and examples.", "citeRegEx": "Owe13", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural computation", "author": ["Liam Paninski. Estimation of entropy", "mutual information"], "venue": "15(6):1191\u20131253,", "citeRegEx": "Pan03", "shortCiteRegEx": null, "year": 2003}, {"title": "Causality", "author": ["Judea Pearl"], "venue": "Cambridge university press,", "citeRegEx": "Pea09", "shortCiteRegEx": null, "year": 2009}, {"title": "In Advances in Neural Information Processing Systems", "author": ["D\u00e1vid P\u00e1l", "Barnab\u00e1s P\u00f3czos", "Csaba Szepesv\u00e1ri. Estimation of r\u00e9nyi entropy", "mutual information based on generalized nearest-neighbor graphs"], "venue": "pages 1849\u20131857,", "citeRegEx": "PPS10", "shortCiteRegEx": null, "year": 2010}, {"title": "and Computing (Allerton)", "author": ["Yury Polyanskiy", "Sergio Verd\u00fa. Arimoto channel coding converse", "r\u00e9nyi divergence. In Communication", "Control"], "venue": "2010 48th Annual Allerton Conference on, pages 1327\u20131333. IEEE,", "citeRegEx": "PV10", "shortCiteRegEx": null, "year": 2010}, {"title": "Nonparametric divergence estimation with applications to machine learning on distributions", "author": ["Barnab\u00e1s P\u00f3czos", "Liang Xiong", "Jeff Schneider"], "venue": "arXiv preprint arXiv:1202.3758,", "citeRegEx": "PXS12", "shortCiteRegEx": null, "year": 2012}, {"title": "Non-parametric causal models", "author": ["Robin J Richardson", "Thomas S Evans"], "venue": null, "citeRegEx": "Richardson and Evans.,? \\Q2015\\E", "shortCiteRegEx": "Richardson and Evans.", "year": 2015}, {"title": "Acta mathematica hungarica", "author": ["Alfr\u00e9d R\u00e9nyi. On measures of dependence"], "venue": "10(3-4):441\u2013451,", "citeRegEx": "R\u00e9n59", "shortCiteRegEx": null, "year": 1959}, {"title": "A mathematical theory of communication", "author": ["C.E. Shannon"], "venue": "Bell System Tech. J., 27:379423 and 623656", "citeRegEx": "Sha48", "shortCiteRegEx": null, "year": 1948}, {"title": "A reliable data-based bandwidth selection method for kernel density estimation", "author": ["Simon J Sheather", "Michael C Jones"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Sheather and Jones.,? \\Q1991\\E", "shortCiteRegEx": "Sheather and Jones.", "year": 1991}, {"title": "Telling cause from effect in deterministic linear dynamical systems", "author": ["N. Shajarisales", "D. Janzing", "B. Sch\u00f6lkopf", "M. Besserve"], "venue": "Proceedings of the 32nd International Conference on Machine Learning, volume 37 of JMLR Workshop and Conference Proceedings, page 285?294. JMLR", "citeRegEx": "SJSB15", "shortCiteRegEx": null, "year": 2015}, {"title": "American journal of mathematical and management sciences", "author": ["Harshinder Singh", "Neeraj Misra", "Vladimir Hnizdo", "Adam Fedorowicz", "Eugene Demchuk. Nearest neighbor estimates of entropy"], "venue": "23(3-4):301\u2013321,", "citeRegEx": "SMH03", "shortCiteRegEx": null, "year": 2003}, {"title": "Empirical estimation of entropy functionals with confidence", "author": ["Kumar Sricharan", "Raviv Raich", "Alfred O Hero III"], "venue": "arXiv preprint arXiv:1012.4188,", "citeRegEx": "SRHI10", "shortCiteRegEx": null, "year": 2010}, {"title": "Scandinavian Journal of Statistics", "author": ["Alexandre B Tsybakov", "EC Van der Meulen. Root-n consistent estimators of entropy for densities with unbounded support"], "venue": "pages 75\u201383,", "citeRegEx": "TVdM96", "shortCiteRegEx": null, "year": 1996}, {"title": "IEEE Transactions on", "author": ["Tim Van Erven", "Peter Harremo\u00ebs. R\u00e9nyi divergence", "kullback-leibler divergence. Information Theory"], "venue": "60(7):3797\u20133820,", "citeRegEx": "VEH14", "shortCiteRegEx": null, "year": 2014}, {"title": "IEEE Transactions on", "author": ["Qing Wang", "Sanjeev R Kulkarni", "Sergio Verd\u00fa. Divergence estimation for multidimensional densities via-nearest-neighbor distances. Information Theory"], "venue": "55(5):2392\u20132405,", "citeRegEx": "WKV09", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 14, "context": "In fact, this measure is taken as a starting point to develop an axiomatic approach to studying causal strength on general graphs in [JBGW13].", "startOffset": 133, "endOffset": 141}, {"referenceID": 20, "context": "In a recent work [KSM14], potential causal influence is posited as a relevant metric to spot \u201ctrends\u201d in gene pathways.", "startOffset": 17, "endOffset": 24}, {"referenceID": 19, "context": "The estimator brings together ideas from three disparate threads in statistical estimation theory: nearest-neighbor methods, a correlation boosting idea in the estimation of (standard) mutual information from samples [KSG04], and importance sampling.", "startOffset": 217, "endOffset": 224}, {"referenceID": 18, "context": "The estimator has only a single hyper parameter (the number of nearest-neighbors considered, set to 4 or 5 in practice), uses an offthe-shelf kernel density estimator of only PX , and has strong connections to the entropy estimator of [KL87].", "startOffset": 235, "endOffset": 241}, {"referenceID": 24, "context": "In simulations, the estimator has very strong performance in terms of sample complexity (compared to a baseline of the partition-based estimator in [Mod89]).", "startOffset": 148, "endOffset": 155}, {"referenceID": 20, "context": "\u2022 Application to gene pathway influence: In [KSM14], considered an important result in single-cell flow-cytometry data analysis, a causal strength metric (termed DREMI) is proposed for measuring the causal influence of a gene \u2013 this estimator is a specific way of implementing UMI along with a \u201cchannel amplification\u201d step, and DREMI was successfully used to spot gene-pathway trends.", "startOffset": 44, "endOffset": 51}, {"referenceID": 7, "context": "Chapter 2 of [CT12].", "startOffset": 13, "endOffset": 19}, {"referenceID": 7, "context": "\u2022 Axiom 2: This is a standard result for Shannon capacity and we refer the interested reader to Chapter 7 of [CT12].", "startOffset": 109, "endOffset": 115}, {"referenceID": 20, "context": "This estimator is motivated by the recent work in [KSM14].", "startOffset": 50, "endOffset": 57}, {"referenceID": 33, "context": "This is also true of other measures of relation such as the Renyi correlation [R\u00e9n59], and in each case the measure is studied in the context of some form penalty term.", "startOffset": 78, "endOffset": 85}, {"referenceID": 20, "context": "1 Uniform Mutual Information The idea of applying UMI to infer the strength of conditional dependence was first proposed in [KSM14].", "startOffset": 124, "endOffset": 131}, {"referenceID": 18, "context": "[KL87]; (b) the correlation boosting idea of the estimator from [KSG04]\u2013which is widely adopted in practice [KBG07]; and (c) the importance sampling techniques to adjust for the uniform prior for UMI.", "startOffset": 0, "endOffset": 6}, {"referenceID": 19, "context": "[KL87]; (b) the correlation boosting idea of the estimator from [KSG04]\u2013which is widely adopted in practice [KBG07]; and (c) the importance sampling techniques to adjust for the uniform prior for UMI.", "startOffset": 64, "endOffset": 71}, {"referenceID": 16, "context": "[KL87]; (b) the correlation boosting idea of the estimator from [KSG04]\u2013which is widely adopted in practice [KBG07]; and (c) the importance sampling techniques to adjust for the uniform prior for UMI.", "startOffset": 108, "endOffset": 115}, {"referenceID": 1, "context": "Note that three applications of the entropy estimator, such as those from [BDGVdM97], gives an estimate of the mutual information, i.", "startOffset": 74, "endOffset": 84}, {"referenceID": 18, "context": "Alternatively, to bypass estimating PXY at every point, the differential entropy estimation can be done via k nearest neighbor (kNN) methods (pioneering work in [KL87]).", "startOffset": 161, "endOffset": 167}, {"referenceID": 19, "context": "Perhaps surprisingly, an innovative approach undertaken in [KSG04] to improve upon three applications of KL estimators provides a solution.", "startOffset": 59, "endOffset": 66}, {"referenceID": 19, "context": "The KSG estimator of [KSG04] is based on kNN distance \u03c1k,i defined as the distance to the k-th nearest neighbor from (Xi, Yi) in `\u221e distance, i.", "startOffset": 21, "endOffset": 28}, {"referenceID": 9, "context": "However, despite its popularity in practice due to its simplicity, no convergence result has been known until very recently (when [GOV16] showed some consistency and rate of convergence properties).", "startOffset": 130, "endOffset": 137}, {"referenceID": 3, "context": "where X \u2286 Rx , Y \u2286 Ry , cd = \u03c0 d 2 /\u0393(d2 + 1) is the volume of d-dimensional unit ball, and wi is the self-normalized importance sampling estimate [CMMR12] of u(Xi) f(Xi) : wi \u2261 N/f\u0303(Xi) \u2211N j=1 ( 1/f\u0303(Xj) ) , (16)", "startOffset": 147, "endOffset": 155}, {"referenceID": 19, "context": "For each sample (Xi, Yi), calculate the Euclidean distance \u03c1k,i (as opposed to the `\u221e distance proposed by [KSG04]) to the k-th nearest neighbor.", "startOffset": 107, "endOffset": 114}, {"referenceID": 18, "context": "As far as we know, the only estimator based on fixed-k nearest neighbors that is known to be consistent is the entropy estimator of [KL87], and the convergence rate is only known for the univariate case [TVdM96] (and that too under significant assumptions on the univariate density).", "startOffset": 132, "endOffset": 138}, {"referenceID": 39, "context": "As far as we know, the only estimator based on fixed-k nearest neighbors that is known to be consistent is the entropy estimator of [KL87], and the convergence rate is only known for the univariate case [TVdM96] (and that too under significant assumptions on the univariate density).", "startOffset": 203, "endOffset": 211}, {"referenceID": 18, "context": "Uniform Mutual Information: As our estimators use the off-the-shelf kernel density estimator of PX [DP84, SJ91] and also the ides from the nearest-neighbor methods [KL87], we make assumptions on the conditional density fY |X that are typical in these literature.", "startOffset": 164, "endOffset": 170}, {"referenceID": 20, "context": "1 Gene Causal Strength from Single Cell Data We briefly describe the setup of [KSM14] to motivate our numerical experiments.", "startOffset": 78, "endOffset": 85}, {"referenceID": 20, "context": "Figure 1: CMI and UMI estimators significantly improve over DREMI in capturing the biological trend in flow-cytometry data: the figures above refer to the same setting as Figure 6 of [KSM14].", "startOffset": 183, "endOffset": 190}, {"referenceID": 20, "context": "These samples are obtained using a technique called single-cell mass flow cytometry, see [KSM14] for details.", "startOffset": 89, "endOffset": 96}, {"referenceID": 20, "context": "Such an analysis is conducted in [KSM14] where the causal strength function C is evaluated via the so-called DREMI estimator (essentially a version of UMI estimation with a \u201cchannel amplification\u201d step and careful choice of hyper parameters therein \u2013 no theoretical properties of this estimator were evaluated).", "startOffset": 33, "endOffset": 40}, {"referenceID": 20, "context": "This demonstrates the utility of DREMI for causal strength inference in gene networks (see Figure 6 of [KSM14]).", "startOffset": 103, "endOffset": 110}, {"referenceID": 23, "context": "As an aside, we note that a somewhat different set of \u201ctrend spotting\u201d estimators, primarily trying to find genes which demonstrate a monotonic trend over time from single-cell RNA-sequencing data, have been proposed very recently in [MJG15].", "startOffset": 234, "endOffset": 241}, {"referenceID": 20, "context": "It is natural to apply our estimators to each time point in the same setting as [KSM14] \u2013 and look to understand two distinct issues in our experiments with the flow-cytometry data.", "startOffset": 80, "endOffset": 87}, {"referenceID": 20, "context": "The second question relates to the sample complexity: how does the ability to recover the trend vary as a function of the sample complexity? To study this, we subsample the original data from [KSM14] multiple times (100 in the experiments) at each subsampling ratio and compute the fraction of times we recover the true biological trend.", "startOffset": 192, "endOffset": 199}, {"referenceID": 24, "context": "Figure 2: The proposed UMI estimator significantly outperforms partition based methods [Mod89] in sample complexity.", "startOffset": 87, "endOffset": 94}, {"referenceID": 24, "context": "This is compared to the ground truth and the state-of-the-art partition based estimators from [Mod89].", "startOffset": 94, "endOffset": 101}, {"referenceID": 19, "context": "The ground truth has been computed via simulations with 8192 samples from the desired distribution PY |XUX using Kraskov\u2019s mutual information estimator [KSG04].", "startOffset": 152, "endOffset": 159}, {"referenceID": 19, "context": "The consistency proofs suggest that a similar analysis the very popular estimator of (traditional) mutual information in [KSG04] can be conducted successfully; such work has been recently conducted in [GOV16].", "startOffset": 121, "endOffset": 128}, {"referenceID": 9, "context": "The consistency proofs suggest that a similar analysis the very popular estimator of (traditional) mutual information in [KSG04] can be conducted successfully; such work has been recently conducted in [GOV16].", "startOffset": 201, "endOffset": 208}, {"referenceID": 39, "context": "Rates of convergence in the nearest-neighbor methods are barely known in the literature even for traditional information theoretic quantities: for instance, [TVdM96] derives a \u221a N consistency for the single dimensional case of differential entropy estimation (under strong assumptions on the underlying", "startOffset": 157, "endOffset": 165}, {"referenceID": 9, "context": "pdf), leaving higher dimensional scenarios open, and which recently have been successfully addressed in [GOV16].", "startOffset": 104, "endOffset": 111}, {"referenceID": 18, "context": "(b) There is a natural generalization of our estimators when the alphabet Y is high dimensional, using the kNN approach (just as in the differential entropy estimator of [KL87] or in the mutual information estimator of [KSG04]).", "startOffset": 170, "endOffset": 176}, {"referenceID": 19, "context": "(b) There is a natural generalization of our estimators when the alphabet Y is high dimensional, using the kNN approach (just as in the differential entropy estimator of [KL87] or in the mutual information estimator of [KSG04]).", "startOffset": 219, "endOffset": 226}, {"referenceID": 28, "context": "This is a widely studied topic with a long lineage [Pea09] but also of strong topical interest [JBGW13, JSSS15, MPJ15, SJSB15].", "startOffset": 51, "endOffset": 58}, {"referenceID": 25, "context": "A natural inclination is to explore the efficacy of UMI and CMI measures to test for direction of causality \u2013 especially in the context of the benchmark data sets collected in [MPJ15].", "startOffset": 176, "endOffset": 183}, {"referenceID": 18, "context": "Directly comparing the marginal entropy H(X) and H(Y ) by the estimator in [KL87] also only provides 45% accuracy.", "startOffset": 75, "endOffset": 81}, {"referenceID": 25, "context": "While in [MPJ15], different entropy estimators (with appropriate hyper parameter choices) were applied to get an accuracy up to 60%-70%.", "startOffset": 9, "endOffset": 16}, {"referenceID": 25, "context": "Further research is needed to shed conclusive light, although we point out that the benchmark data sets in [MPJ15] have substantial confounding factors that make causal direction hard to measure in the first place.", "startOffset": 107, "endOffset": 114}, {"referenceID": 5, "context": "Now define the asymmetric information measure [Csi95]: K\u03bb(PXPY |X) := inf QY D\u03bb(PXY \u2016PXQY ), (25) which converges to the traditional mutual information when \u03bb \u2192 1.", "startOffset": 46, "endOffset": 53}, {"referenceID": 20, "context": "In the light of this result, it would be interesting to design estimators for the more general family of R\u00e9nyi capacity measures and confirm their performance on empirical tasks such as the ones studied in [KSM14].", "startOffset": 206, "endOffset": 213}, {"referenceID": 6, "context": "It would also be very interesting to understand the role of additional axioms that would lead to uniqueness of Shannon capacity (in the same spirit as entropy being uniquely characterized by somewhat similar axioms [Csi08]).", "startOffset": 215, "endOffset": 222}, {"referenceID": 0, "context": "References [Ari72] Suguru Arimoto.", "startOffset": 11, "endOffset": 18}, {"referenceID": 1, "context": "[BDGVdM97] Jan Beirlant, Edward J Dudewicz, L\u00e1szl\u00f3 Gy\u00f6rfi, and Edward C Van der Meulen.", "startOffset": 0, "endOffset": 10}, {"referenceID": 2, "context": "[Bla72] Richard E Blahut.", "startOffset": 0, "endOffset": 7}, {"referenceID": 3, "context": "[CMMR12] Jean Cornuet, Jean-Michel Marin, Antonietta Mira, and Christian P Robert.", "startOffset": 0, "endOffset": 8}, {"referenceID": 5, "context": "[Csi95] Imre Csisz\u00e1r.", "startOffset": 0, "endOffset": 7}, {"referenceID": 6, "context": "[Csi08] Imre Csisz\u00e1r.", "startOffset": 0, "endOffset": 7}, {"referenceID": 7, "context": "[CT12] Thomas M Cover and Joy A Thomas.", "startOffset": 0, "endOffset": 6}, {"referenceID": 8, "context": "[DP84] Luc Devroye and Clark S Penrod.", "startOffset": 0, "endOffset": 6}, {"referenceID": 9, "context": "[GOV16] Weihao Gao, Sewoong Oh, and Pramod Viswanath.", "startOffset": 0, "endOffset": 7}, {"referenceID": 10, "context": "[GSG14] Shuyang Gao, Greg Ver Steeg, and Aram Galstyan.", "startOffset": 0, "endOffset": 7}, {"referenceID": 11, "context": "[GSG15] Shuyang Gao, Greg Ver Steeg, and Aram Galstyan.", "startOffset": 0, "endOffset": 7}, {"referenceID": 13, "context": "[HV15] Siu-Wai Ho and Sergio Verd\u00fa.", "startOffset": 0, "endOffset": 6}, {"referenceID": 14, "context": "[JBGW13] Dominik Janzing, David Balduzzi, Moritz Grosse-Wentrup, Bernhard Sch\u00f6lkopf, et al.", "startOffset": 0, "endOffset": 8}, {"referenceID": 15, "context": "[JSSS15] D.", "startOffset": 0, "endOffset": 8}, {"referenceID": 16, "context": "[KBG07] Shiraj Khan, Sharba Bandyopadhyay, Auroop R Ganguly, Sunil Saigal, David J Erickson III, Vladimir Protopopescu, and George Ostrouchov.", "startOffset": 0, "endOffset": 7}, {"referenceID": 17, "context": "[KKPW15] Kirthevasan Kandasamy, Akshay Krishnamurthy, Barnabas Poczos, and Larry Wasserman.", "startOffset": 0, "endOffset": 8}, {"referenceID": 18, "context": "[KL87] LF Kozachenko and Nikolai N Leonenko.", "startOffset": 0, "endOffset": 6}, {"referenceID": 19, "context": "[KSG04] A.", "startOffset": 0, "endOffset": 7}, {"referenceID": 20, "context": "[KSM14] Smita Krishnaswamy, Matthew H Spitzer, Michael Mingueneau, Sean C Bendall, Oren Litvin, Erica Stone, Dana Pe\u2019er, and Garry P Nolan.", "startOffset": 0, "endOffset": 7}, {"referenceID": 23, "context": "[MJG15] Jonas Mueller, Tommi Jaakkola, and David Gifford.", "startOffset": 0, "endOffset": 7}, {"referenceID": 24, "context": "[Mod89] Rudy Moddemeijer.", "startOffset": 0, "endOffset": 7}, {"referenceID": 25, "context": "[MPJ15] J.", "startOffset": 0, "endOffset": 7}, {"referenceID": 26, "context": "[Owe13] Art B.", "startOffset": 0, "endOffset": 7}, {"referenceID": 27, "context": "[Pan03] Liam Paninski.", "startOffset": 0, "endOffset": 7}, {"referenceID": 28, "context": "[Pea09] Judea Pearl.", "startOffset": 0, "endOffset": 7}, {"referenceID": 29, "context": "[PPS10] D\u00e1vid P\u00e1l, Barnab\u00e1s P\u00f3czos, and Csaba Szepesv\u00e1ri.", "startOffset": 0, "endOffset": 7}, {"referenceID": 30, "context": "[PV10] Yury Polyanskiy and Sergio Verd\u00fa.", "startOffset": 0, "endOffset": 6}, {"referenceID": 31, "context": "[PXS12] Barnab\u00e1s P\u00f3czos, Liang Xiong, and Jeff Schneider.", "startOffset": 0, "endOffset": 7}, {"referenceID": 33, "context": "[R\u00e9n59] Alfr\u00e9d R\u00e9nyi.", "startOffset": 0, "endOffset": 7}, {"referenceID": 34, "context": "[Sha48] C.", "startOffset": 0, "endOffset": 7}, {"referenceID": 36, "context": "[SJSB15] N.", "startOffset": 0, "endOffset": 8}, {"referenceID": 37, "context": "[SMH03] Harshinder Singh, Neeraj Misra, Vladimir Hnizdo, Adam Fedorowicz, and Eugene Demchuk.", "startOffset": 0, "endOffset": 7}, {"referenceID": 38, "context": "[SRHI10] Kumar Sricharan, Raviv Raich, and Alfred O Hero III.", "startOffset": 0, "endOffset": 8}, {"referenceID": 39, "context": "[TVdM96] Alexandre B Tsybakov and EC Van der Meulen.", "startOffset": 0, "endOffset": 8}, {"referenceID": 40, "context": "[VEH14] Tim Van Erven and Peter Harremo\u00ebs.", "startOffset": 0, "endOffset": 7}, {"referenceID": 41, "context": "[WKV09] Qing Wang, Sanjeev R Kulkarni, and Sergio Verd\u00fa.", "startOffset": 0, "endOffset": 7}, {"referenceID": 26, "context": "1 in [Owe13]).", "startOffset": 5, "endOffset": 12}, {"referenceID": 37, "context": "By Theorem 8 of [SMH03], we have", "startOffset": 16, "endOffset": 23}, {"referenceID": 29, "context": "Similar interchange of limit has been used in [KL87, WKV09] without the regularization; in this context [PPS10] claims that this step is not justified (although no counterexample is pointed out).", "startOffset": 104, "endOffset": 111}, {"referenceID": 37, "context": "(52) Moreover, by Theorem11 of [SMH03], we have:", "startOffset": 31, "endOffset": 38}, {"referenceID": 37, "context": "[SMH03], we have lim N\u2192\u221e E [ log f\u0302Y |X(Yi|Xi) \u2223\u2223(Xi, Yi) = (x, y)] = log fY |X(y|x) , (134)", "startOffset": 0, "endOffset": 7}, {"referenceID": 37, "context": "[SMH03], we have:", "startOffset": 0, "endOffset": 7}, {"referenceID": 5, "context": "\u2022 Clearly Axiom 0 holds \u2013 it follows from a standard result that D\u03bb = 0 if and only if P = Q almost everywhere [Csi95].", "startOffset": 111, "endOffset": 118}, {"referenceID": 30, "context": "Equation (55) in [PV10]), we get CMI\u03bb(PY |X) = max PX K\u03bb(PXPY |X) \u2265 K\u03bb(P \u2217 XPY |X) \u2265 K\u03bb(P \u2217 XPZ|X) = CMI\u03bb(PZ|X).", "startOffset": 17, "endOffset": 23}, {"referenceID": 40, "context": "Theorem 27 of [VEH14].", "startOffset": 14, "endOffset": 21}, {"referenceID": 5, "context": "\u2022 Axiom 3a: The information-centroid representation for CMI\u03bb states that (see [Csi95] or Equation (44) of [PV10]): CMI\u03bb(PY |X) = min QY max x D\u03bb(PY |X=x\u2016QY ).", "startOffset": 78, "endOffset": 85}, {"referenceID": 30, "context": "\u2022 Axiom 3a: The information-centroid representation for CMI\u03bb states that (see [Csi95] or Equation (44) of [PV10]): CMI\u03bb(PY |X) = min QY max x D\u03bb(PY |X=x\u2016QY ).", "startOffset": 106, "endOffset": 112}, {"referenceID": 40, "context": "Theorem 13 in [VEH14]), we get, D\u03bb(PY |X=x\u2032\u2016qY ) = D\u03bb( \u2211", "startOffset": 14, "endOffset": 21}, {"referenceID": 13, "context": "Theorem 1 of [HV15]) implies that PY = UY , the uniform distribution and the deterministic function is onto.", "startOffset": 13, "endOffset": 19}], "year": 2016, "abstractText": "We consider axiomatically the problem of estimating the strength of a conditional dependence relationship PY |X from a random variables X to a random variable Y . This has applications in determining the strength of a known causal relationship, where the strength depends only on the conditional distribution of the effect given the cause (and not on the driving distribution of the cause). Shannon capacity, appropriately regularized, emerges as a natural measure under these axioms. We examine the problem of calculating Shannon capacity from the observed samples and propose a novel fixed-k nearest neighbor estimator, and demonstrate its consistency. Finally, we demonstrate an application to single-cell flow-cytometry, where the proposed estimators significantly reduce sample complexity.", "creator": "LaTeX with hyperref package"}}}