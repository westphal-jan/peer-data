{"id": "1602.03552", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2016", "title": "Learning privately from multiparty data", "abstract": "Learning a classifier from private data collected by multiple parties is an important problem that has many potential applications. How can we build an accurate and differentially private global classifier by combining locally-trained classifiers from different parties, without access to any party's private data? We propose to transfer the `knowledge' of the local classifier ensemble by first creating labeled data from auxiliary unlabeled data, and then train a global $\\epsilon$-differentially private classifier. We show that majority voting is too sensitive and therefore propose a new risk weighted by class probabilities estimated from the ensemble. Relative to a non-private solution, our private solution has a generalization error bounded by $O(\\epsilon^{-2}M^{-2})$ where $M$ is the number of parties. This allows strong privacy without performance loss when $M$ is large, such as in crowdsensing applications. We demonstrate the performance of our method with realistic tasks of activity recognition, network intrusion detection, and malicious URL detection.", "histories": [["v1", "Wed, 10 Feb 2016 22:02:43 GMT  (352kb,D)", "http://arxiv.org/abs/1602.03552v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CR", "authors": ["jihun hamm", "yingjun cao", "mikhail belkin"], "accepted": true, "id": "1602.03552"}, "pdf": {"name": "1602.03552.pdf", "metadata": {"source": "META", "title": "Learning Privately from Multiparty Data", "authors": ["Jihun Hamm", "Paul Cao", "Mikhail Belkin"], "emails": ["HAMMJ@CSE.OHIO-STATE.EDU", "YIC242@ENG.UCSD.EDU", "MBELKIN@CSE.OHIO-STATE.EDU"], "sections": [{"heading": "1. Introduction", "text": "This year it is more than ever before."}, {"heading": "2. Preliminary", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Differential privacy", "text": "A randomized algorithm that takes DataD as input and outputs a function f is privately called ifP (f (D), enhanced S), \u2264 e (1) for all measurable S-T of the output range and for all datasets D and D \"that differ in a single item designated by D \u00b2 D. That is, even if an adversary knows the entire dataset D except for a single item, she cannot deduce much about the unknown item from the output f of the algorithm. If an algorithm outputs a real vector f \u00b2 RD, its global L2 sensitivity (Dwork et al., 2006) can be defined as S (f) = max D \u00b2 f (D) \u2212 f (D \u00b2) \u2212 f (D \u00b2), with the L2 standard in effect. An important result from (Dwork et al., 2006) is that a vector-weighted output f with sensitivity (D), D \u2212 f (D) (D \u2212 (D), D \u2212 (D \u2212), and D \u2212 P (D \u2212) can be generated."}, {"heading": "2.2. Output perturbation", "text": "When a classifier that minimizes empirical risk is published in the public, information about the preceding working conditions trickles through. Such a classifier can be cleaned up by interference with additive noise calibrated to the sensitivity of the classifier, known as the output disturbance method (Chaudhuri et al., 2011). Specifically, the authors show that if the minimizer of the regulated empirical risk map (w) = 1N (x, y).S l (h, w), y) + \u03bb 2, (4) then the interfered output wp = ws + \u03b7, p (\u03b7).N\u03bb 2 is differently private for a single sample. Output disturbance was used to remediate the averaged parameters in (Pathak et al., 2010)."}, {"heading": "3. Transferring knowledge of ensemble", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Local classifiers", "text": "In this thesis, we treat the local classifiers as M black boxes h1 (x),..., hM (x). We assume that a local classifier may be hi (x) 1Local classifiers of any type. Training is done with his private i.i.d. training set S (i) S (i) S (i) = {(x (i) 1, y (i) 1),..., (x (i) Ni (i) Ni)}, (5) where (x (i) j, y (i) j) x {\u2212 1, 1} is a sample of a distribution P (x, y) common to all parties. We consider the binary labels y (\u2212 1, 1} in the main paper and present in Appendix B. This distribution of training data between the parties is similar to the dredging process (Breiman, 1996a) with some differences. In Bagging, the training set S (i) for the party is offset with substitution of the entire training process from 1999 as compared to the one D, etc."}, {"heading": "3.2. Privacy issues of direct release", "text": "In the first step of our method, local classifiers are first collected by a trusted entity from multiple parties. A na\u00efve approach to using the ensemble is to release the local classification parameters directly to the parties after cleanup, but this is problematic in terms of efficiency and privacy. Releasing all M classification parameters is a process with constant sensitivity, as opposed to releasing insensitive statistics such as an average whose sensitivity is O (M \u2212 1). Releasing the classifiers requires much more interference than necessary, resulting in a steep loss of performance of sanitized classifiers. In addition, efficient, differentiated private mechanisms are so far known only for certain types of classifiers (see (Ji et al., 2014) for a verification. Another approach is to use the ensemble as a service to make predictions for test data, followed by appropriate cleansing. Suppose we use majority tuning to realistically predict a test sample for a number of responses, such as a private test sample."}, {"heading": "3.3. Leveraging auxiliary data", "text": "s knowledge to a global classifier that uses auxiliary data without labels. Specifically, we use the ensemble to generate (pseudo) labels for the auxiliary data, which in turn are used to train a global classifier. Compared to releasing auxiliary data directly from local classifiers, releasing a global classifier trained on auxiliary data is a much less sensitive process with O (M \u2212 1) (paragraph 4.4) analogous to publishing an average statistic. The number of auxiliary samples does not affect privacy, and the larger the data, the closer the global classifier is to the original ensemble with O (N \u2212 1) limitation (paragraph 4.4). Also, compared to using the ensemble for prediction queries for a Swer, the sanitized global classifier can be used as often as necessary without affecting its privacy."}, {"heading": "4. Finding a global private classifier", "text": "In the first experiment, we use the majority vote of an ensemble to assign auxiliary data to labels, and find a global classifier from the usual ERM procedure. In the second experiment, we present a better approach, using the ensemble to estimate the posterior P (y | x) of the auxiliary samples and to solve a \"softly marked\" weighted empirical minimization."}, {"heading": "4.1. First attempt: ERM with majority voting", "text": "As a first experiment, we use majority decisions of M-Local Classifiers to generate auxiliary data and analyze their impact.Majority decisions for binary classification are the following rule: v (x) = {1, if we use M i = 1 I [hi (x) = 1] \u2265 M 2 \u2212 1, otherwise. (6) Ties can be ignored by assuming an odd number of M parties. Regardless of local classifier types or how they are trained, we can consider the majority choice of ensemble {h1,..., hM} as a deterministic target concept to train a global classification. The majority selected auxiliary data are p = {(x1, v (x1)),..., (xN, v (xN))}}}, (7) where xi-X is an i.i.d. sample from the same distribution P (x) as private data."}, {"heading": "4.2. Performance issues of majority voting", "text": "In (Chaudhuri et al., 2011) it is shown that the expected risk of a production-disrupted ERM solution wp in relation to the risk of a reference hypothesis w0 is limited by two terms - one due to noise and another due to the gap between expected and empirically regulated risks. This result applies to the majority ERM with minor modifications. The sensitivity of the majority ERM from theorem 1 is 2\u03bb compared to 2 N\u03bb of a standard ERM and corresponds to the error-bound isR (wp) \u2264 R (w0) + O (-2) + O (N \u2212 1), (11) with high probability and without taking other variables into account. Unfortunately, the limit does not guarantee successful learning due to the constant gap O (-2), which can be large for a small period of time."}, {"heading": "4.3. Better yet: weighted ERM with soft labels", "text": "The main problem with the majority vote was its sensitivity to the decision of an individual party. Let \u03b1 (x) the fraction of positive votes of the M classifiers (who gave a sample: \u03b1 (x) = 1M M \u2211 j = 0.5 I [hj (x) = 1]. (12) With respect to alpha, the initial loss l (wTxv (x)) is written for the majority vote (x) if the fraction of votes is abruptly changed when the fraction (x) crosses the boundary. We rerepresent the situation by introducing the new weighted loss: l\u03b1 (\u00b7) = alpha (wTx) l (1 \u2212 wTx) l (1 \u2212 alpha) l (x) l (wTx) l: The new loss has the following properties. (14) If the M votes of a sample are clearly positive or alpha, the loss is equal."}, {"heading": "4.4. Privacy and performance", "text": "Compared with theorem 1 for majority decisions with a noise of P (\u03b7) and 2 (\u03bb 2), we have the following result: Theorem 3. Disturbed noise wp = ws + \u03b7 of algorithm 2 with a noise of p (\u03b7), e \u2212 M\u03bb 2 (\u03b7), is -differentially private.That is, we now need 1 / M times less noise to achieve the same -differential privacy. This directly affects the performance of the corresponding global classifier as follows: Theorem 4. Let us be some reference hypothesis. Then, with a probability of at least 1 \u2212 \u03b4s via the data protection mechanism (\u03b4p) and via the selection of samples (wp), \u2264 R (w0) + 4\u00ba (d / \u0441p) log2 (d / \u0441p), the privacy of which 2 + 16 log (1 + wp), (wp), (w0) + 4\u00ba) log2 (d / \u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0441\u0441\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0441\u0438\u0441\u0438\u0441\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0441\u0441\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0441\u0441\u0441\u0438\u0441\u0438\u0441\u0441\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0441\u0441\u0438\u0441\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0441\u0438\u0441\u0438\u043d\u0435\u0441\u0438\u0441\u0441\u0438\u043d\u0435\u0441\u0438\u043d\u0435\u0441\u0438\u043d\u0435\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0441\u0438\u0441\u0441\u0441\u0438\u043d\u0435\u0441\u0441\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u043d\u0438\u0441\u0438\u043d\u0438\u043d\u043d\u0435\u0441"}, {"heading": "4.5. Extensions", "text": "We discuss extensions of algorithms 1 and 2 to provide additional privacy for auxiliary data. Specifically, these algorithms can be differentiated for all private data of a single party and a single sample in auxiliary data by increasing the amount of interference as needed. We outline the proof as follows: In the previous sections, a global classifier was trained on auxiliary data whose identification was generated either by majority voting or soft identification. A change in the local classifier affects only the identification of auxiliary data, but not the characteristics {xi}. Now, we assume that the characteristic of a sample from auxiliary data can also be arbitrarily changed, i.e., xj 6 = x \u00b2 j for some j and xi = x \u00b2 i for all i (1,..., N} {j} {j \u00b2). The sensitivity of the resulting risk mixer can be calculated similarly to the proofs of theorems 1 and 3 = x."}, {"heading": "5. Related work", "text": "To maintain privacy in the area of data disclosure, several approaches have been proposed, such as multi-party anonymity (Sweeney, 2002) and secure multi-party computation (Yao, 1982) (see (Fung et al., 2010) for review.) Recently, differential privacy (Dwork & Nissim, 2004; Dwork et al., 2006; Dwork, 2006) has been addressed several weaknesses of k anonymity (Ganta et al., 2008) and has become popular as a quantifiable measure of privacy risk, a measure that limits privacy loss regardless of additional information an adversary may have. Differential privacy data has been used for a data-preserving data analysis platform (McSherry et al, 2009), and for disinfecting learned model parameters from a standard ERM (Chaudhuri et al., 2011). This paper adopts disruption techniques from the latter area to provide non-standard multi-party ERM solutions."}, {"heading": "6. Experiments", "text": "We use three sets of real-world data to compare the performance of the following algorithms: \u2022 Batch: Classifier trained to take into account all data, ignoring privacy \u2022 Soft: Private ensembles using soft labels (Algorithm 2) \u2022 Avg: Parameter averaging (Pathak et al., 2010) \u2022 Voting: Private ensembles making majority decisions (Algo-rithm 1) \u2022 Indiv: Individually trained classifiers using local data. We can expect Batch to perform better than any private algorithm because it uses all private data for training and ignores privacy. By contrast, Indiv only uses local data for training and will perform significantly worse than Batch, but it achieves perfect privacy as long as the trained classifiers are held locally by the parties. We are interested in the area where private algorithms (soft, avg and voting) perform better than the only basic algorithms - to compare all of the classifier types."}, {"heading": "6.1. Activity recognition using accelerometer", "text": "Consider a scenario where users of portable devices want to train a motion-based activity classifier without sharing their data with others. To test the algorithms, we use the UCI Human Activity Recognition Dataset (Anguita et al., 2012), a collection of motion sensor data on a smart device by multiple people performing 6 activities (walking, walking, downstairs, sitting, standing, lying down), various time and frequency domain variables are extracted from the signal, and we apply PCA to obtain d = 50 dimensional characteristics, the training and test samples are 7K and 3K, respectively, and we simulate a case with M = 1K users (i.e. parties), each user can use only 6 samples to train a local classifier, and the remaining 1K samples are used as auxiliary data that are not labeled."}, {"heading": "6.2. Network intrusion detection", "text": "Consider a scenario in which multiple gateways or routers independently collect suspicious network activities and aim to jointly build a precise network intrusion detector without revealing local traffic data. For this task, we use the KDD-99 dataset, which contains examples of \"bad\" connections, so-called intrusions or attacks, and \"good\" normal connections. Characteristics of this dataset consist of continuous values and categorical attributes. To apply logistical regression, we change categorical attributes to a single vector to obtain d = 123 dimensional characteristics. The training and test samples are 493K and 311K, respectively. We simulate cases with M = 5K / 10K / 20K parties. Each party can only use 22 samples to train a local classifier. The remaining 43K samples are used as private, unlabeled data. Figure 3 shows the test accuracy of using different algorithms with almost every data protection level = 105K / M / 1."}, {"heading": "6.3. Malicious URL prediction", "text": "The malicious URLs dataset (Ma et al., 2009) is a collection of examples of malicious URLs from a major webmail provider. The task is to predict whether or not an URLs is malicious based on various lexical and host-based characteristics of the URLs. We use PCA to obtain d = 50-dimensional trait vectors. We select days 0 to 9 for training and days 10 to 19 for testing, resulting in 200,000 samples for training and 200,000 samples for testing. We simulate cases with M = 5K / 10K / 20K parties. Each party can only use 9 samples to train a local classifier. The remaining 16K samples are used as auxiliary data that are not labeled. Figure 4 shows the testing accuracy of using different algorithms with different privacy levels. The gap between batch and other algorithms is larger than the previous experiment (though very small overall number of samples)."}, {"heading": "7. Conclusion", "text": "In this paper, we propose a method to create global, differentiated private classifiers from local classifiers, using two new ideas: 1) the use of unlabeled auxiliary data to convey the ensemble's knowledge, and 2) the solution of a weighted ERM based on estimates of the ensemble's class probability. In general, privacy implies a loss of classification power. We present a solution to minimize the performance gap between private and non-private ensembles, demonstrated by real-world tasks."}, {"heading": "A. Proofs", "text": "The results of the study show that the risks (1) and the risks (1) are different. Suppose D = (1) is the ordered set of private training data (5) for M parties, and D = (S) (1), S (M) is an adjacent set that differs only from the data of party 1 without the loss of the general public. The local classifiers after the training with D and D are areH = (h1..., hM) and H = (h), hM), which are each different."}, {"heading": "B. Differentially private multiclass logistic regression", "text": "We extend our methods to multilateral classification problems and provide a sketch of differential privacy (=). (...) We assume that we have (...) a stacked (d K) risk (w). (...) Multilateral logistical losses (i.e.) are (h (x), y) = \u2212 wTy x + log (l), (40) and isRiley's regularized empirical risk (w) = \u2212 v (n), i (wTyixi \u2212 log (l), 2). (41) The rill risk (w) is (n). (40) and isRiley's regularized empirical risk (w). (w), i), i (n), i (n), i)."}], "references": [{"title": "Human activity recognition on smartphones using a multiclass hardware-friendly support vector machine", "author": ["Anguita", "Davide", "Ghio", "Alessandro", "Oneto", "Luca", "Parra", "Xavier", "Reyes-Ortiz", "Jorge L"], "venue": "In International Workshop of Ambient Assisted Living (IWAAL", "citeRegEx": "Anguita et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anguita et al\\.", "year": 2012}, {"title": "Bias, variance, and arcing classifiers", "author": ["Breiman", "Leo"], "venue": "Technical report, Statistics Department,", "citeRegEx": "Breiman and Leo.,? \\Q1996\\E", "shortCiteRegEx": "Breiman and Leo.", "year": 1996}, {"title": "Semi-Supervised Learning", "author": ["O. Chapelle", "B. Sch\u00f6lkopf", "Zien", "A. (eds"], "venue": "URL http: //www.kyb.tuebingen.mpg.de/ssl-book", "citeRegEx": "Chapelle et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2006}, {"title": "Differentially private empirical risk minimization", "author": ["Chaudhuri", "Kamalika", "Monteleoni", "Claire", "Sarwate", "Anand D"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Chaudhuri et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2011}, {"title": "Differential privacy. In Automata, languages and programming", "author": ["Dwork", "Cynthia"], "venue": null, "citeRegEx": "Dwork and Cynthia.,? \\Q2006\\E", "shortCiteRegEx": "Dwork and Cynthia.", "year": 2006}, {"title": "Privacy-preserving datamining on vertically partitioned databases", "author": ["Dwork", "Cynthia", "Nissim", "Kobbi"], "venue": "In Advances in Cryptology\u2013CRYPTO", "citeRegEx": "Dwork et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2004}, {"title": "The algorithmic foundations of differential privacy", "author": ["Dwork", "Cynthia", "Roth", "Aaron"], "venue": "Theoretical Computer Science,", "citeRegEx": "Dwork et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2013}, {"title": "Calibrating noise to sensitivity in private data analysis", "author": ["Dwork", "Cynthia", "McSherry", "Frank", "Nissim", "Kobbi", "Smith", "Adam"], "venue": "In Theory of cryptography,", "citeRegEx": "Dwork et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2006}, {"title": "Privacypreserving data publishing: A survey of recent developments", "author": ["Fung", "Benjamin", "Wang", "Ke", "Chen", "Rui", "Yu", "Philip S"], "venue": "ACM Comp. Surveys (CSUR),", "citeRegEx": "Fung et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Fung et al\\.", "year": 2010}, {"title": "Composition attacks and auxiliary information in data privacy", "author": ["Ganta", "Srivatsava Ranjit", "Kasiviswanathan", "Shiva Prasad", "Smith", "Adam"], "venue": "In Proc. ACM SIGKDD,", "citeRegEx": "Ganta et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ganta et al\\.", "year": 2008}, {"title": "Crowd-ML: A privacy-preserving learning framework for a crowd of smart devices", "author": ["Hamm", "Jihun", "Champion", "Adam", "Chen", "Guoxing", "Belkin", "Mikhail", "Xuan", "Dong"], "venue": "In Proceedings of the 35th IEEE International Conference on Distributed Computing Systems (ICDCS). IEEE,", "citeRegEx": "Hamm et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hamm et al\\.", "year": 2015}, {"title": "A semi-supervised learning approach to differential privacy", "author": ["Jagannathan", "Geetha", "Monteleoni", "Claire", "Pillaipakkamnatt", "Krishnan"], "venue": "In Data Mining Workshops (ICDMW),", "citeRegEx": "Jagannathan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Jagannathan et al\\.", "year": 2013}, {"title": "Differentially private distributed logistic regression using private and public data", "author": ["Ji", "Zhanglong", "Jiang", "Xiaoqian", "Wang", "Shuang", "Xiong", "Li", "Ohno-Machado", "Lucila"], "venue": "BMC medical genomics,", "citeRegEx": "Ji et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2014}, {"title": "Identifying suspicious urls: an application of largescale online learning", "author": ["Ma", "Justin", "Saul", "Lawrence K", "Savage", "Stefan", "Voelker", "Geoffrey M"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Ma et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2009}, {"title": "Privacy integrated queries: an extensible platform for privacy-preserving data analysis", "author": ["McSherry", "Frank D"], "venue": "In Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,", "citeRegEx": "McSherry and D.,? \\Q2009\\E", "shortCiteRegEx": "McSherry and D.", "year": 2009}, {"title": "Multiparty differential privacy via aggregation of locally trained classifiers", "author": ["Pathak", "Manas", "Rane", "Shantanu", "Raj", "Bhiksha"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Pathak et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Pathak et al\\.", "year": 2010}, {"title": "A differentially private stochastic gradient descent algorithm for multiparty classification", "author": ["Rajkumar", "Arun", "Agarwal", "Shivani"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Rajkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rajkumar et al\\.", "year": 2012}, {"title": "Boosting the margin: A new explanation for the effectiveness of voting methods", "author": ["Schapire", "Robert E", "Freund", "Yoav", "Bartlett", "Peter", "Lee", "Wee Sun"], "venue": "Annals of statistics,", "citeRegEx": "Schapire et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Schapire et al\\.", "year": 1998}, {"title": "Fast rates for regularized objectives", "author": ["Sridharan", "Karthik", "Shalev-Shwartz", "Shai", "Srebro", "Nathan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sridharan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sridharan et al\\.", "year": 2009}, {"title": "k-anonymity: A model for protecting privacy", "author": ["Sweeney", "Latanya"], "venue": "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems,", "citeRegEx": "Sweeney and Latanya.,? \\Q2002\\E", "shortCiteRegEx": "Sweeney and Latanya.", "year": 2002}, {"title": "Protocols for secure computations", "author": ["Yao", "Andrew C"], "venue": "IEEE Symp. Found. Comp. Sci.,", "citeRegEx": "Yao and C.,? \\Q2013\\E", "shortCiteRegEx": "Yao and C.", "year": 2013}], "referenceMentions": [{"referenceID": 15, "context": "This problem of aggregating classifiers was considered in (Pathak et al., 2010), where the authors proposed averaging of the parameters of local classifiers to get a global classifier.", "startOffset": 58, "endOffset": 79}, {"referenceID": 7, "context": "It provides a strict upper bound on the privacy loss against any adversary (Dwork & Nissim, 2004; Dwork et al., 2006; Dwork, 2006).", "startOffset": 75, "endOffset": 130}, {"referenceID": 3, "context": "an empirical risk minimizer, and release a differentially private classifier using output perturbation (Chaudhuri et al., 2011).", "startOffset": 103, "endOffset": 127}, {"referenceID": 7, "context": "When an algorithm outputs a real-valued vector f \u2208 R, its global L2 sensitivity (Dwork et al., 2006) can be defined as S(f) = max D\u223cD\u2032 \u2016f(D)\u2212 f(D\u2032)\u2016 (2)", "startOffset": 80, "endOffset": 100}, {"referenceID": 7, "context": "An important result from (Dwork et al., 2006) is that a vector-valued output f with sensitivity S(f) can be made -differentially private by perturbing f with an additive noise vector \u03b7 whose density is", "startOffset": 25, "endOffset": 45}, {"referenceID": 3, "context": "Such a classifier can be sanitized by perturbation with additive noise calibrated to the sensitivity of the classifier, known as output perturbation method (Chaudhuri et al., 2011).", "startOffset": 156, "endOffset": 180}, {"referenceID": 15, "context": "Output perturbation was used to sanitize the averaged parameters in (Pathak et al., 2010).", "startOffset": 68, "endOffset": 89}, {"referenceID": 3, "context": "We will assume the following conditions for our global classifier1 similar to (Chaudhuri et al., 2011).", "startOffset": 78, "endOffset": 102}, {"referenceID": 12, "context": "Besides, efficient differentially private mechanisms are known only for certain types of classifiers so far (see (Ji et al., 2014) for a review.", "startOffset": 113, "endOffset": 130}, {"referenceID": 3, "context": "In (Chaudhuri et al., 2011), it is shown that the expected risk of an output-perturbed ERM solution wp with respect to the risk of any reference hypothesis w0 is bounded by two terms \u2013 one due to noise and another due to the gap between expected and empirical regularized risks.", "startOffset": 3, "endOffset": 27}, {"referenceID": 8, "context": "To preserve privacy in data publishing, several approaches such as k-anonymity (Sweeney, 2002) and secure multiparty computation (Yao, 1982) have been proposed (see (Fung et al., 2010) for a review.", "startOffset": 165, "endOffset": 184}, {"referenceID": 7, "context": ") Recently, differential privacy (Dwork & Nissim, 2004; Dwork et al., 2006; Dwork, 2006) has addressed several weaknesses of k-anonymity (Ganta et al.", "startOffset": 33, "endOffset": 88}, {"referenceID": 9, "context": ", 2006; Dwork, 2006) has addressed several weaknesses of k-anonymity (Ganta et al., 2008), and gained popularity as a quantifiable measure of privacy risk.", "startOffset": 69, "endOffset": 89}, {"referenceID": 3, "context": "Differential privacy has been used for a privacy-preserving data analysis platform (McSherry, 2009), and for sanitization of learned model parameters from a standard ERM (Chaudhuri et al., 2011).", "startOffset": 170, "endOffset": 194}, {"referenceID": 15, "context": "In particular, several differentially-private algorithms were proposed, including parameter averaging through secure multiparty computation (Pathak et al., 2010), and private exchange of gradient information to minimize empirical risks incrementally (Rajkumar & Agarwal, 2012; Hamm et al.", "startOffset": 140, "endOffset": 161}, {"referenceID": 10, "context": ", 2010), and private exchange of gradient information to minimize empirical risks incrementally (Rajkumar & Agarwal, 2012; Hamm et al., 2015).", "startOffset": 96, "endOffset": 141}, {"referenceID": 15, "context": "(Pathak et al., 2010) but uses a very different approach to aggregate local classifiers.", "startOffset": 0, "endOffset": 21}, {"referenceID": 17, "context": "Advantages of ensemble approaches in general have been analyzed previously, in terms of bias-variance decomposition (Breiman, 1996b), and in terms of the margin of training samples (Schapire et al., 1998).", "startOffset": 181, "endOffset": 204}, {"referenceID": 2, "context": "Furthermore, we are using unlabeled data to augment labeled data during training, which can be considered a semisupervised learning method (Chapelle et al., 2006).", "startOffset": 139, "endOffset": 162}, {"referenceID": 12, "context": "Augmenting private data with non-private labeled data to lower the sensitivity of the output is straightforward, and was demonstrated in medical applications (Ji et al., 2014).", "startOffset": 158, "endOffset": 175}, {"referenceID": 11, "context": "Using nonprivate unlabeled data, which is more general than using labeled data, was demonstrated specifically to assist learning of random forests (Jagannathan et al., 2013).", "startOffset": 147, "endOffset": 173}, {"referenceID": 15, "context": "\u2022 batch: classifier trained using all data ignoring privacy \u2022 soft: private ensemble using soft-labels (Algorithm 2) \u2022 avg: parameter averaging (Pathak et al., 2010) \u2022 vote: private ensemble using majority voting (Algorithm 1) \u2022 indiv: individually trained classifier using local data", "startOffset": 144, "endOffset": 165}, {"referenceID": 0, "context": "To test the algorithms, we use the UCI Human Activity Recognition Dataset (Anguita et al., 2012), which is a collection of motion sensor data on a smart device by multiple subjects performing 6 activities (walking, walking upstairs, walking downstairs, sitting, standing, laying).", "startOffset": 74, "endOffset": 96}, {"referenceID": 13, "context": "The Malicious URL Dataset (Ma et al., 2009) is a collection of examples of malicious URLs from a large Web mail provider.", "startOffset": 26, "endOffset": 43}], "year": 2016, "abstractText": "Learning a classifier from private data collected by multiple parties is an important problem that has many potential applications. How can we build an accurate and differentially private global classifier by combining locally-trained classifiers from different parties, without access to any party\u2019s private data? We propose to transfer the \u2018knowledge\u2019 of the local classifier ensemble by first creating labeled data from auxiliary unlabeled data, and then train a global -differentially private classifier. We show that majority voting is too sensitive and therefore propose a new risk weighted by class probabilities estimated from the ensemble. Relative to a non-private solution, our private solution has a generalization error bounded by O( \u22122M\u22122) where M is the number of parties. This allows strong privacy without performance loss whenM is large, such as in crowdsensing applications. We demonstrate the performance of our method with realistic tasks of activity recognition, network intrusion detection, and malicious URL detection.", "creator": "LaTeX with hyperref package"}}}