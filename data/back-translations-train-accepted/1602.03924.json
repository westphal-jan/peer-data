{"id": "1602.03924", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Feb-2016", "title": "Modeling Human Ad Hoc Coordination", "abstract": "Whether in groups of humans or groups of computer agents, collaboration is most effective between individuals who have the ability to coordinate on a joint strategy for collective action. However, in general a rational actor will only intend to coordinate if that actor believes the other group members have the same intention. This circular dependence makes rational coordination difficult in uncertain environments if communication between actors is unreliable and no prior agreements have been made. An important normative question with regard to coordination in these ad hoc settings is therefore how one can come to believe that other actors will coordinate, and with regard to systems involving humans, an important empirical question is how humans arrive at these expectations. We introduce an exact algorithm for computing the infinitely recursive hierarchy of graded beliefs required for rational coordination in uncertain environments, and we introduce a novel mechanism for multiagent coordination that uses it. Our algorithm is valid in any environment with a finite state space, and extensions to certain countably infinite state spaces are likely possible. We test our mechanism for multiagent coordination as a model for human decisions in a simple coordination game using existing experimental data. We then explore via simulations whether modeling humans in this way may improve human-agent collaboration.", "histories": [["v1", "Thu, 11 Feb 2016 22:48:59 GMT  (373kb,D)", "http://arxiv.org/abs/1602.03924v1", "AAAI 2016"]], "COMMENTS": "AAAI 2016", "reviews": [], "SUBJECTS": "cs.AI cs.GT cs.MA", "authors": ["peter m krafft", "chris l baker", "alex pentland", "joshua b tenenbaum"], "accepted": true, "id": "1602.03924"}, "pdf": {"name": "1602.03924.pdf", "metadata": {"source": "META", "title": "Modeling Human Ad Hoc Coordination", "authors": ["Peter M. Krafft", "Chris L. Baker", "Alex \u201cSandy\u201d Pentland", "Joshua B. Tenenbaum"], "emails": ["pkrafft@mit.edu", "clbaker@mit.edu", "pentland@mit.edu", "jbt@mit.edu"], "sections": [{"heading": null, "text": "This year it is more than ever before."}, {"heading": "1 Background", "text": "We first give a description of the coordination task that we will examine in this paper: the well-known coordinated attack problem. We then give an overview of the formal definitions of general knowledge and general p-conviction and their relationship to the coordinated attack problem."}, {"heading": "1.1 Coordinated Attack Problem", "text": "The coordination task we are examining in this paper is alternatively referred to as the coordinated attack problem, the problem of the two generals or the e-mail game. The original formulation of this task was put in the literature on distributed computer systems to illustrate the impossibility of reaching a consensus between distributed computer processors using an unreliable message forwarding system (Halpern and Moses 1990), and the problem was later adapted by economists to a game theory context (Rubinstein 1989). Here, we focus on game theory adaptation, since this formulation is more vulnerable to decision theory modeling and therefore more relevant to the modeling of human behavior. 1In this task, the world may be located in one of two states, x = 1 or x = 0. The state of the world determines which two games will play two players together, c =, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c =, c, c, c, c, c, c =, c, c, c, c, c, c, c, c =, c, c, c, c, c, c =, c, c, c, c, c, c, c =, b, c, c, c, b, c, c, c, c, c, c, c, c =, c, c, c, c, c, c, c =, c, c, c, c, c =, c, c, c, c, c, c, c =, c, c, c, c, c =, b, b, b, b, c, b, c, b, c, c, b, c, b, c, c, c, c =, c, c, c, c, c, c =, b, b, b, c =, c, c, c, b, c, b, c, b, c, c =, c, b, c, c, b, b, c, b, (, c, c, c, c, c, c, c, c, (, c, c, c, c, c, c =, (,"}, {"heading": "1.2 Common p-Belief", "text": "To believe that the other player A will play in this game, it is not enough for that player to believe that the other player x = 1. If the second player does not believe that the first player x = 1 knows, then the second player will not try to coordinate. Therefore, the first player must at least believe that the second player believes that the first player x = 1. However, it turns out that even this amount of knowledge p is not sufficient. Indeed, an infinite hierarchy of recursive belief is needed (Morris and Shin 1997). This infinite hierarchy of beliefs has been formalized using a construct called general p-belief, which we now define. Using standard definitions from game theory, we define a two-player Bayesian final game as tupel ((1), (0, 0, A1), which consists of elements we have in common."}, {"heading": "2 Models", "text": "In the coordinated attack game we are examining, we now describe four strategies for coordination. Two of these strategies involve the calculation of p-evident events and the general assumption that we will test whether human coordination behavior can be explained by thinking about p-evident events, and the other two serve as a starting point."}, {"heading": "2.1 Rational p-Belief", "text": "The first strategy we are considering is an agent that maximizes the expected benefit at a balance point of the coordinated attack problem we are examining. The strategy is implemented as follows: player i plays action A if and only if the player believes with probability that both players have at least p * = c \u2212 ba \u2212 b a common p * belief that x = 1. This strategy forms a balance of the coordinated attack problem if p * > Pi (x = 1) and if the proof that x = 1 always leads to the certain belief that x = 1, i.e. Pi (x = 1) > Pi (x = 1) \u21d2 Pi (x = 1 | \u03c9) = 1 for all \u03c9. These conditions are met by the specific statuses and payouts we use to represent the Thomas experiments. We call this model the rational p-belief strategy. The proof that this strategy forms a balance, including a derivation for the specific form of p *, is contained in our complementary material."}, {"heading": "2.2 Matched p-Belief", "text": "The second strategy we are considering is a novel probabilistic loosening of the rational p-belief strategy. It has been shown that people in many decision situations exhibit behavior called probability matching (Herrnstein 1961; Vulkan 2000). Probability matching consists in taking the probability p that a decision is the best decision available, and making that decision with probability p. While probability matching is not a utility maximization, it can be considered rational when players perform sample-based Bayesian conclusions and when taking samples is costly (Vul et al. 2014). Motivated by this frequently observed behavior, we propose a model that we call the adjusted p-belief strategy. A player who applies this strategy chooses Action A with probability p equal to the maximum general p-belief that the player perceives it in due time, i.e. the greatest value, so that he believes that there is a common p-belief that x = 1.3A version of our work contains the most readily available materials (http: / www.text.com)."}, {"heading": "2.3 Iterated Maximization", "text": "Next, we look at a well-known model of boundlessly rational behavior, sometimes referred to as the \"level-k\" mindset, which is therefore a strong starting point. As the term \"level-k\" is used for many slightly different models, we refer to our instantiation of this model as an iterated maximization strategy, which assumes that players have a certain recursive level of social thinking. A player who applies the iterated maximization strategy of level-k chooses the action that maximizes the expected benefit of that player when playing the level (k \u2212 1) strategy with a player. The level-0 player takes action A at a certain level when Pi (x = 1 | E) > c \u2212 ba \u2212 b. This level-0 strategy corresponds to the prediction of the player assuming that the player can have overall control over strategies that are both at level A and at the level of common beliefs that we are considered equal."}, {"heading": "2.4 Iterated Matching", "text": "Finally, we also consider a less common thinking model that combines the iterated maximization strategy with the probability comparison behavior we call iterated matching. Like the iterated maximization strategy, this strategy assumes that players have a certain amount of recursive social thinking k. However, rather than choosing the action that maximizes the expected benefit, a level-k player who applies the iterated matching strategy chooses action A with a probability that corresponds to that player's belief that x = 1, times the expected probability that a level (k \u2212 1) companion would play A. The probability of a level-0 player corresponds to Pi (x = 1 | \u03c9)."}, {"heading": "3 Algorithms", "text": "In this section, we present the algorithms we use to implement each of the models we are looking at. To the best of our knowledge, the existing literature on general p-belief still needs to offer algorithms to calculate general p-faith (or in our case, the perceived maximum common p-faith) for a given world state and observational model. This calculation is central to rational and congruent p-faith strategies. Therefore, we offer the first fully calculated representation of the coordination of thinking about p-obvious events. Iterated thinking algorithms are simple and well-known. The challenge in developing an algorithm to calculate a player's perception of maximum general p-faith is to avoid enumeration about all exponentially many possible subsets of p-faith. While it is easy to judge whether a particular given event is p-obvious, the definition of general p-faith merely requires the existence of such an event."}, {"heading": "3.1 Computing Information Partitions", "text": "Our algorithms require access to each player's information particles. However, it is difficult to directly specify the information particles that humans have in a naturalistic environment, such as the data we use from the Thomas experiments. Instead, we take the approach of defining a plausible generative probabilistic world model, and then construct the information particles from this factor representation using a simple algorithm. The generative world model defines the pieces of information or \"observations\" that each player receives. The information particle generation algorithm, which is precisely specified in our complementary materials, consists of iterating over all combinations of random values of variables in the generative world model, treating each such combination as a state in itself, and summarizing for each player the states that produce identical observations."}, {"heading": "3.2 Computing Common p-Belief", "text": "The algorithms 1-4 represent the functions required to calculate the perceived maximum general p-conviction. Formally, given a player i, a certain state \u03c9 and an event C, algorithm 1 calculates the largest value p, for which player i p believes that there is a common p-conviction in C. Note that it is insufficient to calculate the largest value of p for which there is a general p-conviction in \u03c9, since in general the state player i only knows that the event successi (\u03c9) has occurred, not that it has actually occurred. Note that while algorithm 1 takes the largest value of p as input for convenience, the algorithm depends only on p and could therefore be executed by a player. \u2212 Algorithm 3 min faith (E, C, \u03c9) returns mini-elements {0.1} min (Pi (E | \u03c9), Pi | \u03c9), Pi-elements are a specific element, are a Pi-element, and Pi-elements are included."}, {"heading": "3.3 Iterated Reasoning", "text": "We now present our algorithms for the iterated argumentation strategies: For level k > 0, the iterated maximization strategy computesfki (\u03c9) = 1 (\u2211 \u03c9) Pi (\u03c9) Pi (\u03c9) (\u03c9) (Pi (x = 1 | \u03c9) fk \u2212 11 \u2212 i (\u03c9) a + Pi (x = 0 | \u03c9) fk \u2212 11 \u2212 i (\u03c9) d + (1 \u2212 fk \u2212 11 \u2212 i (\u03c9) b) > c), where 1 () is the indicator function, and f0i (\u03c9) = 1 (Pi (x = 1 | \u03c9) > c \u2212 ba \u2212 b). If f k i (\u03c9) = 1, then player i plays A, and otherwise player i plays B. For the iterated matching strategy, qki (\u03c9) = Pi (x = 1 | \u03c9) \u00b7 kas \u00b2 \u00b2 \u00b2 Pi (qill) = 11 \u2212 ki (subsequently played with probability)."}, {"heading": "4 Data", "text": "We present the data we use for our empirical results, the dataset comes from the Thomas experiments (Thomas et al. 2014). These experiments present participants with a stylized, coordinated attack problem that translates into a story about the butcher and the baker of a city. However, in their history, these traders can either collaborate to produce hot dogs, or they can work separately to produce chicken wings and dinner rolls, respectively. Merchants can sell chicken wings and dinner rolls separately for a constant profit each day, but the profit from hot dogs varies from day to day. Merchants make a profit of x = 1 each on a given day or d if x = 0. There is also a loudspeaker that publicly announces the prices of hot dogs, and a messenger that delivers news around the city. The experiments had four different states of knowledge that the participants received: \""}, {"heading": "5 Results", "text": "We will now present our empirical results. First, we will examine the predictions of each of the coordination strategies we consider in light of the generative processes that represent the Thomas experiments, and then we will examine to what extent a computer agent equipped with the most appropriate model of human coordination is able to achieve higher returns on a simulated coordination problem with human agents."}, {"heading": "5.1 Model Comparison", "text": "To perform a model comparison, we calculate the probability of choosing A, which each model predicts based on our formal representations of each of the four knowledge conditions, and then compare these predicted probabilities with the actual probabilities observed in the Thomas experiments. For the two iterated thinking models, we use a grid search of [0, 1, 2, 3, 4, 5] to find the most appropriate k for each model (ultimately k = 1 in iterated maximization and k = 3 in iterated matching).The specific predictions of each model are shown in Figure 1. As shown in Figure 2, the corresponding belief model achieves the lowest error in the mean square. Qualitatively, the most striking aspect of the data successfully captured by the matched p-belief model is the similarity in the probability of coordination between the secondary and tertiary knowledge conditions. The two models that contain maximimimimimizing agents (making rational p-belief and maximizing predictions) are extreme."}, {"heading": "5.2 Human-Agent Coordination", "text": "In addition to testing the accuracy of the models of human coordination we propose, we are also interested in whether the most suitable model helps us improve the results in the cognitive strategy. We use the data from the Thomas experiments to evaluate this possibility, and for this purpose, our computer agents implement what we call a \"cognitive strategy.\" An agent who applies the cognitive strategy selects the action that maximizes the expected benefit, assuming that the agent's companion uses the corresponding p-belief model. These agents play the human companion in each of the four states of knowledge in the Thomas experiments (Player 1 in all but the secondary condition). We evaluate the payouts from the agents \"actions using the human data from the Thomas experiments. In this simulation, we vary the payouts (a, b, c, d), and we assume that the human remains immutable throughout the range of the payouts we use."}, {"heading": "6 Discussion", "text": "In the present work, we focused on laying the foundations for the use of general p-belief in AI and cognitive modeling. In particular, we developed an efficient algorithm for concluding a maximum perceived p-belief and showed that the coordination strategy of probability matching on the common p-faith explains certain surprising qualitative characteristics of existing data from a previous human experiment, and we showed that this model can also help improve the results of agents in coordination between human and agent. This work has three main limitations. Due to the small amount of human data we had and the absence of a sustained test set, our empirical results are necessarily only suggestive. While the data are incompatible with the rational p-belief model and the iterated maximization model, the predictions of the iterated matching model and the adapted p-faith model are not identical to the ambiguous knowledge we can produce both reasonably well, the strongest evidence we have is the ability to match the amount of the adherent to the two."}, {"heading": "7 Acknowledgments", "text": "Special thanks to Kyle Thomas for providing data and to Moshe Hoffman for drawing our attention to obvious events based on the work supported by the NSF GRFP, Grant # 1122374; the Center for Brains, Minds & Machines (CBMM), under the NSF STC Award CCF-1231216; and the NSF Grant IIS-1227495 and ARO Grant # 6928195. Any opinions, outcomes, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors."}, {"heading": "S1 Definitions", "text": "First, we present the most relevant definitions from our main text in a clearer format, and we provide several additional definitions that will be helpful in our evidence. As described in the main text, we assume that a Bayesian game with a finite state space exists. (..) These pieces of information and a measure defined by the conditional distribution Pi (E | 1) are simply subdivisions of the state space p (.). (.). (.). (.). (.). (.). (.). (.). (.). (.).). (.). (.). (.). (.).). (.). (.).). (.). (.).). (.). (.).). (.). (.).). (.). (.).). (.).). (.).). (.).). (.).). (.).)."}, {"heading": "S3 Models", "text": "In this section, we show that the rational p belief strategy forms a balance in the coordination game we are examining. (Let us remember the rational p belief strategy: player i plays actionA if and only if player i believes with at least the probability p * = c \u2212 ba \u2212 b that both players have common p * belief that x = 1.Proposition 1. Suppose that messages are meaningless and that p * (x = 1) > Pi (x = 1) the rational p belief strategy maximizes the expected return of player i at any time since player 1 \u2212 i also applies the same strategy. Suppose that messages are meaningless p, i.e. Pi (x = 1) > Pi (x = 1) > Pi (x = 1) = 1 for all other players. Suppose that F (x = 1) -evident p. Suppose that F (x = 1) -evident is p."}, {"heading": "S4 Algorithms", "text": "In this section we note that the simple algorithm for converting the generative process descriptions of the probability models into our main text belongs to the information partitions. The \"run (i, \u03c9)\" function takes as input a player i and a state expressed as a tuple, uses the variables in the tuple for each random drawing within the generative process and returns a composition of observed calls for player i. (0, (1,1,0,1,0)]] in the messenger model. Our groups of algorithms together with the same observations return to elements of the information partitions. Computing Common p-Belief We now prove the correctness of the common p belief algorithm given in the main text. We first prove a problem indicating the correctness of the superp evident algorithms."}], "references": [{"title": "Approximating common knowledge with common beliefs", "author": ["D. References Monderer", "D. Samet"], "venue": "Games and Economic Behavior 1(2):170\u2013190.", "citeRegEx": "Monderer and Samet,? 1989", "shortCiteRegEx": "Monderer and Samet", "year": 1989}], "referenceMentions": [{"referenceID": 0, "context": "Common knowledge has been shown to be necessary for exact coordination (Halpern and Moses 1990), and a probabilistic generalization of common knowledge, called common p-belief, has been been shown to be necessary for approximate coordination (Monderer and Samet 1989).", "startOffset": 242, "endOffset": 267}, {"referenceID": 0, "context": "In this work we use a previously established fixed point characterization of common p-belief (Monderer and Samet 1989) to formulate a novel model of human coordination.", "startOffset": 93, "endOffset": 118}, {"referenceID": 0, "context": "A critically important result of (Monderer and Samet 1989) states that this definition of common p-belief is equiv-", "startOffset": 33, "endOffset": 58}], "year": 2016, "abstractText": "Whether in groups of humans or groups of computer agents, collaboration is most effective between individuals who have the ability to coordinate on a joint strategy for collective action. However, in general a rational actor will only intend to coordinate if that actor believes the other group members have the same intention. This circular dependence makes rational coordination difficult in uncertain environments if communication between actors is unreliable and no prior agreements have been made. An important normative question with regard to coordination in these ad hoc settings is therefore how one can come to believe that other actors will coordinate, and with regard to systems involving humans, an important empirical question is how humans arrive at these expectations. We introduce an exact algorithm for computing the infinitely recursive hierarchy of graded beliefs required for rational coordination in uncertain environments, and we introduce a novel mechanism for multiagent coordination that uses it. Our algorithm is valid in any environment with a finite state space, and extensions to certain countably infinite state spaces are likely possible. We test our mechanism for multiagent coordination as a model for human decisions in a simple coordination game using existing experimental data. We then explore via simulations whether modeling humans in this way may improve human-agent collaboration. Forming shared plans that support mutually beneficial behavior within a group is central to collaborative social interaction and collective intelligence (Grosz and Kraus 1996). Indeed, many common organizational practices are designed to facilitate shared knowledge of the structure and goals of organizations, as well as mutual recognition of the roles that individuals in the organizations play. Once teams become physically separated and responsiveness or frequency of communication declines, the challenge of forming shared plans increases. Part of this difficulty is fundamentally computational. In theory, coming to a fully mutually recognized agreement on even a simple action plan among two choices can be literally impossible if communication is even mildly unreliable, even if an arbitrary amount of communication is allowed (Halpern and Moses 1990; Lynch 1996). Copyright c \u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. This problem is well-studied within the AI literature (e.g., (Gmytrasiewicz and Durfee 1992)), though the core difficulties still manifest in contemporary research on \u201cad hoc coordination\u201d\u2014collaborative multiagent planning with previously unknown teammates (Stone et al. 2010). However, surprisingly little is known about the strategies that humans use to overcome the difficulties of coordination (Thomas et al. 2014). Understanding how and when people try to coordinate is critical to furthering our understanding of human group behavior, as well as to the design of agents for humanagent collectives (Jennings et al. 2014). Existing attempts at modeling human coordination have focused either on unstructured predictive models (e.g., (Frieder, Lin, and Kraus 2012)) or bounded depth socially recursive reasoning models (e.g., (Gal and Pfeffer 2008; Yoshida, Dolan, and Friston 2008)), but there is reason to believe that these accounts miss important aspects of human coordination. One concept that appears repeatedly in formal treatments of coordination but has not appeared meaningfully in empirical modeling is common knowledge. Two agents have common knowledge if both agents have infinitely nested knowledge of the other agent\u2019s knowledge of a proposition, i.e. the first agent knows the second agent knows, the first agent knows the second agent knows the first agent knows, etc. Common knowledge has been shown to be necessary for exact coordination (Halpern and Moses 1990), and a probabilistic generalization of common knowledge, called common p-belief, has been been shown to be necessary for approximate coordination (Monderer and Samet 1989). While these notions are clearly important normatively, it is not entirely clear how important they are empirically in human coordination. Indeed, supposing that humans are able to mentally represent an infinitely recursive belief state seems a priori implausible, and the need to represent and infer this infinite recursive belief state has also been a barrier to empirically testing models involving common knowledge. Nevertheless, building on the existing normative results, a group of researchers recently designed a set of experiments to test whether people are able to recognize situations in which common knowledge might obtain (Thomas et al. 2014) (hereafter referred to as the \u201cThomas experiments\u201d). These researchers argued that people do possess a distinct mental representation of common knowledge by showing that people will attempt to coordinate more often in situaar X iv :1 60 2. 03 92 4v 1 [ cs .A I] 1 1 Fe b 20 16 tions where common knowledge can be inferred. However, this previous work did not formalize this claim in a model or rigorously test it against plausible alternative computational models of coordination. This existing empirical work therefore leaves open several important scientific questions that a modeling effort can help address. In particular: How might people mentally represent common p-belief? Do people reason about graded levels of common p-belief, or just \u201csufficiently high\u201d common p-belief? Finally, what computational processes could people use to infer common p-belief? In this work we use a previously established fixed point characterization of common p-belief (Monderer and Samet 1989) to formulate a novel model of human coordination. In finite state spaces this characterization yields an exact finite representation of common p-belief, which we use to develop an efficient algorithm for computing common p-belief. This algorithm allows us to simulate models that rely on common p-belief. Because of the normative importance of common p-belief in coordination problems, our algorithm may also be independently useful for coordination in artificial multiagent systems. We show using data from the Thomas experiments that this model provides a better account of human decisions than three alternative models in a simple coordination task. Finally, we show via simulations based on the data from the Thomas experiments that modeling humans in this way may improve human-agent coordination.", "creator": "TeX"}}}