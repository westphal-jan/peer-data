{"id": "1602.04128", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2016", "title": "Coin Betting and Parameter-Free Online Learning", "abstract": "In the recent years a number of parameter-free algorithms for online linear optimization over Hilbert spaces and for learning with expert advice have been developed. While these two families of algorithms might seem different to a distract eye, the proof methods are indeed very similar, making the reader wonder if such a connection is only accidental.", "histories": [["v1", "Fri, 12 Feb 2016 17:11:42 GMT  (34kb)", "https://arxiv.org/abs/1602.04128v1", null], ["v2", "Mon, 27 Jun 2016 20:16:44 GMT  (57kb,D)", "http://arxiv.org/abs/1602.04128v2", null], ["v3", "Fri, 28 Oct 2016 16:43:55 GMT  (57kb,D)", "http://arxiv.org/abs/1602.04128v3", null], ["v4", "Fri, 4 Nov 2016 01:30:29 GMT  (57kb,D)", "http://arxiv.org/abs/1602.04128v4", "Fixed an compilation error in the latex"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["francesco orabona", "d\u00e1vid p\u00e1l"], "accepted": true, "id": "1602.04128"}, "pdf": {"name": "1602.04128.pdf", "metadata": {"source": "CRF", "title": "Coin Betting and Parameter-Free Online Learning", "authors": ["Francesco Orabona"], "emails": ["francesco@orabona.com", "dpal@yahoo-inc.com"], "sections": [{"heading": null, "text": "We present a new intuitive framework to design parameter-free algorithms for both linear online optimization via Hilbert rooms and for learning with expert advice, based on the reduction of bets to the results of opposing coins. We start with a betting algorithm based on the Krichevsky Trofimov estimator. The resulting algorithms are simple, with no parameters to be tuned, and they improve or match previous results in terms of repentance guarantee and complexity per round."}, {"heading": "1 Introduction", "text": "We are looking at Online Line Optimization (OLO) Cesa-Bianchi and Lugosi (2006), Shalev-Shwartz (2011).We are looking at Online Line Optimization (OLO).We are looking at Online Line Optimization (OLO).We are looking at Online Line Optimization (OLO).We are looking at Online Line Optimization (OLO).We are looking at Algorithms (OLO).O-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-LR-"}, {"heading": "2 Preliminaries", "text": "& & & # 10; & & & # 10; & & & # 10; & & & # 10; & & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & & # 10; & & # 10; & & & & # 10; & & & & & & # 10; & # 10; & # 10; & # 10; & & # 10; & & & & & # 10; & & & & & & & # 10; & & & & & & & # 10; & & & & & & & # 10; & & & & & & & # 10; & & & & # 10; & & & & # 10; & & & & & & # 10; & & & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & & & # 10; & # 10; & & & # 10; & # 10; & & # 10; & & # 10; & & & & # 10; & & & # 10; & & & & & & & # 10; & # 10; & & & & # 10; & & & # 10; & & & & # 10; & & & # 10; & & & & # 10; & # 10; & & & & # 10; & & & & & & # 10; & & & & # 10; & & & & # 10; & & & # 10; & # 10; & & & & # 10; & & & # 10; & & &"}, {"heading": "3 Warm-Up: From Betting to One-Dimensional Online Linear", "text": "The argument for generic Hilbert spaces (Section 5) and LEA (Section 6) will be similar. We will show that the betting view provides a natural way of analyzing and designing online learning algorithms, with the only design choice being the potential function of the betting algorithm (Section 4). A specific example of coin betting potential and the resulting algorithms is in Section 7. As a warm-up, we consider an algorithm for OLO about one-dimensional wealth. Let's consider {wt} s its sequence of predictions about the wealth {gt} and the resulting algorithms. The overall reward of the algorithm after t rounds is Rewardt = expressed as i = 1 giwi. Although there is no concept of \"wealth\" in OLO, we define the wealth of the OLO algorithm as attention."}, {"heading": "4 Designing a Betting Algorithm: Coin Betting Potentials", "text": "The strategy assumes that the coin (1) assumes the probability of heads (1) that the Kelly bet does not make sense (1). With perfect knowledge of the future, the player could always bet anything on the right outcome. Hence, after T-rounds from an initial endowment, the maximum wealth he would receive would be 2T. Instead, he assumes that he accepts the same fraction of his wealth in each round. Betting (\u03b2) is the wealth of such a strategy according to McMahan and Abernethy."}, {"heading": "5 From Coin Betting to OLO over Hilbert Space", "text": "In this section, which generalizes the one-dimensional construction in section 3, we show how to use a sequence of excellent coin bet potentials {Ft} \u221e t = 0 to construct an algorithm for OLO over a Hilbert space and how to prove a regret associated with it. We define reward and prosperity analogous to the one-dimensional case: Rewardt = \u2211 t i = 1 < gi, wi > and Wealtht = + Rewardt. Faced with a sequence of coin bet potentials {Ft} \u221e t = 0, using (7) we define the fractionation = Ft (t \u2212 1i = 1 gi \u2212 1 gi \u2212 1) \u2212 Ft (t \u2212 1i = 1 gi \u2212 1) \u2212 1 gi + 1 gi + 1 (1 gi + 1) \u2212 1 (1 gi + 1) \u2212 1 (1 gi + 1) \u2212 t (t) \u2212 t (1) \u2212 g."}, {"heading": "6 From Coin Betting to Learning with Expert Advice", "text": "In this section we show how to use the algorithm for OLO over the one-dimensional Hilbert space R from section 3 - itself based on a coin bet strategy - to construct an algorithm for LEA. Let A \u2265 2 be the number of experts and \u2206 N be the N-dimensional probability simplex. Let \u03c0 = (\u03c01, \u03c02,.., \u03c0N) \u00b2 N have any previous distribution. Let A have an algorithm for OLO over the one-dimensional Hilbert space R, based on a sequence of coin bet potential {Ft} \u221e t = 0 with initial equipment 3 1. Let's start with N copies of A.Consider each round. Let wt, i-pt. R predict the i-th copy of A. The LEA algorithm composes p-t = (p-t, p-t, p-t, p-t, 2,."}, {"heading": "7 Applications of the Krichevsky-Trofimov Estimator to OLO and", "text": "LEAIn the previous sections, we have shown that a coin bet potential with a guaranteed rapid growth of wealth will give good guarantees of remorse for OLO and LEA. Here, we show that the KT valuer has associated an excellent coin bet potential, which we call KT potential. Then, the optimal wealth guarantee of the KT potentials would lead to optimal parameter-free limits of remorse. 3Any initial equipment > 0 can be switched to 1. \u2212 Instead of Ft (x), we would call Ft (x) /. The wt would become wt /, but pt is invariant for scaling wt. Hence, the LEA algorithm is independent. Algorithm 1 \u2212 Algorithm for OLO via Hilbert room H based on KT potential require: Initial equipment > 01: for t = 1, 2,."}, {"heading": "7.1 OLO in Hilbert Space", "text": "We apply the KT potential for the construction of an OLO algorithm to a Hilbert space. We will use (9), and we only need to compute \u03b2t. According to theorem 13 in appendix E, the formula for \u03b2t simplifiesto \u03b2t = 1 gi-t = 1 gi-t is given as wt = 1 t (+ \u2211 t \u2212 1 i = 1 < gi, wi >) \u2211 t \u2212 1 i = 1 gi.The resulting algorithm is given as algorithm 1. We deduce a regret from this as a very simple sequence of theorem 3 to the KT potential (13). The only technical part of the evidence in appendix F is an upper limit for F-t, since it cannot be expressed as an elementary function. Conclusion 5 (Regret Bound for Algorithm 1). Let's leave > 0. McStreew = 1 any arbitrary sequence of reward vectors in a Hilbert space, so that the algorithm 1 and Bogret [) algorithms."}, {"heading": "7.2 Learning with Expert Advice", "text": "We will now construct an algorithm for LEA based on the \u03b4-shifted KT potential. (We will set the algorithm to T / 2 to know the number of rounds T in advance; we will fix this later with the standard doubling trick.Algorithm 2 Algorithm for learning with expert advice based on \u03b4-shifted KT potential Require: number of experts N, previous distribution N, number of rounds T 1: for t = 1, 2,.,., T do2: For each i-linkage, sentence wt, i-linkage, i-linkage t-1 = 1 g-linkage T-2, number of rounds T 1: for T-2, iwj, i) 3: For each i-linkage, i-linkage, i-linkage, i-linkage, i-linkage, i-linkage, i-linkage, i-linkage, i-linkage."}, {"heading": "8 Discussion of the Results", "text": "This year, the number of job-related redundancies has fallen by 20 per cent compared to the previous year, while the number of job-related redundancies has increased by 20 per cent."}, {"heading": "A From Log Loss to Wealth", "text": "Guarantees for bets or sequential investments are often expressed as upper limits for regret (\u03b2 = \u03b2 = \u03b2 = \u03b2 loss). Here, for the sake of completeness, we show how such a guarantee can be converted into a lower limit for the richness of the corresponding betting algorithm. We consider the problem of predicting a binary result (\u03b2 = \u03b2 loss). The algorithm says pt (1 \u2212 pt) for each round probability. We define regret in relation to a fixed probability vector \u03b2 \u2212 n sequence of results xt {0, 1} and the loss of the algorithm is \"(pt, xt) = \u2212 xt ln pt \u2212 (1 \u2212 xt) ln p \u2212 (1 \u2212 pt).We define regret in relation to a fixed probability vector \u03b2 \u2212 asRegretloglossT = T = 1 '(pt, xt) \u2212 min \u03b2 loss ln (pt)."}, {"heading": "B Optimal Betting Fraction", "text": "Theorem 8 (Optimal Betting Fraction). Let us F: [\u03b2 \u2212 \u03b2 \u2212 \u03b2] (\u03b2 \u2212 \u03b2 1, x + ln1] (R: be a logarithmically convex function. Then, arg min \u03b2 (\u2212 1,1) max g: [\u2212 1,1] F (x + g) 1 + \u03b2g = F (x \u2212 1) F (x + 1) F (x + 1) + F (x \u2212 1). Furthermore, \u03b2 \u2212 ln (x + 1) \u2212 F (x \u2212 1) F (x \u2212 1) F (x \u2212 1) satisfiesln (F (x + 1) \u2212 ln (F (x + 1)."}, {"heading": "C Proof of Lemma 11", "text": "First, we give the following numbers from McMahan and Orabona [2014] and report here with our notation for completeness. Let's h: (\u2212 a) \u00b7 R is actually a doubly differentiable function, the x \u00b7 h \"(x)\" (x) \"(x)\" (x) for all x \"(0, a). Let's c: [0, \u00b2) \u00b7 R is an arbitrary function, the x \u00b7 h\" (x), v \"H\" (x), v \"H\" (x), v \"V.\" < a, thenc \"u,\" v \"V.\""}, {"heading": "D Proof of Theorem 4", "text": "The proof: The first equality results from the definition of gt, i = pt = pt = pt = pt, i = gt, i = gt = > i =. Actually, N = 1 \u03c0ig, i = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p (t, i = p >) + p (t, i = p = p, i = p = p, i = p = p = p = p = p = t (t, i = t) p (t, i = p = t) p (t, i = p = t) p (T = FT, i = p \u2212 t, i \u2212 t, i [gt, i \u2212 p \u2212 t] p = p > t = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = t, i = t (t, i \u2212 t)."}, {"heading": "E Properties of Krichevsky-Trofimov Potential", "text": "The function F: (\u2212 a, a) \u2192 R +, F (x) = x (x) n (f) n (f) n (f) n (f) n (f) n (f) n (f) n (f) n (f) n (f) n (f) n (f) n (f) n (f) n (f) n (f) n (f) n (f) n (f) n (f) n (f) n (f) n (f) n (f) n (f) n (f) n n (f) n n (f) n (f) n (f) n (f) n n n n n (f) n n n (n) n n (f) n (n (n (f) n (n (n) n (n (f) n (n (f) n (n (f) n (n (n (n (f) n (n (n (n (n (n) n n n (n (n (f) n n n n (n (f) n n n n (f) n n n (f) n n n (f) n n n n (f) n n n (f) n n n (f) n (f) n n n (f) n n n (f) n n n (f) n n n (f) n n n (f) n n n (f) n n n n n (f) n n n (f) n n n n n (f) n n n n n n (f) n n n n (f) n n n (f) n n n n (f) n n n n n (f) n (f f) n n n (f) n (f) n (f) n n (f) n n n n n n n n n (f) n (f f f) n (f) n (f) n) n n n n n n n n n n n n n (f f f) f f) f f f) f f f f f f) n (f) f f f f f f) n (f) n f f f f f f f f f) n (f) n (f f f) n n n n n n n n (f f f"}, {"heading": "F Proofs of Corollaries 5 and 6", "text": "We give some technical lemmas that are used in the following evidence. < b > b + > b (Krichevsky-Trofimov (KT) Potential. It is a generalization of the lower limit, which leads to real numbers for integers in Willems and al. [1995]. Lemma 14 (Lower Bound on KT Potential) If c + 1 and a \u2212 b are non-negative realities, so that a + b = c thenln (a + 1 / 2) \u00b7 (b + 2) \u00b7 b (b + 2) Mexico (c + 1) Mexico (c + 12 ln (c), so that a (b c) b) b).Proof. Von Whittaker and Watson [1962] [p. 263 Ex. 45], we have a fortune (a + 1 / 2) that we have (a + 1 / 2) Mexico (b + 1 / 2)."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "In the recent years, a number of parameter-free algorithms have been developed for online linear optimization over Hilbert spaces and for learning with expert advice. These algorithms achieve optimal regret bounds that depend on the unknown competitors, without having to tune the learning rates with oracle choices. We present a new intuitive framework to design parameter-free algorithms for both online linear optimization over Hilbert spaces and for learning with expert advice, based on reductions to betting on outcomes of adversarial coins. We instantiate it using a betting algorithm based on the KrichevskyTrofimov estimator. The resulting algorithms are simple, with no parameters to be tuned, and they improve or match previous results in terms of regret guarantee and per-round complexity.", "creator": "LaTeX with hyperref package"}}}