{"id": "1602.04133", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2016", "title": "Deep Gaussian Processes for Regression using Approximate Expectation Propagation", "abstract": "Deep Gaussian processes (DGPs) are multi-layer hierarchical generalisations of Gaussian processes (GPs) and are formally equivalent to neural networks with multiple, infinitely wide hidden layers. DGPs are nonparametric probabilistic models and as such are arguably more flexible, have a greater capacity to generalise, and provide better calibrated uncertainty estimates than alternative deep models. This paper develops a new approximate Bayesian learning scheme that enables DGPs to be applied to a range of medium to large scale regression problems for the first time. The new method uses an approximate Expectation Propagation procedure and a novel and efficient extension of the probabilistic backpropagation algorithm for learning. We evaluate the new method for non-linear regression on eleven real-world datasets, showing that it always outperforms GP regression and is almost always better than state-of-the-art deterministic and sampling-based approximate inference methods for Bayesian neural networks. As a by-product, this work provides a comprehensive analysis of six approximate Bayesian methods for training neural networks.", "histories": [["v1", "Fri, 12 Feb 2016 17:32:39 GMT  (486kb)", "http://arxiv.org/abs/1602.04133v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["thang d bui", "daniel hern\u00e1ndez-lobato", "jos\u00e9 miguel hern\u00e1ndez-lobato", "yingzhen li", "richard e turner"], "accepted": true, "id": "1602.04133"}, "pdf": {"name": "1602.04133.pdf", "metadata": {"source": "CRF", "title": "Deep Gaussian Processes for Regression using Approximate Expectation Propagation", "authors": ["Thang D. Bui"], "emails": ["tdb40@cam.ac.uk", "daniel.hernandez@uam.es", "yl494@cam.ac.uk", "jmhl@seas.harvard.edu", "ret26@cam.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 160 2.04 133v 1"}, {"heading": "1 Introduction", "text": "This year we will be in a position to put ourselves at the top, \"he said."}, {"heading": "2 Deep Gaussian processes", "text": "In this year we have it in our hands, as we have experienced in recent years, that we are in a position to go into a new world, in which we go into another world, in which we go into another world, in which we go into another world, in which we go into another world, in which we go into another world, in which we go into a world, in a world, in a world, in which we go into another world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world."}, {"heading": "3 The Fully Independent Training Conditional approximation", "text": "The computational complexity of the complete GP models scales cubic with the number of training cases, which makes them insoluble in practice. Therefore, sparse approach techniques are often necessary. They can be roughly divided into two classes: those that are explicitly economical and generate a semi-parametric representation that approximates the original model, and those that maintain the original non-parametric properties and perform a sparse approximation to the exact posterior. The method used here, Fully Independent Training Conditional (FITC), falls into the first category. FITC approximation is formed by taking into account a small set of M function values u in the infinite dimension vector f \u2212 n and assuming conditional independence between the remaining values."}, {"heading": "4 Approximate Bayesian inference via EP", "text": "Having established a probabilistic model for data by means of a deep, sparse Gaussian process, we now consider conclusions for the inductive outputs u = {ul} Ll = 1 and learning the model parameters \u03b1 = {zl, \u03b8l} Ll = 1. The rear distribution over the inductive outputs can be written as p (u | X, y), p (u), p (u), n (yn | u, Xn). This quantity can then be used for predicting output when setting a test input p (y, x, X, y) =, du p (u | X, y, y, y, p (y \u0445 | u, x). However, the hyperparameters of the model can be tractable by maximizing the marginal probability p (y | \u03b1) =, dudh p (u, h) p (y | u, h, \u03b1). Both the posterior and the marginal probability are not analytically tractable if there is more than an approximate source of energy proposed here."}, {"heading": "4.1 EP, Stochastic EP and the EP approximate energy", "text": "In the EP (Minka, 2001) it is assumed that the approximate rear factor q (u) p (u) p (n) n (u) n (u) n (n) n = 1 the approximate data factors are. Each factor captures approximately the contribution of the data points to the back and in this work takes an unnormalized Gaussian form. The factors can be found by performing an iterative procedure that often several passes through the training set for Konvergenz3. The EP algorithm also provides an approximation of the marginal probability that the EP???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "4.2 Direct EP energy minimisation with a tied factor constraint", "text": "In order to reduce the expensive memory footprint of EP, the data factors are interconnected, i.e. the rear q = q = q (u | X, y) is approximated by q (u) p (u) p (u) g (u) N, where the factor g (u) could be considered an average data factor, but which captures the average effect of a probability date on the back part of the EP. Approximations of this form have recently been used in the SEP algorithm (Li et al., 2015) and although in practice the average effect of a probability date on the back part of the EP has been found to be almost as good, while the memory requirements of the EP are significantly reduced, from O (NLM2) to O (LM2) in our case. The original SEP work has developed modified versions of the EP updates that are suitable for the new form of the rear part. Originally, we applied this method to this parameter (including the hypothesis)."}, {"heading": "5 Probabilistic backpropagation for deep Gaussian processes", "text": "The calculation logZn in the objective function above is analytically intractable for L \u2265 1 = q = q = q = q = q = q = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "6 Stochastic optimisation for scalable training", "text": "???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????)??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "7 Approximate predictive distribution", "text": "Considering the approximate posterior test input and a new test input x *, we would like to make a prediction about the test output y *. This means finding p (y * | x *, X, Y) throup (y * | x *, u) q (u | X, Y). This predictive distribution is not analytically comprehensible, but fortunately we can approximate it by a Gaussian method similar to the method described in Section 5. That is, a single forward process is performed in which each layer takes a Gaussian distribution over the input, includes the approximate posterior of the inductive outputs and approximates the output distribution by a Gaussian. An alternative way to obtain the prediction is to pass the sample from the model, but we do not use this approach in the experiments."}, {"heading": "8 Experiments", "text": "We implement and compare the proposed approximation scheme with the latest neural network methods in Bayes. We will first explain our implementation in Section 8.1 and then discuss the experimental results in Section 8.2 and 8.3."}, {"heading": "8.1 Experimental details", "text": "In all the experiments described here, we use Adam with the preset learning rate (Kingma & Ba, 2015) to optimize our objective function. We use an exposed square core with length scales for each layer. Hyperparameters and pseudo-point positions differ between the functions in each layer. We use long length scales and inductive inputs of the first GP layer to meaningfully initialize on the basis of the mean distance between the data points in the entrance space or k-mean cluster centers. We use long length scales and initial inputs between [\u2212 1, 1] for the higher layers to force them to begin identity mapping. We parameterize the natural parameters of the average factor and initialize them with small random values. We evaluate the predictiveness of the test theorem using two popular metrics: square root mean (RMSE) and mean log probability (LL)."}, {"heading": "8.2 Regression on UCI datasets", "text": "We validate the proposed approach for training DGPs in regression experiments using multiple datasets from the UCI repository. Specifically, we use the ten datasets and train / test splits used in Herna \u0301 ndezLobato & Adams (2015) and Gal & Ghahramani (2015). We compare our method (FITC-DGP) against sparse GP regression using FITC-GP) and bayesian neural network (BNN) regression with multiple state-of-the-art deterministic and sampling-based inference techniques (FITC-DGP). We include the results for BNNNNs, Lobato Adams Adams Adams Adams (2015)."}, {"heading": "8.3 Predicting the efficiency of organic photovoltaic molecules", "text": "After demonstrating the performance of our inference scheme for DGPs, we conduct an additional regression experiment based on a sophisticated dataset (Hachmann et al., 2011), using 50,000 molecules for training and 10,000 for testing. Molecules are represented by 512-dimensional binary feature vectors generated with the RDKit package, based on the molecular structures in canonical SMILES format and a binding radius of 2. The power conversion efficiency of these molecules was estimated using density function theory to determine whether a molecule could potentially be used as a solar cell. HCEP's overall goal is to find organic molecules that are as efficient as their silicon counterparts."}, {"heading": "9 Summary", "text": "This paper has introduced a new and powerful deterministic approximation scheme for DGPs based on an approximate EP algorithm and FITC approximation to bypass computational and analytical intractability. A novel enhancement of the probabilistic backpropagation algorithm has been developed to solve a difficult marginalization problem in the approximate EP algorithm used. Results show that the new method for training DGPs is superior to 7 of 11 data sets considered and comparable to the rest, demonstrating that DGPs are a competitive alternative to multi-layered Bayesian neural networks for supervised learning tasks. The proposed method can in principle be applied to classification and unattended learning."}, {"heading": "Acknowledgements", "text": "The authors thank Nilesh Tripuraneni, Alex Matthews, Jes Frellsen and Carl Rasmussen for insightful comments and discussions. TDB thanks Google for funding its European Doctoral Fellowship. JMHL thanks the Rafael del Pino Foundation for its support. DHL and JMHL thank Plan Nacional I + D + i, Grant TIN2013-42351-P and CAM, Grant S2013 / ICE-2845 CASI-CAM-CM. YL thanks the Schlumberger Foundation for their Faculty for the Future PhD Fellowship. RET thanks the EPSRC fellowships EP / G050821 / 1 and EP / L000776 / 1."}, {"heading": "A Extra experimental results", "text": "Dre rf\u00fc the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the"}, {"heading": "B EP and SEP", "text": "In this section we summarize the iterative procedures of EP and SEP. Instead, the EP updates one approximate factor after another by the following procedure: 1. Remove the factor t t t t t t (n) to form the exit or cavity distribution q (u) / t (u), 2. Minimize the factor t t t t t (n) p (n) | | q (u), resulting in a new approximate factor t (u) that can be combined with the cavity chamber to form the new approximate latecomer. This procedure is performed iteratively for each datapoint (n) p (n) and often requires multiple passes through the training set for convergence. One disadvantage of the EP algorithm is the need to form the new approximate latecomer."}, {"heading": "C EP/SEP moment matching step", "text": "We have the EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP) EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-EP-"}, {"heading": "E Computing the gradients of the approximate marginal likelihood", "text": "The approximate marginal probability as discussed in the main text is as follows: F = \u2212 (N \u2212 1) \u03c6 (\u03b8) + N\u03c6 (\u03b8\\ 1) \u2212 \u03c6 (\u03b8prior) + N \u2211 n = 1logZn (18), where the natural parameters q (u), q\\ 1 (u) and p (u) are respectively the log normalization or log division function of a Gaussian distribution with natural parameters or mean m and covariance V. Consider the gradient of this target function: (V | + 1 2 m V \u2212 1m, (19) \u03b1 is the model hyperparameter to adjust, and logZn = log \u00b2 n (u) p (yn | u, Xn) du. Consider the gradient of this target function."}, {"heading": "F Dealing with non-Gaussian likelihoods", "text": "In this case, it is as if it were a matter of the kind that has been observed in recent years in the USA, Europe and the USA. (...) In this case, it is as if it had been able in the USA, Europe and the United States, in the USA and in the USA, in the USA and in the USA, in Europe and in the USA, in the EU and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in Europe and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in Europe and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA, in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA, in the USA, in the USA and in the USA, in the USA, in the USA, in the USA and in the USA, in the USA, in the USA, in the USA and in the USA, in"}], "references": [{"title": "Radial basis functions: a Bayesian treatment", "author": ["D. Barber", "B. Schottky"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Barber and Schottky,? \\Q1998\\E", "shortCiteRegEx": "Barber and Schottky", "year": 1998}, {"title": "Variational auto-encoded deep Gaussian processes", "author": ["Dai", "Zhenwen", "Damianou", "Andreas", "Gonz\u00e1lez", "Javier", "Lawrence", "Neil"], "venue": "arXiv preprint arXiv:1511.06455,", "citeRegEx": "Dai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2015}, {"title": "Deep Gaussian processes and variational propagation of uncertainty", "author": ["Damianou", "Andreas"], "venue": "PhD thesis,", "citeRegEx": "Damianou and Andreas.,? \\Q2015\\E", "shortCiteRegEx": "Damianou and Andreas.", "year": 2015}, {"title": "Deep Gaussian processes", "author": ["Damianou", "Andreas C", "Lawrence", "Neil D"], "venue": "In 16th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Damianou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Damianou et al\\.", "year": 2013}, {"title": "Expectation propagation in Gaussian process dynamical systems", "author": ["Deisenroth", "Marc", "Mohamed", "Shakir"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Deisenroth et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Deisenroth et al\\.", "year": 2012}, {"title": "Statistical comparisons of classifiers over multiple data sets", "author": ["Dem\u0161ar", "Janez"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Dem\u0161ar and Janez.,? \\Q2006\\E", "shortCiteRegEx": "Dem\u0161ar and Janez.", "year": 2006}, {"title": "Avoiding pathologies in very deep networks", "author": ["Duvenaud", "David", "Rippel", "Oren", "Adams", "Ryan P", "Ghahramani", "Zoubin"], "venue": "In 17th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Duvenaud et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Duvenaud et al\\.", "year": 2014}, {"title": "Dropout as a Bayesian approximation: Representing model uncertainty in deep learning", "author": ["Gal", "Yarin", "Ghahramani", "Zoubin"], "venue": "arXiv preprint arXiv:1506.02142,", "citeRegEx": "Gal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gal et al\\.", "year": 2015}, {"title": "Gaussian process priors with uncertain inputs \u2014 application to multiple-step ahead time series forecasting", "author": ["Girard", "Agathe", "Rasmussen", "Carl Edward", "Qui\u00f1onero-Candela", "Joaquin", "Murray-Smith", "Roderick"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Girard et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Girard et al\\.", "year": 2003}, {"title": "Practical variational inference for neural networks", "author": ["Graves", "Alex"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Graves and Alex.,? \\Q2011\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2011}, {"title": "The Harvard clean energy project: large-scale computational screening and design of organic photovoltaics on the world community", "author": ["Hachmann", "Johannes", "Olivares-Amaya", "Roberto", "Atahan-Evrenk", "Sule", "Amador-Bedolla", "Carlos", "S\u00e1nchez-Carrera", "Roel S", "Gold-Parker", "Aryeh", "Vogt", "Leslie", "Brockway", "Anna M", "Aspuru-Guzik", "Al\u00e1n"], "venue": "grid. The Journal of Physical Chemistry Letters,", "citeRegEx": "Hachmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hachmann et al\\.", "year": 2011}, {"title": "Nested variational compression in deep Gaussian processes", "author": ["Hensman", "James", "Lawrence", "Neil D"], "venue": "arXiv preprint arXiv:1412.1370,", "citeRegEx": "Hensman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hensman et al\\.", "year": 2014}, {"title": "Probabilistic backpropagation for scalable learning of Bayesian neural networks", "author": ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", "Adams", "Ryan P"], "venue": "In 32nd International Conference on Machine Learning,", "citeRegEx": "Hern\u00e1ndez.Lobato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hern\u00e1ndez.Lobato et al\\.", "year": 2015}, {"title": "Adam: a method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "In 3rd International Conference on Learning Representations,", "citeRegEx": "Kingma and Ba,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba", "year": 2015}, {"title": "Stochastic gradient VB and the variational auto-encoder", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "In The International Conference on Learning Representations,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Hierarchical Gaussian process latent variable models", "author": ["Lawrence", "Neil D", "Moore", "Andrew J"], "venue": "In 24th International Conference on Machine Learning,", "citeRegEx": "Lawrence et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lawrence et al\\.", "year": 2007}, {"title": "Bayesian warped Gaussian processes", "author": ["L\u00e1zaro-Gredilla", "Miguel"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "L\u00e1zaro.Gredilla and Miguel.,? \\Q2012\\E", "shortCiteRegEx": "L\u00e1zaro.Gredilla and Miguel.", "year": 2012}, {"title": "Stochastic expectation propagation", "author": ["Li", "Yingzhen", "Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", "Turner", "Richard E"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "A family of algorithms for approximate Bayesian inference", "author": ["Minka", "Thomas P"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "Minka and P.,? \\Q2001\\E", "shortCiteRegEx": "Minka and P.", "year": 2001}, {"title": "Bayesian learning via stochastic dynamics", "author": ["Neal", "Radford M"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Neal and M.,? \\Q1993\\E", "shortCiteRegEx": "Neal and M.", "year": 1993}, {"title": "Bayesian learning for neural networks", "author": ["Neal", "Radford M"], "venue": "PhD thesis, University of Toronto,", "citeRegEx": "Neal and M.,? \\Q1995\\E", "shortCiteRegEx": "Neal and M.", "year": 1995}, {"title": "Learning from the Harvard clean energy project: The use of neural networks to accelerate materials discovery", "author": ["Pyzer-Knapp", "Edward O", "Li", "Kewei", "Aspuru-Guzik", "Alan"], "venue": "Advanced Functional Materials,", "citeRegEx": "Pyzer.Knapp et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pyzer.Knapp et al\\.", "year": 2015}, {"title": "A unifying view of sparse approximate Gaussian process regression", "author": ["Qui\u00f1onero-Candela", "Joaquin", "Rasmussen", "Carl Edward"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Qui\u00f1onero.Candela et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Qui\u00f1onero.Candela et al\\.", "year": 2005}, {"title": "Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)", "author": ["Rasmussen", "Carl Edward", "Williams", "Christopher K. I"], "venue": null, "citeRegEx": "Rasmussen et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Rasmussen et al\\.", "year": 2005}, {"title": "Expectation propagation for exponential families", "author": ["Seeger", "Matthias"], "venue": "Technical report, Department of EECS, University of California at Berkeley,", "citeRegEx": "Seeger and Matthias.,? \\Q2007\\E", "shortCiteRegEx": "Seeger and Matthias.", "year": 2007}, {"title": "Sparse Gaussian processes using pseudo-inputs", "author": ["Snelson", "Edward", "Ghahramani", "Zoubin"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Snelson et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Snelson et al\\.", "year": 2006}, {"title": "Warped Gaussian processes", "author": ["Snelson", "Edward", "Rasmussen", "Carl Edward", "Ghahramani", "Zoubin"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Snelson et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Snelson et al\\.", "year": 2004}, {"title": "Introduction to Reinforcement Learning", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Variational learning of inducing variables in sparse Gaussian processes", "author": ["Titsias", "Michalis K"], "venue": "In 12th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Titsias and K.,? \\Q2009\\E", "shortCiteRegEx": "Titsias and K.", "year": 2009}, {"title": "Bayesian Gaussian process latent variable model", "author": ["Titsias", "Michalis K", "Lawrence", "Neil D"], "venue": "In 13th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Titsias et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Titsias et al\\.", "year": 2010}, {"title": "Two problems with variational expectation maximisation for time-series models", "author": ["R.E. Turner", "M. Sahani"], "venue": "Bayesian Time series models,", "citeRegEx": "Turner and Sahani,? \\Q2011\\E", "shortCiteRegEx": "Turner and Sahani", "year": 2011}, {"title": "MCMC methods for MLP-network and Gaussian process and stuff\u2013 a documentation for Matlab toolbox MCMCstuff", "author": ["Vanhatalo", "Jarno", "Vehtari", "Aki"], "venue": null, "citeRegEx": "Vanhatalo et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Vanhatalo et al\\.", "year": 2006}, {"title": "Bayesian learning via stochastic gradient Langevin dynamics", "author": ["Welling", "Max", "Teh", "Yee W"], "venue": "In 28th International Conference on Machine Learning,", "citeRegEx": "Welling et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Welling et al\\.", "year": 2011}, {"title": "Gaussian process kernels for pattern discovery and extrapolation", "author": ["Wilson", "Andrew", "Adams", "Ryan"], "venue": "In 30th International Conference on Machine Learning,", "citeRegEx": "Wilson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 26, "context": "A special case of DGPs whenL = 2 and the sole hidden layer h1 is only one dimensional is warped GPs (Snelson et al., 2004; L\u00e1zaro-Gredilla, 2012).", "startOffset": 100, "endOffset": 145}, {"referenceID": 1, "context": "An extension of Damianou & Lawrence (2013) that has skip links from the inputs to every hidden layer in the network was proposed in Dai et al. (2015), based on suggestions provided in Duvenaud et al.", "startOffset": 132, "endOffset": 150}, {"referenceID": 1, "context": "An extension of Damianou & Lawrence (2013) that has skip links from the inputs to every hidden layer in the network was proposed in Dai et al. (2015), based on suggestions provided in Duvenaud et al. (2014). Recent work by Hensman & Lawrence (2014) introduces a nested variational scheme that only requires a variational distribution over the inducing outputs, removing the parameter scaling problem of Damianou & Lawrence (2013).", "startOffset": 132, "endOffset": 207}, {"referenceID": 1, "context": "An extension of Damianou & Lawrence (2013) that has skip links from the inputs to every hidden layer in the network was proposed in Dai et al. (2015), based on suggestions provided in Duvenaud et al. (2014). Recent work by Hensman & Lawrence (2014) introduces a nested variational scheme that only requires a variational distribution over the inducing outputs, removing the parameter scaling problem of Damianou & Lawrence (2013).", "startOffset": 132, "endOffset": 249}, {"referenceID": 1, "context": "An extension of Damianou & Lawrence (2013) that has skip links from the inputs to every hidden layer in the network was proposed in Dai et al. (2015), based on suggestions provided in Duvenaud et al. (2014). Recent work by Hensman & Lawrence (2014) introduces a nested variational scheme that only requires a variational distribution over the inducing outputs, removing the parameter scaling problem of Damianou & Lawrence (2013). However, both approaches of Dai et al.", "startOffset": 132, "endOffset": 430}, {"referenceID": 1, "context": "An extension of Damianou & Lawrence (2013) that has skip links from the inputs to every hidden layer in the network was proposed in Dai et al. (2015), based on suggestions provided in Duvenaud et al. (2014). Recent work by Hensman & Lawrence (2014) introduces a nested variational scheme that only requires a variational distribution over the inducing outputs, removing the parameter scaling problem of Damianou & Lawrence (2013). However, both approaches of Dai et al. (2015) and Hensman & Lawrence (2014) have not been fully evaluated on benchmark supervised learning tasks or on medium to large scale datasets, nor compared to alternative deep models.", "startOffset": 132, "endOffset": 477}, {"referenceID": 1, "context": "An extension of Damianou & Lawrence (2013) that has skip links from the inputs to every hidden layer in the network was proposed in Dai et al. (2015), based on suggestions provided in Duvenaud et al. (2014). Recent work by Hensman & Lawrence (2014) introduces a nested variational scheme that only requires a variational distribution over the inducing outputs, removing the parameter scaling problem of Damianou & Lawrence (2013). However, both approaches of Dai et al. (2015) and Hensman & Lawrence (2014) have not been fully evaluated on benchmark supervised learning tasks or on medium to large scale datasets, nor compared to alternative deep models.", "startOffset": 132, "endOffset": 507}, {"referenceID": 1, "context": "An extension of Damianou & Lawrence (2013) that has skip links from the inputs to every hidden layer in the network was proposed in Dai et al. (2015), based on suggestions provided in Duvenaud et al. (2014). Recent work by Hensman & Lawrence (2014) introduces a nested variational scheme that only requires a variational distribution over the inducing outputs, removing the parameter scaling problem of Damianou & Lawrence (2013). However, both approaches of Dai et al. (2015) and Hensman & Lawrence (2014) have not been fully evaluated on benchmark supervised learning tasks or on medium to large scale datasets, nor compared to alternative deep models. A special case of DGPs whenL = 2 and the sole hidden layer h1 is only one dimensional is warped GPs (Snelson et al., 2004; L\u00e1zaro-Gredilla, 2012). In L\u00e1zaro-Gredilla (2012) a variational approach, in a similar spirit to Titsias (2009) and Damianou & Lawrence (2013), was used to jointly learn the latent functions.", "startOffset": 132, "endOffset": 828}, {"referenceID": 1, "context": "An extension of Damianou & Lawrence (2013) that has skip links from the inputs to every hidden layer in the network was proposed in Dai et al. (2015), based on suggestions provided in Duvenaud et al. (2014). Recent work by Hensman & Lawrence (2014) introduces a nested variational scheme that only requires a variational distribution over the inducing outputs, removing the parameter scaling problem of Damianou & Lawrence (2013). However, both approaches of Dai et al. (2015) and Hensman & Lawrence (2014) have not been fully evaluated on benchmark supervised learning tasks or on medium to large scale datasets, nor compared to alternative deep models. A special case of DGPs whenL = 2 and the sole hidden layer h1 is only one dimensional is warped GPs (Snelson et al., 2004; L\u00e1zaro-Gredilla, 2012). In L\u00e1zaro-Gredilla (2012) a variational approach, in a similar spirit to Titsias (2009) and Damianou & Lawrence (2013), was used to jointly learn the latent functions.", "startOffset": 132, "endOffset": 890}, {"referenceID": 1, "context": "An extension of Damianou & Lawrence (2013) that has skip links from the inputs to every hidden layer in the network was proposed in Dai et al. (2015), based on suggestions provided in Duvenaud et al. (2014). Recent work by Hensman & Lawrence (2014) introduces a nested variational scheme that only requires a variational distribution over the inducing outputs, removing the parameter scaling problem of Damianou & Lawrence (2013). However, both approaches of Dai et al. (2015) and Hensman & Lawrence (2014) have not been fully evaluated on benchmark supervised learning tasks or on medium to large scale datasets, nor compared to alternative deep models. A special case of DGPs whenL = 2 and the sole hidden layer h1 is only one dimensional is warped GPs (Snelson et al., 2004; L\u00e1zaro-Gredilla, 2012). In L\u00e1zaro-Gredilla (2012) a variational approach, in a similar spirit to Titsias (2009) and Damianou & Lawrence (2013), was used to jointly learn the latent functions.", "startOffset": 132, "endOffset": 921}, {"referenceID": 1, "context": "An extension of Damianou & Lawrence (2013) that has skip links from the inputs to every hidden layer in the network was proposed in Dai et al. (2015), based on suggestions provided in Duvenaud et al. (2014). Recent work by Hensman & Lawrence (2014) introduces a nested variational scheme that only requires a variational distribution over the inducing outputs, removing the parameter scaling problem of Damianou & Lawrence (2013). However, both approaches of Dai et al. (2015) and Hensman & Lawrence (2014) have not been fully evaluated on benchmark supervised learning tasks or on medium to large scale datasets, nor compared to alternative deep models. A special case of DGPs whenL = 2 and the sole hidden layer h1 is only one dimensional is warped GPs (Snelson et al., 2004; L\u00e1zaro-Gredilla, 2012). In L\u00e1zaro-Gredilla (2012) a variational approach, in a similar spirit to Titsias (2009) and Damianou & Lawrence (2013), was used to jointly learn the latent functions. In contrast, the latent function in the second layer is assumed to be deterministic and parameterised by a small set of parameters in Snelson et al. (2004), which can be learnt by maximising the analytically tractable marginal likelihood.", "startOffset": 132, "endOffset": 1126}, {"referenceID": 1, "context": "An extension of Damianou & Lawrence (2013) that has skip links from the inputs to every hidden layer in the network was proposed in Dai et al. (2015), based on suggestions provided in Duvenaud et al. (2014). Recent work by Hensman & Lawrence (2014) introduces a nested variational scheme that only requires a variational distribution over the inducing outputs, removing the parameter scaling problem of Damianou & Lawrence (2013). However, both approaches of Dai et al. (2015) and Hensman & Lawrence (2014) have not been fully evaluated on benchmark supervised learning tasks or on medium to large scale datasets, nor compared to alternative deep models. A special case of DGPs whenL = 2 and the sole hidden layer h1 is only one dimensional is warped GPs (Snelson et al., 2004; L\u00e1zaro-Gredilla, 2012). In L\u00e1zaro-Gredilla (2012) a variational approach, in a similar spirit to Titsias (2009) and Damianou & Lawrence (2013), was used to jointly learn the latent functions. In contrast, the latent function in the second layer is assumed to be deterministic and parameterised by a small set of parameters in Snelson et al. (2004), which can be learnt by maximising the analytically tractable marginal likelihood. However, the performance of warped GPs is often similar to a standard GP, most likely due to the narrow bottleneck in the hidden layer. Our work differs substantially from the above and introduces an alternative approximate inference scheme for DGPs based on three approximations. First, in order to sidestep the cubic computational cost of GPs we leverage a well-known pseudo point sparse approximation (Snelson & Ghahramani, 2006; Qui\u00f1onero-Candela & Rasmussen, 2005). Second, an approximation to the Expectation Propagation (EP) energy function (Seeger, 2007), a marginal likelihood estimate, is optimised directly to find an approximate posterior over the inducing outputs. Third, the optimisation demands analytically intractable moments that are approximated by nesting Assumed Density Filtering (Hern\u00e1ndez-Lobato & Adams, 2015). The proposed algorithm is not restricted to the warped GP case and is applicable to non-Gaussian observation models. The complexity of our method is similar to that of the variational approach proposed in Damianou & Lawrence (2013), O(NLM2), but is much less memory intensive, O(LM2) vs.", "startOffset": 132, "endOffset": 2277}, {"referenceID": 1, "context": "An extension of Damianou & Lawrence (2013) that has skip links from the inputs to every hidden layer in the network was proposed in Dai et al. (2015), based on suggestions provided in Duvenaud et al. (2014). Recent work by Hensman & Lawrence (2014) introduces a nested variational scheme that only requires a variational distribution over the inducing outputs, removing the parameter scaling problem of Damianou & Lawrence (2013). However, both approaches of Dai et al. (2015) and Hensman & Lawrence (2014) have not been fully evaluated on benchmark supervised learning tasks or on medium to large scale datasets, nor compared to alternative deep models. A special case of DGPs whenL = 2 and the sole hidden layer h1 is only one dimensional is warped GPs (Snelson et al., 2004; L\u00e1zaro-Gredilla, 2012). In L\u00e1zaro-Gredilla (2012) a variational approach, in a similar spirit to Titsias (2009) and Damianou & Lawrence (2013), was used to jointly learn the latent functions. In contrast, the latent function in the second layer is assumed to be deterministic and parameterised by a small set of parameters in Snelson et al. (2004), which can be learnt by maximising the analytically tractable marginal likelihood. However, the performance of warped GPs is often similar to a standard GP, most likely due to the narrow bottleneck in the hidden layer. Our work differs substantially from the above and introduces an alternative approximate inference scheme for DGPs based on three approximations. First, in order to sidestep the cubic computational cost of GPs we leverage a well-known pseudo point sparse approximation (Snelson & Ghahramani, 2006; Qui\u00f1onero-Candela & Rasmussen, 2005). Second, an approximation to the Expectation Propagation (EP) energy function (Seeger, 2007), a marginal likelihood estimate, is optimised directly to find an approximate posterior over the inducing outputs. Third, the optimisation demands analytically intractable moments that are approximated by nesting Assumed Density Filtering (Hern\u00e1ndez-Lobato & Adams, 2015). The proposed algorithm is not restricted to the warped GP case and is applicable to non-Gaussian observation models. The complexity of our method is similar to that of the variational approach proposed in Damianou & Lawrence (2013), O(NLM2), but is much less memory intensive, O(LM2) vs. O(NL+ LM2). These costs are competitive to those of the nested variational approach in Hensman & Lawrence (2014).", "startOffset": 132, "endOffset": 2446}, {"referenceID": 17, "context": "As such, approximate inference is needed; here we make use of the EP energy function with a tied factor constraint similar to that proposed in the Stochastic Expectation Propagation (SEP) algorithm (Li et al., 2015) to produce a scalable, convergent approximate inference method.", "startOffset": 198, "endOffset": 215}, {"referenceID": 17, "context": "Approximations of this form were recently used in the SEP algorithm (Li et al., 2015) and although seemingly limited, in practice were found to perform almost as well as full EP while significantly reducing EP\u2019s memory requirement, from O(NLM2) to O(LM2) in our case.", "startOffset": 68, "endOffset": 85}, {"referenceID": 8, "context": "Following (Girard et al., 2003; Barber & Schottky, 1998; Deisenroth & Mohamed, 2012), we can use the law of iterated conditionals to approximate the difficult integral in the equation above by a Gaussian Z \u2248 N (y|m2, v2) where the mean and variance take the following form, m2 = Eq(h1)[m2|h1 ] v2 = Eq(h1)[v2|h1 ] + varq(h1)[m2|h1 ] which results in", "startOffset": 10, "endOffset": 84}, {"referenceID": 10, "context": "org) (Hachmann et al., 2011).", "startOffset": 5, "endOffset": 28}, {"referenceID": 10, "context": "org) (Hachmann et al., 2011). We use 50,000 molecules for training and 10,000 for testing. The molecules are represented using 512-dimensional binary feature vectors, which were generated using the RDKit package, based on the molecular structures in the canonical SMILES format and a bond radius of 2. The power conversion efficiency of these molecules was estimated using density functional theory, determining whether a molecule could be potentially used as solar cell. The overall aim of the HCEP is to find organic molecules that are as efficient as their silicon counterparts. Our aim here is to show DGPs are effective predictive models that provide good uncertainty estimates, which can be used for tasks such as Bayesian Optimisation. We test the method on two DGPs with one hidden layer of 2 and 5 dimensions, denoted by DGP-2 and DGP-5 respectively and each GP is sparsified using 200 inducing outputs. We compare these against two FITC-GP architectures with 200 and 400 pseudo datapoints respectively. We also repeat the experiment using a Bayesian neural network with two hidden layers, each of 400 hidden units. We use the variational approach with the reparameterisation trick of Kingma & Welling (2014) to perform inference in this model.", "startOffset": 6, "endOffset": 1218}, {"referenceID": 10, "context": "org) (Hachmann et al., 2011). We use 50,000 molecules for training and 10,000 for testing. The molecules are represented using 512-dimensional binary feature vectors, which were generated using the RDKit package, based on the molecular structures in the canonical SMILES format and a bond radius of 2. The power conversion efficiency of these molecules was estimated using density functional theory, determining whether a molecule could be potentially used as solar cell. The overall aim of the HCEP is to find organic molecules that are as efficient as their silicon counterparts. Our aim here is to show DGPs are effective predictive models that provide good uncertainty estimates, which can be used for tasks such as Bayesian Optimisation. We test the method on two DGPs with one hidden layer of 2 and 5 dimensions, denoted by DGP-2 and DGP-5 respectively and each GP is sparsified using 200 inducing outputs. We compare these against two FITC-GP architectures with 200 and 400 pseudo datapoints respectively. We also repeat the experiment using a Bayesian neural network with two hidden layers, each of 400 hidden units. We use the variational approach with the reparameterisation trick of Kingma & Welling (2014) to perform inference in this model. The noise variance was fixed to 0.16 based on a suggestion in Pyzer-Knapp et al. (2015). Figure 5 shows the predictive performance by five architectures.", "startOffset": 6, "endOffset": 1342}], "year": 2016, "abstractText": "Deep Gaussian processes (DGPs) are multi-layer hierarchical generalisations of Gaussian processes (GPs) and are formally equivalent to neural networks with multiple, infinitely wide hidden layers. DGPs are nonparametric probabilistic models and as such are arguably more flexible, have a greater capacity to generalise, and provide better calibrated uncertainty estimates than alternative deep models. This paper develops a new approximate Bayesian learning scheme that enables DGPs to be applied to a range of medium to large scale regression problems for the first time. The new method uses an approximate Expectation Propagation procedure and a novel and efficient extension of the probabilistic backpropagation algorithm for learning. We evaluate the new method for non-linear regression on eleven real-world datasets, showing that it always outperforms GP regression and is almost always better than state-of-the-art deterministic and sampling-based approximate inference methods for Bayesian neural networks. As a by-product, this work provides a comprehensive analysis of six approximate Bayesian methods for training neural networks.", "creator": "LaTeX with hyperref package"}}}