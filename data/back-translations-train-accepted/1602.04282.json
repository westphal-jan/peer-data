{"id": "1602.04282", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Feb-2016", "title": "Conservative Bandits", "abstract": "We study a novel multi-armed bandit problem that models the challenge faced by a company wishing to explore new strategies to maximize revenue whilst simultaneously maintaining their revenue above a fixed baseline, uniformly over time. While previous work addressed the problem under the weaker requirement of maintaining the revenue constraint only at a given fixed time in the future, the algorithms previously proposed are unsuitable due to their design under the more stringent constraints. We consider both the stochastic and the adversarial settings, where we propose, natural, yet novel strategies and analyze the price for maintaining the constraints. Amongst other things, we prove both high probability and expectation bounds on the regret, while we also consider both the problem of maintaining the constraints with high probability or expectation. For the adversarial setting the price of maintaining the constraint appears to be higher, at least for the algorithm considered. A lower bound is given showing that the algorithm for the stochastic setting is almost optimal. Empirical results obtained in synthetic environments complement our theoretical findings.", "histories": [["v1", "Sat, 13 Feb 2016 03:47:11 GMT  (38kb,D)", "http://arxiv.org/abs/1602.04282v1", "9 pages, plus 4-page appendix, with 3 figures. Submitted to ICML 2016"]], "COMMENTS": "9 pages, plus 4-page appendix, with 3 figures. Submitted to ICML 2016", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["yifan wu", "roshan shariff", "tor lattimore", "csaba szepesv\u00e1ri"], "accepted": true, "id": "1602.04282"}, "pdf": {"name": "1602.04282.pdf", "metadata": {"source": "CRF", "title": "Conservative Bandits", "authors": ["Yifan Wu", "Roshan Shariff", "Tor Lattimore", "Csaba Szepesv\u00e1ri"], "emails": ["ywu12@ualberta.ca", "rshariff@ualberta.ca", "tlattimo@ualberta.ca", "szepesva@ualberta.ca"], "sections": [{"heading": "1. Introduction", "text": "The manager of Zonlex is aware that a fictitious company, at least initially, has a problem with bandit algorithms, and he is very excited about the opportunity to use this advanced technology to maximize Zonlex's revenue by optimizing the site's contents. Every click on the content of their site pays a small reward; thanks to the high traffic that the Zonlex site enjoys, this translates into a decent revenue stream. currently, Zonlex is unable to provide the site's contents with a strategy designed over the years by its best engineers, but the manager suspects that some alternative strategies could potentially generate significantly more revenue. The manager is willing to explore bandit algorithms to identify the winning strategy. The manager cannot afford to lose more than 10% of his current revenue during his day-to-to-day operations, and at any given time, Zonlex needs a lot of cash to support its operations."}, {"heading": "1.1. Previous Work", "text": "Our limitation is tantamount to a limitation of regret to a standard strategy or, in the language of prediction-withexpert advice, or bandit literature, regret to a standard measure. In the complete information, mostly studied in the opposing setting, much work has been devoted to understanding the price of such limitations (Hutter and Poland, 2005; EvenDar et al., 2008; Koolen, 2013; Sani et al., 2014, e.g.). In particular, Kodez (2013) investigates the Pareto-limit of regret vectors (which contains the non-dominated worst-case regret vectors of all algorithms).The main lesson from this work is that in full information setting even a constant regret to a fixed standard measure can be maintained without substantially no increase in regret to the best measures. The situation is rapidly deteriorating in the bandit setting, as is shown by Lattimore (2015a).This is perhaps not surprising, as opposed to the full information set to one full action in the Bandit setting."}, {"heading": "2. Conservative Multi-Armed Bandits", "text": "The multi-arm bandit problem is a sequential decision-making task in which a learner repeatedly selects an action (called arm) and receives a reward corresponding to that action. We assume that there are K + 1 arms, and designate the arm that the agent selects in turn t (1, 2,...) by It (0,..., K). There is a reward Xt, i that is connected to each arm i in each turn t, and the agent receives the reward corresponding to his chosen arm, Xt, It. The agent does not observe the other Xt, j (j, It). An agent's learning performance over a time horizon n is usually measured by his regret, which is the difference between his reward and what he could have achieved by consistently selecting the only best arm afterwards: Rn = max i (0,..., K) n = 1 Xt, i \u2212 Xt, It (1)."}, {"heading": "2.1. Conservative Exploration", "text": "We want to be able to select some \u03b1 > 0 and force the learner to earn at least a 1 \u2212 \u03b1 fraction of the reward if he simply plays arm 0: t \u2211 s = 1Xs. (2) For the introductory example above \u03b1 = 0.1, which corresponds to a loss of at most 10% of the revenue compared to the standard website. It should be clear that small values of \u03b1 force the learner to be highly conservative, while larger \u03b1 corresponds to a weaker restriction. We introduce a quantity of Zn, called the budget section, which quantifies how close the restriction (2) is to violating the restriction: Zt = t \u0445 s = 1Xs, Is \u2212 (1 \u2212 \u03b1) Xs, 0; (3) the restriction is met if we only comply with the restriction of algorithms (2)."}, {"heading": "3. The Stochastic Setting", "text": "In the stochastic, multi-armed bandit that sets each arm i and round t, there is a stochastic reward Xt, i = \u00b5i + \u03b7t, i, where \u00b5i [0, 1] is the expected reward of the arm i and \u03b7t, i are independent random sound variables that we assume to have 1-subgaussian distributions. We refer to the expected reward of the optimal arm with \u00b5 = maxi \u00b5i and the gap between it and the expected reward of the ith arm with \u0445 i = \u00b5. \u2212 \u00b5i. Regret Rn is now a random variable. Of course, we can include it in expectation, but we are often more interested in the highly probable limits of the weaker notion of a pseudo-regulation: R \u014fn = n\u00b5. \u2212 n = 1\u00b5Es = K \u00b2 i = 0 Ti (n) dsi, (4) whereby the sound in the rewards of the arms is ignored and the randomness of the choice of the arm results."}, {"heading": "3.1. The Budget Constraint", "text": "Just as we have replaced repentance with pseudo-repentance, in the stochastic setting we will apply the following form of constraint (2): t \u0445 s = 1 \u00b5Is \u2265 (1 \u2212 \u03b1) \u00b50t for all t {1,.., n}; (5) the budget will then become Z = t = 1\u00b5Is \u2212 (1 \u2212 \u03b1) t\u00b50. (6) The standard arm is always safe to play because it increases the budget by \u00b50 \u2212 (1 \u2212 \u03b1) \u00b50 = \u03b1\u00b50. The budget will decrease for the poor i with \u00b5i < (1 \u2212 \u03b1) \u00b50; the constraint Z \u0435n \u2265 0 is then in danger of being violated (Fig. 1). In the following sections we will construct algorithms that will satisfy pseudo-repentance limits and reduce the budget constraint (5) with high probability, so that we will know that the constraint of the budget 1 \u2212 \u03b4 (where the limit > 0 is an unworkable parameter)."}, {"heading": "3.2. BudgetFirst \u2014 A Naive Algorithm", "text": "Before introducing the new algorithm, it is worth pointing out the most obvious na\u00efve attempt, which we call the BudgetFirst algorithm. A simple modification of the UCB results in an algorithm that accepts a confidence parameter \u03b4 (0, 1) and feels remorse for mostR-n = O-Kn-log (log (n) \u03b4) = Rworst. (7) Of course, this algorithm alone will not meet the limitation (5), but this can be forced by na\u00efve modification of the algorithm to choose it deterministically = 0 for the first t0 rounds in which (\u0432 t0 \u2264 t \u2264 n) t\u00b50 \u2212 Rworst \u2265 (1 \u2212 \u03b1) t\u00b50. Subsequently, the algorithm plays the highly probable version of UCB and the repentance guarantee (7) ensures that the limitation (5) is satisfied with high probability. The solution of the standard equation of tO = t\u00b50 is significantly higher (the worst), while the R\u00b51 is marginal (the worst), while the value of the guarantee is less than 1)."}, {"heading": "3.3. Conservative UCB", "text": "A better strategy is to play the standard arm only until the budget (6) is large enough to start exploring other arms with a low risk of breaching the constraint. It is safe to maintain the research of a standard bandit algorithm with occasional budget building phases when required. We show that accumulating a budget does not severely restrict confidence and is therefore small regrettable.Conservative UCB (Algorithm 1) is based on the novel twist of maintaining a positive budget. In each round, UCB calculates upper confidence for each arm; let Jt be the arm that maximizes that confidence. Before playing this arm (as UCB would), our algorithm decides whether the budget becomes negative."}, {"heading": "3.4. Considering the Expected Regret and Budget", "text": "One may think about the performance of the algorithm in expectation rather than with high probability, i.e. we want an upper limit for E [R \u0435n] and the constraint (5) becomes E [t \u2211 s = 1\u00b5Is] \u2265 (1 \u2212 \u03b1) \u00b50t, for all t {1,.., n}. (13) We argued in note 3 that the problem is trivially difficult to solve if it is \u03b1 O (1 / n); let us therefore assume that \u03b1 \u2265 c / n is for some c > 1. By performing algorithm 1 with 3 = 1 / n and \u03b1 = (\u03b1 \u2212 \u03b4) / (1 \u2212 \u03b4), we can (13) and achieve a regret in the same order as in theorem 2. To show (13) that we have replaced E [t \u0445 s = 1\u00b5Is] \u2265 P {F} E [t \u0445 s = 1 \u00b5Is = 1 \u00b5Is = theorem (1 \u2212 \u03b1) \u00b50t = (1 \u2212 \u03b1).n), we can have the upper limit E \u00b2 [R] replaced."}, {"heading": "3.5. Learning an Unknown \u00b50", "text": "Two modifications of algorithm 1 allow it to handle the case when \u00b50 is unknown. Firstly, the lower limit of the budget, as with the non-standard arms, must be determined on the basis of confidence intervals, as is the case with the two groups. (14) Theorem 5. Algorithm 1, modified as above to function without the knowledge of \u00b50, but otherwise fulfilling the same conditions as theorem 2, the probability 1 \u2212 \u03b4 and for all time horizons n of constraint (5) and the limit of repentance R: n \u2264 i > 0 (4L \u0445i + \u0445 i) + 2 (K + 1)."}, {"heading": "4. The Adversarial Setting", "text": "Unlike the stochastic case in which all other weapons are generated, we are not satisfied with any assumptions about how the rewards are generated. Instead, we analyze a learner's worst-case performance across all possible sequences of rewards (Xt, i). In fact, we treat the environment as an opponent who has intimate knowledge of the learner's strategy and will develop a sequence of rewards that maximizes his regret. However, in order to maintain a certain degree of hope for success, the learner is allowed to behave randomly: in each turn, he can randomize his choice of arm by constructing a distribution; the opponent cannot influence or predict the outcome of this random decision. Our goal, as before, is to satisfy the limitation, while limiting the regret (1) with high probability (randomness comes from the learner's actions). We assume that the standard gut has a fixed reward: Xerit = 1 for all other weapons."}, {"heading": "5. Lower Bound on the Regret", "text": "For each vector \u00b5 [0, 1] K, we will write E\u00b5 to indicate expectations in an environment where all arms have normal distributed variance rewards and means \u00b5i (i.e., the fixed value \u00b50 is the mean reward of arm 0 and the components of \u00b5 are the mean rewards of the other arms).We assume that normally distributed noise has the same effect for the sake of simplicity: other subgaussian distributions work identically as long as the subgaussian parameter can be fixed independently of the average rewards. Theorem 9. Let's set for any \u00b5i forms [0 > 0] (i > 0) and \u00b50 satisfyingmin {\u00b50, 1 \u2212 \u00b50} (max {1 / 2}) lower than the lower value for the subgaussian parameters can be set, rewards can be considered independently of the mean forms > \u00b5i (max {1 / 2})."}, {"heading": "6. Experiments", "text": "We evaluate the performance of the conservative UCB compared to UCB and Unbalanced MOSS Lattimore (2015a) using simulated data in two regimes. In the second regime, we fix the horizon and traverse \u03b1 [0, 1] to show the deterioration of the average regret of the conservative UCB relative to UCB as the restriction becomes harder (\u03b1 close to zero). In the third regime, we fix \u03b1 = 0.1 and record the long-term average regret, showing that the conservative UCB is ultimately almost as good as UCB despite the restriction. Each data point is an average of N \u2248 4000 i.i.d. samples, making error bars too small to see. All codes and data are provided in each final version. Results are shown for both versions of the conservative UCB: the first knows the mean \u00b50 of the standard arm, while the second does not have to act conservatively while learning this value."}, {"heading": "7. Conclusion", "text": "We have introduced a new family of multi-armed bandit structures motivated by the need to research conservatively in order to preserve revenue. We have also demonstrated various strategies that act effectively while maintaining such constraints. We expect similar strategies to extend to other environments, such as context bandits and reinforcement learning. We would like to emphasize that this is just the beginning of a line of research that has many potential applications. We hope that others will support us in improving current outcomes, closing open problems and generalizing the model so that it is more widely applicable."}, {"heading": "A. Proof of Theorem 2", "text": "Theorem 2: In every stochastic environment in which the poor have expected rewards, we have assumed that \"i\" (0, 1) \"i\" (\"i\") \"i\" (\"i\") i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i."}, {"heading": "B. Proof of Theorem 5", "text": "Theorem 5. Algorithm 1, modified as described above to work without knowing how it works, but otherwise the same conditions as theorem 2 (actually 1) are met with probability 1 \u2212 \u03b4 and for all time horizons n of constraint (5) and the regret limit. (15) Proof. We proceed very similarly before the proof of theorem 2 in appendix A. As we have done, we assume that the confidence intervals apply to all rounds and all arms (including the default), with the probability P {F} 1 \u2212 Proof. To show that the modified algorithm meets the constraint (5), we write the budget (6) asZ-t = K = 1Ti."}, {"heading": "C. Proof of Theorem 7", "text": "Theorem 7: It fulfills the restriction (2) and has a remorse limit of Rn \u2264 t0 + R, the probability being at least 1 \u2212 \u03b4, where t0 = max {t | \u03b1\u00b50t \u2264 R \u03b4t + \u00b50} is met. Proof of Theorem 7: From the description of the safe-play strategy it is clear that it is indeed safe: the restriction (2) is always satisfactory. The algorithm plays it safe if the following value, which represents a lower limit for the Zt budget, is negative: Z \u2032 t = Zt \u2212 Xt, It = t \u2212 1 x s = 1 x s, Is \u2212 (1 \u2212 \u03b1) x 0tTo upper bound the regret, consider only those rounds in which our safe-play strategy does not interfere with the choice of the game arm. Then with probability 1 \u2212 Z: 0,..."}, {"heading": "D. Proof of Theorem 9", "text": "Theoretically, we can assume that the algorithm is consistent in the sense that the algorithm is consistent in relation to the entire environment."}], "references": [{"title": "Sample mean based index policies with o(log n) regret for the multi-armed bandit problem", "author": ["R. Agrawal"], "venue": "Advances in Applied Probability,", "citeRegEx": "Agrawal.,? \\Q1995\\E", "shortCiteRegEx": "Agrawal.", "year": 1995}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Regret to the best vs. regret to the average", "author": ["E. Even-Dar", "M. Kearns", "Y. Mansour", "J. Wortman"], "venue": "Machine Learning,", "citeRegEx": "Even.Dar et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2008}, {"title": "Distributed policy search reinforcement learning for job-shop scheduling", "author": ["T. Gabel", "M. Riedmiller"], "venue": "tasks. International Journal of Production Research,", "citeRegEx": "Gabel and Riedmiller.,? \\Q2011\\E", "shortCiteRegEx": "Gabel and Riedmiller.", "year": 2011}, {"title": "A comprehensive survey on safe reinforcement learning", "author": ["J. Garc\u0131\u0301a", "F. Fern\u00e1ndez"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Garc\u0131\u0301a and Fern\u00e1ndez.,? \\Q2015\\E", "shortCiteRegEx": "Garc\u0131\u0301a and Fern\u00e1ndez.", "year": 2015}, {"title": "Informational confidence bounds for selfnormalized averages and applications", "author": ["A. Garivier"], "venue": "arXiv preprint arXiv:1309.3376,", "citeRegEx": "Garivier.,? \\Q2013\\E", "shortCiteRegEx": "Garivier.", "year": 2013}, {"title": "Adaptive online prediction by following the perturbed leader", "author": ["M. Hutter", "J. Poland"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hutter and Poland.,? \\Q2005\\E", "shortCiteRegEx": "Hutter and Poland.", "year": 2005}, {"title": "lil\u2019UCB: An optimal exploration algorithm for multiarmed bandits", "author": ["K. Jamieson", "M. Malloy", "R. Nowak", "S. Bubeck"], "venue": null, "citeRegEx": "Jamieson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jamieson et al\\.", "year": 2014}, {"title": "Sequential choice from several populations", "author": ["M.N. Katehakis", "H. Robbins"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "Katehakis and Robbins.,? \\Q1995\\E", "shortCiteRegEx": "Katehakis and Robbins.", "year": 1995}, {"title": "On the complexity of best arm identification in multi-armed bandit models", "author": ["E. Kaufmann", "A. Garivier", "O. Capp\u00e9"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Kaufmann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kaufmann et al\\.", "year": 2015}, {"title": "The Pareto regret frontier", "author": ["W.M. Koolen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Koolen.,? \\Q2013\\E", "shortCiteRegEx": "Koolen.", "year": 2013}, {"title": "The Pareto regret frontier for bandits", "author": ["T. Lattimore"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Lattimore.,? \\Q2015\\E", "shortCiteRegEx": "Lattimore.", "year": 2015}, {"title": "Optimally confident UCB : Improved regret for finite-armed bandits", "author": ["T. Lattimore"], "venue": "Technical report,", "citeRegEx": "Lattimore.,? \\Q2015\\E", "shortCiteRegEx": "Lattimore.", "year": 2015}, {"title": "Towards automatic experimentation of educational knowledge", "author": ["Y.-E. Liu", "T. Mandel", "E. Brunskill", "Z. Popovi\u0107"], "venue": "In SIGCHI Conference on Human Factors in Computing Systems (CHI", "citeRegEx": "Liu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "Explore no more: Improved high-probability regret bounds for non-stochastic bandits", "author": ["G. Neu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Neu.,? \\Q2015\\E", "shortCiteRegEx": "Neu.", "year": 2015}, {"title": "Learning effective multimodal dialogue strategies from Wizard-of-Oz data: Bootstrapping and evaluation", "author": ["V. Rieser", "O. Lemon"], "venue": "In ACL-08: HLT,", "citeRegEx": "Rieser and Lemon.,? \\Q2008\\E", "shortCiteRegEx": "Rieser and Lemon.", "year": 2008}, {"title": "Exploiting easy data in online optimization", "author": ["A. Sani", "G. Neu", "A. Lazaric"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sani et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sani et al\\.", "year": 2014}, {"title": "Safe exploration for optimization with Gaussian processes", "author": ["Y. Sui", "A. Gotovos", "J. Burdick", "A. Krause"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Sui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sui et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 13, "context": "In these cases a designer may feel that experimenting with sub-par interaction strategies could cause more harm than good (e.g., Rieser and Lemon, 2008; Liu et al., 2014).", "startOffset": 122, "endOffset": 170}, {"referenceID": 3, "context": ", Gabel and Riedmiller, 2011), but deviating too much from established \u201cbest practices\u201d will often be considered too dangerous. For examples from other domains see the survey paper of Garc\u0131\u0301a and Fern\u00e1ndez (2015).", "startOffset": 2, "endOffset": 213}, {"referenceID": 7, "context": "The manager even thinks that perhaps a best-arm identification strategy from the bandit literature, such as the recent lil\u2019UCB method of Jamieson et al. (2014), could be more suitable.", "startOffset": 137, "endOffset": 160}, {"referenceID": 8, "context": "(ii) We analyze the naive build-budget-then-learn strategy described above (which we call BudgetFirst) and design a significantly better alternative for stochastic bandits that switches between using the default arm and learning (using a version of UCB, a simple yet effective bandit learning algorithm: Agrawal, 1995; Katehakis and Robbins, 1995; Auer et al., 2002) in a \u201csmoother\u201d fashion.", "startOffset": 229, "endOffset": 366}, {"referenceID": 1, "context": "(ii) We analyze the naive build-budget-then-learn strategy described above (which we call BudgetFirst) and design a significantly better alternative for stochastic bandits that switches between using the default arm and learning (using a version of UCB, a simple yet effective bandit learning algorithm: Agrawal, 1995; Katehakis and Robbins, 1995; Auer et al., 2002) in a \u201csmoother\u201d fashion.", "startOffset": 229, "endOffset": 366}, {"referenceID": 8, "context": "In fact, this approach has been studied recently by Lattimore (2015a) (in a slightly more general setting than ours).", "startOffset": 52, "endOffset": 70}, {"referenceID": 8, "context": "In fact, this approach has been studied recently by Lattimore (2015a) (in a slightly more general setting than ours). However, the algorithm of Lattimore (2015a) cannot be guaranteed to maintain the return constraint uniformly in time.", "startOffset": 52, "endOffset": 162}, {"referenceID": 0, "context": "(ii) We analyze the naive build-budget-then-learn strategy described above (which we call BudgetFirst) and design a significantly better alternative for stochastic bandits that switches between using the default arm and learning (using a version of UCB, a simple yet effective bandit learning algorithm: Agrawal, 1995; Katehakis and Robbins, 1995; Auer et al., 2002) in a \u201csmoother\u201d fashion. (iii) We prove that the new algorithm, which we call Conservative UCB, meets the uniform return constraint (in various senses), while it can achieve significantly less regret than BudgetFirst. In particular, while BudgetFirst is shown to pay a multiplicative penalty in the regret for maintaining the return constraint, Conservative UCB only pays an additive penalty. We provide both high probability and expectation bounds, consider both high probability and expectation constraints on the return, and also consider the case when the payoff of the default arm is initially unknown. (iv) We also prove a lower bound on the best regret given the constraint and as a result show that the additive penalty is unavoidable; thus Conservative UCB achieves the optimal regret in a worst-case sense. While Unbalanced MOSS of Lattimore (2015a), when specialized to our setting, also achieves the optimal regret (as follows from the analysis of Lattimore, 2015a), as mentioned earlier it does not maintain the constraint uniformly in time (it will explore too much at the beginning of time); it also relies heavily on the knowledge of the mean payoff of the default strategy.", "startOffset": 304, "endOffset": 1227}, {"referenceID": 0, "context": "(ii) We analyze the naive build-budget-then-learn strategy described above (which we call BudgetFirst) and design a significantly better alternative for stochastic bandits that switches between using the default arm and learning (using a version of UCB, a simple yet effective bandit learning algorithm: Agrawal, 1995; Katehakis and Robbins, 1995; Auer et al., 2002) in a \u201csmoother\u201d fashion. (iii) We prove that the new algorithm, which we call Conservative UCB, meets the uniform return constraint (in various senses), while it can achieve significantly less regret than BudgetFirst. In particular, while BudgetFirst is shown to pay a multiplicative penalty in the regret for maintaining the return constraint, Conservative UCB only pays an additive penalty. We provide both high probability and expectation bounds, consider both high probability and expectation constraints on the return, and also consider the case when the payoff of the default arm is initially unknown. (iv) We also prove a lower bound on the best regret given the constraint and as a result show that the additive penalty is unavoidable; thus Conservative UCB achieves the optimal regret in a worst-case sense. While Unbalanced MOSS of Lattimore (2015a), when specialized to our setting, also achieves the optimal regret (as follows from the analysis of Lattimore, 2015a), as mentioned earlier it does not maintain the constraint uniformly in time (it will explore too much at the beginning of time); it also relies heavily on the knowledge of the mean payoff of the default strategy. (v) We also consider the adversarial setting where we design an algorithm similar to Conservative UCB: the algorithm uses an underlying \u201cbase\u201d adversarial bandit strategy when it finds that the return so far is sufficiently higher than the minimum required return. We prove that the resulting method indeed maintains the return constraint uniformly in time and we also prove a high-probability bound on its regret. We find, however, that the additive penalty in this case is higher than in the stochastic case. Here, the Exp3-\u03b3 algorithm of Lattimore (2015a) is an alternative, but again, this algorithm is not able to maintain the return constraint uniformly in time.", "startOffset": 304, "endOffset": 2117}, {"referenceID": 6, "context": "In the full information, mostly studied in the adversarial setting, much work has been devoted to understanding the price of such constraints (Hutter and Poland, 2005; EvenDar et al., 2008; Koolen, 2013; Sani et al., 2014, e.g.,). In particular, Koolen (2013) studies the Pareto frontier of regret vectors (which contains the non-dominated worst-case regret vectors of all algorithms).", "startOffset": 143, "endOffset": 260}, {"referenceID": 6, "context": "In the full information, mostly studied in the adversarial setting, much work has been devoted to understanding the price of such constraints (Hutter and Poland, 2005; EvenDar et al., 2008; Koolen, 2013; Sani et al., 2014, e.g.,). In particular, Koolen (2013) studies the Pareto frontier of regret vectors (which contains the non-dominated worst-case regret vectors of all algorithms). The main lesson of these works is that in the full information setting even a constant regret to a fixed default action can be maintained with essentially no increase in the regret to the best action. The situation quickly deteriorates in the bandit setting as shown by Lattimore (2015a). This is perhaps unsurprising given that, as opposed to the full information setting, in the bandit setting one needs to actively explore to get improved estimates", "startOffset": 143, "endOffset": 674}, {"referenceID": 17, "context": "Another line of work considers safe exploration in the related context of optimization (Sui et al., 2015).", "startOffset": 87, "endOffset": 105}, {"referenceID": 4, "context": "Garc\u0131\u0301a and Fern\u00e1ndez (2015) provides a comprehensive survey of the relevant literature.", "startOffset": 0, "endOffset": 29}, {"referenceID": 5, "context": "where \u03b6 = K/\u03b4; it can be seen to achieve (8) by more careful analysis motivated by Garivier (2013),", "startOffset": 83, "endOffset": 99}, {"referenceID": 14, "context": "For example, Neu (2015) states the following for the any-time version of their algorithm: given any time horizon n and confidence level \u03b4, P { Rn \u2264 R\u0302n(\u03b4) } \u2265 1 \u2212 \u03b4 for some sub-linear R\u0302t(\u03b4).", "startOffset": 13, "endOffset": 24}, {"referenceID": 14, "context": "The any-time high probability algorithm of Neu (2015) adapted with our safe-playing strategy gives R\u0302t = 7 \u221a Kt log K log(4t2/\u03b4) and", "startOffset": 43, "endOffset": 54}, {"referenceID": 11, "context": "The theorem above almost follows from the lower bound given by Lattimore (2015a), but in that paper \u03bc0 is unknown, while here it may be known.", "startOffset": 63, "endOffset": 81}, {"referenceID": 11, "context": "We evaluate the performance of Conservative UCB compared to UCB and Unbalanced MOSS Lattimore (2015a) using simulated data in two regimes.", "startOffset": 84, "endOffset": 102}, {"referenceID": 11, "context": "The quantity Bi determines the regret of the algorithm with respect to arm i up to constant factors, and must be chosen to lie inside the Pareto frontier given by Lattimore (2015a). It should be emphasised that Unbalanced MOSS does not constraint the return except for the last round, and has no high-probability guarantees.", "startOffset": 163, "endOffset": 181}], "year": 2016, "abstractText": "We study a novel multi-armed bandit problem that models the challenge faced by a company wishing to explore new strategies to maximize revenue whilst simultaneously maintaining their revenue above a fixed baseline, uniformly over time. While previous work addressed the problem under the weaker requirement of maintaining the revenue constraint only at a given fixed time in the future, the algorithms previously proposed are unsuitable due to their design under the more stringent constraints. We consider both the stochastic and the adversarial settings, where we propose, natural, yet novel strategies and analyze the price for maintaining the constraints. Amongst other things, we prove both high probability and expectation bounds on the regret, while we also consider both the problem of maintaining the constraints with high probability or expectation. For the adversarial setting the price of maintaining the constraint appears to be higher, at least for the algorithm considered. A lower bound is given showing that the algorithm for the stochastic setting is almost optimal. Empirical results obtained in synthetic environments complement our theoretical findings.", "creator": "LaTeX with hyperref package"}}}