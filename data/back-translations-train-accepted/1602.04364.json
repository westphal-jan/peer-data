{"id": "1602.04364", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Feb-2016", "title": "Look, Listen and Learn - A Multimodal LSTM for Speaker Identification", "abstract": "Speaker identification refers to the task of localizing the face of a person who has the same identity as the ongoing voice in a video. This task not only requires collective perception over both visual and auditory signals, the robustness to handle severe quality degradations and unconstrained content variations are also indispensable. In this paper, we describe a novel multimodal Long Short-Term Memory (LSTM) architecture which seamlessly unifies both visual and auditory modalities from the beginning of each sequence input. The key idea is to extend the conventional LSTM by not only sharing weights across time steps, but also sharing weights across modalities. We show that modeling the temporal dependency across face and voice can significantly improve the robustness to content quality degradations and variations. We also found that our multimodal LSTM is robustness to distractors, namely the non-speaking identities. We applied our multimodal LSTM to The Big Bang Theory dataset and showed that our system outperforms the state-of-the-art systems in speaker identification with lower false alarm rate and higher recognition accuracy.", "histories": [["v1", "Sat, 13 Feb 2016 18:49:50 GMT  (8463kb)", "http://arxiv.org/abs/1602.04364v1", "The 30th AAAI Conference on Artificial Intelligence (AAAI-16)"]], "COMMENTS": "The 30th AAAI Conference on Artificial Intelligence (AAAI-16)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jimmy s j ren", "yongtao hu", "yu-wing tai", "chuan wang", "li xu 0001", "wenxiu sun", "qiong yan"], "accepted": true, "id": "1602.04364"}, "pdf": {"name": "1602.04364.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Jimmy Ren", "Yongtao Hu", "Yu-Wing Tai", "Chuan Wang", "Li Xu", "Wenxiu Sun Qiong Yan"], "emails": ["yanqiong}@sensetime.com", "wangchuan2400}@gmail.com"], "sections": [{"heading": null, "text": "In fact, it is a way in which people are able to determine for themselves what they want and what they want, and in which people are able to decide what they want and what they want and what they do not want. In fact, it is a way in which people are able to decide what they want and what they do not want."}, {"heading": "Related Work", "text": "The pioneering work of (Taigman et al. 2014) proposed a very deep CNN architecture along with an alignment technology to perform face checks that were achieved near human performance. However, inspired by GoogLeNet (Szegedy et al. 2015), Sun et al. 2014b used a very deep CNN network with multiple levels of surveillance that exceeded the results of human face checks in the LFW datasets (Donang and Learned-Miller 2013). Recent advances in this area (Schruff, Kalenichenko and Philbin 2015) pushed the performance of image transmission even further. In facial recognition, the results of face checks were also achieved by CNN-based models (Yang et al. 2015; Li et al. 2015). For other face-related tasks such as face recognition and face recognition (Zhang et al. 2015a), based models were widely adopted."}, {"heading": "LSTM - Single VS. Multi-Modal", "text": "The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\" The \"The\""}, {"heading": "Experiments", "text": "Three experiments were carefully designed to test the robustness and applicability of our proposed Model 53. Dataset overview We chose the TV series The Big Bang Theory (BBT) as the data source. It was shown that the task of speaker identification to BBT presents a very challenging multimodal learning problem due to various types of image degradation and the high variation of faces in the videos (Bauml, Tapaswi and Stiefelhagen 2013; Hu et al. 2015; Tapaswi, Ba \ufffd uml and Stiefelhagen 2012). In data acquisition, we performed face recognition and extracted all faces in six episodes in the first season and another six episodes in the second season of BBT. We commented on the faces of the five leading characters manually, i.e. Sheldon, Leonard, Raj and Penny. In total, we have more than 310,000 consecutive commented facial images for the five characters."}, {"heading": "LSTM for Face Sequences", "text": "Our first task is to investigate the extent to which modelling the temporal dependence of high-level CNN functions improves the robustness of quality deterioration and facial variation, for two reasons. Firstly, it has been shown that RNN can be successfully used in speech recognition to improve the robustness of noise (Graves, Mohamed and Hinton 2013), it was not clear from the literature whether a similar principle applies to high-level image characteristics. Secondly, we would like to clearly measure the extent to which this approach works for faces, because this is an important cornerstone for the rest of the experiments.Data Only facial data in the data sequence mentioned above was used in the experiment. We stamped 40,000 facial sequences from the training facial images and another 40,000 facial sequences in the test facial images. Note that each sequence was extracted according to the temporal sequence in the data. We did not guarantee that the sequence was derived exclusively from a subordinate segment."}, {"heading": "Comparison among Multimodal LSTMs", "text": "In fact, most people are able to survive themselves if they don't put themselves in a position to survive themselves, and most of them are able to survive themselves, most of them are not able to survive themselves, most of them are not able to survive themselves, most of them are able to survive themselves, most of them are able to survive themselves, most of them are able to survive themselves, most of them are able to survive themselves, most of them are able to survive themselves, most of them are able to survive themselves, most of them are able to survive themselves, most of them are able to survive themselves, most of them are able to survive themselves, and most of them are able to survive themselves."}, {"heading": "Conclusion", "text": "In this paper, we introduced a new multimodal LSTM and applied it to the task of loudspeaker identification, the key idea being to use the cross-modality weight distribution to capture the correlation of two or more time-coherent modalities. As demonstrated in our experiments, our proposed multimodal LSTM is robust against image degradation and deflection and has surpassed the most advanced techniques in loudspeaker identification. To our knowledge, this is the first attempt to model long-term dependencies on high-level multimodal features. We believe that our multimodal LSTM is also useful for other applications that are not limited to the task of loudspeaker identification."}], "references": [{"title": "Semi-supervised learning with constraints for person identification in multimedia data", "author": ["Tapaswi Bauml", "M. Stiefelhagen 2013] Bauml", "M. Tapaswi", "R. Stiefelhagen"], "venue": null, "citeRegEx": "Bauml et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bauml et al\\.", "year": 2013}, {"title": "Attentionbased models for speech recognition", "author": ["Chorowski"], "venue": null, "citeRegEx": "Chorowski,? \\Q2015\\E", "shortCiteRegEx": "Chorowski", "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Donahue"], "venue": null, "citeRegEx": "Donahue,? \\Q2015\\E", "shortCiteRegEx": "Donahue", "year": 2015}, {"title": "Speech recognition with deep recurrent neural networks. In ICASSP", "author": ["Mohamed Graves", "A. Hinton 2013] Graves", "A. Mohamed", "G.E. Hinton"], "venue": null, "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Gregor"], "venue": null, "citeRegEx": "Gregor,? \\Q2015\\E", "shortCiteRegEx": "Gregor", "year": 2015}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["Hannun"], "venue": "Arxiv", "citeRegEx": "Hannun,? \\Q2014\\E", "shortCiteRegEx": "Hannun", "year": 2014}, {"title": "Long short-term memory", "author": ["Hochreiter", "S. Schmidhuber 1997] Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Deep Multimodal Speaker Naming", "author": ["Hu"], "venue": "In ACMMM", "citeRegEx": "Hu,? \\Q2015\\E", "shortCiteRegEx": "Hu", "year": 2015}, {"title": "Labeled faces in the wild: Updates and new reporting procedures", "author": ["Huang", "G.B. Learned-Miller 2013] Huang", "E. Learned-Miller"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["Jean"], "venue": null, "citeRegEx": "Jean,? \\Q2015\\E", "shortCiteRegEx": "Jean", "year": 2015}, {"title": "Deep visual-semantic alignments for generating image description", "author": ["Karpathy", "A. Li 2015] Karpathy", "Li", "F.-F"], "venue": null, "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Sutskever Krizhevsky", "A. Hinton 2012] Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "A convolutional neural network cascade for face detection", "author": ["Li"], "venue": null, "citeRegEx": "Li,? \\Q2015\\E", "shortCiteRegEx": "Li", "year": 2015}, {"title": "Multimodal deep learning", "author": ["Ngiam"], "venue": null, "citeRegEx": "Ngiam,? \\Q2011\\E", "shortCiteRegEx": "Ngiam", "year": 2011}, {"title": "Learning and transferring mid-level image representations using convolutional neural networks", "author": ["Oquab"], "venue": null, "citeRegEx": "Oquab,? \\Q2014\\E", "shortCiteRegEx": "Oquab", "year": 2014}, {"title": "Cnn features off-the-shelf: An astounding baseline for recognition", "author": ["Razavian"], "venue": "In CVPR Workshop", "citeRegEx": "Razavian,? \\Q2014\\E", "shortCiteRegEx": "Razavian", "year": 2014}, {"title": "On vectorization of deep convolutional neural networks for vision", "author": ["Ren", "J. Xu 2015] Ren", "L. Xu"], "venue": null, "citeRegEx": "Ren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "Design, analysis and experimental evaluation of block based transformation in mfcc computation for speaker recognition", "author": ["Sahidullah", "M. Saha 2012] Sahidullah", "G. Saha"], "venue": "Speech Communication", "citeRegEx": "Sahidullah et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sahidullah et al\\.", "year": 2012}, {"title": "Facenet: A unified embedding for face recognition and clustering", "author": ["Kalenichenko Schroff", "F. Philbin 2015] Schroff", "D. Kalenichenko", "J. Philbin"], "venue": null, "citeRegEx": "Schroff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schroff et al\\.", "year": 2015}, {"title": "Neural responding machine for short-text conversation", "author": ["Lu Shang", "L. Li 2015] Shang", "Z. Lu", "H. Li"], "venue": null, "citeRegEx": "Shang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["Srivastava", "R. Salakhutdinov"], "venue": null, "citeRegEx": "Srivastava et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2012}, {"title": "Deep learning face representation by joint identification-verification", "author": ["Sun"], "venue": null, "citeRegEx": "Sun,? \\Q2014\\E", "shortCiteRegEx": "Sun", "year": 2014}, {"title": "Deepid3: Face recognition with very deep neural networks", "author": ["Sun"], "venue": null, "citeRegEx": "Sun,? \\Q2014\\E", "shortCiteRegEx": "Sun", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Vinyals Sutskever", "I. Le 2014] Sutskever", "O. Vinyals", "Q.V. Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Szegedy"], "venue": null, "citeRegEx": "Szegedy,? \\Q2015\\E", "shortCiteRegEx": "Szegedy", "year": 2015}, {"title": "Deepface: Closing the gap to humanlevel performance in face verification", "author": ["Taigman"], "venue": null, "citeRegEx": "Taigman,? \\Q2014\\E", "shortCiteRegEx": "Taigman", "year": 2014}, {"title": "knock! knock! who is it? probabilistic person identification in tv-series", "author": ["B\u00e4uml Tapaswi", "M. Stiefelhagen 2012] Tapaswi", "M. B\u00e4uml", "R. Stiefelhagen"], "venue": null, "citeRegEx": "Tapaswi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tapaswi et al\\.", "year": 2012}, {"title": "Generative image modeling using spatial lstms", "author": ["Theis", "L. Bethge 2015] Theis", "M. Bethge"], "venue": null, "citeRegEx": "Theis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu"], "venue": "In ICML", "citeRegEx": "Xu,? \\Q2015\\E", "shortCiteRegEx": "Xu", "year": 2015}, {"title": "From facial part responses to face detection: A deep learning approach", "author": ["Yang"], "venue": null, "citeRegEx": "Yang,? \\Q2015\\E", "shortCiteRegEx": "Yang", "year": 2015}, {"title": "Facial landmark detection by deep multitask learning", "author": ["Zhang"], "venue": null, "citeRegEx": "Zhang,? \\Q2015\\E", "shortCiteRegEx": "Zhang", "year": 2015}, {"title": "Learning deep representation for face alignment with auxiliary attributes", "author": ["Zhang"], "venue": null, "citeRegEx": "Zhang,? \\Q2015\\E", "shortCiteRegEx": "Zhang", "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "Speaker identification refers to the task of localizing the face of a person who has the same identity as the ongoing voice in a video. This task not only requires collective perception over both visual and auditory signals, the robustness to handle severe quality degradations and unconstrained content variations are also indispensable. In this paper, we describe a novel multimodal Long Short-Term Memory (LSTM) architecture which seamlessly unifies both visual and auditory modalities from the beginning of each sequence input. The key idea is to extend the conventional LSTM by not only sharing weights across time steps, but also sharing weights across modalities. We show that modeling the temporal dependency across face and voice can significantly improve the robustness to content quality degradations and variations. We also found that our multimodal LSTM is robustness to distractors, namely the non-speaking identities. We applied our multimodal LSTM to The Big Bang Theory dataset and showed that our system outperforms the state-of-the-art systems in speaker identification with lower false alarm rate and higher recogni-", "creator": "LaTeX with hyperref package"}}}