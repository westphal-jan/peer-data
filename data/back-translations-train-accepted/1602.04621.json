{"id": "1602.04621", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Feb-2016", "title": "Deep Exploration via Bootstrapped DQN", "abstract": "Efficient exploration in complex environments remains a major challenge for reinforcement learning. We propose bootstrapped DQN, a simple algorithm that explores in a computationally and statistically efficient manner through use of randomized value functions. Unlike dithering strategies such as epsilon-greedy exploration, bootstrapped DQN carries out temporally-extended (or deep) exploration; this can lead to exponentially faster learning. We demonstrate these benefits in complex stochastic MDPs and in the large-scale Arcade Learning Environment. Bootstrapped DQN substantially improves learning times and performance across most Atari games.", "histories": [["v1", "Mon, 15 Feb 2016 10:54:20 GMT  (5872kb,D)", "http://arxiv.org/abs/1602.04621v1", null], ["v2", "Fri, 1 Jul 2016 16:23:55 GMT  (7516kb,D)", "http://arxiv.org/abs/1602.04621v2", null], ["v3", "Mon, 4 Jul 2016 17:11:52 GMT  (7516kb,D)", "http://arxiv.org/abs/1602.04621v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.SY stat.ML", "authors": ["ian osband", "charles blundell", "alexander pritzel", "benjamin van roy"], "accepted": true, "id": "1602.04621"}, "pdf": {"name": "1602.04621.pdf", "metadata": {"source": "META", "title": "Deep Exploration via Bootstrapped DQN", "authors": ["Ian Osband", "Charles Blundell", "Alexander Pritzel", "Benjamin Van Roy"], "emails": ["IOSBAND@GOOGLE.COM", "CBLUNDELL@GOOGLE.COM", "APRITZEL@GOOGLE.COM", "BVR@STANFORD.EDU"], "sections": [{"heading": "1. Introduction", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "2. Uncertainty for neural networks", "text": "It is about the question of whether it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, and in which it is about a way, in which it is about a way, and in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way and in which it is about a way, in which it is about a way and in which it is about a way, in which it is about a way and in which it is about a way in which it is about a way, in which it is about a way and in which it is about a way and in which it is about a way, in which it is about which it is about a way and in which it is about which it is about a way and in which it is about which it is about a way and in which it is about which it is about a way and in which it is about which it is about a way and in which it is about which it is about which it is about a way and in which it is about which it is about which it is about which it is about a way and in which it is about which it is about which it is about a way and in which it is about which it is about which it is about a way and in which it is about which it is about which it is about which it is about a way, in which it is about which it is about which it is about which it is about which it is about a way and which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about"}, {"heading": "3. Bootstrapped DQN", "text": "For a policy \u03c0 we define the value of an action a in the state sQ\u03c0 (s, a): It, a, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p,"}, {"heading": "4. Related work", "text": "Observation that prolonged exploration is necessary for efficient amplification learning can also be highly efficient (2003). Indeed, for any prior distribution via MDPs, the optimal exploration strategy is available through dynamic programming in the Bayesian state of belief. However, the exact solution is insoluble even for very simple systems (Burnetas & Katehakis, 1997). RL algorithms provide understandable approaches to this problem. However, most common RL methods focus on generalization and planning, but addressing via dithering strategies. Many successful applications require only -greedy exploration (Tesauro, 1995; Mnih, 2015). However, such exploration strategies can be highly inefficient, as we show in Section 5.Policy Gradient methods that offer a different approach to RL and induce random exploration as they seek out via the space of chastic strategies. However, such exploration strategies may be highly inefficient, as we show in Section 5.Policy Gradient methods that offer a different approach to RL and induce random exploration through the space of chastic strategies."}, {"heading": "5. Deep Exploration", "text": "The story in RL is not so simple; guided exploration is not enough to ensure efficiency; exploration also needs to be deepened. Deep exploration means exploration directed over multiple time steps; it can also be called \"planning to learn\" or \"farsighted\" exploration. Unlike bandit problems, where actions that are directly worthwhile or immediately informative are balanced, RL settings require multi-time planning (Kakade, 2003). For exploitation, this means that an efficient agent needs to consider future rewards over multiple time steps, not just the short-sighted rewards. Likewise, efficient exploration may require actions that are neither immediately worthwhile nor immediately informative."}, {"heading": "5.1. Testing for deep exploration", "text": "We now present a series of didactic computer experiments aimed at highlighting the need for deep exploration in RL. Drugs are placed in an environment consisting of a long chain of states, right next to a small reward. All other states have zero reward, except at the other end of the chain, where they can find a state with a much higher reward. Experiments in this section are toy problems that are meant to be exponential rather than entirely realistic, but they clearly test whether a drug can efficiently balance the potential benefits of delayed information from deep exploration."}, {"heading": "5.2. Learning from pixels", "text": "The environments in this section can be succinctly described by a finite tabular MDP, similar to RiverSwim (Strehl & Littman, 2005), but we will require that our algorithm interact with the MDP only through raw pixel characteristics. For a chain of length N, we define state \u03c6 characteristics: {1,.., N} \u2192 {0, 1} N. We will consider two attribute mappings, \"one-hot\" inspoh (st): = (1 {x = st}) x = 1,.., N and \"thermometer\" throutherm (st): = (1 {x \u2264 st}) x = 1,.., N. We find that bootstrapped DQN is able to explore both characteristics efficiently. We will focus on thermometer encoding because it better captures generalization between states."}, {"heading": "5.3. Scaling deeper", "text": "We look at a family of deterministic chains of longitudeN > 3 as shown in Figure 4. Each episode of the interaction takes N + 9 steps, after which the agent reverts to the initial state s2. We choose this so that the optimal policy is to move correctly in each step and obtain a yield of 10 in each episode. However, each flat exploration strategy will take two episodes to learn the optimal policy (Osband et al., 2014). We say that the algorithm has successfully learned the optimal policy when it has successfully completed a hundred episodes and an optimal reward of 10. For each chain length, we have applied each learning algorithm for 2000 episodes across three seeds. We plot the mean time to learn in Figure 5, along with a conservative lower limit of 99 + 2N \u2212 11 over the expected time to learn for each flat exploration strategy. Only bootstrapped DQN shows a graceful scaling to long chains requiring a deep exploration."}, {"heading": "5.4. A difficult stochastic MDP", "text": "Figure 5 shows that bootstrapped DQN can implement effective (and deep) exploration approaches where similar deep RL architectures fail. However, since the underlying system is a small and limited MDP, there may be several other simpler strategies that would also solve this problem. We are now looking at a difficult variant of this chain system with significant stochastic noise in transitions, as illustrated in Figure 6. Action \"left\" moves the agent deterministically to the left, but action \"right\" only succeeds 50% of the time and otherwise moves to the left. The agent interacts with the MDP in episodes of length 15 and starts each episode at s1. Once again, the optimal policy is to move the head to the right. Bootstrapped DQN is unique among scalable approaches for efficient exploration with deep RL in stochastic domains. For benchmark performance, we implement three algorithms that, unlike bootstrapped DDP, maintain the true DDP representation."}, {"heading": "6. Arcade Learning Environment", "text": "We are now evaluating our algorithm based on 49 Atari games on the Arcade Learning Environment (Bellemare et al., 2012). Importantly, unlike the experiments in Section 5, these domains are not specifically designed to showcase our algorithm. In fact, many Atari games are structured in such a way that small rewards are always part of an optimal strategy, which may be critical to the strong performance observed by dithering strategies.2 We find that exploring bootstrapped DQN in this environment generates significant gains over -greed. Bootstrapped DQN achieves peak performance roughly on the order of DQN. However, our improved exploration means that we achieve human performance on average 30% faster in all games, resulting in significantly improved cumulative rewards from learning.We are following the setup of (Van Hasselt et al., 2015) for our network architecture and benchmarking our performance compared to their algorithm. Our network structure is identical to the evolutionary structure of QN (2015)."}, {"heading": "6.1. Implementing bootstrapped DQN at scale", "text": "This year we have reached the point where we will be able to put ourselves at the top, \"he said.\" We have never made as many mistakes as this year, \"he said.\" We have never made as many mistakes as this year. \"He added:\" We have never made as many mistakes as this year. \"He added:\" We have never made as many mistakes as this year. \"He added:\" We have never made as many mistakes as this year, but we have never made as many mistakes as this year. \""}, {"heading": "6.2. Efficient exploration in Atari", "text": "We note that bootstrapped DQN drives efficient exploration in multiple Atari games, which means that bootstrapped DQN generally outperforms DQN with -greedy exploration for the same game experience. Figure 10 shows this effect across a diverse selection of games. To summarize this improvement in learning time, we look at the number of frames required to achieve human performance. We say that Algorithm A has human acceleration of x compared to Algorithm B, when it reaches human performance in4p = 0.5, it is double or nothing bootstrap (Owen et al., 2012). 5Our K = 10, p = 1 implementation ran with less than a 20% increase in wall time over DQN at the same number of frames 1 / x as DQN. We note that bootstrapped DQN achieves human performance significantly faster in most games where human performance is achieved."}, {"heading": "6.3. Understanding bootstrapped DQN", "text": "In this game, the agent controls a rescue mission that travels from room to room, blowing up obstacles and rescuing hostages. However, in Figure 12, we show a screenshot of nine different heads of bootstrapped DQN running from their initial state for 3,200 steps. Although each head has learned a good representation of the value function that leads to a policy that they find quite different, this screenshot shows the nine different heads in five different margins. This shows that even after more than 100 training frameworks, with the exchange of data p = 1 we still maintain significant diversity. In contrast, -greedy strategies for small values and totally ineffective for larger values are hard to distinguish. Our heads explore a diverse range of strategies, but nevertheless we manage it for each individual."}, {"heading": "6.4. Overall performance", "text": "We have seen that bootstrapped DQN can learn much faster than DQN. In Figure 14, we see that the best performance achieved by both algorithms is similar in most games. However, the benefits of efficient exploration mean that bootstrapped DQN significantly outperforms DQN in terms of cumulative learning rewards. We present these results in Figure 15. In Appendix C, we present full results for our algorithm in all 49 games. Bootstrapped DQN significantly outperforms several current heuristic approaches to improved exploration of Atari (Stadie et al., 2015), especially in terms of cumulative rewards."}, {"heading": "7. Closing remarks", "text": "In this paper, we present bootstrapped DQN as an algorithm for efficient amplification learning in complex environments. We show that the bootstrap can provide useful uncertainty estimates for deep neural networks. Bootstrapped DQN can use these uncertainty estimates for deep exploration even in difficult stochastic systems; it also provides several state-of-the-art results in Atari 2600.Bootstrapped DQN is computationally tractable and naturally scalable to massive parallel systems (Nair et al., 2015). We believe that randomized value functions are a promising alternative to dithering for exploration beyond our specific implementation. Bootstrapped DQN combines practically efficient generation with the exploration of complex nonlinear value functions."}, {"heading": "A. Uncertainty for neural networks", "text": "In this case, it is that we will be able to assert ourselves, that we will be able to, that we will be able to abide by the rules that we have set ourselves, \"he said in an interview with\" Welt am Sonntag \":\" We must abide by the rules that we have set ourselves. \""}, {"heading": "B. Experiments for deep exploration", "text": "This year, it has reached the stage where it will be able to put itself at the top of the world."}, {"heading": "C. Experiments for Atari", "text": "In fact, it is so that it is a way in which people in the United States and in other parts of the world move in a way that they do in the United States and in other parts of the world. \"(S. S. S. S. S. S. S. S.)\" We have it not easy. \"(S. S. S. S. S. S. S.)\" We have it very difficult. \"(S. S. S. S. S. S. S. S. S.)\" We have it very difficult. \"(S. S. S. S. S.)\" (S. S. S. S.) \"(S. S. S.)\" (S. S.) \"(S. S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.) (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.). (S.) (S.). (S.). (S.). (S.) (S.). (S.). (S.) (S.). (S.). (S.) (S.). (S.) (S.). (S.) (S.). (S.). (S.). (S.) (S.). (S.) (S.). (S.). (S.). (S.). (S.). (S.) (S."}], "references": [{"title": "Bayesian optimal control of smoothly parameterized systems", "author": ["Abbasi-Yadkori", "Yasin", "Szepesv\u00e1ri", "Csaba"], "venue": "In Proceedings of the Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Abbasi.Yadkori et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Abbasi.Yadkori et al\\.", "year": 2015}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["Bellemare", "Marc G", "Naddaf", "Yavar", "Veness", "Joel", "Bowling", "Michael"], "venue": "arXiv preprint arXiv:1207.4708,", "citeRegEx": "Bellemare et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2012}, {"title": "Some asymptotic theory for the bootstrap", "author": ["Bickel", "Peter J", "Freedman", "David A"], "venue": "The Annals of Statistics,", "citeRegEx": "Bickel et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Bickel et al\\.", "year": 1981}, {"title": "Weight uncertainty in neural networks", "author": ["Blundell", "Charles", "Cornebise", "Julien", "Kavukcuoglu", "Koray", "Wierstra", "Daan"], "venue": null, "citeRegEx": "Blundell et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Blundell et al\\.", "year": 2015}, {"title": "R-max - a general polynomial time algorithm for near-optimal reinforcement learning", "author": ["Brafman", "Ronen I", "Tennenholtz", "Moshe"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Brafman et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Brafman et al\\.", "year": 2002}, {"title": "Optimal adaptive policies for markov decision processes", "author": ["Burnetas", "Apostolos N", "Katehakis", "Michael N"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Burnetas et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Burnetas et al\\.", "year": 1997}, {"title": "Sample complexity of episodic fixed-horizon reinforcement learning", "author": ["Dann", "Christoph", "Brunskill", "Emma"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dann et al\\.", "year": 2015}, {"title": "The jackknife, the bootstrap and other resampling plans, volume 38", "author": ["Efron", "Bradley"], "venue": null, "citeRegEx": "Efron and Bradley.,? \\Q1982\\E", "shortCiteRegEx": "Efron and Bradley.", "year": 1982}, {"title": "An introduction to the bootstrap", "author": ["Efron", "Bradley", "Tibshirani", "Robert J"], "venue": "CRC press,", "citeRegEx": "Efron et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Efron et al\\.", "year": 1994}, {"title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning", "author": ["Gal", "Yarin", "Ghahramani", "Zoubin"], "venue": "arXiv preprint arXiv:1506.02142,", "citeRegEx": "Gal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gal et al\\.", "year": 2015}, {"title": "Thompson sampling for learning parameterized markov decision processes", "author": ["Gopalan", "Aditya", "Mannor", "Shie"], "venue": "In Proceedings of the 28th Conference on Learning Theory (COLT),", "citeRegEx": "Gopalan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gopalan et al\\.", "year": 2015}, {"title": "Practical variational inference for neural networks", "author": ["Graves", "Alex"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Graves and Alex.,? \\Q2011\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2011}, {"title": "Efficient bayesadaptive reinforcement learning using sample-based search", "author": ["Guez", "Arthur", "Silver", "David", "Dayan", "Peter"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Guez et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Guez et al\\.", "year": 2012}, {"title": "Bayes-adaptive simulation-based search with value function approximation", "author": ["Guez", "Arthur", "Heess", "Nicolas", "Silver", "David", "Dayan", "Peter"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Guez et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Guez et al\\.", "year": 2014}, {"title": "Probabilistic backpropagation for scalable learning of bayesian neural networks", "author": ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", "Adams", "Ryan P"], "venue": null, "citeRegEx": "Hern\u00e1ndez.Lobato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hern\u00e1ndez.Lobato et al\\.", "year": 2015}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["Jaksch", "Thomas", "Ortner", "Ronald", "Auer", "Peter"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Jaksch et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jaksch et al\\.", "year": 2010}, {"title": "On the Sample Complexity of Reinforcement Learning", "author": ["Kakade", "Sham"], "venue": "PhD thesis,", "citeRegEx": "Kakade and Sham.,? \\Q2003\\E", "shortCiteRegEx": "Kakade and Sham.", "year": 2003}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["Kearns", "Michael J", "Singh", "Satinder P"], "venue": "Machine Learning,", "citeRegEx": "Kearns et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 2002}, {"title": "Variational dropout and the local reparameterization trick", "author": ["Kingma", "Diederik P", "Salimans", "Tim", "Welling", "Max"], "venue": "arXiv preprint arXiv:1506.02557,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "A scalable bootstrap for massive data", "author": ["Kleiner", "Ariel", "Talwalkar", "Ameet", "Sarkar", "Purnamrita", "Jordan", "Michael I"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Kleiner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kleiner et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["Lai", "Tze Leung", "Robbins", "Herbert"], "venue": "Advances in applied mathematics,", "citeRegEx": "Lai et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Lai et al\\.", "year": 1985}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Levine", "Sergey", "Finn", "Chelsea", "Darrell", "Trevor", "Abbeel", "Pieter"], "venue": "arXiv preprint arXiv:1504.00702,", "citeRegEx": "Levine et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2015}, {"title": "Human-level control through deep reinforcement learning", "author": ["Mnih", "Volodymyr"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih and Volodymyr,? \\Q2015\\E", "shortCiteRegEx": "Mnih and Volodymyr", "year": 2015}, {"title": "From bandits to monte-carlo tree search: The optimistic principle applied to optimization and planning", "author": ["Munos", "R\u00e9mi"], "venue": null, "citeRegEx": "Munos and R\u00e9mi.,? \\Q2014\\E", "shortCiteRegEx": "Munos and R\u00e9mi.", "year": 2014}, {"title": "Massively parallel methods for deep reinforcement learning", "author": ["Nair", "Arun", "Srinivasan", "Praveen", "Blackwell", "Sam", "Alcicek", "Cagdas", "Fearon", "Rory", "De Maria", "Alessandro", "Panneershelvam", "Vedavyas", "Suleyman", "Mustafa", "Beattie", "Charles", "Petersen", "Stig"], "venue": "arXiv preprint arXiv:1507.04296,", "citeRegEx": "Nair et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2015}, {"title": "Model-based reinforcement learning and the eluder dimension", "author": ["Osband", "Ian", "Van Roy", "Benjamin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Osband et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2014}, {"title": "Near-optimal reinforcement learning in factored MDPs", "author": ["Osband", "Ian", "Van Roy", "Benjamin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Osband et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2014}, {"title": "Bootstrapped thompson sampling and deep exploration", "author": ["Osband", "Ian", "Van Roy", "Benjamin"], "venue": "arXiv preprint arXiv:1507.00300,", "citeRegEx": "Osband et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2015}, {"title": "More) efficient reinforcement learning via posterior sampling", "author": ["Osband", "Ian", "Russo", "Daniel", "Van Roy", "Benjamin"], "venue": "In NIPS,", "citeRegEx": "Osband et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2013}, {"title": "Generalization and exploration via randomized value functions", "author": ["Osband", "Ian", "Van Roy", "Benjamin", "Wen", "Zheng"], "venue": "arXiv preprint arXiv:1402.0635,", "citeRegEx": "Osband et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2014}, {"title": "Bootstrapping data arrays of arbitrary order", "author": ["Owen", "Art B", "Eckles", "Dean"], "venue": "The Annals of Applied Statistics,", "citeRegEx": "Owen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Owen et al\\.", "year": 2012}, {"title": "The bayesian bootstrap", "author": ["Rubin", "Donald B"], "venue": "The annals of statistics,", "citeRegEx": "Rubin and B,? \\Q1981\\E", "shortCiteRegEx": "Rubin and B", "year": 1981}, {"title": "Learning to optimize via posterior sampling", "author": ["Russo", "Daniel", "Van Roy", "Benjamin"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Russo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Russo et al\\.", "year": 2014}, {"title": "Prioritized experience replay", "author": ["Schaul", "Tom", "Quan", "John", "Antonoglou", "Ioannis", "Silver", "David"], "venue": "arXiv preprint arXiv:1511.05952,", "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Incentivizing exploration in reinforcement learning with deep predictive models", "author": ["Stadie", "Bradly C", "Levine", "Sergey", "Abbeel", "Pieter"], "venue": "arXiv preprint arXiv:1507.00814,", "citeRegEx": "Stadie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Stadie et al\\.", "year": 2015}, {"title": "A theoretical analysis of model-based interval estimation", "author": ["Strehl", "Alexander L", "Littman", "Michael L"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "Strehl et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2005}, {"title": "A bayesian framework for reinforcement learning", "author": ["Strens", "Malcolm J. A"], "venue": "In ICML, pp", "citeRegEx": "Strens and A.,? \\Q2000\\E", "shortCiteRegEx": "Strens and A.", "year": 2000}, {"title": "Reinforcement Learning: An Introduction", "author": ["Sutton", "Richard", "Barto", "Andrew"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Distributed bayesian learning with stochastic natural-gradient expectation propagation and the posterior server", "author": ["Teh", "Yee Whye", "Hasenclever", "Leonard", "Lienart", "Thibaut", "Vollmer", "Sebastian", "Webb", "Stefan", "Lakshminarayanan", "Balaji", "Blundell", "Charles"], "venue": "arXiv preprint arXiv:1512.09327,", "citeRegEx": "Teh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2015}, {"title": "Temporal difference learning and td-gammon", "author": ["Tesauro", "Gerald"], "venue": "Communications of the ACM,", "citeRegEx": "Tesauro and Gerald.,? \\Q1995\\E", "shortCiteRegEx": "Tesauro and Gerald.", "year": 1995}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W.R. Thompson"], "venue": null, "citeRegEx": "Thompson,? \\Q1933\\E", "shortCiteRegEx": "Thompson", "year": 1933}, {"title": "Deep reinforcement learning with double q-learning", "author": ["Van Hasselt", "Hado", "Guez", "Arthur", "Silver", "David"], "venue": "arXiv preprint arXiv:1509.06461,", "citeRegEx": "Hasselt et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2015}, {"title": "Bayesian sparse sampling for on-line reward optimization", "author": ["Wang", "Tao", "Lizotte", "Daniel", "Bowling", "Michael", "Schuurmans", "Dale"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "Wang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2005}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Wang", "Ziyu", "de Freitas", "Nando", "Lanctot", "Marc"], "venue": "arXiv preprint arXiv:1511.06581,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Efficient exploration and value function generalization in deterministic systems", "author": ["Wen", "Zheng", "Van Roy", "Benjamin"], "venue": "In NIPS, pp", "citeRegEx": "Wen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2013}, {"title": "Dropout converges with high certainty to the mean value. On the right hand side of the plot we generate noisy data with wildly different values. Training a neural network using MSE criterion means that the network", "author": ["Ghahramani"], "venue": null, "citeRegEx": "Ghahramani and 2015..,? \\Q2015\\E", "shortCiteRegEx": "Ghahramani and 2015..", "year": 2015}], "referenceMentions": [{"referenceID": 13, "context": ", Jaksch et al. (2010); Guez et al.", "startOffset": 2, "endOffset": 23}, {"referenceID": 12, "context": "(2010); Guez et al. (2012)).", "startOffset": 8, "endOffset": 27}, {"referenceID": 26, "context": "Previous work demonstrated that randomized value functions can efficiently explore in tandem with linearly-parameterized value function generalization (Osband et al., 2014).", "startOffset": 151, "endOffset": 172}, {"referenceID": 20, "context": "Deep neural networks represent the state of the art in many supervised learning domains (Krizhevsky et al., 2012).", "startOffset": 88, "endOffset": 113}, {"referenceID": 3, "context": "These include variational Bayes (Graves, 2011; Blundell et al., 2015), assumed density filtering (Hern\u00e1ndez-Lobato & Adams, 2015), dropout-based variational inference (Gal & Ghahramani, 2015; Kingma et al.", "startOffset": 32, "endOffset": 69}, {"referenceID": 18, "context": ", 2015), assumed density filtering (Hern\u00e1ndez-Lobato & Adams, 2015), dropout-based variational inference (Gal & Ghahramani, 2015; Kingma et al., 2015), and stochastic gradient Langevin dynamics (Teh et al.", "startOffset": 105, "endOffset": 150}, {"referenceID": 40, "context": ", 2015), and stochastic gradient Langevin dynamics (Teh et al., 2015).", "startOffset": 51, "endOffset": 69}, {"referenceID": 19, "context": "The approach can even scale to massive data with sub-linear computational cost (Kleiner et al., 2014).", "startOffset": 79, "endOffset": 101}, {"referenceID": 31, "context": "Each head is trained only on its bootstrapped sub-sample of the data, which can be generated online (Owen et al., 2012).", "startOffset": 100, "endOffset": 119}, {"referenceID": 29, "context": "This is a natural extension of the Thompson sampling heuristic to RL that allows for temporally extended (or deep) exploration (Strens, 2000; Osband et al., 2013).", "startOffset": 127, "endOffset": 162}, {"referenceID": 22, "context": ", Levine et al. (2015)), the form of exploration induced by such algorithms can also be highly inefficient (Kakade, 2003).", "startOffset": 2, "endOffset": 23}, {"referenceID": 44, "context": "Other approaches aim to approximate Bayes-optimal exploration though tree-based search (Wang et al., 2005; Guez et al., 2014).", "startOffset": 87, "endOffset": 125}, {"referenceID": 13, "context": "Other approaches aim to approximate Bayes-optimal exploration though tree-based search (Wang et al., 2005; Guez et al., 2014).", "startOffset": 87, "endOffset": 125}, {"referenceID": 15, "context": "proposed for finite-armed bandits (Lai & Robbins, 1985), but the principle has been extended successfully across bandits with generalization (Rusmevichientong & Tsitsiklis, 2010) and tabular RL (Jaksch et al., 2010).", "startOffset": 194, "endOffset": 215}, {"referenceID": 36, "context": "The work of (Stadie et al., 2015) aims to add an effective bonus through a variation of DQN.", "startOffset": 12, "endOffset": 33}, {"referenceID": 42, "context": "Perhaps the oldest heuristic for balancing exploration with exploitation is given by Thompson sampling (Thompson, 1933).", "startOffset": 103, "endOffset": 119}, {"referenceID": 12, "context": "The agent must also commit to this sample for several time steps in order to achieve deep exploration (Strens, 2000; Guez et al., 2012).", "startOffset": 102, "endOffset": 135}, {"referenceID": 29, "context": "The algorithm PSRL does exactly this, with state of the art guarantees (Osband et al., 2013; Osband & Van Roy, 2014b;a; Abbasi-Yadkori & Szepesv\u00e1ri, 2015; Gopalan & Mannor, 2015).", "startOffset": 71, "endOffset": 178}, {"referenceID": 26, "context": "Surprisingly, this algorithm recovers state of the art guarantees in the setting with tabular basis functions, but its performance is crucially dependent upon a suitable linear representation of the value function (Osband et al., 2014).", "startOffset": 214, "endOffset": 235}, {"referenceID": 26, "context": "However, any shallow exploration strategy will take \u03a9(2 ) episodes to learn the optimal policy (Osband et al., 2014).", "startOffset": 95, "endOffset": 116}, {"referenceID": 15, "context": "These algorithms are based on three state of the art approaches to exploration via dithering ( -greedy), optimism (UCRL2; Jaksch et al., 2010) and posterior sampling (PSRL; Osband et al.", "startOffset": 114, "endOffset": 142}, {"referenceID": 29, "context": ", 2010) and posterior sampling (PSRL; Osband et al., 2013).", "startOffset": 31, "endOffset": 58}, {"referenceID": 15, "context": "Although Figure 7 seems to suggest UCRL2 incurs linear regret, actually it follows its bounds \u00d5(S \u221a AT ) (Jaksch et al., 2010) where S is the number of states and A is the number of actions.", "startOffset": 105, "endOffset": 126}, {"referenceID": 1, "context": "We now evaluate our algorithm across 49 Atari games on the Arcade Learning Environment (Bellemare et al., 2012).", "startOffset": 87, "endOffset": 111}, {"referenceID": 45, "context": "to DDQN (Wang et al., 2015; Schaul et al., 2015).", "startOffset": 8, "endOffset": 48}, {"referenceID": 34, "context": "to DDQN (Wang et al., 2015; Schaul et al., 2015).", "startOffset": 8, "endOffset": 48}, {"referenceID": 31, "context": "5 is double-or-nothing bootstrap (Owen et al., 2012).", "startOffset": 33, "endOffset": 52}, {"referenceID": 36, "context": "Bootstrapped DQN significantly outperforms several recent heuristic approaches for improved exploration on Atari (Stadie et al., 2015), particularly in terms of cumulative rewards.", "startOffset": 113, "endOffset": 134}, {"referenceID": 25, "context": "Bootstrapped DQN is computationally tractable and also naturally scalable to massive parallel systems as per (Nair et al., 2015).", "startOffset": 109, "endOffset": 128}], "year": 2016, "abstractText": "Efficient exploration in complex environments remains a major challenge for reinforcement learning. We propose bootstrapped DQN, a simple algorithm that explores in a computationally and statistically efficient manner through use of randomized value functions. Unlike dithering strategies such as -greedy exploration, bootstrapped DQN carries out temporally-extended (or deep) exploration; this can lead to exponentially faster learning. We demonstrate these benefits in complex stochastic MDPs and in the large-scale Arcade Learning Environment. Bootstrapped DQN substantially improves learning times and performance across most Atari games.", "creator": "LaTeX with hyperref package"}}}