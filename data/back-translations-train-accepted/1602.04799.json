{"id": "1602.04799", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Feb-2016", "title": "Quantum Perceptron Models", "abstract": "We demonstrate how quantum computation can provide non-trivial improvements in the computational and statistical complexity of the perceptron model. We develop two quantum algorithms for perceptron learning. The first algorithm exploits quantum information processing to determine a separating hyperplane using a number of steps sublinear in the number of data points $N$, namely $O(\\sqrt{N})$. The second algorithm illustrates how the classical mistake bound of $O(\\frac{1}{\\gamma^2})$ can be further improved to $O(\\frac{1}{\\sqrt{\\gamma}})$ through quantum means, where $\\gamma$ denotes the margin. Such improvements are achieved through the application of quantum amplitude amplification to the version space interpretation of the perceptron model.", "histories": [["v1", "Mon, 15 Feb 2016 20:45:35 GMT  (173kb,D)", "http://arxiv.org/abs/1602.04799v1", null]], "reviews": [], "SUBJECTS": "quant-ph cs.LG stat.ML", "authors": ["ashish kapoor", "nathan wiebe", "krysta marie svore"], "accepted": true, "id": "1602.04799"}, "pdf": {"name": "1602.04799.pdf", "metadata": {"source": "CRF", "title": "Quantum Perceptron Models", "authors": ["Nathan Wiebe", "Ashish Kapoor", "Krysta M. Svore"], "emails": [], "sections": [{"heading": null, "text": "In fact, we are in a position to go in search of a solution that enables us, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in the position we are in."}, {"heading": "II. BACKGROUND", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Perceptrons and Version Space", "text": "Considering a set of N separable training examples {\u03c61,.., \u03c6N} \"RD with appropriate labels {y1,.., yN}, {1, \u2212 1}, the goal of perceptron learning is to restore a hyperplane that perfectly classifies the training scenario [18]. Formally, however, we want yi \u00b7 wT\u03c6i > 0 for all i.\" There are various simple online algorithms that start with a random hyperplane initialization and perform updates as they come across more and more data [8, 18, 19] but the rule we consider for online perceptron training, after misclassifying a vector, y \"w.,\" is a notable feature of the perctron model that upper limits exist on the number of updates that must be made during this training process."}, {"heading": "B. Grover\u2019s Search", "text": "Both approaches presented in this work, whose corresponding speed-ups can in fact only be understood with the help of arguments, are referred to by a quantum subretin called Grover's. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < &"}, {"heading": "III. ONLINE QUANTUM PERCEPTRON", "text": "To do this, we must first define the quantum model that we want to use as our quantum analogy of perception training. While there are many ways to define such a model, the following approach is perhaps the most direct. Although the traditional features of the quantum model are the perception algorithms online [16], this means that the training examples are made available at a time when we slightly deviate from this model by instead requiring that the algorithms be fed with training examples that are uniformable in their effect. This is a slightly weaker model, as it allows for some training examples to be drawn multiple times. However, the ability to draw quantum states that are contained in a uniform overlay across all vectors in the training set is quantum computer-assisted."}, {"heading": "IV. QUANTUM VERSION SPACE PERCEPTRON", "text": "The strategy for our quantum version is to pose the problem of determining a separate hyperplane as a search. Specifically, the idea is that we first have a hyperplane among the samples that would lie in ambient space and completely separate the data from each other. As already discussed, Grover's algorithm can provide a square speedup over the classical search, so the efficiency of K. Theorem 2's algorithm is determined how to determine this number of hyperplanes to sample. Theorem 2. Given a training set consisting of d-dimensional units vectors 1,...,, yN, which are separated by a margin from a D-dimensional vector."}, {"heading": "V. CONCLUSION", "text": "We have developed two different ways of learning quantum perceptrons that allow each other different accelerations in relation to each other: firstly, a quadratic acceleration in relation to the size of the training data. Secondly, we show that this algorithm is asymptotically optimal in that, if super-quadratic acceleration were possible, it would violate the known lower limits of quantum search. Secondly, a quadratic reduction in the scaling of training time (measured by the number of interactions with the training data) with the distance between the two classes. The latter result is particularly interesting because it represents a quadratic acceleration in relation to the typical limits of perceptron training, which are normally experienced at the rate of literacy. Perhaps the most significant feature of our work is that it shows that quantum computing can provide demonstrable accelerations for perceptron training, which is a basic machine learning method. While our work does not offer possible possibilities to consider quantum acceleration only on the basis of classical perceptron models."}, {"heading": "Appendix A: Proofs", "text": "Here we provide evidence for several of our results, which are given in the main part. < p = p = p = p = p = p = p = p = p = p < p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p p = p p = p"}], "references": [{"title": "Machine learning in a quantum world", "author": ["A\u0131\u0308meur", "Esma", "Brassard", "Gilles", "Gambs", "S\u00e9bastien"], "venue": "In Advances in artificial intelligence,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Quantum boltzmann machine", "author": ["Amin", "Mohammad H", "Andriyash", "Evgeny", "Rolfe", "Jason", "Kulchytskyy", "Bohdan", "Melko", "Roger"], "venue": "arXiv preprint arXiv:1601.02036,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "On the einstein podolsky rosen paradox", "author": ["Bell", "John S"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1964}, {"title": "Tight bounds on quantum searching", "author": ["Boyer", "Michel", "Brassard", "Gilles", "H\u00f8yer", "Peter", "Tapp", "Alain"], "venue": "arXiv preprint quant-ph/9605034,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1996}, {"title": "Quantum amplitude amplification and estimation", "author": ["Brassard", "Gilles", "Hoyer", "Peter", "Mosca", "Michele", "Tapp", "Alain"], "venue": "Contemporary Mathematics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Large margin classification using the perceptron algorithm", "author": ["Freund", "Yoav", "Schapire", "Robert E"], "venue": "Machine learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "Adiabatic quantum algorithm for search engine ranking", "author": ["Garnerone", "Silvano", "Zanardi", "Paolo", "Lidar", "Daniel A"], "venue": "Physical review letters,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "A new approximate maximal margin classification algorithm", "author": ["Gentile", "Claudio"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "A fast quantum mechanical algorithm for database search", "author": ["Grover", "Lov K"], "venue": "In Proceedings of the twenty-eighth annual ACM symposium on Theory of computing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1996}, {"title": "Bayes point machines: Estimating the bayes point in kernel space", "author": ["Herbrich", "Ralf", "Graepel", "Thore", "Campbell", "Colin"], "venue": "In IJCAI Workshop SVMs,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}, {"title": "The perceptron algorithm with uneven margins", "author": ["Li", "Yaoyong", "Zaragoza", "Hugo", "Herbrich", "Ralf", "Shawe-Taylor", "John", "Kandola", "Jaz"], "venue": "In ICML,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Quantum algorithms for supervised and unsupervised machine learning", "author": ["Lloyd", "Seth", "Mohseni", "Masoud", "Rebentrost", "Patrick"], "venue": "arXiv preprint arXiv:1307.0411,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Quantum principal component analysis", "author": ["Lloyd", "Seth", "Mohseni", "Masoud", "Rebentrost", "Patrick"], "venue": "Nature Physics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "A family of algorithms for approximate Bayesian inference", "author": ["Minka", "Thomas P"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2001}, {"title": "Quantum computation and quantum information", "author": ["Nielsen", "Michael A", "Chuang", "Isaac L"], "venue": "Cambridge university press,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "On convergence proofs for perceptrons", "author": ["Novikoff", "Albert BJ"], "venue": "Technical report, DTIC Document,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1963}, {"title": "Quantum support vector machine for big data classification", "author": ["Rebentrost", "Patrick", "Mohseni", "Masoud", "Lloyd", "Seth"], "venue": "Physical review letters,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "The perceptron: a probabilistic model for information storage and organization in the brain", "author": ["Rosenblatt", "Frank"], "venue": "Psychological review,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1958}, {"title": "A new perspective on an old perceptron algorithm", "author": ["Shalev-Shwartz", "Shai", "Singer", "Yoram"], "venue": "In Learning Theory,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Least squares support vector machine classifiers", "author": ["Suykens", "Johan AK", "Vandewalle", "Joos"], "venue": "Neural processing letters,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}, {"title": "Can small quantum systems learn", "author": ["Wiebe", "Nathan", "Granade", "Christopher"], "venue": "arXiv preprint arXiv:1512.03145,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Quantum deep learning", "author": ["Wiebe", "Nathan", "Kapoor", "Ashish", "Svore", "Krysta M"], "venue": "arXiv preprint arXiv:1412.3489,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Quantum nearest-neighbor algorithms for machine learning", "author": ["Wiebe", "Nathan", "Kapoor", "Ashish", "Svore", "Krysta"], "venue": "Quantum Information and Computation,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "The growing importance of machine learning has in recent years led to a host of studies that investigate the promise of quantum computers for machine learning [1, 2, 12, 13, 17, 21\u201323].", "startOffset": 159, "endOffset": 184}, {"referenceID": 1, "context": "The growing importance of machine learning has in recent years led to a host of studies that investigate the promise of quantum computers for machine learning [1, 2, 12, 13, 17, 21\u201323].", "startOffset": 159, "endOffset": 184}, {"referenceID": 11, "context": "The growing importance of machine learning has in recent years led to a host of studies that investigate the promise of quantum computers for machine learning [1, 2, 12, 13, 17, 21\u201323].", "startOffset": 159, "endOffset": 184}, {"referenceID": 12, "context": "The growing importance of machine learning has in recent years led to a host of studies that investigate the promise of quantum computers for machine learning [1, 2, 12, 13, 17, 21\u201323].", "startOffset": 159, "endOffset": 184}, {"referenceID": 16, "context": "The growing importance of machine learning has in recent years led to a host of studies that investigate the promise of quantum computers for machine learning [1, 2, 12, 13, 17, 21\u201323].", "startOffset": 159, "endOffset": 184}, {"referenceID": 20, "context": "The growing importance of machine learning has in recent years led to a host of studies that investigate the promise of quantum computers for machine learning [1, 2, 12, 13, 17, 21\u201323].", "startOffset": 159, "endOffset": 184}, {"referenceID": 21, "context": "The growing importance of machine learning has in recent years led to a host of studies that investigate the promise of quantum computers for machine learning [1, 2, 12, 13, 17, 21\u201323].", "startOffset": 159, "endOffset": 184}, {"referenceID": 22, "context": "The growing importance of machine learning has in recent years led to a host of studies that investigate the promise of quantum computers for machine learning [1, 2, 12, 13, 17, 21\u201323].", "startOffset": 159, "endOffset": 184}, {"referenceID": 1, "context": "The true potential of quantum algorithms may therefore remain underexploited since quantum algorithms have been constrainted to follow the same methodology behind traditional machine learning methods [2, 7, 22].", "startOffset": 200, "endOffset": 210}, {"referenceID": 6, "context": "The true potential of quantum algorithms may therefore remain underexploited since quantum algorithms have been constrainted to follow the same methodology behind traditional machine learning methods [2, 7, 22].", "startOffset": 200, "endOffset": 210}, {"referenceID": 21, "context": "The true potential of quantum algorithms may therefore remain underexploited since quantum algorithms have been constrainted to follow the same methodology behind traditional machine learning methods [2, 7, 22].", "startOffset": 200, "endOffset": 210}, {"referenceID": 17, "context": "We illustrate our approach by focusing on perceptron training [18].", "startOffset": 62, "endOffset": 66}, {"referenceID": 19, "context": "The perceptron is a fundamental building block for various machine learning models including neural networks and support vector machines [20].", "startOffset": 137, "endOffset": 141}, {"referenceID": 17, "context": ", yN}, yi \u2208 {+1,\u22121}, the goal of perceptron learning is to recover a hyperplane w that perfectly classifies the training set [18].", "startOffset": 125, "endOffset": 129}, {"referenceID": 7, "context": "There are various simple online algorithms that start with a random initialization of the hyperplane and make updates as they encounter more and more data [8, 11, 18, 19]; however, the rule that we consider for online perceptron training is, upon misclassifying a vector (\u03c6, y), w \u2190 w + y\u03c6.", "startOffset": 155, "endOffset": 170}, {"referenceID": 10, "context": "There are various simple online algorithms that start with a random initialization of the hyperplane and make updates as they encounter more and more data [8, 11, 18, 19]; however, the rule that we consider for online perceptron training is, upon misclassifying a vector (\u03c6, y), w \u2190 w + y\u03c6.", "startOffset": 155, "endOffset": 170}, {"referenceID": 17, "context": "There are various simple online algorithms that start with a random initialization of the hyperplane and make updates as they encounter more and more data [8, 11, 18, 19]; however, the rule that we consider for online perceptron training is, upon misclassifying a vector (\u03c6, y), w \u2190 w + y\u03c6.", "startOffset": 155, "endOffset": 170}, {"referenceID": 18, "context": "There are various simple online algorithms that start with a random initialization of the hyperplane and make updates as they encounter more and more data [8, 11, 18, 19]; however, the rule that we consider for online perceptron training is, upon misclassifying a vector (\u03c6, y), w \u2190 w + y\u03c6.", "startOffset": 155, "endOffset": 170}, {"referenceID": 15, "context": "In particular, if the training data is composed of unit vectors, \u03c6i \u2208 R, that are separated by a margin of \u03b3 then there are perceptron training algorithms that make at most O( 1 \u03b32 ) mistakes [16], ar X iv :1 60 2.", "startOffset": 192, "endOffset": 196}, {"referenceID": 13, "context": "This figure is from [14].", "startOffset": 20, "endOffset": 24}, {"referenceID": 5, "context": "Similar bounds also exist when the data is not separated [6] and also for other generalizations of perceptron training [8, 11, 19].", "startOffset": 57, "endOffset": 60}, {"referenceID": 7, "context": "Similar bounds also exist when the data is not separated [6] and also for other generalizations of perceptron training [8, 11, 19].", "startOffset": 119, "endOffset": 130}, {"referenceID": 10, "context": "Similar bounds also exist when the data is not separated [6] and also for other generalizations of perceptron training [8, 11, 19].", "startOffset": 119, "endOffset": 130}, {"referenceID": 18, "context": "Similar bounds also exist when the data is not separated [6] and also for other generalizations of perceptron training [8, 11, 19].", "startOffset": 119, "endOffset": 130}, {"referenceID": 13, "context": "Figure 1, which is borrowed from [14], illustrates the version space interpretation of perceptrons.", "startOffset": 33, "endOffset": 37}, {"referenceID": 19, "context": "Naturally, classifiers including SVMs [20] and Bayes point machines [10] lie in the version space.", "startOffset": 38, "endOffset": 42}, {"referenceID": 9, "context": "Naturally, classifiers including SVMs [20] and Bayes point machines [10] lie in the version space.", "startOffset": 68, "endOffset": 72}, {"referenceID": 3, "context": "Both quantum approaches introduced in this work and their corresponding speed-ups stem from a quantum subroutine called Grover\u2019s search [4, 9], which is a special case of a more general method referred to as amplitude amplification [5].", "startOffset": 136, "endOffset": 142}, {"referenceID": 8, "context": "Both quantum approaches introduced in this work and their corresponding speed-ups stem from a quantum subroutine called Grover\u2019s search [4, 9], which is a special case of a more general method referred to as amplitude amplification [5].", "startOffset": 136, "endOffset": 142}, {"referenceID": 4, "context": "Both quantum approaches introduced in this work and their corresponding speed-ups stem from a quantum subroutine called Grover\u2019s search [4, 9], which is a special case of a more general method referred to as amplitude amplification [5].", "startOffset": 232, "endOffset": 235}, {"referenceID": 2, "context": "This basis dependence of measurement is the root of many of the differences between quantum and classical probability theory and also gives rise to many celebrated results in the foundations of quantum mechanics such as Bell\u2019s theorem [3].", "startOffset": 235, "endOffset": 238}, {"referenceID": 3, "context": "Fortunately, methods are known to deal with such issues [4, 5].", "startOffset": 56, "endOffset": 62}, {"referenceID": 4, "context": "Fortunately, methods are known to deal with such issues [4, 5].", "startOffset": 56, "endOffset": 62}, {"referenceID": 15, "context": "Although the traditional feature space perceptron training algorithm is online [16], meaning that the training examples are provided one at a time to it in a streaming fashion, we deviate from this model slightly by instead requiring that the algorithm be fed training examples that are, in effect, sampled uniformly from the training set.", "startOffset": 79, "endOffset": 83}, {"referenceID": 0, "context": "For example, the training vector (\u03c6j , yj) \u2261 ([0, 0, 1, 0] , 1) can be encoded as an unsigned integer 00101 \u2261 5, which in turn can be represented by the unit vector \u03a6 = [0, 0, 0, 0, 0, 1] .", "startOffset": 46, "endOffset": 58}, {"referenceID": 0, "context": "For example, the training vector (\u03c6j , yj) \u2261 ([0, 0, 1, 0] , 1) can be encoded as an unsigned integer 00101 \u2261 5, which in turn can be represented by the unit vector \u03a6 = [0, 0, 0, 0, 0, 1] .", "startOffset": 169, "endOffset": 187}, {"referenceID": 5, "context": "Novikoff\u2019s theorem [6, 16] states that the algorithms described in both lemmas must be applied at most 1/\u03b3 times before finding the result.", "startOffset": 19, "endOffset": 26}, {"referenceID": 15, "context": "Novikoff\u2019s theorem [6, 16] states that the algorithms described in both lemmas must be applied at most 1/\u03b3 times before finding the result.", "startOffset": 19, "endOffset": 26}, {"referenceID": 3, "context": "loss of generality equivalent to Grover\u2019s problem [4, 9].", "startOffset": 50, "endOffset": 56}, {"referenceID": 8, "context": "loss of generality equivalent to Grover\u2019s problem [4, 9].", "startOffset": 50, "endOffset": 56}, {"referenceID": 3, "context": "Since Grover\u2019s search reduces to perceptron training in the case of one marked item the lower bound of \u03a9( \u221a N) queries for Grover\u2019s search [4] applies to perceptron training.", "startOffset": 139, "endOffset": 142}, {"referenceID": 14, "context": "Now using V (and applying the Hadamard transform [15]) we can prepare the following quantum state", "startOffset": 49, "endOffset": 53}, {"referenceID": 14, "context": "This process can be realized using a quantum subroutine that computes fw, an application of V and V \u2020 and also the application of a conditional phase gate (which is a fundamental quantum operation that is usually denoted Z) [15].", "startOffset": 224, "endOffset": 228}, {"referenceID": 0, "context": "Nquant \u2208 O ( N \u221a \u03b3 log [ 1 ])", "startOffset": 23, "endOffset": 28}, {"referenceID": 15, "context": "Since the inner loop is repeated O( \u221a K log(1/ )) times, the maximum number of misclassified vectors that arises from this training process is from Theorem 2 O( 1 \u221a\u03b3 log (1/ )) which, for constant , constitutes a quartic improvement over the standard mistake bound of 1/\u03b3 [16].", "startOffset": 272, "endOffset": 276}, {"referenceID": 4, "context": "Under the assumption that c \u2208 (1, 2) the next loop repeats this sampling process until m \u2265M0 in order to ensure that the probability of finding a misclassified element is at least 1/4 [5].", "startOffset": 184, "endOffset": 187}, {"referenceID": 4, "context": "Therefore, under these assumptions, the probability that the middle loop updates the perceptron weights is at least 1/4 from [5].", "startOffset": 125, "endOffset": 128}, {"referenceID": 4, "context": "The outer loop serves to amplify the success probability to at least 1\u2212 from the (average) success probability for m \u2265M0, which is at least 1/4, given that the perceptron makes a mistake on at least one training vector [5].", "startOffset": 219, "endOffset": 222}, {"referenceID": 0, "context": "[1] A\u0131\u0308meur, Esma, Brassard, Gilles, and Gambs, S\u00e9bastien.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Amin, Mohammad H, Andriyash, Evgeny, Rolfe, Jason, Kulchytskyy, Bohdan, and Melko, Roger.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Bell, John S.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Boyer, Michel, Brassard, Gilles, H\u00f8yer, Peter, and Tapp, Alain.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Brassard, Gilles, Hoyer, Peter, Mosca, Michele, and Tapp, Alain.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Freund, Yoav and Schapire, Robert E.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Garnerone, Silvano, Zanardi, Paolo, and Lidar, Daniel A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Gentile, Claudio.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Grover, Lov K.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Herbrich, Ralf, Graepel, Thore, and Campbell, Colin.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Li, Yaoyong, Zaragoza, Hugo, Herbrich, Ralf, Shawe-Taylor, John, and Kandola, Jaz.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Lloyd, Seth, Mohseni, Masoud, and Rebentrost, Patrick.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Lloyd, Seth, Mohseni, Masoud, and Rebentrost, Patrick.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Minka, Thomas P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Nielsen, Michael A and Chuang, Isaac L.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Novikoff, Albert BJ.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Rebentrost, Patrick, Mohseni, Masoud, and Lloyd, Seth.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Rosenblatt, Frank.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Shalev-Shwartz, Shai and Singer, Yoram.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Suykens, Johan AK and Vandewalle, Joos.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] Wiebe, Nathan and Granade, Christopher.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] Wiebe, Nathan, Kapoor, Ashish, and Svore, Krysta M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Wiebe, Nathan, Kapoor, Ashish, and Svore, Krysta.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "We demonstrate how quantum computation can provide non-trivial improvements in the computational and statistical complexity of the perceptron model. We develop two quantum algorithms for perceptron learning. The first algorithm exploits quantum information processing to determine a separating hyperplane using a number of steps sublinear in the number of data points N , namely O( \u221a N). The second algorithm illustrates how the classical mistake bound of O( 1 \u03b32 ) can be further improved to O( 1 \u221a \u03b3 ) through quantum means, where \u03b3 denotes the margin. Such improvements are achieved through the application of quantum amplitude amplification to the version space interpretation of the perceptron model.", "creator": "LaTeX with hyperref package"}}}