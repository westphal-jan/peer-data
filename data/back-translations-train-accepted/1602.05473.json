{"id": "1602.05473", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Feb-2016", "title": "Auxiliary Deep Generative Models", "abstract": "Deep generative models parameterized by neural networks have recently achieved state-of-the-art performance in unsupervised and semi-supervised learning. We extend deep generative models with auxiliary variables which improves the variational approximation. The auxiliary variables leave the generative model unchanged but make the variational distribution more expressive. Inspired by the structure of the auxiliary variable we also propose a model with two stochastic layers and skip connections. Our findings suggest that more expressive and properly specified deep generative models converge faster with better results. We show state-of-the-art performance within semi-supervised learning on MNIST (0.96%), SVHN (16.61%) and NORB (9.40%) datasets.", "histories": [["v1", "Wed, 17 Feb 2016 16:24:50 GMT  (3490kb,D)", "http://arxiv.org/abs/1602.05473v1", "Under review"], ["v2", "Thu, 26 May 2016 10:21:34 GMT  (3912kb,D)", "http://arxiv.org/abs/1602.05473v2", "Proceedings of the 33rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: Workshop and Conference Proceedings volume 48, Proceedings of the 33rd International Conference on Machine Learning, New York, NY, USA, 2016"], ["v3", "Fri, 3 Jun 2016 09:19:21 GMT  (4071kb,D)", "http://arxiv.org/abs/1602.05473v3", "Proceedings of the 33rd International Conference on Machine Learning, New York, NY, USA, 2016"], ["v4", "Thu, 16 Jun 2016 06:39:08 GMT  (4071kb,D)", "http://arxiv.org/abs/1602.05473v4", "Proceedings of the 33rd International Conference on Machine Learning, New York, NY, USA, 2016, JMLR: Workshop and Conference Proceedings volume 48, Proceedings of the 33rd International Conference on Machine Learning, New York, NY, USA, 2016"]], "COMMENTS": "Under review", "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG", "authors": ["lars maal\u00f8e", "casper kaae s\u00f8nderby", "s\u00f8ren kaae s\u00f8nderby", "ole winther"], "accepted": true, "id": "1602.05473"}, "pdf": {"name": "1602.05473.pdf", "metadata": {"source": "META", "title": "Auxiliary Deep Generative Models", "authors": ["Lars Maal\u00f8e", "Casper Kaae S\u00f8nderby", "S\u00f8ren Kaae S\u00f8nderby", "Ole Winther"], "emails": ["LARSMA@DTU.DK", "CASPERKAAE@GMAIL.COM", "SKAAESONDERBY@GMAIL.COM", "OLWI@DTU.DK"], "sections": [{"heading": "1. Introduction", "text": "In this paper we show that the deeper models with the more explicit distributions are easier to optimize and that we increase the flexibility of the model by introducing auxiliary distributors."}, {"heading": "2. Auxiliary deep generative models", "text": "More recently, Kingma (2013); Rezende et al. (2014) have coupled the variational inference approach with deep learning, producing strong probabilistic models constructed by an inferential neural network q (z | x) and a generative neural network p (x | z). However, this approach can be perceived as a variable equivalent to the deep auto encoder, in which q (z | x) functions as an encoder and p (x | z) as a decoder. However, the difference is that these models provide efficient inference over various continuous distributions in the latent space z and complex input datasets x, in which the posterior distribution p (x | z) is intractable. Furthermore, the gradients of the variable upper limit are easily defined by back propagation by the network (s). In order to keep computational requirements low, the variative distribution model p (z | x) is chosen as the usual inference force."}, {"heading": "2.1. Variational auto-encoder", "text": "The Variation Auto Encoder (VAE) has recently been introduced as a powerful method for unattended learning, where a latent variable generative model p\u03b8 (x | z) for data x is parameterized by a deep neural network with the parameters \u03b8. Parameters are parameterized by maximizing the variable lower limit of probability \u2212 \u2211 i UVAE (xi) withlog p\u03b8 (x) = log \u0445 z p (x, z) dz \u2265 Eq\u03c6 (z | x) [logp\u03b8 (x | z) p\u03b8 (z) q\u03c6 (z | x)] (1) \u2261 \u2212 UVAE (x).The inference model q\u03c6 (z | x) is parameterized as a second deep neural network. Inference and generative parameters are jointly trained by optimizing Equation 1 (e.g. stochastic gradient), where we perform the trick of repairing the variables through recession (backwards)."}, {"heading": "2.2. Auxiliary variables", "text": "We propose to extend the variation distribution by auxiliary variables a: q (a, z | x) = q (z | a, x) q (a | x) so that the boundary distribution q (z | x) can accommodate more complicated background variables p (z | x). In order to have an unchanged generative model, it is necessary that the common mode p (x, z, a) returns the original p (x, z) under marginalization via a, i.e. p (x, z, a) = p (a (x, z) p (x, z). Auxiliary variables are used in the EM algorithm and in scanning Gibbs and were previously considered for variable learning by Agakov and Barber (2004). Recently, Ranganath et al. (2015) proposed to make the parameters of the variation distribution z z, leading to a similar model."}, {"heading": "2.3. Semi-supervised learning", "text": "The main focus of this work is on the use of the supplementary approach to create semi-monitored models that learn the classifiers from the labeled and unlabeled data. To capture class information, we introduce an additional latent variable. The generative model P is defined as (y) p (z) p (z) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (z) p (z) p (p) p (z) p, p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p, p (p), p (p), p (p), p (p, p, p (p), p (p), p (p, p (p), p (p, p (p), p (p, p), p (p (p), p (p, p, p (p), p (p, p), p (p (p), p (p, p), p (p, p, p (p), p (p (p), p (p (p), p (p, p), p (p (p), p (p, p, p (p), p (p (, p (p), p (p, p (p), p (p), p (p (p), p (p (p), p (, p (p), p (p (, p), p (, p (p, p (, p (p), p (p (p), p ("}, {"heading": "2.4. Two stochastic layers with skip connections", "text": "Kingma et al. (2014) proposed a model with two stochastic layers and trained it one layer at a time. In our preliminary analysis, we also found that this model: p\u03b8 (x | z1) p\u03b8 (z1 | z2, y) p (z2) p (y) could not converge when fully formed. On the other hand, the auxiliary model can be converted into a two-layer stochastic model simply by reversing the arrow between a and x in Fig. 1a. We would expect that if the auxiliary model works well in terms of convergence and performance, this two-layer model (a is now part of the generative model): p\u03b8 (x | y, a, z) p\u03b8 (a | z, y) p (y) p (y) p (y) p (y) should work even better because it is a more flexible generative model."}, {"heading": "3. Experiments", "text": "The SDGM and ADGM are each parametrized by 5 neuronal networks (NN = 000). We are then the function linear (approximate). We are each parametrized by 5 neuronal networks (NN = 000): (1) Auxiliary Inference Model q\u03c6 (a | x), (2) latent Inference Model q\u03c6 (z, y, x), (3), (3), (3), (4), (4), (4), (4), (4), (4), (3), (4), (3), (3), (4), (4), (3), (3), (3), (3), (3), (3), (3), (3), (3), (3), (3), (3), (3), (3), (3), (3), (3), (4), (3), (3), (3), (3), (3), (3), (3), (3), (3), (3), (3), (3), (3), (3), (3), (3), (3, (3), (3), (3), (3), (3, (3), (3), (3), (3), (3), (3, (3), (3), (3), (3, (3), (3), (3), (3), (3, (3), (3), (3), (3, (3), (3, (3), (3), (3, (3), (3, (3), (3), (3), (3, (3), (3, (3), (3, (3), (3), (3, (3), (3, (3, (3), (3), (3, (3), (3), (3, (3, (3), (3, (3), (3), (3, (3), (3, ("}, {"heading": "4. Results", "text": "In this section, we present two toy examples that illustrate how the auxiliary variables improve distribution fit. Then, we examine the semi-monitored classification performance in several benchmark datasets. We demonstrate state-of-the-art technology and show that adding auxiliary variables increases both classification performance and convergence speed (see paragraph 3 for details)."}, {"heading": "4.1. Beyond Gaussian latent distributions", "text": "We show that the auxiliary model can fit complicated posterior distributions for latent space. To this end, we consider the 2D potential model p (z) = exp (U (z)) / Z (Rezende and Mohamed, 2015), which leads to the boundary logZ \u2265 Eq\u03c6 (a, z) [logexp (U (z)) p\u03b8 (a) q\u03c6 (a) q\u03c6 (z | a)]. (20) Fig. 2a shows the true posterior and Fig. 2b shows a density diagram of z samples from a (a) and z (z) q\u03c6 (z | a) of a trained ADGM. This is similar to the results of Rezende and Mohamed (2015), in which they show that they can exhibit more complicated posterior distributions by using normalization flows. The most common solution is that two optimization modes are not limited to one, but only to one focused solution."}, {"heading": "4.2. Two half-moons semi-supervised learning", "text": "To illustrate the power of the ADGM for semi-supervised learning, we have created a synthetic 2D dataset consisting of two crescent moons (top and bottom): (xtop, ytop) = (cos ([0, \u03c0]), sin ([0, \u03c0])) = (1 \u2212 cos ([0, \u03c0]), 1 \u2212 sin ([0, \u03c0] \u2212 0.5), with additional Gaussian noise. The training set contains 1e4 samples, divided into lots of 1e2 each with 3 labeled data points in each class, and the test set contains 1e4 samples. A good semi-supervised model will be able to learn the data for each of the crescent moons in a variety of ways and to use these together with the limited labeled information to create the classification. The ADGM is powerful in learning this problem (see Fig. 2c). The model converges nearly 0% classification errors in 10, which is much faster than a comparable model without the 2xable epoch."}, {"heading": "4.3. Semi-supervised benchmarks", "text": "This year, the time has come to cut it by half, but no longer by half."}, {"heading": "5. Discussion", "text": "The ADGM and SDGM are powerful deep generative models with relatively simple neural network architectures. They are feasible, and because they follow the principles of variable inference, there are several improvements to optimize the models. We can take advantage of the importance of weighting and add more layers of stochastic variables, both of which have recently been shown to be beneficial in generative models (Burda et al., 2015). Furthermore, we have proposed only models with a Gaussian latent distribution, but the model can easily be extended to other distributions (Ranganath et al., 2015).One way to approach the stability problems of ADGM is to add the formation to Gaussian distributions x in order to achieve a temperature weighting between discriminatory and stochastic learning processes (Ranganath et al., 2015).One way to address the stability of distributions is that the distributions are distributions x."}, {"heading": "6. Conclusion", "text": "We introduced a novel framework to make the variation distributions used in deep generative models more meaningful. In two toy examples and the benchmarks, we examined how the framework uses the auxiliary variables to learn better approximations of variation. Finally, we demonstrated that the framework performs well in a series of semi-monitored benchmarks and is consistently trainable."}, {"heading": "Acknowledgements", "text": "We thank Durk P. Kingma and Shakir Mohamed for helpful discussions. This research was supported by the Novo Nordisk Foundation, the Danish Innovation Foundation and the NVIDIA Corporation with the donation of TITAN X and Tesla K40 GPUs.A. In this appendix we give an overview of the variation targets used. The generative models P are for the aid project Deep Generative Model and the Skip Deep Generative Model, which is defined as follows: ADGM: pTB (x, a, y, y, z) = pTB (x, y, z) pTB (a, x, y, z) pTB (a, x, x, z) pTB (a, qy, z) pTB (a, z) p (z) p (z) p (z), pz (x, y) and \u2212 U (x) are lower limits of equality (x, y), PTB (z, a), TB (a) p (z), pz (x, y), pz (p), p (a), pz (p)."}, {"heading": "B. Auxiliary model specification", "text": "In this appendix, we examine the theoretical optimum of the auxiliary variation limit found by the functional derivatives of the variation target q (q). In practice, we use limited, net parameterized distributions, but this analysis nevertheless illuminates the properties of the optimum q (q). Without loss of generality, we only consider auxiliary factors a and latent z: p (a, z) = p (z), p (z) = f (z) / Z and q (a, z) = q (z | a) q (a) q (a).The results can be extended to the complete partially monitored setting z without changing the general conclusion. The limit of variation for the auxiliary model islogZ \u2265 Eq (a) [logf (z) p (a) p) p (z) q (a) q (a) q (a) q (a)) q (a)) z (a)."}], "references": [{"title": "An Auxiliary Variational Method", "author": ["F. Agakov", "D. Barber"], "venue": null, "citeRegEx": "Agakov and Barber,? \\Q2004\\E", "shortCiteRegEx": "Agakov and Barber", "year": 2004}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio"], "venue": "In Deep Learning and Unsupervised Feature", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Importance Weighted Autoencoders", "author": ["Y. Burda", "R. Grosse", "R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1509.00519", "citeRegEx": "Burda et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Burda et al\\.", "year": 2015}, {"title": "Lasagne: First release", "author": ["S. Dieleman", "J. Schlter", "C. Raffel", "E. Olson", "S.K. S\u00f8nderby", "D. Nouri", "A. van den Oord", "E. B"], "venue": null, "citeRegEx": "Dieleman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dieleman et al\\.", "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics", "citeRegEx": "Glorot and Bengio,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio", "year": 2010}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "In Proceedings of International Conference of Machine Learning,", "citeRegEx": "Ioffe and Szegedy,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy", "year": 2015}, {"title": "How to train deep variational autoencoders and probabilistic ladder networks. arXiv preprint arXiv:1602.02282", "author": ["C. Kaae S\u00f8nderby", "T. Raiko", "L. Maal\u00f8e", "S. Kaae S\u00f8nderby", "O. Winther"], "venue": null, "citeRegEx": "S\u00f8nderby et al\\.,? \\Q2016\\E", "shortCiteRegEx": "S\u00f8nderby et al\\.", "year": 2016}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "Kingma and Ba,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba", "year": 2014}, {"title": "Semi-Supervised Learning with Deep Generative Models", "author": ["D.P. Kingma", "D.J. Rezende", "S. Mohamed", "M. Welling"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Auto-Encoding Variational Bayes", "author": ["Diederik P Kingma", "M. Welling"], "venue": "arXiv preprint arXiv:1312.6114", "citeRegEx": "Kingma and Welling,? \\Q2013\\E", "shortCiteRegEx": "Kingma and Welling", "year": 2013}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Learning methods for generic object recognition with invariance to pose and lighting", "author": ["Y. LeCun", "F.J. Huang", "L. Bottou"], "venue": "In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "LeCun et al\\.,? \\Q2004\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2004}, {"title": "Distributional Smoothing with Virtual Adversarial Training", "author": ["T. Miyato", "Maeda", "S.-i", "M. Koyama", "K. Nakae", "S. Ishii"], "venue": "arXiv preprint arXiv:1507.00677", "citeRegEx": "Miyato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Miyato et al\\.", "year": 2015}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "In Deep Learning and Unsupervised Feature Learning, workshop at Neural Information Processing Systems", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Deep exponential families", "author": ["R. Ranganath", "L. Tang", "L. Charlin", "D.M. Blei"], "venue": "arXiv preprint arXiv:1411.2581", "citeRegEx": "Ranganath et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ranganath et al\\.", "year": 2014}, {"title": "Hierarchical variational models. arXiv preprint arXiv:1511.02386", "author": ["R. Ranganath", "D. Tran", "D.M. Blei"], "venue": null, "citeRegEx": "Ranganath et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranganath et al\\.", "year": 2015}, {"title": "Semi-supervised learning with ladder networks", "author": ["A. Rasmus", "M. Berglund", "M. Honkala", "H. Valpola", "T. Raiko"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Rasmus et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rasmus et al\\.", "year": 2015}, {"title": "Variational Inference with Normalizing Flows", "author": ["D.J. Rezende", "S. Mohamed"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Rezende and Mohamed,? \\Q2015\\E", "shortCiteRegEx": "Rezende and Mohamed", "year": 2015}, {"title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models. arXiv preprint arXiv:1401.4082", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": null, "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Rnade: The real-valued neural autoregressive density-estimator", "author": ["B. Uria", "I. Murray", "H. Larochelle"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Uria et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Uria et al\\.", "year": 2013}, {"title": "From neural pca to deep unsupervised learning. arXiv preprint arXiv:1411.7783", "author": ["H. Valpola"], "venue": null, "citeRegEx": "Valpola,? \\Q2014\\E", "shortCiteRegEx": "Valpola", "year": 2014}, {"title": "Pixel recurrent neural networks. arXiv preprint arXiv:1601.06759", "author": ["A. van den Oord", "K. Nal", "K. Kavukcuoglu"], "venue": null, "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 18, "context": "Stochastic backpropagation, deep neural networks and approximate Bayesian inference has made deep generative models practical for large scale problems (Kingma, 2013; Rezende et al., 2014), but typically they assume a mean field latent distribution where all latent variables are independent.", "startOffset": 151, "endOffset": 187}, {"referenceID": 0, "context": "We increase the flexibility of the model by introducing auxiliary variables (Agakov and Barber, 2004) allowing more complex latent distributions.", "startOffset": 76, "endOffset": 101}, {"referenceID": 10, "context": "We demonstrate the benefits of the increased flexibility by achieving state-of-the-art performance in the semisupervised setting for the MNIST (LeCun et al., 1998),", "startOffset": 143, "endOffset": 163}, {"referenceID": 13, "context": "SVHN (Netzer et al., 2011) and NORB (LeCun et al.", "startOffset": 5, "endOffset": 26}, {"referenceID": 11, "context": ", 2011) and NORB (LeCun et al., 2004) datasets.", "startOffset": 17, "endOffset": 37}, {"referenceID": 16, "context": "Lately the Ladder network (Rasmus et al., 2015; Valpola, 2014) and virtual adversarial training (VAT) (Miyato et al.", "startOffset": 26, "endOffset": 62}, {"referenceID": 20, "context": "Lately the Ladder network (Rasmus et al., 2015; Valpola, 2014) and virtual adversarial training (VAT) (Miyato et al.", "startOffset": 26, "endOffset": 62}, {"referenceID": 12, "context": ", 2015; Valpola, 2014) and virtual adversarial training (VAT) (Miyato et al., 2015) has improved the performance further with end-toend training.", "startOffset": 62, "endOffset": 83}, {"referenceID": 8, "context": "Kingma et al. (2014) introduced a deep generative model for semisupervised learning by modeling the joint distribution over data and labels.", "startOffset": 0, "endOffset": 21}, {"referenceID": 18, "context": "Recently Kingma (2013); Rezende et al. (2014) have coupled the approach of variational inference with deep learning giving rise to powerful probabilistic models constructed by an inference neural network q(z|x) and a generative neural network p(x|z).", "startOffset": 24, "endOffset": 46}, {"referenceID": 0, "context": "In this paper we propose a variational auxiliary variable approach (Agakov and Barber, 2004) to improve the variational distribution: The generative model is extended with variables a to p(x, z, a) such that the original model is invariant to marginalization over a: p(x, z, a) = p(a|x, z)p(x, z).", "startOffset": 67, "endOffset": 92}, {"referenceID": 18, "context": "stochastic gradient descent), where we use the reparameterization trick for backpropagation through the Gaussian latent variables (Kingma, 2013; Rezende et al., 2014).", "startOffset": 130, "endOffset": 166}, {"referenceID": 0, "context": "Auxiliary variables are used in the EM algorithm and Gibbs sampling and has previously been considered for variational learning by Agakov and Barber (2004). Recently, Ranganath et al.", "startOffset": 131, "endOffset": 156}, {"referenceID": 0, "context": "Auxiliary variables are used in the EM algorithm and Gibbs sampling and has previously been considered for variational learning by Agakov and Barber (2004). Recently, Ranganath et al. (2015) has proposed to make the parameters of the variational distribution stochastic, which leads to a similar model.", "startOffset": 131, "endOffset": 191}, {"referenceID": 0, "context": "Auxiliary variables are used in the EM algorithm and Gibbs sampling and has previously been considered for variational learning by Agakov and Barber (2004). Recently, Ranganath et al. (2015) has proposed to make the parameters of the variational distribution stochastic, which leads to a similar model. It is important to note that in order not to fall back to the original VAE model one has to require p(a|x, z) 6= p(a), see Agakov and Barber (2004) and App.", "startOffset": 131, "endOffset": 451}, {"referenceID": 18, "context": "The expectation over the a and z variables were performed by Monte Carlo sampling using the reparameterization trick (Kingma, 2013; Rezende et al., 2014) and the average over y by exact enumeration so", "startOffset": 117, "endOffset": 153}, {"referenceID": 4, "context": "All parameters are initialized using the Glorot and Bengio (2010) scheme.", "startOffset": 41, "endOffset": 66}, {"referenceID": 7, "context": "For training, we have used the Adam (Kingma and Ba, 2014) optimization framework with a learning rate of 3e4, exponential decay rate for the 1st and 2nd moment at 0.", "startOffset": 36, "endOffset": 57}, {"referenceID": 1, "context": "The models are implemented in Python using Theano (Bastien et al., 2012), Lasagne (Dieleman et al.", "startOffset": 50, "endOffset": 72}, {"referenceID": 3, "context": ", 2012), Lasagne (Dieleman et al., 2015) and Parmesan libraries1.", "startOffset": 17, "endOffset": 40}, {"referenceID": 19, "context": "To avoid the phenomenon on modeling discretized values with a real-valued estimation (Uria et al., 2013), we added uniform noise between 0 and 1 to each pixel value.", "startOffset": 85, "endOffset": 104}, {"referenceID": 1, "context": "The models are implemented in Python using Theano (Bastien et al., 2012), Lasagne (Dieleman et al., 2015) and Parmesan libraries1. For the MNIST dataset we have combined the training set of 50000 examples with the validation set of 10000 examples. The test set remained as is. We used a batch size Implementation will be available in a variational extension framework to the Lasagne library named Parmesan on https://github.com. of 200 with half of the batch always being the 100 labeled samples. The labeled data are chosen randomly, but distributed evenly across classes. To speed up training, we removed the columns with a standard deviation below 0.1 resulting in an input size of dim(x) = 444. Before each epoch the normalized MNIST images were binarized by sampling Bernoulli distributions. For the SVHN dataset we used the vectorized and cropped training set dim(x) = 3072 with classes from 0 to 9, combined with the extra set resulting in 604388 data points. The test set is of size 26032. We trained on the small NORB dataset consisting of 24300 training samples and an equal amount of test samples distributed across 5 classes: animal, human, plane, truck, car. We normalized all NORB images following Miyato et al. (2015) using image pairs of 32x32 resulting in a vectorized input of dim(x) = 2048.", "startOffset": 51, "endOffset": 1233}, {"referenceID": 17, "context": "To do this we consider the 2D potential model p(z) = exp(U(z))/Z (Rezende and Mohamed, 2015) that leads to the bound", "startOffset": 65, "endOffset": 92}, {"referenceID": 17, "context": "This is similar to the findings of Rezende and Mohamed (2015) in which they demonstrate that by using normalizing flows they can fit complicated posterior distributions.", "startOffset": 35, "endOffset": 62}, {"referenceID": 2, "context": "The number of clearly activated units in z and a is quite low \u223c 20, but there is a tail of slightly active units, similar results have been reported by Burda et al. (2015). It is still evident that we have information flowing through", "startOffset": 152, "endOffset": 172}, {"referenceID": 8, "context": "Furthermore we also implemented the M2 model of Kingma et al. (2014) using the exact same hyperparameters as for learning the ADGM.", "startOffset": 48, "endOffset": 69}, {"referenceID": 2, "context": "Both have recently shown to be beneficial in generative models (Burda et al., 2015).", "startOffset": 63, "endOffset": 83}, {"referenceID": 14, "context": "Furthermore we have only proposed the models using a Gaussian latent distribution, but the model can easily be extended to other distributions (Ranganath et al., 2014; 2015).", "startOffset": 143, "endOffset": 173}, {"referenceID": 5, "context": "Another potential stabilizer is to add batch normalization (Ioffe and Szegedy, 2015) that will ensure normalization of each output batch of a fully connected hidden layer.", "startOffset": 59, "endOffset": 84}, {"referenceID": 2, "context": "Both have recently shown to be beneficial in generative models (Burda et al., 2015). Furthermore we have only proposed the models using a Gaussian latent distribution, but the model can easily be extended to other distributions (Ranganath et al., 2014; 2015). One way of approaching the stability issues of the ADGM, when training on Gaussian input distributions x is to add a temperature weighting between discriminative and stochastic learning on the KL-divergence for a and z when estimating the variational lower bound (Kaae S\u00f8nderby et al., 2016). We find similar problems for the Gaussian input distributions in van den Oord et al. (2016), where they restrict the dataset to ordinal values, so that they can apply a softmax function for the output of the generative model p(x|\u00b7).", "startOffset": 64, "endOffset": 645}, {"referenceID": 0, "context": "So we fall back to a model for z without the auxiliary as also noted by Agakov and Barber (2004). We have tested the uninformed auxiliary model in semisupervised learning for the benchmarks and we got competitive results for MNIST but not for the two other benchmarks.", "startOffset": 72, "endOffset": 97}], "year": 2016, "abstractText": "Deep generative models parameterized by neural networks have recently achieved state-ofthe-art performance in unsupervised and semisupervised learning. We extend deep generative models with auxiliary variables which improves the variational approximation. The auxiliary variables leave the generative model unchanged but make the variational distribution more expressive. Inspired by the structure of the auxiliary variable we also propose a model with two stochastic layers and skip connections. Our findings suggest that more expressive and properly specified deep generative models converge faster with better results. We show state-of-theart performance within semi-supervised learning on MNIST (0.96%), SVHN (16.61%) and NORB (9.40%) datasets.", "creator": "LaTeX with hyperref package"}}}