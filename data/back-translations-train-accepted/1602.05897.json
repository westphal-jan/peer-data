{"id": "1602.05897", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2016", "title": "Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity", "abstract": "We develop a general duality between neural networks and compositional kernels, striving towards a better understanding of deep learning. We show that initial representations generated by common random initializations are sufficiently rich to express all functions in the dual kernel space. Hence, though the training objective is hard to optimize in the worst case, the initial weights form a good starting point for optimization. Our dual view also reveals a pragmatic and aesthetic perspective of neural networks and underscores their expressive power.", "histories": [["v1", "Thu, 18 Feb 2016 18:14:19 GMT  (52kb,D)", "http://arxiv.org/abs/1602.05897v1", null], ["v2", "Fri, 19 May 2017 18:39:00 GMT  (61kb,D)", "http://arxiv.org/abs/1602.05897v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CC cs.DS stat.ML", "authors": ["amit daniely", "roy frostig", "yoram singer"], "accepted": true, "id": "1602.05897"}, "pdf": {"name": "1602.05897.pdf", "metadata": {"source": "CRF", "title": "Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity", "authors": ["Amit Daniely", "Roy Frostig", "Yoram Singer"], "emails": ["amitdaniely@google.com", "rf@cs.stanford.edu.", "singer@google.com"], "sections": [{"heading": null, "text": "\u2022 Email: amitdaniely @ google.com \u2020 Email: rf @ cs.stanford.edu. Work at Google. \u2021 Email: singer @ google.comar Xiv: 160 2.05 897v 1 [csgoogle.com]"}, {"heading": "1 Introduction 1", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 Related work 2", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 Setting 3", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 Computation skeletons 4", "text": "4.1 From computational skeletons to neural networks...... 6 4.2 From computational skeletons to reproducible cores..... 7"}, {"heading": "5 Main results 9", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6 Mathematical background 11", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7 Compositional kernel spaces 12", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8 The dual activation function 14", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9 Proofs 19", "text": "........................................................................................................................."}, {"heading": "1 Introduction", "text": "In fact, the majority of people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move"}, {"heading": "2 Related work", "text": "In fact, it is in such a way that most people who are in a position to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "3 Setting", "text": "We assume that the 2 Norm x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "4 Computation skeletons", "text": "In this section we define a simple structure that we call a computational skeleton. The purpose of a computational skeleton is to describe a compact, forward-looking calculation from an input to an output. A single skeleton comprises a family of neural networks that share the same skeletal structure. Likewise, it defines a corresponding kernel space. Definition. A computational skeleton S is a DAG whose non-input nodes are characterized by activations. Although the formal definition of neural networks and skeletons appears identical, we make a conceptual distinction between them as their role in our analysis quite different. Achieved by a series of weights, a neural describes a concrete function, while the skeleton represents a topology common to several networks, as well as to a kernel. To further emphasize the differences, we note that skeletons are naturally more compact than networks."}, {"heading": "4.1 From computation skeletons to neural networks", "text": "The following definition shows how a skeleton, accompanied by a replication parameter r \u2265 1 and a number of output nodes k, induces a neural network architecture. Remember that inputs are ordered sets of vectors in Sd \u2212 1.Definition (realization of a skeleton). Let S be a computational skeleton and view the input coordinates in Sd \u2212 1 as in (1). For r, k \u2265 1 we define the following neural network N = N (S, r, k). For each input node in S, N has d corresponding input coordinates. For each internal node v marked by an activation, N has r neurons v1,.., vr, each activation definition with an activation. In addition, N k has output neurons o1,., ok with the identity activation of neurons (x) = x. There is an edge uj of E (N) always explicit."}, {"heading": "4.2 From computation skeletons to reproducing kernels", "text": "In addition to architectures, a computational skeleton S is also a normalized kernel (> Kernel \u03baS: X \u00b7 X \u2192 1, 1] and a corresponding norm for functions f: X \u2192 R. This norm has the property that f-S is actually small if and only if f can be achieved by certain simple compositions of functions according to the structure of S. To define the kernel, we present a dual activation and dual kernel. Dual activation of an activation is the function of the multivariate Gaussian distribution on R2 with the mean 0 and covariance matrix (1).Definition (dual activation and kernel) is the function we define as such: [\u2212 1, 1] R defines as an associated distribution on R2 with the mean 0 and covariance matrix (1)."}, {"heading": "5 Main results", "text": "There are only a few approaches to corroborate this. First, our analysis implies that a representation generated by a random initialization of N = N (S, r, k) > has an unforeseen depth of distribution. (Then, we also show that the functions achieved by the composition of limited linear functions with RN are roughly the bounded-norm functions in HS. In other words, the functions expressed by N are roughly bounded-norm functions in HS. For simplicity, we limit the analysis to the case of k = 1. We also limit the analysis to either bounded activations, with bounded first and second derivatives, or the ReLU activation."}, {"heading": "6 Mathematical background", "text": "The proofs of all the theorems we quote here are well-known and can be found in chapter 2 of [48] and similar textbooks. Let's leave a Hilbert room of functions from X to R. We say that H is a reproducing Hilbert room, abbreviated to RKHS or kernel room, if for each x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x"}, {"heading": "7 Compositional kernel spaces", "text": "Let S be a skeleton with normalized activations and n input nodes associated with the input coordinates. (1) We show that it is indeed a normalized kernel. (7) Recursion (7) describes a means of creating another kernel. Since kernels correspond to other kernel spaces, it also prescribes an operator that produces a kernel space from other kernel spaces. (7) Recursion (7) describes a different kernel for creating another kernel space. (8) If Hv is the space that corresponds to v."}, {"heading": "8 The dual activation function", "text": "The following problem describes a couple of basic characteristics of dual activation. These characteristics arise easily from the definition of dual activation and equations (2), (4) and (5). - Lemma 11. The following properties of the mapping procedure procedure procedure procedure procedure procedure procedure procedure process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process process"}, {"heading": "9 Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9.1 Well-behaved activations", "text": "The proof of our main results relates to activations that are decent, i.e. dual activity, in a sense that is subsequently defined. We then show that C-delimited activations and ReLU activation are decent. \u2212 Then, we must first broaden the definition of dual activation and kernel to apply it to vectors in Rd, rather than just to Sd. We designate them by M + the capture of 2 \u00b7 2 positive semi-defined matrices, and by M + + the capture of more positively defined matrices. Let's be an activation. Define the following, M2 + Sd."}, {"heading": "9.2 Proofs of Thms. 2 and 3", "text": "We start with an additional theory that serves as a simple springboard for proving the above-mentioned basic theory. (Theorem 14) Let's be a random initialization of the network N = N (S, r) with a random (S, r) -decent activation, 0 < \u2264) and Bd = (Bd =) with a probability of at least 1 \u2212 3. Let w be a random initialization of the network N = N (S, r) with a random (S) -2B2 depth (S). Then it applies to each x, y with a probability of at least 1 \u2212 4 \u2212 4 that there is a (x, y) -decent initialization of the network N = N (S, y). Before proving the theorem, we show that together with Lemmas 12 and 13 we follow theorems 2 and 3 depth, from Theorem 14. We present them as corollaries, and then proceed to the proof of the corollary of the theorem 14.orem."}, {"heading": "9.3 Proofs of Thms. 4 and 5", "text": "Theorems 4 and 5 arise from the use of the following problem combined with theorems 5 and 3. When applying the problem, we always focus on the specific case in which one of the nuclei is constant."}, {"heading": "10 Discussion", "text": "The role of the last layer initialization and training correlates with the formation of the entire network. The effectiveness of the sole optimization of the last layer is essentially an explanation for the success of the neural networks. While we largely leave this question for future research, we would like to point to empirical evidence that supports the important role of initialization. First, numerous researchers and practitioners show that random initialization, similar to the scheme we are analyzing, is crucial for the success of the neural network (see for example [20]), suggesting that starting from arbitrary weights it is unlikely to lead to a good solution. Second, several studies show that the contribution of optimization of the representation layers is relatively small [49, 44, 43, 15]. For example, the competing accuracy on CIFAR-10, STL-10 and MONO datasets can be achieved by optimizing only the last layer."}, {"heading": "Acknowledgments", "text": "We would like to thank Yossi Arjevani, Elad Eban, Moritz Hardt, Elad Hazan, Percy Liang, Nati Linial, Ben Recht and Shai Shalev-Shwartz for fruitful discussions, comments and suggestions."}], "references": [{"title": "Learning polynomials with neural networks", "author": ["A. Andoni", "R. Panigrahy", "G. Valiant", "L. Zhang"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1908\u20131916,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep convolutional networks are hierarchical kernel machines", "author": ["F. Anselmi", "L. Rosasco", "C. Tan", "T. Poggio"], "venue": "arXiv:1508.01084,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural Network Learning: Theoretical Foundations", "author": ["M. Anthony", "P. Bartlet"], "venue": "Cambridge University Press,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1999}, {"title": "Provable bounds for learning some deep representations", "author": ["S. Arora", "A. Bhaskara", "R. Ge", "T. Ma"], "venue": "Proceedings of The 31st International Conference on Machine Learning, pages 584\u2013592,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Breaking the curse of dimensionality with convex neural networks", "author": ["F. Bach"], "venue": "arXiv:1412.8690,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "On the equivalence between kernel quadrature rules and random feature expansions", "author": ["F. Bach"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Universal approximation bounds for superposition of a sigmoidal function", "author": ["A.R. Barron"], "venue": "IEEE Transactions on Information Theory, 39(3):930\u2013945,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1993}, {"title": "Rademacher and Gaussian complexities: Risk bounds and structural results", "author": ["P.L. Bartlett", "S. Mendelson"], "venue": "Journal of Machine Learning Research, 3:463\u2013482,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network", "author": ["P.L. Bartlett"], "venue": "IEEE Transactions on Information Theory, 44(2):525\u2013536, March", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1998}, {"title": "What size net gives valid generalization", "author": ["E.B. Baum", "D. Haussler"], "venue": "Neural Computation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1989}, {"title": "Object recognition with hierarchical kernel descriptors", "author": ["L. Bo", "K. Lai", "X. Ren", "D. Fox"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1729\u20131736. IEEE,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Invariant scattering convolution networks", "author": ["J. Bruna", "S. Mallat"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1872\u20131886,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Kernel methods for deep learning", "author": ["Y. Cho", "L.K. Saul"], "venue": "Advances in neural information processing systems, pages 342\u2013350,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "The loss surfaces of multilayer networks", "author": ["A. Choromanska", "M. Henaff", "M. Mathieu", "G. Ben Arous", "Y. LeCun"], "venue": "Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, pages 192\u2013204,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Beyond simple features: A large-scale feature search approach to unconstrained face recognition", "author": ["D. Cox", "N. Pinto"], "venue": "Automatic Face & Gesture Recognition and Workshops (FG 2011), 2011 IEEE International Conference on, pages 8\u201315. IEEE,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Complexity theoretic limitations on learning halfspaces", "author": ["A. Daniely"], "venue": "STOC,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Complexity theoretic limitations on learning DNFs", "author": ["A. Daniely", "S. Shalev-Shwartz"], "venue": "arXiv:1404.3378 v1,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "From average case complexity to improper learning complexity", "author": ["A. Daniely", "N. Linial", "S. Shalev-Shwartz"], "venue": "STOC,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural networks with random gaussian weights: A universal classification strategy", "author": ["R. Giryes", "G. Sapiro", "A.M. Bronstein"], "venue": "arXiv preprint arXiv:1504.08291,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "International conference on artificial intelligence and statistics, pages 249\u2013256,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "The pyramid match kernel: Discriminative classification with sets of image features", "author": ["K. Grauman", "T. Darrell"], "venue": "Computer Vision, 2005. ICCV 2005. Tenth IEEE International Conference on, volume 2, pages 1458\u20131465. IEEE,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Train faster, generalize better: Stability of stochastic gradient descent", "author": ["M. Hardt", "B. Recht", "Y. Singer"], "venue": "arXiv:1509.01240,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributional structure", "author": ["Z.S. Harris"], "venue": "Word,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1954}, {"title": "Steps toward deep kernel methods from infinite neural networks", "author": ["T. Hazan", "T. Jaakkola"], "venue": "arXiv:1508.05133,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv:1502.03167,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "What is the best multi-stage architecture for object recognition? In Computer Vision, 2009 IEEE 12th International Conference on, pages 2146\u20132153", "author": ["K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. LeCun"], "venue": "IEEE,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Random feature maps for dot product kernels", "author": ["P. Kar", "H. Karnick"], "venue": "arXiv:1201.6530,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Some connections between nonuniform and uniform complexity classes", "author": ["R.M. Karp", "R.J. Lipton"], "venue": "Proceedings of the twelfth annual ACM symposium on Theory of computing, pages 302\u2013309. ACM,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1980}, {"title": "Cryptographic limitations on learning Boolean formulae and finite automata", "author": ["M. Kearns", "L.G. Valiant"], "venue": "STOC, pages 433\u2013444, May", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1989}, {"title": "Cryptographic hardness for learning intersections of halfspaces", "author": ["A.R. Klivans", "A.A. Sherstov"], "venue": "FOCS,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2006}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1998}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, 521(7553):436\u2013444,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["O. Levy", "Y. Goldberg"], "venue": "Advances in Neural Information Processing Systems, pages 2177\u20132185,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "On the computational efficiency of training neural networks", "author": ["R. Livni", "S. Shalev-Shwartz", "O. Shamir"], "venue": "Advances in Neural Information Processing Systems, pages 855\u2013863,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional kernel networks", "author": ["J. Mairal", "P. Koniusz", "Z. Harchaoui", "Cordelia Schmid"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, pages 3111\u20133119,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Bayesian learning for neural networks, volume 118", "author": ["R.M. Neal"], "venue": "Springer Science & Business Media,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}, {"title": "Path-SGD: Path-normalized optimization in deep neural networks", "author": ["B. Neyshabur", "R. R Salakhutdinov", "N. Srebro"], "venue": "Advances in Neural Information Processing Systems, pages 2413\u20132421,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Norm-based capacity control in neural networks", "author": ["B. Neyshabur", "N. Srebro", "R. Tomioka"], "venue": "COLT,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Analysis of boolean functions", "author": ["R. O\u2019Donnell"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2014}, {"title": "Spherical random features for polynomial kernels", "author": ["J. Pennington", "F. Yu", "S. Kumar"], "venue": "Advances in Neural Information Processing Systems, pages 1837\u20131845,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "An evaluation of the invariance properties of a biologicallyinspired system for unconstrained face recognition", "author": ["N. Pinto", "D. Cox"], "venue": "Bio-Inspired Models of Network, Information, and Computing Systems, pages 505\u2013518. Springer,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2012}, {"title": "A high-throughput screening approach to discovering good forms of biologically inspired visual representation", "author": ["N. Pinto", "D. Doukhan", "J.J. DiCarlo", "D.D. Cox"], "venue": "PLoS Computational Biology, 5(11):e1000579,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2009}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "NIPS, pages 1177\u20131184,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2007}, {"title": "Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning", "author": ["A. Rahimi", "B. Recht"], "venue": "Advances in neural information processing systems, pages 1313\u20131320,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2009}, {"title": "On the quality of the initial basin in overspecified neural networks", "author": ["I. Safran", "O. Shamir"], "venue": "arxiv:1511.04210,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}, {"title": "Theory of reproducing kernels and its applications", "author": ["S. Saitoh"], "venue": "Longman Scientific & Technical England,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1988}, {"title": "On random weights and unsupervised feature learning", "author": ["A. Saxe", "P.W. Koh", "Z. Chen", "M. Bhand", "B. Suresh", "A.Y. Ng"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 1089\u20131096,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2011}, {"title": "Positive definite functions on spheres", "author": ["I.J. Schoenberg"], "venue": "Duke Mathematical Journal,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1942}, {"title": "Prior knowledge in support vector kernels", "author": ["B. Sch\u00f6lkopf", "P. Simard", "A. Smola", "Vladimir Vapnik"], "venue": "In Proceedings of the 1997 conference on Advances in neural information processing systems", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 1998}, {"title": "Provable methods for training neural networks with sparse connectivity", "author": ["H. Sedghi", "A. Anandkumar"], "venue": "arXiv:1412.2693,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2014}, {"title": "Understanding Machine Learning: From Theory to Algorithms", "author": ["S. Shalev-Shwartz", "S. Ben-David"], "venue": "Cambridge University Press,", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research, 15(1):1929\u20131958,", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in neural information processing systems, pages 3104\u20133112,", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2014}, {"title": "Computation with infinite neural networks", "author": ["C.K.I. Williams"], "venue": "pages 295\u2013301,", "citeRegEx": "56", "shortCiteRegEx": null, "year": 1997}], "referenceMentions": [{"referenceID": 30, "context": "Neural network (NN) learning has underpinned state of the art empirical results in numerous applied machine learning tasks (see for instance [31, 33]).", "startOffset": 141, "endOffset": 149}, {"referenceID": 32, "context": "Neural network (NN) learning has underpinned state of the art empirical results in numerous applied machine learning tasks (see for instance [31, 33]).", "startOffset": 141, "endOffset": 149}, {"referenceID": 48, "context": "This idea was advocated and tested empirically in [49].", "startOffset": 50, "endOffset": 54}, {"referenceID": 27, "context": "Standard results from complexity theory [28] imply that essentially all functions of interest (that is, any efficiently computable function) can be expressed by a network of moderate size.", "startOffset": 40, "endOffset": 44}, {"referenceID": 31, "context": "Biological phenomena show that many relevant functions can be expressed by even simpler networks, similar to convolutional neural networks [32] that are dominant in ML tasks today.", "startOffset": 139, "endOffset": 143}, {"referenceID": 6, "context": "Barron\u2019s theorem [7] states that even two-layer networks can express a very rich set of functions.", "startOffset": 17, "endOffset": 20}, {"referenceID": 9, "context": "As for question (ii), both classical [10, 9, 3] and more recent [40, 22] results from statistical learning theory show that as the number of examples grows in comparison to the size of the network the empirical loss must be close to the population loss.", "startOffset": 37, "endOffset": 47}, {"referenceID": 8, "context": "As for question (ii), both classical [10, 9, 3] and more recent [40, 22] results from statistical learning theory show that as the number of examples grows in comparison to the size of the network the empirical loss must be close to the population loss.", "startOffset": 37, "endOffset": 47}, {"referenceID": 2, "context": "As for question (ii), both classical [10, 9, 3] and more recent [40, 22] results from statistical learning theory show that as the number of examples grows in comparison to the size of the network the empirical loss must be close to the population loss.", "startOffset": 37, "endOffset": 47}, {"referenceID": 39, "context": "As for question (ii), both classical [10, 9, 3] and more recent [40, 22] results from statistical learning theory show that as the number of examples grows in comparison to the size of the network the empirical loss must be close to the population loss.", "startOffset": 64, "endOffset": 72}, {"referenceID": 21, "context": "As for question (ii), both classical [10, 9, 3] and more recent [40, 22] results from statistical learning theory show that as the number of examples grows in comparison to the size of the network the empirical loss must be close to the population loss.", "startOffset": 64, "endOffset": 72}, {"referenceID": 28, "context": "to depth two networks with very few hidden units, it is hard to predict even marginally better than random [29, 30, 17, 18, 16].", "startOffset": 107, "endOffset": 127}, {"referenceID": 29, "context": "to depth two networks with very few hidden units, it is hard to predict even marginally better than random [29, 30, 17, 18, 16].", "startOffset": 107, "endOffset": 127}, {"referenceID": 16, "context": "to depth two networks with very few hidden units, it is hard to predict even marginally better than random [29, 30, 17, 18, 16].", "startOffset": 107, "endOffset": 127}, {"referenceID": 17, "context": "to depth two networks with very few hidden units, it is hard to predict even marginally better than random [29, 30, 17, 18, 16].", "startOffset": 107, "endOffset": 127}, {"referenceID": 15, "context": "to depth two networks with very few hidden units, it is hard to predict even marginally better than random [29, 30, 17, 18, 16].", "startOffset": 107, "endOffset": 127}, {"referenceID": 46, "context": "Finally, we note that the recent empirical successes of NNs have prompted a surge of theoretical work around NN learning [47, 1, 4, 12, 39, 35, 19, 52, 14].", "startOffset": 121, "endOffset": 155}, {"referenceID": 0, "context": "Finally, we note that the recent empirical successes of NNs have prompted a surge of theoretical work around NN learning [47, 1, 4, 12, 39, 35, 19, 52, 14].", "startOffset": 121, "endOffset": 155}, {"referenceID": 3, "context": "Finally, we note that the recent empirical successes of NNs have prompted a surge of theoretical work around NN learning [47, 1, 4, 12, 39, 35, 19, 52, 14].", "startOffset": 121, "endOffset": 155}, {"referenceID": 11, "context": "Finally, we note that the recent empirical successes of NNs have prompted a surge of theoretical work around NN learning [47, 1, 4, 12, 39, 35, 19, 52, 14].", "startOffset": 121, "endOffset": 155}, {"referenceID": 38, "context": "Finally, we note that the recent empirical successes of NNs have prompted a surge of theoretical work around NN learning [47, 1, 4, 12, 39, 35, 19, 52, 14].", "startOffset": 121, "endOffset": 155}, {"referenceID": 34, "context": "Finally, we note that the recent empirical successes of NNs have prompted a surge of theoretical work around NN learning [47, 1, 4, 12, 39, 35, 19, 52, 14].", "startOffset": 121, "endOffset": 155}, {"referenceID": 18, "context": "Finally, we note that the recent empirical successes of NNs have prompted a surge of theoretical work around NN learning [47, 1, 4, 12, 39, 35, 19, 52, 14].", "startOffset": 121, "endOffset": 155}, {"referenceID": 51, "context": "Finally, we note that the recent empirical successes of NNs have prompted a surge of theoretical work around NN learning [47, 1, 4, 12, 39, 35, 19, 52, 14].", "startOffset": 121, "endOffset": 155}, {"referenceID": 13, "context": "Finally, we note that the recent empirical successes of NNs have prompted a surge of theoretical work around NN learning [47, 1, 4, 12, 39, 35, 19, 52, 14].", "startOffset": 121, "endOffset": 155}, {"referenceID": 50, "context": "[51], Grauman and Darrell [21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[51], Grauman and Darrell [21].", "startOffset": 26, "endOffset": 30}, {"referenceID": 35, "context": "Inspired by deep networks\u2019 success, researchers considered deep composition of kernels [36, 13, 11].", "startOffset": 87, "endOffset": 99}, {"referenceID": 12, "context": "Inspired by deep networks\u2019 success, researchers considered deep composition of kernels [36, 13, 11].", "startOffset": 87, "endOffset": 99}, {"referenceID": 10, "context": "Inspired by deep networks\u2019 success, researchers considered deep composition of kernels [36, 13, 11].", "startOffset": 87, "endOffset": 99}, {"referenceID": 45, "context": "For fully connected two-layer networks, the correspondence between kernels and neural networks with random weights has been examined in [46, 45, 38, 56].", "startOffset": 136, "endOffset": 152}, {"referenceID": 44, "context": "For fully connected two-layer networks, the correspondence between kernels and neural networks with random weights has been examined in [46, 45, 38, 56].", "startOffset": 136, "endOffset": 152}, {"referenceID": 37, "context": "For fully connected two-layer networks, the correspondence between kernels and neural networks with random weights has been examined in [46, 45, 38, 56].", "startOffset": 136, "endOffset": 152}, {"referenceID": 55, "context": "For fully connected two-layer networks, the correspondence between kernels and neural networks with random weights has been examined in [46, 45, 38, 56].", "startOffset": 136, "endOffset": 152}, {"referenceID": 45, "context": "Notably, Rahimi and Recht [46] proved a formal connection (similar to ours) for the RBF kernel.", "startOffset": 26, "endOffset": 30}, {"referenceID": 26, "context": "Their work was extended to include polynomial kernels [27, 42] as well as other kernels [6, 5].", "startOffset": 54, "endOffset": 62}, {"referenceID": 41, "context": "Their work was extended to include polynomial kernels [27, 42] as well as other kernels [6, 5].", "startOffset": 54, "endOffset": 62}, {"referenceID": 5, "context": "Their work was extended to include polynomial kernels [27, 42] as well as other kernels [6, 5].", "startOffset": 88, "endOffset": 94}, {"referenceID": 4, "context": "Their work was extended to include polynomial kernels [27, 42] as well as other kernels [6, 5].", "startOffset": 88, "endOffset": 94}, {"referenceID": 12, "context": "Several authors have further explored ways to extend this line of research to deeper, either fully-connected networks [13] or convolutional networks [24, 2, 36].", "startOffset": 118, "endOffset": 122}, {"referenceID": 23, "context": "Several authors have further explored ways to extend this line of research to deeper, either fully-connected networks [13] or convolutional networks [24, 2, 36].", "startOffset": 149, "endOffset": 160}, {"referenceID": 1, "context": "Several authors have further explored ways to extend this line of research to deeper, either fully-connected networks [13] or convolutional networks [24, 2, 36].", "startOffset": 149, "endOffset": 160}, {"referenceID": 35, "context": "Several authors have further explored ways to extend this line of research to deeper, either fully-connected networks [13] or convolutional networks [24, 2, 36].", "startOffset": 149, "endOffset": 160}, {"referenceID": 36, "context": "[37], Levy and Goldberg [34]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[37], Levy and Goldberg [34]).", "startOffset": 24, "endOffset": 28}, {"referenceID": 22, "context": "Such topology can be useful in the absence of any prior knowledge of how the output label may be computed from an input example, and it is commonly used in natural language processing where the input is represented as a bag-of-words [23].", "startOffset": 233, "endOffset": 237}, {"referenceID": 54, "context": "[55]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "= \u221a 1\u2212\u03c12+(\u03c0\u2212cos\u22121(\u03c1))\u03c1 \u03c0 arccos1 [13] Step \u221a 2 \u00b7 1[x \u2265 0] 1 2 + \u03c1 \u03c0 + \u03c1 3 6\u03c0 + 3\u03c1 5 40\u03c0 + .", "startOffset": 33, "endOffset": 37}, {"referenceID": 12, "context": "= \u03c0\u2212cos \u22121(\u03c1) \u03c0 arccos0 [13] Exponential ex\u22122 1 e + \u03c1 e + \u03c1 2 2e + \u03c1 3 6e + .", "startOffset": 24, "endOffset": 28}, {"referenceID": 35, "context": "= e\u03c1\u22121 RBF [36]", "startOffset": 11, "endOffset": 15}, {"referenceID": 47, "context": "The proofs of all the theorems we quote here are well-known and can be found in Chapter 2 of [48] and similar textbooks.", "startOffset": 93, "endOffset": 97}, {"referenceID": 49, "context": "Theorem 8 (Schoenberg, [50]).", "startOffset": 23, "endOffset": 27}, {"referenceID": 40, "context": "Finally, we use the following facts (see Chapter 11 in [41] and the relevant entry in Wikipedia):", "startOffset": 55, "endOffset": 59}, {"referenceID": 0, "context": "(h) For every \u03c3, \u03c3\u0302 is non-decreasing and convex in [0, 1].", "startOffset": 52, "endOffset": 58}, {"referenceID": 35, "context": "Note that the dual of the exponential activation was calculated in [36] and the duals of the step and the ReLU activations were calculated in [13].", "startOffset": 67, "endOffset": 71}, {"referenceID": 12, "context": "Note that the dual of the exponential activation was calculated in [36] and the duals of the step and the ReLU activations were calculated in [13].", "startOffset": 142, "endOffset": 146}, {"referenceID": 0, "context": "We first claim that there is a unique \u03b1\u03c3 \u2208 [0, 1] such that \u2200x \u2208 (\u22121, \u03b1\u03c3) , \u03c3\u0302(\u03c1) > \u03c1 and \u2200x \u2208 (\u03b1\u03c3, 1) , \u03b1\u03c3 < \u03c3\u0302(\u03c1) < \u03c1 (9)", "startOffset": 43, "endOffset": 49}, {"referenceID": 0, "context": "(b) \u03c3\u0302 is non-decreasing and convex in [0, 1]", "startOffset": 39, "endOffset": 45}, {"referenceID": 0, "context": "Hence, \u03c3\u0302(\u03c1)\u2212 \u03c1 is convex in [0, 1] and in particular intersects with the x-axis at either 0, 1, 2 or infinitely many times in [0, 1].", "startOffset": 29, "endOffset": 35}, {"referenceID": 0, "context": "Hence, \u03c3\u0302(\u03c1)\u2212 \u03c1 is convex in [0, 1] and in particular intersects with the x-axis at either 0, 1, 2 or infinitely many times in [0, 1].", "startOffset": 127, "endOffset": 133}, {"referenceID": 0, "context": "Also, since \u03c3\u0302(1) = 1, we know that there is at least one intersection in [0, 1].", "startOffset": 74, "endOffset": 80}, {"referenceID": 0, "context": "Hence, there are 1 or 2 intersections in [0, 1] and because one of them is in \u03c1 = 1, we conclude that there is at most one intersection in [0, 1).", "startOffset": 41, "endOffset": 47}, {"referenceID": 52, "context": "[53]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "The expression was already calculated in [13], yet we give here a derivation for completeness.", "startOffset": 41, "endOffset": 45}, {"referenceID": 52, "context": "wt = { w\u0303t \u2016w\u0303t\u2016 \u2264M Mw\u0303t \u2016w\u0303t\u2016 \u2016w\u0303t\u2016 > M After T = M 2C2 2 iterations the loss in expectation would be at most (see for instance Chapter 14 in [53]).", "startOffset": 143, "endOffset": 147}, {"referenceID": 7, "context": "Theorem 19 (Bartlett and Mendelson [8]).", "startOffset": 35, "endOffset": 38}, {"referenceID": 19, "context": "First, numerous researchers and practitioners demonstrated that random initialization, similar to the scheme we analyze, is crucial to the success of neural network learning (see for instance [20]).", "startOffset": 192, "endOffset": 196}, {"referenceID": 48, "context": "Second, several studies show that the contribution of optimizing the representation layers is relatively small [49, 26, 44, 43, 15].", "startOffset": 111, "endOffset": 131}, {"referenceID": 25, "context": "Second, several studies show that the contribution of optimizing the representation layers is relatively small [49, 26, 44, 43, 15].", "startOffset": 111, "endOffset": 131}, {"referenceID": 43, "context": "Second, several studies show that the contribution of optimizing the representation layers is relatively small [49, 26, 44, 43, 15].", "startOffset": 111, "endOffset": 131}, {"referenceID": 42, "context": "Second, several studies show that the contribution of optimizing the representation layers is relatively small [49, 26, 44, 43, 15].", "startOffset": 111, "endOffset": 131}, {"referenceID": 14, "context": "Second, several studies show that the contribution of optimizing the representation layers is relatively small [49, 26, 44, 43, 15].", "startOffset": 111, "endOffset": 131}, {"referenceID": 35, "context": "For example, competitive accuracy on CIFAR-10, STL-10, MNIST and MONO datasets can be achieved by optimizing merely the last layer [36, 49].", "startOffset": 131, "endOffset": 139}, {"referenceID": 48, "context": "For example, competitive accuracy on CIFAR-10, STL-10, MNIST and MONO datasets can be achieved by optimizing merely the last layer [36, 49].", "startOffset": 131, "endOffset": 139}, {"referenceID": 48, "context": "[49] show that the performance of training the last layer is quite correlated with training the entire network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "The effectiveness of optimizing solely the last layer is also manifested by the popularity of the random features paradigm [46].", "startOffset": 123, "endOffset": 127}, {"referenceID": 18, "context": "[19] demonstrated that for the MNIST and CIFAR-10 datasets the distances\u2019 histogram of different examples barely changes when moving from the initial to the trained representation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "The rationale we described above was pioneered by LeCun and colleagues [32].", "startOffset": 71, "endOffset": 75}, {"referenceID": 6, "context": "Using for example Barron\u2019s theorem [7], one may claim that visionrelated functions are expressed by fully connected two layer networks, but such networks are inferior to convolutional networks in machine vision applications.", "startOffset": 35, "endOffset": 38}, {"referenceID": 53, "context": "On the algorithmic side, it is yet to be seen whether our framework can help in understanding procedures such as dropout [54] and batch-normalization [25].", "startOffset": 121, "endOffset": 125}, {"referenceID": 24, "context": "On the algorithmic side, it is yet to be seen whether our framework can help in understanding procedures such as dropout [54] and batch-normalization [25].", "startOffset": 150, "endOffset": 154}, {"referenceID": 44, "context": "Randomly generating such embeddings can be also considered on its own, and we are currently working on design and analysis of random features a la Rahimi and Recht [45].", "startOffset": 164, "endOffset": 168}], "year": 2016, "abstractText": "We develop a general duality between neural networks and compositional kernels, striving towards a better understanding of deep learning. We show that initial representations generated by common random initializations are sufficiently rich to express all functions in the dual kernel space. Hence, though the training objective is hard to optimize in the worst case, the initial weights form a good starting point for optimization. Our dual view also reveals a pragmatic and aesthetic perspective of neural networks and underscores their expressive power. \u2217Email: amitdaniely@google.com \u2020Email: rf@cs.stanford.edu. Work performed at Google. \u2021Email: singer@google.com ar X iv :1 60 2. 05 89 7v 1 [ cs .L G ] 1 8 Fe b 20 16", "creator": "LaTeX with hyperref package"}}}