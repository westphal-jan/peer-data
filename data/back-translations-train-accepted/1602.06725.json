{"id": "1602.06725", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2016", "title": "Variational Inference for Monte Carlo Objectives", "abstract": "Recent progress in deep latent variable models has largely been driven by the development of flexible and scalable variational inference methods. Variational training of this type involves maximizing a lower bound on the log-likelihood, using samples from the variational posterior to compute the required gradients. Recently, Burda et al. (2015) have derived a tighter lower bound using a multi-sample importance sampling estimate of the likelihood and showed that optimizing it yields models that use more of their capacity and achieve higher likelihoods. This development showed the importance of such multi-sample objectives and explained the success of several related approaches.", "histories": [["v1", "Mon, 22 Feb 2016 11:06:06 GMT  (189kb,D)", "http://arxiv.org/abs/1602.06725v1", null], ["v2", "Wed, 1 Jun 2016 16:36:06 GMT  (198kb,D)", "http://arxiv.org/abs/1602.06725v2", "Appears in Proceedings of the 33rd International Conference on Machine Learning (ICML), New York, NY, USA, 2016. JMLR: W&amp;CP volume 48"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["andriy mnih", "danilo jimenez rezende"], "accepted": true, "id": "1602.06725"}, "pdf": {"name": "1602.06725.pdf", "metadata": {"source": "META", "title": "Variational inference for Monte Carlo objectives", "authors": ["Andriy Mnih", "Danilo J. Rezende"], "emails": ["AMNIH@GOOGLE.COM", "DANILOR@GOOGLE.COM"], "sections": [{"heading": null, "text": "We extend the multi-sample approach to discrete latent variables and analyze the difficulties involved in assessing the gradients involved, then develop the first unbiased gradient estimator designed for meaning-sampled targets, and evaluate it using generative and structured output prediction models. The resulting estimator, based on low-variance learning signals per sample, is both simpler and more effective than the NVIL estimator (Mnih & Gregor, 2014), which was proposed for the sample variation object and is competitive with the distorted estimators currently used."}, {"heading": "1. Introduction", "text": "This year it is so far that it will be able to erenie.n the aforementioned lcihsrcehnlrc\u00fceF"}, {"heading": "2. Multi-sample stochastic lower bounds", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Estimating the likelihood", "text": "Suppose we would like to fit a recalcitrant latent variable model P (x, h) to the data (hi, h). Since the recalcitrance of the conclusion precludes the use of a maximum probability estimate, we assume that we will maximize a lower limit of the logarithmic probability. A general way to derive such a lower limit is to start with an unbiased estimator that measures the marginal probability P (x) and then transforms it. We will consider estimators of the form I (h1: K) = 1K, K i = 1 f (x, hi) where h1,... hK are independent samples from any distribution Q (h | x) that may potentially depend on observing x. Before showing how to transform such an estimator into a boundary, we will consider some possible options for estimating the probability."}, {"heading": "2.2. Lower-bounding the log-likelihood", "text": "After selecting an estimator I (for probability), we can select an estimator L (h1: K | x) [log I (h1: K | x)] [log I (h1: K | x)] [log I (h1: K | x)] \u2264 logEQ (h1: K | x) [I (h1: K)] = logP (x), where the equality follows from the fact that I am impartial, EQ (h1: K | x) [I (h1: K)] [I (h1: K)] (h1: K)] (h1: K) (h1: K) (h1: K) as the stochastic lower limit of log probability (Burda et al., 2015).We point out that this approach is not specific to the estimators in Section 2.1 and can be used with any arbitrary estimator."}, {"heading": "2.3. Objective", "text": "Therefore, we will be interested in training models by maximizing the targets of the formula LK (x) = EQ (h1: K | x) [log 1K \u2211 i = 1 f (x, hi)], (4), which can be considered as lower limits of log probability. This class of targets is rich, including those used for variation conclusions, generative modeling, structured predictions, and hard attention."}, {"heading": "2.4. Gradient analysis", "text": "This year has seen a multiplication of the number of women in work who have chosen to pursue their profession, and a multiplication of the number of women in work who have chosen to pursue their profession."}, {"heading": "2.5. Gradient estimation", "text": "The difficulties described in the previous section relate only to the gradient for the parameters on which the sample distribution depends. In all other parameters, the first term is zero, leaving only the second, well-preserved term. Consequently, the following naive Monte Carlo estimator, based on averaging over m sets of K samples, works well: Hi-Q (h | x). It is common practice to use a single set of samples (m = 1) for all of these estimators, relying on averaging training cases in a minibatch to reduce the deviation to a reasonable level. We will also apply this approach in this paper and therefore not include a sum above m in the remaining estimate expressions. Now, we will address the more difficult problem of estimating gradients for parameters than influencing the proposal."}, {"heading": "2.5.1. NAIVE", "text": "Let us start with the simplest estimator, which is also based on naive Monte Carlo: the log f (x), (7) with hello \u0445 Q (h | x). This estimator does not attempt to eliminate one of the two sources of deviation described in Section 2.4, but only includes them here for the sake of completeness."}, {"heading": "2.5.2. WITH BASELINES (NVIL)", "text": "A simple way to reduce variance due to the large size of the learning signal is to reduce its size by subtracting a quantity called baseline, which correlates with the learning signal but is not dependent on the latent variables. In our use of baselines, we follow the method NeuralVariational Inference and Learning (NVIL, Mnih & Gregor, 2014) for the generation of generative models based on the optimization of the classical variational lower boundary (equation 3) The basic idea behind the NVIL estimator is to reduce the size of the learning signal for the variational distribution parameters (which is the singleampel counterpart of our suggestion compatible distribution) by subtracting the signal residency of two baselines of quantitative b-compatible b-x (a constant baselline b-compatible qux plus an estimator of quantitative b-compatible b-x)."}, {"heading": "2.5.3. LOCAL LEARNING SIGNALS", "text": "We can reduce the effect of the second source of the predetermined variance by defining a different local learning signal for each single predetermined breaking point that minimizes its dependence on the other diameters in the predetermined breaking point. (D) We will now show that this predetermined breaking point in the predetermined breaking point of the predetermined breaking point of the predetermined breaking point of the predetermined breaking point of the predetermined breaking point of the predetermined breaking point of the predetermined breaking point of the predetermined breaking point of the predetermined breaking point of the predetermined breaking point of the predetermined breaking point of the predetermined breaking point of the predetermined breaking point of the predetermined breaking point of the predetermined breaking point of the predetermined breaking point of the predetermined breaking point of the predetermined breaking point of the predetermined breaking point of the predetermined breaking point of the predetermined breakpoint of the predetermined breakpoint of the predetermined breaking point of the predetermined breakpoint of the predetermined breakpoint of the predetermined breakpoint of the predetermined breakpoint of the predetermined breakpoint of the predetermined breakpoint of the predetermined breakpoint of the predetermined breakpoint of the predetermined breakpoint of the predetermined breakpoint of the predetermined breakpoint of the predetermined breakpoint of the preset.L (H1: K)"}, {"heading": "3. Structured output prediction", "text": "The particular focus of SOP is on capturing the dependencies between the output variables in addition to capturing their dependence on the input factors. To stay in line with the terminology for generative models we have used so far, we will refer to input factors as contexts and to outputs as observations. Given a number of context / observation pairs (c, x), we would like to adapt a latent variable model P (x, h | c) to capture the dependencies between the contexts and the observations, as well as those between the observed dimensions."}, {"heading": "4. Related work", "text": "The idea of a multi-step approach for a multi-step, variable model was proposed by Raiko et al. (2015) The idea of a multi-step approach for a multi-step sleep model was put forward by Raiko et al. (2015) The idea of a multi-step approach for a multi-step sample model was put forward by Raiko et al. (2015) The idea of a two-step approach for a multi-step approach to optimizing sample models that requires a simple, preconceived approach for a two-step approach and shows that a simple, preconceived assessment model that relies on a self-normalized meaning to approach the posterior with a set of weighted sample types is weighted sample types. (2015) It pointed out that the multi-step object of Raiko et al. (2015) was a narrower subconceived approach for the Log Method, which relies on a narrower meaning of sample based on a sample type of weighted idea (2015)."}, {"heading": "5. Results", "text": "We evaluate the effectiveness of the proposed approach to generative modelling training models and structured output predictions. We have chosen these two tasks because they include models with hundreds of latent variables, which presents huge challenges in estimating the gradients for the application distributions. In both cases, we compare the performance of the VIMCO estimator with that of the NVIL estimator, as well as with an effective biased estimator from the literature. We experiment with the variation of the number of samples in the target and see how this affects the performance of the resulting models using different estimators. Details of the training process are included in the supplementary material."}, {"heading": "5.1. Generative modelling", "text": "We begin by applying the proposed estimator to the training of generative models that focus on sigmoid models (SBN) (Neal, 1992), which consist of layers of binary latent variables. SBNs have been used to evaluate a number of variable training methods for models with discrete latent variables (Mnih & Gregor, 2008). We use the default 50000 / 10000 split in training, and the test sets are based on the MNIST data of 28 \u00d7 28 images by Salakhutdinov (2008)."}, {"heading": "5.2. Structured output prediction", "text": "In the second half of the year, the number of working women in the US will increase by 20%, while the number of working women in the US will increase by 20%."}, {"heading": "6. Discussion", "text": "In this article, we introduced VIMCO, the first unbiased general gradient estimator designed specifically for multi-sample objects that generalize the classical lower limit of variation. By utilizing the structure of the objective function, it implements a simple and effective variance reduction without additional computational costs, eliminating the need for the learned baselines to which other general unbiased estimators such as NVIL.We demonstrated the effectiveness of VIMCO by applying it to variation training of generative and structured output prediction models.VIMCO consistently performed better than NVIL and was competitive with the biased estimators currently used. While classical variation methods may perform poorly when using an insufficiently meaningful sequence of variations, multi-stick objects offer a decent way to calculate fit quality simply by increasing the number of samples used within the objective, making the latter combination with such black box more effective."}, {"heading": "ACKNOWLEDGEMENTS", "text": "We thank Alex Graves, Guillaume Desjardins, Koray Kavukcuoglu and Volodymyr Mnih for their helpful comments."}], "references": [{"title": "Learning wake-sleep recurrent attention models", "author": ["Ba", "Jimmy", "Salakhutdinov", "Ruslan R", "Grosse", "Roger B", "Frey", "Brendan J"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2015}, {"title": "Importance weighted autoencoders", "author": ["Burda", "Yuri", "Grosse", "Roger", "Salakhutdinov", "Ruslan"], "venue": "arXiv preprint arXiv:1509.00519,", "citeRegEx": "Burda et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Burda et al\\.", "year": 2015}, {"title": "Predicting distributions with linearizing belief networks", "author": ["Dauphin", "Yann N", "Grangier", "David"], "venue": "arXiv preprint arXiv:1511.05622,", "citeRegEx": "Dauphin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2015}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Rezende", "Danilo Jimenez", "Wierstra", "Daan"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "MuProp: Unbiased backpropagation for stochastic neural networks", "author": ["Gu", "Shixiang", "Levine", "Sergey", "Sutskever", "Ilya", "Mnih", "Andriy"], "venue": "arXiv preprint arXiv:1511.05176,", "citeRegEx": "Gu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2015}, {"title": "The \"wake-sleep\" algorithm for unsupervised neural networks", "author": ["Hinton", "Geoffrey E", "Dayan", "Peter", "Frey", "Brendan J", "Neal", "Radford M"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1995}, {"title": "An introduction to variational methods for graphical models", "author": ["Jordan", "Michael I", "Ghahramani", "Zoubin", "Jaakkola", "Tommi S", "Saul", "Lawrence K"], "venue": "Machine Learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Neural variational inference and learning in belief networks", "author": ["Mnih", "Andriy", "Gregor", "Karol"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Recurrent models of visual attention", "author": ["Mnih", "Volodymyr", "Heess", "Nicolas", "Graves", "Alex"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Connectionist learning of belief networks", "author": ["Neal", "Radford M"], "venue": "Artificial intelligence,", "citeRegEx": "Neal and M.,? \\Q1992\\E", "shortCiteRegEx": "Neal and M.", "year": 1992}, {"title": "Techniques for learning binary stochastic feedforward neural networks", "author": ["Raiko", "Tapani", "Berglund", "Mathias", "Alain", "Guillaume", "Dinh", "Laurent"], "venue": null, "citeRegEx": "Raiko et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Raiko et al\\.", "year": 2015}, {"title": "Variational inference with normalizing flows", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Rezende et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "On the quantitative analysis of Deep Belief Networks", "author": ["Salakhutdinov", "Ruslan", "Murray", "Iain"], "venue": "In Proceedings of the 25th Annual International Conference on Machine Learning", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2008}, {"title": "Markov chain monte carlo and variational inference: Bridging the gap", "author": ["Salimans", "Tim", "Kingma", "Diederik P", "Welling", "Max"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Salimans et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2015}, {"title": "Learning structured output representation using deep conditional generative models", "author": ["Sohn", "Kihyuk", "Lee", "Honglak", "Yan", "Xinchen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sohn et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sohn et al\\.", "year": 2015}, {"title": "Learning stochastic feedforward neural networks", "author": ["Tang", "Yichuan", "Salakhutdinov", "Ruslan R"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Tang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2013}, {"title": "Local expectation gradients for black box variational inference", "author": ["Titsias", "Michalis", "L\u00e1zaro-Gredilla", "Miguel"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Titsias et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Titsias et al\\.", "year": 2015}, {"title": "Reinforcement learning neural turing machines", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1505.00521,", "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "Recently, Burda et al. (2015) have derived a tighter lower bound using a multi-sample importance sampling estimate of the likelihood and showed that optimizing it yields models that use more of their capacity and achieve higher likelihoods.", "startOffset": 10, "endOffset": 30}, {"referenceID": 14, "context": "These methods (Kingma & Welling, 2014; Rezende et al., 2014; Mnih & Gregor, 2014) approximate the intractable posterior of the model with a variational posterior parameterized using a neural network and maximize a lower bound on the intractable marginal log-likelihood, estimating the required gradients using samples from the variational posterior.", "startOffset": 14, "endOffset": 81}, {"referenceID": 16, "context": "The most direct route to addressing this issue to develop more expressive but still tractable variational posteriors as was done in (Salimans et al., 2015; Rezende & Mohamed, 2015; Gregor et al., 2015).", "startOffset": 132, "endOffset": 201}, {"referenceID": 3, "context": "The most direct route to addressing this issue to develop more expressive but still tractable variational posteriors as was done in (Salimans et al., 2015; Rezende & Mohamed, 2015; Gregor et al., 2015).", "startOffset": 132, "endOffset": 201}, {"referenceID": 1, "context": "However, the crippling effect of an excessively simple posterior on the model can alternatively be seen as a consequence of the form of the lower bound optimized by the variational methods (Burda et al., 2015).", "startOffset": 189, "endOffset": 209}, {"referenceID": 1, "context": "The resulting lower bound on the log-likelihood gets tighter as the number of samples increases (Burda et al., 2015), converging to the true value in the limit of infinitely many samples.", "startOffset": 96, "endOffset": 116}, {"referenceID": 1, "context": "Multi-sample objectives of this type have been used for generative modelling (Bornschein & Bengio, 2015; Burda et al., 2015), structured output prediction (Raiko et al.", "startOffset": 77, "endOffset": 124}, {"referenceID": 12, "context": ", 2015), structured output prediction (Raiko et al., 2015), and models with hard attention (Ba et al.", "startOffset": 38, "endOffset": 58}, {"referenceID": 0, "context": ", 2015), and models with hard attention (Ba et al., 2015).", "startOffset": 40, "endOffset": 57}, {"referenceID": 1, "context": "This has been empirically demonstrated in the context of generative models by Burda et al. (2015) and Bornschein & Bengio (2015), who also showed that using more samples in the objective increased the number of latent variables used in the deeper layers.", "startOffset": 78, "endOffset": 98}, {"referenceID": 1, "context": "This has been empirically demonstrated in the context of generative models by Burda et al. (2015) and Bornschein & Bengio (2015), who also showed that using more samples in the objective increased the number of latent variables used in the deeper layers.", "startOffset": 78, "endOffset": 129}, {"referenceID": 1, "context": "As a result, with the exception of (Burda et al., 2015) which used an alternative estimator available for continuous latent variables, none of the above methods update the parameters of the proposal distribution by following the gradient of the multi-sample objective.", "startOffset": 35, "endOffset": 55}, {"referenceID": 1, "context": "Therefore, we can think of L\u0302(h) = log \u00ce(h) as a stochastic lower bound on the log-likelihood (Burda et al., 2015).", "startOffset": 94, "endOffset": 114}, {"referenceID": 9, "context": "Despite the potential pitfalls described above, estimators involving sampling from the prior have been used successfully for training models for structured output prediction (Tang & Salakhutdinov, 2013; Dauphin & Grangier, 2015) and models with hard attention (Mnih et al., 2014; Zaremba & Sutskever, 2015).", "startOffset": 260, "endOffset": 306}, {"referenceID": 1, "context": "The multi-sample (K > 1) version of above estimator has recently been used in variational training of latent variable models (Burda et al., 2015; Bornschein & Bengio, 2015) as well as models with hard attention (Ba et al.", "startOffset": 125, "endOffset": 172}, {"referenceID": 0, "context": ", 2015; Bornschein & Bengio, 2015) as well as models with hard attention (Ba et al., 2015).", "startOffset": 73, "endOffset": 90}, {"referenceID": 6, "context": "The single-sample version of the estimator yields the classical variational lower bound (Jordan et al., 1999)", "startOffset": 88, "endOffset": 109}, {"referenceID": 14, "context": "which is used as the objective by much of the recent work on training generative models (Kingma & Welling, 2014; Rezende et al., 2014; Mnih & Gregor, 2014).", "startOffset": 88, "endOffset": 155}, {"referenceID": 1, "context": "The advantage of using of multi-sample stochastic lower bounds is that increasing the number of samples K is guaranteed to make the bound tighter (Burda et al., 2015), thus making it a better proxy of the log-likelihood.", "startOffset": 146, "endOffset": 166}, {"referenceID": 1, "context": "In the special case of continuous latent variables an alternative approach to gradient estimation based on reparameterization (Kingma & Welling, 2014; Burda et al., 2015) is likely to be preferable to the more general approach we follow in this paper, which is applicable to all types of latent variables.", "startOffset": 126, "endOffset": 170}, {"referenceID": 17, "context": "Here we will take the approach of viewing SOP as conditional probabilistic modelling with latent variables (Tang & Salakhutdinov, 2013; Sohn et al., 2015).", "startOffset": 107, "endOffset": 154}, {"referenceID": 12, "context": "However, historically such models have been trained using samples from the prior P (h|c), with the gradients computed using either importance sampling (Tang & Salakhutdinov, 2013) or heuristic rules for backpropagating through binary units (Raiko et al., 2015).", "startOffset": 240, "endOffset": 260}, {"referenceID": 17, "context": "Though recently variational training has been applied to SOP models with continuous latent variables (Sohn et al., 2015), we are not aware of any work that used a learned proposal distribution conditional on the observations to train SOP models with multi-sample objectives.", "startOffset": 101, "endOffset": 120}, {"referenceID": 12, "context": "Multi-sample objectives: The idea of using a multisample objective for a latent variable model was proposed by Raiko et al. (2015), who thought of it not as a lower bound on the log-likelihood but an objective in its own right.", "startOffset": 111, "endOffset": 131}, {"referenceID": 1, "context": "Burda et al. (2015) pointed out that the multi-sample objective of Raiko et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 1, "context": "Burda et al. (2015) pointed out that the multi-sample objective of Raiko et al. (2015) was a tighter lower bound on the log-likelihood that the single-sample variational lower bound and presented a method for training variational autoencoders by optimizing this multi-sample objective.", "startOffset": 0, "endOffset": 87}, {"referenceID": 1, "context": "Burda et al. (2015) have shown that the RWS gradient estimator for the model parameters is identical to the one given in by Eq.", "startOffset": 0, "endOffset": 20}, {"referenceID": 5, "context": "comes from the original Wake-Sleep algorithm (Hinton et al., 1995).", "startOffset": 45, "endOffset": 66}, {"referenceID": 4, "context": "SBNs have been used to evaluate a number of variational training methods for models with discrete latent variables (Mnih & Gregor, 2014; Bornschein & Bengio, 2015; Gu et al., 2015).", "startOffset": 115, "endOffset": 180}, {"referenceID": 11, "context": "We chose a task that has been used as a benchmark for evaluating gradient estimators for models with binary latent variables by Raiko et al. (2015) and Gu et al.", "startOffset": 128, "endOffset": 148}, {"referenceID": 4, "context": "(2015) and Gu et al. (2015), which involves predicting the lower half of an MNIST digit from its top half.", "startOffset": 11, "endOffset": 28}, {"referenceID": 12, "context": "Finally, to compare to the results of Raiko et al. (2015), we followed their evaluation protocol and estimated the negative log-likelihoods for the trained models using 100 latent samples.", "startOffset": 38, "endOffset": 58}], "year": 2017, "abstractText": "Recent progress in deep latent variable models has largely been driven by the development of flexible and scalable variational inference methods. Variational training of this type involves maximizing a lower bound on the log-likelihood, using samples from the variational posterior to compute the required gradients. Recently, Burda et al. (2015) have derived a tighter lower bound using a multi-sample importance sampling estimate of the likelihood and showed that optimizing it yields models that use more of their capacity and achieve higher likelihoods. This development showed the importance of such multisample objectives and explained the success of several related approaches. We extend the multi-sample approach to discrete latent variables and analyze the difficulty encountered when estimating the gradients involved. We then develop the first unbiased gradient estimator designed for importance-sampled objectives and evaluate it at training generative and structured output prediction models. The resulting estimator, which is based on low-variance per-sample learning signals, is both simpler and more effective than the NVIL estimator (Mnih & Gregor, 2014) proposed for the single-sample variational objective, and is competitive with the currently used biased estimators.", "creator": "LaTeX with hyperref package"}}}