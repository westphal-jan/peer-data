{"id": "1602.06872", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2016", "title": "Principal Component Projection Without Principal Component Analysis", "abstract": "We show how to efficiently project a vector onto the top principal components of a matrix, without explicitly computing these components. Specifically, we introduce an iterative algorithm that provably computes the projection using few calls to any black-box routine for ridge regression.", "histories": [["v1", "Mon, 22 Feb 2016 17:52:02 GMT  (304kb)", "http://arxiv.org/abs/1602.06872v1", null]], "reviews": [], "SUBJECTS": "cs.DS cs.LG stat.ML", "authors": ["roy frostig", "cameron musco", "christopher musco", "aaron sidford"], "accepted": true, "id": "1602.06872"}, "pdf": {"name": "1602.06872.pdf", "metadata": {"source": "CRF", "title": "Principal Component Projection Without Principal Component Analysis", "authors": ["Roy Frostig", "Christopher Musco"], "emails": ["rf@cs.stanford.edu", "cnmusco@mit.edu", "cpmusco@mit.edu", "asid@microsoft.com"], "sections": [{"heading": null, "text": "ar Xiv: 160 2.06 872v 1 [cs.D S] By avoiding explicit major component analysis (PCA), our algorithm is the first without runtime dependence on the number of major components at the top. We show that it can be used to provide a fast iterative method for the popular major component regression problem, which is the first major runtime improvement over the na\u00efve method of combining PCA with regression. To achieve our results, we first observe that grate regression can be used to obtain a \"smooth projection\" on the major components at the top. We then sharpen this approach to real projection by using a low-grade polynomic approach to the matrix step function. Approximation of step functions is a topic of long-term interest in scientific computing. We extend the earlier theory by constructing polynomials with simple iterative structure and analyzing their precise behavior under limited precision."}, {"heading": "1 Introduction", "text": "In machine learning and statistics, it is common - and often indispensable - to present data in a precise form that reduces noise and increases the efficiency of downstream tasks. Perhaps the most common method to do this is to project data onto the linear subspace, which is enveloped by its directions of highest variance - that is, the range of the uppermost major components specified by the main component analysis (PCA). Calculation of the main components can be a costly task, a challenge that raises a fundamental algorithmic question: Can we project a vector onto the span of the uppermost major components of a matrix without performing the main component analysis? This work affirms this question and shows that projection is much easier than PCA itself. We show that it can be solved with a simple iterative algorithm that is based on black box calls to a ridge component projection routine, which is not based on the number of runtime of the algorithm for each component, or even the number of the uppermost algorithm."}, {"heading": "1.1 Motivation: principal component regression", "text": "To motivate our projection problem, we consider one of the most basic downstream applications for PCA: linear regression. Combined, PCA and regression form the problem of principal component regression (PCR): Definition 1.1 (Principal Component Regression (PCR). Let A-Rn \u00b7 d be a design matrix whose rows are data points, and let b-Rd be a vector of data labels. Let A\u03bb denote the result of the projection of each row of A onto the span of the uppermost major components of A - in particular the eigenvectors of covariance matrix 1nA TA, whose corresponding variance (eigenvalue) exceeds a threshold. The task of the PCR is to find a minimizer of quadratic loss when it reduces the quadratic loss of A-b-B-22. In other words, the goal is to calculate \u2020 illusory as the calculation of pseudo-algorithms in which A is based."}, {"heading": "1.2 A first approximation: ridge regression", "text": "Interestingly, our main approach to the efficient projection of the main components is based on a common alternative to PCR: comb regression. This ubiquitous regulation method calculates a minimizer of comb regression that can be efficiently solved by many techniques (see Lemma 2.1). The solution of comb regression corresponds to the application of the matrix (ATA + \u03bbI) \u2212 1AT, an operation that can be regarded as a gentle loosening of PCR. Adding comb punishment (i.e. AI) \"washes\" A's small main components compared to their large ones and achieves an effect similar to PCR at the extreme ends of spectroscopy A. Accordingly, comb regression is significantly greater than a projection. \""}, {"heading": "1.3 Main result: from ridge regression to projection", "text": "We show that it is possible to sharpen the weak approximation given by the ridge regression. Specifically, there is a low degree polynomial p (\u00b7), so that p ((ATA + \u03bbI) \u2212 1ATA) y provides a very precise approximation for each vector y. Furthermore, the polynomial can be evaluated as a recursion, which translates into a simple iterative algorithm: We can apply the sharpened approximation to a vector by repeatedly applying each ridge regression routine a small number of times.Theorem 1,2 (main component projection without PCA)."}, {"heading": "1.4 Related work", "text": "It has been shown that an approach to A\u03bb is sufficient to solve the regression problem [CH90, BMI14]. Unfortunately, even the fastest approaches are much slower than burr regression routines and inherently result in a linear dependence on the number of major components above \u03bb.Closely related to our approach is work on the Thematrix Signing Function, an important operation in control theory, quantum chromodynamics, and scientific computing in general.The approach of the sign function often involves matrix polynomials that resemble our \"sharpening polynomial,\" which converts burr regression into the main component projection. Significant effort is devoted to crylov methods for applying such operators, without explicitly calculating them [vdEFL + 02, FS08].Our work differs from these methods in a significant way: since we only assume access to an approximate drawing function is non-refractable for our postregression, it is a non-refractable feature."}, {"heading": "1.5 Paper layout", "text": "Section 2: Mathematical and algorithmic preparatory work. Section 3: Development of a PCA-free algorithm for the main component projection based on a comb regression subroutine. Section 4: Show how our approximate projection algorithm can be used to solve PCR, also without PCA. Section 5: Detailed our iterative approach to sharpening the smooth comb regression projection towards a real projection using a polynomial. Section 6: Empirical evaluation of our main component projection and regression algorithms."}, {"heading": "2 Preliminaries", "text": "The columns of U and V are the individual vectors of A and V in decreasing order. The columns of V are the eigenvectors of the covariance matrix ATA - that is, the main components of the data - and the eigenvalues of the covariance matrix. The eigenvalues of the covariance matrix ATA - and these are the main components of the data - and the eigenvalues of the covariance matrix are the eigenvalues of the covariance matrix."}, {"heading": "3 From ridge regression to principal component projection", "text": "s leave Bx = (ATA + \u03bbI) \u2212 1 (ATA) x the result of applying the ridge regression to (ATA) x.In the functional language of matrices we have B = r (ATA), wherer (x) def = xx +.The function r (x) is a gentle step over \u03bb (see Figure 1).It is primarily used to map the eigenvalues of ATA to the range [0, 1], with those that exceed the threshold being set to a value above 1 / 2 and the rest to a value below 1 / 2. To approximate the projection PA\u03bb (see Figure 1), it would now suffice to apply a simple symmetric step function: s (x) = < 1 / 2 if true, if it is 1 / 1, if it is equal to x."}, {"heading": "3.1 Polynomial approximation to the step function", "text": "We also show how to apply this polynomial efficiently and steadily with a simple iterative algorithm. Our main result, as demonstrated in Section 5, is: Lemma 3.1 (Step Function Algorithm). Let's name a procedure that produces at x-level A (x) with x-level A (x). \u2212 Sx-Sx-Sx-Sp-S2 = O (2). \u2212 Sx-S2 = O (2). \u2212 Sp-Sp-Sp-S2 = O-S2). \u2212 Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp."}, {"heading": "3.2 Choosing \u03bb and \u03b3", "text": "Theorem 3.2 requires \u03c3k + 1 (A) 21 \u2212 4\u03b3 \u2264 \u03bb \u2264 (1 \u2212 4\u03b3) \u03c3k (A) 2. If \u03bb is chosen at approximately the same distance from the two eigenvalues, we need \u03b3 = O (1 \u2212 \u03c32k + 1 (A) / \u03c32k (A)). In practice, however, it is unnecessary to specify \u03b3 explicitly or to select it so precisely. With q = O (\u03b3 \u2212 2 log (1 / 2)), our projection of all singular values outside the range [(1 \u2212 \u03b3), (1 + \u03b3) \u03bb will be approximately correct. If there are any \"intermediate values\" in this range, as shown in section 5, the approximation step function applied by Lemma 3.1 will map these values to [0, 1] via a monotonously increasing soft step. That is, algorithm 1 gives a slightly softened projection - by cancelling all main directions with value < (1 \u2212 3), with each value (1) within a distance to each other."}, {"heading": "4 From principal component projection to principal component", "text": "RegressionAn important motivation for an efficient, PCA-free method of projecting a vector onto the span of the main components is the principal component regression (PCR). Remember that PCR solves the following problem: A \u2020 \u03bbb = argmin x, Rd, A\u03bbx \u2212 b, 22. In precise arithmetic, A \u2020 \u03bbb is equal to (A TA) \u2212 1PA\u03bbATb. This identity suggests a method for calculating the solution of crest regression without explicitly finding A\u03bb: first apply a main component projection algorithm to ATb and then apply a linear system (ATA) \u2212 1. Unfortunately, this approach is catastrophically unstable, not only when PA\u03bb is applied approximately, but in any finite precision environment. Accordingly, we present a modified method to obtain PCA-free regression from projection."}, {"heading": "4.1 Stable inversion via ridge regression", "text": "The problem with the first mentioned approach is that since (ATA) \u2212 1 on y \u2212 1, a very large maximum eigenvalue (ATA) \u2212 1y (ATA) \u2212 1y (ATA) \u2212 1y (ATA) \u2212 1y (ATA) \u2212 1y (ATA) \u2212 1y (ATA) \u2212 1y (ATA) \u2212 1y (ATA) \u2212 1y (ATA) \u2212 1y (ATA) \u2212 1y (ATA) \u2212 1y (ATA) \u2212 1y (ATA) \u2212 1y (ATA) \u2212 1y (ATA) \u2212 1y (ATA) y (ATA) \u2212 1y (ATA) y (ATA) \u2212 1y (ATA) y (ATA) y (ATA) \u2212 1y (ATA), where f (x) = 1x), but the target in PCR is to apply (ATA) \u2212 1 = h (ATA), where h (x) = 1 / x. So to switch from one function to another, we use a correction (x) (ATA) = 1 \u2212 1."}, {"heading": "5 Approximating the matrix step function", "text": "We now return to the proof of our underlying result on iterative polynomial approximation of the matrix step function: Lemma 3.1 (step function algorithm) Let S-Rd \u00b7 d be symmetrical with each eigenvalue \u03c3 satisfactory."}, {"heading": "5.1 Polynomial approximation to the sign function", "text": "We show that for sufficiently large k, the following polynomial is uniformly close to sgn (x) (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x) x x (x) x (x) x (x) x x x x (x) x x x x x x (x) x x x x x (x) x x x x) x x x x x (x) x x x x x x x) x (x) x x (x) x x x x x x) x x x x (x) x x x (x) x x x x x x (x) x x x x x) x x x (x) x x x x x x x x (x) x x x x x x x x x) x x x x x (x) x x x x x x (x) x x x x x x x) x x x x x x x x x x (x) x x x (x) x x x x x x x x x x x x) x x x x x x x x x (x) x x x x x (x) x x x x x x x x x x x x x x x x x (x) x x x x x x x x x x x x x (x) x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x (x) x x x x x x) x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "5.2 Stable iterative algorithm for the sign function", "text": "We now provide an iterative algorithm for calculating pk (x), which is applied with limited precision. We obtain our formula by taking into account each term of pk (x). Lettk (x) def = x (1 \u2212 x2) and therefore we can calculate tk (x) = 12j \u2212 1 2j. Clearly tk + 1 (x) = tk (x) = tk (1 \u2212 x2) iteratively just as well. We show that this method works when applied to matrices, even if all operations are performed with limited precision. Let us leave B (x) = x) = x x x x x x. Let us be symmetrical with B (x)."}, {"heading": "5.3 Approximating the step function", "text": "Finally, we apply the results of section 5.1 and section 5.2 to approximate the step function and prove that lemmas (q) (q) (x) (x) (1 / 2) (1 + sgn (2x \u2212 1) and perform further error analysis. We first use lemmas 5.6 to show how to calculate (1 / 2) (1 + pk (2x \u2212 1). lemmas 5.7. Let S \u2212 2d be symmetrical with 0 S I. Let A be an approach based on x) with A (x) (x) \u2212 Sx (2) \u2212 Sx (2) x). Given arbitrary y \u2212 Rd sentence s0: = A (y), w0 \u2212 s0 \u2212 (1 / 2) y, and for all k = 0 setwk + 1: = 4 (2k)."}, {"heading": "6 Empirical evaluation", "text": "We conclude with an empirical evaluation of PC-proc and First-pcr (algorithms 1 and 2). Since PCR has already been justified as a statistical technique, we focus on showing that the algorithm can achieve an exact approximation to A \u2020 \u03bbb and PA\u03bby with few iterations. We start with synthetic data that allow us to control the spectral gap that dominates our iteration boundaries (see theorem 3.2).The data is generated randomly by collecting uppermost singular values uniformly from the range [.5 (1 + \u03b3), 1] and posterior singular values from [0,.5 (1 \u2212 \u03b3)].The total sum is set to.5 and A is formed via the SVD U\u0445VT, where U and V are random orthonormatrices containing our random singular values."}, {"heading": "A The matrix step function", "text": "We also prove that we have the existence of one even lower polynome to sgn (x).Lemma 5.2. For all x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-"}, {"heading": "B Principal component regression", "text": "Finally, we prove Lemma 4.1, the main result behind our algorithm, to convert the main component into PCR algorithm. The proof consists of two parts. First, that we provide an error analysis of our iterative method for calculating this polynomial. We start with a very basic polynomial approximation, that of Lemma B.1. Let g (x) def = x1 \u2212 x and pk (x) def = x x x x x. For x-x-x-x values we have: g (x) \u2212 pk values, which we can extend."}], "references": [{"title": "In Proceedings of the 2014 IEEE International Symposium on Information Theory (ISIT)", "author": ["Christos Boutsidis", "Malik Magdon-Ismail. Faster SVD-truncated regularized least-squares"], "venue": "pages 1321\u20131325,", "citeRegEx": "BMI14", "shortCiteRegEx": null, "year": 2014}, {"title": "Computing truncated singular value decomposition least squares solutions by rank revealing qr-factorizations", "author": ["Tony F. Chan", "Per Christian Hansen"], "venue": "SIAM Journal on Scientific and Statistical Computing,", "citeRegEx": "Chan and Hansen.,? \\Q1990\\E", "shortCiteRegEx": "Chan and Hansen.", "year": 1990}, {"title": "In Proceedings of the 6th Conference on Innovations in Theoretical Computer Science (ITCS)", "author": ["Michael B. Cohen", "Yin Tat Lee", "Cameron Musco", "Christopher Musco", "Richard Peng", "Aaron Sidford. Uniform sampling for matrix approximation"], "venue": "pages 181\u2013190,", "citeRegEx": "CLM15", "shortCiteRegEx": null, "year": 2015}, {"title": "The Journal of Machine Learning Research", "author": ["Paramveer S. Dhillon", "Dean P. Foster", "Sham M. Kakade", "Lyle H. Ungar. A risk comparison of ordinary least squares vs ridge regression"], "venue": "14(1):1505\u20131511,", "citeRegEx": "DFKU13", "shortCiteRegEx": null, "year": 2013}, {"title": "Uniform approximation of sgn(x) by polynomials and entire functions", "author": ["Alexandre Eremenko", "Peter Yuditskii"], "venue": "Journal d\u2019Analyse Mathmatique,", "citeRegEx": "Eremenko and Yuditskii.,? \\Q2007\\E", "shortCiteRegEx": "Eremenko and Yuditskii.", "year": 2007}, {"title": "Polynomials of the best uniform approximation to sgn(x) on two intervals", "author": ["Alexandre Eremenko", "Peter Yuditskii"], "venue": "Journal d\u2019Analyse Mathe\u0301matique,", "citeRegEx": "Eremenko and Yuditskii.,? \\Q2011\\E", "shortCiteRegEx": "Eremenko and Yuditskii.", "year": 2011}, {"title": "A statistical view of some chemometrics regression", "author": ["Ildiko E. Frank", "Jerome H. Friedman"], "venue": "tools. Technometrics,", "citeRegEx": "Frank and Friedman.,? \\Q1993\\E", "shortCiteRegEx": "Frank and Friedman.", "year": 1993}, {"title": "Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization", "author": ["Roy Frostig", "Rong Ge", "Sham M. Kakade", "Aaron Sidford"], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML),", "citeRegEx": "FGKS15", "shortCiteRegEx": null, "year": 2015}, {"title": "Model Order Reduction: Theory, Research Aspects and Applications, chapter Matrix Functions, pages 275\u2013303", "author": ["Andreas Frommer", "Valeria Simoncini"], "venue": null, "citeRegEx": "Frommer and Simoncini.,? \\Q2008\\E", "shortCiteRegEx": "Frommer and Simoncini.", "year": 2008}, {"title": "BIT Numerical Mathematics", "author": ["Per Christian Hansen. The truncated SVD as a method for regularization"], "venue": "27(4):534\u2013553,", "citeRegEx": "Han87", "shortCiteRegEx": null, "year": 1987}, {"title": "Functions of Matrices: Theory and Computation", "author": ["Nicholas J. Higham"], "venue": "Society for Industrial and Applied Mathematics,", "citeRegEx": "Higham.,? \\Q2008\\E", "shortCiteRegEx": "Higham.", "year": 2008}, {"title": "The relations of the newer multivariate statistical methods to factor analysis", "author": ["Harold Hotelling"], "venue": "British Journal of Statistical Psychology,", "citeRegEx": "Hotelling.,? \\Q1957\\E", "shortCiteRegEx": "Hotelling.", "year": 1957}, {"title": "Burges", "author": ["Yann LeCun", "Corinna Cortes", "Christopher J.C"], "venue": "MNIST handwritten digit database.", "citeRegEx": "LCB15", "shortCiteRegEx": null, "year": 2015}, {"title": "In Advances in Neural Information Processing Systems 27 (NIPS)", "author": ["Qihang Lin", "Zhaosong Lu", "Lin Xiao. An accelerated proximal coordinate gradient method"], "venue": "pages 3059\u20133067,", "citeRegEx": "LLX14", "shortCiteRegEx": null, "year": 2014}, {"title": "A method for unconstrained convex minimization problem with the rate of convergence o(1/k)", "author": ["Yurii Nesterov"], "venue": "In Soviet Mathematics Doklady,", "citeRegEx": "Nesterov.,? \\Q1983\\E", "shortCiteRegEx": "Nesterov.", "year": 1983}, {"title": "OSNAP: Faster numerical linear algebra algorithms via sparser subspace embeddings", "author": ["Jelani Nelson", "Huy L. Nguy\u00ean"], "venue": "In Proceedings of the 54th Annual IEEE Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "Nelson and Nguy\u00ean.,? \\Q2013\\E", "shortCiteRegEx": "Nelson and Nguy\u00ean.", "year": 2013}, {"title": "Mathematical Programming", "author": ["Shai Shalev-Shwartz", "Tong Zhang. Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization"], "venue": "pages 1\u201341,", "citeRegEx": "SSZ14", "shortCiteRegEx": null, "year": 2014}, {"title": "Faster algorithms via approximation theory", "author": ["Sushant Sachdeva", "Nisheeth K. Vishnoi"], "venue": "Foundations and Trends in Theoretical Computer Science,", "citeRegEx": "Sachdeva and Vishnoi.,? \\Q2014\\E", "shortCiteRegEx": "Sachdeva and Vishnoi.", "year": 2014}, {"title": "Solution of incorrectly formulated problems and the regularization method", "author": ["Andrey Tikhonov"], "venue": "In Soviet Mathematics Doklady,", "citeRegEx": "Tikhonov.,? \\Q1963\\E", "shortCiteRegEx": "Tikhonov.", "year": 1963}, {"title": "Numerical methods for the QCD overlap operator I: Sign-function and error bounds", "author": ["Jasper van den Eshof", "Andreas Frommer", "Thomas Lippert", "Klaus Schilling", "Henk A. van der Vorst"], "venue": "Computer physics communications, 146(2):203\u2013224,", "citeRegEx": "vdEFL02", "shortCiteRegEx": null, "year": 2002}], "referenceMentions": [{"referenceID": 3, "context": "Unfortunately, ridge regression is a very crude approximation to PCR and projection in many settings and may perform significantly worse in certain data analysis applications [DFKU13].", "startOffset": 175, "endOffset": 183}, {"referenceID": 12, "context": "Finally, we consider a large regression problem constructed from MNIST classification data [LCB15], with the goal of distinguishing handwritten digits {1,2,4,5,7} from the rest.", "startOffset": 91, "endOffset": 98}], "year": 2016, "abstractText": "We show how to efficiently project a vector onto the top principal components of a matrix, without explicitly computing these components. Specifically, we introduce an iterative algorithm that provably computes the projection using few calls to any black-box routine for ridge regression. By avoiding explicit principal component analysis (PCA), our algorithm is the first with no runtime dependence on the number of top principal components. We show that it can be used to give a fast iterative method for the popular principal component regression problem, giving the first major runtime improvement over the naive method of combining PCA with regression. To achieve our results, we first observe that ridge regression can be used to obtain a \u201csmooth projection\u201d onto the top principal components. We then sharpen this approximation to true projection using a low-degree polynomial approximation to the matrix step function. Step function approximation is a topic of long-term interest in scientific computing. We extend prior theory by constructing polynomials with simple iterative structure and rigorously analyzing their behavior under limited precision.", "creator": "LaTeX with hyperref package"}}}