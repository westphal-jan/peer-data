{"id": "1602.07415", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2016", "title": "Ensuring Rapid Mixing and Low Bias for Asynchronous Gibbs Sampling", "abstract": "Gibbs sampling is a Markov Chain Monte Carlo technique commonly used for estimating marginal distributions. To speed up Gibbs sampling, there has recently been interest in parallelizing it by executing asynchronously. While empirical results suggest that many models can be efficiently sampled asynchronously, traditional Markov chain analysis does not apply to the asynchronous case, and thus asynchronous Gibbs sampling is poorly understood. In this paper, we derive a better understanding of the two main challenges of asynchronous Gibbs: sampling bias and mixing time. We show experimentally that our theoretical results match practical outcomes.", "histories": [["v1", "Wed, 24 Feb 2016 06:54:43 GMT  (57kb)", "http://arxiv.org/abs/1602.07415v1", null], ["v2", "Mon, 30 May 2016 21:19:52 GMT  (58kb)", "http://arxiv.org/abs/1602.07415v2", null], ["v3", "Thu, 16 Jun 2016 20:55:19 GMT  (58kb)", "http://arxiv.org/abs/1602.07415v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["christopher de sa", "christopher r\u00e9", "kunle olukotun"], "accepted": true, "id": "1602.07415"}, "pdf": {"name": "1602.07415.pdf", "metadata": {"source": "CRF", "title": "Ensuring Rapid Mixing and Low Bias for Asynchronous Gibbs Sampling", "authors": ["Christopher De Sa", "Kunle Olukotun"], "emails": ["cdesa@stanford.edu", "kunle@stanford.edu", "chrismre@stanford.edu"], "sections": [{"heading": null, "text": "ar Xiv: 160 2.07 415v 1 [cs.L G] 24 Fe"}, {"heading": "1 Introduction", "text": "Algorithm 1 Gibbs sampling request: Variables xi for 1 \u2264 i \u2264 n, and distribution problems in which we try to estimate the marginal probabilities of some query events in a given distribution.Algorithm 1 Gibbs sampling request: Variables xi for 1 \u2264 i \u2264 n, and distribution problems in which we try to estimate the marginal probabilities of some query events in a given distribution.Algorithm 1 Gibbs sampling request: Variables xi for 1 \u2264 i \u2264 n, and distribution \u03c0.for t = 1 to T do Sample s uniformly from {1, n}."}, {"heading": "2 Related Work", "text": "A simple way to parallelise Gibbs sampling is to run multiple chains independently: this heuristic analysis uses parallelism to produce more samples overall, but does not produce more accurate samples faster. Furthermore, this strategy is sometimes inferior to other system-level strategies Smola & Narayanamurthy (2010); Zhang & Re \"(2014), typically because it requires extra memory to maintain multiple models of the chain; another strategy for parallelizing Gibbs sampling involves exploiting the structure of the underlying graphics to run in parallel, while still maintaining an execution pattern to which standard sequential Gibbs sampling analysis can be applied. (2011) Much further work has focused on parallelizing problems, such as LDA Newman al. (2007); Smola & Narayanamurthy et al."}, {"heading": "3 Modeling Asynchronicity", "text": "In this section, we describe a statistical model for asynchronous Gibbs sampling by adapting the hardware model outlined in De Sa et al. (2015a) Since we are motivated by the graph inference problem, we will focus on the case where the distribution \u03c0 we want to pitch is based on a sparse, discrete graphical model. (Each HOGWILD! -Gibbs implements a number of strings, each of which repeatedly updates the Gibbs rule to a single copy of the model (typically stored in RAM). We assume that this model serializes all fonts so that we can talk about the state of the system after it is written. We call this time t, and we model the HOGWILD system as a stochastic process adapted to the natural filtration Ft - Ft containing all events that have occurred up to time, and we say an event is measurable Ft when it is known by time.We begin our construction by denying Lxi."}, {"heading": "4 The First Challenge: Bias", "text": "In order to measure the convergence of the Markov chains to their stationary distribution, it is standard to use the total variation distance. Definition 1 (Total variation distance): The total variation distance (Levin et al., 2009, p. 48) between two probability metrics \"\u00b5\" and \"\u03bd\" on probability space \"is defined as the maximum difference between the probabilities attributed to a single event. It is a well-known result that for Gibbs sampling, based on a strictly positive target distribution figure, the maximum difference between the probabilities attributed to\" \u00b5 \"and\" \u03bd \"to a single event applies.\" It is one of the difficulties that arise when applying \"HOGWILD!\" to an example that takes bias into account - To understand the bias of the samples, we can add the \"1.\""}, {"heading": "4.1 Bias Example", "text": "Consider a simple model with two variables X1 and X2, each taking values in {0, 1} and a distributionp (0, 1) = p (1, 0) = p (1, 1) = 13 p (0, 0) = 0.Sequential Gibbs sampling in this model yields unbiased samples from the target distribution. Unfortunately, this is not the case when we perform HOGWILD! Gibbs sampling in this model. Suppose that the state is currently (1, 1) and two threads, T1 and T2, simultaneously update X1 and X2. Since T1 reads the state (1, 0), it updates X1 to 0 or 1 each with a 0.5 probability; the same is true for T2 and X2. Thereafter, each state will have probability 0.25; this includes the state (0, 0) that should never occur! Over time, this race condition samples with a value (0, 0) with a certain probability of bias in this example will be almost biased by the two biases:"}, {"heading": "4.2 Bounding the Bias", "text": "The previous example has shown that asynchronous Gibbs variations will not necessarily produce a sequence of samples that come arbitrarily close to the target distribution. Instead, the samples will approach a different distribution that we hope will be sufficiently similar for some practical purposes. Often, the purpose of the sample is too high to estimate the marginal distributions of individual variables or events that each depend on only a small number of variables in the model. To characterize the accuracy of these estimates, the total variation distance is too conservative: it depends on the difference across all events in space when most of them are events that do not interest us. To address this, we present the following definition.Definition 2 (Sparse Variation Distance)."}, {"heading": "5 The Second Challenge: Mixing Times", "text": "Although the HOGWILD! Gibbs sampler provides distorted estimates, it is still interesting to analyze how long it takes to run it before the samples it produces are independent of its initial conditions. To measure the efficiency of a Markov chain, it is standard to use the mixing time, the time it takes to produce samples that are close to the stationary distribution in terms of the total distance of variation. Definition 5 (Mixing time): The mixing time (Levin et al., 2009, p. 55) of a stochastic process with the distribution of t to time t and the stationary distribution to time t is the first time that the estimated distribution at an initial distribution of \u00b50 is within the TV distance of \u03c0."}, {"heading": "5.1 Mixing Time Example", "text": "As we have done with bias, here we construct an example model for which asynchronous execution catastrophically increases the mixing time. (The model we will construct is quite extreme; we choose this model because simpler, practical models do not seem to have this kind of catastrophic increase in the mixing time. We start for some very large constant N, with N variables X1,., XN all in {\u2212 1, 1}, and a factor with energy sequence X (X) = \u2212 M1, 1 TX. \"For some very large energy parameters M1. The resulting distribution will be almost uniform across all states with 1 TX, 1}. To this model we add another bank of variables Y1,., YN all in {\u2212 1, 1}. These variables also have a single associated factor with energy sources Y (X, Y) = {\u03b2 N (1 TY) 2if."}, {"heading": "5.2 Bounding the Mixing Time", "text": "This example illustrates that fast mixing of the sequential sampler alone is not enough to guarantee fast mixing of the HOGWILD! chain. Consequently, we are trying to look for models for which we can say something about the mixing time of both the sequential and the HOGWILD! Gibbs. Dobrushin's state famously implies fast mixing of the sequential Gibbs, and it turns out that we can use them here too to bind the mixing time of the asynchronous samplers. Theory 4. Let us assume that we perform Gibbs on a distribution that satisfies Dobrushin's state, \u03b1 < 1. Then the mixing time of sequential Gibbs is made by mixing \u2212 seq \u2212 seq \u2012 n 1 \u2212 \u03b1 log (n)."}, {"heading": "5.3 A Positive Example: Ising Model", "text": "To get an intuition here, let's consider a simple example: The Ising Model (1925) on a graph G = (V, E) is a model over the probability space {\u2212 1, 1} V and has distributionp (\u03c3) = 1Z exp (\u03b2 \u2211 (x, y) and E\u03c3 (x) \u03c3 (y) + \u2211 x-VBx\u03c3 (x), where \u03b2 is a parameter designated as a reverse temperature, the Bx are parameters that encode a value before the variables, and Z is the normalization constant necessary for it to be a distribution. For maximum-degree diagrams and sufficiently small \u03b2, a limitation of the mixing time of Gibbs Sampling is known if it turns out that the total influence of the Ising Model can be limited by a value beyond that, and so this condition is simply another way of describing Dobrushin's state."}, {"heading": "5.4 Proof Outline", "text": "Here we briefly describe the technique used to prove theorem 4; for the sake of simplicity, we will focus on the case in which each variable includes values in {\u2212 1, 1}. We begin by introducing the idea of a coupling-based argument (Levin et al., 2009, p. 64). The argument begins by constructing two copies of the same Markov chain, X and X, starting from different states but converging in the same probability space (i.e. using the same sources of randomness). To analyze HOGWILD! Gibbs scan, we divide the randomness by having both chains scan the same variable at each iteration and scan it so that the resulting values correlate to the maximum - in addition, both chains are subject to the same HOGWILD! distortion. At a random time called coupling time Tc, the chains become equal, regardless of their initial conditions."}, {"heading": "6 Experiments", "text": "Now that we have derived a theoretical characterization of the behavior of the HOGWILD technology, we will investigate whether this characterization is maintained under experimental evaluations. First, we are investigating the mixing time claims we made in Section 5. Specifically, we want to verify whether the increase in the expected delay parameters Celsius actually increases the mixing time, as in Equation 2. To do this, we are simulating the mixing time claims we made in Section 5. Specifically, we want to verify whether the increase in the expected delay parameters Celsius = 0.2, and the previous weights Ex = 0. This model has a total impact on the total time of 0.6, and Theorem 4 guarantees that it will mingle quickly. Unfortunately, the mixing time of a chain is difficult to calculate experimentally. While techniques such as the coupling from the past Propp & Wilson (1996) exist to show the mixing time using these techniques to reveal the dependence of the mixing time."}, {"heading": "7 Conclusion", "text": "We analyzed the sampling of HOGWILD! -Gibbs, a heuristics for parallel MCMC sampling, using discreetly evaluated graphical models. First, we constructed a statistical model for HOGWILD! -Gibbs by adapting a model already used for the analysis of asynchronous SGD. Next, we illustrated an important problem with the sampling of HOGWILD! -Gibbs: that it produces distorted samples. To solve this problem, we demonstrated that for some classes of models with limited overall impact, only O (n) sequential Gibbs samples are needed to provide good limit estimates (in terms of small variation spacing), then the sampling of HOGWILD! -Gibbs delivers equally good estimates after only O (1) additional steps. Additionally, we demonstrated that in models that meet Dobrushin's state (\u03b1 < 1) we have time limits for sequential and GILD + gibbons (the GILD + WILD) to mix only."}, {"heading": "Acknowledgments", "text": "The authors acknowledge their support for: DARPA FA8750-12-2-0335; NSF IIS-1247701; NSF CCF-1111943; DOE 108845; NSF CCF-1337375; DARPA FA8750-13-2-0039; NSF IIS-1353606; ONR N000141210041 and N000141310129; NIH U54EB020405; Oracle; NVIDIA; Huawei; SAP Labs; Sloan Research Fellowship; Moore Foundation; American Family Insurance; Google; and Toshiba. \"The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official guidelines or notes of DARPA, AFRL, NSF, ONR, NIH or the U.S. Government.\""}, {"heading": "A Proofs", "text": "In the first section series, we will then prove the results of the two random distributions of two random variables. Proposition 1 (Proposition 4.7 by Levin et al. (2009): Let X and Y be two random variables referring to values in the same set, and let their distributions be for any coupling between the distributions of two random variables. Proposition 1 (Proposition 4.7 by Levin et al. (2009): Let X and Y be two random variables referring to values in the same set, and let their distributions each be for any coupling. (X, Y) It will hold this coupling, (X, Y) it will hold this coupling."}], "references": [{"title": "Taming the wild: A unified analysis of HOGWILD!-style algorithms", "author": ["De Sa", "Christopher", "Zhang", "Ce", "Olukotun", "Kunle", "R\u00e9"], "venue": "In NIPS. NIPS Foundation,", "citeRegEx": "Sa et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sa et al\\.", "year": 2015}, {"title": "Rapidly mixing gibbs sampling for a class of factor graphs using hierarchy width", "author": ["De Sa", "Christopher M", "Zhang", "Ce", "Olukotun", "Kunle", "R\u00e9", "Christopher"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Sa et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sa et al\\.", "year": 2015}, {"title": "Central limit theorem for nonstationary markov chains", "author": ["Dobrushin", "RL"], "venue": "i. Theory of Probability & Its Applications,", "citeRegEx": "Dobrushin and RL.,? \\Q1956\\E", "shortCiteRegEx": "Dobrushin and RL.", "year": 1956}, {"title": "Dobrushin conditions and systematic scan", "author": ["Dyer", "Martin", "Goldberg", "Leslie Ann", "Jerrum", "Mark"], "venue": "In in Proc. 10th International Workshop on Randomization and Computation, Lecture Notes in Computer Science", "citeRegEx": "Dyer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2006}, {"title": "Parallel gibbs sampling: From colored fields to thin junction trees", "author": ["Gonzalez", "Joseph", "Low", "Yucheng", "Gretton", "Arthur", "Guestrin", "Carlos"], "venue": "In AISTATS, pp", "citeRegEx": "Gonzalez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gonzalez et al\\.", "year": 2011}, {"title": "Sampling from probabilistic submodular models", "author": ["Gotovos", "Alkis", "Hassani", "Hamed", "Krause", "Andreas"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Gotovos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gotovos et al\\.", "year": 2015}, {"title": "Rapidly mixing markov chains: A comparison of techniques", "author": ["Guruswami", "Venkatesan"], "venue": null, "citeRegEx": "Guruswami and Venkatesan.,? \\Q2000\\E", "shortCiteRegEx": "Guruswami and Venkatesan.", "year": 2000}, {"title": "A simple condition implying rapid mixing of single-site dynamics on spin systems", "author": ["Hayes", "Thomas P"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Hayes and P.,? \\Q2006\\E", "shortCiteRegEx": "Hayes and P.", "year": 2006}, {"title": "Beitrag zur theorie des ferromagnetismus", "author": ["Ising", "Ernst"], "venue": "Zeitschrift fu\u0308r Physik A Hadrons and Nuclei,", "citeRegEx": "Ising and Ernst.,? \\Q1925\\E", "shortCiteRegEx": "Ising and Ernst.", "year": 1925}, {"title": "Analyzing hogwild parallel gaussian gibbs sampling", "author": ["Johnson", "Matthew", "Saunderson", "James", "Willsky", "Alan"], "venue": "In NIPS, pp", "citeRegEx": "Johnson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2013}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["Koller", "Daphne", "Friedman", "Nir"], "venue": "MIT press,", "citeRegEx": "Koller et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Koller et al\\.", "year": 2009}, {"title": "Markov chains and mixing times", "author": ["Levin", "David Asher", "Peres", "Yuval", "Wilmer", "Elizabeth Lee"], "venue": "American Mathematical Soc.,", "citeRegEx": "Levin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Levin et al\\.", "year": 2009}, {"title": "Asynchronous stochastic coordinate descent: Parallelism and convergence properties. SIOPT", "author": ["Liu", "Ji", "Wright", "Stephen J"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "An asynchronous parallel stochastic coordinate descent algorithm", "author": ["Liu", "Ji", "Wright", "Stephen J", "R\u00e9", "Christopher", "Bittorf", "Victor", "Sridhar", "Srikrishna"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "The BUGS project: evolution, critique and future directions", "author": ["Lunn", "David", "Spiegelhalter", "Thomas", "Andrew", "Best", "Nicky"], "venue": "Statistics in medicine,", "citeRegEx": "Lunn et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lunn et al\\.", "year": 2009}, {"title": "Perturbed iterate analysis for asynchronous stochastic optimization", "author": ["Mania", "Horia", "Pan", "Xinghao", "Papailiopoulos", "Dimitris", "Recht", "Benjamin", "Ramchandran", "Kannan", "Jordan", "Michael I"], "venue": "arXiv preprint arXiv:1507.06970,", "citeRegEx": "Mania et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mania et al\\.", "year": 2015}, {"title": "Factorie: Probabilistic programming via imperatively defined factor graphs", "author": ["McCallum", "Andrew", "Schultz", "Karl", "Singh", "Sameer"], "venue": "In NIPS, pp", "citeRegEx": "McCallum et al\\.,? \\Q2009\\E", "shortCiteRegEx": "McCallum et al\\.", "year": 2009}, {"title": "Frogwild!: Fast pagerank approximations on graph engines", "author": ["Mitliagkas", "Ioannis", "Borokhovich", "Michael", "Dimakis", "Alexandros G", "Caramanis", "Constantine"], "venue": null, "citeRegEx": "Mitliagkas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mitliagkas et al\\.", "year": 2015}, {"title": "Simple, correct parallelization for blocked gibbs sampling", "author": ["Neubig", "Graham"], "venue": "Technical report, Nara Institute of Science and Technology,", "citeRegEx": "Neubig and Graham.,? \\Q2014\\E", "shortCiteRegEx": "Neubig and Graham.", "year": 2014}, {"title": "Distributed inference for latent dirichlet allocation", "author": ["Newman", "David", "Smyth", "Padhraic", "Welling", "Max", "Asuncion", "Arthur U"], "venue": "In NIPS, pp", "citeRegEx": "Newman et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Newman et al\\.", "year": 2007}, {"title": "Distributed gibbs: A memory-bounded sampling-based dcop algorithm", "author": ["Nguyen", "Duc Thien", "Yeoh", "William", "Lau", "Hoong Chuin"], "venue": "In Proceedings of the 2013 international conference on Autonomous agents and multi-agent systems,", "citeRegEx": "Nguyen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2013}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["Niu", "Feng", "Recht", "Benjamin", "Re", "Christopher", "Wright", "Stephen"], "venue": "In NIPS, pp", "citeRegEx": "Niu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Niu et al\\.", "year": 2011}, {"title": "Dogwild!\u2013Distributed Hogwild for CPU & GPU", "author": ["Noel", "Cyprien", "Osindero", "Simon"], "venue": null, "citeRegEx": "Noel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Noel et al\\.", "year": 2014}, {"title": "Exact sampling with coupled markov chains and applications to statistical mechanics", "author": ["Propp", "James Gary", "Wilson", "David Bruce"], "venue": "Random Structures & Algorithms,", "citeRegEx": "Propp et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Propp et al\\.", "year": 1996}, {"title": "Incremental knowledge base construction using deepdive", "author": ["Shin", "Jaeho", "Wu", "Sen", "Wang", "Feiran", "De Sa", "Christopher", "Zhang", "Ce", "R\u00e9"], "venue": null, "citeRegEx": "Shin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shin et al\\.", "year": 2015}, {"title": "An architecture for parallel topic models", "author": ["Smola", "Alexander", "Narayanamurthy", "Shravan"], "venue": null, "citeRegEx": "Smola et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Smola et al\\.", "year": 2010}, {"title": "Asynchronous distributed learning of topic models", "author": ["Smyth", "Padhraic", "Welling", "Max", "Asuncion", "Arthur U"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Smyth et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Smyth et al\\.", "year": 2009}, {"title": "The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software", "author": ["Sutter", "Herb"], "venue": "Dr. Dobb\u2019s Journal,", "citeRegEx": "Sutter and Herb.,? \\Q2005\\E", "shortCiteRegEx": "Sutter and Herb.", "year": 2005}, {"title": "Asynchronous distributed gibbs sampling", "author": ["Terenin", "Alexander", "Simpson", "Daniel", "Draper", "David"], "venue": "arXiv preprint arXiv:1509.08999,", "citeRegEx": "Terenin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Terenin et al\\.", "year": 2015}, {"title": "Training sparse natural image models with a fast gibbs sampler of an extended state space", "author": ["Theis", "Lucas", "Sohl-dickstein", "Jascha", "Bethge", "Matthias"], "venue": "In NIPS,", "citeRegEx": "Theis et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2012}, {"title": "Scalable coordinate descent approaches to parallel matrix factorization for recommender systems", "author": ["Yu", "Hsiang-Fu", "Hsieh", "Cho-Jui", "Si", "Dhillon", "Inderjit S"], "venue": "In ICDM,", "citeRegEx": "Yu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2012}, {"title": "DimmWitted: A study of main-memory statistical analytics", "author": ["Zhang", "Ce", "R\u00e9", "Christopher"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "This proposition relates the concept of a coupling with the total variation distance between the distributions of two random variables", "author": ["Levin"], "venue": null, "citeRegEx": "Levin,? \\Q2009\\E", "shortCiteRegEx": "Levin", "year": 2009}, {"title": "Group representations in probability and statistics", "author": ["Diaconis", "Persi"], "venue": "Lecture Notes-Monograph Series,", "citeRegEx": "Diaconis and Persi.,? \\Q1988\\E", "shortCiteRegEx": "Diaconis and Persi.", "year": 1988}, {"title": "Generating a random permutation with random transpositions", "author": ["Diaconis", "Persi", "Shahshahani", "Mehrdad"], "venue": "Probability Theory and Related Fields,", "citeRegEx": "Diaconis et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Diaconis et al\\.", "year": 1981}, {"title": "Markov chains and mixing times", "author": ["Levin", "David Asher", "Peres", "Yuval", "Wilmer", "Elizabeth Lee"], "venue": "American Mathematical Soc.,", "citeRegEx": "Levin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Levin et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 17, "context": "Because of this and other useful properties of Gibbs sampling, many systems use Gibbs sampling to perform inference on big data Newman et al. (2007); Lunn et al.", "startOffset": 128, "endOffset": 149}, {"referenceID": 14, "context": "(2007); Lunn et al. (2009); McCallum et al.", "startOffset": 8, "endOffset": 27}, {"referenceID": 14, "context": "(2007); Lunn et al. (2009); McCallum et al. (2009); Smola & Narayanamurthy (2010); Theis et al.", "startOffset": 8, "endOffset": 51}, {"referenceID": 14, "context": "(2007); Lunn et al. (2009); McCallum et al. (2009); Smola & Narayanamurthy (2010); Theis et al.", "startOffset": 8, "endOffset": 82}, {"referenceID": 14, "context": "(2007); Lunn et al. (2009); McCallum et al. (2009); Smola & Narayanamurthy (2010); Theis et al. (2012); Zhang & R\u00e9 (2014).", "startOffset": 8, "endOffset": 103}, {"referenceID": 14, "context": "(2007); Lunn et al. (2009); McCallum et al. (2009); Smola & Narayanamurthy (2010); Theis et al. (2012); Zhang & R\u00e9 (2014). Since Gibbs sampling is such a ubiquitous algorithm, it is important to try to optimize its execution speed on modern hardware.", "startOffset": 8, "endOffset": 122}, {"referenceID": 14, "context": "(2007); Lunn et al. (2009); McCallum et al. (2009); Smola & Narayanamurthy (2010); Theis et al. (2012); Zhang & R\u00e9 (2014). Since Gibbs sampling is such a ubiquitous algorithm, it is important to try to optimize its execution speed on modern hardware. Unfortunately, while modern computer hardware has been trending towards more parallel architectures Sutter (2005), traditional Gibbs sampling is an inherently sequential algorithm; that is, the outer loop in Algorithm 1 is not embarrassingly parallel.", "startOffset": 8, "endOffset": 365}, {"referenceID": 14, "context": "(2007); Lunn et al. (2009); McCallum et al. (2009); Smola & Narayanamurthy (2010); Theis et al. (2012); Zhang & R\u00e9 (2014). Since Gibbs sampling is such a ubiquitous algorithm, it is important to try to optimize its execution speed on modern hardware. Unfortunately, while modern computer hardware has been trending towards more parallel architectures Sutter (2005), traditional Gibbs sampling is an inherently sequential algorithm; that is, the outer loop in Algorithm 1 is not embarrassingly parallel. Furthermore, for sparse models, very little work happens within each iteration, meaning it is difficult to extract much parallelism from the body of this outer loop. Since traditional Gibbs sampling parallelizes so poorly, it is interesting to study variants of Gibbs sampling that can be parallelized. Several such variants have been proposed, including applications to latent Dirichlet allocation Newman et al. (2007); Smola & Narayanamurthy (2010) and distributed constraint optimization problems Nguyen et al.", "startOffset": 8, "endOffset": 923}, {"referenceID": 14, "context": "(2007); Lunn et al. (2009); McCallum et al. (2009); Smola & Narayanamurthy (2010); Theis et al. (2012); Zhang & R\u00e9 (2014). Since Gibbs sampling is such a ubiquitous algorithm, it is important to try to optimize its execution speed on modern hardware. Unfortunately, while modern computer hardware has been trending towards more parallel architectures Sutter (2005), traditional Gibbs sampling is an inherently sequential algorithm; that is, the outer loop in Algorithm 1 is not embarrassingly parallel. Furthermore, for sparse models, very little work happens within each iteration, meaning it is difficult to extract much parallelism from the body of this outer loop. Since traditional Gibbs sampling parallelizes so poorly, it is interesting to study variants of Gibbs sampling that can be parallelized. Several such variants have been proposed, including applications to latent Dirichlet allocation Newman et al. (2007); Smola & Narayanamurthy (2010) and distributed constraint optimization problems Nguyen et al.", "startOffset": 8, "endOffset": 954}, {"referenceID": 14, "context": "(2007); Lunn et al. (2009); McCallum et al. (2009); Smola & Narayanamurthy (2010); Theis et al. (2012); Zhang & R\u00e9 (2014). Since Gibbs sampling is such a ubiquitous algorithm, it is important to try to optimize its execution speed on modern hardware. Unfortunately, while modern computer hardware has been trending towards more parallel architectures Sutter (2005), traditional Gibbs sampling is an inherently sequential algorithm; that is, the outer loop in Algorithm 1 is not embarrassingly parallel. Furthermore, for sparse models, very little work happens within each iteration, meaning it is difficult to extract much parallelism from the body of this outer loop. Since traditional Gibbs sampling parallelizes so poorly, it is interesting to study variants of Gibbs sampling that can be parallelized. Several such variants have been proposed, including applications to latent Dirichlet allocation Newman et al. (2007); Smola & Narayanamurthy (2010) and distributed constraint optimization problems Nguyen et al. (2013). In one popular variant, multiple threads run the Gibbs sampling update rule in parallel without locks, a strategy called asynchronous or HOGWILD! execution\u2014in this paper, we use these two terms interchangeably.", "startOffset": 8, "endOffset": 1024}, {"referenceID": 7, "context": "But when can we be sure that HOGWILD! Gibbs sampling will produce accurate results? Except for the case of Gaussian random variables Johnson et al. (2013), there is no existing analysis by which we can ensure that asynchronous Gibbs sampling will be appropriate for a particular application.", "startOffset": 133, "endOffset": 155}, {"referenceID": 7, "context": "But when can we be sure that HOGWILD! Gibbs sampling will produce accurate results? Except for the case of Gaussian random variables Johnson et al. (2013), there is no existing analysis by which we can ensure that asynchronous Gibbs sampling will be appropriate for a particular application. Even the problems posed by HOGWILD!-Gibbs are poorly understood, and their solutions more so. As we will see in the following sections, there are two main issues when analyzing asynchronous Gibbs sampling. Firstly, we will show by example that, surprisingly, HOGWILD!-Gibbs can be biased\u2014unlike sequential Gibbs, it does not always produce samples that are arbitrarily close to the target distribution. Secondly, we will show that the mixing time (the time for the chain to become close to its stationary distribution) of asynchronous Gibbs sampling can be at worst exponentially greater than that of the corresponding sequential chain. To address the issue of bias, we need some way to describe the distance between the target distribution \u03c0 and the distribution of the samples produced by HOGWILD!-Gibbs. The standard notion to use here is the total variation distance, but for the task of computing marginal probabilities, it gives an overestimate on the error caused by bias. To better describe the bias, we introduce a new notion of statistical distance, the sparse variation distance. While this relaxed notion of statistical distance is interesting in its own right, its main benefit here is that it more tightly measures the effect of bias on marginal estimation. Our main goal is to identify conditions under which the bias and mixing time of asynchronous Gibbs can be bounded. One parameter that has been used to great effect in the analysis of Gibbs sampling is the total influence \u03b1 of a model. The total influence measures the degree to which the marginal distribution of a variable can depend on the values of the other variables in the model\u2014this parameter has appeared as part of a celebrated line of work on Dobrushin\u2019s condition (\u03b1 < 1), which ensures the rapid mixing of spin statistics systems Dobrushin (1956); Dyer et al.", "startOffset": 133, "endOffset": 2123}, {"referenceID": 3, "context": "The total influence measures the degree to which the marginal distribution of a variable can depend on the values of the other variables in the model\u2014this parameter has appeared as part of a celebrated line of work on Dobrushin\u2019s condition (\u03b1 < 1), which ensures the rapid mixing of spin statistics systems Dobrushin (1956); Dyer et al. (2006); Hayes (2006).", "startOffset": 325, "endOffset": 344}, {"referenceID": 3, "context": "The total influence measures the degree to which the marginal distribution of a variable can depend on the values of the other variables in the model\u2014this parameter has appeared as part of a celebrated line of work on Dobrushin\u2019s condition (\u03b1 < 1), which ensures the rapid mixing of spin statistics systems Dobrushin (1956); Dyer et al. (2006); Hayes (2006). It turns out that we can use this parameter to bound both the bias and mixing time of HOGWILD!-Gibbs, and so we make the following contributions: \u2022 We describe a way to statistically model the asynchronicity in HOGWILD!-Gibbs sampling.", "startOffset": 325, "endOffset": 358}, {"referenceID": 3, "context": "The total influence measures the degree to which the marginal distribution of a variable can depend on the values of the other variables in the model\u2014this parameter has appeared as part of a celebrated line of work on Dobrushin\u2019s condition (\u03b1 < 1), which ensures the rapid mixing of spin statistics systems Dobrushin (1956); Dyer et al. (2006); Hayes (2006). It turns out that we can use this parameter to bound both the bias and mixing time of HOGWILD!-Gibbs, and so we make the following contributions: \u2022 We describe a way to statistically model the asynchronicity in HOGWILD!-Gibbs sampling. \u2022 To bound the bias, we prove that for classes of models with bounded total influence \u03b1 = O(1), if sequential Gibbs sampling achieves small sparse variation distance to \u03c0 in O(n) steps, where n is the number of variables, then HOGWILD!-Gibbs samples achieve the same distance in at most O(1) more steps. \u2022 For models that satisfy Dobrushin\u2019s condition (\u03b1 < 1), we show that the mixing time bounds of sequential and HOGWILD!-Gibbs sampling differ only by a factor of 1 +O(n). \u2022 We validate our results experimentally and show that, by using asynchronous execution, we can achieve wallclock speedups of up to 2.8\u00d7 on real problems. 2 Related Work Much work has been done on the analysis of parallel Gibbs samplers. One simple way to parallelize Gibbs sampling is to run multiple chains independently in parallel: this heuristic uses parallelism to produce more samples overall, but does not produce accurate samples more quickly. Additionally, this strategy is sometimes worse than other strategies on a systems level Smola & Narayanamurthy (2010); Zhang & R\u00e9 (2014), typically because it requires additional memory to maintain multiple models of the chain.", "startOffset": 325, "endOffset": 1641}, {"referenceID": 3, "context": "The total influence measures the degree to which the marginal distribution of a variable can depend on the values of the other variables in the model\u2014this parameter has appeared as part of a celebrated line of work on Dobrushin\u2019s condition (\u03b1 < 1), which ensures the rapid mixing of spin statistics systems Dobrushin (1956); Dyer et al. (2006); Hayes (2006). It turns out that we can use this parameter to bound both the bias and mixing time of HOGWILD!-Gibbs, and so we make the following contributions: \u2022 We describe a way to statistically model the asynchronicity in HOGWILD!-Gibbs sampling. \u2022 To bound the bias, we prove that for classes of models with bounded total influence \u03b1 = O(1), if sequential Gibbs sampling achieves small sparse variation distance to \u03c0 in O(n) steps, where n is the number of variables, then HOGWILD!-Gibbs samples achieve the same distance in at most O(1) more steps. \u2022 For models that satisfy Dobrushin\u2019s condition (\u03b1 < 1), we show that the mixing time bounds of sequential and HOGWILD!-Gibbs sampling differ only by a factor of 1 +O(n). \u2022 We validate our results experimentally and show that, by using asynchronous execution, we can achieve wallclock speedups of up to 2.8\u00d7 on real problems. 2 Related Work Much work has been done on the analysis of parallel Gibbs samplers. One simple way to parallelize Gibbs sampling is to run multiple chains independently in parallel: this heuristic uses parallelism to produce more samples overall, but does not produce accurate samples more quickly. Additionally, this strategy is sometimes worse than other strategies on a systems level Smola & Narayanamurthy (2010); Zhang & R\u00e9 (2014), typically because it requires additional memory to maintain multiple models of the chain.", "startOffset": 325, "endOffset": 1660}, {"referenceID": 3, "context": "The total influence measures the degree to which the marginal distribution of a variable can depend on the values of the other variables in the model\u2014this parameter has appeared as part of a celebrated line of work on Dobrushin\u2019s condition (\u03b1 < 1), which ensures the rapid mixing of spin statistics systems Dobrushin (1956); Dyer et al. (2006); Hayes (2006). It turns out that we can use this parameter to bound both the bias and mixing time of HOGWILD!-Gibbs, and so we make the following contributions: \u2022 We describe a way to statistically model the asynchronicity in HOGWILD!-Gibbs sampling. \u2022 To bound the bias, we prove that for classes of models with bounded total influence \u03b1 = O(1), if sequential Gibbs sampling achieves small sparse variation distance to \u03c0 in O(n) steps, where n is the number of variables, then HOGWILD!-Gibbs samples achieve the same distance in at most O(1) more steps. \u2022 For models that satisfy Dobrushin\u2019s condition (\u03b1 < 1), we show that the mixing time bounds of sequential and HOGWILD!-Gibbs sampling differ only by a factor of 1 +O(n). \u2022 We validate our results experimentally and show that, by using asynchronous execution, we can achieve wallclock speedups of up to 2.8\u00d7 on real problems. 2 Related Work Much work has been done on the analysis of parallel Gibbs samplers. One simple way to parallelize Gibbs sampling is to run multiple chains independently in parallel: this heuristic uses parallelism to produce more samples overall, but does not produce accurate samples more quickly. Additionally, this strategy is sometimes worse than other strategies on a systems level Smola & Narayanamurthy (2010); Zhang & R\u00e9 (2014), typically because it requires additional memory to maintain multiple models of the chain. Another strategy for parallelizing Gibbs sampling involves taking advantage of the structure of the underlying factor graph to run in parallel while still maintaining an execution pattern to which the standard sequential Gibbs sampling analysis can be applied Gonzalez et al. (2011). Much further work has focused on parallelizing sampling for specific problems, such as LDA Newman et al.", "startOffset": 325, "endOffset": 2034}, {"referenceID": 3, "context": "The total influence measures the degree to which the marginal distribution of a variable can depend on the values of the other variables in the model\u2014this parameter has appeared as part of a celebrated line of work on Dobrushin\u2019s condition (\u03b1 < 1), which ensures the rapid mixing of spin statistics systems Dobrushin (1956); Dyer et al. (2006); Hayes (2006). It turns out that we can use this parameter to bound both the bias and mixing time of HOGWILD!-Gibbs, and so we make the following contributions: \u2022 We describe a way to statistically model the asynchronicity in HOGWILD!-Gibbs sampling. \u2022 To bound the bias, we prove that for classes of models with bounded total influence \u03b1 = O(1), if sequential Gibbs sampling achieves small sparse variation distance to \u03c0 in O(n) steps, where n is the number of variables, then HOGWILD!-Gibbs samples achieve the same distance in at most O(1) more steps. \u2022 For models that satisfy Dobrushin\u2019s condition (\u03b1 < 1), we show that the mixing time bounds of sequential and HOGWILD!-Gibbs sampling differ only by a factor of 1 +O(n). \u2022 We validate our results experimentally and show that, by using asynchronous execution, we can achieve wallclock speedups of up to 2.8\u00d7 on real problems. 2 Related Work Much work has been done on the analysis of parallel Gibbs samplers. One simple way to parallelize Gibbs sampling is to run multiple chains independently in parallel: this heuristic uses parallelism to produce more samples overall, but does not produce accurate samples more quickly. Additionally, this strategy is sometimes worse than other strategies on a systems level Smola & Narayanamurthy (2010); Zhang & R\u00e9 (2014), typically because it requires additional memory to maintain multiple models of the chain. Another strategy for parallelizing Gibbs sampling involves taking advantage of the structure of the underlying factor graph to run in parallel while still maintaining an execution pattern to which the standard sequential Gibbs sampling analysis can be applied Gonzalez et al. (2011). Much further work has focused on parallelizing sampling for specific problems, such as LDA Newman et al. (2007); Smola & Narayanamurthy (2010) and others Nguyen et al.", "startOffset": 325, "endOffset": 2147}, {"referenceID": 3, "context": "The total influence measures the degree to which the marginal distribution of a variable can depend on the values of the other variables in the model\u2014this parameter has appeared as part of a celebrated line of work on Dobrushin\u2019s condition (\u03b1 < 1), which ensures the rapid mixing of spin statistics systems Dobrushin (1956); Dyer et al. (2006); Hayes (2006). It turns out that we can use this parameter to bound both the bias and mixing time of HOGWILD!-Gibbs, and so we make the following contributions: \u2022 We describe a way to statistically model the asynchronicity in HOGWILD!-Gibbs sampling. \u2022 To bound the bias, we prove that for classes of models with bounded total influence \u03b1 = O(1), if sequential Gibbs sampling achieves small sparse variation distance to \u03c0 in O(n) steps, where n is the number of variables, then HOGWILD!-Gibbs samples achieve the same distance in at most O(1) more steps. \u2022 For models that satisfy Dobrushin\u2019s condition (\u03b1 < 1), we show that the mixing time bounds of sequential and HOGWILD!-Gibbs sampling differ only by a factor of 1 +O(n). \u2022 We validate our results experimentally and show that, by using asynchronous execution, we can achieve wallclock speedups of up to 2.8\u00d7 on real problems. 2 Related Work Much work has been done on the analysis of parallel Gibbs samplers. One simple way to parallelize Gibbs sampling is to run multiple chains independently in parallel: this heuristic uses parallelism to produce more samples overall, but does not produce accurate samples more quickly. Additionally, this strategy is sometimes worse than other strategies on a systems level Smola & Narayanamurthy (2010); Zhang & R\u00e9 (2014), typically because it requires additional memory to maintain multiple models of the chain. Another strategy for parallelizing Gibbs sampling involves taking advantage of the structure of the underlying factor graph to run in parallel while still maintaining an execution pattern to which the standard sequential Gibbs sampling analysis can be applied Gonzalez et al. (2011). Much further work has focused on parallelizing sampling for specific problems, such as LDA Newman et al. (2007); Smola & Narayanamurthy (2010) and others Nguyen et al.", "startOffset": 325, "endOffset": 2178}, {"referenceID": 3, "context": "The total influence measures the degree to which the marginal distribution of a variable can depend on the values of the other variables in the model\u2014this parameter has appeared as part of a celebrated line of work on Dobrushin\u2019s condition (\u03b1 < 1), which ensures the rapid mixing of spin statistics systems Dobrushin (1956); Dyer et al. (2006); Hayes (2006). It turns out that we can use this parameter to bound both the bias and mixing time of HOGWILD!-Gibbs, and so we make the following contributions: \u2022 We describe a way to statistically model the asynchronicity in HOGWILD!-Gibbs sampling. \u2022 To bound the bias, we prove that for classes of models with bounded total influence \u03b1 = O(1), if sequential Gibbs sampling achieves small sparse variation distance to \u03c0 in O(n) steps, where n is the number of variables, then HOGWILD!-Gibbs samples achieve the same distance in at most O(1) more steps. \u2022 For models that satisfy Dobrushin\u2019s condition (\u03b1 < 1), we show that the mixing time bounds of sequential and HOGWILD!-Gibbs sampling differ only by a factor of 1 +O(n). \u2022 We validate our results experimentally and show that, by using asynchronous execution, we can achieve wallclock speedups of up to 2.8\u00d7 on real problems. 2 Related Work Much work has been done on the analysis of parallel Gibbs samplers. One simple way to parallelize Gibbs sampling is to run multiple chains independently in parallel: this heuristic uses parallelism to produce more samples overall, but does not produce accurate samples more quickly. Additionally, this strategy is sometimes worse than other strategies on a systems level Smola & Narayanamurthy (2010); Zhang & R\u00e9 (2014), typically because it requires additional memory to maintain multiple models of the chain. Another strategy for parallelizing Gibbs sampling involves taking advantage of the structure of the underlying factor graph to run in parallel while still maintaining an execution pattern to which the standard sequential Gibbs sampling analysis can be applied Gonzalez et al. (2011). Much further work has focused on parallelizing sampling for specific problems, such as LDA Newman et al. (2007); Smola & Narayanamurthy (2010) and others Nguyen et al. (2013). Our approach follows on the paper of Johnson et al.", "startOffset": 325, "endOffset": 2210}, {"referenceID": 3, "context": "The total influence measures the degree to which the marginal distribution of a variable can depend on the values of the other variables in the model\u2014this parameter has appeared as part of a celebrated line of work on Dobrushin\u2019s condition (\u03b1 < 1), which ensures the rapid mixing of spin statistics systems Dobrushin (1956); Dyer et al. (2006); Hayes (2006). It turns out that we can use this parameter to bound both the bias and mixing time of HOGWILD!-Gibbs, and so we make the following contributions: \u2022 We describe a way to statistically model the asynchronicity in HOGWILD!-Gibbs sampling. \u2022 To bound the bias, we prove that for classes of models with bounded total influence \u03b1 = O(1), if sequential Gibbs sampling achieves small sparse variation distance to \u03c0 in O(n) steps, where n is the number of variables, then HOGWILD!-Gibbs samples achieve the same distance in at most O(1) more steps. \u2022 For models that satisfy Dobrushin\u2019s condition (\u03b1 < 1), we show that the mixing time bounds of sequential and HOGWILD!-Gibbs sampling differ only by a factor of 1 +O(n). \u2022 We validate our results experimentally and show that, by using asynchronous execution, we can achieve wallclock speedups of up to 2.8\u00d7 on real problems. 2 Related Work Much work has been done on the analysis of parallel Gibbs samplers. One simple way to parallelize Gibbs sampling is to run multiple chains independently in parallel: this heuristic uses parallelism to produce more samples overall, but does not produce accurate samples more quickly. Additionally, this strategy is sometimes worse than other strategies on a systems level Smola & Narayanamurthy (2010); Zhang & R\u00e9 (2014), typically because it requires additional memory to maintain multiple models of the chain. Another strategy for parallelizing Gibbs sampling involves taking advantage of the structure of the underlying factor graph to run in parallel while still maintaining an execution pattern to which the standard sequential Gibbs sampling analysis can be applied Gonzalez et al. (2011). Much further work has focused on parallelizing sampling for specific problems, such as LDA Newman et al. (2007); Smola & Narayanamurthy (2010) and others Nguyen et al. (2013). Our approach follows on the paper of Johnson et al. (2013), which proposes the HOGWILD!-Gibbs sampling algorithm and analyzes it for the case of Gaussian models.", "startOffset": 325, "endOffset": 2270}, {"referenceID": 3, "context": "The total influence measures the degree to which the marginal distribution of a variable can depend on the values of the other variables in the model\u2014this parameter has appeared as part of a celebrated line of work on Dobrushin\u2019s condition (\u03b1 < 1), which ensures the rapid mixing of spin statistics systems Dobrushin (1956); Dyer et al. (2006); Hayes (2006). It turns out that we can use this parameter to bound both the bias and mixing time of HOGWILD!-Gibbs, and so we make the following contributions: \u2022 We describe a way to statistically model the asynchronicity in HOGWILD!-Gibbs sampling. \u2022 To bound the bias, we prove that for classes of models with bounded total influence \u03b1 = O(1), if sequential Gibbs sampling achieves small sparse variation distance to \u03c0 in O(n) steps, where n is the number of variables, then HOGWILD!-Gibbs samples achieve the same distance in at most O(1) more steps. \u2022 For models that satisfy Dobrushin\u2019s condition (\u03b1 < 1), we show that the mixing time bounds of sequential and HOGWILD!-Gibbs sampling differ only by a factor of 1 +O(n). \u2022 We validate our results experimentally and show that, by using asynchronous execution, we can achieve wallclock speedups of up to 2.8\u00d7 on real problems. 2 Related Work Much work has been done on the analysis of parallel Gibbs samplers. One simple way to parallelize Gibbs sampling is to run multiple chains independently in parallel: this heuristic uses parallelism to produce more samples overall, but does not produce accurate samples more quickly. Additionally, this strategy is sometimes worse than other strategies on a systems level Smola & Narayanamurthy (2010); Zhang & R\u00e9 (2014), typically because it requires additional memory to maintain multiple models of the chain. Another strategy for parallelizing Gibbs sampling involves taking advantage of the structure of the underlying factor graph to run in parallel while still maintaining an execution pattern to which the standard sequential Gibbs sampling analysis can be applied Gonzalez et al. (2011). Much further work has focused on parallelizing sampling for specific problems, such as LDA Newman et al. (2007); Smola & Narayanamurthy (2010) and others Nguyen et al. (2013). Our approach follows on the paper of Johnson et al. (2013), which proposes the HOGWILD!-Gibbs sampling algorithm and analyzes it for the case of Gaussian models. Their main contribution is an analysis framework that includes a sufficient condition under which HOGWILD! Gaussian Gibbs samples are guaranteed to have the correct asymptotic mean. Recent work Terenin et al. (2015) has analyzed a similar algorithm under even stronger regularity conditions.", "startOffset": 325, "endOffset": 2589}, {"referenceID": 3, "context": "The total influence measures the degree to which the marginal distribution of a variable can depend on the values of the other variables in the model\u2014this parameter has appeared as part of a celebrated line of work on Dobrushin\u2019s condition (\u03b1 < 1), which ensures the rapid mixing of spin statistics systems Dobrushin (1956); Dyer et al. (2006); Hayes (2006). It turns out that we can use this parameter to bound both the bias and mixing time of HOGWILD!-Gibbs, and so we make the following contributions: \u2022 We describe a way to statistically model the asynchronicity in HOGWILD!-Gibbs sampling. \u2022 To bound the bias, we prove that for classes of models with bounded total influence \u03b1 = O(1), if sequential Gibbs sampling achieves small sparse variation distance to \u03c0 in O(n) steps, where n is the number of variables, then HOGWILD!-Gibbs samples achieve the same distance in at most O(1) more steps. \u2022 For models that satisfy Dobrushin\u2019s condition (\u03b1 < 1), we show that the mixing time bounds of sequential and HOGWILD!-Gibbs sampling differ only by a factor of 1 +O(n). \u2022 We validate our results experimentally and show that, by using asynchronous execution, we can achieve wallclock speedups of up to 2.8\u00d7 on real problems. 2 Related Work Much work has been done on the analysis of parallel Gibbs samplers. One simple way to parallelize Gibbs sampling is to run multiple chains independently in parallel: this heuristic uses parallelism to produce more samples overall, but does not produce accurate samples more quickly. Additionally, this strategy is sometimes worse than other strategies on a systems level Smola & Narayanamurthy (2010); Zhang & R\u00e9 (2014), typically because it requires additional memory to maintain multiple models of the chain. Another strategy for parallelizing Gibbs sampling involves taking advantage of the structure of the underlying factor graph to run in parallel while still maintaining an execution pattern to which the standard sequential Gibbs sampling analysis can be applied Gonzalez et al. (2011). Much further work has focused on parallelizing sampling for specific problems, such as LDA Newman et al. (2007); Smola & Narayanamurthy (2010) and others Nguyen et al. (2013). Our approach follows on the paper of Johnson et al. (2013), which proposes the HOGWILD!-Gibbs sampling algorithm and analyzes it for the case of Gaussian models. Their main contribution is an analysis framework that includes a sufficient condition under which HOGWILD! Gaussian Gibbs samples are guaranteed to have the correct asymptotic mean. Recent work Terenin et al. (2015) has analyzed a similar algorithm under even stronger regularity conditions. Here, we seek to give more general results for the analysis of HOGWILD!-Gibbs sampling on discrete-valued factor graphs. The HOGWILD!-Gibbs sampling algorithm was inspired by a line of work on parallelizing stochastic gradient descent (SGD) by running it asynchronously. HOGWILD! SGD was first proposed by Niu et al. (2011), who proved that", "startOffset": 325, "endOffset": 2989}, {"referenceID": 11, "context": "The asynchronous execution strategy has been applied to many problems\u2014such as PageRank approximations Mitliagkas et al. (2015), deep learning Noel & Osindero (2014) and recommender systems Yu et al.", "startOffset": 102, "endOffset": 127}, {"referenceID": 11, "context": "The asynchronous execution strategy has been applied to many problems\u2014such as PageRank approximations Mitliagkas et al. (2015), deep learning Noel & Osindero (2014) and recommender systems Yu et al.", "startOffset": 102, "endOffset": 165}, {"referenceID": 11, "context": "The asynchronous execution strategy has been applied to many problems\u2014such as PageRank approximations Mitliagkas et al. (2015), deep learning Noel & Osindero (2014) and recommender systems Yu et al. (2012)\u2014so it is not surprising that it has been proposed for use with Gibbs sampling.", "startOffset": 102, "endOffset": 206}, {"referenceID": 9, "context": "In particular, we are motivated by some recent work on the analysis of HOGWILD! for SGD Liu et al. (2015); De Sa et al.", "startOffset": 88, "endOffset": 106}, {"referenceID": 0, "context": "(2015); De Sa et al. (2015a); Mania et al.", "startOffset": 11, "endOffset": 29}, {"referenceID": 0, "context": "(2015); De Sa et al. (2015a); Mania et al. (2015); Liu & Wright (2015).", "startOffset": 11, "endOffset": 50}, {"referenceID": 0, "context": "(2015); De Sa et al. (2015a); Mania et al. (2015); Liu & Wright (2015). Several of these results suggest modeling the race conditions inherent in HOGWILD! SGD as noise in a stochastic process; this lets them bring a trove of statistical techniques to bear on the analysis of HOGWILD! SGD.", "startOffset": 11, "endOffset": 71}, {"referenceID": 0, "context": "(2015); De Sa et al. (2015a); Mania et al. (2015); Liu & Wright (2015). Several of these results suggest modeling the race conditions inherent in HOGWILD! SGD as noise in a stochastic process; this lets them bring a trove of statistical techniques to bear on the analysis of HOGWILD! SGD. Therefore, in this paper, we will apply a similar stochastic process model to Gibbs sampling. Several recent papers have focused on the mixing time of Gibbs sampling for factor graphs. Gotovos et al. (2015) and De Sa et al.", "startOffset": 11, "endOffset": 496}, {"referenceID": 0, "context": "(2015); De Sa et al. (2015a); Mania et al. (2015); Liu & Wright (2015). Several of these results suggest modeling the race conditions inherent in HOGWILD! SGD as noise in a stochastic process; this lets them bring a trove of statistical techniques to bear on the analysis of HOGWILD! SGD. Therefore, in this paper, we will apply a similar stochastic process model to Gibbs sampling. Several recent papers have focused on the mixing time of Gibbs sampling for factor graphs. Gotovos et al. (2015) and De Sa et al. (2015b) each show that Gibbs sampling mixes in polynomial time for a class of distributions bounded by some parameter.", "startOffset": 11, "endOffset": 521}, {"referenceID": 0, "context": "(2015); De Sa et al. (2015a); Mania et al. (2015); Liu & Wright (2015). Several of these results suggest modeling the race conditions inherent in HOGWILD! SGD as noise in a stochastic process; this lets them bring a trove of statistical techniques to bear on the analysis of HOGWILD! SGD. Therefore, in this paper, we will apply a similar stochastic process model to Gibbs sampling. Several recent papers have focused on the mixing time of Gibbs sampling for factor graphs. Gotovos et al. (2015) and De Sa et al. (2015b) each show that Gibbs sampling mixes in polynomial time for a class of distributions bounded by some parameter. Unfortunately, these results both depend on spectral methods (that try to bound the spectral gap of the Markov transition matrix), which are difficult to apply to HOGWILD! Gibbs sampling for two reasons. First, spectral methods don\u2019t let us represent the sampler as a stochastic process, which limits the range of techniques we can use to model the noise. Secondly, while most spectral methods only apply to reversible Markov chains\u2014and sequential Gibbs sampling is always a reversible chain\u2014for HOGWILD!-Gibbs sampling the asynchronicity and parallelism make the chain non-reversible. Because of this, we were unable to use these spectral results in our asynchronous setting. We are forced to rely on the other method Guruswami (2000) for analyzing Markov processes, coupling\u2014the type of analysis used with the Dobrushin condition\u2014which we will describe in the following sections.", "startOffset": 11, "endOffset": 1368}, {"referenceID": 0, "context": "(2015); De Sa et al. (2015a); Mania et al. (2015); Liu & Wright (2015). Several of these results suggest modeling the race conditions inherent in HOGWILD! SGD as noise in a stochastic process; this lets them bring a trove of statistical techniques to bear on the analysis of HOGWILD! SGD. Therefore, in this paper, we will apply a similar stochastic process model to Gibbs sampling. Several recent papers have focused on the mixing time of Gibbs sampling for factor graphs. Gotovos et al. (2015) and De Sa et al. (2015b) each show that Gibbs sampling mixes in polynomial time for a class of distributions bounded by some parameter. Unfortunately, these results both depend on spectral methods (that try to bound the spectral gap of the Markov transition matrix), which are difficult to apply to HOGWILD! Gibbs sampling for two reasons. First, spectral methods don\u2019t let us represent the sampler as a stochastic process, which limits the range of techniques we can use to model the noise. Secondly, while most spectral methods only apply to reversible Markov chains\u2014and sequential Gibbs sampling is always a reversible chain\u2014for HOGWILD!-Gibbs sampling the asynchronicity and parallelism make the chain non-reversible. Because of this, we were unable to use these spectral results in our asynchronous setting. We are forced to rely on the other method Guruswami (2000) for analyzing Markov processes, coupling\u2014the type of analysis used with the Dobrushin condition\u2014which we will describe in the following sections. 3 Modeling Asynchronicity In this section, we describe a statistical model for asynchronous Gibbs sampling by adapting the hardware model outlined in De Sa et al. (2015a). Because we are motivated by the factor graph inference problem, we will focus on the case where the distribution \u03c0 that we want to sample comes from a sparse, discrete graphical model.", "startOffset": 11, "endOffset": 1685}, {"referenceID": 21, "context": "In this model, the \u03c4 parameter represents everything that is relevant about the hardware; representing the hardware in this way has been successful for the analysis of asynchronous SGD Niu et al. (2011), so it is reasonable to use it for Gibbs sampling.", "startOffset": 185, "endOffset": 203}, {"referenceID": 24, "context": "In many practical systems Neubig (2014); Shin et al. (2015), Gibbs sampling is used without a proof that it works; instead, it is naively run for some fixed number of passes through the dataset.", "startOffset": 41, "endOffset": 60}, {"referenceID": 26, "context": "in practice Smyth et al. (2009); Smola & Narayanamurthy (2010).", "startOffset": 12, "endOffset": 32}, {"referenceID": 26, "context": "in practice Smyth et al. (2009); Smola & Narayanamurthy (2010). To further test this, we ran HOGWILD!-Gibbs sampling on a real-world 11 GB Knowledge Base Population dataset (derived from the TAC-KBP challenge) using a machine with a single-socket, 18-core Xeon E7-8890 CPU and 1 TB RAM.", "startOffset": 12, "endOffset": 63}, {"referenceID": 11, "context": "1 Statements of Lemmas First, we state a proposition from Levin et al. (2009). This proposition relates the concept of a coupling with the total variation distance between the distributions of two random variables.", "startOffset": 58, "endOffset": 78}, {"referenceID": 11, "context": "1 Statements of Lemmas First, we state a proposition from Levin et al. (2009). This proposition relates the concept of a coupling with the total variation distance between the distributions of two random variables. Proposition 1 (Proposition 4.7 from Levin et al. (2009)).", "startOffset": 58, "endOffset": 271}, {"referenceID": 11, "context": "3 from Levin et al. (2009), max \u03bc0 \u2016\u03bct \u2212 \u03c0\u2016TV \u2264 P (Tcoupling > t) .", "startOffset": 7, "endOffset": 27}], "year": 2015, "abstractText": "Gibbs sampling is a Markov Chain Monte Carlo technique commonly used for estimating marginal distributions. To speed up Gibbs sampling, there has recently been interest in parallelizing it by executing asynchronously. While empirical results suggest that many models can be efficiently sampled asynchronously, traditional Markov chain analysis does not apply to the asynchronous case, and thus asynchronous Gibbs sampling is poorly understood. In this paper, we derive a better understanding of the two main challenges of asynchronous Gibbs: sampling bias and mixing time. We show experimentally that our theoretical results match practical outcomes.", "creator": "gnuplot 5.0 patchlevel 0"}}}