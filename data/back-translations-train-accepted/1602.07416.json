{"id": "1602.07416", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2016", "title": "Learning to Generate with Memory", "abstract": "Memory units have been widely used to enrich the capabilities of deep networks on capturing long-term dependencies in reasoning and prediction tasks, but little investigation exists on deep generative models (DGMs) which are good at inferring high-level invariant representations from unlabeled data. This paper presents a deep generative model with a possibly large external memory and an attention mechanism to capture the local detail information that is often lost in the bottom-up abstraction process in representation learning. By adopting a smooth attention model, the whole network is trained end-to-end by optimizing a variational bound of data likelihood via auto-encoding variational Bayesian methods, where an asymmetric recognition network is learnt jointly to infer high-level invariant representations. The asymmetric architecture can reduce the competition between bottom-up invariant feature extraction and top-down generation of instance details. Our experiments on several datasets demonstrate that memory can significantly boost the performance of DGMs and even achieve state-of-the-art results on various tasks, including density estimation, image generation, and missing value imputation.", "histories": [["v1", "Wed, 24 Feb 2016 06:57:14 GMT  (722kb,D)", "http://arxiv.org/abs/1602.07416v1", null], ["v2", "Sat, 28 May 2016 03:41:27 GMT  (943kb,D)", "http://arxiv.org/abs/1602.07416v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["chongxuan li", "jun zhu", "bo zhang"], "accepted": true, "id": "1602.07416"}, "pdf": {"name": "1602.07416.pdf", "metadata": {"source": "META", "title": "Learning to Generate with Memory", "authors": ["Chongxuan Li", "Jun Zhu", "Bo Zhang"], "emails": ["LICX14@MAILS.TSINGHUA.EDU.CN", "DCSZJ@TSINGHUA.EDU.CN", "DCSZB@TSINGHUA.EDU.CN"], "sections": [{"heading": "1. Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2. Related Work", "text": "In fact, it is not that it is an attempt to collect information that is not able to identify itself, and which therefore expands the ability to traditional learning models. Interaction, such as reading and writing on memory, will be characterized by an associated attention mechanism and the entire system. Attention that one receives can be differentiated and extends to the way in which the interaction with memory takes place. Interaction, reading and writing on memory, is characterized by an associated attention mechanism that is trained with supervision."}, {"heading": "3. Probabilistic DGMs with Memory", "text": "We present a probabilistic Deep Generative Model (DGM) with potentially large external memory and a soft attention mechanism."}, {"heading": "3.1. Overall Architecture", "text": "Formally speaking, we assume that each x-plane is generated independently with a set of hierarchically organized latent factors zL,.., z1 as follows. \u2022 For l = L \u2212 1,.., 0, calculate the mean parameters \u00b5l = gl (zl + 1; Ml) and draw the factors zl \u0445 N (0, I), each gl being a nonlinear function, often assumed to be smooth for the ease of learning. To connect with observations, the bottom layer is sandwiched in at z0 = x. Each zl is randomly sampled from a Gaussian distribution, except z0, whose distribution depends on the properties of the data (e.g. Gaussian for continuous data or Multinomial for discrete layers)."}, {"heading": "3.2. General Memory Mechanism for a Single Layer", "text": "We now present a single layer with memory in general, which is our building block for the above-mentioned DGM. For notation simplicity, we leave the sub-script l in the following text. Formally, let hin denote the input information, and hout denote the output after some deterministic transformation with memory. In our model, hin can be either the samples of latent factors or the output from a higher-level deterministic layer; and similarly hout can be used as input either of a stochastic layer or a lower-level deterministic layer. A layer of standard DGMs without memory generates the low-level generative information hg through a proper transformation, which can generally be set as: hg = p; Wg, bg are the weights and distortions of the transformation respectively, and uses it as the final output, i.e. hout = hg hg.In our DGM with memory M, we first calculate the low-threshold information hg, as an evilla layer."}, {"heading": "3.3. A Concrete Example with Hierarchical Memory Mechanisms", "text": "Using the above building blocks, we can stack several layers to form a DGM as in Section 3.1. (For simplicity, here we are looking at a generative model with only one stochastic layer and I deterministic layers to explain our storage mechanism, which can easily be extended to cases with multiple stochastic layers. (Let's leave most of the information as random samples from the previous one, i.e., h (I + 1) = z. Using permutation invariant architecture as an example, we calculate the low generative information h (i) g based on the input h (i + 1) as: h (i) g = p \u2212 p (W (i) g (i) g).We retrieve the knowledge h (i) m from memory. Although there are different strategies, we are looking at the simple one that assumes a linear combination of slots in memory ash (i)."}, {"heading": "4. Inference and Learning", "text": "In order to develop a variative approximation method, it is important to have a rich family of variable distributions that can indeed well characterize nonlinear transformations. In this section, we will develop such an algorithm for our DGM with Memory.Let us put the recording of parameters in the DGM as a model of the common distribution of all data x and the corresponding latent factor into a facialized form: p (x, z; empirical g) p (x; empirical) p (x; empirical) p (z; empirical) p (x; empirical) p (x) p (empirical) (empirical) (empirical) (empirical) (empirical) (empirical) (empirical) (empirical) (empirical) (empirical) (empirical (empirical) (empirical) (empirical) (empirical) (empirical) (empirical (empirical) (empirical) (empirical) (empirical (empirical) (empirical) (empirical) (empirical (empirical) (empirical) (empirical (empirical) (empirical) (empirical) (empirical (empirical) (empirical) (empirical (empirical) (empirical) (empirical) (empirical) (empirical) (empirical) (empirical) (empirical) (empirical (empirical) (empirical) (empirical) (empirical) (empirical) (empirical) (empirical) ((empirical) (empirical) (empirical) (empirical) (empirical) (empirical) (empirical) ((empirical) (empirical) (empirical"}, {"heading": "5. Experiments", "text": "We present both quantitative and qualitative evaluations of our method on the real evaluation level MNIST = 0,2,000, OCR letters and Frey face datasets for various tasks. MNIST datasets (Lecun et al., 1998) consist of 50,000 training, 10,000 validation and 10,000 test images of handwritten digits and each image is of 28 x 28 pixels. OCRS letter datasets consist of 32,152 training, 10,000 validation and 10,000 test letters of size 16 x 8 pixels. Frey face datasets consist of 1,965 real face expression images of size 28 x 20 pixels. We model MNIST and OCR letter datasets as Bernoulli distribution and model Frey face dataset as data set on the data level. We rely on Theano (Bastien et al al al al al, al., 2012, 2.000) We use BADMA as data set (2.000)."}, {"heading": "5.1. Density Estimation", "text": "We follow (Burda et al., 2015) to divide the MNIST dataset into 60,000 training data and 10,000 test data after selecting the hyperparameters. We train both the baseline and our models with 1-, 5- and 50-importance samples and evaluate the test probability with 5,000-importance samples as in (Burda et al., 2015). In each training period, we stochastically binarize the data as input. The results of VAE, IWAE-5 (trained with 5-importance samples) and IWAE-50 (trained with 50-importance samples) with a stochastic layer in (Burda et al., 2015) are -86.76, -85.5, and -84.78 Nats, respectively. However, we use 500 hidden units in the deterministic layers and 100 latent variables in the stochastic layer to achieve a stronger baseline result with a different architecture and more parameters."}, {"heading": "5.2. Analysis of Our Model", "text": "We are dealing with a number of factors that we do not yet know whether they are capable of changing the world or not."}, {"heading": "5.3. Random Generation", "text": "We continue to evaluate random generations from the baseline, and our model empirically evaluates MNIST, and Frey provides datasets shown in Fig. 5 and Fig. 6, respectively. We label unclear or meaningless images with red rectangles. This is done by majority voting of multiple volunteers. We do not select images for both datasets. For the MNIST dataset, the setting is the same as in Sec. 5.1. We observe that the storage mechanism does much to obtain clear and meaningful samples, as in Fig. 5.For Frey Faces dataset, we randomly divide ourselves into 1,865 training data and 100 test data. We use a single deterministic layer with 200 hidden units and a stochastic layer with 10 latent factors, and set n (1) s to be 20, as the number of training samples is small. We use a sample of the detection model in both areas of the training and testing process."}, {"heading": "5.4. Missing Value Imputation", "text": "Finally, we evaluate our method for the task of missing value imputation with three different types of noise, including (1) RECT-12 means that a centered rectangle of size 12 \u00d7 12 is missing; (2) RAND-0.6 means that each pixel is missing with a given probability of 0.6; and (3) HALF means that the left half of the image is missing. For both UAE and MEM-UAE, the missing values are randomly initialized and then derived by a Markov chain, which examines latent factors based on the current presumption of missing values and then refines the missing values based on the current latent factors. We compare the mean square errors (MSE) after 100 epochs of inference as in Tab. 3 on the MNIST dataset. Results show that DGM with external memory can better capture the underlying data structures than vanilla methods under different types of noise."}, {"heading": "6. Conclusions and Future Work", "text": "In this paper, we present a novel building block for deep generative models (DGMs) with an external memory and a related mechanism of soft attention. In the generative process from top to bottom, the additional memory helps restore the detailed local information that is often lost in bottom-up abstraction for learning invariant representations. Various experiments on handwritten digits and letters, as well as real face data, show that our method can significantly improve vanilla DGM in terms of density estimation, random generation and missing value assignment tasks, and that we can achieve state-of-the-art results among a broad family of benchmarks. There are two possible extensions to our method: \u2022 A class-based DGM (Kingma et al., 2014) with memory can potentially achieve better performance both in classification and generation, because the external memory helps to spread the competition between invariant feature extraction and detailed generation across networks, making it easier to train the entire information system and the exponential characteristics."}], "references": [{"title": "Learning the structure of deep sparse graphical models", "author": ["R. Adams", "H. Wallach", "Z. Ghahramani"], "venue": "In AISTATS,", "citeRegEx": "Adams et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Adams et al\\.", "year": 2010}, {"title": "Multiple object recognition with visual attention", "author": ["J.L. Ba", "V. Mnih", "K. Kavukcuoglu"], "venue": "In ICLR,", "citeRegEx": "Ba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I. Goodfellow", "A. Bergeron", "N. Bouchard", "D. Warde-Farley", "Y. Bengio"], "venue": "In Deep Learning and Unsupervised Feature Learning NIPS Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Generalized denoising autoencoders as generative models", "author": ["Y. Bengio", "L. Yao", "G. Alain", "P. Vincent"], "venue": "In NIPS,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Deep generative stochastic networks trainable by backprop", "author": ["Y. Bengio", "E. Thlhodeau-Laufer", "G. Alain", "J. Yosinski"], "venue": "In ICML,", "citeRegEx": "Bengio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2014}, {"title": "Reweighted wake-sleep", "author": ["J. Bornschein", "Y. Bengio"], "venue": "In ICLR,", "citeRegEx": "Bornschein and Bengio,? \\Q2015\\E", "shortCiteRegEx": "Bornschein and Bengio", "year": 2015}, {"title": "Importance weighted autoencoders", "author": ["Y. Burda", "R. Grosse", "R. Salakhutdinov"], "venue": "In arXiv:1509.00519,", "citeRegEx": "Burda et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Burda et al\\.", "year": 2015}, {"title": "The Laplacian pyramid as a compact image code", "author": ["P.J. Burt", "E.H. Adelson"], "venue": "IEEE Transactions on Communications,", "citeRegEx": "Burt and Adelson,? \\Q1983\\E", "shortCiteRegEx": "Burt and Adelson", "year": 1983}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["E. Denton", "S. Chintala", "A. Szlam", "R. Fergus"], "venue": "NPIS,", "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Learning deep sigmoid belief networks with data augmentation", "author": ["Z. Gan", "R. Henao", "D.E. Carlson", "L. Carin"], "venue": "In AISTATS,", "citeRegEx": "Gan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gan et al\\.", "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "In AISTATS,", "citeRegEx": "Glorot and Bengio,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio", "year": 2010}, {"title": "Generative adversarial nets", "author": ["I.J. Goodfellow", "J.P. Abadie", "M. Mirza", "B. Xu", "D.W. Farley", "S.ozair", "A. Courville", "Y. Bengio"], "venue": "In NIPS,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "In arXiv:1308.0850,", "citeRegEx": "Graves,? \\Q2013\\E", "shortCiteRegEx": "Graves", "year": 2013}, {"title": "Learning to transduce with unbounded memory", "author": ["E. Grefenstette", "K.M. Hermann", "M. Suleyman", "P. Blunsom"], "venue": "In NIPS,", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Deep autoregressive networks", "author": ["K. Gregor", "I. Danihelka", "A. Mnih", "C. Blundell", "D. Wierstra"], "venue": "In ICML,", "citeRegEx": "Gregor et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2014}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D.J. Rezende", "D. Wierstra"], "venue": "In ICML,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y. Teh"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "In ICML,", "citeRegEx": "Ioffe and Szegedy,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J.L. Ba"], "venue": "In ICLR,", "citeRegEx": "Kingma and Ba,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba", "year": 2015}, {"title": "Auto-encoding variational Bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "In ICLR,", "citeRegEx": "Kingma and Welling,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling", "year": 2014}, {"title": "Semi-supervised learning with deep generative models", "author": ["D.P. Kingma", "D.J. Rezende", "S. Mohamed", "M. Welling"], "venue": "In NIPS,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Learning to combine foveal glimpses with a third-order Boltzmann machine", "author": ["H. Larochelle", "G. Hinton"], "venue": "In NIPS,", "citeRegEx": "Larochelle and Hinton,? \\Q2010\\E", "shortCiteRegEx": "Larochelle and Hinton", "year": 2010}, {"title": "The neural autoregressive distribution estimator", "author": ["H. Larochelle", "I. Murray"], "venue": "In AISTATS,", "citeRegEx": "Larochelle and Murray,? \\Q2011\\E", "shortCiteRegEx": "Larochelle and Murray", "year": 2011}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "Lecun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Lecun et al\\.", "year": 1998}, {"title": "Max-margin deep generative models", "author": ["C. Li", "J. Zhu", "T. Shi", "B. Zhang"], "venue": "In NIPS,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Recurrent models of visual attention", "author": ["V. Mnih", "N. Heess", "A. Graves", "K. Kavukcuoglu"], "venue": "In NIPS,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Evaluating probabilities under high-dimensional latent variable models", "author": ["I. Murray", "R. Salakhutdinov"], "venue": "In NIPS,", "citeRegEx": "Murray and Salakhutdinov,? \\Q2009\\E", "shortCiteRegEx": "Murray and Salakhutdinov", "year": 2009}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In ICML,", "citeRegEx": "Nair and Hinton,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton", "year": 2010}, {"title": "Connectionist learning of belief networks", "author": ["R.M. Neal"], "venue": "In Artificial intelligence,", "citeRegEx": "Neal,? \\Q1992\\E", "shortCiteRegEx": "Neal", "year": 1992}, {"title": "Semi-supervised learning with ladder networks", "author": ["A. Rasmus", "M. Berglund", "M. Honkala", "H. Valpola", "T. Raiko"], "venue": "In NIPS,", "citeRegEx": "Rasmus et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rasmus et al\\.", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "In ICML,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Learning generative models with visual attention", "author": ["Y. Tang", "N. Srivastava", "R. Salakhutdinov"], "venue": "In NIPS,", "citeRegEx": "Tang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2014}, {"title": "A note on the evaluation of generative models", "author": ["L. Theis", "A. Oord", "M. Bethge"], "venue": "In arXiv:1511.01844,", "citeRegEx": "Theis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2016}, {"title": "From neural PCA to deep unsupervised learning", "author": ["H. Valpola"], "venue": "In arXiv:1411.7783,", "citeRegEx": "Valpola,? \\Q2014\\E", "shortCiteRegEx": "Valpola", "year": 2014}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P. Manzagol"], "venue": "JMLR, 11:3371C340,", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J.L. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Reinforcement learning neural Turing machines", "author": ["W. Zaremba", "I. Sutskever"], "venue": "In arXiv:1505.00521,", "citeRegEx": "Zaremba and Sutskever,? \\Q2015\\E", "shortCiteRegEx": "Zaremba and Sutskever", "year": 2015}], "referenceMentions": [{"referenceID": 30, "context": "Depending on the building blocks, various types of DGMs exist, including undirected models (Salakhutdinov & Hinton, 2009), directed models (Neal, 1992; Hinton et al., 2006), autoregressive models (Larochelle & Murray, 2011; Gregor et al.", "startOffset": 139, "endOffset": 172}, {"referenceID": 18, "context": "Depending on the building blocks, various types of DGMs exist, including undirected models (Salakhutdinov & Hinton, 2009), directed models (Neal, 1992; Hinton et al., 2006), autoregressive models (Larochelle & Murray, 2011; Gregor et al.", "startOffset": 139, "endOffset": 172}, {"referenceID": 16, "context": ", 2006), autoregressive models (Larochelle & Murray, 2011; Gregor et al., 2014), and Markov chain based models (Bengio et al.", "startOffset": 31, "endOffset": 79}, {"referenceID": 6, "context": ", 2014), and Markov chain based models (Bengio et al., 2014).", "startOffset": 39, "endOffset": 60}, {"referenceID": 32, "context": "Recently, DGMs have attracted much attention on developing efficient and (approximately) accurate learning algorithms, such as stochastic variational methods (Kingma & Welling, 2014; Rezende et al., 2014; Bornschein & Bengio, 2015; Burda et al., 2015) and Monte Carlo methods (Adams et al.", "startOffset": 158, "endOffset": 251}, {"referenceID": 8, "context": "Recently, DGMs have attracted much attention on developing efficient and (approximately) accurate learning algorithms, such as stochastic variational methods (Kingma & Welling, 2014; Rezende et al., 2014; Bornschein & Bengio, 2015; Burda et al., 2015) and Monte Carlo methods (Adams et al.", "startOffset": 158, "endOffset": 251}, {"referenceID": 0, "context": ", 2015) and Monte Carlo methods (Adams et al., 2010; Gan et al., 2015).", "startOffset": 32, "endOffset": 70}, {"referenceID": 11, "context": ", 2015) and Monte Carlo methods (Adams et al., 2010; Gan et al., 2015).", "startOffset": 32, "endOffset": 70}, {"referenceID": 26, "context": "This bottom-up abstraction progress is good for identifying predictive patterns, especially when a discriminative objective is optimized (Li et al., 2015); but it also loses the detail information that is necessary in the top-down generating process.", "startOffset": 137, "endOffset": 154}, {"referenceID": 17, "context": "For example, DRAW (Gregor et al., 2015) iteratively constructs complex images over time through a recurrent encoder and decoder together with an attention mechanism and LAPGAN (Denton et al.", "startOffset": 18, "endOffset": 39}, {"referenceID": 10, "context": ", 2015) iteratively constructs complex images over time through a recurrent encoder and decoder together with an attention mechanism and LAPGAN (Denton et al., 2015) employs a cascade of generative adversarial networks (GANs) (Goodfellow et al.", "startOffset": 144, "endOffset": 165}, {"referenceID": 13, "context": ", 2015) employs a cascade of generative adversarial networks (GANs) (Goodfellow et al., 2014) to generate high quality natural images through a Laplacian pyramid framework (Burt & Adelson, 1983).", "startOffset": 68, "endOffset": 93}, {"referenceID": 8, "context": "Different from (Kingma & Welling, 2014; Burda et al., 2015), our recognition network is asymmetric to the generative network.", "startOffset": 15, "endOffset": 59}, {"referenceID": 15, "context": ", 2015) and neural language transduction (Grefenstette et al., 2015).", "startOffset": 41, "endOffset": 68}, {"referenceID": 1, "context": "In addition to the memory-based models mentioned above, attention mechanisms have been used in other deep models for various tasks, such as image classification (Larochelle & Hinton, 2010; Ba et al., 2015), object tracking (Mnih et al.", "startOffset": 161, "endOffset": 205}, {"referenceID": 27, "context": ", 2015), object tracking (Mnih et al., 2014), conditional caption generation (Xu et al.", "startOffset": 25, "endOffset": 44}, {"referenceID": 37, "context": ", 2014), conditional caption generation (Xu et al., 2015), machine translation (Bahdanau et al.", "startOffset": 40, "endOffset": 57}, {"referenceID": 2, "context": ", 2015), machine translation (Bahdanau et al., 2015) and image generation (Graves, 2013; Gregor et al.", "startOffset": 29, "endOffset": 52}, {"referenceID": 14, "context": ", 2015) and image generation (Graves, 2013; Gregor et al., 2015).", "startOffset": 29, "endOffset": 64}, {"referenceID": 17, "context": ", 2015) and image generation (Graves, 2013; Gregor et al., 2015).", "startOffset": 29, "endOffset": 64}, {"referenceID": 17, "context": "Recently, DRAW (Gregor et al., 2015) introduces a novel 2-D attention mechanism to decide \u201cwhere to read and write\u201d on the image and does well in generating objects with clear track, such as handwritten digits and sequences of real digits.", "startOffset": 15, "endOffset": 36}, {"referenceID": 33, "context": "Compared with previous DGMs with visual attention (Tang et al., 2014; Gregor et al., 2015), we make different assumptions about the data, i.", "startOffset": 50, "endOffset": 90}, {"referenceID": 17, "context": "Compared with previous DGMs with visual attention (Tang et al., 2014; Gregor et al., 2015), we make different assumptions about the data, i.", "startOffset": 50, "endOffset": 90}, {"referenceID": 35, "context": "Similar idea is highlighted in the Ladder Network (Valpola, 2014; Rasmus et al., 2015), which reconstructs the input hierarchically using an extension of denoising autoencoders (dAEs) (Vincent et al.", "startOffset": 50, "endOffset": 86}, {"referenceID": 31, "context": "Similar idea is highlighted in the Ladder Network (Valpola, 2014; Rasmus et al., 2015), which reconstructs the input hierarchically using an extension of denoising autoencoders (dAEs) (Vincent et al.", "startOffset": 50, "endOffset": 86}, {"referenceID": 36, "context": ", 2015), which reconstructs the input hierarchically using an extension of denoising autoencoders (dAEs) (Vincent et al., 2010) with the help of lateral connections and achieves excellent performance on semi-supervised learning (Rasmus et al.", "startOffset": 105, "endOffset": 127}, {"referenceID": 31, "context": ", 2010) with the help of lateral connections and achieves excellent performance on semi-supervised learning (Rasmus et al., 2015).", "startOffset": 108, "endOffset": 129}, {"referenceID": 22, "context": "Our method can also be extended to do supervised or semisupervised learning as in (Kingma et al., 2014), which is our future work.", "startOffset": 82, "endOffset": 103}, {"referenceID": 2, "context": "We can view this as an unnormalized version of a soft attention mechanism (Bahdanau et al., 2015) trainable by standard back-propagation, which could be replaced by a hard attention mechanism trained with Reinforcement Learning (Xu et al.", "startOffset": 74, "endOffset": 97}, {"referenceID": 37, "context": ", 2015) trainable by standard back-propagation, which could be replaced by a hard attention mechanism trained with Reinforcement Learning (Xu et al., 2015).", "startOffset": 138, "endOffset": 155}, {"referenceID": 35, "context": "Inspired by the Ladder Network (Valpola, 2014; Rasmus et al., 2015), we specify the combination function of h m and h g as element wise multiple layer perceptron with optionally final nonlinearity \u03c6:", "startOffset": 31, "endOffset": 67}, {"referenceID": 31, "context": "Inspired by the Ladder Network (Valpola, 2014; Rasmus et al., 2015), we specify the combination function of h m and h g as element wise multiple layer perceptron with optionally final nonlinearity \u03c6:", "startOffset": 31, "endOffset": 67}, {"referenceID": 32, "context": "Significant progress has been made recently on stochastic variational inference methods with a sophisticated recognition model to parameterize the variational distributions (Kingma & Welling, 2014; Rezende et al., 2014).", "startOffset": 173, "endOffset": 219}, {"referenceID": 35, "context": "Note that we cannot send the massage of a intermediate layer in the recognition model to a layer in the generative model through a lateral connection as in Ladder Network (Valpola, 2014; Rasmus et al., 2015) because that indeed changes the distribution of p(x|z) according to the data x.", "startOffset": 171, "endOffset": 207}, {"referenceID": 31, "context": "Note that we cannot send the massage of a intermediate layer in the recognition model to a layer in the generative model through a lateral connection as in Ladder Network (Valpola, 2014; Rasmus et al., 2015) because that indeed changes the distribution of p(x|z) according to the data x.", "startOffset": 171, "endOffset": 207}, {"referenceID": 8, "context": "To compare with statof-the-art results, we also train our method as in importance weighted autoencoders (IWAE) (Burda et al., 2015), which uses importance weighting estimate of log likelihood with multiple samples in the training procedure to achieve a strictly tighter variational lower bound.", "startOffset": 111, "endOffset": 131}, {"referenceID": 25, "context": "The MNIST dataset (Lecun et al., 1998) consists of 50,000 training, 10,000 validation and 10,000 testing images of handwritten digits and each image is of 28 \u00d7 28 pixels.", "startOffset": 18, "endOffset": 38}, {"referenceID": 3, "context": "Our implementation is based on Theano (Bastien et al., 2012).", "startOffset": 38, "endOffset": 60}, {"referenceID": 31, "context": "This means that we initialize the output as signals from top-down inference, which is different from the Ladder Network (Rasmus et al., 2015).", "startOffset": 120, "endOffset": 141}, {"referenceID": 31, "context": "1 following Ladder Network (Rasmus et al., 2015).", "startOffset": 27, "endOffset": 48}, {"referenceID": 8, "context": "Our basic competitors are VAE (Kingma & Ba, 2015) and IWAE (Burda et al., 2015).", "startOffset": 59, "endOffset": 79}, {"referenceID": 8, "context": "We follow (Burda et al., 2015) to split the MNIST dataset into 60,000 training data and 10,000 testing data after choosing the hyper-parameters.", "startOffset": 10, "endOffset": 30}, {"referenceID": 8, "context": "We train both the baselines and our models with 1, 5 and 50 importance samples respectively and evaluate the test likelihood with 5,000 importance samples as in (Burda et al., 2015).", "startOffset": 161, "endOffset": 181}, {"referenceID": 8, "context": "The results of VAE, IWAE-5 (trained with 5 importance samples) and IWAE-50 (trained with 50 importance samples) with one stochastic layer in (Burda et al., 2015) are -86.", "startOffset": 141, "endOffset": 161}, {"referenceID": 8, "context": "Our method MEM-IWAE-50 even outperforms S2-IWAE-50, which is the best model in (Burda et al., 2015) with two stochastic layers and four deterministic layers.", "startOffset": 79, "endOffset": 99}, {"referenceID": 16, "context": "Again, our methods outperform the baseline approaches significantly and are comparable with the best competitors, which often employ autoregressive connections (Larochelle & Murray, 2011; Gregor et al., 2014) that are effective on small images with simple structures.", "startOffset": 160, "endOffset": 208}, {"referenceID": 8, "context": "Results are from [1] (Murray & Salakhutdinov, 2009), [2] (Burda et al., 2015), [3] (Bornschein & Bengio, 2015), [4] (Larochelle & Murray, 2011) and [5] (Gregor et al.", "startOffset": 57, "endOffset": 77}, {"referenceID": 16, "context": ", 2015), [3] (Bornschein & Bengio, 2015), [4] (Larochelle & Murray, 2011) and [5] (Gregor et al., 2014).", "startOffset": 82, "endOffset": 103}, {"referenceID": 34, "context": "We find that the minibatch size effects the results a lot, and the quality of visualization and the averaged test log density are inconsistent (Theis et al., 2016).", "startOffset": 143, "endOffset": 163}, {"referenceID": 22, "context": "\u2022 A class conditional DGM (Kingma et al., 2014) with memory can potentially achieve better performance on both classification and generation because the external memory helps to reduce the competition between the invariant feature extraction and detailed generation, and explicit label information can make the whole system be easier to train.", "startOffset": 26, "endOffset": 47}, {"referenceID": 10, "context": "\u2022 Our method can be further applied to convolutional neural networks by sharing parameters across different channels and then employed in non-probabilistic DGMs such as LAPGAN (Denton et al., 2015) to refine generation on high-dimensional data.", "startOffset": 176, "endOffset": 197}], "year": 2017, "abstractText": "Memory units have been widely used to enrich the capabilities of deep networks on capturing long-term dependencies in reasoning and prediction tasks, but little investigation exists on deep generative models (DGMs) which are good at inferring high-level invariant representations from unlabeled data. This paper presents a deep generative model with a possibly large external memory and an attention mechanism to capture the local detail information that is often lost in the bottom-up abstraction process in representation learning. By adopting a smooth attention model, the whole network is trained end-to-end by optimizing a variational bound of data likelihood via auto-encoding variational Bayesian methods, where an asymmetric recognition network is learnt jointly to infer high-level invariant representations. The asymmetric architecture can reduce the competition between bottom-up invariant feature extraction and top-down generation of instance details. Our experiments on several datasets demonstrate that memory can significantly boost the performance of DGMs on various tasks, including density estimation, image generation, and missing value imputation, and DGMs with memory can achieve state-ofthe-art quantitative results.", "creator": "LaTeX with hyperref package"}}}