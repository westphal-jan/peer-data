{"id": "1602.07507", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2016", "title": "A Bayesian Approach to the Data Description Problem", "abstract": "In this paper, we address the problem of data description using a Bayesian framework. The goal of data description is to draw a boundary around objects of a certain class of interest to discriminate that class from the rest of the feature space. Data description is also known as one-class learning and has a wide range of applications.", "histories": [["v1", "Wed, 24 Feb 2016 13:52:52 GMT  (1313kb,D)", "http://arxiv.org/abs/1602.07507v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["alireza ghasemi", "hamid r rabiee", "mohammad taghi manzuri", "mohammad hossein rohban"], "accepted": true, "id": "1602.07507"}, "pdf": {"name": "1602.07507.pdf", "metadata": {"source": "CRF", "title": "A Bayesian Approach to the Data Description Problem", "authors": ["Alireza Ghasemi", "Hamid R. Rabiee", "Mohammad T. Manzuri", "M. H. Rohban"], "emails": ["alireza.ghasemi@epfl.ch", "rabiee@sharif.edu", "manzuri@sharif.edu", "rahban@ce.sharif.edu"], "sections": [{"heading": "Introduction", "text": "In fact, it is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...). It is. (...) It is. It is. (...) It is. It is. (...) It is. (...). It is. (...) It is. (...). It is. It is. (...) It is. (...) It is. (...). It is. It is. (...) It is. (...). It is. (...). It is. It is. It is. (...). It is. It is. It is. (...). It is. It is. (...). It is. It is. (...). It is. It is. (...). It is. (...). It is. (...). It is. It is. It is. (...). It is. It is. It is. (...). It is. It is. It is. It is. (). It is. It is. It is. It is. It is. (). It is. (). It is. (). It is. (). It is. It is. It is. (). It is. It is. It is. (). It is."}, {"heading": "The Traditional Support Vector Data Description", "text": "This is a kernel-based approach that attempts to find a hypersphere that is as small as possible and, in the meantime, contains as much target data as possible, avoiding outlier examples. This goal is achieved by solving a convex optimization problem about the target data points in the kernel space. Provided we receive a dataset {x1,. xn} consisting of the training set, the main idea of the support vector data description is to find a hypersphere in the feature space that contains as many of the training samples as possible while keeping a minimum of possible volume. To achieve this goal, data is first transformed into a higher dimensional core space where the support of the data is a hypersphere."}, {"heading": "The Bayesian Approach", "text": "As we have seen in the previous section, the support vector data description algorithm is ultimately reduced to determining the center of the surrounding hypersphere in embedded space as a weighted mean of sample targets in which many of the weights are zero. Data points for which the corresponding weight is not zero are referred to as support vectors. In this section, we derive the proposed Bavarian data description method. We will look at the problem of data description from a different standpoint. Later, we will show that the interpretation of data and parameters in our model of SVDD. Our method is based on the same set of parameters as the SVDD (in its dual form), i.e. we will try to find a vector of weights, one for each data sample. We assume that we transform all data samples into a higher dimensional space using the mapping methods (kernel), in which transformed data of a Gaussian distribution with coance variance sequences."}, {"heading": "Utilizing Unlabelled Data", "text": "Methods that use unlabeled data to improve learning accuracy have received a lot of attention in recent years. These methods use unlabeled data to derive information about the geometry of the data and the associated diversity. Such information can be used to improve the accuracy of the monitored classifiers. In the Bayesian data description approach, information about the geometry of the data can be used to determine the previous probability distribution of the parameter vector. As we use local area density around points to determine the previous probability distribution of the model parameters, the information we collect from unlabeled samples can be useful to determine the local density one point more accurately. Once unlabeled data is available, we can now determine: mi = \u2212 \u2212 j L UKi, j (13), in which L and U are the set of described and unlabeled samples, the set of described and blank matrix data, which are different in each case, is another parameter that we can modify by using the unlabeled matrix."}, {"heading": "Time Complexity of the Bayesian Data Description", "text": "The construction of the previous vector m can take place at the time of construction of the core matrix and requires O (n2), the same as the minimum complexity of the core construction (in a general sense).The objective function of the BDD is a convex square programming problem that can be solved in O (n3) time. SVDD is also reduced to a square programming problem. Therefore, the time complexity of the BDD is no higher than SVDD.In the semi-supervised settings (SSDD) we have to calculate the inverse of the covariance matrix corresponding to the complexity O (((n + m) 3)) (m is the number of unlabeled samples)."}, {"heading": "Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Experiment Setup", "text": "Various data sets from the UCI repository (Asuncion and Newman 2007), as well as the image databases Corel (Wang, Li and Wiederhold 2001) and Caltech-101 (Fei-Fei, Fergus and Perona 2007) were used for experiments; their properties are shown in Table 1. In each experiment, one of the classes was selected as the target and all other samples were treated as outliers; half of the target samples were selected for training; the remainder of the training samples and outlier data were selected as test samples; for the Caltech 101 and Corel image sets, feature extraction was performed by the CEDD (Chatzichristofis and Boutalis 2008) feature extraction algorithm; SVDD method and the single-class Gaussian process were implemented and compared with the proposed BDD method; the Gaussian function was adjusted by a 10-fold cross-validation in contrast to the parameters of the classifiers and the kernel."}, {"heading": "Experiments", "text": "This year, it is only a matter of time before agreement is reached."}, {"heading": "Conclusions", "text": "In this paper, we proposed a novel Bayesian approach to the problem of data description, which has various applications in machine learning. Our approach is a bridge between probabilistic and kernel-based data description, and can therefore take advantage of both types of approaches, such as the low number of support vector approaches and the use of prior knowledge in the probabilistic approaches. In addition, our approach can use unlabeled data to improve the accuracy of data description.The prior knowledge used in our model can have various applications. For example, the information in the previous data samples can be used to estimate the most likely support vectors and reduce the size of the data set, thereby reducing the time complexity of the training. In addition, the robustness of the algorithm vis-\u00e0-vis noise could be further improved."}, {"heading": "Acknowledgment", "text": "The authors thank the AICTC Research Center and the VAS Laboratory of Sharif University of Technology. (a) Pendigits (b) Corel"}], "references": [{"title": "Cedd: Color and edge directivity descriptor: A compact descriptor for image indexing and retrieval", "author": ["Chatzichristofis", "Y. Boutalis"], "venue": "In Proceedings of the 6th international conference on Computer vision", "citeRegEx": "Chatzichristofis et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chatzichristofis et al\\.", "year": 2008}, {"title": "Novelty detection using one-class parzen density estimator. an application to surveillance of nosocomial infections", "author": ["Sax Cohen", "G. Geissbuhler 2008] Cohen", "H. Sax", "A. Geissbuhler"], "venue": "In EHealth Beyond the Horizon: Get It There: Proceedings of MIE2008 the XXIst International", "citeRegEx": "Cohen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2008}, {"title": "Manhattan world: Orientation and outlier detection by bayesian inference", "author": ["Coughlan", "J. Yuille 2003] Coughlan", "A. Yuille"], "venue": "Neural Computation", "citeRegEx": "Coughlan et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Coughlan et al\\.", "year": 2003}, {"title": "Bayesian method to detect outliers for ordinal data. Communications in StatisticsSimulation and Computation 39(7):1470\u20131484", "author": ["F. Dong"], "venue": null, "citeRegEx": "Dong,? \\Q2010\\E", "shortCiteRegEx": "Dong", "year": 2010}, {"title": "Learning classifiers from only positive and unlabeled data", "author": ["Elkan", "C. Noto 2008] Elkan", "K. Noto"], "venue": "In Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Elkan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Elkan et al\\.", "year": 2008}, {"title": "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. Computer Vision and Image Understanding 106(1):59\u201370", "author": ["Fergus Fei-Fei", "L. Perona 2007] Fei-Fei", "R. Fergus", "P. Perona"], "venue": null, "citeRegEx": "Fei.Fei et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Fei.Fei et al\\.", "year": 2007}, {"title": "Active and semi-supervised data domain description. Machine Learning and Knowledge Discovery in Databases 407\u2013422", "author": ["Kloft Grnitz", "N. Brefeld 2009] Grnitz", "M. Kloft", "U. Brefeld"], "venue": null, "citeRegEx": "Grnitz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Grnitz et al\\.", "year": 2009}, {"title": "One-class classification with gaussian processes", "author": ["Rodner Kemmler", "M. Denzler 2011] Kemmler", "E. Rodner", "J. Denzler"], "venue": "Computer Vision\u2013ACCV", "citeRegEx": "Kemmler et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kemmler et al\\.", "year": 2011}, {"title": "A survey of recent trends in one class classification", "author": ["Khan", "S. Madden 2010] Khan", "M. Madden"], "venue": "Artificial Intelligence and Cognitive Science", "citeRegEx": "Khan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Khan et al\\.", "year": 2010}, {"title": "Density-induced support vector data description", "author": ["Lee"], "venue": "Neural Networks, IEEE Transactions on 18(1):284\u2013289", "citeRegEx": "Lee,? \\Q2007\\E", "shortCiteRegEx": "Lee", "year": 2007}, {"title": "Bagging one-class decision trees", "author": ["Li", "C. Zhang 2008] Li", "Y. Zhang"], "venue": "In Fifth International Conference on Fuzzy Systems and Knowledge Discovery,", "citeRegEx": "Li et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Li et al\\.", "year": 2008}, {"title": "One-class support vector machines and density estimation: The precise relation. Progress in Pattern Recognition, Image Analysis and Applications 253\u2013274", "author": ["Munoz", "A. Moguerza 2004] Munoz", "J. Moguerza"], "venue": null, "citeRegEx": "Munoz et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Munoz et al\\.", "year": 2004}, {"title": "Novelty detection and 3d shape retrieval based on gaussian mixture models for autonomous surveillance robotics", "author": ["Nuez"], "venue": "In Intelligent Robots and Systems,", "citeRegEx": "Nuez,? \\Q2009\\E", "shortCiteRegEx": "Nuez", "year": 2009}, {"title": "Estimating the support of a high-dimensional distribution. Neural computation 13(7):1443\u20131471", "author": ["Schlkopf"], "venue": null, "citeRegEx": "Schlkopf,? \\Q2001\\E", "shortCiteRegEx": "Schlkopf", "year": 2001}, {"title": "Kernel methods for pattern analysis", "author": ["Shawe-Taylor", "N. Cristianini"], "venue": null, "citeRegEx": "Shawe.Taylor et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Shawe.Taylor et al\\.", "year": 2004}, {"title": "Support vector data description. Machine learning 54(1):45\u201366", "author": ["Tax", "D. Duin 2004] Tax", "R. Duin"], "venue": null, "citeRegEx": "Tax et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Tax et al\\.", "year": 2004}, {"title": "Automatic outlier detection: A bayesian approach", "author": ["D\u2019Souza Ting", "J. Schaal 2007] Ting", "A. D\u2019Souza", "S. Schaal"], "venue": "In Robotics and Automation,", "citeRegEx": "Ting et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ting et al\\.", "year": 2007}, {"title": "Simplicity: Semantics-sensitive integrated matching for picture libraries", "author": ["Li Wang", "J.Z. Wiederhold 2001] Wang", "J. Li", "G. Wiederhold"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "citeRegEx": "Wang et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2001}, {"title": "Brain activation detection by neighborhood one-class svm. Cognitive Systems Research 11(1):16\u201324", "author": ["Yang"], "venue": null, "citeRegEx": "Yang,? \\Q2010\\E", "shortCiteRegEx": "Yang", "year": 2010}, {"title": "Single-class classification with mapping convergence", "author": ["H. Yu"], "venue": null, "citeRegEx": "Yu,? \\Q2005\\E", "shortCiteRegEx": "Yu", "year": 2005}, {"title": "Learning from positive and unlabeled examples: A survey", "author": ["Zhang", "B. Zuo 2008] Zhang", "W. Zuo"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}, {"title": "Semi-supervised learning with graphs. Citeseer", "author": ["Lafferty Zhu", "X. Rosenfeld 2005] Zhu", "J. Lafferty", "R. Rosenfeld"], "venue": null, "citeRegEx": "Zhu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 19, "context": "(Zhang and Zuo 2008),(Yu 2005) and (Elkan and Noto 2008) have utilized unlabeled samples as well as positive target samples in the process of one-class learning.", "startOffset": 21, "endOffset": 30}, {"referenceID": 3, "context": "In (Dong 2010) a Bayesian approach has been used to detect outliers in ordinal data.", "startOffset": 3, "endOffset": 14}, {"referenceID": 19, "context": "In addition to SVDD, the mapping-convergence algorithm (Yu 2005) was also implemented and used in comparisons.", "startOffset": 55, "endOffset": 64}], "year": 2016, "abstractText": "In this paper, we address the problem of data description using a Bayesian framework. The goal of data description is to draw a boundary around objects of a certain class of interest to discriminate that class from the rest of the feature space. Data description is also known as one-class learning and has a wide range of applications. The proposed approach uses a Bayesian framework to precisely compute the class boundary and therefore can utilize domain information in form of prior knowledge in the framework. It can also operate in the kernel space and therefore recognize arbitrary boundary shapes. Moreover, the proposed method can utilize unlabeled data in order to improve accuracy of discrimination. We evaluate our method using various real-world datasets and compare it with other state of the art approaches of data description. Experiments show promising results and improved performance over other data description and one-class learning algorithms.", "creator": "LaTeX with hyperref package"}}}