{"id": "1602.07572", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2016", "title": "Ultradense Word Embeddings by Orthogonal Transformation", "abstract": "Embeddings are generic representations that are useful for many NLP tasks. In this paper, we introduce DENSIFIER, a method that learns an orthogonal transformation of the embedding space that focuses the information relevant for a task in an ultradense subspace of a dimensionality that is smaller by a factor of 100 than the original space. We show that ultradense embeddings generated by DENSIFIER reach state of the art on a lexicon creation task in which words are annotated with three types of lexical information - sentiment, concreteness and frequency. On the SemEval2015 10B sentiment analysis task we show that no information is lost when the ultradense subspace is used, but training is an order of magnitude more efficient due to the compactness of the ultradense space.", "histories": [["v1", "Wed, 24 Feb 2016 16:06:25 GMT  (54kb,D)", "http://arxiv.org/abs/1602.07572v1", null], ["v2", "Sun, 8 May 2016 08:50:11 GMT  (53kb,D)", "http://arxiv.org/abs/1602.07572v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sascha rothe", "sebastian ebert", "hinrich sch\u00fctze"], "accepted": true, "id": "1602.07572"}, "pdf": {"name": "1602.07572.pdf", "metadata": {"source": "CRF", "title": "Ultradense Word Embeddings by Orthogonal Transformation", "authors": ["Sascha Rothe", "Sebastian Ebert"], "emails": ["sascha@cis.lmu.de", "ebert@cis.lmu.de"], "sections": [{"heading": null, "text": "Embedding is a generic representation that is useful for many NLP tasks. In this article, we introduce DENSIFIER, a method that learns an orthogonal transformation of the embedding space that focuses the information relevant to a task in an ultra-traditional subspace of a dimensionality that is 100 times smaller than the original space. We show that ultra-traditional embedding generated by DENSIFIER reaches the state of the art in a lexicon creation task in which words are commented with three types of lexical information - feeling, concreteness, and frequency. In the SemEval2015 10B sentiment analysis task, we show that no information is lost when the ultra-traditional subspace is used, but training is an order of magnitude more efficient due to the compactness of the ultra-traditional space."}, {"heading": "1 Introduction", "text": "rf\u00fc ide rf\u00fc ide rf\u00fc rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the if\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the if\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the if\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc"}, {"heading": "2 Model", "text": "Let us consider Q-Rd \u00b7 d as an orthogonal matrix that transforms the original word that embeds space into a space in which certain types of information are represented by a small number of dimensions. Specifically, we learn Q in such a way that the dimensions D-Dc and Df correspond to the feeling information of a word and the remaining dimensions {1,.., d}\\ Ds correspond to the non-feeling information. Similarly, the dimensions Dc and Df correspond to the concreteness information and frequency information of a word. If wn-Rd is the original embedding of the word n, then the transformed representation is Qwn. We use \u0435 as a placeholder for s, c, and f and call d-Dc-D-density = | D-density of the ultradense subspace."}, {"heading": "2.1 Separating Words of Different Groups", "text": "We assume that we have a lexicon resource L in which each word w for a certain property is commented either as L (w) = + 1 (positive, concrete, frequent) or L (w) = \u2212 1 (negative, abstract, rare). Let M * diff be a set of word index pairs (m, n) for which L (wm) applies 6 = L (wn). Let's maximize: EN (m, n) and M * diff (n)."}, {"heading": "2.2 Aligning Words of the Same Group", "text": "Another goal is to minimize the distance between two words of the same group. Let M \u0445 be equal to a series of pairs of word indexes (m, n) for which L (wm) = L (wn) applies. Contrary to Eq.2, we now want to minimize each distance. Therefore, the goal is given by the following yardsticks: argmin Q \u2211 (m, n), n), m \u0445 equal, p \u0445 Q (wn \u2212 wm), where Q is an orthogonal matrix. The intuition behind the two goals is illustrated in Figure 1."}, {"heading": "2.3 Training", "text": "We combine the two targets in Eq.2 / 3 for each subspace, i.e. for tuning, concreteness and frequency, and weight them with \u03b1 * and 1 \u2212 \u03b1 *. This results in a hyperparameter \u03b1 * for each subspace. We then perform a stochastic gradient reduction (SGD). The lot size is 100 and the initial learning rate is 5 multiplied by.99 in each iteration."}, {"heading": "2.4 Orthogonalization", "text": "Each step of the SGD updates Q. The updated matrix Q \u2032 is usually no longer orthogonal, so we reorthogonalize Q \u2032 in each step on the basis of a singular value substitution: Q \u2032 = UPS T, where S is a diagonal matrix and U and V are orthogonal matrices. MatrixQ: = UV T is the next orthogonal matrix to Q \u2032 in both the 2 norm and the Frobenius norm (Fan and Hoffman, 1955).SGD for this problem is sensitive to the learning rate. If the learning rate is too high, there is a big jump, and the reorthogonized matrix Q is basically a random new point in the parameter space. If the learning rate is too small, learning can take a long time. We found that our training regime, starting with a high learning rate (5) and multiplying in each iteration with.99, is effectively less than five minutes of learning time (typically the cost is iQ)."}, {"heading": "3 Lexicon Creation", "text": "For lexicon creation, the input is a set of embeddings and a lexicon resource L = 2010 Czech resource L, in which words are annotated for a lexicon property as sentiment, concreteness or frequency. DENSIFIER is then trained to produce a one-dimensional ultradense subspace representation (which is simply a real number), an indicator of the word's strength for that property.The embeddings and lexicon resources used in this paper cover five languages and three domains (Table 1). Google News embeddings for English2 and the FrWac embeddings for French3 are publicly available. We use word2vec to train 400-dimensional embeddings for English on a 2013 Twitter corpus of size 5.4 109. For Czech, German and English, we train embeddings for French."}, {"heading": "4 Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Top-Ranked Words", "text": "Table 2 shows the top 10 positive / negative words (i.e. the most extreme values to dimension D) if we apply the transformation to the corpora Entwitter, EN-news and DE-web, and the top 10 concrete / abstract words (i.e. the most extreme values to dimension D) to EN-news. For EN-twitter (leftmost double column), the selected words look promising: they contain highly domain-specific words such as hashtags (e.g. # happy). This is surprising because the main result of the following frequency experiment is that even in a setup that is optimistic due to overlaps between training and test, the result is low; probably it would be even lower without overlapping. For the camera-enabled setup, we would use a disjointed testset.There is not a single hashtag in the lexicon resource WHM that DENSIFIER was trained on. The results for the other three double columns also show extreme examples of corresponding property and language."}, {"heading": "4.2 Quality of Predictions", "text": "Table 1 presents experimental results. In each case, we divide the resource into train / test, with the exception of Twitter, where we used the experimental data from SemEval2015 Task 10E for the test. We train DENSIFIER on the train and calculate Kendall's results in the test. The size of the lexicon resource does not have much influence; e.g., the results for Spanish (small resource; line 3) and French (large resource; line 4) are roughly the same. See Section 5 for a more detailed analysis of the impact of resource size. The quality of the lexicon strongly depends on the quality of the underlying word embedding; e.g., the results for French (small embedding of training corpus; line 4) are worse than the results for English (large embedding of training corpus; line 6), although the lexicon resources are of a comparable size. In contrast to sentiment / specificity, the frequency values for the frequency are low (line 8-9), which is not for the other languages (46-9)."}, {"heading": "4.3 Determining Association Strength", "text": "As before, the task is to predict the sentiment score of words / phrases. We use the test data of the task to set the hyperparameter \u03b1s =.4. Words from the vocabulary were predicted to be neutral (7 / 1315). Table 3 shows that the lexicon calculated by DENSIFIER (line 5, table 1) has a rating of.654 (line 6, column all), significantly better than all other systems, including the winner of SemEval 2015 (\u03c4 =.626, line 1). DENSIFIER also beats Sentiment140 (Mohammad et al., 2013), a widely used semi-automatic sentiment lexicon. The last column is located at the intersection of DENSIFIER and Sentiment140. It shows that DENSIFIER again performs significantly better than Sentiment140."}, {"heading": "4.4 Text Polarity Classification", "text": "In fact, most of us are able to surpass ourselves, \"he told the German Press Agency in an interview with\" Welt am Sonntag \":\" I don't think we're going to be able to change the world, and we're going to be able to change the world. \"He added:\" I don't think we're going to be able to change the world. \""}, {"heading": "5 Parameter Analysis", "text": "In this section, we will analyze two parameters: the size of the ultra-traditional subspace and the size of the lexicon resource. (We will leave an evaluation of another parameter, the size of the embedded training corpus, for future work, but empirical results suggest that this corpus should ideally be several billion tokens.Size of the subspace. Except for the two-dimensional polarity classification experiments, we suggest further improvements to our results by increasing the dimensionality of the subspace to values d > 1. The lexical resources we train and test are all binary, so if we use values d > 1, then we must assign the subspace embedding to a one-dimensional scale for evaluation."}, {"heading": "6 Related Work", "text": "In fact, there is much previous work on the post-editing of word embeddings. They show that their approach optimizes word embeddings for a particular application, i.e., word similarity, but also that it aggravates them for other applications such as the detection of syntactic relationships. Faruqui et al., \"It is not possible to explore them.\" Faruqui et al.,, It is also unable to detect the benefit of ultradense embeddings for a particular application, i.e., word similarity, but also that he uses them for other applications such as the detection of syntactic relationships., Faruqui et al., it is an approach that does not have the benefit of ultradense embeddings, in particular the benefit of increased efficiency. In a tensor framework, in which Rothe and Schatz (2015) transform the word embeddings of a language into meaning."}, {"heading": "7 Conclusion", "text": "We have introduced DENSIFIER, a method that is trained to focus embedding used for an application on an ultra-traditional subspace that contains the information relevant to the application. In experiments with SemEval, we show two advantages of ultra-tight subspace. (i) Information is retained even if we focus on a subspace that is 100 times smaller than the original space. This means that unnecessary noisy information is removed from embedding and robust learning without overfitting is better supported. (ii) Since the subspace is 100 times smaller, models that use embedding as an input representation can be trained more efficiently and have a much lower number of parameters. The subspace can be learned with only 80-300 training examples to achieve state-of-the-art results on encyclopedia concretion. We have shown in this paper that up to three orthogonal ultradense subspaces can be created."}], "references": [{"title": "Sandra Bringay", "author": ["Amine Abdaoui", "J\u00e9r\u00f4me Az\u00e9"], "venue": "and Pascal Poncelet.", "citeRegEx": "Abdaoui et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Silva", "author": ["Silvio Amir", "Ram\u00f3n Astudillo", "Wang Ling", "Bruno Martins", "Mario J"], "venue": "and Isabel Trancoso.", "citeRegEx": "Amir et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Amy Beth Warriner", "author": ["Marc Brysbaert"], "venue": "and Victor Kuperman.", "citeRegEx": "Brysbaert et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "2011", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Natural language processing (almost) from scratch. J. Mach. Learn. Res., 12:2493\u2013", "citeRegEx": "Collobert et al.2011", "shortCiteRegEx": null, "year": 2537}, {"title": "Elad Hazan", "author": ["John C. Duchi"], "venue": "and Yoram Singer.", "citeRegEx": "Duchi et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Ngoc Thang Vu", "author": ["Sebastian Ebert"], "venue": "and Hinrich Sch\u00fctze.", "citeRegEx": "Ebert et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Some metric inequalities in the space of matrices", "author": ["Fan", "Hoffman1955] Ky Fan", "Alan J Hoffman"], "venue": null, "citeRegEx": "Fan et al\\.,? \\Q1955\\E", "shortCiteRegEx": "Fan et al\\.", "year": 1955}, {"title": "and Noah A", "author": ["Manaal Faruqui", "Jesse Dodge", "Sujay Kumar Jauhar", "Chris Dyer", "Eduard Hovy"], "venue": "Smith.", "citeRegEx": "Faruqui et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Tom\u00e1\u0161 Pt\u00e1\u010dek", "author": ["Ivan Habernal"], "venue": "and Josef Steinberger.", "citeRegEx": "Habernal et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Patrice Bellot", "author": ["Hussam Hamdan"], "venue": "and Frederic Bechet.", "citeRegEx": "Hamdan et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Alexander Hogenboom", "author": ["Bas Heerschop"], "venue": "and Flavius Frasincar.", "citeRegEx": "Heerschop et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Mining and Summarizing Customer Reviews", "author": ["Hu", "Liu2004] Minqing Hu", "Bing Liu"], "venue": null, "citeRegEx": "Hu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2004}, {"title": "Edward Grefenstette", "author": ["Nal Kalchbrenner"], "venue": "and Phil Blunsom.", "citeRegEx": "Kalchbrenner et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional Neural Networks for Sentence Classification", "author": ["Yoon Kim"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Xiaodan Zhu", "author": ["Svetlana Kiritchenko"], "venue": "and Saif M Mohammad.", "citeRegEx": "Kiritchenko et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Antonio Torralba", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun"], "venue": "and Sanja Fidler.", "citeRegEx": "Kiros et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Quoc V Le", "author": ["Tomas Mikolov"], "venue": "and Ilya Sutskever.", "citeRegEx": "Mikolov et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Mohammad and Peter D", "author": ["M Saif"], "venue": "Turney.", "citeRegEx": "Mohammad and Turney2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Svetlana Kiritchenko", "author": ["Saif M. Mohammad"], "venue": "and Xiaodan Zhu.", "citeRegEx": "Mohammad et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Rectified Linear Units Improve Restricted Boltzmann Machines", "author": ["Nair", "Hinton2010] Vinod Nair", "Geoffrey E. Hinton"], "venue": "In Proceedings of ICML", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Alan Ritter", "author": ["Preslav Nakov", "Sara Rosenthal", "Zornitsa Kozareva", "Veselin Stoyanov"], "venue": "and Theresa Wilson.", "citeRegEx": "Nakov et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Clac-sentipipe: Semeval2015 subtasks 10 b,e, and task", "author": ["\u00d6zdemir", "Bergler2015] Canberk \u00d6zdemir", "Sabine Bergler"], "venue": "Proceedings of SemEval", "citeRegEx": "\u00d6zdemir et al\\.,? \\Q2015\\E", "shortCiteRegEx": "\u00d6zdemir et al\\.", "year": 2015}, {"title": "GloVe: Global Vectors for Word Representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Carmen Banea", "author": ["Ver\u00f3nica P\u00e9rez-Rosas"], "venue": "and Rada Mihalcea.", "citeRegEx": "P\u00e9rez.Rosas et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Natalie Dykes", "author": ["Nataliia Plotnikova", "Micha Kohl", "Kevin Volkert", "Stefan Evert", "Andreas Lerner"], "venue": "and Heiko Ermer.", "citeRegEx": "Plotnikova et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Alan Ritter", "author": ["Sara Rosenthal", "Preslav Nakov", "Svetlana Kiritchenko", "Saif M. Mohammad"], "venue": "and Veselin Stoyanov.", "citeRegEx": "Rosenthal et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Autoextend: Extending word embeddings to embeddings for synsets and lexemes", "author": ["Rothe", "Sch\u00fctze2015] Sascha Rothe", "Hinrich Sch\u00fctze"], "venue": "In Proceedings of ACL", "citeRegEx": "Rothe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rothe et al\\.", "year": 2015}, {"title": "Sentiment translation through lexicon induction", "author": ["Christian Scheible"], "venue": "ACL", "citeRegEx": "Scheible.,? \\Q2010\\E", "shortCiteRegEx": "Scheible.", "year": 2010}, {"title": "UNITN: Training Deep Convolutional Neural Network for Twitter Sentiment Classification", "author": ["Severyn", "Moschitti2015] Aliaksei Severyn", "Alessandro Moschitti"], "venue": "In Proceedings of SemEval", "citeRegEx": "Severyn et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Severyn et al\\.", "year": 2015}, {"title": "Building large-scale twitter-specific sentiment lexicon : A representation learning approach", "author": ["Tang et al.2014a] Duyu Tang", "Furu Wei", "Bing Qin", "Ming Zhou", "Ting Liu"], "venue": "Proceedings of COLING", "citeRegEx": "Tang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2014}, {"title": "Learning Sentiment-Specific Word Embedding for Twitter Sentiment Classification", "author": ["Tang et al.2014b] Duyu Tang", "Furu Wei", "Nan Yang", "Ming Zhou", "Ting Liu", "Bing Qin"], "venue": "Proceedings of ACL", "citeRegEx": "Tang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2014}, {"title": "Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews", "author": ["Peter D. Turney"], "venue": "In Proceedings of ACL", "citeRegEx": "Turney.,? \\Q2002\\E", "shortCiteRegEx": "Turney.", "year": 2002}, {"title": "Janyce Wiebe", "author": ["Theresa Wilson"], "venue": "and Paul Hoffmann.", "citeRegEx": "Wilson et al.2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Chao Liu", "author": ["Chao Xing", "Dong Wang"], "venue": "and Yiye Lin.", "citeRegEx": "Xing et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning composition models for phrase embeddings. Transactions of the Association for Computational Linguistics, 3:227\u2013242", "author": ["Yu", "Dredze2015] Mo Yu", "Mark Dredze"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Guoshun Wu", "author": ["Zhihua Zhang"], "venue": "and Man Lan.", "citeRegEx": "Zhang et al.2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [], "year": 2017, "abstractText": "Embeddings are generic representations that are useful for many NLP tasks. In this paper, we introduce DENSIFIER, a method that learns an orthogonal transformation of the embedding space that focuses the information relevant for a task in an ultradense subspace of a dimensionality that is smaller by a factor of 100 than the original space. We show that ultradense embeddings generated by DENSIFIER reach state of the art on a lexicon creation task in which words are annotated with three types of lexical information \u2013 sentiment, concreteness and frequency. On the SemEval2015 10B sentiment analysis task we show that no information is lost when the ultradense subspace is used, but training is an order of magnitude more efficient due to the compactness of the ultradense space.", "creator": "LaTeX with hyperref package"}}}